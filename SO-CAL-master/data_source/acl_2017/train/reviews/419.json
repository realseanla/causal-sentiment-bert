{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper introduces a simple and effective method for morphological paradigm\ncompletion in low-resource settings. The method uses a character-based seq2seq\nmodel trained on a mix of examples in two languages: a resource-poor language\nand a closely-related resource-rich language; each training example is\nannotated with a paradigm properties and a language ID. Thus, the model enables\ntransfer learning across languages when the two languages share common\ncharacters and common paradigms. While the proposed multi-lingual solution is\nnot novel (similar architectures have been explored in syntax, language\nmodeling, and MT), the novelty of this paper is to apply the approach to\nmorphology. Experimental results show substantial improvements over monolingual\nbaselines, and include a very thorough analysis of the impact of language\nsimilarities on the quality of results. The paper is interesting, very clearly\nwritten, I think it\u2019ll be a nice contribution to the conference program. \n\nDetailed comments: \n\n\u2014 My main question is why the proposed general multilingual methodology was\nlimited to pairs of languages, rather than to sets of similar languages? For\nexample, all Romance languages could be included in the training to improve\nSpanish paradigm completion, and all Slavic languages with Cyrillic script\ncould be mixed to improve Ukrainian. It would be interesting to see the\nextension of the models from bi-lingual to multilingual settings. \n\n\u2014 I think Arabic is not a fair (and fairly meaningless) baseline, given how\ndifferent is its script and morphology from the target languages. A more\ninteresting baseline would be, e.g., a language with a partially shared\nalphabet but a different typology. For example, a Slavic language with Latin\nscript could be used as a baseline language for Romance languages. If Arabic is\nexcluded, and if we consider a most distant language in the same the same\nfamily as a baseline, experimental results are still strong. \n\n\u2014 A half-page discussion of contribution of Arabic as a regularizer also adds\nlittle to the paper; I\u2019d just remove Arabic from all the experiments and\nwould add a regularizer (which, according to footnote 5, works even better than\nadding Arabic as a transfer language).              \n\n\u2014 Related work is missing a line of work on \u201clanguage-universal\u201d RNN\nmodels that use basically the same approach: they learn shared parameters for\ninputs in multiple languages, and add a language tag to the input to mediate\nbetween languages. Related studies include a multilingual parser (Ammar et al.,\n2016), language models (Tsvetkov et al., 2016), and machine translation\n(Johnson et al., 2016 )\n\nMinor: \n\u2014 I don\u2019t think that the claim is correct in line 144 that POS tags are\neasy to transfer across languages. Transfer of POS annotations is also a\nchallenging task.  \n\nReferences: \n\nWaleed              Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah\nA.\nSmith. \"Many languages, one parser.\u201d TACL 2016. \n\nYulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick\nLittell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. \"Polyglot\nneural language models: A case study in cross-lingual phonetic representation\nlearning.\u201d NAACL 2016.\n\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng\nChen, Nikhil Thorat et al. \"Google's Multilingual Neural Machine Translation\nSystem: Enabling Zero-Shot Translation.\" arXiv preprint arXiv:1611.04558 2016.\n\n-- Response to author response: \n\nThanks for your response & I'm looking forward to reading the final version!", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.", "histories": [], "id": 419, "title": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion"}
