{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to an artist whilst being\nfluent and co-herent. The paper is well written and the motivation for the\nmetrics are well explained.  \n\nThe authors describe both hand annotated metrics (fluency, co-herence and\nmatch) and an automatic metric for \u2018Similarity'. Whilst the metric for\nSimilarity is unique and interesting the paper does not give any evidence of\nthis as an effective automatic metric as correlations between this metric and\nthe others are low, (which they say that they should be used separately). The\nauthors claim it can be used to meaningfully analyse system performance but we\nhave to take their word for it as again there is no correlation with any\nhand-annotated performance metric.  Getting worse scores than a baseline system\nisn\u2019t evidence that the metric captures quality (e.g. you could have a very\nstrong baseline).\n\nSome missing references, e.g. recent work looking at automating co-herence,\ne.g. using mutual information density (e.g. Li et al. 2015). In addition, some\nreference to style matching from the NLG community are missing (e.g. Dethlefs\net al. 2014 and the style matching work by Pennebaker).", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper studies how to properly evaluate systems that produce ghostwriting\nof rap lyrics.\nThe authors present manual evaluation along three key aspects: fluency,\ncoherence, and style matching.\nThey also introduce automatic metrics that consider uniqueness via maximum\ntraining similarity, and stylistic similarity via rhyme density.\n\nI can find some interesting analysis and discussion in the paper.\nThe way for manually evaluating style matching especially makes sense to me.\n\nThere also exist a few important concerns for me.\n\nI am not convinced about the appropriateness of only doing fluency/coherence\nratings at line level.\nThe authors mention that they are following Wu (2014), but I find that work\nactually studying a different setting of hip hop lyrical challenges and\nresponses, which should be treated at line level in nature.\nWhile in this work, a full verse consists of multiple lines that normally\nshould be topically and structurally coherent.\nCurrently I cannot see any reason why not to evaluate fluency/coherence for a\nverse as a whole.\n\nAlso, I do not reckon that one should count so much on automatic metrics, if\nthe main goal is to ``generate similar yet unique lyrics''.\nFor uniqueness evaluation, the calculations are performed on verse level.\nHowever, many rappers may only produce lyrics within only a few specific topics\nor themes.\nIf a system can only extract lines from different verses, presumably we might\nalso get a fluent, coherent verse with low verse level similarity score, but we\ncan hardly claim that the system ``generalizes'' well.\nFor stylistic similarity with the specified artist, I do not think rhyme\ndensity can say it all, as it is position independent and therefore may not be\nenough to reflect the full information of style of an artist.\n\nIt does not seem that the automatic metrics have been verified to be well\ncorrelated with corresponding real manual ratings on uniqueness or stylistic\nmatching.\nI also wonder if one needs to evaluate semantic information commonly expressed\nby a specified rapper as well, other than only caring about rhythm.\n\nMeanwhile, I understand the motivation for this study is the lack of *sound*\nevaluation methodology.\nHowever, I still find one statement particularly weird:\n``our methodology produces a continuous numeric score for the whole verse,\nenabling better comparison.''\nIs enabling comparisons really more important than making slightly vague but\nmore reliable, more convincing judgements?\n\nMinor issue:\nIncorrect quotation marks in Line 389", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposes to present a more comprehensive evaluation methodology for\nthe assessment of automatically generated rap lyrics (as being similar to a\ntarget artist).  While the assessment of the generation of creative work is\nvery challenging and of great interest to the community, this effort falls\nshort of its claims of a comprehensive solution to this problem.\n\nAll assessment of this nature ultimately falls to a subjective measure -- can\nthe generated sample convince an expert that the generated sample was produced\nby the true artist rather than an automated preocess?  This is essentially a\nmore specific version of a Turing Test.   The effort to automate some parts of\nthe evaluation to aid in optimization and to understand how humans assess\nartistic similarity is valuable.  However, the specific findings reported in\nthis work do not encourage a belief that these have been reliably identified.\n\nSpecifically -- Consider the central question: Was a sample generated by a\ntarget artist?        The human annotators who were asked this were not able to\nconsistently respond to this question.        This means either 1) the annotators did\nnot have sufficient expertise to perform the task, or 2) the task was too\nchallenging, or some combination of the two.  \n\nThe proposed automatic measures also failed to show a reliable agreement to\nhuman raters performing the same task.        This dramatically limits their efficacy\nin providing a proxy for human assessment.   The low interannotator agreement\nmay be \"expected\" because the task is subjective, but the idea of decomposing\nthe evaluation into fluency and coherence components is meant to make it more\ntractable, and thereby improve the consistency of rater scores.  A low IAA for\nan evaluation metric is a cause for concern and limits its viability as a\ngeneral purpose tool.  \n\nSpecific questions/comments:\n\n* Why is a line-by-line level evaluation prefered to a verse level analysis. \nSpecifically for \"coherence\", a line by line analysis limits the scope of\ncoherence to consequtive lines.\n\n* Style matching -- This term assumes that these 13 artists each have a\ndistinct style, and always operate in that style. I would argue that some of\nthese artists (kanye west, eminem, jay z, drake, tupac and notorious big) have\nproduced work in multiple styles.  A more accurate term for this might be\n\"artist matching\".\n\n* In Section 4.2 The central automated component of the evaluation is low\ntf*idf with existing verses, and similar rhyme density.  Given the limitations\nof rhyme density -- how well does this work.  Even with the manual intervention\ndescribed?\n\n* In Section 6.2 -- This description should include how many judges were used\nin this study? In how many cases did the judges already know the verse they\nwere judging?  In this case the test will not assess how easy it is to match\nstyle, but rather, the judges recall and rap knowledge.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Language generation tasks that seek to mimic human ability to use language creatively are difficult to evaluate, since one must consider creativity, style, and other non-trivial aspects of the generated text. The goal of this paper is to develop evaluation methods for one such task, ghostwriting of rap lyrics, and to provide an explicit, quantifiable foundation for the goals and future directions of this task. Ghostwriting must produce text that is similar in style to the emulated artist, yet distinct in content. We develop a novel evaluation methodology that addresses several complementary aspects of this task, and illustrate how such evaluation can be used to meaningfully analyze system performance. We provide a corpus of lyrics for 13 rap artists, annotated for stylistic similarity, which allows us to assess the feasibility of manual evaluation for generated verse.", "histories": [], "id": 444, "title": "Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting"}
