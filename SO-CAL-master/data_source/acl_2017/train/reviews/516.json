{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThe paper offers a natural and useful extension to recent efforts in\ninteractive topic modeling, namely by allowing human annotators to provide\nmultiple \"anchor words\" to machine-induced topics. The paper is well-organized\nand the combination of synthetic and user experiments make for a strong paper.\n\n- Weaknesses:\n\nThe paper is fairly limited in scope in terms of the interactive topic model\napproaches it compares against. I am willing to accept this, since they do make\nreference to most of them and explain that these other approaches are not\nnecessarily fast enough for interactive experimentation or not conducive to the\ntypes of interaction being considered with an \"anchoring\" interface. Some level\nof empirical support for these claims would have been nice, though.\n\nIt would also have been nice to see experiments on more than one data set (20\nnewsgroups, which is now sort of beaten-to-death).\n\n- General Discussion:\n\nIn general, this is a strong paper that appears to offer an incremental but\nnovel and practical contribution to interactive topic modeling. The authors\nmade the effort to vet several variants of the approach in simulated\nexperiments, and to conduct fairly exhaustive quantitative analyses of both\nsimulated and user experiments using a variety of metrics that measure\ndifferent facets of topic quality.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nClear description of methods and evaluation\nSuccessfully employs and interprets a variety of evaluations\nSolid demonstration of practicality of technique in real-world interactive\ntopic modeling\n\n- Weaknesses:\nMissing related work on anchor words\nEvaluation on 20 Newsgroups is not ideal\nTheoretical contribution itself is small \n\n- General Discussion:\nThe authors propose a new method of interactive user specification of topics\ncalled Tandem Anchors. The approach leverages the anchor words algorithm, a\nmatrix-factorization approach to learning topic models, by replacing the\nindividual anchors inferred from the Gram-Schmidt algorithm with constructed\nanchor pseudowords created by combining the sparse vector representations of\nmultiple words that for a topic facet. The authors determine that the use of a\nharmonic mean function to construct pseudowords is optimal by demonstrating\nthat classification accuracy of document-topic distribution vectors using these\nanchors produces the most improvement over Gram-Schmidt. They also demonstrate\nthat their work is faster than existing interactive methods, allowing\ninteractive iteration, and show in a user study that the multiword anchors are\neasier and more effective for users.\n\nGenerally, I like this contribution a lot: it is a straightforward modification\nof an existing algorithm that actually produces a sizable benefit in an\ninteractive setting. I appreciated the authors\u2019 efforts to evaluate their\nmethod on a variety of scales. While I think the technical contribution in\nitself is relatively small (a strategy to assemble pseudowords based on topic\nfacets) the thoroughness of the evaluation merited having it be a full paper\ninstead of a short paper. It would have been nice to see more ideas as to how\nto build these facets in the absence of convenient sources like category titles\nin 20 Newsgroups or when initializing a topic model for interactive learning.\n\nOne frustration I had with this paper is that I find evaluation on 20\nNewsgroups to not be great for topic modeling: the documents are widely\ndifferent lengths, preprocessing matters a lot, users have trouble making sense\nof many of the messages, and naive bag-of-words models beat topic models by a\nsubstantial margin. Classification tasks are useful shorthand for how well a\ntopic model corresponds to meaningful distinctions in the text by topic; a task\nlike classifying news articles by section or reviews by the class of the\nsubject of the review might be more appropriate. It would also have been nice\nto see a use case that better appealed to a common expressed application of\ntopic models, which is the exploration of a corpus.\n\nThere were a number of comparisons I think were missing, as the paper contains\nlittle reference to work since the original proposal of the anchor word model.\nIn addition to comparing against standard Gram-Schmidt, it would have been good\nto see the method from Lee et. al. (2014), \u201cLow-dimensional Embeddings for\nInterpretable Anchor-based Topic Inference\u201d. I also would have liked to have\nseen references to Nguyen et. al. (2013), \u201cEvaluating Regularized Anchor\nWords\u201d and Nguyen et. al. (2015) \u201cIs Your Anchor Going Up or Down? Fast and\nAccurate Supervised Topic Models\u201d, both of which provide useful insights into\nthe anchor selection process.\n\nI had some smaller notes:\n- 164: \u2026entire dataset\n- 164-166: I\u2019m not quite sure what you mean here. I think you are claiming\nthat it takes too long to do one pass? My assumption would have been you would\nuse only a subset of the data to retrain the model instead of a full sweep, so\nit would be good to clarify what you mean.\n- 261&272: any reason you did not consider the and operator or element-wise\nmax? They seem to correspond to the ideas of union and intersection from the or\noperator and element-wise min, and it wasn\u2019t clear to me why the ones you\nchose were better options.\n- 337: Usenet should be capitalized\n- 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also,\ndid you remove headers, footers, and/or quotes from the messages?\n- 436-440: I would have liked to see a bit more explanation of what this tells\nus about confusion.\n- 692: using tandem anchors\n\nOverall, I think this paper is a meaningful contribution to interactive topic\nmodeling that I would like to see available for people outside the machine\nlearning community to investigate, classify, and test hypotheses about their\ncorpora.\n\nPOST-RESPONSE: I appreciate the thoughtful responses of the authors to my\nquestions. I would maintain that for some of the complimentary related work\nthat it's useful to compare to non-interactive work, even if it does something\ndifferent.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}], "abstract": "Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, go- ing beyond existing single word anchor algorithms\u2014an approach we call \u201cTan- dem Anchors\u201d. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interac- tive and non-interactive approaches. Tan- dem anchors are faster and more intuitive than existing interactive approaches.", "histories": [], "id": "516", "title": "Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling"}
