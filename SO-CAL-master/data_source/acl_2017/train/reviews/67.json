{"reviews": [{"IMPACT": "3", "SUBSTANCE": "1", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n- The paper tackles an important issue, that is building ontologies or thesauri\n- The methods make sense and seem well chosen\n- Methods and setups are well detailed\n- It looks like the authors outperform the state-of-the-art approach (but see\nbelow for my concerns)\n\n- Weaknesses:\nThe main weaknesses for me are evaluation and overall presentation/writing.\n- The list of baselines is hard to understand. Some methods are really old and\nit doesn't seem justified to show them here (e.g., Mpttern).\n- Memb is apparently the previous state-of-the-art, but there is no mention to\nany reference.\n- While it looks like the method outperforms the previous best performing\napproach, the paper is not convincing enough. Especially, on the first dataset,\nthe difference between the new system and the previous state-of-the-art one is\npretty small.\n- The paper seriously lacks proofreading, and could not be published until this\nis fixed \u2013 for instance,\u00a0I noted 11 errors in the first column of page 2.\n- The CilinE hierarchy is very shallow (5 levels only). However apparently, it\nhas been used in the past by other authors. I would expect that the deeper the\nmore difficult it is to branch new hyponym-hypernyms. This can explain the very\nhigh results obtained (even by previous studies)...\n\n- General Discussion:\nThe approach itself is not really original or novel, but it is applied to a\nproblem that has not been addressed with deep learning yet. For this reason, I\nthink this paper is interesting, but there are two main flaws. The first and\neasiest to fix is the presentation. There are many errors/typos that need to be\ncorrected. I started listing them to help, but there are just too many of them.\nThe second issue is the evaluation, in my opinion. Technically, the\nperformances are better, but it does not feel convincing as explained above.\nWhat is Memb, is it the method from Shwartz et al 2016, maybe? If not, what\nperformance did this recent approach have? I think the authors need to\nreorganize the evaluation section, in order to properly list the baseline\nsystems, clearly show the benefit of their approach and where the others fail.\nSignificance tests  also seem necessary given the slight improvement on one\ndataset.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\n  * Knowledge lean, language-independent approach\n\n- Weaknesses:\n\n  * Peculiar task/setting\n  * Marginal improvement over W_Emb (Fu et al, 2014)\n  * Waste of space\n  * Language not always that clear\n\n- General Discussion:\n\nIt seems to me that this paper is quite similar to (Fu et al, 2014) and only\nadds marginal improvements. It contains quite a lot of redundancy (e.g. related\nwork in  sec 1 and sec 2), uninformative figures (e.g. Figure 1 vs Figure 2),\nnot so useful descriptions of MLP and RNN, etc. A short paper might have been a\nbetter fit.\n\nThe task looks somewhat idiosyncratic to me. It is only useful if you already\nhave a method that gives you all and only the hypernyms of a given word. This\nseems to presuppose (Fu et al., 2013). \n\nFigure 4: why are the first two stars connected by conjunction and the last two\nstarts by disjunction?              Why is the output \"1\" (dark star) if the the\nthree\ninputs are \"0\" (white stars)?\n\nSec 4.2, lines 587-589 appears to suggest that thresholds were tuned on the\ntest data (?) \n\nW_Emb is poorly explained (lines 650-652).\n\nSome parts of the text are puzzling. I can't make sense of the section titled\n\"Combined with Manually-Built Hierarchies\". Same for sec 4.4. What do the red\nand dashed lines mean?", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Semantic hierarchies construction means to build structure of concepts linked by hypernym-hyponym (``is-a'') relations. A major challenge for this task is the automatic discovery of hypernym-hyponym (``is-a'') relations. We propose a fusion learning architecture based on word embeddings for constructing semantic hierarchies, composed of discriminative generative fusion architecture and a very simple lexical structure rule for assisting, getting an F1-score of 74.20% with 91.60% precision-value, outperforming the state-of-the-art methods on a manually labeled test dataset. Subsequently, combining our method with manually-built hierarchies can further improve F1-score to 82.01%. Besides, the fusion learning architecture is language-independent.", "histories": [], "id": 67, "title": "Constructing Semantic Hierarchies via Fusion Learning Architecture"}
