{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\n1. The presentation of the paper, up until the final few sections, is excellent\nand the paper reads very well at the start. The paper has a clear structure and\nthe argumentation is, for the most part, good.\n2. The paper addresses an important problem by attempting to incorporate word\norder information into word (and sense) embeddings and the proposed solution is\ninteresting.\n\n- Weaknesses:\n\n 1. Unfortunately, the results are rather inconsistent and one is not left\nentirely convinced that the proposed models are better than the alternatives,\nespecially given the added complexity. Negative results are fine, but there is\ninsufficient analysis to learn from them. Moreover, no results are reported on\nthe word analogy task, besides being told that the proposed models were not\ncompetitive - this could have been interesting and analyzed further.\n2. Some aspects of the experimental setup were unclear or poorly motivated, for\ninstance w.r.t. to corpora and datasets (see details below).\n3. Unfortunately, the quality of the paper deteriorates towards the end and the\nreader is left a little disappointed, not only w.r.t. to the results but with\nthe quality of the presentation and the argumentation.\n\n- General Discussion:\n\n1. The authors aim \"to learn representations for both words and senses in a\nshared emerging space\". This is only done in the LSTMEmbed_SW version, which\nrather consisently performs worse than the alternatives. In any case, what is\nthe motivation for learning representations for words and senses in a shared\nsemantic space? This is not entirely clear and never really discussed in the\npaper.\n2. The motivation for, or intuition behind, predicting pre-trained embeddings\nis not explicitly stated. Also, are the pre-trained embeddings in the\nLSTMEmbed_SW model representations for words or senses, or is a sum of these\nused again? If different alternatives are possible, which setup is used in the\nexperiments?\n3. The importance of learning sense embeddings is well recognized and also\nstressed by the authors. Unfortunately, however, it seems that these are never\nreally evaluated; if they are, this remains unclear. Most or all of the word\nsimilarity datasets considers words independent of context.\n4. What is the size of the training corpora? For instance, using different\nproportions of BabelWiki and SEW is shown in Figure 4; however, the comparison\nis somewhat problematic if the sizes are substantially different. The size of\nSemCor is moreover really small and one would typically not use such a small\ncorpus for learning embeddings with, e.g., word2vec. If the proposed models\nfavor small corpora, this should be stated and evaluated.\n5. Some of the test sets are not independent, i.e. WS353, WSSim and WSRel,\nwhich makes comparisons problematic, in this case giving three \"wins\" as\nopposed to one.\n6. The proposed models are said to be faster to train by using pre-trained\nembeddings in the output layer. However, no evidence to support this claim is\nprovided. This would strengthen the paper.\n7. Table 4: why not use the same dimensionality for a fair(er) comparison?\n8. A section on synonym identification is missing under similarity measurement\nthat would describe how the multiple-choice task is approached.\n9. A reference to Table 2 is missing.\n10. There is no description of any training for the word analogy task, which is\nmentioned when describing the corresponding dataset.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "The Long Short-Term Memory (LSTM) architecture for recurrent neural networks has become the state-of-the-art model for a range of different Natural Language Processing (NLP) tasks especially in language modeling and sequence to sequence learning. In this paper we leverage a bidirectional LSTM while at the same time taking advantage of other semantic resources in order to create a vector space model for words and senses that outperforms most popular algorithms for learning embeddings.  We evaluate our approach on the most well-known benchmarks on vector space representations.", "histories": [], "id": 792, "title": "LSTMEmbed: a Lexical and SemanTic Model of Embeddings with a bidirectional LSTM"}
