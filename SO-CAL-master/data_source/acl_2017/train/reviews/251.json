{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success on the analogy task and for the general\nsuperiority of additive composition models. It also establishes a link between\nskip-gram and Sufficient Dimensionality Reduction.\n\nI liked the focus of this paper on explaining the properties of skip-gram, and\ngenerally found it inspiring to read. I very much appreciate the effort to\nunderstand the assumptions of the model, and the way it affects (or is affected\nby) the composition operations that it is used to perform. In that respect, I\nthink it is a very worthwhile read for the community.\n\nMy main criticism is however that the paper is linguistically rather naive. The\nauthors' use of 'compositionality' (as an operation that takes a set of words\nand returns another with the same meaning) is extremely strange. Two words can\nof course be composed and produce a vector that is a) far away from both; b)\ndoes not correspond to any other concept in the space; c) still has meaning\n(productivity wouldn't exist otherwise!) Compositionality in linguistic terms\nsimply refers to the process of combining linguistic constituents to produce\nhigher-level constructs. It does not assume any further constraint, apart from\nsome vague (and debatable) notion of semantic transparency. The paper's\nimplication (l254) that composition takes place over sets is also wrong:\nordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a\nwell-known shortcoming of additive composition. \n\nAnother important aspect is that there are pragmatic factors that make humans\nprefer certain phrases to single words in particular contexts (and the\nopposite), naturally changing the underlying distribution of words in a large\ncorpus. For instance, talking of a 'male royalty' rather than a 'king' or\n'prince' usually has implications with regard to the intent of the speaker\n(here, perhaps highlighting a gender difference). This means that the equation\nin l258 (or for that matter the KL-divergence modification) does not hold, not\nbecause of noise in the data, but because of fundamental linguistic processes.\nThis point may be addressed by the section on SDR, but I am not completely sure\n(see my comments below).\n\nIn a nutshell, I think the way that the authors present composition is flawed,\nbut the paper convinces me that this is indeed what happens in skip-gram, and I\nthink this is an interesting contribution. \n\nThe part about Sufficient Dimensionality Reduction seems a little disconnected\nfrom the previous argument as it stands. I'm afraid I wasn't able to fully\nfollow the argument, and I would be grateful for some clarification in the\nauthors' response. If I understand it well, the argument is that skip-gram\nproduces a model where a word's neighbours follow some exponential\nparametrisation of a categorical distribution, but it is unclear whether this\nactually reflects the distribution of the corpus (as opposed to what happens\nin, say, a pure count-based model). The fact that skip-gram performs well\ndespite not reflecting the data is that it implements some form of SDR, which\ndoes not need to make any assumption about the underlying form of the data. But\nthen, is it fair to say that the resulting representations are optimised for\ntasks where geometrical regularities are important, regardless of the actual\npattern of the data? I.e. there some kind of denoising going on?\n\nMinor comments:\n\n- The abstract is unusually long and could, I think, be shortened.\n\n- para starting l71: I think it would be misconstrued to see circularity here.\nFirth observed that co-occurrence effects were correlated with similarity\njudgements, but those judgements are the very cognitive processes that we are\ntrying to model with statistical methods. Co-occurrence effects and vector\nspace word representations are in some sense 'the same thing', modelling an\nunderlying linguistic process we do not have direct observations for. So\npair-wise similarity is not there to break any circularity, it is there because\nit better models the kind of judgements humans known to make.\n\n- l296: I think 'paraphrase' would be a better word than 'synonym' here, given\nthat we are comparing a set of words with a unique lexical item.\n\n- para starting l322: this is interesting, and actually, a lot of the zipfian\ndistribution (the long tail) is fairly uniform.\n\n- l336: it is probably worth pointing out that the analogy relation does not\nhold so well in practice and requires to 'ignore' the first returned neighbour\nof the analogy computation (which is usually one of the observed terms).\n\n- para starting l343: I don't find it so intuitive to say that 'man' would be a\nsynonym/paraphrase of anything involving 'woman'. The subtraction involved in\nthe analogy computation is precisely not a straightforward composition\noperation, as it involves an implicit negation. \n\n- A last, tiny general comment. It is usual to write p(w|c) to mean the\nprobability of a word given a context, but in the paper 'w' is actually the\ncontext and 'c' the target word. It makes reading a little bit harder...\nPerhaps change the notation?\n\nLiterature:\n\nThe claim that Arora (2016) is the only work to try and understand vector\ncomposition is a bit strong. For instance, see the work by Paperno & Baroni on\nexplaining the success of addition as a composition method over PMI-weighted\nvectors:\n\nD. Paperno and M. Baroni. 2016. When the whole is less than the sum of its\nparts: How composition affects PMI values in distributional semantic vectors.\nComputational Linguistics 42(2): 345-350.\n\n***\nI thank the authors for their response and hope to see this paper accepted.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected \"side-effect\" of such models is that their vectors often exhibit compositionality, i.e., \\emph{adding} two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., \"man\" + \"royal\" = \"king\".  This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator.   Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.", "histories": [], "id": "251", "title": "Skip-Gram - Zipf + Uniform = Vector Additivity"}
