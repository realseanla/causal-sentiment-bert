{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThis paper presents an extension of many popular methods for learning vector\nrepresentations of text.  The original methods, such as skip-gram with negative\nsampling, Glove, or other PMI based approaches currently use word cooccurrence\nstatistics, but all of those approaches could be extended to n-gram based\nstatistics.  N-gram based statistics would increase the complexity of every\nalgorithm because both the vocabulary of the embeddings and the context space\nwould be many times larger.  This paper presents a method to learn embeddings\nfor ngrams with ngram context, and efficiently computes these embeddings.  On\nsimilarity and analogy tasks, they present strong results.\n\n- Weaknesses:\nI would have loved to see some experiments on real tasks where these embeddings\nare used as input beyond the experiments presented in the paper.  That would\nhave made the paper far stronger.\n\n- General Discussion:\nEven with the aforementioned weakness, I think this is a nice paper to have at\nACL.\n\nI have read the author response.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: The idea to train word2vec-type models with ngrams (here\nspecifically: bigrams) instead of words is excellent. The range of experimental\nsettings (four word2vec-type algorithms, several word/bigram conditions) covers\nquite a bit of ground. The qualitative inspection of the bigram embeddings is\ninteresting and shows the potential of this type of model for multi-word\nexpressions. \n\n- Weaknesses: This paper would benefit from a check by a native speaker of\nEnglish, especially regarding the use of articles. The description of the\nsimilarity and analogy tasks comes at a strange place in the paper (4.1\nDatasets). \n\n- General Discussion: As is done at some point well into the paper, it could be\nclarified from the start that this is simply a generalization of the original\nword2vec idea, redefining the word as an ngram (unigram) and then also using\nbigrams. It would be good to give a rationale why larger ngrams have not been\nused.\n\n(I have read the author response.)", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "2"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "This paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI,\nSVD) to include ngram-ngram cooccurance statistics. To deal with the large\ncomputational costs of storing such expensive matrices, the authors propose an\nalgorithm that uses two different strategies to collect counts.  \n\n- Strengths:\n\n* The proposed work seems like a natural extension of existing work on learning\nword embeddings. By integrating bigram information, one can expect to capture\nricher syntactic and semantic information.\n\n- Weaknesses:\n\n* While the authors propose learning embeddings for bigrams (bi_bi case), they\nactually do not evaluate the embeddings for the learned bigrams except for the\nqualitative evaluation in Table 7. A more quantitative evaluation on\nparaphrasing or other related tasks that can include bigram representations\ncould have been a good contribution.\n\n* The evaluation and the results are not very convincing - the results do not\nshow consistent trends, and some of the improvements are not necessarily\nstatistically significant.\n\n* The paper reads clunkily due to significant grammar and spelling errors,\nand needs a major editing pass.\n\n- General Discussion:\n\nThis paper is an extension of standard embedding learning techniques to include\ninformation from bigram-bigram coocurance. While the work is interesting and a\nnatural extension of existing work, the evaluation and methods leaves some open\nquestions. Apart from the ones mentioned in the weaknesses, some minor\nquestions for the authors :\n\n* Why is there significant difference between the overlap and non-overlap\ncases? I would be more interested in finding out more than the quantitative\ndifference shown on the tasks.\n\nI have read the author response. I look forward to seeing the revised version\nof the paper.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "The existing word representation methods mostly limit their information source to word co-occurrence statistics. In this paper, we introduce ngrams into four representation methods: SGNS, GloVe, PPMI matrix and its SVD factorization. Comprehensive experiments are conducted on word analogy and similarity tasks. The results show that improved word representations are learned from ngram co-occurrence statistics. We also demonstrate that the trained ngram representations are useful in many aspects such as finding antonyms and collocations. Besides, a novel approach of building co-occurrence matrix is proposed to alleviate the hardware burden brought by ngrams.", "histories": [], "id": 56, "title": "Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics"}
