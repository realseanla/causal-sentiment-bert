{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\nZero-shot relation extraction is an interesting problem. The authors have\ncreated a large dataset for relation extraction as question answering which\nwould likely be useful to the community.\n\n- Weaknesses:\nComparison and credit to existing work is severely lacking. Contributions of\nthe paper don't seen particularly novel.\n\n- General Discussion:\n\nThe authors perform relation extraction as reading comprehension. In order to\ntrain reading comprehension models to perform relation extraction, they create\na large dataset of 30m \u201cquerified\u201d (converted to natural language)\nrelations by asking mechanical turk annotators to write natural language\nqueries for relations from a schema. They use the reading comprehension model\nof Seo et al. 2016, adding the ability to return \u201cno relation,\u201d as the\noriginal model must always return an answer. The main motivation/result of the\npaper appears to be that the authors can perform zero-shot relation extraction,\nextracting relations only seen at test time.\n\nThis paper is well-written and the idea is interesting. However, there are\ninsufficient experiments and comparison to previous work to convince me that\nthe paper\u2019s contributions are novel and impactful.\n\nFirst, the authors are missing a great deal of related work: Neelakantan at al.\n2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction\nusing RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804)\nperform relation extraction on unseen entities. The authors cite Bordes et al.\n(https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and\nperform relation extraction using memory networks (which are commonly used for\nreading comprehension). However, they merely note that their data was annotated\nat the \u201crelation\u201d level rather than at the triple (relation, entity pair)\nlevel\u2026 but couldn\u2019t Bordes et al. have done the same in their annotation?\nIf there is some significant difference here, it is not made clear in the\npaper. There is also a NAACL 2016 paper\n(https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation\nextraction using a new model based on memory networks\u2026 and I\u2019m sure there\nare more. Your work is so similar to much of this work that you should really\ncite and establish novelty wrt at least some of them as early as the\nintroduction -- that's how early I was wondering how your work differed, and it\nwas not made clear.\n\nSecond, the authors neither 1) evaluate their model on another dataset or 2)\nevaluate any previously published models on their dataset. This makes their\nempirical results extremely weak. Given that there is a wealth of existing work\nthat performs the same task and the lack of novelty of this work, the authors\nneed to include experiments that demonstrate that their technique outperforms\nothers on this task, or otherwise show that their dataset is superior to others\n(e.g. since it is much larger than previous, does it allow for better\ngeneralization?)", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper presents a method for relation extraction based on converting the\ntask into a question answering task. The main hypothesis of the paper is that\nquestions are a more generic vehicle for carrying content than particular\nexamples of relations, and are easier to create. The results seem to show good\nperformance, though a direct comparison on a standard relation extraction task\nis not performed.\n- Strengths:\nThe technique seems to be adept at identifying relations (a bit under 90\nF-measure). It works well both on unseen questions (for seen relations) and\nrelatively well on unseen relations. The authors describe a method for\nobtaining a large training dataset\n\n- Weaknesses:\nI wish performance was also shown on standard relation extraction datasets - it\nis impossible to determine what types of biases the data itself has here\n(relations are generated from Wikidata via WikiReading - extracted from\nWikipedia, not regular newswire/newsgroups/etc). It seems to me that the NIST\nTAC-KBP slot filling dataset is good and appropriate to run a comparison.\n\nOne comparison that the authors did not do here (but should) is to train a\nrelation detection model on the generated data, and see how well it compares\nwith the QA approach.\n\n- General Discussion:\nI found the paper to be well written and argued, and the idea is interesting,\nand it seems to work decently. I also found it interesting that the zero-shot\nNL method behaved indistinguishably from the single question baseline, and not\nvery far from the multiple questions system.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper models the relation extraction problem as reading comprehension and\nextends a previously proposed reading comprehension (RC) model to extract\nunseen relations. The approach has two main components:\n\n1. Queryfication: Converting a relation into natural question. Authors use\ncrowdsourcing for this part.\n\n2. Applying RC model on the generated questions and sentences to get the answer\nspans. Authors extend a previously proposed approach to accommodate situations\nwhere there is no correct answer in the sentence.\n\nMy comments:\n\n1. The paper reads very well and the approach is clearly explained.\n\n2. In my opinion, though the idea of using RC for relation extraction is\ninteresting and novel, the approach is not novel. A part of the approach is\ncrowdsourced and the other part is taken directly from a previous work, as I\nmention above.\n\n3. Relation extraction is a well studied problem and there are plenty of\nrecently published works on the problem. However, authors do not compare their\nmethods against any of the previous works. This raises suspicion on the\neffectiveness of the approach. As seen from Table 2, the performance numbers of\nthe proposed method on the core task are not very convincing. However, this\nmaybe because of the dataset used in the paper. Hence, a comparison with\nprevious methods would actually help assess how the current method stands with\nthe state-of-the-art.\n\n4. Slot-filling data preparation: You say \"we took the first sentence s in D to\ncontain both e and a\". How can you get the answer sentence for (all) the\nrelations of an entity from the first sentence of the entity's Wikipedia\narticle? Please clarify this. See the following paper. They have a set of rules\nto locate (answer) sentences corresponding to an entity property in its\nWikipedia page:\n\nWu, Fei, and Daniel S. Weld. \"Open information extraction using Wikipedia.\"\nProceedings of the 48th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, 2010.\n\nOverall, I think the paper presents an interesting approach. However, unless\nthe effectiveness of the approach is demonstrated by comparing it against\nrecent works on relation extraction, the paper is not ready for publication.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn high-quality relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relations that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relations with high accuracy, and that zero-shot generalization to unseen relations is possible, at lower accuracy levels, setting the bar for future work on this task.", "histories": [], "id": 562, "title": "Zero-Shot Relation Extraction via Reading Comprehension"}
