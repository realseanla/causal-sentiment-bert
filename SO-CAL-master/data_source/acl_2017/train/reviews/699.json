{"reviews": [{"IMPACT": "5", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes to use an encoder-decoder framework for keyphrase\ngeneration. Experimental results show that the proposed model outperforms other\nbaselines if supervised data is available.\n\n- Strengths:\nThe paper is well-organized and easy to follow (the intuition of the proposed\nmethod is clear). It includes enough details to replicate experiments. Although\nthe application of an encoder-decoder (+ copy mechanism) is straightforward,\nexperimental results are reasonable and support the claim (generation of absent\nkeyphrases) presented in this paper.\n\n- Weaknesses:\nAs said above, there is little surprise in the proposed approach. Also, as\ndescribed in Section 5.3, the trained model does not transfer well to new\ndomain (it goes below unsupervised models). One of the contribution of this\npaper is to maintain training corpora in good quantity and quality, but it is\nnot (explicitly) stated.\n\n- General Discussion:\nI like to read the paper and would be pleased to see it accepted. I would like\nto know how the training corpus (size and variation) affects the performance of\nthe proposed method. Also, it would be beneficial to see the actual values of\np_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my\nexperience in running the CopyNet, the copying mechanism sometimes works\nunexpectedly (not sure why this happens).", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "5", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper divides the keyphrases into two types: (1) Absent key phrases (such\nphrases do not match any contiguous subsequences of the source document) and\n(2) Present key phrases (such key phrases fully match a part of the text). The\nauthors used RNN based generative models (discussed as RNN and Copy RNN) for\nkeyphrase prediction and copy mechanism in RNN to predict the already occurred\nphrases. \n\nStrengths:\n\n1. The formation and extraction of key phrases, which are absent in the current\ndocument is an interesting idea of significant research interests. \n\n2. The paper is easily understandable.\n\n3. The use of RNN and Copy RNN in the current context is a new idea. As, deep\nrecurrent neural networks are already used in keyphrase extraction (shows very\ngood performance also), so, it will be interesting to have a proper motivation\nto justify the use of  RNN and Copy RNN over deep recurrent neural networks. \n\nWeaknesses:\n\n1. Some discussions are required on the convergence of the proposed joint\nlearning process (for RNN and CopyRNN), so that readers can understand, how the\nstable points in probabilistic metric space are obtained? Otherwise, it may be\ntough to repeat the results.\n\n2. The evaluation process shows that the current system (which extracts 1.\nPresent and 2. Absent both kinds of keyphrases) is evaluated against baselines\n(which contains only \"present\" type of keyphrases). Here there is no direct\ncomparison of the performance of the current system w.r.t. other\nstate-of-the-arts/benchmark systems on only \"present\" type of key phrases. It\nis important to note that local phrases (keyphrases) are also important for the\ndocument. The experiment does not discuss it explicitly. It will be interesting\nto see the impact of the RNN and Copy RNN based model on automatic extraction\nof local or \"present\" type of key phrases.\n\n3. The impact of document size in keyphrase extraction is also an important\npoint. It is found that the published results of [1], (see reference below)\nperforms better than (with a sufficiently high difference) the current system\non Inspec (Hulth, 2003) abstracts dataset. \n\n4. It is reported that current system uses 527,830 documents for training,\nwhile 40,000 publications are held out for training baselines. Why are all\npublications not used in training the baselines? Additionally,        The topical\ndetails of the dataset (527,830 scientific documents) used in training RNN and\nCopy RNN are also missing. This may affect the chances of repeating results.\n\n5. As the current system captures the semantics through RNN based models. So,\nit would be better to compare this system, which also captures semantics. Even,\nRef-[2] can be a strong baseline to compare the performance of the current\nsystem.\n\nSuggestions to improve:\n\n1. As, per the example, given in the Figure-1, it seems that all the \"absent\"\ntype of key phrases are actually \"Topical phrases\". For example: \"video\nsearch\", \"video retrieval\", \"video indexing\" and \"relevance ranking\", etc.\nThese all define the domain/sub-domain/topics of the document. So, In this\ncase, it will be interesting to see the results (or will be helpful in\nevaluating \"absent type\" keyphrases): if we identify all the topical phrases of\nthe entire corpus by using tf-idf and relate the document to the high-ranked\nextracted topical phrases (by using Normalized Google Distance, PMI, etc.). As\nsimilar efforts are already applied in several query expansion techniques (with\nthe aim to relate the document with the query, if matching terms are absent in\ndocument).\n\nReference:\n1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to\nfind exemplar terms for keyphrase extraction. In Proceedings of the 2009\nConference on Empirical Methods in Natural Language Processing, pages\n257\u2013266.\n\n2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction\nusing deep recurrent neural networks on Twitter. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing (pp. 836-845).", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "5", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nNovel model.  I particularly like the ability to generate keyphrases not\npresent in the source text.\n\n- Weaknesses:\n\n Needs to be explicit whether all evaluated models are trained and tested on\nthe same data sets.  Exposition of the copy mechanism not quite\nclear/convincing.\n\n- General Discussion:\n\nThis paper presents a supervised neural network approach for keyphrase\ngeneration.  The model uses an encoder-decoder architecture that\nfirst encodes input text with a RNN, then uses an attention mechanism to\ngenerate keyphrases from\nthe hidden states.  There is also a more advanced variant of the\ndecoder which has an attention mechanism that conditions on the\nkeyphrase generated in the previous time step.\n\nThe model is interesting and novel. And I think the ability to\ngenerate keyphrases not in the source text is particularly\nappealing.  My main concern is with the evaluation:  Are all\nevaluated models trained with the same amount of data and evaluated\non the same test sets?              It's not very clear.  For example, on the\nNUS data set, Section 4.2 line 464 says that the supervised baselines\nare evaluated with cross validation.\n\nOther comments:\n\nThe paper is mostly clearly written and easy to follow.  However,\nsome parts are unclear:\n\n- Absent keyphrases vs OOV.  I think there is a need to distinguish\n  between the two, and the usage meaning of OOV should be consistent.  The RNN\nmodels\n  use the most frequent 50000 words as the vocabulary (Section 3.4\n  line 372, Section 5.1 line 568), so I suppose OOV are words not in\n  this 50K vocabulary.              In line 568, do you mean OOV or absent\n  words/keyphrases?  Speaking of this, I'm wondering how many\n  keyphrases fall outside of this 50K?              The use of \"unknown words\"\n  in line 380 is also ambiguous.  I think it's probably clearer to say that\n the RNN models can generate words not present in the source text as long as\nthey appear\nsomewhere else in the corpus (and the 50K vocabulary)\n\n- Exposition of the copy mechanism (section 3.4).  This mechanism has a\n  more specific locality than the attention model in basic RNN model.\n  However, I find the explanation of the intuition misleading.              If I\n  understand correctly, the \"copy mechanism\" is conditioned on the\n  source text locations that matches the keyphrase in the previous\n  time step y_{t-1}.  So maybe it has a higher tendency to generate n-grams\nseen source text (Figure 1).  I buy the argument that the more sophisticated\n  attention model probably makes CopyRNN better than the RNN\n  overall, but why is the former model particularly better for absent\n  keyphrases?  It is as if both models perform equally well on present\nkeyphrases.\n\n- How are the word embeddings initialized?", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks.  We name it as \\textit{deep keyphrase generation} since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.", "histories": [], "id": "699", "title": "Deep Keyphrase Generation"}
