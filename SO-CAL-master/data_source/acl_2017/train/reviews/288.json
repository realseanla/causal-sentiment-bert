{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper analyzes the story endings (last sentence of a 5-sentence story) in\nthe corpus built for the story cloze task (Mostafazadeh et al. 2016), and\nproposes a model based on character and word n-grams to classify story endings.\nThe paper also shows better performance on the story cloze task proper\n(distinguishing between \"right\" and \"wrong\" endings) than prior work.\n\nWhereas style analysis is an interesting area and you show better results than\nprior work on the story cloze task, there are several issues with the paper.\nFirst, how do you define \"style\"? Also, the paper needs to be restructured (for\ninstance, your section\n\"Results\" actually mixes some results and new experiments) and clarified (see\nbelow for questions/comments): right now, it is quite difficult for the reader\nto follow what data is used for the different experiments, and what data the\ndiscussion refers to.\n\n(1) More details about the data used is necessary in order to assess the claim\nthat \"subtle writing task [...] imposes different styles on the author\" (lines\n729-732). How many stories are you looking at, written by how many different\npersons? And how many stories are there per person? From your description of\nthe post-analysis of coherence, only pairs of stories written by the same\nperson in which one was judged as \"coherent\" and the other one as \"neutral\" are\nchosen. Can you confirm that this is the case? So perhaps your claim is\njustified for your \"Experiment 1\". However my understanding is that in\nexperiment 2 where you compare \"original\" vs. \"right\" or \"original\" vs.\n\"wrong\", we do not have the same writers. So I am not convinced lines 370-373\nare correct.\n\n(2) A lot in the paper is simply stated without any justifications. For\ninstance how are the \"five frequent\" POS and words chosen? Are they the most\nfrequent words/POS? (Also theses tables are puzzling: why two bars in the\nlegend for each category?). Why character *4*-grams? Did you tune that on the\ndevelopment set? If these were not the most frequent features, but some that\nyou chose among frequent POS and words, you need to justify this choice and\nespecially link the choice to \"style\". How are these features reflecting\n\"style\"?\n\n(3) I don't understand how the section \"Design of NLP tasks\" connects to the\nrest of the paper, and to your results. But perhaps this is because I am lost\nin what \"training\" and \"test\" sets refer to here.\n\n(4) It is difficult to understand how your model differs from previous work.\nHow do we reconcile lines 217-219 (\"These results suggest that real\nunderstanding of text is required in order to solve the task\") with your\napproach?\n\n(5) The terminology of \"right\" and \"wrong\" endings is coming from Mostafazadeh\net al., but this is a very bad choice of terms. What exactly does a \"right\" or\n\"wrong\" ending mean (\"right\" as in \"coherent\" or \"right\" as in \"morally good\")?\nI took a quick look, but couldn't find the exact prompts given to the Turkers.\nI think this needs to be clarified: as it is, the first paragraph of your\nsection \"Story cloze task\" (lines 159-177) is not understandable.\n\nOther questions/comments:\n\nTable 1. Why does the \"original\" story differ from the coherent and incoherent\none? From your description of the corpus, it seems that one Turker saw the\nfirst 4 sentences of the original story and was then ask to write one sentence\nending the story in a \"right\" way (or did they ask to provide a \"coherent\"\nending?) and one sentence ending the story in a \"wrong\" way (or did they ask to\nprovide an \"incoherent\" ending)? I don't find the last sentence of the\n\"incoherent\" story that incoherent... If the only shoes that Kathy finds great\nare $300, I can see how Kathy doesn't like buying shoes ;-) This led me to\nwonder how many Turkers judged the coherence of the story/ending and how\nvariable the judgements were. What criterion was used to judge a story coherent\nor incoherent? Also does one Turker judge the coherence of both the \"right\" and\n\"wrong\" endings, making it a relative judgement? Or was this an absolute\njudgement? This would have huge implications on the ratings.\n\nLines 380-383: What does \"We randomly sample 5 original sets\" mean?\n\nLine 398: \"Virtually all sentences\"? Can you quantify this?\n\nTable 5: Could we see the weights of the features? \n\nLine 614: \"compared to ending an existing task\": the Turkers are not ending a\n\"task\"\n\nLine 684-686: \"made sure each pair of endings was written by the same author\"\n-> this is true for the \"right\"/\"wrong\" pairs, but not for the \"original\"-\"new\"\npairs, according to your description.\n\nLine 694: \"shorter text spans\": text about what? This is unclear.\n\nLines 873-875: where is this published?", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\nThe paper has a promising topic (different writing styles in finishing a story)\nthat could appeal to Discourse and Pragmatics area participants.  \n\n- Weaknesses:\nThe paper suffers from a convincing and thorough discussion on writing style\nand implications of the experiments on discourse or pragmatics. \n(1) For example, regarding \"style\", the authors could have sought answers to\nthe following questions: what is the implication of starting an incoherent\nend-of-story sentence with a proper noun (l. 582)? Is this a sign of topic\nshift? What is the implication of ending a story coherently with a past tense\nverb, etc. \n(2) It is not clear to me why studies on deceptive language are similar to\nshort or long answers in the current study. I would have liked to see a more\ncomplete comparison here.\n(3) The use of terms such as \"cognitive load\" (l. 134) and \"mental states\" (l.\n671) appears somewhat vague. \n(4) There is insufficient discussion on the use of coordinators (line 275\nonwards); the paper would benefit from a more thorough discussion of this issue\n(e.g. what is the role of coordinators in these short stories and in discourse\nin general? Does the use of coordinators differ in terms of the genre of the\nstory? How about the use of \"no\" coordinators?)  \n(5) The authors do not seem to make it sufficiently clear who the target\nreaders of this research would be (e.g. language teachers? Crowd-sourcing\nexperiment designers? etc.) \n\nThe paper needs revision in terms of organization\n(there are repetitions throughout the text).  Also, the abbreviations in Table\n5 and 6 are not clear to me. \n\n- General Discussion:\nAll in all, the paper would have to be revised particularly in terms of its\ntheoretical standpoint and implications to discourse and pragmatics.\n\n=====\n\nIn their response to the reviewers' comments, the authors indicate their\nwillingness to update the paper and clarify the issues related to what they\nhave experimented with. However, I would have liked to see a stronger\ncommitment to incorporating the implications of this study to the Discourse and\nPragmatics area.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\n*The paper is very well written\n*It shows how stylometric analysis can help in reasoning-like text\nclassification\n*The results have important implications for design on NLP datasets\n*The results may have important implications for many text classification tasks\n\n- Weaknesses:\n*I see few weaknesses in this paper. The only true one is the absence of a\ndefinition of style, which is a key concept in the paper\n\n- General Discussion:\nThis paper describes two experiments that explore the relationship between\nwriting task and writing style. In particular, controlling for vocabulary and\ntopic, the authors show that features used in authorship attribution/style\nanalysis can go a long way towards distinguishing between 1) a natural ending\nof a story 2) an ending added by a different author and 3) a purposefully\nincoherent ending added by a different author.\n\nThis is a great and fun paper to read and it definitely merits being accepted.\nThe paper is lucidly written and clearly explains what was done and why. The\nauthors use well-known simple features and a simple classifier to prove a\nnon-obvious hypothesis. Intuitively, it is obvious that a writing task greatly\nconstraints style. However, proven in such a clear manner, in such a controlled\nsetting, the findings are impressive.\n\nI particularly like Section 8 and the discussion about the implications on\ndesign of NLP tasks. I think this will be an influential and very well cited\npaper. Great work.  \n\nThe paper is a very good one as is. One minor suggestion I have is defining\nwhat the authors mean by \u201cstyle\u201d early on. The authors seem to mean \u201ca\nset of low-level easily computable lexical and syntactic features\u201d.  As is,\nthe usage is somewhat misleading for anyone outside of computational\nstylometrics. \n\nThe set of chosen stylistic features makes sense. However, were there no other\noptions? Were other features tried and they did not work? I think a short\ndiscussion of the choice of features would be informative.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "5", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "A writer's style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the  story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed with stylistic features is able to successfully distinguish between the three cases, without even looking at the story context. In addition, our style-based classifier establishes a new state-of-the-art result on the story cloze challenge, substantially higher than previous results based on deep learning models. Our results demonstrate that different task framings can dramatically affect the way people write.", "histories": [], "id": 288, "title": "The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task"}
