{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n * Elaborate evaluation data creation and evaluation scheme.\n * Range of compared techniques: baseline/simple/complex\n\n- Weaknesses:\n * No in-depth analysis beyond overall evaluation results.\n\n- General Discussion:\nThis paper compares several techniques for robust HPSG parsing.\n\nSince the main contribution of the paper is not a novel parsing technique but\nthe empirical evaluation, I would like to see a more in-depth analysis of the\nresults summarized in Table 1 and 2.\nIt would be nice to show some representative example sentences and sketches of\nits analyses, on which the compared methods behaved differently.\n\nPlease add EDM precision and recall figures to Table 2.\nThe EDM F1 score is a result of a mixed effects of (overall and partial)\ncoverage, parse ranking, efficiency of search, etc.\nThe overall coverage figures in Table 1 are helpful but addition of EDM recall\nto Table 2 would make the situations clearer.\n\nMinor comment:\n- Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3?", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nWell-written.\n\n- Weaknesses:\n\nAlthough the title and abstract of the paper suggest that robust parsing\nmethods for HPSG are being compared, the actual comparison is limited to only a\nfew techniques applied to a single grammar, the ERG (where in the past the \nchoice has been made to create a treebank for only those sentences that are in\nthe coverage of the grammar). Since the ERG is quite idiosyncratic in this\nrespect, I fear that the paper is not interesting for researchers working in\nother precision grammar frameworks.\n\nThe paper lacks comparison with robustness techniques that are routinely\napplied for systems based on other precision grammars such as various systems\nbased on CCG, LFG, the Alpage system for French, Alpino for Dutch and there is\nprobably more. In the same spirit, there is a reference for supertagging to\nDridan 2013 which is about supertagging for ERG whereas supertagging for other\nprecision grammar systems has been proposed at least a decade earlier.\n\nThe paper lacks enough detail to make the results replicable. Not only are\nvarious details not spelled out (e.g. what are those limits on resource\nallocation), but perhaps more importantly, for some of the techniques that are\nbeing compared (eg the robust unification), and for the actual evaluation\nmetric, the paper refers to another paper that is still in preparation.\n\nThe actual results of the various techniques are somewhat disappointing. With\nthe exception of the csaw-tb method, the resulting parsing speed is extreme -\nsometimes much slower than the baseline method - where the baseline method is a\nmethod in which the standard resource limitations do not apply. The csaw-tb\nmethod is faster but not very accurate, and in any case it is not a method\nintroduced in this paper but an existing PCFG approximation technique.\n\nIt would be (more) interesting to have an idea of the results on a\nrepresentative dataset (consisting of both sentences that are in the coverage\nof the grammar and those that are not). In that case, a comparison with the\n\"real\" baseline system (ERG with standard settings) could be obtained.\n\nMethodological issue: the datasets semcor and wsj00ab consist of sentences\nwhich an older version of ERG could not parse, but a newer version could. For\nthis reason, the problems in these two datasets are clearly very much biased.\nIt is no suprise therefore that the various techniques obtain much better\nresults on those datasets. But to this reviewer, those results are somewhat\nmeaningless. \n\nminor:\n\nEDM is used before explained\n\n\"reverseability\"\n\n- General Discussion:", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\n- technique for creating dataset for evaluation of out-of-coverage items, that\ncould possibly be used to evaluation other grammars as well. \n- the writing in this paper is engaging, and clear (a pleasant surprise, as\ncompared to the typical ACL publication.)\n\n- Weaknesses:\n- The evaluation datasets used are small and hence results are not very\nconvincing (particularly wrt to the alchemy45 dataset on which the best results\nhave been obtained)\n- It is disappointing to see only F1 scores and coverage scores, but virtually\nno deeper analysis of the results. For instance, a breakdown by type of\nerror/type of grammatical construction would be interesting. \n- it is still not clear to this reviewer what is the proportion of out of\ncoverage items due to various factors (running out of resources,  lack of\ncoverage for \"genuine\" grammatical constructions in the long tail, lack of\ncoverage due to extra-grammatical factors like interjections, disfluencies,\nlack of lexical coverage, etc. \n\n- General Discussion:\n\nThis paper address the problem of \"robustness\" or lack of coverage for a\nhand-written HPSG grammar (English Resource Grammar). The paper compares\nseveral approaches for increasing coverage, and also presents two creative ways\nof obtaining evaluation datasets (a non-trivial issue due to the fact that gold\nstandard evaluation data is by definition available only for in-coverage\ninputs). \n\nAlthough hand-written precision grammars have been very much out of fashion\nfor a long time now and have been superseded by statistical treebank-based\ngrammars, it is important to continue research on these in my opinion. The\nadvantages of high precision and deep semantic analysis provided by these\ngrammars has not been\nreproduced by non-handwritten grammars as yet. For this reason, I am giving\nthis paper a score of 4, despite the shortcomings mentioned above.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "This paper explores several techniques for enhancing coverage when parsing with HPSG grammars, determines appropriate evaluation methods, and uses them to compare performance.  Depending on the dataset, baseline coverage gaps can be reduced by between 75% and 100%, while simultaneously improving EDM F1 scores.", "histories": [], "id": 524, "title": "A Comparison of Robust Parsing Methods for HPSG"}
