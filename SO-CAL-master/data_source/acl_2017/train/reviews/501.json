{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper proposes a task of selecting the most appropriate textual description\nfor a given scene/image from a list of similar options. It also proposes couple\nof baseline models, an evaluation metrics and human evaluation score. \n\n- Strengths:\n\nThe paper is well-written and well-structured. \nIt is clear with its contributions and well supports them by empirical\nevidence. So the paper is very easy to read. \n\nThe paper is well motivated. A method of selecting the most appropriate caption\ngiven a list of misleading candidates will benefit other\nimage-caption/understanding models, by acting as a post-generation re-ranking\nmethod. \n\n- Weaknesses:\n\nI am not sure if the proposed algorithm for decoys generation is effective,\nwhich as a consequence puts the paper on questions.\n\nFor each target caption, the algorithm basically picks out those with similar\nrepresentation and surface form but do not belong to the same image. But a\nfundamentally issue with this approach is: not belonging to the image-A does\nnot mean not appropriate to describe image-A, especially when the\nrepresentation and surface form are close. So the ground-truth labels might not\nbe valid. As we can see in Figure-1, the generated decoys are either too far\nfrom the target to be a *good* decoy (*giraffe* vs *elephant*), or fair\nsubstitutes for the target (*small boy playing kites* vs *boy flies a kite*).\n\nThus, I am afraid that the dataset generated with this algorithm can not train\na model to really *go beyond key word recognition*, which was claimed as\ncontribution in this paper. As shown in Figure-1, most\ndecoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs\nbread*, *frisbee vs kite*, etc. And when they can not be separated by *key word\nmatch*, they look very tempting to be a correct option.\n\nFurthermore, it is interesting that humans only do correctly on 82.8% on a\nsampled test set. Does it mean that those examples are really too hard even for\nhuman to correctly classify? Or are some of the *decoys* in fact good enough to\nbe the target's substitute (or even better) so that human choose them over\nground-truth targets?\n\n- General Discussion:\n\nI think this is a well-written paper with clear motivation and substantial\nexperiments. \nThe major issue is that the data-generating algorithm and the generated dataset\ndo not seem helpful for the motivation. This in turn makes the experimental\nconclusions less convincing. So I tend to reject this paper unless my concerns\ncan be fully addressed in rebuttal.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nAuthors generate a dataset of \u201crephrased\u201d captions and are planning to make\nthis dataset publicly available.\n\nThe way authors approached DMC task has an advantage over VQA or caption\ngeneration in terms of metrics. It is easier and more straightforward to\nevaluate problem of choosing the best caption. Authors use accuracy metric.\nWhile for instance caption generation requires metrics like BLUE or Meteor\nwhich are limited in handling semantic similarity.\n\nAuthors propose an interesting approach to \u201crephrasing\u201d, e.g. selecting\ndecoys. They draw decoys form image-caption dataset. E.g. decoys for a single\nimage come from captions for other images. These decoys however are similar to\neach other both in terms of surface (bleu score) and semantics (PV similarity).\nAuthors use lambda factor to decide on the balance between these two components\nof the similarity score. I think it would be interesting to employ these for\nparaphrasing.\n\nAuthors support their motivation for the task with evaluation results. They\nshow that a system trained with the focus on differentiating between similar\ncaptions performs better than a system that is trained to generate captions\nonly. These are, however, showing that system that is tuned for a particular\ntask performs better on this task.\n\n- Weaknesses:\n\n It is not clear why image caption task is not suitable for comprehension task\nand why author\u2019s system is better for this. In order to argue that system can\ncomprehend image and sentence semantics better one should apply learned\nrepresentation, e.g. embeddings. E.g. apply representations learned by\ndifferent systems on the same task for comparison.\n\nMy main worry about the paper is that essentially authors converge to using\nexisting caption generation techniques, e.g. Bahdanau et al., Chen et al.\n\nThey way formula (4) is presented is a bit confusing. From formula it seems\nthat both decoy and true captions are employed for both loss terms. However, as\nit makes sense, authors mention that they do not use decoy for the second term.\nThat would hurt mode performance as model would learn to generate decoys as\nwell. The way it is written in the text is ambiguous, so I would make it more\nclear either in the formula itself or in the text. Otherwise it makes sense for\nthe model to learn to generate only true captions while learning to distinguish\nbetween true caption and a decoy.\n\n- General Discussion:\n\nAuthors formulate a task of Dual Machine Comprehension. They aim to accomplish\nthe task by challenging computer system to solve a problem of choosing between\ntwo very similar captions for a given image. Authors argue that a system that\nis able to solve this problem has to \u201cunderstand\u201d the image and captions\nbeyond just keywords but also capture semantics of captions and their alignment\nwith image semantics.\n\nI think paper need to make more focus on why chosen approach is better than\njust caption generation and why in their opinion caption generation is less\nchallenging for learning image and text representation and their alignment.\n\nFor formula (4). I wonder if in the future it is possible to make model to\nlearn \u201cnot to generate\u201d decoys by adjusting second loss term to include\ndecoys but with a negative sign. Did authors try something similar?", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThe DMC task seems like a good test of understanding language and vision. I\nlike that the task has a clear evaluation metric.\n\nThe failure of the caption generation model on the DMC task is quite\ninteresting. This result further demonstrates that these models are good\nlanguage models, but not as good at capturing the semantics of the image.\n\n- Weaknesses:\n\nThe experiments are missing a key baseline: a state-of-the-art VQA model\ntrained with only a yes/no label vocabulary. \n\nI would have liked more details on the human performance experiments. How many\nof the ~20% of incorrectly-predicted images are because the captions are\ngenuinely ambiguous? Could the data be further cleaned up to yield an even\nhigher human accuracy?\n\n- General Discussion:\n\nMy concern with this paper is that the data set may prove to be easy or\ngameable in some way. The authors can address this concern by running a suite\nof strong baselines on their data set and demonstrating their accuracies. I'm\nnot convinced by the current set of experiments because the chosen neural\nnetwork architectures appear quite different from the state-of-the-art\narchitectures in similar tasks, which typically rely on attention mechanisms\nover the image.\n\nAnother nice addition to this paper would be an analysis of the data set. How\nmany tokens does the correct caption share with distractors on average? What\nkind of understanding is necessary to distinguish between the correct and\nincorrect captions? I think this kind of analysis really helps the reader\nunderstand why this task is worthwhile relative to the many other similar\ntasks. \n\nThe data generation technique is quite simple and wouldn't really qualify as a\nsignificant contribution, unless it worked surprisingly well.\n\n- Notes\n\nI couldn't find a description of the FFNN architecture in either the paper or\nthe supplementary material. It looks like some kind of convolutional network\nover the tokens, but the details are very unclear. I'm also confused about how\nthe Veq2Seq+FFNN model is applied to both classification and caption\ngeneration. Is the loglikelihood of the caption combined with the FFNN\nprediction during classification? Is the FFNN score incorporated during caption\ngeneration?\n\nThe fact that the caption generation model performs (statistically\nsignificantly) *worse* than random chance needs some explanation. How is this\npossible?\n\n528 - this description of the neural network is hard to understand. The final\nparagraph of the section makes it clear, however. Consider starting the section\nwith it.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "3", "REVIEWER_CONFIDENCE": "5"}], "abstract": "We introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing ``keywords'' (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded ``understanding'' of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and well-studied metric: the accuracy in detecting the true target among the decoys.  The paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a                    multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning.", "histories": [], "id": 501, "title": "Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task"}
