{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes a novel strategy for zero-resource translation where\n(source, pivot) and (pivot, target) parallel corpora are available. A teacher\nmodel for p(target|pivot) is first trained on the (pivot, target) corpus, then\na student model for p(target|source) is trained to minimize relative entropy\nwith respect to the teacher on the (source, pivot) corpus. When using\nword-level relative entropy over samples from the teacher, this approach is\nshown to outperform previous variants on standard pivoting, as well as other\nzero-resource strategies.\n\nThis is a good contribution: a novel idea, clearly explained, and with\nconvincing empirical support. Unlike some previous work, it makes fairly\nminimal assumptions about the nature of the NMT systems involved, and hence\nshould be widely applicable.\n\nI have only a few suggestions for further experiments. First, it would be\ninteresting to see how robust this approach is to more dissimilar source and\npivot languages, where intuitively the true p(target|source) and\np(target|pivot) will be further apart. Second, given the success of introducing\nword-based diversity, it was surprising not to see a sentence n-best or\nsentence-sampling experiment. This would be more costly, but not much more so\nsince you\u2019re already doing beam search with the teacher. Finally, related to\nthe previous, it might be interesting to explore transition from word-based\ndiversity to sentence-based as the student converges and no longer needs the\nsignal from low-probability words.\n\nSome further comments:\n\nline 241: Despite its simplicity -> Due to its simplicity\n\n277: target sentence y -> target word y\n\n442: I assume that K=1 and K=5 mean that you compare probabilities of the most\nprobable and 5 most probable words in the current context. If so, how is the\ncurrent context determined - greedily or with a beam?\n\nSection 4.2. The comparison with an essentially uniform distribution doesn\u2019t\nseem very informative here: it would be extremely surprising if p(y|z) were not\nsignificantly closer to p(y|x) than to uniform. It would be more interesting to\nknow to what extent p(y|z) still provides a useful signal as p(y|x) gets\nbetter. This would be easy to measure by comparing p(y|z) to models for p(y|x)\ntrained on different amounts of data or for different numbers of iterations.\nAnother useful thing to explore in this section would be the effect of the mode\napproximation compared to n-best for sentence-level scores.\n\n555: It\u2019s odd that word beam does worse than word greedy, since word beam\nshould be closer to word sampling. Do you have an explanation for this?\n\n582: The claimed advantage of sent-beam here looks like it may just be noise,\ngiven the high variance of these curves.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "In this paper the authors present a method for training a zero-resource NMT\nsystem by using training data from a pivot language. Unlike other approaches\n(mostly inspired in SMT), the author\u2019s approach doesn\u2019t do two-step\ndecoding. Instead, they use a teacher/student framework, where the teacher\nnetwork is trained using the pivot-target language pairs, and the student\nnetwork is trained using the source-pivot data and the teacher network\npredictions of the target language.\n\n- Strengths:\n\nThe results the authors present, show that their idea is promising. Also, the\nauthors present several sets of results that validate their assumptions.\n\n- Weaknesses:\n\nHowever, there are many points that need to be address before this paper is\nready for publication.\n\n1)            Crucial information is missing\n\nCan you flesh out more clearly how training and decoding happen in your\ntraining framework? I found out that the equations do not completely describe\nthe approach. It might be useful to use a couple of examples to make your\napproach clearer.\n\nAlso, how is the montecarlo sampling done? \n\n2)            Organization\nThe paper is not very well organized. For example, results are broken into\nseveral subsections, while they\u2019d better be presented together. \u00a0The\norganization of the tables is very confusing. Table 7 is referred before table\n6. This made it difficult to read the results.\n\n3)            Inconclusive results:\nAfter reading the results section, it\u2019s difficult to draw conclusions when,\nas the authors point out in their comparisons, this can be explained by the\ntotal size of the corpus involved in their methods (621 \u00a0). \n\n4)            Not so useful information:\nWhile I appreciate the fleshing out of the assumptions, I find that dedicating\na whole section of the paper plus experimental results is a lot of space. \n\n- General Discussion:\n\nOther:\n578: \u00a0We observe that word-level models tend to have lower valid loss compared\nwith sentence- level methods\u2026.\nIs it valid to compare the loss from two different loss functions?\n\nSec 3.2, the notations are not clear. What does script(Y) means?\nHow do we get p(y|x)? this is never explained\n\nEq 7 deserves some explanation, or better removed.\n320: What approach did you use? You should talk about that here\n392 : Do you mean 2016?\n\nNitty-gritty:\n\n742\u00a0 : import => important\n772 \u00a0: inline citation style\n778: can significantly outperform \n275: Assumption 2 needs to be rewritten \u2026 a target sentence y from x should\nbe close to that from its counterpart z.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "2", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThis is  a well written paper.\nThe paper is very clear for the most part.\nThe experimental comparisons are very well done.\nThe experiments are well designed and executed.\nThe idea of using KD for zero-resource NMT is impressive.\n\n- Weaknesses:\nThere were many sentences in the abstract and in other places in the paper\nwhere the authors stuff too much information into a single sentence. This could\nbe avoided. One can always use an extra sentence to be more clear.\nThere could have been a section where the actual method used could be explained\nin a more detailed. This explanation is glossed over in the paper. It's\nnon-trivial to guess the idea from reading the sections alone.\nDuring test time, you need the source-pivot corpus as well. This is a major\ndisadvantage of this approach. This is played down - in fact it's not mentioned\nat all. I could strongly encourage the authors to mention this and comment on\nit. \n\n- General Discussion:\n\nThis paper uses knowledge distillation to improve zero-resource translation.\nThe techniques used in this paper are very similar to the one proposed in Yoon\nKim et. al. The innovative part is that they use it for doing zero-resource\ntranslation. They compare against other prominent works in the field. Their\napproach also eliminates the need to do double decoding.\n\nDetailed comments:\n- Line 21-27 - the authors could have avoided this complicated structure for\ntwo simple sentences.\nLine 41 - Johnson et. al has SOTA on English-French and German-English.\nLine 77-79 there is no evidence provided as to why combination of multiple\nlanguages increases complexity. Please retract this statement or provide more\nevidence. Evidence in literature seems to suggest the opposite.\n\nLine 416-420 - The two lines here are repeated again. They were first mentioned\nin the previous paragraph.\nLine 577 - Figure 2 not 3!", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "5"}], "abstract": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, our method is able to train a source-to-target NMT model (``student'') without parallel corpora available guided by an existing pivot-to-target NMT model (``teacher'') on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.", "histories": [], "id": "779", "title": "A Teacher-Student Framework for Zero-Resource Neural Machine Translation"}
