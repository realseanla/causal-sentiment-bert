{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\ni. Well organized and easy to understand\nii. Provides detailed comparisons under various experimental settings and shows\nthe state-of-the-art performances\n\n- Weaknesses:\ni. In experiments, this paper compares previous supervised approaches, but the\nproposed method is the semi-supervised approach even if the training data is\nenough to train.\n\n- General Discussion:\nThis paper adopts a pre-training approach to improve Chinese word segmentation.\nBased on the transition-based neural word segmentation, this paper aims to\npre-train incoming characters with external resources (punctuation, soft\nsegmentation, POS, and heterogeneous training data) through multi-task\nlearning. That is, this paper casts each external source as an auxiliary\nclassification task. The experimental results show that the proposed method\nachieves the state-of-the-art performances in six out of seven datasets.\u00a0\n\nThis paper is well-written and easy to understand. A number of experiments\nprove the effectiveness of the proposed method. However, there exist an issue\nin this paper. The proposed method is a semi-supervised learning that uses\nexternal resources to pre-train the characters. Furthermore, this paper uses\nanother heterogeneous training datasets even if it uses the datasets only for\npre-training. Nevertheless, the baselines in the experiments are based on\nsupervised learning. In general, the performance of semi-supervised learning is\nbetter than that of supervised learning because semi-supervised learning makes\nuse of plentiful auxiliary information. In the experiments, this paper should\nhave compared the proposed method with semi-supervised approaches.\n\nPOST AUTHOR RESPONSE\n\nWhat the reviewer concerned is that this paper used additional\n\u201cgold-labeled\u201d dataset to pretrain the character embeddings. Some baselines\nin the experiments used label information, where the labels are predicted\nautomatically by their base models as the authors pointed out. When insisting\nsuperiority of a method, all circumstances should be same. Thus, even if the\ngold dataset isn\u2019t used to train the segmentation model directly, it seems to\nme that it is an unfair comparison because the proposed method used another\n\u201cgold\u201d dataset to train the character embeddings.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.", "histories": [], "id": "343", "title": "Neural Word Segmentation with Rich Pretraining"}
