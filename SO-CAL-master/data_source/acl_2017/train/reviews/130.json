{"reviews": [{"IMPACT": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThis paper proposes to apply NLP to speech transcripts (narratives and\ndescriptions) in order to identify patients with MCI (mild cognitive\nimpairment, ICD-10 code F06.7). The authors claim that they were able to\ndistinguish between healthy control participants and patients with MCI (lines\n141-144). However in the conclusion, lines 781-785, they say that \u201c\u2026\naccuracy ranging from 60% to 85% \u2026. means that it is not easy to distinguish\nbetween healthy subjects and those with cognitive impairments\u201d. So the paper\nbeginning is more optimistic than the conclusion but anyway the message is\nencouraging and the reader becomes curious to see more details about what has\nbeen actually done.\n\nThe corpus submitted in the dataset is constructed for 20 healthy patients and\n20 control participants only (20+20), and it is non-understandable for people\nwho do not speak Portuguese. It would be good to incorporate more technological\ndetails in the article and probably to include at least one example of a short\ntranscript that is translated to English, and eventually a (part of a) sample\nnetwork with embeddings for this transcript.\n\n- Weaknesses:\n\nThe paper starts with a detailed introduction and review of relevant work. Some\nof the cited references are more or less NLP background so they can be omitted\ne.g. (Salton 1989) in section 4.2.3. Other references are not directly related\nto the topic e.g. \u201csentiment classification\u201d and \u201cpedestrian detection in\nimages\u201d, lines 652-654, and they can be omitted too. In general lines\n608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to\ncompress the first 5 pages, focusing the review strictly on the paper topic,\nand consider the technological innovation in more detail, incl. samples of\nEnglish translations of the ABCD and/or Cindarela narratives.\n\nThe relatively short narratives in Portuguese esp. in ABCD dataset open the\nquestion how the similarities between words have been found, in order to\nconstruct word embeddings. In lines 272-289 the authors explain that they\ngenerate word-level networks from continuous word representations. What is the\nsource for learning the continuous word representations; are these the datasets\nABCD+Cinderella only, or external corpora were used? In lines 513-525 it is\nwritten that sub-word level (n-grams) networks were used to generate word\nembeddings. Again, what is the source for the training? Are we sure that the\ntwo kinds of networks together provide better accuracy? And what are the\n\u201cout-of-vocabulary words\u201d (line 516), from where they come?\n\n- General Discussion:\n\nIt is important to study how NLP can help to discover cognitive impairments;\nfrom this perspective the paper is interesting. Another interesting aspect is\nthat it deals with NLP for Portuguese, and it is important to explain how one\ncomputes embeddings for a language with relatively fewer resources (compared to\nEnglish). \n\nThe text needs revision: shortening sections 1-3, compressing 4.1 and adding\nmore explanations about the experiments. Some clarification about the NURC/SP\nN. 338 EF and 331 D2 transcription norms can be given.\n\nTechnical comments:\n\nLine 029: \u2018\u2026 as it a lightweight \u2026\u2019 -> shouldn\u2019t this be \u2018\u2026 as in\na lightweight \u2026\u2019\n\nLine 188: PLN -> NLP\n\nLine 264: \u2018out of cookie out of the cookie\u2019 \u2013 some words are repeated\ntwice \n\nTable 3, row 2, column 3: 72,0 -> 72.0\n\nLines 995-996: the DOI number is the same as the one at lines 1001-1002; the\nlink behind the title at lines 992-993 points to the next paper in the list", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "2", "SUBSTANCE": "2", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\nThis paper explores is problem of identifying patients with Mild Cognitive\nImpairment (MCI) by analyzing speech transcripts available from three different\ndatasets. A graph based method leveraging co-occurrence information between\nwords found in transcripts is described. Features are encoded using different\ncharacteristics of the graph lexical, syntactic properties, and many others. \nResults are reported using 5 fold cross validation using a number of\nclassifiers. Different models exhibit different performance across the three\ndatasets. This work targets a well defined problem and uses appropriate\ndatasets. \n\n- Weaknesses:\nThe paper suffers from several drawbacks\n1. The paper is hard to read due to incorrect usage of English. The current\nmanuscript would benefit a  lot from a review grammar and spellings. \n2. The main machine learning problem being addressed is poorly described. What\nwas a single instance of classification? It seems every transcripts was\nclassified as MCI or No MCI. If this is the case, the dataset descriptions\nshould describe the numbers at a transcript level. Tables 1,2, and 3 should\ndescribe the data not the study that produced the transcripts. The age of the\npatients is irrelevant for the classification task. A lot of text (2 pages) is\nconsumed in simply describing the datasets with details that do not affect the\nend classification task. Also, I was unsure why numbers did not add up. For\ne.g.: in section 4.1.1 the text says 326 people were involved. But the total\nnumber of males and females in Table 1 are less than 100?\n3. What is the motivation behind enriching the graph? Why not represent each\nword by a node in the graph and connect them by the similarity between their\nvectors, irrespective of co-occurrence?\n4. The datsets are from a biomedical domain. No domain specific tools have been\nleveraged.\n5. Since dataset class distribution is unclear, it is unclear to determine if\naccuracy is a good measure for evaluation. In either case, since it is a binary\nclassification task, F1 would have been a desirable metric. \n6. Results are reported unto 4 decimal places on very small datasets (43\ntranscripts) without statistical tests over increments. Therefore, it is\nunclear if the gains are significant.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "3", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper describes a novel application of mostly existing representations,\nfeatures sets, and methods: namely, detecting Mild Cognitive Impairment (MCI) \nin speech narratives. The nature of the problem, datasets, and domain are\nthoroughly described. While missing some detail, the proposed solution and\nexperiments sound reasonable. Overall, I found the study interesting and\ninformative.\n\nIn terms of drawbacks, the paper needs some considerable editing to improve\nreadability. Details on some key concepts appear to be missing. For example, \ndetails on the multi-view learning used are omitted; the set of \u201clinguistic\nfeatures\u201d needs to be clarified; it is not entirely clear what datasets were\nused to generate the word embeddings (presumably the 3 datasets described in\nthe paper, which appear to be too small for that purpose\u2026). It is also not\nclear why disfluencies (filled pauses, false starts, repetitions, etc.) were\nremoved from the dataset. One might suggest that they are important features in\nthe context of MCI. It is also not clear why the most popular tf-idf weighting\nscheme was not used for the BoW classifications. In addition, tests for\nsignificance are not provided to substantiate the conclusions from the\nexperiments. Lastly, the related work is described a bit superficially. \n\nDetailed comments are provided below:\n\nAbstract: The abstract needs to be shortened. See detailed notes below.\n\nLines 22,23 need rephrasing.            \u201cHowever, MCI disfluencies produce\nagrammatical speech impacting in parsing results\u201d \u2192 impacting the parsing\nresults?\n\nLines 24,25: You mean correct grammatical errors in transcripts manually? It is\nnot clear why this should be performed, doesn\u2019t the fact that grammatical\nerrors are present indicate MCI? \u2026 Only after reading the Introduction and\nRelated Work sections it becomes clear what you mean. Perhaps include some\nexamples of disfluencies.\n\nLines 29,30 need rephrasing: \u201cas it a lightweight and language  independent\nrepresentation\u201d\n\nLines 34-38 need rephrasing: it is not immediately clear which exactly are the\n3 datasets. Maybe: \u201cthe other two: Cinderella and \u2026 \u201c            \n\nLine 70: \u201c15% a year\u201d \u2192 Not sure what exactly \u201cper year\u201d means\u2026\n\nLine 73 needs rephrasing.\n\nLines 115 - 117: It is not obvious why BoW will also have problems with\ndisfluencies, some explanation will be helpful.\n\nLines 147 - 149: What do you mean by \u201cthe best scenario\u201d?\n\nLine 157: \u201cin public corpora of Dementia Bank\u201d \u2192 a link or citation to\nDementia Bank will be helpful. \n\nLine 162: A link or citation describing the \u201cPicnic picture of the Western\nAphasia Battery\u201d will be helpful.\n\nLine 170: An explanation as to what the WML subtest is will be helpful.\n\nLine 172 is missing citations.\n\nLines 166 - 182: This appears to be the core of the related work and it is\ndescribed a bit superficially. For example, it will be helpful to know\nprecisely what methods were used to achieve these tasks and how they compare to\nthis study.\n\nLine 185: Please refer to the conference citation guidelines. I believe they\nare something along these lines: \u201cAluisio et al. (2016)  used\u2026\u201d\n\nLine 188: The definition of \u201cPLN\u201d appears to be missing.\n\nLines 233 - 235 could you some rephrasing. Lemmatization is not necessarily a\nlast step in text pre-processing and normalization, in fact there are also\nadditional common normalization/preprocessing steps omitted. \n\nLines 290-299: Did you create the word embeddings using the MCI datasets or\nexternal datasets?\n\nLine 322: consisted of \u2192 consist of\n\nLines 323: 332 need to be rewritten. ... \u201cmanually segmented of the\nDementiaBank and Cinderella\u201d \u2192  What do you mean by segmented, segmented\ninto sentences? Why weren\u2019t all datasets automatically segmented?; \u201cABCD\u201d\nis not defined; You itemized the datasets in i) and ii), but subsequently  you\nrefer to 3 dataset, which is a bit confusing. Maybe one could explicitly name\nthe datasets, as opposed to referring to them as \u201cfirst\u201d, \u201csecond\u201d,\n\u201cthird\u201d.\n\nTable 1 Caption: The demographic information is present, but there are no any\nadditional statistics of the dataset, as described.\n\nLines 375 - 388:  It is not clear why filled pauses, false starts, repetitions,\netc. were removed. One might suggest that they are important features in the\ncontext of MCI \u2026.\n\nLine 399: \u2026 multidisciplinary team with psychiatrists ... \u2192 consisting of\npsychiatrists\u2026\n\nLines 340-440: A link or citation describing the transcription norms will be\nhelpful.\n\nSection 4.2.1. It is not clear what dataset was used to generate the word\nembeddings. \n\nLine 560. The shortest path as defined in feature 6?\n\nSection \u201c4.2.2 Linguistic Features\u201d needs to be significantly expanded for\nclarity. Also, please check the conference guidelines regarding additional\npages (\u201cSupplementary Material\u201d).\n\nLine 620: \u201cIn this work term frequency was \u2026\u201d \u2192 \u201cIn this work, term\nfrequency was \u2026\u201d Also, why not tf-idf, as it seems to be the most common\nweighting scheme? \n\nThe sentence on lines 641-645 needs to be rewritten.\n\nLine 662: What do you mean by \u201cthe threshold parameter\u201d? The threshold for\nthe word embedding cosine distance?\n\nLine 735 is missing a period.\n\nSection 4.3 Classification Algorithms: Details on exactly what scheme of\nmulti-view learning was used are entirely omitted. Statistical significance of\nresult differences is not provided.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts.  In this paper, we modeled transcripts into complex networks and enriched them with word embedding (CNE) to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments.", "histories": [], "id": "130", "title": "Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts"}
