{"reviews": [{"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n   - The paper states clearly the contributions from the beginning \n   - Authors provide system and dataset\n   - Figures help in illustrating the approach\n   - Detailed description of the approach\n   - The authors test their approach performance on other datasets and compare\nto other published work\n\n- Weaknesses:\n   -The explanation of methods in some paragraphs is too detailed and there is\nno mention of other work and it is repeated in the corresponding method\nsections, the authors committed to address this issue in the final version.\n   -README file for the dataset [Authors committed to add README file]\n\n- General Discussion:\n   - Section 2.2 mentions examples of DBpedia properties that were used as\nfeatures. Do the authors mean that all the properties have been used or there\nis a subset? If the latter please list them. In the authors' response, the\nauthors explain in more details this point and I strongly believe that it is\ncrucial to list all the features in details in the final version for clarity\nand replicability of the paper. \n   - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might\nbe beneficial to add that the input is word embeddings (similarly to Lample et\nal.)\n   - Figure 3, KNs in source language or in English? (since the mentions have\nbeen translated to English). In the authors' response, the authors stated that\nthey will correct the figure.\n   - Based on section 2.4 it seems that topical relatedness implies that some\nfeatures are domain dependent. It would be helpful to see how much domain\ndependent features affect the performance. In the final version, the authors\nwill add the performance results for the above mentioned features, as mentioned\nin their response. \n   - In related work, the authors make a strong connection to Sil and Florian\nwork where they emphasize the supervised vs. unsupervised difference. The\nproposed approach is still supervised in the sense of training, however the\ngeneration of training data doesn\u2019t involve human interference", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\n- Very impressive resource\n\n- fully automatic system - particularly suitable for cross-lingual learning\nacross many languages\n\n- Good evaluation both within and outside wikipedia. Good comparison to works\nthat employed manual resources.\n\n- Weaknesses:\n\n- The clarity of the paper can be improved.\n\n- General Discussion:\n\nThis paper presents \"a simple yet effective framework that can extract names\nfrom 282 languages and link them to an English KB\". Importantly, the system is\nfully automatic, which is particularly important when aiming to learn across\nsuch a large number of languages. Although this is far from trivial, the\nauthors are able to put their results in context and provide evaluation both\nwithin and outside of wikipedia - I particularly like the way the put their\nwork in the context of previous work that uses manual resources, it is a good\nscientific practice and I am glad they do not refrain from doing that in worry\nthat this would not look good.\n\nThe clarity of the paper can improve. This is not an easy paper to write due to\nthe quite complex process and the very large scale resource it generates.\nHowever, the paper is not very well organized and at many points I felt that I\nam reading a long list of details. I encourage the authors to try and give the\npaper a better structure. As one example, I would be happy to see a better\nproblem definition and high level motivations from the very beginning. Other\nexamples has to do with better exposition of the motivations, decisions and\ncontributions in each part of the paper (I admire the efforts the authors have\nalready made, but I think this can done even better). This is an important\npaper and it deserves a clearer presentation.\n\nAll in all I like the paper and think it provides an important resource. I\nwould like to see this paper presented in ACL 2017.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating ``silver-standard'' annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.", "histories": [], "id": "71", "title": "Cross-lingual Name Tagging and Linking for 282 Languages"}
