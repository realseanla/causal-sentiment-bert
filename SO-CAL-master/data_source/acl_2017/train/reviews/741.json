{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper presents a graph-based approach for producing sense-disambiguated\nsynonym sets from a collection of undisambiguated synonym sets.  The authors\nevaluate their approach by inducing these synonym sets from Wiktionary and from\na collection of Russian dictionaries, and then comparing pairwise synonymy\nrelations (using precision, recall, and F1) against WordNet and BabelNet (for\nthe English synonym sets) or RuThes and Yet Another RussNet (for the Russian\nsynonym sets).\n\nThe paper is very well written and structured.              The experiments and\nevaluations\n(or at least the prose parts) are very easy to follow.              The methodology\nis\nsensible and the analysis of the results cogent.  I was happy to observe that\nthe objections I had when reading the paper (such as the mismatch in vocabulary\nbetween the synonym dictionaries and gold standards) ended up being resolved,\nor at least addressed, in the final pages.\n\nThe one thing about the paper that concerns me is that the authors do not seem\nto have properly understood the previous work, which undercuts the stated\nmotivation for this paper.\n\nThe first instance of this misunderstanding is in the paragraph beginning on\nline 064, where OmegaWiki is lumped in with Wiktionary and Wikipedia in a\ndiscussion of resources that are \"not formally structured\" and that contain\n\"undisambiguated synonyms\".  In reality, OmegaWiki is distinguished from the\nother two resources by using a formal structure (a relational database) based\non word senses rather than orthographic forms.              Translations, synonyms,\nand\nother semantic annotations in OmegaWiki are therefore unambiguous.\n\nThe second, and more serious, misunderstanding comes in the three paragraphs\nbeginning on lines 092, 108, and 120.  Here the paper claims that both BabelNet\nand UBY \"rely on English WordNet as a pivot for mapping of existing resources\"\nand criticizes this mapping as being \"error-prone\".  Though it is true that\nBabelNet uses WordNet as a pivot, UBY does not.  UBY is basically a\ngeneral-purpose specification for the representation of lexical-semantic\nresources and of links between them.  It exists independently of any given\nlexical-semantic resource (including WordNet) and of any given alignment\nbetween resources (including ones based on \"similarity of dictionary\ndefinitions\" or \"cross-lingual links\").  Its maintainers have made available\nvarious databases adhering to the UBY spec; these contain a variety of\nlexical-semantic resources which have been aligned with a variety of different\nmethods.  A given UBY database can be *queried* for synsets, but UBY itself\ndoes not *generate* those synsets.  Users are free to produce their own\ndatabases by importing whatever lexical-semantic resources and alignments\nthereof are best suited to their purposes.  The three criticisms of UBY on\nlines 120 to 125 are therefore entirely misplaced.\n\nIn fact, I think at least one of the criticisms is not appropriate even with\nrespect to BabelNet.  The authors claim that Watset may be superior to BabelNet\nbecause BabelNet's mapping and use of machine translation are error-prone.  The\nimplication here is that Watset's method is error-free, or at least\nsignificantly less error-prone.  This is a very grandiose claim that I do not\nbelieve is supported by what the authors ought to have known in advance about\ntheir similarity-based sense linking algorithms and graph clustering\nalgorithms, let alone by the results of their study.  I think this criticism\nought to be moderated.              Also, I think the third criticism (BabelNet's\nreliance\non WordNet as a pivot) somewhat misses the point -- surely the most important\nissue to highlight isn't the fact that the pivot is English, but rather that\nits synsets are already manually sense-annotated.\n\nI think the last paragraph of \u00a71 and the first two paragraphs of \u00a72 should be\nextensively revised. They should focus on the *general* problem of generating\nsynsets by sense-level alignment/translation of LSRs (see Gurevych et al., 2016\nfor a survey), rather than particularly on BabelNet (which uses certain\nparticular methods) and UBY (which doesn't use any particular methods, but can\naggregate the results of existing ones).  It may be helpful to point out\nsomewhere that although alignment/translation methods *can* be used to produce\nsynsets or to enrich existing ones, that's not always an explicit goal of the\nprocess.  Sometimes it's just a serendipitous (if noisy) side-effect of\naligning/translating resources with differing granularities.\n\nFinally, at several points in the paper (lines 153, 433), the \"synsets\" of TWSI\nof JoBimText are criticized for including too many words that are hypernyms,\nco-hypnomyms, etc. instead of synonyms.  But is this problem really unique to\nTWSI and JoBimText?  That is, how often do hypernyms, co-hypernyms, etc. appear\nin the output of Watset?  (We can get only a very vague idea of this from\ncomparing Tables 3 and 5, which analyze only synonym relations.)  If Watset\nreally is better at filtering out words with other semantic relations, then it\nwould be nice to see some quantitative evidence of this.\n\nSome further relatively minor points that should nonetheless be fixed:\n\n* Lines 047 to 049: The sentence about Kiselev et al. (2015) seems rather\nuseless.  Why bother mentioning their analysis if you're not going to tell us\nwhat they found?\n\n* Line 091: It took me a long time to figure out how \"wat\" has any relation to\n\"discover the correct word sense\".  I suppose this is supposed to be a pun on\n\"what\".  Maybe it would have been better to call the approach \"Whatset\"?  Or at\nleast consider rewording the sentence to better explain the pun.\n\n* Figure 2 is practically illegible owing to the microscopic font.  Please\nincrease the text size!\n\n* Similarly, Tables 3, 4, and 5 are too small to read comfortably.  Please use\na larger font.              To save space, consider abbreviating the headers (\"P,\n\"R\",\n\"F1\") and maybe reporting scores in the range 0\u2013100 instead of 0\u20131, which\nwill eliminate a leading 0 from each column.\n\n* Lines 517\u2013522: Wiktionary is a moving target.  To help others replicate or\ncompare against your work, please indicate the date of the Wiktionary database\ndump you used.\n\n* Throughout: The constant switching between Times and Computer Modern is\ndistracting.  The root of this problem is a longstanding design flaw in the ACL\n2017 LaTeX style file, but it's exacerbated by the authors' decision to\noccasionally set numbers in math mode, even in running text.  Please fix this\nby removing\n\n\\usepackage{times}\n\nfrom the preamble and replacing it with either\n\n\\usepackage{newtxtext}\n\\usepackage{newtxmath}\n\nor\n\n\\usepackage{mathptmx}\n\nReferences:\n\nI Gurevych, J. Eckle-Kohler, and M. Matuschek, 2016. Linked Lexical Knowledge\nBases: Foundations and Applications, volume 34 of Synthesis Lectures on Human\nLanguage Technologies, chapter 3: Linking Algorithms, pages 29-44. Morgan &\nClaypool.\n\n----\n\nI have read the author response.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThe paper proposes a new method for word sense induction from synonymy\ndictionaries. The method presents a conceptual improvement over existing ones\nand demonstrates robust performance in empirical evaluation. The evaluation was\ndone thoroughly, using a number of benchmarks and strong baseline methods. \n\n- Weaknesses:\n\nJust a couple of small points. I would like to see more discussion of the\nnature of the evaluation. First, one observes that all models' scores are\nrelatively low, under 50% F1. Is there room for much improvement or is there a\nnatural ceiling of performance due to the nature of the task? The authors\ndiscuss lexical sparsity of the input data but I wonder how much of the\nperformance gap this sparsity accounts for. \nSecond, I would also like to see some discussion of the evaluation metric\nchosen. It is known that word senses can be analyzed at different levels of\ngranularity, which can naturally affect the scores of any system.\nAnother point is that it is not clear how the authors obtained vectors for word\nsenses that they used in 3.4, if the senses are only determined after this\nstep, and anyway senses are not marked in the input corpora. \n\n- General Discussion:\n\nI recommend the paper for presentation at the ACL Meeting. Solid work.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}], "abstract": "This paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings. First, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary. Second, we apply word sense induction to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into synsets. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of F-score on three gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources.", "histories": [], "id": "741", "title": "Automatic Induction of Synsets from a Graph of Synonyms"}
