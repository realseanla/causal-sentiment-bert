{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This work describes a gated attention-based recurrent neural network method for\nreading comprehension and question answering. This method employs a\nself-matching attention technique to counterbalance the limited context\nknowledge of gated attention-based recurrent neural networks when processing\npassages. Finally, authors use pointer networks  with signals from the question\nattention-based vector to predict the beginning and ending of the answer.\nExperimental results with the SQuAD dataset offer state-of-the-art performance\ncompared with several recent approaches. \n\nThe paper is well-written, structured and explained. As far as I know, the\nmathematics look also good. In my opinion, this is a very interesting work\nwhich may be useful for the question answering community.\n\nI was wondering if the authors have plans to release the code of this approach.\nFrom that perspective, I miss a bit of information about the technology used\nfor the implementation (theano, CUDA, CuDNN...), which may be useful for\nreaders.\n\nI would appreciate if authors could perform a test of statistical significance\nof the results. That would highlight even more the quality of your results.\n\nFinally, I know that the space may be a constraint, but an evaluation including\nsome additional dataset would validate more your work.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "2"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper presents the gated self-matching network for reading comprehension\nstyle question answering. There are three key components in the solution: \n\n(a) The paper introduces the gated attention-based recurrent network to obtain\nthe question-aware representation for the passage. Here, the paper adds an\nadditional gate to attention-based recurrent networks to determine the\nimportance of passage parts and attend to the ones relevant to the question.\nHere they use word as well as character embeddings to handle OOV words.\nOverall, this component is inspired from Wang and Jiang 2016.\n\n(b) Then the paper proposes a self-matching attention mechanism to improve the\nrepresentation for the question and passage by looking at wider passage context\nnecessary to infer the answer. This component is completely novel in the paper.\n\n(c) At the output layer, the paper uses pointer networks to locate answer\nboundaries. This is also inspired from Wang and Jiang 2016\n\nOverall, I like the paper and think that it makes a nice contribution.\n\n- Strengths:\n\nThe paper clearly breaks the network into three component for descriptive\npurposes, relates each of them to prior work and mentions its novelties with\nrespect to them. It does a sound empirical analysis by describing the impact of\neach component by doing an ablation study. This is appreciated.\n\nThe results are impressive!\n\n- Weaknesses:\n\nThe paper describes the results on a single model and an ensemble model. I\ncould not find any details of the ensemble and how was it created. I believe it\nmight be the ensemble of the character based and word based model. Can the\nauthors please describe this in the rebuttal and the paper.\n\n- General Discussion:\n\nAlong with the ablation study, it would be nice if we can have a\nqualitative analysis describing some example cases where the components of\ngating, character embedding, self embedding, etc. become crucial ... where a\nsimple model doesn't get the question right but adding one or more of these\ncomponents helps. This can go in some form of appendix or supplementary.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.", "histories": [], "id": "335", "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering"}
