{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper presents a neural model for predicting SQL queries directly from\nnatural language utterances, without going through an intermediate formalism.\nIn addition, an interactive online feedback loop is proposed and tested on a\nsmall scale.\n\n- Strengths:\n\n1\\ The paper is very clearly written, properly positioned, and I enjoyed\nreading it.\n\n2\\ The proposed model is tested and shown to perform well on 3 different\ndomains (academic, geographic queries, and flight booking)\n\n3\\ The online feedback loop is interesting and seems promising, despite of the\nsmall scale of the experiment.\n\n4\\ A new semantic corpus is published as part of this work, and additionally\ntwo\nexisting corpora are converted to SQL format, which I believe would be\nbeneficial for future work in this area.\n\n- Weaknesses / clarifications:\n\n1\\ Section 4.2 (Entity anonymization) - I am not sure I understand the choice\nof the length of span for querying the search engine. Why and how is it\nprogressively reduced? (line 333).\n\n2\\ Section 5 (Benchmark experiments) - If I understand correctly, the feedback\nloop (algorithm 1) is *not* used for these experiments. If this is indeed the\ncase, I'm not sure when does data augmentation occur. Is all the annotated\ntraining data augmented with paraphrases? When is the \"initial data\" from\ntemplates added? Is it also added to the gold training set? If so, I think it's\nnot surprising that it doesn't help much, as the gold queries may be more\ndiverse.  In any case, I think this should be stated more clearly. In addition,\nI think it's interesting to see what's the performance of the \"vanilla\" model,\nwithout any augmentation, I think that this is not reported in the paper.\n\n3\\ Tables 2 and 3: I find the evaluation metric used here somewhat unclear. \nDoes the accuracy measure the correctness of the execution of the query (i.e.,\nthe retrieved answer) as the text seem to indicate? (Line 471 mentions\n*executing* the query). Alternatively, are the queries themselves compared? (as\nseems to be the case for Dong and Lapata in Table 2). If this is done\ndifferently for different systems (I.e., Dong and Lapata), how are these\nnumbers comparable? In addition, the text mentions the SQL model has \"slightly\nlower accuracy than the best non-SQL results\" (Line 515), yet in table 2 the\ndifference is almost 9 points in accuracy.  What is the observation based upon?\nWas some significance test performed? If not, I think the results are still\nimpressive for direct to SQL parsing, but that the wording should be changed,\nas the difference in performance does seem significant.\n\n4\\ Line 519 - Regarding the data recombination technique used in Jia and Liang\n(2016): Since this technique is applicable in this scenario, why not try it as\nwell?  Currently it's an open question whether this will actually improve\nperformance. Is this left as future work, or is there something prohibiting the\nuse of this technique?\n\n5\\ Section 6.2 (Three-stage online experiment) - several details are missing /\nunclear:\n\n* What was the technical background of the recruited users?\n\n* Who were the crowd workers, how were they recruited and trained?\n\n* The text says \"we recruited 10 new users and asked them to issue at least 10\nutterances\". Does this mean 10 queries *each* (e.g., 100 overall), or 10 in\ntotal (1 for each).\n\n* What was the size of the initial (synthesized) training  set? \n\n* Report statistics of the queries - some measure of their lexical variability\n/ length / complexity of the generated SQL? This seems especially important for\nthe first phase, which is doing surprisingly well. Furthermore, since SCHOLAR\nuses SQL and NL, it would have been nice if it were attached to this\nsubmission, to allow its review during this period.\n\n6\\ Section 6.3 (SCHOLAR dataset)\n\n* The dataset seems pretty small in modern standards (816 utterances in total),\nwhile one of the main advantages of this process is its scalability. What\nhindered the creation of a much larger dataset?\n\n* Comparing performance - is it possible to run another baseline on this newly\ncreated dataset to compare against the reported 67% accuracy obtained in this\npaper (line 730).\n\n7\\ Evaluation of interactive learning experiments (Section 6): I find the\nexperiments to be somewhat hard to replicate as they involve manual queries of\nspecific annotators. For example, who's to say if the annotators in the last\nphase just asked simpler questions? I realise that this is always problematic\nfor online learning scenarios, but I think that an effort should be made\ntowards an objective comparison. For starters, the statistics of the queries\n(as I mentioned earlier) is a readily available means to assess whether this\nhappens. Second, maybe there can be some objective held out test set? This is\nproblematic as the model relies on the seen queries, but scaling up the\nexperiment (as I suggested above) might mitigate this risk. Third, is it\npossible to assess a different baseline using this online technique? I'm not\nsure whether this is applicable given that previous methods were not devised as\nonline learning methods.\n\n- Minor comments:\n\n1\\ Line 48 - \"requires\" -> \"require\"\n\n2\\ Footnote 1 seems too long to me. Consider moving some of its content to the\nbody of the text.\n\n3\\ Algorithm 1: I'm not sure what \"new utterances\" refers to (I guess it's new\nqueries from users?). I think that an accompanying caption to the algorithm\nwould make the reading easier.\n\n4\\ Line 218 - \"Is is\" -> \"It is\"\n\n5\\ Line 278 mentions an \"anonymized\" utterance. This confused me at the first\nreading, and if I understand correctly it refers to the anonymization described\nlater in 4.2. I think it would be better to forward reference this. \n\n- General Discussion:\n\nOverall, I like the paper, and given answers to the questions I raised above,\nwould like to see it appear in the conference.\n\n- Author Response:\n\nI appreciate the detailed response made by the authors, please include these\ndetails in a final version of the paper.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes a simple attention-based RNN model for generating SQL\nqueries from natural language without any intermediate representation. Towards\nthis end they employ a data augmentation approach where more data is\niteratively collected from crowd annotation, based on user feedback on how well\nthe SQL queries produced by the model do. Results on both the benchmark and\ninteractive datasets show that data augmentation is a promising approach.\n\nStrengths:\n\n- No intermediate representations were used. \n\n- Release of a potentially valuable dataset on Google SCHOLAR.\n\nWeaknesses:\n\n- Claims of being comparable to state of the art when the results on GeoQuery\nand\nATIS do not support it. \n\nGeneral Discussion:\n\nThis is a sound work of research and could have future potential in the way\nsemantic parsing for downstream applications is done. I was a little\ndisappointed with the claims of \u201cnear-state-of-the-art accuracies\u201d on ATIS\nand GeoQuery, which doesn\u2019t seem to be the case (8 points difference from\nLiang et. al., 2011)). And I do not necessarily think that getting SOTA numbers\nshould be the focus of the paper, it has its own significant contribution. I\nwould like to see this paper at ACL provided the authors tone down their\nclaims, in addition I have some questions for the authors.\n\n- What do the authors mean by minimal intervention? Does it mean minimal human\nintervention, because that does not seem to be the case. Does it mean no\nintermediate representation? If so, the latter term should be used, being less\nambiguous.\n\n- Table 6: what is the breakdown of the score by correctness and\nincompleteness?\nWhat % of incompleteness do these queries exhibit?\n\n- What is expertise required from crowd-workers who produce the correct SQL\nqueries? \n\n- It would be helpful to see some analysis of the 48% of user questions which\ncould not be generated.\n\n- Figure 3 is a little confusing, I could not follow the sharp dips in\nperformance without paraphrasing around the 8th/9th stages. \n\n- Table 4 needs a little more clarification, what splits are used for obtaining\nthe ATIS numbers?\n\nI thank the authors for their response.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes an approach to learning a semantic parser using an\nencoder-decoder neural architecture, with the distinguishing feature that the\nsemantic output is full SQL queries. The method is evaluated over two standard\ndatasets (Geo880 and ATIS), as well as a novel dataset relating to document\nsearch.\n\nThis is a solid, well executed paper, which takes a relatively well\nestablished technique in the form of an encoder-decoder with some trimmings\n(e.g. data augmentation through paraphrasing), and uses it to generate SQL\nqueries, with the purported advantage that SQL queries are more expressive\nthan other semantic formalisms commonly used in the literature, and can be\nedited by untrained crowd workers (familiar with SQL but not semantic\nparsing). I buy that SQL is more expressive than the standard semantic\nformalisms, but then again, were there really any queries in any of your three\ndatasets where the standard formalisms are unable to capture the full\nsemantics of the query? I.e. are they really the best datasets to showcase the\nexpressivity of SQL? Also, in terms of what your model learns, what fraction\nof SQL does it actually use? I.e. how much of the extra expressivity in SQL is\nyour model able to capture? Also, does it have biases in terms of the style of\nqueries that it tends to generate? That is, I wanted to get a better sense of\nnot just the *potential* of SQL, but the actuality of what your model is able\nto capture, and the need for extra expressivity relative to the datasets you\nexperiment over. Somewhat related to this, at the start of Section 5, you\nassert that it's harder to directly produce SQL. You never actually show this,\nand this seems to be more a statement of the expressivity of SQL than anything\nelse (which returns me to the question of how much of SQL is the model\nactually generating).\n\nNext, I would really have liked to have seen more discussion of the types of\nSQL queries your model generates, esp. for the second part of the evaluation,\nover the SCHOLAR dataset. Specifically, when the query is ill-formed, in what\nways is it ill-formed? When a crowd worker is required to post-edit the query,\nhow much effort does that take them? Equally, how correct are the crowd\nworkers at constructing SQL queries? Are they always able to construct perfect\nqueries (experience would suggest that this is a big ask)? In a similar vein\nto having more error analysis in the paper, I would have liked to have seen\nagreement numbers between annotators, esp. for Incomplete Result queries,\nwhich seems to rely heavily on pre-existing knowledge on the part of the\nannotator and therefore be highly subjective.\n\nOverall, what the paper achieves is impressive, and the paper is well\nexecuted; I just wanted to get more insights into the true ability of the\nmodel to generate SQL, and a better sense of what subset of the language it\ngenerates.\n\nA couple of other minor things:\n\nl107: \"non-linguists can write SQL\" -- why refer to \"non-linguists\" here? Most\nlinguists wouldn't be able to write SQL queries either way; I think the point\nyou are trying to make is simply that \"annotators without specific training in\nthe semantic translation of queries\" are able to perform the task\n\nl218: \"Is is\" -> \"It is\"\n\nl278: it's not clear what an \"anonymized utterance\" is at this point of the\npaper\n\nl403: am I right in saying that you paraphrase only single words at a time?\nPresumably you exclude \"entities\" from paraphrasing?\n\nl700: introduce a visual variable in terms of line type to differentiate the\nthree lines, for those viewing in grayscale\n\nThere are various inconsistencies in the references, casing issues\n(e.g. \"freebase\", \"ccg\"), Wang et al. (2016) is missing critical publication\ndetails, and there is an \"In In\" for Wong and Mooney (2007)", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.", "histories": [], "id": "726", "title": "Learning a Neural Semantic Parser from User Feedback"}
