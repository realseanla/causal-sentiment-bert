{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper describes an extension of word embedding methods to also provide\nrepresentations for phrases and concepts that correspond to words.  The method\nworks by fixing an identifier for groups of phrases, words and the concept that\nall denote this concept, replace the occurrences of the phrases and words by\nthis identifier in the training corpus, creating a \"tagged\" corpus, and then\nappending the tagged corpus to the original corpus for training.  The\nconcept/phrase/word sets are taken from an ontology.  Since the domain of\napplication is biomedical, the related corpora and ontologies are used.  The\nresearchers also report on the generation of a new test dataset for word\nsimilarity and relatedness for real-world entities, which is novel.\n\nIn general, the paper is nicely written.  The technique is pretty natural,\nthough not a very substantial contribution. The scope of the contribution is\nlimited, because of focused evaluation within the biomedical domain.\n\nMore discussion of the generated test resource could be useful.  The resource\ncould be the true interesting contribution of the paper.\n\nThere is one\nsmall technical problem, but that is probably just a matter of mathematical\nexpression than implementation.\n\nTechnical problem:\nEq. 8: The authors want to define the MAP calculation.                          This\nis a\ngood\nidea,\nthought I think that a natural cut-off could be defined, rather than ranking\nthe entire vocabulary.                          Equation 8 does not define a\nprobability;\nit is\nquite\neasy to show this, even if the size of the vocabulary is infinite.  So you need\nto change the explanation (take out talk of a probability).\n\nSmall corrections:\nline:\n556: most concepts has --> most concepts have", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "Summary: This paper presents a model for embedding words, phrases and concepts\ninto vector spaces. To do so, it uses an ontology of concepts, each of which is\nmapped to phrases. These phrases are found in text corpora and treated as\natomic symbols. Using this, the paper uses what is essentially the skip-gram\nmethod to train embeddings for words, the now atomic phrases and also the\nconcepts associated with them. The proposed work is evaluated on the task of\nconcept similarity and relatedness using UMLS and Yago to act as the backing\nontologies.\n\nStrengths:\n\nThe key question addressed by the paper is that phrases that are not lexically\nsimilar can be semantically close and, furthermore, not all phrases are\ncompositional in nature. To this end, the paper proposes a plausible model to\ntrain phrase embeddings. The trained embeddings are shown to be competitive or\nbetter at identifying similarity between concepts.\n\nThe software released with the paper could be useful for biomedical NLP\nresearchers.\n\n- Weaknesses:\n\nThe primary weakness of the paper is that the model is not too novel. It is\nessentially a tweak to skip-gram. \n\nFurthermore, the full model presented by the paper doesn't seem to be the best\none in the results (in Table 4). On the two Mayo datasets, the Choi baseline is\nsubstantially better. A similar trend seems to dominate Table 6 too. On the\nlarger UMNSRS data, the proposed model is at best competitive with previous\nsimpler models (Chiu).\n\n- General Discussion:\n\nThe paper says that it is uses known phrases as distant supervision to train\nembeddings. However, it is not clear what the \"supervision\" here is. If I\nunderstand the paper correctly, every occurrence of a phrase associated with a\nconcept provides the context to train word embeddings. But this is not\nsupervision in the traditional sense (say for identifying the concept in the\ntext or other such predictive tasks). So the terminology is a bit confusing.\n\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of\nthe paper.\n\nThe use of \\beta to control for compositionality of phrases by words is quite\nsurprising. Essentially, this is equivalent to saying that there is a single\nglobal constant that decides \"how compositional\" any phrase should be. The\nsurprising part here is that the actual values of \\beta chosen by cross\nvalidation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which\nbasically argues against compositionality. \n\nThe experimental setup for table 4 needs some explanation. The paper says that\nthe data labels similarity/relatedness of concepts (or entities). However, if\nthe concepts-phrases mapping is really many-to-many, then how are the\nphrase/word vectors used to compute the similarities? It seems that we can only\nuse the concept vectors.\n\nIn table 5, the approximate phr method (which approximate concepts with the\naverage of the phrases in them) is best performing. So it is not clear why we\nneed the concept ontology. Instead, we could have just started with a seed set\nof phrases to get the same results.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "2", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The authors presents a method to jointly embed words, phrases and concepts,\nbased on plain text corpora and a manually-constructed ontology, in which\nconcepts are represented by one or more phrases. They apply their method in the\nmedical domain using the UMLS ontology, and in the general domain using the\nYAGO ontology. To evaluate their approach, the authors compare it to simpler\nbaselines and prior work, mostly on intrinsic similarity and relatedness\nbenchmarks. They use existing benchmarks in the medical domain, and use\nmechanical turkers to generate a new general-domain concept similarity and\nrelatedness dataset, which they also intend to release. They report results\nthat are comparable to prior work.\n\nStrengths:\n\n- The proposed joint embedding model is straightforward and makes reasonable\nsense to me. Its main value in my mind is in reaching a (configurable) middle\nground between treating phrases as atomic units on one hand to considering\ntheir\ncompositionallity on the other. The same approach is applied to concepts being\n\u2018composed\u2019 of several representative phrases.\n\n-  The paper describes a decent volume of work, including model development,\nan additional contribution in the form of a new evaluation dataset, and several\nevaluations and analyses performed.\n\nWeaknesses:\n\n- The evaluation reported in this paper includes only intrinsic tasks, mainly\non similarity/relatedness datasets. As the authors note, such evaluations are\nknown to have very limited power in predicting the utility of embeddings in\nextrinsic tasks. Accordingly, it has become recently much more common to\ninclude at least one or two extrinsic tasks as part of the evaluation of\nembedding models.\n\n- The similarity/relatedness evaluation datasets used in the paper are\npresented as datasets recording human judgements of similarity between\nconcepts. However, if I understand correctly, the actual judgements were made\nbased on presenting phrases to the human annotators, and therefore they should\nbe considered as phrase similarity datasets, and analyzed as such.\n\n- The medical concept evaluation dataset, \u2018mini MayoSRS\u2019 is extremely small\n(29 pairs), and its larger superset \u2018MayoSRS\u2019 is only a little larger (101\npairs) and was reported to have a relatively low human annotator agreement. The\nother medical concept evaluation dataset, \u2018UMNSRS\u2019, is more reasonable in\nsize, but is based only on concepts that can be represented as single words,\nand were represented as such to the human annotators. This should be mentioned\nin the paper and makes the relevance of this dataset questionable with respect\nto representations of phrases and general concepts. \n\n- As the authors themselves note, they (quite extensively) fine tune their\nhyperparameters on the very same datasets for which they report their results\nand compare them with prior work. This makes all the reported results and\nanalyses questionable.\n\n- The authors suggest that their method is superb to prior work, as it achieved\ncomparable results while prior work required much more manual annotation. I\ndon't think this argument is very strong because the authors also use large\nmanually-constructed ontologies, and also because the manually annotated\ndataset used in prior work comes from existing clinical records that did not\nrequire dedicated annotations.\n\n- In general, I was missing more useful insights into what is going on behind\nthe reported numbers. The authors try to treat the relation between a phrase\nand its component words on one hand, and a concept and its alternative phrases\non the other, as similar types of a compositional relation. However, they\nare different in nature and in my mind each deserves a dedicated analysis. For\nexample, around line 588, I would expect an NLP analysis specific to the\nrelation between phrases and their component words. Perhaps the reason for the\nreported behavior is dominant phrase headwords, etc. Another aspect that was\nabsent but could strengthen the work, is an investigation of the effect of the\nhyperparameters that control the tradeoff between the atomic and compositional\nviews of phrases and concepts.\n\nGeneral Discussion:\n\nDue to the above mentioned weaknesses, I recommend to reject this submission. I\nencourage the authors to consider improving their evaluation datasets and\nmethodology before re-submitting this paper.\n\nMinor comments:\n\n- Line 069: contexts -> concepts\n\n- Line 202: how are phrase overlaps handled?\n\n- Line 220: I believe the dimensions should be |W| x d. Also, the terminology\n\u2018negative sampling matrix\u2019 is confusing as the model uses these embeddings\nto represent contexts in positive instances as well.\n\n- Line 250: regarding \u2018the observed phrase just completed\u2019, it not clear to\nme how words are trained in the joint model. The text may imply that only the\nlast words of a phrase are considered as target words, but that doesn\u2019t make\nsense. \n\n- Notation in Equation 1 is confusing (using c instead of o)\n\n- Line 361: Pedersen et al 2007 is missing in the reference section.\n\n- Line 388: I find it odd to use such a fine-grained similarity scale (1-100) \nfor human annotations.\n\n- Line 430: The newly introduced term \u2018strings\u2019 here is confusing. I\nsuggest to keep using \u2018phrases\u2019 instead.\n\n- Line 496: Which task exactly was used for the hyper-parameter tuning?\nThat\u2019s important. I couldn\u2019t find that even in the appendix.\n\n- Table 3: It\u2019s hard to see trends here, for instance PM+CL behaves rather\ndifferently than either PM or CL alone. It would be interesting to see\ndevelopment set trends with respect to these hyper-parameters.\n\n- Line 535: missing reference to Table 5.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Recent work on embedding ontology concepts has relied on either expensive manual annotation or automated concept tagging methods that ignore the textual contexts around concepts.  We propose a novel method for jointly learning concept, phrase, and word embeddings from an unlabeled text corpus, by using the representative phrases for ontology concepts as distant supervision.  We learn embeddings for medical concepts in the Unified Medical Language System and general-domain concepts in YAGO, using various corpora.  Our embeddings show performance competitive with existing methods on concept similarity and relatedness tasks, while requiring no human corpus annotation and demonstrating more than 3x coverage in the vocabulary size.", "histories": [], "id": 178, "title": "A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words"}
