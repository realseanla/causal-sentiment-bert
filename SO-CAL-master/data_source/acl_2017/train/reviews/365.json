{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "[update after reading author response: the alignment of the hidden units does\nnot match with my intuition and experience, but I'm willing to believe I'm\nwrong in this case.  Discussing the alignment in the paper is important (and\nmaybe just sanity-checking that the alignment goes away if you initialize with\na different seed).  If what you're saying about how the new model is very\ndifferent but only a little better performing -- a 10% error reduction -- then\nI wonder about an ensemble of the new model and the old one.  Seems like\nensembling would provide a nice boost if the failures across models are\ndistinct, right?  Anyhow this is a solid paper and I appreciate the author\nresponse, I raise my review score to a 4.]\n\n- Strengths:\n\n  1)  Evidence of the attention-MTL connection is interesting\n\n  2)  Methods are appropriate, models perform well relative to state-of-the-art\n\n- Weaknesses:\n\n  1)  Critical detail is not provided in the paper\n\n  2)  Models are not particularly novel\n\n- General Discussion:\n\nThis paper presents a new method for historical text normalization.  The model\nperforms well, but the primary contribution of the paper ends up being a\nhypothesis that attention mechanisms in the task can be learned via multi-task\nlearning, where the auxiliary task is a pronunciation task.  This connection\nbetween attention and MTL is interesting.\n\nThere are two major areas for improvement in this paper.  The first is that we\nare given almost no explanation as to why the pronunciation task would somehow\nrequire an attention mechanism similar to that used for the normalization task.\n Why the two tasks (normalization and pronunciation) are related is mentioned\nin the paper: spelling variation often stems from variation in pronunciation. \nBut why would doing MTL on both tasks result in an implicit attention mechanism\n(and in fact, one that is then only hampered by the inclusion of an explicit\nattention mechanism?).                    This remains a mystery.  The paper can\nleave some\nquestions unanswered, but at least a suggestion of an answer to this one would\nstrengthen the paper.\n\nThe other concern is clarity.  While the writing in this paper is clear, a\nnumber of details are omitted.                    The most important one is the\ndescription\nof\nthe attention mechanism itself.  Given the central role that method plays, it\nshould be described in detail in the paper rather than referring to previous\nwork.  I did not understand the paragraph about this in Sec 3.4.\n\nOther questions included why you can compare the output vectors of two models\n(Figure 4), while the output dimensions are the same I don't understand why the\nhidden layer dimensions of two models would ever be comparable.  Usually how\nthe hidden states are \"organized\" is completely different for every model, at\nthe very least it is permuted.                    So I really did not understand\nFigure 4.\n\nThe Kappa statistic for attention vs. MTL needs to be compared to the same\nstatistic for each of those models vs. the base model.\n\nAt the end of Sec 5, is that row < 0.21 an upper bound across all data sets?\n\nLastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL\napproaches make large changes to the model (comparing e.g. Fig 5) but the\nexperimental improvements in accuracy for either model are quite small (2%),\nwhich seems like a bit of a contradiction.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "Summary:\n\nThe paper applies a sequence to sequence (seq2seq) approach for German\nhistorical text normalization, and showed that using a grapheme-to-phoneme\ngeneration as an auxiliary task in a multi-task learning (MTL) seq2seq\nframework improves performance. The authors argue that the MTL approach\nreplaces the need for an attention menchanism, showing experimentally that the\nattention mechanism harms the MTL performance. The authors also tried to show\nstatistical correlation between the weights of an MTL normalizer and an\nattention-based one.\n\nStrengths:\n\n1) Novel application of seq2seq to historical text correction, although it has\nbeen applied recently to sentence grammatical error identification [1]. \n\n2) Showed that using grapheme-to-phoneme as an auxiliary task in a MTL setting\nimproves text normalization accuracy.\n\nWeaknesses:\n\n1) Instead of arguing that the MTL approach replaces the attention mechanism, I\nthink the authors should investigate why attention did not work on MTL, and\nperhaps modify the attention mechanism so that it would not harm performance.\n\n2) I think the authors should reference past seq2seq MTL work, such as [2] and\n[3]. The MTL work in [2] also worked on non-attention seq2seq models.\n\n3) This paper only tested on one German historical text data set of 44\ndocuments. It would be interesting if the authors can evaluate the same\napproach in another language or data set.\n\nReferences:\n\n[1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.\nSentence-level grammatical error identification as sequence-to-sequence\ncorrection. In Proceedings of the 11th Workshop on Innovative Use of NLP for\nBuilding Educational Applications.\n\n[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz\nKaiser. Multi-task Sequence to Sequence Learning. ICLR\u201916. \n\n[3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. \nMulti-task learning for multiple language translation. ACL'15\n\n---------------------------\nHere is my reply to the authors' rebuttal:\n\nI am keeping my review score of 3, which means I do not object to accepting the\npaper. However, I am not raising my score for 2 reasons:\n\n* the authors did not respond to my questions about other papers on seq2seq\nMTL, which also avoided using attention mechanism. So in terms of novelty, the\nmain novelty lies in applying it to text normalization.\n\n* it is always easier to show something (i.e. attention in seq2seq MTL) is not\nworking, but the value would lie in finding out why it fails and changing the\nattention mechanism so that it works.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: well written, solid experimental setup and intriguing qualitative\nanalysis\n\n- Weaknesses: except for the qualitative analysis, the paper may belong better\nto the applications area, since the models are not particularly new but the\napplication itself is most of its novelty\n\n- General Discussion: This paper presents a \"sequence-to-sequence\" model with\nattention mechanisms and an auxiliary phonetic prediction task to tackle\nhistorical text normalization. None of the used models or techniques are new by\nthemselves, but they seem to have never been used in this problem before,\nshowing and improvement over the state-of-the-art. \n\nMost of the paper seem like a better fit for the applications track, except for\nthe final analysis where the authors link attention with multi-task learning,\nclaiming that the two produce similar effects. The hypothesis is intriguing,\nand it's supported with a wealth of evidence, at least for the presented task. \nI do have some questions on this analysis though:\n\n1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two\nmodels are aligned? Is it safe to do so?\n\n2) Section 5.2, I don't get what you mean by the errors that each of the models\nresolve independently of each other. This is like symmetric-difference? That\nis, if we combine the two models these errors are not resolved anymore?\n\nOn a different vein, 3) Why is there no comparison with Azawi's model?\n\n========\n\nAfter reading the author's response.\n\nI'm feeling more concerned than I was before about your claims of alignment in\nthe hidden space of the two models. If accepted, I would strongly encourage the\nauthors to make clear\nin the paper the discussion you have shared with us for why you think that\nalignment holds in practice.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Automated processing of historical texts often relies on pre-normalization to modern word forms. Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task. We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art by an absolute 2% increase in performance. We analyze the induced models across 44 different texts from Early New High German. Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms. This, we believe, is an important step toward understanding how MTL works.", "histories": [], "id": "365", "title": "Learning attention for historical text normalization by learning to pronounce"}
