{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "2", "PRESENTATION_FORMAT": "Poster", "comments": "This paper presents several weakly supervised methods for developing NERs. The\nmethods rely on some form of projection from English into another language. The\noverall approach is not new and the individual methods proposed are\nimprovements of existing methods. For an ACL paper I would have expected more\nnovel approaches.\n\nOne of the contributions of the paper is the data selection scheme. The formula\nused to calculate the quality score is quite straightforward and this is not a\nbad thing. However, it is unclear how the thresholds were calculated for Table\n2. The paper says only that different thresholds were tried. Was this done on a\ndevelopment set? There is no mention of this in the paper. The evaluation\nresults show clearly that data selection is very important, but one may not\nknow how to tune the parameters for a new data set or a new language pair. \n\nAnother contribution of the paper is the combination of the outputs of the two\nsystems developed in the paper. I tried hard to understand how it works, but\nthe description provided is not clear. \n\nThe paper presents a number of variants for each of the methods proposed. Does\nit make sense to combine more than two weakly supervised systems? Did the\nauthors try anything in this direction.\n\nIt would be good to know a bit more about the types of texts that are in the\n\"in-house\" dataset.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "2", "PRESENTATION_FORMAT": "Poster", "comments": "This paper describes a model for cross-lingual named entity recognition (NER).\nThe authors employ conditional random fields, maximum entropy Markov, and\nneural network-based NER methods. In addition, authors propose two methods to\ncombine the output of those methods (probability-based and ranking-based), and\na method to select the best training instances from cross-lingual comparable\ncorpora. The cross-lingual projection is done using a variant of Mikolov\u2019s\nproposal. In general, the paper is easy to follow, well-structured, and the\nEnglish quality is also correct. The results of the combined annotations are\ninteresting.\n\nDetailed comments:\n\nI was wondering which is the motivation behind proposing a Continuous\nBag-of-word (CBOW) model variation. You don\u2019t give much details about this\n(or the parameters employed). Was the original model (or the Continuous\nSkip-gram model) offering low results? I suggest to include also the results\nwith the CBOW model, so readers can analyse the improvements of your approach.\nSince you use a decay factor for the surrounding embeddings, I suggest to take\na look to the exponential decay used in [1].\n\nSimilarly to the previous comment, I would like to look at the differences\nbetween the original Mikolov\u2019s cross-lingual projections and your frequency\nweighted projections. These contributions are more valuable if readers can see\nthat your method is really superior.\n\n\u201cthe proposed data selection scheme is very effective in selecting\ngood-quality projection-labeled data and the improvement is significant\u201d \u2190\nHave you conducted a test of statistical significance? I would like to know if\nthe differences between result in this work are significant. \n\nI suggest to integrate the text of Section 4.4 at the beginning of Section 4.2.\nIt would look cleaner. I also recommend to move the evaluation of Table 2 to\nthe evaluation section.\n\nI miss a related work section. Your introduction includes part of that\ninformation. I suggest to divide the introduction in two sections.\n\nThe evaluation is quite short (1.5 pages with conclusion section there). You\nobtain state-of-the-art results, and I would appreciate more discussion and\nanalysis of the results.\n\nSuggested references:\n\n[1] Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2016). Embeddings for word\nsense disambiguation: An evaluation study. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Vol. 1, pp. 897-907).", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.", "histories": [], "id": "107", "title": "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection"}
