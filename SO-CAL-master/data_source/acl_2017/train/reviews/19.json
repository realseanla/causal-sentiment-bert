{"reviews": [{"SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThis paper introduced a novel method to improve zero pronoun resolution\nperformance.. The main contributions of this papers are: 1) proposed a simple\nmethod to automatically generate a large training set for zero pronoun\nresolution task; 2) adapted a two step learning process to transfer knowledge\nfrom large data set to the specific domain data; 3) differentiate unknown words\nusing different tags. In general, the paper is well written. Experiments are\nthoroughly designed. \n\n- Weaknesses:\n\nBut I have a few questions regarding finding the antecedent of a zero pronoun:\n1. How will an antecedent be identified, when the prediction is a pronoun? The\nauthors proposed a method by matching the head of noun phrases. It\u2019s not\nclear how to handle the situation when the head word is not a pronoun.\n2. What if the prediction is a noun that could not be found in the previous\ncontents?\n3. The system achieves great results on standard data set. I\u2019m curious is it\npossible to evaluate the system in two steps? The first step is to evaluate the\nperformance of the model prediction, i.e. to recover the dropped zero pronoun\ninto a word; the second step is to evaluate how well the systems works on\nfinding an antecedent.\n\nI\u2019m also curious why the authors decided to use attention-based neural\nnetwork. A few sentences to provide the reasons would be helpful for other\nresearchers.\n\nA minor comment:\nIn figure 2, should it be s1, s2 \u2026 instead of d1, d2 \u2026.? \n\n- General Discussion:\nOverall it is a great paper with innovative ideas and solid experiment setup.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThe approach is novel and the results are very promising, beating\nstate-of-the-art.\n\n- Weaknesses:\n\n The linguistic motivation behind the paper is troublesome (see below). I feel\nthat the paper would benefit a lot from a more thoughtful interpretation of the\nresults.\n\n- General Discussion:\n\nThis paper presents an approach for Zero Pronoun Resolution in Chinese. The\nauthors advocate a novel procedure for generating large amount of relevant data\nfrom unlabeled documents. These data are then integrated smartly in an NN-based\narchitecture at a pre-training step. The results improve on state-of-the-art.\n\nI have mixed feelings about this study. On the one hand, the approach seems\nsound and shows promising results, beating very recent systems (e.g., Chen&Ng\n2016). On the other hand, the way the main contribution is framed is very\ndisturbing from the linguistic point of view. In particular, (zero) pronoun\nresolution is, linguistically speaking, a context modeling task, requiring\naccurate interpretation of discourse/salience, semantic and syntactic clues. It\nstarts from the assumption that (zero) pronouns are used in specific contexts,\nwhere full NPs shouldn't normally be possible. From this perspective,\ngenerating ZP data via replacing nominal with zeroes (\"blank\") doesn't sound\nvery convincing. And indeed, as the authors themselves show, the pre-training\nmodule alone doesn't achieve a reasonable performance. To sum it up, i don't\nthink that these generated pseudo-data can be called AZP data. It seems more\nlikely that they encode some form of selectional preferences (?). It would be\nnice if the authors could invest some effort in better understanding what\nexactly the pre-training module learns -- and then reformulate the\ncorresponding sections. \n\nThe paper can benefit from a proofreading by a native speaker of English -- for\nexample, the sentence on lines 064-068 is not grammatical.\n\n-- other points --\n\nlines 78-79: are there any restrictions on the nouns and especially pronouns?\nfor example, do you use this strategy for very common pronouns (as English\n\"it\")? if so, how do you guarantee that the two occurrences of the same token \nare indeed coreferent?\n\nline 91: the term antecedent is typically used to denote a preceding mention\ncoreferent with the anaphor, which is not what you mean here\n\nline 144: OntoNotes (typo)\n\nlines 487-489: it has been shown that evaluation on gold-annotated data does\nnot provide reliable estimation of performance. and, indeed, all the recent\nstudies of coreference evaluate on system mentions. for example, the studies of\nChen&Ng you are citing, provide different types of evaluation, including those\non system mentions. please consider rerunning your experiments to get a more\nrealistic evaluation setup\n\nline 506: i don't understand what the dagger over the system's name means. is\nyour improvement statistically significant on all the domains? including bn and\ntc??\n\nline 565: learn (typo)\n\nsection 3.3: in this section you use the abbreviation AZP instead of ZP without\nintroducing it, please unify the terminology\n\nreferences -- please double-check for capitalization", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task.  Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1\\% F-score on OntoNotes 5.0 data.", "histories": [], "id": 19, "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution"}
