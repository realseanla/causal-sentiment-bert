{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\ni. Motivation is well described.\nii. Provides detailed comparisons with various models across diverse languages\n\n- Weaknesses:\ni.          The conclusion is biased by the selected languages. \nii.           The experiments do not cover the claim of this paper completely.\n\n- General Discussion:\nThis paper issues a simple but fundamental question about word representation:\nwhat subunit of a word is suitable to represent morphologies and how to compose\nthe units. To answer this question, this paper applied word representations\nwith various subunits (characters, character-trigram, and morphs) and\ncomposition functions (LSTM, CNN, and a simple addition) to the language\nmodeling task to find the best combination. In addition, this paper evaluated\nthe task for more than 10 languages. This is because languages are\ntypologically diverse and the results can be different according to the word\nrepresentation and composition function. From their experimental results, this\npaper concluded that character-level representations are more effective, but\nthey are still imperfective in comparing them with a model with explicit\nknowledge of morphology. Another conclusion is that character-trigrams show\nreliable perplexity in the majority of the languages. \n\nHowever, this paper leaves some issues behind.\n-         First of all, there could be some selection bias of the experimental\nlanguages. This paper chose ten languages in four categories (up to three\nlanguages per a category). But, one basic question with the languages is \u201chow\ncan it be claimed that the languages are representatives of each category?\u201d\nAll the languages in the same category have the same tendency of word\nrepresentation and composition function? How can it be proved? For instance,\neven in this paper, two languages belonging to the same typology\n(agglutinative) show different results. Therefore, at least to me, it seems to\nbe better to focus on the languages tested in this paper instead of drawing a\ngeneral conclusions about all languages. \n-         There is some gap between the claim and the experiments. Is the\nlanguage modeling the best task to prove the claim of this paper? Isn\u2019t there\nany chance that the claim of this paper breaks in other tasks? Further\nexplanation on this issue is needed.\n-         In Section 5.2, this paper evaluated the proposed method only for\nArabic. Is there any reason why the experiment is performed only for Arabic?\nThere are plenty of languages with automatic morphological analyzers such as\nJapanese and Turkish.\n-         This paper considers only character-trigram among various n-grams. Is\nthere any good reason to choose only character-trigram? Is it always better\nthan character-bigram or character-fourgram? In general, language modeling with\nn-grams is affected by corpus size and some other factors. \n\nMinor typos: \n- There is a missing reference in Introduction. (88 line in Page 1)\n- root-and-patter -> root-and-pattern (524 line in Page 6)", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "tldr: The authors compare a wide variety of approaches towards sub-word\nmodelling in language modelling, and show that modelling morphology gives the\nbest results over modelling pure characters. Further, the authors do some\nprecision experiments to show that the biggest benefit towards sub-word\nmodelling is gained after words typically exhibiting rich morphology (nouns and\nverbs). The paper is comprehensive and the experiments justify the core claims\nof the paper. \n\n- Strengths:\n\n1) A comprehensive overview of different approaches and architectures towards\nsub-word level modelling, with numerous experiments designed to support the\ncore claim that the best results come from modelling morphemes.\n\n2) The authors introduce a novel form of sub-word modelling based on character\ntri-grams and show it outperforms traditional approaches on a wide variety of\nlanguages.\n\n3) Splitting the languages examined by typology and examining the effects of\nthe models on various typologies is a welcome introduction of linguistics into\nthe world of language modelling.\n\n4) The analysis of perplexity reduction after various classes of words in\nRussian and Czech is particularly illuminating, showing how character-level and\nmorpheme-level models handle rare words much more gracefully. In light of these\nresults, could the authors say something about how much language modelling\nrequires understanding of semantics, and how much it requires just knowing\nvarious morphosyntactic effects?\n\n- Weaknesses:\n\n1) The character tri-gram LSTM seems a little unmotivated. Did the authors try\nother character n-grams as well? As a reviewer, I can guess that character\ntri-grams roughly correspond to morphemes, especially in Semitic languages, but\nwhat made the authors report results for 3-grams as opposed to 2- or 4-? In\naddition, there are roughly 26^3=17576 possible distinct trigrams in the Latin\nlower-case alphabet, which is enough to almost constitute a word embedding\ntable. Did the authors only consider observed trigrams? How many distinct\nobserved trigrams were there?\n\n2) I don't think you can meaningfully claim to be examining the effectiveness\nof character-level models on root-and-pattern morphology if your dataset is\nunvocalised and thus doesn't have the 'pattern' bit of 'root-and-pattern'. I\nappreciate that finding transcribed Arabic and Hebrew with vowels may be\nchallenging, but it's half of the typology.\n\n3) Reduplication seems to be a different kind of phenomenon to the other three,\nwhich are more strictly morphological typologies. Indonesian and Malay also\nexhibit various word affixes, which can be used on top of reduplication, which\nis a more lexical process. I'm not sure splitting it out from the other\nlinguistic typologies is justified.\n\n- General Discussion:\n\n1) The paper was structured very clearly and was very easy to read.\n\n2) I'm a bit puzzled about why the authors chose to use 200 dimensional\ncharacter embeddings. Once the dimensionality of the embedding is greater than\nthe size of the vocabulary (here the number of characters in the alphabet),\nsurely you're not getting anything extra?\n\n-------------------------------\n\nHaving read the author response, my opinions have altered little. I still think\nthe same strengths and weakness that I have already discussed hold.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.", "histories": [], "id": 477, "title": "From Characters to Words to in Between: Do We Capture Morphology?"}
