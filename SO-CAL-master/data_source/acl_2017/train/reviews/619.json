{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "3", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper presents a corpus of annotated essay revisions. \n\nIt includes two examples of application for the corpus:\n\n1) Student Revision Behavior Analysis and 2) Automatic Revision Identification\n\nThe latter is essentially a text classification task using an SVM classifier\nand a variety of features. The authors state that the corpus will be freely\navailable for research purposes.\n\nThe paper is well-written and clear. A detailed annotation scheme was used by\ntwo\nannotators to annotate the corpus which added value to it. I believe the\nresource might be interesting to researcher working on writing process research\nand related topics. I also liked that you provided two very clear usage\nscenarios for the corpus. \n\nI have two major criticisms. The first could be easily corrected in case the\npaper is accepted, but the second requires more work.\n\n1) There are no statistics about the corpus in this paper. This is absolutely\nparamount. When you describe a corpus, there are some information that should\nbe there. \n\nI am talking about number of documents (I assume the corpus has 180 documents\n(60 essays x 3 drafts), is that correct?), number of tokens (around 400 words\neach essay?), number of sentences, etc. \n\nI assume we are talking about 60 unique essays x 400 words, so about 24,000\nwords in total. Is that correct? If we take the 3 drafts we end up with about\n72,000 words but probably with substantial overlap between drafts.\n\nA table with this information should be included in the paper.\n\n2) If the aforementioned figures are correct, we are talking about a very small\ncorpus. I understand the difficulty of producing hand-annotated data, and I\nthink this is one of the strengths of your work, but I am not sure about how\nhelpful this resource is for the NLP community as a whole. Perhaps such a\nresource would be better presented in a specialised workshop such as BEA or a\nspecialised conference on language resources like LREC instead of a general NLP\nconference like ACL.\n\nYou mentioned in the last paragraph that you would like to augment the corpus\nwith more annotation. Are you also willing to include more essays?\n\nComments/Minor:\n\n- As you have essays by native and non-native speakers, one further potential\napplication of this corpus is native language identification (NLI).\n\n- p. 7: \"where the unigram feature was used as the baseline\" - \"word unigram\".\nBe more specific.\n\n- p. 7: \"and the SVM classifier was used as the classifier.\" - redundant.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "This paper presents ArgRewrite, a corpus of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer\u2019s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction.", "histories": [], "id": 619, "title": "A Corpus of Annotated Revisions for Studying Argumentative Writing"}
