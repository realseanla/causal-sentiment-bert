{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "This paper considers the problem of KB completion and proposes ITransF for this\npurpose. Unlike STransE that assigns each relation an independent matrix, this\npaper proposes to share the parameters between different relations. A model is\nproposed where a tensor D is constructed that contains various relational\nmatrices as its slices and a selectional vector \\alpha is used to select a\nsubset of relevant relational matrix for composing a particular semantic\nrelation. The paper then discuss a technique to make \\alpha sparse.\nExperimental results on two standard benchmark datasets shows the superiority\nof ITransF over prior proposals.\n\nThe paper is overall well written and the experimental results are good.\nHowever, I have several concerns regarding this work that I hope the authors\nwill answer in their response.\n\n1. Just by arranging relational matrices in a tensor and selecting (or more\nappropriately considering a linearly weighted sum of the relational matrices)\ndoes not ensure any information sharing between different relational matrices.\nThis would have been the case if you had performed some of a tensor\ndecomposition and projected the different slices (relational matrices) into\nsome common lower-dimensional core tensor. It is not clear why this approach\nwas not taken despite the motivation to share information between different\nrelational matrices.\n2. The two requirements (a) to share information across different relational\nmatrices and (b) make the attention vectors sparse are some what contradictory.\nIf the attention vector is truly sparse and has many zeros then information\nwill not flow into those slices during optimisation. \n3. The authors spend a lot of space discussing techniques for computing sparse\nattention vectors. The authors mention in page 3 that \\ell_1 regularisation did\nnot work in their preliminary experiments. However, no experimental results are\nshown for \\ell_1 regularisation nor they explain why \\ell_1 is not suitable for\nthis task. To this reviewer, it appears as an obvious baseline to try,\nespecially given the ease of optimisation. You use \\ell_0 instead and get into\nNP hard optimisations because of it. Then you propose a technique and a rather\ncrude approximation in the end to solve it. All that trouble could be spared if\n\\ell_1 was used.\n4. The vector \\alpha is performing a selection or a weighing over the slices of\nD. It is slightly misleading to call this as \u201cattention\u201d as it is a term\nused in NLP for a more different type of models (see attention model used in\nmachine translation).\n5. It is not clear why you need to initialise the optimisation by pre-trained\nembeddings from TransE. Why cannot you simply randomly initialise the\nembeddings as done in TransE and then update them? It is not fair to compare\nagainst TransE if you use TransE as your initial point.\n\nLearning the association between semantic relations is an idea that has been\nused in related problems in NLP such as relational similarity measurement\n[Turney JAIR 2012] and relation adaptation [Bollegala et al. IJCAI 2011]. It\nwould be good to put the current work with respect to such prior proposals in\nNLP for modelling inter-relational correlation/similarity.\n\nThanks for providing feedback. I have read it.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}], "abstract": "Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets---WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.", "histories": [], "id": "79", "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion"}
