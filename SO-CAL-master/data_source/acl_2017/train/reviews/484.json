{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n\n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone speech\nrecognition task. \n\n- Weaknesses:\nThe only problem is that the paper sounds too similar with Ref [Kim et al.,\n2016] which will be officially published in the coming IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.\nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,\nand this paper proposes joint CTC-attention using MTL+joint decoding for\nJapanese and Chinese ASR tasks. I guess the difference is on joint decoding and\nthe application to Japanese/Chinese ASR tasks. However, the difference is not\nclearly explained by the authors. So it took sometimes to figure out the\noriginal contribution of this paper.\n\n(a) Title: \nThe title in Ref [Kim et al., 2016] is \u201cJoint CTC- Attention Based End-to-End\nSpeech Recognition Using Multi-task Learning\u201d, while the title of this paper\nis \u201cJoint CTC-attention End-to-end Speech Recognition\u201d. I think the title\nis too general. If this is the first paper about \"Joint CTC-attention\" than it\nis absolutely OK. Or if Ref [Kim et al., 2016] will remain only as\npre-published arXiv, then it might be still acceptable. But since [Kim et al.,\n2016] will officially publish in IEEE conference, much earlier than this paper,\nthen a more specified title that represents the main contribution of this paper\nin contrast with the existing publication would be necessary. \n\n(b) Introduction:\nThe author claims that \u201cWe propose to take advantage of the constrained CTC\nalignment in a hybrid CTC-attention based system. During training, we attach a\nCTC objective to an attention-based encoder network as a regularization, as\nproposed by [Kim at al., 2016].\u201c Taking advantage of the constrained CTC\nalignment in a hybrid CTC-attention is the original idea from [Kim at al.,\n2016]. So the whole argument about attention-based end-to-end ASR versus\nCTC-based ASR, and the necessary of CTC-attention combination is not novel.\nFurthermore, the statement \u201cwe propose \u2026 as proposed by [Kim et al,\n2016]\u201d is somewhat weird. We can build upon someone proposal with additional\nextensions, but not just re-propose other people's proposal. Therefore, what\nwould be important here is to state clearly the original contribution of this\npaper and the position of the proposed method with respect to existing\nliterature\n\n(c) Experimental Results:\nKim at al., 2016 applied the proposed method on English task, while this paper\napplied the proposed method on Japanese and Mandarin Chinese tasks. I think it\nwould be interesting if the paper could explain in more details about the\nspecific problems in Japanese and Mandarin Chinese tasks that may not appear in\nEnglish task. For example, how the system could address multiple possible\noutputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input\nwithout using any linguistic resources. This could be one of the important\ncontributions from this paper.\n\n- General Discussion:\nI think it would be better to cite Ref [Kim et al., 2016] from\nthe official IEEE ICASSP conference, rather than pre-published arXiv:\nKim, S., Hori, T., Watanabe, S., \"Joint CTC- Attention Based End-to-End Speech\nRecognition Using Multi-task Learning\", IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper considers a synergistic combination of two non-HMM based speech\nrecognition techniques: CTC and attention-based seq2seq networks. The\ncombination is two-fold:\n1. first, similarly to Kim et al. 2016 multitask learning is used to train a\nmodel with a joint CTC and seq2seq cost.\n2. second (novel contribution), the scores of the CTC model and seq2seq model\nare ensembled during decoding (results of beam search over the seq2seq model\nare rescored with the CTC model).\n\nThe main novelty of the paper is in using the CTC model not only as an\nauxiliary training objective (originally proposed by Kim et al. 2016), but also\nduring decoding.\n\n- Strengths:\nThe paper identifies several problems stemming from the flexibility offered by\nthe attention mechanism and shows that by combining the seq2seq network with\nCTC the problems are mitigated.\n\n- Weaknesses:\nThe paper is an incremental improvement over Kim et al. 2016 (since two models\nare trained, their outputs can just as well be ensembled). However, it is nice\nto see that such a simple change offers important performance improvements of\nASR systems.\n\n- General Discussion:\nA lot of the paper is spent on explaining the well-known, classical ASR\nsystems. A description of the core improvement of the paper (better decoding\nalgorithm) starts to appear only on p. 5. \n\nThe description of CTC is nonstandard and maybe should either be presented in a\nmore standard way, or the explanation should be expanded. Typically, the\nrelation p(C|Z) (eq. 5) is deterministic - there is one and only one character\nsequence that corresponds to the blank-expanded form Z. I am also unsure about\nthe last transformation of the eq. 5.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}], "abstract": "End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and context-dependency trees, leading to a greatly simplified model-building process. There are two major types of end-to-end architectures for ASR:  attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture, which effectively utilizes both advantages in decoding. We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the comparable performance to conventional state-of-the-art DNN/HMM ASR systems without linguistic resources.", "histories": [], "id": "484", "title": "Joint CTC/attention decoding for end-to-end speech recognition"}
