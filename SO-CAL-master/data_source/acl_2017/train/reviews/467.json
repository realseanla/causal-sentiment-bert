{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThe paper presents an iterative method to induce bilingual word embeddings\nusing large monolingual corpora starting with very few (or automatically\nobtainable numeral) mappings between two languages. Compared to\nstate-of-the-art using larger bilingual dictionaries or parallel/comparable\ncorpora, the results obtained with the presented method that relies on very\nlittle or no manually prepared input are exciting and impressive.\n\n- Weaknesses:\n\nI would have liked to see a discussion on the errors of the method, and\npossibly a discussion on how the method could be adjusted to deal with them.\n\n- General Discussion:\n\nDoes the frequency of the seeds in the monolingual corpora matter?\n\nIt would be interesting to see the partial (in the sense of after n number of\niterations) evolution of the mapping between words in the two languages for a\nfew words. \n\nWhat happens with different translations of the same word (like different\nsenses)?\n\nOne big difference between German and English is the prevalence of compounds in\nGerman. What happens to these compounds? What are they mapped onto? Would a\npreprocessing step of splitting the compounds help? (using maybe only\ncorpus-internal unigram information)\n\nWhat would be the upper bound for such an approach? An analysis of errors --\ne.g. words very far from their counterpart in the other language -- would be\nvery interesting. It would also be interesting to see a discussion of where\nthese errors come from, and if they could be addressed with the presented\napproach.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This work proposes a self-learning bootstrapping approach to learning bilingual\nword embeddings, which achieves competitive results in tasks of bilingual\nlexicon induction and cross-lingual word similarity although it requires a\nminimal amount of bilingual supervision: the method leads to competitive\nperformance even when the seed dictionary is extremely small (25 dictionary\nitems!) or is constructed without any language pair specific information (e.g.,\nrelying on numerals shared between languages). \n\nThe paper is very well-written, admirably even so. I find this work 'eclectic'\nin a sense that its original contribution is not a breakthrough finding (it is\nmore a 'short paper idea' in my opinion), but it connects the dots from prior\nwork drawing inspiration and modelling components from a variety of previous\npapers on the subject, including the pre-embedding work on\nself-learning/bootstrapping (which is not fully recognized in the current\nversion of the paper). I liked the paper in general, but there are few other\nresearch questions that could/should have been pursued in this work. These,\nalong with only a partial recognition of related work and a lack of comparisons\nwith several other relevant baselines, are my main concern regarding this\npaper, and they should be fixed in the updated version(s).\n\n*Self-learning/bootstrapping of bilingual vector spaces: While this work is one\nof the first to tackle this very limited setup for learning cross-lingual\nembeddings (although not the first one, see Miceli Barone and more works\nbelow), this is the first truly bootstrapping/self-learning approach to\nlearning cross-lingual embeddings. However, this idea of bootstrapping\nbilingual vector spaces is not new at all (it is just reapplied to learning\nembeddings), and there is a body of work which used exactly the same idea with\ntraditional 'count-based' bilingual vector spaces. I suggest the authors to\ncheck the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP\n2013), and recognize the fact that their proposed bootstrapping approach is not\nso novel in this domain. There is also related work of Ellen Riloff's group on\nbootstrapping semantic lexicons in monolingual settings.\n\n*Relation to Artetxe et al.: I might be missing something here, but it seems\nthat the proposed bootstrapping algorithm is in fact only an iterative approach\nwhich repeatedly utilises the previously proposed model/formulation of Artetxe\net al. The only difference is the reparametrization (line 296-305). It is not\nclear to me whether the bootstrapping approach draws its performance from this\nreparametrization (and whether it would work with the previous\nparametrization), or the performance is a product of both the algorithm and\nthis new parametrization. Perhaps a more explicit statement in the text is\nneeded to fully understand what is going on here.\n\n*Comparison with prior work: Several very relevant papers have not been\nmentioned nor discussed in the current version of the paper. For instance, the\nrecent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word\nembeddings without bilingual corpora' seems very related to this work (as the\nbasic word overlap between the two titles reveals!), and should be at least\ndiscussed if not compared to. Another work which also relies on mappings with\nseed lexicons and also partially analyzes the setting with only a few hundred\nseed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of\nseed lexicons in learning bilingual word embeddings': these two papers might\nalso help the authors to provide more details for the future work section\n(e.g., the selection of reliable translation pairs might boost the performance\nfurther during the iterative process). Another very relevant work has appeared\nonly recently: Smith et al. (ICLR 2017) discuss 'offline bilingual word\nvectors, orthogonal transformations and the inverted softmax'. This paper also\ndiscusses learning bilingual embeddings in very limited settings (e.g., by\nrelying only on shared words and cognates between two languages in a pair). As\na side note, it would be interesting to report results obtained using only\nshared words between the languages (such words definitely exist for all three\nlanguage pairs used in the experiments). This would also enable a direct\ncomparison with the work of Smith et al. (ICLR 2017) which rely on this setup.\n\n*Seed dictionary size and bilingual lexicon induction: It seems that the\nproposed algorithm (as discussed in Section 5) is almost invariant to the\nstarting seed lexicon, yielding very similar final BLI scores regardless of the\nstarting point. While a very intriguing finding per se, this also seems to\nsuggest an utter limitation of the current 'offline' approaches: they seem to\nhave hit the ceiling with the setup discussed in the paper; Vulic and Korhonen\n(ACL 2016) showed that we cannot really improve the results by simply\ncollecting more seed lexicon pairs, and this work suggests that any number of\nstarting pairs (from 25 to 5k) is good enough to reach this near-optimal\nperformance, which is also very similar to the numbers reported by Dinu et al.\n(arXiv 2015) or Lazaridou et al. (ACL 2015). I would like to see more\ndiscussion on how to break this ceiling and further improve BLI results with\nsuch 'offline' methods. Smith et al. (ICLR 2017) seem to report higher numbers\non the same dataset, so again it would be very interesting to link this work to\nthe work of Smith et al.\n\nIn other words, the authors state that in future work they plan to fine-tune\nthe method so that it can learn without any bilingual evidence. This is an\nadmirable 'philosophically-driven' feat, but from a more pragmatic point of\nview, it seems more pragmatic to detect how we can go over the plateau/ceiling\nwhich seems to be hit with these linear mapping approaches regardless of the\nnumber of used seed lexicon pairs (Figure 2).\n\n*Convergence criterion/training efficiency: The convergence criterion, although\ncrucial for the entire algorithm, both in terms of efficiency and efficacy, is\nmentioned only as a side note, and it is not entirely clear how the whole\nprocedure terminates. I suspect that the authors use the vanishing variation in\ncrosslingual word similarity performance as the criterion to stop the\nprocedure, but that makes the method applicable only to languages which have a\ncross-lingual word similarity dataset. I might be missing here given the\ncurrent description in the paper, but I do not fully understand how the\nprocedure stops for Finnish, given that there is no crosslingual word\nsimilarity dataset for English-Finnish.\n\n*Minor:\n- There is a Finnish 'Web as a Corpus' (WaC) corpus (lines 414-416):\nhttps://www.clarin.si/repository/xmlui/handle/11356/1074\n- Since the authors claim that the method could work with a seed dictionary\ncontaining only shared numerals, it would be very interesting to include an\nadditional language pair which does not share the alphabet (e.g.,\nEnglish-Russian, English-Bulgarian or even something more distant such as\nArabic and/or Hindi).\n\n*After the response: I would like to thank the authors for investing their time\ninto their response which helped me clarify some doubts and points raised in my\ninitial review. I hope that they would indeed clarify these points in the final\nversion, if given the opportunity.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper presents a self-learning framework for learning of bilingual word\nembeddings. The method uses two embeddings (in source and target languages) and\na seed lexicon. On each step of the mapping learning a new bilingual lexicon is\ninduced. Then the learning step is repeated using the new lexicon for learning\nof new mapping. The process stops when a convergence criterion is met.\n\nOne of the strengths is that the seed lexicon is directly encoded in the\nlearning process as a binary matrix. Then the self-learning framework solves a\nglobal optimization problem in which the seed lexicon is not explicitly\ninvolved. Its role is to establish the initial mapping between the two\nembeddings. This guarantees the convergence. The initial seed lexicon could be\nquite small (25 correspondences).\n\nThe small size of the seed lexicon is appealing for mappings between languages\nfor which there are not large bilingual lexicons.\n\nIt will be good to evaluate the framework with respect to the quality of the\ntwo word embeddings. If we have languages (or at least one of the languages)\nwith scarce language resources then the word embeddings for both languages\ncould differ in their structure and coverage. I think it could be simulated on\nthe basis of the available data via training the corresponding word embeddings\non different subcorpora for each language.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.", "histories": [], "id": "467", "title": "Learning bilingual word embeddings with (almost) no bilingual data"}
