{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "3", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This one is a tough call, because I do think that there are some\nimportant, salvageable technial results in here (notably the parsing\nalgorithm), but the paper as a whole has very little cohesion.        It is\nunited around an overarching view of formal languages in which a language\nbeing \"probabilistic\" or not is treated as a formal property of the same \nvariety as being closed under intersection or not.  In my opinion, what it \nmeans for a formal language to be probabilistic in this view has not been \nconsidered with sufficient rigor for this viewpoint to be compelling.\n\nI should note, by the way, that the value of the formal results provided\nmostly does not depend on the flimsiness of the overarching story.  So\nwhat we have here is not bad research, but a badly written paper.  This needs \nmore work.\n\nI find it particulary puzzling that the organization of the paper\nleaves so little space for elucidating the parsing result that\nsoundness and completeness are relegated to a continuation of the\npaper in the form of supplementary notes.  I also find the mention of\nprobabilistic languages in the title of the paper to be very\ndisingenuous --- there is in fact no probabilistic reasoning in this\nsubmission.\n\nThe sigificance of the intersection-closure result of section 3 is\nalso being somewhat overstated, I think.  Unless there is something\nI'm not understanding about the restrictions on the right-hand sides\nof rules (in which case, please elaborate), this is merely a matter of\nfolding a finite intersection into the set of non-terminal labels.", "SOUNDNESS_CORRECTNESS": "2", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper is concerned in finding such a family of graph languages that is\nclosed under intersection and can be made probabilistic.\n\n- Strengths:\n\nThe introduction shows relevance, the overall aim, high level context and is\nnice to read.\nThe motivation is clear and interesting.\n\nThe paper  is extremely clear but requires close reading and much formal\nbackground.\nIt nicely takes into account certain differences in terminology.\n\nIt was interesting to see how the hyper-edge grammars generalize familiar\ngrammars \nand Earley's algorithm.  For example, Predict applies to nonterminal edges, and\nScan applies to terminal edges.  \n\nIf the parsing vs. validation in NLP context is clarified, the paper is useful\nbecause it is formally correct, nice contribution, instructive and can give new\nideas to other researchers.  \n\nThe described algorithm can be used in semantic parsing to rerank hypergraphs\nthat are produced by another parser.   In this restricted way, the method can\nbe part of the machinery what we in NLP use in natural language parsing and\nthus relevant to the ACL.\n\n- Weaknesses:\n\nReranking use is not mentioned in the introduction.\n\nIt would be a great news in NLP context if an Earley parser would run in linear\ntime for NLP grammars (unlike special kinds of formal language grammars). \nUnfortunately, this result involves deep assumptions about the grammar and the\nkind of input. \n\nLinear complexity of parsing of an input graph seem right for a top-down\ndeterministic grammars but the paper does not recognise the fact that an input\nstring in NLP usually gives rise to an exponential number of graphs.  In other\nwords, the parsing complexity result must be interpreted in the context of\ngraph validation or where one wants to find out a derivation of the graph, for\nexample, for the purposes of graph transduction via synchronous derivations.\n\nTo me, the paper should be more clear in this as a random reader may miss the\ndifference between semantic parsing (from strings) and parsing of semantic\nparses \n(the current work).\n\nThere does not seem to be any control of the linear order of 0-arity edges.  It\nmight be useful to mention that if the parser is extended to string inputs with\nthe aim to find the (best?) hypergraph for a given external nodes, then the\nitem representations of the subgraphs must also keep track of the covered\n0-arity edges.                          This makes the string-parser variant\nexponential.  \n\n- Easily correctable typos or textual problems:\n\n1)  Lines 102-106 is misleading.   While intersection and probs are true, \"such\ndistribution\" cannot refer to the discussion in the above.\n\n2) line 173:  I think you should rather talk about validation or recognition\nalgorithms than parsing algorithms as \"parsing\" in NLP means usually completely\ndifferent thing that is much more challenging due to the lexical and structural\nambiguity.\n\n3) lines 195-196 are unclear:  what are the elements of att_G; in what sense\nthey are pairwise distinct.  Compare Example 1 where ext_G and att_G(e_1) are\nnot disjoint sets.\n\n4) l.206.  Move *rank* definition earlier and remove redundancy.\n\n5) l. 267:  rather \"immediately derives\", perhaps.\n\n6) 279: add \"be\"\n\n7) l. 352:  give an example of a nontrivial internal path.\n\n8) l. 472:   define a subgraph of a hypergraph\n\n9) l. 417, l.418:  since there are two propositions, you may want to tell how\nthey contribute to what is quoted.\n\n10) l. 458:  add \"for\"\n\nTable:                          Axiom:              this is only place where this is\nintroduced as an\naxiom.                    Link\nto the text that says it is a trigger.\n\n- General Discussion:\n\nIt might be useful to tell about MSOL graph languages and their yields, which\nare\ncontext-free string languages.                          \n\nWhat happens if the grammar is ambiguous and not top-down deterministic? \nWhat if there are exponential number of parses even for the input graph due to\nlexical ambiguity or some other reasons.  How would the parser behave then? \nWouldn't the given Earley recogniser actually be strictly polynomial to m or k\n?\n\nEven a synchronous derivation of semantic graphs can miss some linguistic\nphenomena where a semantic distinction is expressed by different linguistic\nmeans.                    E.g. one language may add an affix to a verb when another\nlanguage may\nexpress the same distinction by changing the object.  I am suggesting that\nalthough AMR increases language independence in parses it may have such\ncross-lingual\nchallenges.\n\nI did not fully understand the role of the marker in subgraphs.  It was elided\nlater\nand not really used.\n\nl. 509-510:                 I already started to miss the remark of lines 644-647\nat\nthis\npoint.\n\nIt seems that the normal order is not unique.  Can you confirm this?\n\nIt is nice that def 7, cond 1 introduces lexical anchors to predictions. \nCompare the anchors in lexicalized grammars.\n\nl. 760.  Are you sure that non-crossing links do not occur when parsing\nlinearized sentences to semantic graphs?\n\n- Significant questions to the Authors:\n\nLinear complexity of parsing of an input graph seem right for a top-down\ndeterministic grammars but the paper does not recognise the fact that an input\nstring in NLP usually gives rise to an exponential number of graphs.  In other\nwords, the parsing complexity result must be interpreted in the context of\ngraph validation or where one wants to find out a derivation of the graph, for\nexample, for the purposes of graph transduction via synchronous derivations.\n\nWhat would you say about parsing complexity in the case the RGG is a\nnon-deterministic, possibly ambiguous regular tree grammar, but one is\ninterested to use it to assign trees to frontier strings like a context-free\ngrammar?  Can one adapt the given Earley algorithm to this purpose (by guessing\ninternal nodes and their edges)?\nAlthough this question might seem like a confusion, it is relevant in the NLP\ncontext.\n\nWhat prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are\nthen linearised?   What principle determines how they are linearised?               \n  Is\nthe\nlinear order determined by the Earley paths (and normal order used in\nproductions) or can one consider an actual word order in strings of a natural\nlanguage? \n\nThere is no clear connection to (non)context-free string languages or sets of\n(non)projective dependency graphs used in semantic parsing.  What is written on\nlines 757-758 is just misleading:  Lines 757-758 mention that HRGs can be used\nto generate non-context-free languages.  Are these graph languages or string\nlanguages?    How an NLP expert should interpret the (implicit) fact that RGGs\ngenerate only context-free languages?  Does this mean that the graphs are\nnoncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Distributions over strings and trees can be represented by probabilistic regular languages, which characterize many models in natural language processing. Recently, several datasets have become available which represent natural language phenomena as graphs, so it is natural to ask whether there is an equivalent of probabilistic regular languages for graphs. To answer this question, we review three families of graph languages: Hyperedge Replacement Languages (HRL), which can be made probabilistic; Monadic Second Order Languages (MSOL), which support the crucial property of closure under intersection; and Regular Graph Languages (RGL; \\citealt{Courcelle:V}), a subfamily of both HRL and MSOL which inherits these properties, and has not been widely studied or applied to NLP. We prove that RGLs are closed under intersection and provide an efficient parsing algorithm, with runtime linear in the size of the input graph.", "histories": [], "id": 503, "title": "Probabilistic Regular Graph Languages"}
