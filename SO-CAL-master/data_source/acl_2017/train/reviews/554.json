{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\na) The paper presents a Bayesian learning approach for recurrent neural network\nlanguage model. The method outperforms standard SGD with dropout on three\ntasks. \nb) The idea of using Bayesian learning with RNNs appears to be novel. \nc) The computationally efficient Bayesian algorithm for RNN would be of\ninterest to the NLP community for various applications.\n\n- Weaknesses:\n\nPrimary concern is about evaluation:\n\nSec 5.1: The paper reports the performance of difference types of architectures\n(LSTM/GRU/vanilla RNN) on character LM task while comparing the learning\nalgorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are\ncompared for the character LM while SGD +/- dropout is compared with SGLD +/-\ndropout on word language model task. This is inconsistent!  I would suggest\nreporting both these dimensions (i.e. architectures and the exact same learning\nalgorithms) on both character and word LM tasks. It would be useful to know if\nthe results from the proposed Bayesian learning approaches are portable across\nboth these tasks and data sets.\n\nL529: The paper states that 'the performance gain mainly comes from adding\ngradient noise and model averaging'. This statement is not justified\nempirically. To arrive at this conclusion, an A/B experiment with/without\nadding gradient noise and/or model averaging needs to be done. \n\nL724: Gal's dropout is run on the sentence classification task but not on\nlanguage model/captions task. Since Gal's dropout is not specific to sentence\nclassification,  I would suggest reporting the performance of this method on\nall three tasks. This would allow the readers to fully assess the utility of\nthe proposed algorithms relative to all existing dropout approaches.\n\nL544: Is there any sort order for the samples? (\\theta_1, ..., \\theta_K)? e.g.\nare samples with higher posterior probabilities likely to be at higher indices?\nWhy not report the result of randomly selecting K out of S samples, as an\nadditional alternative?\n\nRegular RNN LMs are known to be expensive to train and evaluate. It would be\nvery useful to compare the training/evaluation times for the proposed Bayesian\nlearning algorithms with SGD+ dropout. That would allow the readers to\ntrade-off improvements versus increase in training/run times.\n\nClarifications:\nL346: What does \\theta_s refer to? Is this a MAP estimate of parameters based\non only the sample s?\nL453-454: Clarify what \\theta means in the context of dropout/dropconnect. \n\nTypos:\nL211: output\nL738: RMSProp", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and\nStochastic Optimization in deep learning context. Given dropout/dropConnect and\nvariational inference are commonly used to reduce the overfit, the more\nsystematic way to introduce/analyse such bayesian learning based algorithms\nwould benefit deep learning community.\n2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout\noutperforms RMSProp + dropout, which clearly shows that uncertainty modeling\nwould help reducing the over-fitting, hence improving accuracy.\n3) The paper has provided the details about the model/experiment setups so the\nresults should be easily reproduced.\n\n- Weaknesses:\n1) The paper does not dig into the theory profs and show the convergence\nproperties of the proposed algorithm.\n2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not\nconduct other comparison. It should explain more about the relation between\npSGLD vs RMSProp other than just mentioning they are conterparts in two\nfamilies.\n2) The paper does not talk about the training speed impact with more details.\n\n- General Discussion:", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: This paper explores a relatively under-explored area of practical\napplication of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian\ntreatment of the parameters of RNNs, it is possible to incorporate benefits of\nmodel averaging during inference. Further, their gradient\nbased sampling approximation to the posterior estimation leads to a procedure\nwhich is easy to implement and is potentially much cheaper than other\nwell-known techniques for model averaging like ensembling.  \nThe effectiveness of this approach is shown on three different tasks --\nlanguage modeling, image captioning and sentence classification; and\nperformance gains are observed over the baseline of single model optimization.\n\n- Weaknesses: Exact experimental setup is unclear. The supplementary material\ncontains important details about burn-in, number of epochs and samples\ncollected that should be in the main paper itself. Moreover, details on how the\ninference is performed would be helpful. Were the samples that were taken\nfollowing HMC for a certain number of epochs after burn in on the training data\nfixed for inference (for every \\tilda{Y} during test time, same samples were\nused according to eqn 5) ? Also, an explicit clarification regarding an\nindependence assumption that p(D|\\theta) = p(Y,X| \\theta) = p(Y| \\theta,X)p(X),\nwhich lets one use the conditional RNN model (if I understand correctly) for\nthe potential U(\\theta) would be nice for completeness.\n\nIn terms of comparison, this paper would also greatly benefit from a\ndiscussion/ experimental comparison with ensembling and distillation methods\n(\"Sequence level knowledge distillation\"; Kim and Rush, \"Distilling an Ensemble\nof Greedy Dependency Parsers into One MST Parser\"; Kuncoro et al.) which  are\nintimately related by a similar goal of incorporating effects of model\naveraging.\n\nFurther discussion related to preference of HMC related sampling\nmethods over other sampling methods or variational approximation would be\nhelpful.\n\nFinally, equation 8 hints at the potential equivalence between dropout and the\nproposed approach and the theoretical justification behind combining SGLD and\ndropout (by making the equivalence more concrete) would lead to a better\ninsight into the effectiveness of the proposed approach.  \n\n- General Discussion: Points addressed above.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.", "histories": [], "id": "554", "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling"}
