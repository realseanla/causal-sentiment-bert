{"reviews": [{"IMPACT": "2", "SUBSTANCE": "1", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "2", "PRESENTATION_FORMAT": "Poster", "comments": "This paper describes a system to assist written test scoring.\n\n- Strengths:\nThe paper represents an application of an interesting NLP problem --\nrecognizing textual entailment -- to an important task -- written test scoring.\n\n- Weaknesses:\nThere isn't anything novel in the paper. It consist of an application of an\nexisting technology to a known problem.\n\nThe approach described in the paper is not autonomous -- it still needs a human\nto do the actual scoring. The paper lacks any quantitative or qualitative\nevaluation of how useful such system is. That is, is it making the job of the\nscorer easier? Is the scorer more effective as compared to not having automatic\nscore?\n\nThe system contains multiple components and it is unclear how the quality of\neach one of them contributes to the overall experience.\n\nThe paper needs more work with the writing. Language and style is rough in\nseveral places.\n\nThe paper also contains several detailed examples, which don't necessarily add\na lot of value to the discussion.\n\n For the evaluation of classification, what is the baseline of predicting the\nmost frequent class?\n\n- General Discussion:\nI find this paper not very inspiring. I don't see the message in the paper\napart from announcing having build such a system", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "1", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "3", "MEANINGFUL_COMPARISON": "1", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThis paper tries to tackle a very practical problem: automated short answer\nscoring (SAS), in particular for Japanese which hasn't gotten as much attention\nas, say, English-language SAS.\n\n- Weaknesses:\n\nThe paper simply reads like a system description, and is light on experiments\nor insights. The authors show a lack of familiarity with more recent related\nwork (aimed at English SAS), both in terms of methodology and evaluation. Here\nare a couple:\n\nhttps://www.aclweb.org/anthology/W/W15/W15-06.pdf#page=97\nhttps://www.aclweb.org/anthology/N/N15/N15-1111.pdf\n\nThere was also a recent Kaggle competition that generated several\nmethodologies:\n\nhttps://www.kaggle.com/c/asap-sas\n\n- General Discussion:\n\nTo meet ACL standards, I would have preferred to see more experiments (feature\nablation studies, algorithm comparisons) that motivated the final system\ndesign, as well as some sort of qualitative evaluation with a user study of how\nthe mixed-initiative user interface features led to improved scores. As it is,\nit feels like a work in progress without any actionable new methods or\ninsights.\n\nAlso, Pearson/Spearman correlation and kappa scores are considered more\nappropriate than accuracy for these sorts of ordinal human scores.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "2", "PRESENTATION_FORMAT": "Poster", "comments": "This paper presents a text classification method based on pre-training\ntechnique using both labeled and unlabeled data. The authors reported\nexperimental results with several benchmark data sets including TREC data, and\nshowed that the method improved overall performance compared to other\ncomparative methods.\n\nI think the approach using pre-training and fine-tuning itself is not a novel\none, but the originality is the use of both labeled and unlabeled data in the\npre-training step. \nThe authors compare their results against three baselines, i.e. without\npre-training and a deep learning with unsupervised pre-training using deep\nautoencoders, but I think that I would be interesting to compare the method\nagainst other methods presented in the introduction section.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "3"}], "abstract": "We have developed an automated Japanese short-answer scoring and support machine for new National Center written test exams. Our approach is based on the fact that recognizing textual entailment and/or synonymy has been almost impossible for several years. The system generates automated scores on the basis of evaluation criteria or rubrics, and human raters revise them. The system determines semantic similarity between the model answers and the actual written answers as well as a certain degree of semantic identity and implication. Owing to the need for the scoring results to be classified at multiple levels, we use random forests to utilize many predictors effectively rather than use support vector machines. An experimental prototype operates as a web system on a Linux computer. We compared human scores with the automated scores for a case in which 3--6 allotment points were placed in 8 categories of a social studies test as a trial examination. The differences between the scores were within one point for 70--90 percent of the data when high semantic judgment was not needed.", "histories": [], "id": 97, "title": "AI-based Japanese Short-answer Scoring and Support System"}
