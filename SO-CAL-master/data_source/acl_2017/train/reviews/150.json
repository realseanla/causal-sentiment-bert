{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThe authors present a novel adaptation of encoder-decoder neural MT using an\napproach that starts and ends with characters, but in between works with\nrepresentations of morphemes and characters. \n\nThe authors release both their code as well as their final learned models for\nfr-en, cs-en, and en-cs. This is helpful in validating their work, as well as\nfor others looking to replicate and extends this work.\n\nThe system reported appears to produce translation results of reasonable\nquality even after the first training epoch, with continued progress in future\nepochs.\n\nThe system appears to learn reasonable morphological tokenizations, and appears\nable to handle previously unseen words (even nonce words) by implicitly backing\noff to morphemes.\n\n- Weaknesses:\n\nIn the paper, the authors do not explicitly state which WMT test and dev sets\ntheir results are reported on. This is problematic for readers wishing to\ncompare the reported results to existing work (for example, the results at\nmatrix.statmt.org). The only way this reviewer found to get this information\nwas to look in the README of the code supplement, which indicates that the test\nset was newstest2015 and the dev test was newstest2013. This should have been\nexplicitly described in the paper.\n\nThe instructions given in the software README are OK, but not great. The\ntraining and testing sections each could be enhanced with explicit examples of\nhow to run the respective commands. The software itself should respond to a\n--help flag, which it currently does not.\n\nThe paper describes a 6-level architecture, but the diagram in Figure 2 appears\nto show fewer than 6 layers. What's going on? The caption should be more\nexplicit, and if this figure is not showing all of the layers, then there\nshould be a figure somewhere (even if it's in an appendix) showing all of the\nlayers.\n\nThe results show comparison to other character-based neural systems, but do not\nshow state-of-the-art results for other types of MT system. WMT (and\nmatrix.statmt.org) has reported results for other systems on these datasets,\nand it appears that the state-of-the-art is much higher than any of the results\nreported in this paper. That should be acknowledged, and ideally should be\ndiscussed.\n\nThere are a handful of minor English disfluencies, misspellings, and minor\nLaTeX issues, such as reverse quotation marks. These should be corrected.\n\n- General Discussion:\n\nPaper is a nice contribution to the existing literature on character-based\nneural MT.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: In general, the paper is well structured and clear. It is possible\nto follow most of the explanation, the ideas presented are original and the\nresults obtained are quite interesting.\n\n- Weaknesses: I have some doubts about the interpretation of the results. In\naddition, I think that some of the claims regarding the capability of the\nmethod proposed to learn morphology are not propperly backed by scientific\nevidence.\n\n- General Discussion:\n\nThis paper explores a complex architecture for character-level neural machine\ntranslation (NMT). The proposed architecture extends a classical\nencoder-decoder architecture by adding a new deep word-encoding layer capable\nof encoding the character-level input into sub-word representations of the\nsource-language sentence. In the same way, a deep word-decoding layer is added\nto the output to transform the target-language sub-word representations into a\ncharacter sequence as the final output of the NMT system. The objective of such\narchitecture is to take advantage of the benefits of character-level NMT\n(reduction of the size of the vocabulary and flexibility to deal with unseen\nwords) and, at the same time, improving the performance of the whole system by\nusing an intermediate representation of sub-words to reduce the size of the\ninput sequence of characters. In addition, the authors claim that their deep\nword-encoding model is able to learn morphology better than other\nstate-of-the-art approaches.\n\nI have some concerns regarding the evaluation. The authors compare their\napproach to other state-of-the-art systems taking into account two parameters:\ntraining time and BLEU score. However, I do not clearly see the advantage of\nthe model proposed (DCNMT) in front of other approaches such as bpe2char. The\ndifference between both approaches as regards BLEU score is very small (0.04 in\nCs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming\nthe other one without statistical significance information: has statistical\nsignificance been evaluated? As regards the training time, it is worth\nmentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs\ntraining time is not provided (why not?) and for En-Fr bpe2char is not\nevaluated. I think that a more complete comparison with this system should be\ncarried out to prove the advantages of the model proposed.\n\nMy second concern is on the 5.2 Section, where authors start claiming that they\ninvestigated about the ability of their system to learn morphology. However,\nthe section only contains a examples and some comments on them. Even though\nthese examples are very well chosen and explained in a very didactic way, it is\nworth noting that no experiments or formal evaluation seem to have been\ncarried out to support the claims of the authors. I would definitely encourage\nauthors to extend this very interesting part of the paper that could even\nbecome a different paper itself. On the other hand, this Section does not seem\nto be a critical point of the paper, so for the current work I may suggest just\nto move this section to an appendix and soften some of the claims done\nregarding the capabilities of the system to learn morphology.\n\nOther comments, doubts and suggestions:\n\n - There are many acronyms that are used but are not defined (such as LSTM,\nHGRU, CNN or PCA) or which are defined after starting to use them (such as RNN\nor BPE). Even though some of these acronyms are well known in the field of deep\nlearning, I would encourage the authors to defined them to improve clearness.\n\n - The concept of energy is mentioned for the first time in Section 3.1. Even\nthough the explanation provided is enough at that point, it would be nice to\nrefresh the idea of energy in Section 5.2 (where it is used several times) and\nproviding some hints about how to interpret it: a high energy on a character\nwould be indicating that the current morpheme should be split at that point? In\naddition, the concept of peak (in Figure 5) is not described.\n\n - When the acronym BPE is defined, capital letters are used, but then, for the\nrest of mentions it is lower cased; is there a reason for this?\n\n - I am not sure if it is necessary to say that no monolingual corpus is used\nin Section 4.1.\n\n - It seems that there is something wrong with Figure 4a since the colours for\nthe energy values are not shown for every character.\n\n - In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not\ntaken from the papers, since they are not reported. If the authors computed\nthese results by themselves (as it seems) they should mention it.\n\n - I would not say that French is morphologically poor, but rather that it is\nnot that rich as Slavic languages such as Czech.\n\n - Why a link is provided for WMT'15 training corpora but not for WMT'14?\n\n - Several references are incomplete\n\nTypos:\n\n  - \"..is the bilingual, parallel corpora provided...\" -> \"..are the bilingual,\nparallel corpora provided...\"\n\n  - \"Luong and Manning (2016) uses\" -> \"Luong and Manning (2016) use\"\n\n  - \"HGRU (It is\" -> \"HGRU (it is\"\n\n  - \"coveres\" -> \"covers\"\n\n  - \"both consists of two-layer RNN, each has 1024\" -> \"both consist of\ntwo-layer RNN, each have 1024\"\n\n  - \"the only difference between CNMT and DCNMT is CNMT\" -> \"the only\ndifference between CNMT and DCNMT is that CNMT\"", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models and conventional character-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Moreover, the final BLEU score of our model is comparable to the state-of-the-art systems. Further analyses show that our model is able to learn morphology.", "histories": [], "id": 150, "title": "Deep Character-Level Neural Machine Translation By Learning Morphology"}
