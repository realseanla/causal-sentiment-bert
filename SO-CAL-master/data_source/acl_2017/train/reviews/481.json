{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "In this work, the authors extend MS-COCO by adding an incorrect\ncaption to each existing caption, with only one word of difference.\nThe authors demonstrate that two state-of-the-art methods (one for VQA\nand one for captioning) perform extremely poorly at a) determining if\na caption is fake, b) determining which word in a fake caption is\nwrong, and c) selecting a replacement word for a given fake word.\n\nThis work builds upon a wealth of literature regarding the\nunderperformance of vision/language models relative to their apparent\ncapacities. I think this work makes concrete some of the big,\nfundamental questions in this area: are vision/language models doing\n\"interesting\" things, or not? The authors consider a nice mix of tasks\nand models to shed light on the \"broken-ness\" of these settings, and\nperform some insightful analyses of factors associated with model\nfailure (e.g., Figure 3).\n\nMy biggest concerns with the paper are similarity to Ding et al. That\nbeing said, I do think the authors make some really good points; Ding\net al. generate similar captions, but the ones here differ by only one\nword and *still* break the models -- I think that's a justifiably\nfundamental difference. That observation demonstrates that Ding et\nal.'s engineering is not a requirement, as this simple approach still\nbreaks things catastrophically.\n\nAnother concern is the use of NeuralTalk to select the \"hardest\"\nfoils.              While a clever idea, I am worried that the use of this model\ncreates a risk of self-reinforcement bias, i.e., NeuralTalk's biases\nare now fundamentally \"baked-in\" to FOIL-COCO. \n\nI think the results section could be a bit longer, relative to the\nrest of the paper (e.g. I would've liked more than one paragraph -- I\nliked this part!)\n\nOverall, I do like this paper, as it nicely builds upon some results\nthat highlight defficiencies in vision/language integration. In the\nend, the Ding et al. similarity is not a \"game-breaker,\" I think -- if\nanything, this work shows that vision/language models are so easy to\nfool, Ding et al.'s method is not even required.\n\nSmall things:\n\nI would've liked to have seen another baseline that simply\nconcatenates BoW + extracted CNN features and trains a softmax\nclassifier over them. The \"blind\" model is a nice touch, but what\nabout a \"dumb\" vision+langauge baseline? I bet that would do close to\nas well as the LSTM/Co-attention. That could've made the point of the\npaper even stronger.\n\n330: What is a supercategory? Is this from WordNet? Is this from COCO?\nI understand the idea, but not the specifics.\n\n397: has been -> were\n\n494: that -> than\n\n693: artefact -> undesirable artifacts (?)\n\n701: I would have included a chance model in T1's table -- is 19.53%\n[Line 592] a constant-prediction baseline? Is it 50% (if so, can't we\nflip all of the \"blind\" predictions to get a better baseline?) I am\nnot entirely clear, and I think a \"chance\" line here would fix a lot\nof this confusion.\n\n719: ariplane\n\n~~\nAfter reading the author response...\n\nI think this author response is spot-on. Both my concerns of NeuralTalk biases\nand additional baselines were addressed, and I am confident that these can be\naddressed in the final version, so I will keep my score as-is.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and `foil' captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (`foil word'). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption  classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.", "histories": [], "id": "481", "title": "FOIL it! Find One mismatch between Image and Language caption"}
