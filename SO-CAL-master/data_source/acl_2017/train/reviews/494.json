{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\n- nice, clear application of linguistics ideas to distributional semantics\n- demonstrate very clear improvements on both intrinsic and extrinsic eval\n\n- Weaknesses:\n\n- fairly straightforward extension of existing retrofitting work\n- would be nice to see some additional baselines (e.g. character embeddings)\n\n- General Discussion:\n\nThe paper describes \"morph-fitting\", a type of retrofitting for vector spaces\nthat focuses specifically on incorporating morphological constraints into the\nvector space. The framework is based on the idea of \"attract\" and \"repel\"\nconstraints, where attract constraints are used to pull morphological\nvariations close together (e.g. look/looking) and repel constraints are used to\npush derivational antonyms apart (e.g. responsible/irresponsible). They test\ntheir algorithm on multiple different vector spaces and several language, and\nshow consistent improvements on intrinsic evaluation (SimLex-999, and\nSimVerb-3500). They also test on the extrinsic task of dialogue state tracking,\nand again demonstrate measurable improvements over using\nmorphologically-unaware word embeddings.\n\nI think this is a very nice paper. It is a simple and clean way to incorporate\nlinguistic knowledge into distributional models of semantics, and the empirical\nresults are very convincing. I have some questions/comments below, but nothing\nthat I feel should prevent it from being published.\n\n- Comments for Authors\n\n1) I don't really understand the need for the morph-simlex evaluation set. It\nseems a bit suspect to create a dataset using the same algorithm that you\nultimately aim to evaluate. It seems to me a no-brainer that your model will do\nwell on a dataset that was constructed by making the same assumptions the model\nmakes. I don't think you need to include this dataset at all, since it is a\npotentially erroneous evaluation that can cause confusion, and your results are\nconvincing enough on the standard datasets.\n\n2) I really liked the morph-fix baseline, thank you for including that. I would\nhave liked to see a baseline based on character embeddings, since this seems to\nbe the most fashionable way, currently, to side-step dealing with morphological\nvariation. You mentioned it in the related work, but it would be better to\nactually compare against it empirically.\n\n3) Ideally, we would have a vector space where morphological variants are just\nclose together, but where we can assign specific semantics to the different\ninflections. Do you have any evidence that the geometry of the space you end\nwith is meaningful. E.g. does \"looking\" - \"look\" + \"walk\" = \"walking\"? It would\nbe nice to have some analysis that suggests the morphfitting results in a more\nmeaningful space, not just better embeddings.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The authors propose \u2018morph-fitting\u2019, a method that retrofits any given set\nof trained word embeddings based on a morphologically-driven objective that (1)\npulls inflectional forms of the same word together (as in \u2018slow\u2019 and\n\u2018slowing\u2019) and (2) pushes derivational antonyms apart (as in\n\u2018expensive\u2019 and \u2018inexpensive\u2019). With this, the authors aim to improve\nthe representation of low-frequency inflections of words as well as mitigate\nthe tendency of corpus-based word embeddings to assign similar representations\nto antonyms. The method is based on relatively simple manually-constructed\nmorphological rules and is demonstrated on both English, German, Italian and\nRussian. The experiments include intrinsic word similarity benchmarks, showing\nnotable performance improvements achieved by applying morph-fitting to several\ndifferent corpus-based embeddings. Performance improvement yielding new\nstate-of-the-art results is also demonstrated for German and Italian on an\nextrinsic task - dialog state tracking. \n\nStrengths:\n\n- The proposed method is simple and shows nice performance improvements across\na number of evaluations and in several languages. Compared to previous\nknowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a\nfew manually-constructed rules, instead of a large-scale knowledge base, such\nas an ontology.\n\n- Like previous retrofitting approaches, this method is easy to apply to\nexisting sets of embeddings and therefore it seems like the software that the\nauthors intend to release could be useful to the community.\n\n- The method and experiments are clearly described.\u2028\n\nWeaknesses:\n\n- I was hoping to see some analysis of why the morph-fitted embeddings worked\nbetter in the evaluation, and how well that corresponds with the intuitive\nmotivation of the authors. \n\n- The authors introduce a synthetic word similarity evaluation dataset,\nMorph-SimLex. They create it by applying their presumably\nsemantic-meaning-preserving morphological rules to SimLex999 to generate many\nmore pairs with morphological variability. They do not manually annotate these\nnew pairs, but rather use the original similarity judgements from SimLex999.\nThe obvious caveat with this dataset is that the similarity scores are presumed\nand therefore less reliable. Furthermore, the fact that this dataset was\ngenerated by the very same rules that are used in this work to morph-fit word\nembeddings, means that the results reported on this dataset in this work should\nbe taken with a grain of salt. The authors should clearly state this in their\npaper.\n\n- (Soricut and Och, 2015) is mentioned as a future source for morphological\nknowledge, but in fact it is also an alternative approach to the one proposed\nin this paper for generating morphologically-aware word representations. The\nauthors should present it as such and differentiate their work.\n\n- The evaluation does not include strong morphologically-informed embedding\nbaselines. \n\nGeneral Discussion:\n\nWith the few exceptions noted, I like this work and I think it represents a\nnice contribution to the community. The authors presented a simple approach and\nshowed that it can yield nice improvements using various common embeddings on\nseveral evaluations and four different languages. I\u2019d be happy to see it in\nthe conference.\n\nMinor comments:\n\n- Line 200: I found this phrasing unclear: \u201cWe then query \u2026 of linguistic\nconstraints\u201d.\n\n- Section 2.1: I suggest to elaborate a little more on what the delta is\nbetween the model used in this paper and the one it is based on in Wieting\n2015. It seemed to me that this was mostly the addition of the REPEL part.\n\n- Line 217: \u201cThe method\u2019s cost function consists of three terms\u201d - I\nsuggest to spell this out in an equation.\n\n- Line 223:  x and t in this equation (and following ones) are the vector\nrepresentations of the words. I suggest to denote that somehow. Also, are the\nvectors L2-normalized before this process? Also, when computing \u2018nearest\nneighbor\u2019 examples do you use cosine or dot-product? Please share these\ndetails.\n\n- Line 297-299: I suggest to move this text to Section 3, and make the note\nthat you did not fine-tune the params in the main text and not in a footnote.\n\n- Line 327: (create, creates) seems like a wrong example for that rule.\u2028\n\n* I have read the author response", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for low-frequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that  'inexpensive' is a rephrasing for 'expensive' or may not associate 'acquire' with 'acquires'. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.", "histories": [], "id": 494, "title": "Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules"}
