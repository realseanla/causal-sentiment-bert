{"reviews": [{"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper proposes an approach to sequence labeling with multitask learning,\nwhere language modeling is uses as the auxiliary objective. Thus, a\nbidirectional neural network architecture learns to predict the output labels\nas well as to predict the previous or next word in the sentence. The joint\nobjectives lead to improvements over the baselines in grammatical error\ndetection, chunking, NER, and POS tagging.\n\n- Strengths:\n\nThe contribution is quite well-written and easy to follow for the most part.\nThe model is exposed in sufficient detail, and the experiments are thorough\nwithin the defined framework. The benefits of introducing an auxiliary\nobjective are nicely exposed.\n\n- Weaknesses:\n\nThe paper shows very limited awareness of the related work, which is extensive\nacross the tasks that the experiments highlight. Tables 1-3 only show the three\nsystems proposed by the contribution (Baseline, +dropout, and +LMcost), while\nsome very limited comparisons are sketched textually.\n\nA contribution claiming novelty and advancements over the previous state of the\nart should document these improvements properly: at least by reporting the\nrelevant scores together with the novel ones, and ideally through replication.\nThe datasets used in the experiments are all freely available, the previous\nresults well-documented, and the previous systems are for the most part\npublicly available.\n\nIn my view, for a long paper, it is a big flaw not to treat the previous work\nmore carefully.\n\nIn that sense, I find this sentence particularly troublesome: \"The baseline\nresults are comparable to the previous best results on each of these\nbenchmarks.\" The reader is here led to believe that the baseline system somehow\nsubsumes all the previous contributions, which is shady on first read, and\nfactually incorrect after a quick lookup in related work.\n\nThe paper states \"new state-of-the-art results for error detection on both FCE\nand CoNLL-14 datasets\". Looking into the CoNLL 2014 shared task report, it is\nnot straightforward to discern whether the\nlatter part of the claim does holds true, also as per Rei and Yannakoudakis'\n(2016) paper. The paper should support the claim by inclusion/replication of\nthe related work.\n\n- General Discussion:\n\nThe POS tagging is left as more of an afterthought. The comparison to Plank et\nal. (2016) is at least partly unfair as they test across multiple languages in\nthe Universal Dependencies realm, showing top-level performance across language\nfamilies, which I for one believe to be far more relevant than WSJ\nbenchmarking. How does the proposed system scale up/down to multiple languages,\nlow-resource languages with limited training data, etc.? The paper leaves a lot\nto ask for in that dimension to further substantiate its claims.\n\nI like the idea of including language modeling as an auxiliary task. I like the\narchitecture, and sections 1-4 in general. In my view, there is a big gap\nbetween those sections and the ones describing the experiments (5-8).\n\nI suggest that this nice idea should be further fleshed out before publication.\nThe rework should include at least a more fair treatment of related work, if\nnot replication, and at least a reflection on multilinguality. The data and the\nsystems are all there, as signs of the field's growing maturity. The paper\nshould in my view partake in reflecting this maturity, and not step away from\nit. In faith that these improvements can be implemented before the publication\ndeadline, I vote borderline.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths: The article is well written; what was done is clear and\nstraightforward. Given how simple the contribution is, the gains are\nsubstantial, at least in the error correction task.\n\n- Weaknesses: The novelty is fairly limited (essentially, another permutation\nof tasks in multitask learning), and only one way of combining the tasks is\nexplored. E.g., it would have been interesting to see if pre-training is\nsignificantly worse than joint training; one could initialize the weights from\nan existing RNN LM trained on unlabeled data; etc.\n\n- General Discussion: I was hesitating between a 3 and a 4. While the\nexperiments are quite reasonable and the combinations of tasks sometimes new,\nthere's quite a bit of work on multitask learning in RNNs (much of it already\ncited), so it's hard to get excited about this work. I nevertheless recommend\nacceptance because the experimental results may be useful to others.\n\n- Post-rebuttal: I've read the rebuttal and it didn't change my opinion of the\npaper.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.", "histories": [], "id": "276", "title": "Semi-supervised Multitask Learning for Sequence Labeling"}
