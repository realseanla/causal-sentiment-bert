{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper proposes a recurrent neural architecture that can skip irrelevant\ninput units. This is achieved by specifying R (# of words to read at each\n\"skim\"), K (max jump size), and N (max # of jumps allowed). An LSTM processes R\nwords, predicts the jump size k in {0, 1...K} (0 signals stop), skips the next\nk-1 words and continues until either the number of jumps reaches N or the model\nreaches the last word. While the model is not differentiable, it can be trained\nby standard policy gradient. The work seems to have been heavily influenced by\nShen et al. (2016) who apply a similar reinforcement learning approach\n(including the same variance stabilization) to multi-pass machine reading. \n\n- Strengths:\n\nThe work simulates an intuitive \"skimming\" behavior of a reader, mirroring Shen\net al. who simulate (self-terminated) repeated reading. A major attribute of\nthis work is its simplicity. Despite the simplicity, the approach yields\nfavorable results. In particular, the authors show through a well-designed\nsynthetic experiment that the model is indeed able to learn to skip when given\noracle jump signals. In text classification using real-world datasets, the\nmodel is able to perform competitively with the non-skimming model while being\nclearly faster. \n\nThe proposed model can potentially have meaningful practical implications: for\ntasks in which skimming suffices (e.g., sentiment classification), it suggests\nthat we can obtain equivalent results without consuming all data in a\ncompletely automated fashion. To my knowledge this is a novel finding. \n\n- Weaknesses:\n\nIt's a bit mysterious on what basis the model determines its jumping behavior\nso effectively (other than the synthetic dataset). I'm thinking of a case where\nthe last part of the given sentence is a crucial evidence, for instance: \n\n\"The movie was so so and boring to the last minute but then its ending blew me\naway.\" \n\nIn this example, the model may decide to skip the rest of the sentence after\nreading \"so so and boring\". But by doing so it'll miss the turning point\n\"ending blew me away\" and mislabel the instance as negative. For such cases a\nsolution can be running the skimming model in both directions as the authors\nsuggest as future work. But in general the model may require more sophisticated\narchitecture for controlling skimming.\n\nIt seems one can achieve improved skimming by combining it with multi-pass\nreading (presumably in reverse directions). That's how humans read to\nunderstand text that can't be digested in one skim; indeed, that's how I read\nthis draft. \n\nOverall, the work raises an interesting problem and provides an effective but\nintuitive solution.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\\&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.", "histories": [], "id": "760", "title": "Learning to Skim Text"}
