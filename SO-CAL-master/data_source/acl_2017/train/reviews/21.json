{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "The paper is clearly written, and the claims are well-supported.  The Related\nWork in particular is very thorough, and clearly establishes where the proposed\nwork fits in the field.\n\nI had two main questions about the method: (1) phrases are mentioned in section\n3.1, but only word representations are discussed.  How are phrase\nrepresentations derived?\n(2) There is no explicit connection between M^+ and M^- in the model, but they\nare indirectly connected through the tanh scoring function.  How do the learned\nmatrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what\nwould be the benefits/drawbacks of linking the two together directly, by\nenforcing some measure of dissimilarity?\n\nAdditionally, statistical significance of the observed improvements would be\nvaluable.\n\nTypographical comments:\n- Line 220: \"word/phase pair\" should be \"word/phrase pair\"\n- Line 245: I propose an alternate wording: instead of \"entities are translated\nto,\" say \"entities are mapped to\".  At first, I read that as a translation\noperation in the vector space, which I think isn't exactly what's being\ndescribed.\n- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement\nin F-measure\"\n- Line 636: extraneous commas in citation\n- Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing)\n- Line 727: extraneous period and comma in citation", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\n1. Interesting research problem\n2. The method in this paper looks quite formal.\n3. The authors have released their dataset with the submission.\n4. The design of experiments is good.\n\n- Weaknesses:\n\n1. The advantage and disadvantage of the transductive learning has not yet\ndiscussed.\n\n- General Discussion:\n\nIn this paper, the authors introduce a transductive learning approach for\nChinese hypernym prediction, which is quite interesting problem. The authors\nestablish mappings from entities to hypernyms in the embedding space directly,\nwhich sounds also quite novel. This paper is well written and easy to follow.\nThe first part of their method, preprocessing using embeddings, is widely used\nmethod for the initial stage. But it's still a normal way to preprocess the\ninput data. The transductive model is an optimization framework for non-linear\nmapping utilizing both labeled and unlabeled data. The attached supplementary\nnotes about the method makes it more clear. The experimental results have shown\nthe effectiveness of the proposed method in this paper. The authors also\nreleased dataset, which contributes to similar research for other researchers\nin future.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Finding the correct hypernyms for entities\u00a0is essential for taxonomy learning, fine-grained entity categorization, query understanding, etc. Due to the flexibility\u00a0of the Chinese language, it is challenging\u00a0to identify hypernyms in Chinese accurately. Rather than extracting hypernyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hypernyms in\u00a0the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding\u00a0arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction.", "histories": [], "id": "21", "title": "Transductive Non-linear Learning for Chinese Hypernym Prediction"}
