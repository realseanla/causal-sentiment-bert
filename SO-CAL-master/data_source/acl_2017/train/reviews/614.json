{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes integrating word sense inventories into existing approaches\nfor the lexical substitution task by using these inventories to filter\ncandidates. To do so, the authors first propose a metric to measure the mutual\nsubstitutability of sense inventories with human judgments for the lexsub task,\nand empirically measure the substitutability of inventories from various\nsources such as WordNet and PPDB. Next, they propose clustering different\nparaphrases of a word from PPDB using a multi-view clustering approach, to\nautomatically generate a sense inventory instead of using the aforementioned\ninventories. Finally, they use these clusters with a naive (majority in top 5)\nWSD technique to filter existing ranked list of substitution candidates.\n\n- Strengths:\n\n* The key idea of marrying vector space model based approaches and sense\ninventories for the lexsub task is useful since these two techniques seem to\nhave complementary information, especially since the vector space models are\ntypically unaware of sense and polysemy.\n\n* The oracle evaluation is interesting as it gives a clear indication of how\nmuch gain can one expect in the best case, and while there is still a large gap\nbetween the oracle and actual scores, we can still argue for the usefulness of\nthe proposed approach due to the large difference between the unfiltered GAP\nand the oracle GAP.\n\n- Weaknesses:\n\n* I don't understand effectiveness of the multi-view clustering approach.\nAlmost all across the board, the paraphrase similarity view does significantly\nbetter than other views and their combination. What, then, do we learn about\nthe usefulness of the other views? There is one empirical example of how the\ndifferent views help in clustering paraphrases of the word 'slip', but there is\nno further analysis about how the different clustering techniques differ,\nexcept on the task directly. Without a more detailed analysis of differences\nand similarities between these views, it is hard to draw solid conclusions\nabout the different views.                                  \n\n* The paper is not fully clear on a first read. Specifically, it is not\nimmediately clear how the sections connect to each other, reading more like\ndisjoint pieces of work. For instance, I did not understand the connections\nbetween section 2.1 and section 4.3, so adding forward/backward pointer\nreferences to sections should be useful in clearing up things. Relatedly, the\nmulti-view clustering section (3.1) needs editing, since the subsections seem\nto be out of order, and citations seem to be missing (lines 392 and 393).\n\n* The relatively poor performance on nouns makes me uneasy. While I can expect\nTWSI to do really well due to its nature, the fact that the oracle GAP for\nPPDBClus is higher than most clustering approaches is disconcerting, and I\nwould like to understand the gap better. This also directly contradicts the\nclaim that the clustering approach is generalizable to all parts of speech\n(124-126), since the performance clearly isn't uniform.\n\n- General Discussion:\n\nThe paper is mostly straightforward in terms of techniques used and\nexperiments. Even then, the authors show clear gains on the lexsub task by\ntheir two-pronged approach, with potentially more to be gained by using\nstronger WSD algorithms.\n\nSome additional questions for the authors :\n\n* Lines 221-222 : Why do you add hypernyms/hyponyms?\n* Lines 367-368 : Why does X^{P} need to be symmetric?\n* Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed\narbitrary or is this a principled choice?\n* Is the high performance of SubstClus^{P} ascribable to the fact that the\nnumber of clusters was tuned based on this view? Would tuning the number of\nclusters based on other matrices affect the results and the conclusions?\n* What other related tasks could this approach possibly generalize to? Or is it\nonly specific to lexsub?", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "Strengths:\nThe paper presents a new method that exploits word senses to improve the task\nof lexical substitutability.  Results show improvements over prior methods.\n\nWeaknesses:\nAs a reader of a ACL paper, I usually ask myself what important insight can I\ntake away from the paper, and from a big picture point of view, what does the\npaper add to the fields of natural language processing and computational\nlinguistics.  How does the task of lexical substitutability in general and this\npaper in particular help either in improving an NLP system or provide insight\nabout language?  I can't find a good answer answer to either question after\nreading this paper.\n\nAs a practitioner who wants to improve natural language understanding system, I\nam more focused on the first question -- does the lexical substitutability task\nand the improved results compared to prior work presented here help any end\napplication?  Given the current state of high performing systems, any discrete\nclustering of words (or longer utterances) often break down when compared to\ncontinuous representations words (see all the papers that utilitize discrete\nlexical semantics to achieve a task versus words' distributed representations\nused as an input to the same task; e.g. machine translation, question\nanswering, sentiment analysis, text classification and so forth).  How do the\nauthors motivate work on lexical substitutability given that discrete lexical\nsemantic representations often don't work well?  The introduction cites a few\npapers from several years back that are mostly set up in small data scenarios,\nand given that this word is based on English, I don't see why one would use\nthis method for any task.  I would be eager to see the authors' responses to\nthis general question of mine.\n\nAs a minor point, to further motivate this, consider the substitutes presented\nin Table 1.\n1. Tasha snatched it from him to rip away the paper.\n2. Tasha snatched it from him to rip away the sheet.\n\nTo me, these two sentences have varying meanings -- what if he was holding on\nto a paper bag?  In that scenario, can the word \"paper\" be substituted by\n\"sheet\"?  At least, in my understanding, it cannot.  Hence, there is so much\nsubjectivity in this task that lexical substitutes can completely alter the\nsemantics of the original sentence.\n\nMinor point(s):\n - Citations in Section 3.1.4 are missing.\n\nAddition: I have read the author response and I am sticking to my earlier\nevaluation of the paper.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Current state-of-the-art models for lexical substitution -- the task of nominating substitutes for a word in context -- ignore word sense, instead relying on powerful vector and embedded word representations to find good substitutes. We present a simple method for improving the lexical substitution rankings of existing models by integrating word sense inventories, filtering substitutes from the correct sense to the top of the rankings. To enable maximum coverage of our method, we also propose a novel method for clustering paraphrases by word sense with substitutability in mind. Our method results in sense clusters that are more substitutable and have wider coverage than existing sense inventories. They can be applied as a filter over lexical substitution rankings generated by existing vector- and embedding-based ranking models to significantly improve their performance.", "histories": [], "id": 614, "title": "Clustering Paraphrases for Substitutability"}
