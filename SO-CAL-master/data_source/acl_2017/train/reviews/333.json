{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThe authors propose a selective encoding model as extension to the\nsequence-to-sequence framework for abstractive sentence summarization. The\npaper is very well written and the methods are clearly described. The proposed\nmethods are evaluated on standard benchmarks and comparison to other\nstate-of-the-art tools are presented, including significance scores. \n\n- Weaknesses:\n\nThere are some few details on the implementation and on the systems to which\nthe authors compared their work that need to be better explained. \n\n- General Discussion:\n\n* Major review:\n\n- I wonder if the summaries obtained using the proposed methods are indeed\nabstractive. I understand that the target vocabulary is build out of the words\nwhich appear in the summaries in the training data. But given the example shown\nin Figure 4, I have the impression that the summaries are rather extractive.\nThe authors should choose a better example for Figure 4 and give some\nstatistics on the number of words in the output sentences which were not\npresent in the input sentences for all test sets.\n\n- page 2, lines 266-272: I understand the mathematical difference between the\nvector hi and s, but I still have the feeling that there is a great overlap\nbetween them. Both \"represent the meaning\". Are both indeed necessary? Did you\ntrying using only one of them.\n\n- Which neural network library did the authors use for implementing the system?\nThere is no details on the implementation.\n\n- page 5, section 44: Which training data was used for each of the systems that\nthe authors compare to? Diy you train any of them yourselves?\n\n* Minor review:\n\n- page 1, line 44: Although the difference between abstractive and extractive\nsummarization is described in section 2, this could be moved to the\nintroduction section. At this point, some users might no be familiar with this\nconcept.\n\n- page 1, lines 93-96: please provide a reference for this passage: \"This\napproach achieves huge success in tasks like neural machine translation, where\nalignment between all parts of the input and output are required.\"\n\n- page 2, section 1, last paragraph: The contribution of the work is clear but\nI think the authors should emphasize that such a selective encoding model has\nnever been proposed before (is this true?). Further, the related work section\nshould be moved to before the methods section.\n\n- Figure 1 vs. Table 1: the authors show two examples for abstractive\nsummarization but I think that just one of them is enough. Further, one is\ncalled a figure while the other a table.\n\n- Section 3.2, lines 230-234 and 234-235: please provide references for the\nfollowing two passages: \"In the sequence-to-sequence machine translation (MT)\nmodel, the encoder and decoder are responsible for encoding input sentence\ninformation and decoding the sentence representation to generate an output\nsentence\"; \"Some previous works apply this framework to summarization\ngeneration tasks.\"\n\n- Figure 2: What is \"MLP\"? It seems not to be described in the paper.\n\n- page 3, lines 289-290: the sigmoid function and the element-wise\nmultiplication are not defined for the formulas in section 3.1.\n\n- page 4, first column: many elements of the formulas are not defined: b\n(equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation\n15).\n\n- page 4, line 326: the readout state rt is not depicted in Figure 2\n(workflow).\n\n- Table 2: what does \"#(ref)\" mean?\n\n- Section 4.3, model parameters and training. Explain how you achieved the\nvalues to the many parameters: word embedding size, GRU hidden states, alpha,\nbeta 1 and 2, epsilon, beam size.\n\n- Page 5, line 450: remove \"the\" word in this line? \"SGD as our optimizing\nalgorithms\" instead of \"SGD as our the optimizing algorithms.\"\n\n- Page 5, beam search: please include a reference for beam search.\n\n- Figure 4: Is there a typo in the true sentence? \"council of europe again\nslams french prison conditions\" (again or against?)\n\n- typo \"supper script\" -> \"superscript\" (4 times)", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThe paper is very clear and well-written. It proposes a novel approach to\nabstractive sentence summarization; basically sentence compression that is not\nconstrained to having the words in the output be present in the input. \n\n- Excellent comparison with many baseline systems. \n\n- Very thorough related work. \n\n- Weaknesses:\n\nThe criticisms are very minor:\n\n- It would be best to report ROUGE F-Score for all three datasets. The reasons\nfor reporting recall on one are understandable (the summaries are all the same\nlength), but in that case you could simply report both recall and F-Score. \n\n- The Related Work should come earlier in the paper. \n\n- The paper could use some discussion of the context of the work, e.g. how the\nsummaries / compressions are intended to be used, or why they are needed. \n\n- General Discussion:\n\n- ROUGE is fine for this paper, but ultimately you would want human evaluations\nof these compressions, e.g. on readability and coherence metrics, or an\nextrinsic evaluation.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper presents a new neural approach for summarization. They build on a\nstandard encoder-decoder with attention framework but add a network that gates\nevery encoded hidden state based on summary vectors from initial encoding\nstages. Overall, the method seems to outperform standard seq2seq methods by 1-2\npoints on three different evaluation sets.\n\nOverall, the technical sections of the paper are reasonably clear. Equation 16\nneeds more explanation, I could not understand the notation. The specific\ncontribution,  the selective mechanism, seems novel and could potentially be\nused in other contexts. \n\nThe evaluation is extensive and does demonstrate consistent improvement. One\nwould imagine that adding an additional encoder layer instead of the selective\nlayer is the most reasonable baseline (given the GRU baseline uses only one\nbi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My\none concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch? \n\nThe quality of the writing, especially in the intro/abstract/related work is\nquite bad. This paper does not make a large departure from previous work, and\ntherefore a related work nearby the introduction seems more appropriate. In\nrelated work, one common good approach is highlighting similarities and\ndifferences between your work and previous work, in words before they are\npresented in equations. Simply listing works without relating them to your work\nis not that useful. Placement of the related work near the intro will allow you\nto relieve the intro of significant background detail and instead focus on more\nhigh level.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.", "histories": [], "id": "333", "title": "Selective Encoding for Abstractive Sentence Summarization"}
