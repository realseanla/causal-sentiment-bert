{"reviews": [{"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThis paper presents a step in the direction of developing more challenging\ncorpora for training sentence planners in data-to-text NLG, which is an\nimportant and timely direction. \n\n- Weaknesses:\n\nIt is unclear whether the work reported in this paper represents a substantial\nadvance over Perez-Beltrachini et al.'s (2016) method for selecting content. \nThe authors do not directly compare the present paper to that one. It appears\nthat the main novelty of this paper is the additional analysis, which is\nhowever rather superficial.\n\nIt is good that the authors report a comparison of how an NNLG baseline fares\non this corpus in comparison to that of Wen et al. (2016).  However, the\nBLEU scores in Wen et al.'s paper appear to be much much higher, suggesting\nthat this NNLG baseline is not sufficient for an informative comparison.\n\n- General Discussion:\n\nThe authors need to more clearly articulate why this paper should count as a\nsubstantial advance over what has been published already by Perez-Beltrachini\net al, and why the NNLG baseline should be taken seriously.  In contrast to\nLREC, it is not so common for ACL to publish a main session paper on a corpus\ndevelopment methodology in the absence of some new results of a system making\nuse of the corpus.\n\nThe paper would also be stronger if it included an analysis of the syntactic\nconstructions in the two corpora, thereby more directly bolstering the case\nthat the new corpus is more complex.  The exact details of how the number of\ndifferent path shapes are determined should also be included, and ideally\nassociated with the syntactic constructions.\n\nFinally, the authors should note the limitation that their method does nothing\nto include richer discourse relations such as Contrast, Consequence,\nBackground, etc., which have long been central to NLG. In this respect, the\ncorpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more\ninteresting and should be discussed in comparison to the method here.\n\nReferences\n\nMarilyn Walker, Amanda Stent, Fran\u00e7ois Mairesse, and\nRashmi Prasad. 2007. Individual and domain adaptation\nin sentence planning for dialogue. Journal of\nArtificial Intelligence Research (JAIR), 30:413\u2013456.\n\nAmy Isard, 2016. \u201cThe Methodius Corpus of Rhetorical Discourse\nStructures and Generated Texts\u201d , Proceedings of the Tenth Conference\non Language Resources and Evaluation (LREC 2016), Portoro\u017e, Slovenia,\nMay 2016.\n\n---\nAddendum following author response:\n\nThank you for the informative response.  As the response offers crucial\nclarifications, I have raised my overall rating.  Re the comparison to\nPerez-Beltrachini et al.: While this is perhaps more important to the PC than\nto the eventual readers of the paper, it still seems to this reviewer that the\nadvance over this paper could've been made much clearer.  While it is true that\nPerez-Beltrachini et al. \"just\" cover content selection, this is the key to how\nthis dataset differs from that of Wen et al.  There doesn't really seem to be\nmuch to the \"complete methodology\" of constructing the data-to-text dataset\nbeyond obvious crowd-sourcing steps; to the extent these steps are innovative\nor especially crucial, this should be highlighted.  Here it is interesting that\n8.7% of the crowd-sourced texts were rejected during the verification step;\nrelated to Reviewer 1's concerns, it would be interesting to see some examples\nof what was rejected, and to what extent this indicates higher-quality texts\nthan those in Wen et al.'s dataset.  Beyond that, the main point is really that\ncollecting the crowd-sourced texts makes it possible to make the comparisons\nwith the Wen et al. corpus at both the data and text levels (which this\nreviewer can see is crucial to the whole picture).\n\nRe the NNLG baseline, the issue is that the relative difference between the\nperformance of this baseline on the two corpora could disappear if Wen et al.'s\nsubstantially higher-scoring method were employed.  The assumption that this\nrelative difference would remain even with fancier methods should be made\nexplicit, e.g. by acknowledging the issue in a footnote.  Even with this\nlimitation, the comparison does still strike this reviewer as a useful\ncomponent of the overall comparison between the datasets.\n\nRe whether a paper about dataset creation should be able to get into ACL\nwithout system results:  though this indeed not unprecedented, the key issue is\nperhaps how novel and important the dataset is likely to be, and here this\nreviewer acknowledges the importance of the dataset in comparison to existing\nones (even if the key advance is in the already published content selection\nwork).\n\nFinally, this reviewer concurs with Reviewer 1 about the need to clarify the\nrole of domain dependence and what it means to be \"wide coverage\" in the final\nversion of the paper, if accepted.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n* Potentially valuable resource\n* Paper makes some good points\n\n- Weaknesses:\n* Awareness of related work (see below)\n* Is what the authors are trying to do (domain-independent microplanning) even\npossible (see below)\n* Are the crowdsourced texts appropriate (see below)\n\n- General Discussion:\nThis is an interesting paper which presents a potentially valuable resource,\nand I in many ways I am sympathetic to it.  However, I have some high-level\nconcerns, which are not addressed in the paper.  Perhaps the authors can\naddress these in their response.\n\n(1) I was a bit surprised by the constant reference and comparison to Wen 2016,\nwhich is a fairly obscure paper I have not previously heard of.  It would be\nbetter if the authors justified their work by comparison to well-known corpora,\nsuch as the ones they list in Section 2. Also, there are many other NLG\nprojects that looked at microplanning issue when verbalising DBPedia, indeed\nthere was a workshop in 2016 with many papers on NLG and DBPedia\n(https://webnlg2016.sciencesconf.org/  and\nhttp://aclweb.org/anthology/W/W16/#3500); see also previous work by Duboue and\nKutlak.  I would like to see less of a fixation on Wen (2016), and more\nawareness of other work on NLG and DBPedia.\n\n(2) Microplanning tends to be very domain/genre dependent.  For example,\npronouns are used much more often in novels than in aircraft maintenance\nmanuals.   This is why so much work has focused on domain-dependent resources. \n  So there are some real questions about whether it is possible even in theory\nto train a \"wide-coverage microplanner\".  The authors do not discuss this at\nall; they need to show they are aware of this concern.\n\n(3) I would be concerned about the quality of the texts obtained from\ncrowdsourcing.              A lot of people dont write very well, so it is not at all\nclear\nto me that gathering example texts from random crowdsourcers is going to\nproduce a good corpus for training microplanners.  Remember that the ultimate\ngoal of microplanning is to produce texts that are easy to *read*.  Imitating\nhuman writers (which is what this paper does, along with most learning\napproaches to microplanning) makes sense if we are confident that the human\nwriters have produced well-written easy-to-read texts.              Which is a\nreasonable\nassumption if the writers are professional journalists (for example), but a\nvery dubious one if the writers are random crowdsourcers.\n\nFrom a presentational perspective, the authors should ensure that all text in\ntheir paper meets the ACL font size criteria.  Some of the text in Fig 1 and\n(especially) Fig 2 is tiny and very difficult to read; this text should be the\nsame font size as the text in the body of the paper.\n\nI will initially rate this paper as borderline.  I look forward to seeing the\nauthor's response, and will adjust my rating accordingly.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers.  We apply our framework to DBpedia data and compare the resulting dataset with Wen et al. 2016's. We show that while Wen et al.'s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the WebNLG shared task.", "histories": [], "id": "382", "title": "Creating Training Corpora for NLG Micro-Planners"}
