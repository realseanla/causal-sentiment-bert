{"reviews": [{"IMPACT": "5", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "- Overview:\n\nThe paper proposes a new model for training sense embeddings grounded in a\nlexical-semantic resource (in this case WordNet). There is no direct evaluation\nthat the learned sense vectors are meaningful; instead, the sense vectors are\ncombined back into word embeddings, which are evaluated in a downstream task:\nPP attachment prediction.\n\n- Strengths:\n\nPP attachment results seem solid.\n\n- Weaknesses:\n\nWhether the sense embeddings are meaningful remains uninvestigated. \n\nThe probabilistic model has some details that are hard to understand. Are the\n\\lambda_w_i hyperparameters or trained? Where does \u201crank\u201d come from, is\nthis taken from the sense ranks in WordNet?\n\nRelated work: the idea of expressing embeddings of words as a convex\ncombination of sense embeddings has been proposed a number of times previously.\nFor instance, Johansson and Nieto Pi\u00f1a \u201cEmbedding a semantic network in a\nword space\u201d (NAACL, 2015) decomposed word embeddings into ontology-grounded\nsense embeddings based on this idea. Also in unsupervised sense vector training\nthis idea has been used, for instance by Arora et al \u201cLinear Algebraic\nStructure of Word Senses, with Applications to Polysemy\u201d.\n\nMinor comments:\n\nno need to define types and tokens, this is standard terminology\n\nwhy is the first \\lamba_w_i in equation 4 needed if the probability is\nunnormalized?\n\n- General Discussion:", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "5"}], "abstract": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.", "histories": [], "id": 691, "title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment"}
