{"reviews": [{"IMPACT": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "This paper describes a straightforward extension to left-to-right beam search\nin order to allow it to incorporate lexical constraints in the form of word\nsequences that must appear in MT output. This algorithm is shown to be\neffective for interactive translation and domain adaptation.\n\nAlthough the proposed extension is very simple, I think the paper makes a\nuseful contribution by formalizing it. It is also interesting to know that NMT\ncopes well with a set of unordered constraints having no associated alignment\ninformation. There seem to be potential applications for this technique beyond\nthe ones investigated here, for example improving NMT\u2019s ability to handle\nnon-compositional constructions, which is one of the few areas where it still\nmight lag traditional SMT.\n\nThe main weakness of the paper is that the experiments are somewhat limited.\nThe interactive MT simulation shows that the method basically works, but it is\ndifficult to get a sense of how well - for instance, in how many cases the\nconstraint was incorporated in an acceptable manner (the large BLEU score\nincreases are only indirect evidence). Similarly, adaptation should have been \ncompared to the standard \u201cfine-tuning\u201d baseline, which would be relatively\ninexpensive to run on the 100K Autodesk corpus.\n\nDespite this weakness, I think this is a decent contribution that deserves to\nbe published.\n\nFurther details:\n\n422 Given its common usage in PBMT, \u201ccoverage vector\u201d is a potentially\nmisleading term. The appropriate data structure seems more likely to be a\ncoverage set.\n\nTable 2 should also give some indication of the number of constraints per\nsource sentence in the test corpora, to allow for calibration of the BLEU\ngains.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model's output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.", "histories": [], "id": "564", "title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search"}
