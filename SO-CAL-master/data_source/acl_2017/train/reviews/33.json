{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "Strengths:\n\n- Innovative idea: sentiment through regularization\n- Experiments appear to be done well from a technical point of view\n- Useful in-depth analysis of the model\n\nWeaknesses:\n\n- Very close to distant supervision\n- Mostly poorly informed baselines\n\nGeneral Discussion:\n\nThis paper presents an extension of the vanilla LSTM model that\nincorporates sentiment information through regularization.  The\nintroduction presents the key claims of the paper: Previous CNN\napproaches are bad when no phrase-level supervision is present.\nPhrase-level annotation is expensive. The contribution of this paper is\ninstead a \"simple model\" using other linguistic resources.\n\nThe related work section provides a good review of sentiment\nliterature. However, there is no mention of previous attempts at\nlinguistic regularization (e.g., [YOG14]).\n\nThe explanation of the regularizers in section 4 is rather lengthy and\nrepetitive. The listing on p. 3 could very well be merged with the\nrespective subsection 4.1-4.4. Notation in this section is inconsistent\nand generally hard to follow. Most notably, p is sometimes used with a\nsubscript and sometimes with a superscript.  The parameter \\beta is\nnever explicitly mentioned in the text. It is not entirely clear to me\nwhat constitutes a \"position\" t in the terminology of the paper. t is a\nparameter to the LSTM output, so it seems to be the index of a\nsentence. Thus, t-1 is the preceding sentence, and p_t is the prediction\nfor this sentence. However, the description of the regularizers talks\nabout preceding words, not sentences, but still uses. My assumption here\nis that p_t is actually overloaded and may either mean the sentiment of\na sentence or a word. However, this should be made clearer in the text.\n\nOne dangerous issue in this paper is that the authors tread a fine line\nbetween regularization and distant supervision in their work. The\nproblem here is that there are many other ways to integrate lexical\ninformation from about polarity, negation information, etc. into a model\n(e.g., by putting the information into the features). The authors\ncompare against a re-run or re-implementation of Teng et al.'s NSCL\nmodel. Here, it would be important to know whether the authors used the\nsame lexicons as in their own work. If this is not the case, the\ncomparison is not fair. Also, I do not understand why the authors cannot\nrun NSCL on the MR dataset when they have access to an implementation of\nthe model. Would this not just be a matter of swapping the datasets? The\nremaining baselines do not appear to be using lexical information, which\nmakes them rather poor. I would very much like to see a vanilla LSTM run\nwhere lexical information is simply appended to the word vectors.\n\nThe authors end the paper with some helpful analysis of the\nmodels. These experiments show that the model indeed learns\nintensification and negation to some extent. In these experiments, it\nwould be interesting to know how the model behaves with\nout-of-vocabulary words (with respect to the lexicons). Does the model\nlearn beyond memorization, and does generalization happen for words that\nthe model has not seen in training? Minor remark here: the figures and\ntables are too small to be read in print.\n\nThe paper is mostly well-written apart from the points noted above.  It\ncould benefit from some proofreading as there are some grammatical\nerrors and typos left. In particular, the beginning of the abstract is\nhard to read.\n\nOverall, the paper pursues a reasonable line of research. The largest\npotential issue I see is a somewhat shaky comparison to related\nwork. This could be fixed by including some stronger baselines in the\nfinal model. For me, it would be crucial to establish whether\ncomparability is given in the experiments, and I hope that the authors\ncan shed some light on this in their response.\n\n[YOG14] http://www.aclweb.org/anthology/P14-1074\n\n--------------\n\nUpdate after author response\n\nThank you for clarifying the concerns about the experimental setup. \n\nNSCL: I do now believe that the comparison is with Teng et al. is fair.\n\nLSTM: Good to know that you did this. However, this is a crucial part of the\npaper. As it stands, the baselines are weak. Marginal improvement is still too\nvague, better would be an open comparison including a significance test.\n\nOOV: I understand how the model is defined, but what is the effect on OOV\nwords? This would make for a much more interesting additional experiment than\nthe current regularization experiments.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThis paper proposes a nice way to combine the neural model (LSTM) with\nlinguistic knowledge (sentiment lexicon, negation and intensity). The method is\nsimple yet effective. It achieves the state-of-the-art performance on Movie\nReview dataset and is competitive against the best models on SST dataset.    \n\n- Weaknesses:\nSimilar idea has also been used in (Teng et al., 2016). Though this work is \nmore elegant in the framework design and mathematical representation, the\nexperimental comparison with (Teng et al., 2016) is not as convincing as the\ncomparisons with the rest methods. The authors only reported the\nre-implementation results on the sentence level experiment of SST and did not\nreport their own phrase-level results.\n\nSome details are not well explained, see discussions below.\n\n- General Discussion:\n\nThe reviewer has the following questions/suggestions about this work,\n\n1. Since the SST dataset has phrase-level annotations, it is better to show the\nstatistics of the times that negation or intensity words actually take effect.\nFor example, how many times the word \"nothing\" appears and how many times it\nchanges the polarity of the context.\n\n2. In section 4.5, the bi-LSTM is used for the regularizers. Is bi-LSTM used to\npredict the sentiment label?\n\n3. The authors claimed that \"we only use the sentence-level annotation since\none of\nour goals is to avoid expensive phrase-level annotation\". However, the reviewer\nstill suggest to add the results. Please report them in the rebuttal phase if\npossible.\n\n4. \"s_c is a parameter to be optimized but could also be set fixed with prior\nknowledge.\"  The reviewer didn't find the specific definition of s_c in the\nexperiment section, is it learned or set fixed?  What is the learned or fixed\nvalue?\n\n5. In section 5.4 and 5.5, it is suggested to conduct an additional experiment\nwith part of the SST dataset where only phrases with negation/intensity words\nare included. Report the results on this sub-dataset with and without the\ncorresponding regularizer can be more convincing.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}], "abstract": "This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.", "histories": [], "id": "33", "title": "Linguistically Regularized LSTM for Sentiment Classification"}
