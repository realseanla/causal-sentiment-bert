{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence (seq2seq) model (referred to as the\n\u201cprogrammer\u201d) which encodes a natural language question and produces a\nprogram. The programmer is also equipped with a \u2018key variable\u2019 memory\ncomponent which stores (a) entities in the questions (b) values of intermediate\nvariables formed during execution of intermediate programs. These variables are\nreferred to further build the program.                    The model is also equipped\nwith\ncertain\ndiscrete operations (such as argmax or 'hop to next edges in a KB'). A separate\ncomponent (\"interpreter/computer\") executes these operations and stores\nintermediate values (as explained before). Since the \u2018programmer' is\ninherently a seq2seq model, the \"interpreter/computer\u201d also acts as a\nsyntax/type checker only allowing the decoder to generate valid tokens. For\nexample, the second argument to the \u201chop\u201d operation has to be a KB\npredicate. Finally the model is trained with weak supervision and directly\noptimizes the metric which is used to evaluate the performance (F score).\nBecause of the discrete operations and the non differentiable reward functions,\nthe model is trained with policy gradients (REINFORCE). Since gradients\nobtained through REINFORCE have high variance, it is common to first pretrain\nthe model with a max-likelihood objective or find some good sequences of\nactions trained through some auxiliary objective. This paper takes a latter\napproach in which it finds good sequences via an iterative maximum likelihood\napproach. The results and discussion sections are presented in a very nice way\nand the model achieves SOTA results on the WebQuestions dataset when compared\nto other weakly supervised model.\n\nThe paper is written clearly and is very easy to follow.\n\nThis paper presents a new and exciting direction and there is scope for a lot\nof future research in this direction. I would definitely love to see this\npresented in the conference.\n\nQuestions for the authors (important ones first)\n\n1. Another alternative way of training the model would be to bootstrap the\nparameters (\\theta) from the iterative ML method instead of adding pseudo gold\nprograms in the beam (Line 510 would be deleted). Did you try that and if so\nwhy do you think it didn\u2019t work?\n2. What was the baseline model in REINFORCE. Did you have a separate network\nwhich predicts the value function. This must be discussed in the paper in\ndetail.\n3. Were there programs which required multiple hop operations? Or were they\nlimited to single hops. If there were, can you provide an example? (I will\nunderstand if you are bound by word limit of the response)\n4. Can you give an example where the filter operation would be used?\n5. I did not follow the motivation behind replacing the entities in the\nquestion with special ENT symbol\n\nMinor comments:\nLine 161 describe -> describing\nLine 318 decoder reads \u2018)\u2019 -> decoder generates \u2018)'", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "5", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper introduces Neural Symbolic Machines (NSMs) --- a deep neural model\nequipped with discrete memory to facilitate symbolic execution. An NSM includes\nthree components: (1) a manager that provides weak supervision for learning,\n(2) a differentiable programmer based on neural sequence to sequence model,\nwhich encodes input instructions and predicts simplified Lisp programs using\npartial execution results stored in external discrete memories. (3) a symbolic\ncomputer that executes programs and provide code assistance to the programmer\nto prune search space. The authors conduct experiments on a semantic parsing\ntask (WebQuestionsSP), and show that (1) NSM is able to model language\ncompositionality by saving and reusing intermediate execution results, (2)\nAugmented REINFORCE is superior than vanilla REINFROCE for sequence prediction\nproblems, and (3) NSM trained end-to-end with weak supervision is able to\noutperform existing sate-of-the-art method (STAGG).\n\n- Strengths\n\n* The idea of using discrete, symbolic memories for neural execution models is\nnovel.                    Although in implementation it may simply reduce to copying\npreviously\nexecuted variable tokens from an extra buffer, this approach is still\nimpressive since it works well for a large-scale semantic parsing task.\n\n* The proposed revised REINFORCE training schema using imperfect hypotheses\nderived from maximum likelihood training is interesting and effective, and\ncould inspire future exploration in mixing ML/RL training for neural\nsequence-to-sequence models.\n\n* The scale of experiments is larger than any previous works in modeling neural\nexecution and program induction. The results are impressive.\n\n* The paper is generally clear and well-written, although there are some points\nwhich might require further clarification (e.g., how do the keys ($v_i$'s in\nFig. 2) of variable tokens involved in computing action probabilities?\nConflicting notations: $v$ is used to refer to variables in Tab. 1 and memory\nkeys in Fig 1.).\n\nOverall, I like this paper and would like to see it in the conference.\n\n* Weaknesses\n\n* [Choice of Dataset] The authors use WebQuestionsSP as the testbed. Why not\nusing the most popular WebQuestions (Berant et al., 2013) benchmark set? Since\nNSM only requires weak supervision, using WebQuestions would be more intuitive\nand straightforward, plus it could facilitate direct comparison with\nmain-stream QA research.\n\n* [Analysis of Compositionality] One of the contribution of this work is the\nusage of symbolic intermediate execution results to facilitate modeling\nlanguage compositionality. One interesting question is how well questions with\nvarious compositional depth are handled. Simple one-hop questions are the\neasiest to solve, while complex multi-hop ones that require filtering and\nsuperlative operations (argmax/min) would be highly non-trivial. The authors\nshould present detailed analysis regarding the performance on question sets\nwith different compositional depth.\n\n* [Missing References] I find some relevant papers in this field missing. For\nexample, the authors should cite previous RL-based methods for knowledge-based\nsemantic parsing (e.g., Berant and Liang., 2015), the sequence level REINFORCE\ntraining method of (Ranzato et al., 2016) which is closely related to augmented\nREINFORCE, and the neural enquirer work (Yin et al., 2016) which uses\ncontinuous differentiable memories for modeling neural execution.\n\n* Misc.\n\n* Why is the REINFORCE algorithm randomly initialized (Algo. 1) instead of\nusing parameters pre-trained with iterative ML?\n\n* What is KG server in Figure 5?", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \"computer\", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.", "histories": [], "id": 606, "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision"}
