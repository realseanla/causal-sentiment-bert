{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "2", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\nThe idea of hard monotonic attention is new and substantially different from\nothers.\n\n- Weaknesses:\nThe experiment results on morphological inflection generation is somewhat\nmixed. The proposed model is effective if the amount of training data is small\n(such as CELEX). It is also effective if the alignment is mostly monotonic and\nless context sensitive (such as Russian, German and Spanish).\n\n- General Discussion:\n\nThe authors proposed a novel neural model for morphological inflection\ngeneration which uses \"hard attention\", character alignments separately\nobtained by using a Bayesian method for transliteration. It is substantially\ndifferent from the previous state of the art neural model for the task which\nuses \"soft attention\", where character alignment and conversion are solved\njointly in the probabilistic model.\n\nThe idea is novel and sound. The paper is clearly written. The experiment is\ncomprehensive. The only concern is that the proposed method is not necessarily\nthe state of the art in all conditions. It is suitable for the task with mostly\nmonotonic alignment and with less context sensitive phenomena. The paper would\nbe more convincing if it describe the practical merits of the proposed method,\nsuch as the ease of implementation and computational cost.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "2", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: A new encoder-decoder model is proposed that explicitly takes \ninto account monotonicity.\n\n- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments\nde-coupled.\nOnly evaluated on morphology, no other monotone Seq2Seq tasks.\n\n- General Discussion:\n\nThe authors propose a novel encoder-decoder neural network architecture with\n\"hard monotonic attention\". They evaluate it on three morphology datasets.\n\nThis paper is a tough one. One the one hand it is well-written, mostly very\nclear and also presents a novel idea, namely including monotonicity in\nmorphology tasks. \n\nThe reason for including such monotonicity is pretty obvious: Unlike machine\ntranslation, many seq2seq tasks are monotone, and therefore general\nencoder-decoder models should not be used in the first place. That they still\nperform reasonably well should be considered a strong argument for neural\ntechniques, in general. The idea of this paper is now to explicity enforce a\nmonotonic output character generation. They do this by decoupling alignment and\ntransduction and first aligning input-output sequences monotonically and\nthen training to generate outputs in agreement with the monotone alignments.\nHowever, the authors are unclear on this point. I have a few questions:\n\n1) How do your alignments look like? On the one hand, the alignments seem to\nbe of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input\ncharacter can be aligned with zero, 1, or several output characters. However,\nthis seems to contrast with the description given in lines 311-312 where the\nauthors speak of several input characters aligned to 1 output character. That\nis, do you use 1-to-many, many-to-1 or many-to-many alignments?\n\n2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first\nstage, align input and output characters monotonically with a 1-to-many\nconstraint (one can use any monotone aligner, such as the toolkit of\nJiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to\npredict exactly these 1-to-many alignments. For example, flog->fliege (your\nexample on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger\n(could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length 4)\nfrom \"f-l-o-g\" (sequence of length 4). Such an approach may have been suggested\nin multiple papers, one reference could be [*, Section 4.2] below. \nMy two questions here are: \n\n2a) How does your approach differ from this rather simple idea?\n\n2b) Why did you not include it as a baseline?\n\nFurther issues:\n\n3) It's really a pitty that you only tested on morphology, because there are\nmany other interesting monotonic seq2seq tasks, and you could have shown your\nsystem's superiority by evaluating on these, given that you explicitly model\nmonotonicity (cf. also [*]).\n\n4) You perform \"on par or better\" (l.791). There seems to be a general\ncognitive bias among NLP researchers to map instances where they perform worse\nto\n\"on par\" and all the rest to \"better\". I think this wording should be\ncorrected, but otherwise I'm fine with the experimental results.\n\n5) You say little about your linguistic features: From Fig. 1, I infer that\nthey include POS, etc. \n\n5a) Where did you take these features from?\n\n5b) Is it possible that these are responsible for your better performance in\nsome cases, rather than the monotonicity constraints?\n\nMinor points:\n\n6) Equation (3): please re-write $NN$ as $\\text{NN}$ or similar\n\n7) l.231 \"Where\" should be lower case\n\n8) l.237 and many more: $x_1\\ldots x_n$. As far as I know, the math community\nrecommends to write $x_1,\\ldots,x_n$ but $x_1\\cdots x_n$. That is, dots should\nbe on the same level as surrounding symbols.\n\n9) Figure 1: is it really necessary to use cyrillic font? I can't even address\nyour example here, because I don't have your fonts.\n\n10) l.437: should be \"these\"\n\n[*] \n\n@InProceedings{schnober-EtAl:2016:COLING, \n\n  author    = {Schnober, Carsten  and  Eger, Steffen  and  Do Dinh,\nErik-L\\^{a}n  and  Gurevych, Iryna},\n  title     = {Still not there? Comparing Traditional Sequence-to-Sequence\nModels to Encoder-Decoder Neural Networks on Monotone String Translation\nTasks},\n  booktitle = {Proceedings of COLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers},\n  month     = {December},\n  year                                                      = {2016},\n  address   = {Osaka, Japan},\n  publisher = {The COLING 2016 Organizing Committee},\n  pages     = {1703--1714},\n  url                                               =\n{http://aclweb.org/anthology/C16-1160}\n\n}\n\nAFTER AUTHOR RESPONSE\n\nThanks for the clarifications. I think your alignments got mixed up in the\nresponse somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1,\n1-1, and later make many-to-many alignments from these. \nI know that you compare to  Nicolai, Cherry and Kondrak (2015) but my question\nwould have rather been: why not use 1-x (x in 0,1,2) alignments as in  Schnober\net al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much\nyour results would have differed from such a rather simple baseline. (A tagger\nis a monotone model to start with and given the monotone alignments, everything\nstays monotone. In contrast, you start out with a more general model and then\nput hard monotonicity constraints on this ...)\n\nNOTES FROM AC\n\nAlso quite relevant is Cohn et al. (2016),\nhttp://www.aclweb.org/anthology/N16-1102 .\n\nIsn't your architecture also related to methods like the Stack LSTM, which\nsimilarly predicts a sequence of actions that modify or annotate an input?  \n\nDo you think you lose anything by using a greedy alignment, in contrast to\nRastogi et al. (2016), which also has hard monotonic attention but sums over\nall alignments?", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}], "abstract": "We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft (Bahdanau, 2014) attention models for the task, shedding some light on the features such models extract.", "histories": [], "id": 105, "title": "Morphological Inflection Generation with Hard Monotonic Attention"}
