{"reviews": [{"IMPACT": "4", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThe paper proposes an end-to-end neural model for semantic graph parsing,\nbased on a well-designed transition system. \nThe work is interesting, learning\nsemantic representations of DMRS, which is capable of resolving semantics\nsuch as scope underspecification. This work shows a new scheme for\ncomputational semantics, benefiting from an end-to-end transition-based\nincremental framework, which resolves the parsing with low cost.\n\n- Weaknesses:\n  My major concern is that the paper only gives a very common introduction for\nthe\ndefinition of DMRS and EP, and the example even makes me a little confused\nbecause I cannot see anything special for DMRS. The description can be a little\nmore detailed, I think. However, upon the space limitation, it is\nunderstandable. The same problem exists for the transition system of the\nparsing model. If I do not have any background of MRS and EP, I can hardly\nlearn something from the paper, just seeing that this paper is very good.\n\n- General Discussion:\n  Overall, this paper is very interesting to me. I like the DMRS for semantic\nparsing very much and like the paper very much. Hope that the open-source codes\nand datasets can make this line of research being a hot topic.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "5", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.", "histories": [], "id": 578, "title": "Robust Incremental Neural Semantic Graph Parsing"}
