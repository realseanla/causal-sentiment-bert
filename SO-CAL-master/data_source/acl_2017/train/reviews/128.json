{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "2", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks (in\nparticular, slot-filling of the natural language understanding unit in\nconversation systems). Substructures (e.g. a node in the parse tree) is encoded\nas a vector (a memory slot) and a weighted sum of the substructure embeddings\nare fed in a RNN at each time step as additional context for labeling.\n\n-----Strengths-----\n\nI think the main contribution of this paper is a simple way to \"flatten\"\nstructured information to an array of vectors (the memory), which is then\nconnected to the tagger as additional knowledge. The idea is similar to\nstructured / syntax-based attention (i.e. attention over nodes from treeLSTM);\nrelated work includes Zhao et al on textual entailment, Liu et al. on natural\nlanguage inference, and Eriguchi et al. for machine translation. The proposed\nsubstructure encoder is similar to DCNN (Ma et al.): each node is embedded from\na sequence of ancestor words. The architecture does not look entirely novel,\nbut I kind of like the simple and practical approach compared to prior work.\n\n-----Weaknesses-----\n\nI'm not very convinced by the empirical results, mostly due to the lack of\ndetails of the baselines. Comments below are ranked by decreasing importance.\n\n-  The proposed model has two main parts: sentence embedding and substructure\nembedding. In Table 1, the baseline models are TreeRNN and DCNN, they are\noriginally used for sentence embedding but one can easily take the\nnode/substructure embedding from them too. It's not clear how they are used to\ncompute the two parts.\n\n- The model uses two RNNs: a chain-based one and a knowledge guided one. The\nonly difference in the knowledge-guided RNN is the addition of a \"knowledge\"\nvector from the memory in the RNN input (Eqn 5 and 8). It seems completely\nunnecessary to me to have separate weights for the two RNNs. The only advantage\nof using two is an increase of model capacity, i.e. more parameters.\nFurthermore, what are the hyper-parameters / size of the baseline neural\nnetworks? They should have comparable numbers of parameters.\n\n- I also think it is reasonable to include a baseline that just input\nadditional knowledge as features to the RNN, e.g. the head of each word, NER\nresults etc.\n\n- Any comments / results on the model's sensitivity to parser errors?\n\nComments on the model:\n\n- After computing the substructure embeddings, it seems very natural to compute\nan attention over them at each word. Is there any reason to use a static\nattention for all words? I guess as it is, the \"knowledge\" is acting more like\na filter to mark important words. Then it is reasonable to include the baseline\nsuggest above, i.e. input additional features.\n\n- Since the weight on a word is computed by inner product of the sentence\nembedding and the substructure embedding, and the two embeddings are computed\nby the same RNN/CNN, doesn't it means nodes / phrases similar to the whole\nsentence gets higher weights, i.e. all leaf nodes?\n\n- The paper claims the model generalizes to different knowledge but I think the\nsubstructure has to be represented as a sequence of words, e.g. it doesn't seem\nstraightforward for me to use constituent parse as knowledge here.\n\nFinally, I'm hesitating to call it \"knowledge\". This is misleading as usually\nit is used to refer to world / external knowledge such as a knowledge base of\nentities, whereas here it is really just syntax, or arguably semantics if AMR\nparsing is used.\n\n-----General Discussion-----\n\nThis paper proposes a practical model which seems working well on one dataset,\nbut the main ideas are not very novel (see comments in Strengths). I think as\nan ACL paper there should be more takeaways. More importantly, the experiments\nare not convincing as it is presented now. Will need some clarification to\nbetter judge the results.\n\n-----Post-rebuttal-----\n\nThe authors did not address my main concern, which is whether the baselines\n(e.g. TreeRNN) are used to compute substructure embeddings independent of the\nsentence embedding and the joint tagger. Another major concern is the use of\ntwo separate RNNs which gives the proposed model more parameters than the\nbaselines. Therefore I'm not changing my scores.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Natural language understanding (NLU) is a core component of a dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time.  Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved.  The experiments on the benchmark ATIS data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the state-of-the-art neural network based frameworks.", "histories": [], "id": 128, "title": "Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks"}
