{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "26: An End-to-End Model for Question Answering over Knowledge Base with\nCross-Attention Combining Global Knowledge\n\nThis paper presents an approach for factoid question answering over a knowledge\ngraph (Freebase), by using a neural model that attempts to learn a semantic\ncorrelation/correspondence between various \"aspects\" of the candidate answer\n(e.g., answer type, relation to question entity, answer semantic, etc.) and a\nsubset of words of the question. A separate correspondence component is learned\nfor each \"aspect\" of the candidate answers. The two key contributions of this\nwork are: (1) the creation of separate components to capture different aspects\nof the candidate answer, rather than relying on a single semantic\nrepresentation, and (2) incorporating global context (from the KB) of the\ncandidate answers.\n\nThe most interesting aspect of this work, in my opinion, is the separation of\ncandidate answer representation into distinct aspects, which gives us (the\nneural model developer) a little more control over guiding the NN models\ntowards information that would be more beneficial in its decision making. It\nsort of harkens to the more traditional algorithms that rely on feature\nengineering. But in this case the \"feature engineering\" (i.e., aspects) is more\nsubtle, and less onerous. I encourage the authors to continue refining this\nsystem along these lines.\n\nWhile the high-level idea is fairly clear to a reasonably informed reader, the\ndevil in the details would make it hard for some audience to immediately grasp\nkey insights from this work. Some parts of the paper could benefit from more\nexplanation... Specifically:\n\n(1) Context aspect of candidate answers (e_c) is not clearly explained in the\npaper. Therefore, the last two sentences of Section 3.2.2 seem unclear.\n\n(2) Mention of OOV in the abstract and introduction need more explanation. As\nsuch, I think the current exposition in the paper assumes a deep understanding\nof prior work by the reader.\n\n(3) The experiments conducted in this paper restrict comparison to IR-based\nsystem -- and the reasoning behind this decision is reasonable. But it is not\nclear then why the work of Yang et al. (2014) -- which is described to be\nSP-based -- is part of the comparison. While, I am all for including more\nsystems in the comparison, there seem to be some inconsistencies in what should\nand should not be compared. Additionally, I see not harm in also mentioning the\ncomparable performance numbers for the best SP-based systems.\n\nI observe in the paper that the embeddings are learned entirely from the\ntraining data. I wonder how much impact the random initialization of these\nembeddings has on the end performance. It would be interesting to determine\n(and list) the variance if any. Additionally, if we were to start with\npre-trained embeddings (e.g., from word2vec) instead of the randomly\ninitialized ones, would that have any impact?\n\nAs I read the paper, one possible direction of future work that occurred to me\nwas to possibly include structured queries (from SP-based methods) as part of\nthe cross-attention mechanism. In other words, in addition to using the various\naspects of the candidate answers as features, one could include structured\nqueries that generate the produce that candidate answer as an additional aspect\nof the candidate answer. An attention mechanism could then also focus on\nvarious parts of the structured query, and its (semantic) matches to the input\nquestion as an additional signal for the NN model. Just a thought.\n\nSome notes regarding the positioning of the paper:\n\nI hesitate to call the model proposed here \"attention\" models, because (per my\nadmittedly limited understanding) attention mechanisms apply to\n\"encoder-decoder\" situations, where semantics expressed in one structured form\n(e.g., image, sentence in one language, natural language question, etc.) are\nencoded into an abstract representation, and then generated into another\nstructured form (e.g., caption, sentence in another language, structured query,\netc.). The attention mechanism allows the \"encoder\" to jump around and attend\nto different parts of the input (instead of sequentially) as the output is\nbeing generated by the decoder. This paper does not appear to fit this notion,\nand may be confusing to a broader audience.\n\n------\n\nThank you for clarifications in the author response.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThis paper contributes to the field of knowledge base-based question answering\n(KB-QA), which is to tackle the problem of retrieving results from a structured\nKB based on a natural language question. KB-QA is an important and challenging\ntask.\n\nThe authors clearly identify the contributions and the novelty of their work,\nprovide a good overview of the previous work and performance comparison of\ntheir approach to the related methods.\n\nPrevious approaches to NN-based KB-QA represent questions and answers as fixed\nlength vectors, merely as a bag of words, which limits the expressiveness of\nthe models. And previous work also don\u2019t leverage unsupervised training over\nKG, which potentially can help a trained model to generalize.\nThis paper makes two major innovative points on the Question Answering problem.\n\n1) The backbone of the architecture of the proposed approach is a\ncross-attention based neural network, where attention is used for capture\ndifferent parts of questions and answer aspects. The cross-attention model\ncontains two parts, benefiting each other. The A-Q attention part tries to\ndynamically capture different aspects of the question, thus leading to\ndifferent embedding representations of the question. And the Q-A attention part\nalso offer different attention weight of the question towards the answer\naspects when computing their Q-A similarity score. \n2) Answer embeddings are not only learnt on the QA task but also modeled using\nTransE which allows to integrate more prior knowledge on the KB side. \nExperimental results are obtained on Web questions and the proposed approach\nexhibits better behavior than state-of-the-art end-to-end methods. The two\ncontributions were made particularly clear by ablation experiment. Both the\ncross-attention mechanism and global information improve QA performance by\nlarge margins.\n\nThe paper contains a lot of contents. The proposed framework is quite\nimpressive and novel compared with the previous works.\n\n- Weaknesses:\nThe paper is well-structured, the language is clear and correct. Some minor\ntypos are provided below.\n1. Page 5, column 1, line 421:                                       re-read               \n   \n\uf0e0 \nreread\n2. Page 5, column 2, line 454: pairs be  \uf0e0  pairs to be\n\n- General Discussion:\nIn Equation 2: the four aspects of candidate answer aspects share the same W\nand b. How about using separate W and b for each aspect? \nI would suggest considering giving a name to your approach instead of \"our\napproach\", something like ANN or CA-LSTM\u2026(yet something different from Table\n2).  \n\nIn general, I think it is a good idea to capture the different aspects for\nquestion answer similarity, and cross-attention based NN model is a novel\nsolution for the above task. The experimental results also demonstrate the\neffectiveness of the authors\u2019 approach. Although the overall performance is\nweaker than SP-based methods or some other integrated systems, I think this\npaper is a good attempt in end-to-end KB-QA area and should be encouraged.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the  promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.", "histories": [], "id": "26", "title": "An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge"}
