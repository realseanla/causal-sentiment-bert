{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper outlines a method to learn sense embeddings from unannotated corpora\nusing a modular sense selection and representation process. The learning is\nachieved by a message passing scheme between the two modules that is cast as a\nreinforcement learning problem by the authors.\n\n- Strengths:\n\nThe paper is generally well written, presents most of its ideas clearly and\nmakes apt comparisons to related work where required. The experiments are well\nstructured and the results are overall good, though not outstanding. However,\nthere are several problems with the paper that prevent me from endorsing it\ncompletely.\n\n- Weaknesses:\n\nMy main concern with the paper is the magnification of its central claims,\nbeyond their actual worth.\n\n1) The authors use the term \"deep\" in their title and then several times in the\npaper. But they use a skip-gram architecture (which is not deep). This is\nmisrepresentation.\n\n2) Also reinforcement learning is one of the central claims of this paper.\nHowever, to the best of my understanding, the motivation and implementation\nlacks clarity. Section 3.2 tries to cast the task as a reinforcement learning\nproblem but goes on to say that there are 2 major drawbacks, due to which a\nQ-learning algorithm is used. This algorithm does not relate to the originally\nclaimed policy.\n\nFurthermore, it remains unclear how novel their modular approach is. Their work\nseems to be very similar to EM learning approaches, where an optimal sense is\nselected in the E step and an objective is optimized in the M step to yield\nbetter sense representations. The authors do not properly distinguish their\napproach, nor motivative why RL should be preferred over EM in the first place.\n\n3) The authors make use of the term pure-sense representations multiple times,\nand claim this as a central contribution of their paper. I am not sure what\nthis means, or why it is beneficial.\n\n4) They claim linear-time sense selection in their model. Again, it is not\nclear to me how this is the case. A highlighting of this fact in the relevant\npart of the paper would be helpful. \n\n5) Finally, the authors claim state-of-the-art results. However, this is only\non a single MaxSimC metric. Other work has achieved overall better results\nusing the AvgSimC metric. So, while state-of-the-art isn't everything about a\npaper, the claim that this paper achieves it - in the abstract and intro - is\nat least a little misleading.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper describes a novel approach for learning multi-sense word\nrepresentations using reinforcement learning. A CBOW-like architecture is used\nfor sense selection, computing a score for each sense based on the dot product\nbetween the sum of word embeddings in the current context and the corresponding\nsense vector. A second module based on the skip-gram model is used to train\nsense representations, given results from the sense selection module. In order\nto train these two modules, the authors apply Q-Learning, where the Q-value is\nprovided by the CBOW-based sense selection module. The reward is given by the\nskip-gram negative sampling likelihood. Additionally, the authors propose an\napproach for determining the number of senses for each word non-parametrically,\nby creating new senses when the Q-values for existing scores have a score under\n0.5.\n\nThe resulting approach achieves good results under the \"MaxSimC\" metric, and\nresults comparable to previous approaches under \"AvgSimC\". The authors suggest\nthat their approach could be used to improve the performance for downstream\ntasks by replacing word embeddings with their most probable sense embedding. It\nwould have been nice to see this claim explored, perhaps in a sequential\nlabeling task such as POS-tagging or NER, especially in light of previous work\nquestioning the usefulness of multi-sense representations in downstream tasks.\nI found it somewhat misleading to suggest that relying on MaxSimC could reduce\noverhead in a real world application, as the sense disambiguation step (with\nassociated parameters) would still be required, in addition to the sense\nembeddings. A clustering-based approach using a weighted average of sense\nrepresentations would have similar overhead. The claims about improving over\nword2vec using 1/100 of the data are also not particularly surprising on SCWS.\nThese are misleading contributions, as they do not advance/differ much from\nprevious work.\n\nThe modular quality of their approach results in a flexibility that I think\ncould have been explored further. The sense disambiguation module uses a vector\naveraging (CBOW) approach. A positive aspect of their model is that they should\nbe able to substitute other context composition approaches (using alternative\nneural architecture composition techniques) relatively easily.\n\nThe paper applies an interesting approach to a problem that has been explored\nnow in many ways. The results on standard benchmarks are comparable to previous\nwork, but not particularly surprising/interesting. However, the approach goes\nbeyond a simple extension of the skip-gram model for multi-sense representation\nlearning by providing a modular framework based on reinforcement learning.\nIdeally, this aspect would be explored further. But overall, the approach\nitself may be interesting enough on its own to be considered for acceptance, as\nit could help move research in this area forward.\n\n* There are a number of typos that should be addressed (line\n190--representations*, 331--selects*, 492--3/4th*).\n\nNOTE: Thank you to the authors for their response.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "TMP\nStrength: The paper propose DRL-Sense model that shows a marginal improvement\non SCWS dataset and a significant improvement on ESL-50 and RD-300 datasets.\n\nWeakness:\nThe technical aspects of the paper raise several concerns:\nCould the authors clarify two drawbacks in 3.2? The first drawback states that\noptimizing equation (2) leads to the underestimation of the probability of\nsense. As I understand, eq(2) is the expected reward of sense selection, z_{ik}\nand z_{jl} are independent actions and there are only two actions to optimize.\nThis should be relatively easy. In NLP setting, optimizing the expected rewards\nover a sequence of actions for episodic-task has been proven doable (Sequence\nLevel Training with Recurrent Neural Networks, Ranzato 2015) even in a more\nchallenging setting of machine translation where the number of actions ~30,000\nand the average sequence length ~30 words. The DRL-Sense model has maximum 3\nactions and it does not have sequential nature of RL. This makes it hard to\naccept the claim about the first drawback.\n\nThe second drawback, accompanied with the detail math in Appendix A, states\nthat the update formula is to minimize the likelihood due to the log-likelihood\nis negative. Note that most out-of-box optimizers (Adam, SGD, Adadelta, \u2026)\nminimize a function f, however, a common practice when we want to maximize f we\njust minimize -f. Since the reward defined in the paper is negative, any\nstandard optimizer can be use on the expected of the negative reward, which is\nalways greater than 0. This is often done in many modeling tasks such as\nlanguage model, we minimize negative log-likelihood instead of maximizing the\nlikelihood. The authors also claim that when \u201cthe log-likelihood reaches 0,\nit also indicates that the likelihood reaches infinity and computational flow\non U and V\u201d (line 1046-1049). Why likelihood\u2192infinity? Should it be\nlikelihood\u21921?\n\nCould the authors also explain how DRL -Sense is based on Q-learning? The\nhorizon in the model is length of 1. There is no transition between\nstate-actions and there is not Markov-property as I see it (k, and l are draw\nindependently). I am having trouble to see the relation between Q-learning and\nDRL-Sense.  In (Mnih et al., 2013), the reward is given from the environment\nwhereas in the paper, the rewards is computed by the model. What\u2019s the reward\nin DRL-Sense? Is it 0, for all the (state, action) pairs or the cross-entropy\nin eq(4)?  \n\nCross entropy is defined as H(p, q) = -\\sum_{x} q(x)\\log q(x), which variable\ndo the authors sum over in (4)? I see that q(C_t, z_{i, k}) is a scalar\n(computed in eq(3)), while Co(z_{ik}, z_{jl}) is a distribution over total\nnumber of senses eq(1). These two categorial variables do not have the same\ndimension, how is cross-entropy H in eq(4) is computed then?\n\nCould the authors justify the dropout exploration? Why not epsilon-greedy\nexploration? Dropout is often used for model regularization, preventing\noverfitting. How do the authors know the gain in using dropout is because of\nexploration but regularization?\n\nThe authors states that Q-value is a probabilistic estimation (line 419), can\nyou elaborate what is the set of variables the distribution is defined? When\nyou sum over that set of variable, do you get 1? I interpret that Q is a\ndistribution over senses per word, however  definition of q in eq(3) does not\ncontain a normalizing constant, so I do not see q is a valid distribution. This\nalso related to the value 0.5 in section 3.4 as a threshold for exploration.\nWhy 0.5 is chosen here where q is just an arbitrary number between (0, 1) and\nthe constrain \\sum_z q(z) = 1 does not held? Does the authors allow the\ncreation of a new sense in the very beginning or after a few training epochs? I\nwould image that at the beginning of training, the model is unstable and\ncreating new senses might introduce noises to the model.  Could the authors\ncomment on that?\n\nGeneral discussion\nWhat\u2019s the justification for omitting negative samples in line 517? Negative\nsampling has been use successfully in word2vec due to the nature of the task:\nlearning representation. Negative sampling, however does not work well when the\nmain interest is modeling a distribution p() over senses/words. Noise\ncontrastive estimation is often preferred when it comes to modeling a\ndistribution. The DRL-Sense, uses collocation likelihood to compute the reward,\nI wonder how the approximation presented in the paper affects the learning of\nthe embeddings.\n\nWould the authors consider task-specific evaluation for sense embeddings as\nsuggested in recent research [1,2]\n\n[1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor\nLabutov, David Mimno and Thorsten Joachims.\n\n[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks .\nManaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer\n\n---\nI have read the response.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "This paper proposes DRL-Sense--a multi-sense word representation learning model, to address the word sense ambiguity issue, where a sense selection module and a sense representation module are jointly learned in a reinforcement learning fashion. A novel reward passing procedure is proposed to enable joint training on the selection and representation modules. The modular design implements pure sense-level representation learning with linear time sense selection (decoding). We further develop a non-parametric learning algorithm and a sense exploration mechanism for better flexibility and robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on contextual word similarities and comparable performance with Google's word2vec while using much less training data.", "histories": [], "id": 395, "title": "DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations"}
