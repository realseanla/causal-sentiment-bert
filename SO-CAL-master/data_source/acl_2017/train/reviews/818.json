{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "Thank you for the author response. It addresses some my concerns, though much\nof it are promises (\"we will...\") -- necessarily so, given space constraints,\nbut then, this is precisely the problem: I would like to see the revision to\nthe paper to be able to check that the drawbacks have been fixed. The changes\nneeded are quite substantial, and the new experimental results that they\npromise to include will not have undergone review if the paper is accepted at\nthis stage. I'm still not sure that we can simply leave it to the authors to\nmake the necessary changes without a further reviewing round. I upgrade my\nscore to a 3 to express this ambivalence (I do like the research in the paper,\nbut it's extremely messy in its presentation).\n\n--------------\n\n- Strengths:\n\nThe topic of the paper is very creative and the purpose of the research really\nworthwhile: the paper aims at extracting common knowledge from text, overcoming\nthe well-known problem of reporting bias (the fact that people will not state\nthe obvious, such as the fact that a person is usually bigger than a ball), by\ndoing joint inference on information that is possible to extract from text.\n\n- Weaknesses:\n\n1) Many aspects of the approach need to be clarified (see detailed comments\nbelow). What worries me the most is that I did not understand how the approach\nmakes knowledge about objects interact with knowledge about verbs such that it\nallows us to overcome reporting bias. The paper gets very quickly into highly\ntechnical details, without clearly explaining the overall approach and why it\nis a good idea.\n\n2) The experiments and the discussion need to be finished. In particular, there\nis no discussion of the results of one of the two tasks tackled (lower half of\nTable 2), and there is one obvious experiment missing: Variant B of the\nauthors' model gives much better results on the first task than Variant A, but\nfor the second task only Variant A is tested -- and indeed it doesn't improve\nover the baseline. \n\n- General Discussion:\n\nThe paper needs quite a bit of work before it is ready for publication. \n\n- Detailed comments:\n\n026 five dimensions, not six\n\nFigure 1, caption: \"implies physical relations\": how do you know which physical\nrelations it implies?\n\nFigure 1 and 113-114: what you are trying to do, it looks to me, is essentially\nto extract lexical entailments (as defined in formal semantics; see e.g. Dowty\n1991) for verbs. Could you please explicit link to that literature?\n\nDowty, David. \"Thematic proto-roles and argument selection.\" Language (1991):\n547-619.\n\n135 around here you should explain the key insight of your approach: why and\nhow does doing joint inference over these two pieces of information help\novercome reporting bias?\n\n141 \"values\" ==> \"value\"?\n\n143 please also consider work on multimodal distributional semantics, here\nand/or in the related work section. The\nfollowing two papers are particularly related to your goals:\n\nBruni, Elia, et al. \"Distributional semantics in technicolor.\" Proceedings of\nthe 50th Annual Meeting of the Association for Computational Linguistics: Long\nPapers-Volume 1. Association for Computational Linguistics, 2012.\n\nSilberer, Carina, Vittorio Ferrari, and Mirella Lapata. \"Models of Semantic\nRepresentation with Visual Attributes.\" ACL (1). 2013.\n\n146 please clarify that your contribution is the specific task and approach --\ncommonsense knowledge extraction from language is long-standing task.\n\n152 it is not clear what \"grounded\" means at this point\n\nSection 2.1: why these dimensions, and how did you choose them?\n\n177 explain terms \"pre-condition\" and \"post-condition\", and how they are\nrelevant here\n\n197-198 an example of the full distribution for an item (obtained by the model,\nor crowd-sourced, or \"ideal\") would help.\n\nFigure 2. I don't really see the \"x is slower than y\" part: it seems to me like\nthis is related to the distinction, in formal semantics, between stage-level\nvs. individual-level\npredicates: when a person throws a ball, the ball is faster than the person\n(stage-level) but\nit's not true in general that balls are faster than people (individual-level).\nI guess this is related to the\npre-condition vs. post-condition issue. Please spell out the type of\ninformation that you want to extract.\n\n248 \"Above definition\": determiner missing\n\nSection 3\n\n\"Action verbs\": Which 50 classes do you pick, and you do you choose them? Are\nthe verbs that you pick all explicitly tagged as action verbs by Levin? \n\n306ff What are \"action frames\"? How do you pick them?\n\n326 How do you know whether the frame is under- or over-generating?\n\nTable 1: are the partitions made by frame, by verb, or how? That is, do you\nreuse verbs or frames across partitions? Also, proportions are given for 2\ncases (2/3 and 3/3 agreement), whereas counts are only given for one case;\nwhich?\n\n336 \"with... PMI\": something missing (threshold?)\n\n371 did you do this partitions randomly?\n\n376 \"rate *the* general relationship\"\n\n378 \"knowledge dimension we choose\": ? (how do you choose which dimensions you\nwill annotate for each frame?)\n\nSection 4\n\nWhat is a factor graph? Please give enough background on factor graphs for a CL\naudience to be able to follow your approach. What are substrates, and what is\nthe role of factors? How is the factor graph different from a standard graph?\nMore generally, at the beginning of section 4 you should give a higher level\ndescription of how your model works and why it is a good idea.\n\n420 \"both classes of knowledge\": antecedent missing.\n\n421 \"object first type\"\n\n445 so far you have been only talking about object pairs and verbs, and\nsuddenly selectional preference factors pop in. They seem to be a crucial part\nof your model -- introduce earlier? In any case, I didn't understand their\nrole.\n\n461 \"also\"?\n\n471 where do you get verb-level similarities from?\n\nFigure 3: I find the figure totally unintelligible. Maybe if the text was\nclearer it would be interpretable, but maybe you can think whether you can find\na way to convey your model a bit more intuitively. Also, make sure that it is\nreadable in black-and-white, as per ACL submission instructions.\n\n598 define term \"message\" and its role in the factor graph.\n\n621 why do you need a \"soft 1\" instead of a hard 1?\n\n647ff you need to provide more details about the EMB-MAXENT classifier (how did\nyou train it, what was the input data, how was it encoded), and also explain\nwhy it is an appropriate baseline.\n\n654 \"more skimp seed knowledge\": ?\n\n659 here and in 681, problem with table reference (should be Table 2). \n\n664ff I like the thought but I'm not sure the example is the right one: in what\nsense is the entity larger than the revolution? Also, \"larger\" is not the same\nas \"stronger\".\n\n681 as mentioned above, you should discuss the results for the task of\ninferring knowledge on objects, and also include results for model (B)\n(incidentally, it would be better if you used the same terminology for the\nmodel in Tables 1 and 2)\n\n778 \"latent in verbs\": why don't you mention objects here?\n\n781 \"both tasks\": antecedent missing\n\nThe references should be checked for format, e.g. Grice, Sorower et al\nfor capitalization, the verbnet reference for bibliographic details.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "Summary: This paper aims to learn common sense relationships between object\ncategories (e.g comparative size, weight, strength, rigidness, and speed) from\nunstructured text.  The key insight of the paper is to leverage the correlation\nof action verbs to these comparative relations (e.g x throw y => x larger y).\n\nStrengths:\n\n- The paper proposes a novel method to address an important problem of mining\ncommon sense attribute relations from text.\n\nWeaknesses:\n\n- I would have liked to see more examples of objects pairs, action verbs, and\npredicted attribute relations.                          What are some interesting\naction\nverbs\nand\ncorresponding attributes relations?  The paper also lacks analysis/discussion \non what kind of mistakes their model makes.\n\n- The number of object pairs (3656) in the dataset is very small.  How many\ndistinct object categories are there?  How scalable is this approach to larger\nnumber of object pairs?\n\n- It's a bit unclear how the frame similarity factors and attributes similarity\nfactors are selected.\n\nGeneral Discussion/Suggestions:\n\n- The authors should discuss the following work and compare against mining\nattributes/attribute distributions directly and then getting a comparative\nmeasure.  What are the advantages offered by the proposed method compared to a\nmore direct approach?\n\nExtraction and approximation of numerical attributes from the Web\nDmitry Davidov, Ari Rappoport\nACL 2010\n\nMinor typos:\n\n1. In the abstract (line 026), the authors mention 'six' dimensions, but in the\npaper, there is only five.\n\n2. line 248: Above --> The above\n\n3. line 421: object first --> first\n\n4. line 654: more skimp --> a smaller\n\n5. line 729: selctional --> selectional", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper studies an interesting problem of extracting relative physical\nknowledge of actions and objects from unstructured text, by inference over a\nfactor graph that consists of two types of subgraphs---action graph and object\ngraph. The paper stems from the key insight---common knowledge about physical\nworld influences the way people talk, even though it is rarely explicitly\nstated. \n\n- Strengths:\n\nThe paper tries to solve an interesting and challenging problem. The problem is\nhard due to reporting bias, and the key insight/approach in the paper is\ninspiring.\n\nThe model is innovative and clearly described. And the idea of handling text\nsparsity with semantic similarity factors is also appropriate. \n\nThe empirical evidence well supports the effectiveness of the model (compared\nto other baselines). \n\nThe paper is well-written, with informative visualization, except for some\nminor errors like *six dimensions* in abstract but *five* everywhere else. \n\n- Weaknesses:\n\nThe benefits and drawbacks of model components are still somehow\nunder-discussed, and hard to tell with the limited quantitative results in the\npaper. \n\nFor example, is there any inherent discrepancy between *cross-verb frame\nsimilarity*, *within-verb frame similarity* and *action-object compatibility*?\nFrames of *A throw B* and *C thrown by D* share a verb primitive *throw*, so\nshould it infer C>D (by *within-verb*) if A>B is given? \nOn the other side,\nframes of *C thrown by D* and *E kicked by F* share the frame *XXX by*, so if\nF>E is known, is D>C inferred? How does the current model deal with such\ndiscrepancy?\n\nThe paper might be better if it has more qualitative analysis. And more\nevidence also needs to be provided to gauge how difficult the task/dataset is.\n\nFor example, are the incorrectly-classified actions/objects also ambiguous for\nhuman? On what types of actions/objects does the model tend to make mistakes?\nIs the verb with more frame types usually harder than others for the model?\n\nMore interestingly, how are the mistakes made? Are they incorrectly enforced by\nany\nproposed *semantic similarity*?\n\nI think more analysis on the model components and qualitative results may\ninspire more general framework for this task. \n\n- General Discussion:\n\n/* After author response */\n\nAfter reading the response, I tend to keep my current rating and accept this\npaper. The response well addresses my concerns. And I tend to believe that\nnecessary background and experimental analysis can be added given some\nre-organization of the paper and one extra page, as it is not hard. \n\n/* Before author response */\n\nI think this paper is in general solid and interesting. \nI tend to accept it, but it would be better if the questions above can be\nanswered.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., \u201cMy house is bigger than me.\u201d However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, \u201cTyler entered his house\u201d implies that his house is bigger than Tyler.  In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.", "histories": [], "id": "818", "title": "Verb Physics: Relative Physical Knowledge of Actions and Objects"}
