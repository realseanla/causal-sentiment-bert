{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThis paper presents an extension to A* CCG parsing to include dependency\ninformation.  Achieving this while maintaining speed and tractability is a very\nimpressive feature of this approach.  The ability to precompute attachments is\na nice trick.                  I also really appreciated the evaluation of the\neffect of\nthe\nhead-rules on normal-form violations and would love to see more details on the\nremaining cases.\n\n- Weaknesses:\nI'd like to see more analysis of certain dependency structures.  I'm\nparticularly interested in how coordination and relative clauses are handled\nwhen the predicate argument structure of CCG is at odds with the dependency\nstructures normally used by other dependency parsers.\n\n- General Discussion:\nI'm very happy with this work and feel it's a very nice contribution to the\nliterature.  The only thing missing for me is a more in-depth analysis of the\ntypes of constructions which saw the most improvement (English and Japanese)\nand a discussion (mentioned above) reconciling Pred-Arg dependencies with those\nof other parsers.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an efficient A* decoding algorithm.\nInterestingly, the paper slightly outperforms Lee et al. (2016)'s more\nexpressive global parsing model, presumably because this factorization makes\nlearning easier. It's great that they also report results on another language,\nshowing large improvements over existing work on Japanese CCG parsing. One\nsurprising original result is that modeling the first word of a constituent as\nthe head substantially outperforms linguistically motivated head rules. \n\nOverall this is a good paper that makes a nice contribution. I only have a few\nsuggestions:\n- I liked the way that the dependency and supertagging models interact, but it\nwould be good to include baseline results for simpler variations (e.g. not\nconditioning the tag on the head dependency).\n- The paper achieves new state-of-the-art results on Japanese by a large\nmargin. However, there has been a lot less work on this data - would it also be\npossible to train the Lee et al. parser on this data for comparison?\n- Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging\nmodels for CCG and SRL, and may be worth citing.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}], "abstract": "We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing.", "histories": [], "id": 440, "title": "A* CCG Parsing with a Supertag and Dependency Factored Model"}
