{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThe paper demonstrates that seq2seq models can be comparatively effectively\napplied to the tasks of AMR parsing and AMR realization by linearization of an\nengineered pre-processed version of the AMR graph and associated sentence,\ncombined with 'Paired Training' (iterative back-translation of monolingual data\ncombined with fine-tuning). While parsing performance is worse than other\nreported papers (e.g., Pust et al., 2015), those papers used additional\nsemantic information. \n\nOn the task of AMR realization, the paper demonstrates that utilizing\nadditional monolingual data (via back-translation) is effective relative to a\nseq2seq model that does not use such information. (See note below about\ncomparing realization results to previous non-seq2seq work for the realization\ntask.)\n\n- Weaknesses:\n\n At a high-level, the main weakness is that the paper aims for empirical\ncomparisons, but in comparing to other work, multiple aspects/dimensions are\nchanging at the same time (in some cases, not comparable due to access to\ndifferent information), complicating comparisons. \n\nFor example, with the realization results (Table 2), PBMT (Pourdamghani et al.,\n2016) is apparently trained on LDC2014T12, which consists of 13,051 sentences,\ncompared to the model of the paper, which is trained on LDC2015E86, which\nconsists of 19,572 sentences, according to http://amr.isi.edu/download.html.\nThis is used in making the claim of over 5 points improvement over the\nstate-of-the-art (PBMT) in line 28/29, 120/121, and line 595, and is only\nqualified in the caption of Table 2. To make a valid comparison, the approach\nof the paper or PBMT needs to be re-evaluated after using the same training\ndata.\n\n- General Discussion:\n\nIs there any overlap between the sentences in your Gigaword sample and the test\nsentences of LDC2015E86? Apparently LDC2015E86 contains data from the ''proxy\nreport data in LDC's DEFT Narrative Text Source Data R1 corpus (LDC2013E19)''\n(Accessible with LDC account: https://catalog.ldc.upenn.edu/LDC2015E86). It\nseems LDC2013E19 contains data from Gigaword\n(https://catalog.ldc.upenn.edu/LDC2013E19). Apparently AMR corpus LDC2014T12\nalso contained ''data from newswire articles selected from the English Gigaword\nCorpus, Fifth Edition'' (publicly accessible link:\nhttps://catalog.ldc.upenn.edu/docs/LDC2014T12/README.txt). Please check that\nthere is no test set contamination.\n\nLine 244-249: Did these two modifications to the encoder make a significant\ndifference in effectiveness? What was the motivation behind these changes?\n\nPlease make it clear (in an appendix is fine) for replication purposes whether\nthe implementation is based on an existing seq2seq framework.\n\nLine 321: What was the final sequence length used? (Consider adding such\ndetails in an appendix.)\n\nPlease label the columns of Table 1 (presumably dev and test). Also, there is a\nmismatch between Table 1 and the text: ''Table 1 summarizes our development\nresults for different rounds of self-training.'' It appears that only the\nresults of the second round of self-training are shown.\n\nAgain, the columns for Table 1 are not labeled, but should the results for\ncolumn 1 for CAMR instead be 71.2, 63.9, 67.3--the last line of Table 2 in\nhttp://www.aclweb.org/anthology/S16-1181 which is the configuration for\n+VERB+RNE+SRL+WIKI? It looks like the second from last row of Table 2 in CAMR\n(Wang et al., 2016) is currently being used. On this note, how does your\napproach handle the wikification information introduced in LDC2015E86? \n\n7.1.Stochastic is missing a reference to the example.\n\nLine 713-715: This seems like a hypothesis to be tested empirically rather than\na forgone conclusion, as implied here.\n\nGiven an extra page, please add a concluding section.\n\nHow are you performing decoding? Are you using beam search?\n\nAs a follow-up to line 161-163, it doesn't appear that the actual vocabulary\nsize used in the experiments is mentioned. After preprocessing, are there any\nremaining unseen tokens in dev/test? In other words, is the unknown word\nreplacement mechanism (using the attention weights), as described in Section\n3.2, ever used? \n\nFor the realization case study, it would be of interest to see performance on\nphenomena that are known limitations of AMR, such as quantification and tense\n(https://github.com/amrisi/amr-guidelines/blob/master/amr.md).\n\nThe paper would benefit from a brief discussion (perhaps a couple sentences)\nmotivating the use of AMR as opposed to other semantic formalisms, as well as\nwhy the human-annotated AMR information/signal might be useful as opposed to\nlearning a model (e.g., seq2seq itself) directly for a task (e.g., machine\ntranslation).\n\nFor future work (not taken directly into account in the scores given here for\nthe review, since the applicable paper is not yet formally published in the\nEACL proceedings): For parsing, what accounts for the difference from previous\nseq2seq approaches? Namely, between Peng and Xue, 2017 and AMR-only (as in\nTable 1) is the difference in effectiveness being driven by the architecture,\nthe preprocessing, linearization, data, or some combination thereof? Consider\nisolating this difference. (Incidentally, the citation for Peng and Xue, 2017\n[''Addressing the Data Sparsity Issue in Neural AMR Parsing''] should\napparently be Peng et al. 2017\n(http://eacl2017.org/index.php/program/accepted-papers;\nhttps://arxiv.org/pdf/1702.05053.pdf). The authors are flipped in the\nReferences section.\n\nProofreading (not necessarily in the order of occurrence; note that these are\nprovided for reference and did not influence my scoring of the paper):\n\noutperform state of the art->outperform the state of the art\n\nZhou et al. (2016), extend->Zhou et al. (2016) extend\n\n(2016),Puzikov et al.->(2016), Puzikov et al.\n\nPOS-based features, that->POS-based features that\n\nlanguage pairs, by creating->language pairs by creating\n\nusing a back-translation MT system and mix it with the human\ntranslations.->using a back-translation MT system, and mix it with the human\ntranslations.\n\nProbBank-style (Palmer et al., 2005)->PropBank-style (Palmer et al., 2005)\n\nindependent parameters ,->independent parameters,\n\nfor the 9.6% of tokens->for 9.6% of tokens\n\nmaintaining same embedding sizes->maintaining the same embedding sizes\n\nTable 4.Similar->Table 4. Similar\n\nrealizer.The->realizer. The\n\nNotation: Line 215, 216: The sets C and W are defined, but never subsequently\nreferenced. (However, W could/should be used in place of ''NL'' in line 346 if\nthey are referring to the same vocabulary.)", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The authors use self-training to train a seq2seq-based AMR parser using a small\nannotated corpus and large amounts of unlabeled data. They then train a\nsimilar,\nseq2seq-based AMR-to-text generator using the annotated corpus and automatic\nAMRs produced by their parser from the unlabeled data. They use careful\ndelexicalization for named entities in both tasks to avoid data sparsity. This\nis the first sucessful application of seq2seq models to AMR parsing and\ngeneration, and for generation, it most probably improves upon state-of-the\nart.\n\nIn general, I really liked the approach as well as the experiments and the\nfinal performance analysis.\nThe methods used are not revolutionary, but they are cleverly combined to\nachieve practial results.\nThe description of the approach is quite detailed, and I believe that it is\npossible to reproduce the experiments without significant problems.\nThe approach still requires some handcrafting, but I believe that this can be\novercome in the future and that the authors are taking a good direction.\n\n(RESOLVED BY AUTHORS' RESPONSE) However, I have been made aware by another\nreviewer of a data overlap in the\nGigaword and the Semeval 2016 dataset. This is potentially a very serious\nproblem -- if there is a significant overlap in the test set, this would\ninvalidate the results for generation (which are the main achievemnt of the\npaper). Unless the authors made sure that no test set sentences made their way\nto training through Gigaword, I cannot accept their results.\n\n(RESOLVED BY AUTHORS' RESPONSE)  Another question raised by another reviewer,\nwhich I fully agree with, is the \n5.4 point claim when comparing to a system tested on an earlier version of the\nAMR dataset. The paper could probably still claim improvement over state-of-the\nart, but I am not sure I can accept the 5.4 points claim in a direct comparison\nto Pourdamghani et al. -- why haven't the authors also tested their system on\nthe older dataset version (or obtained Pourdamghani et al.'s scores for the\nnewer version)?\n\nOtherwise I just have two minor comments to experiments: \n\n- Statistical significance tests would be advisable (even if the performance\ndifference is very big for generation).\n\n- The linearization order experiment should be repeated with several times with\ndifferent random seeds to overcome the bias of the particular random order\nchosen.\n\nThe form of the paper definitely could be improved.\nThe paper is very dense at some points and proofreading by an independent\nperson (preferably an English native speaker) would be advisable. \nThe model (especially the improvements over Luong et al., 2015) could be\nexplained in more detail; consider adding a figure. The experiment description\nis missing the vocabulary size used.\nMost importantly, I missed a formal conclusion very much -- the paper ends\nabruptly after qualitative results are described, and it doesn't give a final\noverview of the work or future work notes.\n\nMinor factual notes:\n\n- Make it clear that you use the JAMR aligner, not the whole parser (at\n361-364). Also, do you not use the recorded mappings also when testing the\nparser (366-367)?\n\n- Your non-Gigaword model only improves on other seq2seq models by 3.5 F1\npoints, not 5.4 (at 578).\n\n- \"voters\" in Figure 1 should be \"person :ARG0-of vote-01\" in AMR.\n\nMinor writing notes:\n\n- Try rewording and simplifying text near 131-133, 188-190, 280-289, 382-385,\n650-659, 683, 694-695.\n\n- Inter-sentitial punctuation is sometimes confusing and does not correspond to\nmy experience with English syntax. There are lots of excessive as well as\nmissing commas.\n\n- There are a few typos (e.g., 375, 615), some footnotes are missing full\nstops.\n\n- The linearization description is redundant at 429-433 and could just refer to\nSect. 3.3.\n\n- When refering to the algorithm or figures (e.g., near 529, 538, 621-623),\nenclose the references in brackets rather than commas.\n\n- I think it would be nice to provide a reference for AMR itself and for the\nmulti-BLEU script.\n\n- Also mention that you remove AMR variables in Footnote 3.\n\n- Consider renaming Sect. 7 to \"Linearization Evaluation\".\n\n- The order in Tables 1 and 2 seems a bit confusing to me, especially when your\nsystems are not explicitly marked (I would expect your systems at the bottom).\nAlso, Table 1 apparently lists development set scores even though its\ndescription says otherwise.\n\n- The labels in Table 3 are a bit confusing (when you read the table before\nreading the text).\n\n- In Figure 2, it's not entirely visible that you distinguish month names from\nmonth numbers, as you state at 376.\n\n- Bibliography lacks proper capitalization in paper titles, abbreviations and\nproper names should be capitalized (use curly braces to prevent BibTeX from\nlowercasing everything).\n\n- The \"Peng and Xue, 2017\" citation is listed improperly, there are actually\nfour authors.\n\n***\nSummary:\n\nThe paper presents first competitive results for neural AMR parsing and\nprobably new state-of-the-art for AMR generation, using seq2seq models with\nclever\npreprocessing and exploiting large a unlabelled corpus. Even though revisions\nto the text are advisable, I liked the paper and would like to see it at the\nconference. \n\n(RESOLVED BY AUTHORS' RESPONSE) However, I am not sure if the comparison with\nprevious\nstate-of-the-art on generation is entirely sound, and most importantly, whether\nthe good results are not actually caused by data overlap of Gigaword\n(additional training set) with the test set.\n\n***\nComments after the authors' response:\n\nI thank the authors for addressing both of the major problems I had with the\npaper. I am happy with their explanation, and I raised my scores assuming that\nthe authors will reflect our discussion in the final paper.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs.  We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs.  For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8.  We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.", "histories": [], "id": 752, "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation"}
