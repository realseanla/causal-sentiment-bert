{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper introduces a general method for improving NLP tasks using embeddings\nfrom language models. Context independent word representations have been very\nuseful, and this paper proposes a nice extension by using context-dependent\nword representations obtained from the hidden states of neural language models.\nThey show significant improvements in tagging and chunking tasks from including\nembeddings from large language models. There is also interesting analysis which\nanswers several natural questions.\n\nOverall this is a very good paper, but I have several suggestions:\n- Too many experiments are carried out on the test set. Please change Tables 5\nand 6 to use development data\n- It would be really nice to see results on some more tasks - NER tagging and\nchunking don't have many interesting long range dependencies, and the language\nmodel might really help in those cases. I'd love to see results on SRL or CCG\nsupertagging.\n- The paper claims that using a task specific RNN is necessary because a CRF on\ntop of language model embeddings performs poorly. It wasn't clear to me if they\nwere backpropagating into the language model in this experiment - but if not,\nit certainly seems like there is potential for that to make a task specific RNN\nunnecessary.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "The paper proposes an approach where pre-trained word embeddings and\npre-trained neural language model embeddings are leveraged (i.e., concatenated)\nto improve the performance in English chunking and NER on the respective CoNLL\nbenchmarks, and on an out-of-domain English NER test set. The method records\nstate-of-the-art scores for the two tasks.\n\n- Strengths:\n\nFor the most part, the paper is well-written and easy to follow. The method is\nextensively documented. The discussion is broad and thorough.\n\n- Weaknesses:\n\nSequence tagging does not equal chunking and NER. I am surprised not to see POS\ntagging included in the experiment, while more sequence tagging tasks would be\nwelcome: grammatical error detection, supersense tagging, CCG supertagging,\netc. This way, the paper is on chunking and NER for English, not for sequence\ntagging in general, as it lacks both the multilingual component and the breadth\nof tasks.\n\nWhile I welcomed the extensive description of the method, I do think that\nfigures 1 and 2 overlap and that only one would have sufficed.\n\nRelated to that, the method itself is rather straightforward and simple. While\nthis is by all means not a bad thing, it seems that this contribution could\nhave been better suited for a short paper. Since I do enjoy the more extensive\ndiscussion section, I do not necessarily see it as a flaw, but the core of the\nmethod itself does not strike me as particularly exciting. It's more of a\n\"focused contribution\" (short paper description from the call) than\n\"substantial\" work (long paper).\n\n- General Discussion:\n\nBottomline, the paper concatenates two embeddings, and sees improvements in\nEnglish chunking and NER.\n\nAs such, does it warrant publication as an ACL long paper? I am ambivalent, so\nI will let my score reflect that, even if I slightly lean towards a negative\nanswer. Why? Mainly because I would have preferred to see more breadth: a) more\nsequence tagging tasks and b) more languages.\n\nAlso, we do not know how well this method scales to low(er)-resource scenarios.\nWhat if the pre-trained embeddings are not available? What if they were not as\nsizeable as they are? The experiments do include a notion of that, but still\nfar above the low-resource range. Could they not have been learned in a\nmulti-task learning setup in your model? That would have been more substantial\nin my view.\n\nFor these reasons, I vote borderline, but with a low originality score. The\nidea of introducing context via the embeddings is nice in itself, but this\nparticular instantiation of it leaves a lot to ask for.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Pre-trained word embeddings learned from unlabeled text have become a stan- dard component of neural network archi- tectures for NLP tasks. However, in most cases, the recurrent network that oper- ates on word-level representations to pro- duce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidi- rectional language models to NLP sys- tems and apply it to sequence labeling tasks. We evaluate our model on two stan- dard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "histories": [], "id": 561, "title": "Semi-supervised sequence tagging with bidirectional language models"}
