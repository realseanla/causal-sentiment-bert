{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: A nice, solid piece of work that builds on previous studies in a\nproductive way. Well-written and clear. \n\n- Weaknesses:\n\n Very few--possibly avoid some relatively \"empty\" statements:\n\n191 : For example, if our task is to identify words used similarly across\ncontexts, our scoring function can be specified to give high scores to terms\nwhose usage is similar across the contexts.\n\n537 : It is educational to study how annotations drawn from the same data are\nsimilar or different.\n\n- General Discussion:\nIn the first sections I was not sure that much was being done that was new or\ninteresting, as the methods seemed very reminiscent of previous methods used\nover the past 25 years to measure similarity, albeit with a few new statistical\ntwists, but conceptually in the same vein. Section 5, however, describes an\ninteresting and valuable piece of work that will be useful for future studies\non the topic. In retrospect, the background provided in sections 2-4 is useful,\nif not necessary, to support the experiments in section 5. \n\nIn short, the work and results described will be useful to others working in\nthis area, and the paper is worthy of presentation at ACL.\n\nMinor comments:\n\nWord, punctuation missing?\n264 : For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative\nsampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al.\n(2016).\n\nUnclear what \"multiple methods\" refers to :\n278 : some words were detected by multiple methods with CCLA", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper propose a general framework for analyzing similarities and\ndifferences in term meaning and representation in different contexts.\n\n- Strengths:\n* The framework proposed in this paper is generalizable and can be applied to\ndifferent applications, and accommodate difference notation of context,\ndifferent similarity functions, different type of word annotations. \n* The paper is well written. Very easy to follow.\n\n- Weaknesses:\n* I have concerns in terms of experiment evaluation. The paper uses qualitative\nevaluation metrics, which makes it harder to evaluate the effectiveness, or\neven the validity of proposed method. For example, table 1 compares the result\nwith Hamilton et, al using different embedding vector by listing top 10 words\nthat changed from 1900 to 1990. It's hard to tell, quantitatively, the\nperformances of CCLA. The same issue also applies to experiment 2 (comparative\nlexical analysis over context). The top 10 words may be meaningful, but what\nabout top 20, 100? what about the words that practitioner actually cares?\nWithout addressing the evaluation issue, I find it difficult to claim that CCLA\nwill benefit downstream applications.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "We propose a general framework for performing cross-context lexical                  analysis; that is, analyzing similarities and differences in term meaning         and representation with respect to different, potentially overlapping             partitions of a text collection.                                                       We apply our framework to three different tasks: semantic change detection        (discovering words whose meanings changed over time), comparative lexical         analysis over context (finding context-sensitive and                              context-\\emph{in}sensitive terms), and word representation comparison             (investigating randomness inherent in word embeddings).", "histories": [], "id": 553, "title": "Cross-Context Lexical Analysis"}
