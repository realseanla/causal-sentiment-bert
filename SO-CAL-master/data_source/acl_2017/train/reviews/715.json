{"reviews": [{"IMPACT": "5", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n*- Task\n*- Simple model, yet the best results on SQuAD (single model0\n*- Evaluation and comparison\n\n- Weaknesses:\n*- Analysis of errors/results (See detailed comments below)\n\n- General Discussion:\nIn this paper the authors present a method for directly querying Wikipedia to\nanswer open domain questions. The system consist of two components - a module\nto query/fetch wikipedia articles and a module to answer the question given the\nfetched set of wikipedia articles. \n\nThe document retrieval system is a traditional IR system relying on term\nfrequency models and ngram counts.  The answering system uses a feature\nrepresentation for paragraphs that consists of word embeddings, indicator\nfeatures to determine whether a paragraph word occurs in a question,\ntoken-level features including POS, NER etc and a soft feature for capturing\nsimilarity between question and paragraph tokens in embedding space. A combined\nfeature representation is used as an input to a bi-direction LSTM RNN for\nencoding. For questions an RNN that works on the word embeddings is used. \nThese are then used to train an overall classifier independently for start and\nend spans of sentences within a paragraph to answer questions.\n\nThe system has been trained using different Open Domain QA datasets such as\nSQuAD and WebQuestions by modifying the training data to include articles\nfetched by the IR engine instead of just the actual correct document/passage.\n\nOverall, an easy to follow interesting paper but I had a few questions:\n1) The IR system has a Accuracy@5 of over 75 %, and individually the document\nreader performs well and can beat the best single models on SquAD. What\nexplains the significant drop in Table 6. The authors mention that instead of\nthe fetched results, if they test using the best paragraph the accuracy reaches\njust 0.49 (from 0.26) but that is still significantly below the 0.78-79 in the\nSQuAD task.  So, presumably the error is this large because the neural network\nfor matching isnt doing as good a job in learning the answers when using the\nmodified training set (which includes fetched articles) instead of the case\nwhen training and testing is done for the document understanding task. Some\nanalysis of whats going on here should be provided. What was the training\naccuracy in the both cases? What can be done to improve it? To be fair, the\nauthors to allude to this in the conclusion but I think it still needs to be\npart of the paper to provide some meaningful insights.\n\n2) I understand the authors were interested in treating this as a pure machine\ncomprehension task and therefore did not want to rely on external sources such\nas Freebase which could have helped with entity typing        but that would have\nbeen interesting to use. Tying back to my first question -- if the error is due\nto highly relevant topical sentences as the authors mention, could entity\ntyping have helped?\n\nThe authors should also refer to QuASE (Sun et. al 2015 at WWW2015) and similar\nsystems in their related work. QuASE is also an Open domain QA system that\nanswers using fetched passages - but it relies on the web instead of just\nWikipedia.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "5", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThe authors focus on a very challenging task of answering open-domain question\nfrom Wikipedia. Authors have developed 1) a document retriever to retrieve\nrelevant Wikipedia articles for a question, and 2) Document retriever to\nretrieve the exact answer from the retrieved paragraphs. \nAuthors used Distant Supervision to fine-tune their model. Experiments show\nthat the document reader performs better than WikiSearch API, and Document\nReader model does better than some recent models for QA.\n\n- Weaknesses:\nThe final results are inferior to some other models, as presented by the\nauthors. Also, no error analysis is provided.\n\n- General Discussion:\n\nThe proposed systems by the authors is end-to-end and interesting. However, I\nhave some concerns below.\n\nDocument Retriever: Authors have shown a better retrieval performance than Wiki\nSearch. However, it is not described as to how exactly the API is used.\nWikiSearch may not be a good baseline for querying \"questions\" (API suits\nstructured retrieval more). Why don't the authors use some standard IR\nbaselines for this?\n\nDistant Supervision: How effective and reliable was distant supervision?\nClearly, the authors had to avoid using many training examples because of this,\nbut whatever examples the authors could use, what fraction was actually \"close\nto correct\"? Some statistics would be helpful to understand if some more\nfine-tuning of distant supervision could have helped.\n\nFull Wikipedia results: This was the main aim of the authors and as authors\nthemselves said, the full system gives a performance of 26.7 (49.6 when correct\ndoc given, 69.5 when correct paragraph is given). Clearly, that should be a\nmotivation to work more on the retrieval aspect? For WebQuestions, the results\nare much inferior to YodaQA, and that raises the question -- whether Wikipedia\nitself is sufficient to answer all the open-domain questions? Should authors\nthink of an integrated model to address this? \n\nOverall, the final results shown in Tables 4 and 5 are inferior to some other\nmodels. While authors only use Wikipedia, the results are not indicative of\nthis being the best strategy.\n\nOther points:\nThe F1 value in Table 5 (78.4) is different from that in Table 4 (Both Dev and\nTest).\nTable 5: Why not \"No f_emb\"?\nError analysis: Some error analysis is required in various components of the\nsystem. \nAre there some specific type of questions, where the system does not perform\nwell? Is there any way one can choose which question is a good candidate to be\nanswered by Wikipedia, and use this method only for those questions?\nFor WebQuestions, DS degrades the performance further.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.", "histories": [], "id": "715", "title": "Reading Wikipedia to Answer Open-Domain Questions"}
