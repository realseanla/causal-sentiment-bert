{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, attention-based\nmatching, and aggregation. The model has two variants, one based on TreeRNNs\nand the other based on sequential BiLSTMs. The sequential model outperforms all\npublished results, and an ensemble with the tree model does better still.\n\nThe paper is clear, the model is well motivated, and the results are\nimpressive. Everything in the paper is solidly incremental, but I nonetheless\nrecommend acceptance. \n\nMajor issues that I'd like discussed in the response:\n\u2013 You suggest several times that your system can serve as a new baseline for\nfuture work on NLI. This isn't an especially helpful or meaningful claim\u2014it\ncould be said of just about any model for any task. You could argue that your\nmodel is unusually simple or elegant, but I don't think that's really a major\nselling point of the model.\n\u2013 Your model architecture is symmetric in some ways that seem like\noverkill\u2014you compute attention across sentences in both directions, and run a\nseparate inference composition (aggregation) network for each direction. This\npresumably nearly doubles the run time of your model. Is this really necessary\nfor the very asymmetric task of NLI? Have you done ablation studies on this?**\n\u2013 You present results for the full sequential model (ESIM) and the ensemble\nof that model and the tree-based model (HIM). Why don't you present results for\nthe tree-based model on its own?**\n\nMinor issues:\n\u2013 I don't think the Barker and Jacobson quote means quite what you want it to\nmean. In context, it's making a specific and not-settled point about *direct*\ncompositionality in formal grammar. You'd probably be better off with a more\ngeneral claim about the widely accepted principle of compositionality.\n\u2013 The vector difference feature that you use (which has also appeared in\nprior work) is a bit odd, since it gives the model redundant parameters. Any\nmodel that takes vectors a, b, and (a - b) as input to some matrix\nmultiplication is exactly equivalent to some other model that takes in just a\nand b and has a different matrix parameter. There may be learning-related\nreasons why using this feature still makes sense, but it's worth commenting on.\n\u2013 How do you implement the tree-structured components of your model? Are\nthere major issues with speed or scalability there?\n\u001f\u2013 Typo: (Klein and D. Manning, 2003) \n\u2013 Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much\nmore readable parse trees without crossing lines. I'd suggest using them.\n\n---\n\nThanks for the response! I still solidly support publication. This work is not\ngroundbreaking, but it's novel in places, and the results are surprising enough\nto bring some value to the conference.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper proposes a model for the Stanford Natural Language Inference (SNLI)\ndataset, that builds on top of sentence encoding models and the decomposable\nword level alignment model by Parikh et al. (2016). The proposed improvements\ninclude performing decomposable attention on the output of a BiLSTM and feeding\nthe attention output to another BiLSTM, and augmenting this network with a\nparallel tree variant.\n\n- Strengths:\n\nThis approach outperforms several strong models previously proposed for the\ntask. The authors have tried a large number of experiments, and clearly report\nthe ones that did not work, and the hyperparameter settings of the ones that\ndid. This paper serves as a useful empirical study for a popular problem.\n\n- Weaknesses:\n\nUnfortunately, there are not many new ideas in this work that seem useful\nbeyond the scope the particular dataset used. While the authors claim that the\nproposed network architecture is simpler than many previous models, it is worth\nnoting that the model complexity (in terms of the number of parameters) is\nfairly high. Due to this reason, it would help to see if the empirical gains\nextend to other datasets as well. In terms of ablation studies, it would help\nto see 1) how well the tree-variant of the model does on its own and 2) the\neffect of removing inference composition from the model.\n\nOther minor issues:\n1) The method used to enhance local inference (equations 14 and 15) seem very\nsimilar to the heuristic matching function used by Mou et al., 2015 (Natural\nLanguage Inference by Tree-Based Convolution and Heuristic Matching). You may\nwant to cite them.\n\n2) The first sentence in section 3.2 is an unsupported claim. This either needs\na citation, or needs to be stated as a hypothesis.\n\nWhile the work is not very novel, the the empirical study is rigorous for the\nmost part, and could be useful for researchers working on similar problems.\nGiven these strengths, I am changing my recommendation score to 3. I have read\nthe authors' responses.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}], "abstract": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.", "histories": [], "id": "270", "title": "Enhanced LSTM for Natural Language Inference"}
