{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Update after rebuttal\n\nI appreciate the authors taking the time to clarify their implementation of the\nbaseline and to provide some evidence of the significance of the improvements\nthey report. These clarifications should definitely be included in the\ncamera-ready version. I very much like the idea of using visual features for\nthese languages, and I am looking forward to seeing how they help more\ndifficult tasks in future work.\n\n- Strengths:\n\n- Thinking about Chinese/Japanese/Korean characters visually is a great idea!\n\n- Weaknesses:\n\n- Experimental results show only incremental improvement over baseline, and the\nchoice of evaluation makes it hard to verify one of the central arguments: that\nvisual features improve performance when processing rare/unseen words.\n\n- Some details about the baseline are missing, which makes it difficult to\ninterpret the results, and would make it hard to reproduce the work.\n\n- General Discussion:\n\nThe paper proposes the use of computer vision techniques (CNNs applied to\nimages of text) to improve language processing for Chinese, Japanese, and\nKorean, languages in which characters themselves might be compositional. The\nauthors evaluate their model on a simple text-classification task (assigning\nWikipedia page titles to categories). They show that a simple one-hot\nrepresentation of the characters outperforms the CNN-based representations, but\nthat the combination of the visual representations with standard one-hot\nencodings performs better than the visual or the one-hot alone. They also\npresent some evidence that the visual features outperform the one-hot encoding\non rare words, and present some intuitive qualitative results suggesting the\nCNN learns good semantic embeddings of the characters.\n\nI think the idea of processing languages like Chinese and Japanese visually is\na great one, and the motivation for this paper makes a lot of sense. However, I\nam not entirely convinced by the experimental results. The evaluations are\nquite weak, and it is hard to say whether these results are robust or simply\ncoincidental. I would prefer to see some more rigorous evaluation to make the\npaper publication-ready. If the results are statistically significant (if the\nauthors can indicate this in the author response), I would support accepting\nthe paper, but ideally, I would prefer to see a different evaluation entirely.\n\nMore specific comments below:\n\n- In Section 3, paragraph \"lookup model\", you never explicitly say which\nembeddings you use, or whether they are tuned via backprop the way the visual\nembeddings are. You should be more clear about how the baseline was\nimplemented. If the baseline was not tuned in a task-specific way, but the\nvisual embeddings were, this is even more concerning since it makes the\nperformances substantially less comparable.\n\n- I don't entirely understand why you chose to evaluate on classifying\nwikipedia page titles. It seems that the only real argument for using the\nvisual model is its ability to generalize to rare/unseen characters. Why not\nfocus on this task directly? E.g. what about evaluating on machine translation\nof OOV words? I agree with you that some languages should be conceptualized\nvisually, and sub-character composition is important, but the evaluation you\nuse does not highlight weaknesses of the standard approach, and so it does not\nmake a good case for why we need the visual features. \n\n- In Table 5, are these improvements statistically significant?\n\n- It might be my fault, but I found Figure 4 very difficult to understand.\nSince this is one of your main results, you probably want to present it more\nclearly, so that the contribution of your model is very obvious. As I\nunderstand it, \"rank\" on the x axis is a measure of how rare the word is (I\nthink log frequency?), with the rarest word furthest to the left? And since the\nvisual model intersects the x axis to the left of the lookup model, this means\nthe visual model was \"better\" at ranking rare words? Why don't both models\nintersect at the same point on the x axis, aren't they being evaluated on the\nsame set of titles and trained with the same data? In the author response, it\nwould be helpful if you could summarize the information this figure is supposed\nto show, in a more concise way. \n\n- On the fallback fusion, why not show performance for for different\nthresholds? 0 seems to be an edge-case threshold that might not be\nrepresentative of the technique more generally.\n\n- The simple/traditional experiment for unseen characters is a nice idea, but\nis presented as an afterthought. I would have liked to see more eval in this\ndirection, i.e. on classifying unseen words\n\n- Maybe add translations to Figure 6, for people who do not speak Chinese?", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry topical content which resulting in embeddings that are coherent in visual space.", "histories": [], "id": "543", "title": "Learning Character-level Compositionality with Visual Features"}
