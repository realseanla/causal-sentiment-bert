{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This is a nice paper on morphological segmentation utilizing word \nembeddings. The paper presents a system which uses word embeddings to \nboth measure local semantic similarity of word pairs with a potential \nmorphological relation, and global information about the semantic validity\nof potential morphological segment types. The paper is well written and \nrepresents a nice extension to earlier approaches on semantically driven \nmorphological segmentation.\n\nThe authors present experiments on Morpho Challenge data for three \nlanguages: English, Turkish and Finnish. These languages exhibit varying \ndegrees of morphological complexity. All systems are trained on Wikipedia \ntext. \n\nThe authors show that the proposed MORSE system delivers clear \nimprovements w.r.t. F1-score for English and Turkish compared to the well \nknown Morfessor system which was used as baseline. The system fails to \nreach the performance of Morfessor for Finnish. As the authors note, this \nis probably a result of the richness of Finnish morphology which leads to \ndata sparsity and, therefore, reduced quality of word embeddings. To \nimprove the performance for Finnish and other languages with a similar \ndegree of morphological complexity, the authors could consider word \nembeddings which take into account sub-word information. For example,\n\n@article{DBLP:journals/corr/CaoR16,\n  author    = {Kris Cao and\n               Marek Rei},\n  title     = {A Joint Model for Word Embedding and Word Morphology},\n  journal   = {CoRR},\n  volume    = {abs/1606.02601},\n  year                  = {2016},\n  url                 = {http://arxiv.org/abs/1606.02601},\n  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CaoR16},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}\n\n@article{DBLP:journals/corr/BojanowskiGJM16,\n  author    = {Piotr Bojanowski and\n               Edouard Grave and\n               Armand Joulin and\n               Tomas Mikolov},\n  title     = {Enriching Word Vectors with Subword Information},\n  journal   = {CoRR},\n  volume    = {abs/1607.04606},\n  year                  = {2016},\n  url                 = {http://arxiv.org/abs/1607.04606},\n  timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojanowskiGJM16},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}\n\nThe authors critique the existing Morpho Challenge data sets. \nFor example, there are many instances of incorrectly segmented words in \nthe material. Moreover, the authors note that, while some segmentations \nin the the data set may be historically valid (for example the \nsegmentation of business into busi-ness), these segmentations are no \nlonger semantically motivated. The authors provide a new data set \nconsisting of 2000 semantically motivated segmentation of English word \nforms from the English Wikipedia. They show that MORSE deliver highly \nsubstantial improvements compared to Morfessor on this data set.\n\nIn conclusion, I think this is a well written paper which presents \ncompetitive results on the interesting task of semantically driven \nmorphological segmentation. The authors accompany the submission with \ncode and a new data set which definitely add to the value of the \nsubmission.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper continues the line of work for applying word embeddings for the\nproblem of unsupervised morphological segmentation (e.g. Soricut & Och, 2015;\n\u00dcst\u00fcn & Can, 2016). The proposed method, MORSE, applies a local optimization\nfor segmentation of each word, based on a set of orthographic and semantic\nrules and a few heuristic threshold values associated with them.\n\n- Strengths:\n\nThe paper presents multiple ways to evaluate segmentation hypothesis on word\nembeddings, and these may be useful also in other type of methods. The results\non English and Turkish data sets are convincing.\n\nThe paper is clearly written and organized, and the biliography is extensive.\n\nThe submission includes software for testing the English MORSE model and three\nsmall data sets used in the expriments.\n\n- Weaknesses:\n\nThe ideas in the paper are quite incremental, based mostly on the work by\nSoricut & Och (2015). However, the main problems of the paper concern\nmeaningful comparison to prior work and analysis of the method's limitations.\n\nFirst, the proposed method does not provide any sensible way for segmenting\ncompounds. Based on Section 5.3, the method does segment some of the compounds,\nbut using the terminology of the method, it considers either of the\nconstituents as an affix. Unsuprisingly, the limitation shows up especially in\nthe results of a highly-compounding language, Finnish. While the limitation is\nindicated in the end of the discussion section, the introduction and\nexperiments seem to assume otherwise.\n\nIn particular, the limitation on modeling compounds makes the evaluation of\nSection 4.4/5.3 quite unfair: Morfessor is especially good at segmenting\ncompounds (Ruokolainen et al., 2014), while MORSE seems to segment them only\n\"by accident\". Thus it is no wonder that Morfessor segments much larger\nproportion of the semantically non-compositional compounds. A fair experiment\nwould include an equal number of compounds that _should_ be segmented to their\nconstituents.\n\nAnother problem in the evaluations (in 4.2 and 4.3) concerns hyperparameter\ntuning. The hyperparameters of MORSE are optimized on a tuning data, but\napparently the hyperparameters of Morfessor are not. The recent versions of\nMorfessor (Kohonen et al. 2010, Gr\u00f6nroos et al. 2014) have a single\nhyperparameter that can be used to balance precision and recall of the\nsegmentation. Given that the MORSE outperforms Morfessor both in precision and\nrecall in many cases, this does not affect the conclusions, but should at least\nbe mentioned.\n\nSome important details of the evaluations and results are missing: The\n\"morpheme-level evaluation\" method in 5.2 should be described or referred to.\nMoreover, Table 7 seems to compare results from different evaluation sets: the\nMorfessor and Base Inference methods seem to be from official Morpho Challenge\nevaluations, LLSM is from Narasimhan et al. (2015), who uses aggregated data\nfrom Morpho Challenges (probably including both development and training sets),\nand MORSE is evaluated Morpho Challenges 2010 development set. This might not\naffect the conclusions, as the differences in the scores are rather large, but\nit should definitely be mentioned.\n\nThe software package does not seem to support training, only testing an\nincluded model for English.\n\n- General Discussion:\n\nThe paper puts a quite lot of focus on the issue of segmenting semantically\nnon-compositional compounds. This is problematic in two ways: First, as\nmentioned above, the proposed method does not seem to provide sensible way of\nsegmenting _any_ compound. Second, finding the level of lexicalized base forms\n(e.g. freshman) and the morphemes as smallest meaning-bearing units (fresh,\nman) are two different tasks with different use cases (for example, the former\nwould be more sensible for phrase-based SMT and the latter for ASR). The\nunsupervised segmentation methods, such as Morfessor, typically target at the\nlatter, and critizing the method for a different goal is confusing.\n\nFinally, there is certainly a continuum on the (semantic) compositionality of\nthe compound, and the decision is always somewhat arbitrary. (Unfortunately\nmany gold standards, including the Morpho Challenge data sets, tend to be also\ninconsistent with their decisions.)\n\nSections 4.1 and 5.1 mention the computational efficiency and limitation to one\nmillion input word forms, but does not provide any details: What is the\nbottleneck here? Collecting the transformations, support sets, and clusters? Or\nthe actual optimization problem? What were the computation times and how do\nthese scale up?\n\nThe discussion mentions a few benefits of the MORSE approach: Adaptability as a\nstemmer, ability to control precision and recall, and need for only a small\nnumber of gold standard segmentations for tuning. As far as I can see, all or\nsome of these are true also for many of the Morfessor variants (Creutz and\nLagus, 2005; Kohonen et al., 2010; Gr\u00f6nroos et al., 2014), so this is a bit\nmisleading. It is true that Morfessor works usually fine as a completely\nunsupervised method, but the extensions provide at least as much flexibility as\nMORSE has.\n\n(Ref: Mathias Creutz and Krista Lagus. 2005. Inducing the Morphological Lexicon\nof a Natural Language from Unannotated Text. In Proceedings of the\nInternational and Interdisciplinary Conference on Adaptive Knowledge\nRepresentation and Reasoning (AKRR'05), Espoo, Finland, June 15-17.)\n\n- Miscellaneous:\n\nAbstract should maybe mention that this is a minimally supervised method\n(unsupervised to the typical extent, i.e. excluding hyperparameter tuning).\n\nIn section 3, it should be mentioned somewhere that phi is an empty string.\n\nIn section 5, it should be mentioned what specific variant (and implementation)\nof Morfessor is applied in the experiments.\n\nIn the end of section 5.2, I doubt that increasing the size of the input\nvocabulary would alone improve the performance of the method for Finnish. For a\nlanguage that is morphologically as complex, you never encounter even all the\npossible inflections of the word forms in the data, not to mention derivations\nand compounds.\n\nI would encourage improving the format of the data sets (e.g.  using something\nsimilar to the MC data sets): For example using \"aa\" as a separator for\nmultiple analyses is confusing and makes it impossible to use the format for\nother languages.\n\nIn the references, many proper nouns and abbreviations in titles are written in\nlowercase letters. Narasimhan et al. (2015) is missing all the publication\ndetails.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n I find the idea of using morphological compositionality to make decisions on\nsegmentation quite fruitful.\n\nMotivation is quite clear\n\nThe paper is well-structured\n\n- Weaknesses:\nSeveral points are still unclear: \n  -- how the cases of rule ambiguity are treated (see \"null->er\" examples in\ngeneral discussion)\n  -- inference stage seems to be suboptimal\n  -- the approach is limited to known words only\n\n- General Discussion:\nThe paper presents semantic-aware method for morphological segmentation. The\nmethod considers sets of simple morphological composition rules, mostly\nappearing as 'stem plus suffix or prefix'. The approach seems to be quite\nplausible and the motivation behind is clear and well-argumented.\n\nThe method utilizes the idea of vector difference to evaluate semantic\nconfidence score for a proposed transformational rule. It's been previously\nshown by various studies that morpho-syntactic relations are captured quite\nwell by doing word analogies/vector differences. But, on the other hand, it has\nalso been shown that in case of derivational morphology (which has much less\nregularity than inflectional) the performance substantially drops (see\nGladkova, 2016; Vylomova, 2016). \n\n The search space in the inference stage although being tractable, still seems\nto be far from optimized (to get a rule matching \"sky->skies\" the system first\nneeds to searhc though the whole R_add set and, probably, quite huge set of\nother possible substitutions) and limited to known words only (for which we can\nthere exist rules). \n\n It is not clear how the rules for the transformations which are\northographically the same, but semantically completely different are treated.\nFor instance, consider \"-er\" suffix. On one hand, if used with verbs, it\ntransforms them into agentive nouns, such as \"play->player\". On the other hand,\nit could also be used with adjectives for producing comparative form, for\ninstance, \"old->older\". Or consider \"big->bigger\" versus \"dig->digger\".\nMore over, as mentioned before, there is quite a lot of irregularity in\nderivational morphology. The same suffix might play various roles. For\ninstance, \"-er\" might also represent patiental meanings (like in \"looker\"). Are\nthey merged into a single rule/cluster? \n\n No exploration of how the similarity threshold and measure may affect the\nperformance is presented.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic  information for this task. We also analyze the deficiencies  of  available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality.  We validate our algorithm across datasets and present state-of-the-art results.", "histories": [], "id": "723", "title": "MORSE: Semantic-ally Drive-n MORpheme SEgment-er"}
