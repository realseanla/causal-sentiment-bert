{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThe deviation between \"vocal\" users and \"average users\" is an interesting\ndiscovery that could be applied as a way to identify different types of users.\n\n- Weaknesses:\n\nI see it as an initial work on a new topic that should be expanded in the\nfuture. A possible comparison between matrix factorization and similar topics \nin distributional semantics (e.g. latent semantic analysis) would be useful. \n\n- General Discussion:\n\nIn this paper, the authors describe an approach for modeling the\nstance/sentiment of Twitter users about topics. In particular, they address the\ntask of inter-topic preferences modeling. This task consists of measuring the\ndegree to which the stances about different topics are mutually related.This\nwork is claimed to advance state of the art in this task, since previous works\nwere case studies, while the proposed one is about unlimited topics on\nreal-world data.The adopted approach consists of the following steps: A set of\nlinguistic patterns was manually created and, through them, a large number of\ntweets expressing stance towards various topics was collected. Next, the texts\nwere expressed as triples containing user, topic, and evaluation. The\nrelationships represented by the tuples were arranged as a sparse matrix. After\nmatrix factorization, a low-rank approximation was performed. The optimal rank\nwas identified as 100. The definition of cosine similarity is used to measure\nthe similarity between topics and, thus, detect latent preferences not\nrepresented in the original sparse matrix. Finally, cosine similarity is also\nused to detect inter-topic preferences.A preliminary empirical evaluation shows\nthat the model predicts missing topics preferences. Moreover, predicted\ninter-topic preferences moderately correlate with the corresponding values from\na crowdsourced gold-standard collection of preferences. \nAccording to the overview discussed in the related work section, there are no\nprevious systems to be compared in the latter task (i.e. prediction of\ninter-topic preferences) and, for this reason, it is promising.\n\nI listed some specific comments below.\n\n- Rows 23 and 744, \"high-quality\": What makes them high-quality? If not\nproperly defined, I would remove all the occurrences of \"high-quality\" in the\npaper.\n\n- Row 181 and caption of Figure 1: I would remove the term \"generic.\"\n\n- Row 217, \"This section collect\": -> \"We collected\" or \"This section explains\nhow we collected\"- Row 246: \"ironies\" -> \"irony\"\n\n- Row 269, \"I support TPP\": Since the procedure can detect various patterns\nsuch as \"to A\" or \"this is A,\" maybe the author should explain that all\npossible patterns containing the topic are collected, and next manually\nfiltered?\n\n- Rows 275 and 280, \"unuseful\": -> useless\n\n- Row 306, \"including\": -> are including\n\n- Row 309:  \"of\" or \"it\" are not topics but, I guess, terms retrieved by\nmistakes as topics. \n\n- Rows 317-319: I would remove the first sentence and start with \"Twitter\nuser...\"\n\n- Rows 419-439: \"I like the procedure used to find the optimal k. In previous\nworks, this number is often assumed, while it is useful to find it\nempirically.\"\n\n- Row 446, \"let\": Is it \"call\"?", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "2"}], "abstract": "We presents in this paper our approach for modeling inter-topic preferences of Twitter users: for example, \"those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade\". This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion survey, electoral prediction, electoral campaigns, and online debates. In order to extract users' preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., \"A is completely wrong''). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users' preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.", "histories": [], "id": 777, "title": "Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization"}
