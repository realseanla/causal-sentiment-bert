{"reviews": [{"IMPACT": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "Review: Multimodal Word Distributions\n\n- Strengths:  Overall a very strong paper.\n\n- Weaknesses: The comparison against similar approaches could be extended.\n\n- General Discussion:\n\nThe main focus of this paper is the introduction of a new model for learning\nmultimodal word distributions formed from Gaussian mixtures for multiple word\nmeanings. i. e. representing a word by a set of many Gaussian distributions. \nThe approach, extend the model introduced by Vilnis and McCallum (2014) which\nrepresented word as unimodal Gaussian distribution. By using a multimodal, the\ncurrent approach attain the problem of polysemy.\n\nOverall, a very strong paper, well structured and clear. The experimentation is\ncorrect and the qualitative analysis made in table 1 shows results as expected\nfrom the approach.  There\u2019s not much that can be faulted and all my comments\nbelow are meant to help the paper gain additional clarity. \n\nSome comments: \n\n_ It may be interesting to include a brief explanation of the differences\nbetween the approach from Tian et al. 2014 and the current one. Both split\nsingle word representation into multiple prototypes by using a mixture model. \n\n_ There are some missing citations that could me mentioned in related work as :\n\nEfficient Non-parametric Estimation of Multiple Embeddings per Word in Vector\nSpace Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014\nDo Multi-Sense Embeddings Improve Natural Language Understanding? Li and\nJurafsky, EMNLP 2015\nTopical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015\n\n_ Also, the inclusion of the result from those approaches in tables 3 and 4\ncould be interesting. \n\n_ A question to the authors: What do you attribute the loss of performance of\nw2gm against w2g in the analysis of SWCS?\n\nI have read the response.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This work uses Gaussian mixtures to represent words and demonstrates its\npotential in capturing multiple word meanings for polysemy. The training\nprocess is done based on a max-margin objective. The expected likelihood kernel\nis used as the similarity between two words' distributions. Experiment results\non word similarity and entailment tasks show the effectiveness of the proposed\nwork.\n\n- Strengths:\n\nThe problem is clearly motivated and defined. Gaussian mixtures are much more\nexpressive than deterministic vector representations. It can potentially\ncapture different word meanings by its modes, along with probability mass and\nuncertainty around those modes. This work represents an important contribution\nto word embedding. \n\nThis work propose a max-margin learning objective with closed-form similarity\nmeasurement for efficient training.\n\nThis paper is mostly well written. \n\n- Weaknesses:\n\nSee below for some questions. \n\n- General Discussion:\n\nIn the Gaussian mixture models, the number of gaussian components (k) is\nusually an important parameter. In the experiments of this paper, k is set to\n2. What is your criteria to select k? Does the increase of k hurt the\nperformance of this model? What does the learned distribution look like for a\nword that only has one popular meaning?\n\nI notice that you use the spherical case in all the experiments (the covariance\nmatrix reduces to a single number). Is this purely for computation efficiency?\nI wonder what's the performance of using a general diagonal covariance matrix.\nSince in this more general case, the gaussian mixture defines different degrees\nof uncertainty along different directions in the semantic space, which seems\nmore interesting.\n\nMinor comments:\nTable 4 is not referred to in the text.\nIn reference, Luong et al. lacks the publication year.\n\nI have read the response.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Word embeddings provide point representations of words containing useful semantic information.  We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information.  To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely  expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.", "histories": [], "id": "145", "title": "Multimodal Word Distributions"}
