{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposed to explore discourse structure, as defined by Rhetorical\nStructure Theory (RST) to improve text categorization. A RNN with attention\nmechanism is employed to compute a representation of text. The experiments on\nvarious of dataset shows the effectiveness of the proposed method. Below are my\ncomments:\n\n(1) From Table 2, it shows that \u201cUNLABELED\u201d model performs better on four\nout of five datasets than the \u201cFULL\u201d model. The authors should explain more\nabout this, because intuitively, incorporating additional relation labels\nshould bring some benefits. Is the performance of relation labelling so bad and\nit hurts the performance instead?\n\n(2) The paper also transforms the RST tree into a dependency structure as a\npre-process step. Instead of transforming, how about keep the original tree\nstructure and train a hierarchical model on that?\n\n(3) For the experimental datasets, instead of comparing with only one dataset\nwith each of the previous work, the authors may want to run experiments on more\ncommon datasets used by previous work.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThe main strength of this paper is the incorporation of discourse structure in\nthe DNN's attention model, which allows the model to learn the weights given to\ndifferent EDUs.\n\nAlso the paper is very clear, and provides a good explanation of both RST and\nhow it is used in the model.\nFinally, the evaluation experiments are conducted thoroughly with strong,\nstate-of-the-art baselines.\n\n- Weaknesses:\n\nThe main weakness of the paper is that the results do not strongly support the\nmain claim that discourse structure can help text classification. Even the\nUNLABELED variant, which performs best and does outperform the state of the\nart, only provides minimal gains (and hurts in the legal/bills domain). The\napproach (particularly the FULL variant) seems to be too data greedy but no\nreal solution is provided to address this beyond the simpler UNLABELED and ROOT\nvariants.\n\n- General Discussion:\n\nIn general, this paper feels like a good first shot at incorporating discourse\nstructure into DNN-based classification, but does not fully convince that\nRST-style structure will significantly boost performance on most tasks (given\nthat it is also very costly to build a RST parser for a new domain, as would be\nneeded in the legal/bill domains described in this paper). I wish the authors\nhad explored or at least mentioned next steps in making this approach work, in\nparticular in the face of data sparsity. For example, how about defining\n(task-independent) discourse embeddings? Would it be possible to use a DNN for\ndiscourse parsing that could be incorporated in the main task DNN and optimized\njointly  end-to-end? Again, this is good work, I just wish the authors had\npushed it a little further given the mixed results.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}], "abstract": "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization.  Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task.  Experiments consider variants of the approach and illustrate its strengths and weaknesses.", "histories": [], "id": "447", "title": "Neural Discourse Structure for Text Categorization"}
