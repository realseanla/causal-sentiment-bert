{"reviews": [{"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths: Useful application for teachers and learners; supports\nfine-grained comparison of GEC systems.\n\n- Weaknesses: Highly superficial description of the system; evaluation not\nsatisfying.\n\n- General Discussion:\n\nThe paper presents an approach of automatically enriching the output of GEC\nsystems with error types. This is a very useful application because both\nteachers and learners can benefit from this information (and many GEC systems\nonly output a corrected version, without making the type of error explicit). It\nalso allows for finer-grained comparison of GEC systems, in terms of precision\nin general, and error type-specific figures for recall and precision.\n\nUnfortunately, the description of the system remains highly superficial. The\ncore of the system consists of a set of (manually?) created rules but the paper\ndoes not provide any details about these rules. The authors should, e.g., show\nsome examples of such rules, specify the number of rules, tell us how complex\nthey are, how they are ordered (could some early rule block the application of\na later rule?), etc. -- Instead of presenting relevant details of the system,\nseveral pages of the paper are devoted to an evaluation of the systems that\nparticipated in CoNLL-2014. Table 6 (which takes one entire page) list results\nfor all systems, and the text repeats many facts and figures that can be read\noff the table. \n\nThe evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for\nthe 200 test sentences instead of simply rating the output of the system. Given\na fixed set of tags, it should be possible to produce a gold standard for the\nrather small set of test sentences. It is highly probable that the approach\ntaken in the paper yields considerably better ratings for the annotations than\ncomparison with a real gold standard (see, e.g., Marcus et al. (1993) for a\ncomparison of agreement when reviewing pre-annotated data vs. annotating from\nscratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of\nour rule-based error types to be either \u201cGood\u201d or \u201cAcceptable\u201d\".\nMultiple rates should not be considered individually and their ratings averaged\nthis way, this is not common practice. If each of the \"bad\" scores were\nassigned to different edits (we don't learn about their distribution from the\npaper), 18.5% of the edits were considered \"bad\" by some annotator -- this\nsounds much worse than the average 3.7%, as calculated in the paper.\nThird, no information about the test data is provided, e.g. how many error\ncategories they contain, or which error categories are covered (according to\nthe cateogories rated as \"good\" by the annotators).\nForth, what does it mean that \"edit boundaries might be unusual\"? A more\nprecise description plus examples are at need here. Could this be problematic\nfor the application of the system?\n\nThe authors state that their system is less domain dependent as compared to\nsystems that need training data. I'm not sure that this is true. E.g., I\nsuppose that Hunspell's vocabulary probably doesn't cover all domains in the\nsame detail, and manually-created rules can be domain-dependent as well -- and\nare completely language dependent, a clear drawback as compared to machine\nlearning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)\nare from one domain only: student essays.\n\nIt remains unclear why a new set of error categories was designed. One reason\nfor the tags is given: to be able to search easily for underspecified\ncategories (like \"NOUN\" in general). It seems to me that the tagset presented\nin Nicholls (2003) supports such searches as well. Or why not using the\nCoNLL-2014 tagset? Then the CoNLL gold standard could have been used for\nevaluation.\n\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it\nabout a new system? But the most important details of it are left out. Is it\nabout a new set of error categories? But hardly any motivation or discussion of\nit is provided. Is it about evaluating the CoNLL-2014 systems? But the\npresentation of the results remains superficial.\n\nTypos:\n- l129 (and others): c.f. -> cf.\n- l366 (and others): M2 -> M^2 (= superscribed 2)\n- l319: 50-70 F1: what does this mean? 50-70%?\n\nCheck references for incorrect case\n- e.g. l908: esl -> ESL\n- e.g. l878/79: fleiss, kappa", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper presents a novel approach for evaluating grammatical error\ncorrection (GEC) systems. This approach makes it possible to assess\nthe performance of GEC systems by error type not only in terms of\nrecall but also in terms of precision, which was previously not\npossible in general since system output is usually not annotated with\nerror categories.\n\nStrengths:\n\n - The proposed evaluation is an important stepping stone for\n   analyzing GEC system behavior.\n - The paper includes evaluation for a variety of systems.\n - The approach has several advantages over previous work:\n   - it computes precision by error type\n   - it is independent of manual error annotation\n   - it can assess the performance on multi token errors\n - The automatically selected error tags for pre-computed error spans\n   are mostly approved of by human experts\n\nWeaknesses:\n\n - A key part \u2013 the rules to derive error types \u2013 are not described.\n - The classifier evaluation lacks a thorough error analysis and based\n   upon that it lacks directions of future work on how to improve the\n   classifier.\n - The evaluation was only performed for English and it is unclear how\n   difficult it would be to use the approach on another language.\n\nClassifier and Classifier Evaluation\n====================================\n\nIt is unclear on what basis the error categories were devised. Are\nthey based on previous work?\n\nAlthough the approach in general is independent of the alignment\nalgorithm, the rules are probably not, but the authors don't provide\ndetails on that.  The error categories are a major part of the paper\nand the reader should at least get a glimpse of how a rule to assign\nan error type looks like.\n\nUnfortunately, the paper does not apply the proposed evaluation on\nlanguages other than English.  It also does not elaborate on what\nchanges would be necessary to run the classifier on other languages. I\nassume that the rules used for determining edit boundaries as well as\nfor determining the error tags depend on the language/the\npre-processing pipeline to a certain extent and therefore need to be\nadapted. Also, the error categories might need to be changed.  The\nauthors do not provide any detail on the rules for assigning error\ncategories (how many are there overall/per error type? how complex are\nthey?) to estimate the effort necessary to use the approach on another\nlanguage.\n\nThe error spans computed in the pre-processing step seem to be\ninherently continuous (which is also the case with the M2 scorer), which\nis problematic since there are errors which can only be tagged\naccurately when the error span is discontinuous. In German, for\nexample, verbs with separable prefixes are separated from each other\nin the main clause: [1st constituent] [verb] [other constituents]\n[verb prefix]. Would the classifier be able to tag discontinuous edit\nspans?\n\nThe authors write that all human judges rated at least 95\\% of the\nautomatically assigned error tags as appropriate \"despite the degree\nof noise introduced by automatic edit extraction\" (295). I would be\nmore cautious with this judgment since the raters might also have been\nmore forgiving when the boundaries were noisy. In addition, they were\nnot asked to select a tag without knowing the system output but could\nin case of noisy boundaries be more biased towards the system\noutput. Additionally, there was no rating option between \"Bad (Not\nAppropriate)\" and \"Appropriate\", which might also have led raters to\nselect \"Appropriate\" over \"Bad\". To make the evaluation more sound,\nthe authors should also evaluate how the human judges rate the\nclassifier output if the boundaries were manually created,\ni.e. without the noise introduced by faulty boundaries.\n\nThe classifier evaluation lacks a thorough error analysis. It is only\nmentioned that \"Bad\" is usually traced back to a wrong POS\ntag. Questions I'd like to see addressed: When did raters select\n\"Bad\", when \"Appropriate\"? Does the rating by experts point at\npossibilities to improve the classifier?\n\nGold Reference vs. Auto Reference\n=================================\n\nIt is unclear on what data the significance test was performed\nexactly. Did you test on the F0.5 scores? If so, I don't think this is\na good idea since it is a derived measure with weak discriminative\npower (the performance in terms of recall an precision can be totally\ndifferent but have the same F0.5 score). Also, at the beginning of\nSection 4.1 the authors refer to the mismatch between automatic and\nreference in terms of alignment and classification but as far as I can\ntell, the comparison between gold and reference is only in terms of\nboundaries and not in terms of classification.\n\nError Type Evaluation\n=====================\n\nI do not think it is surprising that 5 teams (~line 473) failed to correct\nany unnecessary token error. For at least two of the systems there is\na straightforward explanation why they cannot handle superfluous\nwords. The most obvious is UFC: Their rule-base approach works on POS\ntags (Ng et al., 2014) and it is just not possible to determine\nsuperfluous words based on POS alone. Rozovskaya & Roth (2016) provide\nan explanation why AMU performs poorly on superfluous words.\n\nThe authors do not analyze or comment the results in Table 6 with\nrespect to whether the systems were designed to handle the error\ntype. For some error types, there is a straight-forward mapping\nbetween error type in the gold standard and in the auto reference, for\nexample for word order error. It remains unclear whether the systems\nfailed completely on specific error types or were just not designed to\ncorrect them (CUUI for example is reported with precision+recall=0.0,\nalthough it does not target word order errors). In the CUUI case (and\nthere are probably similar cases), this also points at an error in the\nclassification which is neither analyzed nor discussed.\n\nPlease report also raw values for TP, FP, TN, FN in the appendix for\nTable 6. This makes it easier to compare the systems using other\nmeasures. Also, it seems that for some error types and systems the\nresults in Table 6 are based only on a few instances. This would also\nbe made clear when reporting the raw values.\n\nYour write \"All but 2 teams (IITB and IPN) achieved the best score in\nat least 1 category, which suggests that different approaches to GEC\ncomplement different error types.\" (606) It would be nice to mention\nhere that this is in line with previous research.\n\nMulti-token error analysis is helpful for future work but the result\nneeds more interpretation: Some systems are probably inherently unable\nto correct such errors but none of the systems were trained on a\nparallel corpus of learner data and fluent (in the sense of Sakaguchi\net al, 2016) corrections.\n\nOther\n=====\n\n- The authors should have mentioned that for some of the GEC\n  approaches, it was not impossible before to provide error\n  annotations, e.g. systems with submodules for one error type each.\n  Admittedly, the system would need to be adapted to include the\n  submodule responsible for a change in the system output. Still, the\n  proposed approach enables to compare GEC systems for which producing\n  an error tagged output is not straightforward to other systems in a\n  unified way.\n- References: Some titles lack capitalizations. URL for Sakaguchi et\n  al. (2016) needs to be wrapped. Page information is missing for\n  Efron and Tibshirani (1993).\n\nAuthor response\n===============\n\nI agree that your approach is not \"fatally flawed\" and I think this review\nactually points out quite some positive aspects. The approach is good, but the\npaper is not ready.\n\nThe basis for the paper are the rules for classifying errors and the lack of\ndescription is a major factor.        This is not just a matter about additional\nexamples. If the rules are not seen as a one-off implementation, they need to\nbe described to be replicable or to adapt them.\n\nGeneralization to other languages should not be an afterthought.  It would be\nserious limitation if the approach only worked on one language by design.  Even\nif you don't perform an adaption for other languages, your approach should be\ntransparent enough for others to estimate how much work such an adaptation\nwould be and how well it could reasonably work.  Just stating that most\nresearch is targeted at ESL only reinforces the problem.\n\nYou write that the error types certain systems tackle would be \"usually obvious\nfrom the tables\".  I don't think it is as simple as that -- see the CUUI\nexample mentioned above as well as the unnecessary token errors.  There are\nfive systems that don't correct them (Table 5) and it should therefore be\nobvious that they did not try to tackle them. However, in the paper you write\nthat \"There\nis also no obvious explanation as to why these teams had difficulty with this\nerror type\".", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}], "abstract": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as ``Good'' or ``Acceptable'' in at least 95\\% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.", "histories": [], "id": 169, "title": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction"}
