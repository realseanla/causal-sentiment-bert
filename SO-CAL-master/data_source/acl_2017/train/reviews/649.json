{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThis paper proposes an evaluation metric for automatically evaluating the\nquality of dialogue responses in non-task-oriented dialogue. The metric\noperates on continuous vector space representations obtained by using RNNs and\nit comprises two components: one that compares the context and the given\nresponse and the other that compares a reference response and the given\nresponse. The comparisons are conducted by means of dot product after\nprojecting the response into corresponding context and reference response\nspaces. These projection matrices are learned by minimizing the squared error\nbetween the model predictions and human annotations.\n\nI think this work gives a remarkable step forward towards the evaluation of\nnon-task-oriented dialogue systems. Different from previous works in this area,\nwhere pure semantic similarity was pursued, the authors are going beyond pure\nsemantic similarity in a very elegant manner by learning projection matrices\nthat transform the response vector into both context and reference space\nrepresentations. I am very curious on how your projection matrices M and N\ndiffer from the original identity initialization after training the models. I\nthink the paper will be more valuable if further discussion on this is\nintroduced, rather than focusing so much on resulting correlations. \n\n- Weaknesses:\n\nThe paper also leaves lots questions related to the implementation. For\ninstance, it is not clear whether the human scores used to train and evaluate\nthe system were single AMT annotations or the resulting average of few\nannotations. Also, it is not clear how the dataset was split into\ntrain/dev/test and whether n-fold cross validation was conducted or not. Also,\nit would be nice to better explain why in table 2 correlation for ADEM related\nscores are presented for the validation and test sets, while for the other\nscores they are presented for the full dataset and test set. The section on\npre-training with VHRED is also very clumsy and confusing, probably it is\nbetter to give less technical details but a better high level explanation of\nthe pre-training strategy and its advantages.\n\n- General Discussion:\n\n\u201cThere are many obvious cases where these metrics fail, as they are often\nincapable of considering the semantic similarity between responses (see Figure\n1).\u201d Be careful with statements like this one. This is not a problem of\nsemantic similarity! Opposite to it, the problem is that completely different\nsemantic cues might constitute pragmatically valid responses. Then, semantic\nsimilarity itself is not enough to evaluate a dialogue system response.\nDialogue system response evaluation must go beyond semantics (This is actually\nwhat your M and N matrices are helping to do!!!) \n\n\u201can accurate model that can evaluate dialogue response quality automatically\n\u2014 what could be considered an automatic Turing test \u2014\u201c The original\nintention of Turing test was to be a proxy to identify/define intelligent\nbehaviour. It actually proposes a test on intelligence based on an\n\u201cintelligent\u201d machine capability to imitate human behaviour in such a way\nthat it would be difficult for a common human to distinguish between such a\nmachine responses and actual human responses. It is of course related to\ndialogue system performance, but I think it is not correct to say that\nautomatically evaluating dialogue response quality is an automatic Turing test.\nActually, the title itself \u201cTowards an Automatic Turing Test\u201d is somehow\nmisleading!\n\n\u201cthe simplifying assumption that a \u2018good\u2019 chatbot is one whose responses\nare scored highly on appropriateness by human evaluators.\u201d This is certainly\nthe correct angle to introduce the problem of non-task-oriented dialogue\nsystems, rather than \u201cTuring Test\u201d. Regarding this, there has been related\nwork you might like to take a look at, as well as to make reference to, in the\nWOCHAT workshop series (see the shared task description and corresponding\nannotation guidelines).\n\nIn the discussion session: \u201cand has has been used\u201d -> \u201cand it has been\nused\u201d", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}], "abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem.  Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem.We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores.   We show that the ADEM model\u2019s predictions correlate significantly,  and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training,                    an important step for automatic dialogue evaluation.", "histories": [], "id": 649, "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses"}
