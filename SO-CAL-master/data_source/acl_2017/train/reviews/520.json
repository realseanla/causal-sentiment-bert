{"reviews": [{"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes a method for generating datasets of pictures from simple\nbuilding blocks, as well as corresponding logical forms and language\ndescriptions.\nThe goal seems to be to have a method where the complexity of pictures and\ncorresponding desciptions can be controlled and parametrized. \n\n - The biggest downside seems to be that the maximally achievable complexity is\nvery limited, and way below the complexity typically faced with\nimage-captioning and other multimodal tasks. \n - The relative simplicity is also a big difference to the referenced bAbI\ntasks (which cover the whole qualitative spectrum of easy-to-hard reasoning\ntasks), whereas in the proposed method a (qualitatively) easy image reconition\ntask can only be quantitatively made harder, by increasing the number of\nobjects, noise etc in unnatural ways.\n - This is also reflected in the experimental section. Whenever the\nexperimental performance results are not satisfying, these cases seem like\nbasic over/underfitting issues that may easily be tackled by\nrestricting/extending the capacity of the networks or using more data. It is\nhard for me to spot any other qualitative insight.\n - In the introduction it is stated that the \"goal is not too achieve optimal\nperformance\" but to find out whether \"architectures are able to successfully\ndemonstrate the desired understanding\" - there is a fundamental contradiction\nhere, in that the proposed task on the one side is meant to provide a measure\nas to whether architectures demontrate \"understanding\", on the other hand the\nscore is not supposed to be taken as meaningful/seriously.\n\nGeneral comments:\nThe general approach should be made more tangible earlier (i.e. in the\nintroction rather than in section 3)", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThe authors introduce a new software package called ShapeWorld for\nautomatically generating data for image captioning problems. The microworld\nused to generate the image captions is simple enough to make the data being\ngenerated and errors by a model readily interpretable. However, the authors\ndemonstrate that configurations of the packages produce data that is\nchallenging enough to serve as a good benchmark for ongoing research.\n\n- Weaknesses:\n\nThe primary weakness of this paper is that it does look a bit like a demo\npaper. The authors do provides experiments that evaluate a reasonable baseline\nimage captioning system on the data generated by ShapeWorld. However, similar\nexperiments are included in demo papers.\n\nThe paper includes a hyperlink to the software package on github that\npresumably unmasks the authors of the paper.\n\n- General Discussion:\n\nScientific progress often involves some something analogous to vygotsky's zone\nof proximal development, whereby progress can be made more quickly if research\nfocuses on problems with just the right level of difficulty (e.g., the use of\ntidigits for speech recognition research in the early 90s). This paper is\nexciting since it offers a simple microworld that is easy for researchers to\ncompletely comprehend but that also is just difficult enough for existing\nmodels.\n\nThe strengths of the work are multiplied by the fact that the software is\nopensource, is readily available on github and generates the data in a format\nthat can be easily used with models built using modern deep learning libraries\n(e.g., TensorFlow).\n\nThe methods used by the software package to generate the artificial data are\nclearly explained. It is also great that the authors did experiments with\ndifferent configurations of their software and a baseline image caption model\nin order to demonstrate the strengths and weakness of existing techniques. \n\nMy only real concern with this paper is whether the community would be better\nserved by placing it in the demo section. Publishing it in the non-demo long\npaper track might cause confusion as well as be unfair to authors who correctly\nsubmitted similar papers to the ACL demo track.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}], "abstract": "We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter's specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created which require generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating a multimodal architecture on four different tasks, and show that our framework gives us insights into the model's capabilities and limitations.", "histories": [], "id": 520, "title": "ShapeWorld: A new test methodology for multimodal language understanding"}
