{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Contents:\nThis paper proposes a new task, and provides a dataset. The task is to predict\nblanked-out named entities from a text with the help of an external\ndefinitional resource, in particular FreeBase. These named entities are\ntypically rare, that is, they do not appear often in the corpus, such that it\nis not possible to train models specifically for each entity. The paper argues\nconvincingly that this is an important setting to explore. Along with multiple\nbaselines, two neural network models for the problem are presented that make\nuse of the external resource, one of which also accumulates evidence across\ncontexts in the same text. \n\n- Strengths:\n\nThe collection of desiderata for the task is well-chosen to advance the field:\npredicting blanked-out named entities, a task that has already shown to be\ninteresting in the CNN/Daily Mail dataset, but in a way that makes the task\nhard for language models; and the focus on rare entities should drive the field\ntowards more interesting models. \n\nThe collection of baselines is well chosen to show that neither a NN model\nwithout external knowledge nor a simple cosine similarity based model with\nexternal knowledge can do the task well.\n\nThe two main models are chosen well.\n\nThe text is clear and well argued. \n\n- Weaknesses:\n\nI was a bit puzzled by the fact that using larger contexts, beyond the\nsentences with blanks in them, did not help the models. After all, you were in\na way using additional context in the HierEnc model, which accumulates\nknowledge from other contexts. There are two possible explanations: Either the\nsentences with blanks in them are across the board more informative for the\ntask than the sentences without. This is the explanation suggested in the\npaper, but it seems a bit unintuitive that this should be the case. Another\npossible explanation is that the way that you were using additional context in\nHierEnc, using the temporal network, is much more useful than by enlarging\nindividual contexts C and feeding that larger C into the recurrent network.  Do\nyou think that that could be what is going on?\n\n- General Discussion:\n\nI particularly like the task and the data that this paper proposes. This setup\ncan really drive the field forward, I think. This in my mind is the main\ncontribution.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- General Discussion:\n\nThe paper deals with the task of predicting missing entities in a given context\nusing the Freebase definitions of those entities. The authors highlight the\nimportance of the problem, given that the entities come from a long-tailed\ndistribution. They use popular sequence encoders to encode the context and the\ndefinitions of candidate entities, and score them based on their similarity\nwith the context. While it is clear that the task is indeed important, and the\ndataset may be useful as a benchmark, the approach has some serious weaknesses\nand the evaluation leaves some questions unanswered. \n\n- Strengths:\n\nThe proposed task requires encoding external knowledge, and the associated\ndataset may serve as a good benchmark for evaluating hybrid NLU systems.\n\n- Weaknesses:\n\n1) All the models evaluated, except the best performing model (HIERENC), do not\nhave access to contextual information beyond a sentence. This does not seem\nsufficient to predict a missing entity. It is unclear whether any attempts at\ncoreference and anaphora resolution have been made. It would generally help to\nsee how well humans perform at the same task.\n\n2) The choice of predictors used in all models is unusual. It is unclear why\nsimilarity between context embedding and the definition of the entity is a good\nindicator of the goodness of the entity as a filler.\n\n3) The description of HIERENC is unclear. From what I understand, each input\n(h_i) to the temporal network is the average of the representations of all\ninstantiations of context filled by every possible entity in the vocabulary.\nThis does not seem to be a good idea since presumably only one of those\ninstantiations is correct. This would most likely introduce a lot of noise.\n\n4) The results are not very informative. Given that this is a rare entity\nprediction problem, it would help to look at type-level accuracies, and \nanalyze how the accuracies of the proposed models vary with frequencies of\nentities.\n\n- Questions to the authors:\n\n1) An important assumption being made is that d_e are good replacements for\nentity embeddings. Was this assumption tested?\n\n2) Have you tried building a classifier that just takes h_i^e as inputs?\n\nI have read the authors' responses. I still think the task+dataset could\nbenefit from human evaluation. This task can potentially be a good benchmark\nfor NLU systems, if we know how difficult the task is. The results presented in\nthe paper are not indicative of this due to the reasons stated above. Hence, I\nam not changing my scores.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\nThe paper empirically verifies that using external knowledge is a benefit.\n\n- Weaknesses:\nReal world NLP applications should utilize external knowledge for making better\npredictions. The authors propose Rare Entity prediction task to demonstrate\nthis is the case. However, the motivation of the task is not fully justified.\nWhy is this task important? How would real world NLP applications benefit from\nthis task? The paper lacks a convincing argument for proposing a new task. For\ncurrent reading comprehension task, the evidence for a correct answer can be\nfound in a given text, thus we are interested in learning a model of the world\n(i.e causality for example), or a basic reasoning model. Comparing to reading\ncomprehension, rare entity prediction is rather unrealistic as humans are\nterrible with remembering name. The authors mentioned that the task is\ndifficult due to the large number of rare entities, however challenging tasks\nwith the same or even more difficult level exist, such as predicting correct\nmorphological form of a word in morphologically rich languages. Such tasks have\nobvious applications in machine translation for example.\n\n- General Discussion:\nIt would be helpful if the authors characterize the dataset in more details.\nFrom figure 1 and table 4, it seems to me that overlapping entities is an\nimportant feature. There is noway i can predict the **blank** in figure 1 if I\ndon't see the word London in Peter Ackoyd description. That's being said,\nbefore brutalizing neural networks, it is essential to understand the\ncharacteristic of the data and the cognitive process that searches for the\nright answer.\n\nGiven the lack of characteristic of the dataset, I find that the baselines are\ninappropriate. First of all, the CONTENC is a natural choice at the first sigh.\nHowever as the authors mentioned that candidate entities are rare, the\nembeddings of those entities are unrealizable. As a consequence, it is expected\nthat CONTENC doesn't work well. Would it is fairer if the embeddings are\ninitialized from pre-trained vectors on massive dataset? One would expect some\nsort of similarity between Larnaca and Cyprus in the embedding space and\nCONTENC would make a correct prediction in Table 4. What would be the\nperformance of TF-IDF+COS and AVGEMB+COS if only entities are used to compute\nthose vectors?\n\nFrom modeling perspective, I appreciate that the authors chose a sigmoid\npredictor that output a numerical score between (0,1). This would help avoiding\nnormalization over the list of candidates, which are rare and is difficult to\nlearn reliable weights for those. However, a sidestep technique does exist,\nsuch as Pointer Network. A representation h_i for C_i (*blank* included) can be\ncomputed by an LSTM or BiLSTM, then Pointer Network would give a probabilistic\ninterpretation p(e_k|C_i) \\propto exp(dot(d_{e_k}, h_i)). In my opinion,\nPointer Network would be an appropriate baseline. Another related note: Does\nthe unbalanced set of negative/positive labels affect the training? During\ntraining, the models make 1 positive prediction while number of negative\npredictions is at least 4 times higher?\n\nWhile I find the task of Rare Entity prediction is unrealistic, having the\ndataset, it would be more interesting to learn about the reasoning process that\nleads to the right answer such as which set of words the model attends to when\nmaking prediction.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Reading comprehension in NLP refers to the ability of models to answer any question about a passage accurately. An important open problem is how to effectively use external knowledge to answer such questions. In this paper, we introduce a new task and derive new models to drive progress towards this goal. In particular, we propose the task of rare entity prediction: given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This task is challenging due to the diversity of language styles and the extremely large number of rare entities. Our experiments show that models that make use of external knowledge in the form of lexical resources, particularly our model using hierarchical LSTMs, perform significantly better at rare entity prediction than those that do not.", "histories": [], "id": 588, "title": "Rare Entity Prediction: Language Understanding with External Knowledge using Hierarchical LSTMs"}
