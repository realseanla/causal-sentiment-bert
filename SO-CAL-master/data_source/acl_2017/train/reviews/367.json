{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThe paper addresses a long standing problem concerning automatic evaluation of\nthe output of generation/translation systems.\n\nThe analysis of all the available metrics is thorough and comprehensive.\n\nThe authors demonstrate a new metric with a higher correlation with human\njudgements\n\nThe bibliography will help new entrants into the field.\n\n- Weaknesses:\n\nThe paper is written as a numerical analysis paper, with very little insights\nto linguistic issues in generation, the method of generation, the differences\nin the output from a different systems and human generated reference.\n\nIt is unclear if the crowd source generated references serve well in the\ncontext of an application that needs language generation.\n\n- General Discussion:\n\nOverall, the paper could use some linguistic examples (and a description of the\ndifferent systems) at the risk of dropping a few tables to help the reader with\nintuitions.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU. In this paper, we investigate a wide range of these metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. A detailed error analysis shows that automatic metrics are particularly bad in distinguishing outputs of medium and good quality, which can be partially attributed to the fact that human judgements and metrics are given on different scales. We also show that metric performance is data and system specific.  We then suggest an alternative metric, called RAINBOW, combining the individual strengths of different automatic scores. This new metric achieves up to rho=.81 correlation with human judgements on the sentence level (compared to a maximum of rho=.33  for existing metrics) and achieves stable results across systems and data sets.", "histories": [], "id": 367, "title": "From BLEU to RAINBOW: Why We Need New Metrics for NLG."}
