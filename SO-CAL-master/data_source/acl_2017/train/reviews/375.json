{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the latent\nrepresentations, and the mutual attention approach seems sensible.\n\nA relatively thorough evaluation is provided, with multiple datasets,\nbaselines, and evaluation tasks.\n\n- Weaknesses:\n\nLike many other papers in the \"network embedding\" literature, which use neural\nnetwork techniques inspired by word embeddings to construct latent\nrepresentations of nodes in a network, the previous line of work on\nstatistical/probabilistic modeling of networks is ignored.  In particular, all\n\"network embedding\" papers need to start citing, and comparing to, the work on\nthe latent space model of Peter Hoff et al., and subsequent papers in both\nstatistical and probabilistic machine learning publication venues:\n\nP.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social\nnetwork analysis. J. Amer. Statist. Assoc., 97(460):1090\u20131098, 2002.\n\nThis latent space network model, which embeds each node into a low-dimensional\nlatent space, was written as far back as 2002, and so it far pre-dates neural\nnetwork-based network embeddings.\n\nGiven that the aim of this paper is to model differing representations of\nsocial network actors' different roles, it should really cite and compare to\nthe mixed membership stochastic blockmodel (MMSB):\n\nAiroldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed\nmembership stochastic blockmodels. Journal of Machine Learning Research.\n\nThe MMSB allows each node to randomly select a different \"role\" when deciding\nwhether to form each edge.\n\n- General Discussion:\n\nThe aforementioned statistical models do not leverage text, and they do not use\nscalable neural network implementations based on negative sampling, but they\nare based on well-principled generative models instead of heuristic neural\nnetwork objective functions and algorithms.  There are more recent extensions\nof these models and inference algorithms which are more scalable, and which do\nleverage text.\n\nIs the difference in performance between CENE and CANE in Figure 3\nstatistically insignificant? (A related question: were the experiments repeated\nmore than once with random train/test splits?)\n\nWere the grid searches for hyperparameter values, mentioned in Section 5.3,\nperformed with evaluation on the test set (which would be problematic), or on a\nvalidation set, or on the training set?", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification. The source code and datasets can be obtained from \\url{https://github.com/thunlp/CANE}.", "histories": [], "id": "375", "title": "CANE: Context-Aware Network Embedding for Relation Modeling"}
