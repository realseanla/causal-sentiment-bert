{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "Update after author response: \n\n1. My major concern about the optimization of model's hyperparameter (which are\nnumerous) has not been addressed. This is very important, considering that you\nreport results from folded cross-validation. \n\n2. The explanation that benefits of their method are experimentally confirmed\nwith 2% difference -- while evaluating via 5-fold CV on 200 examples -- is\nquite unconvincing.\n\n========================================================================\n\nSummary:\n\nIn this paper authors present a complex neural model for detecting factuality\nof event mentions in text. The authors combine the following in their complex\nmodel:                          (1) a set of traditional classifiers for detecting\nevent\nmentions,\nfactuality sources, and source introducing predicates (SIPs), (2) A\nbidirectional attention-based LSTM model that learns latent representations for\nelements on different dependency paths used as input, (2) A CNN that uses\nrepresentations from the LSTM and performs two output predictions (one to\ndetect specific from underspecified cases and another to predict the actual\nfactuality class). \n\nFrom the methodological point of view, the authors are combining a reasonably\nfamiliar methods (att-BiLSTM and CNN) into a fairly complex model. However,\nthis model does not take raw text (sequence of word embeddings) as input, but\nrather hand-crafted features (e.g., different dependency paths combining\nfactuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted\nfeatures is somewhat surprising if coupled with complex deep model. The\nevaluation seems a bit tainted as the authors report the results from folded\ncross-validation but do not report how they optimized the hyperparameters of\nthe model. Finally, the results are not too convincing -- considering the\ncomplexity of the model and the amount of preprocessing required (extraction of\nevent mentions, SIPs, and clues), a 2% macro-average gain over the rule-based\nbaseline and overall 44% performance seems modest, at best (looking at\nMicro-average, the proposed model doesn't outperform simple MaxEnt classifier).\n\nThe paper is generally well-written and fairly easy to understand. Altogether,\nI find this paper to be informative to an extent, but in it's current form not\na great read for a top-tier conference.   \n\nRemarks:\n\n1. You keep mentioning that the LSTM and CNN in your model are combined\n\"properly\" -- what does that actually mean? How does this \"properness\"\nmanifest? What would be the improper way to combine the models?\n\n2. I find the motivation/justification for the two output design rather weak: \n    - the first argument that it allows for later addition of cues (i.e\nmanually-designed features) kind of beats the \"learning representations\"\nadvantage of using deep models. \n        - the second argument about this design tackling the imbalance in the\ntraining set is kind of hand-wavy as there is no experimental support for this\nclaim. \n\n3. You first motivate the usage of your complex DL architecture with learning\nlatent representations and avoiding manual design and feature computation.  And\nthen you define a set of manually designed features (several dependency paths\nand lexical features) as input for the model. Do you notice the discrepancy? \n\n4. The LSTMs (bidirectional, and also with attention) have by now already\nbecome a standard model for various NLP tasks. Thus I find the detailed\ndescription of the attention-based bidirectional LSTM unnecessary. \n5. What you present as a baseline in Section 3 is also part of your model (as\nit generates input to your model). Thus, I think that calling it a baseline\nundermines the understandability of the paper. \n\n6. The results reported originate from a 5-fold CV. However, the model contains\nnumerous hyperparameters that need to be optimized (e.g., number of filters and\nfilter sizes for CNNs). How do you optimize these values? Reporting results\nfrom a folded cross-validation doesn't allow for a fair optimization of the\nhypeparameters: either you're not optimizing the model's hyperparameters at\nall, or you're optimizing their values on the test set (which is unfair). \n\n7. \"Notice that some values are non-application (NA) grammatically, e.g., PRu,\nPSu, U+/-\" -- why is underspecification in ony one dimension (polarity or\ncertainty) not an option? I can easily think of a case where it is clear the\nevent is negative, but it is not specified whether the absence of an event is\ncertain, probable, or possible. \n\nLanguage & style:\n\n1. \"to a great degree\" -> \"great degree\" is an unusual construct, use either\n\"great extent\" or \"large degree\"\n2. \"events that can not\" -> \"cannot\" or \"do not\"\n3. \"describes out networks...in details shown in Figure 3.\" -> \"...shown in\nFigure 3 in details.\"", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "Comments after author response\n\n- Thank you for clarifying that the unclear \"two-step framework\" reference was\nnot about the two facets. I still do not find this use of a pipeline to be a\nparticularly interesting contribution.\n- You state that \"5. de Marneffe (2012) used additional annotated features in\ntheir system. For fair comparison, we re-implement their system with annotated\ninformation in FactBank.\" But the de Marneffe et al. feature cited in the\npaper, \"Predicate Classes\" requires only a dependency parser and vocabulary\nlists from Roser Saur\u00ed's PhD thesis; \"general classes of event\" might be\nreferring to FactML event classes, and while I admit it is not particularly\nclear in their work, I am sure they could clarify.\n- I continue to find the use of \"combined properly\" to be obscure. I agree that\nusing LSTM and CNN where respectively appropriate is valuable, but you seem to\nimply that some prior work has been improper, and that it is their combination\nwhich must be proper.\n- Thank you for reporting on separate LSTMs for each of the paths. I am curious\nas to why this combination may less effective. In any case, experiments with\nthis kind of alternative structure deserve to be reported.\n\n---\n\nThis paper introduces deep neural net technologies to the task of factuality\nclassification as defined by FactBank, with performance exceeding alternative\nneural net models and baselines reimplemented from the literature.\n\n- Strengths:\n\nThis paper is very clear in its presentation of a sophisticated model for\nfactuality classification and of its evaluation.  It shows that the use of\nattentional features and BiLSTM clearly provide benefit over alternative\npooling strategies, and that the model also exceeds the performance of a more\ntraditional feature-based log-linear model.  Given the small amount of training\ndata in FactBank, this kind of highly-engineered model seems appropriate. It is\ninteresting to see that the BiLSTM/CNN model is able to provide benefit despite\nlittle training data.\n\n- Weaknesses:\n\nMy main concerns with this work regard its (a) apparent departure from the\nevaluation procedure in the prior literature; (b) failure to present prior work\nas a strong baseline; and (c) novelty.\n\nWhile I feel that the work is original in engineering deep neural nets for the\nfactuality classification task, and that such work is valuable, its approach is\nnot particularly novel, and \"the proposal of a two-step supervised framework\"\n(line 087) is not particularly interesting given that FactBank was always\ndescribed in terms of two facets (assuming I am correct to interpret \"two-step\"\nas referring to these facets, which I may not be).\n\nThe work cites Saur\u00ed and Pustejovsky (2012), but presents their much earlier\n(2008) and weaker system as a baseline; nor does it consider Qian et al.'s\n(IALP 2015) work which compares to the former.              Both these works are\ndeveloped\non the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT\nTimeBank section, while the present work does not report results on a held-out\nset.\n\nde Marneffe et al.'s (2012) system is also chosen as a baseline, but not all\ntheir features are implemented, nor is the present system evaluated on their\nPragBank corpus (or other alternative representations of factuality proposed in\nPrabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)).  The evaluation is\ntherefore somewhat lacking in comparability to prior work.\n\nThere were also important questions left unanswered in evaluation, such as the\neffect of using gold standard events or SIPs.\n\nGiven the famed success of BiLSTMs with little feature engineering, it is\nsomewhat disappointing that this work does not attempt to consider a more\nminimal system employing deep neural nets on this task with, for instance, only\nthe dependency path from a candidate event to its SIP plus a bag of modifiers\nto that path. The inclusion of heterogeneous information in one BiLSTM was an\ninteresting feature, which deserved more experimentation: what if the order of\ninputs were permuted? what if delimiters were used in concatenating the\ndependency paths in RS instead of the strange second \"nsubj\" in the RS chain of\nline 456? What if each of SIP_path, RS_path, Cue_path were input to a separate\nLSTM and combined? The attentional features were evaluated together for the CNN\nand BiLSTM components, but it might be worth reporting whether it was\nbeneficial for each of these components. Could you benefit from providing path\ninformation for the aux words? Could you benefit from character-level\nembeddings to account for morphology's impact on factuality via tense/aspect?\nProposed future work is lacking in specificity seeing as there are many\nquestions raised by this model and a number of related tasks to consider\napplying it to.\n\n- General Discussion:\n\n194: Into what classes are you classifying events?\n\n280: Please state which are parameters of the model.\n\n321: What do you mean by \"properly\"? You use the same term in 092 and it's not\nclear which work you consider improper nor why.\n\n353: Is \"the chain form\" defined anywhere? Citation? The repetition of nsubj in\nthe example of line 456 seems an unusual feature for the LSTM to learn.\n\n356: It may be worth footnoting here that each cue is classified separately.\n\n359: \"distance\" -> \"surface distance\"\n\n514: How many SIPs? Cues? Perhaps add to Table 3.\n\nTable 2. Would be good if augmented by the counts for embedded and author\nevents. Percentages can be removed if necessary.\n\n532: Why 5-fold? Given the small amount of training data, surely 10-fold would\nbe more useful and not substantially increase training costs.\n\n594: It's not clear that this benefit comes from PSen, nor that the increase is\nsignificant or substantial.  Does it affect overall results substantially?\n\n674: Is this significance across all metrics?\n\n683: Is the drop of F1 due to precision, recall or both?\n\n686: Not clear what this sentence is trying to say.\n\nTable 4: From the corpus sizes, it seems you should only report 2 significant\nfigures for most columns (except CT+, Uu and Micro-A).\n\n711: It seems unsurprising that RS_path is insufficient given that the task is\nwith respect to a SIP and other inputs do not encode that information. It would\nbe more interesting to see performance of SIP_path alone.\n\n761: This claim is not precise, to my understanding. de Marneffe et al (2012)\nevaluates on PragBank, not FactBank.\n\nMinor issues in English usage:\n\n112: \"non-application\" -> \"not applicable\"\n\n145: I think you mean \"relevant\" -> \"relative\"\n\n154: \"can be displayed by a simple source\" is unclear\n\n166: Not sure what you mean by \"basline\". Do you mean \"pipeline\"?", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposes a supervised deep learning model for event factuality\nidentification.  The empirical results show that the model outperforms\nstate-of-the-art systems on the FactBank corpus, particularly in three classes\n(CT-, PR+ and PS+).  The main contribution of the paper is the proposal of an\nattention-based two-step deep neural model for event factuality identification\nusing bidirectional long short-term memory (BiLSTM) and convolutional neural\nnetwork (CNN).\n\n[Strengths:]\n\n- The structure of the paper is (not perfectly but) well organized.\n\n- The empirical results show convincing (statistically significant) performance\ngains of the proposed model over strong baseline.\n\n[Weaknesses:]\n\nSee below for details of the following weaknesses:\n\n- Novelties of the paper are relatively unclear.\n\n- No detailed error analysis is provided.\n\n- A feature comparison with prior work is shallow, missing two relevant papers.\n\n- The paper has several obscure descriptions, including typos.\n\n[General Discussion:]\n\nThe paper would be more impactful if it states novelties more explicitly.  Is\nthe paper presenting the first neural network based approach for event\nfactuality identification?  If this is the case, please state that.\n\nThe paper would crystallize remaining challenges in event factuality\nidentification and facilitate future research better if it provides detailed\nerror analysis regarding the results of Table 3 and 4.              What are dominant\nsources of errors made by the best system BiLSTM+CNN(Att)?  What impacts do\nerrors in basic factor extraction (Table 3) have on the overall performance of\nfactuality identification (Table 4)?  The analysis presented in Section 5.4 is\nmore like a feature ablation study to show how useful some additional features\nare.\n\nThe paper would be stronger if it compares with prior work in terms of\nfeatures.  Does the paper use any new features which have not been explored\nbefore?  In other words, it is unclear whether main advantages of the proposed\nsystem come purely from deep learning, or from a combination of neural networks\nand some new unexplored features.  As for feature comparison, the paper is\nmissing two relevant papers:\n\n- Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection\nand Factuality Assessment with Non-Expert Supervision. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Language Processing, pages\n1643-1648.\n\n- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014.\nModeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics, pages 415-420.\n\nThe paper would be more understandable if more examples are given to illustrate\nthe underspecified modality (U) and the underspecified polarity (u).  There are\ntwo reasons for that.  First, the definition of 'underspecified' is relatively\nunintuitive as compared to other classes such as 'probable' or 'positive'. \nSecond, the examples would be more helpful to understand the difficulties of Uu\ndetection reported in line 690-697.  Among the seven examples (S1-S7), only S7\ncorresponds to Uu, and its explanation is quite limited to illustrate the\ndifficulties.\n\nA minor comment is that the paper has several obscure descriptions, including\ntypos, as shown below:\n\n- The explanations for features in Section 3.2 are somewhat intertwined and\nthus confusing.  The section would be more coherently organized with more\nseparate paragraphs dedicated to each of lexical features and sentence-level\nfeatures, by:\n\n  - (1) stating that the SIP feature comprises two features (i.e.,\nlexical-level\nand sentence-level) and introduce their corresponding variables (l and c) *at\nthe beginning*;\n\n  - (2) moving the description of embeddings of the lexical feature in line\n280-283\nto the first paragraph; and\n\n  - (3) presenting the last paragraph about relevant source identification in a\nseparate subsection because it is not about SIP detection.\n\n- The title of Section 3 ('Baseline') is misleading.  A more understandable\ntitle would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because\nthe section is about how to extract basic factors (features), not about a\nbaseline end-to-end system for event factuality identification.\n\n- The presented neural network architectures would be more convincing if it\ndescribes how beneficial the attention mechanism is to the task.\n\n- Table 2 seems to show factuality statistics only for all sources.  The table\nwould be more informative along with Table 4 if it also shows factuality\nstatistics for 'Author' and 'Embed'.\n\n- Table 4 would be more effective if the highest system performance with\nrespect to each combination of the source and the factuality value is shown in\nboldface.\n\n- Section 4.1 says, \"Aux_Words can describe the *syntactic* structures of\nsentences,\" whereas section 5.4 says, \"they (auxiliary words) can reflect the\n*pragmatic* structures of sentences.\"  These two claims do not consort with\neach other well, and neither of them seems adequate to summarize how useful the\ndependency relations 'aux' and 'mark' are for the task.\n\n- S7 seems to be another example to support the effectiveness of auxiliary\nwords, but the explanation for S7 is thin, as compared to the one for S6.  What\nis the auxiliary word for 'ensure' in S7?\n\n- Line 162: 'event go in S1' should be 'event go in S2'.\n\n- Line 315: 'in details' should be 'in detail'.\n\n- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.\n\n- Line 771: 'recent researches' should be 'recent research' or 'recent\nstudies'.  'Research' is an uncountable noun.\n\n- Line 903: 'Factbank' should be 'FactBank'.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "2", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Event factuality identification plays an important role in deep NLP applications. In this paper, we propose a deep learning framework for this task which first extracts essential information from raw texts as the inputs and then identifies the factuality of events via a deep neural network with a proper combination of Bidirectional Long Short-Term Memory (BiLSTM) neural network and Convolutional Neural Network (CNN). The experimental results on FactBank show that our framework significantly outperforms several state-of-the-art baselines.", "histories": [], "id": 31, "title": "Event Factuality Identification via Deep Neural Networks"}
