{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: 1) an interesting task, 2) the paper is very clearly written, easy\nto follow, 3) the created data set may be\nuseful for other researchers, 4) a detailed analysis of the performance of the\nmodel.\n\n- Weaknesses: 1) no method adapted from related work for a result comparison 2)\nsome explanations about the uniqueness of the task and discussion on\nlimitations of previous research for solving this problem can be added to\nemphasize the research contributions further. \n\n- General Discussion: The paper presents supervised and weakly supervised\nmodels for frame classification in tweets. Predicate rules are generated\nexploiting language-based and Twitter behavior-based signals, which are then\nsupplied to the probabilistic soft logic framework to build classification\nmodels. 17 political frames are classified in tweets in a multi-label\nclassification task. The experimental results demonstrate the benefit of the\npredicates created using the behavior-based signals. Please find my more\nspecific comments below:\n\nThe paper should have a discussion on how frame classification differs from\nstance classification. Are they both under the same umbrella but with different\nlevels of granularity?\n\nThe paper will benefit from adding a brief discussion on how exactly the\ntransition from long congressional speech to short tweets adds to the\nchallenges of the task. For example, does past research rely on any specific\ncross-sentential features that do not apply to tweets? Consider adapting the\nmethod of a frame classification work on\ncongressional speech (or a stance classification work on any text) to the\nextent possible due to its limitations on Twitter data, to compare with the\nresults of this work.\n\nIt seems \u201cweakly supervised\u201d and \u201cunsupervised\u201d \u2013 these two terms\nhave been interchangeably used in the paper (if this is not the case, please\nclarify in author response). I believe \"weakly supervised\" is\nthe\nmore technically correct terminology under the setup of this work that should\nbe used consistently throughout. The initial unlabeled data may not have been\nlabeled by human annotators, but the classification does use weak or noisy\nlabels of some sort, and the keywords do come from experts. The presented\nmethod does not use completely unsupervised data as traditional unsupervised\nmethods such as clustering, topic models or word embeddings would.  \n\nThe calculated Kappa may not be a straightforward reflection of the difficulty\nof\nframe classification for tweets (lines: 252-253), viewing it as a proof is a\nrather strong claim. The Kappa here merely represents the\nannotation difficulty/disagreement. Many factors can contribute to a low value \nsuch as poorly written annotation\nguidelines, selection of a biased annotator, lack of annotator training etc.\n(on\ntop of any difficulty of frame classification for tweets by human annotators,\nwhich the authors actually intend to relate to).\n73.4% Cohen\u2019s Kappa is strong enough for this task, in my opinion, to rely on\nthe annotated labels. \n\nEq (1) (lines: 375-377) will ignore any contextual information (such as\nnegation\nor conditional/hypothetical statements impacting the contributing word) when\ncalculating similarity of a frame and a tweet. Will this have any effect on the\nframe prediction model? Did the authors consider using models that can\ndetermine similarity with larger text units such as perhaps using skip thought\nvectors or vector compositionality methods?  \n\nAn ideal set up would exclude the annotated data from calculating statistics\nused to select the top N bi/tri-grams (line: 397 mentions entire tweets data\nset has been used), otherwise statistics from any test fold (or labeled data in\nthe weakly supervised setup) still leaks into\nthe selection process. I do not think this would have made any difference in\nthe current selection of the bi/tri-grams or results as the size of the\nunlabeled data is much larger, but would have constituted a cleaner\nexperimental setup.  \n\nPlease add precision and recall results in Table 4. \n\nMinor:\nplease double check any rules for footnote placements concerning placement\nbefore or after the punctuation.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: The authors address a very challenging, nuanced problem in\npolitical discourse reporting a relatively high degree of success.\n\nThe task of political framing detection may be of interest to the ACL\ncommunity.\n\nThe paper is very well written.\n\n- Weaknesses: Quantitative results are given only for the author's PSL model\nand not compared against any traditional baseline classification algorithms,\nmaking it unclear to what degree their model is necessary. Poor comparison with\nalternative approaches makes it difficult to know what to take away from the\npaper.\n\nThe qualitative investigation is interesting, but the chosen visualizations are\ndifficult to make sense of and add little to the discussion. Perhaps it would\nmake sense to collapse across individual politicians to create a clearer\nvisual.\n\n- General Discussion: The submission is well written and covers a topic which\nmay be of interest to the ACL community. At the same time, it lacks proper\nquantitative baselines for comparison. \n\nMinor comments:\n\n- line 82: A year should be provided for the Boydstun et al. citation\n\n- It\u2019s unclear to me why similar behavior (time of tweeting) should\nnecessarily be indicative of similar framing and no citation was given to\nsupport this assumption in the model.\n\n- The related work goes over quite a number of areas, but glosses over the work\nmost clearly related (e.g. PSL models and political discourse work) while\nspending too much time mentioning work that is only tangential (e.g.\nunsupervised models using Twitter data).\n\n- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if\nthey used pre-trained embeddings.\n\n- The authors give no intuition behind why unigrams are used to predict frames,\nwhile bigrams/trigrams are used to predict party.\n\n- The authors note that temporal similarity worked best with one hour chunks,\nbut make no mention of how important this assumption is to their results. If\nthe authors are unable to provide full results for this work, it would still be\nworthwhile to give the reader a sense of what performance would look like if\nthe time window were widened.\n\n- Table 4: Caption should make it clear these are F1 scores as well as\nclarifying how the F1 score is weighted (e.g. micro/macro). This should also be\nmade clear in the \u201cevaluation metrics\u201d section on page 6.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.", "histories": [], "id": "727", "title": "Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter"}
