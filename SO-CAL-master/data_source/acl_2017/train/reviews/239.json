{"reviews": [{"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposes a framework for evaluation of word embeddings based on data\nefficiency and simple supervised tasks. The main motivation is that word\nembeddings are generally used in a transfer learning setting, where evaluation\nis done based on how faster is to train a target model. The approach uses a set\nof simple tasks evaluated in a supervised fashion, including common benchmarks\nsuch as word similarity and word analogy. Experiments on a broad set of\nembeddings show that ranks tend to be task-specific and change according to the\namount of training data used.\n\nStrengths\n\n- The transfer learning / data efficiency motivation is an interesting one, as\nit directly relates to the idea of using embeddings as a simple\n\"semi-supervised\" approach.\n\nWeaknesses\n\n- A good evaluation approach would be one that propagates to end tasks.\nSpecifically, if the approach gives some rank R for a set of embeddings, I\nwould like it to follow the same rank for an end task like text classification,\nparsing or machine translation. However, the approach is not assessed in this\nway so it is difficult to trust the technique is actually more useful than what\nis traditionally done.\n- The discussion about injective embeddings seems completely out-of-topic and\ndoes not seem to add to the paper's understanding.\n- The experimental section is very confusing. Section 3.7 points out that the\nanalysis results in answers to questions as \"is it worth fitting syntax\nspecific embeddings even when supervised datset is large?\" but I fail to\nunderstand where in the evaluation the conclusion was made.\n- Still in Section 3.7, the manuscript says \"This hints, that purely\nunsupervised large scale pretraining might not be suitable for NLP\napplications\". This is a very bold assumption and I again fail to understand\nhow this can be concluded from the proposed evaluation approach.\n- All embeddings were obtained as off-the-shelf pretrained ones so there is no\ncontrol over which corpora they were trained on. This limits the validity of\nthe evaluation shown in the paper.\n- The manuscript needs proofreading, especially in terms of citing figures in\nthe right places (why Figure 1, which is on page 3, is only cited in page 6?).\n\nGeneral Discussion\n\nI think the paper starts with a very interesting motivation but it does not\nproperly evaluate if their approach is good or not. As mentioned above, for any\nintrinsic evaluation approach I expect to see some study if the conclusions\npropagate to end tasks and this is not done in the paper. The lack of clarity\nand proofreading in the manuscript also hinders the understanding. In the\nfuture, I think the paper would vastly benefit from some extrinsic studies and\na more controlled experimental setting (using the same corpora to train all\nembeddings, for instance). But in the current state I do not think it is a good\naddition to the conference.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "1", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThis paper proposed an interesting and important metric for evaluating the\nquality of word embeddings, which is the \"data efficiency\" when it is used in\nother supervised tasks.\n\nAnother interesting point in the paper is that the authors separated out three\nquestions: 1) whether supervised task offers more insights to evaluate\nembedding quality; 2) How stable is the ranking vs labeled data set size; 3)\nThe benefit to linear vs non-linear models.\n\nOverall, the authors presented comprehensive experiments to answer those\nquestions, and the results see quite interesting to know for the research\ncommunity.\n\n- Weaknesses:\n\nThe overall result is not very useful for ML practioners in this field, because\nit merely confirms what has been known or suspected, i.e. it depends on the\ntask at hand, the labeled data set size, the type of the model, etc. So, the\nresult in this paper is not very actionable. The reviewer noted that this\ncomprehensive analysis deepens the understanding of this topic.\n\n- General Discussion:\n\nThe paper's presentation can be improved. Specifically: \n\n1) The order of the figures/tables in the paper should match the order they are\nmentioned in the papers. Right now their order seems quite random.\n\n2) Several typos (L250, 579, etc). Please use a spell checker.\n\n3) Equation 1 is not very useful, and its exposition looks strange. It can be\nremoved, and leave just the text explanations.\n\n4) L164 mentions the \"Appendix\", but it is not available in the paper.\n\n5) Missing citation for the public skip-gram data set in L425.\n\n6) The claim in L591-593 is too strong. It must be explained more clearly, i.e.\nwhen it is useful and when it is not.\n\n7) The observation in L642-645 is very interesting and important. It will be\ngood to follow up on this and provide concrete evidence or example from some\nembedding. Some visualization may help too.\n\n8) In L672 should provide examples of such \"specialized word embeddings\" and\nhow they are different than the general purpose embedding.\n\n9) Figuer 3 is too small to read.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Recent practice in word embeddings points towards importance of learning specialized representations.  Additionally, in many applications purely unsupervised training is useful only under limited dataset size. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what \\emph{useful} information is \\emph{easily} accessible. Specifically, we argue that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data should be varied and scores of a \\emph{supervised} model for each subset should be reported (as commonly done in transfer learning).   In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and  brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non--linearly encoded in the embedding space, which questions the cosine--based, unsupervised, evaluation methods.  All results and analysis scripts are available online.", "histories": [], "id": 239, "title": "How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks"}
