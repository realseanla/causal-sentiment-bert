{"reviews": [{"IMPACT": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper compares different ways of inducing embeddings for the task of\npolarity classification. The authors focus on different types of corpora and\nfind that not necessarily the largest corpus provides the most appropriate\nembeddings for their particular task but it is more effective to consider a\ncorpus (or subcorpus) in which a higher concentration of subjective content can\nbe found. The latter type of data are also referred to as \"task-specific data\".\nMoreover, the authors compare different embeddings that combine information\nfrom \"task-specific\" corpora and generic corpora. A combination outperforms\nembeddings just drawn from a single corpus. This combination is not only\nevaluated on English but also on a less resourced language (i.e. Catalan).\n\n- Strengths:\nThe paper addresses an important aspect of sentiment analysis, namely how to\nappropriately induce embeddings for training supervised classifers for polarity\nclassification. The paper is well-structured and well-written. The major claims\nmade by the authors are sufficiently supported by their experiments.\n\n- Weaknesses:\nThe outcome of the experiments is very predictable. The methods that are\nemployed are very simple and ad-hoc. I found hardly any new idea in\nthat paper. Neither are there any significant lessons that the reader learns\nabout embeddings or sentiment analysis. The main idea (i.e. focusing on more\ntask-specific data for training more accurate embeddings) was already published\nin the context of named-entity recognition by Joshi et al. (2015). The\nadditions made in this paper are very incremental in nature.\n\nI find some of the experiments inconclusive as (apparently) no statistical\nsignficance testing between different classifiers has been carried out. In\nTables\n2, 3 and 6, various classifier configurations produce very similar scores. In\nsuch cases, only statistical signficance testing can really give a proper\nindication whether these difference are meaningful. For instance, in Table 3 on\nthe left half reporting results on RT, one may wonder whether there is a\nsignificant difference between \"Wikipedia Baseline\" and any of the\ncombinations. Furthermore, one doubts whether there is any signficant\ndifference between the different combinations (i.e. either using \"subj-Wiki\",\n\"subj-Multiun\" or \"subj-Europarl\") in that table.\nThe improvement by focusing on subjective subsets is plausible in general.\nHowever, I wonder whether in real life, in particular, a situation in which\nresources are sparse this is very helpful. Doing a pre-selection with\nOpinionFinder is some pre-processing step which will not be possible in most\nlanguages other than English. There are no equivalent tools or fine-grained\ndatasets on which such functionality could be learnt. The fact that in the\nexperiments\nfor Catalan, this information is not considered proves that. \n\nMinor details:\n\n- lines 329-334: The discussion of this dataset is confusing. I thought the\ntask is plain polarity classification but the authors here also refer to\n\"opinion holder\" and \"opinion targets\". If these information are not relevant\nto the experiments carried out in this paper, then they should not be mentioned\nhere.\n\n- lines 431-437: The variation of \"splicing\" that the authors explain is not\nvery well motivated. First, why do we need this? In how far should this be more\neffective than simple \"appending\"?\n\n- lines 521-522: How is the subjective information isolated for these\nconfigurations? I assume the authors here again employ OpinionFinder? However,\nthere is no explicit mention of this here.\n\n- lines 580-588: The definitions of variables do not properly match the\nformula (i.e. Equation 3). I do not find n_k in Equation 3.\n\n- lines 689-695: Similar to lines 329-334 it is unclear what precise task is\ncarried out. Do the authors take opinion holders and targets in consideration?\n\n***AFTER AUTHORS' RESPONSE***\nThank you very much for these clarifying remarks.\nI do not follow your explanations regarding the incorporation of opinion\nholders and targets, though.\n\nOverall, I will not change my scores since I think that this work lacks\nsufficient novelty (the things the authors raised in their response are just\ninsufficient to me). This submission is too incremental in nature.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths: An interesting and comprehensive study of the effect of using\nspecial-domain corpora for training word embeddings.  Clear explanation of the\nassumptions, contributions, methodology, and results.  Thorough evaluation of\nvarious aspects of the proposal.\n\n- Weaknesses: Some conclusions are not fully backed up by the numerical\nresults.  E.g., the authors claim that for Catalan, the improvements of using\nspecific corpora for training word vectors is more pronounced than English.  I\nam not sure why this conclusion is made based on the results.  E.g., in Table\n6, none of the combination methods outperform the baseline for the\n300-dimension vectors.\n\n- General Discussion: The paper presents a set of simple, yet interesting\nexperiments that suggest word vectors (here trained using the skip-gram method)\nlargely benefit from the use of relevant (in-domain) and subjective corpora. \nThe paper answers important questions that are of benefit to practitioners of\nnatural language processing.  The paper is also very well-written, and very\nclearly organized.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Current state-of-the-art sentiment analysis techniques rely heavily on pre-trained word embeddings. However, the data used to train these embeddings normally comes from large, generic datasets, such as Wikipedia or GoogleNews, which may not include enough task-specific information to create reliable representations. This paper proposes a method to determine the subjectivity of a corpus using available tools and shows that word embeddings trained on task-specific corpora tend to outperform those trained on generic data. We then examine ways to combine information from generic and task-specific datasets and finally demonstrate that our method can work well for under-resourced languages.", "histories": [], "id": 266, "title": "Improving sentiment classification with task-specific data"}
