{"reviews": [{"IMPACT": "2", "SUBSTANCE": "2", "APPROPRIATENESS": "3", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "This paper describes several ways to encode arbitrarily long sequences of\ndigits using something called the major system. In the major system, each digit\nis mapped to one or more characters representing consonantal phonemes; the\npossible mappings between digit and phoneme are predefined. The output of an\nencoding is typically a sequence of words constrained such that digits in the\noriginal sequence correspond to characters or digraphs in the output sequence\nof words; vowels added surrounding the consonant phonemes to form words are\nunconstrained. This paper describes several ways to encode your sequence of\ndigits such that the output sequence of words is more memorable, generally by\napplying syntactic constraints and heuristics.\n\nI found this application of natural language processing concepts somewhat\ninteresting, as I have not read an ACL paper on this topic before. However, I\nfound the paper and ideas presented here to have a rather old-school feel. With\nmuch of the focus on n-gram models for generation, frequent POS-tag sequences,\nand other heuristics, this paper really could have been written 15-20 years\nago. I am not sure that there is enough novelty in the ideas here to warrant\npublication in ACL in 2017. There is no contribution to NLP itself, e.g. in\nterms of modeling or search, and not a convincing contribution to the\napplication area which is just an instance of constrained generation. \n\nSince you start with one sequence and output another sequence with a very\nstraightforward monotonic mapping, it seems like a character-based\nsequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with\nNeural Networks; Sutskever et al. 2014) would work rather well here, very\nlikely with very fluent output and fewer moving parts (e.g. trigram models and\nPOS tag and scoring heuristics and postprocessing with a bigram model). You can\nuse large amounts of training from an arbitrary genre and do not need to rely\non an already-tagged corpus like in this paper, or worry about a parser. This\nwould be a 2017 paper.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\nThis paper presents a sentence based approach to generating memorable mnemonics\nfor numbers. The evaluation study presented in the paper shows that the\nsentence based approach indeed produces memorable mnemonics for short 8-digit\nnumbers (e.g. 86101521 --> Officiate Wasteland).\nOverall the paper presents the problem, the background literature and the\nsolution in sufficient detail. Because memorizing numbers (e.g. phone numbers\nand account numbers) is sufficiently common, this is an interesting problem.\n\n- Weaknesses:\nThe proposed solution does not seem to scale-up well for longer numbers; seems\nto work well with 8-digit numbers though. But many numbers that people need to\nmemorize such as phone numbers and credit card numbers are longer than\n8-digits. Besides, a number may have a structure (e.g. a phone number has a\ncountry code + area code + personal number) which people exploit while\nmemorizing numbers. As stated above, this paper addresses an important problem\nbut the current solution needs to be improved further (several ideas have been\nlisted by the authors in section 6).\n\n- General Discussion:\nThe current presented approach, in comparison to existing approaches, is\npromising.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nTackles a not very explored task, with obvious practical application\nWell written and motivated\n\n- Weaknesses:\n\nThe only method of validation is a user study, which has several weaknesses.\n\n- Discussion:\n\nThe paper investigates various methods to generate memorable mnemonic encodings\nof numbers based on the \u201cMajor\u201d system. As opposed to other methods that\nrely on this system to encode sequences, the methods proposed in this work\nreturn a single sequence (instead of a set of candidates) which is selected to\nimprove memorability. Since \u201cmemorability\u201d is an ambiguous criterion to\noptimize for, the authors explore various syntactic approaches that aim for\nshort and likely sentences.  Their final model uses a POS template sampled form\na set of \u201cnice\u201d structures, and a tri-gram language model to fill in the\nslots of the template. \n\nThe proposed approach is well motivated: the section on existing tools places\nthis approach in the context of previous work on security and memorability. The\nauthors point to results showing that passwords based on mnemonic phrases offer\nthe best of both worlds in terms of security (vs random passwords) and\nmemorability (vs naive passwords). This solid motivation will appease those\nreaders initially skeptical about the importance/feasibility of such\ntechniques. \n\nIn terms of the proposed methods, the baselines and n-gram models\n(unsurprisingly) generate bad encodings. The results in table 2 show that\nindeed Chunk and Sentence produce shorter sentences, but for short digits such\nas this one, how relevant are the additional characteristics of these methods\n(eg. POS replacements, templates etc)? It seems that a simple n-gram model with\nthe number-of-digits-per-trigram reweighing could perform well here. \n\nThe evaluation is weaker than the rest of the paper. My main concern is that a\none-time memorization setting seems inadequate to test this framework. Mnemonic\ntechniques are meant to aid recall after repeated memorization exercises, not\njust a single \u201cpriming\u201d event. Thus, a more informative setting would have\nhad the users be reminded of the number and encoding daily over a period of\ntime, and after a \u201cbuffer period\u201d, test their recall. This would also more\nclosely resemble the real-life conditions in which such a technique would be\nused (e.g. for password memorization).\n\nIn terms of the results, the difference between (long term) recall and\nrecognition is interesting. Do the authors have some explanation for why in the\nformer most methods performed similarly, but in the latter \u201cSentence\u201d\nperforms better? Could it be that the use of not very likely words (e.g.\n\"officiate\", in the example provided) make the encodings hard to remember but\neasy to spot? If this were the case, it would somewhat defeat the purpose of\nthe approach.\n\nAlso, it would be useful for the reader if the paper provided  (e.g. in an\nappendix) some examples of the digits/encodings that the users were presented\nduring the study, to get a better sense of the difficulty of recall and the\nquality of the encodings. \n\n- Suggestions:\n\nIt would be nice to provide some background on the Major system for those not\nfamiliar with it, which I suspect might be many in the ACL audience, myself\nincluded. Where does it come from? What\u2019s the logic behind those\ndigit-phoneme maps?", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}], "abstract": "The major system is a mnemonic system that can be used to memorize sequences of numbers. In this work, we present a method to automatically generate sentences that encode a given number. We propose several encoding models and compare the most promising ones in a password memorability study. The results of the study show that a model combining part-of-speech sentence templates with an n-gram language model produces the most memorable password representations.", "histories": [], "id": 66, "title": "Generating Memorable Mnemonic Encodings of Numbers"}
