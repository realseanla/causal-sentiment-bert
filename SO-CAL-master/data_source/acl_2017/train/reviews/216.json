{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n1. The idea of assigning variable-length document segments with dependent\ntopics is novel. This prior knowledge is worth incorporated in the LDA-based\nframework.\n2. Whereas we do not have full knowledge on recent LDA literature, we find the\npart of related work quite convincing.\n3. The method proposed for segment sampling with O(M) complexity is impressive.\nIt is crucial for efficient computation. \n\n- Weaknesses:\n1. Compared to Balikas COLING16's work, the paper has a weaker visualization\n(Fig 5), which makes us doubt about the actual segmenting and assigning results\nof document. It could be more convincing to give a longer exemplar and make\ncolor assignment consistent with topics listed in Figure 4.\n2. Since the model is more flexible than that of Balikas COLING16, it may be\nunderfitting, could you please explain this more?\n\n- General Discussion:\nThe paper is well written and structured. The intuition introduced in the\nAbstract and again exemplified in the Introduction is quite convincing. The\nexperiments are of a full range, solid, and achieves better quantitative\nresults against previous works. If the visualization part is stronger, or\nexplained why less powerful visualization, it will be more confident. Another\nconcern is about computation efficiency, since the seminal LDA work proposed to\nuse Variational Inference which is faster during training compared to MCMC, we\nwish to see the author\u2019s future development.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "### Strengths:\n- Well-written, well-organized\n- Incorporate topical segmentation to copula LDA to enable the joint learning\nof segmentation and latent models\n- Experimental setting is well-designed and show the superiority of the\nproposed method from several different indicators and datasets\n\n### Weaknesses:\n- No comparison with \"novel\" segmentation methods\n\n### General Discussion:\nThis paper presents segLDAcop, a joint latent model for topics and segments.\nThis model is based on the copula LDA and incorporates the topical segmentation\nto the copula LDA. The authors conduct comprehensive experiments by using\nseveral different datasets and evaluation metrics to show the superiority of\ntheir model.\n\nThis paper is well-written and well-organized. The proposed model is a\nreasonable extension of the copula LDA to enable the joint inference of\nsegmentations and topics. Experimental setting is carefully designed and the\nsuperiority of the proposed model is fairly validated.\nOne concern is that the authors only use the simple NP segmentation and single\nword segmentation as segments of the previous method. As noted in the paper,\nthere are many work to smartly generate segments before running LDA though it\nis largely affected by the bias of statistical or linguistic tools used. The\ncomparison with more novel (state-of-the-art) segments would be preferable to\nprecisely show the validity of the proposed method.\n\n### Minor comment\n- In line 105, \"latent radom topics\" -> \"latent random topics\"", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "2"}], "abstract": "This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.", "histories": [], "id": 216, "title": "Topical Coherence in LDA-based Models through Induced Segmentation"}
