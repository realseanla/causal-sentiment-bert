{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThe proposed methods can save memory and improve decoding speed on CPUs without\nlosing (or a little loss) performance. \n\n- Weaknesses:\nSince the determination of the convolutional codes of Algorithm 2 and Algorithm\n3 can affect the final performance, I think it would be better if the authors\ncan explore a good method for it. And I think the argument of \u201cExperiments\nshow the proposed model achieves translation accuracies that approach the\nsoftmax, while reducing memory usage on the order of 1/10 to 1/1000, and also\nimproving decoding speed on CPUs by x5 to x20.\u201d in the Abstract is not\nrigorous. As far as I know, your experiments setting with \u201cBinary\u201d and\n\u201cHybrid-512\u201d on ASPEC corpus show the improvements of decoding speed on\nCPUs by x20, but the BLEU scores are too low. So this is not a valid\nconclusion.\n\n- General Discussion:\nThis paper proposes an efficient prediction method for neural machine\ntranslation, which predicts a binary code for each word, to reduce the\ncomplexity of prediction. The authors also proposed to use the improved (error\ncorrection) binary codes method to improve the prediction accuracy and the\nhybrid softmax/binary model to balance the prediction accuracy and efficiency.\nThe proposed methods can save memory and improve decoding speed without losing\n(or a little loss) performance. I think this is a good paper.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n  This paper is well written, and with clear, well-designed\n  figures. The reader can easily understand the methodology even only\n  with those figures.\n\n  Predicting the binary code directly is a clever way to reduce the\n  parameter space, and the error-correction code just works\n  surprisingly well. I am really surprised by how 44 bits can achieve\n  26 out of 31 BLEU.  \n  The parameter reducing technique described in this work is\n  orthogonal to current existing methods: weight pruning and\n  sequence-level knowledge distilling.\n\n  The method here is not restricted by Neural Machine Translation, and\n  can be used in other tasks as long as there is a big output\n  vocabulary.  \n\n- Weaknesses:\n  The most annoying point to me is that in the relatively large\n  dataset (ASPEC), the best proposed model is still 1 BLEU point lower\n  than the softmax model. What about some even larger dataset, like\n  the French-English? There are at most 12 million sentences\n  there. Will the gap be even larger?\n\n  Similarly, what's the performance on some other language pairs ?\n\n  Maybe you should mention this paper,\n  https://arxiv.org/abs/1610.00072. It speeds up the decoding speed by\n  10x and the BLEU loss is less than 0.5.  \n\n- General Discussion:\n\nThe paper describes a parameter reducing method for large vocabulary\nsoftmax. By applying the error-corrected code and hybrid with softmax,\nits BLEU approaches that of the orignal full vocab softmax model.\n\nOne quick question: what is the hidden dimension size of the models?\nI couldn't find this in the experiment setup.\n\nThe 44 bits can achieve 26 out of 31 BLEU on E2J, that was\nsurprisingly good. However, how could you increase the number of bits\nto increase the classification power ? 44 is too small, there's plenty\nof room to use more bits and the computation time on GPU won't even\nchange.\n\nAnother thing that is counter-intuitive is that by predicting the\nbinary code, the model is actually predicting the rank of the\nwords. So how should we interpret these bit-embeddings ? There seems\nno semantic relations of all the words that have odd rank. Is it\nbecause the model is so powerful that it just remembers the data ?", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThis paper has high originality, proposing a fundamentally different way of\npredicting words from a vocabulary that is more efficient than a softmax layer\nand has comparable performance on NMT. If successful, the approach could be\nimpactful because it speeds up prediction.\n\nThis paper is nice to read with great diagrams. it's very clearly presented --\nI like cross-referencing the models with the diagrams in Table 2. Including\nloss curves is appreciated.\n\n- Weaknesses:\nThough it may not be possible in the time remaining, it would be good to see a\ncomparison (i.e. BLEU scores) with previous related work like hierarchical\nsoftmax and differentiated softmax.\n\nThe paper is lacking a linguistic perspective on the proposed method. Compared\nto a softmax layer and hierarchical/differentiated softmax, is binary code\nprediction a natural way to predict words? Is it more or less similar to how a\nhuman might retrieve words from memory? Is there a theoretical reason to\nbelieve that binary code based approaches should be more or less suited to the\ntask than softmax layers?\n\nThough the paper promises faster training speeds in the introduction, Table 3\nshows only modest (less than x2) speedups for training. Presumably this is\nbecause much of the training iteration time is consumed by other parts of the\nnetwork. It would be useful to see the time needed for the output layer\ncomputation only.\n\n- General Discussion:\nIt would be nice if the survey of prior work in 2.2 explicitly related those\nmethods to the desiderata in the introduction (i.e. specify which they\nsatisfy).\n\nSome kind of analysis of the qualitative strengths and weaknesses of the binary\ncode prediction would be welcome -- what kind of mistakes does the system make,\nand how does this compare to standard softmax and/or hierarchical and\ndifferentiated softmax?\n\nLOW LEVEL COMMENTS\n\nEquation 5: what's the difference between id(w) = id(w') and w = w' ?\n\n335: consider defining GPGPU\n\nTable 3: Highlight the best BLEU scores in bold\n\nEquation 15: remind the reader that q is defined in equation 6 and b is a\nfunction of w. I was confused by this at first because w and h appear on the\nLHS but don't appear on the right, and I didn't know what b and q were.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}], "abstract": "In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.", "histories": [], "id": "676", "title": "Neural Machine Translation via Binary Code Prediction"}
