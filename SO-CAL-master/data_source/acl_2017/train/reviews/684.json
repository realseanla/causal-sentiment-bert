{"reviews": [{"IMPACT": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "4", "PRESENTATION_FORMAT": "Poster", "comments": "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop\nreasoning by fine-grained gated filter. \nIt's interesting and intuitive for machine reading. \nI like the idea along with significant improvement on benchmark datasets, but\nalso have major concerns to get it published in ACL.\n\n- The proposed GA mechanism looks promising, but not enough to convince the\nimportance of this technique over other state-of-the-art systems, because\nengineering tricks presented 3.1.4 boost a lot on accuracy and are blended in\nthe result.\n\n- Incomplete bibliography: Nearly all published work in reference section\nrefers arxiv preprint version. \nThis makes me (and future readers) suspicious if this work thoroughly compares\nwith prior work. Please make them complete if the published version is\navailable. \n\n- Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned\nas previous work that is unpublished preprint. \nI don't think this is necessary at all. Alternately, I would like the author to\nreplace it with vanilla GA (or variant of the proposed model for baseline). \nIt doesn't make sense that result from the preprint which will end up being the\nsame as this ACL submission is presented in the same manuscript. \nFor fair blind-review, I didn't search on arvix archive though.\n\n- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2,\nand GA (fix L(w)) is for K=3 in table 2. \nDoes this mean that GA-- is actually AS Reader? \nIt's not clear that GA-- is re-implementation of AS. \nI assumed K=1 (AS) in table 2 uses also GloVe initialization and\ntoken-attention, but it doesn't seem in GA--. \n\n- I wish the proposed method compared with prior work in related work section\n(i.e. what's differ from related work).\n\n- Fig 2 shows benefit of gated attention (which translates multi-hop\narchitecture), and it's very impressive. It would be great to see any\nqualitative example with comparison.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper presents an interesting model for reading comprehension, by\ndepicting the multiplicative interactions between the query and local\ninformation around a word in a document, and the authors proposed a new\ngated-attention strategy to characterize the relationship. The work is quite\nsolid, with almost state of art result on the whole four cloze-style datasets\nachieved. Some of the further improvement can be helpful for the similar tasks.\n\n\nNevertheless, I have some concerns on the following aspect:\n\n1. The authors have referred many papers from arXiv, but I think some really\nrelated works are not included. Such as the works from Caiming Xiong, et al.\nhttps://openreview.net/pdf?id=rJeKjwvclx and the work form Shuohang Wang, et\nal. https://openreview.net/pdf?id=B1-q5Pqxl . Both of them concentrated on\nenhancing the attention operation to modeling the interaction between documents\nand queries. Although these works are not evaluated on the cloze-style corpus\nbut the SQuAD, an experimental or fundamental comparison may be necessary.\n\n2. There have been some studies that adopts attention mechanism or its variants\nspecially designed for the Reading Comprehension tasks, and the work actually\nshare the similar ideas with this paper. My suggestion is to conduct some\ncomparisons with such work to enhance the experiments of this paper.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "5", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\n* Paper is very well-written and every aspect of the model is well-motivated\nand clearly explained.\n* The authors have extensively covered the previous work in the area.\n* The approach achieves state-of-the-art results across several text\ncomprehension data sets. In addition, the experimental evaluation is very\nthorough.\n\n- Weaknesses:\n\n* Different variants of the model achieve state-of-the-art performance across\nvarious data sets. However, the authors do provide an explanation for this\n(i.e. size of data set and text anonymization patterns).\n\n- General Discussion:\n\nThe paper describes an approach to text comprehension which uses gated\nattention modules to achieve state-of-the-art performance. Compared to previous\nattention mechanisms, the gated attention reader uses the query embedding and\nmakes multiple passes (multi-hop architecture) over the document and applies\nmultiplicative updates to the document token vectors before finally producing a\nclassification output regarding the answer. This technique somewhat mirrors how\nhumans solve text comprehension problems. Results show that the approach\nperforms well on large data sets such as CNN and Daily Mail. For the CBT data\nset, some additional feature engineering is needed to achieve state-of-the-art\nperformance. \n\nOverall, the paper is very well-written and model is novel and well-motivated.\nFurthermore, the approach achieves state-of-the-art performance on several data\nsets. \n\nI had only minor issues with the evaluation. The experimental results section\ndoes not mention whether the improvements (e.g. in Table 3) are statistically\nsignificant and if so, which test was used and what was the p-value. Also I\ncouldn't find an explanation for the performance on CBT-CN data set where the\nvalidation performance is superior to NSE but test performance is significantly\nworse.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "3"}], "abstract": "In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention.", "histories": [], "id": 684, "title": "Gated-Attention Readers for Text Comprehension"}
