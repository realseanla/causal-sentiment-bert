{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths: Great paper: Very well-written, interesting results, creative\nmethod, good and enlightening comparisons with earlier approaches. In addition,\nthe corpus, which is very carefully annotated, will prove to be a valuable\nresource for other researchers. I appreciated the qualitative discussion in\nsection 5. Too many ML papers just give present a results table without much\nfurther ado, but the discussion in this paper really provides insights for the\nreader. \n\n- Weaknesses: In section 4.1, the sentence \"The rest of the model\u2019s input is\nset to zeroes...\" is quite enigmatic until you look at Figure 2. Some extra\nsentence here explaining what is going on would be helpful. Furthermore, in\nFigure 2, in the input layers to the LSTMs it says \"5*Embeddings(50D)\" also for\nthe networks taking dependency labels as input. Surely this is wrong? (Or if it\nis correct, please explain what you mean). \n\n- General Discussion: Concerning the comment in 4.2 \"LSTMs are excellent at\nmodelling language sequences ... which is why we use this type of model.\". This\ncomment seems strange to me. This is not a sequential problem in that sense.\nFor each datapoint, you feed the network all 5 words in an example in one go,\nand the next example has nothing to do with the preceding one. The LSTM\narchitecture could still be superior, of course, but not for the reason you\nstate. Or have I misunderstood something? I'd be interested to hear the\nauthors' comments on this point.", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "5", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.", "histories": [], "id": 220, "title": "Vancouver Welcomes You! Minimalist Location Metonymy Resolution"}
