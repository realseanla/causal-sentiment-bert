{"reviews": [{"IMPACT": "2", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "- Strengths:\n\nThe paper is well-written and easy to understand. The methods and results are\ninteresting.\n\n- Weaknesses:\n\nThe evaluation and the obtained results might be problematic (see my comments\nbelow).\n\n- General Discussion:\n\nThis paper proposes a system for end-to-end argumentation mining using neural\nnetworks. The authors model the problem using two approaches: (1) sequence\nlabeling (2) dependency parsing. The paper also includes the results of\nexperimenting with a multitask learning setting for the sequence labeling\napproach. The paper clearly explains the motivation behind the proposed model.\nExisting methods are based on ILP, manual feature engineering and manual design\nof ILP constraints. However, the proposed model avoids such manual effort.\nMoreover, the model jointly learns the subtasks in argumentation mining and\ntherefore, avoids the error back propagation problem in pipeline methods.\nExcept a few missing details (mentioned below), the methods are explained\nclearly.\n\nThe experiments are substantial, the comparisons are performed properly, and\nthe results are interesting. My main concern about this paper is the small size\nof the dataset and the large capacity of the used (Bi)LSTM-based recurrent\nneural networks (BLC and BLCC). The dataset includes only around 320 essays for\ntraining and 80 essays for testing. The size of the development set, however,\nis not mentioned in the paper (and also the supplementary materials). This is\nworrying because very few number of essays are left for training, which is a\ncrucial problem. The total number of tags in the training data is probably only\na few thousand. Compare it to the standard sequence labeling tasks, where\nhundreds of thousands (sometimes millions) of tags are available. For this\nreason, I am not sure if the model parameters are trained properly. The paper\nalso does not analyze the overfitting problem. It would be interesting to see\nthe training and development \"loss\" values during training (after each\nparameter update or after each epoch). The authors have also provided some\ninformation that can be seen as the evidence for overfitting: Line 622 \"Our\nexplanation is that taggers are simpler local models, and thus need less\ntraining data and are less prone to overfitting\".\n\nFor the same reason, I am not sure if the models are stable enough. Mean and\nstandard deviation of multiple runs (different initializations of parameters)\nneed to be included. Statistical significance tests would also provide more\ninformation about the stability of the models and the reliability of results.\nWithout these tests, it is hard to say if the better results are because of the\nsuperiority of the proposed method or chance.\n\nI understand that the neural networks used for modeling the tasks use their\nregularization techniques. However, since the size of the dataset is too small,\nthe authors need to pay more attention to the regularization methods. The paper\ndoes not mention regularization at all and the supplementary material only\nmentions briefly about the regularization in LSTM-ER. This problem needs to be\naddressed properly in the paper.\n\nInstead of the current hyper-parameter optimization method (described in\nsupplementary materials) consider using Bayesian optimization methods.\n\nAlso move the information about pre-trained word embeddings and the error\nanalysis from the supplementary material to the paper. The extra one page\nshould be enough for this.\n\nPlease include some inter-annotator agreement scores. The paper describing the\ndataset has some relevant information. This information would provide some\ninsight about the performance of the systems and the available room for\nimprovement.\n\nPlease consider illustrating figure 1 with different colors to make the quality\nbetter for black and white prints.\n\nEdit:\n\nThanks for answering my questions. I have increased the recommendation score to\n4. Please do include the F1-score ranges in your paper and also report mean and\nvariance of different settings. I am still concerned about the model stability.\nFor example, the large variance of Kiperwasser setting needs to be analyzed\nproperly. Even the F1 changes in the range [0.56, 0.61] is relatively large.\nIncluding these score ranges in your paper helps replicating your work.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The work describes a joint neural approach to argumentation mining. There are\nseveral approaches explored including:\n 1) casting the problem as a dependency parsing problem (trying several\ndifferent parsers)\n 2) casting the problem as a sequence labeling problem\n3) multi task learning (based on sequence labeling model underneath)\n4) an out of the box neural model for labeling entities and relations (LSTM-ER)\n5) ILP based state-of-the art models\nAll the approaches are evaluated using F1 defined on concepts and relations. \nDependency based solutions do not work well, seq. labeling solutions are\neffective.\nThe out-of-the-box LSTM-ER model performs very well. Especially on paragraph\nlevel.\nThe Seq. labeling and LSTM-ER models both outperform the ILP approach.\nA very comprehensive supplement was given, with all the technicalities of\ntraining\nthe models, optimizing hyper-parameters etc.\nIt was also shown that sequence labeling models can be greatly improved by the\nmultitask\napproach (with the claim task helping more than the relation task).\nThe aper  is a very thorough investigation of neural based approaches to\nend-to-end argumentation mining.\n\n- Major remarks  \n  - my one concern is with the data set, i'm wondering if it's a problem that\nessays in the train set and in the test set might\n   be on the same topics, consequently writers might use the same or similar\narguments in both essays, leading to information\n   leakage from the train to the test set. In turn, this might give overly\noptimistic performance estimates. Though, i think the same\n   issues are present for the ILP models, so your model does not have an unfair\nadvantage. Still, this may be something to discuss.\n\n  - my other concern is that one of your best models LSTM-ER is acutally just a\nan out-of-the box application of a model from related\n    work. However, given the relative success of sequence based models and all\nthe experiments and useful lessons learned, I think this \n    work deserves to be published.\n\n- Minor remarks and questions:\n222 - 226 - i guess you are arguing that it's possible to reconstruct the full\ngraph once you get a tree as output? Still, this part is not quite clear.\n443-444 The ordering in this section is seq. tagging -> dependency based -> MTL\nusing seq. tagging, it would be much easier to follow if the order of the first\ntwo were\n                  reversed (by the time I got here i'd forgotten what STag_T\nstood for)\n455 - What does it mean that it de-couples them but jointly models them (isn't\ncoupling them required to jointly model them?)\n         - i checked Miwa and Bansal and I couldn't find it\n477 - 479 -  It's confusing when you say your system de-couples relation info\nfrom entity info, my best guess is that you mean it\n                        learns some tasks as \"the edges of the tree\" and some\nother tasks as \"the labels on those edges\", thus decoupling them. \n                        In any case,  I recommend you make this part clearer\n\nAre the F1 scores in the paragraph and essay settings comparable? In particular\nfor the relation tasks. I'm wondering if paragraph based \nmodels might miss some cross paragraph relations by default, because they will\nnever consider them.", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning `natural' subtasks, in a multi-task learning setup, improves performance.", "histories": [], "id": 134, "title": "Neural End-to-End Learning for Computational Argumentation Mining"}
