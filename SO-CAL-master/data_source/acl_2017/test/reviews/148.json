{"reviews": [{"IMPACT": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\n- this article puts two fields together: text readability for humans and\nmachine comprehension of texts\n\n- Weaknesses:\n\n- The goal of your paper is not entirely clear. I had to read the paper 4 times\nand I still do not understand what you are talking about!\n- The article is highly ambiguous what it talks about - machine comprehension\nor text readability for humans\n- you miss important work in the readability field\n- Section 2.2. has completely unrelated discussion of theoretical topics.\n- I have the feeling that this paper is trying to answer too many questions in\nthe same time, by this making itself quite weak. Questions such as \u201cdoes text\nreadability have impact on RC datasets\u201d should be analyzed separately from\nall these prerequisite skills.\n\n- General Discussion:\n\n- The title is a bit ambiguous, it would be good to clarify that you are\nreferring to machine comprehension of text, and not human reading\ncomprehension, because \u201creading comprehension\u201d and \u201creadability\u201d\nusually mean that.\n- You say that your \u201cdataset analysis suggested that the readability of RC\ndatasets does not directly affect the question difficulty\u201d, but this depends\non the method/features used for answer detection, e.g. if you use\nPOS/dependency parse features.\n- You need to proofread the English of your paper, there are some important\nomissions, like \u201cthe question is easy to solve simply look..\u201d on page 1.\n- How do you annotate datasets with \u201cmetrics\u201d??\n- Here you are mixing machine reading comprehension of texts and human reading\ncomprehension of texts, which, although somewhat similar, are also quite\ndifferent, and also large areas.\n- \u201creadability of text\u201d is not \u201cdifficulty of reading contents\u201d. Check\nthis:\nDuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact\ninformation. \n- it would be good if you put more pointers distinguishing your work from\nreadability of questions for humans, because this article is highly ambiguous.\nE.g. on page 1 \u201cThese two examples show that the readability of the text does\nnot necessarily correlate with the difficulty of the questions\u201d you should\nadd \u201cfor machine comprehension\u201d\n- Section 3.1. - Again: are you referring to such skills for humans or for\nmachines? If for machines, why are you citing papers for humans, and how sure\nare you they are referring to machines too?\n- How many questions the annotators had to annotate? Were the annotators clear\nthey annotate the questions keeping in mind machines and not people?", "SOUNDNESS_CORRECTNESS": "4", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "2", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Knowing the quality of reading comprehension (RC) datasets is important for the development of natural-language understanding systems.   In this study, two classes of metrics were adopted for evaluating RC datasets: prerequisite skills and readability. We applied these classes to six existing datasets, including MCTest and SQuAD, and highlighted the characteristics of the datasets according to each metric and the correlation between the two classes.   Our dataset analysis suggests that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer.", "histories": [], "id": "148", "title": "Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability"}
