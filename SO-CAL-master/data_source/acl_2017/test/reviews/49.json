{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\nThe paper presents an interesting extension to attention-based neural MT\napproaches, which leverages source-sentence chunking as additional piece of\ninformation from the source sentence. The model is modified such that this\nchunking information is used differently by two recurrent layers: while one\nfocuses in generating a chunk at a time, the other focuses on generating the\nwords within the chunk. This is interesting. I believe readers will enjoy\ngetting to know this approach and how it performs.\nThe paper is very clearly written, and alternative approaches are clearly\ncontrasted. The evaluation is well conducted, has a direct contrast with other\npapers (and evaluation tables), and even though it could be strengthened (see\nmy comments below), it is convincing.\n\n- Weaknesses:\nAs always, more could be done in the experiments section to strengthen the case\nfor chunk-based models. For example, Table 3 indicates good results for Model 2\nand Model 3 compared to previous papers, but a careful reader will wonder\nwhether these improvements come from switching from LSTMs to GRUs. In other\nwords, it would be good to see the GRU tree-to-sequence result to verify that\nthe chunk-based approach is still best.\n\nAnother important aspect is the lack of ensembling results. The authors put a\nlot of emphasis is claiming that this is the best single NMT model ever\npublished. While this is probably true, in the end the best WAT system for\nEng-Jap is at 38.20 (if I'm reading the table correctly) - it's an ensemble of\n3. If the authors were able to report that their 3-way chunk-based ensemble\ncomes top of the table, then this paper could have a much stronger impact.\n\nFinally, Table 3 would be more interesting if it included decoding times. The\nauthors mention briefly that the character-based model is less time-consuming\n(presumably based on Eriguchi et al.'16), but no cite is provided, and no\nnumbers from chunk-based decoding are reported either. Is the chunk-based model\nfaster or slower than word-based? Similar? Who know... Adding a column to Table\n3 with decoding times would give more value to the paper.\n\n- General Discussion:\nOverall I think the paper is interesting and worth publishing. I have minor\ncomments and suggestions to the authors about how to improve their presentation\n(in my opinion, of course). \n\n* I think they should clearly state early on that the chunks are supplied\nexternally - in other words, that the model does not learn how to chunk. This\nonly became apparent to me when reading about CaboCha on page 6 - I don't think\nit's mentioned earlier, and it is important.\n\n* I don't see why the authors contrast against the char-based baseline so often\nin the text (at least a couple of times they boast a +4.68 BLEU gain). I don't\nthink readers are bothered... Readers are interested in gains over the best\nbaseline.\n\n* It would be good to add a bit more detail about the way UNKs are being\nhandled by the neural decoder, or at least add a citation to the\ndictionary-based replacement strategy being used here.\n\n* The sentence in line 212 (\"We train a GRU that encodes a source sentence into\na single vector\") is not strictly correct. The correct way would be to say that\nyou do a bidirectional encoder that encodes the source sentence into a set of\nvectors... at least, that's what I see in Figure 2.\n\n* The motivating example of lines 69-87 is a bit weird. Does \"you\" depend on\n\"bite\"? Or does it depend on the source side? Because if it doesn't depend on\n\"bite\", then the argument that this is a long-dependency problem doesn't really\napply.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Summary\n\nThis paper introduces chunk-level architecture for existing NMT models. Three\nmodels are proposed to model the correlation between word and chunk modelling\non the target side in the existing NMT models. \n\n- Strengths:\n\nThe paper is well-written and clear about the proposed models and its\ncontributions. \n\nThe proposed models to incorporating chunk information into NMT models are\nnovel and well-motivated. I think such models can be generally applicable for\nmany other language pairs. \n\n- Weaknesses:\n\nThere are some minor points, listed as follows:\n\n1) Figure 1: I am a bit surprised that the function words dominate the content\nones in a Japanese sentence. Sorry I may not understand Japanese. \n\n2) In all equations, sequences/vectors (like matrices) should be represented\nas bold texts to distinguish from scalars, e.g., hi, xi, c, s, ...\n\n3) Equation 12: s_j-1 instead of s_j.\n\n4) Line 244: all encoder states should be referred to bidirectional RNN states.\n\n5) Line 285: a bit confused about the phrase \"non-sequential information such\nas chunks\". Is chunk still sequential information???\n\n6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k)\nto indicate the word in a chunk.  \n\n7) Some questions for the experiments:\n\nTable 1: source language statistics? \n\nFor the baselines, why not running a baseline (without using any chunk\ninformation) instead of using (Li et al., 2016) baseline (|V_src| is\ndifferent)? It would be easy to see the effect of chunk-based models. Did (Li\net al., 2016) and other baselines use the same pre-processing and\npost-processing steps? Other baselines are not very comparable. After authors's\nresponse, I still think that (Li et al., 2016) baseline can be a reference but\nthe baseline from the existing model should be shown. \n\nFigure 5: baseline result will be useful for comparison? chunks in the\ntranslated examples are generated *automatically* by the model or manually by\nthe authors? Is it possible to compare the no. of chunks generated by the model\nand by the bunsetsu-chunking toolkit? In that case, the chunk information for\nDev and Test in Table 1 will be required. BTW, the authors's response did not\naddress my point here. \n\n8) I am bit surprised about the beam size 20 used in the decoding process. I\nsuppose large beam size is likely to make the model prefer shorter generated\nsentences. \n\n9) Past tenses should be used in the experiments, e.g.,\n\nLine 558: We *use* (used) ...\n\nLine 579-584: we *perform* (performed) ... *use* (used) ...\n\n...\n\n- General Discussion:\n\nOverall, this is a solid work - the first one tackling the chunk-based NMT;\nand it well deserves a slot at ACL.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "3", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "5", "REVIEWER_CONFIDENCE": "5"}], "abstract": "Chunks (or phrases) once played a pivotal role in machine translation. By using a chunk rather than a word as the basic translation unit, local (intra-chunk) and global (inter-chunk) word orders and dependencies can be easily modeled. The chunk structure, despite its importance, has not been considered in the decoders used for neural machine translation (NMT). In this paper, we propose chunk-based decoders for (NMT), each of which consists of a chunk-level decoder and a word-level decoder. The chunk-level decoder models global dependencies while the word-level decoder decides the local word order in a chunk. To output a target sentence, the chunk-level decoder generates a chunk representation containing global information, which the word-level decoder then uses as a basis to predict the words inside the chunk. Experimental results show that our proposed decoders can significantly improve translation performance in a WAT '16 English-to-Japanese translation task.", "histories": [], "id": "49", "title": "Chunk-based Decoder for Neural Machine Translation"}
