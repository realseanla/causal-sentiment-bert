{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "- Strengths:\n\nThis paper presents a sophisticated application of Grid-type Recurrent Neural\nNets to the task of determining predicate-argument structures (PAS) in\nJapanese.  The approach does not use any explicit syntactic structure, and\noutperforms the current SOA systems that do include syntactic structure.  The\nauthors give a clear and detailed description of the implementation and of the\nresults.  In particular, they pay close attention to the performance on dropped\narguments, zero pronouns, which are prevalent in Japanese and especially\nchallenging with respect to PAS. Their multi-sequence model, which takes all of\nthe predicates in the sentence into account, achieves the best performance for\nthese examples.  The paper is detailed and clearly written.\n\n- Weaknesses:\n\nI really only have minor comments. There are some typos listed below, the\ncorrection of which would improve English fluency. I think it would be worth\nillustrating the point about the PRED including context around the \"predicate\"\nwith the example from Fig 6 where the accusative marker is included with the\nverb in the PRED string.  I didn't understand the use of boldface in Table 2,\np. 7.\n\n- General Discussion:\n\nTypos:\n\np1 :  error propagation does not need a \"the\", nor does \"multi-predicate\ninteractions\"\np2: As an solution -> As a solution, single-sequence model -> a single-sequence\nmodel,                    multi-sequence model -> a multi-sequence model \np. 3 Example in Fig 4.                    She ate a bread -> She ate bread.\np. 4 assumes the independence -> assumed independence, the multi-predicate\ninteractions -> multi-predicate interactions, the multi-sequence model -> a\nmulti-sequence model\np.7: the residual connections -> residual connections, the multi-predicate\ninteractions -> multi-predicate interactions (twice)\np8 NAIST Text Corpus -> the NAIST Text Corpus, the state-of-the-art result ->\nstate-of-the-art results\n\nI have read the author response and am satisfied with it.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes new prediction models for Japanese SRL task by adopting the\nEnglish state-of-the-art model of (Zhou and Xu, 2015).\nThe authors also extend the model by applying the framework of Grid-RNNs in\norder to handle the interactions between the arguments of multiple predicates.\n\nThe evaluation is performed on the well-known benchmark dataset in Japanese\nSRL, and obtained a significantly better performance than the current state of\nthe art system.\n\nStrengths:\nThe paper is well-structured and well-motivated.\nThe proposed model obtains an improvement in accuracy compared with the current\nstate of the art system.\nAlso, the model using Grid-RNNs achieves a slightly better performance than\nthat of proposed single-sequential model, mainly due to the improvement on the\ndetection of zero arguments, that is the focus of this paper.\n\nWeakness:\nTo the best of my understanding, the main contribution of this paper is an\nextension of the single-sequential model to the multi-sequential model. The\nimpact of predicate interactions is a bit smaller than that of (Ouchi et al.,\n2015). There is a previous work (Shibata et al., 2016) that extends the (Ouchi\net al., 2015)'s model\nwith neural network modeling. I am curious about the comparison between them.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "5"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper proposes a joint neural modelling approach to PAS analysis in\nJapanese, based on Grid-RNNs, which it compares variously with a conventional\nsingle-sequence RNN approach.\n\nThis is a solidly-executed paper, targeting a well-established task from\nJapanese but achieving state-of-the-art results at the task, and presenting\nthe task in a mostly accessible manner for those not versed in\nJapanese. Having said that, I felt you could have talked up the complexity of\nthe task a bit, e.g. wrt your example in Figure 1, talking through the\ninherent ambiguity between the NOM and ACC arguments of the first predicate,\nas the NOM argument of the second predicate, and better describing how the\ntask contrasts with SRL (largely through the ambiguity in zero pronouns). I\nwould also have liked to have seen some stats re the proportion of zero\npronouns which are actually intra-sententially resolvable, as this further\ncomplicates the task as defined (i.e. needing to implicitly distinguish\nbetween intra- and inter-sentential zero anaphors). One thing I wasn't sure of\nhere: in the case of an inter-sentential zero pronoun for the argument of a\ngiven predicate, what representation do you use? Is there simply no marking of\nthat argument at all, or is it marked as an empty argument? My reading of the\npaper is that it is the former, in which case there is no explicit\nrepresentation of the fact that there is a zero pronoun, which seems like a\nslightly defective representation (which potentially impacts on the ability of\nthe model to capture zero pronouns); some discussion of this would have been\nappreciated.\n\nThere are some constraints that don't seem to be captured in the model (which\nsome of the ILP-based methods for SRL explicitly model, e.g.): (1) a given\npredicate will generally have only one argument of a given type (esp. NOM and\nACC); and (2) a given argument generally only fills one argument slot for a\ngiven predicate. I would have liked to have seen some analysis of the output\nof the model to see how well the model was able to learn these sorts of\nconstraints. More generally, given the mix of numbers in Table 3 between\nSingle-Seq and Multi-Seq (where it is really only NOM where there is any\nimprovement for Multi-Seq), I would have liked to have seen some discussion of\nthe relative differences in the outputs of the two models: are they largely\nidentical, or very different but about the same in aggregate, e.g.? In what\ncontexts do you observe differences between the two models? Some analysis like\nthis to shed light on the internals of the models would have made the\ndifference between a solid and a strong paper, and is the main area where I\nbelieve the paper could be improved (other than including results for SRL, but\nthat would take quite a bit more work).\n\nThe presentation of the paper was good, with the Figures aiding understanding\nof the model. There were some low-level language issues, but nothing major:\n\nl19: the error propagation -> error propagation\nl190: an solution -> a solution\nl264 (and Figure 2): a bread -> bread\nl351: the independence -> independence\nl512: the good -> good\nl531: from their model -> of their model\nl637: significent -> significance\nl638: both of -> both\n\nand watch casing in your references (e.g. \"japanese\", \"lstm\", \"conll\", \"ilp\")", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "The performance of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to the joint modeling of interactions between multiple predicates. However, this approach relies heavily on syntactic information predicted by parsers, and suffers from errorpropagation. To remedy this problem, we introduce a model that uses grid-type recurrent neural networks. The proposed model automatically induces features sensitive to multi-predicate interactions from the word sequence information of a sentence. Experiments on the NAIST Text Corpus demonstrate that without syntactic information, our model outperforms previous syntax-dependent models.", "histories": [], "id": "355", "title": "Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis"}
