{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "This paper develops an LSTM-based model for classifying connective uses for\nwhether they indicate that a causal relation was intended. The guiding idea is\nthat the expression of causal relations is extremely diverse and thus not\namenable to syntactic treatment, and that the more abstract representations\ndelivered by neural models are therefore more suitable as the basis for making\nthese decisions.\n\nThe experiments are on the AltLex corpus developed by Hidley and McKeown. The\nresults offer modest but consistent support for the general idea, and they\nprovide some initial insights into how best to translate this idea into a\nmodel. The paper distribution includes the TensorFlow-based models used for the\nexperiments.\n\nSome critical comments and questions:\n\n* The introduction is unusual in that it is more like a literature review than\na full overview of what the paper contains. This leads to some redundancy with\nthe related work section that follows it. I guess I am open to a non-standard\nsort of intro, but this one really doesn't work: despite reviewing a lot of\nideas, it doesn't take a stand on what causation is or how it is expressed, but\nrather only makes a negative point (it's not reducible to syntax). We aren't\nreally told what the positive contribution will be except for the very general\nfinal paragraph of the section.\n\n* Extending the above, I found it disappointing that the paper isn't really\nclear about the theory of causation being assumed. The authors seem to default\nto a counterfactual view that is broadly like that of David Lewis, where\ncausation is a modal sufficiency claim with some other counterfactual\nconditions added to it. See line 238 and following; that arrow needs to be a\nvery special kind of implication for this to work at all, and there are\nwell-known problems with Lewis's theory (see\nhttp://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments\nelsewhere in the paper that the authors don't endorse the counterfactual view,\nbut then what is the theory being assumed? It can't just be the temporal\nconstraint mentioned on page 3!\n\n* I don't understand the comments regarding the example on line 256. The\nauthors seem to be saying that they regard the sentence as false. If it's true,\nthen there should be some causal link between the argument and the breakage.\nThere are remaining issues about how to divide events into sub-events, and\nthese impact causal theories, but those are not being discussed here, leaving\nme confused.\n\n* The caption for Figure 1 is misleading, since the diagram is supposed to\ndepict only the \"Pair_LSTM\" variant of the model. My bigger complaint is that\nthis diagram is needlessly imprecise. I suppose it's okay to leave parts of the\nstandard model definition out of the prose, but then these diagrams should have\na clear and consistent semantics. What are all the empty circles between input\nand the \"LSTM\" boxes? The prose seems to say that the model has a look-up\nlayer, a Glove layer, and then ... what? How many layers of representation are\nthere? The diagram is precise about the pooling tanh layers pre-softmax, but\nnot about this. I'm also not clear on what the \"LSTM\" boxes represent. It seems\nlike it's just the leftmost/final representation that is directly connected to\nthe layers above. I suggest depicting that connection clearly.\n\n* I don't understand the sentence beginning on line 480. The models under\ndiscussion do not intrinsically require any padding. I'm guessing this is a\nrequirement of TensorFlow and/or efficient training. That's fine. If that's\ncorrect, please say that. I don't understand the final clause, though. How is\nthis issue even related to the question of what is \"the most convenient way to\nencode the causal meaning\"? I don't see how convenience is an issue or how this\nrelates directly to causal meaning.\n\n* The authors find that having two independent LSTMs (\"Stated_LSTM\") is\nsomewhat better than one where the first feeds into the second. This issue is\nreminiscent of discussions in the literature on natural language entailment,\nwhere the question is whether to represent premise and hypothesis independently\nor have the first feed into the second. I regard this as an open question for\nentailment, and I bet it needs further investigation for causal relations too.\nSo I can't really endorse the sentence beginning on line 587: \"This behaviour\nmeans that our assumption about the relation between the meanings of the two\ninput events does not hold, so it is better to encode each argument\nindependently and then to measure the relation between the arguments by using\ndense layers.\" This is very surprising since we are talking about subparts of a\nsentence that might share a lot of information.\n\n* It's hard to make sense of the hyperparameters that led to the best\nperformance across tasks. Compare line 578 with line 636, for example. Should\nwe interpret this or just attribute it to the unpredictability of how these\nmodels interact with data?\n\n* Section 4.3 concludes by saying, of the connective 'which then', that the\nsystem can \"correctly disambiguate its causal meaning\", whereas that of Hidey\nand McKeown does not. That might be correct, but one example doesn't suffice to\nshow it. To substantiate this point, I suggest making up a wide range of\nexamples that manifest the ambiguity and seeing how often the system delivers\nthe right verdict. This will help address the question of whether it got lucky\nwith the example from table 8.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "2", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper proposes a method for detecting causal relations between clauses,\nusing neural networks (\"deep learning\", although, as in many studies, the\nnetworks are not particularly deep).  Indeed, while certain discourse\nconnectives are unambiguous regarding the relation they signal (e.g. 'because'\nis causal) the paper takes advantage of a recent dataset (called AltLex, by\nHidey and McKeown, 2016) to solve the task of identifying causal vs. non-causal\nrelations when the relation is not explicitly marked.  Arguing that\nconvolutional networks are not as adept as representing the relevant features\nof clauses as LSTMs, the authors propose a classification architecture which\nuses a Glove-based representation of clauses, input in an LSTM layer, followed\nby three densely connected layers (tanh) and a final decision layer with a\nsoftmax.\n\nThe best configuration of the system improves by 0.5-1.5% F1 over Hidey and\nMCkeown's 2016 one (SVM classifier).  Several examples of generalizations where\nthe system performs well are shown (indicator words that are always causal in\nthe training data, but are found correctly to be non causal in the test data).\nTherefore, I appreciate that the system is analyzed qualitatively and \nquantitatively.\n\nThe paper is well written, and the description of the problem is particularly\nclear. However a clarification of the differences between this task and the \ntask of implicit connective recognition would be welcome.  This could possibly \ninclude a discussion of why previous methods for implicit connective \nrecognition cannot be used in this case.\n\nIt is very appreciable that the authors uploaded their code to the submission\nsite (I inspected it briefly but did not execute it).  Uploading the (older)\ndata (with the code) is also useful as it provides many examples.  It was not\nclear to me what is the meaning of the 0-1-2 coding in the TSV files, given\nthat the paper mentions binary classification. I wonder also, given that this\nis the data from Hidey and McKeown, if the authors have the right to repost it\nas they do.  -- One point to clarify in the paper would be the meaning of\n\"bootstrapping\", which apparently extends the corpus by about 15%: while the\nconstruction of the corpus is briefly but clearly explained in the paper, the\nadditional bootstrapping is not. \n\nWhile it is certainly interesting to experiment with neural networks on this\ntask, the merits of the proposed system are not entirely convincing.  It seems\nindeed that the best configuration (among 4-7 options) is found on the test\ndata, and it is this best configuration that is announced as improving over\nHidey by \"2.13% F1\".  However, a fair comparison would involve selecting the\nbest configuration on the devset.\n\nMoreover, it is not entirely clear how significant the improvement is. On the\none hand, it should be possible, given the size of the dataset, to compute some\nstatistical significance indicators.  On the other hand, one should consider\nalso the reliability of the gold-standard annotation itself (possibly from the\ncreators of the dataset).  Upon inspection, the annotation obtained from the\nEnglish/SimpleEnglish Wikipedia is not perfect, and therefore the scores might\nneed to be considered with a grain of salt.\n\nFinally, neural methods have been previously shown to outperform human\nengineered features for binary classification tasks, so in a sense the results \nare rather a confirmation of a known property. It would be interesting to see\nexperiments with simpler networks used as baselines, e.g. a 1-layer LSTM.  The\nanalysis of results could try to explain why the neural method seems to favor \nprecision over recall.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "5", "REVIEWER_CONFIDENCE": "4"}], "abstract": "Causation is a psychological tool of humans to understand the world and it is projected in natural language. Causation relates two events, so in order to understand the causal relation of those events and the causal reasoning of humans, the study of causality classification is required. Herein, we propose a neural network architecture for the task of causality classification. We claim that the encoding of the meaning of a sentence is required for the disambiguation of its causal meaning. Our results show that our claim holds, and we outperform the state-of-the-art.", "histories": [], "id": 435, "title": "Neural Disambiguation of Causal Lexical Markers based on Context"}
