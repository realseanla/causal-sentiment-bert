{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper introduces an extension of the entity grid model. A convolutional\nneural network is used to learn sequences of entity transitions indicating\ncoherence, permitting better generalisation over longer sequences of entities\nthan the direct estimates of transition probabilities in the original model.\n\nThis is a nice and well-written paper. Instead of proposing a fully neural\napproach, the authors build on existing work and just use a neural network to\novercome specific issues in one step. This is a valid approach, but it would be\nuseful to expand the comparison to the existing neural coherence model of Li\nand Hovy. The authors admit being surprised by the very low score the Li and\nHovy model achieves on their task. This makes the reader wonder if there was an\nerror in the experimental setup, if the other model's low performance is\ncorpus-dependent and, if so, what results the model proposed in this paper\nwould achieve on a corpus or task where the other model is more successful. A\ndeeper investigation of these factors would strengthen the argument\nconsiderably.\n\nIn general the paper is very fluent and readable, but in many places definite\narticles are missing (e.g. on lines 92, 132, 174, 488, 490, 547, 674, 764 and\nprobably more). I would suggest proofreading the paper specifically with\narticle usage in mind. The expression \"...limits the model to do X...\", which\nis used repeatedly, sounds a bit unusual. Maybe \"limits the model's capacity to\ndo X\" or \"stops the model from doing X\" would be clearer.\n\n--------------\n\nFinal recommendation adjusted to 4 after considering the author response. I\nagree that objective difficulties running other people's software shouldn't be\nheld against the present authors. The efforts made to test the Li and Hovy\nsystem, and the problems encountered in doing so, should be documented in the\npaper. I would also suggest that the authors try to reproduce the results of Li\nand Hovy on their original data sets as a sanity check (unless they have\nalready done so), just to see if that works for them.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "The paper proposes a convolutional neural network approach to model the\ncoherence of texts. The model is based on the well-known entity grid\nrepresentation for coherence, but puts a CNN on top of it. \n\nThe approach is well motivated and described, I especially appreciate the clear\ndiscussion of the intuitions behind certain design decisions (e.g. why CNN and\nthe section titled 'Why it works').\n\nThere is an extensive evaluation on several tasks, which shows that the\nproposed approach beats previous methods. It is however strange that one\nprevious result could not be reproduced: the results on Li/Hovy (2014) suggest\nan implementation or modelling error that should be addressed.\n\nStill, the model is a relatively simple 'neuralization' of the entity grid\nmodel. I didn't understand why 100-dimensional vectors are necessary to\nrepresent a four-dimensional grid entry (or a few more in the case of the\nextended grid). How does this help? I can see that optimizing directly for\ncoherence ranking would help learn a better model, but the difference of\ntransition chains for up to k=3 sentences vs. k=6 might not make such a big\ndifference, especially since many WSJ articles may be very short.\n\nThe writing seemed a bit lengthy, the paper repeats certain parts in several\nplaces, for example the introduction to entity grids. In particular, section 2\nalso presents related work, thus the first 2/3 of section 6 are a repetition\nand should be deleted (or worked into section 2 where necessary). The rest of\nsection 6 should probably be added in section 2 under a subsection (then rename\nsection 2 as related work).\n\nOverall this seems like a solid implementation of applying a neural network\nmodel to entity-grid-based coherence. But considering the proposed\nconsolidation of the previous work, I would expect a bit more from a full\npaper, such as innovations in the representations (other features?) or tasks.\n\nminor points:\n\n- this paper may benefit from proof-reading by a native speaker: there are\narticles missing in many places, e.g. '_the_ WSJ corpus' (2x), '_the_ Brown ...\ntoolkit' (2x), etc.\n\n- p.1 bottom left column: 'Figure 2' -> 'Figure 1'\n\n- p.1 Firstly/Secondly -> First, Second\n\n- p.1 'limits the model to' -> 'prevents the model from considering ...' ?\n\n- Consider removing the 'standard' final paragraph in section 1, since it is\nnot necessary to follow such a short paper.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range en- tity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.", "histories": [], "id": "323", "title": "A Neural Local Coherence Model"}
