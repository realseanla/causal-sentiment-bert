{"reviews": [{"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "This paper introduces new configurations and training objectives for neural\nsequence models in a multi-task setting. As the authors describe well, the\nmulti-task setting is important because some tasks have shared information\nand in some scenarios learning many tasks can improve overall performance.\n\nThe methods section is relatively clear and logical, and I like where it ended\nup, though it could be slightly better organized. The organization that I\nrealized after reading is that there are two problems: 1) shared features end\nup in the private feature space, and 2) private features end up in the \nshared space. There is one novel method for each problem. That organization up\nfront would make the methods more cohesive. In any case, they introduce one \nmethod that keeps task-specific features out of shared representation\n(adversarial\nloss) and another to keep shared features out of task-specific representations\n(orthogonality constraints). My only point of confusion is the adversarial\nsystem.\nAfter LSTM output there is another layer, D(s^k_T, \\theta_D), relying on\nparameters\nU and b. This output is considered a probability distribution which is compared\nagainst the actual. This means it is possible it will just learn U and b that\neffectively mask task-specific information from  the LSTM outputs, and doesn't \nseem like it can guarantee task-specific information is removed.\n\nBefore I read the evaluation section I wrote down what I hoped the experiments\nwould look like and it did most of it. This is an interesting idea and there\nare \na lot more experiments one can imagine but I think here they have the basics\nto show the validity of their methods. It would be helpful to have best known\nresults on these tasks.\n\nMy primary concern with this paper is the lack of deeper motivation for the \napproach. I think it is easy to understand that in a totally shared model\nthere will be problems due to conflicts in feature space. The extension to \npartially shared features seems like a reaction to that issue -- one would \nexpect that the useful shared information is in the shared latent space and \neach task-specific space would learn features for that space. Maybe this works\nand maybe it doesn't, but the logic is clear to me. In contrast, the authors\nseem to start from the assumption that this \"shared-private\" model has this\nissue. I expected the argument flow to be 1) Fully-shared obviously has this\nproblem; 2) shared-private seems to address this; 3) in practice shared-private\ndoes not fully address this issue for reasons a,b,c.; 4) we introduce a method\nthat more effectively constrains the spaces.\nTable 4 helped me to partially understand what's going wrong with\nshared-private\nand what your methods do; some terms are _usually_ one connotation\nor another, and that general trend can probably get them into the shared\nfeature\nspace. This simple explanation, an example, and a more logical argument flow\nwould help the introduction and make this a really nice reading paper.\n\nFinally, I think this research ties into some other uncited MTL work [1],\nwhich does deep hierarchical MTL - supervised POS tagging at a lower level,\nchunking\nat the next level up, ccg tagging higher, etc. They then discuss at the end\nsome of the qualities that make MTL possible and conclude that MTL only works\n\"when tasks are sufficiently similar.\" The ASP-MTL paper made me think of this\nprevious work because potentially this model could learn what sufficiently\nsimilar is -- i.e., if two tasks are not sufficiently similar the shared model\nwould learn nothing and it would fall back to learning two independent systems,\nas compared to a shared-private model baseline that might overfit and perform\npoorly.\n\n[1]\n@inproceedings{sogaard2016deep,\n  title={Deep multi-task learning with low level tasks supervised at lower\nlayers},\n  author={S{\\o}gaard, Anders and Goldberg, Yoav},\n  booktitle={Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics},\n  volume={2},\n  pages={231--235},\n  year={2016},\n  organization={Association for Computational Linguistics}\n}", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "2"}, {"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "# Paper summary\n\nThis paper presents a method for learning well-partitioned shared and\ntask-specific feature spaces for LSTM text classifiers. Multiclass adversarial\ntraining encourages shared space representations from which a discriminative\nclassifier cannot identify the task source (and are thus generic). The models\nevaluates are a fully-shared, shared-private and adversarial shared-private --\nthe lattermost ASP model is one of the main contributions. They also use\northogonality constraints to help reward shared and private spaces that are\ndistinct. The ASP model has lower error rate than single-task and other\nmulti-task neural models. They also experiment with a task-level cross\nvalidation to explore whether the shared representation can transfer across\ntasks, and it seems to favourably. Finally, there is some analysis of shared\nlayer activations suggesting that the ASP model is not being misled by strong\nweights learned on a specific (inappropriate) task.\n\n# Review summary\n\nGood ideas, well expressed and tested. Some minor comments.\n\n# Strengths\n\n* This is a nice set of ideas working well together. I particularly like the\nfocus on explicitly trying to create useful shared representations. These have\nbeen quite successful in the CV community, but it appears that one needs to\nwork quite hard to create them for NLP.\n* Sections 2, 3 and 4 are very clearly expressed.\n* The task-level cross-validation in Section 5.5 is a good way to evaluate the\ntransfer.\n* There is an implementation and data.\n\n# Weaknesses\n\n* There are a few minor typographic and phrasing errors. Individually, these\nare fine, but there are enough of them to warrant fixing:\n** l:84 the \u201cinfantile cart\u201d is slightly odd -- was this a real example\nfrom the data?\n** l:233 \u201care different in\u201d -> \u201cdiffer in\u201d\n** l:341 \u201cworking adversarially towards\u201d -> \u201cworking against\u201d or\n\u201ccompeting with\u201d?\n** l:434 \u201ctwo matrics\u201d -> \u201ctwo matrices\u201d\n** l:445 \u201care hyperparameter\u201d -> \u201care hyperparameters\u201d\n** Section 6 has a number of number agreement errors\n(l:745/746/765/766/767/770/784) and should be closely re-edited.\n** The shading on the final row of Tables 2 and 3 prints strangely\u2026\n* There is mention of unlabelled data in Table 1 and semi-supervised learning\nin Section 4.2, but I didn\u2019t see any results on these experiments. Were they\nomitted, or have I misunderstood?\n* The error rate differences are promising in Tables 2 and 3, but statistical\nsignificance testing would help make them really convincing. Especially between\nSP-MLT and ASP-MTL results to highlight the benefit of adversarial training. It\nshould be pretty straightforward to adapt the non-parametric approximate\nrandomisation test (see\nhttp://www.lr.pi.titech.ac.jp/~takamura/pubs/randtest.pdf for promising notes a\nreference to the Chinchor paper) to produce these.\n* The colours are inconsistent in the caption of Figure 5 (b). In 5 (a), blue\nis used for \u201cOurs\u201d, but this seems to have swapped for 5 (b). This is worth\nchecking, or I may have misunderstood the caption.\n\n# General Discussion\n\n* I wonder if there\u2019s some connection with regularisation here, as the effect\nof the adversarial training with orthogonal training is to help limit the\nshared feature space. It might be worth drawing that connection to other\nregularisation literature.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}], "abstract": "Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at \\url{http://nlp.fudan.edu.cn/data/}", "histories": [], "id": "352", "title": "Adversarial Multi-task Learning for Text Classification"}
