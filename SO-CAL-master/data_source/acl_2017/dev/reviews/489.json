{"reviews": [{"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "COMMENTS AFTER AUTHOR RESPONSE:\n\nThanks for your response, particularly for the clarification wrt the\nhypothesis. I agree with the comment wrt cross-modal mapping. What I don't\nshare is the kind of equation \"visual = referential\" that you seem to assume. A\nreferent can be visually presented, but visual information can be usefully\nadded to a word's representation in aggregate form to encode perceptual aspects\nof the words' meaning, the same way that it is done for textual information;\nfor instance, the fact that bananas are yellow\nwill not frequently be mentioned in text, and adding visual information\nextracted from images will account for this aspect of the semantic\nrepresentation of the word. This is kind of technical and specific to how we\nbuild distributional models, but it's also relevant if you think of human\ncognition (probably our representation for \"banana\" has some aggregate\ninformation about all the bananas we've seen --and touched, tasted, etc.). \nIt would be useful if you could discuss this issue explicitly, differentiating\nbetween multi-modal distributional semantics in general and the use of\ncross-modal mapping in particular.\n\nAlso, wrt the \"all models perform similarly\" comment: I really\nurge you, if the paper is accepted, to state it in this form, even if it\ndoesn't completely align with your hypotheses/goals (you have enough results\nthat do). It is a better description of the results, and more useful for the\ncommunity, than clinging to the\nn-th digit difference (and this is to a large extent independent of whether the\ndifference\nis actually statistical significant or not: If one bridge has 49% chances of\ncollapsing and another one 50%, the difference may be statistically\nsignificant, but that doesn't really make the first bridge a better bridge to\nwalk on).\n\nBtw, small quibble, could you find a kind of more compact and to the point\ntitle? (More geared towards either generally what you explore or to what you\nfind?)\n\n----------\n\nThe paper tackles an extremely interesting issue, that the authors label\n\"referential word meaning\", namely, the connection between a word's meaning and\nthe referents (objects in the external world) it is applied to. If I understood\nit correctly, they argue that\nthis is different from a typical word meaning representation as obtained e.g.\nwith distributional\nmethods, because one thing is the abstract \"lexical meaning\" of a word and the\nother which label is appropriate for a given referent with specific properties\n(in a specific context, although context is something they explicitly leave\naside in this paper). This hypothesis has been previously explored in work by\nSchlangen and colleagues (cited in the paper). The paper explores referential\nword meaning empirically on a specific version of the task of Referential\nExpression Generation (REG), namely, generating the appropriate noun for a\ngiven visually represented object.\n\n- Strengths:\n\n1) The problem they tackle I find extremely interesting; as they argue, REG is\na problem that had previously been addressed mainly using symbolic methods,\nthat did not easily allow for an exploration of how speakers choose the names\nof the objects. The scope of the research goes beyond REG as such, as it\naddresses the link between semantic representations and reference more broadly.\n\n2) I also like how they use current techniques and datasets (cross-modal\nmapping and word classifiers, the ReferIt dataset containing large amounts of\nimages with human-generated referring expressions) to address the problem at\nhand. \n\n3) There are a substantial number of experiments as well as analysis into the\nresults. \n\n- Weaknesses:\n\n1) The main weakness for me is the statement of the specific hypothesis, within\nthe general research line, that the paper is probing: I found it very\nconfusing.  As a result, it is also hard to make sense of the kind of feedback\nthat the results give to the initial hypothesis, especially because there are a\nlot of them and they don't all point in the same direction.\n\nThe paper says:\n\n\"This paper pursues the hypothesis that an accurate\nmodel of referential word meaning does not\nneed to fully integrate visual and lexical knowledge\n(e.g. as expressed in a distributional vector\nspace), but at the same time, has to go beyond\ntreating words as independent labels.\"\n\nThe first part of the hypothesis I don't understand: What is it to fully\nintegrate (or not to fully integrate) visual and lexical knowledge? Is the goal\nsimply to show that using generic distributional representation yields worse\nresults than using specific, word-adapted classifiers trained on the dataset?\nIf so, then the authors should explicitly discuss the bounds of what they are\nshowing: Specifically, word classifiers must be trained on the dataset itself\nand only word classifiers with a sufficient amount of items in the dataset can\nbe obtained, whereas word vectors are available for many other words and are\nobtained from an independent source (even if the cross-modal mapping itself is\ntrained on the dataset); moreover, they use the simplest Ridge Regression,\ninstead of the best method from Lazaridou et al. 2014, so any conclusion as to\nwhich method is better should be taken with a grain of salt. However, I'm\nhoping that the research goal is both more constructive and broader. Please\nclarify. \n\n2) The paper uses three previously developed methods on a previously available\ndataset. The problem itself has been defined before (in Schlangen et al.). In\nthis sense, the originality of the paper is not high. \n\n3) As the paper itself also points out, the authors select a very limited\nsubset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm\nnot even sure why they limited it this way (see detailed comments below).\n\n4) Some aspects could have been clearer (see detailed comments).\n\n5) The paper contains many empirical results and analyses, and it makes a\nconcerted effort to put them together; but I still found it difficult to get\nthe whole picture: What is it exactly that the experiments in the paper tell us\nabout the underlying research question in general, and the specific hypothesis\ntested in particular? How do the different pieces of the puzzle that they\npresent fit together?\n\n- General Discussion: [Added after author response]\n\nDespite the weaknesses, I find the topic of the paper very relevant and also\nnovel enough, with an interesting use of current techniques to address an \"old\"\nproblem, REG and reference more generally, in a way that allows aspects to be\nexplored that have not received enough attention. The experiments and analyses\nare a substantial contribution, even though, as mentioned above, I'd like the\npaper to present a more coherent overall picture of how the many experiments\nand analyses fit together and address the question pursued.\n\n- Detailed comments:\n\nSection 2 is missing the following work in computational semantic approaches to\nreference:\n\nAbhijeet  Gupta,  Gemma  Boleda,  Marco  Baroni,  and Sebastian  Pado. 2015.  \nDistributional                                            vectors  encode \nreferential        \n\nattributes.\nProceedings of\nEMNLP,\n12-21\n\nAurelie Herbelot and Eva Maria Vecchi.                                           \n2015. \nBuilding\na\nshared\nworld:\nmapping\ndistributional to model-theoretic semantic spaces. Proceedings of EMNLP,\n22\u201332.\n\n142 how does Roy's work go beyond early REG work?\n\n155 focusses links\n\n184 flat \"hit @k metric\": \"flat\"?\n\nSection 3: please put the numbers related to the dataset in a table, specifying\nthe image regions, number of REs, overall number of words, and number of object\nnames in the original ReferIt dataset and in the version you use. By the way,\nwill you release your data? I put a \"3\" for data because in the reviewing form\nyou marked \"Yes\" for data, but I can't find the information in the paper.\n\n229 \"cannot be considered to be names\" ==> \"image object names\"\n\n230 what is \"the semantically annotated portion\" of ReferIt?\n\n247 why don't you just keep \"girl\" in this example, and more generally the head\nnouns of non-relational REs? More generally, could you motivate your choices a\nbit more so we understand why you ended up with such a restricted subset of\nReferIt?\n\n258 which 7 features? (list) How did you extract them?\n\n383 \"suggest that lexical or at least distributional knowledge is detrimental\nwhen learning what a word refers to in the world\": How does this follow from\nthe results of Frome et al. 2013 and Norouzi et al. 2013? Why should\ncross-modal projection give better results? It's a very different type of\ntask/setup than object labeling.\n\n394-395 these numbers belong in the data section\n\nTable 1: Are the differences between the methods statistically significant?\nThey are really numerically so small that any other conclusion to \"the methods\nperform similarly\" seems unwarranted to me. Especially the \"This suggests...\"\npart (407). \n\nTable 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost\nidentical to wac); this is counter-intuitive given the @1 and @2 results. Any\nidea of what's going on?\n\nSection 5.2: Why did you define your ensemble classifier by hand instead of\nlearning it? Also, your method amounts to majority voting, right? \n\nTable 2: the order of the models is not the same as in the other tables + text.\n\nTable 3: you report cosine distances but discuss the results in terms of\nsimilarity. It would be clearer (and more in accordance with standard practice\nin CL imo) if you reported cosine similarities.\n\nTable 3: you don't comment on the results reported in the right columns. I\nfound it very curious that the gold-top k data similarities are higher for\ntransfer+sim-wap, whereas the results on the task are the same. I think that\nyou could squeeze more information wrt the phenomenon and the models out of\nthese results.\n\n496 format of \"wac\"\n\nSection 6 I like the idea of the task a lot, but I was very confused as to how\nyou did and why: I don't understand lines 550-553. What is the task exactly? An\nexample would help. \n\n558 \"Testsets\"\n\n574ff Why not mix in the train set examples with hypernyms and non-hypernyms?\n\n697 \"more even\": more wrt what?\n\n774ff \"Previous cross-modal mapping models ... force...\": I don't understand\nthis claim.\n\n792 \"larger test sets\": I think that you could even exploit ReferIt more (using\nmore of its data) before moving on to other datasets.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "3", "PRESENTATION_FORMAT": "Poster", "comments": "AFTER AUTHOR RESPONSE\n\nI accept the response about emphasizing novelty of the task and comparison with\nprevious work. Also increase ratings for the dataset and software that are\npromised to become public before the article publishing.\n\n======================\n\nGENERAL \nThe paper presents an interesting empirical comparison of 3 referring\nexpression generation models. The main novelty lies in the comparison of a yet\nunpublished model called SIM-WAP (in press by Anonymous). The model is\ndescribed in SECTION 4.3 but it is not clear whether it is extended or modified\nanyhow in the current paper.  \n\nThe novelty of the paper may be considered as the comparison of the unpublished\nSIM-WAP model to existing 2 models. This complicates evaluation of the novelty\nbecause similar experiments were already performed for the other two models and\nit is unclear why this comparison was not performed in the paper where SIM-WAP\nmodel was presented. A significant novelty might be the combined model yet this\nis not stated clearly and the combination is not described with enough details.\n\nThe contribution of the paper may be considered the following: the side-by-side\ncomparison of the 3 methods for REG; analysis of zero-shot experiment results\nwhich mostly confirms similar observations in previous works; analysis of the\ncomplementarity of the combined model.                     \n\nWEAKNESSES\nUnclear novelty and significance of contributions. The work seems like an\nexperimental extension of the cited Anonymous paper where the main method was\nintroduced.    \n\nAnother weakness is the limited size of the vocabulary in the zero-shot\nexperiments that seem to be the most contributive part. \n\nAdditionally, the authors never presented significance scores for their\naccuracy results. This would have solidified the empirical contribution of the\nwork which its main value.   \n\nMy general feeling is that the paper is more appropriate for a conference on\nempirical methods such as EMNLP. \n\nLastly, I have not found any link to any usable software. Existing datasets\nhave been used for the work.  \n\nObservations by Sections: \n\nABSTRACT\n\"We compare three recent models\" -- Further in the abstract you write that you\nalso experiment with the combination of approaches. In Section 2 you write that\n\"we present a model that exploits distributional knowledge for learning\nreferential word meaning as well, but explore and compare different ways of\ncombining visual and lexical aspects of referential word meaning\" which\neventually might be a better summarization of the novelty introduced in the\npaper and give more credit to the value of your work. \n\nMy suggestion is to re-write the abstract (and eventually even some sections in\nthe paper) focusing on the novel model and results and not just stating that\nyou compare models of others.                  \n\nINTRODUCTION \n\"Determining such a name is is\" - typo \n\"concerning e.g.\" -> \"concerning, e.g.,\" \n\"having disjunct extensions.\" - specify or exemplify, please \n\"building in Figure 1\" -> \"building in Figure 1 (c)\"\n\nSECTION 4\n\"Following e.g. Lazaridou et al. (2014),\" - \"e.g.\" should be omitted  \n\nSECTION 4.2\n\"associate the top n words with their corresponding distributional vector\" -\nWhat are the values of N that you used? If there were any experiments for\nfinding the optimal values, please, describe because this is original work. The\nuse top N = K is not obvious and not obvious why it should be optimal (how\nabout finding similar vectors to each 5 in top 20?)    \n\nSECTION 4.3 \n\"we annotate its training instances with a fine-grained similarity signal\naccording to their object names.\" - please, exemplify. \n\nLANGUAGE   \nQuite a few typos in the draft. Generally, language should be cleaned up (\"as\nwell such as\"). \nAlso, I believe the use of American English spelling standard is preferable\n(e.g., \"summarise\" -> \"summarize\"). Please, double check with your conference\ntrack chairs.", "SOUNDNESS_CORRECTNESS": "5", "ORIGINALITY": "5", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "4"}], "abstract": "We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings.  We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.", "histories": [], "id": 489, "title": "Obtaining referential word meanings from visual and distributional information: Experiments on object naming"}
