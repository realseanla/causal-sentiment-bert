{"reviews": [{"IMPACT": "3", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Oral Presentation", "comments": "The paper presents two approaches for generating English poetry. The first\napproach combine a neural phonetic encoder predicting the next phoneme with a\nphonetic-orthographic HMM decoder computing the most likely word corresponding\nto a sequence of phonemes. The second approach combines a character language\nmodel with a weigthed FST to impose rythm constraints on the output of the\nlanguage model. For the second approach, the authors also present a heuristic\napproach which permit constraining the generated poem according to theme (e.g;,\nlove) or poetic devices (e.g., alliteration). The generated poems are evaluated\nboth instrinsically by comparing the rythm of the generated lines with a gold\nstandard and extrinsically by asking 70 human evaluators to (i) determine\nwhether the poem was written by a human or a machine and (ii) rate poems wrt to\nreadability, form and evocation.  The results indicate that the second model\nperforms best and that human evaluators find it difficult to distinguish\nbetween human written and machine generated poems.\n\nThis is an interesting, clearly written article with novel ideas (two different\nmodels for poetry generation, one based on a phonetic language model the other\non a character LM) and convincing results.\n\n For the evaluation, more precision about the evaluators and the protocol would\nbe good. Did all evaluators evaluate all poems and if not how many judgments\nwere collected for each poem for each task ? You mention 9 non English native\nspeakers. Poems are notoriously hard to read. How fluent were these ? \n\nIn the second model (character based), perhaps I missed it, but do you have a\nmechanism to avoid generating non words ? If not, how frequent are non words in\nthe generated poems ?\n\nIn the first model, why use an HMM to transliterate from phonetic to an\norhographic representation rather than a CRF? \n\nSince overall, you rule out the first model as a good generic model for\ngenerating poetry, it might have been more interesting to spend less space on\nthat model and more on the evaluation of the second model. In particular, I\nwould have been interested in a more detailed discussion of the impact of the\nheuristic you use to constrain theme or poetic devices. How do these impact\nevaluation results ? Could they be combined to jointly constrain theme and\npoetic devices ? \n\nThe combination of a neural mode with a WFST is reminiscent of the following\npaper which combine character based neural model to generate from dialog acts\nwith an WFST to avoid generating non words. YOu should relate your work to\ntheirs and cite them. \n\nNatural Language Generation through Character-Based RNNs with Finite-State\nPrior Knowledge\nGoyal, Raghav and Dymetman, Marc and Gaussier, Eric and LIG, Uni\nCOLING 2016", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "4", "CLARITY": "4", "REVIEWER_CONFIDENCE": "3"}, {"IMPACT": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "MEANINGFUL_COMPARISON": "5", "PRESENTATION_FORMAT": "Poster", "comments": "The paper describes two methodologies for the automatic generation of rhythmic\npoetry. Both rely on neural networks, but the second one allows for better\ncontrol of form.\n\n- Strengths:\n\nGood procedure for generating rhythmic poetry.\n\nProposals for adding control of theme and poetic devices (alliteration,\nconsonance, asonance).\n\nStrong results in evaluation of rhythm.\n\n- Weaknesses:\n\nPoor coverage of existing literature on poetry generation.\n\nNo comparison with existing approaches to poetry generation.\n\nNo evaluation of results on theme and poetic devices.\n\n- General Discussion:\n\nThe introduction describes the problem of poetry generation as divided into two\nsubtasks: the problem of content (the poem's semantics) and the problem of form\n(the \n\naesthetic rules the poem follows). The solutions proposed in the paper address\nboth of these subtasks in a limited fashion. They rely on neural networks\ntrained over corpora \n\nof poetry (represented at the phonetic or character level, depending on the\nsolution) to encode the linguistic continuity of the outputs. This does indeed\nensure that the \n\noutputs resemble meaningful text. To say that this is equivalent to having\nfound a way of providing the poem with appropriate semantics would be an\noverstatement. The \n\nproblem of form can be said to be addressed for the case of rhythm, and partial\nsolutions are proposed for some poetic devices. Aspects of form concerned with\nstructure at a \n\nlarger scale (stanzas and rhyme schemes) remain beyond the proposed solutions.\nNevertheless, the paper constitutes a valuable effort in the advancement of\npoetry generation.\n\nThe review of related work provided in section 2 is very poor. It does not even\ncover the set of previous efforts that the authors themselves consider worth\nmentioning in their paper (the work of Manurung et al 2000 and Misztal and\nIndurkhya 2014 is cited later in the paper - page 4 - but it is not placed in\nsection 2 with respect to the other authors mentioned there).\n\nA related research effort of particular relevance that the authors should\nconsider is:\n\n- Gabriele Barbieri, Fran\u00e7ois Pachet, Pierre Roy, and Mirko Degli Esposti.\n2012. Markov constraints for generating lyrics with style. In Proceedings of\nthe 20th European Conference on Artificial Intelligence (ECAI'12), Luc De\nRaedt, Christian Bessiere, Didier Dubois, Patrick Doherty, and Paolo Frasconi\n(Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 115-120. DOI:\nhttps://doi.org/10.3233/978-1-61499-098-7-115\n\nThis work addresses very similar problems to those discussed in the present\npaper (n-gram based generation and the problem of driving generation process\nwith additional constraints). The authors should include a review of this work\nand discuss the similarities and differences with their own.\n\nAnother research effort that is related to what the authors are attempting (and\nhas bearing on their evaluation process) is:\n\n- Stephen McGregor, Matthew Purver and Geraint Wiggins, Process Based\nEvaluation of Computer Generated Poetry,  in: Proceedings of the INLG 2016\nWorkshop on Computational Creativity and Natural Language Generation, pages\n51\u201360,Edinburgh, September 2016.c2016 Association for Computational\nLinguistics\n\nThis work is also similar to the current effort in that it models language\ninitially at a phonological level, but considers a word n-gram level\nsuperimposed on that, and also features a layer representint sentiment. Some of\nthe considerations McGregor et al make on evaluation of computer generated\npoetry are also relevant for the extrinsic evaluation described in the present\npaper.\n\nAnother work that I believe should be considered is:\n\n- \"Generating Topical Poetry\" (M. Ghazvininejad, X. Shi, Y. Choi, and K.\nKnight), Proc. EMNLP, 2016.\n\nThis work generates iambic pentameter by combining finite-state machinery with\ndeep learning. It would be interesting to see how the proposal in the current\npaper constrasts with this particular approach.\n\nAlthough less relevant to the present paper, the authors should consider\nextending their classification of poetry generation systems (they mention\nrule-based expert systems and statistical approaches) to include evolutionary\nsolutions. They already mention in their paper the work of Manurung, which is\nevolutionary in nature, operating over TAG grammars.\n\nIn any case, the paper as it stands holds little to no effort of comparison to\nprior approaches to poetry generation. The authors should make an effort to\ncontextualise their work with respect to previous efforts, specially in the\ncase were similar problems are being addressed (Barbieri et al, 2012) or\nsimilar methods are being applied (Ghazvininejad,  et al, 2016).", "SOUNDNESS_CORRECTNESS": "3", "ORIGINALITY": "4", "is_meta_review": null, "RECOMMENDATION": "3", "CLARITY": "3", "REVIEWER_CONFIDENCE": "4"}], "abstract": "We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machine-generated poem to be the best amongst all evaluated.", "histories": [], "id": "660", "title": "Automatically Generating Rhythmic Verse with Neural Networks"}
