{"title": "Measuring Topic Quality using Word Buckets", "abstract": "Measuring topic quality is essential for scoring the learned topics and their subsequent use in Information Retrieval and Text classification. To measure quality of Latent Dirichlet Allocation (LDA) based topics learned from text, we propose a novel approach based on grouping of topic words into buckets (TBuckets). A single large bucket signifies a single coherent theme, in turn indicating high topic coherence. TBuckets represents topic words using their word embeddings and employs 3 different techniques for creating buckets of words - i) clustering based, ii) using singular value decomposition (SVD) and iii) SVD with reorganization. The TBuckets approach outperforms the state-of-the-art techniques when evaluated using three publicly available datasets. Further, we demonstrate the usefulness of TBuckets for the task of weakly supervised text classification.", "id": "103", "reviews": [{"comments": "This paper proposes a method for evaluating topic quality based on using word\nembeddings to calculate similarity (either directly or indirectly via matrix\nfactorisation), achieving impressive results over standard datasets.\n\nThe proposed method represents a natural but important next step in the\nevolutionary path of research on topic evaluation. The thing that troubled me\nmost with the results was that, while you achieve state-of-the-art results for\nall three datasets, there are large inconsistencies in which methods perform\nand which methods perform less well (below the state of the art). In practice,\nnone of the proposed methods consistently beats the state of the art, and the\nSVD-based methods perform notably badly over the genomics dataset. For someone\nwho wants to take your method off the shelf and use it over any arbitrary\ndataset, this is a considerable worry. I suspect that the lower results for\nSVD over genomics relate to the proportion of OOV terms (see comment below),\nand that it may be possible to automatically predict which method will perform\nbest based on vocab match with GloVe etc., but there is no such discussion in\nthe paper.\n\nOther issues:\n\n- the proposed method has strong similarities with methods proposed in the\n  lexical chaining literature, which I would encourage the authors to read up\n  on and include in any future version of the paper\n\n- you emphasis that your method has no parameters, but the word embedding\n  methods have a large number of parameters, which are implicit in your\n  method. Not a huge deal, but worth acknowledging\n\n- how does your method deal with OOV terms, e.g. in the genomics dataset\n  (i.e. terms not present in the pretrained GloVe embeddings)? Are they simply\n  ignored? What impact does this have on the method?\n\nLow-level issues:\n\n- in your description of word embeddings in Section 2.1, you implicitly assume\n  that the length of the vector is unimportant (in saying that cosine\n  similarity can be used to measure the similarity between two vectors). If\n  the vectors are unit length, this is unproblematic, but word2vec actually\n  doesn't return unit-length vectors (the pre-trained vectors have been\n  normalised post hoc, and if you run word2vec yourself, the vector length is\n  certainly not uniform). A small detail, but important.\n\n- the graphs in Figure 1 are too small to be readable", "is_meta_review": null, "RECOMMENDATION": "2", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "3", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "3", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}, {"comments": "This paper proposes a new method for the evaluation of topic models that\npartitions the top n words of each topic into clusters or \"buckets\" based on\ncosine similarity of their associated word embeddings. In the simplest setup,\nthe words are considered one by one, and each is either put into an existing\n\"bucket\" \u00e2\u0080\u0093 if its cosine similarity to the other words in the bucket is below\na certain threshold \u00e2\u0080\u0093 or a new bucket is created for the word. Two more\ncomplicated methods based on eigenvectors and reorganisation are also\nsuggested. The method is evaluated on three standard data sets and in a  weakly\nsupervised text classification setting. It outperforms or is en par with the\nstate of the art (R\u00c3\u00b6der et al., 2015).\n\nThe basic idea behind the paper is rather simple and has a certain ad\nhoc-flavour. The authors do not offer any new explanations for why topic\nquality should be measurable in terms of word\u00e2\u0080\u0093word similarity. It is not\nobvious to me why this should be so, given that topics and word embeddings are\ndefined with respect to two rather different notions of context (document vs.\nsequential context). At the same time, the proposed method seems to work quite\nwell. (I would like to see some significance tests for Table 1 though.)\n\nOverall the paper is clearly written, even though there are some language\nissues. Also, I found the description of the techniques in Section 3 a bit hard\nto follow; I believe that this is mostly due to the authors using passive voice\n(\"the threshold is computed as\") in places were they were actually making a\ndesign choice. I find that the authors should try to explain the different\nmethods more clearly, with one subsection per method. There seems to be some\nspace for that: The authors did not completely fill the 8 pages of content, and\nthey could easily downsize the rather uninformative \"trace\" of the method on\npage 3.\n\nOne question that I had was how sensitive the proposed technique was to\ndifferent word embeddings. For example, how would the scores be if the authors\nhad used word2vec instead of GloVe?", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "3", "REVIEWER_CONFIDENCE": "3", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}]}
