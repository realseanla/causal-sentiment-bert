{"title": "When a Red Herring is Not a Red Herring: Using Compositional Methods to Improve the Detection of Non-Compositional Phrases", "abstract": "Non-compositional phrases such as \\emph{red herring} and weakly compositional phrases such as \\emph{spelling bee} are an integral part of natural language.  They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics.  Compositionality detection therefore provides a good testbed for compositional methods. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings.", "id": "137", "reviews": [{"comments": "General comments\n=============================\nThe paper reports experiments on predicting the level of compositionality of\ncompounds in English. \nThe dataset used is a previously existing set of 90 compounds, whose\ncompositionality was ranked from 1 to 5\n(by a non specified number of judges).\nThe general form of each experiment is to compute a cosine similarity between\nthe vector of the compound (treated as one token) and a composition of the\nvectors of the components.\nEvaluation is performed using a Spearman correlation between the cosine\nsimilarity and the human judgments.\n\nThe experiments vary\n- for the vectors used: neural embeddings versus syntactic-context count\nvectors\n- and for the latter case, whether plain or \"aligned\" vectors should be used,\nfor the dependent component of the compound. The alignment tries to capture a\nshift from the dependent to the head. Alignment were proposed in a previous\nsuppressed reference.\n\nThe results indicate that syntactic-context count vectors outperform\nembeddings, and the use of aligned alone performs less well than non-modified\nvectors, and a highly-tuned combination of aligned and unaligned vectors\nprovides a slight improvement.\n\nRegarding the form of the paper, I found the introduction quite well written,\nbut other parts (like section 5.1) are difficult to read, although the\nunderlying notions are not very complicated. Rephrasing with running examples\ncould help.\n\nRegarding the substance, I have several concerns:\n\n- the innovation with respect to Reddy et al. seems to be the use of the\naligned vectors\nbut they have been published in a previous \"suppressed reference\" by the\nauthors.\n\n- the dataset is small, and not enough described. In particular, ranges of\nfrequences are quite likely to impact the results. \nSince the improvements using aligned vectors are marginal, over a small\ndataset, in which it is unclear how the choice of the compounds was performed,\nI find that the findings in the paper are quite fragile.\n\nMore detailed comments/questions\n================================\n\nSection 3\n\nI don't understand the need for the new name \"packed anchored tree\".\nIt seems to me a plain extraction of the paths between two lexical items in a\ndependency tree,\nnamely a plain extension of what is traditionally done in syntactic\ndistributional representations of words\n(which typically (as far as Lin 98) use paths of length one, or length 2, with\ncollapsed prepositions).\n\nFurther, why is it called a tree? what are \"elementary APTs\" (section 5.1) ?\n\nTable 2 : didn't you forget to mention that you discard features of order more\nthan 3 \n(and that's why for instance NMOD.overline(NSUBJ).DOBJ does not appear in\nleftmost bottom cell of table 2\nOr does it have to do with the elimination of some incompatible types you\nmention\n(for which an example should be provided, I did not find it very clear).\n\nSection 4:\n\nSince the Reddy et al. dataset is central to your work, it seems necessary to\nexplain how the 90 compounds were selected. What are the frequency ranges of\nthe compounds / the components etc... ? There is a lot of chance that results\nvary depending on the frequency ranges.\n\nHow many judgments were provided for a given compound? Are there many compounds\nwith same final compositionality score? Isn't it a problem when ranking them to\ncompute the Spearman correlation ?\n\nApparently you use \"constituent\" for a component of the N N sequence. I would\nsuggest \"component\", as \"constituent\" also has the sense of \"phrase\" (syntagm).\n\n\"... the intuition that if a constituent is used literally within a phrase then\nit is highly likely that the compound and the constituent share co-occurrences\"\n: note the intuition is certainly true if the constituent is the head of the\nphrase, otherwise much less true (e.g. \"spelling bee\" does not have the\ndistribution of \"spelling\").\n\nSection 5\n\n\"Note that the elementary representation for the constituent of a compound\nphrase will not contain any of the contextual features associated with the\ncompound phrase token unless they occurred with the constituent in some other\ncontext. \"\nPlease provide a running example in order to help the reader follow which\nobject you're talking about.\nDoes \"compound phrase token\" refer to the merged components of the compound?\n\nSection 5.1\n\nI guess that \"elementary APTs\" are a triplet target word w + dependency path r\n+ other word w'?\nI find the name confusing.\n\nClarify whether \"shifted PMI\" refer to PMI as defined in equation (3).\n\n\"Removing features which tend to go with lots of\n things (low positive PMI) means that these phrases\n appear to have been observed in a very small num-\n ber of (highly informative) contexts.\"\nDo \"these phrases\" co-refer with \"things\" here?\nThe whole sentence seems contradictory, please clarify.\n\n\"In general, we would expect there to be little 558\noverlap between APTs which have not been prop-\nerly aligned.\"\nWhat does \"not properly aligned\" means? You mean not aligned at all?\n\nI don't understand paragraph 558 to 563.\nWhy should the potential overlap be considerable\nin the particular case of the NMOD relation between the two components?\n\nParagraph 575 to 580 is quite puzzling.\nWhy does the whole paper make use of higher order dependency features\nand then suddenly, at the critical point of actually measuring the crucial\nmetric\nof similarity between composed and observed phrasal vectors, you use\nfirst order features only?\n\nNote 3 is supposed to provide an answer, but I don't understand the explanation\nof why the 2nd order paths in the composed representations are not reliable,\nplease clarify.\n\nSection 6\n\n\"Smoothing the PPMI calculation with a value of \u00ce\u00b1 = 0.75 generally has a 663\nsmall positive effect.\"\ndoes not seem so obvious from table 3.\n\nWhat are the optimal values for h and q in equation 8 and 9? They are important\nin order to estimate\nhow much of \"hybridity\" provides the slight gains with respect to the unaligned\nresults.\n\nIt seems that in table 4 results correspond to using the add combination, it\ncould help to have this in the legend.\nAlso, couldn't you provide the results from the word2vec vectors for the\ncompound phrases?\n\nI don't understand the intuition behind the FREQ baseline. Why would a frequent\ncompound tend to be compositional? This suggests maybe a bias in the dataset.", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "3", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "2", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "2", "ORIGINALITY": "2"}]}
