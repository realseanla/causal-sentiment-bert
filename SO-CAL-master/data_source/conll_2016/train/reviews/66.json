{"title": "Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs", "abstract": "We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008--9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.", "id": "66", "reviews": [{"comments": "This paper presents a Stack LSTM parser based on the work of Henderson et al.\n(2008, 2013) on joint syntactic/semantic transition-based parsing and Dyer et\nal. (2015) on stack LSTM syntactic parsing. The use of the transition system\nfrom the former and the stack LSTM from the latter shows interesting results\ncompared to the joint systems on the CoNLL 2008 and 2009 shared tasks.\n\nI like this paper a lot because it is well-written, well-explained, the related\nwork is good and the results are very interesting. The methodology is sound\n(with a minor concern regarding the Chinese embeddings, leading me to believe\nthan very good embeddings can be more informative than a very clever model...).\n\nMoreover, the description of the system is clear, the hyperparameters are\njustified and the discussion is interesting.\n\nThe only thing I would say is that the proposed system lacks originality in the\nsense that the work of Henderson et al. puts the basis of semi-synchronised\njoint syntax-semantic transition-based parsing several years ago and Dyer et\nal. came up with the stack LSTM last year, so it is not a new method, per say.\nBut in my opinion, we were waiting for such a parser to be designed and so I'm\nglad it was done here.", "is_meta_review": null, "RECOMMENDATION": "5", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "5", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "5", "REVIEWER_CONFIDENCE": "5", "SOUNDNESS_CORRECTNESS": "5", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "3"}, {"comments": "General comments\n================\n\nThe paper presents a joint syntactic and semantic transition-based dependency\nparser,\ninspired from the joint parser of Henderson et al. (2008).\nThe authors claim two main differences:\n- vectorial representations are used for the whole parser's state, instead of\nthe top elements of the stack / the last parser's configurations\n- the algorithm is a plain greedy search\n\nThe key idea is to take advantage of stack LSTMs so that the vector\nrepresenting the state of the parser\nkeeps memory of potentially large scoped syntactic features, which\nare known to be decisive features for semantic role labeling\n(such as the path between the predicate and the candidate role filler head).\n\nThe system is tested on the CoNLL 2008 data set (English) and on the\nmultilingual CoNLL 2009 data set.\nThe authors compare their system's performance to previously reported\nperformances,\nshowing their system does well compared to the 2008 / 2009 systems, \nbut less compared to more recent proposals (cf. bottom of table 3).\nThey emphasized though that the proposed system does not require any hand-craft\nfeatures,\nand is fast due to the simple greedy algorithm.\n\nThe paper is well written and describes a substantial amount of work,\nbuilding on the recently popular LSTMs, applied to the Henderson et al.\nalgorithm\nwhich appears now to have been somewhat visionary.\n\nI have reservations concerning the choice of the simple greedy algorithm:\nit renders results not comparable to some of the cited works.\nIt would not have been too much additional work nor space to provide for\ninstance beam-searched performance.\n\nMore detailed comments / questions\n==================================\n\nSection 2:\n\nA comment on the presence of both A1 and C-A1 links would help understanding\nbetter the target task of the paper.\n\nA summary of the differences between the set of transitions used in this work\nand that of Henderson et al. should be provided. In its current form, it is\ndifficult to \ntell what is directly reused from Henderson et al. and what is new / slightly\nmodified.\n\nSection 3.3\n\nWhy do you need representations concatenating the word predicate and its\ndisambiguated sense,\nthis seems redundant since the disambiguated sense are specific to a predicate\n?\n\nSection 4\n\nThe organization if the 4.1 / 4.2 sections is confusing concerning\nmultilinguality.\nConll 2008 focused on English, and CoNLL 2009 shared task extended it to a few\nother languages.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "3", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}, {"comments": "This paper performs an overdue circling-back to the problem of joint semantic\nand syntactic dependency parsing, applying the recent insights from neural\nnetwork models. Joint models are one of the most promising things about the\nsuccess of transition-based neural network parsers.\n\nThere are two contributions here. First, the authors present a new transition\nsystem, that seems better than the Hendersen (2008) system it is based on. The\nother contribution is to show that the neural network succeeds on this problem,\nwhere linear models had previously struggled. The authors attribute this\nsuccess to the ability of the neural network to automatically learn which\nfeatures to extract. However, I think there's another advantage to the neural\nnetwork here, that might be worth mentioning. In a linear model, you need to\nlearn a weight for each feature/class pair. This means that if you jointly\nlearn two problems, you have to learn many more parameters. The neural network\nis much more economical in this respect.\n\nI suspect the transition-system would work just as well with a variety of other\nneural network models, e.g. the global beam-search model of Andor (2016). There\nare many other orthogonal improvements that could be made. I expect extensions\nto the authors' method to produce state-of-the-art results.\n\nIt would be nice to see an attempt to derive a dynamic\noracle for this transition system, even if it's only in an appendix or in\nfollow-up work. At first glance, it seems similar to the\narc-eager oracle. The M-S action excludes all semantic arcs between the word at\nthe start of the buffer and the words on the semantic stack, and the M-D action\nexcludes all semantic arcs between the word at the top of the stack and the\nwords in the buffer. The L and R actions seem to each exclude the reverse arc,\nand no other.", "is_meta_review": null, "RECOMMENDATION": "5", "REPLICABILITY": "5", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "5", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "5", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "3"}]}
