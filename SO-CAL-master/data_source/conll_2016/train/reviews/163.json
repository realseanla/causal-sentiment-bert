{"title": "Redefining part-of-speech classes with distributional semantic models", "abstract": "This paper studies how word embeddings trained on the British National Corpus interact with part of speech boundaries. Our work targets the Universal PoS tag set, which is currently actively being used for annotation of a range of languages. We experiment with training classifiers for predicting PoS tags for words based on their embeddings. The results show that the information about PoS affiliation contained in the distributional vectors allows us to discover groups of words with distributional patterns that differ from other words of the same part of speech.   This data often reveals hidden inconsistencies of the annotation process or guidelines. At the same time, it supports the notion of `soft' or `graded' part of speech affiliations. Finally, we show that information about PoS is distributed among dozens of vector components, not limited to only one or two features.", "id": "163", "reviews": [{"comments": "The aim of this paper is to show that distributional information stored in word\nvector models contain information about POS labels. They use a version of the\nBNC annotated with UD POS and in which words have been replaced by lemmas. They\ntrain word embeddings on this corpus, then use the resulting vectors to train a\nlogistic classifier to predict the word POS. Evaluations are performed on the\nsame corpus (using cross-validation) as well as on other corpora. Results are\nclearly presented and discussed and analyzed at length.\n\nThe paper is clear and well-written. The main issue with this paper is that it\ndoes not contain anything new in terms of NLP or ML. It describe a set of\nstraightforward experiments without any new NLP or ML ideas or methods. Results\nare interesting indeed, in so far that they provide an empirical grounding to\nthe notion of POS. In that regard, it is certainly worth being published in a\n(quantitative/emprirical) linguistic venue.\n\nOn another note, the literature on POS tagging and POS induction using word\nembeddings should be cited more extensively (cf. for instance Lin, Ammar, Duer\nand Levin 2015; Ling et al. 2015 [EMNLP]; Plank, S\u00c3\u00b8gaard and Goldberg\n2016...).", "is_meta_review": null, "RECOMMENDATION": "2", "REPLICABILITY": "5", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "3", "SUBSTANCE": "1", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "3", "IMPACT": "2", "ORIGINALITY": "2"}, {"comments": "## General comments:\nThis paper presents an exploration of the connection between part-of-speech\ntags and word embeddings. Specifically the authors use word embeddings to draw\nsome interesting (if not somewhat straightforward) conclusions about the\nconsistency of PoS tags and the clear connection of word vector representations\nto PoS. The detailed error analysis (outliers of classification) is definitely\na strong point of this paper.\n\nHowever, the paper seems to have missing one critical main point: the reason\nthat corpora such as the BNC were PoS tagged in the first place. Unlike a\npurely linguistic exploration of morphosyntactic categories (which are\nunderlined by a semantic prototype theory - e.g. see Croft, 1991), these\ncorpora were created and tagged to facilitate further NLP tasks, mostly\nparsing. The whole discussion could then be reframed as whether the\ndistinctions made by the distributional vectors are more beneficial to parsing\nas compared to the original tags (or UPOS for that matter). \n\nAlso, this paper is missing a lot of related work in the context of\ndistributional PoS induction. I recommend starting with the review\nChristodoulopoulos et al. 2010 and adding some more recent non-DNN work\nincluding Blunsom and Cohn (2011), Yatbaz et al. (2012), etc. In light of this\nbody of work, the results of section 5 are barely novel (there are systems with\nmore restrictions in terms of their external knowledge that achieve comparable\nresults).\n\n## Specific issues\nIn the abstract one of the contributed results is that \"distributional vectors\ndo contain information about PoS affiliation\". Unless I'm misunderstanding the\nsentence, this is hardly a new result, especially for English: every\ndistributionally-based PoS induction system in the past 15 years that presents\n\"many-to-one\" or \"cluster purity\" numbers shows the same result.\n\nThe assertion in lines 79-80 (\"relations between... vectors... are mostly\nsemantic\") is not correct: the <MIKOLOV or COLOBERT> paper (and subsequent\nwork) shows that there is a lot of syntactic information in these vectors. Also\nsee previous comment about cluster purity scores. In fact you revert that\nstatement in the beginning of section 2 (lines 107-108).\n\nWhy move to UPOS? Surely the fine-grained distinctions of the original tagset\nare more interesting.\n\nI do not understand footnote 3. Were these failed attempts performed by you or\nother works? Under what criteria did they fail? What about Brown cluster\nvectors? They almost perfectly align with UPOS tags.\n\nIs the observation that \"proper nouns are not much similar to common nouns\"\n(lines 331-332) that interesting? Doesn't the existence of \"the\" (the most\nfrequent function word) almost singlehandedly explain this difference?\n\nWhile I understand the practical reasons for analysing the most frequent\nword/tag pairs, it would be interesting to see what happens in the tail, both\nin terms of the vectors and also for the types of errors the classifier makes.\nYou could then try to imagine alternatives to pure distributional (and\nmorphological - since you're lemmatizing) features that would allow better\ngeneralizations of the PoS tags to these low-frequency words.\n\n## Minor issues\nChange the sentential references to \\newcite{}: e.g. \"Mikolov et al. (2013b)\nshowed\"", "is_meta_review": null, "RECOMMENDATION": "2", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "1", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "5", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "2", "ORIGINALITY": "2"}]}
