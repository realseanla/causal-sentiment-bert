{"title": "Coreference in Wikipedia: Main Concept Resolution", "abstract": "Wikipedia is a resource of choice exploited in many NLP applications, yet we are not aware of recent attempts to adapt coreference resolution to this resource. In this work, we revisit a seldom studied task which consists in identifying in a Wikipedia article all the mentions of the main concept being described.  We show that by exploiting the Wikipedia markup of a document, as well as links to external knowledge bases such as Freebase, we can  acquire useful information on entities that helps to classify mentions as coreferent or not. We designed a classifier which drastically outperforms fair baselines built on top of state-of-the-art coreference resolution systems. We also measure the benefits of this classifier in a full coreference resolution pipeline applied to Wikipedia texts.", "id": "11", "reviews": [{"comments": "The authors present a new version of the coreference task tailored to\nWikipedia. The task is to identify the coreference chain specifically\ncorresponding to the entity that the Wikipedia article is about.  The authors\nannotate 30 documents with all coreference chains, of which roughly 25% of the\nmentions refer to the \"main concept\" of the article. They then describe some\nsimple baselines and a basic classifier which outperforms these. Moreover, they\nintegrate their classifier into the Stanford (rule-based) coreference system\nand see substantial benefit over all state-of-the-art systems on Wikipedia.\n\nI think this paper proposes an interesting twist on coreference that makes good\nsense from an information extraction perspective, has the potential to somewhat\nrevitalize and shake up coreference research, and might bridge the gap in an\ninteresting way between coreference literature and entity linking literature. \nI am sometimes unimpressed by papers that dredge up a new task that standard\nsystems perform poorly on and then propose a tweak so that their system does\nbetter. However, in this case, the actual task itself is quite motivating to me\nand rather than the authors fishing for a new domain to run things in, it\nreally does feel like \"hey, wait, these standard systems perform poorly in a\nsetting that's actually pretty important.\"\n\nTHE TASK: Main concept resolution is an intriguing task from an IE perspective.\n I can imagine many times where documents revolve primarily around a particular\nentity (biographical documents, dossiers or briefings about a person or event,\nclinical records, etc.) and where the information we care about extracting is\nspecific to that entity. The standard coreference task has always had the issue\nof large numbers of mentions that would seemingly be pretty irrelevant for most\nIE problems (like generic mentions), and this task is unquestionably composed\nof mentions that actually do matter.\n\nFrom a methodology standpoint, the notion of a \"main concept\" provides a bit of\na discourse anchor that is useful for coreference, but there appears to still\nbe substantial overhead to improve beyond the baselines, particularly on\nnon-pronominal mentions. Doing coreference directly on Wikipedia also opens the\ndoors for more interesting use of knowledge, which the authors illustrate here.\nSo I think this domain is likely to be an interesting testbed for ideas which\nwould improve coreference overall, but which in the general setting would be\nmore difficult to get robust improvements with and which would be dwarfed by\nthe amount of work dealing with other aspects of the problem.\n\nMoreover, unlike past work which has carved off a slice of coreference (e.g.\nthe Winograd schema work), this paper makes a big impact on the metrics of the\n*overall* coreference problem on a domain (Wikipedia) that many in the ACL\ncommunity are pretty interested in.\n\nTHE TECHNIQUES: Overall, the techniques are not the strong point of this paper,\nthough they do seem to be effective. The features seem pretty sensible, but it\nseems like additional conjunctions of these may help (and it's unclear whether\nthe authors did any experimentation in this vein).  The authors should also\nstate earlier in the work that their primary MC resolution system is a binary\nclassifier; this is not explicitly stated early enough and the model is left\nundefined throughout the description of featurization.\n\nMINOR DETAILS:\n\nOrganization: I would perhaps introduce the dataset immediately after \"Related\nWorks\" (i.e. have it be the new Section 3) so that concrete results can be\ngiven in \"Baselines\", further motivating \"Approach\".\n\nWhen Section 4 refers to Dcoref and Scoref, you should cite the Stanford papers\nor make it clear that it's the Stanford coreference system (many will be\nunfamiliar with the Dcoref/Scoref names).\n\nThe use of the term \"candidate list\" was unclear, especially in the following:\n\n\"We leverage the hyperlink structure of the article in order to enrich the list\nof mentions with shallow semantic attributes. For each link found within the\narticle under consideration, we look through the candidate list for all\nmentions that match the surface string of the link.\"\n\nPlease make it clear that the \"candidate list\" is the set of mentions in the\narticle that are possible candidates for being coreferent with the MC.        I think\nmost readers will understand that this module is supposed to import semantic\ninformation from the link structure of Wikipedia (e.g. if a mention is\nhyperlinked to an article that is female in Freebase, that mention is female),\nso try to keep the terminology clear.\n\nSection 6.1 says \"we consider the union of WCR mentions and all mentions\npredicted by the method described in (Raghunathan et al., 2010).\" However,\nSection 4.1 implies that these are the same? I'm missing where additional WCR\nmentions would be extracted.", "is_meta_review": null, "RECOMMENDATION": "5", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "3", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "5", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "4"}]}
