{"title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "abstract": "Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors themselves and linear relationships between them.", "id": "132", "reviews": [{"comments": "A combination of word2vec and LDA could be potentially interesting. The main\nproblem with the current paper is that the technical details are\nincomprehensible. Section 2 needs a complete rewrite so that a reader familiar\nwith word2vec and LDA could relatively easily get a high-level picture of how\nthe models are being combined. The current presentation doesn't achieve that.\n\nMore detailed comments:\n\nThe third paragraph of the introduction makes no sense to me. \"requires\nderiving a new approximation\" - approximation of what? why is it time consuming\nto develop prototypes? Why is it easier to evaluate features?\n\nWhy use the same word vectors for pivot and target (unlike in word2vec)? What's\nthe motivation for that decision?\n\nwhat does it mean to separate words from a marginal distribution?\n\nwhat's co-adaptation?\n\n\"If we only included structure up to this point\" - what kind of structure?\n\n\"it's similarity\" -> its\n\nFootnote 1 breaks anonymity.\n\nThere doesn't appear to be any evaluation. The days when it was ok to just give\nsome example clusters are long gone in NLP. Figure 2 looks like it might be a\nquantitative evaluation, but it's only described in the overly long caption.\n\nThe statement in the conclusion that the model solves word analogies is\noverstating what was shown, which was just a few cherry-picked examples of king\n+ queen etc. sort.\n\nThe Chang ref has the conference/journal name as \"Advances in ...\" You'd like\nme to guess the venue?", "is_meta_review": null, "RECOMMENDATION": "2", "REPLICABILITY": "3", "PRESENTATION_FORMAT": "Poster", "CLARITY": "1", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "2", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}, {"comments": "This paper proposes a neural-styled topic model, extending the objective of\nword2vec to also learn document embeddings, which it then constrains through\nsparsification, hence mimicking the output of a topic model.\n\nI really liked the model that the authors proposed, and found the examples\npresented by the authors to be highly promising. What was really missing from\nthe paper, however, was any empirical evaluation of the model -- evaluation\nentirely falls back on tables of examples, without any indication of how\nrepresentative the examples are, or any attempt to directly compare with\nstandard or neural topic models. Without empirical evaluation, it is\nimpossible to get a sense of the true worth of the model, making it very hard\nto accept the paper. Some ideas of how the authors could have achieved this:\n(1) use the topic representation of each document in a supervised document\ncategorisation setup to compare against a topic model with the same topic\ncardinality (i.e. as an indirect evaluation of the quality of the\nrepresentation); or (2) through direct evaluation over a dataset with document\nsimilarity annotations (based on pairwise comparison over topic vectors).\n\nIt's fantastic that you are releasing code, but you have compromised anonymity\nin publishing the github link in the submitted version of the paper (strictly\nspeaking, this is sufficient for the paper to be rejected outright, but I\nleave that up to the PCs)\n\nOther issues:\n\n- how did you select the examples in Figures 3-6? presenting a subset of the\n  actual topics etc. potentially reeks of cherry picking.\n\n- in Section 2.2.1 you discuss the possibility of calculating word\n  representations for topics based on pairwise comparison with each word in\n  the vocabulary, but this is going to be an extremely expensive process for a\n  reasonable vocab size and number of topics; is this really feasible?\n\n- you say that you identify \"tokens\" using SpaCy in Section 3.1 -- how? You\n  extract noun chunks (but not any other chunk type), similarly to the Section\n  3.2, or something else? Given that you go on to say that you use word2vec\n  pre-trained embeddings (which include only small numbers of multiword\n  terms), it wasn't clear what you were doing here.\n\n- how does your model deal with OOV terms? Yes, in the experiments you report\n  in the paper you appear to train the model over the entire document\n  collection so it perhaps isn't an immediate problem, but there will be\n  contexts where you want to apply the trained model to novel documents, in\n  which case the updating of the word2vec token embeddings is going to mean\n  that any non-updated (OOV, relative to the training collection) word2vec\n  embeddings are not going to be directly comparable to the tuned embeddings.\n\n- the finding that 20 topics worked best over the 20 Newsgroups corpus wasn't\n  surprising given its composition. Possibly another (very simple) form of\n  evaluation here could have been based on some information-theoretic\n  comparison relative to the true document labels, where again you would have\n  been able to perform a direct comparison with LDA etc.\n\n- a couple of other neural topic models that you meed to compare yourself with\n  are:\n\nCao, Ziqiang, Sujian Li, Yang Liu, Wenjie Li, and Heng Ji. \"A Novel Neural\nTopic Model and Its Supervised Extension.\" In AAAI, pp. 2210-2216. 2015.\n\nNguyen, Dat Quoc, Richard Billingsley, Lan Du, and Mark Johnson. \"Improving\nTopic Models with Latent Feature Word Representations.\" Transactions of the\nAssociation for Computational Linguistics 3 (2015): 299-313.\n\nShamanta, Debakar, Sheikh Motahar Naim, Parang Saraf, Naren Ramakrishnan, and\nM. Shahriar Hossain. \"Concurrent Inference of Topic Models and Distributed\nVector Representations.\" In Machine Learning and Knowledge Discovery in\nDatabases, pp. 441-457. Springer International Publishing, 2015.\n\nLow-level things:\n\nline 315: \"it's similarity\" -> \"its similarity\"\n\nline 361: what does it mean for the \"topic basis\" to be affected (and the\n\"are\" is awkward here)\n\n- in the caption of Figure 5, the examples should perhaps be \"terms\" rather\n  than \"words\"\n\n- the reference formatting is all over the place, e.g. \"Advances in ...\",\n  \"Advances in Neural ...\", Roder et al. is missing the conference name, etc.", "is_meta_review": null, "RECOMMENDATION": "2", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "4", "MEANINGFUL_COMPARISON": "3", "SUBSTANCE": "3", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "3"}]}
