{"title": "Semi-supervised Convolutional Networks for Translation Adaptation with Tiny Amount of In-domain Data", "abstract": "In this paper, we propose a method which uses semi-supervised convolutional neural networks (CNNs) to select in-domain training data for statistical machine translation. This approach is particularly effective when only tiny amounts of in-domain data are available. The in-domain data and randomly sampled general-domain data are used to train a data selection model with semi-supervised CNN, then this model computes domain relevance scores for all the sentences in the general-domain data set. The sentence pairs with top scores are selected to train the system. We carry out experiments on 4 language directions with three test domains. Compared with strong baseline systems trained with large amount of data,   this method can improve the performance up to 3.1 BLEU.  Its performances are significant better than three state-of-the-art language model based data selection methods. We also show that the in-domain data used to train the selection model could be as few as 100 sentences, which makes fine-grained topic-dependent translation adaptation possible.", "id": "129", "reviews": [{"comments": "The paper describes an MT training data selection approach that scores and\nranks general-domain sentences using a CNN classifier. Comparison to prior work\nusing continuous or n-gram based language models is well done, even though  it\nis not clear of the paper also compared against bilingual data selection (e.g.\nsum of difference of cross-entropies).\nThe motivation to use a CNN instead of an RNN/LSTM was first unclear to me, but\nit is a strength of the paper to argue that certain sections of a text/sentence\nare more important than others and this is achieved by a CNN. However, the\npaper does not experimentally show whether a BOW or SEQ (or the combination of\nboth( representation is more important and why.\nThe textual description of the CNN (one-hot or semi-supervised using\npre-trained embeddings) \nis clear, detailed, and points out the important aspects. However, a picture of\nthe layers showing how inputs are combined would be worth a thousand words.\n\nThe paper is overall well written, but some parentheses for citations are not\nnecessary (\\citet vs. \\citep) (e.g line 385).\n\nExperiments and evaluation support the claims of the paper, but I am a little\nbit concerned about the method of determining the number of selected in-domain\nsentences (line 443) based on a separate validation set:\n- What validation data is used here? It is also not clear on what data\nhyperparameters of the CNN models are chosen. How sensitive are the models to\nthis?\n- Table 2 should really compare scores of different approaches with the same\nnumber of sentences selected. As Figure 1 shows, the approach of the paper\nstill seems to outperform the baselines in this case. \n\nOther comments:\n- I would be interested in an experiment that compares the technique of the\npaper against baselines when more in-domain data is available, not just the\ndevelopment set.\n- The results or discussion section could feature some example sentences\nselected by the different methods to support the claims made in section 5.4.\n- In regards to the argument of abstracting away from surface forms in 5.4:\nAnother baseline to compare against could have been the work of Axelrod, 2015,\nwho replace some words with POS tags to reduce LM data sparsity to see whether\nthe word2vec embeddings provide an additional advantage over this.\n- Using the sum of source and target classification scores is very similar to\nsource & target Lewis-Moore LM data selection: sum of difference of\ncross-entropies. A reference to this work around line 435 would be reasonable.\n\nFinally, I wonder if you could learn weights for the sum of both source &\ntarget classification scores by extending the CNN model to the\nbilingual/parallel setting.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "3", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "3"}, {"comments": "The paper describes a method for in-domain data selection for SMT with a\nconvolutional neural network classifier, applying the same framework as Johnson\nand Zhang, 2015. The method performs about 0.5 BLEU points better than language\nmodel based data selection, and, unlike the other methods, is robust even if\nonly a very small in-domain data set is provided. \n\nThe paper claims improvements of 3.1 BLEU points. However, from the results we\nsee that improvements of this magnitude are only achieved if there are\nin-domain data in the training set - training only on the in-domain data\nalready produces +2.8 BLEU. It might be interesting to also compare this to a\nsystem which interpolates separate in- and out-domain models. \n\nThe more impressive result, in my opinion, comes from the second experiment,\nwhich demonstrates that the CNN classifier is still effective if there is very\nlittle in-domain data. However, the second experiment is only run on the zh2en\ntask which includes actual in-domain data in the training set, possibly making\nselection easier. Would the result also hold for the other tasks, where there\nis no in-domain data in the training set? The results for the en2es and en2zh\ntask already point in this direction, since the development sets only contain a\nfew hundred sentence pairs. I think the claim would be better supported if\nresults were reported for all tasks when only 100 sentence pairs are used for\ntraining.  \n\nWhen translating social media text one often has to face very different\nproblems from other domains, the most striking being a high OOV rate due to\nnon-conventional spelling (for Latin scripts, at least). The texts can also\ncontain special character sequences such as usernames, hashtags or emoticons.\nWas there any special preprocessing or filtering step applied to the data?  \nSince data selection cannot address the OOV problem, it would be interesting to\nknow in more detail what kinds of improvements are made through adaptation via\ndata selection, maybe by providing examples.   \n\nThe following remarks concern specific sections:\n\nSection 3.2:\n- It could be made clearer how the different vectors (word embeddings, segment\nvectors and one-hot vectors) are combined in the model. An illustration of the\narchitecture would be very helpful. \n- What was the \"designated loss function\"?\n\nSection 5.2:\nFor completeness' sake, it could be mentioned how the system weights were\ntuned.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "3", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "3", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}]}
