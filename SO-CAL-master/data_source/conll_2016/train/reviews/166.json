{"title": "Cross-Lingual Named Entity Recognition via Wikification", "abstract": "Named Entity Recognition (NER) models for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on another, source, language (or multiple source languages).  We introduce a language independent method for NER, building on cross-lingual wikification, a technique that grounds words and phrases in non-English text into English Wikipedia entries. Thus, mentions in any language can be described using a set of categories and FreeBase types, yielding, as we show, strong language-independent features.  With this insight, we propose an NER model that can be applied to all languages in Wikipedia.                        When trained on English, our model outperforms comparable approaches on the standard CoNLL datasets (Spanish, German, and Dutch) and also performs very well on low-resource languages (e.g., Turkish, Tagalog, Yoruba, Bengali, and Tamil) that have significantly smaller Wikipedia. Moreover, our method allows us to train on multiple source languages, typically  improving NER results on the target languages. Finally, we show that our language-independent features can be used also to enhance monolingual NER systems, yielding improved results for all 9 languages.", "id": "166", "reviews": [{"comments": "This paper proposes an approach for multi-lingual named entity recognition\nusing features from Wikipedia. By relying on a cross-lingual Wikifier, it\nidentifies English Wikipedia articles for phrases in a target language and uses\nfeatures based on the wikipedia entry. Experiments show that this new feature\nhelps not only in the monolingual case, but also in the more interesting direct\ntransfer setting, where the English model is tested on a target language.\n\nI liked this paper. It proposes a new feature for named entity recognition and\nconducts a fairly thorough set of experiments to show the utility of the\nfeature. The analysis on low resource and the non-latin languages are\nparticularly interesting.\n\nBut what about named entities that are not on Wikipedia? In addition to the\nresults in the paper, it would be interesting to see results on how these\nentities are affected by the proposed method. \n\nThe proposed method is strongly dependent on the success of the cross-lingual\nwikifier. With this additional step in the pipeline, how often do we get errors\nin the prediction because of errors in the wikifier?\n\nGiven the poor performance of direct transfer on Tamil and Bengali when lexical\nfeatures are added, I wonder if it is possible to regularize the various\nfeature classes differently, so that the model does not become over-reliant on\nthe lexical features.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "5", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "4"}, {"comments": "This paper is concerned with cross-lingual direct transfer of NER models using\na very recent cross-lingual wikification model. In general, the key idea is not\nhighly innovative and creative, as it does not really propose any core new\ntechnology. The contribution is mostly incremental, and marries the two\nresearch paths: (1) direct transfer for downstream NLP tasks (such as NER,\nparsing, or POS tagging), and (2) very recent developments in the cross-lingual\nwikification technology. However, I pretty much liked the paper, as it is built\non a coherent and clear story with enough experiments and empirical evidence to\nsupport its claims, with convincing results. I still have several comments\nconcerning the presentation of the work.\n\nRelated work: a more detailed description in related work on how this paper\nrelates to work of Kazama and Torisawa (2007) is needed. It is also required to\nstate a clear difference with other related NER system that in one way or\nanother relied on the encyclopaedic Wikipedia knowledge. The differences are\nindeed given in the text, but they have to be further stressed to facilitate\nreading and placing the work in context. \n\nAlthough the authors argue why they decided to leave out POS tags as features,\nit would still be interesting to report experiments with POS tags features\nsimilar to Tackstrom et al.: the reader might get an overview supported by\nempirical evidence regarding the usefulness (or its lack) of such features for\ndifferent languages (i.e., for the languages for which universal POS are\navailable at least). \n\nSection 3.3 could contribute from a running example, as I am still not exactly\nsure how the edited model from Tsai and Roth works now (i.e., the given\ndescription is not entirely clear).\n\nSince the authors mention several times that the approaches from Tackstrom et\nal. (2012) and Nothman et al. (2012) are orthogonal to theirs and that they can\nbe combined with the proposed approach, it would be beneficial if they simply\nreported some preliminary results on a selection of languages using the\ncombination of the models. It will add more flavour to the discussion. Along\nthe same line, although I do acknowledge that this is also orthogonal approach,\nwhy not comparing with a strong projection baseline, again to put the results\ninto more even more context, and show the usefulness (or limitations) of\nwikification-based approaches.\n\nWhy is Dutch the best training language for Spanish, and Spanish the best\nlanguage for Yoruba? Only a statistical coincidence or something more\ninteresting is going on there? A paragraph or two discussing these results in\nmore depth would be quite interesting.\n\nAlthough the idea is sound, the results from Table 4 are not that convincing\nwith only small improvements detected (and not in all scenarios). A statistical\nsignificance test reported for the results from Table 4 could help support the\nclaims.\n\nMinor comments:\n\n- Sect. 2.1: Projection can also be performed via methods that do not require\nparallel data, which makes such models more widely applicable (even for\nlanguages that do not have any parallel resources): e.g., see the work of\nPeirsman and Pado (NAACL 2009) or Vulic and Moens (EMNLP 2013) which exploit\nbilingual semantic spaces instead of direct alignment links to perform the\ntransfer.\n\n- Several typos detected in the text, so the paper should gain quite a bit from\na more careful proofreading (e.g., first sentence of Section 3: \"as a the base\nmodel\"; This sentence is not 'parsable', Page 3: \"They avoid the traditional\npipeline of NER then EL by...\", \"to disambiguate every n-grams\" on Page 8)", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}]}
