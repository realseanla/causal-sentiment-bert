{"title": "Leveraging Cognitive Features for Sentiment Analysis", "abstract": "Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers.Statistical classification using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features.  We perform feature significance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment analyzers to handle complex constructs.", "id": "124", "reviews": [{"comments": "This paper proposed a very interesting idea of using cognitive features for\nsentiment analysis and sarcasm detection. More specifically, the eye-movement\npatterns of human annotators are recorded to derive a new set of features. The\nauthors claim that this is the first work to include cognitive features into\nthe NLP community. \n\nStrength: \n1. The paper is generally well written and easy to follow\n2. Very interesting idea which may inspire research in other NLP tasks.\n\nWeakness:\n1. The motivation of using cognitive features for sentiment analysis is not\nvery well justified. I can imagine these features may help reflect the reading\nease, but I don't see why they are helpful in detecting sentiment polarities.\n2. The improvement is marginal after considering cognitive features by\ncomparing Sn+Sr+Gz with Sn+Sr.\n3. Although the authors discussed about the feasibility of the approach in\nSection 7, but I'm not convinced, especially about the example given in section\n7.2, I don't see why this technique is helpful in such a scenario.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "2", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "3", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "4"}, {"comments": "This paper is about introducing eye-tracking features for sentiment analysis as\na type of cognitive feature.  I think that the idea of introducing eye-tracking\nfeatures as a proxy for cognitive load for sentiment analysis is an interesting\none.  \n\nI think the discussion on the features and comparison of feature sets is clear\nand very helpful.  I also like that the feasibility of the approach is\naddressed in section 7.\n\nI wonder if it would help the evaluation if the datasets didn't conflate\ndifferent domains, e.g., the movie review corpus and the tweet corpus.             \nFor one\nit might improve the prediction of movie review (resp. tweets) if the tweets\n(resp. movie reviews) weren't in the training.              It would also make the\nresults\neasier to interpret.  The results in Table 2 would seem rather low compared to\nstate-of-the art results for the Pang and Lee data, but look much better if\ncompared to results for Twitter data.\n\nIn Section 3.3, there are no overlapping snippets in the training data and\ntesting data of datasets 1 and 2, right?  Even if they come from the same\nsources (e.g., Pang & Lee and Sentiment 140).\n\nMinor: some of the extra use of bold is distracting (or maybe it's just me);", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "5", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "2", "SOUNDNESS_CORRECTNESS": "5", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "4"}]}
