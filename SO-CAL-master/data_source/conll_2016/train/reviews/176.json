{"title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "abstract": "We review the task of Sentence Pair Scor- ing, popular in the literature in various forms \u00e2\u0080\u0094 viewed as Answer Sentence Se- lection, Semantic Text Scoring, Next Ut- terance Ranking, Recognizing Textual En- tailment, Paraphrasing or e.g. a component of Memory Networks.  We argue that all such tasks are similar from the model perspective and propose new baselines by comparing the perfor- mance of common IR metrics and popu- lar convolutional, recurrent and attention- based neural models across many Sen- tence Pair Scoring tasks and datasets. We discuss the problem of evaluating ran- domized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored bench- marks. We introduce a unified open source software framework with easily pluggable models and tasks, which enables us to experiment with multi-task reusability of trained sentence models.", "id": "176", "reviews": [{"comments": "This paper proposes the new (to my knowledge) step of proposing to treat a\nnumber of sentence pair scoring tasks (e.g. Answer Set Scoring, RTE,\nParaphrasing,\namong others) as instances of a more general task of understanding semantic\nrelations\nbetween two sentences. Furthermore, they investigate the potential of learning\ngenerally-\napplicable neural network models for the family of tasks. I find this to be an\nexciting\nproposal that's worthy of both presentation at CoNLL and further discussion and\ninvestigation.\n\nThe main problem I have with the paper is that it in fact feels unfinished. It\nshould be\naccepted for publication only with the proviso that a number of updates will be\nmade\nfor the final version:\n1 - the first results table needs to be completed\n2 - given the large number of individual results, the written discussion of\nresults\nis terribly short. Much more interpretation and discussion of the results is\nsorely needed.\n3 - the abstract promises presentation of a new, more challenging dataset which\nthe paper\ndoes not seem to deliver. This incongruity needs to be resolved.\n4 - the results vary quite a bit across different tasks - could some\ninvestigation be made into\nhow and why the models fail for some of the tasks, and how and why they succeed\nfor others?\nEven if no solid answer is found, it would be interesting to hear the authors'\nposition regarding\nwhether this is a question of modeling or rather dissimilarity between the\ntasks. Does it really\nwork to group them into a unified whole?\n5 - please include example instances of the various datasets used, including\nboth prototypical\nsentence pairs and pairs which pose problems for classification\n6 - the Ubu. RNN transfer learning model is recommended for new tasks, but is\nthis because\nof the nature of the data (is it a more general task) or rather the size of the\ndataset? How can\nwe determine an answer to that question?\n\nDespite the unpolished nature of the paper, though, it's an exciting approach\nthat\ncould generate much interesting discussion, and I'd be happy to see it\npublished\nIN A MORE FINISHED FORM.\nI do recognize that this view may not be shared by other reviewers!\n\nSome minor points about language:\n* \"weigh\" and \"weighed\" are consistently used in contexts that rather require\n\"weight\" and\n\"weighted\"\n* there are several misspellings of \"sentence\" (as \"sentene\")\n* what is \"interpunction\"?\n* one instance of \"world overlap\" instead of \"word overlap\"", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "3", "REVIEWER_CONFIDENCE": "2", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "4"}]}
