{"title": "Compression of Neural Machine Translation Models via Pruning", "abstract": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture.  Our main result is that with with retraining, we can recover and even surpass the original performance with an 80%-pruned model.", "id": "91", "reviews": [{"comments": "This paper investigates three simple weight-pruning techniques for NMT, and\nshows that pruning weights based on magnitude works best, and that retraining\nafter pruning can recover original performance, even with fairly severe\npruning.\n\nThe main strength of paper is that the technique is very straightforward and\nthe results are good. It\u00e2\u0080\u0099s also clearly written and does a nice job covering\nprevious work.\n\nA weakness is that the work isn\u00e2\u0080\u0099t very novel, being just an application of a\nknown technique to a new kind of neural net and application (namely NMT), with\nresults that aren\u00e2\u0080\u0099t very surprising. \n\nIt\u00e2\u0080\u0099s not clear to me what practical significance these results have, since to\ntake advantage of them you would need sparse matrix representations, which are\ntrickier to get working fast on a GPU - and after all, speed is the main\nproblem with NMT, not space. (There may be new work that changes this picture,\nsince the field is evolving fast, but if so you need to describe it, and\ngenerally do a better job explaining why we should care about pruning.)\n\nA suggestion for dealing with the above weakness would be to use the pruning\nresults to inform architecture changes. For instance, figure 3 suggests that\nyou might be able to reduce the number of hidden layers to two, and also\npotentially reduce the dimension of source and target embeddings.\n\nAnother suggestion is that you try to make a link between pruning+retraining\nand dropout (eg \u00e2\u0080\u009cA Theoretically Grounded Application of Dropout in Recurrent\nNeural Networks\u00e2\u0080\u009d, Gal, arXiv 2016).\n\nDetailed comments:\n\nLine 111: \u00e2\u0080\u009csoftmax weights\u00e2\u0080\u009d - \u00e2\u0080\u009coutput embeddings\u00e2\u0080\u009d may be a preferable\nterm\n\nS3.2: It\u00e2\u0080\u0099s misleading to call n the \u00e2\u0080\u009cdimension\u00e2\u0080\u009d of the network, and\nspecify all parameter sizes as integer multiples of this number as if this were\na logical constraint.\n\nLine 319: You should cite Bahdanau et al here for the attention idea, rather\nthan Luong for their use of it.\n\nS3.3: Class-uniform and class-distribution seem very similar (and naturally get\nvery similar results); consider dropping one or the other.\n\nFigure 3 suggestion that you could hybridize pruning: use class-blind for most\nclasses, but class-uniform for the embeddings.\n\nFigure 4 should show perplexity too.\n\nWhat pruning is used in section 4.2 & figure 6?\n\nFigure 7: does loss pertain to training or test corpora?\n\nFigure 8: This seems to be missing softmax weights. I found this diagram\nsomewhat hard to interpret; it might be better to give relevant statistics,\nsuch as the proportion of each class that is removed by class-blind pruning at\nvarious levels.\n\nLine 762: You might want to cite Le et al, \u00e2\u0080\u009cA Simple Way to Initialize\nRecurrent Networks of Rectified Linear Units\u00e2\u0080\u009d, arXiv 2015.", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "3", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "2"}, {"comments": "This paper applies the idea of translation model pruning to neural MT. The\nauthors explore three simple threshold and histogram pruning schemes, two of\nwhich are applied separately to each weight class, while the third is applied\nto the entire model. The authors also show that retraining the models produces\nperformance equal to the full model, even when 90% of the weights are pruned.\nAn extensive analysis explains the superiority of the class-blind pruning\nscheme, as well as the performance boost through retraining. \n\nWhile the main idea of the paper is simple, it seems quite useful for\nmemory-restricted applications of NMT. I particularly liked the analysis\nsection which gives further insight into the model components that are usually\ntreated like black boxes. While these insights are interesting by themselves,\nthe paper's main motivation is model compression. This argument would be\nstronger if the paper included some numbers on actual memory consumption of the\ncompressed model in comparison to the uncompressed model.     \n\nSome minor remarks:\n- There is a substantial amount of work on pruning translation models in\nphrase-based SMT, which could be referenced in related work, e.g. \nJohnson, J., Martin, J., Foster, G. and Kuhn, R.: Improving Translation Quality\nby Discarding Most of the Phrasetable. EMNLP 07 or\nZens, R., Stanton, D. and Peng X.: A Systematic Comparison of Phrase Table\nPruning Techniques. EMNLP 12\n\n- It took me a while to understand Figure 5. I would find it more informative\nto add an additional barplot under figure 4 showing highest discarded weight\nmagnitude by class. This would also allow a comparison across all pruning\nmethods.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}]}
