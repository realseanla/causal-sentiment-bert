{"title": "Exploring Prediction Uncertainty in Machine Translation Quality Estimation", "abstract": "Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows.", "id": "142", "reviews": [{"comments": "The paper explores the use of probabilistic models (gaussian processes) to\nregress on the target variable of post-editing time/rates for quality\nestimation of MT output.\nThe paper is well structured with a clear introduction that highlights the\nproblem of QE point estimates in real-world applications. I especially liked\nthe description of the different asymmetric risk scenarios and how they entail\ndifferent estimators.\nFor readers familiar with GPs the paper spends quite some space to reflect\nthem, but I think it is worth the effort to introduce these concepts to the\nreader.\nThe GP approach and the choices for kernels and using warping are explained\nvery clearly and are easy to follow. In general the research questions that are\nto be answered by this paper are interesting and well phrased.\n\nHowever, I do have some questions/suggestions about the Results and Discussion\nsections for Intrinsic Uncertainty Evaluation:\n- Why were post-editing rates chosen over prediction (H)TER? TER is a common\nvalue to predict in QE research and it would have been nice to justify the\nchoice made in the paper.\n- Section 3.2: I don't understand the first paragraph at all: What exactly is\nthe trend you see for fr-en & en-de that you do not see for en-es? NLL and NLPD\n'drastically' decrease with warped GPs for all three datasets.\n- The paper indeed states that it does not want to advance state-of-the-art\n(given that they use only the standard 17 baseline features), but it would have\nbeen nice to show another point estimate model from existing work in the result\ntables, to get a sense of the overall quality of the models.\n- Related to this, it is hard to interpret NLL and NLPD values, so one is\nalways tempted to look at MAE in the tables to get a sense of 'how different\nthe predictions are'. Since the whole point of the paper is to say that this is\nnot the right thing to do, it would be great provide some notion of what is a\ndrastic reduce in NLL/NLPD worth: A qualitative analysis with actual examples.\n\nSection 4 is very nicely written and explains results very intuitively!\n\nOverall, I like the paper since it points out the problematic use of point\nestimates in QE. A difficult task in general where additional information such\nas confidence arguably are very important. The submission does not advance\nstate-of-the-art and does not provide a lot of novelty in terms of modeling\n(since GPs have been used before), but its research questions and goals are\nclearly stated and nicely executed.\n\nMinor problems:\n- Section 4: \"over and underestimates\" -> \"over- and underestimates\"\n- Figure 1 caption: Lines are actually blue and green, not blue and red as\nstated in the caption.\n- If a certain toolkit was used for GP modeling, it would be great to refer to\nthis in the final paper.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "3", "SOUNDNESS_CORRECTNESS": "5", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}]}
