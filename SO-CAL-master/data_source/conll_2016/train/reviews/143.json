{"title": "Massively Multilingual Word Embeddings", "abstract": "We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVECCCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.", "id": "143", "reviews": [{"comments": "This paper describes four methods of obtaining multilingual word embeddings and\na modified QVEC metric for evaluating the efficacy of these embeddings. The\nembedding methods are: \n\n(1) multiCluster : Uses a dictionary to map words to multilingual clusters.\nCluster embeddings are then obtained which serve as embeddings for the words\nthat reside in each cluster. \n\n(2) multiCCA : Extends the approach presented by Faruqui and Dyer (2014) for\nembedding bilingual words, to multilingual words by using English embeddings as\nthe anchor space. Bilingual dictionaries (other_language -> English) are then\nused to obtain projections from other monolingual embeddings for words in other\nlanguages to the anchor space. \n\n(3) multiSkip : Extends the approach presented by Luong et al. (2015b) for\nembedding using source and target context (via alignment), to the multilingual\ncase by extending the objective function to include components for all\navailable parallel corpora. \n\n(4) Translation invariance : Uses a low rank decomposition of the word PMI\nmatrix with an objective with includes bilingual alignment frequency\ncomponents. May only work for  bilingual embeddings. \n\nThe evaluation method uses CCA to maximize the correlation between the word\nembeddings and possibly hand crafted linguistic data. Basis vectors are\nobtained for the aligned dimensions which produce a score which is invariant to\nrotation and linear transformations. The proposed method also extends this to\nmultilingual evaluations. \n\nIn general, the paper is well written and describes the work clearly. A few\nmajor issues:\n\n(1) What is the new contribution with respect to the translation invariance\nembedding approach of Gardner et al.? If it is the extension to multilingual\nembeddings, a few lines explaining the novelty would help. \n\n(2) The use of super-sense annotations across multiple languages is a problem.\nThe number of features in the intersection of multiple languages may become\nreally small. How do the authors propose to address this problem (beyond\nfootnote 9)?\n\n(3) How much does coverage affect the score in table 2? For example, for\ndependency parsing, multi cluster and multiCCA have significantly different\ncoverage numbers with scores that are close. \n\n(4) In general, the results in table 3 do not tell a consistent story. Mainly,\nfor most of the intrinsic metrics, the multilingual embedding techniques do not\nseem to perform the best.  Given that one of the primary goals of this paper\nwas to create embeddings that perform well under the word translation metric\n(intra-language), it is disappointing that the method that performs best (by\nfar) is the invariance approach. It is also strange that the multi-cluster\napproach, which discards inter-cluster (word and language) semantic information\nperforms the best with respect to the extrinsic metrics.\n\nOther questions for the authors:\n\n(1) What is the loss in performance by fixing the word embeddings in the\ndependency parsing task? What was the gain by simply using these embeddings as\nalternatives to the random embeddings in the LSTM stack parser? \n\n(2) Is table 1 an average over the 17 embeddings described in section 5.1? \n\n(3) Are there any advantages of using the multi-Skip approach instead of\nlearning bilingual embeddings and performing multi-CCA to learning projections\nacross the distinct spaces?\n\n(4) The dictionary extraction approach (from parallel corpora via alignments or\nfrom google translate) may not reflect the challenges of using real lexicons.\nDid you explore the use of any real multi-lingual dictionaries?", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "5", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "3", "SUBSTANCE": "3", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "4", "IMPACT": "3", "ORIGINALITY": "3"}, {"comments": "This paper proposes two dictionary-based methods for estimating multilingual\nword embeddings, one motivated in clustering (MultiCluster) and another in\ncanonical correlation analysis (MultiCCA).\nIn addition, a supersense similarity measure is proposed that improves on QVEC\nby substituting its correlation component with CCA, and by taking into account\nmultilingual evaluation.\n The evaluation is performed on a wide range of tasks using the web portal\ndeveloped by the authors; it is shown that in some cases the proposed\nrepresentation methods outperform two other baselines.\n\nI think the paper is very well written, and represents a substantial amount of\nwork done. The presented representation-learning and evaluation methods are\ncertainly timely. I also applaud the authors for the meticulous documentation.\n\nMy general feel about this paper, however, is that it goes (perhaps) in too\nmuch breadth at the expense of some depth. I'd prefer to see a thorougher\ndiscussion of results (e.g. regarding the conflicting outcome for MultiCluster\nbetween 59- and 12-language set-up; regarding the effect of estimation\nparameters and decisions in MultiCluster/CCA). So, while I think the paper is\nof high practical value to me and the research community (improved QVEC\nmeasure, web portal), I frankly haven't learned that much from reading it, i.e.\nin terms of research questions addressed and answered.\n\nBelow are some more concrete remarks.\n\nIt would make sense to include the correlation results (Table 1) for\nmonolingual QVEC and QVEC-CCA as well. After all, it is stated in l.326--328\nthat the proposed QVEC-CCA is an improvement over QVEC.\n\nMinor:\nl. 304: \"a combination of several cross-lingual word similarity datasets\" ->\nthis sounds as though they are of different nature, whereas they are really of\nthe same kind, just different languages, right?\n\np. 3: two equations exceed the column margin\n\nLines 121 and 147 only mention Coulmance et al and Guo et al when referring to\nthe MultiSkip baseline, but section 2.3 then only mentions Luong et al. So,\nwhat's the correspondence between these works?\n\nWhile I think the paper does reasonable justice in citing the related works,\nthere are more that are relevant and could be included:\n\nMultilingual embeddings and clustering:\nChandar A P, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B.,\nRaykar, V. C., and Saha, A. (2014). An autoencoder approach to learning\nbilingual word representations. In NIPS.\nHill, F., Cho, K., Jean, S., Devin, C., and Bengio, Y. (2014). Embedding word\nsimilarity with neural machine translation. arXiv preprint arXiv:1412.6448.\nLu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep\nmultilingual correlation for improved word embeddings. In NAACL.\nFaruqui, M., & Dyer, C. (2013). An Information Theoretic Approach to Bilingual\nWord Clustering. In ACL.\n\nMultilingual training of embeddings for the sake of better source-language\nembeddings:\nSuster, S., Titov, I., and van Noord, G. (2016). Bilingual learning of\nmulti-sense embeddings with discrete autoencoders. In NAACL-HLT.\nGuo, J., Che, W., Wang, H., and Liu, T. (2014). Learning sense-specific word\nembeddings by exploiting bilingual resources. In COLING.\n\nMore broadly, translational context has been explored e.g. in\nDiab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging\nusing parallel corpora. In ACL.", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "5", "PRESENTATION_FORMAT": "Poster", "CLARITY": "5", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "5", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "5", "APPROPRIATENESS": "4", "IMPACT": "4", "ORIGINALITY": "3"}]}
