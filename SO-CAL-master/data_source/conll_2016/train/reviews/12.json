{"title": "Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection", "abstract": "Cross lingual projection of linguistic annotation suffers from many sources of bias and noise, leading to unreliable annotations that cannot be used directly. In this paper, we introduce a novel approach to sequence tagging that learns to correct the errors from cross-lingual projection using an explicit debiasing layer. This is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags. We evaluated with only 1,000 tokens tagged with gold standard tags, along with more plentiful parallel data. Our system equals or exceeds the state-of-the-art on eight simulated low-resource settings, as well as two real low-resource languages, Malagasy and Kinyarwanda.", "id": "12", "reviews": [{"comments": "I reviewed this paper earlier, when it was an ACL 2016 short paper draft. At\nthat point, it had a flaw in the experiment setup, which is now corrected.\n\nSince back then I suggested I'd be willing to accept the draft for another *ACL\nevent provided that the flaw is corrected, I now see no obstacles in doing so.\n\nAnother reviewer did point out that the setup of the paper is somewhat\nartificial if we focus on real low-resource languages, relating to the costs of\n*finding* vs. *paying* the annotators. I believe this should be exposed in the\nwriteup not to oversell the method.\n\nThere are relevant lines of work in annotation projection for extremely\nlow-resource languages, e.g., Johannsen et al. (2016, ACL) and Agic et al.\n(2015, ACL). It would be nice to reflect on those in the related work\ndiscussion for completeness.\n\nIn summary, I think this is a nice contribution, and I vote accept.\n\nIt should be indicated whether the data is made available. I evaluate those\nparts in good faith now, presuming public availability of research.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "3", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}, {"comments": "The paper describes a modification to the output layer of recurrent neural\nnetwork models which enables learning the model parameters from both gold and\nprojected annotations in a low-resource language. The traditional softmax\noutput layer which defines a distribution over possible labels is further\nmultiplied by a fully connected layer which models the noise generation\nprocess, resulting in another output layer representing the distribution over\nnoisy labels. \n\nOverall, this is a strong submission. The proposed method is apt, simple and\nelegant. The paper reports good results on POS tagging for eight simulated\nlow-resource languages and two truly low-resource languages, making use of a\nsmall set of gold annotations and a large set of cross-lingually projected\nannotations for training. The method is modular enough that researchers working\non different NLP problems in low-resource scenarios are likely to use it.\n\nFrom a practical standpoint, the experimental setup is unusual. While I can\nthink of some circumstances where one needs to build a POS tagger with as\nlittle as 1000 token annotations (e.g., evaluations in some DARPA-sponsored\nresearch projects), it is fairly rare. A better empirical validation of the\nproposed method would have been to plot the tagging accuracy of the proposed\nmethod (and baselines) while varying the size of gold annotations. This plot\nwould help answer questions such as: Does it hurt the performance on a target\nlanguage if we use this method while having plenty of gold annotations? What is\nthe amount of gold annotations, approximately, below which this method is\nbeneficial? Does the answer depend on the target language?\n\nBeyond cross-lingual projections, noisy labels could potentially be obtained\nfrom other sources (e.g., crowd sourcing) and in different tag sets than gold\nannotations. Although the additional potential impact is exciting, the paper\nonly shows results with cross-lingual projections with the same tag set. \n\nIt is surprising that the proposed training objective gives equal weights to\ngold vs. noisy labels. Since the setup assumes the availability of a small gold\nannotated corpus, it would have been informative to report whether it is\nbeneficial to tune the contribution of the two terms in the objective function.\n\n\nIn line 357, the paper describes the projected data as pairs of word tokens\n(x_t) and their vector representations \\tilde{y}, but does not explicitly\nmention what the vector representation looks like (e.g., a distribution over\ncross-lingually projected POS tags for this word type). A natural question to\nask here is whether the approach still works if we construct \\tilde{y} using\nthe projected POS tags at the token level (rather than aggregating all\npredictions for the same word type). Also, since only one-to-one word\nalignments are preserved, it is not clear how to construct \\tilde{y} for words\nwhich are never aligned.\n\nLine 267, replace one of the two closing brackets with an opening bracket.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "3", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "4", "MEANINGFUL_COMPARISON": "4", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "5", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "4"}]}
