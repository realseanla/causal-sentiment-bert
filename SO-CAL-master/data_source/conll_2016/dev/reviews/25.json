{"title": "Identifying Temporal Orientation of Word Senses", "abstract": "The ability to capture time information is essential to many natural language processing and information retrieval applications. Therefore, a lexical resource associating word senses to their temporal orientation might be crucial for the computational tasks aiming at the interpretation of language of time in texts. In this paper, we propose a semi-supervised minimum cuts strategy that makes use of WordNet glosses and semantic relations to supplement WordNet entries with temporal information. Intrinsic and extrinsic evaluations show that our approach outperforms prior semi-supervised non-graph classifiers.", "id": "25", "reviews": [{"comments": "This paper presents an approach to tag word senses with temporal information\n(past, present, future or atemporal). They model the problem using a\ngraph-based semi-supervised classification algorithm that allows to combine\nitem specific information - such as the presence of some temporal indicators in\nthe glosses - and the structure of Wordnet - that is semantic relations between\nsynsets \u00e2\u0080\u0093, and to take into account unlabeled data. They perform a full\nannotation of Wordnet, based on a set of training data labeled in a previous\nwork and using the rest of Wordnet as unlabeled data. Specifically, they take\nadvantage of the structure of the label set by breaking the task into a binary\nformulation (temporal vs atemporal), then using the data labeled as temporal to\nperform a finer grained tagging (past, present or future). In order to\nintrinsically evaluate their approach, they annotate a subset of synsets in\nWordnet using crowd-sourcing. They compare their system to the results obtained\nby a state-of-the-art time tagger (Stanford's SUTime) using an heuristic as a\nbackup strategy, and to previous works. They obtain improvements around 11% in\naccuracy, and show that their approach allows performance higher than previous\nsystems using only 400 labeled data. Finally, they perform an evaluation of\ntheir resource on an existing task (TempEval-3) and show improvements of about\n10% in F1 on 4 labels.\n\nThis paper is well-constructed and generally clear, the approach seems sound\nand well justified. This work led to the development of a resource with fine\ngrained temporal information at the word sense level that would be made\navailable and could be used to improve various NLP tasks. I have a few remarks,\nespecially concerning the settings of the experiments.\n\nI think that more information should be given on the task performed in the\nextrinsic evaluation section. An example could be useful to understand what the\nsystem is trying to predict (the features describe \u00e2\u0080\u009centity pairs\u00e2\u0080\u009d but it\nhas not been made clear before what are these pairs) and what are the features\n(especially, what are the entity attributes? What is the POS for a pair, is it\none dimension or two? Are the lemmas obtained automatically?). The sentence\ndescribing the labels used is confusing, I'm not sure to understand what\n\u00e2\u0080\u009cevent to document creation time\u00e2\u0080\u009d and \u00e2\u0080\u009cevent to same sentence event\u00e2\u0080\u009d\nmeans, are they the kind of pairs considered? Are they relations (as they are\ndescribed as relation at the beginning of p.8)? I find unclear the footnote\nabout the 14 relations: why the other relations have to be ignored, what makes\na mapping too \u00e2\u0080\u009ccomplex\u00e2\u0080\u009d? Also, are the scores macro or micro averaged?\nFinally, the ablation study seems to indicate a possible redundancy between\nLexica and Entity with quite close scores, any clue about this behavior?\n\nI have also some questions about the use of the SVM.  For the extrinsic\nevaluation, the authors say that they optimized the parameters of the\nalgorithm: what are these parameters?  And since a SVM is also used within the\nMinCut framework, is it optimized and how? Finally, if it's the LibSVM library\nthat is used (Weka wrapper), I think a reference to LibSVM should be included. \n\nOther remarks:\n- It would be interesting to have the number of examples per label in the gold\ndata, the figures are given for coarse grained labels (127 temporal vs 271\natemporal), but not for the finer grained.\n- It would also be nice to have an idea of the number of words that are\nambiguous at the temporal level, words like \u00e2\u0080\u009cpresent\u00e2\u0080\u009d.\n- It is said in the caption of the table 3 that the results presented are\n\u00e2\u0080\u009csignificantly better\u00e2\u0080\u009d but no significancy test is indicated, neither any\np-value.\n\nMinor remarks:\n- Related work: what kind of task was performed in (Filannino and Nenadic,\n2014)?\n- Related work: \u00e2\u0080\u009crequires a post-calibration procedure\u00e2\u0080\u009d, needs a reference\n(and p.4 in 3.3 footnote it would be clearer to explain calibration)\n- Related work: \u00e2\u0080\u009ctheir model differ from ours\u00e2\u0080\u009d, in what?\n- Table 3 is really too small: maybe, remove the parenthesis, put the\n\u00e2\u0080\u009c(p,r,f1)\u00e2\u0080\u009d in the caption and give only two scores, e.g. prec and f1. The\ncaption should also be reduced.\n- Information in table 4 would be better represented using a graph.\n- Beginning of p.7: 1064 \u00e2\u0086\u0092 1264\n- TempEval-3: reference ?\n- table 6: would be made clearer by ordering the scores for one column\n- p.5, paragraph 3: atemporal) \u00e2\u0086\u0092 atemporal", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "4", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "4", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "2", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "4", "ORIGINALITY": "3"}]}
