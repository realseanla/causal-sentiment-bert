{"title": "Event Linking with Sentential Features from Convolutional Neural Networks", "abstract": "Coreference resolution for event mentions enables extraction systems to process document-level information. Current systems in this area base their decisions on rich semantic features from various knowledge bases, thus restricting them to domains where such external sources are available. We propose a model for this task which does not rely on such features but instead utilizes sentential features coming from convolutional neural networks. Two such networks first process coreference candidates and their respective context, thereby generating latent-feature representations which are tuned towards event aspects relevant for a linking decision. These representations are augmented with lexical-level and pairwise features, and serve as input to a trainable similarity function producing a coreference score. Our model achieves state-of-the-art performance on two datasets, one of which is publicly available. An error analysis points out directions for further research.", "id": "13", "reviews": [{"comments": "This paper models event linking using CNNs. Given event mentions, the authors\ngenerate vector representations based on word embeddings passed through a CNN\nand followed by max-pooling. They also concatenate the resulting\nrepresentations with several word embeddings around the mention. Together with\ncertain pairwise features, they produce a vector of similarities using a\nsingle-layer neural network, and compute a coreference score. \nThe model is tested on an ACE dataset and an expanded version with performance\ncomparable to previous feature-rich systems.\nThe main contribution of the paper, in my opinion, is in developing a neural\napproach for entity linking that combines word embeddings with several\nlinguistic features. It is interesting to find out that just using the word\nembeddings is not sufficient for good performance. Fortunately, the linguistic\nfeatures used are limited and do not require manually-crafted external\nresources.  \n\nExperimental setting\n- It appears that gold trigger words are used rather than predicted ones. The\nauthors make an argument why this is reasonable, although I still would have\nliked to see performance with predicted triggers. This is especially\nproblematic as one of the competitor systems used predicted triggers, so the\ncomparison isn't fair. \n- The fact that different papers use different train/test splits is worrisome.\nI would encourage the authors to stick to previous splits as much as possible. \n\nUnclear points\n- The numbers indicating that cross-sentential information is needed are\nconvincing. However, the last statement in the second paragraph (lines 65-70)\nwas not clear to me.\n- Embeddings for positions are said to be generaties \"in a way similar to word\nembeddings\". How exactly? Are they randomly initialized? Are they lexicalized?\nIt is not clear to me why a relative position next to one word should have the\nsame embedding as a relative position next to a different word.\n- How exactly are left vs right neighbors used to create the representation\n(lines 307-311)? Does this only affect the max-pooling operation?\n- The word embeddings of one word before and one word after the trigger words\nare appended to it. This seems a bit arbitrary. Why one word before and after\nand not some other choice?  \n- It is not clear how the event-mention representation v_e (line 330) is used?\nIn the following sections only v_{sent+lex} appear to be used, not v_e.\n- How are pairwise features used in section 3.2? Most features are binary, so I\nassume they are encoded as a binary vector, but what about the distance feature\nfor example? And, are these kept fixed during training?\n\nOther issues and suggestions\n- Can the approach be applied to entity coreference resolution as well? This\nwould allow comparing with more previous work and popular datasets like\nOntoNotes. \n- The use of a square function as nonlinearity is interesting. Is it novel? Do\nyou think it has applicability in other tasks?\n- Datasets: one dataset is publicly available, but results are also presented\nwith ACE++, which is not. Do you have plans to release it? It would help other\nresearchers compare new methods. At least, it would have been good to see a\ncomparison to the feature-rich systems also on this dataset.\n- Results: some of the numbers reported in the results are quite close.\nSignificance testing would help substantiating the comparisons.\n- Related work: among the work on (entity) coreference resolution, one might\nmention the neural network approach by Wiseman et al. (2015)  \n\nMinor issues\n- line 143, \"that\" is redundant. \n- One of the baselines is referred to as \"same type\" in table 6, but \"same\nevent\" in the text (line 670).        \n\nRefs\n- Learning Anaphoricity and Antecedent Ranking Features for Coreference\nResolution. Sam Wiseman, Alexander M. Rush, Jason Weston, and Stuart M.\nShieber. ACL 2015.", "is_meta_review": null, "RECOMMENDATION": "4", "REPLICABILITY": "3", "PRESENTATION_FORMAT": "Poster", "CLARITY": "4", "MEANINGFUL_COMPARISON": "5", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "4", "APPROPRIATENESS": "5", "IMPACT": "3", "ORIGINALITY": "3"}, {"comments": "This paper presents a model for the task of event entity linking, where they\npropose to use sentential features from CNNs in place of external knowledge\nsources which earlier methods have used. They train a two-part model: the first\npart learns an event mention representation, and the second part learns to\ncalculate a coreference score given two event entity mentions.\n\nThe paper is well-written, well-presented and is easy to follow. I rather like\nthe analysis done on the ACE corpus regarding the argument sharing between\nevent coreferences. Furthermore, the analysis on the size impact of the\ndataset is a great motivation for creating their ACE++ dataset. However, there\nare a few\nmajor issues that need to be addressed:\n\n- The authors fail to motivate and analyze the pros and cons of using CNN for\ngenerating mention representations. It is not discussed why they chose CNN and\nthere are no comparisons to the other models (e.g., straightforwardly an RNN).\nGiven that the improvement their model makes according various metrics against\nthe\nstate-of-the-art is only 2 or 3 points on F1 score, there needs to be more\nevidence that this architecture is indeed superior.\n\n- It is not clear what is novel about the idea of tackling event linking with\nsentential features, given that using CNN in this fashion for a classification\ntask is not new. The authors could explicitly point out and mainly compare to\nany existing continuous space methods for event linking. The choice of methods\nin Table 3 is not thorough enough.\n\n- There is no information regarding how the ACE++ dataset is collected. A major\nissue with the ACE dataset is its limited number of event types, making it too\nconstrained and biased. It is important to know what event types ACE++ covers.\nThis can also help support the claim in Section 5.1 that 'other approaches are\nstrongly tied to the domain where these semantic features are available\u00e2\u0080\u00a6our\napproach does not depend on resources with restricted\u00e2\u0080\u00a6', you need to show\nthat those earlier methods fail on some dataset that you succeed on. Also,\nfor enabling any meaningful comparison in future, the authors should think\nabout making this dataset publicly available.\n\nSome minor issues:\n- I would have liked to see the performance of your model without gold\nreferences in Table 3 as well.\n\n- It would be nice to explore how this model can or cannot be augmented with a\nvanilla coreference resolution system. For the specific example in line 687,\nthe off-the-shelf CoreNLP system readily links 'It' to 'bombing', which can be\nsomehow leveraged in an event entity linking baseline.\n\n- Given the relatively small size of the ACE dataset, I think having a\ncompelling model requires testing on the other available resources as well.\nThis further motivates working on entity and event coreference simultaneously.\nI also believe that testing on EventCorefBank in parallel with ACE is\nessential. \n\n- Table 5 shows that the pairwise features have been quite effective, which\nsignals that feature engineering may still be crucial for having a competitive\nmodel (at least on the scale of the ACE dataset). One would wonder which\nfeatures were the most effective, and why not report how the current set was\nchosen and what else was tried.", "is_meta_review": null, "RECOMMENDATION": "3", "REPLICABILITY": "3", "PRESENTATION_FORMAT": "Oral Presentation", "CLARITY": "5", "MEANINGFUL_COMPARISON": "3", "SUBSTANCE": "4", "REVIEWER_CONFIDENCE": "4", "SOUNDNESS_CORRECTNESS": "3", "APPROPRIATENESS": "5", "IMPACT": "2", "ORIGINALITY": "3"}]}
