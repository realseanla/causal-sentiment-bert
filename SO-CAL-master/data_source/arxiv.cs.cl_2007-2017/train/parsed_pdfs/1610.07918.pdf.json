{
  "name" : "1610.07918.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SEQUENCE SEGMENTATION USING JOINT RNN AND STRUCTURED PREDICTION MODELS",
    "authors" : [ "Yossi Adi", "Joseph Keshet", "Emily Cibelli", "Matthew Goldrick" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— Sequence segmentation, recurrent neural networks (RNNs), structured prediction, word segmentation, voice onset time\n1. INTRODUCTION\nSequence segmentation is an important task for many speech and audio applications such as speaker diarization, laboratory phonology research, speech synthesis, and automatic speech recognition (ASR). Segmentation models can be used as a pre-process step to clean the data (e.g., removing non-speech regions such as music or noise to reduce ASR error [1, 2]). They can also be used as tools in clinically- or theoreticallyfocused phonetic studies that utilize acoustic properties as a dependent measure. For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].\nPrevious work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].\nInspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep\nSupported in part by NIH grant 1R21HD077140.\nnetwork and structure prediction model. Specifically, we jointly optimize RNN and structured loss parameters by using RNN outputs as feature functions for a structured prediction model. First, an RNN encodes the entire speech utterance and outputs new representation for each of the frames. Then, an efficient search is applied over all possible segments so that the most probable one can be selected. We evaluate this approach using two tasks: word segmentation and voice onset time segmentation. In both tasks the input is a speech segment and the goal is to determine the boundaries of the defined event. We show that the proposed approach outperforms previous methods on these two segmentation tasks.\n2. PROBLEM SETTING\nIn the problem of speech segmentation we are provided with a speech utterance, denoted as x̄ = (x1, . . . ,xT ), represented as a sequence of acoustic feature vectors, where each xt ∈ RD (1 ≤ t ≤ T ) is a D-dimensional vector. The length of the speech utterance, T , is not a fixed value, since the input utterances can have different durations.\nEach input utterance is associated with a timing sequence, denoted by ȳ = (y1, . . . , yp), where p can vary across different inputs. Each element yi ∈ Y , where Y = {1, . . . , T} indicates the start time of a new event in the speech signal. We annotate all the possible timing sequence of size p by Yp\nFor example, in word segmentation the goal is to segment a word from silence and noise in the signal. In this case the size of ȳ is 2, namely word onset and offset. However, in phoneme segmentation the goal is to segment every phoneme in a spoken word. In this case the size of ȳ is different for each input sequence.\nGenerally, our method is suitable for different sequence size |ȳ|. In this paper we focused on |ȳ|= 2, and leave the problem of |ȳ|> 2 to future work.\n3. MODEL DESCRIPTION\nWe now describe our model in greater detail. First, we present the structured prediction framework and then discuss how it is combined with an RNN.\nar X\niv :1\n61 0.\n07 91\n8v 1\n[ cs\n.C L\n] 2\n5 O\nct 2\n01 6"
    }, {
      "heading" : "3.1. Structured Prediction",
      "text" : "We consider the following prediction rule withw ∈ Rd, such that ȳ′w is a good approximation to the true label of x̄, as follows:\n(1)ȳ′w(x̄) = argmax ȳ∈Y w>φ(x̄, ȳ)\nFollowing the structured prediction framework, we assume there exists some unknown probability distribution ρ over pairs (x̄, ȳ) where ȳ is the desired output (or reference output) for input x̄. Both x̄ and ȳ are usually structured objects such as sequences, trees, etc. Our goal is to set w so as to minimize the expected cost, or the risk,\nw∗ = argmin w E(x̄,ȳ)∼ρ[`(ȳ, ȳ′w(x̄))]. (2)\nThis objective function is hard to minimize directly since the distribution ρ is unknown. We use a training set S = {(x̄1, ȳ1), . . . , (x̄m, ȳm)} of m examples that are drawn i.i.d. from ρ, and replace the expectation in (2) with a mean over the training set.\nThe cost is often a combinatorial non-convex quantity, which is hard to minimize. Hence, instead of minimizing the cost directly, we minimize a slightly different function called a surrogate loss, denoted ¯̀(w, x̄, ȳ), and closely related to the cost. Overall, the objective function in (2) transforms into the following objective function, denoted as F :\n(3)F (w, x̄, ȳ) = 1\nm m∑ i=1 ¯̀(w, x̄, ȳ)\nIn this work the surrogate loss function is the structural hinge loss [17] defined as\n¯̀(w, x̄, ȳ) = max ȳ′∈Y\n[ `(ȳ, ȳ′)−w>φ(x̄, ȳ) +w>φ(x̄, ȳ′) ] Usually, φ(x̄, ȳ) is manually chosen using data analysis techniques and involves manipulation on local and global features. In the next subsection we describe how to use an RNN as feature functions."
    }, {
      "heading" : "3.2. Recurrent Neural Networks as Feature Functions",
      "text" : "RNN is a deep network architecture that can model the behavior of dynamic temporal sequences using an internal state which can be thought of as memory [18, 19]. RNN provides the ability to predict the current frame label based on the previous frames. Bidirectional RNN is a model composed of two RNNs: the first is a standard RNN while the second reads the input backwards. Such a model can predict the current frame based on both past and future frames. By using the RNN outputs we can jointly train the structured and network models.\nRecall our prediction rule in Eq. (1): notice that φ(x̄, ȳ) can be viewed as ∑p i=1 φ\n′(x̄, yi) where each φ can be extracted using different techniques, e.g., hand-crafted, feedforward neural network, RNNs, etc. We can formulate the\nprediction rule as follows:\n(4)\nȳ′w(x̄) = argmax ȳ∈Yp w>φ(x̄, ȳ)\n= argmax ȳ∈Yp w> p∑ i=1 φ′(x̄, yi)\n= argmax ȳ∈Yp w> p∑ i=1 RNN(x̄, yi),\nwhere the RNN can be of any type and architecture. For example, we can use bidirectional RNN and consider φ as the concatenation of both outputs BI-RNNforward⊕BI-RNNbackward. This is depicted in Figure 1. We call our model DeepSegmentor .\nOur goal is to find the model parameters so as to minimize the risk as in Eq. (2). Recall, we use the structural hinge loss function, and since both the loss function and the RNN are differentiable we can optimize them using gradient based methods such as stochastic gradient descent (SGD). In order to optimize the network parameters using the backpropagation algorithm [20], we must find the outer derivative of each layer with respect to the model parameters and inputs.\nThe derivative of the loss layer with respect to the layer parameters w for the training example (x̄, ȳ) is\n∂F ∂w = φ(x̄, ȳ`w)− φ(x̄, ȳ),\nwhere\nȳ`w = argmax ȳ′∈Yp w>φ(x̄, ȳ′) + `(ȳ, ȳ′). (5)\nSimilarly, the derivatives with respect to the layer’s inputs are\n∂F ∂φ(x̄, ȳ) = −w ∂F ∂φ(x̄, ȳ′) = w.\nThe derivatives of the rest of the layers are the same as an RNN model.\n4. EXPERIMENTAL RESULTS\nWe investigate two segmentation problems; word segmentation and voice onset time segmentation. We describe each of them in details in the following subsections.1"
    }, {
      "heading" : "4.1. Word Segmentation",
      "text" : "In the problem of word segmentation we are provided with a speech utterance which contains a single word; our goal is to predict its start and end times. The ability to determine these timings is crucial to phonetic studies that measure speaker properties (e.g. response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24]."
    }, {
      "heading" : "4.1.1. Dataset",
      "text" : "Our dataset comes from a laboratory study by Fink and Goldrick [23]. Native English speakers were shown a set of 90 pictures. Some participants produced the name of the picture (e.g., saying “cat”, “chair”) while others performed a semantic classification task (e.g., saying “natural”, “manmade”). Productions other than the intended response or disfluencies were excluded. Recordings were randomly assigned to two transcribers who annotated the onset and offset of each word. We analyze a subset of the recordings, including data from 60 participants, evenly distributed across tasks."
    }, {
      "heading" : "4.1.2. Results",
      "text" : "We compare our model to an RNN that was trained using the Negative-Log-Liklihood (NLL). The NLL model makes a binary decision in every frame to predict whether there is voice activity or not. Recall, our goal is to find the start and end times of the word; in this task, the RNN leaves us with a distribution over all possible onsets. To account for this, we apply a smoothing algorithm and find the most probable pair of timings.\nWe trained the DeepSegmentor model using the structured loss function as in (6), denoted as Combined Duration (CD) loss. The motivation for using this function is due to disparities in the manual annotations, which are common and depend both on human errors and objective difficulties in placing the\n1All models were implemented using Torch7 toolkit [21, 22]\nboundaries. Hence we chose a loss function that takes into account the variations in the annotations.\nγ(ȳ, ȳ′) = [|y1 − y′1|−τ ]+ + [|y2 − y′2|−τ ]+, (6)\nwhere [π]+ = max{0, π}, and τ is a user defined tolerance parameter.\nWe use two layers of bidirectional LSTMs for the DeepSegmentor model with dropout [25] after each recurrent layer. We extracted the 13 Mel-Frequency Cepstrum Coefficients (MFCCs), without the deltas, every 10 ms, and use them as inputs to the network. We optimize the networks using AdaGrad [26]. All parameters were tuned on a dedicated development set for both of the models. As for the NLL models, we trained 4 different models; LSTM with one and two layers, and bidirectional LSTM with one and two layers, denoted as RNN, 2RNN, BI-RNN and BI-2-RNN, respectively. Table 1 summarizes the results for both models.\nBesides being efficient and more elegant, DeepSegmentor is superior to the NLL models when measuring (6), with the exception of BI-2-RNN, which was slightly better for the offset measurement."
    }, {
      "heading" : "4.2. VOT Segmentation",
      "text" : "Voice onset time (VOT) is the time between the onset of a stop burst and the onset of voicing. As noted in the introduction, it is widely used in theoretical and clinical studies as well as ASR tasks. In this problem the input is a speech utterance containing a single stop consonant, and the output is the VOT onset and offset times.\nWe compared our model to two other methods for VOT measurement. First is the AutoVOT algorithm [9]. This algorithm follows the structured prediction approach of linear classifier with hand-crafted features and feature-functions. The second algorithm is the DeepVOT algorithm [11]. This algorithm uses RNNs with NLL as loss function. Hence, it predicts for each frame whether it is related to the VOT or not. Using the RNN predictions, a dynamic programming algorithm is applied to find the best onset and offset times. Our approach combines both of these methods while jointly training RNN with structured loss function."
    }, {
      "heading" : "4.2.1. Datasets",
      "text" : "We use two different datasets. The first one, PGWORDS, is from a laboratory study by Paterson and Goldrick [6]. American English monolinguals and Brazilian Portuguese (L1)-English bilinguals (24 participants each) named a set of 144 pictures. Productions other than the intended label as well as those with code-switching or disfluencies were excluded. VOT of remaining words was annotated by one transcriber.\nThe second dataset, BB, consists of spontaneous speech from the 2008 season of Big Brother UK, a British reality television show [27, 9]. The speech comes from 4 speakers recorded in the “diary room,” an acoustically clean environment. VOTs were manually measured by two transcribers.\n4.2.2. Results — PGWORDS\nFor the PGWORDS dataset we use two layers of bidirectional LSTMs with dropout after each recurrent layer. We use (6) as our loss function. The input features are the same as in [9, 11]; overall we have 63 features per frame. We optimize the networks using AdaGrad optimization. All parameters were tuned on a dedicated development set. Table 2 summarizes the results using the same loss function as in [9]. Results suggests that DeepSegmentor outperforms the AutoVOT model over all tolerance values. However, when comparing to DeepVOT, the picture is mixed. In the lower tolerance values DeepSegmentor is superior to the DeepVOT while for higher values DeepVOT performs better. We believe these results are due to the DeepVOT being less delicate and solving a much coarser problem than the DeepSegmentor ; hence, it performs better when considering high tolerance values. We believe the integration between these two systems, (using DeepVOT as pre-training for the DeepSegmentor ), will yield more accurate and robust results. We leave this investigation for future work."
    }, {
      "heading" : "4.2.3. Results — BB",
      "text" : "For the BB dataset we use two layers of LSTMs with dropout after each recurrent layer. We have experiences with bidirectional LSTMs as well but only forward LSTM performs better on this dataset. We use (6) as our loss function. We use the same features as in [9, 11], overall we have 51 features per frame. We optimize the networks using AdaGrad optimization. All parameters were tuned on a dedicated development set. Table 3 summarize the results using the loss function as in [9]. It is worth notice that we see the same behavior on this dataset as well, regarding the DeepVOT preforms better then the DeepSegmentor in hight tolerance values.\nFuture work will explore timing sequence of length greater than 2 - for instance, in phoneme segmentation, where the sequence varies across training examples. The model’s robustness to noise and length as well as its ability to generalize are also key areas of future development. We would therefore like to explore training the model in two stages: first as a multi-class version and then fine-tuning using structured loss. With respect to machine learning, future directions include the effect of network size, depth, and loss function on model performance.\nIn this paper we present a new algorithm for speech segmentation and evaluate its performance to two different tasks. The proposed algorithm combines structured loss function with recurrent neural networks and outperforms current stateof- the-art methods.\n6. REFERENCES\n[1] Francis Kubala, Tasos Anastasakos, Hubert Jin, Long Nguyen, and Richard Schwartz, “Transcribing radio news,” in ICSLP, 1996, vol. 2, pp. 598–601.\n[2] David Rybach, Christian Gollan, Ralf Schluter, and Hermann Ney, “Audio segmentation for speech recognition using segment features,” in ICASSP, 2009, pp. 4197– 4200.\n[3] L. Lisker and A. Abramson, “A cross-language study of voicing in initial stops: acoustical measurements,” Word, vol. 20, pp. 384–422, 1964.\n[4] J.H.L. Hansen, S.S. Gray, and W. Kim, “Automatic voice onset time detection for unvoiced stops (/p/,/t/,/k/) with application to accent classification,” Speech Commun., vol. 52, pp. 777–789, 2010.\n[5] P. Auzou, C. Ozsancak, R.J. Morris, M. Jan, F. Eustache, and D. Hannequin, “Voice onset time in aphasia, apraxia of speech and dysarthria: a review,” Clin. Linguist. Phonet., vol. 14, pp. 131–150, 2000.\n[6] Nattalia Paterson, Interactions in Bilingual Speech Processing, Ph.D. thesis, Northwestern University, 2011.\n[7] Doroteo Torre Toledano, Luis A Hernández Gómez, and Luis Villarrubia Grande, “Automatic phonetic segmentation,” IEEE transactions on speech and audio processing, vol. 11, no. 6, pp. 617–625, 2003.\n[8] Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer, and Dan Chazan, “A large margin algorithm for speech-tophoneme and music-to-score alignment,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2373–2382, 2007.\n[9] Morgan Sonderegger and Joseph Keshet, “Automatic measurement of voice onset time using discriminative structured predictiona),” JASA, vol. 132, no. 6, pp. 3965–3979, 2012.\n[10] Yossi Adi, Joseph Keshet, and Matthew Goldrick, “Vowel duration measurement using deep neural networks,” in MLSP, 2015, pp. 1–6.\n[11] Yossi Adi, Joseph Keshet, Olga Dmitrieva, and Matt Goldrick, “Automatic measurement of voice onset time and prevoicing using recurrent neural networks,” .\n[12] Trinh Do, Thierry Arti, et al., “Neural conditional random fields,” in AISTATS, 2010, pp. 177–184.\n[13] Shuai Zheng, Sadeep Jayasumana, Bernardino RomeraParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr, “Conditional random fields as recurrent neural networks,” in ICCV, 2015, pp. 1529–1537.\n[14] Liang-Chieh Chen, Alexander G Schwing, Alan L Yuille, and Raquel Urtasun, “Learning deep structured models,” in ICML, 2015.\n[15] Eliyahu Kiperwasser and Yoav Goldberg, “Simple and accurate dependency parsing using bidirectional lstm feature representations,” arXiv preprint, 2016.\n[16] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer, “Neural architectures for named entity recognition,” arXiv preprint, 2016.\n[17] Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun, “Large margin methods for structured and interdependent output variables,” in JMLR, 2005, pp. 1453–1484.\n[18] Jeffrey L. Elman, “Distributed representations, simple recurrent networks, and grammatical structure,” Machine learning, vol. 7, no. 2-3, pp. 195–225, 1991.\n[19] Alan Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, “Speech recognition with deep recurrent neural networks,” in ICASSP, 2013, pp. 6645–6649.\n[20] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams, “Learning representations by backpropagating errors,” Cognitive modeling, vol. 5, no. 3, pp. 1, 1988.\n[21] Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet, “Torch7: A matlab-like environment for machine learning,” in BigLearn, NIPS Workshop, 2011, number EPFL-CONF-192376.\n[22] Nicholas Léonard, Sagar Waghmare, and Yang Wang, “rnn: Recurrent library for torch,” arXiv preprint, 2015.\n[23] Angela Fink, The Role of Domain-General Executive Functions, Conceptualization, and Articulation during Spoken Word Production, Ph.D. thesis, Northwestern University, 2016.\n[24] Ingrid Rosenfelder, Josef Fruehwald, Keelan Evanini, Scott Seyfarth, Kyle Gorman, Hilary Prichard, and Jiahong Yuan, “Fave (forced alignment and vowel extraction),” Program suite v1.2.2 10.5281/zenodo.22281, 2014.\n[25] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, “Improving neural networks by preventing co-adaptation of feature detectors,” CoRR, 2012.\n[26] John Duchi, Elad Hazan, and Yoram Singer, “Adaptive subgradient methods for online learning and stochastic optimization,” JMLR, vol. 12, pp. 2121–2159, 2011.\n[27] Max Bane, Peter Graff, and Morgan Sonderegger, “Longitudinal phonetic variation in a closed system,” Proc. CLS, vol. 46, pp. 43–58, 2010."
    } ],
    "references" : [ {
      "title" : "Transcribing radio news",
      "author" : [ "Francis Kubala", "Tasos Anastasakos", "Hubert Jin", "Long Nguyen", "Richard Schwartz" ],
      "venue" : "ICSLP, 1996, vol. 2, pp. 598–601.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Audio segmentation for speech recognition using segment features",
      "author" : [ "David Rybach", "Christian Gollan", "Ralf Schluter", "Hermann Ney" ],
      "venue" : "ICASSP, 2009, pp. 4197– 4200.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A cross-language study of voicing in initial stops: acoustical measurements",
      "author" : [ "L. Lisker", "A. Abramson" ],
      "venue" : "Word, vol. 20, pp. 384–422, 1964.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Automatic voice onset time detection for unvoiced stops (/p/,/t/,/k/) with application to accent classification",
      "author" : [ "J.H.L. Hansen", "S.S. Gray", "W. Kim" ],
      "venue" : "Speech Commun., vol. 52, pp. 777–789, 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Voice onset time in aphasia, apraxia of speech and dysarthria: a review",
      "author" : [ "P. Auzou", "C. Ozsancak", "R.J. Morris", "M. Jan", "F. Eustache", "D. Hannequin" ],
      "venue" : "Clin. Linguist. Phonet., vol. 14, pp. 131–150, 2000.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Interactions in Bilingual Speech Processing",
      "author" : [ "Nattalia Paterson" ],
      "venue" : "Ph.D. thesis, Northwestern University,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Automatic phonetic segmentation",
      "author" : [ "Doroteo Torre Toledano", "Luis A Hernández Gómez", "Luis Villarrubia Grande" ],
      "venue" : "IEEE transactions on speech and audio processing, vol. 11, no. 6, pp. 617–625, 2003.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A large margin algorithm for speech-tophoneme and music-to-score alignment",
      "author" : [ "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer", "Dan Chazan" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2373–2382, 2007.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Automatic measurement of voice onset time using discriminative structured predictiona)",
      "author" : [ "Morgan Sonderegger", "Joseph Keshet" ],
      "venue" : "JASA, vol. 132, no. 6, pp. 3965–3979, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Vowel duration measurement using deep neural networks",
      "author" : [ "Yossi Adi", "Joseph Keshet", "Matthew Goldrick" ],
      "venue" : "MLSP, 2015, pp. 1–6.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Automatic measurement of voice onset time and prevoicing using recurrent neural networks",
      "author" : [ "Yossi Adi", "Joseph Keshet", "Olga Dmitrieva", "Matt Goldrick" ],
      "venue" : ".",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Neural conditional random fields",
      "author" : [ "Trinh Do", "Thierry Arti" ],
      "venue" : "AISTATS, 2010, pp. 177–184.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Conditional random fields as recurrent neural networks",
      "author" : [ "Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera- Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip HS Torr" ],
      "venue" : "ICCV, 2015, pp. 1529–1537.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning deep structured models",
      "author" : [ "Liang-Chieh Chen", "Alexander G Schwing", "Alan L Yuille", "Raquel Urtasun" ],
      "venue" : "ICML, 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional lstm feature representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg" ],
      "venue" : "arXiv preprint, 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer" ],
      "venue" : "arXiv preprint, 2016.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Large margin methods for structured and interdependent output variables",
      "author" : [ "Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun" ],
      "venue" : "JMLR, 2005, pp. 1453–1484.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Distributed representations, simple recurrent networks, and grammatical structure",
      "author" : [ "Jeffrey L. Elman" ],
      "venue" : "Machine learning, vol. 7, no. 2-3, pp. 195–225, 1991.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton" ],
      "venue" : "ICASSP, 2013, pp. 6645–6649.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning representations by backpropagating errors",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "Cognitive modeling, vol. 5, no. 3, pp. 1, 1988.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "Ronan Collobert", "Koray Kavukcuoglu", "Clément Farabet" ],
      "venue" : "BigLearn, NIPS Workshop, 2011, number EPFL-CONF-192376.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "rnn: Recurrent library for torch",
      "author" : [ "Nicholas Léonard", "Sagar Waghmare", "Yang Wang" ],
      "venue" : "arXiv preprint, 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The Role of Domain-General Executive Functions, Conceptualization, and Articulation during Spoken Word Production, Ph.D",
      "author" : [ "Angela Fink" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Fave (forced alignment and vowel extraction)",
      "author" : [ "Ingrid Rosenfelder", "Josef Fruehwald", "Keelan Evanini", "Scott Seyfarth", "Kyle Gorman", "Hilary Prichard", "Jiahong Yuan" ],
      "venue" : "Program suite v1.2.2 10.5281/zenodo.22281, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "CoRR, 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "JMLR, vol. 12, pp. 2121–2159, 2011.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Longitudinal phonetic variation in a closed system",
      "author" : [ "Max Bane", "Peter Graff", "Morgan Sonderegger" ],
      "venue" : "Proc. CLS, vol. 46, pp. 43–58, 2010.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", removing non-speech regions such as music or noise to reduce ASR error [1, 2]).",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : ", removing non-speech regions such as music or noise to reduce ASR error [1, 2]).",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 6,
      "context" : "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].",
      "startOffset" : 180,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].",
      "startOffset" : 180,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].",
      "startOffset" : 180,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].",
      "startOffset" : 211,
      "endOffset" : 219
    }, {
      "referenceID" : 10,
      "context" : "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].",
      "startOffset" : 211,
      "endOffset" : 219
    }, {
      "referenceID" : 11,
      "context" : "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "In this work the surrogate loss function is the structural hinge loss [17] defined as",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "RNN is a deep network architecture that can model the behavior of dynamic temporal sequences using an internal state which can be thought of as memory [18, 19].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 18,
      "context" : "RNN is a deep network architecture that can model the behavior of dynamic temporal sequences using an internal state which can be thought of as memory [18, 19].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "In order to optimize the network parameters using the backpropagation algorithm [20], we must find the outer derivative of each layer with respect to the model parameters and inputs.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "Our dataset comes from a laboratory study by Fink and Goldrick [23].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "1All models were implemented using Torch7 toolkit [21, 22] boundaries.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "1All models were implemented using Torch7 toolkit [21, 22] boundaries.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 24,
      "context" : "We use two layers of bidirectional LSTMs for the DeepSegmentor model with dropout [25] after each recurrent layer.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 25,
      "context" : "We optimize the networks using AdaGrad [26].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "First is the AutoVOT algorithm [9].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "The second algorithm is the DeepVOT algorithm [11].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "The first one, PGWORDS, is from a laboratory study by Paterson and Goldrick [6].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "The second dataset, BB, consists of spontaneous speech from the 2008 season of Big Brother UK, a British reality television show [27, 9].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "The second dataset, BB, consists of spontaneous speech from the 2008 season of Big Brother UK, a British reality television show [27, 9].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "The input features are the same as in [9, 11]; overall we have 63 features per frame.",
      "startOffset" : 38,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "The input features are the same as in [9, 11]; overall we have 63 features per frame.",
      "startOffset" : 38,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "Table 2 summarizes the results using the same loss function as in [9].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "We use the same features as in [9, 11], overall we have 51 features per frame.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "We use the same features as in [9, 11], overall we have 51 features per frame.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "Table 3 summarize the results using the loss function as in [9].",
      "startOffset" : 60,
      "endOffset" : 63
    } ],
    "year" : 2016,
    "abstractText" : "We describe and analyze a simple and effective algorithm for sequence segmentation applied to speech processing tasks. We propose a neural architecture that is composed of two modules trained jointly: a recurrent neural network (RNN) module and a structured prediction model. The RNN outputs are considered as feature functions to the structured model. The overall model is trained with a structured loss function which can be designed to the given segmentation task. We demonstrate the effectiveness of our method by applying it to two simple tasks commonly used in phonetic studies: word segmentation and voice onset time segmentation. Results suggest the proposed model is superior to previous methods, obtaining state-of-the-art results on the tested datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}