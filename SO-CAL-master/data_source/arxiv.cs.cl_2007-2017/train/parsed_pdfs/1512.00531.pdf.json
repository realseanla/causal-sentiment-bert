{
  "name" : "1512.00531.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Benchmarking sentiment analysis methods for large-scale texts: A case for using continuum-scored words and word shift graphs",
    "authors" : [ "Andrew Reagan", "Brian Tivnan", "Jake Ryland Williams", "Christopher M. Danforth", "Peter Sheridan Dodds" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Benchmarking sentiment analysis methods for large-scale texts: A case for using continuum-scored words and word shift graphs.\nAndrew Reagan,1, 2 Brian Tivnan,2, 3 Jake Ryland Williams,4\nChristopher M. Danforth,1, 2 and Peter Sheridan Dodds1, 2\n1Department of Mathematics & Statistics, Computational Story Lab, & the Vermont Advanced Computing Core, University of Vermont, Burlington, VT, 05405\n2Vermont Complex Systems Center, University of Vermont, Burlington, VT, 05405 3The MITRE Corporation, 7525 Colshire Drive, McLean, VA, 22102\n4School of Information, University of California, Berkeley, Berkeley, CA, 94720 (Dated: September 13, 2017)\nThe emergence and global adoption of social media has rendered possible the real-time estimation of population-scale sentiment, bearing profound implications for our understanding of human behavior. Given the growing assortment of sentiment measuring instruments, comparisons between them are evidently required. Here, we perform detailed tests of 6 dictionary-based methods applied to 4 different corpora, and briefly examine a further 8 methods. We show that a dictionary-based method will only perform both reliably and meaningfully if (1) the dictionary covers a sufficiently large enough portion of a given text’s lexicon when weighted by word usage frequency; and (2) words are scored on a continuous scale.\nI. INTRODUCTION\nAs we move further into what might be called the Sociotechnocene—with increasingly more interactions, decisions, and impact being made by globally distributed people and algorithms—the myriad human social dynamics that have shaped our history have become far more visible and measurable than ever before. Driven by the broad implications of being able to characterize social systems in microscopic detail, sentiment detection for populations at all scales has become a prominent research arena. Attempts to leverage online expression for sentiment mining include prediction of stock markets [1–4], assessing responses to advertising, real-time monitoring of global happiness [5], and measuring a health-related quality of life [6]. The diverse set of instruments produced by this work now provide indicators that help scientists understand collective behavior, inform public policy makers, and in industry, and gauge the sentiment of public response to marketing campaigns. Given their widespread usage and potential to influence social systems, understanding how these instruments perform and how they compare with each other has become an imperative.\nWe identify sentiment detection methods as belonging to one of three categories, each carrying their own advantages and disadvantages:\n1. Dictionary-based methods [5, 7–11],\n2. Supervised learning methods [10], and\n3. Unsupervised (or deep) learning methods [12].\nHere, we focus on dictionary-based methods, which all center around the determination of a text T ’s average\nhappiness through the equation:\nhavg(T ) = ∑N i=1 havg(wi) · fi(T )∑N\ni=1 fi(T ) = N∑ i=1 havg(wi) · pi(T ),\n(1) where we denote each of the N words in a given dictionary as wi, word sentiment scores as havg(wi), word frequency as fi(T ), and normalized frequency of wi in T\nas pi(T ) = fi(T )/ ∑N\ni=1 fi(T ). Dictionary-based methods rely upon two distinct advantages we will capitalize on: (1) they are in principle corpora agnostic (including those without training data available) and (2) in contrast to black box methods, they offer the ability to “look under the hood” at words contributing to a particular score through “word shifts” (defined fully later; see also [13, 14]). Indeed, if we are at all concerned with understanding why a particular scoring method varies—e.g,, our undertaking is scientific—then word shifts are essential tools. In the absence of word shifts or similar, any explanation of sentiment trends cannot be trusted [15–18].\nAs all methods must, dictionary-based “bag-of-words” approaches suffer from various drawbacks, and three are worth stating up front. First, they are only applicable to corpora of sufficient size, well beyond that of a single sentence. Second, dictionary-based methods also under perform learning methods on corpora for which a tagged training set is available though in practice, the domains and topics to which sentiment analysis is applied are highly varied. Third: words may be evaluated out of context or with the wrong meaning. A simple example is the word “new” occurring frequently when evaluating articles in the New York Times. Nevertheless, this kind of contextual error is something we can readily identify and correct for through word shifts, but would remain hidden to naive learning methods without new training.\nWe lay out our paper as follows. We list and describe\nar X\niv :1\n51 2.\n00 53\n1v 1\n[ cs\n.C L\n] 2\nD ec\n2 01\n5\n2 the dictionary-based methods we consider in Sec. II, and outline the corpora we use for tests in Sec. II B. We present our results in Sec. III, comparing all methods in how they perform for specific analyses of the New York Times (NYT) (Sec. III A), movie reviews (Sec. III B), Google Books (Sec. III C), and Twitter (Sec. III D). In Sec. III E, we make some basic comparisons between dictionary-based methods and machine learning approaches. We bolster our findings with supporting figures in the Supplementary Material, and provide concluding remarks in Sec. IV.\nII. DICTIONARIES AND CORPORA\nA. Dictionaries\nThe words “dictionary,” “lexicon,” and “corpus” are often used interchangeably, and for clarity we define our usage as follows.\nDictionary: Set of words (possibly including word stems) with ratings.\nCorpus: Collection of texts which we seek to analyze.\nLexicon: The words contained within a corpus (often said to be “tokenized”).\nWe test the following six dictionaries in depth:\nLabMT — Language Assessment by Mechanical Turk [5].\nANEW — Affective Norms of English Words [7].\nWK — Warriner and Kuperman rated words from SUBTLEX by Mechanical Turk [11].\nMPQA — The Multi-Perspective Question Answering (MPQA) Subjectivity Dictionary [9].\nLIWC — Linguistic Inquiry and Word Count, 2007 [8].\nLiu — Developed by Bing Liu [10].\nWe also make note of 7 other dictionaries:\nPANAS-X — The Positive and Negative Affect Schedule — Expanded [19].\nPattern 2.6 — A web mining module for the Python programming language [20].\nSentiWordNet — WordNet synsets each assigned three sentiment scores: positivity, negativity, and objectivity [21].\nAFINN — Words manually rated -5 to 5 with impact scores by Finn Nielsen [22].\nGeneral Inquirer — Database of words and manually created semantic and cognitive categories, including positive and negative connotations [23].\nWDAL — About 9000 words rated in terms of their Pleasantness, Activation, and Imagery (concreteness) [24].\nNRC — Created from the “sentiment140” corpus of tweets, using emoticons as positive and negative labels [25].\nAll of these dictionaries were produced by academic groups, and with the exception of LIWC, they are provided free of charge. In Table I, we supply the main aspects—such as word count, score type (continuum or binary), and license information—for the dictionaries listed above. In the github repository associated with our paper, https://github.com/andyreagan/ sentiment-analysis-comparison, we include all of the dictionaries but LIWC.\nThe LabMT, ANEW, and WK dictionaries have scores ranging on a continuum from 1 (sad) to 9 (happy) with 5 as neutral, whereas the others have scores of ±1, and either explicitly or implicitly 0 (neutral). We will refer to the latter dictionaries as being binary, even if neutral is included. For coverage tests, we include all available words, to gain a full sense of the breadth of each dictionary. In scoring, we do not include neutral words from any dictionary.\nWe test the LabMT, ANEW, and WK dictionaries for a range of stop words (starting with the removal of words scoring within ∆h = 1 of the neutral score of 5) [14]. The ability to remove stop words is one advantage of dictionaries that have a continuum range of scores, allowing us to tune the instrument for maximum performance, while retaining all of the benefits of a dictionary method. We will show that, in agreement with the original paper introducing LabMT and looking at Twitter data, a ∆h = 1 is a pragmatic choice in general [14].\nSince we do not apply a part of speech tagger, when using the MPQA dictionary we are obliged to exclude words with scores of both +1 and -1. The words and stems with both scores are: blood, boast* (we denote stems with an asterisk), conscience, deep, destiny, keen, large, and precious. We choose to match a text’s words using the fixed word set from each dictionary before stems, hence words with overlapping matches (a fixed word that also matches a stem) are first matched by the fixed word.\nB. Corpora tested\nFor each dictionary, we test both the coverage and the ability to detect previously observed and/or known patterns within each of the following corpora, noting the pattern we hope to discern:\n1. The New York Times (NYT) [26]: Goal of ranking sections by sentiment (Sec. III A).\n2. Movie reviews [27]: Goal of discerning positive and negative reviews (Sec. III B).\n3. Google Books [28]: Goal of creating time series (Sec. III C).\n4. Twitter: Goal of creating time series (Sec. III D).\nFor the corpora other than the movie reviews, there is no ground truth sentiment, so we instead make comparisons between methods and examine how words contribute to scores. We note that comparison to societal measures of well being would also be possible [29]. We offer greater detail on corpus processing below, and we also provide the relevant scripts on github at https://github.com/andyreagan/ sentiment-analysis-comparison.\nIII. RESULTS\nIn Fig. 1, we show a direct comparison between word scores for each pair of the 6 dictionaries tested. Overall, we find strong agreement between all dictionaries with exceptions we note below. As a guide, we will provide more detail on the individual comparison between the LabMT dictionary and the other five dictionaries by examining the words whose scores disagree across dictionaries shown in 2. We refer the reader to the Supplementary Material for the remaining individual comparisons.\nTo start with, consider the comparison the labMT and ANEW on a word for word basis. Because these dictionaries share the same range of values, a scatterplot is the natural way to visualize the comparison. Across the top row of Fig. 1, which compares LabMT to the other 5 dictionaries, we see in Panel B for the LabMT-ANEW comparison that the RMA best fit [30] is\nhwLabMT = 0.92 ∗ hwANEW + 0.40\nfor words wLabMT in LabMT and words wANEW in ANEW. The 10 words with farthest from the line of best fit shown in Panel B of Fig. 2 are, with LabMT and ANEW scores: lust (4.64, 7.12), bees (5.60, 3.20), silly (5.30, 7.41), engaged (6.16, 8.00), book (7.24, 5.72), hospital (3.50, 5.04), evil (1.90, 3.23), gloom (3.56, 1.88), anxious (3.42, 4.81), and flower (7.88, 6.64). These are words whose individual ranges have high standard deviations in LabMT. While the overall agreement is very\ngood, we should expect some variation in the emotional associations of words, due to chance, time of survey, and demographic variability. Indeed, the Mechanical Turk users who scored the words for the LabMT set in 2011 are evidently different from the University of Florida students who took the ANEW survey before 2000 as a class requirement for Introductory Psychology.\nComparing LabMT with WK in Panel C of Fig. 1, we again find a fit with slope near 1, and a smaller positive shift: hwLabMT = 0.96 ∗ hwWK + 0.26. The 10 words farthest from this line, shown in Panel B of Fig. 2, are (LabMT, WK): sue (4.30, 2.18), boogie (5.86, 3.80), exclusive (6.48, 4.50), wake (4.72, 6.57), federal (4.94, 3.06), stroke (2.58, 4.19), gay (4.44, 6.11), patient (5.04, 6.71), user (5.48, 3.67), and blow (4.48, 6.10). Like LabMT, the WK dictionary used a Mechanical Turk online survey to gather word ratings. We speculate that the minor variation is due in part to the low number of scores required for each word in the WK survey, with as few as 14 ratings per words and 18 ratings for the majority of the words. By contrast, LabMT scores represent 50 ratings of each word. For an in depth comparison, see reference [17].\nNext, in comparing binary dictionaries with ±1 or ±1, 0 scores to one with a 1–9 range, we can look at the distribution of scores within the continuum score dictionary for each score in the binary dictionary. Looking at the LabMT-MPQA comparison in Panel D of Fig. 1, we see that most of the matches are between words without stems (blue histograms), and that each score in -1, 0, +1 from MPQA corresponds to a distribution of scores in LabMT. To examine deviations, we take the words from LabMT that are happiest when MPQA is -1, both the happiest and the saddest when MPQA is 0, and the saddest when MPQA is 1 (Fig. 2 Panels C-E). The happiest 10 words in LabMT matched by MPQA words with score -1 are: moonlight (7.50), cutest (7.62), finest (7.66), funniest (7.76), comedy (7.98), laughs (8.18), laughing (8.20), laugh (8.22), laughed (8.26), laughter (8.50). This is an immediately troubling list of evidently positive words somehow rated as -1 in MPQA. We also see that\nthe top 5 are matched by the stem “laugh*” in MPQA. The saddest 5 words and happiest 5 words in LabMT matched by words in MPQA with score 0 are: sorrows (2.69), screaming (2.96), couldn’t (3.32), pressures (3.49), couldnt (3.58), and baby (7.28), precious (7.34), strength\n(7.40), surprise (7.42), song (7.58). Again, we see MPQA word scores are questionable. The saddest words in LabMT with score +1 in MPQA that are matched by MPQA are: vulnerable (3.34), court (3.78), sanctions (3.86), defendant (3.90), conviction (4.10), backwards\n5 A: LabMT comparison with ANEW B: LabMT comparison with WK C: LabMT comparison with MPQA’s negative words\n(4.22), courts (4.24), defendants (4.26), court’s (4.44), and correction (4.44). Clearly, these words are not positive words in most contexts.\nWhile it would be simple to correct these ratings in the MPQA dictionary going forward, we have are naturally led to be concerned about existing work using MPQA. We note again that the use of word shifts of some kind would have exposed these problematic scores immediately.\nFor the LabMT-LIWC comparison in Panel E of Fig. 1 we examine the same matched word lists as before. The happiest 10 words in LabMT matched by words in LIWC with score -1 are: trick (5.22), shakin (5.29), number (5.30), geek (5.34), tricks (5.38), defence (5.39), dwell (5.47), doubtless (5.92), numbers (6.04), shakespeare (6.88). From Panel F of Fig. 2, the saddest 5 neutral words and happiest 5 neutral words in LIWC, matched\nwith LIWC, are: negative (2.42), lack (3.16), couldn’t (3.32), cannot (3.32), never (3.34), millions (7.26), couple (7.30), million (7.38), billion (7.56), millionaire (7.62). The saddest words in LabMT with score +1 in LIWC that are matched by LIWC are: merrill (4.90), richardson (5.02), dynamite (5.04), careful (5.10), richard (5.26), silly (5.30), gloria (5.36), securities (5.38), boldface (5.40), treasury’s (5.42). The +1 and -1 words in LIWC match some neutral words in LabMT, which is not alarming. However, the problems with the “neutral” words in the LIWC set are immediate: these are not emotionally neutral words. The range of scores in LabMT for these 0- score words in LIWC formed the basis for Garcia et al.’s response to [5], and we point out here that the authors must have not looked at the words, and all-too-common problem in studies using sentiment analysis [16, 17].\nFor the LabMT-Liu comparison in Panel E of Fig. 1\n6 we again examine the same matched word lists as before, except the neutral word list because Liu has no explicit neutral words. The happiest 10 words in LabMT matched by Liu’s negative list are: myth (5.90), puppet (5.90), skinny (5.92), jam (6.02), challenging (6.10), fiction (6.16), lemon (6.16), tenderness (7.06), joke (7.62), funny (7.92). The saddest words in LabMT with score +1 in Liu that are matched by Liu are: defeated (2.74), defeat (3.20), envy (3.33), obsession (3.74), tough (3.96), dominated (4.04), unreal (4.57), striking (4.70), sharp (4.84), sensitive (4.86). Despite nearly twice as many negative words in Liu as positive words (at odds with the frequency-dependent positivity bias of language [5]), these dictionaries generally agree.\nA. New York Times word shift analysis\nThe New York Times corpus [26] is split into 24 sections of the newspaper that are roughly contiguous throughout the data from 1987–2008. With each dictionary, we rate each section and then compute word shifts (described below) against the baseline, and produce a happiness ranked list of the sections. In Fig. S4, we show scatterplots for each comparison, and compute the Reduced Major Axes (RMA) regression fit [30]. In Fig. S5 we show the sorted bar chart from each dictionary.\nTo gain understanding of the sentiment expressed by any given text relative to another text, it is necessary to inspect the words which contribute most significantly by their emotional strength and the change in frequency of usage. We do this through the use of word shift graphs, which plot the contribution of each word wi from the dictionary (denoted δhavg(wi)) to the shift in average happiness between two texts, sorted by the absolute value of the contribution. We use word shift graphs to both analyze a single text and to compare two texts, here focusing on comparing text within corpora. For a derivation of the algorithm used to make word shift graphs while separating the frequency and sentiment information, we refer the reader to Equations 2 and 3 in [14]. We consider both the sentiment difference and frequency difference parts of δhavg(wi) by writing each term of Eq. 1 as in [14]:\nδhavg(wi) =\n100 havg(wi)− havg(Tref)\nhavg(Tcomp)− havg(Tref) [pi(Tcomp)− pi(Tref)] . (2)\nAn in-depth explanation of how to interpret the word shift graph can also be found at http://hedonometer. org/instructions.html#wordshifts.\nTo both demonstrate the necessity of using word shift graphs in carrying out sentiment analysis, and to gain understanding about the ranking of New York Times sections by each dictionary, we look at word shifts for the “Society” section of the newspaper from each dictionary in Fig. 3, with the reference text being the whole\nof the New York Times. The “Society” section ranks 1, 1, 1, 18, 1, and 11 out of 24 sections in the dictionaries LabMT, ANEW, WK, MPQA, LIWC, and Liu, respectively. These shifts show only the very top of the distributions which range in length from 1030 (ANEW) to 13915 words (WK).\nFirst, using the LabMT dictionary, we see that the words 1. “graduated”, 2. “father”, and 3. “university” top the list, which is dominated by positive words that occur more frequently. These more frequent positive words paint a clear picture of family life (relationships, weddings, and divorces), as well as university accomplishment (graduations and college). In general, we are able to observe with only these words that the “Society” section is where we find the details of these positive events.\nFrom the ANEW dictionary, we see that a few positive words are up, lead by 1. “mother”, 2. “father”, and 3. “bride”. Looking at this shift in isolation, we see only these words with three more (“graduate”, “wedding”, and “couple”) that would lead us to suspect these events are at least common in the “Society” section.\nThe WK dictionary, with the most individual word scores of any dictionary tested, agrees with LabMT and ANEW that the “Society” section is number 1, with somewhat similar set of words at the top: 1. “new”, 2. “university”, and 3. “father”. Less coverage of the New York Times corpus (see Fig. S3) results in the top of the shift showing less of the character of the “Society” section than LabMT, with more words that go down in frequency in the shift. With the words “bride” and “wedding” up, as well as “university”, “graduate”, and “college”, we glean that the “Society” section covers both graduations and weddings, as we have seen so far.\nThe MPQA dictionary ranks the “Society” section 18th of the 24 NYT sections, a complete departure from the other rankings, with the words 1. “mar*”, 2. “retire*”, and 3. “yes*” the top three contributing words. Negative words increasing in frequency are the most common type near the top, and of these, the words with the biggest contributions are being scored incorrectly in this context (specifically words 1. “mar*”, 2. “retire*”, 6. “bar*”, 12. “division”, and 14. “miss*”). Looking more in depth at the problems created by the first of these, we find 1211 unique words match “mar*” with the five most frequent being married (36750), marriage (5977), marketing (5382), mary (4403), and mark (2624). The score for these words in, for example, LabMT are 6.76, 6.7, 5.2, 5.88, and 5.48, confirming our suspicion about these words being categorized incorrectly with a broad stem match. These problems plague the MPQA dictionary for scoring the New York Times corpus, and without using word shifts would have gone completely unseen. With so many words that match and then score incorrectly, we do not attempt to fix contextual issues by blocking corpus-specific words.\nThe second ±1 dictionary, LIWC, agrees well with the first three dictionaries and places the “Society” section at the top with the words 1. “rich*”, 2. “miss”, and\n7 1. graduated+↑ 2. father+↑\n3. university+↑ 4. new+↑ 5. not-↓ 6. mother+↑\n7. son+↑ 8. daughter+↑\n9. late-↑ 10. bride+↑ 11. married+↑ 12. college+↑ 13. no-↓\n14. weddings+↑ 15. bridegroom+↑ 16. against-↓\n17. old-↑ 18. divorce-↑ 19. million+↓\n20. received+↑ 21. graduate+↑ 22. war-↓\n23. like+↓ 24. she+↑ 25. last-↓\n26. hospital-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nA: LabMT Wordshift\nNYT as a whole happiness: 5.91 Society section happiness: 6.42 Why society section is happier than NYT as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. mother+↑ 2. father+↑\n3. bride+↑ 4. graduate+↑\n5. divorce-↑ 6. war-↓ 7. people+↓ 8. wedding+↑ 9. death-↓ 10. lost-↓ 11. money+↓ 12. cut-↓ 13. beach+↑\n14. good+↓ 15. couple+↑\n16. free+↓ 17. prison-↓ 18. dead-↓ 19. debt-↓\n20. win+↓ 21. victory+↓\n22. pressure-↓ 23. fire-↓\n24. game+↓ 25. crime-↓ 26. home+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nB: ANEW Wordshift\nNYT as a whole happiness: 6.30 Society section happiness: 6.98 Why society section is happier than NYT as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. new+↑ 2. university+↑\n3. father+↑ 4. mother+↑\n5. late-↑ 6. son+↑\n7. vice-↑ 8. daughter+↑\n9. old-↑ 10. bride+↑\n11. million+↓ 12. graduate+↑\n13. like+↓ 14. divorce-↑\n15. corporation-↑ 16. college+↑ 17. government-↓ 18. war-↓\n19. hospital-↑ 20. wedding+↑ 21. federal-↓ 22. partner+↑ 23. evening+↑\n24. first+↓ 25. marriage+↑ 26. ceremony+↑\n∑+↑ ∑-↓ ∑ ∑+↓ ∑-↑\nC: WK Wordshift\nNYT as a whole happiness: 6.00 Society section happiness: 6.43 Why society section is happier than NYT as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. mar*-↑ 2. retire*-↑\n3. yes*+↑ 4. vice-↑\n5. laud*+↑ 6. bar*-↑\n7. profess*+↑ 8. brook*+↑\n9. divorce-↑ 10. like*+↓\n11. minister*+↑ 12. division-↑\n13. against-↓ 14. miss*-↑\n15. concern-↑ 16. even+↓\n17. woo*+↑ 18. game-↓\n19. back*+↓ 20. real+↑\n21. just+↓ 22. heal*+↑ 23. benefit+↑\n24. want*+↓ 25. force*-↓ 26. down-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nD: MPQA Wordshift\nNYT as a whole happiness: 0.06 Society section happiness: 0.04 Why society section is less happy than NYT as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. rich*+↑ 2. miss-↑\n3. engag*+↑ 4. honor*+↑ 5. friend*+↑\n6. benefit+↑ 7. trust*+↑ 8. problem*-↓\n9. share+↓ 10. best+↑ 11. loss*-↓ 12. secur*+↑ 13. attack*-↓ 14. merr*+↑ 15. special+↑ 16. kill*-↓ 17. great+↑ 18. war-↓ 19. love+↑ 20. fail*-↓ 21. fight*-↓ 22. numb*-↓ 23. argu*-↓ 24. threat*-↓\n25. gross*-↑ 26. joy*+↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nE: LIWC Wordshift\nNYT as a whole happiness: 0.21 Society section happiness: 0.52 Why society section is happier than NYT as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. vice-↑ 2. miss-↑\n3. concern-↑ 4. works+↑ 5. benefit+↑ 6. honor+↑ 7. best+↑ 8. work+↑ 9. great+↑ 10. trust+↑ 11. love+↑ 12. providence+↑ 13. master+↑ 14. holy+↑ 15. fine+↑ 16. loss-↓ 17. issue-↓ 18. problems-↓\n19. stern-↑ 20. supreme+↑ 21. grace+↑\n22. right+↓ 23. problem-↓\n24. well+↓ 25. victory+↓\n26. win+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nF: Liu Wordshift\nNYT as a whole happiness: 0.03 Society section happiness: 0.17 Why society section is happier than NYT as a whole:\nPer word average happiness shift\nW or\nd R\nan k\nFIG. 3: New York Times (NYT) “Society” section shifted against the entire NYT corpus for each of the six dictionaries listed in tiles A–F. We provide a detailed analysis in Sec.III A. Generally, we are able to glean the greatest understanding of the sentiment texture associated with this NYT section using the LabMT dictionary. Additionally we note the LabMT dictionary has the most coverage quantified by word match count (Fig. S3), we are able to identify and correct problematic words scores in the Liu dictionary, and we see that the MPQA dictionary disagrees entirely with the others because of an overly broad stem match.\n3. “engage*” at the head of the list. We immediately notice that the word “miss” is being used frequently in the “Society” section in a different context than was rated LIWC: it is used in the corpus to mean the title prefixed to the name of an unmarried woman, but is scored as negative in LIWC as meaning to fail to reach an target or to acknowledge loss. We would remove this word from LIWC for further analysis of this corpus (we would also remove the word “trust” here). The words matched by “miss*” aside, LIWC finds some positive words going up, with “engage*” hinting at weddings. Otherwise, without words that capture the specific behavior happening in the “Society” section, we are unable to see anything about college, graduations, or marriages, and there is much less\nto be gained about the text from the words in LIWC than some of the other dictionaries we have seen. Without these words, it is confirming that LIWC still finds the “Society” section to be the top section, due in large part to a lack of negative words 18. “war” and 21. “fight*”.\nThe final dictionary from Liu disagrees with the others and puts the “Society” section at 11th out of the 24 sections. The top three words, 1. “vice”, 2. “miss”, and 3. “concern”, contribute largely with respect to the rest of distribution, of which two are clearly being used in an inappropriate context. For a more reasonable analysis we would remove both “vice” and “miss” from the Liu dictionary to score this text, making the “Society” section the second happiest of the 24 sections. With this fix,\nfocusing on the words, we see that Liu finds many positive words increasing in frequency that are mostly generic. In the word shift we do not find the wedding or university events as in dictionaries with more coverage, but rather a variety of positive language surrounding these events, for example 4. “works”, 5. “benefit”, 6. “honor”, 7. “best”, 9. “great”, 10. “trust”, 11. “love”, etc.\nIn conclusion, we find that 4 of the 6 dictionaries score the “Society” section at number 1, and in these cases we use the word shift to uncover the nuances of the language used. We find, unsurprisingly, that the most matches are found by the LabMT dictionary, which is in part built from the NYT corpus (see Fig. S3 for coverage plots). Without as much corpus-specific coverage, we note that while the nuances of the text remain hidden, the LIWC and Liu dictionaries still find the positivity surrounding these unknown events. Of the two that did not score the “Society” section at the top, we deem MPQA beyond repair while finding just two words need to be adjusted for Liu, thereby using the word shift to directly correct the score for the New York Times corpus.\nB. Movie Reviews classification and word shift analysis\nFor the movie reviews, we test the ability to discern positive and negative reviews. The entire dataset consists of 1000 positive and 1000 negative reviews, as rated with 4 or 5 stars and 1 or 2 stars, respectively. We show how well each dictionary covers the review database in Fig. 4. The average review length is 650 words, and we plot the distribution of review lengths in Fig. S8. We average the sentiment of words in each review individually, using each dictionary. We also combine random samples of\nN positive or N negative reviews for N varying from 2 to 900 on a logarithmic scale, without replacement, and rate the combined text. With an increase in the size of the text, we expect that the dictionaries will be better able to distinguish positive from negative. The simple statistic we use to describe this ability is the percentage of distributions that overlap the average.\nIn the lower right panel of Fig. 5, the percentage overlap of positive and negative review distributions presents us with a simple summary of dictionary performance on this tagged corpus. The ANEW dictionary stands out as being considerably less capable of distinguishing positive from negative. In order, we then see WK is slightly better overall, LabMT and LIWC perform similarly better than WK overall, and then MPQA and Liu are each a degree better again, across the review lengths (see below for hard numbers at 1 review length). Fig. S6 and S7 show the distributions for 1 review and for 15 combined reviews.\nTo analyze which words are being used by each dictionary, we compute word shift graphs of the entire positive corpus versus the entire negative corpus in Fig. 6. Across the board, we see that a decrease in negative words is the most important word type for each dictionary, with the word “bad” being the top word for every dictionary in which it is scored (ANEW does not have it). Other observations that we can make from the word shifts include a few words that are potentially being used out of context: “movie”, “comedy”, “plot”, “horror”, “war”, “just”.\nClassifying single reviews as positive or negative, the accuracies are: LabMT 65%, ANEW 55%, LIWC 65%, MPQA 65%, Liu 69%, and WK 63%. We roughly confirm the rule-of-thumb that 10,000 words are enough to score with a dictionary confidently, with all dictionaries\n9 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 S en ti m en t A: LabMT Pos. reviews Neg. reviews 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 B: ANEW Pos. reviews Neg. reviews −0.2 −0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 C: LIWC Pos. reviews Neg. reviews −0.15 −0.10 −0.05 0.00 0.05 0.10 0.15 0.20 0.25 D: MPQA Pos. reviews Neg. reviews\nexcept MPQA and ANEW achieving 90% accuracy with this many words. We sample the number of reviews evenly in log space, generating sets of reviews with average word counts of 4550, 6500, 9750, 16250, and 26000 words. Specifically, the number of reviews necessary to achieve 90% accuracy is 15 reviews (9750 words) for LabMT, 100 reviews (65000 words) for ANEW, 10 reviews (6500 words) for LIWC, 10 reviews (6500 words) for MPQA, 7 reviews (4550 words) for Liu, and 25 reviews (16250 words) for WK.\nC. Google Books time series and word shift analysis\nWe use the Google books 2012 dataset with all English books [28], from which we remove part of speech tagging and split into years. From this, we make time series by year, and word shifts of decades versus the baseline. In addition, to assess the similarity of each time series, we produce correlations between each of the time series.\nDespite grand claims from research based on the Google Books corpus [31], we keep in mind that there are several deep problems with this beguiling data set [32].\nLeaving aside these issues, the Google Books corpus nevertheless provides a substantive test of our six dictionaries.\nIn Fig. 7, we plot the sentiment time series for Google Books. Three immediate trends stand out: a dip near the Great Depression, a dip near World War II, and a general upswing in the 1990’s and 2000’s. From these general trends, a few dictionaries waver: Liu does not dip very much for WW2, Liu and LIWC stay lower in the 90’s and 2000’s, and LabMT with ∆h = 0.5, 1.0 go downward near the end of the 2000’s. We take a closer look into the 1940’s to see what each dictionary is picking up in Google Books around World War 2 in Fig. S11.\nIn each panel of Fig. S11, we see that the top word making the 1940’s less positive than the the rest of Google Books is “war”, which is the top contributor for every dictionary except Liu. Rounding out the top three contributing words are “no” and “great”, and we infer that the word “great” is being seen from mention of “The Great Depression” or “The Great War”, and is possibly being used out of context. All dictionaries but ANEW have “great” in the top 3 words, and each dictionary could be made more accurate if we remove this word.\nIn Panel A of Fig. S11, beyond the top words,\n10\nincreasing words are mostly negative and war-related: “against”, “enemy”, “operation”, which we could expect from this time period.\nIn Panel B, the ANEW dictionary scores the 1940’s of Google Books lower than the baseline as well, finding “war”, “cancer”, and “cell” to be the most important three words. With only 1030 words, there is not enough coverage to see anything beyond the top word “war,” and the shift is dominated by words that go down in frequency.\nIn Panel C, the WK dictionary finds the the 1940’s\nslightly less happy than the baseline, with the top three words being “war”, “great”, and “old”. We see many of the same war-related words as in LabMT, and in addition some positive words like “good” and “be” are up in frequency. The word “first” could be an artifact of first aid.\nIn Panel D, the MPQA dictionary rates the 1940’s slightly less happy than the baseline, with the top three words being “war”, “great”, and “differ*”. Beyond the top word “war”, the score is dominated by words decreasing in frequency, with only a few words up in frequency.\n11\nWithout specific words being up in frequency, it is difficult to obtain a good glance at the nature of the text here.\nIn Panel E, the LIWC dictionary rates the 1940’s nearly the same as the baseline, with the top three words being “war”, “great”, and “argu*”. When the scores are nearly the same, although the 1940’s are slightly happier here, the word shift is a view into how the words of the reference and comparison text vary. In addition to a few war related words being up and bringing the score down (“fight”, “enemy”, “attack”), we see some positive words up that could also be war related: “certain”, “interest”, and “definite”. Although LIWC does not manage to find World War II as a low point of the 20th century, the words that it generates are useful in understanding the corpus.\nIn Panel F, the Liu dictionary rates the 1940’s as happier than the baseline, with the top three words being “great”, “support”, and “like”. With 7 positive words up, and 1 negative word up, we see how the Liu dictionary misses the war without the word “war” itself and with only “enemy” contributing from the words surrounding the conflict. The nature of the positive words that are up is unclear, and could justify a more detailed analysis of why the Liu dictionary fails here.\nD. Twitter time series analysis\nFor Twitter data, we use the Gardenhose feed, a random 10% of the full stream. We store data on the Vermont Advanced Computing Core (VACC), and process the text first into hash tables (with approximately 8 million unique English words each day) and then into word vectors for each 15 minutes, for each dictionary tested. From this, we build sentiment time series for time resolutions of 15 minutes, 1 hour, 3 hours, 12 hours, and 1\nday. In addition to the raw time series, we compute correlations between each time series to assess the similarity of the ratings between dictionaries.\nIn Fig. 8, we present a daily sentiment time series of twitter processed using each of the dictionaries being tested. With the exception of LIWC and MPQA we observe that the dictionaries generally track well together across the entire range. A strong weekly cycle is present in all, although muted for ANEW.\nWe plot the Pearson’s correlation between all time series in Fig. 9, and confirm some of the general observations that we can make from the time series. Namely, the LIWC and MPQA time series disagree the most from the others, and even more so with each other. Generally, we see strong agreement within dictionaries with varying stop values ∆h.\nAll of the dictionaries are choppy at the start of the time frame, when Twitter volume is low in 2008 and into 2009. As more people join Twitter and the tweet volume increases through 2010, we see that LIWC becomes happier, while the rest start a slow decline that is led by MPQA in the negative direction. In 2010, the LIWC dictionary is more positive than the rest with words like “haha”, “lol” and “hey” being used more frequently and swearing being less frequent than the all years of Twitter put together. The other dictionaries with more coverage find a decrease in positive words to balance this increase, with the exception of MPQA which finds many negative words going up in frequency Fig. S17. All of the dictionaries agree most strongly in 2012, all finding a lot of negative language and swearing that brings scores down (see Fig. S18). From the bottom at 2012, LIWC continues to go downward while the others trend back up. The signal from MPQA jumps to the most positive, and LIWC does start trending back up eventually. We analyze the words in 2014 with a word shift against all 7 years of tweets for each dictionary in each panel in Fig. S19:\n12\nA. LabMT finds 2014 less happy with more negative language. B. ANEW finds it more happy with a few positive words up. C. WK finds it happier with more negative words (like LabMT). D. MPQA finds it more positive with less negative words. E. LIWC finds it less positive with more negative and less positive words. F. Liu finds\nit to be of the same sentiment as the background with a balance in positive and negative word usage. From these word shifts, we can analyze which words cause MPQA and LIWC to disagree with the other dictionaries: the disagreement of MPQA is again marred by broad stem matches, and the disagreement of LIWC is due to a lack of coverage.\nE. Brief comparison to machine learning methods\nWe implement a Naive Bayes (NB) classifier (sometimes harshly called idiot Bayes [33]) on the tagged movie review dataset, using 10% of the reviews for training and then testing performance on the rest. Because the usage of the most frequent words does not vary, we remove the top 30 ranked words (“stop words”) from the 5000 most frequent words, and use the remaining 4970 words in our classifier.\nAs we should expect, at the level of single review, NB outperforms the dictionary-based methods, which had no knowledge of this dataset (Fig. S20). As the size of the reviews is increased, the overlap from NB diminishes, and using our simple “fraction overlapping” metric, the error drops to 0 with more than 200 reviews. Interestingly, NB starts to do worse with more reviews put together, and with more than 500 of the 1000 reviews stuck together, it rates both the positive and negative reviews as positive (Fig. S21). The rating curves do not touch, and neither do the error bars, but they both go very slightly above 0. It classifies a higher percentage of individual reviews correctly, but has substantially more variance.\nIn Table S1 and Table S2 we compute the words which the NB classifier uses to classify all of the positive reviews as positive, and all of the negative reviews as positive. The Natural Language Toolkit [34] implements a method to obtain the “most informative” words, by taking the\n13\nratio of the posterior probability of words between all available classes, and looking for the largest ratio:\nmax all words w P (w|ci) P (w|cj)\n(3)\nfor all combinations of classes ci, cj . However, since the probability is computed as a sum in log space, this makes no sense, and we report the largest differences (not ratio) between classes for each word:\nmax all words w\n|P (w|ci)− P (w|cj)| (4)\nfor classes ci, cj . We find that the trained NB classifier relies heavily on words that are very specific to the training set including the names of actors of the movies themselves, making them useful as classifiers but not in understanding the nature of the text. We report the top 10 words for both positive and negative classes using both the ratio and difference methods in Table S1.\nWe next take the movie review trained NB classifier and apply it to the New York Times sections, both ranking them and looking at the words (the above ratio and difference weighted by the occurrence of the words). As expected, the NB classifier is useless in assessing sentiment outside of the corpus on which it is trained. We ranked the sections 5 different times, and among those find the “Television” section both by far the happiest, and by far the saddest in independent tests. We show these rankings and report the top 10 words used to score the “Society” section in Table S2. More sophisticated learning methods, including unsupervised ones, make looking at the words (or phrases) used to score text increasingly more difficult, leaving open the possibility for the train to leave the rails without the conductor even taking notice.\nIV. CONCLUSION\nWe have shown that measuring sentiment in various corpora presents unique challenges, and that dictionary\nperformance is situation dependent. Across the board, the ANEW dictionary performs poorly, and the continued use of this dictionary with clearly better alternatives is a questionable choice. We have seen that the MPQA dictionary does not agree with the other five dictionaries on the NYT corpus and Twitter corpus due to a variety of context and stem matching issues, and we would not recommend this dictionary. And in comparison to LabMT, the WK, LIWC, and Liu dictionaries fail to provide much detail in corpora where their coverage is lower, including all four corpora tested. Sufficient coverage is essential or producing meaningful word shifts and thereby enabling deeper understanding.\nIn each case, to analyze the output of the dictionary method, we rely on the use of word shift graphs. With this tool, we can produce a finer grained analysis of the lexical content, and we can also detect words that are used out of context and can mask them directly. It should be clear that using any of the dictionary-based sentiment detecting method without looking at how individual words contribute is indefensible, and analyses that do not use word shifts or similar tools cannot be trusted. The poor word shift performance of binary dictionaries in particular gravely limits their ability to reveal underlying stories.\nIn sum, we believe that dictionary-based methods will continue to play a powerful role—they are fast and well suited for web-scale data sets—and that the best instruments will be based on dictionaries with excellent coverage and continuum scores. Going forward, a move from scoring words to scoring phrases and words should realize considerable improvement for many languages of interest. With phrase dictionaries, the resulting phrase shift graphs will provide more detailed analysis of a corpus sentiment score. In addition we note that all dictionaries, LabMT included, should be updated using the methodology that created the LabMT dictionary to capture changing lexicons, word usage, and demographics.\n[1] J. Bollen, H. Mao, and X. Zeng. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1–8, 2011. [2] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng. Exploiting topic based twitter sentiment for stock prediction. In ACL (2), pages 24–29, 2013. [3] S. Chung and S. Liu. Predicting stock market fluctuations from twitter. Berkeley, California, 2011. [4] E. J. Ruiz, V. Hristidis, C. Castillo, A. Gionis, and A. Jaimes. Correlating financial time series with microblogging activity. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 513–522. ACM, 2012. [5] P. S. Dodds, E. M. Clark, S. Desu, M. R. Frank, A. J.\nReagan, J. R. Williams, L. Mitchell, K. D. Harris, I. M. Kloumann, J. P. Bagrow, K. Megerdoomian, M. T. McMahon, B. F. Tivnan, and C. M. Danforth. Human language reveals a universal positivity bias. PNAS, 112(8):2389–2394, 2015. [6] S. E. Alajajian, J. R. Williams, A. J. Reagan, S. C. Alajajian, M. R. Frank, L. Mitchell, J. Lahne, C. M. Danforth, and P. S. Dodds. The lexicocalorimeter: Gauging public health through caloric input and output on social media. 2015. [7] M. M. Bradley and P. J. Lang. Affective norms for english words (anew): Stimuli, instruction manual and affective ratings. Technical report c-1, University of Florida, Gainesville, FL, 1999.\n14\n[8] J. W. Pennebaker, M. E. Francis, and R. J. Booth. Linguistic inquiry and word count: Liwc 2001. Mahway: Lawrence Erlbaum Associates, 71:2001, 2001. [9] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual polarity in phrase-level sentiment analysis. Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), 2005. [10] B. Liu. Sentiment analysis and subjectivity. Handbook of natural language processing, 2:627–666, 2010. [11] A. B. Warriner and V. Kuperman. Affective biases in english are bi-dimensional. Cognition and Emotion, pages 1–21, 2014. [12] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642. Citeseer, 2013. [13] P. S. Dodds and C. M. Danforth. Measuring the Happiness of Large-Scale Written Expression: Songs, Blogs, and Presidents. Journal of Happiness Studies, 11(4):441– 456, July 2009. [14] P. S. Dodds, K. D. Harris, I. M. Kloumann, C. A. Bliss, and C. M. Danforth. Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter. PLoS ONE, 6(12):e26752, 12 2011. [15] S. A. Golder and M. W. Macy. Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures. Science Magazine, 333:1878–1881, 2011. [16] D. Garcia, A. Garas, and F. Schweitzer. The languagedependent relationship between word happiness and frequency. Proceedings of the National Academy of Sciences, 112(23):E2983, 2015. [17] P. S. Dodds, E. M. Clark, S. Desu, M. R. Frank, A. J. Reagan, J. R. Williams, L. Mitchell, K. D. Harris, I. M. Kloumann, J. P. Bagrow, K. Megerdoomian, M. T. McMahon, B. F. Tivnan, and C. M. Danforth. Reply to garcia et al.: Common mistakes in measuring frequency-dependent word characteristics. Proceedings of the National Academy of Sciences, 112(23):E2984–E2985, 2015. [18] S. P. Wojcik, A. Hovasapian, J. Graham, M. Motyl, and P. H. Ditto. Conservatives report, but liberals display, greater happiness. Science, 347(6227):1243–1246, 2015. [19] D. Watson and L. A. Clark. The panas-x: Manual for the positive and negative affect schedule-expanded form, 1999. [20] T. De Smedt and W. Daelemans. Pattern for python. The Journal of Machine Learning Research, 13(1):2063–2067, 2012. [21] S. Baccianella, A. Esuli, and F. Sebastiani. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200– 2204, 2010. [22] F. Å. Nielsen. A new anew: Evaluation of a word list for sentiment analysis in microblogs. arXiv preprint arXiv:1103.2903, 2011. [23] P. J. Stone, D. C. Dunphy, and M. S. Smith. The general inquirer: A computer approach to content analysis. MIT Press, 1966. [24] C. Whissell, M. Fournier, R. Pelland, D. Weir, and K. Makarec. A dictionary of affect in language: Iv. reliability, validity, and applications. Perceptual and Motor\nSkills, 62(3):875–888, 1986. [25] S. M. Mohammad, S. Kiritchenko, and X. Zhu. Nrc-\ncanada: Building the state-of-the-art in sentiment analysis of tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval2013), Atlanta, Georgia, USA, June 2013. [26] E. Sandhaus. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia, 2008. [27] B. Pang and L. Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the ACL, 2004. [28] Y. Lin, J.-B. Michel, E. L. Aiden, J. Orwant, W. Brockman, and S. Petrov. Syntactic annotations for the google books ngram corpus. In Proceedings of the ACL 2012 system demonstrations, pages 169–174. Association for Computational Linguistics, 2012. [29] L. Mitchell, M. R. Frank, K. D. Harris, P. S. Dodds, and C. M. Danforth. The Geography of Happiness: Connecting Twitter Sentiment and Expression, Demographics, and Objective Characteristics of Place. PLoS ONE, 8(5):e64417, May 2013. [30] J. M. V. Rayner. Linear relations in biomechanics: the statistics of scaling functions. J. Zool. Lond. (A), 206:415–439, 1985. [31] J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant, et al. Quantitative analysis of culture using millions of digitized books. science, 331(6014):176–182, 2011. [32] E. A. Pechenick, C. M. Danforth, and P. S. Dodds. Characterizing the google books corpus: Strong limits to inferences of socio-cultural and linguistic evolution. arXiv preprint arXiv:1501.00960, 2015. [33] D. J. Hand and K. Yu. Idiot’s bayes—not so stupid after all? International statistical review, 69(3):385–398, 2001. [34] S. Bird. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions, pages 69–72. Association for Computational Linguistics, 2006.\nS1\nMETHODS\nAll of the code to perform these tests is available and document on GitHub. The repository can be found here: https://github.com/andyreagan/sentiment-analysis-comparison.\nStem matching\nOf the dictionaries tested, both LIWC and MPQA use “word stems”. Here we quickly note some of the technical difficulties with using word stems, and how we processed them, for future research to build upon and improve.\nAn example is abandon*, which is intended to the match words of the standard RE form abandon[a-z]*. A naive approach is to check each word against the regular expression, but this is prohibitively slow. We store each of the dictionaries in a “trie” data structure with a record. We use the easily available “marisa-trie” Python library, which wraps the C++ counterpart. The speed of these libraries made the comparison possible over large corpora, in particular for the dictionaries with stemmed words, where the prefix search is necessary. In particular, we construct two tries for each dictionary: a fixed and stemmed trie. We first attempt to match words against the fixed list, and then turn to the prefix match on the stemmed list.\nDETAILED COMPARISONS\nContinued individual comparisons\nPicking up right where we left off in Section III, we next compare ANEW with the other dictionaries. The ANEWWK comparison in Panel I of Fig. 1 contains all 1030 words of ANEW, with a fit of hwANEW = 1.07 ∗ hwWK − 0.30, making ANEW more positive and with increasing positivity for more positive words. The 20 most different scores are (ANEW,WK): fame (7.93,5.45), god (8.15,5.90), aggressive (5.10,3.08), casino (6.81,4.68), rancid (4.34,2.38), bees (3.20,5.14), teacher (5.68,7.37), priest (6.42,4.50), aroused (7.97,5.95), skijump (7.06,5.11), noisy (5.02,3.21), heroin (4.36,2.74), insolent (4.35,2.74), rain (5.08,6.58), patient (5.29,6.71), pancakes (6.08,7.43), hospital (5.04,3.52), valentine (8.11,6.40), and book (5.72,7.05). We again see some of the same words from the LabMT comparisons with these dictionaries, and again can attribute some differences to small sample sizes and differing demographics.\nFor the ANEW-MPQA comparison in Panel J of Fig. 1 we show the same matched word lists as before. The happiest 10 words in ANEW matched by MPQA are: clouds (6.18), bar (6.42), mind (6.68), game (6.98), sapphire (7.00), silly (7.41), flirt (7.52), rollercoaster (8.02), comedy (8.37), laughter (8.45). The saddest 5 neutral words and happiest 5 neutral words in MPQA, matched with MPQA, are: pressure (3.38), needle (3.82), quiet (5.58), key (5.68), alert (6.20), surprised (7.47), memories (7.48), knowledge (7.58), nature (7.65), engaged (8.00), baby (8.22). The saddest words in ANEW with score +1 in MPQA that are matched by MPQA are: terrified (1.72), meek (3.87), plain (4.39), obey (4.52), contents (4.89), patient (5.29), reverent (5.35), basket (5.45), repentant (5.53), trumpet (5.75). Again we see some very questionable matches by the MPQA dictionary, with broad stems capturing words with both positive and negative scores.\nFor the ANEW-LIWC comparison in Panel K of Fig. 1 we show the same matched word lists as before. The happiest 10 words in ANEW matched by LIWC are: lazy (4.38), neurotic (4.45), startled (4.50), obsession (4.52), skeptical (4.52), shy (4.64), anxious (4.81), tease (4.84), serious (5.08), aggressive (5.10). There are only 5 words in ANEW that are matched by LIWC with LIWC score of 0: part (5.11), item (5.26), quick (6.64), couple (7.41), millionaire (8.03). The saddest words in ANEW with score +1 in LIWC that are matched by LIWC are: heroin (4.36), virtue (6.22), save (6.45), favor (6.46), innocent (6.51), nice (6.55), trust (6.68), radiant (6.73), glamour (6.76), charm (6.77).\nFor the ANEW-Liu comparison in Panel L of Fig. 1 we show the same matched word lists as before, except the neutral word list because Liu has no explicit neutral words. The happiest 10 words in ANEW matched by Liu are: pig (5.07), aggressive (5.10), tank (5.16), busybody (5.17), hard (5.22), mischief (5.57), silly (7.41), flirt (7.52), rollercoaster (8.02), joke (8.10). The saddest words in ANEW with score +1 in Liu that are matched by Liu are: defeated (2.34), obsession (4.52), patient (5.29), reverent (5.35), quiet (5.58), trumpet (5.75), modest (5.76), humble (5.86), salute (5.92), idol (6.12).\nFor the WK-MPQA comparison in Panel P of Fig. 1 we show the same matched word lists as before. The happiest 10 words in WK matched by MPQA are: cutie (7.43), pancakes (7.43), panda (7.55), laugh (7.56), marriage (7.56), lullaby (7.57), fudge (7.62), pancake (7.71), comedy (8.05), laughter (8.05). The saddest 5 neutral words and happiest 5 neutral words in MPQA, matched with MPQA, are: sociopath (2.44), infectious (2.63), sob (2.65), soulless (2.71), infertility (3.00), thinker (7.26), knowledge (7.28), legacy (7.38), surprise (7.44), song (7.59). The saddest words in\nS2\nWK with score +1 in MPQA that are matched by MPQA are: kidnapper (1.77), kidnapping (2.05), kidnap (2.19), discriminating (2.33), terrified (2.51), terrifying (2.63), terrify (2.84), courtroom (2.84), backfire (3.00), indebted (3.21).\nFor the WK-LIWC comparison in Panel Q of Fig. 1 we show the same matched word lists as before. The happiest 10 words in WK matched by LIWC are: geek (5.56), number (5.59), fiery (5.70), trivia (5.70), screwdriver (5.76), foolproof (5.82), serious (5.88), yearn (5.95), dumpling (6.48), weeping willow (6.53). The saddest 5 neutral words and happiest 5 neutral words in LIWC, matched with LIWC, are: negative (2.52), negativity (2.74), quicksand (3.62), lack (3.68), wont (4.09), unique (7.32), millionaire (7.32), first (7.33), million (7.55), rest (7.86). The saddest words in WK with score +1 in LIWC that are matched by LIWC are: heroin (2.74), friendless (3.15), promiscuous (3.32), supremacy (3.48), faithless (3.57), laughingstock (3.77), promiscuity (3.95), tenderfoot (4.26), succession (4.52), dynamite (4.79).\nFor the WK-Liu comparison in Panel R of Fig. 1 we show the same matched word lists as before, except the neutral word list because Liu has no explicit neutral words. The happiest 10 words in WK matched by Liu are: goofy (6.71), silly (6.72), flirt (6.73), rollercoaster (6.75), tenderness (6.89), shimmer (6.95), comical (6.95), fanciful (7.05), funny (7.59), fudge (7.62), joke (7.88). The saddest words in WK with score +1 in Liu that are matched by Liu are: defeated (2.59), envy (3.05), indebted (3.21), supremacy (3.48), defeat (3.74), overtake (3.95), trump (4.18), obsession (4.38), dominate (4.40), tough (4.45).\nNow we’ll focus our attention on the MPQA row, and first we see comparisons against the three full range dictionaries. For the first match against LabMT in Panel D of Fig. 1, the MPQA match catches 431 words with MPQA score 0, while LabMT (without stems) matches 268 words in MPQA in Panel S (1039/809 and 886/766 for the positive and negative words of MPQA). Since we’ve already highlighted most of these words, we move on and focus our attention on comparing the ±1 dictionaries.\nIn Panels V–X, BB–DD, and HH–JJ of Fig. 1 there are a total of 6 bins off of the diagonal, and we focus out attention on the bins that represent words that have opposite scores in each of the dictionaries. For example, consider the matches made my MPQA in Panel BB: the words in the top left corner and bottom right corner with are scored in a opposite manner in LIWC, and are of particular concern. Looking at the words from Panel W with a +1 in MPQA and a -1 in LIWC (matched by LIWC) we see: stunned, fiery, terrified, terrifying, yearn, defense, doubtless, foolproof, risk-free, exhaustively, exhaustive, blameless, low-risk, low-cost, lower-priced, guiltless, vulnerable, yearningly, and yearning. The words with a -1 in MPQA that are +1 in LIWC (matched by LIWC) are: silly, madly, flirt, laugh, keen, superiority, supremacy, sillily, dearth, comedy, challenge, challenging, cheerless, faithless, laughable, laughably, laughingstock, laughter, laugh, grating, opportunistic, joker, challenge, flirty.\nIn Panel W of 1, the words with a +1 in MPQA and a -1 in Liu (matched by Liu) are: solicitude, flair, funny, resurgent, untouched, tenderness, giddy, vulnerable, and joke. The words with a -1 in MPQA that are +1 in Liu, matched by Liu, are: superiority, supremacy, sharp, defeat, dumbfounded, affectation, charisma, formidable, envy, empathy, trivially, obsessions, and obsession.\nIn Panel BB of 1, the words with a +1 in LIWC and a -1 in MQPA (matched by MPQA) are: silly, madly, flirt, laugh, keen, determined, determina, funn, fearless, painl, cute, cutie, and gratef. The words with a -1 in LIWC and a +1 in MQPA, that are matched by MPQA, are: stunned, terrified, terrifying, fiery, yearn, terrify, aversi, pressur, careless, helpless, and hopeless.\nIn Panel DD of 1, the words with a -1 in LIWC and a +1 in Liu, that are matched by Liu, are: silly, and madly. The words with a +1 in LIWC and a -1 in Liu, that are matched by Liu, are: stunned, and fiery.\nIn Panel HH of 1, the words with a -1 in Liu and a +1 in MPQA, that are matched by MPQA, are: superiority, supremacy, sharp, defeat, dumbfounded, charisma, affectation, formidable, envy, empathy, trivially, obsessions, obsession, stabilize, defeated, defeating, defeats, dominated, dominates, dominate, dumbfounding, cajole, cuteness, faultless, flashy, fine-looking, finer, finest, panoramic, pain-free, retractable, believeable, blockbuster, empathize, errfree, mind-blowing, marvelled, marveled, trouble-free, thumb-up, thumbs-up, long-lasting, and viewable. The words with a +1 in Liu and a -1 in MPQA, that are matched by MPQA, are: solicitude, flair, funny, resurgent, untouched, tenderness, giddy, vulnerable, joke, shimmer, spurn, craven, aweful, backwoods, backwood, back-woods, back-wood, back-logged, backaches, backache, backaching, backbite, tingled, glower, and gainsay.\nIn Panel II of 1, the words with a +1 in Liu and a -1 in LIWC, that are matched by LIWC, are: stunned, fiery, defeated, defeating, defeats, defeat, doubtless, dominated, dominates, dominate, dumbfounded, dumbfounding, faultless, foolproof, problem-free, problem-solver, risk-free, blameless, envy, trivially, trouble-free, tougher, toughest, tough, low-priced, low-price, low-risk, low-cost, lower-priced, geekier, geeky, guiltless, obsessions, and obsession. The words with a -1 in Liu and a +1 in LIWC, that are matched by LIWC, are: silly, madly, sillily, dearth, challenging, cheerless, faithless, flirty, flirt, funnily, funny, tenderness, laughable, laughably, laughingstock, grating, opportunistic, joker, and joke.\nIn the off-diagonal bins for all of the ±1 dictionaries, we see many of the same words. Again MPQA stem matches are disparagingly broad. We also find matches by LIWC that are concerning, and should in all likelihood be removed from the dictionary.\nS3\n0 5000 10000 15000 Word Rank\n0.0\n0.2\n0.4\n0.6\n0.8 1.0 P e rc e n ta g e o f in d iv id u a l w o rd s co v e re d LabMT ANEW LIWC MPQA Liu WK\n0 5000 10000 15000 Word Rank\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP e rc\ne n ta\ng e o\nf to\nta l w\no rd\ns co\nv e re\nd\nLabMT ANEW LIWC MPQA Liu WK\nLabMT ANEW LIWC MPQA Liu WK 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP e rc\ne n ta\ng e\nTotal Coverage Individual Word Coverage\nFIG. S1: Coverage of the words on twitter by each of the dictionaries.\n0 5000 10000 15000 Word Rank\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP e rc\ne n ta\ng e o\nf in\nd iv\nid u a l w\no rd\ns co\nv e re\nd LabMT ANEW\nLIWC MPQA Liu WK\n0 5000 10000 15000 Word Rank\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP e rc\ne n ta\ng e o\nf to\nta l w\no rd\ns co\nv e re\nd\nLabMT ANEW LIWC MPQA Liu WK\nLabMT ANEW LIWC MPQA Liu WK 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP e rc\ne n ta\ng e\nTotal Coverage Individual Word Coverage\nFIG. S2: Coverage of the words in Google books by each of the dictionaries.\n0 5000 10000 15000 Word Rank\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP e rc\ne n ta\ng e o\nf in\nd iv\nid u a l w\no rd\ns co\nv e re\nd LabMT ANEW\nLIWC MPQA Liu WK\n0 5000 10000 15000 Word Rank\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP e rc\ne n ta\ng e o\nf to\nta l w\no rd\ns co\nv e re\nd\nLabMT ANEW LIWC MPQA Liu WK\nLabMT ANEW LIWC MPQA Liu WK 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP e rc\ne n ta\ng e\nTotal Coverage Individual Word Coverage\nFIG. S3: Coverage of the words in the New York Times by each of the dictionaries.\nS4\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nA N\nE W arts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome leisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevisiontravel\nweek_in_review\nweekend\nRMA β = 1.34, α = 0.01\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nLI W\nC\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinan i l\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 2.93, α = -0.49\n0.1\n0.0\n0.1\n0.2\n0.3\nM P Q\nA\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome leisureliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 1.90, α = -0.39\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nLi u\narts\nbooks\nclassified cultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety sports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 2.98, α = -0.66\n0.10 0.15 0.20 0.25 0.30 0.35 0.40\nLabMT\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nW a rr\nin e r\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 0.83, α = 0.02\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 2.22, α = -0.53\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhomeleisureliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 1.21, α = -0.34\narts\nbooks\nclassified cultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety sports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 2.20, α = -0.68\n0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55\nANEW\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyl\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 0.62, α = 0.01\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome leisureliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 0.36, α = -0.00\narts\nbooks\nclassified cultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregion l\nscience\nsociety sports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 0.95, α = -0.14\n0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6\nLIWC\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nspor s\nstyle\ntelevision\ntravel\nwe k_in_review\nweekend\nRMA β = 0.28, α = 0.16\narts\nbooks\nclassified cultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety sports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 2.01, α = -0.08\n0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20 0.25\nMPQA\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure living\nmagazine\nmetropolitan\nmov es\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 0.44, α = 0.19\n0.2 0.1 0.0 0.1 0.2 0.3 0.4\nLiu\narts\nbooks\nclassified\ncultural\neditorial\neducation\nfinancial\nforeign\nhome\nleisure\nliving\nmagazine\nmetropolitan\nmovies\nnational\nregional\nscience\nsociety\nsports\nstyle\ntelevision\ntravel\nweek_in_review\nweekend\nRMA β = 0.30, α = 0.20\nFIG. S4: NYT Sections scatterplot.\nS5\nA: LabMT B: ANEW C: Warriner\n0.5 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 Happs diff from unweighted average\n1. Society 2. Leisure 3. Weekend\n4. Movies\n5. Living 6. Home 7. Cultural\n8. Style\n9. Arts 10. Education\n11. Regional\n12. Classified\n13. Travel 14. Television 15. Books 16. Magazine 17. Financial 18. Sports 19. Science 20. Editorial 21. Week_in_review 22. Metropolitan 23. National 24. Foreign\n0.6 0.4 0.2 0.0 0.2 0.4 0.6 Happs diff from unweighted average\n1. Society 2. Leisure 3. Education\n4. Style 5. Classified\n6. Home 7. Weekend\n8. Movies 9. Regional\n10. Arts 11. Cultural\n12. Living 13. Sports\n14. Television 15. Travel 16. Magazine 17. Financial 18. Books 19. Metropolitan 20. National 21. Week_in_review 22. Science 23. Editorial 24. Foreign\n0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 Happs diff from unweighted average\n1. Society 2. Leisure 3. Weekend\n4. Home 5. Living\n6. Movies\n7. Style 8. Cultural\n9. Arts 10. Regional\n11. Travel 12. Classified 13. Education\n14. Sports 15. Television\n16. Magazine 17. Books 18. Financial 19. Science 20. Metropolitan 21. Week_in_review 22. Editorial 23. National 24. Foreign\nD: MPQA E: LIWC F: Liu\n0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 Happs diff from unweighted average\n1. Travel 2. Style 3. Education\n4. Home 5. Classified\n6. Leisure\n7. Living 8. Regional\n9. Arts 10. Cultural 11. Movies\n12. Weekend\n13. Sports\n14. Magazine 15. Financial 16. Editorial 17. Books 18. Society 19. National 20. Metropolitan 21. Week_in_review 22. Science 23. Foreign 24. Television\n0.3 0.2 0.1 0.0 0.1 0.2 0.3 Happs diff from unweighted average\n1. Society 2. Leisure 3. Weekend\n4. Style 5. Movies 6. Cultural\n7. Living 8. Regional\n9. Arts 10. Home 11. Classified 12. Financial\n13. Sports\n14. Television 15. Education 16. Magazine 17. Books 18. Travel 19. Editorial 20. National 21. Science 22. Metropolitan 23. Week_in_review 24. Foreign\n0.3 0.2 0.1 0.0 0.1 0.2 0.3 Happs diff from unweighted average\n1. Leisure\n2. Travel 3. Living 4. Style 5. Weekend\n6. Home 7. Classified\n8. Sports 9. Regional 10. Cultural 11. Society 12. Movies\n13. Arts 14. Education\n15. Magazine 16. Television 17. Financial 18. Books 19. Editorial 20. Science 21. Week_in_review 22. National 23. Metropolitan 24. Foreign\nFIG. S5: Sorted bar charts ranking each of the 24 New York Times Sections for each dictionary tested.\nS6\nA: LabMT B: ANEW C: Warriner\nD: MPQA E: LIWC F: Liu Wordshift\nFIG. S6: Binned scores for each review by each corpus with a stop value of ∆h = 1.0.\nA: LabMT B: ANEW C: Warriner\n5.6 5.7 5.8 5.9 6.0 6.1 6.2 Score\n0\n20\n40\n60\n80\n100\nN u m\nb e r\no f\nre v ie\nw s\nPositive reviews Negative reviews\n5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 Score\n0\n20\n40\n60\n80\n100\nN u m\nb e r\no f\nre v ie\nw s\nPositive reviews Negative reviews\n5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 Score\n0\n10\n20\n30\n40\n50\n60\n70\n80 90 N u m b e r o f re v ie w s\nPositive reviews Negative reviews\nD: MPQA E: LIWC F: Liu Wordshift\n0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20 Score\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nN u m\nb e r\no f\nre v ie\nw s\nPositive reviews Negative reviews\n0.1 0.0 0.1 0.2 0.3 0.4 0.5 Score\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nN u m\nb e r\no f\nre v ie\nw s\nPositive reviews Negative reviews\n0.3 0.2 0.1 0.0 0.1 0.2 0.3 Score\n0\n20\n40\n60\n80\n100\nN u m\nb e r\no f\nre v ie\nw s\nPositive reviews Negative reviews\nFIG. S7: Binned scores for random samples of 15 reviews for each dictionary with a stop value of ∆h = 1.0.\nS7\nFIG. S8: Binned length of positive reviews, in words.\nS8\nLa bM\nT 0.\n5 La bM T 1. 0 La bM T 1. 5 AN EW 0 .5 AN EW 1 .0 AN EW 1 .5 W K 0. 5 W K 1. 0 W K 1. 5 LI W C M PQ A Li u\nLabMT 0.5\nLabMT 1.0\nLabMT 1.5\nANEW 0.5\nANEW 1.0\nANEW 1.5\nWK 0.5\nWK 1.0\nWK 1.5\nLIWC\nMPQA\nLiu\n0.15\n0.00\n0.15\n0.30\n0.45\n0.60\n0.75\n0.90\nFIG. S9: Google Books correlations. Here we include correlations for the google books time series, and word shifts for selected decades (1920’s,1940’s,1990’s,2000’s).\nS9\n1. great+↑ 2. no-↑\n3. war-↑ 4. old-↑\n5. without-↑ 6. never-↑\n7. last-↑ 8. you+↓\n9. family+↓ 10. all+↑ 11. good+↑\n12. nothing-↑ 13. issues-↓ 14. women+↓ 15. risk-↓ 16. life+↑ 17. negative-↓\n18. against-↑ 19. doubt-↑\n20. cancer-↓ 21. relationship+↓ 22. low-↓ 23. information+↓\n24. critical-↓ 25. costs-↓\n26. new+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nA: LabMT Wordshift\nGoogle Books as a whole happiness: 5.87 1920's happiness: 5.87 Why 1920's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. war-↑ 2. family+↓\n3. stress-↓ 4. cell-↓ 5. cancer-↓\n6. man+↑ 7. social+↓\n8. failure-↓ 9. good+↑ 10. crisis-↓ 11. abuse-↓\n12. fire-↑ 13. damage-↓ 14. surgery-↓\n15. danger-↑ 16. mother+↓\n17. sex+↓ 18. dead-↑\n19. depression-↓ 20. illness-↓\n21. alone-↑ 22. rejected-↓ 23. infection-↓ 24. life+↑ 25. gold+↑\n26. broken-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nB: ANEW Wordshift\nGoogle Books as a whole happiness: 6.19 1920's happiness: 6.22 Why 1920's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. great+↑ 2. old-↑\n3. war-↑ 4. good+↑\n5. can+↓ 6. relationship+↓\n7. new+↓ 8. government-↓ 9. give+↑ 10. stress-↓ 11. be+↑ 12. doubt-↑ 13. negative-↓\n14. acid-↑ 15. family+↓\n16. economy-↓ 17. cancer-↓ 18. first+↑ 19. disease-↓ 20. federal-↓\n21. enemy-↑ 22. water+↑\n23. difficulty-↑ 24. create+↓\n25. user-↓ 26. mean-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nC: WK Wordshift\nGoogle Books as a whole happiness: 5.98 1920's happiness: 6.00 Why 1920's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. great+↑ 2. little-↑\n3. differ*-↓ 4. fun*-↓\n5. need-↓ 6. want*+↓\n7. back*+↓ 8. mind*-↑\n9. will+↑ 10. important+↓\n11. rail*-↑ 12. good+↑\n13. like*+↓ 14. just+↓ 15. war-↑ 16. support+↓ 17. help+↓ 18. significant+↓ 19. basic+↓\n20. values+↓ 21. risk-↓ 22. numb*-↓\n23. heal*+↓ 24. argue*-↓ 25. complex-↓ 26. mar*-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nD: MPQA Wordshift\nGoogle Books as a whole happiness: 0.09 1920's happiness: 0.10 Why 1920's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. great+↑ 2. argu*-↓\n3. low*-↓ 4. risk*-↓ 5. numb*-↓\n6. war-↑ 7. importan*+↓\n8. doubt*-↑ 9. support+↓ 10. create*+↓\n11. certain*+↑ 12. stress*-↓\n13. values+↓ 14. good+↑ 15. critical-↓ 16. domina*-↓ 17. beaut*+↑ 18. energ*+↓ 19. threat*-↓\n20. creati*+↓ 21. challeng*+↓\n22. interest*+↑ 23. reject*-↓ 24. damag*-↓ 25. avoid*-↓\n26. care+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nE: LIWC Wordshift\nGoogle Books as a whole happiness: 0.22 1920's happiness: 0.26 Why 1920's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. great+↑ 2. important+↓ 3. available+↓ 4. support+↓\n5. good+↑ 6. significant+↓\n7. issues-↓ 8. like+↓\n9. risk-↓ 10. appropriate+↓\n11. complex-↓ 12. work+↑ 13. critical-↓ 14. issue-↓\n15. effective+↓ 16. fine+↑\n17. doubt-↑ 18. limited-↓ 19. negative-↓ 20. benefits+↓ 21. gold+↑ 22. best+↑ 23. stress-↓ 24. greatest+↑ 25. concern-↓\n26. impossible-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nF: Liu Wordshift\nGoogle Books as a whole happiness: 0.04 1920's happiness: 0.07 Why 1920's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\nFIG. S10: Google Books shifts in the 1920’s against the baseline of Google Books.\nS10\n1. war-↑ 2. no-↑\n3. great+↑ 4. you+↓\n5. against-↑ 6. without-↑\n7. old-↑ 8. women+↓\n9. risk-↓ 10. issues-↓\n11. acid-↑ 12. last-↑ 13. family+↓ 14. enemy-↑\n15. cancer-↓ 16. good+↑\n17. never-↑ 18. information+↓\n19. air+↑ 20. all+↑\n21. operation-↑ 22. computer+↓\n23. first+↑ 24. drug-↓ 25. nuclear-↓\n26. relationship+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nA: LabMT Wordshift\nGoogle Books as a whole happiness: 5.87 1940's happiness: 5.85 Why 1940's are less happy than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. war-↑ 2. cancer-↓ 3. cell-↓\n4. family+↓ 5. stress-↓ 6. abuse-↓\n7. love+↓ 8. man+↑ 9. good+↑\n10. mother+↓ 11. social+↓\n12. cut-↑ 13. death-↓ 14. failure-↓ 15. surgery-↓\n16. danger-↑ 17. crisis-↓\n18. fire-↑ 19. crime-↓\n20. sex+↓ 21. anger-↓ 22. damage-↓ 23. trouble-↑ 24. criminal-↓ 25. victim-↓ 26. illness-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nB: ANEW Wordshift\nGoogle Books as a whole happiness: 6.19 1940's happiness: 6.17 Why 1940's are less happy than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. war-↑ 2. great+↑\n3. old-↑ 4. good+↑ 5. can+↓ 6. acid-↑\n7. be+↑ 8. operation-↑\n9. enemy-↑ 10. relationship+↓\n11. first+↑ 12. cancer-↓ 13. give+↑ 14. water+↑\n15. like+↓ 16. user-↓ 17. stress-↓\n18. care+↓ 19. family+↓\n20. oil-↑ 21. abuse-↓\n22. attack-↑ 23. air+↑ 24. create+↓ 25. issue-↓\n26. support+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nC: WK Wordshift\nGoogle Books as a whole happiness: 5.98 1940's happiness: 5.97 Why 1940's are less happy than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. war-↑ 2. great+↑\n3. differ*-↓ 4. fun*-↓\n5. little-↑ 6. need-↓ 7. want*+↓ 8. mar*-↓\n9. like*+↓ 10. will+↑\n11. just+↓ 12. support+↓\n13. risk-↓ 14. back*+↓ 15. against-↑\n16. necessary+↑ 17. good+↑\n18. care*+↓ 19. temper*-↑\n20. rail*-↑ 21. argue*-↓ 22. object*-↓\n23. allow*+↓ 24. too*-↑ 25. significant+↓ 26. long*-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nD: MPQA Wordshift\nGoogle Books as a whole happiness: 0.09 1940's happiness: 0.08 Why 1940's are less happy than Google Books as a whole:\nPer word average happiness shift\nW o rd\nR an\nk\n1. war-↑ 2. great+↑\n3. argu*-↓ 4. risk*-↓\n5. support+↓ 6. certain*+↑\n7. create*+↓ 8. good+↑\n9. fight*-↑ 10. care+↓\n11. critical-↓ 12. numb*-↓\n13. enemy*-↑ 14. doubt*-↑\n15. interest*+↑ 16. challeng*+↓\n17. creati*+↓ 18. stress*-↓ 19. threat*-↓ 20. definite+↑\n21. values+↓ 22. cut-↑ 23. attack*-↑ 24. satisf*+↑ 25. energ*+↓ 26. domina*-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nE: LIWC Wordshift\nGoogle Books as a whole happiness: 0.22 1940's happiness: 0.22 Why 1940's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. great+↑ 2. support+↓\n3. like+↓ 4. issues-↓ 5. risk-↓ 6. good+↑\n7. significant+↓ 8. appropriate+↓\n9. complex-↓ 10. issue-↓\n11. available+↓ 12. important+↓\n13. enemy-↑ 14. critical-↓ 15. greatest+↑ 16. satisfactory+↑\n17. benefits+↓ 18. love+↓\n19. cancer-↓ 20. gold+↑ 21. fine+↑\n22. commitment+↓ 23. work+↑ 24. modern+↑ 25. stress-↓\n26. right+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nF: Liu Wordshift\nGoogle Books as a whole happiness: 0.04 1940's happiness: 0.05 Why 1940's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\nFIG. S11: Google Books shifts in the 1940’s against the baseline of Google Books.\nS11\n1. no-↓ 2. great+↓\n3. war-↓ 4. women+↑\n5. not-↓ 6. without-↓\n7. never-↓ 8. old-↓ 9. against-↓ 10. last-↓ 11. family+↑ 12. you+↑\n13. issues-↑ 14. all+↓ 15. good+↓ 16. nothing-↓\n17. risk-↑ 18. abuse-↑\n19. information+↑ 20. we+↓\n21. drug-↑ 22. doubt-↓ 23. children+↑\n24. cancer-↑ 25. critical-↑\n26. enemy-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nA: LabMT Wordshift\nGoogle Books as a whole happiness: 5.87 1990's happiness: 5.88 Why 1990's are happier than Google Books as a whole:\nPer word average happiness shift\nW o rd\nR a n k\n1. war-↓ 2. family+↑\n3. abuse-↑ 4. man+↓ 5. stress-↑ 6. cancer-↑\n7. cell-↑ 8. good+↓\n9. social+↑ 10. failure-↑\n11. infection-↑ 12. mother+↑ 13. fire-↓\n14. crisis-↑ 15. alone-↓\n16. damage-↑ 17. danger-↓ 18. surgery-↑ 19. debt-↑\n20. sex+↑ 21. rape-↑\n22. injury-↑ 23. child+↑ 24. dead-↓ 25. anger-↑ 26. death-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nB: ANEW Wordshift\nGoogle Books as a whole happiness: 6.19 1990's happiness: 6.18 Why 1990's are less happy than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. great+↓ 2. war-↓\n3. old-↓ 4. good+↓\n5. AIDS-↑ 6. can+↑\n7. be+↓ 8. abuse-↑ 9. relationship+↑ 10. give+↓ 11. stress-↑\n12. enemy-↓ 13. HIV-↑\n14. doubt-↓ 15. family+↑ 16. new+↑ 17. care+↑\n18. first+↓ 19. issue-↑\n20. death-↓ 21. acid-↓\n22. disease-↑ 23. user-↑\n24. cancer-↑ 25. economy-↑ 26. negative-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nC: WK Wordshift\nGoogle Books as a whole happiness: 5.98 1990's happiness: 5.97 Why 1990's are less happy than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. great+↓ 2. fun*-↑\n3. differ*-↑ 4. little-↓\n5. mar*-↑ 6. want*+↑\n7. need-↑ 8. war-↓ 9. support+↑ 10. good+↓\n11. care*+↑ 12. mind*-↓ 13. back*+↑ 14. important+↑ 15. like*+↑\n16. will+↓ 17. argue*-↑\n18. just+↑ 19. heal*+↑ 20. against-↓\n21. risk-↑ 22. object*-↑ 23. numb*-↑\n24. allow*+↑ 25. significant+↑ 26. help+↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nD: MPQA Wordshift\nGoogle Books as a whole happiness: 0.09 1990's happiness: 0.08 Why 1990's are less happy than Google Books as a whole:\nPer word average happiness shift\nW o rd\nR an\nk\n1. great+↓ 2. argu*-↑\n3. risk*-↑ 4. war-↓\n5. numb*-↑ 6. low*-↑\n7. support+↑ 8. doubt*-↓ 9. create*+↑ 10. importan*+↑\n11. certain*+↓ 12. stress*-↑\n13. domina*-↑ 14. care+↑\n15. good+↓ 16. critical-↑ 17. values+↑ 18. abuse*-↑ 19. damag*-↑\n20. creati*+↑ 21. threat*-↑\n22. challeng*+↑ 23. attack*-↓\n24. avoid*-↑ 25. enemy*-↓\n26. beaut*+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nE: LIWC Wordshift\nGoogle Books as a whole happiness: 0.22 1990's happiness: 0.20 Why 1990's are less happy than Google Books as a whole:\nPer word average happiness shift\nW o rd\nR an\nk\n1. great+↓ 2. issues-↑\n3. support+↑ 4. important+↑\n5. good+↓ 6. available+↑\n7. risk-↑ 8. significant+↑ 9. like+↑ 10. appropriate+↑\n11. issue-↑ 12. complex-↑\n13. critical-↑ 14. doubt-↓ 15. benefits+↑\n16. abuse-↑ 17. stress-↑ 18. limited-↑ 19. negative-↑\n20. enemy-↓ 21. effective+↑\n22. concerns-↑ 23. right+↑ 24. greatest+↓ 25. commitment+↑\n26. concern-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nF: Liu Wordshift\nGoogle Books as a whole happiness: 0.04 1990's happiness: 0.03 Why 1990's are less happy than Google Books as a whole:\nPer word average happiness shift\nW o rd\nR an\nk\nFIG. S12: Google Books shifts in the 1990’s against the baseline of Google Books.\nS12\n1. no-↓ 2. you+↑\n3. great+↓ 4. war-↓\n5. me+↑ 6. like+↑ 7. against-↓\n8. risk-↑ 9. without-↓\n10. down-↑ 11. she+↑ 12. acid-↓\n13. first+↓ 14. issues-↑ 15. my+↑ 16. cancer-↑\n17. operation-↓ 18. old-↓\n19. not-↑ 20. violence-↑\n21. force-↓ 22. love+↑ 23. last-↓ 24. women+↑\n25. special+↓ 26. all+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nA: LabMT Wordshift\nGoogle Books as a whole happiness: 5.87 2000's happiness: 5.88 Why 2000's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\na n k\n1. war-↓ 2. cancer-↑\n3. love+↑ 4. home+↑\n5. man+↓ 6. abuse-↑ 7. nature+↓ 8. free+↓\n9. mother+↑ 10. death-↓\n11. hurt-↑ 12. danger-↓ 13. car+↑ 14. interest+↓ 15. family+↑ 16. terrorist-↑ 17. cut-↓\n18. hell-↑ 19. crime-↑\n20. alone-↓ 21. loved+↑ 22. heart+↑\n23. anger-↑ 24. surgery-↑ 25. water+↓\n26. baby+↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nB: ANEW Wordshift\nGoogle Books as a whole happiness: 6.19 2000's happiness: 6.20 Why 2000's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. war-↓ 2. like+↑\n3. great+↓ 4. be+↓\n5. first+↓ 6. acid-↓ 7. operation-↓\n8. old-↓ 9. know+↑\n10. can+↑ 11. create+↑ 12. government-↓\n13. cancer-↑ 14. user-↑ 15. care+↑ 16. special+↓\n17. love+↑ 18. difficulty-↓\n19. violence-↑ 20. HIV-↑\n21. water+↓ 22. right+↑ 23. home+↑\n24. abuse-↑ 25. help+↑ 26. doubt-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nC: WK Wordshift\nGoogle Books as a whole happiness: 5.98 2000's happiness: 5.99 Why 2000's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. like*+↑ 2. just+↑ 3. back*+↑ 4. want*+↑\n5. need-↑ 6. great+↓\n7. down-↑ 8. risk-↑\n9. less-↓ 10. too*-↑\n11. force*-↓ 12. numb*-↓\n13. necessary+↓ 14. heal*+↑ 15. war-↓ 16. temper*-↓ 17. help+↑ 18. care*+↑ 19. against-↓ 20. right+↑\n21. above+↓ 22. allow*+↑ 23. sure+↑\n24. argue*-↑ 25. interest+↓\n26. rail*-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nD: MPQA Wordshift\nGoogle Books as a whole happiness: 0.09 2000's happiness: 0.09 Why 2000's are happier than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. risk*-↑ 2. great+↓\n3. numb*-↓ 4. certain*+↓ 5. war-↓ 6. interest*+↓\n7. create*+↑ 8. argu*-↑\n9. difficult*-↓ 10. low*-↓ 11. doubt*-↓ 12. challeng*+↑ 13. care+↑\n14. special+↓ 15. sure*+↑ 16. smil*+↑\n17. terror*-↑ 18. kill*-↑\n19. support+↑ 20. creati*+↑ 21. love+↑\n22. satisf*+↓ 23. worr*-↑ 24. importan*+↓ 25. critical-↑\n26. hit-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nE: LIWC Wordshift\nGoogle Books as a whole happiness: 0.22 2000's happiness: 0.21 Why 2000's are less happy than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\n1. like+↑ 2. great+↓\n3. risk-↑ 4. issues-↑\n5. right+↑ 6. support+↑ 7. love+↑ 8. concerned-↓\n9. work+↓ 10. hard-↑ 11. cancer-↑ 12. sufficient+↓\n13. regard+↓ 14. critical-↑\n15. doubt-↓ 16. top+↑ 17. significant+↑\n18. greatest+↓ 19. difficulty-↓ 20. benefits+↑ 21. impossible-↓ 22. satisfactory+↓ 23. difficulties-↓\n24. bad-↑ 25. smile+↑\n26. concerns-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nF: Liu Wordshift\nGoogle Books as a whole happiness: 0.04 2000's happiness: 0.04 Why 2000's are less happy than Google Books as a whole:\nPer word average happiness shift\nW or\nd R\nan k\nFIG. S13: Google Books shifts in the 2000’s against the baseline of Google Books.\nS13\n2009 2010 2011 2012 2013 2014 2015 0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8 LabMT\nANEW Warriner LIWC MPQA Liu\nFIG. S14: Normalized time series on Twitter using ∆h of 1.0 for all. For resolution of 3 hours. We do not include any of the time series with resolution below 3 hours here because there are too many data points to see.\n2009 2010 2011 2012 2013 2014 2015 0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8 LabMT\nANEW Warriner LIWC MPQA Liu\nFIG. S15: Normalized time series on Twitter using ∆h of 1.0 for all. For resolution of 12 hours.\nS14\nA: 15 Minute B: 1 Hour\nLa bM\nT 0.\n5\nLa bM\nT 1.\n0\nLa bM\nT 1.\n5\nLa bM\nT 2.\n0 AN EW 0 .5 AN EW 1 .0 AN EW 1 .5 AN EW 2 .0 W K 0. 5 W K 1. 0 W K 1. 5 W K 2. 0 LI W C M PQ A Li u\nLabMT 0.5\nLabMT 1.0\nLabMT 1.5\nLabMT 2.0\nANEW 0.5\nANEW 1.0\nANEW 1.5\nANEW 2.0\nWK 0.5\nWK 1.0\nWK 1.5\nWK 2.0\nLIWC\nMPQA\nLiu\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nLa bM\nT 0.\n5\nLa bM\nT 1.\n0\nLa bM\nT 1.\n5\nLa bM\nT 2.\n0 AN EW 0 .5 AN EW 1 .0 AN EW 1 .5 AN EW 2 .0 W K 0. 5 W K 1. 0 W K 1. 5 W K 2. 0 LI W C M PQ A Li u\nLabMT 0.5\nLabMT 1.0\nLabMT 1.5\nLabMT 2.0\nANEW 0.5\nANEW 1.0\nANEW 1.5\nANEW 2.0\nWK 0.5\nWK 1.0\nWK 1.5\nWK 2.0\nLIWC\nMPQA\nLiu\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nC: 3 Hours D: 12 Hours\nLa bM\nT 0.\n5\nLa bM\nT 1.\n0\nLa bM\nT 1.\n5\nLa bM\nT 2.\n0 AN EW 0 .5 AN EW 1 .0 AN EW 1 .5 AN EW 2 .0 W K 0. 5 W K 1. 0 W K 1. 5 W K 2. 0 LI W C M PQ A Li u\nLabMT 0.5\nLabMT 1.0\nLabMT 1.5\nLabMT 2.0\nANEW 0.5\nANEW 1.0\nANEW 1.5\nANEW 2.0\nWK 0.5\nWK 1.0\nWK 1.5\nWK 2.0\nLIWC\nMPQA\nLiu 0.00\n0.15\n0.30\n0.45\n0.60\n0.75\n0.90\nLa bM\nT 0.\n5\nLa bM\nT 1.\n0\nLa bM\nT 1.\n5\nLa bM\nT 2.\n0 AN EW 0 .5 AN EW 1 .0 AN EW 1 .5 AN EW 2 .0 W K 0. 5 W K 1. 0 W K 1. 5 W K 2. 0 LI W C M PQ A Li u\nLabMT 0.5\nLabMT 1.0\nLabMT 1.5\nLabMT 2.0\nANEW 0.5\nANEW 1.0\nANEW 1.5\nANEW 2.0\nWK 0.5\nWK 1.0\nWK 1.5\nWK 2.0\nLIWC\nMPQA\nLiu\n0.00\n0.15\n0.30\n0.45\n0.60\n0.75\n0.90\nFIG. S16: Pearson’s r correlation between Twitter time series for all resolutions below 1 day.\nS15\n1. no-↑ 2. don't-↓\n3. haha+↑ 4. not-↓\n5. love+↓ 6. hahaha+↑ 7. can't-↓ 8. never-↓\n9. like+↓ 10. shit-↓ 11. lol+↑\n12. you+↓ 13. hate-↓\n14. happy+↓ 15. bitch-↓ 16. youtube+↑\n17. life+↓ 18. bad-↓ 19. miss-↓ 20. ain't-↓ 21. last-↓ 22. down-↓\n23. birthday+↓ 24. die-↑\n25. friends+↓ 26. con-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nA: LabMT Wordshift\nTwitter all years combined happiness: 6.10 Twitter 2010 happiness: 6.07 Why twitter 2010 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. hate-↓ 2. free+↑\n3. people+↓ 4. hell-↑\n5. love+↓ 6. happy+↓\n7. bored-↑ 8. ugly-↓\n9. good+↑ 10. life+↓\n11. hurt-↓ 12. birthday+↓\n13. wit+↑ 14. sad-↓ 15. win+↑\n16. sin-↑ 17. lost-↑\n18. mad-↓ 19. christmas+↓\n20. home+↑ 21. death-↓\n22. debt-↑ 23. beautiful+↓\n24. money+↑ 25. alone-↓\n26. proud+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nB: ANEW Wordshift\nTwitter all years combined happiness: 6.63 Twitter 2010 happiness: 6.64 Why twitter 2010 is happier than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. boa-↑ 2. like+↓\n3. hate-↓ 4. love+↓\n5. shit-↓ 6. bitch-↓\n7. live+↑ 8. con-↑\n9. happy+↓ 10. die-↑\n11. mean-↓ 12. free+↑\n13. awkward-↓ 14. bout-↑\n15. ugly-↓ 16. old-↓\n17. thank+↓ 18. sad-↓ 19. bad-↓ 20. kill-↓ 21. hurt-↓\n22. christmas+↓ 23. wrong-↓ 24. wit+↑ 25. fake-↓ 26. one-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nC: WK Wordshift\nTwitter all years combined happiness: 6.34 Twitter 2010 happiness: 6.26 Why twitter 2010 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. lag*-↑ 2. mar*-↑ 3. like*+↓ 4. faze*-↑\n5. bar*-↑ 6. just+↓ 7. want*+↓ 8. love+↓\n9. live+↑ 10. bitch*-↓\n11. jam-↑ 12. pan*-↑\n13. hate*-↓ 14. back*+↓\n15. will+↓ 16. need-↓ 17. even+↓ 18. trying-↓ 19. little-↓\n20. happy+↓ 21. pro+↑\n22. gain+↓ 23. dim*-↑ 24. try*-↑\n25. free+↑ 26. please*+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nD: MPQA Wordshift\nTwitter all years combined happiness: 0.24 Twitter 2010 happiness: 0.18 Why twitter 2010 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. haha*+↑ 2. lol+↑\n3. fuck-↓ 4. shit*-↓\n5. heh*+↑ 6. love+↓\n7. bitch*-↓ 8. fuckin*-↓ 9. hate-↓\n10. friend*+↓ 11. good+↓\n12. happy+↓ 13. miss-↓\n14. please*+↓ 15. wrong*-↓ 16. amor*+↑ 17. kill*-↓ 18. ugl*-↓ 19. worst-↓ 20. lmao+↑ 21. sad-↓\n22. best+↓ 23. thank+↓\n24. stupid*-↓ 25. ache*-↑\n26. dumb*-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nE: LIWC Wordshift\nTwitter all years combined happiness: 0.41 Twitter 2010 happiness: 0.45 Why twitter 2010 is happier than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. like+↓ 2. fuck-↓\n3. jam-↑ 4. fucking-↓ 5. shit-↓ 6. free+↑\n7. gain+↓ 8. bitch-↓ 9. hate-↓ 10. love+↓ 11. die-↑\n12. happy+↓ 13. bs-↑\n14. damn-↑ 15. super+↑ 16. good+↑ 17. well+↑ 18. thank+↓ 19. wow+↑\n20. best+↓ 21. perfect+↓\n22. hard-↓ 23. favor+↑ 24. awkward-↓\n25. better+↓ 26. beautiful+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nF: Liu Wordshift\nTwitter all years combined happiness: 0.18 Twitter 2010 happiness: 0.17 Why twitter 2010 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\nFIG. S17: Word Shifts for Twitter in 2010. The reference word usage is all of Twitter (the 10% Gardenhose feed) from September 2008 through April 2015, with the word usage normalized by year.\nS16\n1. no-↓ 2. shit-↑\n3. not-↓ 4. new+↓\n5. haha+↑ 6. hate-↑ 7. great+↓ 8. bitch-↑ 9. don't-↑ 10. free+↓\n11. ass-↑ 12. last-↓\n13. bitches-↑ 14. lol+↑ 15. hahaha+↑\n16. love+↓ 17. good+↓ 18. thanks+↓ 19. hurt-↑\n20. happy+↓ 21. dont-↑\n22. bad-↓ 23. attack-↓\n24. home+↓ 25. fail-↓ 26. war-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nA: LabMT Wordshift\nTwitter all years combined happiness: 6.10 Twitter 2012 happiness: 5.98 Why twitter 2012 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. hate-↑ 2. love+↑\n3. mad-↑ 4. free+↓ 5. hurt-↑ 6. hell-↑\n7. lie-↑ 8. ugly-↑ 9. stupid-↑ 10. fat-↑\n11. war-↓ 12. fun+↓\n13. alone-↑ 14. home+↓\n15. people+↑ 16. win+↓\n17. god+↑ 18. death-↓ 19. baby+↑\n20. bored-↑ 21. fire-↓\n22. scared-↑ 23. hungry-↑\n24. crisis-↓ 25. crash-↓ 26. dead-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nB: ANEW Wordshift\nTwitter all years combined happiness: 6.63 Twitter 2012 happiness: 6.58 Why twitter 2012 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. new+↓ 2. bitch-↑ 3. hate-↑ 4. shit-↑\n5. free+↓ 6. great+↓\n7. mad-↑ 8. like+↑\n9. awkward-↑ 10. good+↓\n11. lie-↑ 12. live+↓ 13. fun+↓ 14. bout-↑\n15. boa-↓ 16. thanks+↓\n17. hell-↑ 18. old-↓ 19. dick-↑ 20. war-↓\n21. hurt-↑ 22. happy+↓ 23. home+↓ 24. awesome+↓ 25. stupid-↑\n26. ugly-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nC: WK Wordshift\nTwitter all years combined happiness: 6.34 Twitter 2012 happiness: 6.20 Why twitter 2012 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. bitch*-↑ 2. lag*-↑\n3. mar*-↓ 4. hate*-↑\n5. like*+↑ 6. great+↓\n7. please*+↓ 8. thank*+↓\n9. miss*-↑ 10. faze*-↓ 11. free+↓ 12. pan*-↑ 13. will+↓ 14. gain+↓ 15. live+↓\n16. awkward-↑ 17. good+↓ 18. damn-↑ 19. help+↓\n20. fun*-↓ 21. mad-↑\n22. want*+↑ 23. trying-↓\n24. just+↓ 25. okay+↑\n26. awesome+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nD: MPQA Wordshift\nTwitter all years combined happiness: 0.24 Twitter 2012 happiness: 0.18 Why twitter 2012 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. haha*+↑ 2. bitch*-↑\n3. shit*-↑ 4. fuck-↑\n5. lol+↑ 6. hate-↑ 7. good+↓ 8. great+↓\n9. please*+↓ 10. fuckin*-↑\n11. miss-↑ 12. free+↓\n13. awkward*-↑ 14. thanks+↓\n15. love+↓ 16. lmao+↑ 17. nag*-↑ 18. hope+↓ 19. hurt*-↑\n20. terror*-↓ 21. slut*-↑ 22. well+↓\n23. grr*-↓ 24. amaz*+↓ 25. kill*-↓ 26. thank+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nE: LIWC Wordshift\nTwitter all years combined happiness: 0.41 Twitter 2012 happiness: 0.32 Why twitter 2012 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. shit-↑ 2. fuck-↑\n3. bitch-↑ 4. hate-↑\n5. like+↑ 6. great+↓\n7. miss-↑ 8. work+↓\n9. free+↓ 10. fucking-↑\n11. good+↓ 12. gain+↓\n13. awkward-↑ 14. awesome+↓\n15. fun+↓ 16. damn-↑\n17. love+↑ 18. mad-↑ 19. win+↓ 20. wow+↓\n21. jam-↑ 22. lie-↑ 23. right+↑ 24. top+↓\n25. thank+↓ 26. hurt-↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nF: Liu Wordshift\nTwitter all years combined happiness: 0.18 Twitter 2012 happiness: 0.09 Why twitter 2012 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\nFIG. S18: Word Shifts for Twitter in 2012. The reference word usage is all of Twitter (the 10% Gardenhose feed) from September 2008 through April 2015, with the word usage normalized by year.\nS17\n1. no-↓ 2. haha+↓\n3. not-↓ 4. lol+↓\n5. hahaha+↓ 6. die-↓ 7. love+↑\n8. good+↓ 9. bad-↓ 10. con-↓ 11. ill-↓ 12. down-↓ 13. hell-↓\n14. home+↓ 15. thanks+↓\n16. damn-↓ 17. last-↓ 18. sin-↓ 19. sorry-↓\n20. don't-↑ 21. tired-↓\n22. hahahaha+↓ 23. great+↓ 24. new+↓\n25. bored-↓ 26. sick-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nA: LabMT Wordshift\nTwitter all years combined happiness: 6.10 Twitter 2014 happiness: 6.03 Why twitter 2014 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. love+↑ 2. happy+↑\n3. good+↓ 4. home+↓\n5. people+↑ 6. hell-↓\n7. hate-↑ 8. life+↑ 9. sick-↓\n10. fun+↓ 11. birthday+↑ 12. bored-↓ 13. win+↑ 14. sin-↓ 15. ugly-↑ 16. mad-↑ 17. bed+↓\n18. stupid-↓ 19. sad-↑\n20. cute+↑ 21. party+↓\n22. proud+↑ 23. war-↓\n24. person-↑ 25. free+↓\n26. gold+↑\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nB: ANEW Wordshift\nTwitter all years combined happiness: 6.63 Twitter 2014 happiness: 6.68 Why twitter 2014 is happier than twitter all years combined:\nPer word average happiness shift\nW or\nd R\na n k\n1. die-↓ 2. love+↑\n3. good+↓ 4. con-↓\n5. mean-↑ 6. happy+↑ 7. boa-↓\n8. new+↓ 9. live+↓\n10. home+↓ 11. fun+↓\n12. bad-↓ 13. bout-↓ 14. old-↓\n15. thanks+↓ 16. hell-↓ 17. bored-↓\n18. great+↓ 19. shit-↑\n20. thank+↑ 21. bitch-↑\n22. sick-↓ 23. late-↓ 24. sin-↓\n25. christmas+↓ 26. awesome+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nC: WK Wordshift\nTwitter all years combined happiness: 6.34 Twitter 2014 happiness: 6.27 Why twitter 2014 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. please*+↑ 2. mar*-↓\n3. bar*-↓ 4. love+↑ 5. too*-↓ 6. gain+↑\n7. lag*-↓ 8. good+↓\n9. faze*-↓ 10. want*+↑\n11. just+↓ 12. pan*-↓\n13. like*+↑ 14. well+↓ 15. happy+↑ 16. mean-↑ 17. live+↓ 18. bitch*-↑ 19. yes*+↓\n20. long*-↓ 21. jam-↓\n22. woo*+↓ 23. vie*-↓ 24. fun*-↓ 25. dim*-↓\n26. dig*+↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nD: MPQA Wordshift\nTwitter all years combined happiness: 0.24 Twitter 2014 happiness: 0.26 Why twitter 2014 is happier than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. haha*+↓ 2. lol+↓\n3. please*+↑ 4. love+↑\n5. good+↓ 6. heh*+↓ 7. bitch*-↑ 8. fuckin*-↑ 9. fuck-↑\n10. damn*-↓ 11. ok+↓ 12. shit*-↑ 13. happy+↑ 14. well+↓ 15. bore*-↓\n16. ha+↓ 17. hell-↓ 18. best+↑ 19. grr*-↓ 20. sorry-↓\n21. crying-↑ 22. amor*+↓ 23. ignor*-↑\n24. hate-↑ 25. ugh-↓ 26. sin-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nE: LIWC Wordshift\nTwitter all years combined happiness: 0.41 Twitter 2014 happiness: 0.33 Why twitter 2014 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\n1. gain+↑ 2. love+↑\n3. good+↓ 4. work+↓\n5. die-↓ 6. well+↓\n7. fuck-↑ 8. happy+↑\n9. fucking-↑ 10. great+↓\n11. damn-↓ 12. shit-↑\n13. jam-↓ 14. like+↑\n15. nice+↓ 16. thank+↑\n17. fun+↓ 18. best+↑\n19. cool+↓ 20. wow+↓\n21. awesome+↓ 22. sorry-↓ 23. bad-↓\n24. yay+↓ 25. cold-↓ 26. hell-↓\n∑+↑ ∑-↓\n∑\n∑+↓ ∑-↑\nF: Liu Wordshift\nTwitter all years combined happiness: 0.18 Twitter 2014 happiness: 0.18 Why twitter 2014 is less happy than twitter all years combined:\nPer word average happiness shift\nW or\nd R\nan k\nFIG. S19: Word Shifts for Twitter in 2014. The reference word usage is all of Twitter (the 10% Gardenhose feed) from September 2008 through April 2015, with the word usage normalized by year.\nS18\n0.0 0.5 1.0 1.5 2.0 2.5 log10(num reviews)\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5 2.0 se n ti m e n t\nsentiment over many random samples for naive bayes\npositive reviews negative reviews\n0.0 0.5 1.0 1.5 2.0 2.5 log10(num reviews)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nfr a ct\nio n o\nv e rl\na p p in\ng\nsentiment over many random samples for naive bayes\nFIG. S20: Results of the NB classifier on the Movie Reviews corpus.\n0.010 0.005 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 Happs diff from unweighted average\n1. Television 2. Education\n3. Society\n4. Leisure 5. Movies 6. Weekend 7. Living 8. Cultural 9. Arts 10. Classified 11. Books 12. Style 13. Home 14. Regional 15. Sports 16. Week_in_review 17. Financial 18. Editorial 19. Magazine 20. Science 21. Metropolitan 22. Foreign 23. National 24. Travel\nRanking of NYT Sections by NB\n0.05 0.04 0.03 0.02 0.01 0.00 0.01 0.02 Happs diff from unweighted average\n1. Travel\n2. Movies\n3. Books 4. Weekend\n5. Arts 6. Society 7. Cultural 8. Leisure 9. Regional 10. Week_in_review\n11. Editorial 12. Foreign 13. Metropolitan\n14. National 15. Financial 16. Magazine\n17. Sports\n18. Style 19. Education 20. Classified\n21. Living\n22. Home 23. Science 24. Television\nRanking of NYT Sections by NB\nFIG. S21: NYT Sections ranked by Naive Bayes in two of the five runs produced.\nS19\nMost informative Ratio Difference\nPositive Negative Positive Negative Word Value Word Value Word Value Word Value pie 16.93 godzilla 20.19 apostle 6.50 jackal 5.97 october 13.36 species 16.82 sheen 5.54 data 5.83 titanic 12.47 julie 14.58 pie 5.47 kombat 5.25 ripley 12.47 alice 14.58 uplifting 5.34 vacation 5.13 truman 11.58 data 13.46 maximus 5.26 charlie’s 4.89 natural 11.58 wrestling 12.34 blake 5.19 gal 4.60 sky 11.58 alicia 12.34 october 5.08 alicia 4.55 mulan 9.80 flubber 11.21 fincher 4.89 libby 4.50 fincher 9.80 imagination 11.21 margaret 4.84 stranded 4.19 questions 8.91 failure 10.09 lambeau 4.82 legends 4.12\nNYT Society Ratio Difference\nPositive Negative Positive Negative Word Value Word Value Word Value Word Value titanic 12.47 godzilla 20.19 mr 31406.38 mrs 63960.66 natural 11.58 species 16.82 senior 29581.98 university 63750.65 mulan 9.80 julie 14.58 n 28302.83 bride 47376.02 questions 8.91 alice 14.58 father 27658.32 college 45227.24 normal 8.02 imagination 11.21 ms 27547.63 j 44915.01 mood 8.02 patch 8.97 m 27255.51 firm 43096.76 colors 6.23 theatre 8.97 she 27022.48 york 33568.67 melvin 6.23 schumacher 7.85 mother 26682.79 medical 22049.99 town 5.52 bits 7.85 her 24464.02 law 17779.18 luckily 5.34 ice 7.85 executive 15188.65 degree 17127.96\nTABLE S1: First result of Naive Bayes trained on a random 10% of the movie review corpus, and applied to the New York Times Society section.\nS20\nMost informative Ratio Difference\nPositive Negative Positive Negative Word Value Word Value Word Value Word Value leila 27.19 mars 33.09 leila 16.43 tango 10.98 contact 13.59 patch 19.85 joe’s 7.33 martian 9.53 bulworth 12.68 tango 17.65 advocate 6.76 jakob 9.16 joe’s 12.68 williamson 16.54 devil’s 6.70 patch 8.88 outstanding 11.78 4 15.44 genetically 6.04 garofalo 8.15 advocate 11.78 conspiracy 15.44 craig 5.99 hewitt 7.20 commentary 11.78 martian 15.44 hurricane 5.98 computers 5.95 tarzan 10.87 jakob 15.44 maximus 5.92 cynthia 5.61 epic 10.87 garofalo 14.34 gladiator 5.82 connor 5.56 intense 10.87 scale 13.23 shaw 5.74 kim 5.37\nNYT Society Ratio Difference\nPositive Negative Positive Negative Word Value Word Value Word Value Word Value contact 13.59 mars 33.09 york 67189.05 received 35983.20 intense 10.87 poorly 9.92 washington 44217.44 medical 31968.26 tarzan 10.87 julie 9.92 senior 44020.17 company 28012.08 epic 10.87 worst 9.65 mother 41016.22 daughter 27601.81 scorsese 9.97 melvin 8.82 university 40960.26 school 15942.80 mature 9.97 harry 8.09 president 39849.25 mrs 14067.43 born 9.06 promising 7.72 firm 35653.01 degree 13594.45 x-files 9.06 marc 7.72 dr 27525.60 graduate 11288.28 shocking 9.06 batman 7.17 her 26295.32 national 10144.55 cole 9.06 blair 6.98 performed 25037.84 hospital 9500.19\nTABLE S2: Second result of Naive Bayes trained on a random 10% of the movie review corpus, and applied to the New York Times Society section."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Andrew Reagan, 2 Brian Tivnan, 3 Jake Ryland Williams, Christopher M. Danforth, 2 and Peter Sheridan Dodds 2 Department of Mathematics & Statistics, Computational Story Lab, & the Vermont Advanced Computing Core, University of Vermont, Burlington, VT, 05405 Vermont Complex Systems Center, University of Vermont, Burlington, VT, 05405 The MITRE Corporation, 7525 Colshire Drive, McLean, VA, 22102 School of Information, University of California, Berkeley, Berkeley, CA, 94720 (Dated: September 13, 2017)",
    "creator" : "LaTeX with hyperref package"
  }
}