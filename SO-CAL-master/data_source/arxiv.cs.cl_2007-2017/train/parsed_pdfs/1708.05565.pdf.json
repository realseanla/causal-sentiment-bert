{
  "name" : "1708.05565.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online Auctions",
    "authors" : [ "Yu Wang", "Jiayi Liu", "Yuxiang Liu", "Jun Hao", "Yang He", "Jinghe Hu", "Weipeng Yan", "Mantian Li" ],
    "emails" : [ "limantian}@jd.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Introduction\nResearchers have made great progress recently in learning to control agents directly from raw high-dimensional sensory inputs like vision in domains such as Atari 2600 games (Mnih et al. 2015), where reinforcement learning (RL) agents have human-level performance. However, most real-world problems have high-level semantic information inputs rather than sensory inputs, where what human experts usually do is to read and understand inputs in plain-text form and act after judging by expertise. Realworld problems are much more challenging than video games in that they always have a larger solution space and in that their states can only be partially observed. Such real-world problems have not been tackled by any state-ofthe-art RL agents until now.\nThis paper demonstrates an agent named LADDER for such a problem. Using a deep asynchronous stochastic Qnetwork (DASQN), the agent improves the performance of JD’s real-time bidding (RTB) ad business.\nRTB is the most promising field in online advertising which greatly promotes the effectiveness of the industry (Yuan, Wang, and Zhao 2013). A typical RTB environment (Figure 1) consists of ad exchanges (ADXs), supply side platforms (SSPs), data management platforms (DMPs) and demand side platforms (DSPs). ADXs and DSPs utilize algorithms to buy/sell ads in real-time. SSPs integrate information of publishers (i.e. online media) and offer ads requests of the publishers to ADXs. An ADX puts the offers out to DSPs for bidding. DSPs target appropriate ads to the involved user based on information supplied by DMPs and return the ads with their bids to the ADX which displays ads of the highest bidder and charges the winner DSP with general second price (Varian 2007).\nObviously, the process of many DSPs/ADXs bidding for an ad offer is an auction game (Myerson 1981) of incomplete information. However, the online ads industry just ignores this fact and considers RTB a solved problem: all existing DSPs model auction games as supervised learning (SL) problems by predicting the click through rate (CTR) (McMahan et al. 2013) or conversion rate (CVR) (Yuan, Wang, and Zhao 2013) of ads and using effective cost per mille (ECPM) as bids (Chen et al. 2011).\nJD.com started its DSP business in 2014, at first we employed the industry state-of-the-art approach of ECPM bidding with a calibrated CTR model (McMahan et al. 2013) as depicted in Figure 2. Soon we found it impossible for the SL calibration model to have a stable performance in practice, which was critical for the business to keep breaking even. As a result, we introduced a method with fine grained bid coefficients calibrated by human experts. In a nutshell, our bidding mechanism then was a humanmachine hybrid control system where operators modified the calibration coefficients tens of times per day.\nFor the obvious inefficiency of the hybrid system, we started research on utilizing RL algorithms to solve the auction game, during which we met several problems:\nFirst, the solution space of the auction game is tremendous. JD DSP system is bidding for 100,000s of auctions per second, assume we have 10 actions and each day is an episode (ad plans are usually on a daily basis), simple math shows the solution space is of 10 . For comparison, the solution space of the game of Go is about 10 (Allis and others 1994; Silver et al. 2016).\nSecond, state-of-the-art RL algorithms are inherently sequential, hence cannot be applied to large-scale practical problems such as the auction game, for our online service cannot afford the inefficiencies of sequential algorithms.\nThird, auction requests are actually triggered by JD users and randomness of human behaviors implies stochastic transitions of states. That’s very different from Atari games, text-based games (Narasimhan, Kulkarni, and Barzilay 2015) and the game of Go (Silver et al. 2016).\nBesides, we have widely ranged rewards of which the maximum may be 100,000 times larger than the minimum, which implies only very expressive models are suitable.\nLast but not least, there’s much human-readable high level semantic information in JD which is crucial for bidding, e.g. the stock keeping units (SKUs) that a customer viewed or bought recently, how long ago she viewed or bought them, the price of the advertised SKU, etc. Although sophisticated feature engineering can utilize these information in a model like wide and deep models (Cheng et al. 2016) or factorization machines (Rendle 2012) as is already in place in the hybrid system, taking into account JD’s scale, such models will be of billions of features and therefore too heavy to react instantly to the rapidly varying auction environment, leading to poor performance.\nIn this paper, we model the auction game as a partially observable Markov decision process (POMDP) and present the DASQN algorithm which successfully solve the inherently synchronousness of RL algorithms and the stochastic transitions of the game. We encode each auction request into plain text in a domain specific natural language, feed the encoded request to a deep convolutional neural networks (CNN) and make full use of the high-level semantic information without any sophisticated feature engineering.\nThis results in a lightweight model both responsive and expressive which can update in real-time and reacts to the changes of the auction environment rapidly. Our whole architecture is named LADDER.\nWe evaluated LADDER on a significant portion of JD DSP business with online A/B test and the experimental results indicate that the industry was far from solving the RTB problem: LADDER easily outperformed the human expert calibrated ECPM policy: during JD.com’s June 18th anniversary sale, the agent raised the company’s ads revenue from the portion by more than 50%, while the ROI of the advertisers also improved as much as 17%."
    }, {
      "heading" : "Background",
      "text" : "RL provides the ability for an agent to learn from interactions with an environment \uD835\uDC38. In this paper, we consider the auction environment as \uD835\uDC38. At each time step \uD835\uDC61, the agent observes an auction \uD835\uDC65 from a publisher and a set of ads will participate in the auction. The agent selects a legal action \uD835\uDC4E ∈ Α = {0, … , \uD835\uDC36} and acts in \uD835\uDC38. After a while, the agent gets a real number reward \uD835\uDC5F from \uD835\uDC38. We formularize this sequential process as (\uD835\uDC65 , \uD835\uDC4E , \uD835\uDC5F , … , \uD835\uDC65 , \uD835\uDC4E , \uD835\uDC5F , … ) whose dynamics can be defined by the joint probability distribution Pr{\uD835\uDC4B = \uD835\uDC65 , \uD835\uDC45 = \uD835\uDC5F | \uD835\uDC65 , \uD835\uDC4E , \uD835\uDC5F , … , \uD835\uDC65 , \uD835\uDC4E }.\nObviously \uD835\uDC65 cannot fully reflect the state of \uD835\uDC38. In fact, we define the state of \uD835\uDC38 at \uD835\uDC61 as \uD835\uDC60 = \uD835\uDF19(\uD835\uDC65 , \uD835\uDC4E , \uD835\uDC5F , … , \uD835\uDC65 ) . \uD835\uDC60 depends on \uD835\uDC60 and \uD835\uDC4E with a certain probability. We define dynamics of \uD835\uDC38 as p(\uD835\uDC60 , \uD835\uDC5F |\uD835\uDC60 , \uD835\uDC4E ) = Pr{\uD835\uDC46 = \uD835\uDC60 , \uD835\uDC45 = \uD835\uDC5F |\uD835\uDC46 = \uD835\uDC60 , \uD835\uDC34 = \uD835\uDC4E }.\nWe model the auction game as a POMDP rather than a standard MDP because in such a real-world problem very little of the state can be observed (e.g. we never know the users’ behaviors in physical stores). The game is assumed to terminate and restart in cycle. The state space of the POMDP is huge but still finite, standard RL methods such as Q-learning or policy gradient can be applied to learn an agent through the interaction with \uD835\uDC38.\nQ-learning and its variants especially DQN (Mnih et al. 2015) learns a value function \uD835\uDC44(\uD835\uDC60, \uD835\uDC4E; \uD835\uDF03) which indicates the future rewards since current state and derives a policy \uD835\uDF0B∗(\uD835\uDC60, \uD835\uDC4E) = \uD835\uDC4E\uD835\uDC5F\uD835\uDC54\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 \uD835\uDC44(\uD835\uDC60, \uD835\uDC4E; \uD835\uDF03), \uD835\uDC4E ∈ {1,2, … , \uD835\uDC36} . The loss function of DQN at step \uD835\uDC61 is defined as:\n\uD835\uDC3F(\uD835\uDF03 ) = (\uD835\uDC5F + \uD835\uDEFE\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 \uD835\uDC44(\uD835\uDC60 , \uD835\uDC4E; \uD835\uDF03 ) − \uD835\uDC44(\uD835\uDC60 , \uD835\uDC4E ; \uD835\uDF03 )) (1) where \uD835\uDC60 is the state next to \uD835\uDC60 and \uD835\uDF03 keeps a periodic\ncopy of \uD835\uDF03. \uD835\uDC3F(\uD835\uDF03 ) is the foundation of our formulation."
    }, {
      "heading" : "Related Work",
      "text" : "(Mnih et al. 2015) proposed DQN which combined RL and CNN and learned directly from screen pixels and outperformed human experts in Atari 2600 games. (Gu et al. 2016) improved the method with a new network architec-\nture. (Van Hasselt, Guez, and Silver 2016) proposed Double DQN to tackle the overestimate problem in DQN.\nPOMDPs were well studied in (Jaakkola, Singh, and Jordan 1995), (Kaelbling, Littman, and Cassandra 1998), and (Monahan 1982). (Hausknecht and Stone 2015) modeled Atari 2600 games as POMDPs by replacing a full connection lay in DQN by an LSTM.\nAll these algorithms are sequential in that they can only act once after each step of SGD, which is unacceptable in our application scenario. (Mnih et al. 2016) presented A3C and n-step Q-learning among other asynchronous algorithms which decoupled RL algorithms to some extent in that agents could act n steps between 2 training steps, as well as could learn from several copies of the same game at the same time. However, A3C and n-step Q-learning still cannot solve the auction game because our scale requires full decoupling rather than semi decoupling.\nIn parallel with our work, (Cai et al. 2017) presented a RL method for RTB problem based on dynamic programming and CTR prediction, which also went beyond the traditional ECPM policy.\n(Silver et al. 2016) applied RL, CNNs and Monte Carlo Tree Search to the game of Go and their agent namely AlphaGo beat the top human experts in an open competition. We argue that our auction game has a much larger solution space than Go, which makes tree search methods thoroughly impractical. Furthermore, Go is of perfect information, while auction games are of incomplete information in the form of human readable high-level semantic info.\nRecurrent neural networks, especially LSTMs are extensively used in NLP tasks. Text-based game was researched by (Narasimhan, Kulkarni, and Barzilay 2015), they used LSTMs instead of the CNN in DQN. However, the two game studied had a tiny state space compared to auction games. In addition, RNNs need very sophisticated feature engineering to understand high-level semantic information, which makes the model too large to react instantly.\nCharacter-level CNNs were proposed by (Zhang, Zhao, and LeCun 2015), which perform well on text classification tasks without word embedding. (Kim et al. 2016) introduced another character-level CNN with character embedding as inputs.\nThe Architecture of the DSP System in JD\nAs the largest retailer in China, JD.com started its DSP business as early as 2014 to satisfy merchants’ increasing demands for more sales. An overview of the architecture of our DSP system is illustrated in Figure 2. When an auction request from an ADX arrives, the system recalls hundreds of ads inventories as candidates from an ads repository with millions of ads. The ranking module ranks these candidates and identifies the top few ads for bidding (typically\ntop 1). The bidding module computes and returns the ads and bid to the ADX as described in the induction section.\nThe industrially proven auction mechanism in such auction games is the general second price (GSP) method which has a Nash equilibrium in position auctions and is extensively used all over the world. In a GSP auction, a winner DSP knows only the bid of the DSP in the place immediately behind it because that’s the winner’s charge, but none of the losers knows anything about any rivals’ bid. DSPs don’t even know how many rivals are bidding in the auction. The auction game is a typical game of incomplete information where each DSP is a player (Gibbons 1992).\nThe universal business mode of DSPs is that ad impressions from ADX are bought by cost per mille (CPM) and sold to advertisers by cost per click/action (CPC/CPA) to maximize ads performance. Though JD has several charging mechanisms other than CPC (CPA, for example), we speak of CPC in this paper for simplicity and the methods discussed are applicable to others.\nWe used ECPM = Q ∗ bid as described in (Varian 2007) for ranking, in which Q reflects business requirements (e.g. predicted CTR/CVR of the ads) and bid was advertisers’ CPC bids for their clicks. So there is a natural gap between revenue and expenditure which we must control in the bidding module.\nSince 2014, we were using the state-of-the-art ECPM bidding policy. We tried to calibrate Q to a click through rate (CTR ) as depicted in (McMahan et al. 2013), except that we used a factorization machine (Rendle 2012) instead of Poisson regression for calibration. Our ranking model has a structure similar to wide and deep models (Cheng et al. 2016) with billions of weights and tens of gigabyte of memory and disk space requirements, meaning Q can hardly react to the rapidly changing auction environment without delay because the model is too huge to update in time, leading us to design a real-time impression-click data stream for online learning of the calibration CTR model. Afterwards the data stream was reused by LADDER.\nHandling the huge amount of SKUs and hundreds of millions of active users of JD and tens of unknown rival DSPs exceeded the system’s capabilities. Moreover, business requirements demand tradeoffs between profits and\ntotal revenue, e.g. maximize revenue while keeping certain net profit margins to generate economies of scale. To fulfill such requirements, at the end of 2015 we introduced a mechanism with traffic-type level coefficients of bid calibrated by human experts.\nConsequently, the human-machine hybrid control system computed the bid of every auction as bid = Coef ∗ CTR ∗ bid / where human experts modified Coef tens of times per day.\nThe Learning Ad Exchange Bidder\nIn early 2016, we began the research of applying RL algorithms to the RTB auction games. Finally we succeeded in devising a RL agent named LADDER (short for the learning ad exchange bidder)."
    }, {
      "heading" : "Modeling",
      "text" : "We model the auction game as a POMDP. Here we give some important definitions about the POMDP.\nEpisodes. Naturally, we define every day as an episode. Rewards. To control deficits, we use net profits of every auction as rewards of LADDER. Assume our expense (by CPM) and income (by CPC) at time \uD835\uDC61 is \uD835\uDC56 and \uD835\uDC52 respectively, the reward of the auction at time \uD835\uDC61 is r = \uD835\uDC56 − \uD835\uDC52 . For simplicity we use CNY as units of all related variables. Notice that \uD835\uDC56 is always zero unless the user click the ad.\nIn practice, non-zero \uD835\uDC56 is usually 10 ~10 times larger than \uD835\uDC52 . Considering the relatively low click rate, the function we are fitting is extremely steep with most of its values negative while a small subset are high positive. To avoid financial loss, both the tiny negative values and the positive ones must be caught exactly by our model. This implies that high expressive models as CNNs are required.\nActions. We define actions of the auction game at time \uD835\uDC61 as \uD835\uDC4E = \uD835\uDC4F\uD835\uDC56\uD835\uDC51 because bids happen to be discrete. Assume our bid ceiling is \uD835\uDC36, our actions would be from the set \uD835\uDC34 = [0, 0.01, 0.02, … , \uD835\uDC36] because the minimal unit of CNY is 0.01. As a result, our action space is in thousands and expected to be very sparse in the training data.\nStates. The high-level semantic information \uD835\uDC3C\uD835\uDC5B\uD835\uDC53\uD835\uDC5C we can get in JD at time \uD835\uDC61 is about active users, SKUs and ads. That’s the partially observable state. Generally, the \uD835\uDC61 th auction can be formularized as a text description in a domain specific natural language according to \uD835\uDC3C\uD835\uDC5B\uD835\uDC53\uD835\uDC5C , as shown in the following example. All high-level semantic information in the example is in italic:\nHere’s an auction from publisher p: user u is accessing some_site.com/p, u has bought SKUs of ID s1, s2 and s3 a days ago, u browsed SKUs of ID s4 and s5 b days ago… The candidate ad is SKU s6 which is delivered by JD logistic network...\nNotice that all the numbers above (a, b, x, y, c1, c2, s1… s6) are in plain text. There is a practical reason: ID numbering rule of JD requires that similar entities have close IDs, e.g. iphone7’s ID is 3133817, and iphone7 plus’s ID is 3133857 which seems similar, so an experienced expert can judge from plain text that 3133857 would have similar performance as 3133817 in the same auction context even if she’s never seen the former. RNN-based NLP models need elaborate feature engineering (e.g. character n-grams) to utilize such semantic, but such models will comprise billions of weights and therefore too large to react instantly to the auction environment, as discussed earlier. On the contrary, CNNs are good at recognizing similar patterns.\nBased on this interesting observation as well as the definitions, we manage to build a solution. For the \uD835\uDC61th auction, we have a function \uD835\uDF19 which generates a text description from \uD835\uDC3C\uD835\uDC5B\uD835\uDC53\uD835\uDC5C as the above example and one-hot encodes the text as described in (Zhang, Zhao, and LeCun 2015) and feeds the encoded content to a CNN. In fact, the model works well without elaborate feature engineering, thus is space-efficient enough (less than 1Mb) to update instantly.\nIn our productive model, the input text is encoded into a 600 × 71 matrix, of which 600 is the max length of the description and 71 is the alphabet size. In order to save response time of the online service, we formulize the input text in a sort of shorthand with only key information rather than in full text. Also, we use a traditional architecture rather than the state-of-the-art Inception networks or ResNets (Szegedy et al. 2017) for the same reason.\nTable 1 depicts the architecture of our model with output number of the linear layer (aka action space and bid ceiling) omitted deliberately for commercial privacy. All layers except the last one use RELU as activation functions."
    }, {
      "heading" : "Deep Asynchronous Stochastic Q-learning",
      "text" : "RL algorithms are inherently sequential, though A3C and other algorithms in (Mnih et al. 2016) made it possible to act an entire episode between each training step, they are still sequential in nature because the two processes of acting and training in those algorithms are still serially exe-\ncuted. That’s unacceptable for an online DSP service that must respond to each of the huge amount of auctions in several milliseconds. From this perspective, training during serving is absolutely unfeasible, needless to say it requires hundreds of times more servers, which is uneconomical.\nDistinguishingly, we solve this problem by introducing a fully decoupled parallel mechanism which results in a fully asynchronous RL algorithm in which all three processes (learning from the environment, acting in the environment, and observing the environment) are running simultaneously without waiting for each other. Observing is also decoupled because whether an action would result in a positive reward can only be observed asynchronously after tens of minutes when the ad is clicked. Each of the three processes in our algorithm can be deployed to threads in multiple machines to improve runtime performance (Figure 3).\nThough every auction in which we participate shares the same ads budgets and stock units, state transitions in the auction game are stochastic for the uncertainty of user activity. Under this consideration, our algorithm samples the next state of the \uD835\uDC61th auction from the set (\uD835\uDC61, \uD835\uDC61 + \uD835\uDC3C ] where \uD835\uDC3C is a hyper parameter of the algorithm.\nBesides, different publishers always have very different CTR, CVR or ROI. Therefore, auctions from different publishers should be considered as different games. It’s challenging for an agent to bid different auction games at the same time. However, training independent agents for different games as (Mnih et al. 2015) will make more states unobservable. Our solution is requiring the next state of the \uD835\uDC61th auction to be from the same publisher \uD835\uDC43 ."
    }, {
      "heading" : "Data Augmentation and the Loss",
      "text" : "Assume we have a stochastic transition (\uD835\uDF19 , \uD835\uDC4E , \uD835\uDC5F , \uD835\uDF19 ) as discussed above, considering the property of GSP auctions and the definition of \uD835\uDC4E and \uD835\uDC5F , we have a deduction that any bid above \uD835\uDC4E would win the auction \uD835\uDC61 and any bid below \uD835\uDC5F would lose the auction. Given the deduction, for all \uD835\uDC4E ∈ \uD835\uDC34, we redefine rewards of the auction \uD835\uDC61 as:\n\uD835\uDC5F , ≔ 0 \uD835\uDC5F\nfor all \uD835\uDC4E < \uD835\uDC4E otherwise (2)\nCombining Formula (1) and Formula (2) results in the following definition:\n\uD835\uDC66 , ≔ \uD835\uDC5F , \uD835\uDC5F , + γmax \uD835\uDC44(\uD835\uDF19 , \uD835\uDC4E ; \uD835\uDF03 ) terminal \uD835\uDF19 otherwise (3) And we define the loss function of LADDER as:\n\uD835\uDC3F (\uD835\uDF03) = ∑ (y , − Q(ϕ , \uD835\uDC4E; θ)) (4)\nThe original loss of DQN as Formula (1) still works, especially for application whose actions are not as correlated as auction games. Although we use DQN in this paper, Double DQN and Dueling Double DQN (Wang et al. 2015) can be naturally incorporated in out algorithm.\nTo maximize revenue while keeping breakeven, we introduce a weighted sampling method to tune the im-\nportance of positive rewards, which is controlled by the hyper parameter \uD835\uDF0B. We also use an experience memory as in DQN. The full algorithm, which we call deep asynchronous stochastic Q-learning, is presented in Algorithm 1.\nAlgorithm 1 Deep asynchronous stochastic Q-learning\nInitialize experience memory \uD835\uDC37 to capacity \uD835\uDC41 Initialize parameters (\uD835\uDF03, \uD835\uDF03 , \uD835\uDF03 ) of action-value function \uD835\uDC44\nwith random weights procedure Serving\nwhile true do Get auction \uD835\uDC34 of publisher \uD835\uDC43 at timestamp t \uD835\uDF19 ≔ \uD835\uDF19(\uD835\uDC34 , \uD835\uDC3C\uD835\uDC5B\uD835\uDC53\uD835\uDC5C , ) Asynchronously fetch snapshot of parameters \uD835\uDF03 to \uD835\uDF03 With probability ε select a random bid \uD835\uDC4E otherwise select \uD835\uDC4E = \uD835\uDC5A\uD835\uDC4E\uD835\uDC65 \uD835\uDC44∗(\uD835\uDF19 , \uD835\uDC4E; \uD835\uDF03 ) Respond to bidding request \uD835\uDC34 with bid \uD835\uDC4E\nend while procedure OBSERVING\nwhile true do Update \uD835\uDC3C\uD835\uDC5B\uD835\uDC53\uD835\uDC5C Observe reward \uD835\uDC5F and store (\uD835\uDF19 , \uD835\uDC4E , \uD835\uDC5F ) in \uD835\uDC37\nend while procedure TRAINING\nwhile true do Sample random mini-batch of stochastic transitions\n(\uD835\uDF19 , \uD835\uDC4E , \uD835\uDC5F , \uD835\uDF19 ) from \uD835\uDC37 where: \uD835\uDC61 ~\uD835\uDC48((\uD835\uDC61 , \uD835\uDC61 + \uD835\uDC3C ]) and \uD835\uDC43 = \uD835\uDC43 with probability:\nprob = \uD835\uDF0B 1\nif \uD835\uDC5F < 0 otherwise\nFor all \uD835\uDC4E ∈ \uD835\uDC34, perform: \uD835\uDC66 , ≔ \uD835\uDC5F , \uD835\uDC5F , + \uD835\uDEFE\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 \uD835\uDC44(\uD835\uDF19 , \uD835\uDC4E ; \uD835\uDF03 ) terminal \uD835\uDF19 otherwise\nPerform an SGD step on ∑ (y , − Q(ϕ , \uD835\uDC4E; θ))\n\uD835\uDF03 ≔ \uD835\uDF03 every \uD835\uDC3C steps end while\nmain Asynchronously start SERVING, OBSERVING and TRAINING"
    }, {
      "heading" : "Experimental Results",
      "text" : "The experiments of LADDER were run on four important publishers that occupy a significant part of the revenues of JD.com’s DSP business. We try to improve both revenue and profits of the publishers in the experiments."
    }, {
      "heading" : "Experiment Setup",
      "text" : "We run the training procedure of Algorithm 1 on 4 Tesla K80 GPUs and the serving procedure on 24 PC servers (Figure 3) with a high-performance C++ sparse convolver. We use RMSProp to optimize the loss in Formula (4) with a learning rate of 5 . The usage of ε-greedy was restricted with an ε of 1 to minimize negative influence on the business. Training was decomposed into 2 phases:\nImitation. We filled the experience memory with data generated by the ECPM policy of the hybrid system. At this stage, before enough self-generated data get into the memory, LADDER is just learning the ECPM policy. In this cold-starting phase, LADDER interacts little with the environment thus ensures that losses are under control.\nIntrospection. After several hours of imitation (the time actually required depends on \uD835\uDC41), LADDER starts to learn from data generated by its own policy."
    }, {
      "heading" : "Evaluation",
      "text" : "In May 2017, we evaluated LADDER with online A/B test in an overlapping experiment system similar to (Tang et al. 2010) and regarded the ECPM policy as baseline. In the beginning, LADDER was bidding 10% of the auctions per day and the remaining 90% was running the baseline. We launched LADDER at 90% of the auctions in the 8th day and keep the rest 10% as holdback which run the baseline policy for months for the sake of scientific rigor. The experiment system performed a proportional normalization to all experiments for ease of comparison.\nFigures 4 shows the performance comparisons between LADDER and baseline. We normalized all data in the figures into range [0,1] for privacy. Figure 4(a) shows the rewards (profits) comparison, where we can see that LADDER incurred huge losses on the first day in the imitation phase because it tended to bid up all requests for exploration. It soon turned into the second phase and caught up with the baseline the next day, and eventually outperformed the baseline since day 5. Notice that the Q curve well fitted the curve of rewards of LADDER. There was a retreat at day 8 because we launched LADDER that day, therefore the experimental data were mixed up.\nFigure 4(b) and Figure 4(c) shows the revenue growth: LADDER made a huge improvement by more than 50% since the first day. It seems that LADDER had learned the key of economies of scale that more revenues always generate more profits. Figure 4(d) shows that LADDER also raised CTR as much as about 35%, which is reasonable because the experimented publishers was on a CPC basis.\nAccording to the holdback, the improvements are permanent. Especially, during JD.com’s June 18th anniversary sale of 2017, LADDER increased the revenue of the 4 publishers by 54% and the advertisers ROI by 17% as shown in Figure 5, thus contributed a growth of 17% to the total revenue of JD’s DSP business and 7% to the total ROI of the sale. The improvement during the sale proves the adaptability and responsiveness of LADDER in a highly volatile and competitive environment."
    }, {
      "heading" : "Exploration and Exploitation",
      "text" : "The hyper parameter \uD835\uDF45 controls the balance between exploration and exploitation. To maximize the revenues, our launched deployment set \uD835\uDF45 to 0.6. As Figures 6 depict, when we decrease \uD835\uDF45 from 0.6 to 0.55, revenue decrease while rewards and CTR increase, which means the agent tends to explore less aggressively."
    }, {
      "heading" : "Visualization",
      "text" : "In order to figure out LADDER’s capability of understanding high-level semantic information embedded in the plaintext description inputs, we use t-SNE to visualize the outputs of the hidden layer. Our analysis is from two angles."
    }, {
      "heading" : "Multiple Games in a Single Model",
      "text" : "As mentioned earlier, LADDER serves very different publishers (aka different auction games) simultaneously. Although challenging, Figure 5 shows that LADDER had successfully learned from the plain-text inputs the difference of the publishers. It surpassed the baseline in revenue and CTR for each publisher evaluated. To technically verify how well LADDER can distinguish different publishers, we use publisher type as label and visualize 1000,000 random samples in Figure 7(a). As expected, all 4 publishers are mapped into separate clusters perfectly."
    }, {
      "heading" : "Complex Semantic",
      "text" : "In Figure 7(a), samples of the same publisher scatter into several clusters. In fact, LADDER’s learned semantic much more complex than publisher IDs. For further analysis, we visualize data only of publisher 1 from the same 1000,000 samples. As Figure 7(b) shows, LADDER has learned rather complex conditions from the plain-text inputs, which are essential to bid an auction. E.g. SKUs delivered by JD logistic network (JDLN) may be more attractive for a cold-start user because JDLN is well-known to feature a superior user experience. As the left part of Figure 7(b) indicates, LADDER recognizes these situations."
    }, {
      "heading" : "Future Work",
      "text" : "Real-time online auctions are not the only large scale real world problems in which human-level agents excel. Considering that ADXs mimics stock exchanges, applying LADDER in quantitative trading is also of great interest and challenge.\nRecommendation system is a domain with similarities to online advertising, so our approach should work in the area with a domain specific loss function.\nWhat we are working on is applying LADDER not only for bidding but also in the ranking phase of online advertising, which may also bring significant business benefits."
    }, {
      "heading" : "Conclusions",
      "text" : "We present a reinforcement learning agent namely LADDER in this paper for solving the auction game of JD DSP. Our aim is to create a human-level agent that is capable of not only saving manpower while performing as well as or even better than humans, but also directly understanding the situation of an auction from a plain-text description, as human experts do. As the result, LADDER reach the goal by easily outperforming the existing industrial state-of-theart solution in A/B tests, which means it has made full use the high-level semantic information in the auction game without sophisticated feature engineering and reacts to the changing auction environment immediately.\nWe also introduce DASQN, an asynchronous stochastic Q-network which totally decouples the learning, observing and acting processes in Q-learning, hence greatly improving its run-time performance and enabling the algorithm to solve large scale real-world problems."
    } ],
    "references" : [ {
      "title" : "Searching for solutions in games and artificial intelligence",
      "author" : [ "L.V. Allis" ],
      "venue" : null,
      "citeRegEx" : "Allis,? \\Q1994\\E",
      "shortCiteRegEx" : "Allis",
      "year" : 1994
    }, {
      "title" : "Realtime bidding algorithms for performance-based display ad allocation",
      "author" : [ "Y. Chen", "P. Berkhin", "B. Anderson", "N.R. Devanur" ],
      "venue" : "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining",
      "citeRegEx" : "Chen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2011
    }, {
      "title" : "Wide & deep learning for recommender systems",
      "author" : [ "H.T. Cheng", "L. Koc", "J. Harmsen", "T. Shaked", "T. Chandra", "H. Aradhye", "G. Anderson", "G. Corrado", "W. Chai", "M. Ispir" ],
      "venue" : "Proceedings of the 1st Workshop on Deep Learning for Recommender Systems",
      "citeRegEx" : "Cheng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "A primer in game theory. Harvester Wheatsheaf",
      "author" : [ "R. Gibbons" ],
      "venue" : null,
      "citeRegEx" : "Gibbons,? \\Q1992\\E",
      "shortCiteRegEx" : "Gibbons",
      "year" : 1992
    }, {
      "title" : "Continuous deep q-learning with model-based acceleration",
      "author" : [ "S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine" ],
      "venue" : "International Conference on Machine Learning.,",
      "citeRegEx" : "Gu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep recurrent q-learning for partially observable mdps. CoRR, abs/1507.06527",
      "author" : [ "M. Hausknecht", "P. Stone" ],
      "venue" : null,
      "citeRegEx" : "Hausknecht and Stone,? \\Q2015\\E",
      "shortCiteRegEx" : "Hausknecht and Stone",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning algorithm for partially observable Markov decision problems",
      "author" : [ "T. Jaakkola", "S.P. Singh", "M.I. Jordan" ],
      "venue" : "Advances in neural information processing systems.,",
      "citeRegEx" : "Jaakkola et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Jaakkola et al\\.",
      "year" : 1995
    }, {
      "title" : "Planning and acting in partially observable stochastic domains",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra" ],
      "venue" : "Artificial intelligence.,",
      "citeRegEx" : "Kaelbling et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kaelbling et al\\.",
      "year" : 1998
    }, {
      "title" : "Ad click prediction: a view from the trenches",
      "author" : [ "H.B. McMahan", "G. Holt", "D. Sculley", "M. Young", "D. Ebner", "J. Grady", "L. Nie", "T. Phillips", "E. Davydov", "D. Golovin" ],
      "venue" : "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining",
      "citeRegEx" : "McMahan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "McMahan et al\\.",
      "year" : 2013
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "International Conference on Machine Learning.,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "State of the art—a survey of partially observable Markov decision processes: theory, models, and algorithms",
      "author" : [ "G.E. Monahan" ],
      "venue" : "Management Science.,",
      "citeRegEx" : "Monahan,? \\Q1982\\E",
      "shortCiteRegEx" : "Monahan",
      "year" : 1982
    }, {
      "title" : "Optimal auction design",
      "author" : [ "R.B. Myerson" ],
      "venue" : "Mathematics of operations research.,",
      "citeRegEx" : "Myerson,? \\Q1981\\E",
      "shortCiteRegEx" : "Myerson",
      "year" : 1981
    }, {
      "title" : "Language understanding for text-based games using deep reinforcement learning",
      "author" : [ "K. Narasimhan", "T. Kulkarni", "R. Barzilay" ],
      "venue" : "arXiv preprint arXiv:1506.08941",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Factorization machines with libfm",
      "author" : [ "S. Rendle" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST).,",
      "citeRegEx" : "Rendle,? \\Q2012\\E",
      "shortCiteRegEx" : "Rendle",
      "year" : 2012
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "author" : [ "C. Szegedy", "S. Ioffe", "V. Vanhoucke", "A.A. Alemi" ],
      "venue" : null,
      "citeRegEx" : "Szegedy et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2017
    }, {
      "title" : "Overlapping experiment infrastructure: More, better, faster experimentation",
      "author" : [ "D. Tang", "A. Agarwal", "D. O’Brien", "M. Meyer" ],
      "venue" : "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining",
      "citeRegEx" : "Tang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2010
    }, {
      "title" : "Deep Reinforcement Learning with Double Q-Learning",
      "author" : [ "H. Van Hasselt", "A. Guez", "D. Silver" ],
      "venue" : null,
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "Dueling network architectures for deep reinforcement learning",
      "author" : [ "Z. Wang", "T. Schaul", "M. Hessel", "H. Van Hasselt", "M. Lanctot", "N. De Freitas" ],
      "venue" : "arXiv preprint arXiv:1511.06581",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Real-time bidding for online advertising: measurement and analysis",
      "author" : [ "S. Yuan", "J. Wang", "X. Zhao" ],
      "venue" : "Proceedings of the Seventh International Workshop on Data Mining for Online Advertising. ACM,",
      "citeRegEx" : "Yuan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2013
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "X. Zhang", "J. Zhao", "Y. LeCun" ],
      "venue" : "Advances in neural information processing systems.,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Researchers have made great progress recently in learning to control agents directly from raw high-dimensional sensory inputs like vision in domains such as Atari 2600 games (Mnih et al. 2015), where reinforcement learning (RL) agents have human-level performance.",
      "startOffset" : 174,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : "Obviously, the process of many DSPs/ADXs bidding for an ad offer is an auction game (Myerson 1981) of incomplete information.",
      "startOffset" : 84,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "However, the online ads industry just ignores this fact and considers RTB a solved problem: all existing DSPs model auction games as supervised learning (SL) problems by predicting the click through rate (CTR) (McMahan et al. 2013) or conversion rate (CVR) (Yuan, Wang, and Zhao 2013) of ads and using effective cost per mille (ECPM) as bids (Chen et al.",
      "startOffset" : 210,
      "endOffset" : 231
    }, {
      "referenceID" : 1,
      "context" : "2013) or conversion rate (CVR) (Yuan, Wang, and Zhao 2013) of ads and using effective cost per mille (ECPM) as bids (Chen et al. 2011).",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : "com started its DSP business in 2014, at first we employed the industry state-of-the-art approach of ECPM bidding with a calibrated CTR model (McMahan et al. 2013) as depicted in Figure 2.",
      "startOffset" : 142,
      "endOffset" : 163
    }, {
      "referenceID" : 15,
      "context" : "For comparison, the solution space of the game of Go is about 10 (Allis and others 1994; Silver et al. 2016).",
      "startOffset" : 65,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "That’s very different from Atari games, text-based games (Narasimhan, Kulkarni, and Barzilay 2015) and the game of Go (Silver et al. 2016).",
      "startOffset" : 118,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "Although sophisticated feature engineering can utilize these information in a model like wide and deep models (Cheng et al. 2016) or factorization machines (Rendle 2012) as is already in place in the hybrid system, taking into account JD’s scale, such models will be of billions of features and therefore too heavy to react instantly to the rapidly varying auction environment, leading to poor performance.",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "2016) or factorization machines (Rendle 2012) as is already in place in the hybrid system, taking into account JD’s scale, such models will be of billions of features and therefore too heavy to react instantly to the rapidly varying auction environment, leading to poor performance.",
      "startOffset" : 32,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "Q-learning and its variants especially DQN (Mnih et al. 2015) learns a value function Q(s, a; θ) which indicates the future rewards since current state and derives a policy π(s, a) = argmax Q(s, a; θ), a ∈ {1,2, .",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "(Mnih et al. 2015) proposed DQN which combined RL and CNN and learned directly from screen pixels and outperformed human experts in Atari 2600 games.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "(Gu et al. 2016) improved the method with a new network architec-",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 11,
      "context" : "POMDPs were well studied in (Jaakkola, Singh, and Jordan 1995), (Kaelbling, Littman, and Cassandra 1998), and (Monahan 1982).",
      "startOffset" : 110,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "(Hausknecht and Stone 2015) modeled Atari 2600 games as POMDPs by replacing a full connection lay in DQN by an LSTM.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "(Mnih et al. 2016) presented A3C and n-step Q-learning among other asynchronous algorithms which decoupled RL algorithms to some extent in that agents could act n steps between 2 training steps, as well as could learn from several copies of the same game at the same time.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "(Silver et al. 2016) applied RL, CNNs and Monte Carlo Tree Search to the game of Go and their agent namely AlphaGo beat the top human experts in an open competition.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "The auction game is a typical game of incomplete information where each DSP is a player (Gibbons 1992).",
      "startOffset" : 88,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "We tried to calibrate Q to a click through rate (CTR ) as depicted in (McMahan et al. 2013), except that we used a factorization machine (Rendle 2012) instead of Poisson regression for calibration.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "2013), except that we used a factorization machine (Rendle 2012) instead of Poisson regression for calibration.",
      "startOffset" : 51,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "Our ranking model has a structure similar to wide and deep models (Cheng et al. 2016) with billions of weights and tens of gigabyte of memory and disk space requirements, meaning Q can hardly react to the rapidly changing auction environment without delay because the model is too huge to update in time, leading us to design a real-time impression-click data stream for online learning of the calibration CTR model.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "Also, we use a traditional architecture rather than the state-of-the-art Inception networks or ResNets (Szegedy et al. 2017) for the same reason.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "RL algorithms are inherently sequential, though A3C and other algorithms in (Mnih et al. 2016) made it possible to act an entire episode between each training step, they are still sequential in nature because the two processes of acting and training in those algorithms are still serially exe-",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "However, training independent agents for different games as (Mnih et al. 2015) will make more states unobservable.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "Although we use DQN in this paper, Double DQN and Dueling Double DQN (Wang et al. 2015) can be naturally incorporated in out algorithm.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "In May 2017, we evaluated LADDER with online A/B test in an overlapping experiment system similar to (Tang et al. 2010) and regarded the ECPM policy as baseline.",
      "startOffset" : 101,
      "endOffset" : 119
    } ],
    "year" : 2017,
    "abstractText" : "We present LADDER, the first deep reinforcement learning agent that can successfully learn control policies for largescale real-world problems directly from raw inputs composed of high-level semantic information. The agent is based on an asynchronous stochastic variant of DQN (Deep Q Network) named DASQN. The inputs of the agent are plain-text descriptions of states of a game of incomplete information, i.e. real-time large scale online auctions, and the rewards are auction profits of very large scale. We apply the agent to an essential portion of JD’s online RTB (real-time bidding) advertising business and find that it easily beats the former state-of-the-art bidding policy that had been carefully engineered and calibrated by human experts: during JD.com’s June 18th anniversary sale, the agent increased the company’s ads revenue from the portion by more than 50%, while the advertisers’ ROI (return on investment) also improved significantly.",
    "creator" : null
  }
}