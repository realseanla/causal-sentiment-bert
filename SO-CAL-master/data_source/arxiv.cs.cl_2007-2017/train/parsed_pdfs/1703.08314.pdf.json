{
  "name" : "1703.08314.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Bob Coecke", "Fabrizio Genovese", "Martha Lewis", "Dan Marsden", "Robin Piedeleu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "How should we represent concepts and how can they be composed to form new concepts, phrases and sentences? These questions are fundamental to cognitive science. Conceptual spaces theory gives a way of describing structured concepts [12, 13], not starting from linguistic assumptions, but from cognitive considerations about human reasoning. Conceptual spaces describe a “semantics of the mind”, modelling mental descriptions of concepts. The key idea is that human beings represent concepts geometrically in certain fundamental domains of understanding such as space, motion, taste and colour. These domains are combined to form a conceptual space describing the features of interest. Conceptual spaces provide a middle ground between symbolic and connectionist representations of concepts, oriented towards tasks of interest in cognitive science, such as categorization and assessing similarity. A concept is then described by convex subsets of the relevant domains. The convexity requirement can be seen as a means of identifying robust, meaningful concepts. For example, if two points in some space are considered to represent the colour red, then intuitively we would expect that every point “in between” would also be considered red.\nCategorical compositional distributional models [11] successfully exploit the compositional structure of natural language in a principled manner, and have outperformed other approaches in Natural Language Processing (NLP) [14, 18]. The approach works as follows. A mathematical formalization of grammar is chosen, for example Lambek’s pregroup grammars [21], although the approach is equally effective with other categorial grammars [7]. Such a categorial grammar allows one to verify whether a phrase of a sentence is grammatically well-formed by means of a computation that establishes the overall grammatical type, referred to as a type reduction. The meanings of individual words are established using a distributional model of language, where they are described as vectors of co-occurrence statistics derived automatically from corpus data [23]. The categorical compositional distributional programme\n∗This paper is a significantly extended version of the workshop paper [3] †authors in alphabetical order\nar X\niv :1\n70 3.\n08 31\n4v 1\n[ cs\n.L O\n] 2\n4 M\nar 2\nunifies these two aspects of language in a compositional model where grammar mediates composition of meanings. This allows us to derive the meaning of sentences from their grammatical structure, and the meanings of their constituent words. The key insight that allows this approach to succeed is that both pregroups and the category of vector spaces carry the same abstract structure [11], and the same holds for other categorial grammars since they typically have a weaker categorical structure.\nThe abstract framework of the categorical compositional scheme is actually broader in scope than NLP. It can be applied in other settings in which we wish to compose meanings in a principled manner, guided by structure. The outline of the general programme is as follows:\n1. (a) Choose a compositional structure, such as a pregroup or any other categorial grammar.\n(b) Interpret this structure as a category, the grammar category.\n2. (a) Choose or craft appropriate meaning or concept spaces, such as spaces of propositions, vector spaces, density matrices [25, 2] or conceptual spaces.\n(b) Organize these spaces into a category, the semantics category, with the same abstract structure as the grammar category.\n3. Interpret the compositional structure of the grammar category in the semantics category via a functor preserving the type reduction structure.\n4. Bingo! This functor maps type reductions in the grammar category onto algorithms for composing meanings in the semantics category.\nIn order to move away from vector spaces, we construct a new categorical setting for interpreting meanings which respects the important convex structure emphasized in conceptual spaces theory. We show that this category has the necessary abstract structure required by categorical compositional models. We then construct convex spaces for interpreting the types for nouns, adjective and verbs. Finally, this allows us to use the reductions of the pregroup grammar to compose meanings in conceptual spaces. We illustrate our approach with concrete examples, and go on to discuss directions for further research."
    }, {
      "heading" : "2 Categorical compositional meaning",
      "text" : "In this section we describe the details of the categorical compositional approach to meaning. We provide examples of semantic categories and grammar categories, and show the general method by which the grammar category induces a notion of concept composition in the semantic category."
    }, {
      "heading" : "2.1 Monoidal categories",
      "text" : "We begin by briefly review some of the category theory underpinning categorical compositional models.\nDefinition 1. A monoidal category is a tuple (C,⊗, I, α, λ, ρ) where\n• C is a category\n• ⊗, the tensor, is a functor C × C → C where we write A⊗B for ⊗(A,B)\n• I, the unit, is an object of C\nThe remaining data are natural isomorphisms, with components of type:\n• αA,B,C : ((A⊗B)⊗ C)→ (A⊗ (B ⊗ C))\n• ρA : A⊗ I → A\n• λA : I ⊗A→ A\nThese natural isomorphisms, moreover, must be such that any formal and well-typed diagram made up from ⊗, α, λ, ρ, α−1, ρ−1, λ−1 and identities commutes. Here ‘formal’ means ‘not dependent on the structure of any particular monoidal category.’\nFor a precise statement and discussion of the above definition, we direct the reader to [24]. A more gentle introduction can be found in [9]. For the purpose of our paper, the objects of a monoidal category should be thought of as system types. A morphism f : A→ B is then a process taking inputs of type A and giving outputs of type B. The object A⊗B represents the systems A and B composed in parallel. Hence, a morphism f ⊗ g : A⊗B → C ⊗D is to be thought of as running the process f : A→ C whilst running the process g : B → D. The object I is thought of as the trivial system.\nExample 1. The category Rel of sets and relations is monoidal. The tensor ⊗ is the Cartesian product and I is any singleton set {?}.\nExample 2. The category FdVectR of finite dimensional real vector spaces and linear maps is monoidal. The tensor ⊗ is the tensor product, the trivial system I is the one-dimensional real vector space R.\nMonoidal categories admit an elegant and powerful graphical notation that we will exploit heavily in later sections. In this notation, an object A is denoted by a wire:\nA\nA morphism f : A→ B is represented by a box:\nf\nB\nA\nIf g : B → C, the composite g ◦ f : A→ C is given by:\nf\ng\nA\nB\nC\nIf h : A→ B and k : C → D, the morphism h⊗ k : A⊗ C → B ⊗D is depicted by:\nh k\nB D\nCA\nThe trivial system I is the empty diagram. Morphisms u : I → A and v : A→ I are drawn respectively as\nu and v\nThese special morphisms are referred to as states and effects."
    }, {
      "heading" : "2.2 Compact closed categories",
      "text" : "A specific class of monoidal categories, the compact closed categories, will be of particular importance.\nDefinition 2. A monoidal category (C,⊗, I) is compact closed if for each object A ∈ C there are objects Al, Ar ∈ C (the left and right duals of A) and morphisms\nηlA : I → A⊗Al ηrA : I → Ar ⊗A lA : A l ⊗A→ I rA : A⊗Ar → I\nsatisfying the snake equations\n(1A ⊗ l) ◦ (ηl ⊗ 1A) = 1A ( r ⊗ 1A) ◦ (1A ⊗ ηr) = 1A ( ⊗ 1Al) ◦ (1Al ⊗ ηl) = 1Al (1Ar ⊗ r) ◦ (ηr ⊗ 1Ar ) = 1Ar\nThe and η maps are called caps and cups respectively, and are depicted graphically as:\nAr A A Al AAlArA\nr l\nηr ηl\nGraphically, the snake equations become\n= =\nA Al A A Ar A AA\nArAr A Ar\n==\nA Al AlAl\nCompact closed categories are a convenient level of abstraction at which to work. Many of the categories one would think to use as either grammar or meaning categories have a compact closed structure.\nExample 3. All objects in Rel are self-dual. Both caps are given by\nX : X ×X → {?} :: {((x, x), ?) | x ∈ X}\nThe associated cup ηX is the converse of the above. The snake equations can be verified by direct calculation.\nExample 4. FHilb is the category of finite dimensional real inner product spaces. As in the case of FdVectR, the tensor ⊗ is the tensor product of vector spaces and I is the one-dimensional space R. In defining cups and caps, we make use of the fact that if {vi}i and {uj}j are bases for vector spaces V and U respectively, then {vi ⊗ uj}i,j is a basis for V ⊗U . Moreover, any linear map is fully determined by its action on a basis. Every finite-dimensional vector space is self-dual, and the cups and caps are\ngiven by\nV : V ⊗ V → R :: ∑ i,j ci,j (vi ⊗ vj) 7→ ∑ i,j ci,j〈vi|vj〉\nηV : R→ V ⊗ V :: 1 7→ ∑ i (vi ⊗ vi)\nVerifying that these maps satisfy the snake equations again follows from a straightforward calculation.\nRemark 1. The tensor in a compact closed category is not necessarily a categorical product. Specifically, we cannot expect to describe every state of A ⊗ B as a tensor of two states taken from A and B. We can understand this as showing composite systems have interesting behaviour that cannot be explained in terms of the behaviour of their component parts. In fact, if the tensor of a compact closed category happens to be a categorical product, then that category must be trivial, in a precise mathematical sense [8]."
    }, {
      "heading" : "2.3 Grammar categories",
      "text" : "Many algebraic gadgets exist to model grammar, as detailed in [6] for example. In the present work, we use Lambek’s pregroup grammars, as many grammars have a pregroup structure [28]. Moreover, pregroups can be viewed as compact closed categories. It should be emphasized though that our approach does not depend on pregroups, and can be applied to other grammatical models.\nDefinition 3. A pregroup is a tuple (A, ·, 1,−l,−r,≤) where (A, ·, 1,≤) is a partially ordered monoid and −r,−l are functions A→ A such that ∀x ∈ A,\nx · xr ≤ 1 (1) xl · x ≤ 1 (2) 1 ≤ xr · x (3) 1 ≤ x · xl (4)\nThe · will usually be omitted, writing xy for x · y. One interprets grammar by freely generating a pregroup from a set of atomic linguistic types. Words are then assigned an element of the pregroup depending on their linguistic function. A string of words is then mapped to an element of the pregroup by multiplying together the elements associated with its constituent words in their syntactic order. If s1 · · · sn ≤ t, we say that the type s1 · · · sn reduces to the type t. The pregroup freely generated by a set A is denoted by PregA.\nExample 5. For simplicity, we only use the linguistic types n, for noun, and s, for sentence. Hence, we will work with Preg{n,s}. Consider the sentence ‘Chickens cross roads.’ The nouns chickens and roads are of type n, and the transitive verb cross is assigned the type nrsnl. ‘Chickens cross roads’ therefore has type n(nrsnl)n. Then we have the following type reductions:\nn(nrsnl)n = (nnr)s(nln)\n(1) ≤ s(nln) (2) ≤ s\nNote that we could also have performed these two steps in the opposite order. The above reduction can be given a neat graphical interpretation as follows:\nn nrsnl n\nchickens cross roads\nwhere it is now very clear that the order of the reductions doesn’t matter. This is a feature that is typical for pregroup grammars, while other categorial grammars such as Lambek’s original categorial grammar [20] have more constraints on the order of the reductions.\nA pregroup can be considered as a compact closed category. The objects of this category are the elements of the pregroup. The morphisms are given by the order structure of the pregroup. That is, there is a unique morphism p→ q if and only if p ≤ q. The tensor ⊗ is the monoid multiplication and the monoidal unit is the element 1. Unsurprisingly, the left and right duals of p are pl and pr respectively. The cups and caps are the unique morphisms given by the pregroup axioms (1)–(4). In the reduction diagram, note how the cups correspond to the cups of the compact closed structure."
    }, {
      "heading" : "2.4 Meaning categories",
      "text" : "Distributional models of meaning use vector spaces to represent the meaning of words. In this paper we move away from this approach, choosing instead to work in ConvexRel, the category of convex sets and convexity-respecting relations. Before describing this new setting, we first present the conventional vector space approach as a demonstration of the categorical compositional method. This will prepare the ground for detailed discussion of ConvexRel to later sections.\nOne way to model meanings in a vector space is to use co-occurrence statistics [4]. The meaning of a word is identified with the frequency with which it appears near other words. One first chooses a collection of context words. These will be the basis vectors. One then analyses a large corpus of writing to determine how often words co-occur with the context words. For example, suppose the word dog appears in the same context as cat, companion, and cuisine respectively 19, 25, and 2 times. If our collection of context words is {cat, companion, cuisine}, then dog would be assigned the vector (19, 25, 2). A drawback of the co-occurrence approach is that antonyms appear in similar contexts and, hence, words such as ‘win’ and ‘lose’ are indistinguishable (despite evidently being different in meaning). Another related difficulty is that vector spaces are notoriously bad for representing basic propositional logic. Nonetheless, the vector space model is highly successful in NLP.\nOne can also use basis vectors to represent quality dimensions. For example, [27, 31, 15] all represent concepts as feature vectors, with basis dimensions representing attributes of the concept. So, for instance, one might represent the word dog with certain values on the fluffy, loyal, and wolf-like. This approach closely resembles ours in this paper, though we move away from the vector space setting."
    }, {
      "heading" : "2.5 Putting it all together",
      "text" : "We now have all the necessary components: a grammar category and a meaning category. We used the examples of pregroup grammars and vector spaces, but we stress yet again that the abstract method applies equally well to any two compact closed categories.\nSuppose we have a string of words w1 · · ·wn and a pregroup P . Suppose further that wi ∈Wi, where Wi is the vector space associated with wi’s linguistic type. The meaning of w1 · · ·wn is computed as follows:\n1. Assign a pregroup element pi to each word wi based on its linguistic type.\n2. Apply pregroup reduction rules (cups and caps) to the element p1p2 · · · pn to obtain a simpler type x such that:\np1p2 · · · pn ≤ x.\n3. Thinking of the above reduction as a morphism in the pregroup built up from , η and identities (as given by its reduction diagram), apply the corresponding vector space morphism of type:\nW1 ⊗ · · · ⊗Wn →WX\nto the string of word meanings represented as w1 ⊗ · · · ⊗ wn.\nExample 6. Consider again the sentence ‘chickens cross roads’. The nouns chickens and roads have type n and so are represented in some vector space N of nouns. The transitive verb cross has type nrsnl and, hence, is represented by a vector in the vector space N ⊗S⊗N where S is a vector space modelling sentence meaning. The meaning of ‘chickens cross roads’ is the image of\n−−−−−→ chickens ⊗−−−→cross ⊗ −−−→ roads (5)\nunder the map ηN ⊗ 1S ⊗ ηN : N ⊗ (N ⊗ S ⊗N)⊗N → S (6)\nThis nicely illustrates the general method. Our meaning category supplies the qualitative meanings of chickens, cross, and roads. Our grammar category then tells us how to stitch these together. This corresponds to ‘telling us where to put cups and caps.’ The essence of the method should be thought of as the diagram\nchickens cross roads\nN NS\nwhere we think of the words as meaning vectors (5) and the wires as the map (6). In fact, rather than just using cups and identity wires, which are enough to account for the grammatical structure captured by pregroups, we can enrich the graphical language to directly account for meanings of functional words, such as relative pronouns."
    }, {
      "heading" : "2.6 Beyond standard categorial grammar",
      "text" : "A first rather trivial example of a functional word is “does”, which can be accounted for by means of caps [11]:\nchickens cross roads\ndo\nSN N\nThings become more interesting when we introduce, rather than just wires, the idea of a multi-wire (also called spider [8]) which can have more than two ends, or less:\nµN := ιS :=\nThe way these behave is just like wires. The only thing that matters is: ‘either being connected by a multi-wire, or not’. As a consequence, multi-wires ‘fuse’ together:\n=\n. . .\n. . .\n. . .. . .\n. . .. . .\nThese multi-wires can be defined in category-theoretic terms for any symmetric monoidal category, where they are called commutative special dagger Frobenius structures [10, 8].\nExample 7. On each set X in the category Rel one can take the relations\nX × . . .×X → X × . . .×X :: {((x, . . . , x), (x, . . . , x)) | x ∈ X}\nto be the multi-wires, and note in particular that these include identities, cups and caps.\nExample 8. On each inner product space V in the category FdVectR, given any orthonormal basis {vi}i for V , one can take the linear maps\nV ⊗ . . .⊗ V → V ⊗ . . .⊗ V :: vi1 ⊗ . . .⊗ vin 7→ δi1...invi1 ⊗ . . .⊗ vi1\nto be the multi-wires, and again these include identities, cups and caps.\nUsing these multi-wires we can now express the meaning of relative pronouns [29, 30]:\nchickens cross roads\nthat\nN NN S\nFirstly, note that what we obtain is a noun rather than a sentence, and one that is closely related to ‘dead chicken’. Secondly, note that the use of the three-wire is mainly conjunction, conjoining ‘[is] chicken’ and ‘crosses road’. In this paper we will use them to directly express conjunctions. The one-wire gets rid of the sentence type."
    }, {
      "heading" : "3 Conceptual Spaces",
      "text" : "Conceptual spaces are proposed in [12] as a framework for representing information at the conceptual level. Gärdenfors contrasts his theory with both a symbolic approach to concepts, and an associationist approach where concepts are represented as associations between different kinds of information elements. Instead, conceptual spaces are structures based on quality dimensions such as weight, height, hue and brightness. Concepts are roughly interpreted as convex subsets of a conceptual space. Conceptual spaces have an internal structure based on how quality dimensions interact with each other. A pair (or set) of dimensions is called integral if assignment of a value on one dimension requires assignment of a value on another dimension. For example, the dimensions of hue, saturation, and value in the HSV colour space are integral. Dimensions are called separable if values on one dimension can be assigned independently from the others. For example, hue and height are separable. The way in which this interaction is specified in Gärdenfors’s model is by using different distance metrics. Within a set of integral dimensions, distance is Euclidean, and between sets of integral dimensions, the city block metric is used.\nConcept composition within conceptual spaces has been formalized in [26, 1, 22] for example. All these approaches focus on noun-noun composition, rather than utilising any more complex structure, and the way in which nouns compose often focuses on correlations between attributes in concepts. Since then, Gärdenfors has started to formalise verb spaces, adjectives, and other linguistic structures [13]. However, he has not provided a systematic method for how to utilise grammatical structures within conceptual spaces. In this sense, the category-theoretic approach to concept composition we describe below will introduce a more general approach to concept composition that can apply to varying grammatical types."
    }, {
      "heading" : "4 The Category of Convex Relations",
      "text" : "In NLP applications, meanings are typically interpreted in categories of real vector spaces. For our intended cognitive application, we now introduce a category that emphasizes convex structure. The\nfamiliar definition of convex set is a subset of a vector space which is closed under forming convex combinations. In this paper we consider a more general setting that includes convex subsets of vector spaces, but also allows us to consider some further, more discrete, examples.\nWe begin with some convenient notation. For a set X we write ∑\ni pi|xi〉 for a finite formal convex sum of elements of X, where pi ∈ R≥0 and ∑ i pi = 1. We then write D(X) for the set of all such sums. Here we abuse the physicists ket notation to highlight that our sums are formal, following a convention introduced in [16]. Equivalently, these sums can be thought of as finite probability distributions on the elements of X.\nA convex algebra is a set A with a function α : D(A)→ A satisfying the following conditions:\nα(|a〉) = a and α ∑ i,j piqi,j |ai,j〉  = α ∑ i pi|α( ∑ j qi,j |ai,j〉)〉  (7) Informally, α is a mixing operation that allows us to form convex combinations of elements, and the equations in (7) require the following good behaviour:\n• Forming a convex combination of a single element a returns a as we would expect\n• Iterating forming convex combinations interacts with flattening sums of sums in the way we would expect\nWe consider some examples of convex algebras.\nExample 9. The closed real interval [0, 1] has an obvious convex algebra structure. Similarly, every real or complex vector space has a natural convex algebra structure using the underlying linear structure.\nExample 10 (Simplices). For any set X, the formal convex sums of elements of X themselves form the free convex algebra on X, which can also be seen as a simplex with vertices the elements of X. Mixtures are formed as follows: ∑\ni pi| ∑ j qi,j |xi,j〉〉 7→ ∑ i,j piqi,j |xi,j〉\nExample 11. The convex space of density matrices provides another example, with the convex structure given by the usual vector space structure on linear operators.\nExample 12. For a set X, the functions of type X → [0, 1] form a convex algebra pointwise, with mixing operation: ∑\ni pi|fi〉 7→ (λx. ∑ i pifi(x))\nWe can see this as a convex algebra of fuzzy sets.\nExample 13 (Semilattices). As a slightly less straightforward example, every affine join semilattice (that is, one that has all finite non-empty joins) has a convex algebra structure given by:∑\ni pi|ai〉 = ∨ i {ai | pi > 0}\nNotice that here the scalars pi are discarded and play no active role. These “discrete” types of convex algebras allow us to consider objects such as the Boolean truth values.\nExample 14 (Trees). Given a finite tree, perhaps describing some hierarchical structure, we can construct an affine semilattice in a natural way. For example, consider a limited universe of foods, consisting of bananas, apples, and beer. Given two members of the hierarchy, their join will be the lowest level of the hierarchy which is above them both. For instance, the join of bananas and apples would be fruit.\nfood\nfruit\napples bananas\nbeer\nWhen α can be understood from the context, we abbreviate our notation for convex combinations by writing: ∑\ni piai := α( ∑ i pi|ai〉)\nUsing this convention, we define a convex relation of type (A,α) → (B, β) as a binary relation R : A→ B between the underlying sets that commutes with forming convex mixtures as follows:\n(∀i.R(ai, bi))⇒ R (∑ i piai, ∑ i pibi ) We note that identity relations are convex, and convex relations are closed under relational composition and converse.\nExample 15 (Homomorphisms). If (A,α) and (B, β) are convex algebras, functions f : A → B satisfying:\nf( ∑ i pixi) = ∑ i pif(xi)\nare convex relations. These functions are the homomorphisms of convex algebras. The identity function and constant functions are examples of homomorphisms of convex algebras.\nThe singleton set {∗} has a unique convex algebra structure, denoted I. Convex relations of the form I → (A,α) correspond to convex subsets, that is, subsets of A closed under forming convex combinations.\nDefinition 4. We define the category ConvexRel as having convex algebras as objects and convex relations as morphisms, with composition and identities as for ordinary binary relations.\nGiven a pair of convex algebras (A,α) and (B, β) we can form a new convex algebra on the cartesian product A×B, denoted (A,α)⊗ (B, β), with mixing operation:∑\ni\npi|(ai, bi)〉 7→ (∑ i piai, ∑ i pibi ) This induces a symmetric monoidal structure on ConvexRel. In fact, the category ConvexRel has the necessary categorical structure for categorical compositional semantics:\nTheorem 1. The category ConvexRel is a compact closed category. The symmetric monoidal structure is given by the unit and monoidal product outlined above. The caps for an object (A,α) are given by:\n: I → (A,α)⊗ (A,α) :: {(∗, (a, a)) | a ∈ A}\nthe cups by:\n: (A,α)⊗ (A,α)→ I :: {((a, a), ∗) | a ∈ A} and more generally, the multi-wires by:\n. . .\n. . .\n: A⊗ . . .⊗A→ A⊗ . . .⊗A :: {((a, . . . , a), (a, . . . , a)) | a ∈ A}\nRemark 2. As observed in remark 1, as ConvexRel is compact closed, its tensor cannot be a categorical product. For example, there are convex subsets of [0, 1]× [0, 1] such as the diagonal:\n{(x, x) | x ∈ [0, 1]}\nthat cannot be written as the cartesian product of two convex subsets of [0, 1]. This behaviour exhibits non-trivial correlations between the different components of the composite convex algebra.\nRemark 3. We have given an elementary description of ConvexRel. More abstractly, it can be seen as the category of relations in the Eilenberg-Moore category of the finite distribution monad. Its compact closed structure then follows from general principles [5]."
    }, {
      "heading" : "5 Adjective and Verb Concepts",
      "text" : "We define a conceptual space to be an object of ConvexRel. In order to match the structure of the pregroup grammar, we require two distinct objects: a noun space N and a sentence space S.\nThe noun space N is given by a composite\nNcolour ⊗Ntaste ⊗ ...\ndescribing different attributes such as colour and taste. A noun is then a convex subset of such a space. In our examples, we take our sentence space to be a convex algebra in which the individual points are events. Our general scheme can incorporate other sentence space structures, such choices are generally specific to the application under consideration. A sentence is then a convex subset of S.\nWe now describe some example noun and sentence spaces. We then show how these can be combined to form spaces describing adjectives and verbs. Once we have these types available, we show in section 6 how concepts interact within sentences."
    }, {
      "heading" : "5.1 Example: Food and Drink",
      "text" : "We consider a conceptual space for food and drink as our running example. The space N is composed of the domains Ncolour, Ntaste, Ntexture, so that\nN = Ncolour ⊗Ntaste ⊗Ntexture\nThe domain Ncolour is the RGB colour domain, i.e. triples (R,G,B) ∈ [0, 1]3 with R, G, B standing for intensity of red, green, and blue light respectively. Ntaste is defined as the simplex of combinations of four tastes: sweet, sour, bitter, and salt. We therefore have\nNtaste = {~t|~t = ∑ i∈I wi~ti} (8)\nwhere I = {sweet, sour, bitter, salt}, ~ti is the vector in some chosen basis of R4 whose elements are all zero except for the ith element whose value is one, and ∑ i wi = 1. Ntexture is just the set [0, 1] ranging from completely liquid (0) to completely solid (1). We define a property pproperty to be a convex subset of a domain, and specify the following examples (see figures 1 and 2):\npyellow = {(R,G,B)|(R ≥ 0.7), (G ≥ 0.7), (B ≤ 0.5)} pgreen = {(R,G,B)|(R ≤ G), (B ≤ G), (R ≤ 0.7), (B ≤ 0.7), (G ≥ 0.3)} psweet = {~t|tsweet ≥ tl for l 6= sweet}\nThe properties psour and pbitter are defined analogously."
    }, {
      "heading" : "5.1.1 Nouns",
      "text" : "We define some nouns below. Properties in the colour domain are specified using sets of linear inequalities, and colours in the taste domain are specified using the convex hull of sets of points. We use Conv(A) to refer to the convex hull of a set A.\nbanana = {(R,G,B)|(0.9R ≤ G ≤ 1.5R), (R ≥ 0.3), (B ≤ 0.1)} × Conv({tsweet, 0.25tsweet + 0.75tbitter, 0.7tsweet + 0.3tsour})× [0.2, 0.5]\napple = {(R,G,B)|R− 0.7 ≤ G ≤ R+ 0.7), (G ≥ 1−R), (B ≤ 0.1)} × [0.5, 1]× Conv({tsweet, 0.75tsweet + 0.25tbitter, 0.3tsweet + 0.7tsour})× [0.5, 0.8]\nbeer = {(R,G,B)|(0.5R ≤ G ≤ R), (G ≤ 1.5− 0.8R), (B ≤ 0.1)} × Conv({tbitter, 0.7tsweet + 0.3tbitter, 0.6tsour + 0.4tbitter})× [0, 0.01]\nwhere ti are as given in (8). The subsets of points representing tastes are explained as follows using the case of banana as an example. Bananas are not at all salty, and therefore wsalt is set to 0. Bananas are sweet, and therefore the point tsweet is chosen as an extremal point in the set of banana tastes. Bananas can also be somewhat but not totally bitter, and therefore the point 0.25tsweet + 0.75tbitter is chosen as an extremal point. Similarly bananas can be a little sour, and therefore 0.7tsweet + 0.3tsour is also chosen as an extremal point. Finally the convex hull of these points is formed giving a set of points corresponding to banana taste.\nPictorially, we have:\nbanana = × × 0 10.2 0.5\napple = × × 0 10.5 0.8\nbeer = × × 0 10.01\nWhat is an appropriate choice of sentence space for describing food and drink? We need to describe the events associated with eating and drinking. We choose very simple structure where the events are either positive or negative, and surprising or unsurprising. We therefore use a sentence space of 2-tuples. The first element of the tuple states whether the sentence is positive (1) or negative (0) and the second states whether it is surprising (1) or unsurprising (0). The convex structure on this space is the convex algebra on a join semilattice induced by element-wise max, as in example 13. We therefore have four points in the space: positive, surprising (1, 1); positive, unsurprising (1, 0); negative, surprising (0, 1); and negative, unsurprising (0, 0). Sentence meanings are convex subsets of this space, so they could be singletons, or larger subsets such as negative = {(0, 1), (0, 0)}."
    }, {
      "heading" : "5.1.2 Adjectives",
      "text" : "Recall that in a pregroup setting the adjective type is nnl. In ConvexRel, the adjective therefore has type N ⊗ N . Adjectives are convex relations on the noun space, so can be written as sets of ordered pairs. We give two examples, yellowadj and softadj. The adjective yellowadj has the simple form:\n{(−→x ,−→x )|xcolour ∈ pyellow}\nbecause it depends only on one area of the conceptual space. An adjective such as ‘soft’ behaves differently to this. We cannot simply define soft as one area of the conceptual space, because whether or not something is soft depends what it was originally. Using relations, we can start to write down the right type of structure for the adjective, as long as the objects are sufficiently distinct. Restricting our universe just to bananas and apples, we can write softadj as\n{(−→x ,−→x )|−→x ∈ banana and xtexture ≤ 0.35 or −→x ∈ apples and xtexture ≤ 0.6}\nAn analysis of the difficulties in dealing with adjectives set-theoretically, breaking them down into (roughly) three categories, is given in [17]. Under this view, both adjectives and nouns are viewed as one-place predicates, so that, for example red = {x|x is red} and dog = {x|x is a dog}. There are then three classes of adjective. For intersective adjectives, the meaning of adj noun is given by adj ∩ noun. For subsective adjectives, the meaning of adj noun is a subset of noun. For privative adjectives, however, adj noun 6⊆ noun.\nIntersective adjectives are a simple modifier that can be thought of as the intersection between two concepts. We can make explicit the internal structure of these adjectives exploiting the multi-wires of theorem 1. For example, in the case of yellow banana, we take the intersection of yellow and banana. We then show how to understand yellow as an adjective. While the general case of adjectives is depicted as:\nsoft\nN\nbanana\nN\n= soft\nN\nbanana\nN\nin the case of intersective adjectives the diagrams specialise to:\nyellow\nN\nbanana\n=\nbanana\nyellow\nN\n=\nN\nbanana\nyellow\nN\nThis shows us how the internal structure of an intersective adjective is derived directly from a noun."
    }, {
      "heading" : "5.1.3 Verbs",
      "text" : "The pregroup type for a transitive verb is nrsnl, mapping to N ⊗ S ⊗N in ConvexRel. To define the verb, we use concept names as shorthand, where these can easily be calculated. For example, since green is an intersective adjective, green banana is calculated by taking the intersection of green and banana by\ncombining the inequalities specifying the colour property, giving:\ngreen banana = {(R,G,B)|(R ≤ G ≤ 1.5R), (G ≥ B), (0.3 ≤ R ≤ 0.7), (G ≥ 0.3), (B ≤ 0.1)} × Conv({tsweet, 0.25tsweet + 0.75tbitter, 0.7tsweet + 0.3tsour})× [0.2, 0.5]\nAlthough a full specification of a verb would take in all the nouns it could possibly apply to, for expository purposes we restrict our nouns to just bananas and beer which do not overlap, due to the fact that they have different textures. We define the verb taste : I → N ⊗ S ⊗N as follows:\ntaste = (green banana× {(0, 0)} × bitter) ∪ (green banana× {(1, 1)} × sweet) ∪ (yellow banana× {(1, 0)} × sweet) ∪ (beer× {(0, 1)} × sweet) ∪ (beer× {(1, 0)} × bitter)"
    }, {
      "heading" : "5.2 Example: Robot Navigation",
      "text" : "We now present another example describing a simple formulation of robot navigation . We will describe our choices of noun space N and sentence space S, and show how to form nouns and verbs."
    }, {
      "heading" : "5.2.1 Nouns",
      "text" : "The types of nouns we wish to describe are objects, such as armchair and ball, the robots Cathy and David, and places such as kitchen and living room. For shorthand, we call these nouns a, b, c, d, k, and l. These are specified in the noun space N which is itself composed of a number of domains\nNlocation ⊗Ndirection ⊗Nshape ⊗Nsize ⊗Ncolour ⊗ ...\nWe firstly consider the kitchen and living rooms as being defined by convex subsets of points in the domain Nlocation, defining properties in the location domain as:\npkitchen location = {(x1, x2)|x1 ∈ [0, 5], x2 ∈ [0, 10]} pliving room location = {(x1, x2)|x1 ∈ [5, 10], x2 ∈ [0, 10]}\nwhich can be depicted as follows:\nkitchen living room\n0 5 10 0\n10\nx1\nx2\nThen the nouns kitchen and living room are given by these properties together with other sets of characteristics in the shape domain, size domain, and so on, which we won’t specify here.\nkitchen = pkitchen location ⊗ pkitchen shape ⊗ pkitchen size ⊗ ... living room = pliving room location ⊗ pliving room shape ⊗ pliving room size ⊗ ...\nSimilarly, the other nouns are defined by combinations of properties in the noun space. For this example, we do not worry too much about what they are, but assume that they allow us to distinguish between the objects."
    }, {
      "heading" : "5.2.2 Verbs",
      "text" : "In order to define some verbs, we need consider what a suitable sentence space should look like. We want to give sentences of the form:\nThe ball is in the living room Cathy moves to the kitchen\nIn these sentences, an object or agent is related to a path through time and space. Note that in the case of the verb ‘is in’, this path is in fact trivially just a point, however for ‘moves to’, the path actually is a path through time and space, and we will need to use subsets of the time and location domains to specify one single event. We therefore define the sentence space to be comprised of the noun space N , a time dimension T , and the location domain Nlocation:\nS = N ⊗ T ⊗Nlocation\nN ⊗ T x2\nx1\nThe agent is represented by a point in the noun space N , and the path they take as described in the sentence is represented as a subset of the time and location domains.\nIn what follows, we think of 0 on the time dimension T as referring to ‘now’, with negative values of T referring to the past and positive values referring to the future.\nAs in the food example, transitive verbs are of the form N⊗S⊗N . This means that in this example, they are of the form N ⊗N ⊗ T ⊗Nlocation ⊗N, and can be thought of as sets of ordered tuples of the form\n(n1, n2, t, l, n3),\nwhere ni stands for points in the noun space, t is a time, l is a location. We will consider the following verbs: is in, moves to. It could be argued that these are not transitive verbs, but intransitive verbs plus preposition. However, we can parse the combination as a transitive verb, since a preposition has type srsnl and therefore the combination reduces to type of a transitive verb\n(nrs)(srsnl) ≤ nrsnl,\nThe verb is in can take any of the nouns a, b, c, or d as subject, and any of k, l as object. This verb refers to just one timepoint, i.e. now, or 0. The verb is as follows:\nis in = {(~n, ~n, tnow,mlocation, ~m)|~n ∈ a ∪ b ∪ c ∪ d, tnow = 0, ~m ∈ k ∪ l} (9)\nThe verb moves to refers to more than one point in time. We need to talk about an object moving from being at one location at a past time, to another location at time 0, or now. This movement should be continuous, since the objects we are talking about do not teleport from one point to another. We will also restrict the subject of the sentence to being one of the nouns a, b, c, or d, as we don’t want to talk about the kitchen and living rooms moving at this point. The object of the verb, however, can be any of the nouns, so we can say, for example, that ‘Cathy moves to the armchair’, or ‘The ball moves to Dave’ (presumably because Cathy kicked it). The most specific event that can be described in the space will track the exact path that an object takes through space and time. The meaning of a less specific sentence will be a convex subset of these trajectories. We now define the verb as follows:\nmoves to = {(~n, ~n, [t, 0], f([t, 0]), ~m) | ~n ∈ a ∪ b ∪ c ∪ d, t < 0, f continuous, f(t) ∈ ~nlocation, f(0) ∈ ~mlocation} (10)\nThe constraints on t ensure that the movement happened in the past, and the constraints on f ensure that the movement is from the location of the subject to the location of the object of the verb. These definitions are a little complex, but we will see how they work in interaction. Note that the location nouns kitchen and living room might seem to be of a different type to the object and agent nouns armchair, ball, Cathy and David. For example, we have specified that kitchen and living room do not move around. In future research we will be extending to a richer type system which can take account of these sorts of differences, and which will in fact be closer to that proposed by Gärdenfors in [13]."
    }, {
      "heading" : "6 Concepts in Interaction",
      "text" : "We have given descriptions of how to form the different word types within our model of categorical conceptual spaces. In this section we show how to apply the type reductions of the pregroup grammar within the conceptual spaces formalism."
    }, {
      "heading" : "6.1 Sentences in the Food Space",
      "text" : "The application of yellowadj to banana works as follows.\nyellow banana = (1N × N )(yellowadj × banana) = (1N × N ){(−→x ,−→x )|xcolour ∈ yellow} × ({(R,G,B)|(0.9R ≤ G ≤ 1.5R), (R ≥ 0.3), (B ≤ 0.1)} × Conv({tsweet, 0.25tsweet + 0.75tbitter, 0.7tsweet + 0.3tsour})× [0.2, 0.5])\n= {(R,G,B)|(0.9R ≤ G ≤ 1.5R), (R ≥ 0.7), (G ≥ 0.7), (B ≤ 0.1)} × Conv({tsweet, 0.25tsweet + 0.75tbitter, 0.7tsweet + 0.3tsour})× [0.2, 0.5]\nNotice, in the last line, how the colour property has altered. This alteration restricts to yellow hues. This assumes that we can tell bananas and apples apart by shape, colour and so on. Then the same calculation gives us\nsoft apple = {(R,G,B)|R− 0.7 ≤ G ≤ R+ 0.7), (G ≥ 1−R), (B ≤ 0.1)} × Conv({tsweet, 0.75tsweet + 0.25tbitter, 0.3tsweet + 0.7tsour})× [0.4, 0.6]\nUsing the definition of taste that we gave, we find that although sweet bananas are good:\nbananas taste sweet = ( N × 1S × N )(bananas× taste× sweet) = ( N × 1S)(banana× (green banana× {(1, 1)} ∪ yellow banana× {(1, 0)}) = {(1, 1), (1, 0)} = positive\nsweet beer is not so desirable:\nbeer tastes sweet = ( N × 1S × N )(beer× taste× sweet) = {(0, 1)} = negative and surprising\nRelative Pronouns The compositional semantics we use can also deal with relative pronouns, described in detail in [19]. Relative pronouns are words such as ‘which’. For example, we can form the noun phrase Fruit which tastes bitter. This has the following structure:\nfruit tastes bitter\nwhich\nN NN S\nwhich simplifies to:\nfruit tastes bitter\nIn our example, we find that Fruit which tastes bitter = green banana:\nFruit which tastes bitter = (µN × ιS × N )(Conv(bananas ∪ apples)× taste× bitter) = (µN × ιS)(Conv(bananas ∪ apples)× (green banana× {(0, 0)})) = µN (Conv(bananas ∪ apples)× (green banana)) = green banana\nwhere µN is the converse of the Frobenius copy map on N and ιS is the delete map on S from theorem 1."
    }, {
      "heading" : "6.2 Sentences about Robot Movement",
      "text" : "In this section we describe how to compute the meaning of sentences about robot movement. Our first example is the sentence ‘Cathy moves to the living room’. In order to compute the meaning of this sentence, we assume that Cathy has a location.\nCathy moves to the living room\n= ( N ⊗ 1s ⊗ N )(C ⊗moves to⊗ L) = ( N ⊗ 1s⊗)(C ⊗ {(~n, ~n, [t, 0], f([t, 0]))|f(0) ∈ Llocation} (11) = {C, [t, 0], f([t, 0])|f(t) ∈ Clocation, f(0) ∈ Llocation}\nIn line (11) further constraints apply to t and f as described in equation (10). This calculation gives us a set of continuous line segments starting from Cathy’s location at time t and ending in the living room at time 0.\nWe now need to check that this set of line segments is convex. We assume that Cathy can take any possible location, and her other attributes remain static. This means that the set of possible Cathys is convex. The set of time segments [t, 0] such that t < 0 forms a convex set. Consider two such time segments. We define a convex mixture of these segments pointwise:\np[t1, 0] + (1− p)[t2, 0] = [pt1 + (1− p)t2, p0 + (1− p)0] = [pt1 + (1− p)t2, 0]\nwhich clearly satisfies the condition that the start point is in the past and the end point is now. We then consider the convex mixture of two sets of locations f1([t1, 0]) and f2([t2, 0]). In order to carry this out, we first of all transform the intervals [t1, 0] and [t2, 0] to [−1, 0] by dividing through by −ti, renaming the rescaled functions f ′i . We then form a convex combination:\npf ′1 + (1− p)f ′2 : [0, 1]→ Nlocation\npointwise by taking:\n(pf ′1 + (1− p)f ′2)(τ) = pf ′1(τ) + (1− p)f ′2(τ) = pf1((−t1)τ) + (1− p)f2((−t2)τ)\nSince both f1 and f2 are continuous in T , the result will be continuous in T . The constraints on these sets are that f1(t1) and f2(t2) are in Clocation and that f1(0) and f2(0) are in Llocation, and we need that their convex combinations are also in these respective locations. We know that Clocation and Llocation are convex, meaning that\npf1(t1) + (1− p)f2(t2) ∈ Clocation and pf1(0) + (1− p)f2(0) ∈ Llocation\nas required. In this section, we have shown how we can use the interaction of words, represented by convex sets in conceptual spaces, to map sentence meanings down to a convex set in a conceptual space for sentences."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have applied the categorical compositional scheme to cognition and conceptual spaces. In order to do this we introduced a new model for categorical compositional semantics, the category ConvexRel of convex algebras and binary relations respecting convex structure. We consider this model as a proof of concept that we can describe convex structures within our framework. Conceptual spaces are often considered to have further mathematical structure such as distance measures and notions of convergence or fixed points. It is also possible to vary the notion of convexity under consideration, for example by considering a binary betweenness relation on a space as primitive, rather than a mixing operation. Identifying a good setting for rich conceptual spaces models, and incorporating those structures into a compositional framework is a direction for further work."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was partially funded by AFSOR grant “Algorithmic and Logical Aspects when Composing Meanings”, the FQXi grant “Categorical Compositional Physics”, and EPSRC PhD scholarships."
    } ],
    "references" : [ {
      "title" : "A metric conceptual space algebra. In: Spatial information",
      "author" : [ "B. Adams", "M. Raubal" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Graded Entailment for Compositional Distributional Semantics. arXiv:1601.04908",
      "author" : [ "D. Bankova", "B. Coecke", "M. Lewis", "D. Marsden" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Interacting Conceptual Spaces",
      "author" : [ "J. Bolt", "B. Coecke", "F. Genovese", "M. Lewis", "R.D. Marsden" ],
      "venue" : "Piedeleu",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Extracting semantic representations from word co-occurrence statistics: A computational study",
      "author" : [ "J.P.J.A. Bullinaria" ],
      "venue" : "Behavior research methods",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Cartesian bicategories I",
      "author" : [ "A. Carboni", "R.F.C. Walters" ],
      "venue" : "Journal of pure and applied algebra",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1987
    }, {
      "title" : "An alternative Gospel of structure: order, composition, processes",
      "author" : [ "B. Coecke" ],
      "venue" : "editors: Quantum Physics and Linguistics. A Compositional, Diagrammatic Discourse,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Lambek vs. Lambek: Functorial vector space semantics and string diagrams for Lambek calculus",
      "author" : [ "B. Coecke", "E. Grefenstette", "M. Sadrzadeh" ],
      "venue" : "Annals of Pure and Applied Logic",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Picturing Quantum Processes. A First Course in Quantum Theory and Diagrammatic Reasoning",
      "author" : [ "B. Coecke", "A. Kissinger" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Categories for the practising physicist",
      "author" : [ "B. Coecke", "E.O. Paquette" ],
      "venue" : "New Structures for Physics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "A new description of orthogonal bases",
      "author" : [ "B. Coecke", "J.D. Pavlović" ],
      "venue" : "Vicary",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Mathematical Foundations for a Compositional Distributional Model of Meaning",
      "author" : [ "B. Coecke", "M. Sadrzadeh", "S. Clark" ],
      "venue" : "Linguistic Analysis",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Conceptual spaces: The geometry of thought",
      "author" : [ "P. Gärdenfors" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "The geometry of meaning: Semantics based on conceptual spaces",
      "author" : [ "P. Gärdenfors" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Experimental Support for a Categorical Compositional Distributional Model of Meaning",
      "author" : [ "E. Grefenstette", "M. Sadrzadeh" ],
      "venue" : "The 2014 Conference on Empirical Methods on Natural Language Processing.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Inheritance of attributes in natural concept conjunctions",
      "author" : [ "J. Hampton" ],
      "venue" : "Memory & Cognition",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1987
    }, {
      "title" : "Coalgebraic walks, in quantum and Turing computation",
      "author" : [ "B. Jacobs" ],
      "venue" : "Foundations of Software Science and Computational Structures,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Prototype theory and compositionality",
      "author" : [ "B.H. Kamp" ],
      "venue" : "Partee",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1995
    }, {
      "title" : "Prior disambiguation of word tensors for constructing Sentence vectors",
      "author" : [ "D. Kartsaklis", "M. Sadrzadeh" ],
      "venue" : "The 2013 Conference on Empirical Methods on Natural Language Processing.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Reasoning about meaning in natural language with compact closed categories and Frobenius algebras",
      "author" : [ "D. Kartsaklis", "M. Sadrzadeh", "S. Pulman", "B. Coecke" ],
      "venue" : "editors: Logic and Algebraic Structures in Quantum Computing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Hierarchical conceptual spaces for concept combination",
      "author" : [ "M. Lewis", "J. Lawry" ],
      "venue" : "Artificial Intelligence 237,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Producing high-dimensional semantic spaces from lexical cooccurrence",
      "author" : [ "K. Lund", "C. Burgess" ],
      "venue" : "Behavior Research Methods, Instruments, & Computers",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1996
    }, {
      "title" : "Open System Categorical Quantum Semantics in Natural Language Processing",
      "author" : [ "R. Piedeleu", "D. Kartsaklis", "B. Coecke", "M Sadrzadeh" ],
      "venue" : "editors: 6th Conference on Algebra and Coalgebra in Computer Science, CALCO 2015,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Reformulation of the theory of conceptual spaces",
      "author" : [ "J.T. Rickard", "J. Aisbett", "G. Gibbon" ],
      "venue" : "Information Sciences",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "Family resemblances: Studies in the internal structure of categories",
      "author" : [ "Eleanor Rosch", "Carolyn B Mervis" ],
      "venue" : "Cognitive psychology",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1975
    }, {
      "title" : "High Level Quantum Structures in Linguistics and Multi Agent Systems",
      "author" : [ "M. Sadrzadeh" ],
      "venue" : "AAI Spring Symposium: Quantum Interaction,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    }, {
      "title" : "The Frobenius anatomy of word meanings I: subject and object relative pronouns",
      "author" : [ "M. Sadrzadeh", "S. Clark", "B. Coecke" ],
      "venue" : "Journal of Logic and Computation",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "The Frobenius anatomy of word meanings II: possessive relative pronouns",
      "author" : [ "M. Sadrzadeh", "S. Clark", "B. Coecke" ],
      "venue" : "Journal of Logic and Computation,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Features of similarity",
      "author" : [ "A. Tversky" ],
      "venue" : "Psychological Review 84(4),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1977
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Conceptual spaces theory gives a way of describing structured concepts [12, 13], not starting from linguistic assumptions, but from cognitive considerations about human reasoning.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "Conceptual spaces theory gives a way of describing structured concepts [12, 13], not starting from linguistic assumptions, but from cognitive considerations about human reasoning.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "Categorical compositional distributional models [11] successfully exploit the compositional structure of natural language in a principled manner, and have outperformed other approaches in Natural Language Processing (NLP) [14, 18].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "Categorical compositional distributional models [11] successfully exploit the compositional structure of natural language in a principled manner, and have outperformed other approaches in Natural Language Processing (NLP) [14, 18].",
      "startOffset" : 222,
      "endOffset" : 230
    }, {
      "referenceID" : 17,
      "context" : "Categorical compositional distributional models [11] successfully exploit the compositional structure of natural language in a principled manner, and have outperformed other approaches in Natural Language Processing (NLP) [14, 18].",
      "startOffset" : 222,
      "endOffset" : 230
    }, {
      "referenceID" : 6,
      "context" : "A mathematical formalization of grammar is chosen, for example Lambek’s pregroup grammars [21], although the approach is equally effective with other categorial grammars [7].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 20,
      "context" : "The meanings of individual words are established using a distributional model of language, where they are described as vectors of co-occurrence statistics derived automatically from corpus data [23].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 2,
      "context" : "The categorical compositional distributional programme ∗This paper is a significantly extended version of the workshop paper [3] †authors in alphabetical order",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "The key insight that allows this approach to succeed is that both pregroups and the category of vector spaces carry the same abstract structure [11], and the same holds for other categorial grammars since they typically have a weaker categorical structure.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 21,
      "context" : "(a) Choose or craft appropriate meaning or concept spaces, such as spaces of propositions, vector spaces, density matrices [25, 2] or conceptual spaces.",
      "startOffset" : 123,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "(a) Choose or craft appropriate meaning or concept spaces, such as spaces of propositions, vector spaces, density matrices [25, 2] or conceptual spaces.",
      "startOffset" : 123,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "A more gentle introduction can be found in [9].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 7,
      "context" : "In fact, if the tensor of a compact closed category happens to be a categorical product, then that category must be trivial, in a precise mathematical sense [8].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : "3 Grammar categories Many algebraic gadgets exist to model grammar, as detailed in [6] for example.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "In the present work, we use Lambek’s pregroup grammars, as many grammars have a pregroup structure [28].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "One way to model meanings in a vector space is to use co-occurrence statistics [4].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "For example, [27, 31, 15] all represent concepts as feature vectors, with basis dimensions representing attributes of the concept.",
      "startOffset" : 13,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "For example, [27, 31, 15] all represent concepts as feature vectors, with basis dimensions representing attributes of the concept.",
      "startOffset" : 13,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "For example, [27, 31, 15] all represent concepts as feature vectors, with basis dimensions representing attributes of the concept.",
      "startOffset" : 13,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "6 Beyond standard categorial grammar A first rather trivial example of a functional word is “does”, which can be accounted for by means of caps [11]:",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : "Things become more interesting when we introduce, rather than just wires, the idea of a multi-wire (also called spider [8]) which can have more than two ends, or less:",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "These multi-wires can be defined in category-theoretic terms for any symmetric monoidal category, where they are called commutative special dagger Frobenius structures [10, 8].",
      "startOffset" : 168,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "These multi-wires can be defined in category-theoretic terms for any symmetric monoidal category, where they are called commutative special dagger Frobenius structures [10, 8].",
      "startOffset" : 168,
      "endOffset" : 175
    }, {
      "referenceID" : 25,
      "context" : "Using these multi-wires we can now express the meaning of relative pronouns [29, 30]:",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 26,
      "context" : "Using these multi-wires we can now express the meaning of relative pronouns [29, 30]:",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "Conceptual spaces are proposed in [12] as a framework for representing information at the conceptual level.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "Concept composition within conceptual spaces has been formalized in [26, 1, 22] for example.",
      "startOffset" : 68,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "Concept composition within conceptual spaces has been formalized in [26, 1, 22] for example.",
      "startOffset" : 68,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "Concept composition within conceptual spaces has been formalized in [26, 1, 22] for example.",
      "startOffset" : 68,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "Since then, Gärdenfors has started to formalise verb spaces, adjectives, and other linguistic structures [13].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "Here we abuse the physicists ket notation to highlight that our sums are formal, following a convention introduced in [16].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "The closed real interval [0, 1] has an obvious convex algebra structure.",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "For a set X, the functions of type X → [0, 1] form a convex algebra pointwise, with mixing operation: ∑",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "For example, there are convex subsets of [0, 1]× [0, 1] such as the diagonal: {(x, x) | x ∈ [0, 1]} that cannot be written as the cartesian product of two convex subsets of [0, 1].",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "For example, there are convex subsets of [0, 1]× [0, 1] such as the diagonal: {(x, x) | x ∈ [0, 1]} that cannot be written as the cartesian product of two convex subsets of [0, 1].",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "For example, there are convex subsets of [0, 1]× [0, 1] such as the diagonal: {(x, x) | x ∈ [0, 1]} that cannot be written as the cartesian product of two convex subsets of [0, 1].",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "For example, there are convex subsets of [0, 1]× [0, 1] such as the diagonal: {(x, x) | x ∈ [0, 1]} that cannot be written as the cartesian product of two convex subsets of [0, 1].",
      "startOffset" : 173,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : "Its compact closed structure then follows from general principles [5].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "triples (R,G,B) ∈ [0, 1] with R, G, B standing for intensity of red, green, and blue light respectively.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Ntexture is just the set [0, 1] ranging from completely liquid (0) to completely solid (1).",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "An analysis of the difficulties in dealing with adjectives set-theoretically, breaking them down into (roughly) three categories, is given in [17].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "We firstly consider the kitchen and living rooms as being defined by convex subsets of points in the domain Nlocation, defining properties in the location domain as: pkitchen location = {(x1, x2)|x1 ∈ [0, 5], x2 ∈ [0, 10]} pliving room location = {(x1, x2)|x1 ∈ [5, 10], x2 ∈ [0, 10]} which can be depicted as follows:",
      "startOffset" : 201,
      "endOffset" : 207
    }, {
      "referenceID" : 9,
      "context" : "We firstly consider the kitchen and living rooms as being defined by convex subsets of points in the domain Nlocation, defining properties in the location domain as: pkitchen location = {(x1, x2)|x1 ∈ [0, 5], x2 ∈ [0, 10]} pliving room location = {(x1, x2)|x1 ∈ [5, 10], x2 ∈ [0, 10]} which can be depicted as follows:",
      "startOffset" : 214,
      "endOffset" : 221
    }, {
      "referenceID" : 4,
      "context" : "We firstly consider the kitchen and living rooms as being defined by convex subsets of points in the domain Nlocation, defining properties in the location domain as: pkitchen location = {(x1, x2)|x1 ∈ [0, 5], x2 ∈ [0, 10]} pliving room location = {(x1, x2)|x1 ∈ [5, 10], x2 ∈ [0, 10]} which can be depicted as follows:",
      "startOffset" : 262,
      "endOffset" : 269
    }, {
      "referenceID" : 9,
      "context" : "We firstly consider the kitchen and living rooms as being defined by convex subsets of points in the domain Nlocation, defining properties in the location domain as: pkitchen location = {(x1, x2)|x1 ∈ [0, 5], x2 ∈ [0, 10]} pliving room location = {(x1, x2)|x1 ∈ [5, 10], x2 ∈ [0, 10]} which can be depicted as follows:",
      "startOffset" : 262,
      "endOffset" : 269
    }, {
      "referenceID" : 9,
      "context" : "We firstly consider the kitchen and living rooms as being defined by convex subsets of points in the domain Nlocation, defining properties in the location domain as: pkitchen location = {(x1, x2)|x1 ∈ [0, 5], x2 ∈ [0, 10]} pliving room location = {(x1, x2)|x1 ∈ [5, 10], x2 ∈ [0, 10]} which can be depicted as follows:",
      "startOffset" : 276,
      "endOffset" : 283
    }, {
      "referenceID" : 12,
      "context" : "In future research we will be extending to a richer type system which can take account of these sorts of differences, and which will in fact be closer to that proposed by Gärdenfors in [13].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 18,
      "context" : "Relative Pronouns The compositional semantics we use can also deal with relative pronouns, described in detail in [19].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "We then form a convex combination: pf ′ 1 + (1− p)f ′ 2 : [0, 1]→ Nlocation pointwise by taking: (pf ′ 1 + (1− p)f ′ 2)(τ) = pf ′ 1(τ) + (1− p)f ′ 2(τ) = pf1((−t1)τ) + (1− p)f2((−t2)τ) Since both f1 and f2 are continuous in T , the result will be continuous in T .",
      "startOffset" : 58,
      "endOffset" : 64
    } ],
    "year" : 2017,
    "abstractText" : "The categorical compositional approach to meaning has been successfully applied in natural language processing, outperforming other models in mainstream empirical language processing tasks. We show how this approach can be generalized to conceptual space models of cognition. In order to do this, first we introduce the category of convex relations as a new setting for categorical compositional semantics, emphasizing the convex structure important to conceptual space applications. We then show how to construct conceptual spaces for various types such as nouns, adjectives and verbs. Finally we show by means of examples how concepts can be systematically combined to establish the meanings of composite phrases from the meanings of their constituent parts. This provides the mathematical underpinnings of a new compositional approach to cognition.",
    "creator" : "LaTeX with hyperref package"
  }
}