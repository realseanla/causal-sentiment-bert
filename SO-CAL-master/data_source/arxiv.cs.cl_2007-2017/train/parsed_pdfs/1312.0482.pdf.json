{
  "name" : "1312.0482.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Semantic Representations for the Phrase Translation Model",
    "authors" : [ "Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The phrase translation model, also known as the phrase table, is one of the core components of phrase-based statistical machine translation (SMT) systems. The most common method of constructing the phrase table takes a two-phase approach [17]. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase, which is the focus of this paper, is parameter estimation where each phrase pair is assigned with some scores that are estimated based on counting these phrases using the same word-aligned training data. Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context information can be captured in selecting translations. However, longer phrases occur less often in training data, leading to a severe data sparseness problem in parameter estimation. There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model [e.g., 6, 8, 10, 25].\nThis paper revisits the problem of scoring a phrase translation pair by developing a novel, Semantic-based Phrase Translation Model (SPTM). The translation score of a phrase pair in this model is computed as follows. First, we represent each phrase as a bag-of-words vector, called word vector henceforth. We then project the word vector, in either the source language or the target language, into a respective continuous feature vector in a common low-dimensional latent semantic space that is intended to be language independent. The projection is performed by a multi-layer neural network. The projected feature vector forms the semantic representation of a phrase. Finally, the translation score of a source-target phrase pair is computed by the distance between their feature vectors.\nThe main motivation behind the SPTM is to alleviate the data sparseness problem associated with the traditional counting-based methods by grouping phrases with a similar meaning across different languages. In this model, semantically related phrases, in both the source and the target languages, would have similar (close) feature vectors in the semantic space. Since the translation\nscore is a smooth function of these feature vectors, a small change in semantics (e.g., the phrases that differ only in morphological forms) should only lead to a small change in the translation score.\nThe primary research task in developing the SPTM is learning the semantic representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (LM) [3, 19], we use a neural network to project a word vector to a feature vector. Ideally, the projection would discover those latent semantic features that are useful to differentiate good translations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences, as measured by BLEU, which contain the phrase pair. In order to overcome this challenge and let the BLEU metric guide the projection learning, we propose a new method to learn the parameters of a neural network. This new method automatically forces the feature vector of a source phrase to be closer to the feature vectors of its candidate translations that lead to a better BLEU score, if these translations are selected by an SMT decoder to produce final, sentence-level translations. The new learning method makes use of the L-BFGS algorithm and the expected BLEU as the objective function defined on N-best lists. To the best of our knowledge, the SPTM proposed in this paper is the first continuous-space phrase translation model that is shown to lead to significant improvement over a standard phrasebased SMT system (to be detailed in Section 6). Like the traditional phrase translation model, the translation score of each bilingual phrase pair is modeled explicitly in our model. However, instead of estimating the phrase translation score on aligned parallel data, our model intends to capture the semantic similarity between a source phrase and its paired target phrase by projecting them into a common, latent semantic space that is language independent."
    }, {
      "heading" : "2. Related Work",
      "text" : "Latent Semantic Analysis (LSA) [5], originally designed for information retrieval (IR), is arguably the earliest continuous semantic model. Unlike LSA which is a linear projection model, generative topic models, such as Probabilistic LSA [13] and Latent Dirichlet Allocation (LDA) [4] give a clear probabilistic interpretation of the semantic representation. In contrast, recent work on continuous space language models, e.g., the feed-forward neural network language model (NNLM) [3] and the recurrent neural network language model (RNNLM) [19, 2], provide a different kind of latent semantic representation. Because these latent semantic models are developed for mono-lingual settings, they are not directly applicable to translation. As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space. In principle, a phrase translation table can be derived using these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance.\nDespite the success of latent semantic representations in various applications, there is, however, much less work on continuous-space translation models. The only exception we are aware of is the work of continuous space n-gram translation models [23, 24], where the feed-forward NNLM is extended to represent translation probabilities. However, these earlier studies focused on the socalled n-gram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of word n-gram probabilities of the same form as that in a standard n-gram LM. Therefore, it is not clear how their approaches can be applied to the phrase translation model, which is much more widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language.\nThere has been much recent research on improving the phrase table [6, 8, 10, 18, 25]. Among them, [8] is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT and an N-best list based expected BLEU as the objective function. In this study we use the same objective function to learn the semantic representations of phrases, integrating the strengths associated with both of these earlier studies."
    }, {
      "heading" : "3. The Log-Linear Model for SMT",
      "text" : "Phrase-based SMT is based on a log-linear model which requires learning a mapping between inputs \uD835\uDC39 ∈ ℱ to outputs \uD835\uDC38 ∈ ℰ. We are given\n Training samples (\uD835\uDC39\uD835\uDC56 , \uD835\uDC38\uD835\uDC56) for \uD835\uDC56 = 1 … \uD835\uDC41, where each source sentence \uD835\uDC39\uD835\uDC56 is paired with a reference translation in target language \uD835\uDC38\uD835\uDC56;  A procedure GEN to generate a list of N-best candidates GEN(\uD835\uDC39\uD835\uDC56) for an input \uD835\uDC39\uD835\uDC56, where GEN in this study is the baseline phrase-based SMT system, i.e., a reimplementation of the Moses\nsystem [15] that does not use the SPTM, and each \uD835\uDC38 ∈ GEN(\uD835\uDC39\uD835\uDC56) is labeled by the sentencelevel BLEU score [10], denoted by sBleu(\uD835\uDC38\uD835\uDC56 , \uD835\uDC38), which meaures the quality of \uD835\uDC38 with respect to its reference translation \uD835\uDC38\uD835\uDC56;\n A vector of features \uD835\uDC21 ∈ ℝ\uD835\uDC40 that maps each (\uD835\uDC39\uD835\uDC56 , \uD835\uDC38) to a vector of feature values; and  A parameter vector \uD835\uDECC ∈ ℝ\uD835\uDC40, which assigns a real-valued weight to each feature.\nSMT involves hidden-variable models such that a hidden variable \uD835\uDC34 is assumed to be constructed during the process of generating \uD835\uDC38. In the phrase-based SMT, \uD835\uDC34 consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases.\nThe components GEN(. ), \uD835\uDC21 and \uD835\uDECC define a log-linear model that maps \uD835\uDC39\uD835\uDC56 to an output as follows:\n\uD835\uDC38∗ = argmax (\uD835\uDC38,\uD835\uDC34)∈GEN(\uD835\uDC39\uD835\uDC56) \uD835\uDECCT\uD835\uDC21(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38, \uD835\uDC34) (1)\nwhich states that given \uD835\uDECC and \uD835\uDC21, argmax returns the highest scoring translation \uD835\uDC38∗, maximizing over correspondences \uD835\uDC34. In phrase translation models, computing the argmax exactly is intractable, so it is performed approximatedly by beam search. We assume that every translation candidate is always coupled with a corresponding \uD835\uDC34, called Viterbi derivation, generated by (1)."
    }, {
      "heading" : "4. A Semantic-Based Phrase Translation Model (SPTM)",
      "text" : "The architecture of the SPTM is shown in Figure 1, where for each pair of source and target phrases (\uD835\uDC53\uD835\uDC56, \uD835\uDC52\uD835\uDC57) in a source-target sentence pair, we first project them into feature vectors \uD835\uDC32\uD835\uDC53\uD835\uDC56 and \uD835\uDC32\uD835\uDC52\uD835\uDC57 in a latent semantic space via a neural network, and then compute the translation score, score(\uD835\uDC53\uD835\uDC56 , \uD835\uDC52\uD835\uDC57), by the distance of their feature vectors in that space.\nWe start with a bag-of-words representation of a phrase \uD835\uDC31 ∈ ℝ\uD835\uDC51, where \uD835\uDC31 is a word vector and \uD835\uDC51 is the size of the vocabulary consisting of words in both source and target languages. We then learn to project \uD835\uDC31 to a low-dimensional semantic space ℝ\uD835\uDC58: \uD835\uDF19(\uD835\uDC31): ℝ\uD835\uDC51 → ℝ\uD835\uDC58. The projection is performed using a fully connected neural network with one hidden layer and tanh activation functions. Let \uD835\uDC161 be the projection matrix from the input layer to the hidden layer and \uD835\uDC162 the projection matrix from the hidden layer to the output layer, we have\n\uD835\uDC32 ≡ \uD835\uDF19(\uD835\uDC31) = tanh (\uD835\uDC162 T(tanh(\uD835\uDC161 T\uD835\uDC31))) (2)\nThe translation score of a source phrase f and a target phrase e can be measured as the similarity\n(or distance) between their feature vectors. We choose the dot product as the similarity function1:\nscore(\uD835\uDC53, \uD835\uDC52) ≡ sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) = \uD835\uDC32\uD835\uDC53 T\uD835\uDC32\uD835\uDC52 (3)\nAccording to (2), we see that the value of the scoring function is determined by the projection matrices \uD835\uDEC9 = {\uD835\uDC161, \uD835\uDC162}. The SPTM of (2) and (3) can be incoporated into the log-linear model for SMT (1) by introducing a new feature ℎ\uD835\uDC40+1 and a new feature weight \uD835\uDF06\uD835\uDC40+1. The new feature is defined as\nℎ\uD835\uDC40+1(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38, \uD835\uDC34) = ∑ sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52)(\uD835\uDC53,\uD835\uDC52 )∈\uD835\uDC34 . (4)\n1 In our experiments, we compared dot product and the cosine similarity function and found that the former\nworks better for nonlinear multi-layer neural networks, and the latter works better for linear neural networks. For the sake of clarity, we choose dot product when we describe the SPTM and its training in Sections 4 and 5, respectively.\nThus, the phrase-based SMT system, into which the SPTM is incorporated, is parameterized by (\uD835\uDECC, \uD835\uDEC9), where \uD835\uDECC is a vector of a handful of parameters used in the log-linear model of (1), with one weight for each feature; and \uD835\uDEC9 is the projection matrices used in the SPTM defined by (2) and (3). In our experiments we take three steps to learn (\uD835\uDECC, \uD835\uDEC9):\n1. Given a baseline phrase-based SMT system and a pre-set \uD835\uDECC where \uD835\uDF06\uD835\uDC40+1 = 0, we generate for each source sentence in training data an N-best list of translation hypotheses. 2. We fix \uD835\uDECC and set \uD835\uDF06\uD835\uDC40+1 = 1, and optimize \uD835\uDEC9 w.r.t. a loss function on training data. 3. We fix \uD835\uDEC9, and optimize \uD835\uDECC using MERT [20] to maximize the BLEU score on development\ndata.\nIn Section 5, we will describe Step 2 in detail because it is directly related to the SPTM training."
    }, {
      "heading" : "5. Training SPTM",
      "text" : "This section describes the kind of loss function we employ with the SPTM and the algorithm to train the neural network weights using the loss function as the optimization objective.\nWe define the loss function ℒ(\uD835\uDEC9) as the negative of the N-best list based expected BLEU, denoted by xBleu(\uD835\uDEC9). In the reranking framework of SMT outlined in Section 3, xBleu(\uD835\uDEC9) over one training sample (\uD835\uDC39\uD835\uDC56 , \uD835\uDC38\uD835\uDC56) is defined as\nxBleu(\uD835\uDEC9) = ∑ \uD835\uDC43(\uD835\uDC38|\uD835\uDC39\uD835\uDC56)sBleu(\uD835\uDC38\uD835\uDC56 , \uD835\uDC38)\n\uD835\uDC38∈GEN(\uD835\uDC39\uD835\uDC56)\n(5)\nwhere sBleu(\uD835\uDC38\uD835\uDC56 , \uD835\uDC38) is the sentence-level BLEU score, and \uD835\uDC43(\uD835\uDC38|\uD835\uDC39\uD835\uDC56) is a normalized translation probability from \uD835\uDC39\uD835\uDC56 to \uD835\uDC38 computed using softmax as\n\uD835\uDC43(\uD835\uDC38|\uD835\uDC39\uD835\uDC56) = exp(\uD835\uDECCT\uD835\uDC21(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38, \uD835\uDC34))\n∑ exp(\uD835\uDECCT\uD835\uDC21(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38, \uD835\uDC34))\uD835\uDC38∈GEN(\uD835\uDC39\uD835\uDC56) (6)\nwhere \uD835\uDECCT\uD835\uDC21 is the log-linear model of (1), which also includes the feature derived from the SPTM as defined by (4).\nLet ℒ(\uD835\uDEC9) be a loss function which is differentiable w.r.t. the parameters of the SPTM, \uD835\uDEC9. We can compute the gradient of the loss and learn \uD835\uDEC9 using gradient-based numerical optimization algorithms, such as L-BFGS or stochastic gradient descent (SGD)."
    }, {
      "heading" : "5.1 Computing the Gradient",
      "text" : "Since the loss does not explicitly depend on \uD835\uDEC9, we use the chain rule for differentiation:\n\uD835\uDF15ℒ(\uD835\uDEC9)\n\uD835\uDF15\uD835\uDEC9 = ∑\n\uD835\uDF15ℒ(\uD835\uDEC9)\n\uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52)\n\uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52)\n\uD835\uDF15\uD835\uDEC9 (\uD835\uDC53,\uD835\uDC52 )\n= ∑ −\uD835\uDEFF(\uD835\uDC53,\uD835\uDC52) \uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52)\n\uD835\uDF15\uD835\uDEC9 (\uD835\uDC53,\uD835\uDC52 )\n(7)\nwhich takes the form of summation over all phrase pairs occurring either in a training sample (stochastic mode) or in the entire training data (batch mode). \uD835\uDEFF(\uD835\uDC53,\uD835\uDC52) in (7) is known as the error term of the phrase pair (\uD835\uDC53, \uD835\uDC52), and is defined as\n\uD835\uDEFF(\uD835\uDC53,\uD835\uDC52) = − \uD835\uDF15ℒ(\uD835\uDEC9)\n\uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) (8)\nIt describes how the overall loss changes with the translation score of the phrase pair (\uD835\uDC53, \uD835\uDC52). We will leave the derivation of \uD835\uDEFF(\uD835\uDC53,\uD835\uDC52) to Section 5.1.2, and will first describe how the gradient of sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) w.r.t. \uD835\uDEC9 is computed.\n5.1.1 Computing \uD835\uDF4F\uD835\uDC94\uD835\uDC8A\uD835\uDC8E\uD835\uDF3D(\uD835\uDC99\uD835\uDC87, \uD835\uDC99\uD835\uDC86)/\uD835\uDF4F\uD835\uDF3D\nWithout loss of generality, we use the following notations to describe a neural network:\n \uD835\uDC16\uD835\uDC59 is the projection matrix for the l-th layer of the neural network;  \uD835\uDC31 is the input word vector of a phrase;  \uD835\uDC33\uD835\uDC59 = \uD835\uDC16\uD835\uDC59 T\uD835\uDC32\uD835\uDC59−1 is the sum vector of the l-th layer; and  \uD835\uDC32\uD835\uDC59 = \uD835\uDF0E(\uD835\uDC33\uD835\uDC59) is the output vector of the l-th layer, where \uD835\uDF0E is an activation function;\nThus, the SPTM defined by (2) and (3) can be represented as\n\uD835\uDC331 = \uD835\uDC161 T\uD835\uDC31 \uD835\uDC321 = \uD835\uDF0E(\uD835\uDC331) \uD835\uDC332 = \uD835\uDC162 T\uD835\uDC321 \uD835\uDC322 = \uD835\uDF0E(\uD835\uDC332)\nsim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) = (\uD835\uDC32\uD835\uDC53 2) T \uD835\uDC32\uD835\uDC52 2\nThe gradient of the matrix \uD835\uDC162 which projects the hidden vector to the output vector is computed as:\n∂sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52)\n∂\uD835\uDC162 =\n∂(\uD835\uDC32\uD835\uDC53 2)\nT\n∂\uD835\uDC162 \uD835\uDC32\uD835\uDC52\n2 + (\uD835\uDC32\uD835\uDC53 2)\nT ∂\uD835\uDC32\uD835\uDC52 2\n∂\uD835\uDC162 = \uD835\uDC32\uD835\uDC53\n1 (\uD835\uDC32\uD835\uDC52 2 ∘ \uD835\uDF0E′(\uD835\uDC33\uD835\uDC53\n2)) T\n+ \uD835\uDC32\uD835\uDC52 1 (\uD835\uDC32\uD835\uDC53 2 ∘ \uD835\uDF0E′(\uD835\uDC33\uD835\uDC52 2))\nT\n(9)\nwhere ∘ is the element-wise multiplication (Hadamard product). Applying the back propagation principle, the gradient of the projection matrix mapping the input vector to the hidden vector \uD835\uDC161 is computed as\n∂sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52)\n∂\uD835\uDC161 = \uD835\uDC31\uD835\uDC53 (\uD835\uDC162 (\uD835\uDC32\uD835\uDC52\n2 ∘ \uD835\uDF0E′(\uD835\uDC33\uD835\uDC53 2)) ∘ \uD835\uDF0E′(\uD835\uDC33\uD835\uDC53\n1)) T\n+ \uD835\uDC31\uD835\uDC52 (\uD835\uDC162 (\uD835\uDC32\uD835\uDC53 2 ∘ \uD835\uDF0E′(\uD835\uDC33\uD835\uDC52 2)) ∘ \uD835\uDF0E′(\uD835\uDC33\uD835\uDC52 1))\nT\n(10)\nThe derivation can be easily extended to a neural network with multiple hidden layers."
    }, {
      "heading" : "5.1.2 Computing \uD835\uDF39(\uD835\uDC87,\uD835\uDC86)",
      "text" : "To simplify the notation, we rewrite our loss function of (5) and (6) over one training sample as\nℒ(\uD835\uDEC9) = −xBleu(\uD835\uDEC9) = − G(\uD835\uDEC9)\nZ(\uD835\uDEC9) (11)\nwhere\nG(\uD835\uDEC9) = ∑ sBleu(\uD835\uDC38, \uD835\uDC38\uD835\uDC56) exp(\uD835\uDECC T\uD835\uDC21(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38, \uD835\uDC34))\n\uD835\uDC38\nZ(\uD835\uDEC9) = ∑ exp(\uD835\uDECCT\uD835\uDC21(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38, \uD835\uDC34)) \uD835\uDC38\nCombining (8) and (11), we have\n\uD835\uDEFF(\uD835\uDC53,\uD835\uDC52) = \uD835\uDF15xBleu(\uD835\uDEC9)\n\uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) =\n1\nZ(\uD835\uDEC9) (\n\uD835\uDF15G(\uD835\uDEC9)\n\uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) −\n\uD835\uDF15Z(\uD835\uDEC9)\n\uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) xBleu(\uD835\uDEC9)) (12)\nBecause \uD835\uDEC9 is only relevant to ℎ\uD835\uDC40+1 which is defined in (4), we have\n\uD835\uDF15\uD835\uDECCT\uD835\uDC21(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38, \uD835\uDC34) \uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) = \uD835\uDF06\uD835\uDC40+1 \uD835\uDF15ℎ\uD835\uDC40+1(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38, \uD835\uDC34) \uD835\uDF15sim\uD835\uDEC9(\uD835\uDC31\uD835\uDC53 , \uD835\uDC31\uD835\uDC52) = \uD835\uDF06\uD835\uDC40+1\uD835\uDC41(\uD835\uDC53, \uD835\uDC52; \uD835\uDC34) (13)\nwhere \uD835\uDC41(\uD835\uDC53, \uD835\uDC52; \uD835\uDC34) is the number of times the phrase pair (\uD835\uDC53, \uD835\uDC52) occur in \uD835\uDC34. Combining (12) and (13), we end up with the following equation\n\uD835\uDEFF(\uD835\uDC53,\uD835\uDC52) = ∑ U(\uD835\uDEC9, \uD835\uDC38)\uD835\uDC43(\uD835\uDC38|\uD835\uDC39\uD835\uDC56)\uD835\uDF06\uD835\uDC40+1\uD835\uDC41(\uD835\uDC53, \uD835\uDC52; \uD835\uDC34)\n(\uD835\uDC38,\uD835\uDC34)∈\uD835\uDC3A\uD835\uDC38\uD835\uDC41(\uD835\uDC39\uD835\uDC56)\n(14)\nwhere U(\uD835\uDEC9, \uD835\uDC38) = sBleu(\uD835\uDC38\uD835\uDC56 , \uD835\uDC38) − xBleu(\uD835\uDEC9)."
    }, {
      "heading" : "5.2 The Training Algorithm",
      "text" : "In our experiments we train the parameters of the SPTM \uD835\uDEC9 using the L-BFGS optimizer described in [1], together with the loss function described in (5). The gradient is computed as described in Sections 5.1. Even though the loss function is not convex, we found that the L-BFGS iterations over the complete training data (batch mode) minimizes the loss in practice in a desirable fashion; e.g., convergence of the algorithm was found to be smooth."
    }, {
      "heading" : "6. Experiments",
      "text" : "We conducted our experiments on two Europarl translation tasks, English-to-French (EN-FR) and German-to-English (DE-EN). The data sets are published for the shared task in NAACL 2006 Workshop on Statistical Machine Translation (WMT06) [16]. For EN-FR, the training set contains 688K sentence pairs, with 21 words per sentence on average. The development set contains 2000 sentences. We used 2000 sentences from the WMT05 shared task as the first test set (TEST1), and the 2000 sentences from the WMT06 shared task as the second test set (TEST2). For DE-EN, the training set contains 751K sentence pairs, with 21 words per sentence on average. The official development set used for the shared task contains 2000 sentences. We used 2000 sentences from the WMT05 shared task as TEST1, and the 2000 sentences from the WMT06 shared task as TEST2. We used the Moses system [15] as our baseline phrase-based SMT system. We built two baseline systems, each for one language pair. The metric used for evaluation is case insensitive BLEU score [21]. We also performed a significance test using the paired t-test. Differences are considered statistically significant when the p-value is less than 0.05."
    }, {
      "heading" : "6.1 Results",
      "text" : "Table 2 shows the main results measured in BLEU evaluated on TEST1 and TEST2, where Row 1 is the baseline system. Rows 2 to 5 are the systems enhanced by integrating different versions of the SPTM. SPTM in Row 2 is the model described in Sections 4. As illustrated in Figure 2, the number of the nodes in the input layer is the vocabulary size \uD835\uDC51. Both the hidden layer and the output layer have 100 nodes. That is, \uD835\uDC161 is a \uD835\uDC51 × 100 matrix and \uD835\uDC162 a 100 × 100 matrix. Table 2 shows that SPTM leads to a substantial improvement over the baseline system across all test sets, with a statistically significant margin from 0.7 to 1.0 BLEU points.\nWe have developed a set of variants of SPTM, as shown in Rows 3 to 5, to investigate two design choices we made in developing the SPTM: (1) whether to use a linear projection or a multilayer nonlinear projection; and (2) whether to compute the phrase similarity using word-word similarities as suggested by e.g., the lexical weighting model [17].\nSPTML (Row 3) uses a linear neural network to project a word vector of a phrase \uD835\uDC31 to a feature vector \uD835\uDC32 ≡ \uD835\uDF19(\uD835\uDC31) = \uD835\uDC16T\uD835\uDC31, where \uD835\uDC16 is a \uD835\uDC51 × 100 projection matrix. The translation score of a\nsource phrase f and a target phrase e is measured as the similarity of their feature vectors. We choose cosine similarity because it works better than dot product for linear projection. SPTMW (Row 4) computes the phrase similarity using word-word similarity scores. This follows the common smoothing strategy of addressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model [17] and the word factored n-gram translation model [24].\n# Systems EN-FR DE-EN TEST1 TEST2 TEST1 TEST2 1 Baseline 32.79 32.84 26.04 26.04 2 SPTM 33.79 α 33.81 α 26.82 α 26.72 α 3 SPTML 33.56 αβ 33.51 αβ 26.67 α 26.50 αβ 4 SPTMW 33.21 αβ 33.27 αβ 26.56 αβ 26.49 αβ 5 SPTML-W 33.25 αβ 33.35 αβ 26.46 αβ 26.33 αβ 6 2 + 4 33.79 α 33.81 α 26.81 α 26.73 α 7 BLTMPR 32.78β 32.95 26.06β 26.09β 8 DPM 32.90β 32.99αβ 26.20αβ 26.16β Table 2: Main results (BLEU scores) of semantic-based phrase translation models. The superscripts α and β indicate statistically significant difference (p < 0.05) from Baseline and SPTM, respectively.\nTwo observations can be made by comparing SPTM in Row 2 to its variants in Rows 3-5. First of all, it is more effective to model the phrase translation directly than decomposing it into wordword translations in the SPTMs (Row 2 vs. Row 4 and Row 3 vs. Row 5). Moreover, unlike the case of traditional phrase translation models, combining the phrase model and the word model does not lead to any visible improvement (Row 6 vs. Row 2), indicating that with semantic representations, a phrase model is no longer sparser than a word model. Second, we see that in phrase models (Rows 2 and 3) the nonlinear projection is able to capture more sophisticated semantic information and leads to better results than the linear projection."
    }, {
      "heading" : "6.2 Comparing with Previous Latent Semantic Models",
      "text" : "This section compares the best version of the SPTM i.e., SPTM in Row 2 of Table 2, with two stateof-the-art latent semantic models that are originally trained on clicked query-document pairs (i.e., clickthrough data extracted from search logs) for query-document matching [9]. To adopt these models for SMT, we view source-target sentence pairs as clicked query-document pairs, and trained both models using the same methods as in [9] on the parallel bilingual training data described earlier.\nThe results are shown in Table 2. BTLMPR (Row 7) is an extension to PLSA, and is the best performer among different versions of the Bi-Lingual Topic Model (BLTM) described in [9]. BLTM with Posterior Regularization (BLTMPR) is trained on parallel training data using the EM algorithm with a constraint enforcing a source sentence and its paralleled target sentence to not only share the same prior topic distribution, but to also have similar fractions of words assigned to each topic. We incorporated the model into the log-linear model for SMT (1) as follows. First of all, the topic distribution (i.e., semantic representation) of a source sentence \uD835\uDC39\uD835\uDC56, denoted by \uD835\uDC43(\uD835\uDC67|\uD835\uDC39\uD835\uDC56), is induced from the learned topic-word distributions using EM. Then, each translation candidate \uD835\uDC38 in the N-best list GEN(\uD835\uDC39\uD835\uDC56) is scored as\n\uD835\uDC43(\uD835\uDC38|\uD835\uDC39\uD835\uDC56) = ∏ ∑ \uD835\uDC43(\uD835\uDC64|\uD835\uDC67)\uD835\uDC43(\uD835\uDC67|\uD835\uDC39\uD835\uDC56)\n\uD835\uDC67\uD835\uDC64∈\uD835\uDC38\n\uD835\uDC43(\uD835\uDC39\uD835\uDC56|\uD835\uDC38) can be similarly computed. Finally, the logarithms of the two probabilities are incorporated into the log-linear model of (1) as two additional features.\nDPM (Row 8) is the Discriminative Projection Model described in [9]. DPM uses a matrix \uD835\uDC16 to project a word vector of a sentence to a feature vector. \uD835\uDC16 is trained on parallel training data using a Siamese neural network approach, S2Net [26], as follows. For each source sentence in training data, we treat it and its paralleled translation in target language as a positive pair, and we randomly\nselected 4 other target sentences from training data to form 4 negative pairs. \uD835\uDC16 is trained in such a way that a positive source-target sentence pair has a higher similarity (i.e., cosine similarity) than that of the negative ones of the same source sentence. DPM can be incorporated into the log-linear model for SMT (1) by introducing a new feature ℎ\uD835\uDC40+1. Let \uD835\uDC31 be the word vector of a source sentence \uD835\uDC39\uD835\uDC56 (or its translation candidate \uD835\uDC38), and \uD835\uDC32 be the projected feature vector, i.e., \uD835\uDC32 = \uD835\uDC16\nT\uD835\uDC31. The new feature is defined as\nℎ\uD835\uDC40+1(\uD835\uDC39\uD835\uDC56 , \uD835\uDC38) ≡ sim\uD835\uDC16(\uD835\uDC31\uD835\uDC39\uD835\uDC56 , \uD835\uDC31\uD835\uDC38) = \uD835\uDC32\uD835\uDC39\uD835\uDC56\nT \uD835\uDC32\uD835\uDC38\n‖\uD835\uDC32\uD835\uDC39\uD835\uDC56‖‖\uD835\uDC32\uD835\uDC38‖\nSimilar to that BLTM is an extension to PLSA, DPM can be viewed as an extension of LSA where bilingual parallel data can be explored for translation model training. As we see from Table 2, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than SPTM which is based on a multi-layer neural network trained on the Nbest lists using a loss function that tailors to the BLEU metric. However, we found in our experiments that these models can be useful for “pre-training” to provide a good initial model that not only speeds up the SPTM training but also leads to a better final model."
    }, {
      "heading" : "6.3 Discussion",
      "text" : "Although SGD has been advocated for neural network training due to its simplicity and its robustness to local minimum, we found that in our task the L-BFGS based batch training performs well despite the non-convexity in our loss. Another merit of batch training is that the gradient over all training data can be computed efficiently. As shown in Section 5, computing \uD835\uDF15simθ(x\uD835\uDC53 , x\uD835\uDC52)/\uD835\uDF15θ requires large-scale matrix multiplications, and is expensive for multi-layer neural networks. Eq. (7) suggests that \uD835\uDF15simθ(x\uD835\uDC53 , x\uD835\uDC52)/\uD835\uDF15θ and \uD835\uDEFF(\uD835\uDC53,\uD835\uDC52) can be computed separately, thus making the computation cost of the former term only depends on the number of phrase pairs in the phrase table, but not the size of training data. Therefore, the training method described in Section 5 can be used on larger amounts of training data with little difficulty."
    }, {
      "heading" : "7. Conclusions",
      "text" : "The work presented in this paper makes two important contributions. First, we develop a novel phrase translation model for SMT, where the translation score of a pair of source-target phrases is represented as the distance between their feature vectors in a low-dimensional, continuous-valued semantic space. The semantic space is derived from the representations generated using a multilayer neural network. Second, we present a new learning method to train the weights in the multilayer neural network for the end-to-end BLEU metric directly. The training method is based on LBFGS. We describe in detail how the gradient in closed form, as required for efficient optimization, is derived. The objective function, which takes the form of the expected BLEU computed from Nbest lists, is very different from the usual objective functions used in most existing neural networks, e.g., cross entropy or mean square error [11, 12]. We hence have provided details in the derivation of the gradient, which can serve as an example to guide the derivation of neural network learning with other non-standard objective functions in the future.\nOur evaluation on two Europal translation tasks show that incorporating the SPTM into the loglinear framework of SMT significantly improves the performance of a state-of-the-art phrase-based SMT system, leading to a gain between 0.7 to 1.0 BLEU points. Careful implementation of the LBFGS optimization based on the BLEU-centric objective function, together with the associated closed-form gradient, is a key to the success.\nA natural extension of this work is to expand the model and learning algorithm from shallow to deep neural networks. The deep models are expected to produce more powerful and flexible semantic representations, and thus greater performance gain than what is presented in this paper."
    } ],
    "references" : [ {
      "title" : "Scalable training of L1-regularized log-linear models",
      "author" : [ "G. Andrew", "J. Gao" ],
      "venue" : "In ICML",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Joint language and translation modeling with recurrent neural networks",
      "author" : [ "M. Auli", "M. Galley", "C. Quirk", "G. Zweig" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Y. Bengio", "R. Duharme", "P. Vincent", "C. Janvin" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T. Landauer", "R. Harshman" ],
      "venue" : "Journal of the American Society for Information Science,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1990
    }, {
      "title" : "Why generative phrase models underperform surface heuristics",
      "author" : [ "J. DeNero", "D. Gillick", "J. Zhang", "D. Klein" ],
      "venue" : "In Workshop on Statistical Machine Translation,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Automatic cross-language retrieval using latent semantic indexing",
      "author" : [ "S. Dumais", "T. Letsche", "M. Littman", "T. Landauer" ],
      "venue" : "Spring Symposium Series: Cross-Language Text and Speech Retrieval",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "Training MRF-based translation models using gradient ascent",
      "author" : [ "J. Gao", "X. He" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Clickthrough-based latent semantic models for web search",
      "author" : [ "J. Gao", "K. Toutanova", "Yih", "W-T" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Maximum expected bleu training of phrase and lexicon translation models",
      "author" : [ "X. He", "L. Deng" ],
      "venue" : "In ACL,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Discovering Binary Codes for Documents by Learning Deep Generative Models",
      "author" : [ "G. Hinton", "R. Salakhutdinov" ],
      "venue" : "Topics in Cognitive Science,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Probabilistic latent semantic indexing",
      "author" : [ "T. Hofmann" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Learning deep structured semantic models for web search using clickthrough data",
      "author" : [ "Huang", "P-S", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck" ],
      "venue" : "In CIKM",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Moses: open source toolkit for statistical machine translation",
      "author" : [ "P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Manual and automatic evaluation of machine translation between European languages",
      "author" : [ "P. Koehn", "C. Monz" ],
      "venue" : "In Workshop on Statistical Machine Translation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Statistical phrase-based translation",
      "author" : [ "P. Koehn", "F. Och", "D. Marcu" ],
      "venue" : "In HLT-NAACL,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "A phrase-based, joint probability model for statistical machine translation",
      "author" : [ "D. Marcu", "W. Wong" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "Minimum error rate training in statistical machine translation",
      "author" : [ "F. Och" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2003
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "K. Papineni", "S. Roukos", "T. Ward", "Zhu W-J" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "Translingual Document Representations from Discriminative Projections",
      "author" : [ "J. Platt", "K. Toutanova", "W. Yih" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Smooth bilingual n-gram translation",
      "author" : [ "H. Schwenk", "M.R. Costa-Jussa", "J.A.R. Fonollosa" ],
      "venue" : "In EMNLP-CoNLL,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Continuous space translation models with neural networks",
      "author" : [ "L.H. Son", "A. Allauzen", "F. Yvon" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Training phrase translation models with leavingone-out",
      "author" : [ "J. Wuebker", "A. Mauser", "H. Ney" ],
      "venue" : "In ACL,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Learning discriminative projections for text similarity measures",
      "author" : [ "W. Yih", "K. Toutanova", "J. Platt", "C. Meek" ],
      "venue" : "In CoNLL",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "The most common method of constructing the phrase table takes a two-phase approach [17].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Motivated by recent studies on continuous-space language models (LM) [3, 19], we use a neural network to project a word vector to a feature vector.",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "Motivated by recent studies on continuous-space language models (LM) [3, 19], we use a neural network to project a word vector to a feature vector.",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "Latent Semantic Analysis (LSA) [5], originally designed for information retrieval (IR), is arguably the earliest continuous semantic model.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "Unlike LSA which is a linear projection model, generative topic models, such as Probabilistic LSA [13] and Latent Dirichlet Allocation (LDA) [4] give a clear probabilistic interpretation of the semantic representation.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : ", the feed-forward neural network language model (NNLM) [3] and the recurrent neural network language model (RNNLM) [19, 2], provide a different kind of latent semantic representation.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : ", the feed-forward neural network language model (NNLM) [3] and the recurrent neural network language model (RNNLM) [19, 2], provide a different kind of latent semantic representation.",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : ", the feed-forward neural network language model (NNLM) [3] and the recurrent neural network language model (RNNLM) [19, 2], provide a different kind of latent semantic representation.",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space.",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space.",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 20,
      "context" : "As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space.",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : "As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space.",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "The only exception we are aware of is the work of continuous space n-gram translation models [23, 24], where the feed-forward NNLM is extended to represent translation probabilities.",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 22,
      "context" : "The only exception we are aware of is the work of continuous space n-gram translation models [23, 24], where the feed-forward NNLM is extended to represent translation probabilities.",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 23,
      "context" : "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "Among them, [8] is most relevant to the work described in this paper.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : ", a reimplementation of the Moses system [15] that does not use the SPTM, and each E ∈ GEN(Fi) is labeled by the sentencelevel BLEU score [10], denoted by sBleu(Ei , E), which meaures the quality of E with respect to its reference translation Ei;  A vector of features h ∈ R that maps each (Fi , E) to a vector of feature values; and  A parameter vector λ ∈ R, which assigns a real-valued weight to each feature.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : ", a reimplementation of the Moses system [15] that does not use the SPTM, and each E ∈ GEN(Fi) is labeled by the sentencelevel BLEU score [10], denoted by sBleu(Ei , E), which meaures the quality of E with respect to its reference translation Ei;  A vector of features h ∈ R that maps each (Fi , E) to a vector of feature values; and  A parameter vector λ ∈ R, which assigns a real-valued weight to each feature.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 18,
      "context" : "We fix θ, and optimize λ using MERT [20] to maximize the BLEU score on development data.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "In our experiments we train the parameters of the SPTM θ using the L-BFGS optimizer described in [1], together with the loss function described in (5).",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "The data sets are published for the shared task in NAACL 2006 Workshop on Statistical Machine Translation (WMT06) [16].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "We used the Moses system [15] as our baseline phrase-based SMT system.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 19,
      "context" : "The metric used for evaluation is case insensitive BLEU score [21].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : ", the lexical weighting model [17].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "This follows the common smoothing strategy of addressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model [17] and the word factored n-gram translation model [24].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 22,
      "context" : "This follows the common smoothing strategy of addressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model [17] and the word factored n-gram translation model [24].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 7,
      "context" : ", clickthrough data extracted from search logs) for query-document matching [9].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "To adopt these models for SMT, we view source-target sentence pairs as clicked query-document pairs, and trained both models using the same methods as in [9] on the parallel bilingual training data described earlier.",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "BTLMPR (Row 7) is an extension to PLSA, and is the best performer among different versions of the Bi-Lingual Topic Model (BLTM) described in [9].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "DPM (Row 8) is the Discriminative Projection Model described in [9].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 24,
      "context" : "W is trained on parallel training data using a Siamese neural network approach, S2Net [26], as follows.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : ", cross entropy or mean square error [11, 12].",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : ", cross entropy or mean square error [11, 12].",
      "startOffset" : 37,
      "endOffset" : 45
    } ],
    "year" : 2013,
    "abstractText" : "This paper presents a novel semantic-based phrase translation model. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent semantic space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a multi-layer neural network whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of end-to-end machine translation results. Experimental evaluation has been performed on two Europarl translation tasks, English-French and German-English. The results show that the new semantic-based phrase translation model significantly improves the performance of a state-of-the-art phrase-based statistical machine translation system, leading to a gain of 0.7-1.0 BLEU points.",
    "creator" : "Microsoft® Word 2013"
  }
}