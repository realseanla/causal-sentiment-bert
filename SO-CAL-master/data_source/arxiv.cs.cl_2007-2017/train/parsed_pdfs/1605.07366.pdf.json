{
  "name" : "1605.07366.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "NLG is the task of generating natural language from nonlinguistic inputs. Most NLG                          systems can be classified into two broad camps  template based and statistical. Template                            based systems are generally characterized by structure gapped text (slotfiller structure)                      which is predominantly manually created (Reiter 1995) which generally result in high quality                          text but also limited linguistic coverage. Statistical systems such as (Langkilde 1998) on the                            other hand, use datadriven algorithms for text generation and have littletono reliance on                          repetitive manual resources making them more adaptable and maintainable, albeit with lesser                        text quality (Reiter 1995). \nIn this work, we consider generation on a sentence level and output gapped text. We                              define templates as gapped text which can be filled to generate textual output. A (partial)                              sentence is a linear sequence of such templates. Since there are a large number of choices                                (templates) at every step of sentence generation (sequence of templates), it naturally gives                          rise to a search space which contains all such sequences  grammatical and ungrammatical.                            The challenge is to navigate this large search space and arrive at reasonable grammatical                            sentences.    RELEVANT WORK \nTemplate based systems have been considered to have a very shallow linguistic                          representation and mapping of the nonlinguistic data to the generated text. General                        consensus among the NLG community has been that template based systems are not as                            flexible, maintainable and expressive (linguistic coverage) as full fledged NLG systems                      (Reiter 1995).  (Reiter 1997) mentions that template based systems and NLG systems are “turing                         \n1 \nequivalent” meaning that at least in terms of expressiveness, there is no theoretical disparity                            between the two. Early template based systems (Kukich 1983) used large phrasal units as                            templates to generate text. However, later systems (van Deemter and Odijk, 1997, Theune,                          2001) in addition to working with smaller templates also exhibit templates with gaps (slots)                            which can be filled recursively which made the systems a lot more maintainable and robust to                                output text requirement changes. NaturalOWL (Androustopoulos, 2007) is a template based                      system which generates descriptions of artifacts in a museum based on the user expertise                            using an OWL ontology as its data source. (Deemter 2005) gives convincing arguments that                            the line between templatebased systems and NLG is quite blurry. (Kondadadi, 2013) present                          a hybrid NLG system which generates text by ranking tagged clusters of templates.                          Combining that with the huge amount of text data available today, corpus based template                            approaches seem feasible. \nIn this work, we focus on template structure and combination. A template is gapped text                              extracted from a corpus. Eg. “in the NN”, “The JJ NN”, etc. We observe that a template is                                    grammatical locally, within its span. Thus, it doesn’t need to be “generated” itself. It follows                              that linear template combination (juxtaposition) can then be used to generate sentences. The                          problem is then two fold: how to determine what templates to combine and constrain that                              with a measure of grammaticality of the generated partial sentence. We make use of NGram                              models to address both problems. Because the templates are grammatical locally, we can use                            the NGram probabilities at the template edges (junction) to discriminate whether two                        templates can be combined or not. Also, we use an abstraction of the template  a “syntactic                                  signature” to compute the grammaticality of the partial sentence. \nSince the search space for the partial sentences (template sequences) contains all the                          permutations of all lengths, it is also called a permutation space. Since we do not have a                                  reliable “best” sentence metric, we use GAs to explore the search space to get at a “good                                  enough” solution. We describe how the template combination and sentence grammaticality                      are used in the genetic algorithm in the following sections.    APPROACH \nTemplate structure and extraction \nIn this paper, we restrict ourselves to linear, subsentential templates. To reduce their                          number, we consider chunks as templates. It also helps that chunks are a linguistically                            contained structure. We chunk the UkWaC corpus (Ferraresi 2008) (first 10M sentences)                        using CRFChunker (XuanHieu Phan, 2006) for extracting the templates.    Template Factoring \nIn this step, we introduce gaps in the templates. Instead of the gaps being blanks, we                                specify some linguistic features for the gaps so that not all information is abstracted away.                              These features are called factors. For the baseline, we use the partofspeech as the factor. We                                use the following strategies for factoring  \n● Absolute (count threshold): All tokens(words in a sequence) with counts below a                        threshold are replaced with their factor (POS).  ● Relative (rank threshold): All tokens(words in a sequence) with rank below a                        threshold are replaced with their factor (POS). \nObviously, more aggressive the factoring, more abstract are the templates. \n2 \n  GENETIC ALGORITHM \nGenetic algorithms are a class of optimization algorithms which mimic a natural process  namely natural selection (GA primer ref). Following is a general working of a GA: \n1. Initialize starting population.  2. While stopping criterion is false, do 3  6  3. Selection: Select a sample from the population for reproduction.  4. Crossover: Derive the offspring from the parent sample.  5. Mutation: Mutate the offsprings given the mutation probability.  6. Selection: Select fit individuals from the parent and offspring populations for the next \ngeneration.  These components are described below.    Chromosome \nEach partial solution in the space is an ordered list of templates. That is the chromosome.                                The collection of chromosomes is the total population. We used four factoring strategies:                          count < 100, count < 1000, rank > 1000 and rank > 100. We extracted 18M unique templates                                    from unfactored text and 15M, 12M, 4.6M and 1M unique templates for factored text                            respectively.    Crossover \nThe template combination problem is addressed in the crossover function. Crossover                      determines how and with what probability two parent chromosomes produce an offspring.                        We use juxtaposition as the crossover process. So, if two templates “in the NN” and “VBZ a                                  NN” are combined, the resulting offspring is “in the NN VBZ a NN”. To determine the                                crossover probability, we train a trigram model on the corresponding factored text with                          modified KneserNey smoothing (Sundermeyer 2011) using SRILM (Stolcke 2002). The                    crossover probability is the trigram probability of the token/factors at the junction of the two                              partial sentences. \n  Mutation \nTo mutate a chromosome, we replace it with a single template chosen randomly from the                              total population. Note that the chromosome being replaced can have multiple templates in it.                            That helps curb the sentence length increase rate. The mutation probability is 0.05. \n  Fitness function \n   The sentence grammaticality issue is handled in the fitness function. The fitness function  determines how “good” a particular solution is. It is critical for selecting “good” candidates  each generation, which helps converge to a set of “good” solutions.     There are a few issues for the fitness function to address  \n● Variable length  grammaticality does not depend on length  ● Partial sentences  extra incentive to fully formed sentences \nSince the templates are locally grammatical, we simplify the sentential grammaticality to a                           \n3 \nsequence of templatelevel features  the syntactic signature. For the baseline, use use the                            chunk tags as the syntactic signature. We train a 5gram model over the sequence of chunk                                tags using modified Kneser Ney smoorthing. The fitness value of a partial sentence is the                              total probability of the chunk tag sequence of the templates constituting it. The crossover                            probability is the total probability of the chunk tag sequence of the templates constituting it                              normalized by the length of the chromosome and divided by difference of the target sentence                              length.  Pcrossover = 10 ^ max(Ppartial / lengthpartial, Ptotal / (lengthpartial + 2) / abs(L  lengthpartial)  lengthpartial is the length of the chromosome  L is the length of the solution chromosomes.  Total probability is computed by considering begin and end of sentence tags as well.  We take the max so that we can distinguish between partial or a full (total) sentence.      Evolution policy \nWe use a basic tournament policy with nbest selection. To do that, a tournament is                              conducted between a small sample of the population (tournament size say, 10) and the nbest                              (fittest) offspring are selected. This is done until enough offspring are created. We use 10 as                                the tournament size and select the 10 best offspring from each tournament. We maintained                            1M as the population size with a 5050 parentoffspring split and ran the search for 100                                generations.    RESULTS \nUnfactored:  1. the muslim salutation is so well drawn unanticipated changes hoping to wring                        community pool timebank a bbc rob & bev  Rank  100: \n1. the NN UH 1 EX EX  Rank  1000: \n1. two RB related NNS tell the NNP NNS 1 john 5 , 20 NNP the same time NNS � CD                                        and plus � 1 ' NNS and NNS , telephone , NN young NNP school NNS  Count  1000:  1. the granary holiday cottages NNP helps to sit a JJ cray NNP supercomputer                         \ninternationally of these other gentlemen com NNP magazine  Count  100 \n1. sometimes western countries may also have been located her the personal                      organisation skills had not been tested or used a window dialogue box should be                            properly studied such a safe labour seat \n  OBSERVATIONS \n    We observe that the fitness values fluctuate around 0.18 for all factoring experiments (that  is because the syntactic signature remains the same). The chromosome length did stabilize  around generation 40. Clearly, the sentences don’t end with punctuations. Long distance \n4 \ndependencies are not handled well by NGram models, so that may not be a good  grammaticality measure.     CONCLUSIONS AND FUTURE WORK \nClearly, we can observe that the fitness function needs improement to better reflect                          grammaticality. For future work, we see multiple areas of improvement: \n1. Length desensitization to N in the fitness function  percentile measures We currently                          divide logprob by length.  2. Check fitness function behavior with correct sentences to help tune parameters.  3. Make selection depend on average length of the sentences.  4. Human evaluation and BLEU scores can be incorporated into fitness function as well.  5. Create better syntactic signatures and use a factored language model, perhaps.  6. Use semantic categorization factors in crossover probability to make the selection                     \nmore robust.  7. Explore using skipgrams for grammaticality.  8. Explore methods to distinguish partial sentences from full sentences in the fitness                       \nfunction, and use that as a stopping criterion in tournament selection.   \nREFERENCES \nKondadadi, R., Howald, B., & Schilder, F. (2013, August). A Statistical NLG Framework for  Aggregated Planning and Realization. In ACL (1) (pp. 14061415).    Van Deemter, K., & Odijk, J. (1997). Context modeling and the generation of spoken  discourse. Speech Communication, 21(1), 101121.    Galanis, D., & Androutsopoulos, I. (2007, June). Generating multilingual descriptions from  linguistically annotated OWL ontologies: the NaturalOWL system. In Proceedings of the  Eleventh European Workshop on Natural Language Generation (pp. 143146). Association  for Computational Linguistics.    Langkilde, I., & Knight, K. (1998, August). The practical value of ngrams in generation. In  Proceedings of the ninth international workshop on natural language generation (pp.  248255).    Reiter, E., & Dale, R. (1997). Building applied natural language generation systems. Natural  Language Engineering, 3(01), 5787.    Kukich, K. (1983, June). Design of a knowledgebased report generator. In Proceedings of  the 21st annual meeting on Association for Computational Linguistics (pp. 145150).  Association for Computational Linguistics.    XuanHieu Phan, CRFChunker: CRF English Phrase Chunker,  http://crfchunker.sourceforge.net/, 2006.   \n5 \nReiter, E. (1995). NLG vs. templates. arXiv preprint cmplg/9504013. \nVan Deemter, K., Krahmer, E., & Theune, M. (2005). Real versus templatebased natural  language generation: A false opposition?. Computational Linguistics, 31(1), 1524. \nFerraresi, A., Zanchetta, E., Baroni, M., & Bernardini, S. (2008, June). Introducing and  evaluating ukWaC, a very large webderived corpus of English. In Proceedings of the 4th  Web as Corpus Workshop (WAC4) Can we beat Google (pp. 4754). \nStolcke, A. (2002, September). SRILMan extensible language modeling toolkit. In  INTERSPEECH (Vol. 2002, p. 2002). \nSundermeyer, M., Schlüter, R., & Ney, H. (2011, August). On the Estimation of Discount  Parameters for Language Model Smoothing. In INTERSPEECH (pp. 14331436). \n \n6 "
    } ],
    "references" : [ {
      "title" : "A Statistical NLG Framework for Aggregated Planning and Realization",
      "author" : [ "R. Kondadadi", "B. Howald", "Schilder", "August" ],
      "venue" : "(pp. 1406­1415)",
      "citeRegEx" : "Kondadadi et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Kondadadi et al\\.",
      "year" : 1997
    }, {
      "title" : "Generating multilingual descriptions from linguistically annotated OWL ontologies: the NaturalOWL system",
      "author" : [ "D. Galanis", "Androutsopoulos", "June" ],
      "venue" : "In Proceedings of the Eleventh European Workshop on Natural Language Generation (pp. 143­146)",
      "citeRegEx" : "Galanis et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Galanis et al\\.",
      "year" : 2007
    }, {
      "title" : "The practical value of n­grams in generation",
      "author" : [ "I. Langkilde", "Knight", "August" ],
      "venue" : "In Proceedings of the ninth international workshop on natural language generation​",
      "citeRegEx" : "Langkilde et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Langkilde et al\\.",
      "year" : 1998
    }, {
      "title" : "Building applied natural language generation systems",
      "author" : [ "E. Reiter", "R. Dale" ],
      "venue" : "​Natural Language Engineering​,",
      "citeRegEx" : "Reiter and Dale,? \\Q1997\\E",
      "shortCiteRegEx" : "Reiter and Dale",
      "year" : 1997
    }, {
      "title" : "NLG vs",
      "author" : [ "E. Reiter" ],
      "venue" : "templates. arXiv preprint cmp­lg/9504013.",
      "citeRegEx" : "Reiter,? 1995",
      "shortCiteRegEx" : "Reiter",
      "year" : 1995
    }, {
      "title" : "Real versus template­based natural language generation: A false opposition",
      "author" : [ "K. Van Deemter", "E. Krahmer", "M. Theune" ],
      "venue" : "​Computational Linguistics​,",
      "citeRegEx" : "Deemter et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Deemter et al\\.",
      "year" : 2005
    }, {
      "title" : "Introducing and evaluating ukWaC, a very large web­derived corpus of English. In ​Proceedings of the 4th Web as Corpus Workshop (WAC­4) Can we beat Google​",
      "author" : [ "A. Ferraresi", "E. Zanchetta", "M. Baroni", "Bernardini", "June" ],
      "venue" : null,
      "citeRegEx" : "Ferraresi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ferraresi et al\\.",
      "year" : 2008
    }, {
      "title" : "September). SRILM­an extensible language modeling toolkit",
      "author" : [ "A. Stolcke" ],
      "venue" : "In INTERSPEECH​ (Vol. 2002,",
      "citeRegEx" : "Stolcke,? \\Q2002\\E",
      "shortCiteRegEx" : "Stolcke",
      "year" : 2002
    }, {
      "title" : "On the Estimation of Discount Parameters for Language Model Smoothing",
      "author" : [ "M. Sundermeyer", "R. Schlüter", "Ney", "August" ],
      "venue" : "In ​INTERSPEECH​ (pp. 1433­1436)",
      "citeRegEx" : "Sundermeyer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sundermeyer et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Template based systems are generally characterized by structure gapped text (slot­filler structure) which is predominantly manually created (Reiter 1995) which generally result in high quality text but also limited linguistic coverage.",
      "startOffset" : 140,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : "Statistical systems such as (Langkilde 1998) on the other hand, use data­driven algorithms for text generation and have little­to­no reliance on repetitive manual resources making them more adaptable and maintainable, albeit with lesser text quality (Reiter 1995).",
      "startOffset" : 250,
      "endOffset" : 263
    }, {
      "referenceID" : 4,
      "context" : "General consensus among the NLG community has been that template based systems are not as flexible, maintainable and expressive (linguistic coverage) as full fledged NLG systems (Reiter 1995).",
      "startOffset" : 178,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "To determine the crossover probability, we train a trigram model on the corresponding factored text with modified Kneser­Ney smoothing (Sundermeyer 2011) using SRILM (Stolcke 2002).",
      "startOffset" : 166,
      "endOffset" : 180
    } ],
    "year" : 0,
    "abstractText" : "Natural Language Generation systems typically have two parts ­ strategic (“what to say”) and tactical (“how to say”). We present our experiments in building an unsupervised corpus­driven template based tactical NLG system. We consider templates as a sequence of words containing gaps. Our idea is based on the observation that templates are grammatical locally (within their textual span). We posit the construction of a sentence as a highly restricted sequence of such templates. This work is an attempt to explore the resulting search space using Genetic Algorithms to arrive at acceptable solutions. We present a baseline implementation of this approach which outputs gapped text.",
    "creator" : null
  }
}