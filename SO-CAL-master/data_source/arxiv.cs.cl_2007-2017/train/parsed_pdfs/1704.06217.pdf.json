{
  "name" : "1704.06217.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reinforcement Learning with External Knowledge and Two-Stage Q-functions for Predicting Popular Reddit Threads",
    "authors" : [ "Ji He", "Mari Ostendorf", "Xiaodong He" ],
    "emails" : [ "ostendor}@uw.edu", "xiaohe@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper addresses the problem of predicting popularity of comments in an online discussion forum using reinforcement learning, particularly addressing two challenges that arise from having natural language state and action spaces. First, the state representation, which characterizes the history of comments tracked in a discussion at a particular point, is augmented to incorporate the global context represented by discussions on world events available in an external knowledge source. Second, a two-stage Q-learning framework is introduced, making it feasible to search the combinatorial action space while also accounting for redundancy among sub-actions. We experiment with five Reddit communities, showing that the two methods improve over previous reported results on this task."
    }, {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning refers to learning strategies for sequential decision-making tasks, where a system takes actions at a particular state with the goal of maximizing a long-term reward. Recently, several tasks that involve states and actions described by natural language have been studied, such as text-based games (Narasimhan et al., 2015; He et al., 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al., 2016), Reddit popularity prediction and tracking (He et al., 2016b), and human-computer dialogue systems (Wen et al., 2016; Li et al., 2016). Some of these studies ignore the use of external knowledge or world knowledge, while others (such as information extraction\nand task-oriented dialogue systems) directly interact with an (often) static database.\nExternal knowledge – both general and domainspecific – has been shown to be useful in many natural language tasks, such as in question answering (Yang et al., 2003; Katz et al., 2005; Lin, 2002), information extraction (Agichtein and Gravano, 2000; Etzioni et al., 2011; Wu and Weld, 2010), computer games (Branavan et al., 2012), and dialog systems (Ammicht et al., 1999; Yan et al., 2016). However, in reinforcement learning, incorporating external knowledge is relatively rare, mainly due to the domain-specific nature of reinforcement learning tasks, e.g. Atari games (Mnih et al., 2015) and the game of Go (Silver et al., 2016). Of particular interest in our work is external knowledge represented by unstructured text, such as news feeds, Wikipedia pages, search engine results, and manuals, as opposed to a structured knowledge base.\nOur study is conducted on the task of Reddit popularity prediction proposed in He et al. (2016b), which is a sequential decision-making problem based on a large-scale real-world natural language data set. In this task, a specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. The authors proposed a reinforcement learning solution in which the state is formed by the collection of comments in the threads being tracked, and actions correspond to selecting a subset of new comments to follow (sub-actions) from the set of recent contributions to the discussion. Since comments are potentially redundant (multiple respondents can have similar reactions), the study found that sub-actions were best evaluated in combination. The computational complexity of the combinatorial action space was sidestepped by random sampling a fixed number of candidates from the full action space. A ar X iv :1 70 4.\n06 21\n7v 1\n[ cs\n.C L\n] 2\n0 A\npr 2\n01 7\nmajor drawback of random sampling in this application is that popular comments are rare and easily missed.\nWe make two main contributions in this paper. The first is a novel architecture for incorporating unstructured external knowledge into reinforcement learning. More specifically, information from the original state is used to query the knowledge source (here, an evolving collection of documents corresponding to other online discussions about world events), and the state representation is augmented by the outcome of the query. Thus, the agent can use both the local context (reinforcement learning environment) and the global context (e.g. recent discussions about world news) when making decisions. Second, we propose to use a two-stage Q-learning framework that makes it feasible to explore the full combinatorial natural language action space. A first Q-function is used to efficiently generate a list of sub-optimal candidate actions, and a second more sophisticated Qfunction reranks the list to pick the best action."
    }, {
      "heading" : "2 Task",
      "text" : "On Reddit, users reply to posts and other comments in a threaded (tree-structured) discussion. Comments (and posts) are associated with a karma score, which is a combination of positive and negative votes from registered users indicating popularity of the comment. In prior work (He et al., 2016b), popularity prediction in Reddit discussions (comment recommendation) is proposed for studying reinforcement learning with a large scale natural language action space. At each time step t, the agent receives a string of text that describes the state st and several strings of text that describe the potential actions {ait} ∈ At (new comments to consider). The agent attempts to pick the best action for the purpose of maximizing the long-term reward. In a real-time scenario, the final karma of a comment is not immediately available, so prediction of popularity is based on the text in the comment as well as the context of discussion history. It is common that a lower karma comment will eventually lead to more discussion and popular comments in the future. Thus it is natural to formulate this task as a reinforcement learning problem.\nMore specifically, the set of comments that are being tracked at time t is denoted asMt. The state, action, and immediate rewards are defined as follows:\n• State: all previously tracked comments, as well as the post (root node of the tree), i.e. st = {M0,M1, · · · ,Mt}\n• Action: an action is taken when a total of N new comments Ct = {ct,1, ct,2, · · · , ct,N}, appear as nodes in the subtrees of Mt, and the agent picks a set of K comments to be tracked in the next time step: at = Mt+1 = {c1t , c2t , · · · , cKt }, cit ∈ Ct and cit 6= c j t if i 6= j\n• Reward: rt+1 is the accumulated karma scores1 in comments in Mt+1\nIn this task, because an action corresponds to a set of comments (sub-actions) chosen from a larger set of candidates, the action space is combinatorial. Ct and At are also time-varying, reflecting the flow of the discussion in the paths chosen.\nThe standard Q-learning defines a function Q(s, a) as the expected return starting from s and taking the action a:\nQ(s, a) = E { +∞∑ l=0 γlrt+1+l|st = s, at = a }\nwhere γ ∈ [0, 1] denotes a discount factor. The Q-function associated with an optimal policy can be found by the Q-learning recursion (Watkins and Dayan, 1992):\nQ(st, at)←Q(st, at) + ηt · ( rt+1+\nγ · max a′∈At+1\nQ(st+1, a ′)−Q(st, at) ) where ηt is the learning rate of the algorithm. In He et al. (2016b), two deep Q-learning architectures are proposed, both with separate networks for the state and action spaces yielding embeddings hs and hia, respectively. Those embeddings are combined with a general interaction function g(·) to approximate the Q-values, Q(st, ait) = g ( hs, h i a ) , as in He et al. (2016a), where the approach of using separate networks for natural language state and action spaces is termed a Deep Reinforcement Relevance Network (DRRN)."
    }, {
      "heading" : "3 Related Work",
      "text" : "There has been increasing interest in applying deep reinforcement learning to a variety of problems, including tasks involving natural language.\n1The karma score is observed from an archived version of the discussion, not immediately shown at the time of the comment, so not available to a real-time system.\nTo control agents directly given high-dimensional sensory inputs, a Deep Q-Network (Mnih et al., 2015) has been proposed and shown high capacity and scalability for handling a large state space. Another stream of work in recent deep learning research is the attention mechanism (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Vinyals et al., 2015), where a probability distribution is computed to pay attention to certain parts of a collection of data. It has been shown that the attention mechanism can handle long sequences or a large collection of data, while being quite interpretable. The attention mechanism work that is closest to ours is memory network (MemNN) (Weston et al., 2014; Sukhbaatar et al., 2015). Most work on MemNNs uses embeddings of a query and documents to compute the attention weights for memory slots. Here, we propose models that also use non-content based features (time, popularity) for memory addressing. This helps retrieve content that provides complementary information to what is modeled in the query embedding vector. In addition, the content-based component of our query scheme uses TF-IDF based semantic-similarity, since the memory comprises a very large corpus of external documents that makes end-to-end learning of attention features impractical.\nMultiple studies have explored interacting with a database (or knowledge base) using reinforcement learning. Narasimhan et al. (2016) presents a framework of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. In task-oriented human-computer dialogue interactions, Wen et al. (2016) introduce\na neural network-based trainable dialogue system with a database operator module. Dhingra et al. (2016) proposed a dialogue agent that provides users with an entity from a knowledge base by interactively asking for its attributes. In question answering, knowledge representation and reasoning also plays a central role (Ferrucci et al., 2010; Boyd-Graber et al., 2012). Our goal differs from these studies in that we do not directly optimize a domain-specific knowledge search, instead we use external world knowledge to enrich the state representation in a reinforcement learning task.\nOur task of tracking popular Reddit comments is somewhat related to an approach to multidocument summarization described in (Daumé III et al., 2009). A difference with respect to our problem is that the space of text for selection evolves over time. In addition, in our case, the agent has no access to optimal policy, in contrast to the SEARN algorithm used in that work.\nTo address overestimations of action values, double Q-learning (Hasselt, 2010; Van Hasselt et al., 2015) has been proposed and it leads to better performance gains on several Atari games. Dulac-Arnold et al. (2016) present a policy architecture that works efficiently with a large number of actions. While a combinatorial action space can be large and discrete, this method does not directly apply in our case, because the possible actions are changing over different states. Instead, we borrow the philosophy from double Q and propose a two-stage Q-learning approach to reduce computational complexity by using a first Q-function to construct a quick yet rough estimate in the combinatorial action space, and then a second Q-\nfunction to rerank a set of sub-optimal actions. The work described in our paper improves over (He et al., 2016b) by augmenting the state representation with external knowledge and by combining the two architectures that they proposed in two-stage Q-learning to enable exploration of the full action space. The first DRRN evaluates an action by treating sub-actions as independent and summing their contributions to the Q-value (Figure 1(a)), and the second models potential redundancy of sub-actions by using a BiLSTM at the comment level (Figure 1(b))."
    }, {
      "heading" : "4 Incorporating External Knowledge into the State Representation",
      "text" : "This approach is inspired by the observation that in a real-world decision making process, it is usually beneficial to consider background knowledge. Here, we introduce a mechanism to incorporate external language knowledge into decision making. The intuition is that the agent will keep track of a memory space that helps with decision making, and when a new state comes, the agent refers to this external knowledge and picks relevant resources to help with decision making.\nThe architecture we propose is illustrated in Figure 2. Every time the agent reads the state information from the environment, it performs a lookup operation in external knowledge in its memory. This external knowledge could be a static knowledge base, or more generally it can be a dynamic database. In our experiments, the\nagent keeps an evolving collection of documents from the worldnews subreddit. We use an attention mechanism that produces a probability distribution over the entire external knowledge resource. This weight vector is computed by considering a set of features measuring the relevance between the current state and the “world knowledge” of the agent. More specifically, we consider the following three types of relevance:\n• Timing features: when users express their opinions on a website such as Reddit, it is likely they are referring to more recent news events. We use two indicator features to represent whether a document from the external knowledge is within the past 24 hours, or the past 7 days relative to the time of the new state. We denote these features as 1day and 1wk, respectively.\n• Semantic similarity: we use the standard tf-idf (term-frequency inverse-documentfrequency) (Salton and McGill, 1986) and compute cosine similarity scores as a measure for semantic relevance between the current state and each document in the external knowledge. We denote this semantic similarity as usem ∈ [−1, 1].\n• Popularity: for reddit posts/comments, we may use karma score as a measure for popularity. It is possible that high popularity topics will occur more often in the environment. To compensate the range difference in different relevance measures, we normalize karma scores2 so the feature values fall in the range [0, 1]. We denote this normalized popularity score as upop.\nFor each state the agent extracts the above features for each document in the external knowledge, and form a 4-dimensional feature vector f = [1day,1wk, usem, upop]. The attention weights are then computed as a linear combination followed by a softmax over the entire external knowledge:\np = Softmax([1day,1wk, usem, upop]·β)\nwhere the Softmax operates over the collection of documents and p has dimension equals the number of documents. Note in our experimental setting, the softmax applies for only documents that exist before the new comments appear,\n2Detailed descriptions are given in Section 6.\nand this simulates a “real-time” dynamic external knowledge resource. The attention weights p are then multiplied with document embeddings {di} to form a vector representation (embedding) of “world” knowledge:\no = ∑ i pidi\nThe world embedding is concatenated with the original state embedding to enrich understanding of the environment."
    }, {
      "heading" : "5 Two-Stage Q-learning for a Combinatorial Action Space",
      "text" : "There are two challenges associated with a combinatorial action space. One is the development of a Q-function framework for estimating the longterm reward. This is addressed in (He et al., 2016b). The other is the potentially high computational complexity, due to evaluating Q over every possible pair of (st, ait). In the case of deep Qlearning, most of the time has been spent on the forward-pass from ( N K ) actions to ( N K ) Q-values. For back-propagation, since we only need to backpropagate one particular action the agent has chosen, complexity is not affected by the combinatorial action space.\nOne solution to sidestep computational complexity is to randomly pick a fixed number, say m candidate actions, and perform a max operation. While this is widely used in the reinforcement learning literature, it is problematic in our application because the large and highly skewed action space makes it likely that good actions are missed. Here we propose to use two-stage Q-learning for reducing search complexity. More specifically, we can rewrite the max operation as:\nmax at∈At Q2(st, at) ≈ max at∈Bt Q2(st, at)\nwhere\nBt = m\narg max at∈At Q1(st, at) (1)\nwhere arg maxmat∈At means picking the top-m actions from the whole action set At.\nIn the case of Q1 being DRRN-Sum, we can rewrite Q1(st, at) as:\nQ1(st, at) = K∑ i=1 Q0(st, c i t) = K∑ i=1 qit\nwhich is simplified by precomputing sub-action value qit = Q0(st, c i t), i = 1, · · · , N . Q0 is the simple DRRN introduced in He et al. (2016a). To elaborate, the idea is to use a first Q function Q1 to perform a quick but rough ranking of ait. The second Q function Q2, which can be more sophisticated, is used to rerank the top-m candidate actions. This is effectively a beam search with coarse-to-fine models and reranking. This ensures that all comments are explored, and at the same time, the architecture can be sophisticated enough to capture detailed dependencies between sub-actions, such as information redundancy. In our experiments, we pick Q1 to be DRRN-Sum andQ2 to be DRRN-BiLSTM. While the independence assumption on sub-action interdependency is too strong, the DRRN-Sum model is relatively easy to train. Since the parameters on the action side are tied for different sub-actions, we can train a DRRN with K = 1 and then apply the model for each pair of (st, cit). This will result in N subaction Q-values Q0(st, cit), i = 1, 2, · · · , N . Thus computing Equation 1 is equivalent to sorting ( N K\n) values. Thus, we avoid the huge computational cost of first generating ( N K ) actions from N subactions, then applying a general Q-function approximation to come up with ( N K ) . In Section 6, we train a DRRN (with K = 1) and then copy the parameters to DRRN-Sum, which can be used to evaluate the full action space.3"
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Data set and preprocessing",
      "text" : "We carry out experiments on the task of predicting popular discussion threads on Reddit, as proposed by He et al. (2016b). Specifically, we conduct experiments on data from 5 subreddits including askscience, askmen, todayilearned, askwomen, and politics, which cover diverse genres and topics. In order to have long enough discussion threads, we filter out discussion trees with fewer than 100 comments. For each of the 5 subreddits, we randomly partition 90% of the data for online training, and 10% of the data for testing. Our evaluation metric is accumulated karma scores. For each setting we obtain mean (average reward) and standard deviation (shown as error bars or numbers in brackets) by 5 independent runs, each over 10,000 episodes. In all our ex-\n3The whole two-stage Q framework is summarized in Algorithm 1 in Appendix.\nperiments we set N = 10. The basic subreddit statistics are shown in Table 1. We also report random policy performances and oracle upper bound performances.4\nIn preprocessing we remove punctuation and lowercase all words. We use a bag-of-words representations for each state st, and comment cit in discussion tracking, and for each document in the external knowledge source. The vocabulary contains the most frequent 5,000 words and the out-ofvocabulary rate is 7.1%. We use fully-connected feed-forward neural networks to compute state, action and document embeddings, withL = 2 hidden layers and hidden dimension 20.\nOur Q-learning agent uses -greedy ( = 0.1) throughout online training and testing. The discounting factor γ = 0.9. During training, we use experience replay (Lin, 1992) and the memory size is set to 10,000. For each experience replay, 500 episodes are generated and tuples are stored in a first-in-first-out fashion. We use mini-batch stochastic gradient descent with batch size of 100, and constant learning rate η = 0.000001. We train separate models for different subreddits."
    }, {
      "heading" : "6.2 Incorporating external knowledge",
      "text" : "We first study the effect of incorporating external knowledge, without considering the combinatorial action space. More specifically, we set K = 1 and use the simple DRRN. Each action is to pick a comment {c1t } from Ct to track. Our proposed method uses a state representation augmented by the world knowledge, as illustrated in Figure 2.\nWe utilize the worldnews subreddit as our external knowledge source. This subreddit consists of 9.88k posts. We define each document in the\n4Upper bounds are estimated by exhaustively searching through each discussion tree to find K max karma discussion threads (overlapped comments are counted only once). This upper bound may not be attainable in a real-time setting. For askscience, N = 10 and different K’s, the upper bound performances range from 1991.3 (K = 2) to 2298.0 (K = 5).\nworld knowledge to be the post plus its top-5 comments ranked by karma scores. The agent keeps a growing collection of documents. That is, at each time t, the external knowledge contains documents from worldnews that appear before time t. To compute popularity score of each document, we simply sum the karma scores of post and top5 comments. Then the karma scores are normalized by dividing the highest score in the external knowledge.5 Thus the popularity feature values for computing attention fall in the range [0, 1].\nFor comparison, we experiment with a baseline DRRN without any external knowledge. We also construct a baseline DRRN with hand-crafted rules for picking documents from external knowledge. Those rules include: i) documents within the past-day, ii) documents within the past-week, iii) 10 semantically most similar documents, iv) 10 most popular documents. We use a bag-of-words representation and construct the world embedding used to augment the state representation.\nWe compare multiple ways of incorporating external knowledge for different subreddits and show performance gains over a baseline DRRN (without any external knowledge) in Figure 3. The experimental results show that the DRRN using a learned attention mechanism to retrieve relevant knowledge outperforms all other configurations of DRRNs with rules for knowledge retrieval, and significantly outperforms the DRRN baseline that does not use external knowledge. Also we observe that different relevance features have different impact across subreddits. For example, for askscience, past-day documents have higher impact than past-week documents, while for politics past-week documents are more important. The most-popular documents actually have a negative effect for todayilearned, mainly because those are documents which are most popular throughout the entire history, while todayilearned discussions value information about recent events.6 Nevertheless, the attention mechanism learns to rely on proper features to retrieve useful knowledge for the needs of different domains.\n5Unlike in Fang et al. (2016), the summed karma scores do not follow a Zipfian distribution, so we do not use quantization or any nonlinear transformation.\n6In principle, since we are concatenating the world embedding to obtain an augmented state representation, the result should not get worse. We hypothesize this is due to overfitting and use of mismatched documents, as in the mostpopular setting for todayilearned."
    }, {
      "heading" : "6.3 Two-stage Q-learning for a combinatorial action space",
      "text" : "In this subsection we study the effect of two-stage Q-learning, without considering external knowledge. We train DRRN (K = 1) first, and copy over the parameters to DRRN-Sum as Q1. We then train Q2=DRRN-BiLSTM as before, except that we use Q1=DRRN-Sum to explore the whole action space to obtain Bt.\nOn askscience, we try multiple settings with K = 2, 3, 4, 5 and the results are shown in Table 2. We compare the proposed two-stage Q-learning with two single-stage Q-learning baselines. The first baseline, following the method in He et al. (2016b), uses a random subsampling approach to obtain Bt (with m = 10) and takes the max over them using DRRN-BiLSTM. The second baseline uses DRRN-Sum and explores the whole action space. The proposed two-stage Q-learning uses DRRN-Sum for picking a Bt and DRRN-BiLSTM for reranking. We observe a large improvement by switching from “random” to “all”, showing that exploring the entire action space is critical in this task. There is a consistent gain by using twostage Q-learning instead of a single-stage Q with DRRN-Sum. This shows that using a more sophisticated value function for reranking also helps with\nperformance. In Table 3, we compare two-stage Q-learning with the two baselines across different subreddits, with N = 10,K = 3. The findings are consistent with those for askscience. Since different subreddits may have very different karma score distributions and language style, our results suggest that the algorithm applies well to different community interaction styles.\nDuring testing, we compare runtime of the DRRN-BiLSTM Q-function with different Bt, simulating over 10,000 episodes with N = 10 and K = 2, 3, 4, 5. The search time for the random selection and the two-stage Q-function are similar, both nearly constant for different K. Using two-stage Q the test runtime is reduced by 6× for K = 3 and 11× for K = 5 comparing to exploring the whole action space.7"
    }, {
      "heading" : "6.4 Combined results",
      "text" : "In Figure 4, we present an ablation study on effects of incorporating external knowledge and/or twostage Q-learning (with N = 10,K = 3) across different subreddits. The two contributions we proposed each help improve reinforcement learning performance in a natural language scenario with a combinatorial action space. In addition, combining these two approaches further improves performance. In our task, two-stage Q-learning provides a larger gain. However, in all cases, incorporating external knowledge consistently gives additional gain on top of two-stage Q-learning.\n7Training DRRN-BiLSTM with the whole action space is intractable, so we just used a subspace trained DRRNBiLSTM model for testing. This however achieves worse performance compared to the two-stage Q probably due to mismatch in training and testing.\nWe conduct case studies in Table 4. We show examples of most/least attended documents in the external knowledge given the state description. The documents are shortened for brevity. In the first example, the state is about a question about the atmosphere on Mars. The most-attended documents are correctly related to Mars living conditions, in various sources and aspects. The second example has the state talking about sun’s features compared to other stars. Interestingly, although the agent is able to attend to top documents\ndue to some topic word matching (e.g. sun, star), the picked documents reflect popularity more than topic relevance. The least-attended documents are totally irrelevant in both examples, as expected."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we introduce two approaches for improving natural language based decision making in a combinatorial action space. The first is to augment the state representation of the environment by incorporating external knowledge through a learnable attention mechanism. The second is to use a two-stage Q-learning framework for exploring the entire combinatorial action space, while avoiding enumeration of all possible action combinations. Our experimental results show that both proposed approaches improve the performance in the task of predicting popular Reddit threads."
    }, {
      "heading" : "A Algorithm table for two-stage Q-learning",
      "text" : "As shown in Algorithm 1."
    }, {
      "heading" : "B URLs for subreddits used in this paper",
      "text" : "As shown in Table 5. All post ids will be released for future work on this task.\nAlgorithm 1 Two-stage Q-learning in combinatorial action space (Q1: DRRN-Sum, Q2: DRRNBiLSTM)\n1: Initialize Reddit popularity prediction environment and load dictionary. 2: Initialize DRRN Q0(st, cit; Θ1) (equivalent as DRRN-Sum with K = 1) with small random weights\nand train. The DRRN-Sum Q1(st, at; Θ1) = Q1(st, {c1t , c2t , · · · , cKt }; Θ1) = ∑K i=1Q0(st, c i t; Θ1)\nshares the same parameters as DRRN. 3: Initialize replay memory D to capacity |D|. 4: for episode = 1, . . . ,M do 5: Randomly pick a discussion tree. 6: Read raw state text and a list of sub-action text from the simulator, and convert them to representation s1 and c1,1, c1,2, . . . , c1,N . 7: Compute q1,j = Q0(s1, c1,j ; Θ1) for the list of sub-actions using DRRN forward activation. 8: For each a1 ∈ A1, form value of Q1(s1, a1; Θ1) = ∑K i=1Q0(s1, c i 1; Θ1) = ∑K i=1 q i 1.\n9: Keep a list of top m actions B1 = [a11, a21, · · · , am1 ], where each ai1 consists of K sub-actions. 10: for t = 1, . . . , T do 11: Compute Q2(st, ait; Θ2), i = 1, 2, · · · ,m for Bt, the list of top m actions using DRRNBiLSTM forward activation. 12: Select an action at based on policy π(at = ait|st) derived from Q2. Execute at in simulator. 13: Observe reward rt+1. Read the next state text and the next list of sub-action texts, and convert them to representation st+1 and ct+1,1, ct+1,2, . . . , ct+1,N . 14: Compute qt+1,j = Q0(st+1, ct+1,j ; Θ1) for the list of sub-actions using DRRN. 15: For each at+1 ∈ At+1, form value of Qt+1(st+1, at+1; Θ1) = ∑K i=1Q0(st+1, c\ni t+1; Θ1) =∑K\ni=1 q i t+1.\n16: Keep a list of top m actions Bt+1 = [a1t+1, a2t+1, · · · , amt+1], where each ait+1 consists of K sub-actions. 17: Store transition (st, at, rt+1, st+1,Bt+1) in D. 18: if during training then 19: Sample random mini batch of transitions (sk, ak, rk+1, sk+1,Bk+1) from D.\n20: Set yk = { rk+1 if sk+1 is terminal rk+1 + γmaxa′∈Bk+1 Q2(sk+1, a\n′; Θ2)) otherwise 21: Perform a gradient descent step on (yk − Q2(sk, ak; Θ2))2 with respect to the network parameters Θ2. Back-propagation is performed only for ak though there are |Ak| actions. 22: end if 23: end for 24: end for"
    } ],
    "references" : [ {
      "title" : "Snowball: Extracting relations from large plain-text collections",
      "author" : [ "E. Agichtein", "L. Gravano." ],
      "venue" : "Proceedings of the fifth ACM conference on Digital libraries. ACM, pages 85–94.",
      "citeRegEx" : "Agichtein and Gravano.,? 2000",
      "shortCiteRegEx" : "Agichtein and Gravano.",
      "year" : 2000
    }, {
      "title" : "Knowledge collection for natural language spoken dialog systems",
      "author" : [ "E. Ammicht", "A. L Gorin", "T. Alonso." ],
      "venue" : "EUROSPEECH.",
      "citeRegEx" : "Ammicht et al\\.,? 1999",
      "shortCiteRegEx" : "Ammicht et al\\.",
      "year" : 1999
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Besting the quiz master: Crowdsourcing incremental classification games",
      "author" : [ "J. Boyd-Graber", "B. Satinoff", "H. He", "H. Daumé III." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
      "citeRegEx" : "Boyd.Graber et al\\.,? 2012",
      "shortCiteRegEx" : "Boyd.Graber et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning to win by reading manuals in a monte-carlo framework",
      "author" : [ "S. Branavan", "D. Silver", "R. Barzilay." ],
      "venue" : "Journal of Artificial Intelligence Research 43:661–704.",
      "citeRegEx" : "Branavan et al\\.,? 2012",
      "shortCiteRegEx" : "Branavan et al\\.",
      "year" : 2012
    }, {
      "title" : "Search-based structured prediction",
      "author" : [ "H. Daumé III", "J. Langford", "D. Marcu." ],
      "venue" : "Machine learning 75(3):297–325.",
      "citeRegEx" : "III et al\\.,? 2009",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2009
    }, {
      "title" : "End-to-end reinforcement learning of dialogue agents for information access",
      "author" : [ "B. Dhingra", "L. Li", "X. Li", "J. Gao", "Y-N Chen", "F. Ahmed", "L. Deng." ],
      "venue" : "arXiv preprint arXiv:1609.00777 .",
      "citeRegEx" : "Dhingra et al\\.,? 2016",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning in large discrete action spaces",
      "author" : [ "G. Dulac-Arnold", "R. Evans", "H. Van Hasselt", "P. Sunehag", "T. Lillicrap", "J. Hunt." ],
      "venue" : "arXiv preprint arXiv:1512.07679 .",
      "citeRegEx" : "Dulac.Arnold et al\\.,? 2016",
      "shortCiteRegEx" : "Dulac.Arnold et al\\.",
      "year" : 2016
    }, {
      "title" : "Open information extraction: The second generation",
      "author" : [ "O. Etzioni", "A. Fader", "J. Christensen", "S. Soderland", "M. Mausam." ],
      "venue" : "IJCAI. volume 11, pages 3–10.",
      "citeRegEx" : "Etzioni et al\\.,? 2011",
      "shortCiteRegEx" : "Etzioni et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning latent local conversation modes for predicting community endorsement in online discussions",
      "author" : [ "H. Fang", "H. Cheng", "M. Ostendorf." ],
      "venue" : "Proc. Int. Workshop Natural Language Processing for Social Media. page 55.",
      "citeRegEx" : "Fang et al\\.,? 2016",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2016
    }, {
      "title" : "Building watson: An overview of the DeepQA project. AI magazine 31(3):59–79",
      "author" : [ "D. Ferrucci", "E. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A. A Kalyanpur", "A. Lally", "J W. Murdock", "E. Nyberg", "J. Prager" ],
      "venue" : null,
      "citeRegEx" : "Ferrucci et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ferrucci et al\\.",
      "year" : 2010
    }, {
      "title" : "Double Q-learning",
      "author" : [ "Hado V. Hasselt." ],
      "venue" : "J. D. Lafferty, C. K. I. Williams, J. ShaweTaylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23. Curran Associates, Inc., pages 2613–",
      "citeRegEx" : "Hasselt.,? 2010",
      "shortCiteRegEx" : "Hasselt.",
      "year" : 2010
    }, {
      "title" : "Deep reinforcement learning with a natural language action space",
      "author" : [ "J. He", "J. Chen", "X. He", "J. Gao", "L. Li", "L. Deng", "M. Ostendorf." ],
      "venue" : "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL).",
      "citeRegEx" : "He et al\\.,? 2016a",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "2016b. Deep reinforcement learning with a combinatorial action space for predicting popular",
      "author" : [ "J. He", "M. Ostendorf", "X. He", "J. Chen", "J. Gao", "L. Li", "L. Deng" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "External knowledge sources for question answering",
      "author" : [ "B. Katz", "G. Marton", "G. C Borchardt", "A. Brownell", "S. Felshin", "D. Loreto", "J. Louis-Rosenberg", "B. Lu", "F. Mora", "S. Stiller" ],
      "venue" : null,
      "citeRegEx" : "Katz et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Katz et al\\.",
      "year" : 2005
    }, {
      "title" : "Deep reinforcement learning for dialogue generation",
      "author" : [ "J. Li", "W. Monroe", "A. Ritter", "D. Jurafsky", "M. Galley", "J. Gao." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "The web as a resource for question answering: Perspectives and challenges",
      "author" : [ "J. J Lin." ],
      "venue" : "LREC.",
      "citeRegEx" : "Lin.,? 2002",
      "shortCiteRegEx" : "Lin.",
      "year" : 2002
    }, {
      "title" : "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "author" : [ "L-J Lin." ],
      "venue" : "Machine Learning 8(3–4):293–321.",
      "citeRegEx" : "Lin.,? 1992",
      "shortCiteRegEx" : "Lin.",
      "year" : 1992
    }, {
      "title" : "Humanlevel control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. A Rusu", "J. Veness", "M. G Bellemare", "A. Graves", "M. Riedmiller", "A. K Fidjeland", "G. Ostrovski" ],
      "venue" : "Nature",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Language understanding for text-based games using deep reinforcement learning",
      "author" : [ "K. Narasimhan", "T. Kulkarni", "R. Barzilay." ],
      "venue" : "Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1–11.",
      "citeRegEx" : "Narasimhan et al\\.,? 2015",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving information extraction by acquiring external evidence with reinforcement learning",
      "author" : [ "K. Narasimhan", "A. Yala", "R. Barzilay." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Narasimhan et al\\.,? 2016",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-end goal-driven web navigation",
      "author" : [ "R. Nogueira", "K. Cho." ],
      "venue" : "Advances in Neural Information Processing Systems 29. pages 1903–1911.",
      "citeRegEx" : "Nogueira and Cho.,? 2016",
      "shortCiteRegEx" : "Nogueira and Cho.",
      "year" : 2016
    }, {
      "title" : "Introduction to modern information retrieval",
      "author" : [ "G. Salton", "M. J McGill." ],
      "venue" : "McGraw-Hill, Inc.",
      "citeRegEx" : "Salton and McGill.,? 1986",
      "shortCiteRegEx" : "Salton and McGill.",
      "year" : 1986
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search",
      "author" : [ "D. Silver", "A. Huang", "C. J Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus." ],
      "venue" : "Advances in neural information processing systems. pages 2440–2448.",
      "citeRegEx" : "Sukhbaatar et al\\.,? 2015",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep reinforcement learning with double Q-learning",
      "author" : [ "H. Van Hasselt", "A. Guez", "D. Silver." ],
      "venue" : "CoRR, abs/1509.06461 .",
      "citeRegEx" : "Hasselt et al\\.,? 2015",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2015
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "O. Vinyals", "Ł. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2773–2781.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Q-learning",
      "author" : [ "C. JCH Watkins", "P. Dayan." ],
      "venue" : "Machine learning 8(3-4):279–292.",
      "citeRegEx" : "Watkins and Dayan.,? 1992",
      "shortCiteRegEx" : "Watkins and Dayan.",
      "year" : 1992
    }, {
      "title" : "A network-based end-to-end trainable task-oriented dialogue system",
      "author" : [ "T.-H. Wen", "M. Gasic", "N. Mrksic", "L. M Rojas-Barahona", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young." ],
      "venue" : "arXiv preprint arXiv:1604.04562 .",
      "citeRegEx" : "Wen et al\\.,? 2016",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "Memory networks",
      "author" : [ "J. Weston", "S. Chopra", "A. Bordes." ],
      "venue" : "arXiv preprint arXiv:1410.3916 .",
      "citeRegEx" : "Weston et al\\.,? 2014",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2014
    }, {
      "title" : "Open information extraction using wikipedia",
      "author" : [ "F. Wu", "D. S Weld." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 118–127.",
      "citeRegEx" : "Wu and Weld.,? 2010",
      "shortCiteRegEx" : "Wu and Weld.",
      "year" : 2010
    }, {
      "title" : "Docchat: An information retrieval approach for chatbot engines using unstructured documents",
      "author" : [ "Z. Yan", "N. Duan", "J. Bao", "P. Chen", "M. Zhou", "Z. Li", "J. Zhou." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Yan et al\\.,? 2016",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2016
    }, {
      "title" : "Structured use of external knowledge for eventbased open domain question answering",
      "author" : [ "H. Yang", "T-S Chua", "S. Wang", "C-K Koh." ],
      "venue" : "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in infor-",
      "citeRegEx" : "Yang et al\\.,? 2003",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Recently, several tasks that involve states and actions described by natural language have been studied, such as text-based games (Narasimhan et al., 2015; He et al., 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al.",
      "startOffset" : 130,
      "endOffset" : 173
    }, {
      "referenceID" : 12,
      "context" : "Recently, several tasks that involve states and actions described by natural language have been studied, such as text-based games (Narasimhan et al., 2015; He et al., 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al.",
      "startOffset" : 130,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : ", 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al.",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : ", 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al., 2016), Reddit popularity prediction and tracking (He et al.",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : ", 2016b), and human-computer dialogue systems (Wen et al., 2016; Li et al., 2016).",
      "startOffset" : 46,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : ", 2016b), and human-computer dialogue systems (Wen et al., 2016; Li et al., 2016).",
      "startOffset" : 46,
      "endOffset" : 81
    }, {
      "referenceID" : 32,
      "context" : "External knowledge – both general and domainspecific – has been shown to be useful in many natural language tasks, such as in question answering (Yang et al., 2003; Katz et al., 2005; Lin, 2002),",
      "startOffset" : 145,
      "endOffset" : 194
    }, {
      "referenceID" : 14,
      "context" : "External knowledge – both general and domainspecific – has been shown to be useful in many natural language tasks, such as in question answering (Yang et al., 2003; Katz et al., 2005; Lin, 2002),",
      "startOffset" : 145,
      "endOffset" : 194
    }, {
      "referenceID" : 16,
      "context" : "External knowledge – both general and domainspecific – has been shown to be useful in many natural language tasks, such as in question answering (Yang et al., 2003; Katz et al., 2005; Lin, 2002),",
      "startOffset" : 145,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "information extraction (Agichtein and Gravano, 2000; Etzioni et al., 2011; Wu and Weld, 2010), computer games (Branavan et al.",
      "startOffset" : 23,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "information extraction (Agichtein and Gravano, 2000; Etzioni et al., 2011; Wu and Weld, 2010), computer games (Branavan et al.",
      "startOffset" : 23,
      "endOffset" : 93
    }, {
      "referenceID" : 30,
      "context" : "information extraction (Agichtein and Gravano, 2000; Etzioni et al., 2011; Wu and Weld, 2010), computer games (Branavan et al.",
      "startOffset" : 23,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : ", 2011; Wu and Weld, 2010), computer games (Branavan et al., 2012), and dialog systems (Ammicht et al.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : ", 2012), and dialog systems (Ammicht et al., 1999; Yan et al., 2016).",
      "startOffset" : 28,
      "endOffset" : 68
    }, {
      "referenceID" : 31,
      "context" : ", 2012), and dialog systems (Ammicht et al., 1999; Yan et al., 2016).",
      "startOffset" : 28,
      "endOffset" : 68
    }, {
      "referenceID" : 18,
      "context" : "Atari games (Mnih et al., 2015) and the game of Go (Silver et al.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : ", 2015) and the game of Go (Silver et al., 2016).",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "Our study is conducted on the task of Reddit popularity prediction proposed in He et al. (2016b), which is a sequential decision-making problem based on a large-scale real-world natural language data set.",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "In He et al. (2016b), two deep Q-learning architectures are proposed, both with separate networks for the state and action spaces yielding embeddings hs and ha, respectively.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "In He et al. (2016b), two deep Q-learning architectures are proposed, both with separate networks for the state and action spaces yielding embeddings hs and ha, respectively. Those embeddings are combined with a general interaction function g(·) to approximate the Q-values, Q(st, at) = g ( hs, h i a ) , as in He et al. (2016a), where the approach of using separate networks for natural language state and action spaces is termed a Deep Reinforcement Relevance Network (DRRN).",
      "startOffset" : 3,
      "endOffset" : 329
    }, {
      "referenceID" : 18,
      "context" : "To control agents directly given high-dimensional sensory inputs, a Deep Q-Network (Mnih et al., 2015) has been proposed and shown high capacity and scalability for handling a large state space.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "Another stream of work in recent deep learning research is the attention mechanism (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Vinyals et al., 2015), where a probability distribution is computed to pay attention to certain parts of a collec-",
      "startOffset" : 83,
      "endOffset" : 153
    }, {
      "referenceID" : 24,
      "context" : "Another stream of work in recent deep learning research is the attention mechanism (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Vinyals et al., 2015), where a probability distribution is computed to pay attention to certain parts of a collec-",
      "startOffset" : 83,
      "endOffset" : 153
    }, {
      "referenceID" : 26,
      "context" : "Another stream of work in recent deep learning research is the attention mechanism (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Vinyals et al., 2015), where a probability distribution is computed to pay attention to certain parts of a collec-",
      "startOffset" : 83,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "Narasimhan et al. (2016) presents a framework of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 18,
      "context" : "Narasimhan et al. (2016) presents a framework of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. In task-oriented human-computer dialogue interactions, Wen et al. (2016) introduce a neural network-based trainable dialogue system with a database operator module.",
      "startOffset" : 0,
      "endOffset" : 255
    }, {
      "referenceID" : 6,
      "context" : "Dhingra et al. (2016) proposed a dialogue agent that provides users with an entity from a knowledge base by",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "In question answering, knowledge representation and reasoning also plays a central role (Ferrucci et al., 2010; Boyd-Graber et al., 2012).",
      "startOffset" : 88,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "In question answering, knowledge representation and reasoning also plays a central role (Ferrucci et al., 2010; Boyd-Graber et al., 2012).",
      "startOffset" : 88,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "To address overestimations of action values, double Q-learning (Hasselt, 2010; Van Hasselt et al., 2015) has been proposed and it leads to better performance gains on several Atari games.",
      "startOffset" : 63,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "Dulac-Arnold et al. (2016) present a policy architecture that works efficiently with a large number of actions.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "• Semantic similarity: we use the standard tf-idf (term-frequency inverse-documentfrequency) (Salton and McGill, 1986)",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "Q0 is the simple DRRN introduced in He et al. (2016a).",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "We carry out experiments on the task of predicting popular discussion threads on Reddit, as proposed by He et al. (2016b). Specifically, we conduct experiments on data from 5 subreddits including askscience, askmen, todayilearned,",
      "startOffset" : 104,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "use experience replay (Lin, 1992) and the memory size is set to 10,000.",
      "startOffset" : 22,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "Unlike in Fang et al. (2016), the summed karma scores do not follow a Zipfian distribution, so we do not use quantization or any nonlinear transformation.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "The first baseline, following the method in He et al. (2016b), uses a random subsampling approach to obtain Bt (with m = 10) and takes the max over them using DRRN-BiLSTM.",
      "startOffset" : 44,
      "endOffset" : 62
    } ],
    "year" : 2017,
    "abstractText" : "This paper addresses the problem of predicting popularity of comments in an online discussion forum using reinforcement learning, particularly addressing two challenges that arise from having natural language state and action spaces. First, the state representation, which characterizes the history of comments tracked in a discussion at a particular point, is augmented to incorporate the global context represented by discussions on world events available in an external knowledge source. Second, a two-stage Q-learning framework is introduced, making it feasible to search the combinatorial action space while also accounting for redundancy among sub-actions. We experiment with five Reddit communities, showing that the two methods improve over previous reported results on this task.",
    "creator" : "LaTeX with hyperref package"
  }
}