{
  "name" : "1602.06797.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-supervised Clustering for Short Text via Deep Representation Learning",
    "authors" : [ "Zhiguo Wang" ],
    "emails" : [ "abei}@us.ibm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nText clustering is a fundamental problem in text mining and information retrieval. Its task is to group similar texts together such that texts within a cluster are more similar to texts in other clusters. Usually, a text is represented as a bag-of-words or term frequency-inverse document frequency (TFIDF) vector, and then the k-means algorithm (MacQueen, 1967) is performed to partition a set of texts into homogeneous groups.\nHowever, when dealing with short texts, the characteristics of short text and clustering task raise several issues for the conventional unsupervised clustering algorithms. First, the number of uniqe words\nin each short text is small, as a result, the lexcical sparsity issue usually leads to poor clustering quality (Dhillon and Guan, 2003). Second, for a specific short text clustering task, we have prior knowledge or paticular intenstions before clustering, while fully unsupervised approaches may learn some classes the other way around. Take the sentences in Table 1 for example, those sentences can be clustered into different partitions based on different intentions: apple {a, b, c} and orange {d, e, f} with a fruit type intension, or what-question {a, d}, when-question {b, e}, and yes/no-question cluster {c, f} with a question type intension.\nTo address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia (Banerjee et al., 2007) or an ontology (Fodeh et al., 2011). But this approach requires the annotated knowlege, which is also language dependent. So the other direction, which directly encode texts into distributed vectors with neural networks (Hinton and Salakhutdinov, 2006; Xu et al., 2015), becomes more interesing. To tackle the second problem, semi-supervised approaches (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013)) have gained significant popularity in the past decades. Our question is can\nar X\niv :1\n60 2.\n06 79\n7v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\nwe have a unified model to integrate netural networks into the semi-supervied framework?\nIn this paper, we propose a unified framework for the short text clustering task. We employ a deep neural network model to represent short sentences, and integrate it into a semi-supervised algorithm. Concretely, we extend the objective in the classical unsupervised k-means algorithm by adding a penalty term from labeled data. Thus, the new objective covers three key groups of parameters: centroids of clusters, the cluster assignment for each text, and the parameters within deep neural networks. In the training procedure, we start from random initialization of centroids and neural networks, and then optimize the objective iteratively through three steps until converge:\n(1) assign each short text to its nearest centroid based on its representation from the current neural networks;\n(2) re-estimate cluster centroids based on cluster assignments from step (1);\n(3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.\nExperimental results on four different datasets show that our method achieves significant improvements over several other text clustering methods.\nIn following parts, we first describe our neural network models for text representaion (Section 2). Then we introduce our semi-supervised clustering method and the learning algorithm (Section 3). Finally, we evaluate our method on four different datasets (Section 4).\n2 Representation Learning for Short Texts\nWe represent each word with a dense vector w, so that a short text s is first represented as a matrix S = [w1, ..., w|s|], which is a concatenation of all vectors of w in s, |s| is the length of s. Then we design two different types of neural networks to ingest the word vector sequence S: the convolutional neural networks (CNN) and the long short-term memory (LSTM). More formally, we define the presentation function as x = f(s), where x is the represent vector\nof the text s. We test two encoding functions (CNN and LSTM) in our experiments.\nInspired from Kim (2014), our CNN model views the sequence of word vectors as a matrix, and applies two sequential operations: convolution and maxpooling. Then, a fully connected layer is employed to convert the final representation vector into a fixed size. Figure 1 gives the diagram of the CNN model. In the convolution operation, we define a list of filters {wo}, where the shape of each filter is d × h, d is the dimension of word vectors and h is the window size. Each filter is applied to a patch (a window size h of vectors) of S, and generates a feature. We apply this filter to all possible patches in S, and produce a series of features. The number of features depends on the shape of the filter wo and the length of the input short text. To deal with variable feature size, we perform a max-pooling operation over all the features to select the maximum value. Therefore, after the two operations, each filter generates only one feature. We define several filters by varying the window size and the initial values. Thus, a vector of features is captured after the max-pooling operation, and the feature dimension is equal to the number of filters.\nFigure 2 gives the diagram of our LSTM model. We implement the standard LSTM block described in Graves (2012). Each word vector is fed into the\nLSTM model sequentially, and the mean of the hidden states over the entire sentence is taken as the final representation vector.\n3 Semi-supervised Clustering for Short Texts\n3.1 Revisiting K-means Clustering Given a set of texts {s1, s2, ..., sN}, we represent them as a set of data points {x1, x2, ..., xN}, where xi can be a bag-of-words or TF-IDF vector in traditional approaches, or a dense vector in Section 2. The task of text clustering is to partition the data set into some numberK of clusters, such that the sum of the squared distance of each data point to its closest cluster centroid is minimized. For each data point xn, we define a set of binary variables rnk ∈ {0, 1}, where k ∈ {1, ...,K} describing which of the K clusters xn is assigned to. So that if xn is assigned to cluster k, then rnk = 1, and rnj = 0 for j 6= k. Let’s define µk as the centroid of the k-th cluster. We can then formulate the objective function as\nJunsup = N∑ n=1 K∑ k=1 rnk‖xn − µk‖2 (1)\nOur goal is the find the values of {rnk} and {µk} so as to minimize Junsup.\nThe k-means algorithm optimizes Junsup through the gradient descent approach, and results in an iterative procedure (Bishop, 2006). Each iteration involves two steps: E-step and M-step. In the Estep, the algorithm minimizes Junsup with respect to {rnk} by keeping {µk} fixed. Junsup is a linear function for {rnk}, so we can optimize for each data point separately by simply assigning the n-th data point to the closest cluster centroid. In the M-step, the algorithm minimizes Junsup with respect to {µk}\nby keeping {rnk} fixed. Junsup is a quadratic function of {µk}, and it can be minimized by setting its derivative with respect to {µk} to zero.\n∂Junsup ∂µk\n= 2 N∑\nn=1\nrnk(xn − µk) = 0 (2)\nThen, we can easily solve {µk} as\nµk = ∑N n=1 rnkxn∑N n=1 rnk\n(3)\nIn other words, µk is equal to the mean of all the data points assigned to cluster k.\n3.2 Semi-supervised K-means with Neural Networks\nThe classical k-means algorithm only uses unlabeled data, and solves the clustering problem under the unsupervised learning framework. As already mentioned, the clustering results may not be consistent to our intention. In order to acquire useful clustering results, some supervised information should be introduced into the learning procedure. To this end, we employ a small amount of labeled data to guide the clustering process.\nFollowing Section 2, we represent each text s as a dense vector x via neural networks f(s). Instead of training the text representation model separately, we integrate the training process into the k-means algorithm, so that both the labeled data and the unlabeled data can be used for representation learning and text clustering. Let us denote the labeled data set as {(s1, y1), (s2, y2), ..., (sL, yL)}, and the unlabeled data set as {sL+1, sL+2, ..., sN}, where yi is the given label for si. We then define the objective function as:\nJsemi = α N∑\nn=1 K∑ k=1 rnk‖f(sn)− µk‖2\n+ (1− α) L∑\nn=1 {‖f(sn)− µgn‖2+∑ j 6=gn [l + ‖f(sn)− µgn‖2 − ‖f(sn)− µj‖2]+}\n(4)\nThe objective function contains two terms. The first term is adapted from the unsupervised k-means algorithm in Eq. (1), and the second term is defined to encourage labeled data being clustered in correlation with the given labels. α ∈ [0, 1] is used to tune the importance of unlabeled data. The second term contains two parts. The first part penalizes large distance between each labeled instance and its correct cluster centroid, where gn = G(yn) is the cluster ID mapped from the given label yn, and the mapping function G(·) is implemented with the Hungarian algorithm (Munkres, 1957). The second part is denoted as a hinge loss with a margin l, where [x]+ = max(x, 0). This part incurs some loss if the distance to the correct centroid is not shorter (by the margin l) than distances to any of incorrect cluster centroids.\nThere are three groups of parameters in Jsemi: the cluster assignment of each text {rnk}, the cluster centroids {µk}, and the parameters within the neural network model f(·). Our goal is the find the values of {rnk}, {µk} and parameters in f(·), so as to minimize Jsemi. Inspired from the k-means algorithm, we design an algorithm to successively minimize Jsemi with respect to {rnk}, {µk}, and parameters in f(·). Table 2 gives the corresponding pseudocode. First, we initialize the cluster centroids {µk} with the k-means++ strategy (Arthur and Vassilvitskii, 2007), and randomly initialize all the parameters in the neural network model. Then, the algorithm iteratively goes through three steps (assign cluster, estimate centroid, and update parameter) until Jsemi converges.\nThe assign cluster step minimizes Jsemi with respect to {rnk} by keeping f(·) and {µk} fixed. Its goal is to assign a cluster ID for each data point. We can see that the second term in Eq. (4) has no relation with {rnk}. Thus, we only need to minimize the first term by assigning each text to its nearest cluster centroid, which is identical to the E-step in the\nk-means algorithm. In this step, we also calculate the mappings between the given labels {yi} and the cluster IDs (with the Hungarian algorithm) based on cluster assignments of all labeled data.\nThe estimate centroid step minimizes Jsemi with respect to {µk} by keeping {rnk} and f(·) fixed, which corresponds to the M-step in the k-means algorithm. It aims to estimate the cluster centroids {µk} based on the cluster assignments {rnk} from the assign cluster step. The second term in Eq. (4) makes each labeled instance involved in the estimating process of cluster centroids. By solving ∂Jsemi/∂µk = 0, we get\nµk =\n∑N n=1 αrnkf(sn) + ∑L n=1wnkf(sn)∑N\nn=1 αrnk + ∑L n=1wnk (5)\nwnk = (1− α)(I ′ nk + ∑ j 6=gn I ′′ nkj − ∑ j 6=gn I ′′′ nkj) I ′ nk = δ(k, gn) I ′′ nkj = δ(k, j) · δ ′ nj I ′′′ nkj = (1− δ(k, j)) · δ ′ nj δ ′ nj = δ(l + ‖f(sn)− µgn‖2 − ‖f(sn)− µj‖2 > 0)\n(6)\nwhere δ(x1, x2)=1 if x1 is equal to x2, otherwise δ(x1, x2)=0, and δ(x)=1 if x is true, otherwise δ(x)=0. The first term in the numerator of Eq. (5) is the contributions from all data points, and αrnk is the weight of sn for µk. The second term is acquired from labeled data, and wnk is the weight of a labeled instance sn for µk.\nThe update parameter step minimizes Jsemi with respect to f(·) by keeping {rnk} and {µk} fixed, which has no counterpart in the k-means algorithm. The main goal is to update parameters for the text representation model. We take Jsemi as the loss function, and train neural networks with the Adam algorithm (Kingma and Ba, 2014).\n4 Experiment\n4.1 Experimental Setting\nWe evaluate our method on four short text datasets. (1) question type is the TREC question dataset (Li and Roth, 2002), where all the questions are classified into 6 categories: abbreviation, description, entity, human, location and numeric. (2) ag news dataset contains short texts extracted from the AG’s news corpus, where all the texts are classified into 4 categories: World, Sports, Business, and Sci/Tech (Zhang and LeCun, 2015). (3) dbpedia is the DBpedia ontology dataset, which is constructed by picking 14 non-overlapping classes from DBpedia 2014 (Lehmann et al., 2014). (4) yahoo answer is the 10 topics classification dataset extracted from Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset by Zhang and LeCun (2015). We use all the 5,952 questions for the question type dataset. But the other three datasets contain too many instances (e.g. 1,400,000 instances in yahoo answer). Running clustering experiments on such a large dataset is quite inefficient. Following the same solution in (Xu et al., 2015), we randomly choose 1,000 samples for each classes individually for the other three datasets. Within each dataset, we randomly sample 10% of the instances as labeled data, and evaluate the performance on the remaining 90% instances. Table 3 summarizes the statistics of these datasets.\nIn all experiments, we set the size of word vector dimension as d=300 1, and pre-train the word vectors with the word2vec toolkit (Mikolov et al., 2013) on the English Gigaword (LDC2011T07). The number of clusters is set to be the same number of labels in the dataset. The clustering performance is eval-\n1We tuned different dimensions for word vectors. When the size is small (50 or 100), performance drops significantly. When the size is larger (300, 500 or 1000), the curve flattens out. To make our model more efficient, we fixed it as 300.\nnum_dim  \n  \n   alpha     \n     \n0 0.05\n0.1 0.15\n0.2 0.25\n0.3 0.35\n0.4 0.45\n0.5\n0.00001 0.0001 0.001 0.01 0.1\nA M\nI\nCNN\nLSTM\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1\nA M\nI\nCNN\nLSTM\nuated with two metrics: Adjusted Mutual Information (AMI) (Vinh et al., 2009) and accuracy (ACC) (Amigó et al., 2009). In order to show the statistical significance, the performance of each experiment is the average of 10 trials.\n4.2 Model Properties\nThere are several hyper-parameters in our model, e.g., the output dimension of the text representation models, and the α in Eq. (4). The choice of these hyper-parameters may affect the final performance. In this subsection, we present some experiments to demonstrate the properties of our model, and find a good configuration that we use to evaluate our final model. All the experiments in this subsection were performed on the question type dataset.\nFirst, we evaluated the effectiveness of the output dimension in text representation models. We switched the dimension size among {50, 100, 300, 500, 1000}, and fixed the other options as: α = 0.5, the filter types in the CNN model including {unigram, bigram, trigram} and 500 filters for each type. Figure 3 presents the AMIs from both CNN and LSTM models. We found that 100 is the best output dimension for both CNN and LSTM models. Therefore, we set the output dimension as 100 in the following experiments.\nSecond, we studied the effect of α in Eq. (4), which tunes the importance of unlabeled data. We varied α among {0.00001, 0.0001, 0.001, 0.01, 0.1}, and remain the other options as the last experiment. Figure 4 shows the AMIs from both CNN and LSTM models. We found that the clustering performance is not good when using a very small α.\n      num_dim     \n  \nalpha  \n  \n     \n0.2\n0.25\n0.3 0.35 0.4 0.45\n50 100 300 500 1000\nA M I\nCNN LSTM\n0 0.05 0.1\n0.15 0.2\n0.25 0.3\n0.35 0.4\n0.45 0.5\n0.00001 0.0001 0.001 0.01 0.1\nA M\nI\nCNN\nLSTM\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1\nA M\nI\nCNN\nLSTM\nFigure 4: Influence of unlabeled data, where the x-axis is α in Eq. (4).\n      num_dim     \n  \nalpha  \n  \n     \n0.2\n0.25\n0.3\n0.35 0.4 0.45\n50 100 300 500 1000\nA M I\nCNN LSTM\n0 0.05\n0.1 0.15\n0.2 0.25\n0.3 0.35\n0.4 0.45\n0.5\n0.00001 0.0001 0.001 0.01 0.1\nA M\nI\nCNN\nLSTM\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1\nA M\nI\nCNN\nLSTM\nFigure 5: Influence of labeled data, where the x-axis is the ratio of data with given labels.\nBy increasing the value of α, we acquired progressive improvements, and reached to the peak point at α=0.01. After that, the performance dropped. Therefore, we choose α=0.01 in the following experiments. This results also indicate that the unlabeled data are useful for the text representation learning process.\nThird, we tested the influence of the size of labeled data. We tuned the ratio of labeled instances from the whole dataset among [1%, 10%], and kept the other configurations as the previous experiment. The AMIs are shown in Figure 5. We can see that the more labeled data we use, the better performance we get. Therefore, the labeled data are quite useful for the clustering process.\nFourth, we checked the effect of the pre-training strategy for our models. We added a softmax layer on top of our CNN and LSTM models, where the size of the output layer is equal to the number of\nratio     \npretrian  \nlabels in the dataset. We then trained the model through the classification task using all labeled data. After this process, we removed the top layer, and used the remaining parameters to initialize our CNN and LSTM models. The performance for our models with and without pre-training strategy are given in Figure 6. We can see that the pre-training strategy is quite effective for our models. Therefore, we use the pre-training strategy in the following experiments.\n4.3 Comparing with other Models In this subsection, we compared our method with some representative systems. We implemented a series of clustering systems. All of these systems are based on the k-means algorithm, but they represent short texts differently:\nbow represents each text as a bag-of-words vector.\ntf-idf represents each text as a TF-IDF vector.\naverage-vec represents each text with the average of all word vectors within the text.\nmetric-learn-bow employs the metric learning method proposed by Weinberger et al. (2005), and learns to project a bag-of-words vector into a 300-dimensional vector based on labeled data.\nmetric-learn-idf uses the same metric learning method, and learns to map a TF-IDF vector\ninto a 300-dimensional vector based on labeled data.\nmetric-learn-ave-vec also uses the metric learning method, and learns to project an averaged word vector into a 100-dimensional vector based on labeled data.\nWe designed two classifiers (cnn-classifier and lstm-classifier) by adding a softmax layer on top of our CNN and LSTM models. We trained these two classifiers with labeled data, and utilized them to predict labels for unlabeled data. We also built two text representation models (“cnn-represent.” and “lstm-represent.”) by setting parameters of our CNN and LSTM models with the corresponding parameters in cnn-classifier and lstm-classifier. Then, we used them to represent short texts into vectors, and applied the k-means algorithm for clustering.\nTable 4 summarizes the results of all systems on each dataset, where “semi-cnn” is our semisupervised clustering algorithm with the CNN model, and “semi-lstm” is our semi-supervised clustering algorithm with the LSTM model. We grouped all the systems into three categories: unsupervised (Unsup.), supervised (Sup.), and semi-supervised (Semisup.) 2. We found that the supervised systems worked much better than the unsupervised counterparts, which implies that the small amount of labeled data is necessary for better performance. We also noticed that within the supervised systems, the systems using deep learning (CNN or LSTM) models worked better than the systems using metric learning method, which shows the power of deep learning models for short text modeling. Our “semi-cnn” system got the best performance on almost all the datasets.\nFigure 7 visualizes clustering results on the question type dataset from four representative systems. In Figure 7(a), clusters severely overlap with each other. When using the CNN sentence representation model, we can clearly identify all clusters in Figure 7(b), but the boundaries between clusters are\n2All clustering systems are based on the same number of instances (total# in Table 3). For the semi-supervised and supervised systems, the labels for 1% of the instances are given (labeled# in Table 3). And the evaluation was conducted only on the unlabeled portion.\nstill obscure. The clustering results from our semisupervised clustering algorithm are given in Figure 7(c) and Figure 7(d). We can see that the boundaries between clusters become much clearer. Therefore, our algorithm is very effective for short text clustering.\n5 Related Work\nExisting semi-supervised clustering methods fall into two categories: constraint-based and representation-based. In constraint-based methods (Davidson and Basu, 2007), some labeled information is used to constrain the clustering process. In representation-based methods (Bair, 2013), a representation model is first trained to satisfy the labeled information, and all data points are clustered based on representations from the representation model. Bilenko et al. (2004) proposed to integrate there two methods into a unified framework, which shares the same idea of our proposed method. However, they only employed the metric learning model for representation learning, which is a linear projection. Whereas, our method utilized deep learning models to learn representations in a more flexible non-linear space. Xu et al. (2015) also employed deep learning models for short text clustering. However, their method separated the representation learning process from the clustering process, so it belongs to the representation-based method. Whereas, our method combined the representation learning process and the clustering process together, and utilized both labeled data and unlabeled data for representation learning and clustering.\n6 Conclusion\nIn this paper, we proposed a semi-supervised clustering algorithm for short texts. We utilized deep learning models to learn representations for short texts, and employed a small amount of labeled data to specify our intention for clustering. We integrated the representation learning process and the clustering process into a unified framework, so that both of the two processes get some benefits from labeled data and unlabeled data. Experimental results on four datasets show that our method is more effective than other competitors.\nReferences [Amigó et al.2009] Enrique Amigó, Julio Gonzalo, Javier\nArtiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information retrieval, 12(4):461–486. [Arthur and Vassilvitskii2007] David Arthur and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027–1035. Society for Industrial and Applied Mathematics. [Bair2013] Eric Bair. 2013. Semi-supervised clustering methods. Wiley Interdisciplinary Reviews: Computational Statistics, 5(5):349–361. [Banerjee et al.2007] Somnath Banerjee, Krishnan Ramanathan, and Ajay Gupta. 2007. Clustering short texts using wikipedia. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 787–788. ACM. [Bilenko et al.2004] Mikhail Bilenko, Sugato Basu, and Raymond J Mooney. 2004. Integrating constraints and metric learning in semi-supervised clustering. In Proceedings of the twenty-first international conference on Machine learning, page 11. ACM. [Bishop2006] Christopher M Bishop. 2006. Pattern recognition and machine learning. springer. [Davidson and Basu2007] Ian Davidson and Sugato Basu. 2007. A survey of clustering with instance level constraints. ACM Transactions on Knowledge Discovery from Data, 1:1–41. [Dhillon and Guan2003] Inderjit S. Dhillon and Yuqiang Guan. 2003. Information theoretic clustering of sparse co-occurrence data. pages 517–520. IEEE Computer Society. [Fodeh et al.2011] Samah Fodeh, Bill Punch, and PangNing Tan. 2011. On ontology-driven document clustering using core semantic features. Knowledge and information systems, 28(2):395–421. [Graves2012] Alex Graves. 2012. Supervised sequence labelling with recurrent neural networks, volume 385. Springer. [Hinton and Salakhutdinov2006] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507. [Kim2014] Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1746–1751. [Kingma and Ba2014] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n[Lehmann et al.2014] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, Sören Auer, et al. 2014. Dbpediaa large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web Journal, 5:1–29. [Li and Roth2002] Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguisticsVolume 1, pages 1–7. Association for Computational Linguistics. [MacQueen1967] James MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281–297. Oakland, CA, USA. [Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. [Munkres1957] James Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society for Industrial and Applied Mathematics, 5(1):32–38. [Vinh et al.2009] Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2009. Information theoretic measures for clusterings comparison: is a correction for chance necessary? In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1073–1080. ACM. [Weinberger et al.2005] Kilian Q Weinberger, John Blitzer, and Lawrence K Saul. 2005. Distance metric learning for large margin nearest neighbor classification. In Advances in neural information processing systems, pages 1473–1480. [Xu et al.2015] Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun Zhao, Fangyuan Wang, and Hongwei Hao. 2015. Short text clustering via convolutional neural networks. In Proceedings of NAACL-HLT, pages 62– 69. [Zhang and LeCun2015] Xiang Zhang and Yann LeCun. 2015. Text understanding from scratch. arXiv preprint arXiv:1502.01710."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the kmeans clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) reestimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed. Experimental results on four datasets show that our method works significantly better than several other text clustering methods. 1 Introduction Text clustering is a fundamental problem in text mining and information retrieval. Its task is to group similar texts together such that texts within a cluster are more similar to texts in other clusters. Usually, a text is represented as a bag-of-words or term frequency-inverse document frequency (TFIDF) vector, and then the k-means algorithm (MacQueen, 1967) is performed to partition a set of texts into homogeneous groups. However, when dealing with short texts, the characteristics of short text and clustering task raise several issues for the conventional unsupervised clustering algorithms. First, the number of uniqe words (a) What’s the color of apples? (b) When will this apple be ripe? (c) Do you like apples? (d) What’s the color of oranges? (e) When will this orange be ripe? (f) Do you like oranges? Table 1: Examples for short text clustering. in each short text is small, as a result, the lexcical sparsity issue usually leads to poor clustering quality (Dhillon and Guan, 2003). Second, for a specific short text clustering task, we have prior knowledge or paticular intenstions before clustering, while fully unsupervised approaches may learn some classes the other way around. Take the sentences in Table 1 for example, those sentences can be clustered into different partitions based on different intentions: apple {a, b, c} and orange {d, e, f} with a fruit type intension, or what-question {a, d}, when-question {b, e}, and yes/no-question cluster {c, f} with a question type intension. To address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia (Banerjee et al., 2007) or an ontology (Fodeh et al., 2011). But this approach requires the annotated knowlege, which is also language dependent. So the other direction, which directly encode texts into distributed vectors with neural networks (Hinton and Salakhutdinov, 2006; Xu et al., 2015), becomes more interesing. To tackle the second problem, semi-supervised approaches (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013)) have gained significant popularity in the past decades. Our question is can ar X iv :1 60 2. 06 79 7v 1 [ cs .C L ] 2 2 Fe b 20 16 we have a unified model to integrate netural networks into the semi-supervied framework? In this paper, we propose a unified framework for the short text clustering task. We employ a deep neural network model to represent short sentences, and integrate it into a semi-supervised algorithm. Concretely, we extend the objective in the classical unsupervised k-means algorithm by adding a penalty term from labeled data. Thus, the new objective covers three key groups of parameters: centroids of clusters, the cluster assignment for each text, and the parameters within deep neural networks. In the training procedure, we start from random initialization of centroids and neural networks, and then optimize the objective iteratively through three steps until converge: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) re-estimate cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed. Experimental results on four different datasets show that our method achieves significant improvements over several other text clustering methods. In following parts, we first describe our neural network models for text representaion (Section 2). Then we introduce our semi-supervised clustering method and the learning algorithm (Section 3). Finally, we evaluate our method on four different datasets (Section 4). 2 Representation Learning for Short Texts We represent each word with a dense vector w, so that a short text s is first represented as a matrix S = [w1, ..., w|s|], which is a concatenation of all vectors of w in s, |s| is the length of s. Then we design two different types of neural networks to ingest the word vector sequence S: the convolutional neural networks (CNN) and the long short-term memory (LSTM). More formally, we define the presentation function as x = f(s), where x is the represent vector ...\t\r  ... ...\t\r  ... convolution operation max-pooling operation fully connected layer Figure 1: CNN for text representation learning. of the text s. We test two encoding functions (CNN and LSTM) in our experiments. Inspired from Kim (2014), our CNN model views the sequence of word vectors as a matrix, and applies two sequential operations: convolution and maxpooling. Then, a fully connected layer is employed to convert the final representation vector into a fixed size. Figure 1 gives the diagram of the CNN model. In the convolution operation, we define a list of filters {wo}, where the shape of each filter is d × h, d is the dimension of word vectors and h is the window size. Each filter is applied to a patch (a window size h of vectors) of S, and generates a feature. We apply this filter to all possible patches in S, and produce a series of features. The number of features depends on the shape of the filter wo and the length of the input short text. To deal with variable feature size, we perform a max-pooling operation over all the features to select the maximum value. Therefore, after the two operations, each filter generates only one feature. We define several filters by varying the window size and the initial values. Thus, a vector of features is captured after the max-pooling operation, and the feature dimension is equal to the number of filters. Figure 2 gives the diagram of our LSTM model. We implement the standard LSTM block described in Graves (2012). Each word vector is fed into the LSTM LSTM LSTM ...... w1 w2 wn Mean Figure 2: LSTM for text representation learning. LSTM model sequentially, and the mean of the hidden states over the entire sentence is taken as the final representation vector. 3 Semi-supervised Clustering for Short Texts 3.1 Revisiting K-means Clustering Given a set of texts {s1, s2, ..., sN}, we represent them as a set of data points {x1, x2, ..., xN}, where xi can be a bag-of-words or TF-IDF vector in traditional approaches, or a dense vector in Section 2. The task of text clustering is to partition the data set into some numberK of clusters, such that the sum of the squared distance of each data point to its closest cluster centroid is minimized. For each data point xn, we define a set of binary variables rnk ∈ {0, 1}, where k ∈ {1, ...,K} describing which of the K clusters xn is assigned to. So that if xn is assigned to cluster k, then rnk = 1, and rnj = 0 for j 6= k. Let’s define μk as the centroid of the k-th cluster. We can then formulate the objective function as Junsup = N ∑ n=1 K ∑ k=1 rnk‖xn − μk‖ (1) Our goal is the find the values of {rnk} and {μk} so as to minimize Junsup. The k-means algorithm optimizes Junsup through the gradient descent approach, and results in an iterative procedure (Bishop, 2006). Each iteration involves two steps: E-step and M-step. In the Estep, the algorithm minimizes Junsup with respect to {rnk} by keeping {μk} fixed. Junsup is a linear function for {rnk}, so we can optimize for each data point separately by simply assigning the n-th data point to the closest cluster centroid. In the M-step, the algorithm minimizes Junsup with respect to {μk} by keeping {rnk} fixed. Junsup is a quadratic function of {μk}, and it can be minimized by setting its derivative with respect to {μk} to zero. ∂Junsup ∂μk = 2 N ∑ n=1 rnk(xn − μk) = 0 (2) Then, we can easily solve {μk} as μk = ∑N n=1 rnkxn ∑N n=1 rnk (3) In other words, μk is equal to the mean of all the data points assigned to cluster k. 3.2 Semi-supervised K-means with Neural Networks The classical k-means algorithm only uses unlabeled data, and solves the clustering problem under the unsupervised learning framework. As already mentioned, the clustering results may not be consistent to our intention. In order to acquire useful clustering results, some supervised information should be introduced into the learning procedure. To this end, we employ a small amount of labeled data to guide the clustering process. Following Section 2, we represent each text s as a dense vector x via neural networks f(s). Instead of training the text representation model separately, we integrate the training process into the k-means algorithm, so that both the labeled data and the unlabeled data can be used for representation learning and text clustering. Let us denote the labeled data set as {(s1, y1), (s2, y2), ..., (sL, yL)}, and the unlabeled data set as {sL+1, sL+2, ..., sN}, where yi is the given label for si. We then define the objective function as: Jsemi = α<lb>N<lb>∑<lb>n=1<lb>K<lb>∑<lb>k=1<lb>rnk‖f(sn)− μk‖<lb>+ (1− α)<lb>L<lb>∑<lb>n=1<lb>{‖f(sn)−<lb>μgn‖+<lb>∑<lb>j<lb>6=gn<lb>[l +<lb>‖f(sn)−<lb>μgn‖ − ‖f(sn)− μj‖]+}<lb>(4) 1. Initialize<lb>{μk} and f(·).<lb>2. assign cluster: Assign each text to its nearest cluster centroid.<lb>3. estimate centroid: Estimate the cluster centroids based on the cluster assignments from step 2.<lb>4. update parameter: Update parameters in neural networks.<lb>5. Repeat step 2 to 4 until convergence.<lb>Table 2: Pseudocode for semi-supervised clustering<lb>The objective function contains two terms. The first<lb>term is adapted from the unsupervised k-means al-<lb>gorithm in Eq. (1), and the second term is defined<lb>to encourage labeled data being clustered in correla-<lb>tion with the given labels. α ∈ [0, 1] is used to tune<lb>the importance of unlabeled data. The second term<lb>contains two parts. The first part penalizes large dis-<lb>tance between each labeled instance and its correct<lb>cluster centroid, where gn = G(yn) is the cluster<lb>ID mapped from the given label yn, and the map-<lb>ping function G(·) is implemented with the Hun-<lb>garian algorithm (Munkres, 1957). The second part<lb>is denoted as a hinge loss with a margin l, where<lb>[x]+ = max(x, 0). This part incurs some loss if the<lb>distance to the correct centroid is not shorter (by the<lb>margin l) than distances to any of incorrect cluster<lb>centroids.<lb>There are three groups of parameters in Jsemi: the<lb>cluster assignment of each text<lb>{rnk}, the cluster<lb>centroids<lb>{μk}, and the parameters within the neural<lb>network model f(·). Our goal is the find the values<lb>of<lb>{rnk}, {μk} and parameters in f(·), so as to min-<lb>imize Jsemi. Inspired from the k-means algorithm,<lb>we design an algorithm to successively minimize<lb>Jsemi with respect to<lb>{rnk}, {μk}, and parameters in<lb>f(·). Table 2 gives the corresponding pseudocode.<lb>First, we initialize the cluster centroids<lb>{μk} with<lb>the k-means++ strategy (Arthur and Vassilvitskii,<lb>2007), and randomly initialize all the parameters in<lb>the neural network model. Then, the algorithm iter-<lb>atively goes through three steps (assign cluster, es-<lb>timate centroid, and update parameter) until Jsemi<lb>converges.<lb>The assign cluster step minimizes Jsemi with re-<lb>spect to<lb>{rnk} by keeping f(·) and<lb>{μk} fixed. Its<lb>goal is to assign a cluster ID for each data point. We<lb>can see that the second term in Eq. (4) has no rela-<lb>tion with<lb>{rnk}. Thus, we only need to minimize the<lb>first term by assigning each text to its nearest clus-<lb>ter centroid, which is identical to the E-step in the<lb>k-means algorithm. In this step, we also calculate<lb>the mappings between the given labels<lb>{yi} and the<lb>cluster IDs (with the Hungarian algorithm) based on<lb>cluster assignments of all labeled data.<lb>The estimate centroid step minimizes Jsemi with<lb>respect to<lb>{μk} by keeping<lb>{rnk} and f(·) fixed,<lb>which corresponds to the M-step in the k-means al-<lb>gorithm. It aims to estimate the cluster centroids<lb>{μk} based on the cluster assignments<lb>{rnk} from<lb>the assign cluster step. The second term in Eq.<lb>(4) makes each labeled instance involved in the es-<lb>timating process of cluster centroids. By solving<lb>∂Jsemi/∂μk = 0, we get<lb>μk =<lb>∑N<lb>n=1 αrnkf(sn) + ∑L<lb>n=1wnkf(sn)<lb>∑N<lb>n=1 αrnk + ∑L<lb>n=1wnk<lb>(5)<lb>wnk = (1− α)(I<lb>′<lb>nk +<lb>∑<lb>j<lb>6=gn<lb>I<lb>′′<lb>nkj −<lb>∑<lb>j<lb>6=gn<lb>I<lb>′′′<lb>nkj)<lb>I<lb>′<lb>nk = δ(k, gn)<lb>I<lb>′′<lb>nkj = δ(k, j) · δ<lb>′<lb>nj<lb>I<lb>′′′<lb>nkj = (1− δ(k, j)) · δ<lb>′<lb>nj<lb>δ<lb>′<lb>nj = δ(l +<lb>‖f(sn)−<lb>μgn‖ − ‖f(sn)− μj‖ > 0)<lb>(6)<lb>where δ(x1, x2)=1 if x1 is equal to x2, otherwise<lb>δ(x1, x2)=0, and δ(x)=1 if x is true, otherwise<lb>δ(x)=0. The first term in the numerator of Eq. (5)<lb>is the contributions from all data points, and αrnk is<lb>the weight of sn for μk. The second term is acquired<lb>from labeled data, and wnk is the weight of a labeled<lb>instance sn for μk.<lb>The update parameter step minimizes Jsemi with<lb>respect to f(·) by keeping<lb>{rnk} and<lb>{μk} fixed,<lb>which has no counterpart in the k-means algorithm.<lb>The main goal is to update parameters for the text<lb>representation model. We take Jsemi as the loss<lb>function, and train neural networks with the Adam<lb>algorithm (Kingma and Ba, 2014). dataset<lb>class# total# labeled#<lb>question type 6 5,953 595<lb>ag news<lb>4 4,000 400<lb>dbpedia<lb>14 14,000 1,400<lb>yahoo answer 10 10,000 1,000<lb>Table 3: Statistics for the short text datasets<lb>4 Experiment<lb>4.1 Experimental Setting<lb>We evaluate our method on four short text datasets.<lb>(1) question type is the TREC question dataset (Li<lb>and Roth, 2002), where all the questions are clas-<lb>sified into 6 categories: abbreviation, description,<lb>entity, human, location and numeric. (2) ag news<lb>dataset contains short texts extracted from the AG’s<lb>news corpus, where all the texts are classified into<lb>4 categories: World, Sports, Business, and Sci/Tech<lb>(Zhang and LeCun, 2015). (3) dbpedia is the DBpe-<lb>dia ontology dataset, which is constructed by pick-<lb>ing 14 non-overlapping classes from DBpedia 2014<lb>(Lehmann et al., 2014). (4) yahoo answer is the<lb>10 topics classification dataset extracted from Ya-<lb>hoo! Answers Comprehensive Questions and An-<lb>swers version 1.0 dataset by Zhang and LeCun<lb>(2015). We use all the 5,952 questions for the ques-<lb>tion type dataset. But the other three datasets con-<lb>tain too many instances (e.g. 1,400,000 instances in<lb>yahoo answer). Running clustering experiments on<lb>such a large dataset is quite inefficient. Following<lb>the same solution in (Xu et al., 2015), we randomly<lb>choose 1,000 samples for each classes individually<lb>for the other three datasets. Within each dataset, we<lb>randomly sample 10% of the instances as labeled<lb>data, and evaluate the performance on the remain-<lb>ing 90% instances. Table 3 summarizes the statistics<lb>of these datasets.<lb>In all experiments, we set the size of word vector<lb>dimension as d=300 1, and pre-train the word vec-<lb>tors with the word2vec toolkit (Mikolov et al., 2013)<lb>on the English Gigaword (LDC2011T07). The num-<lb>ber of clusters is set to be the same number of labels<lb>in the dataset. The clustering performance is eval-<lb>We tuned different dimensions for word vectors. When the<lb>size is small (50 or 100), performance drops significantly. When<lb>the size is larger (300, 500 or 1000), the curve flattens out. To<lb>make our model more efficient, we fixed it as 300.<lb>num_dim",
    "creator" : "LaTeX with hyperref package"
  }
}