{
  "name" : "1510.01717.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "U T\nF II C  D H\nLanguage Segmentation\nAuthor: David A\nSupervisors: Prof. Dr. Caroline S\nDr. Sven N\nAugust 18, 2015"
    }, {
      "heading" : "Erklärung zur Masterarbeit",
      "text" : "Hiermit erkläre ich, dass ich die Masterarbeit selbstständig verfasst und keine anderen als die angegebenenellen und Hilfsmiel benutzt und die aus fremdenellen direkt oder indirekt übernommenen Gedanken als solche kenntlich gemacht habe.\nDie Arbeit habe ich bisher keinem anderen Prüfungsamt in gleicher oder vergleichbarer Form vorgelegt. Sie wurde bisher nicht veröffentlicht.\nDatum Unterschri\ni\nAbstract\nLanguage segmentation consists in finding the boundaries where one language ends and another language begins in a text wrien in more than one language. is is important for all natural language processing tasks.\ne problem can be solved by training language models on language data. However, in the case of low- or no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform beer than supervised methods when it is difficult or impossible to train supervised approaches.\nA special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language.\nI compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words.\ne weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. e results look promising, but there is room for improvement and a more thorough investigation should be undertaken.\nii\nAnowledgements My thanks go to professor Caroline Sporleder for sharing her knowledge with me, for her inspiring ideas and for agreeing to supervise my Bachelor’s and Master’s esis despite her busy schedule. It was also thanks to the topic she suggested for my Bachelor’s esis that I met Jürgen Knauth and later was able to get a research assistant position at the SeNeReKo project, collaborating closely with Jürgen.\nWhich brings me to the next person on the list. I would like to thank Jürgen Knauth for the wonderful collaboration, for his patience, for his contagious enthusiasm, and all the interesting conversations in passing that always lasted longer than intended.\nI would like to thank Stephan Faber for his insightful comments when I couldn’t see the wood for the trees, for his patience and optimism, for pushing me to go further and to persevere.\nI would also like to thank Julian Vaudroz for accompanying me throughout the degree program. We both didn’t know what we were in for when we started, but we persevered and it paid off. It wouldn’t have been the same without you.\nFinally, I would like to thank all the people that volunteered to proofread my thesis and all the people that helped me during the writing of this thesis. Unfortunately, I cannot list everyone. You know who you are!\niii"
    }, {
      "heading" : "List of Figures",
      "text" : "1 Out-of-place metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 Simple text illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3 Initial model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Initial model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 5 Model update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 7 New model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 8 Multiple model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 14 9 Updating relevant model . . . . . . . . . . . . . . . . . . . . . . . . . . 14 10 Multiple model evaluation 2 . . . . . . . . . . . . . . . . . . . . . . . . 14 11 New model creation 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 12 Problematic text sample . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 13 Finding the most similar models . . . . . . . . . . . . . . . . . . . . . . 16 14 Merging most similar models . . . . . . . . . . . . . . . . . . . . . . . . 16 15 Word-Model assignment . . . . . . . . . . . . . . . . . . . . . . . . . . 16 16 Clustering preprocessor . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 17 WEKA: Cluster visualization . . . . . . . . . . . . . . . . . . . . . . . . 28 18 ELKI: Cluster visualization . . . . . . . . . . . . . . . . . . . . . . . . . 29 19 Language model: Distribution 1 . . . . . . . . . . . . . . . . . . . . . . 35 20 Language Model: Distribution 2 . . . . . . . . . . . . . . . . . . . . . . 35 21 Language model: Distribution 3 . . . . . . . . . . . . . . . . . . . . . . 36 22 Alternating language structure . . . . . . . . . . . . . . . . . . . . . . . 54\niv"
    }, {
      "heading" : "List of Tables",
      "text" : "1 Training data: Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2 Unambiguous encoding: distances . . . . . . . . . . . . . . . . . . . . . 27 3 Simplified encoding: distances . . . . . . . . . . . . . . . . . . . . . . . 27 4 N-Gram language model results: Latin script . . . . . . . . . . . . . . . 38 5 N-Gram language model results: Mixed script . . . . . . . . . . . . . . 39 6 N-Gram language model results: Pali data . . . . . . . . . . . . . . . . . 40 7 N-Gram language model results: Twier data . . . . . . . . . . . . . . . 41 8 Textcat results: Latin script . . . . . . . . . . . . . . . . . . . . . . . . . 42 9 Textcat results: Mixed script . . . . . . . . . . . . . . . . . . . . . . . . 43 10 Textcat results: Pali data . . . . . . . . . . . . . . . . . . . . . . . . . . 44 11 Textcat results: Twier data . . . . . . . . . . . . . . . . . . . . . . . . 45 12 Clustering results: Latin script . . . . . . . . . . . . . . . . . . . . . . . 46 13 Clustering results: Mixed script . . . . . . . . . . . . . . . . . . . . . . 47 14 Clustering results: Pali data . . . . . . . . . . . . . . . . . . . . . . . . . 48 15 Clustering results: Twier data . . . . . . . . . . . . . . . . . . . . . . . 49 16 Induction results: Latin script . . . . . . . . . . . . . . . . . . . . . . . . 50 17 Induction results: Mixed script . . . . . . . . . . . . . . . . . . . . . . . 51 18 Induction results: Pali data . . . . . . . . . . . . . . . . . . . . . . . . . 52 19 Induction results: Twier data . . . . . . . . . . . . . . . . . . . . . . . 53 20 ‘Twier 3’: Textcat versus Gold clustering . . . . . . . . . . . . . . . . 58 21 ‘Twier 4’: Textcat versus Gold clustering . . . . . . . . . . . . . . . . 58\nv"
    }, {
      "heading" : "List of Algorithms",
      "text" : "1 N-gram numerical encoding . . . . . . . . . . . . . . . . . . . . . . . . 26 2 Model induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3 Initial model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4 Max model and max score . . . . . . . . . . . . . . . . . . . . . . . . . 32 5 Model merger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6 Distributional Similarity Calculation . . . . . . . . . . . . . . . . . . . . 36\nvi"
    }, {
      "heading" : "Contents",
      "text" : ""
    }, {
      "heading" : "1 Introduction 1",
      "text" : ""
    }, {
      "heading" : "2 Related work 2",
      "text" : "2.1 N-Grams and rank order statistics . . . . . . . . . . . . . . . . . . . . . 2 2.2 N-Grams and maximum likelihood estimator . . . . . . . . . . . . . . . 3 2.3 Trigrams and short words . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.4 N-Grams and clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.5 Inclusion detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.6 Clustering and speech . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.7 Monolingual training data . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.8 Predictive suffix trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3 eory 9 3.1 Supervised language model . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.1.1 N-Gram models . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.1.2 Formal definition . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.1.3 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2 Unsupervised clustering . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.3 Weakly supervised language model induction . . . . . . . . . . . . . . 12"
    }, {
      "heading" : "4 Experimental setup 18",
      "text" : "4.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.2 Supervised language model . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.2.1 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.2.2 Training phase . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.2.3 Application of the approach . . . . . . . . . . . . . . . . . . . . 21 4.2.4 Textcat and language segmentation . . . . . . . . . . . . . . . . 21\n4.3 Unsupervised clustering . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.3.1 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.3.2 Defining features . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.3.3 Mapping features to a common scale . . . . . . . . . . . . . . . 25 4.3.4 e problem of unambiguous encoding . . . . . . . . . . . . . . 26 4.3.5 e clusterer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.3.6 Evaluating clusterings . . . . . . . . . . . . . . . . . . . . . . . 29 4.4 Weakly supervised language model induction . . . . . . . . . . . . . . 31 4.4.1 Distributional similarity . . . . . . . . . . . . . . . . . . . . . . 34 4.4.2 Evaluating results . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.4.3 Estimating the parameters . . . . . . . . . . . . . . . . . . . . . 37\nvii"
    }, {
      "heading" : "5 Results 38",
      "text" : "5.1 N-Gram language model . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5.2 Textcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.3 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.4 Language model induction . . . . . . . . . . . . . . . . . . . . . . . . . 50"
    }, {
      "heading" : "6 Discussion 54",
      "text" : "6.1 N-Gram language models . . . . . . . . . . . . . . . . . . . . . . . . . . 54 6.2 Textcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 6.3 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 6.4 Language model induction . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.5 Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64"
    }, {
      "heading" : "7 Conclusion 65",
      "text" : ""
    }, {
      "heading" : "8 Appendix 72",
      "text" : "8.1 Development data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n8.1.1 Latin script data . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.1.2 Mixed script data . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.1.3 Twier data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.1.4 Pali dictionary data . . . . . . . . . . . . . . . . . . . . . . . . . 73\n8.2 Test data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 8.2.1 Latin script data . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 8.2.2 Mixed script data . . . . . . . . . . . . . . . . . . . . . . . . . . 75 8.2.3 Twier data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 8.2.4 Pali dictionary data . . . . . . . . . . . . . . . . . . . . . . . . . 78 8.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 8.3.1 N-Gram Language Models . . . . . . . . . . . . . . . . . . . . . 80 8.3.2 Textcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 8.3.3 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 8.3.4 Language Model Induction . . . . . . . . . . . . . . . . . . . . . 112\nviii"
    }, {
      "heading" : "1 Introduction",
      "text" : "Language segmentation and identification are important for all natural language processing operations that are language-specific, such as taggers, parsers or machine translation (Jain and Bhat, 2014; Zubiaga et al., 2014). Indeed, using “traditional”monolingual natural language processing components on mixed language data leads to miserable results (Jain and Bhat, 2014). Even if the results are not terrible, language identification and segmentation can improve the overall results. For example, by identifying foreign language inclusions in an otherwise monolingual text, parser accuracy can be increased (Alex et al., 2007).\nOne important point that has to be borne in mind is the difference between language identification and language segmentation. Language identification is concerned with recognizing the language at hand. It is possible to use language identification for language segmentation. Indeed, by identifying the languages in a text, the segmentation is implicitly obtained. Language segmentation on the other hand is only concerned with identifying language boundaries. No claims about the languages involved are made.\nAer giving an overview over related work and different approaches that can be taken for language segmentation, I will present the theory behind supervised methods as well as unsupervised methods. Finally, I will introduce a weakly supervised method for language segmentation that I developed.\nAer the theoretical part, I will present experiments done with the different approaches, comparing their effectiveness on the task of language segmentation on different text types. A special focus will be given to difficult text types, such as short texts, texts containing under-resourced languages or texts containing a lot of abbreviations or other non-standard features.\nA big advantage of unsupervised methods is language independence. If the approach used does not rely on language-specific details, the approach is more flexible as no language resources have to be adapted for the method to work on other languages. ese advantages might be especially useful for under-resourced languages. When there is no or insufficient data available to train a supervised language model, an unsupervised approach might yield beer results.\nAnother advantage is that unsupervised methods do not require prior training. ey are not dependent on training data and thus cannot be skewed by the data. Indeed, supervised approaches that are trained on data are qualitatively tied to their training data; different training data will, in all probability, yield different models.\nis thesis aims at answering the question whether unsupervised language segmentation approaches work beer on difficult text types than supervised language approaches.\n1"
    }, {
      "heading" : "2 Related work",
      "text" : ""
    }, {
      "heading" : "2.1 N-Grams and rank order statistics",
      "text" : "Cavnar and Trenkle (1994) use an n-gram language model for language identification purposes. eir program ‘Textcat’ is intended to classify documents by language. e system calculates n-grams for 1 6 n 6 5 from training data and orders the n-grams according to inverse frequency, i.e. from the most frequent n-grams to the most infrequent n-grams. e numerical frequency data is then discarded and only inherently present.\nDuring training, the program calculates an n-gram profile consisting of these ngram lists for each category (i.e. language to classify).\nNew data is classified by first calculating the n-gram profile and then comparing the profile to existing profiles. e category with the lowest difference score is taken as the category for the document.\ne score they use for classification is called out-of-place metric. For each n-gram in the document n-gram profile, the corresponding n-gram in the category profile is looked up and the absolute difference of ranks is taken as score. e sum is calculated over all n-grams. More formally, the out-of-place metricmoop is calculated as:\nmoop = n∑\ni=1\n(|r(xi, d)− r(xi, c)|) (1)\nWith n the number of n-grams in the document profile, xi the i-th n-gram, r(xi, d) the rank of the i-th n-gram in the document profile, r(xi, c) the rank of the i-th n-gram in the category profile.\nFigure 1 illustrates the out-of-place metric.\nCategory profile Document profile\nScore\n2\nIn figure 1, the document profile has ‘ER’ as most frequent n-gram, at rank 1, followed by ‘ING’ at rank 2, etc. e category profile does not contain the n-gram ‘ER’; in that case, an arbitrary fixed maximum value is assigned. e category profile contains the n-gram ‘ING’ at rank 2, the same rank as in the document profile; the difference is 0. e category profile contains the n-gram ‘AT’ at rank 1, while in the document profile, it occurs at rank 3. e absolute difference is 2. e out-of-place metric consists of the sum of all scores thus calculated.\nCavnar and Trenkle (1994) collected 3713 Usenet texts with a cultural theme in different languages. ey filtered out non-monolingual texts and texts that had no useful content for language classification. In the end, they had 3478 articles ranging from a single line of text to 50 KB of text.\neir results indicated that length had no significant impact on the classification, contrary to what they thought. Also, they found that training the system with 400 n-grams yielded the best result with a precision of 99.8%.\ney also showed that their approach could be used for subject classification of texts in the same language with reasonable precision. is finding indicates that language and domain are linked to a certain degree."
    }, {
      "heading" : "2.2 N-Grams and maximum likelihood estimator",
      "text" : "Dunning (1994) also uses an n-gram language model for language identification purposes. e program calculates n-grams and their frequencies from the training data and estimates the probability P of a given string using the Maximum Likelihood Estimator (MLE) with Laplace add-one smoothing.More formally:\nP (wi|w1, . . . , wi−1) = C(w1, . . . , wi) + 1\nC(w1, . . . , wi−1) + |V | (2)\nwith C(w1, . . . , Ci) the number of times the n-gram w1, . . . , wi occurred, C(w1, . . . , Ci−1) the number of times the (n− 1)-gram w1, . . . , wi−1 occurred and |V | the size of the vocabulary.\nFor a string S, the string is decomposed into n-grams and the log probability lk is calculated as:\nlk = ∑\nw1,...,wk∈S\nC(w1, . . . , wk) logP (wk|w1, . . . , wk−1) (3)\nwhere k is the order of the n-gram (k = n) used. In order to test the system, Dunning (1994) uses a specially constructed test corpus from a bilingual parallel translated English-Spanish corpus containing English and Spanish texts with 10 texts varying from 1000 to 50000 bytes for the training set and 100 texts varying from 10 to 500 bytes for the test set.\n3\ne results indicate that bigram models perform beer for shorter strings and less training data while trigram models work beer for larger strings and more training data.\nDunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words. e system implemented by Dunning (1994) can classify strings of 10 characters in length “moderately well”, while strings of 50 characters or more are classified “very well”. Accuracies given vary from 92% for 20 bytes of training data to 99.9% for 500 bytes of text."
    }, {
      "heading" : "2.3 Trigrams and short words",
      "text" : "Grefenstee (1995) compares trigrams versus short words for language identification. Short words are oen function words that are typical for and highly frequent in a given language.\ne trigram language guesser was trained on one million characters of text in 10 languages: Danish, Dutch, English, French, German, Italian, Norwegian, Portuguese, Spanish and Swedish. From the same texts, all words with 5 or less characters were counted for the short-word-strategy.\ne results indicate that the trigram approach works beer for small text fragments of up to 15 words, while for any text longer than 15 words, both methods work equally well with reported accuracies of up to 100% in the 11-15 word range."
    }, {
      "heading" : "2.4 N-Grams and clustering",
      "text" : "Gao et al. (2001) present a system that augments n-gram language models with clustering techniques. ey cluster words by similarity and use these clusters in order to overcome the data sparsity problem.\nIn traditional cluster-based n-gram models, the probability P (wi) is defined as the product of the probability of a word given a cluster ci and the probability of the cluster ci given the preceding clusters. For a trigram model, the probability P (wi) of a word wi is calculated as\nP (wi|wi−2wi−1) = P (wi|ci)× P (ci|ci−2ci−1) (4)\ne probability of a word given a cluster is calculated as\nP (wi|ci) = C(wi)\nC(ci) (5)\nwith C(wi) the count of the word wi and C(ci) the count of the cluster ci.\n4\ne probability of a cluster given the preceding clusters is calculated using the Maximum Likelihood Estimator\nP (ci|ci−2ci−1) = C(ci−2ci−1ci)\nC(ci−2ci−1) (6)\nGao et al. (2001) derive from this three ways of using clusters to augment language models: predictive clustering (7), conditional clustering (8) and combined clustering (9).\nP (wi|wi−2wi−1) = P (ci|wi−2wi−1)× P (wi|wi−2wi−1ci) (7)\nP (wi|wi−2wi−1) = P (wi|ci−2ci−1) (8)\nP (wi|wi−2wi−1) = P (ci|ci−2ci−1)× P (wi|ci−2ci−1ci) (9)\nSimilarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in order to improve trigram language models. In addition to Gao et al. (2001), they also use information about the subject-verb and verb-object relations of the sentence.\ney show that their model, using clustering, subject-verb information, verb-object information, and the Porter stemmer outperforms a traditional trigram model.\nCarter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al. (1994)). Carter (1994) shows that the subdivision into smaller clusters increases the accuracy of bigram language models, but not trigram models."
    }, {
      "heading" : "2.5 Inclusion detection",
      "text" : "Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010)) addresses the problem of English inclusions in mainly non-English texts. For the language pair German-English, inclusions are detected using a German and an English lexicon as first resource. If a word is found only in the English lexicon, it is tagged as unambiguously English. If the word is found in neither lexicon, a web search is conducted, restricting the search options to either German or English and counting the number of results. If the German search yields more results, the word is tagged as German, otherwise as English inclusion. If a word is found in both lexicons, a postprocessing module resolves the ambiguity.\nAlex is mainly concerned with the improvement of parsing results by inclusion detection. For example in (Alex et al., 2007) they report an increase in F-Score of 4.3\n5\nby using inclusion detection when parsing a German text with a parser trained on the TIGER corpus (Brants et al., 2002)."
    }, {
      "heading" : "2.6 Clustering and spee",
      "text" : "In the area of clustering and spoken language identification, Yin et al. (2007) present a hierarchical clusterer for spoken language. ey cluster 10 languages1 using prosodic features and Mel Frequency Cepstral Coefficients (MFCC). MFCC vectors are a way of representing acoustic signals (Logan et al., 2000). e signal is first divided into smaller ‘frames’, each frame is passed through the discrete Fourier transform and only the logarithm of the amplitude spectrum is retained (Logan et al., 2000). e spectrum is then projected onto the ‘Mel frequency scale’, a scale that maps actual pitch to perceived pitch, “as apparently the human auditory system does not perceive pitch in a linear manner” (Logan et al., 2000). Finally, a discrete cosine transform is applied to the spectrum to get the MFCC representations of the original signal (Logan et al., 2000).\nYin et al. (2007) show that their hierarchical clusterer outperforms traditionalAcoustic Gaussian Mixture Model systems.\nAs spoken language will not be further investigated in this thesis, I will not dive deeper into the maer at this point."
    }, {
      "heading" : "2.7 Monolingual training data",
      "text" : "Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text.\nYamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively ‘closed’ data set of the Universal Declaration of Human Rights2 and 0.84 on the more ‘open’ Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation.\nKing and Abney (2013) use weakly supervised methods to label the languages of words. ey consider the task as sequence labeling task. ey have limited themselves to bilingual documents with a single language boundary and the task consists\n1e authors do not explicitly list the languages clustered, except for two-leer abbreviations which seem to correspond to ISO 639-1. e languages under investigation could have been Vietnamese, German, Farsi, French, Japanese, Spanish, Korean, English, Tamil, and ‘ma’, though it is impossible to tell.\n2http://www.un.org/en/documents/udhr/\n6\nin discriminating between English and non-English text. ey found that a Conditional Random Field model augmented with Generalized Expectation criteria worked best, yielding accuracies of 88% with as lile as 10 words used for training.\nLui et al. (2014) consider the task as multi-label classification task. ey represent a document as an n-gram distribution of byte sequences in a bag-of-words manner. ey report F-scores of 0.957 and 0.959. ey note that similar languages will pose problems when trying to identify a language, and solve this problem by identifying a set of languages that most probably are correct instead of a single language.\nOne problem that these approaches all have is that they need to know the languages that will occur in the test data (King and Abney, 2013; Lui et al., 2014)."
    }, {
      "heading" : "2.8 Predictive suffix trees",
      "text" : "Seldin et al. (2001) propose a system for automatic unsupervised language segmentation and protein sequence segmentation. eir system uses Variable Memory Markov (VMM) sources, an alternative to Hidden Markov Models (HMM) implemented as Predictive Suffix Trees (PST).\nWhereas HMMs require substantial amounts of training data and a deep understanding of the problem in order to restrict the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to “solve many applications with notable success” (Begleiter et al., 2004). In contrast to n-gram models that estimate the probability of w as P (w|N) with N the context (typically the n previous words), VMMs can vary N in function of the available context (Begleiter et al., 2004). us, they can capture both small and large order dependencies, depending on the training data (Begleiter et al., 2004).\nere is no single VMM algorithm, but rather a family of related algorithms. One of these algorithms is called Predictive Suffix Tree (PST) (Ron et al., 1996). A PST is a tree over an alphabet Σ, with each node either having 0 (leaf nodes) or |Σ| children (non-terminal nodes) (Ron et al., 1996). Each node is labeled with the result of the walk from that node up to the root (Ron et al., 1996). Each edge is labeled by a symbol s ∈ Σ and the probability for the next symbol being s (Ron et al., 1996).\nBy modifying the Predictive Suffix Tree (PST) algorithm using the Minimum Description Length (MDL) principle, Seldin et al. (2001) end up with a non-parametric self-regulating algorithm. e MDL principle avoids overfiing of the model by favoring low complexity over goodness-of-fit (Grünwald, 2007).\ney embed the algorithm in a deterministic annealing (DA) procedure to refine the results. Finally, they use the Blahut-Arimoto algorithm, a rate-distortion function, until convergence of the system.\nFor the language segmentation task, they use 150000 leers of text, 30000 from each of the following languages: English, German, French, Italian, transliterated Russian. ey used continuous language fragments of approximately 100 leers, yielding a\n7\nsynthetic multilingual text that switches language approximately every two sentences. One important point that they note is that “too short segments do not enable reliable discrimination between different models”. erefore, they disallow switching models aer every word.\ney report very good results on the language segmentation task (and on the protein segmentation task). Aer 2000-3000 iterations of the Blahut-Arimoto algorithm, the correct number of languages is identified and the segmentation is accurate up to a few leers.\n8\n3 eory"
    }, {
      "heading" : "3.1 Supervised language model",
      "text" : ""
    }, {
      "heading" : "3.1.1 N-Gram models",
      "text" : "Among supervised languagemodels, n-grammodels are very popular (Gao et al., 2001). An n-gram is a slice from the original string (Cavnar and Trenkle, 1994). ese slices can be contiguous or not. Non-contiguous n-grams are also called skip-grams (Guthrie et al., 2006). In skip-grams, an additional parameter k indicates the maximum distance that is allowed between units. In this parlance, contiguous n-grams can be regarded as 0-skip-n-grams (Guthrie et al., 2006).\ne following example demonstrates the difference between (traditional) n-grams and skip-grams. Given the following sentence:\nTh i s i s a sample s en t en c e . We can construct, for example, the following word k-skip-n-grams:\n(0-skip-)2-grams: is is, is a, a sample, sample sentence 2-skip-2-grams: is is, is a, is sample, is a, is sample, is sentence, a sample, a sentence, sample sentence (0-skip-)3-grams:is is a, is a sample, a sample sentence 2-skip-3-grams:is is a, is is sample, is is sentence, is a sample, is a sentence, is sample sentence, is a sample, is a sentence, is sample sentence, a sample sentence\ne results for 2-skip-2-grams does not include the skip-gram “is sentence”, as the distance in words between these two words is 3, higher than the allowed k of 2. As can be seen from this example, the number of skip-grams ismore than two times higher than the number of contiguous n-grams, and this trend continues the more skips are allowed (Guthrie et al., 2006). Skip-grams, unlike n-grams, do not incur the problem of data sparseness with an increase of n.\nInstead of using words as unit for n-gram decompositions, we can also choose characters. Each word is then decomposed into sequences of n characters. For example, the word\nmodel can be decomposed into the 2-grams: mo, de, el. Oen, the word to decompose is padded with start and end tags in order to improve the model (Cavnar and Trenkle, 1994). If we pad the word with <w> and </w>, the 2-gram decomposition yields: <w>m, mo, de, el, l </w>. e use of paddings allows themodel to capture details about character distribution with regard to the start and end of words (Cavnar and Trenkle, 1994). For example, in English the leer ‘y’ occurs more oen at the end of words than\n9\nat the beginning of words, while the leer ‘w’ occurs mainly at the beginning of words (Taylor, 2015). A non-padding model cannot capture this distinction, while a padding model can.\nOne advantage of n-gram models is that the decomposition of a string into smaller units reduces the impact of typing errors (Cavnar and Trenkle, 1994). Indeed, a typing error only affects a limited number of units (Cavnar and Trenkle, 1994). Due to this property, n-gram models have been shown to be able to deal well with noisy text (Cavnar and Trenkle, 1994)."
    }, {
      "heading" : "3.1.2 Formal definition",
      "text" : "Traditional n-gram languagemodels predict the next wordwi given the previouswords w1, . . . , wi−1. is prediction uses the conditional probability P (wi|w1, . . . , wi−1). Instead of using the entire historyw1, . . . , wi−1, the probability is approximated by using only the n previous words wi−n+1, . . . , wi−1.\nP (wi|w1, . . . , wi−1) = P (wi|wi−n+1, . . . , wi−1) (10)\ne probability can be estimated using theMaximum Likelihood Estimation (MLE):\nP (wi|wi−n+1, . . . , wi−1) = C(wi−n+1, . . . , wi)\nC(wi−n+1, . . . , wi−1) (11)\nWhere C(wi−n+1, . . . , wi) represents the number of times the n-gram sequence wi−n+1, . . . , wi occurred in the training corpus andC(wi−n+1, . . . , wi−1) represents the number of times the (n− 1)-gram sequence wi−n+1, . . . , wi−1 was seen in the training corpus."
    }, {
      "heading" : "3.1.3 Smoothing",
      "text" : "e problem with MLE is that sequences not seen during training will have a probability of zero. In order to avoid this problem, different smoothing techniques can be used (Chen and Goodman, 1996). e simplest smoothing technique is additive (Laplace) smoothing (Chen and Goodman, 1996). Let V be the vocabulary size (i.e. the total number of unique words in the test corpus). e smoothed probability PLaplace becomes:\nPLaplace(wi|wi−n+1, . . . , wi−1) = C(wi−n+1, . . . , wi) + λ\nC(wi−n+1, . . . , wi−1) + λV (12)\nWith λ the smoothing factor. If we choose λ = 1, we speak of “add one” smoothing (Jurafsky and Martin, 2000). In practice, λ < 1 is oen chosen (Manning and Schütze, 1999).\n10\nAn important estimation is theGood-Turing estimation (Chen andGoodman, 1996). While not directly a smoothing method, it estimates the frequency of a given observation with\nc∗ = (c+ 1) Nc+1 Nc\n(13)\nwhere c is the number of times the observationwasmade,Nc is the number of times the frequency c was observed and Nc+1 the frequency of the frequency c + 1. us, instead of using the actual count c, the count is taken to be c∗ (Chen and Goodman, 1996).\nAnother way to avoid assigning probabilities of zero to unseen sequences is by using back-off models. ere are linear and non-linear back-off models. In non-linear back-offmodels, if the original n-gram probability falls below a certain threshold value, the probability is estimated by the next lowest n-gram model. Katz’s back-off model (Katz, 1987) for instance calculates probability Pbo using the formula:\nPbo =\n{ dwi−n+1,...,wi C(wi−n+1,...,wi) C(wi−n+1,...,wi−1)\nif C(wi−n+1, . . . , wi) > k αwi−n+1,...,wi−1Pbo(wi|wi−n+2, . . . , wi−1) otherwise\n(14)\nWith d and α as smoothing parameters. e parameter k is oen chosen k = 0. is means that if the probability given a high-order n-gram model is zero, we back off to the next lowest model. For tri-gram models, the formula becomes:\nPbo(wi|wi−2, wi−1) =  P (wi|wi−2, wi−1) if C(wi−2, wi−1) > 0 α1P (wi|wi−1) if C(wi−2, wi−1) = 0 and C(wi−1, wi) > 0 α2P (wi) otherwise\n(15) In contrast, linear back-offmodels use an interpolated probability estimate by combiningmultiple probability estimates andweighting each estimate. e probabilityPLI for a tri-gram model is:\nPLI(wi|wi−2, wi−1) = λ3P (wi|wi−2, wi−1) + λ2P (wi|wi−1) + λ1P (wi) (16) with ∑ λi = 1"
    }, {
      "heading" : "3.2 Unsupervised clustering",
      "text" : "Clustering consists in the grouping of objects based on their mutual similarity (Biemann, 2006). Objects to be clustered are typically represented as feature vectors (Biemann, 2006); from the original objects, a feature representation is calculated and used for further processing.\n11\nClustering can be partitional or hierarchical (Yin et al., 2007). Partitional clustering divides the initial objects into separate groups in one step, whereas hierarchical clustering builds a hierarchy of objects by first grouping the most similar objects together and then clustering the next level hierarchy with regard to the existing clusters (Yin et al., 2007).\ne clustering algorithm uses a distancemetric tomeasure the distance between the feature vectors of objects (Biemann, 2006). e distance metric defines the similarity of objects based on the feature space in which the objects are represented (Jain et al., 1999). ere are different metrics available. A frequently chosen metric is the cosine similarity that calculates the distance between two vectors, i.e. the angle between them (Biemann, 2006).\nIn order for a clustering algorithm to work, features that represent the object to be clustered have to be defined (Jain et al., 1999). Features can be quantitative (e.g. word length) or qualitative (e.g. word starts with a capital leer) (Jain et al., 1999).\nMost clustering algorithms, e.g. k-means, need the number of clusters to generate (Jain et al., 1999). e question how to best choose this key number has been addressed in-depth by Dubes (1987).\nClustering can be so or hard. When hard-clustering, an object can belong to one class only, while in so-clustering, an object can belong to one or more classes, sometimes with different probabilities (Jain et al., 1999)."
    }, {
      "heading" : "3.3 Weakly supervised language model induction",
      "text" : "e main idea behind language model induction is that by inducing language models from the text itself, the models are highly specialized but the approach is generally more flexible since genre or text specific issues do not arise.\nis approach is similar in character to the work by Seldin et al. (2001) in that the text itself is used as data set. However, the realization differs greatly. Whereas Seldin et al. (2001) use predictive suffix trees, I use n-gram language models.\ne intuition is to learn the language models from the text itself, in an iterative manner. Suppose we have a document as follows where wi represents the word at position i in the text. Suppose the text contains two languages, marked in red and blue.\nw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 …\nWe then evaluate the second word using the first language model. If the language model score is high enough, we update the language model with the second word.\nIf the score is below a certain threshold, the existing languagemodel does notmodel the word well enough and a new model is created.\n13\nWhen there is more than one language model, each word is evaluated by every language model, and the highest scoring model is updated, or a new model is created if no language model models the word well enough.\n14\ne last example shows that it is not necessarily the case that exactly one language model is created per language; it oen is the case that many language models are created for one language.\nAt the beginning, the models are not very reliable, as they only have a few words as basis, but the more text is analyzed, the more reliable the models become.\nHowever, the approach is problematic in that the text structure itself influences the language models created. If the text starts with a foreign language inclusion, as illustrated in figure 12, the initial model might be too frail to recognize the following words as being a different language, updating the first model with the second and third word and so on. us, the approach would fail at recognizing the foreign language inclusion.\nw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 …\nFigure 12: Problematic text sample\nIf we were to start from the end of the text and work towards the beginning, the probability of having a relatively robust language model for the ‘blue’ language would be high, and so, it would theoretically be easier to recognize the first word as not being ‘blue’.\nerefore, one induction step involves one forward generation and one backwards generation. is yields two sets, the set of models from the forward generation F = {f1, f2, . . . , fn} and the set from the backwards generation B = {b1, b2, . . . , bm}. en, from the two sets of models, the most similar models are selected. For this, every model from F is compared to every model fromB, as figure 13 shows. e most similar models are then merged, as illustrated in figure 14. Indeed, if both the forward and backwards generation yielded a similar language model, it is probable that the model is correct.\nEven so, both forward and backwards generation can not guarantee ideal results, there is the option to run the generation from a random position. is random induction picks a random position in the text and runs one induction step from that position, meaning one forward and one backwards generation. Finally, the most similar models are merged as for the general generation.\n15\nis only yields one probable language model, therefore the induction is repeated with the difference that all probable models are taken into consideration as well. For each word, if a probable model models the word well enough, no newmodel is created, otherwise a new model is created.\nAt the end of the induction loop, the set of probable models P is examined. As long as there are two models that have a similarity score below a certain threshold, the two most similar models are merged.\nFinally, aer the language models have been induced, another pass is made over the text and each word is assigned to the language model which yields the highest score for that word, resulting in a word-to-model assignment as illustrated in figure 15.\nI have made the approach parametric with parameters being:\n• Induction iterations: Number of induction iterations\n• Random iterations: Number of random iterations\n16\n• Forward/Backwards threshold: reshold for forward/backwards merging\n• Silver threshold: reshold for P model merging\nese parameters can be adapted, in the hope that some parameter configurations will work beer on certain data sets than other configurations. Since the approach has parameters that have to be learned from a development set, the approach is said to be weakly supervised; the development set is not used to train any language specifics, only for the estimation of the parameters of the approach.\n17"
    }, {
      "heading" : "4 Experimental setup",
      "text" : "In this chapter I present experiments done using the approaches delineated in the previous section in order to find out whether there are approaches that work beer on certain types of text.\ne central hypothesis is that unsupervised language segmentation approaches are more successful on difficult data. Difficult data is data for which there is not enough data to train a language model or data which contains a lot of non-standard language such as abbreviations.\nFirst, I present the data used to test the language segmentation systems and elaborate on the different aspects that had to be considered for the data compilation.\nI then present two supervised language segmentation experiments using n-gram language models and Textcat.\nFor unsupervised language segmentation, I will first present experiments using clustering algorithms before presenting experiments using language model induction."
    }, {
      "heading" : "4.1 Data",
      "text" : "In order to test the different language segmentation approaches, I compiled different sets of test data. As I want to focus on short texts, most texts from the test corpus are rather small, sometimes consisting of only one sentence. However, in order to test the general applicability of the approach, the test corpus also contains larger text samples.\ne test corpus can be subdivided into different sub-corpora:\n• Latin-based: Texts consisting of languages using Latin-based scripts, such as German, English, Finnish or Italian\n• Mixed script: Texts consisting of languages using Latin-based scripts and languages using non-Latin-based scripts\n• Twier data: Short texts taken from Twier\n• Pali dictionary data: Unstructured texts containing many different language inclusions such as Vedic Sanskrit, Sanskrit, Indogermanic reconstructions, Old Bulgarian, Lithuanian, Greek, Latin, Old Irish, many abbreviations and references to text passages\nAs every outcome has to bemanually checked, the test corpus is rather small. Every category consists of five texts. Each texts consists of two or three languages with the exception of the Pali dictionary data that oen contains inclusions frommany different languages in the etymological explanations.\nFor each text, I also created a gold standard version with the expected clusters. In some cases it is not clear how to cluster certain objects. In that case, I use a clustering\n18\nthat makes sense to me, but this need not mean that it is the correct or only possible clustering.\nFor the parameter estimation of the language model induction approach, I also compiled a set of development data. All texts can be found in the appendix under 8.1 and 8.2."
    }, {
      "heading" : "4.2 Supervised language model",
      "text" : ""
    }, {
      "heading" : "4.2.1 Implementation",
      "text" : "For the supervised language segmentation method, I implemented an n-gram language model as described by Dunning (1994). e n-gram language model is implemented as a character trigram model with non-linear back-off to bigram and unigram models. e conditional probability P is calculated using the formula:\nP (wi|wi−2, wi−1) =  α1 C(wi−2,wi−1,wi) C(wi−2,wi−1) if C(wi−2, wi−1, wi) > 0 α2 C(wi−1,wi) C(wi−1) if C(wi−1, wi) > 0 α3 C(wi) V\nif C(wi) > 0 α4 1 V+W+X otherwise\n(17)\nwith α1 = 0.7, α2 = 0.2, α3 = 0.09, α4 = 0.01, V the number of unigrams,W the number of bigrams and X the number of trigrams.\nEach word is padded by two different start symbols and two different end symbols. e joint probability for a word w of length n is calculated as\nP (w) = 1∑n\ni=2 | logP (wi|wi−2, wi−1)| (18)\nIn the denominator, I use the log probability instead of the probability to increase numerical stability. Indeed, multiplying very small numbers can lead to the result being approximated as zero by the computer when the numbers become too small to be represented as normalized number (Goldberg, 1991). Using the sum of logarithms avoids this problem and is less computationally expensive (Bürgisser et al., 1997).\nAs the logarithm of a number approaching zero tends to infinity, rare observations get a higher score than frequent observations. As such, the denominator can be seen as a scale of rarity, with a higher score corresponding to a rarer word. By taking the inverse of this scale, we get a score corresponding to the “commonness” (≈ frequency) of a word."
    }, {
      "heading" : "4.2.2 Training phase",
      "text" : "First, models are trained on training data in the relevant languages. I have not included the languages from the Pali dictionary data, as there are too many different languages\n19\nand there are typically only small inclusions of different languages in a dictionary entry; as such, it would not have made sense to train a language model just to recognize a single word. Another reason for not using the Pali dictionary data languages is that sometimes it is not possible to find data for a language, e.g. Old Bulgarian or reconstructed Indogermanic. In some cases, it would have been conceivable to train models on similar languages, but again, the effort of training a model is disproportionately high compared to the (uncertain) result of recognizing a single inclusion. Instead, an additional catch-all languagemodel is used to capture words that do not seem to belong to a trained model.\ne training data consists ofWikipedia dumps from the months June and July 2015; a dump is a copy of the whole encyclopedia for a given language. Due to the difference in size of theWikipedia of the different languages, I choose the full dump for languages with less than 3 GB of compressed data and limited the amount of data to maximally 3 GB of compressed data.\ne Wikipedia data was processed using the Wikipedia Extractor3 version 2.8 in order to extract the textual content from the article pages. Indeed, theWikipedia pages are wrien using the MediaWiki Markup Language4. While this markup is useful for meta-data annotation and cross-referencing, the encoded information is superfluous for language model training and has to be removed before training a model on the data. Table 1 shows the size of the training data per language aer text extraction.\n3http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 4https://www.mediawiki.org/wiki/Help:Formatting\n20\nAs the test data only contains transliterated Amharic text, theWikipedia data, written in the Ge’ez script, had to be transliterated. e text was transliterated according to the EAE transliteration scheme by the Encyclopaedia Aethiopica.\nAs the test data contains transliterated Greek, the Greek data was used once as-is and once transliterated according to the ELOT (Hellenic Organization for Standardization) transliteration scheme for Modern monotonic Greek.\nIt should be borne inmind that the training data influences the quality and accuracy of the model. Furthermore, a model might work well on certain text types and less well on other text types. It is not possible to train a perfect, universal model."
    }, {
      "heading" : "4.2.3 Application of the approa",
      "text" : "In the second step, an input text is segmented into words. en, each word is evaluated by each language model and the model with the highest score is assigned as the word’s language model.\ne approach taken consists in classifying words as either belonging to a trained language model or to the additional, catch-all model other, which simply means that the word could not be assigned to a trained model class."
    }, {
      "heading" : "4.2.4 Textcat and language segmentation",
      "text" : "I also tested how well Textcat is suited to the task of language segmentation. e approach is similar to the n-gram approach, with the exception that I do not train any models and rely on Textcat’s classifier for language prediction.\nIn the first step, an input text is segmented into words. en, each word is passed to Textcat and the guess made by Textcat is taken as the word’s language."
    }, {
      "heading" : "4.3 Unsupervised clustering",
      "text" : "In order to test the efficiency of clustering algorithms on the task of language segmentation, I looked at various algorithms readily available throughWEKA, “a collection of machine learning algorithms for data mining tasks” by the University of Waikato in New Zealand (Hall et al., 2009) and the Environment for Developing KDD-Applications Supported by Index-Structures (ELKI), “an open source data mining soware […] with an emphasis on unsupervised methods in cluster analysis and outlier detection” by the Ludwig-Maximilians-Universität München (Achtert et al., 2013). I also looked at JavaML, “a collection of machine learning and data mining algorithms” (Abeel et al., 2009), in order to integrate clusterers into my own code framework. JavaML offers different clustering algorithms and also offers access toWEKA’s clustering algorithms. In contrast to WEKA and ELKI, which can be used in stand-alone mode, JavaML is meant\n21\nto be integrated into bigger programs and provides an application programming interface (API) that allows the provided algorithms to be accessed in a programmatic way, i.e. from inside a program."
    }, {
      "heading" : "4.3.1 Preprocessing",
      "text" : "However, in order for the clustering algorithms to work, the document to segment has to be preprocessed in a number of ways, as shown in figure 16.\nFirst of all, the document has to be read in by the program. is step is straightforward.\n22\ne document then has to be tokenized. Tokenization is not trivial and depends on the definition of a ‘word’. For this task I have used a whitespace tokenizer that defines a word as a continuous sequence of character literals separated by one or more whitespace characters. While it can be objected that for scripts that don’t use whitespace to separate words, such as Chinese, tokenization fails, this is not too big a concern. Indeed, if a continuous block of Chinese characters is treated as one word, it is likely to be clustered separately due to the different in ”word” length and the different character set. If, however, a document contains two scripts that do not separate words by whitespace, the approach totally fails. It is beyond the scope of this thesis, and possibly of any thesis, to implement a universal tokenizer that works regardless of language without prior knowledge about the languages at hand.\nEach token is then normalized. Normalization of a non-Latin-based input (e.g. Arabic or Cyrillic script) returns the input without modification. Otherwise, the following modifications are made, if applicable:\n• remove leading and trailing whitespace\n• remove punctuation\n• remove control characters\nControl characters are defined as the set ( [ ] ) \\\nPunctuation is defined as the set . , ” ’ : ; ! ? −\ne token is then stripped of XML-like tags, if applicable. e following example illustrates this step. Let us assume we have the following token:\n<word i d =”1 ” lemma=” go ”> goes </word> e token is replaced by the text content of the node, thus the resulting token is ‘goes’.\nIf, aer all these modifications, the token corresponds to the empty string, we continue with the next token. Otherwise, the token is passed on to the feature extraction module. e algorithm terminates when all tokens have been consumed."
    }, {
      "heading" : "4.3.2 Defining features",
      "text" : "e final step consists in defining features by which to cluster and implementing feature extractors that build the feature vectors from the input. Since the features are to be language independent, using features such as ‘occurs in an English lexicon’ cannot be used. e following features were devised:\n23\n1. word length: the length of the word in characters\n2. X tail bigrams: bigrams calculated from the end of the word\n3. Y tail trigrams: trigrams calculated from the end of the word\n4. X first bigrams: bigrams calculcated from the beginning of the word\n5. Y first trigrams: trigrams calculated from the beginning of the word\n6. latin basic: is the word latin basic?\n7. latin extended: is the word latin extended?\n8. capitalized: is the word capitalized?\n9. contains non-word: does the word contain a non-word?\n10. is non-word: is the word a non-word?\n11. number of latin leers: number of latin leers\n12. number of non-latin leers: number of non-latin leers\n13. vowel ratio: number of vowels divided by the word length\n14. basic latin leer ratio: number of latin leers divided by the word length\n15. max consonant cluster: the longest consonant cluster size in characters\n16. is digit: is the word a digit?\n17. is ideographic: is the word ideographic?\n18. directionality: what directionality does first character of the word have?\n19. is BMP codepoint: does the word contain non-BMP characters?\n20. general type: what is the general type of the first character of the word?\ne last two features are based on the Java Character class. is class provides methods to check for specific implementation-based properties of characters.\nWhile most features are rather self-explanatory, a few require further explanation. For the n-grams, the number of n-grams is restricted so as to keep the resulting vectors the same size. is is important because the clustering algorithm considers one data column as one feature, and having vectors of different length would disrupt this precondition. Implementing the comparison of vectors of different lengths, or rather\n24\nor vectors containing vectors as features would have been possible, but rather timeconsuming. If a word is too short to generate the required number of n-grams, only the possible n-grams are generated and all other positions filled with 0.\ne ‘latin’ features check whether the word consists only of the basic latin leers A-Z and a-z (‘basic’) while the ‘extended’ feature also covers leers derived from the latin leers (e.g. ë, ç, ṃ, ñ).\nNon-words are defined as anything not consisting of leers, such as punctuation marks or digits.\nDirectionality indicates which direction a character should be wrien. While the actual list is much more exhaustive, this property basically indicates whether the character is wrien from le to right or from right to le. 5\nBMP stands for Basic Multilingual Plane and refers to an encoding unit known as plane, which consists of 216 = 65536 codepoints (i.e. encoding slots for characters) (e Unicode Consortium, 2014). e BMP is the first plane, covering the codepoints U+0000 to U+FFFF (e Unicode Consortium, 2014). While it is not important to understand the technical details fully, it is interesting to note that most characters are covered by the BMP, including Chinese, Japanese and Korean characters (e Unicode Consortium, 2014). e next plane, called Supplementary Multilingual Plane or Plane 1 contains historic scripts such as Egyptian hieroglyphs and cuneiform scripts, but also musical notation, game symbols and various other scripts and symbols (e Unicode Consortium, 2014). ere are 17 planes in total (e Unicode Consortium, 2014).\ne last feature in the list, General Type is also an implementation-related property. Type can be, for example5, END_PUNCTUATION, LETTER_NUMBER or MATH_SYMBOL. ese constants are represented as numbers internally, which are taken as feature for the clustering algorithm."
    }, {
      "heading" : "4.3.3 Mapping features to a common scale",
      "text" : "As JavaML requires numerical features, all features were mapped to numerical scales:\n• Binary features were mapped to 0 (false) and 1 (true)\n• Ternary features were mapped to 0 (false), 1 (true) and 99 (not applicable)\n• Numerical features were represented as themselves, either as whole numbers (e.g. word length) or as floating point numbers (e.g. vowel ratio)\n• Java specific features (18,20) take the underlying numerical value as feature\n• N-grams were encoded numerically using algorithm 1\n5e full list can be found under the documentation of the Java Character class hp://docs.oracle.com/javase/7/docs/api/java/lang/Character.html\n25\nAlgorithm 1 N-gram numerical encoding 1: function (word) 2: sum ← 0 3: for character in word do 4: value ←code-point of character 5: sum ← sum+ value 6: end for 7: return sum 8: end function\nWhile algorithm 1 does not encode n-grams in an unambiguous way (“en” and “ne” are both encoded as 211), it provides a sufficiently good encoding."
    }, {
      "heading" : "4.3.4 e problem of unambiguous encoding",
      "text" : "I have tried using unambiguous encodings. e main problem with unambiguous encoding is that the notion of “distance” is distorted. e idea behind the unambiguous encoding is that each “word” (i.e. string of characters) is encoded numerically so that no two “words” are represented as the same number. Besides the encoding of each separate character, the position of the character inside the string also has to be encoded. A possible encoding e for a string w1w2w3 could be\new1w2w3 = n(w1) + x ∗ n(w2) + y ∗ n(w3) (19)\nwith wi the character of the string at position i, n(wi) the numerical encoding of the character wi and x and y parameters. If |A| is the alphabet size of the alphabet A in which the word is encoded, the following constraints must be true for the encoding to be unambiguous:\nx ≥ |A| (20)\ny ≥ |A|2 (21)\nIf we take for example the English alphabet with 26 lowercase and 26 uppercase leers, not counting punctuation, digits and other characters, it has to be true that x ≥ 52 and y ≥ 2704. e problem is that we cannot know in advance what size the alphabet will be. If we have English and German texts, the size can be estimated around 60. However, if we have English, Russian and Arabic text, the size drastically increases. We could choose any two very big numbers, but if we want to guarantee our encoding to be unambiguous, we run the risk of ending up with numbers too big to be represented efficiently.\n26\nIn this encoding scheme, distance is skewed: changes to the first character result in linear distance. ‘man’ and ‘nan’ have a distance of 1, because ‘m’ and ‘n’ have a distance of 1. ‘man’ and ‘lan’ have a distance of 2, etc. Changes to the second character are multiplied by x. ‘man’ and ‘men’ have a distance of x ∗ (distance(a, e)) = 4 ∗ x. Changes to the third character are scaled by y. For any sufficiently big x and y, the distances are too skewed to be used for automatic cluster analysis. Let us consider the following example with only two characters for simplicity. For this example, let us assume x = 1373.\nIt should be apparent from table 2 that the notion of “distance” is distorted. In comparison, table 3 shows the encoding achieved with algorithm 1.\nWhile this encoding is not unambiguous, it is considered sufficiently good for our purposes."
    }, {
      "heading" : "4.3.5 e clusterer",
      "text" : "Most clustering algorithms such as k-means need to be passed the number of clusters to generate. As we want to work as flexibly as possible, I ignored all algorithms that need the number of clusters before clustering. In contrast, the x-means algorithm (Pelleg and Moore, 2000) estimates the number of clusters to generate itself. is algorithm has been chosen to perform the language clustering tasks.\nWhile WEKA and ELKI offer a graphical user interface and various graphical representations of the results, the output is not easily interpretable. Indeed, we can get a visualization of a clustering operation as shown in figures 17 (WEKA) and 18 (ELKI). However, all data points have to be manually checked by either clicking each point\n27\nin order to get additional information about that data point (WEKA) or by hovering over the data points aer having selected the Object Label Tooltip option (ELKI). Figure 18 shows the information for the lowest orange rectangle data point in the ELKI visualization.\n28\nerefore, I have decided to embed the x-means clustering algorithm into a custom framework. Originally part of the WEKA algorithms, the x-means algorithm has been integrated into a Java program via the JavaML library. e framework takes an input file, constructs the aforementioned feature vectors from the input, performs normalization, passes the calculated feature vectors to the clustering algorithm and displays the results in a text-based easily interpretable manner.\nPreliminary analyses have shown that the first clustering result oen is not discriminating enough. Hence, I perform a first clustering analysis, followed by a second clustering analysis on the clusters obtained from the first analysis."
    }, {
      "heading" : "4.3.6 Evaluating clusterings",
      "text" : "e clustering results are evaluated using four common similarity measures used in evaluating the accuracy of clustering algorithms. esemethods are based on counting\n29\npairs (Wagner and Wagner, 2007). Let us consider the clustering C = {C1, . . . , Ck}. C is a set of non-empty disjoint clusters C1, . . . , Ck. Let us consider the reference clustering C ′ = {C1, . . . , Cl}. We define the following sets.\n• S11: set of pairs that are in the same cluster in C and C ′\n• S00: set of pairs that are in different clusters in C and C ′\n• S10: set of pairs that are in the same cluster in C and in different clusters in C ′\n• S01: set of pairs that are in different clusters in C and in the same cluster in C ′\nLet nij = |Sij|, with i, j ∈ {0, 1} be the size of a given set Sij . e Rand Index is defined as\nRI = n11 + n00\nn11 + n10 + n01 + n00 (22)\ne Rand Index measures the accuracy of the clustering given a reference partition (Wagner and Wagner, 2007). However, it is criticized for being highly dependent on the number of clusters (Wagner and Wagner, 2007).\ne Jaccard Index measures the similarity of sets. It is similar to the Rand Index, but it disregards S00, the set of pairs that are clustered into different clusters in C and C ′ (Wagner and Wagner, 2007). It is calculated as\nJ = n11\nn11 + n10 + n01 (23)\ne Fowlkes-Mallows Index measures precision. It is calculated as\nFM = n11√\n(n11 + n10)(n11 + n01) (24)\ne Fowlkes-Mallows Index has the undesired property of yielding high values when the number of clusters is small (Wagner and Wagner, 2007).\nFinally, I will indicate the F-Score. According toManning et al. (2008), in the context of clustering evaluation the F(β) score is defined as\nF (β) = (β2 + 1) ∗ P ∗R\n(β2)P +R (25)\nwith precision P and recall R defined as\nP = n11\nn11 + n10 (26)\n30\nR = n11\nn11 + n01 (27)\nBy varying β, it is possible to give more weight to either precision (β < 0) or recall (β > 1) (Manning et al., 2008). As I value recall higher than precision, I will indicate F1 (β = 1) and F5 (β = 5) scores. Indeed, I want to penalize the algorithm for clustering together pairs that are separate in the gold standard while not penalizing the algorithm for spliing pairs that are together in the gold standard.\nAll measures of similarity fall between [0, 1]with 0 being most dissimilar and 1 being identical. As there is no ultimate measure and all measures of similarity have their drawbacks (Wagner and Wagner, 2007), all measures will be indicated in the results section."
    }, {
      "heading" : "4.4 Weakly supervised language model induction",
      "text" : "e language model induction approach works in two stages. In the first stage, n-gram language models are induced from the text. In the second stage, the text is mapped to the induced models. e algorithm for the language model induction is as follows:\nAlgorithm 2Model induction 1: IM 2: for word in words do 3: modelAndScore ← MS(word) 4: score ← modelAndScore.score 5: if score < threshold then 6: model ← M(word) 7: models.add(model) 8: else 9: maxModel ← modelAndScore.model 10: maxModel.update(word) 11: end if 12: end for\nFirst of all, an initial languagemodel is created. For eachword, themaximummodel and maximum score is calculated. ese values correspond to the language model that yielded the highest probability for the word in question, and the associated probability. If the score falls below a threshold t (i.e. none of the existing language models model the word well enough), a new language model is created on the basis of the word and added to the list of language models. Otherwise, the top scoring language model is updated with the word in question.\n31\nAs the text structure itself influences the quality of the induced models, the language model induction is run i times (i 6 1), with one iteration consisting of two induction steps, once forward and once backward, and j times from a random position (j 6 0). e initial model creation thus either picks the first word of the text (as shown in algorithm 3 line 2), or the last word of the text, or a random word.\nAlgorithm 3 Initial model creation 1: function IM 2: word ← words.first 3: model ← createModel(word) 4: models.add(model) 5: end function\nAlgorithm 4Max model and max score 1: function MS(word) 2: maxScore ← 0 3: maxModel ← none 4: for model in models do 5: score ← model.probability(word) 6: if score > maxScore then 7: maxScore ← score 8: maxModel ← model 9: end if 10: end for 11: returnmaxModel,maxScore 12: end function\nAlgorithm 4 returns both the max model and the max score wrapped as a custom object. e individual values can then be read as necessary.\nAer the models have been induced, the most similar models are merged based on distributional similarity. Distributional similarity is calculated as explained below. is merging step only merges one model from the forward induction group with one model from the backward induction group. e resulting model is added to the set of probable (“silver” ) models.\nMerging is performed according to algorithm 5. e merging algorithm only retains the common set of unigrams from both models, and all resulting bi- and trigrams, excluding any bi- and trigrams that contain character that occur only in one of the models. e values for the resulting language model are calculated according to one of four different merge modes.\ne merge modes are:\n32\nAlgorithm 5Model merger 1: function (model1,model2,mode) 2: merged ← ∅ 3: for unigram u1 inmodel1.unigrams do 4: for unigram u2 inmodel2.unigrams do 5: if u1 = u2 then 6: v1 ← f(u1) ◃ f(u1) is the frequency of u1 7: v2 ← f(u2) 8: value ← mode(v1, v2) 9: unigram ← u1 ◃ or u2, since both are equal 10: merged ← (unigram, value) 11: else 12: exclude ← u1 13: exclude ← u2 14: end if 15: end for 16: end for 17: for all bigrams b inmodel1 andmodel2 do 18: if not exclude contains any char in b then 19: v1 ← f(b,model1) or 0 ◃ frequency of b inmodel1 20: ◃ or 0 if it does not exist 21: v2 ← f(b,model2) or 0 22: value ← mode(v1, v2) 23: merged ← (b, value) 24: end if 25: end for 26: for all trigrams t inmodel1 andmodel2 do 27: if not exclude contains any char in t then 28: v1 ← f(t,model1) or 0 29: v2 ← f(t,model2) or 0 30: value ← mode(v1, v2) 31: merged ← (t, value) 32: end if 33: end for 34: returnmerged 35: end function\n33\n• MAX: use the maximum value (max(v1, v2))\n• MIN: use the minimum value (min(v1, v2))\n• MEAN: use the mean value (v1+v2 2 )\n• ADD: use the sum of the values (v1 + v2)\nIf the random iteration count j > 0, a random word is chosen and the induction is run once forward and once backward starting from this position. en, the most similar models from each set are merged and added to the set of probable models. It should be noted that seing the parameter j > 0 will make the algorithm nondeterministic.\ne model induction is then repeated while the iteration count i has not been reached or until no more models are induced, with the difference that for each word, each probable model is first consulted. If any of the probable models yields a score higher than the threshold value t, it is assumed that the word is already well represented by one of the probable models and no models are induced for this word. If the score falls below the threshold value t, induction is run as described.\nAt the end of the induction loop, all probablemodels are checked against each other. While there are two models that have a similarity below the silver threshold value s, the two models are merged and added to the set of very probable (“gold” ) models.\nIf the set of probable models is not empty aer this merging step, all remaining probable models are added to the set of very probable models.\nIn the second stage, the text is segmented according to the induced “gold” models. For each word, the language model with the highest probability for the word is chosen as that word’s hypothetical language model."
    }, {
      "heading" : "4.4.1 Distributional similarity",
      "text" : "Suppose we have three models with the distributions of leers as shown in figures 19, 20 and 216. Similarity could be calculated based on the occurrence of unigrams/leers alone, i.e. ifmodel1 contains the leer ‘a’ andmodel2 also contains the leer ‘a’, their similarity increases by 1.\nHowever, if we calculate similarity in such a way, all three models are equally similar to each other, as each of the leers occurs at least once in each model. Yet, it should be clear that models 1 and 2 are very similar to each other while model 3 is dissimilar.\nerefore, in order to include the distribution of leers in the similarity measure, similarity is calculated as shown in algorithm 6.\n6efigures shown are used for illustration purposes only and do not necessarily reflect real language models.\n34\n35\na b c d e f g h i 0\n2\n4\n6\nwith f(c) returning the frequency of the character c. e number 2 in (2−q) in line 10 can be explained as follows: q expresses the dissimilarity of the models with regard to a unigram distribution with 0 6 q 6 1, hence (1 − q) expresses the similarity. To this, we add 1, as we increase similarity by 1 due to the match; we augment the simple increase of 1 by the similarity of the distribution.\n36"
    }, {
      "heading" : "4.4.2 Evaluating results",
      "text" : "e results of this approach can be interpreted as clusters, where each language model represents one cluster core and all words assigned to that model making up that cluster. Evaluation will hence be analogous to the evaluation of the clustering approach."
    }, {
      "heading" : "4.4.3 Estimating the parameters",
      "text" : "As the language model induction can be controlled by parameters, we have to find a combination of parameters thatworkswell for our task. e parameters i, j and “merge mode” have been estimated on the development set. e development set contains similar documents to those in the test set. e development set can be found in the appendix.\nIt has been found that the parameter combination i = 4, j = 2, ADD yields good results across the development set. Hence, these values have been used for the test set evaluation.\n37"
    }, {
      "heading" : "5 Results",
      "text" : "‘Baseline’ indicates the measurement where all words have been thrown into one cluster, measured against the gold standard. For ‘Baseline 2’, every word has been put into its own cluster and this clustering is evaluated against the gold standard. e column ‘F1’ stands for the F1 score and the ‘F5’ column stands for the F5 score.\nIf any of the ‘runs’ yields a higher score than any of the baseline values, the maximum score is indicated in bold. If a field contains ‘n/a’, this means that the value could not be calculated for whatever reason (most oen a division by zero would have occurred)."
    }, {
      "heading" : "5.1 N-Gram language model",
      "text" : "38\n39\n40\n41"
    }, {
      "heading" : "5.2 Textcat",
      "text" : "42\n43\n44\n45"
    }, {
      "heading" : "5.3 Clustering",
      "text" : "e first run indicates the value aer one clustering step, and the second run indicates the value aer applying the clustering algorithm to the results of the first run.\n46\n47\n48\n49"
    }, {
      "heading" : "5.4 Language model induction",
      "text" : "In addition to highlighting results that outperform the baseline values, the following tables have been color coded. Results that outperform the clustering algorithm are indicated in red and results that outperform both the clustering algorithm and the ngram language model are indicated in blue.7\n7Results that outperform only the n-gram language model would have been indicated in green, but\nthere is no score that outperforms only the n-gram language model.\n50\n51\n52\n53"
    }, {
      "heading" : "6 Discussion",
      "text" : "e work by Seldin et al. (2001) is similar to the work presented here. ey propose an unsupervised language (and protein sequence) segmentation approach that yields accurate segmentations. While their work looks promising, it also has its drawbacks. eir method requires longer monolingual text fragments and a sizable amount of text. Furthermore, they disallow switching languagemodels aer each word. is presumption will fail to detect single-word inclusions and structures as shown in figure 22, where the language alternates aer each word.\nw1 w2 w3 w4 w5 w6 w7 …\nFigure 22: Alternating language structure\nWhile this structure looks very artificial, such a structure is found, for instance, in the fih Pali dictionary text, in the passage “Pacati, [Ved. pacati, Igd. *peqǔō, Av. pac-;”. In this case, ‘red’ corresponds to Pali, ‘blue’ to (abbreviations in) English and ‘green’ to reconstructed Indo-european."
    }, {
      "heading" : "6.1 N-Gram language models",
      "text" : "e trained n-gram languagemodel approachworks well on the Latin script data, managing to single out the German inclusion from the English–German text (even though it is classified as “other” instead of German).\nFor German–Finnish–Turkish, English–French, English–Transliterated Greek and Italian–German, the separation of the main languages involved is good, although there appear to be some problems when words contain non-word characters such as quotes or parentheses.\nSome puzzling misclassifications happen in the English–Transliterated Greek case: agápe is considered English and éros is considered Transliterated Amharic.\nIn the Italian–German text, the Italian language leads to a rather important Spanish cluster due to the relatedness of the two Romance languages.\nOn the mixed script data set, the results are more diverse. Greek–Russian, English–Spanish–Arabic andUkrainian–Russian are segmentedwell, with English– Spanish–Arabic having Spanish split into Spanish, French and Italian due to the relatedness of the languages.\nIn contrast, the segmentation of English–Greek did not work well at all. Of the two Greek words ἀγάπη and ἔρως, ἀγάπη was considered French and ἔρως was considered Russian. It must be noted, though, that these words bear polytonic diacritics, whereas the model was trained on monotonic Greek.\n54\nAlso, the segmentation of English–Chinese did not work well. is is probably due to the way the model was trained. Chinese script is wrien without whitespace characters between words, and the correct segmentation of a text wrien in Chinese requires in-depth knowledge of the language. Some words are wrien with only one character, but others are composed of two or more characters, with the meaning oen being non-compositional; the meaning of a two-character word is different from the sum of the meaning of the two characters. Sometimes, more than one segmentation would be possible and the context decides on which segmentation is correct. In other cases, more than one segmentation might be correct. is problem occurs with all scripts that are wrien without whitespace.\nAs with the simplified assumption in the tokenization of whitespace-scripts, where I consider a word to be a character sequence delineated by whitespace, I have treated each character as a word. Adapting the method to Chinese and similar scripts would have been possible, but would have introduced the need for large amounts of external linguistic knowledge. Indeed, every possible non-whitespace-script would have to be considered, and each of the tokenizers would be language dependent, i.e. a tokenizer for Chinese would not work on Korean or Japanese.\ne supervised approach did not work well on the Pali dictionary data. While English words could be isolated somewhat successfully, the rest of the data proved difficult to segment. As an example, let us look at the first Pali text. e English cluster contains almost only English words, but not all, the “other” cluster contains mainly marked up words, and the rest is seemingly haphazardly distributed among the other models.\nPali 1: abbha\n• (AR) ., 134., 289.\n• (DE) Miln), imber, dark), Miln\n• (EL) (=, (abbhaŋ\n• (EN) water, mountain, of, free, (used, or, like, referred, (also, A, is, cloudy, clouds, later, a, froth, 1, summit, thundering, by, mass, Pv, Oir, obscure, scum, that, water]., thick, As, from, It, is, at, as, the, in, clouds, things, also\n• (ES) (dense, f., sense, expl, rajo\n• (FI) 239., rain;, Lat., Vin, perhaps, SnA\n• (FR) cloud, Dh, adj., point, cloud, Dhs, A), rain, VvA, DhsA, list\n• (IT) \\”dark, &, ambha, 3, 1, 317, J, sunshine, cp., abhra, [Vedic, (megho\n55\n• (PL) 487, =, S, 295, <br, moon–, 249\n• (RU) 348, 53\n• (TR) viz., ambu, Vv\n• (TrAM) 687, PvA, (°sama, 101, (nīl°, (cp., 64;, (nt.), 581, m., Sn, 1064;\n• (TrEL) , Gr., Sk., Idg., to, pabbata, nt.\n• (UK) 12)., 273, 617, 348)., 250;, 251)., 382).\n• (other) <b> –saŋvilāpa </b>, <b> –mua </b>, <smallcaps> vi. </smallcaps>, (mahiyā, <smallcaps> iv. </smallcaps>, cloud\\”;, <b> Rāhu </b>, <b> abbhā </b>, <b> abbhaŋ, <superscript> 9 </superscript>, marajo </b>, abbhāmua, valāhaka);, <smallcaps> i. </smallcaps>, <b> abbhāmaa </b>, valāhaka–sikhara, <superscript> s. </superscript>, <smallcaps> ii. </smallcaps>, <b> dhū-, storm– cloud, /><b> –kūṭa </b>, thunder–cloud);, <at> a)fro\\\\s </at>, <b> –paṭala </b>, <at>o)/mbros</at>, nīla–megha, <superscript>1</superscript>, *m̊bhro, \\”dull\\”;, acchādesi);, mahikā</b>, <b> –ghana </b>\nOn the Twier data, the supervised approach achieved passable results. While the numbers look great, the actual segmentations do not. For Twier 1, too many clusters were generated, for Twier 2 and 3, the recognition of French words worked somewhat, also recognizing English words as French and French words as English. For Twier 4, the Polish inclusion was isolated but recognized as “other”, together with “strawberries”. e recognition of transliterated Amharic worked satisfactorily, yielding ‘naw’ to the Polish model.\nAs the number of language models increases, so does the risk of misclassification. As can be seen, we already have quite some misclassification with only 15 language models. For example, in our data, the English preposition ‘to’ is oen erroneously classified as ‘transliterated Greek’. e Greek particle το ‘to’ can be either the neuter singular accusative or nominative definite article ‘the’, the masculine singular accusative or nominative definite article ‘the’ or the 3rd person neuter singular nominative/accusative weak pronoun ‘it’, and as such is rather frequent in the language. is is especially problematic with the transliterated Greek language model, which tends to misclassify the English preposition ‘to’ as transliterated Greek.\nA quick corpus study using the Corpus of Modern Greek8 and the Corpus of Contemporary American English9 reveals that the frequency per million words for the Greek particle το is 22666, while the English preposition ‘to’ has a frequency per million words of 25193. eir relative frequencies are very close together, and it might\n8http://web-corpora.net/GreekCorpus/ 9http://corpus.byu.edu/coca/\n56\njust have happened that the training data used in this work contained more Greek ‘to’s than English ‘to’s, leading to this misclassification.\nOther reasons for misclassification include relatedness of the modeled languages as in the case of Germanic or Romance language families. Also, the text types used for training and the text types used for testing play an important role, as well as the amount of training data.\nFor n-gram language models, the quality of the model is dependent on the texts used for training and the texts used in evaluation. It is probable that a different training set would have yielded different results. is is also the problem with the supervised approach; it is necessary to have language data for training and the trained models reflect the training data to some extent."
    }, {
      "heading" : "6.2 Textcat",
      "text" : "Textcat works well on monolingual texts. However, it fails on multilingual texts and does not workwell on short fragments of text, such as single words. Many of the words are tagged as unknown, and if a language has been identified, the language guess oen is not correct. Hence, Textcat cannot be used for language segmentation purposes.\nIndeed, Textcat fails to exceed the baseline values except for two cases: ‘Twier 3’ and ‘Twier 4’ yield beer values than the baseline values. However, upon closer inspection, it is clear that the numerical index values do not give a reliable picture of the quality of the clustering.\nIndeed, while the clustering of ‘Twier 3’ is not nonsensical, it is not very good, failing to extract the French insertion ‘breuvages’. e Rand Index also only shows a slightly beer value than the baseline values. It seems that the outstanding score for ‘Twier 4’ is achieved because both the clustering by Textcat and the gold standard have the same number of clusters.\nTables 20 and 21 show the clusterings side by side. Clearly, Textcat performed poorly despite the high numerical index values. A closer inspection of all the Textcat results shows that Textcat performs poorly at the task of language segmentation; oen, a word cannot be assigned a language and thus is added to the cluster of ‘unknown’ language words. For the words where a language has been identified, it most oen is not the correct language. While language identification is not necessary for the task of language segmentation, it helps to understand why Textcat failed at the task of language segmentation.\n57"
    }, {
      "heading" : "6.3 Clustering",
      "text" : "e clustering results are more difficult to interpret. Oen, the first distinction made seems to be based on case, i.e. words that begin with a capital leer versus words that are all lowercase leers. e second run on the ‘mixed script: English – Greek’ data shows that the first cluster from the first run has been separated into a cluster with words that begin with a capital leer and two clusters with words that don’t begin with a capital leer.\nEnglish–Greek: First run: First cluster\n• “intimate, “without, Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether, affection, ancient, another.”, appreciation, aspires, araction, araction.”, becomes, benevolence., biblical, brotherly, chapter,”, charity;, children, children., contemplation, content, continues, contributes, definition:, described, existence;, explained, express, feeling, feelings, finding, further, holding, initially, inspired, knowledge, marriage., necessary, non-corporeal, passage, passion.”, philosophers, physical, platonic, refined, relationships, returned, self-benefit)., sensually, spiritual, subject, suggesting, through, throughout, transcendence., unconditional, understanding, without, youthful\n58\nEnglish–Greek: Second run: Splitting of first cluster\n• affection, ancient, another.”, aspires, becomes, biblical, chapter,”, charity;, children, children., content, definition:, feeling, feelings, finding, holding, marriage., necessary, passage, passion.”, platonic, refined, returned, subject, through, without\n• Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether\n• “intimate, appreciation, araction, araction.”, benevolence., brotherly, contemplation, continues, contributes, described, existence;, explained, express, further, initially, inspired, knowledge, non-corporeal, philosophers, physical, relationships, self-benefit)., sensually, spiritual, suggesting, throughout, transcendence., unconditional, understanding, youthful\nAnother important distinction seems to be the length of words. Indeed, the results oen show clusters that clearly are based on the length of the contained words. e first run on the ‘latin script: German – Italian’ data shows that short words have been singled out into the first cluster.\nItalian–German: First run: First cluster\n• (il, E, So, a, ad, da, di, e, es, ha, i, il, in, la, le, lo, ma, ne, se, si, un, va, zu\ne clustering works well when the scripts involved are dissimilar, as in the case of the English–Chinese text, where the Chinese characters were isolated aer the first run, and also the English–Spanish–Arabic example, where the Arabic part was completely isolated in the first run.\ne closer the scripts become, the less well clear cut the results are. For Greek– Russian, the results are acceptable, with one mixed cluster. However, the number of clusters is too high for the number of languages involved and the separation is only achieved aer two consecutive clusterings.\ne clustering of closer scripts, such as Ukrainian–Russian does not work well. e clusters, with the exception of the cluster containing the datum ‘9—13’ are all impure, consisting of Ukrainian and Russian words. e second run also fails at improving the clustering.\nFinally, clustering of latin based scripts does not perform well unless diacritics are involved and the diacritics form the most salient distinction. Word containing leers with diacritics are then generally separated from words containing no diacritics, as in the German–Finnish-Turkish example. e first run generates a cluster for numbers, two clusters with diacritics and one cluster without diacritics.\n59\nProbably for this reason, the clustering of TransliteratedGreek–English andGreek– English worked surprisingly well. In both cases, the first run managed to separate the (transliterated) Greek parts from the English words. However, unaccented Greek words such as Agape, erotas or eros were clustered with English.\nEnglish–Transliterated Greek: First run: Transliterated Greek cluster\n• agápe, philía, storgē., éros\nEnglish–Greek: First run: Greek cluster\n• (ἀγάπη, (ἔρως, Agápe, agápē), Éros, érōs), –\ne problem is that when there are other salient distinguishing features besides diacritics, the result is less good, as can be seen on the Pali data."
    }, {
      "heading" : "Pali: abhijjhitar: Second run",
      "text" : "• abhijjhita, abhijjhātar, covets, function], med., one, who, °itar), °itar, °ātar).\n• (T., A, M\n• =, l., v.\n• <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, ag., fr., in\n• 265, 287\n• [n.\nIn some cases, the clustering fails at the task of language segmentation, as in the case of the various English–French texts and the English–German example with the German inclusion. We can thus say that the surface structure or morphology, or in other words the basis from which we can extract features, is not sufficient to deduce relevant information about ‘language’.\nWhen there are more than two languages that are to be separated, the clustering also does not work well. Indeed, the most dissimilar objects are separated first. In the case of English–Spanish–Arabic, the Arabic part is separated first, as well as words with diacritics, while English and Spanish words without diacritics are thrown together. Subsequent runs show no improvement of the clustering concerning the separation of English and Spanish.\nIn the case of German–Finnish–Turkish, the clustering algorithm seems to cluster out Turkish first, followed by Finnish. e results are however much less clear-cut than for English–Spanish–Arabic.\n60"
    }, {
      "heading" : "6.4 Language model induction",
      "text" : "e languagemodel induction does not seem towork verywell on the Latin script data. ere are almost only impure clusters, containing more than one language. However, the approach consistently outperforms the clustering approach when we look at the F5 score. For the English–French data set, the clustering approach even outperforms the n-gram language model approach. Indeed, the French words are relatively well separated from the English text, with the exception of ‘sucré’, which is still thrown together with English words."
    }, {
      "heading" : "Latin script: English–Fren",
      "text" : "• both, “so”, in, English, although, their, is, is, the, opposite, of, “rough”, or, is, the, opposite, of, sweet, only, for, wines, (otherwise, is\n• mou, :, mou, but\n• doux,\n• Doux, (rugueux), Doux\n• while\n• “hard”., used).,\n• translate, as, meaning, very, different., ”coarse”, can, also, mean, almost,sucré,\nIn contrast, the approach works well on the mixed script data. Indeed, we achieve a good separation of the languages by script. However, when there are also Latin based scripts, we encounter the same problems as mentioned above with rather modest results. For example, for the English–Greek text, the approach separates out the Greek character words but it fails to separate transliterated Greek and English. Also, for the English–Spanish–Arabic text, Arabic is separated out, but English and Spanish are not separated well.\nOne interesting observation can be made in the case of the English–Chinese text. e Chinese characters have been isolated, but the Pinyin transcription is thrown together with the Chinese characters. Based on the prior observations, this is rather unexpected. is raises the question of whether Pinyin ought to be clustered out, or clustered together with English or Chinese.\nAgain, the languagemodel induction approach outperforms the clustering approach, and also the n-gram language model approach in the case of the English–Greek text.\nOn the larger Pali dictionary entries, the language model induction approach yields acceptable results. On the shorter Pali dictionary entries, the languagemodel induction approach yields good results.\n61\ne quite low performance must be blamed on the data. Indeed, the Pali dictionary data contain various problematic characters such as ‘comma/dot and whitespace’ as one character. On such characters, whitespace tokenization fails, yielding big chunks of nonsense tokens. For example, the fourth Pali dictionary entry was split into five chunks (while it might not be displayed as such, all commata and all dots are in fact not followed by whitespace, the whitespace is part of the character,10 hence whitespace tokenization fails).\nPali: gūhanā: Chunks\n• Gūhanā，（f.）\n• [abstr．fr．gūhati]=gūhanā\n•（q．v.）\n• Pug．19．Cp．pari°．（Page\n• 253）\nFurthermore, the data contains markup, abbreviations, references, typing mistakes and signs such as <-> that are difficult to assign to a language.\nOn the Twier data, the language model induction approach works rather well. For example, on the first text, separation is not perfect with the Greek cluster still containing some English words.\nTwitter 1: English–Greek\n• BUSINESS, EXCELLENCE.\n• Μόλις, ψήφισα, αυτή, τη, λύση, Internet, of, στο, διαγωνισμό\n• ings, IT\nFor the third and fourth text, the approachmanages to single out the other-language inclusions, but not exclusively. Both times, there is one additional item in the cluster (the relevant clusters are marked in red).\n10e comma has the Unicode codepoint U+FF0C (FULLWIDTH COMMA) and the dot has the Unicode codepoint U+FF0E (FULLWIDTH FULL STOP)\n62\nTwitter 3: Fren–English\n• #FWWC2015\n• breuvages, go\n• Food, Edmonton, to, for, the\n• in, waiting, #bilingualism\n• and, are, ready, just, fans\nTwitter 4: English–Polish\n• comes, from, with, two, crates, of, strawberries, jackets, omg\n• my, dad, poland, and, adidas\n• back, żubrówka\ne approach exceeded expectations on the second and fih Twier text. On the second text, the ‘French’ cluster does not only contain the French words ‘Demain’ and ‘par’, but also the French way of notating time ‘18h’.\nTwitter 2: Fren–English\n• Keynote, “e, collective, of, science-publish, or, perish;, it, all, that, counts?”\n• Demain, 18h, par\n• #dhiha6, David\n• @dhiparis, dynamics, is\nOn the fih text, an almost perfect result was achieved, with only one additional subdivision of the ‘English’ cluster.\nTwitter 5: Transliterated Amharic–English\n• (coffee\n• bread). is, our\n• Buna, dabo, naw\n63\nIt seems that the language model approach does not work very well on longer texts, especially on longer texts in Latin-based scripts, with the chosen parameter set; still, the approach outperforms the clustering approach and achieves scores in the vicinity of the scores achieved with the supervised trained n-gram language model approach. On mixed script texts, the approach consistently outperforms the clustering approach and we also reach scores in the vicinity of the scores achieved with the supervised trained n-gram language model approach.\nMoreover, on short texts, the approach works rather well. We succeed in outperforming the supervised trained n-gram language model approach on a number of texts, and we achieve scores close to the scores achieved with the supervised trained n-gram language model approach.\nAlthough the language model induction approach tends to generate too many clusters, it also generally succeeds at separating the languages involved."
    }, {
      "heading" : "6.5 Scores",
      "text" : "Of the scores I used for evaluation purposes, it seems that a combination of a high Rand Index and a high F5 score indicate a good language segmentation. A high F5 score alone is not significant. For example, the clustering algorithm achieves an F5 score of 0.7215 on ‘Twier 3’. is score looks good, but the Rand Index score is at 0.4571, and the segmentation is not good.\nTwitter 3: Cluster analysis\n• Edmonton, Food\n• go, in, to\n• and, are, breuvages, fans, for, just, ready, the, waiting Similarly, a high Rand Index score alone is not significant. For example, the clustering algorithm achieves a Rand Index score of 0.6738 on the ‘Pali 2’ text, but the F5 score is at 0.3825 and the clustering is not good.\nPali 2: Cluster analysis\n• abhijjhita, abhijjhātar, covets, function], med., one, who, °itar), °itar, °ātar).\n• (T., <smallcaps>i.</smallcaps>, <smallcaps>v.</smallcaps>, =, A, M, ag., fr., in, l., v.\n• 265, 287\n• [n.\n64"
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this thesis, I have asked the question of whether unsupervised approaches to language segmentation perform beer on short and difficult texts than supervised approaches by overcoming some of the difficulties associatedwith supervised approaches, such as the need for (enough and adequate)11 training data, the language-specificity of the language model or the inflexibility of trained language models when it comes to spelling variation and abbreviations, unless the training data also contained spelling variation and abbreviations.\nI have given an overview over related work, presenting supervised approaches that have been used in monolingual language identification and the amelioration of such approaches through unsupervised approaches such as clustering.\nUnfortunately, the body of literature covering the topic of language segmentation is sparse. e work by Yin et al. (2007) and the work by Seldin et al. (2001) are closest in topic to this thesis. However, Yin et al. (2007) concern themselves with spoken language, with requires a different approach than dealing with wrien language. As I concentrated on wrien language, their work was not conducive to this thesis.\nIn contrast, Seldin et al. (2001) present a work that looks promising. ey present a system that finds language borders in a text with great accuracy using unsupervised algorithms. However, they restrict their algorithm in such a way that switching language models aer each word is disallowed. us, they are unable to detect singleword inclusions and cannot handle situationswhere the language switches everyword, as has been shown to occur in the test data used in section 4.\nAnother major drawback of the approach is that it also needs longer fragments of monolingual text and an overall longer text. Hence, their approach would not work well on short texts, if at all.\nNext, I have presented the theoretical foundations of a supervised n-gram language model approach and an unsupervised clustering approach. Finally, I have introduced a weakly supervised n-gram language model inducing approach devised by myself. All of these approaches can be used for language segmentation. In order to test how well the different approaches perform on different text types, I have performed experiments.\nSection 4 presents the experiments made. I have first compiled a small corpus of texts ranging from longer texts with clearly separated languages to one-sentence Twier messages containing foreign language inclusions. I have also included a set of dictionary entries from the Pali dictionary by the Pali Text Society. Indeed, these entries contain a lot of different languages and abbreviations, and (unfortunately) are not consistently formaed.\nI have then presented my implementations of the supervised and weakly super11e question of what is to be considered ‘enough’ or ‘adequate’ is another point of contention; the\ndata always influences the resulting models.\n65\nvised approaches and the choice of the unsupervised clustering algorithms. en, I have presented the results of their application to the data.\nIt can be said that the supervised approach works reasonably well. e drawbacks are that the approach needs training data to train the models on. e problems of the training data and its influence on the models have been raised more than once.\ne supervised approach failed for non-whitespace scripts. emodels would have to be adapted for non-whitespace scripts, introducing more complexity. Also, the training and test texts would have to be split in meaningful ways, introducing the need for a vast array of language-specific text spliers, should the approach work on a wide range of languages.\ne unsupervised approach generally succeeded in separating languages by script when different scripts were involved. Other than that, it seems that the chosen morphological features, or possibly morphological features in general, are insufficient for the algorithm to separate languages effectively.\ne weakly supervised approach worked well on short texts and on difficult short texts, but less well on long texts, while still outperforming the clustering approach on long texts. e approach consistently outperforms the clustering approach and reaches scores in the vicinity of the scores achieved by the supervised approach, even surpassing the supervised approach in some cases. ese results are promising, but more thorough investigations have to be undertaken.\nIn conclusion, it can be said that some unsupervised (or weakly supervised) approaches can perform beer on the task of language segmentation on difficult and short texts. e presented weakly supervised approach does not only outperform the unsupervised clustering approach, it also achieves scores comparable to the scores achieved with the supervised approach.\nFuture work could concentrate on the reduction of the number of generated clusters, ideally geing down to one cluster per language; it would also be thinkable to prevent overly frequent language model switching by taking a word’s context into account. Finally, the parameters could conceivably be adapted automatically. With an increased interest in the area of multilingual text processing lately, the emergence and evolution of the texts themselves will influence the direction of the work in that direction.\n“Il est venu le temps des cathédrales le monde est entré dans un nouveau millénaire\nL’homme a voulu monter vers les étoiles écrire son histoire dans le verre ou dans la pierre”\n— Gringoire\n66"
    }, {
      "heading" : "8 Appendix",
      "text" : ""
    }, {
      "heading" : "8.1 Development data",
      "text" : ""
    }, {
      "heading" : "8.1.1 Latin script data",
      "text" : "Karl Marx anses som en af de fire klassiske sociologer. Marx er epokegørende for den historiske videnskab. Og Marx spillede en vigtig rolle for den samtidige og eerfølgende arbejderbevægelse.\n1891, nach einer Tuberkuloseerkrankung Hopes, eröffnete das Ehepaar ein modernes Lungensanatorium in Nordrach im Schwarzwald, das sie bis 1893 gemeinsam ührten. 1895 wurde die Ehe geschieden.\nSources: hps://da.wikipedia.org/wiki/Karl_Marx hps://de.wikipedia.org/wiki/Hope_Bridges_Adams_Lehmann"
    }, {
      "heading" : "8.1.2 Mixed script data",
      "text" : "Capitalism is an economic system and a mode of production in which trade, industries, and the means of production are largely or entirely privately owned. Private firms and proprietorships usually operate in order to generate profit, but may operate as private nonprofit organizations.\nایدولو .دش روهشم نینل مسا هب ایند رد یلو دوب فونایلوا چیلیا ریمدالو وا یلصا مان تسا ریمدالو ففخم هک دندرکیم باطخ ایدولو ار وا کسیربمیس رد هفرم هداوناخ کی رد ،سیراپ نومک زا لبق لاس کی ینعی ۱۸۷۰ لاس رد هک دوب فونایلوا هداوناخ دنزرف شش زا دنزرف نیموس کی شردپ .دیدرگ دلوتم دمآ رد کسفون ءایلوا مان هب یگرزب رهش تروص هب اهدعب یلو دوبن شیب یکرهش نامز نآ رد هک اگلو دور لحاس رد رکفت زرط و اهیناملا هب رمع تدم مامت رد نینل تهج نیمه هبو دوب یناملا کشزپ کی رتخد شردام و یضایر ملعم و لاربیل یاوژروب هدرخ رد یلو تشاد یناشخرد لالدتسا هوق و دوب یبوخ درگاش ناتسریبد رد ایدولو .تسیرگنیم ضامغا هدید هب دوب نآ دولوم سکرام هک یناملا .دوب یذوم یاهچب لاح نیع\nSources: hps://en.wikipedia.org/wiki/Capitalism hps://fa.wikipedia.org/wiki/نینل_ریمیدالو"
    }, {
      "heading" : "8.1.3 Twitter data",
      "text" : "Twitter 1 »Fallo ergo sum«: On being wrong."
    }, {
      "heading" : "Source:",
      "text" : "Roland Hieber (daniel_bohrer). “»Fallo ergo sum«: On being wrong.”. 26 July 2015, 16:47. Tweet.\n72\nTwitter 2 Music for Airports > le piano en libre-accès dans l’aéroport Charles-deGaulles"
    }, {
      "heading" : "Source:",
      "text" : "Yannick Rochat (yrochat). “Music for Airports > le piano en libre-accès dans l’aéroport Charles-de-Gaulles”. 26 July 2015, 18:12. Tweet."
    }, {
      "heading" : "8.1.4 Pali dictionary data",
      "text" : "All entries have been taken from the Pali Text Society’s Pali-English dictionary (T. W. Rhys Davids, William Stede, editors, e Pali Text Society’s Pali–English dictionary. Chipstead: Pali Text Society, 1921–5). 8 parts [738 pp.].)\nHambho Hambho，（indecl.）[haṁ+bho] a particle expressing surprise or haughtiness J.I，184，494．See also ambho．（Page 729）\nUssada Ussada，[most likely to ud + syad；see ussanna]：this word is beset with difficulties，the phrase sa-ussada is applied in all kinds of meanings，evidently the result of an original application & meaning having become obliterated．sa° is taken as *sapta（seven）as well as *sava（being），ussada as prominence，protuberance， fulness，arrogance．emeanings may be tabulated as follows：（1）prominence（cp． Sk．utsedha），used in characterisation of the Nirayas，as“projecting，prominent hells”，ussadanirayā（but see also below 4）J．I，174；IV，3，422（pallaṅkaṁ， v．l．caturassạṁ，with four corners）；V，266．– adj．prominent A．13（tejussadehi ariyamaggadhammehi，or as below 4?）．– 2．protuberance，bump，swelling J．IV，188；also in phrase saussada having 7 protuberances，a qualification of the Mahāpurisa D．III，151（viz．on both hands，feet，shoulders，and on his back）． – 3．rubbing in，anointing，ointment；adj．anointed with（-°），in candan° J．III， 139；IV，60；．1，267；Vv 537；DhA．I，28；VvA．237．– 4．a crowd adj．full of（-°）in phrase saussada crowded with（human beings）D．I，87（cp．DA．I， 245：aneka-saa-samākiṇṇa；but in same sense BSk．sapt-otsada Divy 620，621）；Pv IV．18（of Niraya = full of beings，expld．by saehi ussanna uparûpari nicita PvA． 221．– 5．qualification，characteristic，mark，aribute，in catussada“having the four qualifications（of a good village）”J．IV，309（viz．plenty of people，corn， wood and water C．）．e phrase is evidently shaped aer D．I，87（under 4）．As “preponderant quality，characteristic”we find ussada used at Vism．103（cf．Asl． 267）in combns．lobh°，dos°，moh°，alobh° etc．（quoted from the“Ussadakiana”）， and similarly at VvA．19 in Dhammapāla’s definition of manussa（lobh’ādīhi alobh’ ādīhi sahitassa manassa ussannatāya manussā），viz．saā manussa-jātikā tesu lobh’ ‹-› ādayo alobh’ādayo ca ussadā．– 6．（metaph．）self-elevation，arrogance，conceit， haughtiness Vin．I，3；Sn．515，624（an° = taṇhā-ussada-abhāvena SnA 467），783\n73\n（expld．by Nd1 72 under formula saussada；i．e．showing 7 bad qualities，viz．rāga， dosa，moha etc．），855．– See also ussādana，ussādeti etc．（Page 157）"
    }, {
      "heading" : "8.2 Test data",
      "text" : ""
    }, {
      "heading" : "8.2.1 Latin script data",
      "text" : "English - German e German word Nabelschau means ”navel-gazing” or ”staring at your navel”. But in this case, it doesn’t refer to anyone else’s belly buon – just your own."
    }, {
      "heading" : "Source:",
      "text" : "Glass, Nicole (2015): ”German Missions in the United States - Word of the Week”. Germany.info.\nEnglish - Fren doux, mou : both translate as ”so” in English, although theirmeaning is very different. Doux is the opposite of ”rough” or ”coarse” (rugueux), while mou is the opposite of ”hard”. Doux can also mean sweet, but almost only for wines (otherwise sucré is used)."
    }, {
      "heading" : "Source:",
      "text" : "Maciamo, (2015): ”French words and nuances that don’t exist in English”. Eupedia.\nEnglish - Transliterated Greek e Greek language distinguishes at least four different ways as to how the word love is used. Ancient Greek has four distinct words for love: agápe, éros, philía, and storgē. However, as with other languages, it has been historically difficult to separate the meanings of these words when used outside of their respective contexts. Nonetheless, the senses in which these words were generally used are as follows.\nSource: hps://en.wikipedia.org/wiki/Greek_words_for_love\nItalian - German Milano ne custodisce l’esempio più struggente: quel Cenacolo che il vinciano affrescò con amore, cura e rivoluzionaria psicologia (il Giuda non viene privato dell’aureola, ma si condanna da solo, con la consapevolezza del peccato) cominciò subito ad autodistruggersi, con un cancro che solo un lunghissimo restauro ha di recente arginato.\nKaum eine Woche vergeht, in der es keine neue Studie, Umfrage oder Warnung zumema Fachkräemangel in Deutschland gibt.\nCerto, lo faceva per definire le idee, ma anche perché consapevole che le intuizioni sono periture, che la vita stessa va caurata in qualche modo.\n74\nDabei mehren sich letzter Zeit auch Stimmen, die Entwarnung geben. So kam jüngst eine Studie des Stierverbands ür die Deutsche Wissenscha zu dem Ergebnis, dass ”ein allgemeiner Fachkräemangel in den MINT-Berufen eher nicht mehr” drohe.\nCome anche i riccioli del Baista richiamano il movimento delle acque, moto che poi Leonardo studierà più approfonditamente a Venezia, nelle ricerche sui bacini in chiave di difesa anti-Turchi. E si vada alla bellissima Annunciazione, con un occhio aento alle ali dell’angelo: la delicatezza delle punte all’insù che cosa sono se non il barbaglio di un sogno che lo ossessionava da anni, ovvero quello di volare?\nIst das seit Jahren angemahnte Szenario vom drohenden Fachkräemangel bei Ingenieuren und Naturwissenschalern also nur ein Mythos?"
    }, {
      "heading" : "Source:",
      "text" : "Stalinski, Sandra (2015): ”Ingenieure: Mythos Fachkräemangel?”. tagesschau.de. Scorranese, Roberta (2015): ”Nelle grandi opere il racconto sofferto della natura mortale”. Archiviostorico.corriere.it.\nGerman - Finnish - Turkish Der Sommer ist die wärmste der vier Jahreszeiten in der gemäßigten und arktischen Klimazone. Je nachdem, ob er gerade auf der Nord- oder Südhalbkugel herrscht, spricht man vom Nord- oder Südsommer. Der Nordsommer findet gleichzeitig mit dem Südwinter sta.\nKesä eli suvi on vuodenaika kevään ja syksyn välissä. Kesä on vuodenajoista lämpimin, koska maapallo on silloin kallistunut niin, eä aurinko säteilee maan pinnalle jyrkemmässä kulmassa kuin muina vuodenaikoina. Pohjoisella pallonpuoliskolla kesäkuukausiksi lasketaan tavallisesti kesä-. heinä- ja elokuu, eteläisellä pallonpuoliskolla joulu-, tammi- ja helmikuu.\nYaz, en sıcak mevsimdir. Kuzey Yarım Küre’de en uzun günler yazda gerçekleşir. Dünya ısıyı depo eiği için en sıcak günler genellikle yaklaşık iki ay sonra ortaya çıkar. Sıcak günler Kuzey Yarım Küre’de 21 Haziran ile 22 Eylül arasında, Güney Yarım Küre’de ise 22 Aralık ile 21 Mart arasındadır.\nSource: hps://fi.wikipedia.org/wiki/Kesä hps://de.wikipedia.org/wiki/Sommer hps://tr.wikipedia.org/wiki/Yaz"
    }, {
      "heading" : "8.2.2 Mixed script data",
      "text" : "Greek - Russian Η ελληνική γλώσσα είναι μία από τις ινδοευρωπαϊκές γλώσσες. Αποτελεί το μοναδικό μέλος ενός ανεξάρτητου κλάδου της ινδοευρωπαϊκής οικογένειας γλωσσών. Ανήκει επίσης στον βαλκανικό γλωσσικό δεσμό. Στην ελληνική γλώσσα, έχουμε γραπτά κείμενα από τον 15ο αιώνα π.Χ. μέχρι σήμερα.\nНа греческом языке на всех этапах его существования была создана богатейшая литература. В Римской империи знание греческого языка считалось обяза-\n75\nтельным для всякого образованного человека. В латинском языке присутствует большое количество греческих заимствований, а в греческом —значительное количество латинских и романских слов. В новое время древнегреческий язык стал (наряду с латинским) источником создания новых научных и технических терминов (так называемая международная лексика). В русский язык греческие слова проникали в основном двумя путями—через международную лексику и через церковнославянский язык.\nSource: hps://el.wikipedia.org/wiki/Ελληνική_γλώσσα hps://ru.wikipedia.org/wiki/Греческий_язык\nEnglish - Greek - Transliterated Greek Agápe (ἀγάπη agápē) means ”love: esp. brotherly love, charity; the love of God for man and of man for God.” Agape is used in the biblical passage known as the ”love chapter,” 1 Corinthians 13, and is described there and throughout the New Testament as brotherly love, affection, good will, love, and benevolence. Whether the love given is returned or not, the person continues to love (even without any self-benefit). Agape is also used in ancient texts to denote feelings for one’s children and the feelings for a spouse, and it was also used to refer to a love feast. It can also be described as the feeling of being content or holding one in high regard. Agape is used by Christians to express the unconditional love of God for his children. is type of love was further explained by omas Aquinas as ”to will the good of another.”\nÉros (ἔρως érōs) means ”love, mostly of the sexual passion.” e Modern Greek word ”erotas” means ”intimate love.” It can also apply to dating relationships as well as marriage. Plato refined his own definition: Although eros is initially felt for a person, with contemplation it becomes an appreciation of the beauty within that person, or even becomes appreciation of beauty itself. Plato does not talk of physical araction as a necessary part of love, hence the use of the word platonic to mean, ”without physical araction.”\nIn the Symposium, the most famous ancient work on the subject, Plato has Socrates argue that eros helps the soul recall knowledge of beauty, and contributes to an understanding of spiritual truth, the ideal ”Form” of youthful beauty that leads us humans to feel erotic desire – thus suggesting that even that sensually based love aspires to the non-corporeal, spiritual plane of existence; that is, finding its truth, just like finding any truth, leads to transcendence. Lovers and philosophers are all inspired to seek truth through the means of eros.\nSource: hps://en.wikipedia.org/wiki/Greek_words_for_love\n76\nEnglish - Spanish - Arabic A black ribbon is a symbol of remembrance or mourning. Wearing or displaying a black ribbon has been used for POW/MIA remembrance, mourning tragedies or as a political statement.\nEl crespón negro o lazo negro es un símbolo utilizado por personas, estados, sociedades y organizaciones, representando un sentimiento político-social en señal de duelo.\nنم و تاملك ةيال ةجاحلا نود ةدحاو ةرظنب اهتلاسر لقنت نأ يغبني ةمالعلا نأف امومعو نيعم ءيش نع ربعي يذلا مسرلا ينعي زمرلا مه تامالعلا مدختسا نم رثكأ نكلو تامالعلا اومدختسأ قيرغألاو نييرصملا ءامدق نأ فورعملا\nSource:\nhps://es.wikipedia.org/?title=Lazo_negro hps://en.wikipedia.org/wiki/Black_ribbon hps://ar.wikipedia.org/wiki/زمر\nEnglish - Chinese - (Pinyin) e Chinese word for ”crisis” (simplified Chinese: 危 机; traditional Chinese: 危機; pinyin: wēijī) is frequently invoked in Western motivational speaking because the word is composed of two Chinese characters that can represent ”danger” and ”opportunity”. Some linguists have criticized this usage because the component pronounced jī (simplified Chinese: 机; traditional Chinese: 機) has other meanings besides ”opportunity”. In Chinese tradition, certain numbers are believed by some to be auspicious (吉利) or inauspicious (不利) based on the Chinese word that the number name sounds similar to. e numbers 0, 6, 8, and 9 are believed to have auspicious meanings because their names sound similar to words that have positive meanings. Source: hps://en.wikipedia.org/w/index.php?title=Chinese_word_for_”crisis”\nUkrainian - Russian Віддавна на території України існували держави скіфів, сарматів, готів та інших народів, але відправним пунктом української державності й культури вважається Київська Русь 9—13 століття. На юге омывается водами Чёрного и Азовского морей. Имеет сухопутную границу с Россией, Белоруссией, Польшей, Словакией, Венгрией, Румынией и Молдавией. Source: hps://uk.wikipedia.org/wiki/Україна Surgut-safari.ru, (2015): ”Страны - Safari Tour”.\n77"
    }, {
      "heading" : "8.2.3 Twitter data",
      "text" : "Tweet 1: Greek – English Μόλις ψήφισα αυτή τη λύση Internet of ings, στο διαγωνισμό BUSINESS IT EXCELLENCE."
    }, {
      "heading" : "Source:",
      "text" : "GaloTyri. ”Μόλις ψήφισα αυτή τη λύση Internet of ings, στο διαγωνισμό BUSINESS IT EXCELLENCE.”. 19 June 2015, 12:06. Tweet\nTweet 2: English – Fren Demain #dhiha6 Keynote 18h @dhiparis “e collective dynamics of science-publish or perish; is it all that counts?” par David @chavalarias"
    }, {
      "heading" : "Source:",
      "text" : "Claudine Moulin (ClaudineMoulin). ”Demain #dhiha6 Keynote 18h @dhiparis ”e collective dynamics of science-publish or perish; is it all that counts?” par David @chavalarias”. 10 June 2015, 17:35. Tweet.\nTweet 3: English – Fren Food and breuvages in Edmonton are ready to go, just waiting for the fans #FWWC2015 #bilingualism"
    }, {
      "heading" : "Source:",
      "text" : "HBS (HBS_Tweets). ”Food and breuvages in Edmonton are ready to go, just waiting for the fans #FWWC2015 #bilingualism”. 6 June 2015, 23:29. Tweet.\nTweet 4: English – Polish my dad comes back from poland with two crates of strawberries, żubrówka and adidas jackets omg"
    }, {
      "heading" : "Source:",
      "text" : "katarzyne (wifeyriddim). ”my dad comes back from poland with two crates of strawberries, żubrówka and adidas jackets omg”. 8 June 2015, 08:49. Tweet.\nTweet 5: Transliterated Amharic – English Buna dabo naw (coffee is our bread)."
    }, {
      "heading" : "Source:",
      "text" : "eCodeswitcher. ”Buna dabo naw (coffee is our bread).”. 9 June 2015, 02:12. Tweet."
    }, {
      "heading" : "8.2.4 Pali dictionary data",
      "text" : "All entries have been taken from the Pali Text Society’s Pali-English dictionary (T. W. Rhys Davids, William Stede, editors, e Pali Text Society’s Pali–English dictionary. Chipstead: Pali Text Society, 1921–5. 8 parts [738 pp.].)\nabbha (nt.) [Vedic abhra nt. & later Sk. abhra m. ”dark cloud”; Idg. *m̊bhro, cp. Gr. <at>a)fro\\\\s</at> scum, froth, Lat. imber rain; also Sk. ambha water, Gr. <at>o)/mbros</at> rain, Oir ambu water]. A (dense & dark) cloud, a cloudy mass A <smallcaps>ii.</smallcaps> 53 = Vin <smallcaps>ii.</smallcaps> 295 = Miln 273 in\n78\nlist of to things that obscure moon– & sunshine, viz. <b>abbhaŋ mahikā</b> (mahiyā A) <b>dhū- marajo</b> (megho Miln), <b>Rāhu</b> . is list is referred to at SnA 487 & VvA 134. S <smallcaps>i.</smallcaps> 101 (°sama pabbata a mountain like a thunder–cloud); J <smallcaps>vi.</smallcaps> 581 (abbhaŋ rajo acchādesi); Pv <smallcaps>iv.</smallcaps> 3 <superscript>9</superscript> (nīl° = nīla–megha PvA 251). As f. <b>abbhā</b> at Dhs 617 & DhsA 317 (used in sense of adj. ”dull”; DhsA expl <superscript>s.</superscript> by valāhaka); perhaps also in <b>abbhāmaa</b> . <br /><b>–kūṭa</b> the point or summit of a storm–cloud  1, 1064; J <smallcaps>vi.</smallcaps> 249, 250; Vv 1 <superscript>1</superscript> (= valāhaka–sikhara VvA 12). <b>–ghana</b> a mass of clouds, a thick cloud It 64; Sn 348 (cp. SnA 348). <b>–paṭala</b> a mass of clouds DhsA 239. <b>–mua</b> free from clouds Sn 687 (also as abbhāmua Dh 382). <b>–saŋvilāpa</b> thundering S <smallcaps>iv.</smallcaps> 289.\nabhijjhitar [n. ag. fr. abhijjhita in med. function] one who covets M <smallcaps>i.</smallcaps> 287 (T. abhijjhātar, v. l. °itar) = A <smallcaps>v.</smallcaps> 265 (T. °itar, v. l. °ātar).\najja Ajja，& Ajjā（adv.）[Vedic adya & adyā，a + dyā，a° being base of demonstr. pron. （see a3）and dyā an old Loc. of dyaus（see diva），thus“on this day”] to-day，now Sn.75，153，158，970，998；Dh.326；J.I，279；III，425（read bahutaṁ ajjā；not with Kern，Toev. s. v. as“food”）；Pv.I，117（= idāni PvA.59）；PvA.6， 23；Mhvs 15，64. ‹-› Freq. in phrase ajjatagge（= ajjato + agge（?）or ajja-tagge， see agga3）from this day onward，henceforth Vin.I，18；D.I，85；DA.I，235. –kālaṁ（adv.）this morning J.VI，180；–divasa the present day Mhvs 32，23. （Page 10）\ngūhanā Gūhanā，（f.）[abstr．fr．gūhati]=gūhanā（q．v.）Pug．19．Cp． pari°．（Page 253）\npacati Pacati，[Ved．pacati，Idg．*peqǔō，Av．pac-；Obulg．peka to fry，roast， Lith，kepū bake，Gr．pέssw cook，pέpwn ripe] to cook，boil，roast Vin．IV，264； fig．torment in purgatory（trs．and intrs．）：Niraye pacitvā aer roasting in N．S． II，225，PvA．10，14．– ppr．pacanto tormenting，Gen．pacato（+Caus． pācayato）D．I，52（expld at DA．I，159，where read pacato for paccato，by pare daṇḍena pīḷentassa）．– pp．pakka（q．v．）．‹-› Caus．pacāpeti & pāceti（q．v．）． – Pass．paccati to be roasted or tormented（q．v．）．（Page 382）\n79"
    }, {
      "heading" : "8.3 Results",
      "text" : ""
    }, {
      "heading" : "8.3.1 N-Gram Language Models",
      "text" : "For the n-gram language model approach, the identified language is indicated in parentheses. e language abbreviations are:\nAbbreviation Language\nAR Arabic DE German EL Greek EN English ES Spanish FI Finnish FR French IT Italian PL Polish RU Russian UK Ukrainian TR Turkish TrAM Transliterated Amharic TrEL Transliterated Greek ZH Chinese\nData: Latin script: German – English\n• (EN) own., belly, refer, buon, But, it, or, your, at, in, ”staring, anyone, doesn’t, else’s, word, this\n• (FI) –\n• (FR) case, just, means, navel”.\n• (TrAM) e\n• (TrEL) to, German\n• (other) Nabelschau, ”navel-gazing”\n80\nData: Latin script: German – Finnish – Turkish • (DE) ob, oder, Sommer, und, Nord-, arktischen, der, Der, dem, gemäßigten, mit, er, Südsommer., spricht, Jahreszeiten, Südwinter, herrscht, wärmste, vom, die, sta., nachdem, auf\n• (EN) ist, Nordsommer, Mart, in\n• (ES) en, depo\n• (FI) joulu-, kevään, suvi, on, eli, vuodenajoista, syksyn, koska, kesä-., kuin, Pohjoisella, man, helmikuu., tammi-, lämpimin, heinä-, niin, maapallo, maan, pinnalle, Kesä, säteilee, tavallisesti, vuodenaika, kallistunut, lasketaan, muina, eiği, jyrkemmässä, elokuu, välissä., eä, eteläisellä, silloin, ja, kulmassa\n• (FR) vier, Je\n• (PL) aurinko\n• (RU) 22, 21\n• (TR) yaklaşık, ortaya, genellikle, Eylül, Sıcak, çıkar., Yaz, sonra, arasında, Kuzey, Güney, Aralık, gerade, ısıyı, gerçekleşir., Küre’de, günler, için, findet, mevsimdir., arasındadır., Haziran, iki, yazda, uzun, ise, ay, sıcak, ile, Yarım, Dünya\n• (TrAM) Der\n• (other) Klimazone., gleichzeitig,kesäkuukausiksi, vuodenaikoina., pallonpuoliskolla,Südhalbkugel\nData: Latin script: English – French • (EL) ”coarse”\n• (EN) but, both, for, while, wines, almost, sweet, of, although, only, is, ”rough”, used)., or, as, meaning, the, in, translate, ”hard”., their, English, also, different., very\n• (ES) can\n• (FI) mean\n• (FR) opposite, Doux, doux, sucré, :\n• (RU) ”so”\n• (TrEL) mou\n• (other) (otherwise, (rugueux)\n81\nData: Latin script: English – Transliterated Greek\n• (EN) for, meanings, least, used, been, distinct, love, of, were, are, when, agápe, these, how, and, Greek, word, used., outside, ways, different, other, follows., words, respective, generally, However, is, with, it, at, as, historically, the, in, which, their\n• (ES) has, separate\n• (FR) language, senses, Ancient, languages, difficult, four\n• (IT) contexts.\n• (TrAM) éros, e, love:\n• (TrEL) to, storgē., philía\n• (other) Nonetheless, distinguishes\nData: Latin script: Italian – German\n• (DE) drohe., geben., allgemeiner, Studie, jüngst, ür, Ergebnis, keine, kam, drohenden, oder, und, letzter, neue, Mythos?, Deutschland, Ist, sich, der, vergeht, studierà, Dabei, Studie, den, dem, auch, Entwarnung, dass, nur, eher, nicht, gibt., Umfrage, Woche, eine, Kaum, Jahren, bei, mehren, Stimmen, Deutsche, das, zum, mehr”, angemahnte, ”ein, Zeit, ein, So, vom, zu, die, seit, Warnung, Wissenscha\n• (EL) affrescò\n• (EN) moto, aento, a, in, ad, also\n• (ES) custodisce, cura, subito, Certo, Giuda, lo, del, difesa, con, definire, restauro, se, modo., la, arginato., recente, vada, movimento, Leonardo, Szenario, quel, cominciò\n• (FI) va, si, Baista, ema\n• (FR) l’esempio, non, des, acque, perché, un, es, le, sui, condanna\n• (IT) solo, faceva, caurata, chiave, peccato), periture, (il, delicatezza, cancro, privato, bellissima, anni, bacini, ovvero, delle, sogno, di, barbaglio, ma, qualche, e, amore, ricerche, Come, per, richiamano, ne, intuizioni, punte, occhio, struggente:, nelle, vita, riccioli, solo, che, volare?, sono, alla, alle, anche, Cenacolo, quello, cosa, ali, viene, il, psicologia, vinciano, Venezia\n82\n• (PL) i\n• (TR) ha, più, da\n• (TrAM) Milano, E\n• (TrEL) poi, idee, stessa\n• (other)MINT-Berufen, Fachkräemangel, dell’angelo:, consapevole, anti-Turchi., Annunciazione, lunghissimo, consapevolezza, ossessionava, dell’aureola, approfonditamente, autodistruggersi, rivoluzionaria, Stierverbands, all’insù, Naturwissenschalern, Ingenieuren\nData: Mixed script: Greek – Russian\n• (EL) κείμενα, βαλκανικό, από, το, αιώνα, Αποτελεί, ελληνική, μία, επίσης, στον, γλωσσικό, γλωσσών., είναι, Στην, έχουμε, μέλος, ανεξάρτητου, τις, γλώσσες., 15ο, Ανήκει, γραπτά, π.Χ., σήμερα., γλώσσα, γλώσσα, κλάδου, οικογένειας, τον, της, δεσμό., μέχρι, μοναδικό, ενός\n• (RU) слов., с, богатейшая, образованного, человека., этапах, значительное, знание, научных, лексика)., называемая, технических, источником, стал, латинских, существования, слова, греческом, всех, —, В, романских, новых, Римской, и, проникали, в, греческие, терминов, присутствует, греческих, новое, русский, империи, латинском, литература., создана, создания, путями, основном, язык., язык, (так, его, количество, считалось, обязательным, время, двумя, была, греческого, большое, языке, языка\n• (TrAM) Η\n• (UK) лексику, (наряду, через, всякого, а, На, для, на\n• (other) ινδοευρωπαϊκές, ινδοευρωπαϊκής,латинским), международную, международная, церковнославянский, заимствований, древнегреческий\nData: Mixed script: English – Greek\n• (DE) Symposium, Modern, being, felt\n• (EL) ”Form”\n83\n• (EN) sensually, platonic, for, holding, existence;, refined, its, explained, araction, of, (even, are, spiritual, given, refer, Agape, beauty, or, araction.”, like, without, not, further, will, own, love, knowledge, will, one’s, most, use, express, is, another.”, e, leads, truth, suggesting, dating, relationships, inspired, ”love, mostly, hence, definition:, regard., appreciation, a, ideal, us, helps, seek, Agápe, plane, recall, feeling, within, returned, chapter,”, based, described, apply, physical, Although, good, by, used, love, God.”, children., his, any, charity;, Socrates, be, work, throughout, and, that, Greek, even, word, agápē), love.”, known, biblical, feelings, does, famous, In, subject, becomes, one, understanding, children, ”love, through, beauty, well, It, was, initially, feast., finding, itself., 13, all, ”without, feel, with, is, it, thus, New, as, the, brotherly, in, is, an, there, God, youthful, necessary, high, Lovers, also, Whether\n• (ES) person, Aquinas, esp., continues, has, omas, truth, can, erotic, sexual, desire\n• (FI) on, –, man, mean\n• (FR) (ἀγάπη, spouse, not, ancient, marriage., soul, person, content, Christians, Testament, Éros, just, part, type, passage, means, humans, passion.”, aspires, contemplation, contributes, argue, affection\n• (IT) texts, 1, ”intimate, Plato, ”to\n• (RU) (ἔρως\n• (TR) talk\n• (TrAM) érōs), ”love:\n• (TrEL) ”erotas”, denote, eros., to, eros\n• (other) non-corporeal, Corinthians, self-benefit)., benevolence., unconditional, philosophers, transcendence.\nData: Mixed script: English – Spanish – Arabic\n• (AR) ,تاملك ,نم ,ءامدق ,ةدحاو ,تامالعلا ,ةمالعلا ,يغبني ,و ,اومدختسأ ,رثكأ ,ةيال ,نود ,نأف ,مسرلا ,يذلا ,اهتلاسر ,نييرصملا ةجاحلا ,نيعم ,قيرغألاو ,امومعو ,لقنت ,فورعملا ,ةرظنب ,نع ,ربعي ,مدختسا ,ينعي ,ءيش ,نأ ,مه ,زمرلا ,نكلو\n• (EN) for, used, been, displaying, of, ribbon, black, or, mourning., statement., tragedies, is, political, a, Wearing, as, mourning\n• (ES) por, has, crespón, sociedades, personas, sentimiento, representando, estados, de, El, señal, lazo, símbolo, en, utilizado, y\n84\n• (FR) remembrance, remembrance, un, es\n• (IT) negro, duelo., POW/MIA\n• (TrAM)\n• (TrEL) symbol, o\n• (other) político-social, organizaciones\nData: Mixed script: English – Chinese • (DE)机;, Chinese:, Western\n• (EL)機)\n• (EN) Some, for, meanings, by, of, are, 8, positive, speaking, be, composed, or, meanings., tradition, number, and, that, sound, linguists, word, some, this, other, In, have, invoked, criticized, 6, because, e, believed, words, numbers, sounds, frequently, is, pronounced, besides, traditional, the, in, represent, two, motivational, usage, their, based\n• (ES)危機;, has,危机;, can, Chinese, ”crisis”, similar\n• (FI) on\n• (FR) (吉利), component, ”danger”, characters, (不利), certain, jī\n• (PL) pinyin:\n• (RU) 0, 9, wēijī)\n• (TrEL) to, to., names, name\n• (other) inauspicious, ”opportunity”., (simplified, auspicious\nData: Mixed script: Ukrainian – Russian • (RU) Польшей, Румынией, Венгрией, юге, границу, с, омывается, Имеет, 9 —13, Молдавией., Азовского, водами, Россией, Чёрного, Русь, и, пунктом, Словакией\n• (TrAM) й\n• (UK) держави, скіфів, України, народів, На, державності, вважається, відправним, території, української, готів, культури, але, сарматів, існували, століття., Київська, на, Віддавна, інших, та, морей.\n• (other) сухопутную, Белоруссией\n85\nData: Pali: abbha\n• (AR) ., 134., 289.\n• (DE) Miln), imber, dark), Miln\n• (EL) (=, (abbhaŋ\n• (EN) water, mountain, of, free, (used, or, like, referred, (also, A, is, cloudy, clouds, later, a, froth, 1, summit, thundering, by, mass, Pv, Oir, obscure, scum, that, water]., thick, As, from, It, is, at, as, the, in, clouds, things, also\n• (ES) (dense, f., sense, expl, rajo\n• (FI) 239., rain;, Lat., Vin, perhaps, SnA\n• (FR) cloud, Dh, adj., point, cloud, Dhs, A), rain, VvA, DhsA, list\n• (IT) \\”dark, &, ambha, 3, 1, 317, J, sunshine, cp., abhra, [Vedic, (megho\n• (PL) 487, =, S, 295, <br, moon–, 249\n• (RU) 348, 53\n• (TR) viz., ambu, Vv\n• (TrAM) 687, PvA, (°sama, 101, (nīl°, (cp., 64;, (nt.), 581, m., Sn, 1064;\n• (TrEL) , Gr., Sk., Idg., to, pabbata, nt.\n• (UK) 12)., 273, 617, 348)., 250;, 251)., 382).\n• (other) <b> –saŋvilāpa </b>, <b> –mua </b>, <smallcaps> vi. </smallcaps>, (mahiyā, <smallcaps> iv. </smallcaps>, cloud\\”;, <b> Rāhu </b>, <b> abbhā </b>, <b> abbhaŋ, <superscript> 9 </superscript>, marajo </b>, abbhāmua, valāhaka);, <smallcaps> i. </smallcaps>, <b> abbhāmaa </b>, valāhaka–sikhara, <superscript> s. </superscript>, <smallcaps> ii. </smallcaps>, <b> dhū-, storm– cloud, /><b> –kūṭa </b>, thunder–cloud);, <at>a)fro\\\\s</at>, <b>–paṭala</b>, <at>o)/mbros</at>, nīla–megha, <superscript>1</superscript>, *m̊bhro, \\”dull\\”;, acchādesi);, mahikā</b>, <b> –ghana </b>\n86\nData: Pali: abhijjhitar\n• (DE) v.\n• (EN) A, one, in, who, covets, med., function]\n• (IT) ag., M, fr.\n• (PL) 287, =\n• (RU) 265\n• (TrAM) l., [n.\n• (TrEL) (T.\n• (other) <smallcaps> v. </smallcaps>, abhijjhātar, abhijjhita, °ātar)., <smallcaps> i. </smallcaps>, °itar, °itar)\nData: Pali: ajja\n• (DE)（see, v., being, Ajjā\n• (EN) of, or, and, not, present, Freq., day, this,“on, from, adyā，a, with, as, the, morning, in, day”], an\n• (ES) bahutaṁ,\n• (FI) 32，23., ajjato\n• (FR) Loc., dyaus, 15，64., dyā, pron.\n• (IT) [Vedic, Mhvs, &, –divasa\n• (PL)（=, +, demonstr., s.\n• (RU) III，425, agge（?）\n• (TR) old, adya, 10）, idāni\n• (TrAM) ‹-›\n• (TrEL) phrase, base\n• (UK) a3）\n• (other) onward，henceforth, ajjā；, DA.I，235.,（adv.）, J.I，279；, D.I，85；, ajja-tagge，see, Sn.75，153，158，970，998；, J.VI，180；, PvA.6，23；, –kālaṁ, diva），thus, PvA.59）；, agga3）, Kern，Toev., Pv.I，117, Dh.326；, ajjatagge, （read,（Page, Vin.I，18；, dyā，a°, Ajja，&, to-day，now,“food”）；\n87\nData: Pali: gūhanā\n• (ES) 253）\n• (other) [abstr．fr．gūhati]=gūhanā, Pug．19．Cp．pari°．（Page, （q．v.）, Gūhanā，（f.）\nData: Pali: pacati\n• (EL) 382）\n• (EN) for, aer, roasting, read, roasted, be, or, at, tormented, in\n• (FR) pare, D．I，52\n• (IT) &, pacato, purgatory\n• (TrAM) pāceti, ripe]\n• (TrEL) to, daṇḍena\n• (other) bake，Gr．pέssw,（+Caus．pācayato）,（q．v．）．（Page, DA．I，159，where, Caus．pacāpeti, intrs．）：Niraye, pacitvā, Pass．paccati,（trs．and, tormenting， Gen．pacato, pīḷentassa）．–, fig．torment, cook，pέpwn, Pacati，[Ved．pacati， Idg．*peqǔō，Av．pac-；, paccato，by, ppr．pacanto, cook，boil，roast, fry， roast，Lith，kepū,（q．v．）．–,（expld, Vin．IV，264；, Obulg．peka, pp． pakka,（q．v．）．‹-›, N．S．II，225，PvA．10，14．–\nData: Twier 1 (Greek–English)\n• (DE) Internet\n• (EL) στο, τη, αυτή, διαγωνισμό, λύση, ψήφισα\n• (EN) of, IT, ings\n• (ES) BUSINESS\n• (TrAM) Μόλις\n• (other) EXCELLENCE.\n88\nData: Twier 2 (French–English)\n• (EN) David, ”e, is, it, perish;, or, collective, Demain, counts?”, that, of, dynamics, all\n• (FI) 18h\n• (FR) par, Keynote\n• (other) #dhiha6, @dhiparis, science-publish\nData: Twier 3 (French–English)\n• (EN) for, Food, waiting, the, in, ready, and, are\n• (ES) go\n• (FI) Edmonton\n• (FR) just, breuvages, fans\n• (TrEL) to\n• (other) #bilingualism, #FWWC2015\nData: Twier 4 (English–Polish)\n• (EN) with, back, from, comes, crates, and, poland, two, of, jackets\n• (ES) dad, adidas\n• (TrAM) my\n• (TrEL) omg\n• (other) żubrówka, strawberries\nData: Twier 5 (Transliterated Amharic–English)\n• (EN) is, bread).\n• (FR) our\n• (IT) (coffee\n• (PL) naw\n• (TrAM) Buna, dabo\n89"
    }, {
      "heading" : "8.3.2 Textcat",
      "text" : "For Textcat, the identified language is indicated in parentheses. As Textcat returns unknown for many words, I merely indicate the non-unknown categories to save space and write rest to indicate that all other words of the text have been classified as unknown. e language abbreviations are:\nAbbreviation Language\nDA Danish DE German EL Greek EN English ES Spanish FI Finnish FR French HU Hungarian ID Indonesian IT Italian LT Lithuanian LV Latvian NL Dutch PT Portuguese RU Russian TH ai ZH Chinese\nData: Latin script: German – English\n• (HU) “navel-gazing”\n• (ZH) Nabelschau\n• (unknown) rest\nData: Latin script: German – Finnish – Turkish\n• (DA) Südsommer., genellikle,\n• (DE) Jahreszeiten, arktischen,\n• (FI) vuodenajoista, kallistunut, tavallisesti,\n90\n• (ZH) gemäßigten, Klimazone., Südhalbkugel, Nordsommer, gleichzeitig, vuodenaika, jyrkemmässä, vuodenaikoina., Pohjoisella, pallonpuoliskolla, kesäkuukausiksi, eteläisellä, mevsimdir., gerçekleşir., arasındadır.,\n• (unknown) rest\nData: Latin script: English – French\n• (HU) different.,\n• (ZH) (rugueux),(otherwise,\n• (unknown) rest\nData: Latin script: English – Transliterated Greek\n• (EN) historically, respective,\n• (LT) languages,\n• (ZH) distinguishes, Nonetheless,\n• (unknown) rest\nData: Latin script: Italian – German\n• (DE) allgemeiner, angemahnte,\n• (ES) delicatezza,\n• (HU) bellissima,\n• (IT) dell’aureola, consapevole, richiamano, anti-Turchi., ossessionava,\n• (NL) Ingenieuren,\n• (PT) approfonditamente,\n• (ZH) custodisce, struggente:, rivoluzionaria, psicologia, consapevolezza, autodistruggersi, lunghissimo, Fachkräemangel, Deutschland, intuizioni, Entwarnung, Stierverbands, Wissenscha, MINT-Berufen, Annunciazione, dell’angelo:, Naturwissenschalern,\n• (unknown) rest\n91\nData: Mixed script: Greek – Russian\n• (EL) ανεξάρτητου, οικογένειας,\n• (RU) существования, богатейшая, литература., греческого, обязательным, образованного, присутствует, количество, заимствований, значительное, источником, технических, называемая, международная,\n• (TH) латинским),\n• (ZH) ινδοευρωπαϊκές, ινδοευρωπαϊκής, древнегреческий, международную, церковнославянский,\n• (unknown) rest\nData: Mixed script: English – Greek\n• (DA) definition:, understanding,\n• (EN) affection, unconditional, suggesting,\n• (FR) relationships, contemplation, appreciation, araction, araction.”, transcendence.,\n• (HU) benevolence., self-benefit).,\n• (IT) non-corporeal,\n• (PT) contributes,\n• (ZH) Corinthians, throughout, Christians, Symposium, existence;, philosophers,\n• (unknown) rest\nData: Mixed script: English – Spanish – Arabic\n• (ES) sociedades, organizaciones, sentimiento, político-social,\n• (FR) remembrance, remembrance, statement.,\n• (ID) displaying,\n• (PT) representando,\n• (unknown) rest\n92\nData: Mixed script: English – Chinese\n• (EN) traditional, motivational, pronounced, tradition„\n• (FR) characters,\n• (ZH) simplified, frequently, ”opportunity”., criticized, auspicious, inauspicious,\n• (unknown) rest\nData: Mixed script: Ukrainian – Russian\n• (RU) державності, Словакией, Молдавией.,\n• (TH) вважається,\n• (ZH) відправним, української, сухопутную, Белоруссией,\n• (unknown) rest\nData: Pali: abbha\n• (DA) storm–cloud, thundering,\n• (HU) marajo</b>, nīla–megha, valāhaka–sikhara,\n• (ZH) <at> a)fro\\\\</at>, <at> o)/mbros </at>, <smallcaps> ii. </smallcaps>, mahikā</b>, <b> Rāhu </b>, <smallcaps> i. </smallcaps>, thunder–cloud);, <smallcaps> vi. </smallcaps>, acchādesi);, <smallcaps> iv. </smallcaps>, <superscript> 9 </superscript>, <b> abbhā </b>, <superscript> s. </superscript>, valāhaka);, <b> abbhāmaa </b>, /><b> –kūṭa </b>, <superscript> 1 </superscript>, <b> –ghana </b>, <b> –paṭala </b>, <b> –mua </b>, abbhāmua, <b> –saŋvilāpa </b>\n• (unknown) rest\nData: Pali: abhijjhitar\n• (ZH) abhijjhita, <smallcaps> i. </smallcaps>, abhijjhātar, <smallcaps> v. </smallcaps>,\n• (unknown) rest\n93\nData: Pali: ajja\n• (ZH) diva），thus, to-day，now, Sn.75，153，158，970，998；, Kern，Toev., ajja-tagge，see, onward，henceforth,\n• (unknown) rest\nData: Pali: gūhanā\n• (ZH) Gūhanā，（f.）, [abstr．fr．gūhati]hanā, Pug．19．Cp．pari°．（Page,\n• (unknown) rest\nData: Pali: pacati\n• (ZH) fig．torment, Pacati，[Ved．pacati，Idg．*peqǔō，Av．pac-；, Obulg．peka, fry，roast，Lith，kepū, bake，Gr．pέssw, cook，pέpwn, cook，boil，roast, Vin． IV，264；, intrs．）：Niraye, N．S．II，225，PvA．10，14．–, ppr．pacanto, tormenting，Gen．pacato,（+Caus．pācayato）, DA．I，159，where, paccato，by, pīḷentassa）．–,（q．v．）．‹-›, Caus．pacāpeti, Pass．paccati,（q．v．）．（Page,\n• (unknown) rest\nData: Twier 1 (Greek–English)\n• (ZH) διαγωνισμό, EXCELLENCE.,\n• (unknown) rest\nData: Twier 2 (French–English)\n• (IT) collective,\n• (ZH) science-publish,\n• (unknown) rest\nData: Twier 3 (French–English)\n• (ZH) #bilingualism,\n• (unknown) rest\n94\nData: Twier 4 (English–Polish)\n• (LV) strawberries,\n• (unknown) rest\nData: Twier 5 (Transliterated Amharic–English)\n• (unknown) rest"
    }, {
      "heading" : "8.3.3 Clustering",
      "text" : "Clustering the different data sets produced the following clusters. e second run uses the clusters from the first run and possibly subdivides each cluster into two or more clusters.\nData: Latin script: German – English"
    }, {
      "heading" : "First run",
      "text" : "• “navel-gazing”, doesn’t, else’s\n• “staring, But, German, Nabelschau, anyone, belly, buon, case, just, means, navel”., own., refer, this, word, your\n• at, in, it, or, to\n• –, e\nSecond run\n• doesn’t, else’s\n• “navel-gazing”\n• “staring, But, German, Nabelschau, belly, case, means, navel”., refer, this\n• anyone, buon, just, own., word, your\n• it, or, to\n• at, in\n• –, e\n95\nData: Latin script: German – Finnish – Turkish"
    }, {
      "heading" : "First run",
      "text" : "• Dünya, Güney, Küre’de, Südhalbkugel, Südsommer., Südwinter, Sıcak, arasında, gemäßigten, günler, için, kesäkuukausiksi, lämpimin, säteilee, sıcak, wärmste, çıkar., Der\n• Aralık, Eylül, Kesä, Yarım, arasındadır., eteläisellä, eiği, eä, gerçekleşir., heinä, jyrkemmässä, kesä-., kevään, välissä., yaklaşık, ısıyı\n• 21, 22\n• Der, Haziran, Jahreszeiten, Je, Klimazone., Kuzey, Mart, Nord-, Nordsommer, Pohjoisella, Sommer, Yaz, arktischen, auf, aurinko, ay, dem, depo, der, die, eli, elokuu, en, er, findet, genellikle, gerade, gleichzeitig, helmikuu., herrscht, iki, ile, in, ise, ist, ja, joulu-, kallistunut, koska, kuin, kulmassa, lasketaan, maan, maapallo, man, mevsimdir., mit, muina, nachdem, niin, ob, oder, on, ortaya, pallonpuoliskolla, pinnalle, silloin, sonra, spricht, sta., suvi, syksyn, tammi-, tavallisesti, und, uzun, vier, vom, vuodenaika, vuodenaikoina., vuodenajoista, yazda"
    }, {
      "heading" : "Second run",
      "text" : "• Südhalbkugel, Südsommer., Südwinter, arasında, gemäßigten, kesäkuukausiksi, lämpimin, säteilee, wärmste\n• Dünya, Güney, Küre’de, Sıcak, günler, için, sıcak, çıkar., Der\n• arasındadır., eteläisellä, eiği, eä, gerçekleşir., heinä-, jyrkemmässä, kesä-., kevään, välissä., yaklaşık, ısıyı\n• Aralık, Eylül, Yarım\n• Kesä\n• 22\n• 21\n• Der, Haziran, Jahreszeiten, Klimazone., Kuzey, Mart, Nord-, Nordsommer, Pohjoisella, Sommer, Yaz,\n96\n• arktischen, auf, aurinko, dem, depo, der, die, eli, elokuu, findet, genellikle, gerade, gleichzeitig, helmikuu., herrscht, iki, ile, ise, ist, joulu-, kallistunut, koska, kuin, kulmassa, lasketaan, maan, maapallo, man, mevsimdir., mit, muina, nachdem, niin, oder, ortaya, pallonpuoliskolla, pinnalle, silloin, sonra, spricht, sta., suvi, syksyn, tammi-, tavallisesti, und, uzun, vier, vom, vuodenaika, vuodenaikoina., vuodenajoista, yazda\n• Je, ay, en, er, in, ja, ob, on\nData: Latin script: English – French"
    }, {
      "heading" : "First run",
      "text" : "• ”coarse”, ”hard”., ”rough”, ”so”, (otherwise, (rugueux), Doux, English, almost, also, although, both, but, can, different., doux, for, mean, meaning, mou, only, opposite, sucré, sweet, the, their, translate, used)., very, while, wines\n• is, or\n• as, in, of\nSecond run\n• Doux, English,\n• “coarse”, (otherwise, (rugueux), almost, although, different., meaning, opposite, translate\n• “hard”., ”rough”, ”so”, also, both, but, can, doux, for, mean, mou, only, sucré, sweet, the, their, used)., very, while, wines\n• or\n• is\n• in\n• of\n• as\n97\nData: Latin script: English – Transliterated Greek\nFirst run\n• e\n• agápe, philía, storgē., éros,\n• Ancient, However, Nonetheless, contexts., different, difficult, distinct, distinguishes, follows., generally, historically, language, languages, meanings, outside, respective, senses, separate, which, words\n• Greek, and, are, as, at, been, for, four, has, how, in, is, it, least, love, love:, of, other, the, their, these, to, used, used., ways, were, when, with, word\nSecond run\n• e\n• philía, storgē.\n• agápe, éros,\n• Ancient, However, Nonetheless, contexts., different, difficult, distinct, distinguishes, follows., generally, historically, meanings, respective\n• words\n• language, languages, outside, senses, separate, which\n• and, are, as, at, been, for, four, has, how, in, is, it, least, love, love:, of, other, the, their, these, to, used, used., ways, were, when, with, word\n• Greek\nData: Latin script: German – Italian"
    }, {
      "heading" : "First run",
      "text" : "• (il, E, So, a, ad, da, di, e, es, ha, i, il, in, la, le, lo, ma, ne, se, si, un, va, zu\n98\n• “ein , Annunciazione, Baista, Cenacolo, Certo, Come, Dabei, Deutsche, Deutschland, Entwarnung, Ergebnis, Giuda, Ingenieuren, Ist, Jahren, Kaum, Leonardo, MINT-Berufen, Mythos?, Naturwissenschalern, Stierverbands, Stimmen, Studie, Studie, Szenario, ema, Umfrage, Venezia, Warnung, Wissenscha, Woche, Zeit, acque, ali, alla, alle, allgemeiner, also, amore, anche, angemahnte, anni, anti-Turchi., approfonditamente, arginato., aento, auch, autodistruggersi, bacini, barbaglio, bei, bellissima, cancro, caurata, che, chiave, con, condanna, consapevole, consapevolezza, cosa, cura, custodisce, das, dass, definire, del, delicatezza, delle, dem, den, der, des, die, difesa, drohe., drohenden, eher, ein, eine, faceva, geben., gibt., idee, intuizioni, kam, keine, letzter, lunghissimo, mehr”, mehren, modo., moto, movimento, nelle, neue, nicht, non, nur, occhio, oder, ossessionava, ovvero, peccato), per, periture, poi, privato, psicologia, punte, qualche, quel, quello, recente, restauro, riccioli, ricerche, richiamano, rivoluzionaria, seit, sich, sogno, solo, solo, sono, stessa, struggente:, subito, sui, und, vada, vergeht, viene, vinciano, vita, volare?, vom, zum\n• all’insù, dell’angelo:, dell’aureola, l’esempio, Milano\n• Fachkräemangel, affrescò, cominciò, ür, jüngst, perché, più, studierà"
    }, {
      "heading" : "Second run",
      "text" : "• a, e, i\n• E\n• So\n• (il, ad, da, di, es, ha, il, in, la, le, lo, ma, ne, se, si, un, va, zu\n• Annunciazione, Baista, Cenacolo, Certo, Come, Dabei, Deutsche, Deutschland, Entwarnung, Ergebnis, Giuda, Ingenieuren, Ist, Jahren, Kaum, Leonardo, MINTBerufen, Mythos?, Naturwissenschalern, Stierverbands, Stimmen, Studie, Studie, Szenario, ema, Umfrage, Venezia, Warnung, Wissenscha, Woche, Zeit\n• “ein, acque, ali, alla, alle, allgemeiner, also, amore, anche, angemahnte, anni, anti-Turchi., approfonditamente, arginato., aento, auch, autodistruggersi, bacini, barbaglio, bei, bellissima, cancro, caurata, che, chiave, con, condanna, consapevole, consapevolezza, cosa, cura, custodisce, das, dass, definire, del, delicatezza, delle, dem, den, der, des, die, difesa, drohe., drohenden, eher, ein, eine, faceva, geben., gibt., idee, intuizioni, kam, keine, letzter, lunghissimo, mehr”, mehren, modo., moto, movimento, nelle, neue, nicht, non, nur, occhio, oder, ossessionava, ovvero, peccato), per, periture, poi, privato, psicologia,\n99\npunte, qualche, quel, quello, recente, restauro, riccioli, ricerche, richiamano, rivoluzionaria, seit, sich, sogno, solo, solo, sono, stessa, struggente:, subito, sui, und, vada, vergeht, viene, vinciano, vita, volare?, vom, zum\n• all’insù, dell’angelo:, dell’aureola, l’esempio, Milano\n• Fachkräemangel\n• affrescò, cominciò, jüngst, perché, studierà\n• ür\n• più\nData: Mixed script: Greek – Russian\nFirst run\n• 15ο,—, Η\n• το, В, На, а, в, и, на, с\n• (наряду, (так, γλωσσών., γλώσσα, γλώσσες., δεσμό., π.Χ., σήμερα., заимствований, латинским), лексика)., литература., слов., человека., язык.\n• Ανήκει, Αποτελεί, Στην, έχουμε, αιώνα, ανεξάρτητου, από, βαλκανικό, γλωσσικό, γλώσσα, γραπτά, είναι, ελληνική, ενός, επίσης, ινδοευρωπαϊκές, ινδοευρωπαϊκής, κείμενα, κλάδου, μέλος, μέχρι, μία, μοναδικό, οικογένειας, στον, της, τις, τον, Римской, богатейшая, большое, была, время, всех, всякого, греческие, греческих, греческого, греческом, двумя, для, древнегреческий, его, знание, значительное, империи, источником, количество, латинских, латинском, лексику, международная, международную, называемая, научных, новое, новых, образованного, обязательным, основном, присутствует, проникали, путями, романских, русский, слова, создана, создания, стал, существования, считалось, терминов, технических, церковнославянский, через, этапах, язык, языка, языке\nSecond run\n• 15ο\n• —\n• Η\n100\n• а, в, и, с\n• В\n• το, На, на\n• (наряду, (так\n• γλωσσών., γλώσσα, γλώσσες., δεσμό., π.Χ., σήμερα., заимствований, латинским), лексика)., литература., слов., человека., язык.\n• έχουμε, αιώνα, ανεξάρτητου, από, βαλκανικό, γλωσσικό, γλώσσα, γραπτά, είναι, ελληνική, ενός, επίσης, ινδοευρωπαϊκές, ινδοευρωπαϊκής, κείμενα, κλάδου, μέλος, μέχρι, μία, μοναδικό, οικογένειας, στον, της, τις, τον\n• Ανήκει, Αποτελεί, Στην\n• богатейшая, греческие, греческих, греческого, греческом, древнегреческий, значительное, источником, количество, латинских, латинском, международная, международную, называемая, образованного, обязательным, основном, присутствует, проникали, романских, создания, существования, считалось, терминов, технических, церковнославянский\n• Римской, большое, была, время, всех, всякого, двумя, для, его, знание, империи, лексику, научных, новое, новых, путями, русский, слова, создана, стал, через, этапах, язык, языка, языке\nData: Mixed script: English – Greek"
    }, {
      "heading" : "First run",
      "text" : "• “intimate, “without, Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether, affection, ancient, another.”, appreciation, aspires, araction, araction.”, becomes, benevolence., biblical, brotherly, chapter,”, charity;, children, children., contemplation, content, continues, contributes, definition:, described, existence;, explained, express, feeling, feelings, finding, further, holding, initially, inspired, knowledge, marriage., necessary, non-corporeal, passage, passion.”, philosophers, physical, platonic, refined, relationships, returned, self-benefit)., sensually, spiritual, subject, suggesting, through, throughout, transcendence., unconditional, understanding, without, youthful\n• (ἀγάπη, (ἔρως, Agápe, agápē), Éros, érōs), –\n101\n• “Form”, “erotas”, “love, “love, “love:, (even, Agape, Greek, Lovers, Modern, Plato, is, omas, also, apply, argue, based, beauty, beauty, being, dating, denote, desire, does, eros, eros., erotic, even, famous, feast., feel, felt, given, good, helps, hence, high, humans, ideal, itself., just, known, leads, like, love, love, love.”, mean, means, most, mostly, one’s, part, person, person, plane, recall, refer, regard., seek, sexual, soul, spouse, talk, texts, that, there, thus, truth, truth, type, used, well, will, will, with, within, word, work\n• “to, 1, 13, God, God.”, In, It, New, e, a, all, an, and, any, are, as, be, by, can, esp., for, has, his, in, is, is, it, its, man, not, not, of, on, one, or, own, the, to, us, use, was"
    }, {
      "heading" : "Second run",
      "text" : "• affection, ancient, another.”, aspires, becomes, biblical, chapter,”, charity;, children, children., content, definition:, feeling, feelings, finding, holding, marriage., necessary, passage, passion.”, platonic, refined, returned, subject, through, without\n• Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether\n• “intimate, appreciation, araction, araction.”, benevolence., brotherly, contemplation, continues, contributes, described, existence;, explained, express, further, initially, inspired, knowledge, non-corporeal, philosophers, physical, relationships, self-benefit)., sensually, spiritual, suggesting, throughout, transcendence., unconditional, understanding, youthful\n• Agápe, agápē), Éros, érōs)\n• (ἀγάπη, (ἔρως\n• –\n• “erotas”, beauty, beauty, dating, denote, desire, erotic, famous, humans, itself., mostly, person, person, recall, regard., sexual, spouse, within\n• “Form”, Agape, Greek, Lovers, Modern, Plato, is, omas, based, being, feast., hence, ideal, leads, means, plane, refer, there\n• apply, felt, helps, high, just, known, most, part, talk, texts, that, thus, truth, truth, type, well, will, will, with, word, work\n• “love, “love, “love:, (even, also, argue, does, eros, eros., even, feel, given, good, like, love, love, love.”, mean, one’s, seek, soul, used\n102\n• 1, 13, In, It\n• “to, a, an, as, be, by, in, is, is, it, of, on, or, to, us\n• God, God.”, New, e, all, and, any, esp., its, own, the\n• are, can, for, has, his, man, not, not, one, use, was\nData: Mixed script: English – Spanish – Arabic"
    }, {
      "heading" : "First run",
      "text" : "• El, POW/MIA, Wearing, a, as, been, black, de, displaying, duelo., en, es, estados, for, has, is, lazo, mourning, mourning., negro, o, of, or, organizaciones, personas, political, por, remembrance, remembrance, representando, ribbon, sentimiento, sociedades, statement., symbol, tragedies, un, used, utilizado, y\n• crespón, político-social, señal, símbolo\n• A\n• ،اهتلاسر ،نود ،لقنت ،ةرظنب ،فورعملا ،نييرصملا ،ةمالعلا ،تامالعلا ،زمرلا ،مسرلا ،يذلا ،ةجاحلا ،مدختسا ،نأ ،رثكأ ،اومدختسأ يغبني ،ينعي ،ربعي ،نكلو ،امومعو ،قيرغألاو ،ةدحاو ،و ،مه ،نم ،نيعم ،ةيال ،تاملك ،ءامدق ،نأف ،نع ،ءيش"
    }, {
      "heading" : "Second run",
      "text" : "• a, o, y\n• El, as, de, en, es, is, of, or, un\n• Wearing, been, black, displaying, duelo., estados, for, has, lazo, mourning, mourning., negro, organizaciones, personas, political, por, remembrance, remembrance, representando, ribbon, sentimiento, sociedades, statement., symbol, tragedies, used, utilizado\n• POW/MIA\n• político-social, símbolo\n• crespón, señal\n• A\n• امومعو ،قيرغألاو ،اهتلاسر ،فورعملا ،نييرصملا ،ةمالعلا ،تامالعلا ،ةجاحلا ،مدختسا ،اومدختسأ\n• يغبني ،ينعي ،ربعي ،نكلو ،ةدحاو ،نيعم ،ةيال ،تاملك ،ءامدق ،نأف ،ءيش ،نود ،لقنت ،ةرظنب ،زمرلا ،مسرلا ،يذلا ،رثكأ\n103\n• مه ،نم ،نع ،نأ\n• و\nData: Mixed script: English – Chinese"
    }, {
      "heading" : "First run",
      "text" : "• “crisis”, “danger”, “opportunity”., (simplified, Chinese, Chinese:, Western, auspicious, because, believed, besides, certain, characters, component, composed, criticized, frequently, inauspicious, invoked, linguists, meanings, meanings., motivational, number, numbers, pinyin:, positive, pronounced, represent, similar, sounds, speaking, tradition, traditional, wēijī)\n• (不利), (吉利),危机;,危機;,机;,機)\n• 0, 6, 8, 9\n• In, Some, e, and, are, based, be, by, can, for, has, have, in, is, jī, name, names, of, on, or, other, some, sound, that, the, their, this, to, to., two, usage, word, words\nSecond run\n• Chinese, Chinese:\n• Western\n• “crisis”, “danger”, “opportunity”., (simplified, auspicious, because, believed, besides, certain, characters, component, composed, criticized, frequently, inauspicious, invoked, linguists, meanings, meanings., motivational, number, numbers, pinyin:, positive, pronounced, represent, similar, sounds, speaking, tradition, traditional, wēijī)\n• (不利), (吉利)\n• 危机;,危機;\n• 机;,機)\n• 6, 8, 9\n• 0,\n• Some, e, and, are, based, can, for, has, have, name, names, other, some, sound, that, the, their, this, two, usage, word, words\n104\n• In, be, by, in, is, of, on, or, to, to.\n• jī\nData: Mixed script: Ukrainian – Russian\nFirst run\n• 9—13\n• Белоруссией, Венгрией, Молдавией., Польшей, Россией, Словакией, морей., народів, сарматів, скіфів, століття.\n• Азовского, Віддавна, Київська, Румынией, України, , Чёрного, вважається, відправним, , границу, держави, державності, , культури, омывается, пунктом, сухопутную, території, української, існували,\n• Имеет, На, Русь, але, водами, готів, и, й, на, с, та, юге, інших\nSecond run\n• 9—13\n• морей., народів, сарматів, скіфів, століття.\n• Белоруссией, Венгрией, Молдавией., Польшей, Россией, Словакией,\n• Азовского, Віддавна, Київська, Румынией, України, Чёрного, границу, держави, культури, пунктом, існували\n• вважається, відправним, державності, омывается, сухопутную, території, української\n• и, й, с\n• На, на, та\n• але, водами, готів, юге, інших\n• Имеет, Русь\n105\nData: Pali: abbha"
    }, {
      "heading" : "First run",
      "text" : "• (also, (cp., (dense, (megho, (used, (°sama, 1, 1, 101, 1064;, 12)., 134., 239., 249, 250;, 251)., 273, 289., 295, 3, 317, 348, 348)., 382)., 487, 53, 581, 617, 64;, 687, <at> a)fro\\\\s </at>, <at> o)/mbros </at>, <smallcaps> i. </smallcaps>, <smallcaps> ii. </smallcaps>, <smallcaps> iv. </smallcaps>, <smallcaps> vi. </smallcaps>, <superscript> 1 </superscript>, <superscript> 9 </superscript>, <superscript> s. </superscript>, A, A), As, Dh, Dhs, DhsA, Gr., Idg., It, J, Lat., Miln, Miln), Oir, Pv, PvA, S, Sk., Sn, SnA, , is, Vin, Vv, VvA, [Vedic, a, abhra, adj., also, ambha, ambu, as, at, by, cloud, cloud, cloud\\”;, clouds, clouds, cloudy, cp., dark), expl, f., free, from, froth, imber, in, is, later, like, list, m., marajo</b>, mass, moon–, mountain, nt., obscure, of, or, pabbata, perhaps, point, rain, rain;, rajo, referred, scum, sense, storm–cloud, summit, sunshine, that, the, thick, things, thunder–cloud);, thundering, to, viz., water, water].\n• &, (=, <b>–ghana</b>, <b>–mua</b>, <br, =, \\”dark, \\”dull\\”;\n• (abbhaŋ, (mahiyā, (nīl°, <b> –saŋvilāpa </b>, <b> Rāhu </b>, <b> abbhā </b>, <b> abbhāmaa </b>, abbhāmua, acchādesi);, mahikā </b>, nīla–megha, valāhaka);, valāhaka– sikhara\n• *m̊bhrocite /><b>–kūṭa</b>, <b>–paṭala</b>, <b>abbhaŋ, <b>dhū-, (nt.)"
    }, {
      "heading" : "Second run",
      "text" : "• (cp., Dhs, DhsA, Idg., Lat., Miln, Miln), Oir, PvA, SnA, is, Vin, VvA, [Vedic, as, at, by, cp., in, is, nt., of, or, to\n• (also, (dense, (megho, (used, (°sama, <at> a)fro\\\\s </at>, <at> o)/mbros </at>, <smallcaps> ii. </smallcaps>, <smallcaps> iv. </smallcaps>, <smallcaps> vi. </smallcaps>, abhra, adj., also, ambha, ambu, cloud, cloud, cloud\\”;, clouds, clouds, cloudy, dark), expl, free, from, froth, imber, later, like, list, marajo </b>, mass, moon–, mountain, obscure, pabbata, perhaps, point, rain, rain;, rajo, referred, scum, sense, storm– cloud, summit, sunshine, that, the, thick, things, thunder– cloud);, thundering, viz., water, water].\n• 1, 1, 101, 1064;, 12)., 134., 239., 249, 250;, 251)., 273, 289., 295, 3, 317, 348, 348)., 382)., 487, 53, 581, 617, 64;, 687, <superscript> 1 </superscript>, <superscript> 9 </superscript>\n• <smallcaps> i. </smallcaps>, <superscript> s. </superscript>, A, A), As, Dh, Gr., It, J, Pv, S, Sk., Sn, , Vv, a, f., m.\n106\n• <b> –ghana </b>, <b> –mua </b>, <br, \\”dark, \\”dull\\”;\n• &, (=, =\n• (abbhaŋ, (mahiyā, (nīl°, <b> Rāhu </b>, <b> abbhā </b>, nīla–megha\n• <b> –saŋvilāpa </b>, <b> abbhāmaa </b>, abbhāmua, acchādesi);, mahikā </b>, valāhaka);, valāhaka–sikhara\n• *m̊bhro, /><b> –kūṭa </b>, <b> –paṭala </b>, <b> abbhaŋ, <b> dhū-\n• (nt.)\nData: Pali: abhijjhitar"
    }, {
      "heading" : "First run",
      "text" : "• abhijjhita, abhijjhātar, covets, function], med., one, who, °itar), °itar, °ātar).\n• (T., <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, =, A, M, ag., fr., in, l., v.\n• 265, 287\n• [n."
    }, {
      "heading" : "Second run",
      "text" : "• abhijjhita, abhijjhātar, covets, function], med., one, who, °itar), °itar, °ātar).\n• (T., A, M\n• =, l., v.\n• <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, ag., fr., in\n• 265, 287\n• [n.\n107\nData: Pali: ajja"
    }, {
      "heading" : "First run",
      "text" : "• –divasa, Freq., Loc., [Vedic, adya, ajjatagge, ajjato, an, and, as, base, being, day, demonstr., dyaus, from, in, morning, not, of, old, or, phrase, present, pron., the, this, with\n• &, +, Mhvs, s., v.\n• –kālaṁ, 10）, 15，64., 32，23., Ajjā, D.I，85；, DA.I，235., Dh.326；, III，425, J.I， 279；, J.VI，180；, Kern，Toev., Pv.I，117, PvA.59）；, PvA.6，23；, Sn.75，153， 158，970，998；, Vin.I，18；, a3）, adyā，a, agga3）, agge（?）, ajja-tagge，see, ajjā；, bahutaṁ, day”], diva），thus, dyā, dyā，a°, idāni, onward，henceforth, to-day，now,“food”）；,“on, ‹-›, Ajja，&,（=,（Page,（adv.）,（read,（see"
    }, {
      "heading" : "Second run",
      "text" : "• an, as, in, of, or\n• Freq., Loc., [Vedic\n• –divasa, adya, ajjatagge, ajjato, and, base, being, day, demonstr., dyaus, from, morning, not, old, phrase, present, pron., the, this, with\n• &\n• +\n• Mhvs\n• s., v.\n•“on, ‹-›, Ajja，&,（=,（Page,（adv.）,（read,（see\n• –kālaṁ, 10）, 15，64., 32，23., Ajjā, D.I，85；, DA.I，235., Dh.326；, III，425, J.I，279；, J.VI，180；, Kern，Toev., Pv.I，117, PvA.6，23；, Sn.75，153，158， 970，998；, Vin.I，18；, a3）, agga3）, ajja-tagge，see, ajjā；, bahutaṁ, day” ], diva），thus, dyā, idāni, onward，henceforth, to-day，now\n• PvA.59）；, adyā，a, agge（?）, dyā，a°,“food”）；\n108\nData: Pali: gūhanā"
    }, {
      "heading" : "First run",
      "text" : "• 253）, Pug．19．Cp．pari°．（Page, [abstr．fr．gūhati]=gūhanā, Gūhanā，（f.） ,（q．v.）"
    }, {
      "heading" : "Second run",
      "text" : "• 253）, Pug．19．Cp．pari°．（Page, [abstr．fr．gūhati]=gūhanā, Gūhanā，（f.） ,（q．v.）\nData: Pali: pacati"
    }, {
      "heading" : "First run",
      "text" : "• 382）, Caus．pacāpeti, DA．I，159，where, Obulg．peka, Pass．paccati, Vin． IV，264；, bake，Gr．pέssw, cook，boil，roast, cook，pέpwn, daṇḍena, fig． torment, fry，roast，Lith，kepū, intrs．）：Niraye, paccato，by, ppr．pacanto, pp．pakka, pīḷentassa）．–, tormenting，Gen．pacato\n• D．I，52, N．S．II，225，PvA．10，14．–, Pacati，[Ved．pacati，Idg．*peqǔō， Av．pac-；,（+Caus．pācayato）,（expld,（q．v．）．–,（q．v．）．‹-›,（q． v．）．（Page,（trs．and\n• aer, at, be, for, in, or, pacato, pare, purgatory, read, ripe], roasted, roasting, to, tormented\n• &, pacitvā, pāceti"
    }, {
      "heading" : "Second run",
      "text" : "• Caus．pacāpeti, DA．I，159，where, Obulg．peka, Pass．paccati, Vin．IV，264； , bake，Gr．pέssw, cook，boil，roast, cook，pέpwn, daṇḍena, fig．torment, fry， roast，Lith，kepū, intrs．）：Niraye, paccato，by, ppr．pacanto, pīḷentassa）．–, tormenting，Gen．pacato\n• 382）, pp．pakka\n• D．I，52, N．S．II，225，PvA．10，14．–,（q．v．）．–\n• Pacati，[Ved．pacati，Idg．*peqǔō，Av．pac-；,（+Caus．pācayato）,（expld, （q．v．）．‹-›,（q．v．）．（Page,（trs．and\n109\n• for, pacato, pare, read, ripe]\n• aer, purgatory, roasted, roasting, tormented\n• or, to\n• at, be, in\n• &\n• pacitvā, pāceti\nData: Twier 1 (Greek–English)"
    }, {
      "heading" : "First run",
      "text" : "• αυτή, διαγωνισμό, λύση, στο, τη, ψήφισα, Μόλις\n• BUSINESS, EXCELLENCE., IT, Internet, ings, of\nSecond run\n• Μόλις\n• αυτή, διαγωνισμό, λύση, στο, τη, ψήφισα\n• IT, of\n• Internet, ings,\n• BUSINESS, EXCELLENCE.\nData: Twier 2 (French–English)"
    }, {
      "heading" : "First run",
      "text" : "• “e, 18h, @dhiparis, David, Demain, Keynote, all, collective, counts?”, dynamics, par, perish;, science-publish, that\n• is, it, of, or\n110"
    }, {
      "heading" : "Second run",
      "text" : "• “e, @dhiparis, David, Demain, Keynote, all, collective, counts?”, dynamics, par, perish;, science-publish, that\n• 18h\n• is, it, or\n• of\nData: Twier 3 (French–English)\nFirst run\n• Edmonton, Food\n• go, in, to\n• and, are, breuvages, fans, for, just, ready, the, waiting\nSecond run\n• Edmonton, Food\n• to\n• go, in\n• for, just\n• and, are, breuvages, fans, ready, the, waiting\nData: Twier 4 (English–Polish)\nFirst run\n• żubrówka, my\n• adidas, and, back, comes, crates, dad, from, jackets, of, omg, poland, strawberries, two, with\n111\nSecond run\n• żubrówka, my\n• adidas, comes, dad, of\n• and, back, crates, from, jackets, omg, poland, strawberries, two, with\nData: Twier 5 (Transliterated Amharic–English)\nFirst run\n• Buna\n• (coffee, bread)., dabo, is, naw, our\nSecond run\n• Buna\n• our\n• (coffee, bread)., dabo, is, naw"
    }, {
      "heading" : "8.3.4 Language Model Induction",
      "text" : "For all language model induction tasks, the threshold value t has been set t = 0.02 and the silver threshold value s has been set s = 0.1. e other parameters have been set to “maximum iteration count” i = 4, “maximum random iteration count” j = 2 and “merge mode ADD”.\nData: Latin script: German–English\n• e, German, word, Nabelschau, means, or, “staring, at, your, But, in, this, it, doesn’t, refer, to, anyone, else’s, buon, just, your, own.,\n• –\n• “navel-gazing”, navel”., case, belly\n112\nData: Latin script: German–Finnish–Turkish\n• die, in, und, Klimazone., Je, ob, auf, Südhalbkugel, vom, eli, on, vuodenaika, ja, on, vuodenajoista, koska, maapallo, on, silloin, kallistunut, aurinko, maan, pinnalle, kulmassa, muina, vuodenaikoina., Pohjoisella, pallonpuoliskolla, lasketaan, tavallisesti, ja, elokuu, eteläisellä, pallonpuoliskolla, joulu-, ja, helmikuu., en, sıcak, en, yazda, Dünya, depo, en, sıcak, yaklaşık, ay, sonra, ortaya, Sıcak, Haziran, Eylül, ise, Aralık, arasındadır.\n• Der, ist, wärmste, der, vier, Jahreszeiten, der, arktischen, nachdem, er, der, Nord-, oder, herrscht, spricht, Nord-, oder, Der, findet, mit, Südwinter, sta., suvi, lämpimin, niin, eä, säteilee, heinä-, Yaz, mevsimdir., Küre’de, Küre’de, 21, 22, arasında, Küre’de, 22, 21, Mart\n• gemäßigten, gerade, gleichzeitig, kuin, Kuzey, uzun, günler, gerçekleşir., eiği, için, günler, genellikle, iki, günler, Kuzey, ile, ile\n• Sommer, man, Südsommer., Nordsommer, dem, Kesä, kevään, syksyn, välissä., Kesä, jyrkemmässä, kesäkuukausiksi, kesä-., tammi-, Yarım, ısıyı, çıkar., Yarım, Güney, Yarım\nData: Latin script: English–French\n• both, “so”, in, English, although, their, is, is, the, opposite, of, “rough”, or, is, the, opposite, of, sweet, only, for, wines, (otherwise, is\n• mou, :, mou, but\n• doux,\n• Doux, (rugueux), Doux\n• while\n• “hard”., used).,\n• translate, as, meaning, very, different., ”coarse”, can, also, mean, almost,sucré,\nData: Latin script: English–Transliterated Greek\n• at, least, ways, as, to, is, has, philía, and, storgē., as, has, historically, difficult, to, which, generally, as\n113\n• e, language, distinguishes, different, the, Ancient, distinct, with, languages, it, been, separate, the, meanings, these, used, outside, their, respective, the, senses, in, these, used\n• Greek, how, word, Greek, agápe, éros, However, other, when, were, are\n• four, love, used., four, words, for, love:, of, words, of, contexts., Nonetheless, words, follows.\nData: Latin script: Italian–German\n• affrescò, privato, Studie, definire, periture,Stierverbands,Wissenscha,studierà, difesa, ovvero, Szenario, Naturwissenschalern\n• dell’aureola, da, del, di, der, zum, modo., dem, den, drohe., Come, vom\n• custodisce, quel, es, oder, per, le, idee, stessa, des, dass, delle, E, se, Ist, das, seit\n• più, Cenacolo, vinciano, rivoluzionaria, Giuda, condanna, con, peccato), cominciò, con, cancro, faceva, intuizioni, vita, va, Dabei, Ergebnis, in, i, riccioli, poi, più, bacini, in, Annunciazione, con, ali, la, cosa, barbaglio, anni, bei,\n• ne, struggente:, che, amore, e, non, viene, ma, consapevolezza, ad, che, ha, recente, Kaum, eine, Woche, vergeht, keine, neue, Umfrage, Warnung, ema, Fachkräemangel, Deutschland, Certo, ma, anche, consapevole, che, qualche, mehren, letzter, Zeit, Stimmen, Entwarnung, geben., kam, jüngst, eine, Deutsche, ”ein, allgemeiner, Fachkräemangel, eher, mehr”, anche, Baista, che, Leonardo, approfonditamente, a, Venezia, nelle, vada, alla, aento, alle, dell’angelo:, delicatezza, punte, che, non, che, volare?, Jahren, angemahnte, drohenden, Fachkräemangel, Ingenieuren, ein\n• Milano, l’esempio, psicologia, (il, subito, autodistruggersi, solo, lunghissimo, So, il, movimento, moto, sui, si, bellissima, occhio, all’insù, sono, sogno, lo, ossessionava, quello, und, also, Mythos?\n• un, ür, MINT-Berufen\n• cura, restauro, arginato., gibt., perché, caurata, sich, auch, zu, nicht, richiamano, acque, ricerche, chiave, anti-Turchi., nur\n114\nData: Mixed script: Greek–Russian\n• ελληνική, γλώσσα, είναι, μία, από, τις, ινδοευρωπαϊκές, γλώσσες., Αποτελεί, το, μοναδικό, μέλος, ενός, ανεξάρτητου, κλάδου, της, ινδοευρωπαϊκής, οικογένειας, γλωσσών., Ανήκει, επίσης, στον, βαλκανικό, γλωσσικό, δεσμό., Στην, ελληνική, γλώσσα, έχουμε, γραπτά, κείμενα, από, τον, 15ο, αιώνα, μέχρι, σήμερα.\n• На, греческом, на, всех, его, существования, была, создана, богатейшая, греческого, обязательным, всякого, образованного, большое, заимствований, а, в, греческом, новое, время, (наряду, новых, научных, терминов, называемая, международная, слова, в, основном, двумя, через\n• Η, π.Χ.,языке, этапах, литература., В, Римской, империи, знание, языка, считалось, для, человека., В, латинском, языке, присутствует, количество, греческих,—, значительное, количество, латинских, и, романских, слов., В, древнегреческий, язык, стал, с, латинским), источником, создания, и, технических, (так, лексика)., В, русский, язык, греческие, проникали, путями, —, международную, лексику, и, церковнославянский, язык.\nData: Mixed script: English–Greek\n• is, biblical, is, will, is, without, self-benefit)., is, feelings, feelings, it, be, feeling, being, high, is, by, his, is, by, will, mostly, sexual, ”intimate, well, refined, his, definition:, is, initially, felt, with, it, beauty, within, beauty, itself., use, ”without, helps, soul, beauty, spiritual, youthful, beauty, feel, suggesting, sensually, spiritual, finding, its, like, finding, all, seek\n• (ἀγάπη, (ἔρως\n• Agápe, ”love:, brotherly, love, love, of, God, for, of, for, in, known, ”love, 1, 13, throughout, New, brotherly, love, affection, good, love, love, given, or, not, person, continues, love, (even, in, for, one’s, for, spouse, refer, love, of, content, or, holding, one, in, unconditional, love, of, God, for, of, love, ”to, good, of, Éros, ”love, of, e, Modern, Greek, word, love.”, own, Although, eros, for, person, contemplation, becomes, of, person, or, even, becomes, of, not, of, of, love, of, word, mean, In, Symposium, work, on, subject, eros, knowledge, of, of, ”Form”, of, erotic, –, even, love, non-corporeal, of, is, Lovers, philosophers, through, of,\n• agápē), means, esp., charity;, the, man, and, man, God.”, Agape, used, the, passage, as, the, chapter,”, Corinthians, and, described, there, and, the, Testament, as, and, benevolence., Whether, the, returned, the, to, any, Agape, also, used, ancient, texts, to, denote, children, and, the, a, and, was, also, used, to, to, a, feast., It, can, also, described, as, the, regard., Agape, used, Christians, to, express,\n115\nthe, children., type, was, further, explained, omas, Aquinas, as, the, another.”, érōs), means, the, passion.”, ”erotas”, means, It, can, also, apply, to, dating, relationships, as, as, marriage., Plato, a, an, appreciation, the, that, appreciation, Plato, does, talk, physical, araction, as, a, necessary, part, hence, the, the, platonic, to, physical, araction.”, the, the, most, famous, ancient, the, Plato, has, Socrates, argue, that, the, recall, and, contributes, to, an, understanding, truth, the, ideal, that, leads, us, humans, to, desire, thus, that, that, based, aspires, to, the, plane, existence;, that, truth, just, any, truth, leads, to, transcendence., and, are, inspired, to, truth, the, means, eros.\nData: Mixed script: English–Spanish–Arabic • ،نود ،ةدحاو ،ةرظنب ،اهتلاسر ،لقنت ،نأ ،يغبني ،ةمالعلا ،نأف ،امومعو ،نيعم ،ءيش ،نع ،ربعي ،يذلا ،مسرلا ،ينعي ،زمرلا\n،مدختسا ،نم ،رثكأ ،نكلو ،تامالعلا ،اومدختسأ ،قيرغألاو ،نييرصملا ،ءامدق ،نأ ،فورعملا ،نم ،و ،تاملك ،ةيال ،ةجاحلا مه ،تامالعلا\n• ribbon, symbol, mourning., ribbon, mourning, El, un, y, un, en\n• black, is, a, of, remembrance, or, Wearing, or, displaying, a, black, has, been, used, for, remembrance, tragedies, or, as, a, political, statement., crespón, negro, o, lazo, negro, es, símbolo, utilizado, por, personas, estados, sociedades, organizaciones, representando, sentimiento, político-social, señal, de, duelo.\n• A, POW/MIA\nData: Mixed script: English–Chinese • e, Chinese, (simplified, traditional, Chinese:, invoked, motivational, speaking, because, the, composed, characters, that, represent, linguists, have, criticized, this, usage, because, the, component, (simplified, Chinese:, traditional, Chinese:, has, other, besides, Chinese, certain, some, be, based, the, Chinese, that, the, e, numbers, believed, have, because, their, similar, words, that, have, positive\n• (不利)\n• Western, can, and, Some, meanings, In, are, number, name, and, are, meanings, names, meanings.\n• 0, 6, 8, 9\n• ”crisis”, is, auspicious, inauspicious, sounds, sound\n• for, pinyin:, frequently, in, word, of, two, ”danger”, ”opportunity”., pronounced, tradition, by, or, on, word, to., to\n• 危机;,危機;, wēijī), jī,机;,機), (吉利)\n116\nData: Mixed script: Ukrainian–Russian\n• й, Русь, морей., Россией, Белоруссией, Польшей, Словакией, Венгрией, Румынией\n• але, 9—13, юге, Имеет, Молдавией.\n• існували, інших\n• Чёрного, Азовского, границу\n• культури\n• території, України, пунктом, української, и, сухопутную, и\n• Віддавна, на, держави, скіфів, сарматів, готів, народів, відправним, державності, На, водами\n• та, вважається, Київська, століття., омывается, с\nData: Pali: abbha\n• (nt.), nt., Sk., \\”dark, Idg., cp., Gr., Lat., Sk., water, Gr., water]., dark), at, SnA, S, at, It, Sn, (cp., SnA, Sn, S\n• &, A, A), ., J, 251)., 1, 1064;, 249, 250;, 12)., 64;, 348)., 382).\n• viz., 134., 101, 581, f., 289.\n• 53, 295, 273, 487, 3, 617, 317, 348, 239., 687\n• cloud\\”;, also, cloud, cloudy, <smallcaps> ii. </smallcaps>, =, list, is, <smallcaps> i. </smallcaps>, (°sama, <smallcaps> vi. </smallcaps>, (abbhaŋ, <smallcaps> iv. </smallcaps>, (nīl°, As, Dhs, DhsA, (used, (=, clouds, cloud, (also, as\n• m., adj.\n• abhra, (mahiyā, VvA, acchādesi);, Pv, PvA, \\”dull\\”;, valāhaka);, Vv, valāhaka– sikhara\n• <at>a)fro\\\\s</at>, froth, of, <superscript> 9 </superscript>, <superscript> s. </superscript>, <superscript> 1 </superscript>\n• later, scum, rain;, ambha, rain, a, Miln, (megho, Miln), nīla–megha, sense, expl, , Dh\n117\n• *m̊bhro, <at>o)/mbros</at>, ambu, mass, to, obscure, moon–, <b>abbhaŋ, mahikā </b>, <b>dhū-, marajo</b>, <b>Rāhu</b>, pabbata, rajo, <b>abbhā</b>, by, perhaps, <b>abbhāmaa</b>, <br, /><b>–kūṭa</b>, or, summit, storm–cloud, <b>–ghana</b>, <b>–paṭala</b>, mass, <b>–mua</b>, from, abbhāmua, <b> –saŋvilāpa</b>,\n• [Vedic, imber, Oir, (dense, Vin, in, things, that, sunshine, is, referred, mountain, like, thunder–cloud);, the, point, thick, free, thundering\nData: Pali: abhijjhitar\n• <smallcaps>i.</smallcaps>, v., l., <smallcaps>v.</smallcaps>\n• abhijjhita, abhijjhātar, °itar), °itar, °ātar).,\n• [n., ag., fr., med., M, 287, (T., =, A, 265\n• in, function], one, who, covets\nData: Pali: ajja\n• Ajja，&, Ajjā,（adv.）, base, a3）, diva），thus, Dh.326；, ajjā；, v., PvA.59）；, PvA.6，23；, phrase, ajjatagge, ajjato, agge（?）, ajja-tagge，see, agga3）,（adv.） , the, 32，23.,（Page\n• ‹-›, –kālaṁ\n• [Vedic, &, +, being,（see,（see,（read, as,（=, Mhvs,（=, +, Mhvs\n• of, of,“on,“food”）；\n• and, an, old, not\n• adya, adyā，a, dyā，a°, dyā, dyaus, day”], to-day，now, bahutaṁ, with, day, –divasa, day\n• demonstr., pron., Loc., this, Kern，Toev., s., Freq., or, from, this, onward，henceforth, this, morning, present\n• Sn.75，153，158，970，998；, J.I，279；, III，425, Pv.I，117, idāni, 15，64., in, Vin.I，18；, D.I，85；, DA.I，235., J.VI，180；, 10）\n118\nData: Pali: gūhanā • Pug．19．Cp．pari°．（Page\n• Gūhanā，（f.）, [abstr．fr．gūhati]=gūhanā\n• 253）,（q．v.）\nData: Pali: pacati • Vin．IV，264；, N．S．II，225，PvA．10，14．–, D．I，52\n• DA．I，159，where, 382）\n• in\n• at, &\n• cook，pέpwn, cook，boil，roast\n• Pacati，[Ved．pacati，Idg．*peqǔō，Av．pac-；, Obulg．peka, to, fry，roast， Lith，kepū, ripe], to, fig．torment, purgatory,（trs．and, pacitvā, aer, roasting, ppr．pacanto, tormenting，Gen．pacato,（+Caus．pācayato）, read, pacato, for, paccato，by, pare, pp．pakka, Caus．pacāpeti, pāceti, Pass．paccati, to, roasted, or, tormented\n• bake，Gr．pέssw, intrs．）：Niraye,（expld, daṇḍena, pīḷentassa）．–,（q．v．）． ‹-›,（q．v．）．–, be,（q．v．）．（Page"
    }, {
      "heading" : "Normalized data",
      "text" : "• pacati, peka, pέssw, pέpwn, pacitvā, ppr., pacanto, Gen., pacato, (+Caus., pācayato), pacato, paccato, pare, pīḷentassa)., pp., pakka, Caus., pacāpeti, Pass., paccati\n• *peqǔō, bake\n• pac-;, 264;, 52, &, 382)\n• 10,14.–, 159, –, <->, –\n• fry, Niraye, I, I, by\n• Av., Obulg., Gr., (trs., D., DA., (q.v.)., (q.v.)., (q.v.).\n• [Ved., to, roast, kepū, cook, ripe], to, cook, roast, torment, purgatory, and, aer, roasting, tormenting, (expld, at, where, read, for, daṇḍena, pāceti, to, be, roasted, or, tormented, (Page\n• Pacati, Idg., Lith, boil, Vin.IV, fig., in, intrs.):, in, N.S.II,225,PvA.\n119\nData: Twier 1 (Greek–English)\n• BUSINESS, EXCELLENCE.\n• Μόλις, ψήφισα, αυτή, τη, λύση, Internet, of, στο, διαγωνισμό\n• ings, IT\nData: Twier 2 (French–English)\n• Keynote, “e, collective, of, science-publish, or, perish;, it, all, that, counts?”\n• Demain, 18h, par\n• #dhiha6, David\n• @dhiparis, dynamics, is\nData: Twier 3 (French–English)\n• #FWWC2015\n• breuvages, go,\n• Food, Edmonton, to, for, the\n• in, waiting, #bilingualism\n• and, are, ready, just, fans\nData: Twier 4 (English–Polish)\n• comes, from, with, two, crates, of, strawberries, jackets, omg\n• my, dad, poland, and, adidas\n• back, żubrówka\nData: Twier 5 (Transliterated Amharic–English)\n• (coffee\n• bread). is, our\n• Buna, dabo, naw\n120"
    } ],
    "references" : [ {
      "title" : "Java-ML: A Machine Learning Library",
      "author" : [ "T. Abeel", "Y.V. de Peer", "Y. Saeys" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Abeel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Abeel et al\\.",
      "year" : 2009
    }, {
      "title" : "Interactive data mining with 3D-parallel-coordinate-trees",
      "author" : [ "E. Achtert", "H. Kriegel", "E. Schubert", "A. Zimek" ],
      "venue" : "In Proceedings of the ACM SIGMOD International Conference on Management of Data,",
      "citeRegEx" : "Achtert et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Achtert et al\\.",
      "year" : 2013
    }, {
      "title" : "An unsupervised system for identifying English inclusions in German text",
      "author" : [ "B. Alex" ],
      "venue" : "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL",
      "citeRegEx" : "Alex,? \\Q2005\\E",
      "shortCiteRegEx" : "Alex",
      "year" : 2005
    }, {
      "title" : "Integrating language knowledge resources to extend the English inclusion classifier to a new language",
      "author" : [ "B. Alex" ],
      "venue" : "In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC). European Language Resources Association",
      "citeRegEx" : "Alex,? \\Q2006\\E",
      "shortCiteRegEx" : "Alex",
      "year" : 2006
    }, {
      "title" : "Automatic detection of English inclusions in mixed-lingual data with an application to parsing",
      "author" : [ "B. Alex" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Alex,? \\Q2007\\E",
      "shortCiteRegEx" : "Alex",
      "year" : 2007
    }, {
      "title" : "Using Foreign Inclusion Detection to Improve Parsing Performance",
      "author" : [ "B. Alex", "A. Dubey", "F. Keller" ],
      "venue" : "In EMNLP-CoNLL,",
      "citeRegEx" : "Alex et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Alex et al\\.",
      "year" : 2007
    }, {
      "title" : "Zum Erkennen von Anglizismen im Deutschen: der Vergleich von einer automatisierten mit einer manuellen Erhebung",
      "author" : [ "B. Alex", "A. Onysko" ],
      "venue" : "Strategien der Integration und Isolation nicht-nativer Einheiten und Strukturen,",
      "citeRegEx" : "Alex and Onysko,? \\Q2010\\E",
      "shortCiteRegEx" : "Alex and Onysko",
      "year" : 2010
    }, {
      "title" : "On prediction using variable order Markov models",
      "author" : [ "R. Begleiter", "R. El-Yaniv", "G. Yona" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Begleiter et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Begleiter et al\\.",
      "year" : 2004
    }, {
      "title" : "Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of the first workshop on graph based methods for natural language processing, pages 73–80",
      "author" : [ "C. Biemann" ],
      "venue" : null,
      "citeRegEx" : "Biemann,? \\Q2006\\E",
      "shortCiteRegEx" : "Biemann",
      "year" : 2006
    }, {
      "title" : "e TIGER treebank",
      "author" : [ "S. Brants", "S. Dipper", "S. Hansen", "W. Lezius", "G. Smith" ],
      "venue" : "In Proceedings of the workshop on treebanks and linguistic theories,",
      "citeRegEx" : "Brants et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brants et al\\.",
      "year" : 2002
    }, {
      "title" : "Algebraic complexity theory, volume",
      "author" : [ "P. Bürgisser", "M. Clausen", "M.A. Shokrollahi" ],
      "venue" : null,
      "citeRegEx" : "Bürgisser et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Bürgisser et al\\.",
      "year" : 1997
    }, {
      "title" : "Improving language models by clustering training sentences",
      "author" : [ "D. Carter" ],
      "venue" : "In Proceedings of the fourth conference on Applied natural language processing,",
      "citeRegEx" : "Carter,? \\Q1994\\E",
      "shortCiteRegEx" : "Carter",
      "year" : 1994
    }, {
      "title" : "N-gram-based text categorization",
      "author" : [ "W.B. Cavnar", "J.M. Trenkle" ],
      "venue" : "In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,",
      "citeRegEx" : "Cavnar and Trenkle,? \\Q1994\\E",
      "shortCiteRegEx" : "Cavnar and Trenkle",
      "year" : 1994
    }, {
      "title" : "An empirical study of smoothing techniques for language modeling",
      "author" : [ "S.F. Chen", "J. Goodman" ],
      "venue" : "In Proceedings of the 34th annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Chen and Goodman,? \\Q1996\\E",
      "shortCiteRegEx" : "Chen and Goodman",
      "year" : 1996
    }, {
      "title" : "Clustering Methods for Improving Language Models",
      "author" : [ "E. Dreyfuss", "I. Goodfellow", "P. Baumstarck" ],
      "venue" : null,
      "citeRegEx" : "Dreyfuss et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dreyfuss et al\\.",
      "year" : 2007
    }, {
      "title" : "How many clusters are best?-an experiment",
      "author" : [ "R.C. Dubes" ],
      "venue" : "Paern Recognition,",
      "citeRegEx" : "Dubes,? \\Q1987\\E",
      "shortCiteRegEx" : "Dubes",
      "year" : 1987
    }, {
      "title" : "Statistical Identification of Language",
      "author" : [ "T. Dunning" ],
      "venue" : "Computing Research Laboratory,",
      "citeRegEx" : "Dunning,? \\Q1994\\E",
      "shortCiteRegEx" : "Dunning",
      "year" : 1994
    }, {
      "title" : "Good-turing smoothing without tears",
      "author" : [ "W. Gale", "G. Sampson" ],
      "venue" : "Journal of antitative Linguistics,",
      "citeRegEx" : "Gale and Sampson,? \\Q1995\\E",
      "shortCiteRegEx" : "Gale and Sampson",
      "year" : 1995
    }, {
      "title" : "e use of clustering techniques for language modeling–application to Asian languages",
      "author" : [ "J. Gao", "J. Goodman", "J Miao" ],
      "venue" : "International Journal of Computational Linguistics and Chinese Language Processing,",
      "citeRegEx" : "Gao et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2001
    }, {
      "title" : "What every computer scientist should know about floating-point arithmetic",
      "author" : [ "D. Goldberg" ],
      "venue" : "ACM Computing Surveys (CSUR),",
      "citeRegEx" : "Goldberg,? \\Q1991\\E",
      "shortCiteRegEx" : "Goldberg",
      "year" : 1991
    }, {
      "title" : "Language model size reduction by pruning and clustering",
      "author" : [ "J. Goodman", "J. Gao" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Goodman and Gao,? \\Q2000\\E",
      "shortCiteRegEx" : "Goodman and Gao",
      "year" : 2000
    }, {
      "title" : "A bit of progress in language modeling",
      "author" : [ "J.T. Goodman" ],
      "venue" : "Computer Speech and Language,",
      "citeRegEx" : "Goodman,? \\Q2001\\E",
      "shortCiteRegEx" : "Goodman",
      "year" : 2001
    }, {
      "title" : "Comparing two language identification schemes",
      "author" : [ "G. Grefenstee" ],
      "venue" : "In Proceedings of the 3rd International conference on Statistical Analysis of Textual Data. JADT",
      "citeRegEx" : "Grefenstee,? \\Q1995\\E",
      "shortCiteRegEx" : "Grefenstee",
      "year" : 1995
    }, {
      "title" : "e minimum description length principle",
      "author" : [ "P.D. Grünwald" ],
      "venue" : null,
      "citeRegEx" : "Grünwald,? \\Q2007\\E",
      "shortCiteRegEx" : "Grünwald",
      "year" : 2007
    }, {
      "title" : "A closer look at skip-gram modelling",
      "author" : [ "D. Guthrie", "B. Allison", "W. Liu", "L. Guthrie", "Y. Wilks" ],
      "venue" : "In Proceedings of the 5th international Conference on Language Resources and Evaluation",
      "citeRegEx" : "Guthrie et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Guthrie et al\\.",
      "year" : 2006
    }, {
      "title" : "e WEKA Data Mining Soware: An Update",
      "author" : [ "M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Wien" ],
      "venue" : "SIGKDD Explorations,",
      "citeRegEx" : "Hall et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2009
    }, {
      "title" : "Data clustering: a review",
      "author" : [ "A.K. Jain", "M.N. Murty", "P.J. Flynn" ],
      "venue" : "ACM computing surveys (CSUR),",
      "citeRegEx" : "Jain et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 1999
    }, {
      "title" : "Language Identification in Code-Switching Scenario",
      "author" : [ "N. Jain", "R.A. Bhat" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,",
      "citeRegEx" : "Jain and Bhat,? \\Q2014\\E",
      "shortCiteRegEx" : "Jain and Bhat",
      "year" : 2014
    }, {
      "title" : "Speech and language processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition",
      "author" : [ "D. Jurafsky", "J.H. Martin" ],
      "venue" : "Pearson Education India,",
      "citeRegEx" : "Jurafsky and Martin,? \\Q2000\\E",
      "shortCiteRegEx" : "Jurafsky and Martin",
      "year" : 2000
    }, {
      "title" : "Estimation of probabilities from sparse data for the language model component of a speech recognizer",
      "author" : [ "S. Katz" ],
      "venue" : "Acoustics, Speech and Signal Processing, IEEE Transactions",
      "citeRegEx" : "Katz,? \\Q1987\\E",
      "shortCiteRegEx" : "Katz",
      "year" : 1987
    }, {
      "title" : "Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods",
      "author" : [ "B. King", "S.P. Abney" ],
      "venue" : "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies,",
      "citeRegEx" : "King and Abney,? \\Q2013\\E",
      "shortCiteRegEx" : "King and Abney",
      "year" : 2013
    }, {
      "title" : "Language clustering with word co-occurrence networks based on parallel texts",
      "author" : [ "H. Liu", "J. Cong" ],
      "venue" : "Chinese Science Bulletin,",
      "citeRegEx" : "Liu and Cong,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu and Cong",
      "year" : 2013
    }, {
      "title" : "Mel frequency cepstral coefficients for music modeling",
      "author" : [ "B Logan" ],
      "venue" : "In Proceedings of the 1st International Symposium onMusic Information Retrieval (ISMIR)",
      "citeRegEx" : "Logan,? \\Q2000\\E",
      "shortCiteRegEx" : "Logan",
      "year" : 2000
    }, {
      "title" : "Automatic detection and language identification of multilingual documents. Transactions of the Association for Computational Linguistics, 2:27–40",
      "author" : [ "M. Lui", "J.H. Lau", "T. Baldwin" ],
      "venue" : null,
      "citeRegEx" : "Lui et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lui et al\\.",
      "year" : 2014
    }, {
      "title" : "Introduction to information retrieval, volume 1",
      "author" : [ "C.D. Manning", "P. Raghavan", "H. Schütze" ],
      "venue" : null,
      "citeRegEx" : "Manning et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2008
    }, {
      "title" : "Foundations of statistical natural language processing",
      "author" : [ "C.D. Manning", "H. Schütze" ],
      "venue" : null,
      "citeRegEx" : "Manning and Schütze,? \\Q1999\\E",
      "shortCiteRegEx" : "Manning and Schütze",
      "year" : 1999
    }, {
      "title" : "Novelty detection in learning systems",
      "author" : [ "S. Marsland" ],
      "venue" : "Neural computing surveys,",
      "citeRegEx" : "Marsland,? \\Q2003\\E",
      "shortCiteRegEx" : "Marsland",
      "year" : 2003
    }, {
      "title" : "TweetSafa: Tweet language identification",
      "author" : [ "I. Mendizabal", "J. Carandell", "D. Horowitz" ],
      "venue" : "TweetLID @ SEPLN",
      "citeRegEx" : "Mendizabal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mendizabal et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "On structuring probabilistic dependences in stochastic language modelling",
      "author" : [ "H. Ney", "U. Essen", "R. Kneser" ],
      "venue" : "Computer Speech & Language,",
      "citeRegEx" : "Ney et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Ney et al\\.",
      "year" : 1994
    }, {
      "title" : "X-means: Extending K-means with Efficient Estimation of the Number of Clusters",
      "author" : [ "D. Pelleg", "A.W. Moore" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML",
      "citeRegEx" : "Pelleg and Moore,? \\Q2000\\E",
      "shortCiteRegEx" : "Pelleg and Moore",
      "year" : 2000
    }, {
      "title" : "Distributional clustering of english words",
      "author" : [ "F. Pereira", "N. Tishby", "L. Lee" ],
      "venue" : "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Pereira et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 1993
    }, {
      "title" : "Twier Language Identification using Rational Kernels and its potential application to Sociolinguistics",
      "author" : [ "J. Porta" ],
      "venue" : "TweetLID @ SEPLN",
      "citeRegEx" : "Porta,? \\Q2014\\E",
      "shortCiteRegEx" : "Porta",
      "year" : 2014
    }, {
      "title" : "Parallel Algorithms for Unsupervised Tagging",
      "author" : [ "S. Ravi", "S. Vassilivitskii", "V. Rastogi" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Ravi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ravi et al\\.",
      "year" : 2014
    }, {
      "title" : "e power of amnesia: Learning probabilistic automata with variable memory length",
      "author" : [ "D. Ron", "Y. Singer", "N. Tishby" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Ron et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Ron et al\\.",
      "year" : 1996
    }, {
      "title" : "Support vector method for novelty detection",
      "author" : [ "B. Schölkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor", "J.C. Pla" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1999
    }, {
      "title" : "Unsupervised sequence segmentation by a mixture of switching variable memory Markov sources",
      "author" : [ "Y. Seldin", "G. Bejerano", "N. Tishby" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Seldin et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Seldin et al\\.",
      "year" : 2001
    }, {
      "title" : "Overview for the First Shared Task on Language Identification in Code-Switched Data",
      "author" : [ "T. Solorio", "E. Blair", "S. Maharjan", "S. Bethard", "M. Diab", "M. Gohneim", "A. Hawwari", "F. AlGhamdi", "J. Hirschberg", "A Chang" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,",
      "citeRegEx" : "Solorio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Solorio et al\\.",
      "year" : 2014
    }, {
      "title" : "Graphing the distribution of English leers towards the beginning, middle or end of words. http://www.prooffreader.com/2014/05/ graphing-distribution-of-english.html",
      "author" : [ "D. Taylor" ],
      "venue" : null,
      "citeRegEx" : "Taylor,? \\Q2015\\E",
      "shortCiteRegEx" : "Taylor",
      "year" : 2015
    }, {
      "title" : "Distributed word clustering for large scale classbased language modeling in machine translation",
      "author" : [ "J. Uszkoreit", "T. Brants" ],
      "venue" : "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Uszkoreit and Brants,? \\Q2008\\E",
      "shortCiteRegEx" : "Uszkoreit and Brants",
      "year" : 2008
    }, {
      "title" : "Comparing clusterings: an overview. Universität Karlsruhe, Fakultät für Informatik Karlsruhe",
      "author" : [ "S. Wagner", "D. Wagner" ],
      "venue" : null,
      "citeRegEx" : "Wagner and Wagner,? \\Q2007\\E",
      "shortCiteRegEx" : "Wagner and Wagner",
      "year" : 2007
    }, {
      "title" : "Text segmentation by language using minimum description length. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969–978",
      "author" : [ "H. Yamaguchi", "K. Tanaka-Ishii" ],
      "venue" : null,
      "citeRegEx" : "Yamaguchi and Tanaka.Ishii,? \\Q2012\\E",
      "shortCiteRegEx" : "Yamaguchi and Tanaka.Ishii",
      "year" : 2012
    }, {
      "title" : "Hierarchical language identification based on automatic language clustering",
      "author" : [ "B. Yin", "E. Ambikairajah", "F. Chen" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Yin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2007
    }, {
      "title" : "Language model based on word clustering",
      "author" : [ "L. Yuan" ],
      "venue" : "In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation,",
      "citeRegEx" : "Yuan,? \\Q2006\\E",
      "shortCiteRegEx" : "Yuan",
      "year" : 2006
    }, {
      "title" : "Overview of TweetLID: Tweet language identification",
      "author" : [ "A. Zubiaga", "I. San Vicente", "P. Gamallo", "J.R. Pichel", "I. Alegria", "N. Aranberri", "A. Ezeiza", "V. Fresno" ],
      "venue" : "SEPLN",
      "citeRegEx" : "Zubiaga et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zubiaga et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Language segmentation and identification are important for all natural language processing operations that are language-specific, such as taggers, parsers or machine translation (Jain and Bhat, 2014; Zubiaga et al., 2014).",
      "startOffset" : 178,
      "endOffset" : 221
    }, {
      "referenceID" : 54,
      "context" : "Language segmentation and identification are important for all natural language processing operations that are language-specific, such as taggers, parsers or machine translation (Jain and Bhat, 2014; Zubiaga et al., 2014).",
      "startOffset" : 178,
      "endOffset" : 221
    }, {
      "referenceID" : 27,
      "context" : "Indeed, using “traditional”monolingual natural language processing components on mixed language data leads to miserable results (Jain and Bhat, 2014).",
      "startOffset" : 128,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "For example, by identifying foreign language inclusions in an otherwise monolingual text, parser accuracy can be increased (Alex et al., 2007).",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "1 N-Grams and rank order statistics Cavnar and Trenkle (1994) use an n-gram language model for language identification purposes.",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "Cavnar and Trenkle (1994) collected 3713 Usenet texts with a cultural theme in different languages.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : "2 N-Grams and maximum likelihood estimator Dunning (1994) also uses an n-gram language model for language identification purposes.",
      "startOffset" : 43,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "In order to test the system, Dunning (1994) uses a specially constructed test corpus from a bilingual parallel translated English-Spanish corpus containing English and Spanish texts with 10 texts varying from 1000 to 50000 bytes for the training set and 100 texts varying from 10 to 500 bytes for the test set.",
      "startOffset" : 29,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words.",
      "startOffset" : 26,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words. e system implemented by Dunning (1994) can classify strings of 10 characters in length “moderately well”, while strings of 50 characters or more are classified “very well”.",
      "startOffset" : 26,
      "endOffset" : 245
    }, {
      "referenceID" : 22,
      "context" : "3 Trigrams and short words Grefenstee (1995) compares trigrams versus short words for language identification.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "4 N-Grams and clustering Gao et al. (2001) present a system that augments n-gram language models with clustering techniques.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "P (wi|wi−2wi−1) = P (ci|ci−2ci−1)× P (wi|ci−2ci−1ci) (9) Similarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in order to improve trigram language models.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "P (wi|wi−2wi−1) = P (ci|ci−2ci−1)× P (wi|ci−2ci−1ci) (9) Similarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in order to improve trigram language models. In addition to Gao et al. (2001), they also use information about the subject-verb and verb-object relations of the sentence.",
      "startOffset" : 68,
      "endOffset" : 218
    }, {
      "referenceID" : 11,
      "context" : "Carter (1994) clusters training sentences (i.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al.",
      "startOffset" : 0,
      "endOffset" : 252
    }, {
      "referenceID" : 11,
      "context" : "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al.",
      "startOffset" : 0,
      "endOffset" : 326
    }, {
      "referenceID" : 11,
      "context" : "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al. (1994)).",
      "startOffset" : 0,
      "endOffset" : 348
    }, {
      "referenceID" : 11,
      "context" : "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al. (1994)). Carter (1994) shows that the subdivision into smaller clusters increases the accuracy of bigram language models, but not trigram models.",
      "startOffset" : 0,
      "endOffset" : 364
    }, {
      "referenceID" : 5,
      "context" : "For example in (Alex et al., 2007) they report an increase in F-Score of 4.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "5 Inclusion detection Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010)) addresses the problem of English inclusions in mainly non-English texts.",
      "startOffset" : 31,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "5 Inclusion detection Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010)) addresses the problem of English inclusions in mainly non-English texts.",
      "startOffset" : 31,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "by using inclusion detection when parsing a German text with a parser trained on the TIGER corpus (Brants et al., 2002).",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 51,
      "context" : "6 Clustering and spee In the area of clustering and spoken language identification, Yin et al. (2007) present a hierarchical clusterer for spoken language.",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 32,
      "context" : "MFCC vectors are a way of representing acoustic signals (Logan et al., 2000). e signal is first divided into smaller ‘frames’, each frame is passed through the discrete Fourier transform and only the logarithm of the amplitude spectrum is retained (Logan et al., 2000). e spectrum is then projected onto the ‘Mel frequency scale’, a scale that maps actual pitch to perceived pitch, “as apparently the human auditory system does not perceive pitch in a linear manner” (Logan et al., 2000). Finally, a discrete cosine transform is applied to the spectrum to get the MFCC representations of the original signal (Logan et al., 2000). Yin et al. (2007) show that their hierarchical clusterer outperforms traditionalAcoustic Gaussian Mixture Model systems.",
      "startOffset" : 57,
      "endOffset" : 650
    }, {
      "referenceID" : 49,
      "context" : "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al.",
      "startOffset" : 28,
      "endOffset" : 62
    }, {
      "referenceID" : 30,
      "context" : "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 30,
      "context" : "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text.",
      "startOffset" : 63,
      "endOffset" : 107
    }, {
      "referenceID" : 30,
      "context" : "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language.",
      "startOffset" : 63,
      "endOffset" : 259
    }, {
      "referenceID" : 30,
      "context" : "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively ‘closed’ data set of the Universal Declaration of Human Rights2 and 0.84 on the more ‘open’ Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al.",
      "startOffset" : 63,
      "endOffset" : 659
    }, {
      "referenceID" : 30,
      "context" : "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively ‘closed’ data set of the Universal Declaration of Human Rights2 and 0.84 on the more ‘open’ Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation.",
      "startOffset" : 63,
      "endOffset" : 752
    }, {
      "referenceID" : 30,
      "context" : "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively ‘closed’ data set of the Universal Declaration of Human Rights2 and 0.84 on the more ‘open’ Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation.",
      "startOffset" : 63,
      "endOffset" : 832
    }, {
      "referenceID" : 30,
      "context" : "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively ‘closed’ data set of the Universal Declaration of Human Rights2 and 0.84 on the more ‘open’ Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation. King and Abney (2013) use weakly supervised methods to label the languages of words.",
      "startOffset" : 63,
      "endOffset" : 930
    }, {
      "referenceID" : 30,
      "context" : "One problem that these approaches all have is that they need to know the languages that will occur in the test data (King and Abney, 2013; Lui et al., 2014).",
      "startOffset" : 116,
      "endOffset" : 156
    }, {
      "referenceID" : 33,
      "context" : "One problem that these approaches all have is that they need to know the languages that will occur in the test data (King and Abney, 2013; Lui et al., 2014).",
      "startOffset" : 116,
      "endOffset" : 156
    }, {
      "referenceID" : 32,
      "context" : "Lui et al. (2014) consider the task as multi-label classification task.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "Whereas HMMs require substantial amounts of training data and a deep understanding of the problem in order to restrict the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to “solve many applications with notable success” (Begleiter et al., 2004).",
      "startOffset" : 261,
      "endOffset" : 285
    }, {
      "referenceID" : 7,
      "context" : "In contrast to n-gram models that estimate the probability of w as P (w|N) with N the context (typically the n previous words), VMMs can vary N in function of the available context (Begleiter et al., 2004).",
      "startOffset" : 181,
      "endOffset" : 205
    }, {
      "referenceID" : 7,
      "context" : "us, they can capture both small and large order dependencies, depending on the training data (Begleiter et al., 2004).",
      "startOffset" : 94,
      "endOffset" : 118
    }, {
      "referenceID" : 44,
      "context" : "One of these algorithms is called Predictive Suffix Tree (PST) (Ron et al., 1996).",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 44,
      "context" : "A PST is a tree over an alphabet Σ, with each node either having 0 (leaf nodes) or |Σ| children (non-terminal nodes) (Ron et al., 1996).",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 44,
      "context" : "Each node is labeled with the result of the walk from that node up to the root (Ron et al., 1996).",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 44,
      "context" : "Each edge is labeled by a symbol s ∈ Σ and the probability for the next symbol being s (Ron et al., 1996).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 23,
      "context" : "e MDL principle avoids overfiing of the model by favoring low complexity over goodness-of-fit (Grünwald, 2007).",
      "startOffset" : 96,
      "endOffset" : 112
    }, {
      "referenceID" : 43,
      "context" : "8 Predictive suffix trees Seldin et al. (2001) propose a system for automatic unsupervised language segmentation and protein sequence segmentation.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "Whereas HMMs require substantial amounts of training data and a deep understanding of the problem in order to restrict the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to “solve many applications with notable success” (Begleiter et al., 2004). In contrast to n-gram models that estimate the probability of w as P (w|N) with N the context (typically the n previous words), VMMs can vary N in function of the available context (Begleiter et al., 2004). us, they can capture both small and large order dependencies, depending on the training data (Begleiter et al., 2004). ere is no single VMM algorithm, but rather a family of related algorithms. One of these algorithms is called Predictive Suffix Tree (PST) (Ron et al., 1996). A PST is a tree over an alphabet Σ, with each node either having 0 (leaf nodes) or |Σ| children (non-terminal nodes) (Ron et al., 1996). Each node is labeled with the result of the walk from that node up to the root (Ron et al., 1996). Each edge is labeled by a symbol s ∈ Σ and the probability for the next symbol being s (Ron et al., 1996). By modifying the Predictive Suffix Tree (PST) algorithm using the Minimum Description Length (MDL) principle, Seldin et al. (2001) end up with a non-parametric self-regulating algorithm.",
      "startOffset" : 262,
      "endOffset" : 1247
    }, {
      "referenceID" : 18,
      "context" : "1 N-Gram models Among supervised languagemodels, n-grammodels are very popular (Gao et al., 2001).",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "An n-gram is a slice from the original string (Cavnar and Trenkle, 1994).",
      "startOffset" : 46,
      "endOffset" : 72
    }, {
      "referenceID" : 24,
      "context" : "Non-contiguous n-grams are also called skip-grams (Guthrie et al., 2006).",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 24,
      "context" : "In this parlance, contiguous n-grams can be regarded as 0-skip-n-grams (Guthrie et al., 2006).",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : "As can be seen from this example, the number of skip-grams ismore than two times higher than the number of contiguous n-grams, and this trend continues the more skips are allowed (Guthrie et al., 2006).",
      "startOffset" : 179,
      "endOffset" : 201
    }, {
      "referenceID" : 12,
      "context" : "Oen, the word to decompose is padded with start and end tags in order to improve the model (Cavnar and Trenkle, 1994).",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "e use of paddings allows themodel to capture details about character distribution with regard to the start and end of words (Cavnar and Trenkle, 1994).",
      "startOffset" : 125,
      "endOffset" : 151
    }, {
      "referenceID" : 48,
      "context" : "at the beginning of words, while the leer ‘w’ occurs mainly at the beginning of words (Taylor, 2015).",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "One advantage of n-gram models is that the decomposition of a string into smaller units reduces the impact of typing errors (Cavnar and Trenkle, 1994).",
      "startOffset" : 124,
      "endOffset" : 150
    }, {
      "referenceID" : 12,
      "context" : "Indeed, a typing error only affects a limited number of units (Cavnar and Trenkle, 1994).",
      "startOffset" : 62,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "Due to this property, n-gram models have been shown to be able to deal well with noisy text (Cavnar and Trenkle, 1994).",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "In order to avoid this problem, different smoothing techniques can be used (Chen and Goodman, 1996).",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "e simplest smoothing technique is additive (Laplace) smoothing (Chen and Goodman, 1996).",
      "startOffset" : 64,
      "endOffset" : 88
    }, {
      "referenceID" : 28,
      "context" : "If we choose λ = 1, we speak of “add one” smoothing (Jurafsky and Martin, 2000).",
      "startOffset" : 52,
      "endOffset" : 79
    }, {
      "referenceID" : 35,
      "context" : "In practice, λ < 1 is oen chosen (Manning and Schütze, 1999).",
      "startOffset" : 34,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "us, instead of using the actual count c, the count is taken to be c∗ (Chen and Goodman, 1996).",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 29,
      "context" : "Katz’s back-off model (Katz, 1987) for instance calculates probability Pbo using the formula:",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "2 Unsupervised clustering Clustering consists in the grouping of objects based on their mutual similarity (Biemann, 2006).",
      "startOffset" : 106,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "Objects to be clustered are typically represented as feature vectors (Biemann, 2006); from the original objects, a feature representation is calculated and used for further processing.",
      "startOffset" : 69,
      "endOffset" : 84
    }, {
      "referenceID" : 52,
      "context" : "Clustering can be partitional or hierarchical (Yin et al., 2007).",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 52,
      "context" : "Partitional clustering divides the initial objects into separate groups in one step, whereas hierarchical clustering builds a hierarchy of objects by first grouping the most similar objects together and then clustering the next level hierarchy with regard to the existing clusters (Yin et al., 2007).",
      "startOffset" : 281,
      "endOffset" : 299
    }, {
      "referenceID" : 8,
      "context" : "e clustering algorithm uses a distancemetric tomeasure the distance between the feature vectors of objects (Biemann, 2006).",
      "startOffset" : 108,
      "endOffset" : 123
    }, {
      "referenceID" : 26,
      "context" : "e distance metric defines the similarity of objects based on the feature space in which the objects are represented (Jain et al., 1999).",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "the angle between them (Biemann, 2006).",
      "startOffset" : 23,
      "endOffset" : 38
    }, {
      "referenceID" : 26,
      "context" : "In order for a clustering algorithm to work, features that represent the object to be clustered have to be defined (Jain et al., 1999).",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 26,
      "context" : "word starts with a capital leer) (Jain et al., 1999).",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 26,
      "context" : "k-means, need the number of clusters to generate (Jain et al., 1999).",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : "When hard-clustering, an object can belong to one class only, while in so-clustering, an object can belong to one or more classes, sometimes with different probabilities (Jain et al., 1999).",
      "startOffset" : 171,
      "endOffset" : 190
    }, {
      "referenceID" : 8,
      "context" : "e clustering algorithm uses a distancemetric tomeasure the distance between the feature vectors of objects (Biemann, 2006). e distance metric defines the similarity of objects based on the feature space in which the objects are represented (Jain et al., 1999). ere are different metrics available. A frequently chosen metric is the cosine similarity that calculates the distance between two vectors, i.e. the angle between them (Biemann, 2006). In order for a clustering algorithm to work, features that represent the object to be clustered have to be defined (Jain et al., 1999). Features can be quantitative (e.g. word length) or qualitative (e.g. word starts with a capital leer) (Jain et al., 1999). Most clustering algorithms, e.g. k-means, need the number of clusters to generate (Jain et al., 1999). e question how to best choose this key number has been addressed in-depth by Dubes (1987). Clustering can be so or hard.",
      "startOffset" : 109,
      "endOffset" : 902
    }, {
      "referenceID" : 46,
      "context" : "is approach is similar in character to the work by Seldin et al. (2001) in that the text itself is used as data set.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 46,
      "context" : "is approach is similar in character to the work by Seldin et al. (2001) in that the text itself is used as data set. However, the realization differs greatly. Whereas Seldin et al. (2001) use predictive suffix trees, I use n-gram language models.",
      "startOffset" : 52,
      "endOffset" : 189
    }, {
      "referenceID" : 16,
      "context" : "1 Implementation For the supervised language segmentation method, I implemented an n-gram language model as described by Dunning (1994). e n-gram language model is implemented as a character trigram model with non-linear back-off to bigram and unigram models.",
      "startOffset" : 121,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "Indeed, multiplying very small numbers can lead to the result being approximated as zero by the computer when the numbers become too small to be represented as normalized number (Goldberg, 1991).",
      "startOffset" : 178,
      "endOffset" : 194
    }, {
      "referenceID" : 10,
      "context" : "Using the sum of logarithms avoids this problem and is less computationally expensive (Bürgisser et al., 1997).",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 25,
      "context" : "3 Unsupervised clustering In order to test the efficiency of clustering algorithms on the task of language segmentation, I looked at various algorithms readily available throughWEKA, “a collection of machine learning algorithms for data mining tasks” by the University of Waikato in New Zealand (Hall et al., 2009) and the Environment for Developing KDD-Applications Supported by Index-Structures (ELKI), “an open source data mining soware [.",
      "startOffset" : 295,
      "endOffset" : 314
    }, {
      "referenceID" : 1,
      "context" : "] with an emphasis on unsupervised methods in cluster analysis and outlier detection” by the Ludwig-Maximilians-Universität München (Achtert et al., 2013).",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "I also looked at JavaML, “a collection of machine learning and data mining algorithms” (Abeel et al., 2009), in order to integrate clusterers into my own code framework.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 40,
      "context" : "In contrast, the x-means algorithm (Pelleg and Moore, 2000) estimates the number of clusters to generate itself.",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 50,
      "context" : "pairs (Wagner and Wagner, 2007).",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 50,
      "context" : "e Rand Index measures the accuracy of the clustering given a reference partition (Wagner and Wagner, 2007).",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 50,
      "context" : "However, it is criticized for being highly dependent on the number of clusters (Wagner and Wagner, 2007).",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 50,
      "context" : "It is similar to the Rand Index, but it disregards S00, the set of pairs that are clustered into different clusters in C and C ′ (Wagner and Wagner, 2007).",
      "startOffset" : 129,
      "endOffset" : 154
    }, {
      "referenceID" : 50,
      "context" : "e Fowlkes-Mallows Index has the undesired property of yielding high values when the number of clusters is small (Wagner and Wagner, 2007).",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "According toManning et al. (2008), in the context of clustering evaluation the F(β) score is defined as",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 34,
      "context" : "By varying β, it is possible to give more weight to either precision (β < 0) or recall (β > 1) (Manning et al., 2008).",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 50,
      "context" : "As there is no ultimate measure and all measures of similarity have their drawbacks (Wagner and Wagner, 2007), all measures will be indicated in the results section.",
      "startOffset" : 84,
      "endOffset" : 109
    }, {
      "referenceID" : 46,
      "context" : "e work by Seldin et al. (2001) is similar to the work presented here.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 51,
      "context" : "e work by Yin et al. (2007) and the work by Seldin et al.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 46,
      "context" : "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 46,
      "context" : "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis. However, Yin et al. (2007) concern themselves with spoken language, with requires a different approach than dealing with wrien language.",
      "startOffset" : 23,
      "endOffset" : 108
    }, {
      "referenceID" : 46,
      "context" : "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis. However, Yin et al. (2007) concern themselves with spoken language, with requires a different approach than dealing with wrien language. As I concentrated on wrien language, their work was not conducive to this thesis. In contrast, Seldin et al. (2001) present a work that looks promising.",
      "startOffset" : 23,
      "endOffset" : 336
    } ],
    "year" : 2015,
    "abstractText" : "Language segmentation consists in finding the boundaries where one language ends and another language begins in a text wrien in more than one language. is is important for all natural language processing tasks. e problem can be solved by training language models on language data. However, in the case of lowor no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform beer than supervised methods when it is difficult or impossible to train supervised approaches. A special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language. I compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words. e weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. e results look promising, but there is room for improvement and a more thorough investigation should be undertaken.",
    "creator" : " XeTeX output 2015.08.28:1436"
  }
}