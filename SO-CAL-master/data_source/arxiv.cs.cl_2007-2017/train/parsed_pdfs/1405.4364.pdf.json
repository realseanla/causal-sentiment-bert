{
  "name" : "1405.4364.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Thematically Reinforced Explicit Semantic Analysis",
    "authors" : [ "Yannis Haralambous" ],
    "emails" : [ "yannis.haralambous@telecom-bretagne.eu", "vkluev@u-aizu.ac.jp" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 Explicit Semantic Analysis",
      "text" : "Unlike semantic similarity measures, which are limited to ontological relations such as synonymy, hyponymy, meronymy, etc., semantic relatedness measures detect and quantify semantic relations of a more general kind. The typical example is the one involving the concepts car, vehicle and gasoline. A car is a special kind of vehicle, so we have an hyperonym relation between the concepts, which can easily be quantified by a semantic similarity measure (for example, by taking the inverse of the length of the shortest path between the corresponding synsets in WordNet). But between car and gasoline, there is no semantic similarity, since a car is a solid object and fuel is a liquid. Nevertheless, there is an obvious semantic relation between them since most cars use gasoline as their energy source, and such a relation can be quantified by a semantic relatedness measure.\nGabrilovich & Markovitch [1] introduce the semantic relatedness measure ESA (= Explicit Semantic Analysis, as opposed to the classical method of Latent Semantic Analysis [2]). ESA is based on the Wikipedia corpus. Here is the\nar X\niv :1\n40 5.\n43 64\nv1 [\ncs .C\nL ]\nmethod: after cleaning and filtering Wikipedia pages (keeping only those with a sufficient amount of text and a given minimal number of incoming and outgoing links), they remove stop words, stem all words and calculate their tfidfs. Wikipedia pages can then be represented as vectors in the space of (nonempty, stemmed, distinct) words, the vector coordinates being normalized tfidf values. By the encyclopedic nature of Wikipedia, one can consider that every page corresponds to a concept. We thus have a matrix whose columns are concepts and whose lines are words. By transposing it we obtain a representation of words in the space of concepts. The ESA measure of two words is simply the cosine of their vectors in this space.\nRoughly, two words are closely ESA-related if they appear frequently in the same Wikipedia pages (so that their tfs are high), and rarely in the corpus as a whole (for their dfs to be low).\nDespite the good results obtained by this method, it has given rise to some criticism. Thus, Haralambous & Klyuev [3] note that ESA has poor performance when the relation between words is mainly ontological. As an example, in the English corpus, the word “mile” (length unit) does not appear in the page of the word “kilometer” and the latter appears only once in the page of the former: this is hardly sufficient to establish a nonzero semantic relatedness value; however, such a relation is obvious, since both words refer to units of length measurement. As pointed out in [3], an ontological component, obtained from a WordNet-based measure, can, at least partially, fill this gap.\nAnother, more fundamental, criticism is that of Gottron et al. [4], who argue that the choice of Wikipedia is irrelevant, and that any corpus of comparable size would give the same results. To prove it, they base ESA not on Wikipedia, but on the Reuters news corpus, and get even better results than with standard ESA. According to the authors, the semantic relatedness value depends only on the collocational frequency of the terms, and this whether documents correspond to concepts or not. In other words they deny the “concept hypothesis,” namely that ESA specifically uses the correspondence between concepts and Wikipedia pages. Also they state that while “the application of ESA in a specific domain benefits from taking an index collection from the same topic domain while, on the other hand, a “general topic corpus” such as Wikipedia introduces noise,” and this has precisely been our motivation for strengthening the thematic robustness of ESA. Indeed, in this article we will enhance ESA by adopting a different approach: the persistence of tfidfs of terms when leaving pages and entering the category graph."
    }, {
      "heading" : "1.2 Wikipedia Categories",
      "text" : "A Wikipedia page can belong to one or more categories. Categories are represented by specific pages using the “Category:” prefix; these pages can again belong to other categories, so that we obtain a directed graph structure, the nodes of which can be standard pages (only outgoing edges) or categories (inand outgoing edges). A page can belong to several categories and there is no ranking of their semantic relevance. For this reason, to be able to use categories,\nwe first need an algorithm to determine the single semantically most relevant category, and for this we use, once again, ESA.\nWikipedia’s category graph has been studied thoroughly in [5] (for the English corpus)."
    }, {
      "heading" : "1.3 Related Work",
      "text" : "Scholl et al. [6] also enhance the performance of ESA using categories. They proceed as follows: let T be the matrix whose rows represent the Wikipedia pages and whose columns represent words. The value ti,j of cell (i, j) is the normalized tfidf of the jth word in the ith page. For each word m there is therefore a vector vm whose dimension is equal to the number of pages. Now let C be the matrix whose columns are pages and whose lines are categories. The value of a cell ci,j is 1 when page j belongs to category i and 0 otherwise. They take the product of matrices vm ·C which provides a vector whose jth component is ∑ i|Di∈cj ti,j , that is the sum of tfidfs of word m for all pages belonging to the jth category. They use the concatenation of vector vm and of the transpose of vm ·C to improve system performance on the text classification task. They call this method XESA (eXtended ESA).\nWe see that in this attempt, page tfidf is extended to categories by simply taking the sum of tfidfs of all pages belonging to a given category. This approach has a disadvantage when it comes to high-level categories: instead of being a way to find the words that characterize a given category, the tfidf of a word tends to become nothing more than the average density of the word in the corpus, since for large categories, tf tends to be the total number of occurrences of the word in the corpus, while the denominator idf remains constant and equal to the number of documents containing the given word. Thus, this type of tfidf loses its power of discrimination for high-level categories. As we will see in Section 1, we propose another extension of tfidf to categories, which we call categorical tfidf. The difference lies in the denominator, where we take the number, not of all documents containing the term, but only of those not belonging to the category. Thus our categorical tfidf (which is equal to the usual tfidf in the case of pages) is high when the term is common in the category and rare elsewhere (as opposed to rare on the entire corpus of Scholl et al.).\nIn [7], the authors examine the problem of inconsistency of Wikipedia’s category graph and propose a shortest path approach (based on the number of edges) between a page and the category “Article,” which is at the top of the hierarchy. The shortest path provides them with a semantic and thematic hierarchy and they calculate similarity as shortest length between vertices on these paths, a technique already used in WordNet [8]. However, as observed in [8, p. 275], the length (in number of edges) of the shortest path can vary randomly, depending on the density of pages (synsets, in the case of WordNet) in a given domain of knowledge. On the other hand, the distance (in number of edges) between a leaf and the top of the hierarchy is often quite short, frequently requiring an arbitrary choice between paths of equal length.\nWhat is common with our approach is the intention to simplify Wikipedia’s category graph. But instead of counting edges, we weight the graph using ESA\nmeasure and use this weight, which is based on the statistical presence of words on pages belonging to a given category, to calculate a maximum spanning tree. The result of this operation is that any page (or category other than “Article”) has exactly one parent category that is semantically closest to it. This calculation is global, in the sense that the total weight of the tree is maximum.\nWe use this tree to define thematically reinforced ESA. Our goal is to avoid words which, by accident, have a high tfidf in a given page despite the fact that they thematically do not really belong to it. This happens in the very frequent case where words have low frequencies (in the order of 1–3) so that the presence of an unsuitable word in a page results in a tfidf value as high (or even higher, if the word is seldom elsewhere) as the one of relevant words. Our hypothesis is that a word having an unduly high tfidf will disappear when we calculate its (categorical) tfidf in categories above the page, while, on the contrary, relevant words will be shared by other pages under the same category and their tfidfs will continue to be nonzero when switching to them. Such words will “survive” when we move away from leaves of the page-and-category tree and towards the root."
    }, {
      "heading" : "2 Thematic Reinforcement",
      "text" : ""
    }, {
      "heading" : "2.1 Standard Tfidf, Concept Vector and ESA Measure",
      "text" : "Let us first formalize the standard ESA model.1\nLetW be the Wikipedia corpus pruned by the standard ESA method, p ∈ W a Wikipedia page, and w ∈ p a word.2 The tfidf tp(w) of the word w on page p is defined as:\ntp(w) := (1 + log(fp(w))) · log  #W∑ p∈W w∈p 1  , where fp(w) is the frequency of w on page p, #W the cardinal ofW and ∑ p∈W w∈p 1, also known as the df (= document frequency) of w, is the number of Wikipedia pages containing w.\nConsider the space R#W , where dimensions correspond to pages p of W. Then we define the “concept vector” w of word w as\nw := ∑ p∈W tp(w) · 1p ∈ R#W\nwhere 1p is the unitary vector of R#W corresponding to page p. Let w and w′ be words appearing in Wikipedia (and hence the Euclidean norms ‖w‖ and ‖w′‖ of their concept vectors are nonzero). The ESA semantic\n1 All definitions in Section 2.1 are from [1]. 2 By “word” we mean an element of the set of character strings remaining after re-\nmoving stopwords and stemming the Wikipedia corpus.\nrelatedness measure µ is defined as follows:\nµ(w,w′) := 〈w,w′〉 ‖w‖ · ‖w′‖ ."
    }, {
      "heading" : "2.2 Categorical Tfidf",
      "text" : "Let c be a Wikipedia category. We define F(c) as the set of all pages p such that\n– either p belongs to c, – or p belongs to c1, and there a sequence of subcategory relations c1 → c2 → · · · → c, ending with c.\nDefinition 1 Let w ∈ p be a word of p ∈ W, tp(w) its standard tfidf in p, and c a category of W. We define the categorical tfidf tc(w) of w for category c as follows:\ntc(w) := 1 + log  ∑ p∈F(c) fp(w)  · log  #W 1 + ∑ p∈W\\F(c)\nw∈p 1  . The difference with the tfidf defined by [6] is in the calculation of df: instead\nof ∑ p∈W w∈p 1, that is the amount of pages containing w in the entire Wikipedia corpus, we focus on those in W \\ F(c), namely the set difference between the whole corpus and pages that are ancestors of c in the category graph, and we use 1 + ∑ p∈W\\F(c)\nw∈p 1 instead (the unit is added to prevent a zero df in the case\nwhere the word does not appear outside F(c)). We believe that this extension of tfidf to categories improves discriminatory potential, even when the sets of pages become large (see discussion in Section 1.3)."
    }, {
      "heading" : "2.3 Vectors of Pages and Categories",
      "text" : "Let p ∈ W be a page. We define the page vector p as the normalized sum of concept vectors of its words, weighted by their tfidfs:\nd :=\n∑ w∈p tp(w) ·w\n‖ ∑ w∈p tp(w) ·w‖ .\nSimilarly let c be a category of Wikipedia, we define the category vector c as\nc :=\n∑ w∈F(c) tc(w) ·w\n‖ ∑ w∈F(c) tc(w) ·w‖ .\nwhere w ∈ F(c) means that there exists a page p such that p ∈ F(c) and w ∈ p."
    }, {
      "heading" : "2.4 Wikipedia Arborification",
      "text" : "Definition 2 Let p be a Wikipedia page and c, c′ Wikipedia categories. Let p→ c be the membership of page d to category c, and c → c′ the subcategory relation between c and c. We define the weight of semantic relatedness of these relations as\np(p→ c) = 〈p, c〉. p(c→ c′) = 〈c, c′〉,\nwhere 〈 . , .〉 is the Euclidean scalar product of two vectors.\nThis product is equal to the cosine metric since the vectors are all unitary. By this property we also have Im(p) ⊂ [0, 1].\nThe relations considered in Definition 2 correspond to vertices of the Wikipedia category graph. Let W ′ be the weighted Wikipedia digraph (whose vertices are pages and categories, whose edges are memberships of pages and inclusions of categories, and whose weight is the weight of semantic relatedness).\nAt this point we can already reinforce the standard tfidf of words on pages, by the categorical tfidf of the same words in related categories. But how can we choose these categories? Taking all those containing a page would result in cacophony since categories can be more or less relevant and sometimes have no semantic relation whatsoever. Not to mention the fact that the Wikipedia category graph is quite complex, and using it as such would be computationally prohibiting.\nThe solution we present to this problem is to simplify W ′ by extracting a maximal spanning tree. It should be noted that standard minimal/maximal spanning tree algorithms such as Kruskal or Prim cannot be applied because W ′ is directed, has a global sink, namely the “Article” page, and we want the orientation of the directed spanning tree to be compatible with the one of the directed graph3.\nTo obtain the maximal spanning tree, we utilized Chu-Liu & Edmonds’ algorithm [9, p. 113-119], published for the first time in 1965. This semi-linear algorithm returns a minimum weight forest of rooted trees covering the digraph. The orientation of these rooted trees is compatible with the one of the graph. In the general case, connectivity is not guaranteed (even though the graph may be connected). But in the case of a digraph containing a global sink, the forest becomes a single tree, and we get a true directed maximal spanning tree of the graph. If our case, the global sink is obviously the category that is hierarchically at the top, namely “Article.”4\n3 It is a known fact that every rooted tree has exactly two possible orientations: one going from the root to the leaves and one in the opposite direction. 4 It should be noted, however, that the path between a page and the root on the maximal spanning tree is not a maximal path per se, since the importance is given to the global maximality of weight, for the whole tree. If our goal were to find the most appropriate taxonomy for a specific page, i.e., the most relevant path from this page to the top, then it would be more appropriate to use a shortest/longest path\nLet T be the maximal spanning tree of W ′ obtained by our method. As in any tree, there is a unique path between any two nodes. In particular, there is a unique path between any page-node and the root; we call it the sequence of ancestors of the page."
    }, {
      "heading" : "2.5 Thematically Reinforced ESA",
      "text" : "We will use the page ancestors in the maximal spanning tree to update tfidf values of words in the page vectors. Indeed, a word in a given page may have a high tfidf value simply because it occurred one or two times, this does not guarantee a significant semantic proximity between the word and the page. But if the word appears also in ancestor categories (and hence, in other pages belonging to the same category), then we have stronger chances for semantic pertinence.\nDefinition 3 Let p be a Wikipedia page, w a word w ∈ p, tp(w) the standard tfidf of w in p, (πi(p))i the sequence of ancestors of p, and (λi)i a decreasing sequence of positive real numbers converging to 0. We define the thematically reinforced tfidf tp,λ∗(w) as\ntp,λ∗(w) = tp(w) + ∑ i≥0 λitπi(p)(w).\nThe sum is finite because the Wikipedia maximal spanning tree is finite and hence there is a maximal distance from the root, after which the πi become vacuous.\nDefinition 4 With the notations of Definition 3, we define the thematically reinforced concept vector wλ∗ as\nwλ∗ := ∑ p∈W tp,λ∗(w) · 1p ∈ R#W ."
    }, {
      "heading" : "In other words, it is the usual concept vector definition, but using thematically reinforced tfidf.",
      "text" : "With these tools we can define our extended version of ESA, as follows:\nDefinition 5 With the notations of Definition 3 and w,w′ ∈ W, we define the thematically reinforced ESA semantic relatedness measure µλ∗ as:\nµλ∗(w,w ′) := 〈wλ∗ ,w′λ∗〉 ‖wλ∗‖ · ‖w′λ∗‖ ."
    }, {
      "heading" : "In other words, it is the usual ESA measure definition, but using thematically reinforced concept vectors and tfidf.",
      "text" : "algorithm, such as Dijkstra. This has already been proposed in [7], but for the metric of the number of edges; in our case we would rather use the measure given by the weight of the graph.\nLog-distribution of ingoing degrees\nLog-distribution of outgoing degrees"
    }, {
      "heading" : "3 Corpus",
      "text" : "We have chosen to work on the French Wikipedia corpus (version of December 31, 2011), which is smaller than the English one and, to our knowledge, has not yet been used for ESA. To adapt ESA to French Wikipedia, we followed the same steps as [1] and [10] except for one: we have preceded stemming by lemmatization, to avoid loss of information due to poor stemming of inflected words. (In English, inflection is negligible, so that stemming can be performed directly.)\nOriginally, the authors of [1] pruned the 2005 English Wikipedia corpus down to 132,689 pages. In our case, by limiting the minimum size of pages to 125 (nonstop, lemmatized, stemmed and distinct) words, 15 incoming and 15 outgoing links, we obtained a number of Wikipedia pages comparable to that of the original ESA implementation, namely 128,701 pages (out of 2,782,242 in total) containing 1,446,559 distinct words (only 339,679 of which appear more than three times in the corpus).\nFurthermore, the French corpus contains 293,244 categories, 680,912 edges between categories and 12,935,688 edges between pages and categories. As can be seen on Fig. 1, by the logarithmic distribution of incoming and outgoing degrees, this graph follows a power distribution p−α with α = 2.08 for incoming degrees and α = 7.51 for outgoing degrees. According to [11, p. 248], the former value is typical, while the latter can be considered very high, and this was another motivation for simplifying the Wikipedia graph by extracting the maximal spanning tree, instead of performing heavy calculations on the entire graph.\nThe French Wikipedia category graph is fairly complex and, in particular, contains cycles. Indeed, according to [12], “cycles are not encouraged but may be tolerated in rare cases.” The very simple example of categories “Zoologie” (= Zoology) and “Animal” (in French Wikipedia) pointing to each other, shows that the semantic relation underlying subcategories is not always hyperonymy.\nHere animal is the object of study of the discipline zoology. We attempted the following experiment: starting from the 2,782,242 (unfiltered) French Wikipedia pages, we followed random paths formed by the category links. The choice of each subsequent category was made at random, but did not change during the experiment. 78% of these paths contained cycles, but it turned out that it was always the same 50 cycles, 12 of which were of length 3 (triangles) and all others of length 2 (categories pointing to each other, as in the example above, which was detected by this method). Hence, we were able to turn this directed graph acyclic by merely removing 50 edges."
    }, {
      "heading" : "4 Evaluation",
      "text" : "Gabrilovich and Markovitch [1] evaluate their method on WS-353, a set of 352 English word pairs, the semantic relatedness of which has been evaluated by 15– 16 human judges. Their criterion is the Spearman correlation coefficient between the rank of pairs obtained by ESA and that obtained by taking the average of human judgments. Our first attempt was to translate these pairs into French, but the result was rather disappointing.5\nWe have therefore chosen to evaluate our implementation of ESA in a more traditional way, by performing a text classification task. We have extracted a total of 20,000 French language messages from the 20 most popular French newsgroups. The characteristics of our evaluation corpus can be seen on Table 1, where the second column represents the number of messages for a given newsgroup, the third the number of words, and the fourth, the number of distinct stemmed nonstop words that also occur in Wikipedia.\nTo perform text classification we need to extend the definitions of tfidf and document vector to the evaluation corpus. Let C be the evaluation corpus and d a document d ∈ C. We define the tfidf td(w) of a word w ∈ d in C as\ntd(w) := (1 + log(fd(w))) · log (\n#C df(w)\n) ,\nwhere fd is the frequency of w in d; #C the total number of documents; df(w) the number of documents in C, containing w.\nFurthermore, our ESA implementation provides us with a concept vector w for every word w. We define the document vector d as:\nd :=\n∑ w∈d td(w) ·w\n‖ ∑ w∈d td(w) ·w‖ .\n5 Indeed, some twenty words are untranslatable into a simple term (the current version of ESA covers only single-word terms), such as “seafood” which can be translated only as “fruits de mer.” Furthermore there are ambiguities of translation resulting from word polysemy: When we translate the pair “flight/car” by “vol/voiture,” we obtain a high semantic relatedness due to the criminal sense of “vol” (= theft) while the sense of the English word “flight” is mainly confined to the domain of aviation. Finally, some obvious collocations disappear when translating word for word, such as “soap/opera” which is unfortunately not comparable to “savon/opéra”. . .\nwhere the denominator is used for normalization.\nUsing these vectors, text classification becomes standard classification in R#W for the cosine metric. We applied the linear multi-class SVM classifier SVMmulticlass [13] to the set of these vectors and the corresponding document classes, and after a tenfold cross-validation, we obtained an average precision of 65.58% for a C coefficient of 3.0. The classification required 324 support vectors. Admittedly the precision obtained is rather low, which is partly due to the thematic proximity of some classes (like, for example, Religion and Sects, or Writing and French language). However, our goal is not to compare ESA to other classification methods, but to show that our approach improves ESA. So, this result is our starting point and we intend to improve it.\nWe followed the same modus operandi using thematically reinforced methods and obtained the results displayed on Table 2. The results show a significant improvement over the standard ESA version (that corresponds to λi = 0 for all i. This confirms our approach. On Fig. 2 the reader can see the precision obtained as function of the two first parameters λ1 and λ2, as well the number of support vectors used. We notice that the precision varies slightly (between 74.36% and 75.015%, that is less than 1%) as long as λ1 or λ2 are nonzero, and abruptly goes down to 65.58% when they are both zero. For nonzero values of λi the variation of precision follows no recognizable pattern. On the other hand, the number of support vectors shows a pattern: it is clearly correlated with λ1 and λ2, the highest value being 995, number of support vectors used when both\nTable 2. Evaluation results (ordered by decreasing precision)\nλ1 λ2 λ3 λ4 λ5 C # SVs Precision 1.5 0 0.5 0.25 0.125 3.0 786 75.015% 1 0 0.5 0.25 0.125 3.0 709 74.978% 1.5 1 0.5 0.25 0.125 3.0 827 74.899% 0.25 1.5 0.5 0.25 0.125 3.0 761 74.87% 0.5 0 0.5 0.25 0.125 3.0 698 74.867% 1 0.5 0.25 0.125 0.0625 3.0 736 74.845% 0.5 1 0.5 0.25 0.125 3.0 736 74.795% 1 1.5 0.5 0.25 0.125 3.0 865 74.791% 0.5 0.5 0.5 0.25 0.125 3.0 682 74.789% 0.5 1.5 0.5 0.25 0.125 3.0 778 74.814% 1.5 0.5 0.2 0.1 0.05 3.0 775 74.780% λ1 λ2 λ3 λ4 λ5 C # SVs Precision 0 1 0.5 0.25 0.125 3.0 710 74.716% 2 1 0.5 0.25 0.125 3.0 899 74.705% 2 0 0.5 0.25 0.125 3.0 852 74.675% 0.5 0.25 0.125 0.0625 0.0312 3.0 653 74.67% 2 0.5 0.5 0.25 0.125 3.0 899 74.641% 0.25 0.125 0.0625 0.0312 0.015 3.0 615 74.613% 1 1 1 0.5 0.25 3.0 796 74.61% 0 1.5 1 0.5 0.25 3.0 792 74.548% 1.5 1.5 1 0.75 0.25 3.0 900 74.471% 2 1.5 1 0.5 0.25 3.0 995 74.36% 0 0 0 0 0 3.0 324 65.58%\nλ1 and λ2 take their highest values. Since CPU time is roughly proportional to the number of support vectors, it is most interesting to take small (but nonzero) values of λi so that, at the same time, precision is high and the number of support vectors (and hence CPU time) is kept small."
    }, {
      "heading" : "5 Conclusion and Hints for Further Research",
      "text" : "By reinforcing the thematic context of words in Wikipedia pages, context obtained through the category structure, we claim to be able to improve the performance of the ESA measure.\nWe evaluated our method on a text classification task based on messages from the 20 most popular French language newsgroups: thematic reinforcement allowed us to improve the classification precision by 9–10%.\nHere are some hints for research to be done:\n1. propose the notion of the “most relevant category” to Wikipedia users and use their feedback to improve the system; 2. when we take the “most relevant category” for each page, we don’t consider by how much it is better than the others. For small differences of semantic relevance weight between categories one could imagine alternative “slightly worse” spanning trees and compare the results; 3. by comparing relevance between alternative “most relevant” categories for the same page one could quantify a “global potential” of the Wikipedia corpus. Compare with Wikipedia corpora in other languages; 4. aggregate the thematically reinforced measure with collocational and ontological components, as in [3]; 5. define another measure, based on links between pages (or categories), proportional to the number of links (or link paths) between pages and inversely proportional to the length of these paths. Compare it to ESA (which uses the number of links between pages to filter Wikipedia, but does not include it in semantic relatedness calculations) and thematically reinforced ESA; 6. and, more generally, explore the applications of graph theory to the formidable mathematical-linguistic objects represented by the different graphs extracted from Wikipedia."
    } ],
    "references" : [ {
      "title" : "Computing semantic relatedness using Wikipediabased explicit semantic analysis",
      "author" : [ "E. Gabrilovich", "S. Markovitch" ],
      "venue" : "IJCAI’07: Proceedings of the 20th international joint conference on Artifical intelligence",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Computer information retrieval using latent semantic structure (1989) US Patent 4,839,853",
      "author" : [ "S.C. Deerwester", "S.T. Dumais", "G.W. Furnas", "R.A. Harshman", "T.K. Landauer", "K.E. Lochbaum", "L.A. Streeter" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1989
    }, {
      "title" : "A Semantic Relatedness Measure Based on Combined Encyclopedic, Ontological and Collocational Knowledge",
      "author" : [ "Y. Haralambous", "V. Klyuev" ],
      "venue" : "International Joint Conference on Natural Language Processing, Chiang-Mai, Thailand",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Insights into explicit semantic analysis",
      "author" : [ "T. Gottron", "M. Anderka", "B. Stein" ],
      "venue" : "CIKM ’11: Proceedings of the 20th ACM international conference on Information and knowledge management.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Analysis of the Wikipedia category graph for NLP applications",
      "author" : [ "T. Zesch", "I. Gurevych" ],
      "venue" : "Workshop TextGraphs-2 : Graph-Based Algorithms for Natural Language Processing.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Extended explicit semantic analysis for calculating semantic relatedness of web resources",
      "author" : [ "P. Scholl", "D. Böhnstedt", "R.D. Garćıa", "C. Rensing", "R. Steinmetz" ],
      "venue" : "EC-TEL’10: Proceedings of the 5th European conference on Technology enhanced learning conference on Sustaining TEL: from innovation to learning and practice, Springer",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Constitution d’une ressource sémantique issue du treillis des catégories de wikipedia",
      "author" : [ "O. Collin", "B. Gaillard", "J.L. Bouraoui" ],
      "venue" : "TALN 2010.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Combining local context and WordNet similarity for word sense identification",
      "author" : [ "C. Leacock", "M. Chodorow" ],
      "venue" : "In Fellbaum, C., ed.: WordNet, an electronic lexical database, The MIT Press",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Efficient algorithms for finding minimum spanning trees in undirected and directed graphs",
      "author" : [ "H.N. Gabow", "Z. Galil", "T. Spencer", "R.E. Tarjan" ],
      "venue" : "Combinatorica 6",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Improving search result clustering by integrating semantic information from Wikipedia",
      "author" : [ "Çallı", "c." ],
      "venue" : "Master’s thesis, Middle East Technical University, Ankara",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Networks",
      "author" : [ "M. Newman" ],
      "venue" : "An Introduction. Oxford University Press",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Mining meaning from Wikipedia",
      "author" : [ "O. Medelyan", "C. Legg", "D. Milne", "I.H. Witten" ],
      "venue" : "International Journal of Human-Computer Studies 67(9)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Making large-scale SVM learning practical",
      "author" : [ "T. Joachims" ],
      "venue" : "In Schölkopf, B., Burges, C., Smola, A., eds.: Advances in Kernel Methods - Support Vector Learning, MIT Press",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Gabrilovich & Markovitch [1] introduce the semantic relatedness measure ESA (= Explicit Semantic Analysis, as opposed to the classical method of Latent Semantic Analysis [2]).",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "Gabrilovich & Markovitch [1] introduce the semantic relatedness measure ESA (= Explicit Semantic Analysis, as opposed to the classical method of Latent Semantic Analysis [2]).",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "Thus, Haralambous & Klyuev [3] note that ESA has poor performance when the relation between words is mainly ontological.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "As pointed out in [3], an ontological component, obtained from a WordNet-based measure, can, at least partially, fill this gap.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "[4], who argue that the choice of Wikipedia is irrelevant, and that any corpus of comparable size would give the same results.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Wikipedia’s category graph has been studied thoroughly in [5] (for the English corpus).",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "[6] also enhance the performance of ESA using categories.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "In [7], the authors examine the problem of inconsistency of Wikipedia’s category graph and propose a shortest path approach (based on the number of edges) between a page and the category “Article,” which is at the top of the hierarchy.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "The shortest path provides them with a semantic and thematic hierarchy and they calculate similarity as shortest length between vertices on these paths, a technique already used in WordNet [8].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : "1 are from [1].",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "The difference with the tfidf defined by [6] is in the calculation of df: instead of ∑ p∈W w∈p 1, that is the amount of pages containing w in the entire Wikipedia",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "By this property we also have Im(p) ⊂ [0, 1].",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "This has already been proposed in [7], but for the metric of the number of edges; in our case we would rather use the measure given by the weight of the graph.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "To adapt ESA to French Wikipedia, we followed the same steps as [1] and [10] except for one: we have preceded stemming by lemmatization, to avoid loss of information due to poor stemming of inflected words.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "To adapt ESA to French Wikipedia, we followed the same steps as [1] and [10] except for one: we have preceded stemming by lemmatization, to avoid loss of information due to poor stemming of inflected words.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Originally, the authors of [1] pruned the 2005 English Wikipedia corpus down to 132,689 pages.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "Indeed, according to [12], “cycles are not encouraged but may be tolerated in rare cases.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "Gabrilovich and Markovitch [1] evaluate their method on WS-353, a set of 352 English word pairs, the semantic relatedness of which has been evaluated by 15– 16 human judges.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "We applied the linear multi-class SVM classifier SVM [13] to the set of these vectors and the corresponding document classes, and after a tenfold cross-validation, we obtained an average precision of 65.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "aggregate the thematically reinforced measure with collocational and ontological components, as in [3]; 5.",
      "startOffset" : 99,
      "endOffset" : 102
    } ],
    "year" : 2014,
    "abstractText" : "We present an extended, thematically reinforced version of Gabrilovich and Markovitch’s Explicit Semantic Analysis (ESA), where we obtain thematic information through the category structure of Wikipedia. For this we first define a notion of categorical tfidf which measures the relevance of terms in categories. Using this measure as a weight we calculate a maximal spanning tree of the Wikipedia corpus considered as a directed graph of pages and categories. This tree provides us with a unique path of “most related categories” between each page and the top of the hierarchy. We reinforce tfidf of words in a page by aggregating it with categorical tfidfs of the nodes of these paths, and define a thematically reinforced ESA semantic relatedness measure which is more robust than standard ESA and less sensitive to noise caused by out-of-context words. We apply our method to the French Wikipedia corpus, evaluate it through a text classification on a 37.5 MB corpus of 20 French newsgroups and obtain a precision increase of 9–10% compared with standard ESA.",
    "creator" : "LaTeX with hyperref package"
  }
}