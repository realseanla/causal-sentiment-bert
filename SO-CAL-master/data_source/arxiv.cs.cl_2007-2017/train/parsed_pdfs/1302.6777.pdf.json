{
  "name" : "1302.6777.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Ending-based Strategies for Part-of-speech Tagging",
    "authors" : [ "Beth Millar", "Eric Neufeld", "Tim Philip" ],
    "emails" : [ "eric@spr.usask.ca" ],
    "sections" : null,
    "references" : [ {
      "title" : "Automated word­ class tagging of unseen words in text",
      "author" : [ "G. Adams", "E. Neufeld" ],
      "venue" : "In Pro­ ceedings of the Sixth International Symposium on Artificial Intelligence,",
      "citeRegEx" : "Adams and Neufeld,? \\Q1993\\E",
      "shortCiteRegEx" : "Adams and Neufeld",
      "year" : 1993
    }, {
      "title" : "Equations for part-of-speech tagging",
      "author" : [ "E. Charniak", "C. Henrickson", "N. Jacobson", "M. Perkowitz" ],
      "venue" : "In Proceedings of the Eleventh National Conference on Artificial Intelligence,",
      "citeRegEx" : "Charniak et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Charniak et al\\.",
      "year" : 1993
    }, {
      "title" : "A stochastic parts program and noun phrase parser for unrestricted text",
      "author" : [ "K.W. Church" ],
      "venue" : "In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Church,? \\Q1989\\E",
      "shortCiteRegEx" : "Church",
      "year" : 1989
    }, {
      "title" : "A comparison of the enhanced Good-Thring and deleted estima­ tion methods for estimating probabilities of En­ glish bigrams",
      "author" : [ "A. W" ],
      "venue" : "Computer Speech and Language,",
      "citeRegEx" : "W.,? \\Q1991\\E",
      "shortCiteRegEx" : "W.",
      "year" : 1991
    }, {
      "title" : "Grammatical category disam­ biguation by statistical optimization",
      "author" : [ "S.J. DeRose" ],
      "venue" : "Computa­ tional Linguistics,",
      "citeRegEx" : "DeRose,? \\Q1988\\E",
      "shortCiteRegEx" : "DeRose",
      "year" : 1988
    }, {
      "title" : "Statistical lexical disambiguation",
      "author" : [ "G.F. Foster" ],
      "venue" : "Master's thesis,",
      "citeRegEx" : "Foster,? \\Q1991\\E",
      "shortCiteRegEx" : "Foster",
      "year" : 1991
    }, {
      "title" : "The Computational Analysis of English: A Corpus­ Based Approach",
      "author" : [ "R. Garside", "G. Leech", "G. Sampson" ],
      "venue" : null,
      "citeRegEx" : "Garside et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Garside et al\\.",
      "year" : 1987
    }, {
      "title" : "The population frequencies of species and the estimation of population",
      "author" : [ "I.J. Good" ],
      "venue" : "parame­ ters. Biometrika,",
      "citeRegEx" : "Good,? \\Q1953\\E",
      "shortCiteRegEx" : "Good",
      "year" : 1953
    }, {
      "title" : "The LOB Corpus of British En­ glish texts: Presentation and comments",
      "author" : [ "S. Johansson" ],
      "venue" : "ALLC Journal,",
      "citeRegEx" : "Johansson,? \\Q1980\\E",
      "shortCiteRegEx" : "Johansson",
      "year" : 1980
    }, {
      "title" : "T he Tagged LOB Corpus: Users' Manual. Norwegian Computing Centre for the Humanities, Bergen, Norway",
      "author" : [ "S. Johansson", "E. Atwell", "R. Garside", "G. Leech" ],
      "venue" : null,
      "citeRegEx" : "Johansson et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Johansson et al\\.",
      "year" : 1986
    }, {
      "title" : "A cache-based nat­ ural language model for speech recognition",
      "author" : [ "R. Kuhn", "R. De Mori" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Kuhn and Mori,? \\Q1990\\E",
      "shortCiteRegEx" : "Kuhn and Mori",
      "year" : 1990
    }, {
      "title" : "Computa­ tional Analysis of Present-Day",
      "author" : [ "H. Kucera", "W.N. Francis" ],
      "venue" : null,
      "citeRegEx" : "Kucera and Francis,? \\Q1967\\E",
      "shortCiteRegEx" : "Kucera and Francis",
      "year" : 1967
    }, {
      "title" : "Tagging text with a probabilistic model",
      "author" : [ "B. Merialdo" ],
      "venue" : "In Proceedings of the IBM Natural Lan­ guage ITL,",
      "citeRegEx" : "Merialdo,? \\Q1990\\E",
      "shortCiteRegEx" : "Merialdo",
      "year" : 1990
    }, {
      "title" : "POST: Using probabilities in language process­",
      "author" : [ "M. Meteer", "R. Schwartz", "R. Weischedel" ],
      "venue" : "Proceedings of the 19th Interna­ tional Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Meteer et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Meteer et al\\.",
      "year" : 1991
    }, {
      "title" : "Annotation Manual for the Penn Treebank Project",
      "author" : [ "B. Santorini" ],
      "venue" : "Technical report, CIS Depart­ ment,",
      "citeRegEx" : "Santorini,? \\Q1990\\E",
      "shortCiteRegEx" : "Santorini",
      "year" : 1990
    }, {
      "title" : "Selected Studies of the Principle of Relative Frequency in Language. Harvard Univer­",
      "author" : [ "G.K. Zipf" ],
      "venue" : null,
      "citeRegEx" : "Zipf,? \\Q1932\\E",
      "shortCiteRegEx" : "Zipf",
      "year" : 1932
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Although probabilistic approaches to linguistic problems were attempted earlier in the cen­ tury (Zipf, 1932), they were hampered by the very real difficulties of collecting meaningful statistics and of performing subsequent calculations.",
      "startOffset" : 97,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "Recently prob­ abilistic approaches have overcome these difficulties with the availability of electronic corpora such as the LOB Corpus (Johansson, 1980; Johansson et al., 1986), the Brown Corpus (Kucera and Francis, 1967) or the UPenn corpus (Santorini, 1990), as well as the existence of powerful and inexpensive computers.",
      "startOffset" : 136,
      "endOffset" : 177
    }, {
      "referenceID" : 9,
      "context" : "Recently prob­ abilistic approaches have overcome these difficulties with the availability of electronic corpora such as the LOB Corpus (Johansson, 1980; Johansson et al., 1986), the Brown Corpus (Kucera and Francis, 1967) or the UPenn corpus (Santorini, 1990), as well as the existence of powerful and inexpensive computers.",
      "startOffset" : 136,
      "endOffset" : 177
    }, {
      "referenceID" : 11,
      "context" : ", 1986), the Brown Corpus (Kucera and Francis, 1967) or the UPenn corpus (Santorini, 1990), as well as the existence of powerful and inexpensive computers.",
      "startOffset" : 26,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : ", 1986), the Brown Corpus (Kucera and Francis, 1967) or the UPenn corpus (Santorini, 1990), as well as the existence of powerful and inexpensive computers.",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "A common approach to POS tagging is the hidden Markov model (HMM) (Jelinek, 1985; Church, 1989; Foster, 1991; Merialdo, 1990; Kuhn and De Mori, 1990) or variations thereof (DeRose, 1988; Garside et al.",
      "startOffset" : 66,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "A common approach to POS tagging is the hidden Markov model (HMM) (Jelinek, 1985; Church, 1989; Foster, 1991; Merialdo, 1990; Kuhn and De Mori, 1990) or variations thereof (DeRose, 1988; Garside et al.",
      "startOffset" : 66,
      "endOffset" : 149
    }, {
      "referenceID" : 12,
      "context" : "A common approach to POS tagging is the hidden Markov model (HMM) (Jelinek, 1985; Church, 1989; Foster, 1991; Merialdo, 1990; Kuhn and De Mori, 1990) or variations thereof (DeRose, 1988; Garside et al.",
      "startOffset" : 66,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "A common approach to POS tagging is the hidden Markov model (HMM) (Jelinek, 1985; Church, 1989; Foster, 1991; Merialdo, 1990; Kuhn and De Mori, 1990) or variations thereof (DeRose, 1988; Garside et al., 1987) where language is assumed to be produced by a hidden model that cannot be observed directly but whose effects can be observed.",
      "startOffset" : 172,
      "endOffset" : 208
    }, {
      "referenceID" : 6,
      "context" : "A common approach to POS tagging is the hidden Markov model (HMM) (Jelinek, 1985; Church, 1989; Foster, 1991; Merialdo, 1990; Kuhn and De Mori, 1990) or variations thereof (DeRose, 1988; Garside et al., 1987) where language is assumed to be produced by a hidden model that cannot be observed directly but whose effects can be observed.",
      "startOffset" : 172,
      "endOffset" : 208
    }, {
      "referenceID" : 5,
      "context" : "Since performance improves only marginally for k > 1 (Foster, 1991), we use k == 1.",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "An interesting problem is handling unseen words (Adams and Neufeld, 1993; Church, 1989; Fos�er, 1991; Merialdo, 1990; Meteer et al., 1991; Kuptec, 1992), that is, words not occurring in the tra��i�g cor­ pus, and therefore words for which probabihties �re not known.",
      "startOffset" : 48,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "An interesting problem is handling unseen words (Adams and Neufeld, 1993; Church, 1989; Fos�er, 1991; Merialdo, 1990; Meteer et al., 1991; Kuptec, 1992), that is, words not occurring in the tra��i�g cor­ pus, and therefore words for which probabihties �re not known.",
      "startOffset" : 48,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : "An interesting problem is handling unseen words (Adams and Neufeld, 1993; Church, 1989; Fos�er, 1991; Merialdo, 1990; Meteer et al., 1991; Kuptec, 1992), that is, words not occurring in the tra��i�g cor­ pus, and therefore words for which probabihties �re not known.",
      "startOffset" : 48,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "An interesting problem is handling unseen words (Adams and Neufeld, 1993; Church, 1989; Fos�er, 1991; Merialdo, 1990; Meteer et al., 1991; Kuptec, 1992), that is, words not occurring in the tra��i�g cor­ pus, and therefore words for which probabihties �re not known.",
      "startOffset" : 48,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "Testing the tagger on a subset of the tram­ ing corpus or only on known words (Foster, 1991; Meteer et al., 1991) inflates accuracy because much of the vocabulary is used infrequently.",
      "startOffset" : 78,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "Testing the tagger on a subset of the tram­ ing corpus or only on known words (Foster, 1991; Meteer et al., 1991) inflates accuracy because much of the vocabulary is used infrequently.",
      "startOffset" : 78,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "For example, (Adams and Neufeld, 1993) reports that after training a tagger on a 900,000 token subset of the LOB corpus, about 3700 of 100,000 tokens in the test corpus are unseen.",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "Building on this approach, and observing that such a set must be chosen by a native speaker of the language, in (Adams and Neufeld, 1993) statistics are collected on arbi�rary 2-, 3- and 4-letter word endings and other promu�ent word features such as capitalization or punctuatiOn.",
      "startOffset" : 112,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "In other work, Church (1989) uses capitalization to identify unseen proper nouns.",
      "startOffset" : 15,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "In other work, Church (1989) uses capitalization to identify unseen proper nouns. Meteer et al. (1991) attack unseen words by identifying combinations of prominent word features, in particular, statistics �n a definitive set of 32 predefined inflectional and denva­ tional word endings, such as -ed and -ion.",
      "startOffset" : 15,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "Building on this approach, and observing that such a set must be chosen by a native speaker of the language, in (Adams and Neufeld, 1993) statistics are collected on arbi�rary 2-, 3- and 4-letter word endings and other promu�ent word features such as capitalization or punctuatiOn. That work also uses an external lexicon with no statis­ tics but containing associated part of speech tags with words that helps the tagger to avoid bad guesses. Ku­ piec (1992) uses estimates of frequency of features with tags.",
      "startOffset" : 113,
      "endOffset" : 460
    }, {
      "referenceID" : 7,
      "context" : "We also compared the \"add-one\" estimate against the Good-Turing estimator (Good, 1953).",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "4 per cent) (Meteer et al., 1991; Adams and Neufeld, 1993) and guessing randomly at unseen words.",
      "startOffset" : 12,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "4 per cent) (Meteer et al., 1991; Adams and Neufeld, 1993) and guessing randomly at unseen words.",
      "startOffset" : 12,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "This work also sup­ ports the kind of observations in (Meteer et al., 1991) that taggers perhaps don't require the huge training",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "In (Meteer et al., 1991), it is noted that a tagger trained on 64,000 words rather than 1,000,000 suffers a relatively small decline in per­",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "One is suggested by the result of (Adams and Neufeld, 1993), where it was observed that a tagger containing all whole-word statistics from the training corpus performed better with three-letter endings than with either two- or four-letter endings.",
      "startOffset" : 34,
      "endOffset" : 59
    } ],
    "year" : 2011,
    "abstractText" : "Probabilistic approaches to part-of-speech tagging rely primarily on whole-word statis­ tics about word/tag combinations as well as contextual information. But experience shows about 4 per cent of tokens encountered in test sets are unknown even when the train­ ing set is as large as a million words. Unseen words are tagged using secondary strategies that exploit word features such as endings, capitalizations and punctuation marks. In this work, word-ending statistics are pri­ mary and whole-word statistics are sec­ ondary. First, a tagger was trained and tested on word endings only. Subsequent ex­ periments added back whole-word statistics for the N words occurring most frequently in the training set. As N grew larger, per­ formance was expected to improve, in the limit performing the same as word-based tag­ gers. Surprisingly, the ending-based tag­ ger initially performed nearly as well as the word-based tagger; in the best case, its per­ formance significantly exceeded that of the word-based tagger. Lastly, and unexpect­ edly, an effect of negative returns was ob­ servedas N grew larger, performance gen­ erally improved and then declined. By vary­ ing factors such as ending length and tag-list strategy, we achieved a success rate of 97.5 percent.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}