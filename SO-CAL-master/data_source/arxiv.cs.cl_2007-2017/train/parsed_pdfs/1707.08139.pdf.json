{
  "name" : "1707.08139.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Analogs of Linguistic Structure in Deep Representations",
    "authors" : [ "Jacob Andreas", "Dan Klein" ],
    "emails" : [ "jda@cs.berkeley.edu", "klein@cs.berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The past year has seen a renewal of interest in endto-end learning of communication strategies between pairs of agents represented with deep networks (Wagner et al., 2003). Approaches of this kind make it possible to learn decentralized policies from scratch (Foerster et al., 2016; Sukhbaatar et al., 2016), with multiple agents coordinating via learned communication protocol. More generally, any encoder–decoder model (Sutskever et al., 2014) can be viewed as implementing an analogous communication protocol, with the input encoding playing the role of a message in an artificial “language” shared by the encoder and decoder (Yu et al., 2016). Earlier work has found that under suitable conditions, these protocols acquire simple interpretable lexical (Dircks and Stoness, 1999; Lazaridou et al., 2016) and sequential structure (Mordatch and Abbeel, 2017), even without natural language training data.\n1 Code and data are available at http://github. com/jacobandreas/rnn-syn.\nOne of the distinguishing features of natural language is compositionality: the existence of operations like negation and coordination that can be applied to utterances with predictable effects on meaning. RNN models trained for natural language processing tasks have been found to learn representations that encode some of this compositional structure—for example, sentence representations for machine translation encode explicit features for certain syntactic phenomena (Shi et al., 2016) and represent some semantic relationships translationally (Levy et al., 2014). It is thus natural to ask whether these “language-like” structures also arise spontaneously in models trained directly from an environment signal. Rather than using language as a form of supervision, we propose to use it as a probe—exploiting post-hoc statistical correspondences between natural language descriptions and neural encodings to discover regular structure in representation space.\nTo do this, we need to find (vector, string) pairs with matching semantics, which requires first aligning unpaired examples of human–human\nar X\niv :1\n70 7.\n08 13\n9v 1\n[ cs\n.C L\n] 2\n5 Ju\nl 2 01\n7\ncommunication with network hidden states. This is similar to the problem of “translating” RNN representations recently investigated in Andreas et al. (2017). Here we build on that approach in order to perform a detailed analysis of compositional structure in learned “languages”. We investigate a communication game previously studied by FitzGerald et al. (2013), and make two discoveries: in a model trained without any access to language data,\n1. The strategies employed by human speakers in a given communicative context are surprisingly good predictors of RNN behavior in the same context: humans and RNNs send messages whose interpretations agree on nearly 90% of object-level decisions, even outside the contexts in which they were produced.\n2. Interpretable language-like structure naturally arises in the space of representations. We identify geometric regularities corresponding to negation, conjunction, and disjunction, and show that it is possible to linearly transform representations in ways that approximately correspond to these logical operations."
    }, {
      "heading" : "2 Task",
      "text" : "We focus our evaluation on a communication game due to FitzGerald et al. (2013) (Figure 1, top). In this game, the speaker observes (1) a world W of 1–20 objects labeled with with attributes and (2) a designated target subset X of objects in the world. The listener observes only W , and the speaker’s goal is to communicate a representation of X that enables the listener to accurately reconstruct it. The GENX dataset collected for this purpose contains 4170 human-generated natural-language referring expressions and corresponding logical forms for 273 instances of this game. Because these human-generated expressions have all been pre-annotated, we treat language and logic interchangeably and refer to both with the symbol e. We write e(W ) for the expression generated by a human for a particular world W , and JeKW for the result of evaluating the logical form e against W .\nWe are interested in using language data of this kind to analyze the behavior of a deep model trained to play the same game. We focus our analysis on a standard RNN encoder–decoder, with the encoder playing the role of the speaker and the\ndecoder playing the role of the listener. The encoder is a single-layer RNN with GRU cells (Cho et al., 2014) that consumes both the input world and target labeling and outputs a 64-dimensional hidden representation. We write f(W ) for the output of this encoder model on a world W . To make predictions, this representation is passed to a decoder implemented as a multilayer perceptron. The decoder makes an independent labeling decision about every object in W (taking as input both f and a feature representation of a particular object Wi). We write JfKW for the full vector of decoder outputs on W . We train the model maximize classification accuracy on randomly-generated scenes and target sets of the same form as in the GENX dataset."
    }, {
      "heading" : "3 Approach",
      "text" : "We are not concerned with the RNN model’s raw performance on this task (it achieves nearly perfect accuracy). Instead, our goal is to explore what kinds of messages the model computes in order to achieve this accuracy—and specifically whether these messages contain high-level semantics and low-level structure similar to the referring expressions produced by humans. But how do we judge semantic equivalence between natural language and vector representations? Here, as in Andreas et al. (2017), we adopt an approach inspired by formal semantics, and represent the meaning of messages via their truth conditions (Figure 1).\nFor every problem instance W in the dataset, we have access to one or more human messages e(W ) as well as the RNN encoding f(W ). The truth-conditional account of meaning suggests that we should judge e and f to be equivalent if they designate the same set of of objects in the world (Davidson, 1967). But it is not enough to compare their predictions solely in the context where they were generated—testing if JeKW = JfKW— because any pair of models that achieve perfect accuracy on the referring expression task will make the same predictions in this initial context, regardless of the meaning conveyed.\nInstead, we sample a collection of alternative worlds {Wi} observed elsewhere in the dataset, and compute a tabular meaning representation rep(e) = {JeKWi} by evaluating e in each world Wi. We similarly compute rep(f) = {JfKWi}, allowing the learned decoder model to play the role of logical evaluation for message vectors. For\nlogically equivalent messages, these tabular representations are guaranteed to be identical, so the sampling procedure can be viewed as an approximate test of equivalence. It additionally allows us to compute softer notions of equivalence by measuring agreement on individual worlds or objects."
    }, {
      "heading" : "4 Interpreting the meaning of messages",
      "text" : "We begin with the simplest question we can answer with this tool: how often do the messages generated by the encoder model have the same meaning as messages generated by humans for the same context? Again, our goal is not to evaluate the performance of the RNN model, but instead our ability to understand its behavior. Does it send messages with human-like semantics? Is it more explicit? Or does it behave in a way indistinguishable from a random classifier?\nFor each scene in the GENX test set, we compute the model-generated message f and its tabular representation rep(f), and measure the extent to which this agrees with representations produced by three “theories” of model behavior (Figure 2): (1) a random theory that accepts or rejects objects with uniform probability, (2) a literal theory that predicts membership only for objects that exactly match some object in the original target set, and (3) a human theory that predicts according to the most frequent logical form associated with natural language descriptions of the target set (as described in the preceding section). We evaluate agreement at the level of individual objects, worlds, and full tabular meaning representations.\nResults are shown in Table 1. Model behavior is well explained by human decisions in the same context: object-level decisions can be predicted with close to 90% accuracy based on human judgments alone, and a third of message pairs agree exactly in every sampled scene, providing strong evidence that they carry the same semantics.\nThese results suggest that the model has learned a communication strategy that is at least superficially language-like: it admits representations of the same kinds of communicative abstractions that humans use, and makes use of these abstractions with some frequency. But this is purely a statement about the high-level behavior of the model, and not about the structure of the space of representations. Our primary goal is to determine whether this behavior is achieved using lowlevel structural regularities in vector space that can themselves be associated with aspects of natural language communication."
    }, {
      "heading" : "5 Interpreting the structure of messages",
      "text" : "For this we turn to a focused investigation of three specific logical constructions used in natural language: a unary operation (negation) and two binary operations (conjunction and disjunction). All are used in the training data, with a variety of scopes (e.g. all green objects that are not a triangle, all the pieces that are not tan arches).\nBecause humans often find it useful to specify the target set by exclusion rather than inclusion, we first hypothesize that the RNN language might find it useful to incorporate some mechanism cor-\nresponding to negation, and that messages can be predictably “negated” in vector space. To test this hypothesis, we first collect examples of the form (e, f, e′, f ′), where e′ = ¬e, rep(e) = rep(f), and rep(e′) = rep(f ′). In other words, we find pairs of pairs of RNN representations f and f ′ for which the natural language messages (e, e′) serve as a denotational certificate that f ′ behaves as a negation of f . If the learned model does not have any kind of primitive notion of negation, we expect that it will not be possible to find any kind of predictable relationship between pairs (f, f ′). (As an extreme example, we could imagine every possible prediction rule being associated with a different point in the representation space, with the correspondence between position and behavior essentially random.) Conversely, if there is a first-class notion of negation, we should be able to select an arbitrary representation vector f with an associated referring expression e, apply some transformation N to f , and be able to predict a priori how the decoder model will interpret the representation Nf—i.e. in correspondence with ¬e.\nHere we make the strong assumption that the negation operation is not only predictable but linear. Previous work has found that linear operators are powerful enough to capture many hierarchical and relational structures (Paccanaro and Hinton, 2002; Bordes et al., 2014). Using examples (f, f ′) collected from the training set as described above, we compute the least-squares estimate N̂ = argminN\n∑ ||Nf − f ′||22 . To evaluate, we collect example representations from the test set that are equivalent to known logical forms, and measure how frequently model behaviors rep(Nf) agree with the logical predictions\nrep(¬e)—in other words, how often the linear operator N actually corresponds to logical negation. Results are shown in the top portion of Table 2. Correspondence with the logical form is quite high, resulting in 97% agreement at the level of individual objects and 45% agreement on full representations. We conclude that the estimated linear operator N̂ is analogous to negation in natural language. Indeed, the behavior of this operator is readily visible in Figure 3: predicted negated forms (in red) lie close in vector space to their true values, and negation corresponds roughly to mirroring across a central point.\nIn our final experiment, we explore whether the same kinds of linear maps can be learned for the binary operations of conjunction and disjunction. As in the previous section, we collect examples from the training data of representations whose denotations are known to correspond to groups of logical forms in the desired relationship—in this case tuples (e, f, e′, f ′, e′′, f ′′), where rep(e) = rep(f), rep(e′) = rep(f ′), rep(e′′) = rep(f ′′) and either e′′ = e ∧ e′ (conjunction) or e′′ = e ∨ e′ (disjunction). Since we expect that our operator will be symmetric in its arguments, we solve for M̂ = argminM ∑ ||Mf +Mf ′ − f ′′||22.\nResults are shown in the bottom portions of Table 2. Correspondence between the behavior predicted by the contextual logical form and the model’s actual behavior is less tight than for negation. At the same time, the estimated operators are clearly capturing some structure: in the case of disjunction, for example, model interpretations are correctly modeled by the logical form 92% of the time at the object level and 19% of the time at the denotation level. This suggests that the operations of conjunction and disjunction do have some functional counterparts in the RNN language, but that these functions are not everywhere well approximated as linear."
    }, {
      "heading" : "6 Conclusions",
      "text" : "Building on earlier tools for identifying neural codes with natural language strings, we have presented a technique for exploring compositional structure in a space of vector-valued representations. Our analysis of an encoder–decoder model trained on a reference game identified a number of language-like properties in the model’s representation space, including transformations corresponding to negation, disjunction, and conjunction. One major question left open by this analysis is what happens when multiple transformations are applied hierarchically, and future work might focus on extending the techniques in this paper to explore recursive structure. We believe our experiments so far highlight the usefulness of a denotational perspective from formal semantics when interpreting the behavior of deep models."
    } ],
    "references" : [ {
      "title" : "Translating neuralese",
      "author" : [ "Jacob Andreas", "Anca Dragan", "Dan Klein." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Andreas et al\\.,? 2017",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2017
    }, {
      "title" : "Question answering with subgraph embeddings",
      "author" : [ "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1406.3676 .",
      "citeRegEx" : "Bordes et al\\.,? 2014",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.1259 .",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Truth and meaning",
      "author" : [ "Donald Davidson." ],
      "venue" : "Synthese 17(1):304–323.",
      "citeRegEx" : "Davidson.,? 1967",
      "shortCiteRegEx" : "Davidson.",
      "year" : 1967
    }, {
      "title" : "Effective lexicon change in the absence of population flux",
      "author" : [ "Christopher Dircks", "Scott Stoness." ],
      "venue" : "Advances in Artificial Life pages 720–724.",
      "citeRegEx" : "Dircks and Stoness.,? 1999",
      "shortCiteRegEx" : "Dircks and Stoness.",
      "year" : 1999
    }, {
      "title" : "Learning distributions over logical forms for referring expression generation",
      "author" : [ "Nicholas FitzGerald", "Yoav Artzi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "FitzGerald et al\\.,? 2013",
      "shortCiteRegEx" : "FitzGerald et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning to communicate with deep multi-agent reinforcement learning",
      "author" : [ "Jakob Foerster", "Yannis M Assael", "Nando de Freitas", "Shimon Whiteson." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2137–2145.",
      "citeRegEx" : "Foerster et al\\.,? 2016",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards multi-agent communicationbased language learning",
      "author" : [ "Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni." ],
      "venue" : "arXiv preprint arXiv:1605.07133 .",
      "citeRegEx" : "Lazaridou et al\\.,? 2016",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2016
    }, {
      "title" : "Linguistic regularities in sparse and explicit word representations",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan." ],
      "venue" : "pages 171–180.",
      "citeRegEx" : "Levy et al\\.,? 2014",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2014
    }, {
      "title" : "Emergence of grounded compositional language in multi-agent populations",
      "author" : [ "Igor Mordatch", "Pieter Abbeel." ],
      "venue" : "arXiv preprint arXiv:1703.04908 .",
      "citeRegEx" : "Mordatch and Abbeel.,? 2017",
      "shortCiteRegEx" : "Mordatch and Abbeel.",
      "year" : 2017
    }, {
      "title" : "Learning hierarchical structures with linear relational embedding",
      "author" : [ "Alberto Paccanaro", "Jefferey Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems. Vancouver, BC, Canada, volume 14, page 857.",
      "citeRegEx" : "Paccanaro and Hinton.,? 2002",
      "shortCiteRegEx" : "Paccanaro and Hinton.",
      "year" : 2002
    }, {
      "title" : "Does string-based neural mt learn source syntax",
      "author" : [ "Xing Shi", "Inkit Padhi", "Kevin Knight" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Shi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning multiagent communication with backpropagation",
      "author" : [ "Sainbayar Sukhbaatar", "Rob Fergus" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sukhbaatar and Fergus,? \\Q2016\\E",
      "shortCiteRegEx" : "Sukhbaatar and Fergus",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Progress in the simulation of emergent communication and language",
      "author" : [ "Kyle Wagner", "James A Reggia", "Juan Uriagereka", "Gerald S Wilkinson." ],
      "venue" : "Adaptive Behavior 11(1):37–69.",
      "citeRegEx" : "Wagner et al\\.,? 2003",
      "shortCiteRegEx" : "Wagner et al\\.",
      "year" : 2003
    }, {
      "title" : "A joint speaker-listener-reinforcer model for referring expressions",
      "author" : [ "Licheng Yu", "Hao Tan", "Mohit Bansal", "Tamara L Berg." ],
      "venue" : "arXiv preprint arXiv:1612.09542 .",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "The past year has seen a renewal of interest in endto-end learning of communication strategies between pairs of agents represented with deep networks (Wagner et al., 2003).",
      "startOffset" : 150,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : "Approaches of this kind make it possible to learn decentralized policies from scratch (Foerster et al., 2016; Sukhbaatar et al., 2016), with multiple agents coordinating via learned communication protocol.",
      "startOffset" : 86,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "More generally, any encoder–decoder model (Sutskever et al., 2014) can be viewed as implementing an analogous communication protocol, with the input encoding playing the role of a message in an artificial “language” shared by the encoder and decoder (Yu et al.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : ", 2014) can be viewed as implementing an analogous communication protocol, with the input encoding playing the role of a message in an artificial “language” shared by the encoder and decoder (Yu et al., 2016).",
      "startOffset" : 191,
      "endOffset" : 208
    }, {
      "referenceID" : 4,
      "context" : "Earlier work has found that under suitable conditions, these protocols acquire simple interpretable lexical (Dircks and Stoness, 1999; Lazaridou et al., 2016) and sequential structure (Mordatch and Abbeel, 2017), even without natural language training data.",
      "startOffset" : 108,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "Earlier work has found that under suitable conditions, these protocols acquire simple interpretable lexical (Dircks and Stoness, 1999; Lazaridou et al., 2016) and sequential structure (Mordatch and Abbeel, 2017), even without natural language training data.",
      "startOffset" : 108,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : ", 2016) and sequential structure (Mordatch and Abbeel, 2017), even without natural language training data.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "RNN models trained for natural language processing tasks have been found to learn representations that encode some of this compositional structure—for example, sentence representations for machine translation encode explicit features for certain syntactic phenomena (Shi et al., 2016) and represent some semantic relationships translationally (Levy et al.",
      "startOffset" : 266,
      "endOffset" : 284
    }, {
      "referenceID" : 8,
      "context" : ", 2016) and represent some semantic relationships translationally (Levy et al., 2014).",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "This is similar to the problem of “translating” RNN representations recently investigated in Andreas et al. (2017). Here we build on that approach in order to perform a detailed analysis of compositional structure in learned “languages”.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "This is similar to the problem of “translating” RNN representations recently investigated in Andreas et al. (2017). Here we build on that approach in order to perform a detailed analysis of compositional structure in learned “languages”. We investigate a communication game previously studied by FitzGerald et al. (2013), and make two discoveries: in a model trained without any access to language data,",
      "startOffset" : 93,
      "endOffset" : 321
    }, {
      "referenceID" : 5,
      "context" : "We focus our evaluation on a communication game due to FitzGerald et al. (2013) (Figure 1, top).",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "The encoder is a single-layer RNN with GRU cells (Cho et al., 2014) that consumes both the input world and target labeling and outputs a 64-dimensional hidden representation.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "But how do we judge semantic equivalence between natural language and vector representations? Here, as in Andreas et al. (2017), we adopt an approach inspired by formal semantics, and represent the meaning of messages via their truth conditions (Figure 1).",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "The truth-conditional account of meaning suggests that we should judge e and f to be equivalent if they designate the same set of of objects in the world (Davidson, 1967).",
      "startOffset" : 154,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "Previous work has found that linear operators are powerful enough to capture many hierarchical and relational structures (Paccanaro and Hinton, 2002; Bordes et al., 2014).",
      "startOffset" : 121,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "Previous work has found that linear operators are powerful enough to capture many hierarchical and relational structures (Paccanaro and Hinton, 2002; Bordes et al., 2014).",
      "startOffset" : 121,
      "endOffset" : 170
    } ],
    "year" : 2017,
    "abstractText" : "We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a “syntax” with functional analogues to qualitative properties of natural language.1",
    "creator" : "LaTeX with hyperref package"
  }
}