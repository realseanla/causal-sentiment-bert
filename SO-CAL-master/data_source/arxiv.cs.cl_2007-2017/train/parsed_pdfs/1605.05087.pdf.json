{
  "name" : "1605.05087.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Hirotaka Niitsuma" ],
    "emails" : [ "niitsuma@cs.okayama-u.ac.jp", "mholee@knu.ac.kr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n05 08"
    }, {
      "heading" : "1 Introduction",
      "text" : "Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are well-known multivariate data reduction techniques. These two methods are analyses of a correlation or variance–covariance matrix of a multivariate quantitative dataset. CA is an analysis of CD.\nAlso, CA is a statistical visualization method for picturing the associations between the levels of a two-way contingency table. To illustrate CA, one might consider the contingency table presented in Table 1, which shows data well known as Fisher’s data[4] for eye and hair color of people in Caithness, Scotland. The CA of these data yield the graphical display presented in Figure 1. Figure 1 shows the correspondence between eye and hair color.\nPCA is the eigenvalue decomposition of a covariance matrix. CCA is the singular value decomposition (SVD) based on the matrix representing correlation with modification of the average and the inhomogeneousness. CA can be regarded as SVD of the covariance of CD with modification of the average and inhomogeneousness like that of CCA. And the Gini-index is another way of addressing the variance of CD. Okada [11, 10] reported that defining the Giniindex using one-hot encoding on appropriately rotated space yields reasonable values for the covariance. This research describes the rotated Gini-index with modifying inhomogeneousness is equivalent to CA.\nPicca et al. [13] introduced nonlinear kernel extension of CA. Niitsuma et al. [9] introduced nonlinear kernel extension of the rotated Gini-index. Equivalence between CA and the modified Gini-index can define more general nonlinear kernel extension of CA. Calling this nonlinear kernel extension kernel Correspondence Analysis (KCA), we note that gives well-known analysis for CD and natural language processing by specializing kernels. For example, our KCA can give G-test, skip-gram with negative-sampling (SGNS)[8, 6], and GloVe[12] as a special case.\nWe apply KCA to natural language processing. Especially we specifically examine vector representations of words. Mikolov et al.[8] introduced dense vector representations of words referred to as word2vec. The vector representations give meaning subtraction or addition operations of the vectors. For example, the analogy “King is to queen as man is to woman.” is encoded in the vector space by the following vector equation: king − queen = man − woman. Sometimes CA also has a similar property. For example, we can see the vector equation: (eye:dark)− (eye:medium) = (hair:dark)− (hair:medium) in Figure 1.\nLevy et al. [6] showed that skip-gram with negative-sampling (SGNS), which is one of word2vec, can be regarded as analysis of the two-way contingency table in which the row represents a word, and the column represents context. [6] showed that SGNS is the matrix factorization of pointwise mutual information (PMI) of the contingency table. Actually, PMI can be regarded as a log-scale representation of a contingency table. We formulate this mechanism within KCA by providing the appropriate kernels."
    }, {
      "heading" : "2 Correspondence Analysis",
      "text" : "Actually, CA is a generalized singular value decomposition (GSVD) 1 based on a contingency table for two categorical variables such that\nŬ ŜV̆ t = Ξ (1)\nΞ = N/n− rct/n2 (2)\nŬ tD(r)−1Ŭ = V̆ tD(c)−1V̆ = E (3)\nwhere N is an nr × nc contingency table, with entries nij giving the frequency with which row categorical variables xr = i occur together with column categorical variable xc = j. In addition, xr ∈ {xr1, x r 2, ..., x r nr} is the categorical variable representing the row side of the contingency table N. xc ∈ {xc1, x c 2, ..., x c nc} represents the categorical variable representing the column side. r = N1 denotes the vector of row marginals, c = Nt1 is the vector of column marginals and n = 1tc = 1tr stands for the total number of observations. where 1 = (1, 1, 1...)t. E denotes an identity matrix. D(v) stands for the diagonal matrix for which diagonal entries are the components of vector v. Ŭ ŜV̆ t = Ξ is GSVD of Ξ so\n1 http://forrest.psych.unc.edu/research/vista-frames/pdf/chap11.pdf\nthat Ŭ tD(r)−1Ŭ = V̆ tD(c)−1V̆ = E. Usually CA use the following matrix for the visualize instead of using Ŭ and V̆ .\nF = D(r)−1Ŭ Ŝ (4)\nG = D(c)−1V̆ Ŝ (5)\nFor example, let us consider the contingency table presented in Table 1 . The table presents the joint population distribution of the categorical variable for eye color xeye ∈ { fair red medium dark black} and the categorical variable for hair color xhair ∈ { blue light medium dark}. For these data, r = (718, 1580, 1774, 1315)t, c = (1455, 286, 2137, 1391, 118)t, and n = 5387. CA using Fisher data is associated with the following GSVD.\n1 n N− 1 n2 rc t = 1 5387\n\n   326 38 241 110 3 688 116 584 188 4 343 84 909 412 26 98 48 403 681 85\n\n  \n+ 1\n53872\n\n   718 1580 1774 1315\n\n  \n\n     1455 286 2137 1391 118\n\n    \nt\n.\n(6)"
    }, {
      "heading" : "3 Gini’s definition of Variance and its Extension",
      "text" : "A contingency table can be constructed using the matrix product of two indicator matrices of\nN = HrtHc (7)\nwhere\nHr = [er(xr(a1)), e r(xr(a2)), ..., e r(xr(an))] t Hc = [ec(xc(a1)), e c(xc(a2)), ..., e c(xc(an))] t. (8)\nHr is the n×nr indicator matrix of xr. er(xr) is one-hot encoding of xr. a1..., an represent the observations. xr(ab) is the value of x\nr of b-th observation. Hc is an n×nc indicator matrix of xc. ec(xc) is one-hot encoding of xc. xc(ab) is the value of xc of b-th observation.\nFor example, the one-hot encodings and indicator matrix of xeys in Fisher’s data are\ne eye(blue) = (1, 0, 0, 0)t\ne eye(light) = (0, 1, 0, 0)t\ne eye(medium) = (0, 0, 1, 0)t\ne eye(dark) = (0, 0, 0, 1)t\n, Heye =\n\n    \ne eye(eye color of 1st person))t\n... 0, 0, 1, 0\n... e eye(eye color of n th person))t\n\n    \n(9)\nHhair can be constructed similarly. The contingency table of Fisher’s data is\nN = HeyetHhair. (10)\nBased on each observation, we can define the Gini-index as\nσ2(x) = 1\n2n2\nn ∑\na=1\nn ∑\nb=1\n(x(a)− x(b))2 (11)\nwhere\n(x(a) − x(b))2 = |ex(x(a)) − ex(x(b))|2\n2 =\n{\n0 x(a) = x(b) 1 x(a) 6= x(b). (12)\nGini-index is the variance of a categorical variable[5]. For example, σ2(xeye) using Fisher’s data is\nσ2(xeye) = 1\n2 · 53872\n5387 ∑\na=1\n5387 ∑\nb=1\n((eye color of a th person)−(eye color of b th person))2.\nA simple extension of this definition to the covariance σ2(xr , xc) by replacing (x(a)− x(b))2 to (xr(a)− xr(b))(xc(a)− xc(b)) does not give reasonable values for the covariance σ2(xr, xc) [11]. Okada[11, 10] shown using rotated one-hot encoding provides reasonable values for the covariance of CD:\nσ2(xr , xc) =maximize R\n1\n2n2\nn ∑\na=1\nn ∑\nb=1\n1 2 (er(xr(a))− er(xr(a)))tR(ec(xc(a))− ec(xc(b))\nsubject to RtR = E (13)\nwhere R is a rotation matrix to maximize the covariance. We can rewrite Eq. (13) using Ξ, which is used in CA.\nσ2(xr , xc) =maximize R\n1 2 tr(RtΞ)\nsubject to RtR = E. (14)\nHere we use the following relation\n1\n2n2\nn ∑\na=1\nn ∑\nb=1\n1 2 (er(xr(a))− er(xr(b)))tR(ec(xc(a))− ec(xc(b)))\n= 1\n4n2 tr(Rt\nn ∑\na=1\nn ∑\nb=1\n(er(xr(a)) − er(xr(b)))(ec(xc(a))− ec(xc(b)))t)\n= tr(Rt( 1\n2n\nn ∑\na=1\ne r(xr(a))ec(xc(a))t −\n1\n2n2\nn ∑\na=1\ne r(xr(a))\nn ∑\nb=1\ne c(xc(b))t))\n= 1\n2 tr(Rt(\nHrtHc\nn −\nrc t n2 )) = 1 2 tr(Rt( N n − rc t n2 )) = 1 2 tr(RtΞ) (15)\nOne can solve the maximization problem by differentiating the following Lagrangian.\nL = tr(RtΞ) − tr(Λt(RtR−E)) (16)\nTherein, Λ is a Lagrange multiplier. The conditions for this Lagrangian L to be stationary with respect to R are RtΞ = (Λ+Λt). This result demonstrates that (RtΞ) must be a symmetric matrix. We can give a solution to this maximization problem using the following SVD.\nUSV t = Ξ (17)\nIt is readily apparent that R = UV t satisfies all necessary conditions. One can conclude that R = UV t is the solution of the problem (13).\nRecalling that CA is the GSVD of Ξ, then the GSVD Ŭ ŜV̆ t = Ξ can be computed using SVD of\nÛ ŜV̂ t = Ξ̂ (18)\nΞ̂ = D(r)−1/2ΞD(c)−1/2 (19)\nas shown below. Ŭ = D(r)1/2Û , V̆ = D(c)1/2V̂ (20)\nSubstituting this relation to Eq. (13) produces the following.\nmaximize R̆\n1 2 tr(R̆tD(r)−1ΞD(c)−1)\nsubject to R̆tD(r)−1R̆D(c)−1 = E (21)\nThe following relations show that the solution of this optimization problem is R̆ = Ŭ V̆ t . And we can say CA is equivalent to the problem (21).\nR̂ = Û V̂ t (22)\nR̆ = D(r)1/2R̂D(c)1/2 (23)\nR̂ = D(r)−1/2R̆D(c)−1/2 (24)\nR̆tD(r)−1R̆D(c)−1 = D(c)1/2R̂tR̂D(c)−1/2 = E ⇔ R̂tR̂ = E (25)\ntr(R̂tΞ̂) = tr(D(c)−1/2R̆tD(r)−1/2Ξ̂) = tr(R̆tD(r)−1ΞD(c)−1) (26)\nBased on these relation, we introduce scaled one-hot encoding as the following.\nê r(xr) = D(r)−1er(xr) , êc(xc) = D(c)−1ec(xc) (27)\nIt is noteworthy that\n[êr(xr1), ê r(xr2), ..., ê r(xrnr )] = D(r) −1[er(xr1), e r(xr2), ..., e r(xrnr )] = D(r) −1 E = D(r)−1 [êc(xc1), ê c(xc2), ..., ê c(xcnc)] = D(c) −1[ec(xc1), e c(xc2), ..., e c(xcnc)] = D(c) −1 E = D(c)−1\n(28)\nSubstituting this relation into problem (21) yields the following.\nmaximize R̆\n1\n4n2\n∑\na,b\n(êr(xr(a))− êr(xr(b)))tR̆(êc(xc(a))− êc(xc(b)))\nsubject to R̆t[êr(xr1), ê r(xr2), ...] tR̆[êc(xc1), ê c(xc2), ...] = E (29)\nIt is readily apparent that this problem defines the rotated Gini-index using scaled one-hot encoding. This generalized Gini-index is equivalent to CA."
    }, {
      "heading" : "4 Non-linear extension",
      "text" : "This section presents an attempt to generalize CA as possible to generalize while keeping the relation between Gini-index and CA. Let us introduce nonlinear mapping Φr,Φc and operators on nonlinear mapped spaces: ⊖r,⊖c,⊖,⊕. Rewriting the problem (29) using this nonlinear extension yields the following.\nmaximize R\n1\n4n2\n∑\na,b\n(Φr(êr(xr(a)))⊖r Φr(êr(xr(b))))tR(Φc(êc(xc(a)))⊖c Φc(êc(xc(b))))\nsubject to Rt[Φr(êr(xr1)),Φ r(êr(xr2)), ...] tR[Φc(êc(xc1)),Φ c(êc(xc2)), ...] = E\n(30)\nConsider expansion of this expression using below rules:\nX(Y1 ⊖ c Y2) t = XY t1 ⊖XY t 2 (31) (X1 ⊖ r X1)Y t = X1Y t ⊖X2Y t (32)\nX ⊖ (Y ⊖ Z) = (X ⊕ Z)⊖ Y (33)\n(X ⊖ Y )⊕ Z = (X ⊕ Z)⊖ Y (34)\n(X ⊖ Y )⊖ Z = X ⊖ (Z ⊕ Y ) (35)\nX ⊕ (Y ⊖ Z) = (X ⊕ Y )⊖ Z (36) θij = Φ r(êr(xri ))RΦ c(êc(xcj)) (37)\nΘ = [θij ] (38)\nWe consider the case of how to evaluate an expression including the operators ⊖, ⊖r, and ⊖c, which are not defined. Subsequently, we move these operators outside parentheses. To move the operators outside parentheses, some rules (31)–(36) are useful. In these rules, rewriting the left-hand side to the righthand side moves the operators outside parentheses. Applying these rewriting rules to the problem (30) of yields the following.\nmaximize R\n1\n4n2 tr(Rt(2nN⊕ ⊖ 2Υ⊕)\nsubject to RtΘ = E (39)\nwhere\nN ⊕ =\nn ∑\na=1\nΦr(êr(xr(a)))Φc(êc(xc(a)))t\n2Υ⊕ =\nn ∑\na=1\nn ∑\nb=1\nΦr(êr(xr(a)))Φc(êc(xc(b)))t (40)\nWhen ⊕ = +, and introducing kernel matrices Kr and Kc to be\nΘ = [θij ] = K rRKc (41)\ngives\nmaximize R\n1 2 tr(RtKr( 1 n N⊖ 1 n2 rc t)Kc) (42)\nsubject to RtKrRKc = E. (43)\nSpecifying the operators and kernel matrices provides various well known analyses for CD and natural language processing. Table 2 presents the relation between the specification and known methods. Linear CA is a simple CA without the nonlinear extension. Levy et al. [7] showed GloVe[12] is matrix factorization of sifted PMI. Based on that discussion, one can regard GloVe[12] as that of the KCA, as shown in Table 2. When some element of a contingency table is zero: nij = 0 then PMI = −∞. The G-test for contingency tables can avoid this problem. Let us call SVD of Ξ when X ⊖ Y = X(logX − log Y ) the G-test in this research."
    }, {
      "heading" : "5 Kernels for Natural language processing",
      "text" : "In this section, we specifically examine vector representations of words. We consider the following contingency table\nN = [nij ] = [#(wi, cj)] (44)\nwhere #(w, c) is the number of times that word w appears in context c. Using this contingency table, we can formulate SGNS as one specialized KCA, as shown in Table 2. Linear CA and G-test also provide vector representations of words using the contingency table.\nKCA can introduce tunable weights for stop words (SW) using the following kernel as\nKr = D(StopWordVector(αw))D(r)−1 Kc = D(StopWordVector(αc))D(c)−1 (45) where\nStopWordVector(α) = (..., 1 + αδi∈Stop Words, ...) (46)\nis the vector for which the weight α is added when i-th world is SW.We designate this mechanism as a stop word kernel.\nWhen we have datasets about similarity scores between two words, the scores can be encoded into the vector representations of words using our kernel mechanism. For example[3],\nscore(movie, theater) = 7.92\nscore(movie, star) = 7.38\nscore(movie, popcorn) = 6.19\nWe can encode such scores using defined operators ⊖r and ⊖c in Eq. (30) as presented below.\nΦr(êr(xra))⊖ r Φr(êr(xrb)) = (ê r(xra)− ê r(xrb))γ r(xa, xb)\nΦc(êc(xca))⊖ c Φc(êc(xcb)) = (ê c(xca)− ê c(xcb))γ c(xca, x c b) (47)\nwhere γrij = α rscore(i, j) + βr γcij = α cscore(i, j) + βc (48)\nαr, βr, αc, βc are tunable parameters. When ⊖ = − and ⊕ = +, the problem (30) is\nmaximize R\n1\n2n2 tr(RtD(r)−1(N ◦ (ΓrNΓc)− (ΓrN) ◦ (NΓc))D(c)−1\nsubject to RtD(r)−1RD(c)−1 = E\nr = (ΓrN) ◦ (NΓc)1\nc = ((ΓrN) ◦ (NΓc))t1 Γr = [γrij ] Γc = [γcij ] (49)\nwhere ◦ is the Hadamard product. We designate this mechanism as the word similarity kernel."
    }, {
      "heading" : "6 Experiments",
      "text" : "This section explains experimental evaluations of vector representations of words based on our proposed system. To calculate the contingency table N = [nij ] = [#(wi, cj)], we use the program code\n2 used in [7]. For the SW kernel, we use stop words set in NLTK[1]. For WS, we use the score of the MEN dataset[2] reported by Bruni et al. First, 20% data of Text8 corpus3 are used as training text data. The stop words set and word similarity data are too small compared to the entire Text8 corpus. Consequently, the stop word kernel and word similarity kernel have a small effect when using the whole Text8 corpus.\nFor evaluation, we use WordSim3533 dataset[3], the MEN dataset[2] of Bruni et al. and the Mechanical Turk dataset[14] reported by Radinsky et al.\n2https://bitbucket.org/omerlevy/hyperwords 3 http://mattmahoney.net/dc/text8.zip\nThe 100 dimensional word vectors presentations are evaluated by comparing ranking of their cosine similarities and the ranking in the test dataset. The rankings are compared using Spearman’s σ. We also show the SGNS result for comparison. Table 3 presents evaluation results. Linear CA provides vector representations of two types: F and G . We present the results of these two representations. We also show G-test results. The results for Linear CA and G-test with SW kernel and WS kernel are shown as “with SW” or “with WS” in the table.\nAlthough Linear CA and G-test have no tunable parameters, these results are comparable to SGNS. In most cases, the SW kernel enhances accuracy. The WS kernel enhances accuracy slightly in some cases."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We demonstrate that Linear Correspondence Analysis (CA) is equivalent to defining the Gini-index with rotated and scaled one-hot encoding. Moreover, we attempted to generalize CA as possible to be able to generalize while maintaining the relation between the Gini-index and CA. Kernel Correspondence Analysis (KCA) is introduced based on the nonlinear generalization of CA. KCA gives various known analyses for categorical data and natural language processing by specializing kernels. For example, KCA can give G-test, skip-gram with negative-sampling (SGNS), and GloVe as a special case. We introduce two kernels for natural language processing based on KCA. The proposed mechanism is evaluated by application of the problem of vector representations of words. Although Linear CA and G-test have no tunable parameter, these results are comparable to those obtained for SGNS. Additionally, we show that kernels with tunable parameters can enhance accuracy."
    } ],
    "references" : [ {
      "title" : "Natural Language Processing with Python",
      "author" : [ "S. Bird", "E. Klein", "E. Loper" ],
      "venue" : "O’Reilly Media,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distributional semantics in technicolor",
      "author" : [ "E. Bruni", "G. Boleda", "M. Baroni", "N.K. Tran" ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 136–145, Jeju Island, Korea, July",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin" ],
      "venue" : "ACM Trans. Inf. Syst., 20(1):116–131, Jan.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The precision of discriminant functions",
      "author" : [ "R.A. Fisher" ],
      "venue" : "Annals of Eugenics (London), 10:422–429,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1940
    }, {
      "title" : "Variability and mutability, contribution to the study of statistical distributions and relations",
      "author" : [ "C. Gini" ],
      "venue" : "studi economico-giuridici della r. universita de cagliari (1912). reviewed in: Light, r.j., margolin, b.h.: An analysis of variance for categorical data. J. American Statistical Association, 66:534– 544,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "O. Levy", "Y. Goldberg" ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8- 13 2014, Montreal, Quebec, Canada, pages 2177–2185,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "O. Levy", "Y. Goldberg", "I. Dagan" ],
      "venue" : "TACL, 3:211–225,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111– 3119. Curran Associates, Inc.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Kernel PCA for categorical data",
      "author" : [ "H. Niitsuma", "T. Okada" ],
      "venue" : "IE- ICE technical report. Artificial intelligence and knowledge-based processing, 103(305):13–17, sep",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Covariance and PCA for categorical variables",
      "author" : [ "H. Niitsuma", "T. Okada" ],
      "venue" : "Advances in Knowledge Discovery and Data Mining, 9th Pacific-Asia Conference, PAKDD 2005, Hanoi, Vietnam, May 18-20, 2005, Proceedings, pages 523–528,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A note on covariances for categorical data",
      "author" : [ "T. Okada" ],
      "venue" : "K. Leung, L. Chan, and H. Meng, editors, Intelligent Data Engineering and Automated Learning - IDEAL 2000,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "J. Pennington", "R. Socher", "C.D. Manning" ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Non-linear correspondence analysis in text retrieval: A kernel view",
      "author" : [ "D. Picca", "B. Curdy", "F. Bavaud" ],
      "venue" : "Proceedings of JADT,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A word at a time: Computing word relatedness using temporal semantic analysis",
      "author" : [ "K. Radinsky", "E. Agichtein", "E. Gabrilovich", "S. Markovitch" ],
      "venue" : "Proceedings of the 20th International Conference on World Wide Web, WWW ’11, pages 337–346, New York, NY, USA,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "To illustrate CA, one might consider the contingency table presented in Table 1, which shows data well known as Fisher’s data[4] for eye and hair color of people in Caithness, Scotland.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Okada [11, 10] reported that defining the Giniindex using one-hot encoding on appropriately rotated space yields reasonable values for the covariance.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "Okada [11, 10] reported that defining the Giniindex using one-hot encoding on appropriately rotated space yields reasonable values for the covariance.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : "[13] introduced nonlinear kernel extension of CA.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[9] introduced nonlinear kernel extension of the rotated Gini-index.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "For example, our KCA can give G-test, skip-gram with negative-sampling (SGNS)[8, 6], and GloVe[12] as a special case.",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "For example, our KCA can give G-test, skip-gram with negative-sampling (SGNS)[8, 6], and GloVe[12] as a special case.",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "For example, our KCA can give G-test, skip-gram with negative-sampling (SGNS)[8, 6], and GloVe[12] as a special case.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "[8] introduced dense vector representations of words referred to as word2vec.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] showed that skip-gram with negative-sampling (SGNS), which is one of word2vec, can be regarded as analysis of the two-way contingency table in which the row represents a word, and the column represents context.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] showed that SGNS is the matrix factorization of pointwise mutual information (PMI) of the contingency table.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Gini-index is the variance of a categorical variable[5].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "A simple extension of this definition to the covariance σ(x , x) by replacing (x(a)− x(b)) to (x(a)− x(b))(x(a)− x(b)) does not give reasonable values for the covariance σ(x, x) [11].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 10,
      "context" : "Okada[11, 10] shown using rotated one-hot encoding provides reasonable values for the covariance of CD: σ(x , x) =maximize R 1 2n2 n",
      "startOffset" : 5,
      "endOffset" : 13
    }, {
      "referenceID" : 9,
      "context" : "Okada[11, 10] shown using rotated one-hot encoding provides reasonable values for the covariance of CD: σ(x , x) =maximize R 1 2n2 n",
      "startOffset" : 5,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : "Name K K X ⊖ Y X ⊕ Y Linear CA D(r) D(c) X − Y X + Y Gini-index [11, 10] E E X − Y X + Y G-test E E X(logX − log Y ) X + Y SGNS[8, 6] E E (logX − log Y − log k) X + Y GloVe[12] E E (logX − log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )−e r(xrj )| 2 E X − Y X + Y",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "Name K K X ⊖ Y X ⊕ Y Linear CA D(r) D(c) X − Y X + Y Gini-index [11, 10] E E X − Y X + Y G-test E E X(logX − log Y ) X + Y SGNS[8, 6] E E (logX − log Y − log k) X + Y GloVe[12] E E (logX − log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )−e r(xrj )| 2 E X − Y X + Y",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "Name K K X ⊖ Y X ⊕ Y Linear CA D(r) D(c) X − Y X + Y Gini-index [11, 10] E E X − Y X + Y G-test E E X(logX − log Y ) X + Y SGNS[8, 6] E E (logX − log Y − log k) X + Y GloVe[12] E E (logX − log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )−e r(xrj )| 2 E X − Y X + Y",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "Name K K X ⊖ Y X ⊕ Y Linear CA D(r) D(c) X − Y X + Y Gini-index [11, 10] E E X − Y X + Y G-test E E X(logX − log Y ) X + Y SGNS[8, 6] E E (logX − log Y − log k) X + Y GloVe[12] E E (logX − log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )−e r(xrj )| 2 E X − Y X + Y",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "Name K K X ⊖ Y X ⊕ Y Linear CA D(r) D(c) X − Y X + Y Gini-index [11, 10] E E X − Y X + Y G-test E E X(logX − log Y ) X + Y SGNS[8, 6] E E (logX − log Y − log k) X + Y GloVe[12] E E (logX − log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )−e r(xrj )| 2 E X − Y X + Y",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : "Name K K X ⊖ Y X ⊕ Y Linear CA D(r) D(c) X − Y X + Y Gini-index [11, 10] E E X − Y X + Y G-test E E X(logX − log Y ) X + Y SGNS[8, 6] E E (logX − log Y − log k) X + Y GloVe[12] E E (logX − log Y + b + b) X + Y kernel PCA for CD[9] e r(xri )−e r(xrj )| 2 E X − Y X + Y",
      "startOffset" : 227,
      "endOffset" : 230
    }, {
      "referenceID" : 6,
      "context" : "[7] showed GloVe[12] is matrix factorization of sifted PMI.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[7] showed GloVe[12] is matrix factorization of sifted PMI.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "Based on that discussion, one can regard GloVe[12] as that of the KCA, as shown in Table 2.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "For example[3], score(movie, theater) = 7.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 6,
      "context" : "To calculate the contingency table N = [nij ] = [#(wi, cj)], we use the program code 2 used in [7].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "For the SW kernel, we use stop words set in NLTK[1].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "For WS, we use the score of the MEN dataset[2] reported by Bruni et al.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "For evaluation, we use WordSim3533 dataset[3], the MEN dataset[2] of Bruni et al.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "For evaluation, we use WordSim3533 dataset[3], the MEN dataset[2] of Bruni et al.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "and the Mechanical Turk dataset[14] reported by Radinsky et al.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "Name WordSim3533[3] Bruni[2] Randinsky[14] SGNS[7] 0.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Name WordSim3533[3] Bruni[2] Randinsky[14] SGNS[7] 0.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "Name WordSim3533[3] Bruni[2] Randinsky[14] SGNS[7] 0.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "Name WordSim3533[3] Bruni[2] Randinsky[14] SGNS[7] 0.",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2016,
    "abstractText" : "We show that Correspondence Analysis (CA) is equivalent to defining the Gini-index with appropriately scaled one-hot encoding. Using this relation, we introduce nonlinear kernel extension of CA. The extended CA gives well-known analysis for categorical data (CD) and natural language processing by specializing kernels. For example, our formulation can give G-test, skip-gram with negative-sampling (SGNS), and GloVe as special cases. We introduce two kernels for natural language processing based on our formulation: a stop word (SW) kernel and a word similarity (WS) kernel. The SW kernel is a system introducing appropriate weights for SW. The WS kernel enables the use of WS test data as training data for vector space representations of words. We show that these kernels enhance accuracy when training data are insufficiently numerous.",
    "creator" : "LaTeX with hyperref package"
  }
}