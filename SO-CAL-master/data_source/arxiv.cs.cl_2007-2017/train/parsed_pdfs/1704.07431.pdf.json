{
  "name" : "1704.07431.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "first.last@nrc.gc.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n07 43\n1v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nexciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system’s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach."
    }, {
      "heading" : "1 Introduction",
      "text" : "The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality. For “easy” language pairs such as English/French or English/Spanish in particular, neural (NMT) systems are much closer to human performance than previous statistical techniques (Wu et al., 2016). This puts pressure on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which exploit surface-matching heuristics that are relatively insensitive to subtle differences. As NMT continues to improve, these metrics will inevitably lose their effectiveness. Another challenge posed by NMT systems is their opacity: while it was usually clear which phenomena were ill-handled by previous statistical systems—and why—these questions are more difficult to answer for NMT.\nWe propose a new evaluation methodology centered around a challenge set of difficult examples that are designed using expert linguistic knowledge to probe an MT system’s capabilities. This methodology is complementary to the standard practice of randomly selecting a test set from “real text,” which remains necessary in order to predict performance on new text. By concentrating on difficult examples, a challenge set is intended to provide a stronger signal to developers. Although we believe that the general approach is compatible with automatic metrics, we used manual evaluation for the work presented here. Our challenge set consists of short sentences that each focus on one particular phenomenon, which makes it easy to collect reliable manual assessments of MT output by asking direct yes-no questions. An example is shown in Figure 1.\nWe generated a challenge set for English to French translation by canvassing areas of linguistic divergence between the two language pairs, especially those where errors would be made visible by French morphology. Example choice was also partly motivated by extensive knowledge of the weaknesses of phrase-based MT (PBMT). Neither of these characteristics is essential to our method, however, which we envisage evolving as NMT progresses. We used our challenge set to evaluate in-house PBMT and NMT systems as well as Google’s GNMT system.\nIn addition to proposing the novel idea of a challenge set evaluation, our contribution includes our annotated English-French challenge set, which we provide in an appendix and will make available in a separate machine-readable file. We also supply further evidence that NMT is systematically better than PBMT, even when BLEU score differences are small. Finally, we give an analysis of the challenges that remain to be solved in NMT, an area that has received little attention thus far."
    }, {
      "heading" : "2 Related Work",
      "text" : "A number of recent papers have evaluated NMT using broad performance metrics. The WMT 2016 News Translation Task (Bojar et al., 2016) evaluated submitted systems according to both BLEU and human judgments. NMT systems were submitted to 9 of the 12 translation directions, winning 4 of these and tying for first or second in the other 5, according to the official human ranking. Since then, controlled comparisons have used BLEU to show that NMT outperforms strong PBMT systems on 30 translation directions from the United Nations Parallel Corpus (Junczys-Dowmunt et al., 2016a), and on the IWSLT English-Arabic tasks (Durrani et al., 2016). These evaluations indicate that NMT performs better on average than previous technologies, but they do not help us understand what aspects of the translation have improved.\nSome groups have conducted more detailed error analyses. Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories. Compared to PBMT, NMT required less postediting effort over-all, with substantial improvements in lexical, morphological and word order errors. NMT consistently out-performed PBMT, but its performance degraded faster as sentence length increased. Later, Toral and Sánchez-Cartagena (2017) conducted a similar study, examining the outputs of competition-grade systems for the 9 WMT 2016 directions that included NMT competitors. They reached similar conclusions regarding morphological inflection and word order, but found an even greater degradation in NMT performance as sentence length increased, perhaps due to these systems’ use of subword units.\nMost recently, Sennrich (2016) proposed an ap-\nproach to perform targeted evaluations of NMT through the use of contrastive translation pairs. This method introduces a particular type of error automatically in reference sentences, and then checks whether the NMT system’s conditional probability model prefers the original reference or the corrupted version. Using this technique, they are able to determine that a recently-proposed character-based model improves generalization on unseen words, but at the cost of introducing new grammatical errors.\nOur approach differs from these studies in a number of ways. First, whereas others have analyzed sentences drawn from an existing bitext, we conduct our study on sentences that are manually constructed to exhibit canonical examples of specific linguistic phenomena. This challenge set methodology allows us to emphasize the difficult cases in an otherwise “easy” language pair. These sentences are designed to allow us to dive deep into phenomena of interest, and do a much finergrained analysis of the strengths of NMT than has come before. However, this strategy also necessitates that we work on many fewer sentences. We leverage the small size of our challenge set to manually evaluate whether the system’s actual output correctly handles our phenomena of interest. Manual evaluation side-steps some of the pitfalls that can come with Sennrich (2016)’s contrastive pairs, as a ranking of two contrastive sentences may not necessarily reflect whether the error in question will occur in the system’s actual output."
    }, {
      "heading" : "3 Challenge Set Evaluation",
      "text" : "Our challenge set is meant to measure the ability of MT systems to deal with some of the more difficult problems that arise in translating English into French. This particular language pair happened to be most convenient for us, but similar sets can be built for any language pair.\nOne aspect of MT performance that we aimed to exclude from our evaluation is robustness to sparse data. To control for this, when crafting source and reference sentences, we chose words that occurred at least 100 times in the training corpus described in section 4.1.1\n1With three principled exceptions: boeuf (87 occurrences) and spilt (58 occurrences)—both part of idiomatic phrases—and guitared (0 occurrences)."
    }, {
      "heading" : "3.1 Building the Challenge Set",
      "text" : "The challenging aspect of the test set we are presenting stems from the fact that the source English sentences have been chosen so that their closest French equivalent will be structurally divergent from the source in some crucial way. Translational divergences have been extensively studied in the past – see for example (Vinay and Darbelnet, 1958; Dorr, 1994). We expect the level of difficulty of an MT test set to correlate well with its density in divergence phenomena. We classify divergence phenomena into three main types: morpho-syntactic, lexicosyntactic and purely syntactic divergences."
    }, {
      "heading" : "Morpho-syntactic divergences",
      "text" : "In some languages, word morphology (e.g. inflections) carries more grammatical information than in others. When translating a word towards the richer language, there is a need to recover additional grammatically-relevant information from the context of the target language word. Note that we only include in our set cases where the relevant information is available in the linguistic context.2\nWe lack the space to describe all the subtypes of morpho-syntactic divergences that appear in our challenge set, but illustrate through representative examples. One particularly important case is that of subject-verb agreement. French verbs typically have more than 30 different inflected forms, while English verbs typically have 4 or 5. As a result, English verb forms strongly underspecify their French counterparts. Much of the missing information must be filled in through forced agreement in person, number and gender with the grammatical subject of the verb. But extracting these parameters can prove difficult. For example, the agreement features of a coordinated noun phrase are a complex function of the coordinated elements: a) the gender is feminine if all conjuncts are feminine, otherwise masculine wins; b) the conjunct with the smallest person (p1<p2<p3) wins; and c) the number is always plural when the coordination is “et” but the case is more complex with “ou”.\n2The so-called Winograd Schema Challenges (https://en.wikipedia.org/wiki/Winograd Schema Challenge) often involve cases where common-sense reasoning is required to correctly choose between two potential antecedent phrases for a pronoun. Such cases become En → Fr translation challenges when the pronoun in the source sentence is they and its alternative antecedents happen to have different grammatical genders in French: they → ils/elles.\nA second example of morpho-syntactic divergence between English and French is the more explicit marking of the subjunctive mood in French subordinate clauses. In the following example, the verb “partiez”, unlike its English counterpart, is marked as subjunctive:\nHe demanded that you leave immediately. → Il a exigé que vous partiez immédiatement.\nWhen translating an English verb within a subordinate clause, the context must be examined for possible subjunctive triggers. Typically these are specific lexical items found in a governing position with respect to the subordinate clause: verbs such as “exiger que”, adjectives such as “regrettable que” or subordinate conjunctions such as “à condition que”."
    }, {
      "heading" : "Lexico-syntactic divergences",
      "text" : "Syntactically governing words such as verbs tend to impose specific requirements on their complements: they subcategorize for complements of a certain syntactic type. But a source language governor and its target language counterpart can diverge on their respective requirements. The translation of such words must then trigger adjustments in the target language complement pattern. We can only examine here a few of the subtypes instantiated in our challenge set.\nA good example is argument switching. This refers to the situation where the translation of a source verb Vs as Vt is correct but only provided the arguments (usually the subject and the object) are flipped around. The translation of “to miss” as “manquer à” is such a case:\nJohn misses Mary → Mary manque à John.\nFailing to perform the switch results in a severe case of mistranslation.\nA second example of lexico-syntactic divergence is that of “crossing movement” verbs. Consider the following example:\nTerry swam across the river → Terry a traversé la rivière à la nage.\nThe French translation could be glossed as, “Terry crossed the river by swimming.” A literal translation such as “Terry a nagé à travers la rivière,” is ruled out."
    }, {
      "heading" : "Syntactic divergences",
      "text" : "Some syntactic divergences are not relative to the presence of a particular lexical item but rather stem from differences in the basic set of available syntactic patterns. Source-language instances of structures that do not exist in the target language must be mapped onto equivalent structures. Here are some of the subtypes appearing in our challenge set.\nThe position of French pronouns is a major case of divergence from English. French is basically an SVO language just like English but it departs from that canonical order when post-verbal complements are pronominalized: the pronouns must then be cliticized, that is phonetically attached to the verb, in this case to the left side of the verb.\nHe gave Mary a book. → Il a donné un livre à Marie.\nHe gavei itj to herk . → Il lej luik a donnéi.\nAnother example of syntactic divergence between English and French is that of stranded prepositions. When forming a relative clause or a question on a prepositional phrase, English can leave the preposition stranded, fronting only the pronominalized object of that preposition. In French, the preposition needs to be fronted alongside its object:\nThe girl whomi he was dancing withj is rich. → La fille avecj quii il dansait est riche.\nA final example of syntactic divergence is the use of the so-called middle voice. While English uses the passive voice in subjectless generic statements, French tends to prefer the use of a special pronominal construction where the pronoun “se” has no real referent:\nCaviar is eaten with bread. → Le caviar se mange avec du pain.\nThis completes our exemplification of morphosyntactic, lexico-syntactic and purely syntactic divergences. Our actual test set includes several more subcategories of each type. The ability of MT systems to deal with each such subcategory is then tested using at least three different test sentences. We use short test sentences so as to keep the targeted divergence in focus. The 108 sentences that constitute our current challenge set can be found in Appendix A."
    }, {
      "heading" : "3.2 Evaluation Methodology",
      "text" : "Given the very small size of our challenge set, it is easy to perform a human evaluation of the respective outputs of a handful of different systems. The obvious advantage is that the assessment is then absolute instead of relative to one or a few reference translations.\nThe intent of each challenge sentence is to test one and only one system capability, namely that of coping correctly with the particular associated divergence subtype. As illustrated in Figure 1, we provide annotators with a question that specifies the divergence phenomenon currently being tested, along with a reference translation with the areas of divergence highlighted. As a result, judgments become straightforward: was the targeted divergence correctly bridged, yes or no?3 There is no need to mentally average over a number of different aspects of the test sentence as one does when rating the global translation quality of a sentence, e.g. on a 5-point scale. However, we acknowledge that measuring translation performance on complex sentences exhibiting many different phenomena remains crucial. We see our approach as being complementary to evaluations of overall translation quality.\nOne consequence of our divergence-focused approach is that faulty translations will be judged as successes when the faults lie outside of the targeted divergence zone. However, this problem is mitigated by our use of short test sentences."
    }, {
      "heading" : "4 Machine Translation Systems",
      "text" : "We trained state-of-the-art neural and phrasebased systems for English-French translation on data from the WMT 2014 evaluation."
    }, {
      "heading" : "4.1 Data",
      "text" : "We used the LIUM shared-task subset of theWMT 2014 corpora,4 retaining the provided tokenization and corpus organization, but mapping characters to lowercase. Table 1 gives corpus statistics.\n3 Sometimes the system produces a translation that circumvents the divergence issue. For example, it may dodge a divergence involving adverbs by reformulating the translation to use an adjective instead. In these rare cases, we instruct our annotators to abstain from making a judgment, regardless of whether the translation is correct or not.\n4http://www.statmt.org/wmt14/translation-task.html http://www-lium.univ-lemans.fr/∼schwenk/nnmt-sharedtask"
    }, {
      "heading" : "4.2 Phrase-based systems",
      "text" : "To ensure a competitive PBMT baseline, we performed phrase extraction using both IBM4 and HMM alignments with a phrase-length limit of 7; after frequency pruning, the resulting phrase table contained 516M entries. For each extracted phrase pair, we collected statistics for the hierarchical reordering model of Galley and Manning (2008).\nWe trained an NNJM model (Devlin et al., 2014) on the HMM-aligned training corpus, with input and output vocabulary sizes of 64K and 32K. Words not in the vocabulary were mapped to one of 100 mkcls classes. We trained for 60 epochs of 20K x 128 minibatches, yielding a final dev-set perplexity of 6.88.\nOur set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7.\nWe include two phrase-based systems in our comparison: PBMT-1 has data conditions that exactly match those of the NMT system, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models."
    }, {
      "heading" : "4.3 Neural systems",
      "text" : "To build our NMT system, we used the Nematus toolkit,5 which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent\n5https://github.com/rsennrich/nematus\nunits (Cho et al., 2014). We used 512-dimensional word embeddings with source and target vocabulary sizes of 90K, and 1024-dimensional state vectors. The model contains 172M parameters.\nWe preprocessed the data using a BPE model learned from source and target corpora (Sennrich et al., 2016). Sentences longer than 50 words were discarded. Training used the Adadelta algorithm (Zeiler, 2012), with a minibatch size of 100 and gradients clipped to 1.0. It ran for 5 epochs, writing a checkpoint model every 30K minibatches. Following Junczys-Dowmunt et al. (2016b), we averaged the parameters from the last 8 checkpoints. To decode, we used the AmuNMT decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4.\nWhile our primary results will focus on the above PBMT and NMT systems, where we can describe replicable configurations, we have also evaluated Google’s production system,6 which has recently moved to NMT (Wu et al., 2016). Notably, the “GNMT” system uses (at least) 8 encoder and 8 decoder layers, compared to our 1 layer for each, and it is trained on corpora that are “two to three decimal orders of magnitudes bigger than the WMT.” The evaluated outputs were downloaded in December 2016."
    }, {
      "heading" : "5 Experiments",
      "text" : "The 108-sentence English-French challenge set presented in Appendix A was submitted to the four MT systems described in section 4: PBMT-1, PBMT-2, NMT, and GNMT. Three bilingual native speakers of French rated each translated sentence as either a success or a failure according to the protocol described in section 3.2. For example, the 26 sentences of the subcategories S1-S5 of Appendix A are all about different cases of subjectverb agreement. The corresponding translations were judged successful if and only if the translated verb correctly agrees with the translated subject."
    }, {
      "heading" : "5.1 Quantitative comparison",
      "text" : "Table 2 summarizes our results in terms of percentage of successful translations, globally and over each main type of divergence. For comparison with traditional metrics, we also include BLEU scores measured on the WMT 2014 test set.\nAs we can see, the two PBMT systems fare very poorly on our challenge set, especially in\n6https://translate.google.com\nthe morpho-syntactic and purely syntactic types. Their relatively better handling of lexico-syntactic cases probably reflects the fact that PBMT systems are naturally more attuned to lexical cues than to morphology or syntax. The two NMT systems are clear winners in all three categories. The GNMT system is best overall with a success rate of 68%, likely due to the data and architectural factors mentioned in section 4.3.7\nWMT BLEU scores correlate poorly with challenge-set performance. The large gap of 2.3 BLEU points between PBMT-1 and PBMT-2 corresponds to only a 1% gain on the challenge set, while the small gap of 0.4 BLEU between PBMT-2 and NMT corresponds to a 21% gain.\nInter-annotator agreement (final column in table 2) is excellent overall, with all three annotators agreeing on almost 90% of system outputs. Syntactic divergences appear to be somewhat harder to judge than other categories."
    }, {
      "heading" : "5.2 Qualitative assessment of NMT",
      "text" : "We now turn to an analysis of the strengths and weaknesses of neural MT through the microscope of our divergence categorization system, hoping that this may help focus future research on key issues. In this discussion we ignore the results obtained by PBMT-2 and compare: a) the results obtained by PBMT-1 to those of NMT, both systems having been trained on the same dataset; and b) the results of these two systems with those of Google NMT which was trained on a much larger dataset.\nIn the remainder of the present section we will reference the sentences of our challenge set using the subcategory-based numbering scheme S1-S26\n7We cannot offer a full comparison with the pre-NMT Google system. However, in October 2016 we ran a smaller 35-sentence version of our challenge set on both the Google system and our PBMT-1 system. The Google system only got 4 of those examples right (11.4%) while our PBMT-1 got 6 (17.1%).\nas assigned in Appendix A."
    }, {
      "heading" : "Strengths of neural MT",
      "text" : "Overall, both neural MT systems do much better than PBMT-1 at bridging divergences. Their dramatic advantage on morpho-syntactic divergences (a jump from 16% to 72% in the case of our two local systems) results from achievements such as the following:\n• The subject’s head noun agreement features\nget correctly passed to the verb phrase across intervening noun phrase complements (sentences S1a-c).\n• Subject agreement marks appear to be cor-\nrectly distributed to each element of a coordinated verb phrase (S3a-c).\n• Much of the calculus that is at stake in de-\ntermining the agreement features of a subject noun phrase (cf. our relevant description in section 3.1) appears to be correctly captured in the 12 translations of S4.\n• Most instances of the difficult case of past\nparticiple agreement after the “avoir” auxiliary are correctly handled (S5b-e).\nThe NMT systems are also better at handling\nlexico-syntactic divergences. For example:\n• They can perform the required restructuring\nof English double object constructions (sentences S8a-S8c).\n• They can discriminate between an NP com-\nplement and a sentential complement starting with an NP: cf. to know NP versus to know NP is VP (S11b-e)\n• They often correctly restructure English NP-\nto-VP complements (S12a-c).\nFinally, NMT systems also turn out to better handle purely syntactic divergences. For example:\n• The differences in yes-no question syntax is\ncorrectly bridged (S17a-c).\n• English pronouns in verb complement po-\nsition are often correctly cliticized, that is, moved before the main verb and caseinflected correctly (S23a-e).\n• The Google NMT system manages to cor-\nrectly translate tag questions (S18a-c), most cases of the “inalienable possession” construction (S25a-e), zero relative pronouns (S26a-c) and constructions with stranded prepositions (S19a-f).\nThe large gap observed between the results of the in-house and Google NMT systems indicates that current neural MT systems are extremely data hungry. But given enough data, they can successfully tackle some challenges that are often thought of as extremely difficult. A case in point is that of stranded prepositions, in which English and French happen to diverge in their handling of the celebrated “WH-movement” long-distance dependencies. Specifically, in the French translation, the preposition must be repatriated with its fronted WH object no matter how far on the left it happens to be."
    }, {
      "heading" : "Weaknesses of neural MT",
      "text" : "In spite of its clear edge over PBMT, NMT is not without some serious shortcomings. Some of them have been mentioned already, such as the tendency of system output to degrade with sentence length. By design this particular problem could not be observed with our challenge set. But many others get highlighted by an analysis of our results. Globally, we note that even using a staggering quantity of data and a highly sophisticated NMT model, the Google system fails to reach the 70% mark on our challenge set. Thus, there is ample room for improvement. The fine-grained error categorization associated with the challenge set makes it possible to single out precise areas where more research is needed. A first analysis of our results yields the following observations.\nIncomplete generalizations. In several cases, while partial results might suggest that NMT has correctly captured a basic generalization about linguistic data, further instances reveal that this is not fully the case. Here are some examples:\n• The calculus governing the agreement fea-\ntures of coordinated noun phrases (see section 3.1) appears to be handled correctly most\nof the time. However unlike our NMT system, the Google NMT system gets into difficulty with mixed-person subjects (sentences S4d1-3).\n• While some subjunctive mood triggers are\ncorrectly captured (e.g. “demander que” and “malheureux que”), others such as the very common subordinate conjunction provided that → à condition que are getting missed (sentence S6a).\n• The NMT systems often appear to have suc-\ncessfully captured the semantic relation that ties together the two nouns of an English noun compound, thereby giving rise to the correct preposition in the French translation N1 N2 → N2 Prep N1. However, some cases that one might think of as easy are being missed. For example, the Google translation of “steak knife” (sentence S14c) fails to convey that this is a knife intended to cut steak; similarly, the Google translation of “paper filter” (sentence S14i) suggests the filter is intended to filter paper rather being made of it.\n• The so-called French “inalienable posses-\nsion” construction arises when an agent performs an action on one of her body parts, e.g. I brushed my teeth. In such cases the French translation normally follows a pattern that can be glossed as He brushed the teeth to himself. In our dataset, the Google system gets this right for examples in the first and third persons (sentences S25a,b) but fails to do the same with the example in the second person (sentence S25c).\nThen there are also phenomena that current NMT systems, even with massive amounts of data, appear to be completely missing:\n• Idioms. While PBMT-1 produces an accept-\nable translation for half of the idiomatic expressions of S15 and S16, the local NMT system misses them all and the Google system does just slightly better. It looks as if NMT systems lack sufficient capacity for rawmemorization.\n• Control verbs. Two different classes of verbs\ncan govern a subject NP, an object NP plus an infinitival complement. With verbs of the “object-control” class (e.g. “persuade”), the\nobject of the verb is understood as the semantic subject of the infinitive. But with those of the subject class (e.g. “promise”), it is rather the subject of the verb which plays that semantic role. None of the systems tested here appear to get a grip on subject control cases, as evidenced by the lack of correct feminine agreement on the French adjectives in sentences S2b-d.\n• Argument switching verbs. All systems tested\nhere mistranslate sentences S7a-c by failing to perform the required argument switch: NP1 misses NP2 → NP2 manque à NP1.\n• Crossing movement verbs. None of the sys-\ntems managed to correctly restructure the regular manner-of-movement verbs e.g. swim across X→ traverser X à la nage in sentences S10a-c, let alone the even harder example S10d, in which the word “guitar” is spontaneously recast as a manner-of-movement verb.\n• Middle voice. None of the systems tested\nhere were able to recast the English “generic passive” of S21a-c into the expected French “middle voice” pronominal construction."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We have presented a radically different kind of evaluation for machine translation systems: the use of challenge sets designed to stress-test MT systems on “hard” linguistic material, while providing a fine-grained linguistic classification of their successes and failures. This approach is not meant to replace our community’s traditional evaluation tools but to supplement them.\nOur proposed error categorization scheme makes it possible to bring to light different strengths and weaknesses of PBMT and neural MT. With the exception of idiom processing, in all cases where a clear difference was observed it turned out to be in favor of neural MT. A key factor in NMT’s superiority appears to be its ability to overcome many limitations of n-gram language modeling. This is clearly at play in dealing with subject-verb agreement, double-object verbs, overlapping subcategorization frames and last but not least, the pinnacle of Chomskyan linguistics, WH-movement (in this case, stranded prepositions).\nBut our challenge set also brings to light some important shortcomings of current neural MT, re-\ngardless of the massive amounts of training data it may have been fed. As may have been already known or suspected, NMT systems struggle with the translation of idiomatic phrases. Perhaps more interestingly, we notice that neural MT’s impressive generalizations still seem somewhat brittle. For example, the NMT system can appear to have mastered the rules governing subject-verb agreement or inalienable possession in French, only to trip over a rather obvious instantiation of those rules. Probing where these boundaries are, and how they relate to the neural system’s training data and architecture is an obvious next step."
    }, {
      "heading" : "7 Future Work",
      "text" : "It is our hope that the insights derived from our challenge set evaluation will help inspire future MT research, and call attention to the fact that even “easy” language pairs like English-French still have many linguistic issues left to be resolved. But there are also several ways to improve and expand upon our challenge set approach itself.\nFirst, though our human judgments of output sentences allowed us to precisely assess the phenomena of interest, this approach is not scalable to large sets, and requires access to native speakers in order to replicate the evaluation. It would be interesting to see whether similar scores could be achieved through automatic means. The existence of human judgments for this set provides a goldstandard by which proposed automatic judgments may be meta-evaluated.\nSecond, the construction of such a challenge set is as much an art as a science, and requires indepth knowledge of the structural divergences between the two languages of interest. A method to automatically create such a challenge set for a new language pair would be extremely useful. One could imagine approaches that search for divergences, indicated by atypical output configurations, or perhaps by a system’s inability to reproduce a reference from its own training data. Localizing a divergence within a difficult sentence pair would be another useful subtask.\nFinally, and perhaps most interestingly, we would like to explore how to train anMT system to improve its performance on these divergence phenomena. This could take the form of designing a curriculum to demonstrate a particular divergence to the machine, or altering the network structure to more easily capture such generalizations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Cyril Goutte, Eric Joanis and Michel Simard, who graciously spent the time required to rate the output of four different MT systems on our challenge sentences. We also thank Roland Kuhn for valuable discussions, and comments on an earlier version of the paper."
    }, {
      "heading" : "A Supplemental Material",
      "text" : "We include a rendering of our challenge set in the pages that follow, along with system output for the PBMT-1, NMT and Google systems. Sentences are grouped by linguistic category and subcategory. For convenience, we also include a reference translation, which is a manually-crafted translation that is designed to be the most straightforward solution to the divergence problem at hand. Needless to say, this reference translation is seldom the only acceptable solution to the targeted divergence problem. Our judges were provided these references, but were instructed to use their knowledge of French to judge whether the divergence was correctly bridged, regardless of the translation’s similarity to the reference.\nIn all translations, the locus of the targeted divergence is highlighted in boldface and it is specifically on that portion that our annotators were asked to provide a judgment. For each system output, we provide a summary of our annotator’s judgments on its handling of the phenomenon of interest. We label the translation with a✓ if two or more annotators judged the divergence to be correctly bridged, and with an ✗ otherwise.\nWe will also release a machine-readable version of this same data, including all of the individual judgments, in the hope that others will find interesting new uses for this data."
    }, {
      "heading" : "Morpho-Syntactic",
      "text" : ""
    }, {
      "heading" : "S-V agreement, across distractors",
      "text" : "Is subject-verb agrement correct? (Possible interference distractors between the subject’s head and the verb). S1a Source The repeated calls from his mother should have alerted us.\nRef Les appels répétés de sa mère auraient dû nous alerter. PBMT-1 Les appels répétés de sa mère aurait dû nous a alertés. ✗ NMT Les appels répétés de sa mère devraient nous avoir alertés. ✓ Google Les appels répétés de sa mère auraient dû nous alerter. ✓\nS1b Source The sudden noise in the upper rooms should have alerted us.\nRef Le bruit soudain dans les chambres supérieures aurait dû nous alerter. PBMT-1 Le bruit soudain dans les chambres supérieures auraient dû nous a alertés. ✗ NMT Le bruit soudain dans les chambres supérieures devrait nous avoir alerté. ✓ Google Le bruit soudain dans les chambres supérieures devrait nous avoir alerté. ✓\nS1c Source Their repeated failures to report the problem should have alerted us.\nRef Leurs échecs répétés à signaler le problème auraient dû nous alerter. PBMT-1 Leurs échecs répétés de signaler le problème aurait dû nous a alertés. ✗ NMT Leurs échecs répétés pour signaler le problème devraient nous avoir alertés. ✓ Google Leur échec répété à signaler le problème aurait dû nous alerter. ✓\nS-V agreement, through control verbs Does the flagged adjective agree correctly with its subject? (Subject-control versus object-control verbs). S2a Source She asked her brother not to be arrogant.\nRef Elle a demandé à son frère de ne pas se montrer arrogant. PBMT-1 Elle a demandé à son frère de ne pas être arrogant. ✓ NMT Elle a demandé à son frère de ne pas être arrogant. ✓ Google Elle a demandé à son frère de ne pas être arrogant. ✓\nS2b Source She promised her brother not to be arrogant.\nRef Elle a promis à son frère de ne pas être arrogante. PBMT-1 Elle a promis son frère à ne pas être arrogant. ✗ NMT Elle a promis à son frère de ne pas être arrogant . ✗ Google Elle a promis à son frère de ne pas être arrogant. ✗\nS2c Source She promised her doctor to remain active after retiring.\nRef Elle a promis à son médecin de demeurer active après s’être retirée. PBMT-1 Elle a promis son médecin pour demeurer actif après sa retraite. ✗ NMT Elle a promis à son médecin de rester actif après sa retraite. ✗ Google Elle a promis à son médecin de rester actif après sa retraite. ✗\nS2d Source My mother promised my father to be more prudent on the road.\nRef Ma mère a promis à mon père d’être plus prudente sur la route. PBMT-1 Ma mère , mon père a promis d’être plus prudent sur la route. ✗ NMT Ma mère a promis à mon père d’être plus prudent sur la route. ✗ Google Ma mère a promis à mon père d’être plus prudent sur la route. ✗"
    }, {
      "heading" : "S-V agreement, coordinated targets",
      "text" : "Do the marked verbs/adjective agree correctly with their subject? (Agreement distribution over coordinated predicates)\nS3a Source The woman was very tall and extremely strong.\nRef La femme était très grande et extrêmement forte. PBMT-1 La femme était très gentil et extrêmement forte. ✗ NMT La femme était très haute et extrêmement forte. ✓ Google La femme était très grande et extrêmement forte. ✓\nS3b Source Their politicians were more ignorant than stupid.\nRef Leurs politiciens étaient plus ignorants que stupides. PBMT-1 Les politiciens étaient plus ignorants que stupide. ✗ NMT Leurs politiciens étaient plus ignorants que stupides. ✓ Google Leurs politiciens étaient plus ignorants que stupides. ✓\nS3c Source We shouted an insult and left abruptly.\nRef Nous avons lancé une insulte et nous sommes partis brusquement. PBMT-1 Nous avons crié une insulte et a quitté abruptement. ✗ NMT Nous avons crié une insulte et nous avons laissé brusquement. ✓ Google Nous avons crié une insulte et nous sommes partis brusquement. ✓\nS-V agreement, feature calculus on coordinated source Do the marked verbs/adjective agree correctly with their subject? (Masculine singular ET masculine singular yields masculine plural). S4a1 Source The cat and the dog should be watched.\nRef Le chat et le chien devraient être surveillés. PBMT-1 Le chat et le chien doit être regardée. ✗ NMT Le chat et le chien doivent être regardés. ✓ Google Le chat et le chien doivent être surveillés. ✓\nS4a2 Source My father and my brother will be happy tomorrow.\nRef Mon père et mon frère seront heureux demain. PBMT-1 Mon père et mon frère sera heureux de demain. ✗ NMT Mon père et mon frère seront heureux demain. ✓ Google Mon père et mon frère seront heureux demain. ✓\nS4a3 Source My book and my pencil could be stolen.\nRef Mon livre et mon crayon pourraient être volés. PBMT-1 Mon livre et mon crayon pourrait être volé. ✗ NMT Mon livre et mon crayon pourraient être volés. ✓ Google Mon livre et mon crayon pourraient être volés. ✓\nDo the marked verbs/adjectives agree correctly with their subject? (Feminine singular ET feminine singular yields feminine plural). S4b1 Source The cow and the hen must be fed.\nRef La vache et la poule doivent être nourries. PBMT-1 La vache et de la poule doivent être nourris. ✗ NMT La vache et la poule doivent être alimentées. ✓ Google La vache et la poule doivent être nourries. ✓\nS4b2 Source My mother and my sister will be happy tomorrow.\nRef Ma mère et ma sœur seront heureuses demain. PBMT-1 Ma mère et ma sœur sera heureux de demain. ✗ NMT Ma mère et ma sœur seront heureuses demain. ✓ Google Ma mère et ma sœur seront heureuses demain. ✓\nS4b3 Source My shoes and my socks will be found.\nRef Mes chaussures et mes chaussettes seront retrouvées. PBMT-1 Mes chaussures et mes chaussettes sera trouvé . ✗ NMT Mes chaussures et mes chaussettes seront trouvées. ✓ Google Mes chaussures et mes chaussettes seront trouvées. ✓\nDo the marked verbs/adjectives agree correctly with their subject? (Masculine singular ET feminine singular yields masculine plural.) S4c1 Source The dog and the cow are nervous.\nRef Le chien et la vache sont nerveux. PBMT-1 Le chien et la vache sont nerveux. ✓ NMT Le chien et la vache sont nerveux. ✓ Google Le chien et la vache sont nerveux. ✓\nS4c2 Source My father and my mother will be happy tomorrow.\nRef Mon père et ma mère seront heureux demain. PBMT-1 Mon père et ma mère se fera un plaisir de demain. ✗ NMT Mon père et ma mère seront heureux demain. ✓ Google Mon père et ma mère seront heureux demain. ✓\nS4c3 Source My refrigerator and my kitchen table were stolen.\nRef Mon réfrigérateur et ma table de cuisine ont été volés. PBMT-1 Mon réfrigérateur et ma table de cuisine ont été volés. ✓ NMT Mon réfrigérateur et ma table de cuisine ont été volés. ✓ Google Mon réfrigérateur et ma table de cuisine ont été volés. ✓\nDo the marked verbs/adjectives agree correctly with their subject? (Smallest coordinated grammatical person wins.) S4d1 Source Paul and I could easily be convinced to join you.\nRef Paul et moi pourrions facilement être convaincus de se joindre à vous. PBMT-1 Paul et je pourrais facilement être persuadée de se joindre à vous. ✗ NMT Paul et moi avons facilement pu être convaincus de vous rejoindre. ✓ Google Paul et moi pourrait facilement être convaincu de vous rejoindre. ✗\nS4d2 Source You and he could be surprised by her findings.\nRef Vous et lui pourriez être surpris par ses découvertes. PBMT-1 Vous et qu’il pouvait être surpris par ses conclusions. ✗ NMT Vous et lui pourriez être surpris par ses conclusions. ✓ Google Vous et lui pourrait être surpris par ses découvertes. ✗\nS4d3 Source We and they are on different courses.\nRef Nous et eux sommes sur des trajectoires différentes. PBMT-1 Nous et ils sont en cours de différents. ✗ NMT Nous et nous sommes sur des parcours différents. ✗ Google Nous et ils sont sur des parcours différents. ✗"
    }, {
      "heading" : "S-V agreement, past participles",
      "text" : "Are the agreement marks of the flagged participles the correct ones? (Past participle placed after auxiliary AVOIR agrees with verb object iff object precedes auxiliary. Otherwise participle is in masculine singular form).\nS5a Source The woman who saw a mouse in the corridor is charming.\nRef La femme qui a vu une souris dans le couloir est charmante. PBMT-1 La femme qui a vu une souris dans le couloir est charmante. ✓ NMT La femme qui a vu une souris dans le couloir est charmante. ✓ Google La femme qui a vu une souris dans le couloir est charmante. ✓\nS5b Source The woman that your brother saw in the corridor is charming.\nRef La femme que votre frère a vue dans le couloir est charmante. PBMT-1 La femme que ton frère a vu dans le couloir est charmante. ✗ NMT La femme que votre frère a vu dans le corridor est charmante. ✗ Google La femme que votre frère a vue dans le couloir est charmante. ✓\nS5c Source The house that John has visited is crumbling.\nRef La maison que John a visitée tombe en ruines. PBMT-1 La maison que John a visité est en train de s’écrouler. ✗ NMT La maison que John a visitée est en train de s’ effondrer. ✓ Google La maison que John a visité est en ruine. ✗\nS5d Source John sold the car that he had won in a lottery.\nRef John a vendu la voiture qu’ il avait gagnée dans une loterie. PBMT-1 John a vendu la voiture qu’il avait gagné à la loterie. ✗ NMT John a vendu la voiture qu’ il avait gagnée dans une loterie. ✓ Google John a vendu la voiture qu’ il avait gagnée dans une loterie. ✓"
    }, {
      "heading" : "Subjunctive mood",
      "text" : "Is the flagged verb in the correct mood? (Certain triggering verbs, adjectives or subordinate conjunctions, induce the subjunctive mood in the subordinate clause that they govern).\nS6a Source He will come provided that you come too.\nRef Il viendra à condition que vous veniez aussi. PBMT-1 Il viendra à condition que vous venez aussi. ✗ NMT Il viendra lui aussi que vous le faites. ✗ Google Il viendra à condition que vous venez aussi. ✗\nS6b Source It is unfortunate that he is not coming either.\nRef Il est malheureux qu’il ne vienne pas non plus. PBMT-1 Il est regrettable qu’il n’est pas non plus à venir. ✗ NMT Il est regrettable qu’ il ne soit pas non plus. ✗ Google Il est malheureux qu’il ne vienne pas non plus. ✓\nS6c Source I requested that families not be separated.\nRef J’ai demandé que les familles ne soient pas séparées. PBMT-1 J’ai demandé que les familles ne soient pas séparées. ✓ NMT J’ai demandé que les familles ne soient pas séparées. ✓ Google J’ai demandé que les familles ne soient pas séparées. ✓"
    }, {
      "heading" : "Lexico-Syntactic",
      "text" : ""
    }, {
      "heading" : "Argument switch",
      "text" : "Are the experiencer and the object of the “missing” situation correctly preserved in the French translation? (Argument switch). S7a Source Mary sorely misses Jim.\nRef Jim manque cruellement à Mary. PBMT-1 Marie manque cruellement de Jim. ✗ NMT Mary a lamentablement manqué de Jim . ✗ Google Mary manque cruellement à Jim. ✗\nS7b Source My sister is really missing New York.\nRef New York manque beaucoup à ma sœur. PBMT-1 Ma sœur est vraiment absent de New York. ✗ NMT Ma sœur est vraiment manquante à New York. ✗ Google Ma sœur manque vraiment New York. ✗\nS7c Source What he misses most is his dog.\nRef Ce qui lui manque le plus, c’est son chien. PBMT-1 Ce qu’il manque le plus, c’est son chien. ✗ NMT Ce qu’il manque le plus, c’est son chien. ✗ Google Ce qu’il manque le plus, c’est son chien. ✗"
    }, {
      "heading" : "Double-object verbs",
      "text" : "Are “gift” and “recipient” arguments correctly rendered in French? (English double-object constructions) S8a Source John gave his wonderful wife a nice present.\nRef John a donné un beau présent à sa merveilleuse épouse. PBMT-1 John a donné sa merveilleuse femme un beau cadeau. ✗ NMT John a donné à sa merveilleuse femme un beau cadeau. ✓ Google John a donné à son épouse merveilleuse un présent gentil. ✓\nS8b Source John told the kids a nice story.\nRef John a raconté une belle histoire aux enfants. PBMT-1 John a dit aux enfants une belle histoire. ✓ NMT John a dit aux enfants une belle histoire. ✓ Google John a raconté aux enfants une belle histoire. ✓\nS8c Source John sent his mother a nice postcard.\nRef John a envoyé une belle carte postale à sa mère. PBMT-1 John a envoyé sa mère une carte postale de nice. ✗ NMT John a envoyé sa mère une carte postale de nice. ✗ Google John envoya à sa mère une belle carte postale. ✓"
    }, {
      "heading" : "Fail to",
      "text" : "Is the meaning of “fail to” correctly rendered in the French translation?\nS9a Source John failed to see the relevance of this point.\nRef John n’a pas vu la pertinence de ce point. PBMT-1 John a omis de voir la pertinence de ce point. ✗ NMT John n’a pas vu la pertinence de ce point. ✓ Google John a omis de voir la pertinence de ce point. ✗\nS9b Source He failed to respond.\nRef Il n’a pas répondu. PBMT-1 Il n’a pas réussi à répondre. ✓ NMT Il n’a pas répondu. ✓ Google Il n’a pas répondu. ✓\nS9c Source Those who fail to comply with this requirement will be penalized.\nRef Ceux qui ne se conforment pas à cette exigence seront pénalisés. PBMT-1 Ceux qui ne se conforment pas à cette obligation seront pénalisés. ✓ NMT Ceux qui ne se conforment pas à cette obligation seront pénalisés. ✓ Google Ceux qui ne respectent pas cette exigence seront pénalisés. ✓"
    }, {
      "heading" : "Manner-of-movement verbs",
      "text" : "Is the movement action expressed in the English source correctly rendered in French? (Mannerof-movement verbs with path argument may need to be rephrased in French). S10a Source John would like to swim across the river.\nRef John aimerait traverser la rivière à la nage. PBMT-1 John aimerait nager dans la rivière. ✗ NMT John aimerait nager à travers la rivière. ✗ Google John aimerait nager à travers la rivière. ✗\nS10b Source They ran into the room.\nRef Ils sont entrés dans la chambre à la course. PBMT-1 Ils ont couru dans la chambre. ✗ NMT Ils ont couru dans la pièce. ✗ Google Ils coururent dans la pièce. ✗\nS10c Source The man ran out of the park.\nRef L’homme est sorti du parc en courant. PBMT-1 L’homme a manqué du parc. ✗ NMT L’ homme s’ enfuit du parc. ✗ Google L’homme sortit du parc. ✗\nHard example featuring spontaneous noun-to-verb derivation (“nonce verb”). S10d Source John guitared his way to San Francisco.\nRef John s’est rendu jusqu’à San Francisco en jouant de la guitare. PBMT-1 John guitared son chemin à San Francisco. ✗ NMT John guitared sa route à San Francisco. ✗ Google John a guité son chemin à San Francisco. ✗"
    }, {
      "heading" : "Overlapping subcat frames",
      "text" : "Is the French verb for “know” correctly chosen? (Choice between “savoir”/“connaı̂tre” depends on syntactic nature of its object) S11a Source Paul knows that this is a fact.\nRef Paul sait que c’est un fait. PBMT-1 Paul sait que c’est un fait. ✓ NMT Paul sait que c’est un fait. ✓ Google Paul sait que c’est un fait. ✓\nS11b Source Paul knows this story.\nRef Paul connaı̂t cette histoire. PBMT-1 Paul connaı̂t cette histoire. ✓ NMT Paul connaı̂t cette histoire. ✓ Google Paul connaı̂t cette histoire. ✓\nS11c Source Paul knows this story is hard to believe.\nRef Paul sait que cette histoire est difficile à croire. PBMT-1 Paul connaı̂t cette histoire est difficile à croire. ✗ NMT Paul sait que cette histoire est difficile à croire. ✓ Google Paul sait que cette histoire est difficile à croire. ✓\nS11d Source He knows my sister will not take it.\nRef Il sait que ma soeur ne le prendra pas. PBMT-1 Il sait que ma soeur ne prendra pas. ✓ NMT Il sait que ma soeur ne le prendra pas. ✓ Google Il sait que ma soeur ne le prendra pas. ✓\nS11e Source My sister knows your son is reliable.\nRef Ma sœur sait que votre fils est fiable. PBMT-1 Ma soeur connaı̂t votre fils est fiable. ✗ NMT Ma sœur sait que votre fils est fiable. ✓ Google Ma sœur sait que votre fils est fiable. ✓\nNP to VP Is the English “NP to VP” complement correctly rendred in the French translation? (Sometimes one needs to translate this structure as a finite clause). S12a Source John believes Bill to be dishonest.\nRef John croit que Bill est malhonnête. PBMT-1 John estime que le projet de loi soit malhonnête. ✓ NMT John croit que le projet de loi est malhonnête. ✓ Google John croit que Bill est malhonnête. ✓\nS12b Source He liked his father to tell him stories.\nRef Il aimait que son père lui raconte des histoires. PBMT-1 Il aimait son père pour lui raconter des histoires. ✗ NMT Il aimait son père pour lui raconter des histoires. ✗ Google Il aimait son père à lui raconter des histoires. ✗\nS12c Source She wanted her mother to let her go.\nRef Elle voulait que sa mère la laisse partir. PBMT-1 Elle voulait que sa mère de lui laisser aller. ✗ NMT Elle voulait que sa mère la laisse faire. ✓ Google Elle voulait que sa mère la laisse partir. ✓"
    }, {
      "heading" : "Factitives",
      "text" : "Is the English verb correctly rendered in the French translation? (Agentive use of some French verbs require embedding under “faire”). S13a Source John cooked a big chicken.\nRef John a fait cuire un gros poulet. PBMT-1 John cuit un gros poulet. ✗ NMT John cuit un gros poulet. ✗ Google John a fait cuire un gros poulet. ✓\nS13b Source John melted a lot of ice.\nRef John a fait fondre beaucoup de glace. PBMT-1 John fondu a lot of ice. ✗ NMT John a fondu beaucoup de glace. ✗ Google John a fondu beaucoup de glace. ✗\nS13c Source She likes to grow flowers.\nRef Elle aime faire pousser des fleurs. PBMT-1 Elle aime à se développer des fleurs. ✗ NMT Elle aime à cultiver des fleurs. ✓ Google Elle aime faire pousser des fleurs. ✓"
    }, {
      "heading" : "Noun Compounds",
      "text" : "Is the English nominal compound rendered with the right preposition in the French translation? S14a Source Use the meat knife.\nRef Utilisez le couteau à viande. PBMT-1 Utilisez le couteau de viande. ✗ NMT Utilisez le couteau à viande. ✓ Google Utilisez le couteau à viande. ✓\nS14b Source Use the butter knife.\nRef Utilisez le couteau à beurre. PBMT-1 Utilisez le couteau à beurre. ✓ NMT Utilisez le couteau au beurre. ✗ Google Utilisez le couteau à beurre. ✓\nS14c Source Use the steak knife.\nRef Utilisez le couteau à steak. PBMT-1 Utilisez le steak couteau. ✗ NMT Utilisez le couteau à steak. ✓ Google Utilisez le couteau de steak. ✗\nS14d Source Clean the water filter.\nRef Nettoyez le filtre à eau. PBMT-1 Nettoyez le filtre à eau. ✓ NMT Nettoyez le filtre à eau. ✓ Google Nettoyez le filtre à eau. ✓\nS14e Source Clean the juice filter.\nRef Nettoyez le filtre à jus. PBMT-1 Nettoyez le filtre de jus. ✗ NMT Nettoyez le filtre de jus. ✗ Google Nettoyez le filtre à jus. ✓\nS14f Source Clean the tea filter.\nRef Nettoyez le filtre à thé. PBMT-1 Nettoyez le filtre à thé. ✓ NMT Nettoyez le filtre de thé. ✗ Google Nettoyez le filtre à thé. ✓\nS14g Source Clean the cloth filter.\nRef Nettoyez le filtre en tissu. PBMT-1 Nettoyez le filtre en tissu. ✓ NMT Nettoyez le filtre en tissu. ✓ Google Nettoyez le filtre en tissu. ✓\nS14h Source Clean the metal filter.\nRef Nettoyez le filtre en métal. PBMT-1 Nettoyez le filtre en métal. ✓ NMT Nettoyez le filtre en métal. ✓ Google Nettoyez le filtre métallique. ✓\nS14i Source Clean the paper filter.\nRef Nettoyez le filtre en papier. PBMT-1 Nettoyez le filtre en papier. ✓ NMT Nettoyez le filtre en papier. ✓ Google Nettoyez le filtre à papier. ✗"
    }, {
      "heading" : "Common idioms",
      "text" : "Is the English idiomatic expression correctly rendered with a suitable French idiomatic expression? S15a Source Stop beating around the bush.\nRef Cessez de tourner autour du pot. PBMT-1 Cesser de battre la campagne. ✗ NMT Arrêtez de battre autour de la brousse. ✗ Google Arrêter de tourner autour du pot. ✓\nS15b Source You are putting the cart before the horse.\nRef Vous mettez la charrue devant les bœufs. PBMT-1 Vous pouvez mettre la charrue avant les bœufs. ✓ NMT Vous mettez la charrue avant le cheval. ✗ Google Vous mettez le chariot devant le cheval. ✗\nS15c Source His comment proved to be the straw that broke the camel’s back.\nRef Son commentaire s’est avéré être la goutte d’eau qui a fait déborder le vase. PBMT-1 Son commentaire s’est révélé être la goutte d’eau qui fait déborder le vase. ✓ NMT Son commentaire s’est avéré être la paille qui a brisé le dos du chameau. ✗ Google Son commentaire s’est avéré être la paille qui a cassé le dos du chameau. ✗\nS15d Source His argument really hit the nail on the head.\nRef Son argument a vraiment fait mouche. PBMT-1 Son argument a vraiment mis le doigt dessus. ✓ NMT Son argument a vraiment frappé le clou sur la tête. ✗ Google Son argument a vraiment frappé le clou sur la tête. ✗\nS15e Source It’s no use crying over spilt milk.\nRef Ce qui est fait est fait. PBMT-1 Ce n’est pas de pleurer sur le lait répandu. ✗ NMT Il ne sert à rien de pleurer sur le lait haché. ✗ Google Ce qui est fait est fait. ✓\nS15f Source It is no use crying over spilt milk.\nRef Ce qui est fait est fait. PBMT-1 Il ne suffit pas de pleurer sur le lait répandu. ✗ NMT Il ne sert à rien de pleurer sur le lait écrémé. ✗ Google Il est inutile de pleurer sur le lait répandu. ✗"
    }, {
      "heading" : "Syntactically flexible idioms",
      "text" : "Is the English idiomatic expression correctly rendered with a suitable French idiomatic expression? S16a Source The cart has been put before the horse.\nRef La charrue a été mise devant les bœufs. PBMT-1 On met la charrue devant le cheval. ✗ NMT Le chariot a été mis avant le cheval. ✗ Google Le chariot a été mis devant le cheval. ✗\nS16b Source With this argument, the nail has been hit on the head.\nRef Avec cet argument, la cause est entendue. PBMT-1 Avec cette argument, l’ongle a été frappée à la tête. ✗ NMT Avec cet argument , l’ongle a été touché à la tête. ✗ Google Avec cet argument, le clou a été frappé sur la tête. ✗"
    }, {
      "heading" : "Syntactic",
      "text" : ""
    }, {
      "heading" : "Yes-no question syntax",
      "text" : "Is the English question correctly rendered as a French question? S17a Source Have the kids ever watched that movie?\nRef Les enfants ont-ils déjà vu ce film? PBMT-1 Les enfants jamais regardé ce film? ✗ NMT Les enfants ont-ils déjà regardé ce film? ✓ Google Les enfants ont-ils déjà regardé ce film? ✓\nS17b Source Hasn’t your boss denied you a promotion?\nRef Votre patron ne vous a-t-il pas refusé une promotion? PBMT-1 N’a pas nié votre patron vous un promotion? ✗ NMT Est-ce que votre patron vous a refusé une promotion ? ✓ Google Votre patron ne vous a-t-il pas refusé une promotion? ✓\nS17c Source Shouldn’t I attend this meeting?\nRef Ne devrais-je pas assister à cette réunion? PBMT-1 Ne devrais-je pas assister à cette réunion? ✓ NMT Est-ce que je ne devrais pas assister à cette réunion? ✓ Google Ne devrais-je pas assister à cette réunion? ✓"
    }, {
      "heading" : "Tag questions",
      "text" : "Is the English “tag question” element correctly rendered in the translation? S18a Source Mary looked really happy tonight, didn’t she?\nRef Mary avait l’air vraiment heureuse ce soir, n’est-ce pas? PBMT-1 Marie a regardé vraiment heureux de ce soir, n’est-ce pas elle ? ✗ NMT Mary s’est montrée vraiment heureuse ce soir, ne l’ a pas fait ? ✗ Google Mary avait l’air vraiment heureuse ce soir, n’est-ce pas? ✓\nS18b Source We should not do that again, should we?\nRef Nous ne devrions pas refaire cela, n’est-ce pas? PBMT-1 Nous ne devrions pas faire qu’une fois encore , faut-il? ✗ NMT Nous ne devrions pas le faire encore , si nous ? ✗ Google Nous ne devrions pas recommencer, n’est-ce pas? ✓\nS18c Source She was perfect tonight, was she not?\nRef Elle était parfaite ce soir, n’est-ce pas? PBMT-1 Elle était parfait ce soir, elle n’était pas? ✗ NMT Elle était parfaite ce soir , n’était-elle pas? ✗ Google Elle était parfaite ce soir, n’est-ce pas? ✓"
    }, {
      "heading" : "WH-MVT and stranded preps",
      "text" : "Is the dangling preposition of the English sentence correctly placed in the French translation? S19a Source The guy that she is going out with is handsome.\nRef Le type avec qui elle sort est beau. PBMT-1 Le mec qu’elle va sortir avec est beau. ✗ NMT Le mec qu’ elle sort avec est beau. ✗ Google Le mec avec qui elle sort est beau. ✓\nS19b Source Whom is she going out with these days?\nRef Avec qui sort-elle ces jours-ci? PBMT-1 Qu’est-ce qu’elle allait sortir avec ces jours ? ✗ NMT À qui s’ adresse ces jours-ci? ✗ Google Avec qui sort-elle de nos jours? ✓\nS19c Source The girl that he has been talking about is smart.\nRef La fille dont il a parlé est brillante. PBMT-1 La jeune fille qu’il a parlé est intelligent. ✗ NMT La fille qu’ il a parlé est intelligente. ✗ Google La fille dont il a parlé est intelligente. ✓\nS19d Source Who was he talking to when you left?\nRef À qui parlait-il au moment où tu est parti? PBMT-1 Qui est lui parler quand vous avez quitté? ✗ NMT Qui a-t-il parlé à quand vous avez quitté ? ✗ Google Avec qui il parlait quand vous êtes parti? ✓\nS19e Source The city that he is arriving from is dangerous.\nRef La ville d’où il arrive est dangereuse. PBMT-1 La ville qu’il est arrivé de est dangereuse. ✗ NMT La ville qu’ il est en train d’ arriver est dangereuse. ✗ Google La ville d’où il vient est dangereuse. ✓\nS19f Source Where is he arriving from?\nRef D’où arrive-t-il? PBMT-1 Où est-il arrivé? ✗ NMT De quoi s’ agit-il ? ✗ Google D’où vient-il? ✓"
    }, {
      "heading" : "Adverb-triggered inversion",
      "text" : "Is the adverb-triggered subject-verb inversion in the English sentence correctly rendered in the French translation? S20a Source Rarely did the dog run.\nRef Rarement le chien courait-il. PBMT-1 Rarement le chien courir. ✗ NMT Il est rare que le chien marche. ✗ Google Rarement le chien courir. ✗\nS20b Source Never before had she been so unhappy.\nRef Jamais encore n’avait-elle été aussi malheureuse. PBMT-1 Jamais auparavant , si elle avait été si malheureux. ✗ NMT Jamais auparavant n’ avait été si malheureuse. ✗ Google Jamais elle n’avait été aussi malheureuse. ✓\nS20c Source Nowhere were the birds so colorful.\nRef Nulle part les oiseaux n’étaient si colorés. PBMT-1 Nulle part les oiseaux de façon colorée . ✗ NMT Les oiseaux ne sont pas si colorés . ✗ Google Nulle part les oiseaux étaient si colorés. ✗"
    }, {
      "heading" : "Middle voice",
      "text" : "Is the generic statement made in the English sentence correctly and naturally rendered in the French translation? S21a Source Soup is eaten with a large spoon.\nRef La soupe se mange avec une grande cuillère PBMT-1 La soupe est mangé avec une grande cuillère. ✗ NMT La soupe est consommée avec une grosse cuillère. ✗ Google La soupe est consommée avec une grande cuillère. ✗\nS21b Source Masonry is cut using a diamond blade.\nRef La maçonnerie se coupe avec une lame à diamant. PBMT-1 La maçonnerie est coupé à l’aide d’une lame de diamant. ✗ NMT La maçonnerie est coupée à l’aide d’une lame de diamant. ✗ Google La maçonnerie est coupée à l’aide d’une lame de diamant. ✗\nS21c Source Champagne is drunk in a glass called a flûte.\nRef Le champagne se boit dans un verre appelé flûte. PBMT-1 Le champagne est ivre dans un verre appelé une flûte. ✗ NMT Le champagne est ivre dans un verre appelé flûte. ✗ Google Le Champagne est bu dans un verre appelé flûte. ✗"
    }, {
      "heading" : "Fronted “should”",
      "text" : "Fronted “should” is interpreted as a conditional subordinator. It is normally translated as “si” with imperfect tense. S22a Source Should Paul leave, I would be sad.\nRef Si Paul devait s’en aller, je serais triste. PBMT-1 Si le congé de Paul, je serais triste. ✗ NMT Si Paul quitte , je serais triste. ✗ Google Si Paul s’en allait, je serais triste. ✓\nS22b Source Should he become president, she would be promoted immediately.\nRef S’il devait devenir président, elle recevrait immédiatement une promotion. PBMT-1 S’il devait devenir président , elle serait encouragée immédiatement . ✓ NMT S’ il devait devenir président , elle serait immédiatement promue . ✓ Google Devrait-il devenir président, elle serait immédiatement promue. ✗\nS22c Source Should he fall, he would get up again immediately.\nRef S’ il venait à tomber, il se relèverait immédiatement. PBMT-1 S’il devait tomber, il allait se lever immédiatement de nouveau. ✓ NMT s’ il tombe , il serait de nouveau immédiatement. ✗ Google S’il tombe, il se lèvera immédiatement. ✗"
    }, {
      "heading" : "Clitic pronouns",
      "text" : "Are the English pronouns correctly rendered in the Freench translations? S23a Source She had a lot of money but he did not have any.\nRef Elle avait beaucoup d’argent mais il n’en avait pas. PBMT-1 Elle avait beaucoup d’argent mais il n’en avait pas. ✓ NMT Elle avait beaucoup d’ argent , mais il n’ a pas eu d’ argent. ✓ Google Elle avait beaucoup d’argent mais il n’en avait pas. ✓\nS23b Source He did not talk to them very often.\nRef Il ne leur parlait pas très souvent. PBMT-1 Il n’a pas leur parler très souvent. ✗ NMT Il ne leur a pas parlé très souvent. ✓ Google Il ne leur parlait pas très souvent. ✓\nS23c Source The men are watching each other.\nRef Les hommes se surveillent l’un l’autre PBMT-1 Les hommes se regardent les uns les autres. ✓ NMT Les hommes se regardent les uns les autres. ✓ Google Les hommes se regardent. ✗\nS23d Source He gave it to the man.\nRef Il le donna à l’homme. PBMT-1 Il a donné à l’homme. ✗ NMT Il l’ a donné à l’ homme. ✓ Google Il le donna à l’homme. ✓\nS23e Source He did not give it to her.\nRef Il ne le lui a pas donné. PBMT-1 Il ne lui donner. ✗ NMT Il ne l’ a pas donné à elle. ✗ Google Il ne lui a pas donné. ✗"
    }, {
      "heading" : "Ordinal placement",
      "text" : "Is the relative order of the ordinals and numerals correct in the French tranlation? S24a Source The first four men were exhausted.\nRef Les quatre premiers hommes étaient tous épuisés. PBMT-1 Les quatre premiers hommes étaient épuisés. ✓ NMT Les quatre premiers hommes ont été épuisés. ✓ Google Les quatre premiers hommes étaient épuisés. ✓\nS24b Source The last three candidates were eliminated.\nRef Les trois derniers candidats ont été éliminés. PBMT-1 Les trois derniers candidats ont été éliminés. ✓ NMT Les trois derniers candidats ont été éliminés. ✓ Google Les trois derniers candidats ont été éliminés. ✓\nS24c Source The other two guys left without paying.\nRef Les deux autres types sont partis sans payer. PBMT-1 Les deux autres mecs ont laissé sans payer. ✓ NMT Les deux autres gars à gauche sans payer. ✓ Google Les deux autres gars sont partis sans payer. ✓"
    }, {
      "heading" : "Inalienable possession",
      "text" : "Is the French translation correct and natural both in: a) its use of a particular determiner on the body part noun; and b) the presence or absence of a reflexive pronoun before the verb? S25a Source He washed his hands.\nRef Ils s’est lavé les mains. PBMT-1 Ils se lavait les mains. ✓ NMT Il a lavé ses mains. ✗ Google Ils se lava les mains. ✓\nS25b Source I brushed my teeth.\nRef Jeme suis brossé les dents. PBMT-1 J’ai brossé mes dents. ✗ NMT J’ ai brossé mes dents. ✗ Google Je me suis brossé les dents. ✓\nS25c Source You brushed your teeth.\nRef Tu t’es brossé les dents PBMT-1 Vous avez brossé vos dents. ✗ NMT vous avez brossé vos dents. ✗ Google Tu as brossé les dents. ✗\nS25d Source I raised my hand.\nRef J’ai levé la main. PBMT-1 J’ai levé la main. ✓ NMT J’ai soulevé mamain. ✗ Google Je levai la main. ✓\nS25e Source He turned his head.\nRef Il a tourné la tête. PBMT-1 Il a transformé sa tête. ✗ NMT Il a tourné sa tête. ✗ Google Il tourna la tête. ✓\nS25f Source He raised his eyes to heaven.\nRef Il leva les yeux au ciel. PBMT-1 Il a évoqué les yeux au ciel. ✓ NMT Il a levé les yeux sur le ciel. ✓ Google Il leva les yeux au ciel. ✓"
    }, {
      "heading" : "Zero REL PRO",
      "text" : "Is the English zero relative pronoun correctly translated as a non-zero one in the French translation? S26a Source The strangers the woman saw were working.\nRef Les inconnus que la femme vit travaillaient. PBMT-1 Les étrangers la femme vit travaillaient. ✗ NMT Les inconnus de la femme ont travaillé. ✗ Google Les étrangers que la femme vit travaillaient. ✓\nS26b Source The man your sister hates is evil.\nRef L’homme que votre sœur déteste est méchant. PBMT-1 L’homme ta soeur hait est le mal. ✗ NMT L’ homme que ta soeur est le mal est le mal. ✓ Google L’homme que votre sœur hait est méchant. ✓\nS26c Source The girl my friend was talking about is gone.\nRef La fille dont mon ami parlait est partie. PBMT-1 La jeune fille mon ami a parlé a disparu. ✗ NMT La petite fille de mon ami était révolue. ✗ Google La fille dont mon ami parlait est partie. ✓"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Third International Conference on Learning Representations (ICLR). San Diego, USA.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural versus phrase-based machine translation quality: a case study",
      "author" : [ "Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Bentivogli et al\\.,? 2016",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2016
    }, {
      "title" : "Findings of the 2016 conference on machine translation",
      "author" : [ "Popel", "Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri." ],
      "venue" : "Proceedings of the First Conference on Ma-",
      "citeRegEx" : "Popel et al\\.,? 2016",
      "shortCiteRegEx" : "Popel et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved reordering for phrase-based translation using sparse features",
      "author" : [ "Colin Cherry." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Cherry.,? 2013",
      "shortCiteRegEx" : "Cherry.",
      "year" : 2013
    }, {
      "title" : "Batch tuning strategies for statistical machine translation",
      "author" : [ "Colin Cherry", "George Foster." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Cherry and Foster.,? 2012",
      "shortCiteRegEx" : "Cherry and Foster.",
      "year" : 2012
    }, {
      "title" : "Fast and robust neural network joint models for statistical machine translation",
      "author" : [ "Jacob Devlin", "Rabih Zbib", "ZhongqiangHuang", "Thomas Lamar", "Richard Schwartz", "John Makhoul." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Asso-",
      "citeRegEx" : "Devlin et al\\.,? 2014",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2014
    }, {
      "title" : "Machine translation divergences: a formal description and proposed solution",
      "author" : [ "Bonnie J. Dorr." ],
      "venue" : "Computational Linguistics 20:4.",
      "citeRegEx" : "Dorr.,? 1994",
      "shortCiteRegEx" : "Dorr.",
      "year" : 1994
    }, {
      "title" : "QCRI machine translation systems for IWSLT 16",
      "author" : [ "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "Stephan Vogel." ],
      "venue" : "Proceedings of the 13th InternationalWorkshop on Spoken Language Translation (IWSLT). Seattle, Washington.",
      "citeRegEx" : "Durrani et al\\.,? 2016",
      "shortCiteRegEx" : "Durrani et al\\.",
      "year" : 2016
    }, {
      "title" : "A simple and effective hierarchical phrase reordering model",
      "author" : [ "Michel Galley", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
      "citeRegEx" : "Galley and Manning.,? 2008",
      "shortCiteRegEx" : "Galley and Manning.",
      "year" : 2008
    }, {
      "title" : "Forest rescoring: Faster decoding with integrated language models",
      "author" : [ "Liang Huang", "David Chiang." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Lin-",
      "citeRegEx" : "Huang and Chiang.,? 2007",
      "shortCiteRegEx" : "Huang and Chiang.",
      "year" : 2007
    }, {
      "title" : "Is neural machine translation ready for deployment? a case study on 30 translation directions",
      "author" : [ "Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang." ],
      "venue" : "Proceedings of the 13th International Workshop on Spoken Language Translation",
      "citeRegEx" : "Junczys.Dowmunt et al\\.,? 2016a",
      "shortCiteRegEx" : "Junczys.Dowmunt et al\\.",
      "year" : 2016
    }, {
      "title" : "The amu-uedin submission to the wmt16 news translation task: Attention-based nmt models as feature functions in phrase-based smt",
      "author" : [ "Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich." ],
      "venue" : "Proceedings of the First Conference",
      "citeRegEx" : "Junczys.Dowmunt et al\\.,? 2016b",
      "shortCiteRegEx" : "Junczys.Dowmunt et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs",
      "author" : [ "Rico Sennrich." ],
      "venue" : "CoRR abs/1612.04629. http://arxiv.org/abs/1612.04629.",
      "citeRegEx" : "Sennrich.,? 2016",
      "shortCiteRegEx" : "Sennrich.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27. Curran Associates, Inc., pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A multifaceted evaluation of neural versus statistical machine translation for 9 language directions",
      "author" : [ "Antonio Toral", "Vı́ctorM. Sánchez-Cartagena" ],
      "venue" : "In Proceedings of the The 15th Conference of the European Chapter of the Association",
      "citeRegEx" : "Toral and Sánchez.Cartagena.,? \\Q2017\\E",
      "shortCiteRegEx" : "Toral and Sánchez.Cartagena.",
      "year" : 2017
    }, {
      "title" : "Stylistique comparée du français et de l’anglais, volume 1",
      "author" : [ "Jean-Paul Vinay", "Jean Darbelnet." ],
      "venue" : "Didier, Paris.",
      "citeRegEx" : "Vinay and Darbelnet.,? 1958",
      "shortCiteRegEx" : "Vinay and Darbelnet.",
      "year" : 1958
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality.",
      "startOffset" : 60,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality.",
      "startOffset" : 60,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "This puts pressure on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which exploit surface-matching heuristics that are relatively insensitive to subtle differences.",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Since then, controlled comparisons have used BLEU to show that NMT outperforms strong PBMT systems on 30 translation directions from the United Nations Parallel Corpus (Junczys-Dowmunt et al., 2016a), and on the IWSLT English-Arabic tasks (Durrani et al.",
      "startOffset" : 168,
      "endOffset" : 199
    }, {
      "referenceID" : 7,
      "context" : ", 2016a), and on the IWSLT English-Arabic tasks (Durrani et al., 2016).",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories. Compared to PBMT, NMT required less postediting effort over-all, with substantial improvements in lexical, morphological and word order errors. NMT consistently out-performed PBMT, but its performance degraded faster as sentence length increased. Later, Toral and Sánchez-Cartagena (2017) conducted a similar study, examining the outputs of competition-grade systems for the 9 WMT 2016 directions that included NMT competitors.",
      "startOffset" : 0,
      "endOffset" : 520
    }, {
      "referenceID" : 14,
      "context" : "Most recently, Sennrich (2016) proposed an approach to perform targeted evaluations of NMT through the use of contrastive translation pairs.",
      "startOffset" : 15,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "Manual evaluation side-steps some of the pitfalls that can come with Sennrich (2016)’s contrastive pairs, as a ranking of two contrastive sentences may not necessarily reflect whether the error in question will occur in the system’s actual output.",
      "startOffset" : 69,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "Translational divergences have been extensively studied in the past – see for example (Vinay and Darbelnet, 1958; Dorr, 1994).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "Translational divergences have been extensively studied in the past – see for example (Vinay and Darbelnet, 1958; Dorr, 1994).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "We trained an NNJM model (Devlin et al., 2014) on the HMM-aligned training corpus, with input and output vocabulary sizes of 64K and 32K.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012).",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "For each extracted phrase pair, we collected statistics for the hierarchical reordering model of Galley and Manning (2008). We trained an NNJM model (Devlin et al.",
      "startOffset" : 97,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2).",
      "startOffset" : 260,
      "endOffset" : 274
    }, {
      "referenceID" : 3,
      "context" : "Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7.",
      "startOffset" : 260,
      "endOffset" : 583
    }, {
      "referenceID" : 0,
      "context" : "To build our NMT system, we used the Nematus toolkit, which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent",
      "startOffset" : 142,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "We preprocessed the data using a BPE model learned from source and target corpora (Sennrich et al., 2016).",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "Training used the Adadelta algorithm (Zeiler, 2012), with a minibatch size of 100 and gradients clipped to 1.",
      "startOffset" : 37,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "To decode, we used the AmuNMT decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4.",
      "startOffset" : 38,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "Following Junczys-Dowmunt et al. (2016b), we averaged the parameters from the last 8 checkpoints.",
      "startOffset" : 10,
      "endOffset" : 41
    } ],
    "year" : 2017,
    "abstractText" : "Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system’s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.",
    "creator" : "LaTeX with hyperref package"
  }
}