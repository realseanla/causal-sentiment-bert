{
  "name" : "1706.05087.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Plan, Attend, Generate: Character-Level Neural Machine Translation with Planning",
    "authors" : [ "Caglar Gulcehre", "Francis Dutil", "Adam Trischler", "Yoshua Bengio" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Character-level neural machine translation (NMT) is an attractive research problem (Lee et al., 2016; Chung et al., 2016; Luong and Manning, 2016) because it addresses important issues encountered in word-level NMT. Word-level NMT systems can suffer from problems with rare words(Gulcehre et al., 2016) or data sparsity, and the existence of compound words without explicit segmentation in certain language pairs can make learning alignments and translations more difficult. Character-level neural machine translation mitigates these issues.\nIn this work we propose to augment the encoderdecoder model for character-level NMT by integrating a planning mechanism. Specifically, we develop a model that uses planning to improve the alignment between input and output sequences. Our model’s encoder is a recurrent neural network (RNN) that\n∗Equal Contribution\nreads the source (a sequence of byte pairs representing text in some language) and encodes it as a sequence of vector representations; the decoder is a second RNN that generates the target translation characterby-character in the target language. The decoder uses an attention mechanism to align its internal state to vectors in the source encoding. It creates an explicit plan of source-target alignments to use at future time-steps based on its current observation and a summary of its past actions. At each time-step it may follow or modify this plan. This enables the model to plan ahead rather than attending to what is relevant primarily at the current generation step. More concretely, we augment the decoder’s internal state with (i) an alignment plan matrix and (ii) a commitment plan vector. The alignment plan matrix is a template of alignments that the model intends to follow at future time-steps, i.e., a sequence of probability distributions over input tokens. The commitment plan vector governs whether to follow the alignment plan at the current step or to recompute it, and thus models discrete decisions. This planning mechanism is inspired by the strategic attentive reader and writer (STRAW) of Vezhnevets et al. (2016).\nOur work is motivated by the intuition that, although natural language is output step-by-step because of constraints on the output process, it is not necessarily conceived and ordered according to only local, step-by-step interactions. Sentences are not conceived one word at a time. Planning, that is, choosing some goal along with candidate macro-actions to arrive at it, is one way to induce coherence in sequential outputs like language. Learning to generate long coherent sequences, or how to form alignments over long input contexts, is difficult for existing models. NMT performance of encoder-decoder models with attention deteriorates as sequence length increases (Cho et al., 2014; Sutskever et al., 2014), and this effect can be more pronounced at the character-level NMT. This is because character sequences are longer than word sequences. ar X iv :1\n70 6.\n05 08\n7v 2\n[ cs\n.C L\n] 2\n3 Ju\nn 20\n17\nA planning mechanism can make the decoder’s search for alignments more tractable and more scalable.\nWe evaluate our proposed model and report results on character-level translation tasks from WMT’15 for English to German, English to Finnish, and English to Czech language pairs. On almost all pairs we observe improvements over a baseline that represents the state of the art in neural character-level translation. In our NMT experiments, our model outperforms the baseline despite using significantly fewer parameters and converges faster in training."
    }, {
      "heading" : "2 Planning for Character-level Neural Machine Translation",
      "text" : "We now describe how to integrate a planning mechanism into a sequence-to-sequence architecture with attention (Bahdanau et al., 2015). Our model first creates a plan, then computes a soft alignment based on the plan, and generates at each time-step in the decoder. We refer to our model as PAG (Plan-Attend-Generate)."
    }, {
      "heading" : "2.1 Notation and Encoder",
      "text" : "As input our model receives a sequence of tokens, X = (x0,··· ,x|X|), where |X| denotes the length of X. It processes these with the encoder, a bidirectional RNN. At each input position i we obtain annotation vector hi by concatenating the forward and backward encoder states, hi=[h→i ;h ← i ], where h → i denotes the hidden state of the encoder’s forward RNN andh←i denotes the hidden state of the encoder’s backward RNN.\nThrough the decoder the model predicts a sequence of output tokens, Y =(y1,···,y|Y |). We denote by st the hidden state of the decoder RNN generating the target output token at time-step t."
    }, {
      "heading" : "2.2 Alignment and Decoder",
      "text" : "Our goal is a mechanism that plans which parts of the input sequence to focus on for the next k time-steps of decoding. For this purpose, our model computes an alignment plan matrix At∈Rk×|X| and commitment plan vector ct ∈ Rk at each time-step. Matrix At stores the alignments for the current and the next k−1 timesteps; it is conditioned on the current input, i.e. the token predicted at the previous time-step yt, and the current context ψt, which is computed from the input annotations hi. The recurrent decoder function, fdec-rnn(·), receives st−1, yt, ψt as inputs and computes the hidden state vector\nst=fdec-rnn(st−1,yt,ψt). (1)\nContext ψt is obtained by a weighted sum of the encoder annotations,\nψt= |X|∑ i αtihi. (2)\nThe alignment vector αt = softmax(At[0])∈R|X| is a function of the first row of the alignment matrix. At each time-step, we compute a candidate alignmentplan matrix Āt whose entry at the ith row is\nĀt[i]=falign(st−1, hj, β i t, yt), (3)\nwhere falign(·) is an MLP and βit denotes a summary of the alignment matrix’s ith row at time t−1. The summary is computed using an MLP, fr(·), operating row-wise on At−1: βit=fr(At−1[i]).\nThe commitment plan vector ct governs whether to follow the existing alignment plan, by shifting it forward from t−1, or to recompute it. Thus, ct represents a discrete decision. For the model to operate discretely, we use the recently proposed Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2016) in conjunction with the straight-through estimator (Bengio et al., 2013) to backpropagate through ct.1 The model further learns the temperature for the Gumbel-Softmax as proposed in Gulcehre et al. (2017). Both the commitment vector and the action plan matrix are initialized with ones; this initialization is not modified through training.\nAlignment-plan update Our decoder updates its alignment plan as governed by the commitment plan. Denoted by gt the first element of the discretized commitment plan c̄t. In more detail, gt = c̄t[0], where the discretized commitment plan is obtained by setting ct’s largest element to 1 and all other elements to 0. Thus, gt is a binary indicator variable; we refer to it as the commitment switch. When gt = 0, the decoder simply advances the time index by shifting the action plan matrix At−1 forward via the shift function ρ(·). When gt = 1, the controller reads the action-plan matrix to produce the summary of the plan, βit. We then compute the updated alignment plan by interpolating the previous alignment plan matrix At−1 with the candidate alignment plan matrix Āt. The mixing ratio is determined by a learned update gate ut∈Rk×|X|, whose elements uti correspond to tokens in the input sequence and are\n1We also experimented with training ct using REINFORCE (Williams, 1992) but found that Gumbel-Softmax led to better performance.\ncomputed by an MLP with sigmoid activation, fup(·):\nuti=fup(hi, st−1),\nAt[:,i]=(1−uti) At−1[:,i]+uti Āt[:,i].\nTo reiterate, the model only updates its alignment plan when the current commitment switch gt is active. Otherwise it uses the alignments planned and committed at previous time-steps.\nAlgorithm 1: Pseudocode for updating the alignment plan and commitment vector.\nfor j∈{1,···|X|} do for t∈{1,···|Y |} do\nif gt=1 then ct=softmax(fc(st−1)) βjt =fr(At−1[j]) {Read alignment plan} Āt[j]=falign(st−1, hj, β j t , yt)\n{Compute candidate alignment plan} utj =fup(hj, st−1, ψt−1) {Compute update gate} At = (1 − utj) At−1+utj Āt {Update alignment plan}\nelse At=ρ(At−1) {Shift alignment plan} ct=ρ(ct−1) {Shift commitment plan} end if Compute the alignment as αt=softmax(At[0])\nend for end for\nCommitment-plan update The commitment plan also updates when gt becomes 1. If gt is 0, the\nshift function ρ(·) shifts the commitment vector forward and appends a 0-element. If gt is 1, the model recomputes ct using a single layer MLP (fc(·)) followed by a Gumbel-Softmax, and c̄t is recomputed by discretizing ct as a one-hot vector:\nct=gumbel_softmax(fc(st−1)), (4)\nc̄t=one_hot(ct). (5)\nWe provide pseudocode for the algorithm to compute the commitment plan vector and the action plan matrix in Algorithm 1. An overview of the model is depicted in Figure 1."
    }, {
      "heading" : "2.2.1 Alignment Repeat",
      "text" : "In order to reduce the model’s computational cost, we also propose an alternative approach to computing the candidate alignment-plan matrix at every step. Specifically, we propose a model variant that reuses the alignment from the previous time-step until the commitment switch activates, at which time the model computes a new alignment. We call this variant repeat, plan, attend, and generate (rPAG). rPAG can be viewed as learning an explicit segmentation with an implicit planning mechanism in an unsupervised fashion. Repetition can reduce the computational complexity of the alignment mechanism drastically; it also eliminates the need for an explicit alignment-plan matrix, which reduces the model’s memory consumption as well. We provide pseudocode for rPAG in Algorithm 1.\nAlgorithm 2: Pseudocode for updating the repeat alignment and commitment vector.\nfor j∈{1,···|X|} do for t∈{1,···|Y |} do\nif gt=1 then ct=softmax(fc(st−1,ψt−1)) αt=softmax(falign(st−1, hj, yt)) else ct=ρ(ct−1) {Shift the commitment vector ct−1} αt=αt−1 {Reuse the old the alignment}\nend if end for\nend for"
    }, {
      "heading" : "2.3 Training",
      "text" : "We use a deep output layer (Pascanu et al., 2013) to compute the conditional distribution over output tokens,\np(yt|y<t,x)∝y>t exp(Wofo(st,yt−1,ψt)), (6)\nwhere Wo is a matrix of learned parameters and we have omitted the bias for brevity. Function fo is an MLP with tanh activation.\nThe full model, including both the encoder and decoder, is jointly trained to minimize the (conditional) negative log-likelihood\nL=− 1 N N∑ n=1 logpθ(y (n)|x(n)),\nwhere the training corpus is a set of (x(n),y(n)) pairs and θ denotes the set of all tunable parameters. As noted in (Vezhnevets et al., 2016), the proposed model can learn to recompute very often which decreases the utility of planning. In order to avoid this behavior, we introduce a loss that penalizes the model for committing too often,\nLcom =λcom |X|∑ t=1 k∑ i=0 ||1 k −cti||22, (7)\nwhere λcom is the commitment hyperparameter and k is the timescale over which plans operate."
    }, {
      "heading" : "3 Experiments",
      "text" : "In our NMT experiments we use byte pair encoding (BPE) (Sennrich et al., 2015) for the source sequence and character representation for the target, the same setup described in Chung et al. (2016). We also use the same preprocessing as in that work.2\nWe test our planning models against a baseline on the WMT’15 tasks for English to German (En→De), English to Czech (En→Cs), and English to Finnish (En→Fi) language pairs. We present the experimental results in Table 1.\n2Our implementation is based on the code available at https://github.com/nyu-dl/dl4mt-cdec\nAs a baseline we use the biscale GRU model of Chung et al. (2016), with the attention mechanisms in both the baseline and (r)PAG conditioned on both layers of the encoder’s biscale GRU (h1 and h2 – see (Chung et al., 2016) for more detail). Our implementation reproduces the results in the original paper to within a small margin.\nTable 1 shows that our planning mechanism generally improves translation performance over the baseline. It does this with fewer updates and fewer parameters. We trained (r)PAG for 350K updates on the training set, while the baseline was trained for 680K updates. We used 600 units in (r)PAG’s encoder and decoder, while the baseline used 512 in the encoder and 1024 units in the decoder. In total our model has about 4M fewer parameters than the baseline. We tested all models with a beam size of 15.\nAs can be seen from Table 1, layer normalization (Ba et al., 2016) improves the performance of the PAG model significantly. However, according to our results on En→De, layer norm affects the performance of rPAG only marginally. Thus, we decided not to train rPAG with layer norm on other language pairs.\nIn Figure 3, we show qualitatively that our model constructs smoother alignments. In contrast to (r)PAG, we see that the baseline decoder aligns the first few characters of each word that it generates to a byte in the source sequence; for the remaining characters it places the largest alignment weight on the final, empty token of the source sequence. This is because the baseline becomes confident of which word to generate after the first few characters, and generates the remainder of the word mainly by relying on language-model predictions. As illustrated by the learning curves in Figure 2, we observe further that (r)PAG converges faster with the help of its improved alignments."
    }, {
      "heading" : "4 Conclusions and Future Work",
      "text" : "In this work, we addressed a fundamental issue in neural generation of long sequences by integrating planning into the alignment mechanism of sequenceto-sequence architectures on machine translation problem. We proposed two different planning mechanisms: PAG, which constructs explicit plans in the form of stored matrices, and rPAG, which plans implicitly and is computationally cheaper. The (r)PAG approach empirically improves alignments over long input sequences. In machine translation experiments, models with a planning mechanism outperforms a state-of-the-art baseline on almost all language pairs using fewer parameters. As a future work, we plan\nto test our planning mechanism at the outputs of the model and other sequence-to-sequence tasks as well."
    }, {
      "heading" : "A Qualitative Translations from both Models",
      "text" : "In Table 2, we present example translations from our model and the baseline along with the ground-truth. 3\n1 Eine republikanische Strategie , um der Wiederwahl von Obama entgegenzutreten Eine republikanische Strategie gegen die Wiederwahl von Obama Eine republikanische Strategie zur Bekämpfung der Wahlen von Obama 2 Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit , den Wahlbetrug zu bekämpfen . Republikanische Führungspersönlichkeiten haben ihre Politik durch die Notwendigkeit gerechtfertigt , Wahlbetrug zu bekämpfen .\nDie politischen Führer der Republikaner haben ihre Politik durch die Notwendigkeit der Bekämpfung des Wahlbetrugs gerechtfertigt .\n3 Der Generalanwalt der USA hat eingegriffen , um die umstrittensten Gesetze auszusetzen . Die Generalstaatsanwälte der Vereinigten Staaten intervenieren , um die umstrittensten Gesetze auszusetzen . Der Generalstaatsanwalt der Vereinigten Staaten hat dazu gebracht , die umstrittensten Gesetze auszusetzen . 4 Sie konnten die Schäden teilweise begrenzen Sie konnten die Schaden teilweise begrenzen Sie konnten den Schaden teilweise begrenzen . 5 Darüber hinaus haben Sie das Recht von Einzelpersonen und Gruppen beschränkt , jenen Wählern Hilfestellung zu leisten , die sich registrieren möchten . Darüber hinaus begrenzten sie das Recht des Einzelnen und der Gruppen , den Wählern Unterstützung zu leisten , die sich registrieren möchten . Darüber hinaus unterstreicht Herr Beaulieu die Bedeutung der Diskussion Ihrer Bedenken und Ihrer Familiengeschichte mit Ihrem Arzt .\n3These examples are randomly chosen from the first 100 examples of the development set. None of the authors of this paper can speak or understand German."
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450 .",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations (ICLR) .",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "author" : [ "Yoshua Bengio", "Nicholas Léonard", "Aaron Courville." ],
      "venue" : "arXiv preprint arXiv:1308.3432 .",
      "citeRegEx" : "Bengio et al\\.,? 2013",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.1259 .",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "A character-level decoder without explicit segmentation for neural machine translation",
      "author" : [ "Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1603.06147 .",
      "citeRegEx" : "Chung et al\\.,? 2016",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Pointing the unknown words",
      "author" : [ "Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1603.08148 .",
      "citeRegEx" : "Gulcehre et al\\.,? 2016",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2016
    }, {
      "title" : "Memory augmented neural networks with wormhole connections",
      "author" : [ "Caglar Gulcehre", "Sarath Chandar", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1701.08718 .",
      "citeRegEx" : "Gulcehre et al\\.,? 2017",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2017
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole." ],
      "venue" : "arXiv preprint arXiv:1611.01144 .",
      "citeRegEx" : "Jang et al\\.,? 2016",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2016
    }, {
      "title" : "Fully character-level neural machine translation without explicit segmentation",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Thomas Hofmann." ],
      "venue" : "arXiv preprint arXiv:1610.03017 .",
      "citeRegEx" : "Lee et al\\.,? 2016",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models",
      "author" : [ "Minh-Thang Luong", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1604.00788 .",
      "citeRegEx" : "Luong and Manning.,? 2016",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2016
    }, {
      "title" : "The concrete distribution: A continuous relaxation of discrete random variables",
      "author" : [ "Chris J Maddison", "Andriy Mnih", "Yee Whye Teh." ],
      "venue" : "arXiv preprint arXiv:1611.00712 .",
      "citeRegEx" : "Maddison et al\\.,? 2016",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2016
    }, {
      "title" : "How to construct deep recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1312.6026 .",
      "citeRegEx" : "Pascanu et al\\.,? 2013",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1508.07909 .",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Strategic attentive writer for learning macro-actions",
      "author" : [ "Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 3486–3494.",
      "citeRegEx" : "Vezhnevets et al\\.,? 2016",
      "shortCiteRegEx" : "Vezhnevets et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams." ],
      "venue" : "Machine learning 8(3-4):229–256.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Character-level neural machine translation (NMT) is an attractive research problem (Lee et al., 2016; Chung et al., 2016; Luong and Manning, 2016) because it addresses important issues encountered in word-level NMT.",
      "startOffset" : 83,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "Character-level neural machine translation (NMT) is an attractive research problem (Lee et al., 2016; Chung et al., 2016; Luong and Manning, 2016) because it addresses important issues encountered in word-level NMT.",
      "startOffset" : 83,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "Character-level neural machine translation (NMT) is an attractive research problem (Lee et al., 2016; Chung et al., 2016; Luong and Manning, 2016) because it addresses important issues encountered in word-level NMT.",
      "startOffset" : 83,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "Word-level NMT systems can suffer from problems with rare words(Gulcehre et al., 2016) or data sparsity, and the existence of compound words without explicit segmentation in certain language pairs can make learning alignments and translations more difficult.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "This planning mechanism is inspired by the strategic attentive reader and writer (STRAW) of Vezhnevets et al. (2016).",
      "startOffset" : 92,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "NMT performance of encoder-decoder models with attention deteriorates as sequence length increases (Cho et al., 2014; Sutskever et al., 2014), and this effect can be more pronounced at the character-level NMT.",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "NMT performance of encoder-decoder models with attention deteriorates as sequence length increases (Cho et al., 2014; Sutskever et al., 2014), and this effect can be more pronounced at the character-level NMT.",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : "We now describe how to integrate a planning mechanism into a sequence-to-sequence architecture with attention (Bahdanau et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "For the model to operate discretely, we use the recently proposed Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2016) in conjunction with the straight-through estimator (Bengio et al.",
      "startOffset" : 87,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "For the model to operate discretely, we use the recently proposed Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2016) in conjunction with the straight-through estimator (Bengio et al.",
      "startOffset" : 87,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : ", 2016) in conjunction with the straight-through estimator (Bengio et al., 2013) to backpropagate through ct.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : ", 2016) in conjunction with the straight-through estimator (Bengio et al., 2013) to backpropagate through ct. The model further learns the temperature for the Gumbel-Softmax as proposed in Gulcehre et al. (2017). Both the commitment vector and the action plan matrix are initialized with ones; this initialization is not modified through training.",
      "startOffset" : 60,
      "endOffset" : 212
    }, {
      "referenceID" : 15,
      "context" : "We also experimented with training ct using REINFORCE (Williams, 1992) but found that Gumbel-Softmax led to better performance.",
      "startOffset" : 54,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "We use a deep output layer (Pascanu et al., 2013) to compute the conditional distribution over output tokens,",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "As noted in (Vezhnevets et al., 2016), the proposed model can learn to recompute very often which decreases the utility of planning.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "In our NMT experiments we use byte pair encoding (BPE) (Sennrich et al., 2015) for the source sequence and character representation for the target, the same setup described in Chung et al.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : ", 2015) for the source sequence and character representation for the target, the same setup described in Chung et al. (2016). We also use the same preprocessing as in that work.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "(2016), with the attention mechanisms in both the baseline and (r)PAG conditioned on both layers of the encoder’s biscale GRU (h1 and h2 – see (Chung et al., 2016) for more detail).",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "com/nyu-dl/dl4mt-cdec As a baseline we use the biscale GRU model of Chung et al. (2016), with the attention mechanisms in both the baseline and (r)PAG conditioned on both layers of the encoder’s biscale GRU (h1 and h2 – see (Chung et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "As can be seen from Table 1, layer normalization (Ba et al., 2016) improves the performance of the PAG model significantly.",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "(†) denotes the results of the baseline that we trained using the hyperparameters reported in (Chung et al., 2016) and the code provided with that paper.",
      "startOffset" : 94,
      "endOffset" : 114
    } ],
    "year" : 2017,
    "abstractText" : "We investigate the integration of a planning mechanism into an encoder-decoder architecture with attention for character-level machine translation. We develop a model that plans ahead when it computes alignments between the source and target sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the strategic attentive reader and writer (STRAW) model. Our proposed model is end-to-end trainable with fully differentiable operations. We show that it outperforms a strong baseline on three characterlevel translation tasks from WMT’15. Analysis demonstrates that our model computes qualitatively intuitive alignments and achieves superior performance with fewer parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}