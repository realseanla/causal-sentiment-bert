{
  "name" : "1703.02031.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "  Random vector genera on of a seman c space  \nJean‐François Delpech       Sabine Ploux   Ins tut des Sciences Cogni ves \nUMR5304 CNRS ‐ Université de Lyon  67, boulevard Pinel \n69675 BRON cedex, France   jfdelpech@gmail.com     sploux@isc.cnrs.fr   \n \nWe show how random vectors and random projec on can be implemented in the usual vector space model to construct a Euclidean seman c space from a French synonym dic onary. We evaluate theore cally the resul ng noise and show the experimental distribu on of the similari es of terms in a neighborhood according to the choice of parameters. We also show that the Schmidt orthogonaliza on process is applicable and can be used to separate homonyms with dis nct seman c meanings. Neighboring terms are easily arranged into seman cally significant clusters which are well suited to the genera on of realis c lists of synonyms and to such applica ons as word selec on for automa c text genera on. This process, applicable to any language, can easily be extended to colloca ons, is extremely fast and can be updated in real  me, whenever new synonyms are proposed.\n1. Introduc on\nIn their seminal work, Ploux and Victorri 1 have used synonymy rela ons deduced from French electronic dic onaries to create seman c spaces around French words and their neighbors. Their defini on of “synonymy” is fairly broad and includes hyponymy (moineau and oiseau), hyperonymy (arme and pistolet) or even non‐synonymous, but related terms (autocar and automobile); however, in their work, true synonyms (i.e. terms which are more or less interchangeable) form cliques of the graph of synonyms, i.e. maximally complete subgraphs. While this is very interes ng from a theore cal standpoint, as it then becomes straigh跀�orward to evaluate an interclique distance (or degree of separa on) between any two terms in the graph (as long as neither belongs to an island, such as lapereau and lapinot), it is not very useful in prac ce. For example, an author in search of the right term may well not be interested in strict synonyms; terms with related or even opposed meanings can o en be preferable in rhetorical figures. Also, in many applica ons such as automa c text genera on, a well‐defined and mathema cally well behaved seman c distance between terms is o en a prerequisite.\nIn this report, we show how an Euclidean seman c distance can quickly and easily be constructed from Ploux and Victorri's database (which contains 54,685 terms and 116,694 cliques).\n2. Construc on of a seman c space 2.1. The vector space model\nSince the pioneering work of Salton 2 , 3, it is well understood that any combina on of terms, such as a clique, can be seen as a vector in a space where each dimension represents a dis nct term (or lemma.)\n(1)\nThis representa on is extremely frui跀�ul and forms the basis of numerous informa on retrieval systems; it suffers however from a severe limita on in that each term is orthogonal to each other. Of course, the dual equa on from Equa on 1,\n(2)\n= ( , ,… , )Cj t1,j t2,j tt,j\n= ( , ,… , )Tk c1,k c2,k cs,k\n1/10\nmay be used to compute term distances (or similari es) but the very high dimensionality of the subtending space makes such distances difficult to compute and to interpret: this is the “curse of dimensionality”.\n2.2. Overlap similarity between terms\nIf   with cardinality   is the set of dis nct terms occurring in all the cliques containing term  , we define the overlap similarity between two terms as the cardinality   of the intersec on   (each word being counted only once.) Obviously,  for most   pairs, since for any   the total number of dis nct terms in the database is much larger than  , which ranges from  to   in Ploux and Victorri's 3 database with an average value of 8.5.\n2.3. Contexonyms\n“Contexonyms” are words which co‐occur in a given context (as for example in the same sentence of a corpus); while they are not synonyms, they are obviously closely related. Ji, Ploux and Wehrli 4 have proposed an automa c contexonym organizing model (ACOM) which relies on coun ng of co‐occurrences and evalua ng their probabili es to automa cally produce and organize contexonyms for a target word. The test results, a er training on an English corpus maintained by Project Gutenberg, show that the model is able to classify contexonyms as well as to reflect words' minute usage and nuance.\n2.4. Latent seman c indexing\nDimensionality reduc on can be achieved by a low‐rank approxima on of the term‐document matrix. This can be done by Latent Seman c Indexing 5, which reduces dimensionality through a singular value decomposi on (SVD) of the term‐document matrix, retaining only a compara vely small number of the largest singular values. This method has been very successfully used for document indexing and retrieval. It suffers nevertheless from limita ons:\nSVD is computa onally intensive, even though the large term‐document matrix is very sparse; There is no really sa sfactory way to increment the results as new terms/documents become available.\nMore importantly, it is not well suited to genera ng a seman c space from cliques. The resul ng, lower dimensional space is the best approxima on, in the least squares sense, of the posi on of any term belonging to the whole set of cliques: the distance between any pair of terms will be op mal, while what is really of interest from the present perspec ve is the accurate determina on of distances between seman c neighbors.\nAs a test, a SVD decomposi on of the clique‐term matrix (of which eq. 2 is a row) was performed. It reduced the matrix size from 54,685 x 116,694 to 54,685 x 250, meaning that each term was associated with a vector having 250 orthogonal components. The decomposi on, which took 134 sec. on a desktop computer, was clearly unsa sfactory as the singular values decayed very slowly, from 63.31 for the first coordinate to 37.4 for the 250th one. According to this computa on, the first few neighbors of rapsode would be rimailleur, rimeur, versificateur, métromane, fils d'Apollon, favori des Muses, favori du Parnasse, nourrisson des Muses, héros du Pinde, mâche‐laurier, all with similari es extremely close to 1.0. While clearly in the right neighborhood, this seems to be of limited usefulness; note however that in prac ce a restric on to the first two or three largest singular values may o en yield useful informa on 6.\n2.5. Neural networks\nWord order is not considered in this publica on, but it should be men oned for completeness that neural networks are o en used in natural language processing to encode word sequences (see 7 for an extended review and bibliography). In a recent publica on, Mikolov et al. 8 have introduced two novel model architectures for compu ng con nuous vector representa ons of words from very large data sets. They report large improvements in accuracy at a computa onal cost which is s ll substan al, but that they claim is much lower than previous architectures. An interes ng considera on is that, according to Mikolov et al. 9, the learned vectors explicitly encode many linguis c regulari es and pa erns.\nDi di ti di,k ∩Di Dk = 0di,k\n(i, k) i di 2\n243\n2/10\n2.6. Random vectors and random projec on\n2.6.1. Remarks on high‐dimensionality spaces\nDasgupta 10 points out that intui ons valid in a low‐dimensionality space may be totally misleading in a high‐dimensionality space. For example, a set of points picked at random from the unit ball\n(3)\nwill have some significant frac on near the origin, say within distance   if  , but this frac on becomes rapidly vanishingly small as the dimension   becomes large; for example for  ,  .\nAnother useful remark is that while obviously one cannot create more than   orthogonal vectors in a space of dimension  , one can create an exponen ally large number of vectors quasi‐orthogonal to each other; in other words 10, a set of  vectors picked at random will with high probability be quasi‐orthogonal, i.e. have angles of   with each others. The seed vectors referred to below will be selected from such a set \nWhile an orthogonal projec on will in general reduce the average distance between points, it is also known, as shown by Johnson and Lindenstrauss in an o en cited paper 11, that distances may be almost perfectly preserved for any   points in an arbitrary number   of dimensions when projected to a random subspace of   dimension.\n2.6.2. Building random vectors\nThe compara vely recent method of random projec on 12 , 13 , 14 is based on these three preceding remarks and proceeds as follows:\n1. Uniquely associate with each term   a random seed vector   having   independent coordinates; 2. Associate with each clique   the vector   where   refers to the set of terms found in clique   and \nis a func on of the number of occurrences in   of term   and of the weights associated with  ; 3. Finally, associate with each term   the (suitably weighted) sum of the vectors of the cliques in which   appears, \n where   refers to the cliques containing  . In what follows, we shall assume without loss of generality that the term vectors   are normalized to unity.\nObviously, each term vector   is now embedded in a  ‐dimensional Euclidean seman c space and the similarity   between terms   and   is the scalar product of the associated term vectors:\n(4)\nIt is easy to see that   ranges from ‐1 to 1. It is some mes more convenient to consider the distance   which is related to the\nsimilarity by   and ranges from   (same   and  ) to   (exactly opposite terms; note however that owing to\nthe extreme sparsity of a high‐dimensional space, the neighborhood exactly opposite a term is in prac ce always empty.)\n2.6.3. Locality property\nBuilding a term vector   by the process described above involves only the terms pertaining to the set   defined in sec on 2.2. It is thus a purely local process: upda ng the seman c space requires only a few ten or a few hundred opera ons, orders of magnitude less than its ini al genera on (provided small changes to the weights are neglected, which is usually acceptable as they are logarithmic in term frequency and inverse document frequency.)\nThis does not imply that the similarity of term   with term   is zero, even though  , since   and  may well have neighbors in common. For example caro e and fraude have a degree of separa on of 2 but a similarity of 0.364.\nThe seed vectors   are not quite orthogonal and the scalar product   will usually be small, but not zero. Thus, even for uncorrelated term vectors   and  , their similarity   will usually be small but non‐zero. This induce an unavoidable noise which is studied below in some detail.\n{x ∈ V : ∥x∥ < 1}\n1/2 d = 3 d d = 250 = ≈ 5.5 ×d250 0.5250 10−76\nd d\nexp(O( d))ϵ2\n90 ± ϵ .Sd\nn d O(logn)\nti ∈si Sd d ck =Ck ∑i∈ck ρ k i si i ∈ ck ck ρ k i\nck ti ti ti ti\n=Ti ∑k∋ti Ck k ∋ ti ti Ti\nTi d σij ti tj\n= ⟨ | ⟩σij Ti Tj\nσij Dij\n=Dij 2(1 − )σij − −−−−−−−√ 0 ti tk 2\nTi Di\nti ∉tk Di ∉ ⟹ ∉tk Di ti Dk ti tk\nsi ⟨ | ⟩si sj Ti Tj σij\n3/10\n  2.7. Prac cal implementa on\nA normalized seed vector embedded in a  ‐dimensional space has   coordinates of which   are  ,   are   and  are  . Having the same number of posi ve and nega ve coordinates ensures that the scalar product of two seed vectors is 0 on the average. As seed vectors need to be very close to orthogonal with each others, the number   of non‐zero coefficients must be substan ally smaller than the dimension  .\nThe number of available, dis nct seed vectors is the product of the number of combina ons   of   non‐zero coordinates amongst   coordinates,  mes the number of ways   of distribu ng   posi ve and   nega ve coordinates amongst these non‐zero coordinates :\n(5)\nIn prac ce,   should be much larger than the number of dis nct terms to guarantee a negligible collision probability (i.e. two dis nct terms having the same seed vector). This condi on is already amply met with   as  when a dimension   is selected.\nGiven   and   and no ng for simplicity  , the probability   of an overlap   between two randomly selected seed vectors is\n(6)\nWhen two randomly selected, normalized seed vectors have an overlap of   non‐zero coordinates, their scalar products will be arranged symmetrically around zero and vary in discrete steps of  . An overlap of   will generate the two scalars   and   with probabili es  , an overlap of 2 will generate   with probability  ,   with probability  , and  with probability  ; more generally, an overlap   will generate the scalar   with the probability\n(7)\nwhere the factors   are restricted to integer values and by virtue of the iden ty  .\nIt can be seen from equa ons 6 and 7 that the noise decreases more or less linearly with   Theore cal and experimental scalar products of two seed vectors, as computed from equa ons 6 and 7, are plo ed in the next figure (next page) where:\nThe light ver cal lines are increments of  , the heavier ver cal lines are at 0,   and  . The two horizontal lines are at 1.0 and 0.136. The red dots are computed by taking the scalar products of 1,000,000 'term vectors' each synthesized by the addi on of 5 random seed vectors. If instead we do the sta s cs directly on seed vectors, the result is unchanged except that the dots now occur only at mul ples of 0.01 and the total is accordingly 5  mes larger. The black dots are sta s cs over 40,000 points, star ng with 10,000, taken from the tail of the neighbors of an arbitrary term (here rapsode.) The purple dots are a Gaussian with a standard devia on of \nd d d − 2m 0 m +1/ 2m −−−√ m\n−1/ 2m −−−√\n2m d\n( )d2m 2m d ( )2m\nm m m\n(d,m) = ( )× ( )Nseed d 2m 2m m\nNseed m ≥ 4 (250, 4) ≈ 2.4 ×Nseed 10 16\nd = 250\nd m p = 2m/d (v, d)Poverlap v\n(v, d) = ( )× × (1 − pPoverlap 2m v pv )(2m−v)\nv\n1/2m 1 1/2m −1/2m 1/2 2/2m 1/4 0 1/2 −2/2m\n1/4 v s/2m\n(v, s) = ( )×Pscalar v qs 2−v\n= (v+ s)/2qs ( ) ≡∑ v k=−v k qk 2v\nd.\n0.01σ −0.1σ +0.1σ\n0.063.\n4/10\n  Figure 1 ‐ Slice Noise\n100%100%\n13.6%13.6%\n \n3. Results\nObviously, the size of the database of the term vectors   is itself linearly dependent on the dimension; for  , each vector occupies 10 kB if a single coordinate is represented by a 4‐byte floa ng point number. A database of 1,000,000 dis nct terms would thus occupy 10 GB with this elementary data structure; however, for many applica ons,   will be sufficient and/or more sophis cated data structures may be implemented. Computa on  mes will also increase more or less linearly with  , because they mostly involve stepping through all the dimensions.\n  3.1. Term neighbors\nThe following four figures have been constructed by compiling eight independent databases of   term vectors for each of the four indicated   couples; 跀�‐idf sta s cal weights were used. Typically, on a small desktop computer, the compila on me is 3 to 4 seconds for   and 20 to 30 seconds for  .\n  Abscissas are propor onal to the logarithm of the neighbor's rank (From 1 for maison in the upper right corner to 1000 in the lower le  corner) and ordinates are the similari es to maison. For a given neighbor, the horizontally aligned red dots represent the eight scalars   computed from the eight databases and the thicker, black dot is the average value   of the eight scalars. The neighbors are arranged in non‐increasing order of their   with maison.   Even though the diameter   of maison as defined in sec on 2.2 is only 98, there are several hundred significant neighbors: while most close neighbors belong to the set  , many lie at more than one degree of separa on from each others (their cliques are separated by more than one vertex). Also, it can be seen, as expected, that the noise is inversely propor onal to  but not very dependent on   (in fact, the only no ceable effect of a lower   is that there are more outliers) and that a standard devia on of about   is not unrealis c for  .\nTi d = 2500\nd = 250 d\n54, 685 (m, d)\nd = 250 d = 2500\nσi σavg σavg\ndmaison Dmaison\nd√ m m\n0.063 d = 2500\n5/10\nFigure 2 ‐ Neighbors of maison\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nscalar\n1\n3\n10\n30\n100\n300\n1000\nrank\nFigure 3 ‐ Neighbors of maison\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nscalar\n1\n3\n10\n30\n100\n300\n1000\nrank\nFigure 4 ‐ Neighbors of maison\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nscalar\n1\n3\n10\n30\n100\n300\n1000\nrank\nFigure 5 ‐ Neighbors of maison\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nscalar\n1\n3\n10\n30\n100\n300\n1000\nrank\n  In what follows, unless otherwise noted, we'll use   and  . The size of the file containing the 54,685 term vectors is then 547,724,964 bytes, including some overhead. The number of available seed vectors being   the risk of collision is totally negligible.\nThe 100 first neighbors of maison are listed by decreasing similarity in table 1 next page. It is clear that the proximity decreases with  , but that those neighboring words are all reasonably close to maison in its various meanings.\nTable 1 ‐ First 100 neighbors of maison\nFrom 1 to 20 From 21 to 40 From 41 to 60 From 61 to 80 From 81 to 100\n1 1.000 maison 21 0.499 chez‐soi 41 0.309 ménage 61 0.207 ermitage 81 0.163 domes que 2 0.843 demeure 22 0.498 cassine 42 0.307 bâ ment 62 0.204 appartement 82 0.162 plaque_de_blindage 3 0.820 habita on 23 0.491 cabane 43 0.306 case 63 0.198 cagna 83 0.159 mas 4 0.810 logis 24 0.486 gourbi 44 0.301 taudis 64 0.191 reposée 84 0.155 tanière 5 0.767 domicile 25 0.480 gîte 45 0.300 chalet 65 0.190 chartreuse 85 0.154 cache 6 0.762 pénates 26 0.477 bercail 46 0.292 pavillon 66 0.188 domes cité 86 0.152 niche 7 0.677 home 27 0.460 masure 47 0.292 villa 67 0.187 garde‐meubles 87 0.151 rendez‐vous_de_chasse 8 0.669 mesnil 28 0.436 asile 48 0.287 chaumière 68 0.186 gabionnade 88 0.151 repaire 9 0.661 chacunière 29 0.428 château 49 0.278 manse 69 0.182 lignée 89 0.149 clinique 10 0.659 foyer 30 0.404 immeuble 50 0.275 hôtel_par culier 70 0.180 caponnière 90 0.148 glorie e 11 0.653 train_de_maison 31 0.403 hu e 51 0.265 isba 71 0.172 ferme e 91 0.147 havre 12 0.603 logement 32 0.385 maisonne e 52 0.254 bas‐lieu 72 0.171 grand_ensemble 92 0.147 lapinière 13 0.596 maisonnée 33 0.382 ménil 53 0.249 manoir 73 0.171 lieu 93 0.146 kiosque 14 0.593 nid 34 0.374 abri 54 0.244 hôtel 74 0.171 famille 94 0.146 intérieur 15 0.576 résidence 35 0.369 galetas 55 0.241 standing 75 0.167 garde‐meuble 95 0.144 bouverie 16 0.572 toit 36 0.368 lares 56 0.240 H.L.M. 76 0.167 deck‐house 96 0.144 parents 17 0.545 bicoque 37 0.350 lare 57 0.233 habitacle 77 0.166 habitat 97 0.143 tourelle 18 0.537 bâ sse 38 0.338 clapier 58 0.223 retraite 78 0.166 édifice 98 0.143 mantelet 19 0.532 cahute 39 0.331 palais 59 0.211 carbet 79 0.164 firme 99 0.142 hangar 20 0.502 baraque 40 0.314 train_de_vie 60 0.208 séjour 80 0.163 tranchée‐abri 100 0.141 pigeonnier\n \nm = 5 d = 250 m = 10 d = 2500\nm = 40 d = 250 m = 50 d = 2500\nd = 2500 m = 50 ≈ 2.0 × 10255\nσ\n6/10\n3.2. Similarity matrices and clusteriza on\n  It is also straigh跀�orward to build a similarity matrix (see table 2) and to use such matrices to group terms by clusters, i.e. lists of terms which do not all belong to the same clique, but which are closely related seman cally. We use nearest‐neighbor clustering in this work.\n  Table 2 ‐ Similarity matrix\n   chacunière      1.000     mesnil      0.665    1.000 \n   train_de_maison      0.657    0.659    1.000     maisonnée      0.546    0.554    0.560    1.000     demeure      0.456    0.470    0.449    0.394    1.000     habita on      0.414    0.433    0.403    0.344    0.852    1.000     maison      0.661    0.669    0.653    0.596    0.843    0.820    1.000     pénates      0.481    0.496    0.481    0.417    0.837    0.605    0.762    1.000     logement      0.257    0.278    0.250    0.223    0.796    0.724    0.603    0.616    1.000     domicile      0.457    0.464    0.462    0.394    0.780    0.670    0.767    0.620    0.588    1.000     logis      0.507    0.514    0.506    0.447    0.795    0.666    0.810    0.706    0.623    0.892    1.000 \n   résidence      0.305    0.310    0.308    0.262    0.727    0.609    0.576    0.511    0.554    0.840    0.627    1.000 \n  In table 3, the headers are the members of the original cliques including maison, grouped in seman cally homogeneous clusters, and the associated lists are terms similar with   to the center of mass of their header. Terms in blue are from the original cliques, terms in gray are repeats from a previous cluster, and the others could reasonably be aggregated to their head cluster, especially at similari es above \n \nTable 3 ‐ Clusters around maison and their cohorts\nchacunière, mesnil, train_de_maison, maisonnée, demeure, habita on, pénates, maison, logement, domicile, logis, résidence\nabri, clapier, gîte, nid, asile, retraite, bercail, toit, foyer, habitacle\nbaraque, bicoque, cahute, cabane, hu e, gourbi, masure, case, cassine, chaumière, maisonne e\nbas‐lieu, naissance, origine, descendance, famille, lignée, race, parents, chez‐soi, home, intérieur, ménil, lare, lares, ménage, standing, train_de_vie\nappartement, bouge, taudis, galetas, chalet, pavillon, villa, château, manoir, palais, réduit\nbuilding, édifice, bâ ment, immeuble, construc on, bâ sse, hôtel, campagne, propriété, ferme\nboîte, entreprise, firme, établissement, prison, commerce, temple, ins tut, ins tu on, branche, couvert, domes cité, serviteur, domes que, gens, monde, suite\nclinique, hôpital, nom, couronne, trône, pigeonnier, lieu, place, séjour, feu\n3.3. Orthogonaliza on\nThings get more complicated when two homonyms are seman cally disjoint, as is the case with le barde and la barde:\n0.73 maison  0.70 demeure  0.67 logis  0.66 domicile  0.63 pénates  0.62 habita on  0.55 chacunière \n0.55 chacunière  0.55 résidence 0.55 logement  0.54 mesnil 0.54 train_de_maison  0.49 home  0.49 foyer \n0.49 foyer  0.47 maisonnée  0.42 nid  0.42 toit  0.39 bâ sse  0.34 chez‐soi  0.33 bercail \n0.33 bercail  0.33 château  0.33 gîte  0.33 bicoque  0.33 cassine  0.33 gourbi  0.32 cahute \n0.32 cahute  0.32 ménil  0.30 asile  0.30 masure  0.29 baraque  0.29 immeuble  0.28 lare \n0.28 lare  0.28 lares  0.26 cabane \n0.54 abri  0.54 nid  0.50 toit  0.49 gîte  0.47 bercail  0.46 asile  0.43 maison  0.43 demeure  0.41 pénates  0.41 clapier \n0.41 clapier  0.40 foyer  0.38 deck‐house  0.38 retraite  0.37 tranchée‐abri  0.37 habita on  0.37 gabionnade  0.37 garde‐meubles  0.37 cache  0.37 caponnière \n0.37 caponnière  0.36 repaire  0.36 garde‐meuble  0.36 plaque_de_blindage  0.35 tanière  0.35 logement  0.33 havre  0.33 habitacle  0.32 logis \n0.32 logis  0.32 refuge  0.31 bouverie  0.30 lapinière  0.30 hangar  0.30 mantelet  0.30 étable  0.29 lieu_sûr  0.29 tenderolle  0.29 pare‐éclats \n0.29 pare‐éclats  0.29 domicile  0.29 home  0.28 taud  0.27 reposée  0.27 reposoir  0.27 cache e  0.27 auvent  0.27 darse  0.26 soue \n0.26 soue  0.26 kiosque  0.26 niche  0.26 abrivent  0.25 cagna  0.25 porcherie  0.25 tourelle  0.25 brise‐vent \n0.69 cabane  0.65 baraque  0.64 bicoque  0.60 cahute  0.59 hu e \n0.59 hu e  0.50 masure  0.50 maisonne e  0.50 gourbi  0.45 chaumière \n0.45 chaumière  0.43 maison  0.42 case  0.40 cassine  0.37 habita on \n0.37 habita on  0.36 chaumine  0.35 carbet  0.32 cagna  0.30 buron \n0.30 buron  0.27 train_de_maison  0.26 chalet  0.26 demeure  0.25 chacunière \n0.25 chacunière  0.25 appen s \n0.33 famille  0.33 race  0.31 descendance \n0.31 descendance  0.31 lignée  0.27 maison \n0.27 maison  0.26 filia on  0.25 bas‐lieu \n0.25 bas‐lieu \n0.33 château  0.33 habita on  0.32 pavillon \n0.32 pavillon  0.30 taudis  0.30 galetas \n0.30 galetas  0.29 villa  0.29 maison \n0.29 maison  0.29 chartreuse  0.27 manoir \n0.27 manoir  0.27 chalet  0.26 bouge \n0.26 bouge \n0.43 building  0.41 immeuble  0.40 bâ ment \n0.40 bâ ment  0.36 édifice  0.34 bâ sse \n0.34 bâ sse  0.30 construc on  0.30 H.L.M. \n0.30 H.L.M. \nσ > 0.25\n0.35.\n7/10\nTable 4 ‐ First 100 neighbors of barde\nFrom 1 to 20 From 21 to 40 From 41 to 60 From 61 to 80 From 81 to 100\n1 1.000 barde 21 0.298 versificateur 41 0.167 croque‐notes 61 0.097 vic maire 81 0.080 injurié 2 0.839 aède 22 0.297 mâche‐laurier 42 0.157 choriste 62 0.096 flamine 82 0.079 septemvir 3 0.717 tranche_de_lard 23 0.290 héros_du_Pinde 43 0.148 harnais 63 0.096 prestolet 83 0.079 lama 4 0.625 chantre 24 0.290 favori_des_Muses 44 0.147 prêtre 64 0.094 iman 84 0.078 salien 5 0.533 poète 25 0.289 amant_du_Parnasse 45 0.146 cigale 65 0.094 mu i 85 0.078 brachyne 6 0.521 chanteur 26 0.286 favori_du_Parnasse 46 0.124 coryphée 66 0.094 ra chon 86 0.077 ménestrier 7 0.503 rhapsode 27 0.285 nourrisson_du_Parnasse 47 0.119 trouveur 67 0.093 ovate 87 0.076 curé 8 0.493 bardit 28 0.285 poétereau 48 0.114 corybante 68 0.093 utopiste 88 0.076 talapoin 9 0.468 trouvère 29 0.284 métromane 49 0.110 luperque 69 0.091 ministre_du_culte 89 0.075 épulon 10 0.450 scalde 30 0.284 citharède 50 0.110 muezzin 70 0.090 pope 90 0.074 me re_dans_le_même_sac 11 0.427 troubadour 31 0.282 crooner 51 0.109 druide 71 0.090 mystagogue 91 0.074 immodérément 12 0.421 minnesinger 32 0.276 félibre 52 0.105 eubage 72 0.090 cantatrice 92 0.074 sacrificateur 13 0.328 nourrisson_du_Pinde 33 0.274 due迀�ste 53 0.105 parolier 73 0.089 archiprêtre 93 0.074 bombardier 14 0.323 amant_des_Muses 34 0.267 rapsode 54 0.103 quindecemvir 74 0.086 sous‐ventrière 94 0.073 lévite 15 0.315 lamelle 35 0.256 ménestrel 55 0.103 mollah 75 0.085 abbé 95 0.073 englober 16 0.313 favori_d'Apollon 36 0.240 rimeur 56 0.103 padre 76 0.084 curète 96 0.072 chiennerie 17 0.313 fils_d'Apollon 37 0.233 rimailleur 57 0.102 hiérogrammate 77 0.084 papas 97 0.072 rabbin 18 0.308 nourrisson_des_Muses 38 0.216 panne 58 0.100 saronide 78 0.082 avarice 98 0.072 passivité 19 0.302 enfant_d'Apollon 39 0.207 choreute 59 0.097 chansonnier 79 0.081 quindécemvir 99 0.072 capelan 20 0.300 maître_du_Pinde 40 0.176 vocaliste 60 0.097 chapelain 80 0.080 officiant 100 0.072 eschatologique\nIf we meant barde as aède, the third neighbor, tranche_de_lard is clearly not appropriate, and conversely.\nHowever, in a Euclidean space, the Schmidt orthogonaliza on procedure does remove this kind of interference. Since term vectors are normalized to unity, one needs simply to subtract from the vector   the collinear component of the vector \n:\n(8)\nwith the following result, where the perturba on due to tranche_de_lard is totally eliminated:\nTable 5 ‐ First 100 neighbors of barde orthogonalized w.r.t. tranche_de_lard\nFrom 1 to 20 From 21 to 40 From 41 to 60 From 61 to 80 From 81 to 100\n1 0.744 aède 21 0.396 poétereau 41 0.217 croque‐notes 61 0.162 harnais 81 0.130 lama 2 0.697 barde 22 0.396 maître_du_Pinde 42 0.215 lamelle 62 0.161 prestolet 82 0.130 papas 3 0.634 chantre 23 0.394 mâche‐laurier 43 0.205 coryphée 63 0.161 curète 83 0.126 soliste 4 0.634 scalde 24 0.393 favori_des_Muses 44 0.188 cantatrice 64 0.157 ministre_du_culte 84 0.124 talapoin 5 0.629 poète 25 0.389 nourrisson_du_Parnasse 45 0.185 quindecemvir 65 0.157 chapelain 85 0.121 directeur_de_conscience 6 0.622 chanteur 26 0.389 amant_du_Parnasse 46 0.183 mystagogue 66 0.157 eubage 86 0.120 utopiste 7 0.582 minnesinger 27 0.387 héros_du_Pinde 47 0.178 luperque 67 0.156 mu i 87 0.120 prêtraille 8 0.548 trouvère 28 0.380 félibre 48 0.177 padre 68 0.155 saronide 88 0.115 ceinture_de_sécurité 9 0.525 rhapsode 29 0.380 rapsode 49 0.177 mollah 69 0.153 trouveur 89 0.113 curé 10 0.523 troubadour 30 0.364 versificateur 50 0.175 muezzin 70 0.152 ovate 90 0.113 sacrificateur 11 0.429 nourrisson_du_Pinde 31 0.337 ménestrel 51 0.173 hiérogrammate 71 0.144 parolier 91 0.109 rabbin 12 0.415 crooner 32 0.333 métromane 52 0.172 ra chon 72 0.140 chansonnier 92 0.106 salien 13 0.414 amant_des_Muses 33 0.319 choreute 53 0.172 quindécemvir 73 0.139 hiérophante 93 0.105 aumônier 14 0.414 nourrisson_des_Muses 34 0.292 rimeur 54 0.171 corybante 74 0.139 pope 94 0.105 sous‐ventrière 15 0.409 favori_d'Apollon 35 0.287 rimailleur 55 0.170 archiprêtre 75 0.137 diva 95 0.105 capelan 16 0.408 citharède 36 0.282 bardit 56 0.169 druide 76 0.137 épulon 96 0.105 affublement 17 0.405 due迀�ste 37 0.242 choriste 57 0.168 vic maire 77 0.135 abbé 97 0.104 virtuose 18 0.404 favori_du_Parnasse 38 0.231 vocaliste 58 0.168 cigale 78 0.134 septemvir 98 0.104 ménestrier 19 0.400 fils_d'Apollon 39 0.224 panne 59 0.165 flamine 79 0.133 musicien 99 0.102 exécutant 20 0.397 enfant_d'Apollon 40 0.218 prêtre 60 0.165 iman 80 0.131 officiant 100 0.096 ténor\nThe number of terms which can be subtracted is only limited by the noise.\n|barde⟩\n|tranche_de_lard⟩\n= |barde⟩ − ⟨barde|tranche_de_lard⟩ × |tranche_de_lard⟩|barde⟩⊥tranche_de_lard\n8/10\n  The corresponding clusters associated with   now are:\nTable 6 ‐ Clusters around barde and their cohorts (orthogonalized w.r.t. tranche_de_lard)\naède, barde, poète, chanteur, chantre\nbardit, harnais, prêtre, lamelle, panne, rhapsode, troubadour, trouvère\nto be compared to the non‐orthogonalized result:\nTable 7 ‐ Clusters around barde and their cohorts aède, barde, chanteur, chantre, poète\nbardit, tranche_de_lard, lamelle, rhapsode, troubadour, trouvère\nharnais, panne, prêtre\nConclusion and future work\nWe have shown how to construct a Euclidean seman c space from a French synonym dic onary by combining random vectors and random projec on with the usual vector space model. The process is extremely fast and introduces an amount of noise acceptable in most situa ons. Upda ng the seman c space with new synonyms involves handling only a limited number of terms and is thus easily done in real  me. As usual, the inner product between two normalized term vectors in the resul ng seman c space is interpreted as the similarity between the terms. In addi ons, the Schmidt orthogonaliza on process is immediately applicable and can be used to separate seman cally disjoint homonyms. The resul ng seman c space is directly suited to the genera on of prac cal lists of synonyms and to applica ons such as word selec on for automa c text genera on.\nThis process can be applied to any language, including English, where for example WordNet could be used as a basis. It can also be easily extended to colloca ons or to contexonyms 15 , 16 , 17. In future work, we plan on expanding this method by building a dynamic seman c space from Wikipedia and its “history” pages, whereby the evolu on of the meaning of terms as a func on of me can be viewed through the evolu on of their seman c neighborhoods. This could also be done using  me series from newspapers, from radio and television. We are also considering building a real‐ me seman c space from news and social networking services such as Twi er.\n0.71 chanteur  0.71 aède  0.70 scalde  0.70 chantre  0.66 poète  0.61 minnesinger  0.55 trouvère \n0.55 trouvère  0.55 troubadour  0.52 barde  0.48 rhapsode  0.47 due迀�ste  0.46 crooner  0.46 citharède  0.44 poétereau \n0.44 poétereau  0.44 fils_d'Apollon  0.44 maître_du_Pinde  0.43 nourrisson_du_Pinde  0.43 mâche‐laurier  0.43 nourrisson_du_Parnasse  nourrisson_du_Parnasse  0.43 amant_du_Parnasse  0.42 nourrisson_des_Muses  0.42 enfant_d'Apollon  0.42 favori_d'Apollon  0.42 favori_du_Parnasse  0.42 favori_du_Parnasse  0.42 favori_des_Muses  0.42 héros_du_Pinde  0.42 choreute  0.41 amant_des_Muses  0.40 félibre  0.37 versificateur  0.36 ménestrel  0.36 ménestrel  0.36 rapsode  0.35 choriste  0.34 métromane  0.31 rimailleur  0.30 rimeur  0.28 vocaliste  0.27 coryphée \n0.29 trouvère  0.29 troubadour \n0.29 troubadour  0.28 minnesinger \n0.79 aède  0.72 chantre  0.70 barde  0.69 chanteur  0.64 scalde  0.63 poète  0.56 minnesinger \n0.56 minnesinger  0.54 trouvère  0.52 troubadour  0.50 rhapsode  0.42 due迀�ste  0.42 citharède  0.42 crooner  0.41 poétereau \n0.41 poétereau  0.40 fils_d'Apollon  0.40 maître_du_Pinde  0.40 mâche‐laurier  0.40 nourrisson_du_Pinde  0.40 amant_du_Parnasse \namant_du_Parnasse  0.40 nourrisson_du_Parnasse  0.40 nourrisson_des_Muses  0.39 héros_du_Pinde  0.39 enfant_d'Apollon  0.39 favori_des_Muses  0.39 favori_des_Muses  0.39 favori_d'Apollon  0.39 choreute  0.38 favori_du_Parnasse  0.38 amant_des_Muses  0.38 félibre  0.34 versificateur  0.33 ménestrel  0.33 ménestrel  0.32 choriste  0.32 tranche_de_lard  0.32 rapsode  0.32 métromane  0.28 rimailleur  0.27 rimeur  0.25 vocaliste \n0.48 barde  0.42 trouvère  0.41 troubadour \n0.41 troubadour  0.41 aède  0.37 minnesinger  0.35 poète \n0.35 poète  0.34 rhapsode  0.33 tranche_de_lard  0.32 ménestrel \n0.32 ménestrel  0.31 chanteur  0.30 félibre  0.29 chantre \n0.29 chantre  0.28 bardit  0.25 scalde \n0.35 panne  0.34 harnais  0.34 prêtre  0.28 muezzin \n0.28 muezzin  0.27 corybante  0.27 mu i  0.27 iman  0.27 ovate\n0.27 ovate 0.27 flamine  0.27 curète  0.27 hiérogrammate  0.27 archiprêtre \n0.27 archiprêtre  0.27 quindecemvir  0.27 mollah  0.27 padre  0.27 luperque \n0.27 luperque  0.26 mystagogue  0.26 chapelain  0.26 quindécemvir  0.26\n0.26 ceinture_de_sécurité  0.26 affublement  0.25 harnois \n|barde⟩⊥tranche_de_lard\n9/10\nBibliography\n[1] Ploux, S. and Victorri, B., Construc on d'espaces séman ques à l'aide de dic onnaires de synonymes, Traitement Automa que des Langues, ATALA (1998) Pages 161‐182.\n[2] Salton, G., Automa c Informa on Extrac on and Retrieval, 1968. [3] Salton, G., Automa c Text Processing, Addison‐Wesley Publishing Company, ISBN 0‐201‐12227‐8, 1989. [4] Ji, H., J., Ploux, S., Wehrli, E., Lexical Knowledge Representa on with Contexonyms, Proceedings of the 9th MT summit, 2008, pp. 194‐201. [5] Deerwester, S. et al., Improving Informa on Retrieval with Latent Seman c Indexing, Proceedings of the 51st Annual Mee ng of the American Society for Informa on Science (25), 1988. [6] Lebart, L. and Salem, A., Sta s que Textuelle, p. 91, Dunod, 1994. [7] Goldberg, Y., A Primer on Neural Network Models for Natural Language Processing, Dra , 2015. [8] Mikolov, T., Chen, K., Corrado, G. and Dean, J., Efficient Es ma on of Word Representa ons in Vector Space, arXiv:1301.3781v3, 2013. [9] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. and Dean, J., Distributed Representa ons of Words and Phrases and their Composi onality, arXiv:1310.4546v1, 2013. [10] Dasgupta S., Technical Perspec ve: Strange Effects in High Dimension, Communica ons of the ACM, Vol. 53 (2010) No. 2, Page 96. [11] Johnson, W. and Lindenstrauss, J., Extensions of Lipschitz maps into a Hilbert space, Contemporary Mathema cs 26, (1984), pp. 189‐206. [12] Sahlgren, M., The Word‐Space Model: Using Distribu onal Analysis to Represent Syntagma c and Paradigma c Rela ons between Words in High‐dimensional Vector Spaces, PhD Disserta on, Stockholm University, Sweden, 2006.. [13] QasemiZadeh, B. and Handschuh, S., Random Indexing Explained with High Probability, Proceedings of the 18th Interna onal Conference on Text, Speech and Dialog, Springer Interna onal Publishing, 2015. [14] Paradis, R., Guo, J.K., Moulton, J., Cameron, D. and Kanerva, P., Finding Seman c Equivalence of Text Using Random Index Vectors , Procedia Computer Science 20, 2013, pp. 454‐459. [15] Boussidan, A., Dynamics of seman c change, PhD Disserta on, Université Lumière‐Lyon2, France, 2013. [16] Ji, H., J., Ploux, S., Wehrli, E., ibid.. [17] Boussidan, A., Renon A.‐L., Franco C., Lupone S. and Ploux, S., Repérage automa que de la néologie séman que en corpus\nà travers des représenta ons cartographiques évolu ves, To be published, 2010?.\n10/10"
    } ],
    "references" : [ {
      "title" : "Construc on d'espaces séman ques à l'aide de dic onnaires de synonymes",
      "author" : [ "S. Ploux", "B. Victorri" ],
      "venue" : "Traitement Automa que des Langues,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Automa c Text Processing, Addison‐Wesley Publishing Company, ISBN 0‐201‐12227‐8",
      "author" : [ "G. Salton" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1989
    }, {
      "title" : "Lexical Knowledge Representa on with Contexonyms",
      "author" : [ "H. Ji", "S. Ploux", "E. Wehrli" ],
      "venue" : "Proceedings of the 9th MT summit,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Improving Informa on Retrieval with Latent Seman c Indexing",
      "author" : [ "S Deerwester" ],
      "venue" : "Proceedings of the 51st Annual Mee ng of the American Society for Informa on Science",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1988
    }, {
      "title" : "A Primer on Neural Network Models for Natural Language Processing, Dra",
      "author" : [ "Y. Goldberg" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Technical Perspec ve: Strange Effects in High Dimension",
      "author" : [ "S. Dasgupta" ],
      "venue" : "Communica ons of the ACM,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Extensions of Lipschitz maps into a Hilbert space",
      "author" : [ "W. Johnson", "J. Lindenstrauss" ],
      "venue" : "Contemporary Mathema cs 26,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1984
    }, {
      "title" : "The Word‐Space Model: Using Distribu onal Analysis to Represent Syntagma c and Paradigma c Rela ons between Words in High‐dimensional Vector Spaces",
      "author" : [ "M. Sahlgren" ],
      "venue" : "PhD Disserta on, Stockholm University,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Random Indexing Explained with High Probability",
      "author" : [ "B. QasemiZadeh", "S. Handschuh" ],
      "venue" : "Proceedings of the 18th Interna onal Conference on Text, Speech and Dialog, Springer Interna onal Publishing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Finding Seman c Equivalence of Text Using Random Index Vectors",
      "author" : [ "R. Paradis", "J.K. Guo", "J. Moulton", "D. Cameron", "P. Kanerva" ],
      "venue" : "Procedia Computer Science",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Dynamics of seman c change",
      "author" : [ "A. Boussidan" ],
      "venue" : "PhD Disserta on, Université Lumière‐Lyon2, France,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Repérage automa que de la néologie séman que en corpus à travers des représenta ons cartographiques évolu ves",
      "author" : [ "A. Boussidan", "Renon A.‐L", "Franco C", "Lupone S", "S. Ploux" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We show how random vectors and random projec on can be implemented in the usual vector space model to construct a Euclidean seman c space from a French synonym dic onary. We evaluate theore cally the resul ng noise and show the experimental distribu on of the similari es of terms in a neighborhood according to the choice of parameters. We also show that the Schmidt orthogonaliza on process is applicable and can be used to separate homonyms with dis nct seman c meanings. Neighboring terms are easily arranged into seman cally significant clusters which are well suited to the genera on of realis c lists of synonyms and to such applica ons as word selec on for automa c text genera on. This process, applicable to any language, can easily be extended to colloca ons, is extremely fast and can be updated in real me, whenever new synonyms are proposed.",
    "creator" : "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36"
  }
}