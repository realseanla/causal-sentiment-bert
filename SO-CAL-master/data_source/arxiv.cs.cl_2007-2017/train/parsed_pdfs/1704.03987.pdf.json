{
  "name" : "1704.03987.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MOBILE KEYBOARD INPUT DECODING WITH FINITE-STATE TRANSDUCERS",
    "authors" : [ "Tom Ouyang", "David Rybach", "Françoise Beaufays", "Michael Riley" ],
    "emails" : [ "ouyang@google.com", "rybach@google.com", "fsb@google.com", "riley@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We describe the general framework of what we call for short the keyboard “FST decoder” as well as the implementation details that are new compared to a speech FST decoder. We demonstrate that the FST decoder enables new UX features such as post-corrections. Finally, we sketch how this decoder can support advanced features such as personalization and contextualization.\nIndex Terms— Keyboard, decoder, FST."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "With the fast-growing penetration of mobile devices in every aspect of modern life, offering an efficient and pleasant mobile input experience has recently become a topic of interest to researchers and technology providers. Speech recognition for example has flourished in the last few years, mostly fueled by the need for convenient mobile input methods [1]. Likewise, handwriting recognition has gained more traction, especially in languages with complex scripts such as Chinese and Indic languages [2].\nKeyboard input has received relatively less attention from the research community, even though it remains a primary input method as it is often considered, correctly or not, as the most convenient way to compose text on a mobile device. At first glance, keyboard input may seem trivial, but soft keyboards have long surpassed in capabilities the hardware keyboards used on laptops and desktops. They compensate with a rich feature set for the difficulty of typing on a small screen, they can generalize to a broad variety of languages, and being implemented on smart devices, they hold the promise of leveraging rich information about the user and their environment. An FST decoder, with its mathematical formalism and principled implementation, seems a natural choice to power such features.\nThe use of FSTs in the context of keyboard input is not totally new: a report by Klarlund and Riley suggested using FSTs to disambiguate entries on a hardware cluster keyboard, but this was before the invention of smart phones, so the issue of decoding (soft) noisy input sequences was not addressed [3]. FSTs have also been used for similar tasks like spelling correction [4], though not in the context of mobile input. The present paper contains to our best knowledge the very first description of an FST decoder for mobile keyboard input with production- level constraints.\nWhile there have been many mobile keyboard products targeted to smartphones (e.g., Shapewriter [5], Swype, Swiftkey, etc.), the published literature and availability of datasets in this area is very limited. Where relevant, we will compare the FST decoder to a prior in-house implementation using a time-synchronous token-passing decoder model. General background on FST decoding for speech can be found in [6].\nThe rest of this paper contains a summary of major keyboard features and associated terminology (Section 2), a detailed description of the various keyboard transducers (Section 3) and decoder implementation (Section 4), and extensions to more advanced features (Section 5)."
    }, {
      "heading" : "2. KEYBOARD FEATURES AND TERMINOLOGY",
      "text" : "A mobile keyboard must above all be reliable and fast. For these reasons, all run-time processing happens on device. Latency constraints are tight: a key press is expected to produce visible feedback within about 20 msec. RAM and CPU usage must be kept under strict control to prevent the keyboard from being evicted by concurrent processes and to protect the device’s battery life. Memory is also limited: as in embedded speech recognition, keyboard language models should not exceed 5 to 10 Mb, which typically allows them to model a couple hundred thousand words at most.\nThe main function of a soft keyboard is to decode touch inputs into words and sentences, just like a speech recognizer would decode input waveforms. Advanced keyboards support “tap typing”, where users tap the keys corresponding to the characters of a word, and “gesture typing”, where they swipe their finger across the keyboard layout. Users can switch freely from one input mode to the other within a sentence.\nIn tap mode, when the user enters a white space after a character sequence, the composed word is said to be “committed”. In gesture mode, this happens when they lift their finger at the end of the swipe. Because of “fat finger” errors, the user may not tap the exact keys they intended. The sequence of characters indicated by the “bounding boxes” of the keys they actually pressed is called the “literal” decoding of the word. Recognition candidates with high combined spatial and language model scores are typically shown as “suggestions” on the “suggestion strip” right above the keyboard layout. When the score of the best suggestion exceeds that of the literal by some margin, the literal is replaced with the suggestion as an “autocorrection”. The same mechanism is used to correct user typos. Literal decoding also allows users to enter words that are out-of-vocabulary (OOV) to the language model. Balancing OOV recognition and autocorrections is tricky. Gesture input typically does not allow OOV recognition and does not have a concept of literal decoding. It is more similar to speech recognition than tap input.\nWhile a user starts entering a word key by key, word “comple-\nar X\niv :1\n70 4.\n03 98\n7v 1\n[ cs\n.C L\n] 1\n3 A\npr 2\n01 7\ntions” are proposed that they can click on. When they commit a word, “next word predictions” are proposed instead. These two features help users enter fewer keystrokes and type faster. Completions are typically not offered in gesture mode as clicking on them would require the user to lift their finger which could signal they want to commit the word composed so far instead.\nOf course, a soft keyboard also offers character deletions (backspace), and repositioning of the cursor at any time.\nWhile gesture input is easily 10 to 20% faster than tap input [7], relatively few users rely primarily on gesture input."
    }, {
      "heading" : "3. KEYBOARD TRANSDUCERS",
      "text" : "A tapped input consists of a time series of touch points, x, that encodes the coordinates of the user’s key presses. For gesture input, the input trajectory is sampled, e.g. every 100 milliseconds, to provide a similar time series. The task of the decoder is to find the word sequence w that best matches the input sequence x."
    }, {
      "heading" : "3.1. Key Context Dependency and Spatial Model",
      "text" : "The equivalent of speech phonemes in the keyboard world is the set of keys offered in the layout. Accordingly, a spatial model is used to provide a probability distribution over these units. Note that the spatial model does not resolve fully the written language: For example, the letters “é” and “è” in a French keyboard are typically obtained by long-pressing “e” and choosing from a small pop-up menu. All three letters have the same spatial score. This confusability is mitigated by imposing context dependency constraints and enforcing a strong language model.\nJust like acoustic context dependency is encoded in speech with a C transducer, we implemented spatial context dependency in keyboard, and chose to do so with a bi-key model. Accordingly, the arcs of the transducer represent a b : b transitions, where a b means the key b with a as left context (previous key). This places keyboard somewhere in between our server-based speech recognizer which relies on triphone models and our embedded recognizer which uses monophones [8].\nThe spatial model for tap input is typically a Gaussian distribution centered on each key center. Gesture inputs instead are often modeled with the so-called “minimum-jerk model” that imposes smoothness maximization constraints on the input trajectory [9]. Alternatively, a recurrent neural network model can be used [10]."
    }, {
      "heading" : "3.2. Lexicon",
      "text" : "The lexicon transducer L for the keyboard decoder is a simple key to word mapping, like a speech grapheme lexicon. Some keys like the apostrophe may be made optional, allowing users to type “Ive” for “I’ve”. Similarly, repeated keys can be defined optional, e.g. the second “o” in “Google”. The closure, which allows transducing sequences of words, is implemented with a space symbol for finger lift-up or taps on the space bar between words. If this space symbol is optional, the decoder can correct missing space taps between words or decode multi-word gestures. An example of a lexicon FST is shown in Figure 1."
    }, {
      "heading" : "3.3. Language Model",
      "text" : "Similar to the language models in embedded speech recognition systems, keyboard language models are typically low order n-grams over a limited vocabulary, e.g. 64K words [8].\nBecause of features like suggestions, completions and predictions where the language model is more prominent than the spatial model, the language model should be carefully crafted. For example, the user who gestures “Google” may expect the keyboard to suggest “goggle” as an alternative, but not “gogle” or “gooogle”, which chances are would be present in the training corpus as training corpora are often noisy. For this reason, keyboard language models are typically trained to a fixed vocabulary that has been hand-curated to eliminate misspellings, erroneous capitalizations, and other undesired artifacts.\nWith this, the decoder graph for keyboard is constructed using the composition of context, lexicon, and language transducers - the familiar C ◦ L ◦ G. For memory efficiency, we use the on-the-fly composition of (C ◦L)◦G using look-ahead composition filters [11].\nFigure 2A shows a gesture input with its possible alignments to a simplistic, two-word decoder graph. Figure 2B shows the input sequence and its alignment for a similar tap sequence. The same decoder graph (Figure 2C) is shared for both types of input."
    }, {
      "heading" : "4. KEYBOARD DECODING WITH FSTS",
      "text" : ""
    }, {
      "heading" : "4.1. Suggestions and Autocorrections",
      "text" : "Text entry on a mobile touch-screen device is a very error-prone process, with per-letter error rates around 8-9% [12]. This is commonly known as the ”fat finger” problem. One reason we don’t constantly notice these errors in our everyday smartphone usage is because of the autocorrection functionality available on all modern keyboards. They work in the background, silently correcting our typos and misspellings.\nSubstitutions: One basic type of error is mistakenly substituting one letter for another (e.g., ”tjis” instead of ”this”). These include both mechanical ”fat finger” errors, where one accidentally taps on a neighboring key and semantic spelling mistakes . The decoder’s spatial model handles substitutions by producing a probability distribution over the set of possible keys for each frame/tap. This substitution model takes into account the proximity between the touch point and the key.\nDeletions: Deletions occur when the user does not produce any input tap for one or more letters in the intended word (e.g., “farenheit” instead of “fahrenheit”). To handle these errors, the decoder allows to skip one (non-epsilon) arc per frame in the lexicon FST without any input. The weight for these extra epsilon-transitions should represent the probability of omitting the letter in the given context.\nInsertions: The spatial model also allows taps to be treated as extraneous inputs due to accidental touches or spelling errors (e.g., “truely” instead of ”truly”). These inputs are consumed without advancing the state in the decoder graph.\nAutocorrection: No suggestion mechanism is perfect, so when the decoder finds a correction or prefix-completion candidate for the user’s input, it must decide whether or not it is confident enough to trigger an autocorrection. If so, this candidate will automatically replace whatever was typed after the user enters the next separator (e.g., space or punctuation).\nIf the model is too conservative, it will leave many typos uncorrected - even though the decoder may have generated the correct candidate. If the model is too aggressive, it will start making false autocorrections away from correctly typed words, sometimes without the user noticing. This can be very frustrating, since it can drastically change the meaning of the text. For example, imagine the keyboard autocorrecting “you’re far” to “you’re fat” (since the latter may be preferred by the language model and on a QWERTY layout the R is adjacent to the T).\nTo moderate its autocorretion aggressiveness, the decoder evaluates the tapped characters (known as the “literal”) using a characterlevel model. This allows it to calculate a probabilistic score for OOV words, and rank it against the in- vocabulary corrections. It also applies an additional cost for correcting away from an already valid word, like the “far” to “fat” example. This reflects the higher user annoyance whenever the keyboard “auto-corrupts” an already correctly-typed word."
    }, {
      "heading" : "4.2. Gesture Typing",
      "text" : "For gesture typing, the interaction model typically assumes each sliding stroke corresponds to a single word, and space between words are inserted automatically if omitted. This type of input can be especially challenging because the finger often slides past many keys while in-transit to the actual intended letter. For example, on a typical QWERTY keyboard layout, the words “pit”, “pot”, “put” all have the same canonical gesture pattern (a straight line from P to T).\nThe same FST decoding framework used for tap typing also works naturally for gesture typing. Here, each input frame is interpreted as either in-transit to a key (e.g., the “a b” label in Figure 2A) or aligned to a key (e.g., the “b” label). The in-transit label is analogous to an insertion in tap input, where the input can be consumed in a self-loop without advancing the search state."
    }, {
      "heading" : "4.3. Literals Decoding",
      "text" : "In order to include the literal decoding of tap input in the hypotheses returned by the keyboard decoder, the decoder graph must contain paths for arbitrary key sequences. The lexicon includes for each key an entry with a corresponding “literal word” on the output side. In contrast to regular words, the lexicon does not permit a space character between these literal word symbols. The input symbols for the literals words in the lexicon differ from regular key symbols, such that the spatial model can assign only that “literal key” symbol a non zero probability when it was actually tapped.\nThe LM FST contains a subgraph for the literal words grammar, which assigns weights to sequences of literal words. It is connected to the unigram state of the model. The grammar also inserts a marker token at the end of the literal sequence. This marker enforces a space tap to separate it from the next word (or the end of the input).\nIn a post-processing step after decoding, sequences of literal words in the decoder result are combined to form word symbols in the output."
    }, {
      "heading" : "4.4. Word Completions and Predictions",
      "text" : "For word completion prediction, the decoder computes the most likely extensions for each of the currently hypothesized word prefixes in their respective sentence contexts. The decoder keeps as hypotheses a set of states in the decoder graph along with their score (and a traceback pointer). The states in the decoder graph correspond to a tuple of states in the lexicon, the LM FST, and the composition filters.\nFrom a lexicon state, we can compute the set of reachable word (output) labels. In fact, the label look-ahead composition filter requires that information as well and it is therefore available as precomputed interval set [11]. Due to minimization of the lexicon FST and label pushing, the output labels may appear already on an arc before the last key of a word (for example, see states 2 and 3 in Figure 1). Therefore, the word completion procedure must not only look forward in the graph but also backwards in the traceback data and the label look-ahead composition filter state.\nThe reachable words are looked up in the LM at the state obtained from the corresponding hypothesis’ decoder graph state. Note that parts of the LM score may already be incorporated in the hypothesis score, due to on-the-fly weight pushing. The LM probability is combined with the hypothesis score to obtain a score for the word completion. We collect only a small set of the n-best unique words.\nThe next word is predicted from the current best (partial) sentence. We find the highest order n-gram state for the sentence prefix and collect the set of n-best unique output labels from that state, including paths over backoff transitions. The arcs in the LM FST are sorted by label, not by score, for efficient composition. Hence, the next word prediction must traverse all arcs of a state. Caching must be used to decrease the high computational cost for states with large outdegree, in particular the unigram state.\nThe integration of dynamic models (cf. Section 5.1) and the prediction of next words and word completions from the dynamically added vocabulary requires additional lookups in these models."
    }, {
      "heading" : "4.5. Post-Corrections",
      "text" : "It can be difficult for the language model to determine if a word makes sense without seeing what comes after it. For example, there is nothing wrong with the word “food” at the start of a sentence, but if the user goes on to type “food luck”, there is a good chance that she actually meant “good luck” (especially since on the QWERTY layout the F key is adjacent to the G key).\nHowever, traditional smartphone keyboards treat the space bar as a hard commit, with no ability to change its mind about previously typed words. This is analogous to the early state of speech recognition, when most systems operated on isolated words. By adopting a modern continuous recognition architecture, the FST decoder enables a streaming interaction model where the keyboard can adjust its interpretation of previous words based on new evidence.\nThere are also new user interaction challenges in allowing the keyboard to modify previously committed words. Users may be initially surprised by this unexpected behavior, and false-postcorrections could be especially frustrating (and time consuming to fix). One approach to mitigating these risks is to make postcorrections more conservative. E.g., by permitting them only within a small temporal window of the most recently typed word, and only when the decoder has a very high confidence in the replacement."
    }, {
      "heading" : "5. FST DECODER AND COMPLEX FEATURE NEEDS",
      "text" : "Perhaps one of the most powerful aspects of the FST framework for keyboard input is the ease with which dynamic models can be exploited for personalization and contextualization. This is especially important with on-device decoding, where personalization can compensate for the limited size of the models: one rarely needs more than 100K words to express themselves in a given language, but they may need the right 100K words."
    }, {
      "heading" : "5.1. Dynamic Models",
      "text" : "Dynamic models can be used to accumulate n-grams the user has previously typed, or information such as their contact list, or other contextual information. These models are constantly updated, so it would be challenging and costly to perform such updates directly on the decoder graph. Instead, they are incorporated in the decoding process using an on-the-fly lattice rescoring technique, similar to the n-gram biasing approach described in [13].\nThe vocabulary of the dynamic LM can contain words which are not covered by the main LM and are therefore not part of the lexicon FST. We integrate these OOV words in the decoder graph by splicing a character to word transducer to the LM FST, connected to the unigram state. This FST has “character words” on the input side and regular words on the output side. The lexicon FST has arcs that map these character word sequences to the corresponding (bi-) key sequences."
    }, {
      "heading" : "6. EVALUATION",
      "text" : "We evaluate performance on a dataset of tapped and gestured sentences (over 3000 words total for each input modality). In the data collection study, participants tapped and gestured English sentences on a smartphone QWERTY keyboard. The prompts were sentences sampled from transcribed speech interactions. Since we wanted to collect natural errors to test the correction capability of the decoder, participants were asked to type quickly and not worry about fixing any mistakes.\nThe results below show the performance of three keyboard decoders. A Baseline system without label look-ahead or postcorrection (p.c.), the proposed FST decoder without p.c., and the same FST decoder with p.c. enabled.\nTable 1 shows that the proposed FST decoder is able to outperform the baseline for both tap and gesture input. The relative reduction in WER becomes even greater when post-correction is enabled. The example from Table 2 shows this in action. By taking into account the following word, the p.c. decoder was able to revise the erroneous “a while lot” to the correct “a whole lot”."
    }, {
      "heading" : "7. CONCLUSION",
      "text" : "We showed how the FST framework developed over the last decade for speech recognition can be ported to the world of mobile keyboard input, from the basic decoding of tapped key sequences to more advanced features such as next word prediction. Our experiments so far show that an FST decoder brings strong accuracy advantages over a more traditional decoder, that it can elegantly scale to the challenges of internationalization, and that it can readily exploit on-device data to personalize the user’s typing experience."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Johan Schalkwyk, Doug Beeferman, Françoise Beaufays, Bill Byrne, Ciprian Chelba, Mike Cohen, Maryam Kamvar, and Brian Strope, “Your word is my command: Google search by voice: A case study,” in Advances in Speech Recognition, pp. 61–90. Springer, 2010.\n[2] Daniel Keysers, Thomas Deselaers, Henry A Rowley, Li-Lun Wang, and Victor Carbune, “Multi-language online handwriting recognition,” 2016.\n[3] Nils Klarlund and Michael Riley, “Word n-grams for cluster keyboards,” in Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods. Association for Computational Linguistics, 2003, pp. 51–58.\n[4] Ahmed Hassan, Sara Noeman, and Hany Hassan, “Language independent text correction using finite state automata,” IJCNLP. Hyderabad, India, 2008.\n[5] Shumin Zhai, Per Ola Kristensson, Pengjun Gong, Michael Greiner, Shilei Allen Peng, Liang Mico Liu, and Anthony Dunnigan, “Shapewriter on the iphone: from the laboratory to the real world,” in CHI’09 Extended Abstracts on Human Factors in Computing Systems. ACM, 2009, pp. 2667–2670.\n[6] Mehryar Mohri, Fernando Pereira, and Michael Riley, “Speech recognition with weighted finite-state transducers,” in Handbook of Speech Processing, Jacob Benesty, M. Sondhi, and Yiteng Huang, Eds., chapter 28, pp. 559–582. Springer, 2008.\n[7] Shyan Reyal, Shumin Zhai, and Per Ola Kristensson, “Performance and user experience of touchscreen and gesture keyboards ina lab setting and in the wild,” in CHI’15 Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems.\n[8] Ian McGraw, Rohit Prabhavalkar, Raziel Alvarez, Montse Gonzalez Arenas, Kanishka Rao, David Rybach, Ouais Alsharif, Alexander Gruenstein, Françoise Beaufays, and Carolina Parada, “Personalized speech recognition on mobile devices,” in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016, pp. 5955–5959.\n[9] Philip Quinn and Zhai Shumin, “Modelling gesture typing movements,” in Human-Computer Interaction. Taylor & Francis Online, 2016.\n[10] Ouais Alsharif, Tom Ouyang, Françoise Beaufays, Shumin Zhai, Thomas Breuel, and Johan Schalkwyk, “Long short term memory neural network for keyboard gesture decoding,” in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 2076–2080.\n[11] Cyril Allauzen, Michael Riley, and Johan Schalkwyk, “A generalized composition algorithm for weighted finite-state transducers,” in Interspeech, 2009, pp. 1203–1206.\n[12] Shiri Azenkot and Shumin Zhai, “Touch behavior with different postures on soft smartphone keyboards,” in Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services, New York, NY, USA, 2012, MobileHCI ’12, pp. 251–260, ACM.\n[13] Keith Hall, Eunjoon Cho, Cyril Allauzen, Françoise Beaufays, Noah Coccaro, Kaisuke Nakajima, Michael Riley, Brian Roark, David Rybach, and Linda Zhang, “Composition-based on-the-fly rescoring for salient n-gram biasing,” in Interspeech, 2015, pp. 1418–1422."
    } ],
    "references" : [ {
      "title" : "Your word is my command: Google search by voice: A case study",
      "author" : [ "Johan Schalkwyk", "Doug Beeferman", "Françoise Beaufays", "Bill Byrne", "Ciprian Chelba", "Mike Cohen", "Maryam Kamvar", "Brian Strope" ],
      "venue" : "Advances in Speech Recognition, pp. 61–90. Springer, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-language online handwriting recognition",
      "author" : [ "Daniel Keysers", "Thomas Deselaers", "Henry A Rowley", "Li-Lun Wang", "Victor Carbune" ],
      "venue" : "2016.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Word n-grams for cluster keyboards",
      "author" : [ "Nils Klarlund", "Michael Riley" ],
      "venue" : "Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods. Association for Computational Linguistics, 2003, pp. 51–58.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Language independent text correction using finite state automata",
      "author" : [ "Ahmed Hassan", "Sara Noeman", "Hany Hassan" ],
      "venue" : "IJC- NLP. Hyderabad, India, 2008.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Shapewriter on the iphone: from the laboratory to the real world",
      "author" : [ "Shumin Zhai", "Per Ola Kristensson", "Pengjun Gong", "Michael Greiner", "Shilei Allen Peng", "Liang Mico Liu", "Anthony Dunnigan" ],
      "venue" : "CHI’09 Extended Abstracts on Human Factors in Computing Systems. ACM, 2009, pp. 2667–2670.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Speech recognition with weighted finite-state transducers",
      "author" : [ "Mehryar Mohri", "Fernando Pereira", "Michael Riley" ],
      "venue" : "Handbook of Speech Processing, Jacob Benesty, M. Sondhi, and Yiteng Huang, Eds., chapter 28, pp. 559–582. Springer, 2008.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Performance and user experience of touchscreen and gesture keyboards ina lab setting and in the wild",
      "author" : [ "Shyan Reyal", "Shumin Zhai", "Per Ola Kristensson" ],
      "venue" : "CHI’15 Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Personalized speech recognition on mobile devices",
      "author" : [ "Ian McGraw", "Rohit Prabhavalkar", "Raziel Alvarez", "Montse Gonzalez Arenas", "Kanishka Rao", "David Rybach", "Ouais Alsharif", "Alexander Gruenstein", "Françoise Beaufays", "Carolina Parada" ],
      "venue" : "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016, pp. 5955–5959.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Modelling gesture typing movements",
      "author" : [ "Philip Quinn", "Zhai Shumin" ],
      "venue" : "Human-Computer Interaction. Taylor & Francis Online, 2016.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Long short term memory neural network for keyboard gesture decoding",
      "author" : [ "Ouais Alsharif", "Tom Ouyang", "Françoise Beaufays", "Shumin Zhai", "Thomas Breuel", "Johan Schalkwyk" ],
      "venue" : "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 2076–2080.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A generalized composition algorithm for weighted finite-state transducers",
      "author" : [ "Cyril Allauzen", "Michael Riley", "Johan Schalkwyk" ],
      "venue" : "Interspeech, 2009, pp. 1203–1206.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Touch behavior with different postures on soft smartphone keyboards",
      "author" : [ "Shiri Azenkot", "Shumin Zhai" ],
      "venue" : "Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services, New York, NY, USA, 2012, MobileHCI ’12, pp. 251–260, ACM.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Composition-based on-the-fly rescoring for salient n-gram biasing",
      "author" : [ "Keith Hall", "Eunjoon Cho", "Cyril Allauzen", "Françoise Beaufays", "Noah Coccaro", "Kaisuke Nakajima", "Michael Riley", "Brian Roark", "David Rybach", "Linda Zhang" ],
      "venue" : "Interspeech, 2015, pp. 1418–1422.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Speech recognition for example has flourished in the last few years, mostly fueled by the need for convenient mobile input methods [1].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "Likewise, handwriting recognition has gained more traction, especially in languages with complex scripts such as Chinese and Indic languages [2].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "The use of FSTs in the context of keyboard input is not totally new: a report by Klarlund and Riley suggested using FSTs to disambiguate entries on a hardware cluster keyboard, but this was before the invention of smart phones, so the issue of decoding (soft) noisy input sequences was not addressed [3].",
      "startOffset" : 300,
      "endOffset" : 303
    }, {
      "referenceID" : 3,
      "context" : "FSTs have also been used for similar tasks like spelling correction [4], though not in the context of mobile input.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : ", Shapewriter [5], Swype, Swiftkey, etc.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "General background on FST decoding for speech can be found in [6].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "While gesture input is easily 10 to 20% faster than tap input [7], relatively few users rely primarily on gesture input.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "This places keyboard somewhere in between our server-based speech recognizer which relies on triphone models and our embedded recognizer which uses monophones [8].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 8,
      "context" : "Gesture inputs instead are often modeled with the so-called “minimum-jerk model” that imposes smoothness maximization constraints on the input trajectory [9].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "Alternatively, a recurrent neural network model can be used [10].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "64K words [8].",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : "For memory efficiency, we use the on-the-fly composition of (C ◦L)◦G using look-ahead composition filters [11].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "Text entry on a mobile touch-screen device is a very error-prone process, with per-letter error rates around 8-9% [12].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "In fact, the label look-ahead composition filter requires that information as well and it is therefore available as precomputed interval set [11].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "Instead, they are incorporated in the decoding process using an on-the-fly lattice rescoring technique, similar to the n-gram biasing approach described in [13].",
      "startOffset" : 156,
      "endOffset" : 160
    } ],
    "year" : 2017,
    "abstractText" : "We propose a finite-state transducer (FST) representation for the models used to decode keyboard inputs on mobile devices. Drawing from learnings from the field of speech recognition, we describe a decoding framework that can satisfy the strict memory and latency constraints of keyboard input. We extend this framework to support functionalities typically not present in speech recognition, such as literal decoding, autocorrections, word completions, and next word predictions. We describe the general framework of what we call for short the keyboard “FST decoder” as well as the implementation details that are new compared to a speech FST decoder. We demonstrate that the FST decoder enables new UX features such as post-corrections. Finally, we sketch how this decoder can support advanced features such as personalization and contextualization.",
    "creator" : "LaTeX with hyperref package"
  }
}