{
  "name" : "1611.01259.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalized Topic Modeling",
    "authors" : [ "Avrim Blum", "Nika Haghtalab" ],
    "emails" : [ "avrim@cs.cmu.edu", "nhaghtal@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this work we consider a broad generalization of this framework in which words are no longer assumed to be drawn i.i.d. and instead a topic is a complex distribution over sequences of paragraphs. Since one could not hope to even represent such a distribution in general (even if paragraphs are given using some natural feature representation), we aim instead to directly learn a document classifier. That is, we aim to learn a predictor that given a new document, accurately predicts its topic mixture, without learning the distributions explicitly. We present several natural conditions under which one can do this efficiently and discuss issues such as noise tolerance and sample complexity in this model. More generally, our model can be viewed as a generalization of the multi-view or co-training setting in machine learning.\n∗Supported in part by National Science Foundation grants CCF-1525971 and CCF-1535967. †Supported in part by National Science Foundation grant CCF-1525971 and by a Microsoft Research Graduate Fellowship and\nan IBM Ph.D Fellowship.\nar X\niv :1\n61 1.\n01 25\n9v 1\n[ cs\n.L G\n] 4\nN ov\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "Topic modeling is an area with significant recent work in the intersection of algorithms and machine learning (Arora et al., 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures.\nAlgorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999).\nAs a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here.\nNote that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents—that is, a function that given a document, predicts its mixture over topics—without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D− over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is.\nWe begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al. (2004); Sun (2013). We then describe several natural assumptions under which we can indeed efficiently solve the problem, learning accurate topic mixture predictors."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We assume that paragraphs are described by n real-valued features and so can be viewed as points x in an instance space X ⊆ Rn. We assume that each document consists of at least two paragraphs and denote it\nby (x1,x2). Furthermore, we consider k topics and partial membership functions f1, . . . , fk : X → [0, 1], such that fi(x) determines the degree to which paragraph x belongs to topic i, and, ∑k i=1 fi(x) = 1. For any vector of probabilities w ∈ Rk — which we sometimes refer to as mixture weights — we define Xw = {x ∈ Rn | ∀i, fi(x) = wi} to be the set of all paragraphs with partial membership values w. We assume that both paragraphs of a document have the same partial membership values, that is (x1,x2) ∈ ⋃ w Xw ×Xw, although we also allow some noise later on. To better relate to the literature on multi-view learning, we will also refer to topics as “classes” and refer to paragraphs as “views” of the document.\nMuch like the standard topic models, we consider an unlabeled sample set that is generated by a twostep process. First, we consider a distribution P over vectors of mixture weights and draw w according to P . Then we consider distribution Dw over the set Xw × Xw and draw a document (x1,x2) according to Dw. We consider two settings. In the first setting, which is addressed in Section 3, the learner receives the instance (x1,x2). In the second setting, the learner receives samples (x̂1, x̂2) that have been perturbed by some noise. We discuss two noise models in Sections 4 and 5.2. In both cases, the goal of the learner is to recover the partial membership functions fi.\nMore specifically, in this work we consider partial membership functions of the form fi(x) = f(vi ·x), where v1, . . . ,vk ∈ Rn are linearly independent and f : R → [0, 1] is a monotonic function. For the majority of this work, we consider f to be the identity function, so that fi(x) = vi · x. Define ai ∈ span{v1, . . . ,vk} such that vi·ai = 1 and vj ·ai = 0 for all j 6= i. That is, ai can be viewed as the projection of a paragraph that is purely of topic i onto the span of v1, . . . ,vk. Define ∆ = CH({a1, . . . ,ak}) to be the convex hull of a1, . . . ,ak.\nThroughout this work, we use ‖ · ‖2 to denote the spectral norm of a matrix or the L2 norm of a vector. When it is clear from the context, we simply use ‖ · ‖ to denote these quantities. We denote by Br(x) the ball of radius r around x. For any matrix M , we use M+ to denote the pseudoinverse of M .\nGeneralization of Standard Topic Modeling Let us briefly discuss how the above model is a generalization of the standard topic modeling framework. In the standard framework, a topic is modeled as a probability distribution over n words, expressed as a vector ai ∈ [0, 1]n, where aij is the probability of word j in topic i. A document is generated by first selecting a mixture w ∈ [0, 1]k over k topics, and then choosing words i.i.d. from the associated mixture distribution∑k\ni=1wiai. The document vector x̂ is then the vector of word counts, normalized by dividing by the number of words in the document so that the L1 norm of x̂ is 1.\nAs a thought experiment, consider infinitely long documents. In the standard framework, all infinitely long documents of a mixture weight w have the same representation x = ∑k i=1wiai. This representation implies x·vi = wi for all i ∈ [k], where V = (v1, . . . ,vk) is the pseudo-inverse of matrixA = (a1, . . . ,ak). Thus, by partitioning the document into two halves (views) x1 and x2, our noise-free model with fi(x) = vi · x generalizes the standard topic model for long documents. However, our model is substantially more general: features within a view can be arbitrarily correlated, the views themselves can be correlated with each other, and even in the zero-noise case, documents of the same mixture can look very different so long as they have the same projection to the span of the a1, . . . ,ak.\nFor a shorter document x̂, each feature x̂i is drawn according to a distribution with mean xi, where x = ∑k i=1wiai. Therefore, x̂ can be thought of as a noisy measurement of x. The fewer the words in a document, the larger is the noise in x̂. Existing work in topic modeling, such as Arora et al. (2012b); Anandkumar et al. (2014), provide elegant procedures for handling large noise that is caused by drawing only 2 or 3 words according to the distribution induced by x. As we show in Section 4, our method can also tolerate large amounts of noise under some conditions. While our method cannot deal with documents that are only 2- or 3-words long, the benefit is a model that is much more general in many other respects."
    }, {
      "heading" : "3 An Easier Case with Simplifying Assumptions",
      "text" : "We make two main simplifying assumptions in this section, both of which will be relaxed in Section 4: 1) The documents are not noisy, i.e., x1 · vi = x2 · vi; 2) There is non-negligible probability density on instances that belong purely to one class. In this section we demonstrate ideas and techniques, which we will develop further in the next section, to learn the topic vectors from a corpus of unlabeled documents.\nThe Setting: We make the following assumptions. The documents are not noisy, that is for any document (x1,x2) and for all i ∈ [k], x1 · vi = x2 · vi. Regarding distribution P , we assume that a non-negligible probability density is assigned to pure samples for each class. More formally, for some ξ > 0, for all i ∈ [k], Prw∼P [w = ei] ≥ ξ. Regarding distribution Dw, we allow the two paragraphs in a document, i.e., the two views (x1,x2) drawn from Dw, to be correlated as long as for any subspace Z ⊂ null{v1 . . . ,vk} of dimension strictly less than n− k, Pr(x1,x2)∼Dw [(x1 − x2) 6∈ Z] ≥ ζ for some non-negligible ζ. One way to view this in the context of topic modeling is that if, say, “sports” is a topic, then it should not be the case that the second paragraph always talks about the exact same sport as the first paragraph; else “sports” would really be a union of several separate but closely-related topics. Thus, while we do not require independence we do require some non-correlation between the paragraphs.\nAlgorithm and Analysis: The main idea behind our approach is to use the consistency of the two views of the samples to first recover the subspace spanned by v1, . . . ,vk (Phase 1). Once this subspace is recovered, we show that a projection of a sample on this space corresponds to the convex combination of class vectors using the appropriate mixture weight that was used for that sample. Therefore, we find vectors a1, . . . ,ak that purely belong to each class by taking the extreme points of the projected samples (Phase 2). The class vectors v1, . . . ,vk are the unique vectors (up to permutations) that classify a1, . . . ,ak as pure samples. Phase 2 is similar to that of Arora et al. (2012b). Algorithm 1 formalizes the details of this approach.\nAlgorithm 1 ALGORITHM FOR GENERALIZED TOPIC MODELS — NO NOISE Input: A sample set S = {(x1i ,x2i ) | i ∈ [m]} such that for each i, first a vector w is drawn from P and then (x1i ,x 2 i ) is drawn from Dw. Phase 1: 1. Let X1 and X2 be matrices where the ith column is x1i and x 2 i , respectively.\n2. Let P be the projection matrix on the last k left singular vectors of (X1 −X2). Phase 2:\n1. Let S = {Pxji | i ∈ [m], j ∈ {1, 2}}. 2. Let A be a matrix whose columns are the extreme points of the convex hull of S . (This can be found\nusing farthest traversal or linear programming.) Output: Return columns of A+ as v1, . . . ,vk.\nIn Phase 1 for recovering span{v1, . . . ,vk}, note that for any sample (x1,x2) drawn fromDw, we have that vi · x1 = vi · x2 = wi. Therefore, regardless of what w was used to produce the sample, we have that vi · (x1 − x2) = 0 for all i ∈ [k]. That is, v1, . . . ,vk are in the null-space of all such (x1 − x2). So, if samples (x1i − x2i ) span a n− k dimensional subspace, then span{v1, . . . ,vk} can be recovered by taking null{(x1 − x2) | (x1,x2) ∈ Xw × Xw, ∀w ∈ Rk}. Using singular value decomposition, this null space is spanned by the last k singular vectors of X1 −X2, where X1 and X2 are matrices with columns x1i and x2i , respectively.\nThis is where the assumptions on Dw come into play. By assumption, for any strict subspace Z of span{(x1 − x2) | (x1,x2) ∈ Xw × Xw, ∀w ∈ Rk}, Dw has non-negligible probability on instances (x1−x2) /∈ Z. Therefore, after seeing sufficiently many samples we can recover the space of all (x1−x2). The next lemma, whose proof appears in Appendix A.1, formalizes this discussion.\nLemma 3.1. Let Z = span{(x1i − x2i ) | i ∈ [m]}. Then, m = O(n−kζ log( 1 δ )) is sufficient such that with probability 1− δ, rank(Z) = n− k.\nUsing Lemma 3.1, Phase 1 of Algorithm 1 recovers span{v1, . . . ,vk}. Next, we show that pure samples are the extreme points of the convex hull of all samples when projected on the subspace span{v1, . . . ,vk}. Figure 1 demonstrates the relation between the class vectors, vi, projection of samples, and the projection of pure samples ai. The next lemma, whose proof appears in Appendix A.2, formalizes this claim.\nLemma 3.2. For any x, let x represent the projection of x on span{v1, . . . ,vk}. Then, x = ∑ i∈[k](vi · x)ai.\nWith ∑\ni∈[k](vi·x)ai representing the projection of x on span{v1, . . . ,vk}, it is clear that the extreme points of the set of all projected instances that belong to Xw for all w are a1, . . . ,ak. Since in a large enough sample set, with high probability for all i ∈ [k], there is a pure sample of type i, taking the extreme points of the set of projected samples is also a1, . . . ,ak. The following lemma, whose proof appears in Appendix A.3, formalizes this discussion.\nLemma 3.3. Let m = c(1ξ log( k δ )) for a large enough constant c > 0. Let P be the projection matrix for span{v1, . . . ,vk} and S = {Pxji | i ∈ [m], j ∈ {1, 2}} be the set of projected samples. With probability 1− δ, {a1, . . . ,ak} is the set of extreme points of CH(S ).\nTherefore, a1, . . . ,ak can be learned by taking the extreme points of the convex hull of all samples projected on span({v1, . . . ,vk}). Furthermore, V = A+ is unique, therefore v1, . . . ,vk can be easily found by taking the pseudoinverse of matrix A. Together with Lemma 3.1 and 3.3 this proves the next theorem regarding learning class vectors in the absence of noise.\nTheorem 3.4 (No Noise). There is a polynomial time algorithm for which m = O ( n−k ζ ln( 1 δ ) + 1 ξ ln( k δ ) ) is sufficient to recover vi exactly for all i ∈ [k], with probability 1− δ."
    }, {
      "heading" : "4 Relaxing the Assumptions",
      "text" : "In this section, we relax the two main simplifying assumptions from Section 3. We relax the assumption on non-noisy documents and allow a large fraction of the documents to not satisfy vi · x1 = vi · x2. In the standard topic model, this corresponds to having a large fraction of short documents. Furthermore, we relax the assumption on the existence of pure documents to an assumption on the existence of “almostpure” documents. We further develop the approach discussed in the previous section and introduce efficient algorithms that approximately recover the topic vectors in this setting. The Setting: We assume that any sampled document has a non-negligible probability of being non-noisy and with the remaining probability, the two views of the document are perturbed by additive Gaussian noise, independently. More formally, for a given sample (x1,x2), with probability p0 > 0 the algorithm receives (x1,x2) and with the remaining probability 1− p0, the algorithm receives (x̂1, x̂2), such that x̂j = xj + ej , where ej ∼ N (0, σ2In).\nWe assume that for each topic the probability that a document is mostly about that topic is non-negligible. More formally, for any topic i ∈ [k], Prw∼P [‖ei −w‖1 ≤ ‖] ≥ g( ), where g is a polynomial function of its input. A stronger form of this assumption, better known as the dominant admixture assumption, assumes that every document is mostly about one topic and has been empirically shown to hold on several real world\ndata sets (Bansal et al., 2014). Furthermore, in the Latent Dirichlet Allocation model, Prw∼P [maxi∈[k]wi ≥ 1− ] ≥ O( 2) for typical values of the concentration parameter.\nWe also make mild assumptions on the distribution over instances. We assume that the covariance of the distribution over (x1i − x2i )(x1i − x2i )> is significantly larger than the noise covariance σ2. That is, for some δ0 > 0, the least significant non-zero eigen value of E(x1i ,x2i )[(x 1 i − x2i )(x1i − x2i )>], equivalently its (n− k)th eigen value, is greater than 6σ2 + δ0. At a high level, these assumptions are necessary, because if ‖x1i − x2i ‖ is too small compared to ‖x1i ‖ and ‖x2i ‖, then even a small amount of noise affects the structure present in x1i − x2i completely. Moreover, we assume that the L2 norm of each view of a sample is bounded by some M > 0. We also assume that for all i ∈ [k], ‖ai‖ ≤ α for some α > 0. At a high level, ‖ai‖s are inversely proportional to the non-zero singular values of V = (v1, . . . ,vk). Therefore, ‖ai‖ ≤ α implies that the k topic vectors are sufficiently different.\nAlgorithm and Results: Our approach follows the general theme of the previous section: First, recover span{v1, . . . ,vk} and then recover a1, . . . ,ak by taking the extreme points of the projected samples. In this case, in the first phase we recover span{v1, . . . ,vk} approximately, by finding a projection matrix P̂ such that ‖P − P̂‖ ≤ for an arbitrarily small , where P is the projection matrix on span{v1, . . . ,vk}. At this point in the algorithm, the projection of samples on P̂ can include points that are arbitrarily far from ∆. This is due to the fact that the noisy samples are perturbed by N (0, σ2In), so, for large values of σ some noisy samples map to points that are quite far from ∆. Therefore, we have to detect and remove these samples before continuing to the second phase. For this purpose, we show that the low density regions of the projected samples can safely be removed such that the convex hull of the remaining points is close to ∆. In the second phase, we consider projections of each sample using P̂ . To approximately recover a1, . . . ,ak, we recover samples, x, that are far from the convex hull of the remaining points, when x and a ball of points close to it are removed. We then show that such points are close to one of the pure class vectors, ai. Algorithm 2 and the details of the above approach and its performance are as follows.\nAlgorithm 2 ALGORITHM FOR GENERALIZED TOPIC MODELS — WITH NOISE Input: A sample set {(x̂1i , x̂2i ) | i ∈ [m]} such that for each i, first a vector w is drawn from P , then (x1i ,x2i ) is drawn from Dw, then with probability p0, x̂ji = x j i , else with probability 1− p0, x̂ j i = x j i +N (0, σ2In) for i ∈ [m] and j ∈ {1, 2}. Phase 1:\n1. Take m1 = Ω ( n−k ζ ln( 1 δ ) + nσ4r2M2\nδ20 2 ln(\n1 δ ) +\nnσ2M4r2\nδ20 2 polylog(\nnrM δ ) +\nM4\nδ20 ln(nδ )\n) samples.\n2. Let X̂1 and X̂2 be matrices where the ith column is x̂1i and x̂ 2 i , respectively.\n3. Let P̂ be the projection matrix on the last k left singular vectors of X̂1 − X̂2. Denoising Phase:\n4. Let ′ = 8r and γ = g ( ′ 8kα ) .\n5. Take m2 = Ω (\nk p0γ ln 1δ\n) fresh samples1 and let Ŝ = { P̂ x̂1i | ∀i ∈ [m2] } .\n6. Remove x̂ from Ŝ , for which there are less than p0γm2/2 points within distance of ′\n2 in Ŝ . Phase 2:\n6. For all x̂ in Ŝ , if dist(x ,CH(Ŝ \\B6r ′(x̂)) ≥ 2 ′ add x̂ to C. 7. Cluster C using single linkage with threshold 16r ′. Assign any point from cluster i as âi.\nOutput: Return â1, . . . , âk.\n1For the denoising step, we use a fresh set of samples that were not used for learning the projection matrix. This guarantees that the noise distribution in the projected samples remain a Gaussian.\nTheorem 4.1. Consider any , δ > 0 such that ≤ O ( rσ √ k )\n, where r is a parameter that depends on the geometry of the simplex a1, . . . ,ak and will be defined later. There is an efficient algorithm for which an unlabeled sample set of size\nm = O ( n− k ζ ln( 1 δ ) + nσ4r2M2\nδ20 2\nln( 1\nδ ) +\nnσ2M4r2\nδ20 2\npolylog( nrM\nδ ) +\nM4\nδ20 ln(\nn δ ) + k ln(1/δ)\np0 g( /(krα)) ) is sufficient to recover âi such that ‖âi − ai‖2 ≤ for all i ∈ [k], with probability 1− δ.\nThe proof of Theorem 4.1 involves the next three lemmas on the performance of the phases of the above algorithm. We formally state these two lemmas here, but defer their proofs to Sections 4.1, 4.2 and 4.3.\nLemma 4.2 (Phase 1). For any σ > 0 and > 0, an unlabeled sample set of size\nm = O ( n− k ζ ln( 1 δ ) + nσ4\nδ20 2\nln( 1\nδ ) +\nnσ2M2\nδ20 2\npolylog( n\nδ ) +\nM4\nδ20 ln(\nn δ )\n) .\nis sufficient, such that with probability 1 − δ, Phase 1 of Algorithm 2 returns a projection matrix P̂ , such that ‖P − P̂‖2 ≤ .\nLemma 4.3 (Denoising). Let ′ ≤ 13σ √ k, ‖P − P̂‖ ≤ ′/8M , and γ = g\n( ′\n8kα\n) . An unlabeled sample\nsize of m = O (\nk p0γ ln(1δ ) ) is sufficient such that for Ŝ defined in Step 6 of Algorithm 2 the following holds\nwith probability 1− δ: For any x ∈ Ŝ , dist(x,∆) ≤ ′, and, for all i ∈ [k], there exists âi ∈ Ŝ such that ‖âi − ai‖ ≤ ′.\nLemma 4.4 (Phase 2). Let Ŝ be a set of points for which the conclusion of Lemma 4.3 holds with the value of ′ = /8r. Then, Phase 2 of Algorithm 2 returns â1, . . . , âk such that for all i ∈ [k], ‖ai − âi‖ ≤ .\nWe now prove our main Theorem 4.1 by directly leveraging the three lemmas we just stated.\nProof of Theorem 4.1. By Lemma 4.2, sample set of size m1 is sufficient such that Phase 1 of Algorithm 2 leads to ‖P − P̂‖ ≤ 32Mr , with probability 1 − δ/2. Let\n′ = 8r and take a fresh sample of size m2. By Lemma 4.3, with probability 1 − δ/2, for any x ∈ Ŝ , dist(x,∆) ≤ ′, and, for all i ∈ [k], there exists âi ∈ Ŝ such that ‖âi− ai‖ ≤ ′. Finally, applying Lemma 4.4 we have that Phase 2 of Algorithm 2 returns âi, such that for all i ∈ [k], ‖ai − âi‖ ≤ .\nTheorem 4.1 discusses the approximation of ai for all i ∈ [k]. It is not hard to see that such an approximation also translates to the approximation of class vectors, vi for all i ∈ [k]. That is, using the properties of perturbation of pseudoinverse matrices (see Proposition B.5) one can show that ‖Â+− V ‖ ≤ O(‖Â−A‖). Therefore, V̂ = Â+ is a good approximation for V ."
    }, {
      "heading" : "4.1 Proof of Lemma 4.2 — Phase 1",
      "text" : "For j ∈ {1, 2}, let Xj and X̂j be n × m matrices with the ith column being xji and x̂ j i , respectively. As we demonstrated in Lemma 3.1, with high probability rank(X1 − X2) = n − k. Note that the nullspace of columns of X1 − X2 is spanned by the left singular vectors of X1 − X2 that correspond to its k zero singular values. Similarly, consider the space spanned by the k least left singular vectors of X̂1 − X̂2. We show that the nullspace of columns of X1 −X2 can be approximated within any desirable accuracy by the space spanned by the k least left singular vectors of X̂1− X̂2, given a sufficiently large number of samples.\nLet D = X1 − X2 and D̂ = X̂1 − X̂2. For ease of exposition, assume that all samples are perturbed by Gaussian noise N (0, σ2In).2 Since each view of a sample is perturbed by an independent draw from a Gaussian noise distribution, we can view D̂ = D + E, where each column of E is drawn i.i.d from distribution N (0, 2σ2In). Then, 1mD̂D̂ > = 1mDD > + 1mDE > + 1mED > + 1mEE\n>. As a thought experiment, consider this equation in expectation. Since E[ 1mEE\n>] = 2σ2In is the covariance matrix of the noise and E[DE> + ED>] = 0, we have\n1 m E [ D̂D̂> ] − 2σ2In = 1 m E [ DD> ] . (1)\nMoreover, the eigen vectors and their order are the same in 1mE[D̂D̂ >] and 1mE[D̂D̂ >]− 2σ2In. Therefore, one can recover the nullspace of 1mE[DD >] by taking the space of the least k eigen vectors of 1mE[D̂D̂ >]. Next, we show how to recover the nullspace using D̂D̂>, rather than E[D̂D̂>]. Assume that the following properties hold:\n1. Equation 1 holds not only in expectation, but also with high probability. That is, with high probability, ‖ 1mD̂D̂ > − 2σ2In − 1mDD >‖2 ≤ . 2. With high probability λn−k( 1mD̂D̂ >) > 4σ2 + δ0/2, where λi(·) denotes the ith most significant\neigen value. Let D = UΣV > and D̂ = Û Σ̂V̂ > be SVD representations. We have that 1mD̂D̂ > − 2σ2In = Û( 1m Σ̂ 2 − 2σ2In)Û >. By property 2, λn−k( 1m Σ̂\n2) > 4σ2 + δ0/2. That is, the eigen vectors and their order are the same in 1mD̂D̂ > − 2σ2In and 1mD̂D̂ >. As a result the projection matrix, P̂ , on the least k eigen vectors of 1 mD̂D̂ >, is the same as the projection matrix, Q, on the least k eigen vectors of 1mD̂D̂ > − 2σ2In.\nRecall that P̂ and P andQ are the projection matrices on the least significant k eigen vectors of 1mD̂D̂ >,\n1 mDD >, and 1mD̂D̂ > − 2σ2I , respectively. As we discussed, P̂ = Q. Now, using the Davis and Kahan (1970) or Wedin (1972) sin θ theorem (see Proposition B.1) from matrix perturbation theory, we have,\n‖P − P̂‖2 = ‖P −Q‖ ≤ ‖ 1mD̂D̂ > − 2σ2In − 1mDD >‖2∣∣∣λn−k( 1mD̂D̂>)− 2σ2 − λn−k+1( 1mDD>)∣∣∣ ≤ 2 δ0\nwhere we use Properties 1 and 2 and the fact that λn−k+1( 1mDD >) = 0, in the last transition."
    }, {
      "heading" : "4.1.1 Concentration",
      "text" : "It remains to prove Properties 1 and 2. We briefly describe our approach for obtaining concentration results and prove that when the number of samples m is large enough, with high probability ‖ 1mD̂D̂\n> − 2σ2In − 1 mDD >‖2 ≤ and λn−k( 1mD̂D̂ >) > 4σ2 + δ0/2.\nLet us first describe 1mD̂D̂ > − 2σ2In − 1mDD > in terms of the error matrices. We have\n1\nm D̂D̂> − 2σ2In −\n1\nm DD> =\n( 1\nm EE> − 2σ2In\n) + ( 1\nm DE> +\n1\nm ED>\n) . (2)\nIt suffices to show that for large enough m > m ,δ, Pr[‖ 1mEE > − 2σ2In‖2 ≥ ] ≤ δ and Pr[‖ 1mDE > + 1 mED >‖2 ≥ ] ≤ δ. In the former, note that 1mEE > is the sample covariance of the Gaussian noise matrix and 2σ2In is the true covariance matrix of the noise distribution. The next claim is a direct consequence of the convergence properties of sample covariance of the Gaussian distribution (see Proposition B.2).\n2The assumption that with a non-negligible probability a sample is non-noisy is not needed for the analysis and correctness of Phase 1 of Algorithm 2. This assumption only comes into play in the denoising phase.\nClaim 4.5. For m > nσ 4 2 log(1δ ), with probability 1− δ, ‖ 1 mEE > − 2σ2In‖2 ≤ . 3\nWe use the Matrix Bernstein inequality (Tropp, 2015), described in Appendix B, to demonstrate the concentration of ‖ 1mDE > + 1mED >‖2. The proof of the next Claim is relegated to Appendix C.1.\nClaim 4.6. m = O(nσ 2M2 2 polylog n δ ) is sufficient so that with probability 1−δ, ∥∥ 1 mDE > + 1mED >∥∥ 2 ≤ ,\nNext, we prove that λn−k( 1mD̂D̂ >) > 4σ2 + δ0/2. Since for any two matrices, the difference in λn−k\ncan be bounded by the spectral norm of their difference (see Proposition B.4), using Equation 2, we have∣∣∣∣λn−k ( 1mD̂D̂> ) − λn−k ( 1 m DD> )∣∣∣∣ ≤ ∥∥∥∥2σ2I + ( 1mEE> − 2σ2In ) − ( 1 m DE> + 1 m ED> )∥∥∥∥ ≤ 2σ2 + δ04 , where in the last transition we use Claims 4.5 and 4.6 with the value of δ0/8 to bound the last two terms by a total of δ0/4. Since λn−k(E[ 1mDD >]) ≥ 6σ2 + δ0, it is sufficient to show that |λn−k(E[ 1mDD >]) − λn−k([ 1 mDD >])| ≤ δ0/4. Similarly as before, this is bounded by ‖ 1mDD > − E[ 1mDD\n>]‖. We use the Matrix Bernstein inequality (Proposition B.3) to prove this concentration result. The rigorous proof of this claim appears in Appendix C.2,\nClaim 4.7. m = O ( M4\nδ20 log nδ\n) is sufficient so that with probability 1−δ, ∥∥ 1 mDD > − E [ 1 mDD >]∥∥ 2 ≤ δ04 .\nThis completes the analysis of Phase 1 of our algorithm and the proof of Lemma 4.2 follows directly from the above analysis and the application of Claims 4.5 and 4.6 with the error of δ0, and Claim 4.7."
    }, {
      "heading" : "4.2 Proof of Lemma 4.3 — Denoising Step",
      "text" : "Having approximately recovered a projection matrix P̂ for span{v1, . . . ,vk}, we can now use this subspace to partially denoise the samples while approximately preserving ∆ = CH({a1, . . . ,ak}). At a high level, when considering the projection of samples on P̂ , one can show that 1) the regions around ai have sufficiently high density, and, 2) the regions that are far from ∆ have low density.\nWe claim that if x̂ ∈ Ŝ is non-noisy and corresponds almost purely to one class then Ŝ also includes a non-negligible number of points within O( ′) distance of x̂ . This is due to the fact that a non-negligible number of points (about p0γm points) correspond to non-noisy and almost-pure samples that using P would get projected to points within a distance of O( ′) of each other. Furthermore, the inaccuracy in P̂ can only perturb the projections up to O( ′) distance. So, the projections of all non-noisy samples that are purely of class i fall within O( ′) of ai. The following lemma, whose proof appears in Appendix D.1, formalizes this claim.\nIn the following lemmas, let D denote the flattened distribution of the first paragraphs. That is, the distribution over x̂1 where we first take w ∼ P , then take (x1,x2) ∼ Dw, and finally take x̂1.\nClaim 4.8. For all i ∈ [k], Prx∼D [ P̂x ∈ B ′/4(ai) ] ≥ p0γ.\nOn the other hand, any projected point that is far from the convex hull of a1, . . . ,ak has to be noisy, and as a result, has been generated by a Gaussian distribution with variance σ2. For a choice of ′ that is small with respect to σ, such points do not concentrate well within any ball of radius ′. In the next lemma we show that the regions that are far from the convex hull have low density.\nClaim 4.9. For any z such that dist(z,∆) ≥ ′, we have Prx∼D [ P̂x ∈ B ′/2(z) ] ≤ p0γ4 .\n3At first sight, the dependence of this sample complexity on σ might appear unintuitive. But, note that even without seeing any samples we can approximate the noise covariance within 2σ2In. Therefore, if = 2σ2 our work is done.\nProof. We first show that B ′/2(z) does not include any non-noisy points. Take any non-noisy sample x. Note that Px = ∑k i=1wiai, where wi are the mixture weights corresponding to point x. We have,∥∥∥z− P̂x∥∥∥ = ∥∥∥∥∥z− k∑ i=1 wiai + (P − P̂ )x ∥∥∥∥∥ ≥ ∥∥∥∥∥z− k∑ i=1 wiai ∥∥∥∥∥− ‖P − P̂‖‖x‖ ≥ ′/2\nTherefore, B ′/2(z) only contains noisy points. Since noisy points are perturbed by a spherical Gaussian, the projection of these points on any k-dimensional subspace can be thought of points generated from a k-dimensional Gaussian distributions with variance σ2\nand potentially different centers. One can show that the densest ball of any radius is at the center of a Gaussian. Here, we prove a slightly weaker claim. Consider one such Gaussian distribution, N (0, σ2Ik). Note that the pdf of the Gaussian distribution decreases as we get farther from its center. By a coupling between the density of the points, B ′/2(0) has higher density than any B ′/2(c) with ‖c‖2 > ′. Therefore,\nsup c Pr x∼N (0,σ2Ik) [x ∈ B ′/2(c)] ≤ Pr x∼N (0,σ2Ik) [x ∈ B3 ′/2(0)].\nSo, over D this value will be maximized if the Gaussians had the same center (see Figure 2). Moreover, in N (0, σ2Ik), Pr[‖x‖2 ≤ σ √ k(1− t)] ≤ exp(−kt2/16). Since 3 ′/2 ≤ σ √ k/2 ≤ σ √ k(1− √ 16 k ln 4 p0γ )\nwe have Pr x̂∼D [x ∈ B ′/2(c)] ≤ Pr x∼N (0,σ2Ik) [‖x‖2 ≤ 3 ′/2] ≤ p0γ 4 .\nThe next claim shows that in a large sample set, the fraction of samples that fall within any of the described regions in Claims 4.8 and 4.9 is close to the density of that region. The proof of this claim follows from VC dimension of the set of balls.\nClaim 4.10. Let D be any distribution over Rk and x1, . . . ,xm be m points drawn i.i.d from D. Then m = O(kγ ln 1 δ ) is sufficient so that with probability 1 − δ, for any ball B ⊆ R\nk such that Prx∼D[x ∈ B] ≥ 2γ, |{xi | xi ∈ B}| > γm and for any ball B ⊆ Rk such that Prx∼D[x ∈ B] ≤ γ/2, |{xi | xi ∈ B}| < γm.\nTherefore, upon seeing Ω( kp0γ ln 1 δ ) samples, with probability 1 − δ, for all i ∈ [k] there are more than p0γm/2 projected points within distance ′/4 of ai (by Claims 4.8 and 4.10), and, no point that is ′ far from ∆ has more than p0γm/2 points in its ′/2-neighborhood (by Claims 4.9 and 4.10). Phase 2 of Algorithm 2 leverages these properties of the set of projected points for denoising the samples while preserving ∆: Remove any point from Ŝ that has fewer than p0γm/2 neighbors within distance ′/2.\nWe conclude the proof of Lemma 4.3 by noting that the remaining points in Ŝ are all within distance ′ of ∆. Furthermore, any point in B ′/4(ai) has more than p0γm/2 points within distance of ′/2. Therefore, such points remain in Ŝ and any one of them can serve as âi for which ‖ai − âi‖ ≤ ′/4."
    }, {
      "heading" : "4.3 Proof of Lemma 4.4 — Phase 2",
      "text" : "At a high level, we consider two balls around each projected sample point x̂ ∈ Ŝ with appropriate choice of radii r1 < r2 (see Figure 3a). Consider the set of projections Ŝ when points in Br2(x) are removed from it. For points that are far from all ai, this set still includes points that are close to ai for all topics i ∈ [k]. So,\nthe convex hull of Ŝ \\ Br2(x) is close to ∆, and in particular, intersects Br1(x). On the other hand, for x that is close to ai, Ŝ \\ Br2(x) does not include an extreme point of ∆ or points close to it. So, the convex hull of Ŝ \\Br2(x) is considerably smaller than ∆, and in particular, does not intersect Br1(x).\nThe geometry of the simplex and the angles between a1, . . . ,ak play an important role in choosing the appropriate r1 and r2. Note that when the samples are perturbed by noise, a1, . . . ,ak can only be approximately recovered if they are sufficiently far apart and the angles of the simplex at each ai is far from being flat. That is, we assume that for all i 6= j, ‖ai − aj‖ ≥ 3 . Furthermore, define r ≥ 1 to be the smallest value such that the distance between ai and CH(∆\\Br (ai)) is at least . Note that such a value of r always exists and depends entirely on the angles of the simplex defined by the class vectors. Therefore, the number of samples needed for our method depends on the value of r. The smaller the value of r, the larger is the separation between the topic vectors and the easier it is to identify them. See Figure 3b for a demonstration of this concept.\nClaim 4.11. Let ′ = /8r. Let Ŝ be the set of denoised projections, as in step 6 of Algorithm 2. For any x̂ ∈ Ŝ such that for all i, ‖x̂− ai‖ > 8r ′, dist(x̂,CH(Ŝ \\B6r ′(x̂))) ≤ 2 ′. Furthermore, for all i ∈ [k] there exists âi ∈ Ŝ such that ‖âi − ai‖ < ′ and dist(âi,CH(Ŝ \\B6r ′(âi))) > 2 ′.\nProof. Recall that by Lemma 4.3, for any x̂ ∈ Ŝ there exists x ∈ ∆ such that ‖x̂ − x‖ ≤ ′ and for all i ∈ [k], there exists âi ∈ Ŝ such that ‖âi − ai‖ ≤ ′. For the first part, let x = ∑ i αiai ∈ ∆ be the corresponding point to x̂, where αi’s are the coefficients of the convex combination. Furthermore, let x′ = ∑ i αiâi. We have,\n‖x′ − x̂‖ ≤ ∥∥∥∥∥ k∑ i=1 αiâi − k∑ i=1 αiai + x− x̂ ∥∥∥∥∥ ≤ ∥∥∥∥maxi∈[k] (âi − ai) ∥∥∥∥+ ‖x− x̂‖ ≤ 2 ′. The first claim follows from the fact that ‖x̂ − ai‖ > 8r ′ and as a result x′ ∈ CH(Ŝ \\ B6r ′(x̂)). Next, note that B4r ′(ai) ⊆ B5r ′(âi). So, by the fact that ‖ai − âi‖ ≤ ′,\ndist (âi,CH(∆ \\B5r ′(âi))) ≥ dist (ai,CH(∆ \\B4r ′(ai)))− ′ ≥ 3 ′.\nFurthermore, we argue that if there is x̂ ∈ CH(Ŝ \\ B5r ′(âi)) then there exists x ∈ CH(∆ \\ B4r ′(âi)), such that ‖x − x̂‖ ≤ ′. The proof of this claim is relegated to Appendix E.1. Using this claim, we have dist ( âi,CH(Ŝ \\B6r ′(âi)) ) ≥ 2 ′.\nGiven the above structure, it is clear that set of points in C are all within of one of the ai’s. So, we can cluster C using single linkage with threshold to recover ai up to accuracy ."
    }, {
      "heading" : "5 Additional Results, Extensions, and Open Problems",
      "text" : ""
    }, {
      "heading" : "5.1 Sample Complexity Lower bound",
      "text" : "As we observed the number of samples required by our method is poly(n). However, as the number of classes can be much smaller than the number of features, one might hope to recover v1, . . . ,vk, with a number of samples that is polynomial in k rather than n. Here, we show that in the general case Ω(n) samples are needed to learn v1, . . . ,vk regardless of the value of k.\nFor ease of exposition, let k = 1 and note that in this case every sample should be purely of one type. Assume that the class vector, v, is promised to be in the set C = {vj | vj` = 1/ √ 2, if ` = 2j − 1 or 2j, else vj` = 0}. Consider instances (x 1 j ,x 2 j ) such that the ` th coordinate of x1j is x 1 j` = −1/ √ 2 if\n` = 2j− 1 and 1/ √ 2 otherwise, and x2j` = −1/ √ 2 if ` = 2j and 1/ √ 2 otherwise. For a given (x1j ,x 2 j ), we have that vj · x1j = vj · x2j = 0. On the other hand, for all ` 6= j, v` · x1j = v` · x2j = 1. Therefore, sample (x1j ,x 2 j ) is consistent with v = v\n` for any ` 6= j, but not with v = vj . That is, each instance (x1j ,x2j ) renders only one candidate of C invalid. Even after observing at most n2 − 2 samples of this types, at least 2 possible choices for v remain. So, Ω(n) samples are indeed needed to find the appropriate v. The next theorem, whose proof appears in Appendix F generalizes this construction and result to the case of any k. Theorem 5.1. For any k ≤ n, any algorithm that for all i ∈ [k] learns v′i such that ‖vi − v′i‖2 ≤ 1/ √\n2, requires Ω(n) samples.\nNote that in the above construction samples have large components in the irrelevant features. It would be interesting to see if this lower bound can be circumvented using additional natural assumptions in this model, such as assuming that the samples have length poly(k)."
    }, {
      "heading" : "5.2 Alternative Noise Models",
      "text" : "Consider the problem of recovering v1, . . . ,vk in the presence of agnostic noise, where for an fraction of the samples (x1,x2), x1 and x2 correspond to different mixture weights. Furthermore, assume that the distribution over the instance space is rich enough such that any subspace other than span{v1, . . . ,vk} is inconsistent with a set of instances of non-negligible density.4 Since the VC dimension of the set of k dimensional subspaces in Rn is min{k, n − k}, from the information theoretic point of view, one can recover span{v1, . . . ,vk} as it is the only subspace that is inconsistent with less than O( ) fraction of Õ( k\n2 ) samples. Furthermore, we can detect and remove any noisy sample, for which the two views of the sample are not consistent with span{v1, . . . ,vk}. And finally, we can recover a1, . . . ,ak using phase 2 of Algorithm 1.\nIn the above discussion, it is clear that once we have recovered span{v1, . . . ,vk}, denoising and finding the extreme points of the projections can be done in polynomial time. For the problem of recovering a kdimensional nullspace, Hardt and Moitra (2013) introduced an efficient algorithm that tolerates agnostic noise up to = O(k/n). Furthermore, they provide an evidence that this result might be tight. It would be interesting to see whether additional structure present in our model, such as the fact that samples are convex combination of classes, can allow us to efficiently recover the nullspace in presence of more noise.\nAnother interesting open problem is whether it is possible to handle the case of p0 = 0. That is, when every document is affected by Gaussian noise N (0, σ2In), for σ . A simpler form of this problem is\n4This assumption is similar to the richness assumption made in the standard case, where we assume that there is enough “entropy” between the two views of the samples such that even in the non-noisy case the subspace can be uniquely determined by taking the nullspace of X1 −X2.\nas follows. Consider a distribution induced by first drawing x ∼ D, where D is an arbitrary and unknown distribution over ∆ = CH({a1, . . . ,ak}), and taking x̂ = x+N (0, σ2In). Can we learn ai’s within error of using polynomially many samples? Note that when D is only supported on the corners of ∆, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010). It would be interesting to see under what regimes ai (and not necessarily the mixture weights) can be learned when D is an arbitrary distribution over ∆."
    }, {
      "heading" : "5.3 General function f(·)",
      "text" : "Consider the general model described in Section 2, where fi(x) = f(vi ·x) for an unknown strictly increasing function f : R+ → [0, 1] such that f(0) = 0. We describe how variations of the techniques discussed up to now can extend to this more general setting.\nFor ease of exposition, consider the non-noisy case. Since f is a strictly increasing function, f(vi ·x1) = f(vi ·x2) if and only if vi ·x1 = vi ·x2. Therefore, we can recover span(v1, . . . ,vk) by the same approach as in Phase 1 of Algorithm 1. Although, by definition of pseudoinverse matrices, the projection of x is still represented by x = ∑ i(vi ·x)ai, this is not necessarily a convex combination of ai’s anymore. This is due to the fact that vi · x can add up to values larger than 1 depending on x. However, x is still a non-negative combination of ai’s. Moreover, ai’s are linearly independent, so ai can not be expressed by a nontrivial nonnegative combination of other samples. Therefore, for all i, ai/‖ai‖ can be recovered by taking the extreme rays of the convex cone of the projected samples. So, we can recover v1, . . . ,vk, by taking the psuedoinverse of ai/‖ai‖ and re-normalizing the outcome such that ‖vi‖2 = 1. When samples are perturbed by noise, a similar argument that also takes into account the smoothness of f proves similar results.\nIt would be interesting to see whether a more general class of similarity functions, such as kernels, can be also learned in this context."
    }, {
      "heading" : "A Omitted Proof from Section 3 — No Noise",
      "text" : "A.1 Proof of Lemma 3.1\nFor all j ≤ n − k, let Zj = {(x1i − x2i ) | i ≤ j ζ ln n δ }. We prove by induction that for all j, rank(Zj) < j with probability at most j δn . For j = 0, the claim trivially holds. Now assume that the induction hypothesis holds for some j. Furthermore, assume that rank(Zj) ≥ j. Then, rank(Zj+1) < j+1 only if the additional 1ζ ln n δ samples inZj+1 all belong to span(Zj). Since, the space of such samples has rank< n−k, this happens with probability at most\n(1−ζ) 1 ζ ln n\nδ ≤ δn . Together with the induction hypothesis that rank(Zj) ≥ j with probability at most j δ n , we\nhave that rank(Zj+1) < j + 1 with probability at most (j+1)δ n . Therefore rank(Z) = rank(Zn−k) = n− k with probability at least 1− δ.\nA.2 Proof of Lemma 3.2 First note that V is a the pseudo-inverse ofA, so their span is equal. Hence, ∑ i∈[k](vi·x)ai ∈ span{v1, . . . ,vk}.\nIt remains to show that ( x− ∑ i∈[k](vi · x)ai ) ∈ null{v1, . . . ,vk}. We do so by showing that this vector is orthogonal to vj for all j. We have( x−\nk∑ i=1 (vi · x)ai\n) · vj = x · vj −\nk∑ i=1 (vi · x)(ai · vj)\n= x · vj − ∑ i 6=j (vi · x)(ai · vj)− (vj · x)(aj · vj)\n= x · vj − x · vj = 0.\nWhere, the second equality follows from the fact whenA = V +, for all i, ai ·vi = 1 and aj ·vi = for j 6= i. Therefore, ∑ i∈[k](vi · x)ai is the projection of x on span{v1, . . . ,vk}.\nA.3 Proof of Lemma 3.3 Assume that S included samples that are purely of type i, for all i ∈ [k]. That is, for all i ∈ [k] there is j ≤ m, such that vi · x1j = vi · x2j = 1 and vi′ · x1j = vi′ · x2j = 0 for i′ 6= i. By Lemma 3.2, the set of projected vectors form the set { ∑k i=1(vi · xj)ai | j ∈ [m]}. Note that ∑k i=1(vi · xj)ai is in the simplex with vertices a1, . . . ,ak. Moreover, for each i, there exists a pure sample in S of type i. Therefore, CH{ ∑k i=1(vi · xj)ai | j ∈ [m]} is the simplex on linearly independent vertices a1, . . . ,ak. As a result, a1, . . . ,ak are the extreme points of it. It remains to prove that with probability 1 − δ, the sample set has a document of purely type j, for all j ∈ [k]. By the assumption on the probability distribution P , with probability at most (1− ξ)m, there is no document of type purely j. Using the union bound, we get the final result."
    }, {
      "heading" : "B Technical Spectral Lemmas",
      "text" : "Proposition B.1 (Davis and Kahan (1970) sin θ theorem). . Let B, B̂ ∈ Rp×p be symmetric, with eigen values λ1 ≥ · · · ≥ λp and λ̂1 ≥ · · · ≥ λ̂p, respectively. Fix 1 ≤ r ≤ s ≤ p and let V = (vr, . . . ,vs) and V̂ = (v̂r, . . . , v̂s) be the orthonormal eigenvectors corresponding to λr, . . . , λs and λ̂r, . . . , λ̂s. Let δ = inf{|λ̂− λ| : λ ∈ [λs, λr], λ̂ ∈ (−∞, λ̂s−1] ∪ [λ̂r+1,∞)} > 0. Then ,\n‖ sin Θ(V, V̂ )‖2 ≤ ‖B̂ −B‖2\nδ .\nwhere sin Θ(V, V̂ ) = PV − PV̂ , where PV and PV̂ are the projection matrices for V and V̂ .\nProposition B.2 (Corollary 5.50 (Vershynin, 2010)). Consider a Gaussian distribution in Rn with covariance matrix Σ. Let A ∈ Rn×m be a matrix whose rows are drawn i.i.d from this distribution, and let Σm = 1mAA\n>. For every ∈ (0, 1), and t, if m ≥ cn(t/ )2 for some constant c, then with probability at least 1− 2 exp(−t2n), ‖Σm − Σ‖2 ≤ ‖Σ‖2\nProposition B.3 (Matrix Bernstein (Tropp, 2015)). Let S1, . . . , Sn be independent, centered random matrices with common dimension d1× d2, and assume that each one is uniformly bounded. That is, ESi = 0 and\n‖Si‖2 ≤ L for all i ∈ [n]. Let Z = ∑n i=1 Si, and let v(Z) denote the matrix variance:\nv(Z) = max {∥∥∥∥∥ n∑ i=1 E[SiS>i ] ∥∥∥∥∥ , ∥∥∥∥∥ n∑ i=1 E[S>i Si] ∥∥∥∥∥ } .\nThen,\nP[‖Z‖ ≥ t] ≤ (d1 + d2) exp (\n−t2/2 v(Z) + Lt/3\n) .\nProposition B.4 (Theorem 4.10 of Stewart and Sun (1990)). Let Â = A + E and let λ1, . . . , λn and λ′1, . . . , λ ′ n be the eigen values of A and A+ E. Then, max{|λ′i − λi|} ≤ ‖E‖2.\nProposition B.5 (Theorem 3.3 of Stewart (1977)). For any A and B = A+ E, ‖B+ −A+‖ ≤ max 3 { ‖A+‖2, ‖B+‖2 } ‖E‖,\nwhere ‖ · ‖ is an arbitrary norm."
    }, {
      "heading" : "C Omitted Proof from Section 4.1 — Phase 1",
      "text" : "C.1 Proof of Claim 4.6\nLet ei and di be the ith row of E and D. Then ED> = ∑m i=1 eid > i and DE > = ∑m i=1 die > i . Let Si =\n1 m\n[ 0 eid > i\ndie > i 0\n] . Then, ‖ 1mDE > + 1mED >‖2 ≤ 2‖ ∑m i=1 Si‖2. We will use matrix Bernstein to show\nthat ∑\ni∈[m] Si is small with high probability. First note that the distribution of ei is a Gaussian centered at 0, therefore, E[Si] = 0. Furthermore, for each i, with probability 1 − δ, ‖ei‖2 ≤ σ √ n log 1δ . So, with probability 1 − δ, for all samples i ∈ [m],\n‖ei‖2 ≤ σ √ n log mδ . Moreover, by assumption ‖di‖ = ‖x 1 i − x2i ‖ ≤ 2M . Therefore, with probability 1− δ, L = max\ni ‖Si‖2 =\n1\nm max i ‖ei‖‖di‖ ≤\n2 m σ √ nM polylog n δ .\nNote that, ∥∥E[SiS>i ]∥∥ = 1m2 ∥∥E[(eid>i )2]∥∥ ≤ L2. Since Si is Hermitian, the matrix covariance defined\nby Matrix Bernstein inequality is\nv(Z) = max {∥∥∥∥∥ m∑ i=1 E[SiS>i ] ∥∥∥∥∥ , ∥∥∥∥∥ m∑ i=1 E[S>i Si] ∥∥∥∥∥ } = ∥∥∥∥∥ m∑ i=1 E[SiS>i ] ∥∥∥∥∥ ≤ mL2. If ≤ v(Z)/L and m ∈ Ω(nσ2M2\n2 polylog n δ ) or ≥ v(Z)/L and m ∈ Ω(\n√ nσM polylog\nn δ ), using\nMatrix Bernstein inequality (Proposition B.3), we have\nPr [∥∥∥∥ 1mDE> + 1mED> ∥∥∥∥ ≥ ] = Pr [∥∥∥∥∥ m∑ i=1 Si ∥∥∥∥∥ ≥ 2 ] ≤ δ.\nC.2 Proof of Claim 4.7 Let di be the ith row D. Then DD> = ∑m i=1 did > i . Let Si = 1 mdid > i − 1mE[did > i ]. Then, ‖ 1mDD\n> − E [ 1 mDD >] ‖2 = ‖∑mi=1 Si‖2. Since, di = x1i − x2i and ‖xji‖ ≤ M , we have that for any i, ‖did>i − E[did>i ]‖ ≤ 4M2. Then,\nL = max i ‖Si‖2 =\n1\nm max i ‖did>i − E[did>i ]‖2 ≤\n4\nm M2,\nand ‖E[SiS>i ] ≤ L2. Note that Si is Hermitian, so, the matrix covariance is\nv(Z) = max {∥∥∥∥∥ m∑ i=1 E[SiS>i ] ∥∥∥∥∥ , ∥∥∥∥∥ m∑ i=1 E[S>i Si] ∥∥∥∥∥ } = ∥∥∥∥∥ m∑ i=1 E[SiS>i ] ∥∥∥∥∥ ≤ mL2. If δ0 ≤ 4M2 and m ∈ Ω(M 4\nδ20 log nδ ) or δ0 ≥ 4M 2 and m ∈ Ω(M2δ0 log n δ ), then by Matrix Bernstein\ninequality (Proposition B.3), we have\nPr [∥∥∥∥∥ m∑ i=1 Si ∥∥∥∥∥ ≥ δ02 ] ≤ δ."
    }, {
      "heading" : "D Omitted Proof from Section 4.2 — Denoising",
      "text" : "D.1 Proof of Claim 4.8 Recall that for any i ∈ [k], with probability γ = g( ′/(8kα)) a nearly pure weight vector w is generated from P , such that ‖w − ei‖ ≤ ′/(8kα). And independently, with probability p0 the point is not noisy. Therefore, there is p0γ density on non-noisy points that are almost purely of class i. Note that for such points, x,\n‖Px− ai‖ = ∥∥∥∥∥∥ k∑ j=1 wjaj − ai ∥∥∥∥∥∥ ≤ k( ′/(8kα))(α) ≤ ′ 8 .\nSince ‖P − P̂‖ ≤ ′/8M , we have\n‖ai − P̂x‖ = ‖ai − Px‖+ ‖Px− P̂x‖ ≤ ′ 8 + ′ 8 ≤ ′ 4\nThe claim follows immediately."
    }, {
      "heading" : "E Omitted Proof from Section 4.3 — Phase 2",
      "text" : "E.1 Omitted proof from Claim 4.11\nHere, we prove that x̂ ∈ CH(Ŝ \\Bd+ ′(âi)) then there exists x ∈ CH(∆\\Bd(âi)), such that ‖x− x̂‖ ≤ ′. Let x = ∑ i αiẑi be the convex combination of ẑ1, . . . , ẑ` ∈ Ŝ \\ Bd+ ′(âi). By Claim 4.3, there are z1, . . . , z` ∈ ∆, such that ‖zi − ẑi‖ ≤ ′ for all i ∈ [k]. Furthermore, by the proximity of zi to ẑi we have that zi 6∈ Bd(âi). Therefore, z1, . . . , z` ∈ ∆ \\Bd(âi). Then, x = ∑ i αizi is also within distance ′."
    }, {
      "heading" : "F Proof of Theorem 5.1 — Lower Bound",
      "text" : "For ease of exposition assume that n is a multiple of k. Furthermore, in this proof we adopt the notion (xi,x ′ i) to represent the two views of the i\nth sample. For any vector u ∈ Rn and i ∈ [k], we use (u)i to denote the ith nk -dimensional block of u, i.e., coordinates u(i−1)nk+1, . . . , uink .\nConsider the nk -dimensional vector uj , such that uj` = 1 if ` = 2j − 1 or 2j, and uj` = 0, otherwise. And consider nk -dimensional vectors zj and z ′ j , such that zj` = −1 if ` = 2j−1 and zj` = 1 otherwise, and z′j` = −1 if ` = 2j and z′j` = 1 otherwise. Consider a setting where vi is restricted to the set of candidate Ci = {vji | (v j i )i = uj/ √ 2 and (vji )i′ = 0 for i ′ 6= i}. In other words, the `th coordinate of vji is 1/ √ 2 if ` = (i − 1)nk + 2j − 1 or (i − 1) n k + 2j, else 0. Furthermore, consider instances (x j i ,x ′j i ) such that\n(xji )i = zj/ √ 2 and (x′ji )i = z ′ j/ √ 2 and for all i′ 6= i, (xji )i′ = (x ′j i )i′ = 0. In other words,\nxji = 1√ 2 (0, . . . , 0, 1, . . . , 1,\n(i−1)n k +2j−1,(i−1)n k +2j︷ ︸︸ ︷\n1,−1 , 1, . . . , 1, 0, . . . , 0),\nx′ji = 1√ 2 (0, . . . , 0, 1, . . . , 1,−1, 1 , 1, . . . , 1, 0, . . . , 0),\nvji = 1√ 2 (0, . . . , 0, 0, . . . , 0, 1, 1 , 0, . . . , 0,︸ ︷︷ ︸ ith block 0, . . . , 0).\nFirst note that, for any i, i′ ∈ [k] and any j, j′ ∈ [ n2k ], v j i · x\nj′ i′ = v j i · x ′j′ i′ . That is, the two views of all\ninstances are consistent with each other with respect to all candidate vectors. Furthermore, for any i and i′ such that i 6= i′, for all j, j′, vji · x j′ i′ = 0. Therefore, for any observed sample (x j i ,x ′j i ), the sample should be purely of type i. For a given i, consider all the samples (xji ,x ′j i ) that are observed by the algorithm. Note that v j i · x j i = vji · x ′j i = 0. And for all j ′ 6= j, vj ′ i · x j i = v j′ i · x ′j i = 1. Therefore, observing (x j i ,x ′j i ) only rules out v j i as a candidate, while this sample is consistent with candidates vj ′\ni for j ′ 6= j. Therefore, even after observing\n≤ n2k − 2 samples of this types, at least 2 possible choices for vi remain valid. Moreover, the distance between any two vji ,v j′ i ∈ Ci is √\n2. Therefore, n2k − 1 samples are needed to learn vi to an accuracy better than √\n2/2. Note that consistency of the data with vi′ is not affected by the samples of type x j i that are observed by the algorithms when i′ 6= i. So, Ω(knk ) = Ω(n) samples are required to approximate all vi’s to an accuracy better than √ 2/2."
    } ],
    "references" : [ {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky" ],
      "venue" : "Journal of Machine Learning Research, 15(1):2773–2832.",
      "citeRegEx" : "Anandkumar et al\\.,? 2014",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2014
    }, {
      "title" : "A spectral algorithm for latent dirichlet allocation",
      "author" : [ "A. Anandkumar", "Liu", "Y.-k.", "D.J. Hsu", "D.P. Foster", "S.M. Kakade" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 917–925.",
      "citeRegEx" : "Anandkumar et al\\.,? 2012",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "A practical algorithm for topic modeling with provable guarantees",
      "author" : [ "S. Arora", "R. Ge", "Y. Halpern", "D.M. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning (ICML), pages 280–288.",
      "citeRegEx" : "Arora et al\\.,? 2013",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2013
    }, {
      "title" : "Computing a nonnegative matrix factorization– provably",
      "author" : [ "S. Arora", "R. Ge", "R. Kannan", "A. Moitra" ],
      "venue" : "Proceedings of the 44th Annual ACM Symposium on Theory of Computing (STOC), pages 145–162. ACM.",
      "citeRegEx" : "Arora et al\\.,? 2012a",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning topic models – going beyond svd",
      "author" : [ "S. Arora", "R. Ge", "A. Moitra" ],
      "venue" : "Proceedings of the 53rd Symposium on Foundations of Computer Science (FOCS), pages 1–10.",
      "citeRegEx" : "Arora et al\\.,? 2012b",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Co-training and expansion: Towards bridging theory and practice",
      "author" : [ "Balcan", "M.-F.", "A. Blum", "K. Yang" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 89–96.",
      "citeRegEx" : "Balcan et al\\.,? 2004",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2004
    }, {
      "title" : "A provable svd-based algorithm for learning topics in dominant admixture corpus",
      "author" : [ "T. Bansal", "C. Bhattacharyya", "R. Kannan" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1997–2005.",
      "citeRegEx" : "Bansal et al\\.,? 2014",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2014
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "Journal of machine Learning research, 3(Jan):993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T. Mitchell" ],
      "venue" : "Proceedings of the 11th Conference on Computational Learning Theory (COLT), pages 92–100. ACM.",
      "citeRegEx" : "Blum and Mitchell,? 1998",
      "shortCiteRegEx" : "Blum and Mitchell",
      "year" : 1998
    }, {
      "title" : "Semi-Supervised Learning",
      "author" : [ "O. Chapelle", "B. Schlkopf", "A. Zien" ],
      "venue" : "The MIT Press, 1st edition.",
      "citeRegEx" : "Chapelle et al\\.,? 2010",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2010
    }, {
      "title" : "Pac generalization bounds for co-training",
      "author" : [ "S. Dasgupta", "M.L. Littman", "D. McAllester" ],
      "venue" : "Advances in Neural Information Processing Systems, 1:375–382.",
      "citeRegEx" : "Dasgupta et al\\.,? 2002",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2002
    }, {
      "title" : "The rotation of eigenvectors by a perturbation",
      "author" : [ "C. Davis", "W.M. Kahan" ],
      "venue" : "iii. SIAM Journal on Computing, 7(1):1–46.",
      "citeRegEx" : "Davis and Kahan,? 1970",
      "shortCiteRegEx" : "Davis and Kahan",
      "year" : 1970
    }, {
      "title" : "Algorithms and hardness for robust subspace recovery",
      "author" : [ "M. Hardt", "A. Moitra" ],
      "venue" : "Proceedings of the 26th Conference on Computational Learning Theory (COLT), pages 354–375.",
      "citeRegEx" : "Hardt and Moitra,? 2013",
      "shortCiteRegEx" : "Hardt and Moitra",
      "year" : 2013
    }, {
      "title" : "Probabilistic latent semantic analysis",
      "author" : [ "T. Hofmann" ],
      "venue" : "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 289–296. Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Hofmann,? 1999",
      "shortCiteRegEx" : "Hofmann",
      "year" : 1999
    }, {
      "title" : "Disentangling gaussians",
      "author" : [ "A.T. Kalai", "A. Moitra", "G. Valiant" ],
      "venue" : "Communications of the ACM, 55(2):113–120.",
      "citeRegEx" : "Kalai et al\\.,? 2012",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2012
    }, {
      "title" : "Settling the polynomial learnability of mixtures of gaussians",
      "author" : [ "A. Moitra", "G. Valiant" ],
      "venue" : "Proceedings of the 53rd Symposium on Foundations of Computer Science (FOCS), pages 93–102. IEEE.",
      "citeRegEx" : "Moitra and Valiant,? 2010",
      "shortCiteRegEx" : "Moitra and Valiant",
      "year" : 2010
    }, {
      "title" : "Latent semantic indexing: A probabilistic analysis",
      "author" : [ "C.H. Papadimitriou", "H. Tamaki", "P. Raghavan", "S. Vempala" ],
      "venue" : "Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems, pages 159–168. ACM.",
      "citeRegEx" : "Papadimitriou et al\\.,? 1998",
      "shortCiteRegEx" : "Papadimitriou et al\\.",
      "year" : 1998
    }, {
      "title" : "Matrix perturbation theory (computer science and scientific computing)",
      "author" : [ "G. Stewart", "Sun", "J.-G" ],
      "venue" : null,
      "citeRegEx" : "Stewart et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Stewart et al\\.",
      "year" : 1990
    }, {
      "title" : "On the perturbation of pseudo-inverses, projections and linear least squares problems",
      "author" : [ "G.W. Stewart" ],
      "venue" : "SIAM Review, 19(4):634–662.",
      "citeRegEx" : "Stewart,? 1977",
      "shortCiteRegEx" : "Stewart",
      "year" : 1977
    }, {
      "title" : "A survey of multi-view machine learning",
      "author" : [ "S. Sun" ],
      "venue" : "Neural computing and applications, 23:2031– 2038.",
      "citeRegEx" : "Sun,? 2013",
      "shortCiteRegEx" : "Sun",
      "year" : 2013
    }, {
      "title" : "An introduction to matrix concentration inequalities",
      "author" : [ "J.A. Tropp" ],
      "venue" : "arXiv preprint arXiv:1501.01571.",
      "citeRegEx" : "Tropp,? 2015",
      "shortCiteRegEx" : "Tropp",
      "year" : 2015
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "arXiv preprint arXiv:1011.3027.",
      "citeRegEx" : "Vershynin,? 2010",
      "shortCiteRegEx" : "Vershynin",
      "year" : 2010
    }, {
      "title" : "Perturbation bounds in connection with singular value decomposition",
      "author" : [ "Wedin", "P.-Å." ],
      "venue" : "BIT Numerical Mathematics, 12(1):99–111.",
      "citeRegEx" : "Wedin and P..Å.,? 1972",
      "shortCiteRegEx" : "Wedin and P..Å.",
      "year" : 1972
    }, {
      "title" : "Rp×p be symmetric, with eigen values",
      "author" : [ ],
      "venue" : "B Technical Spectral Lemmas Proposition",
      "citeRegEx" : "B and ∈,? \\Q1970\\E",
      "shortCiteRegEx" : "B and ∈",
      "year" : 1970
    }, {
      "title" : "PV̂ , where PV and PV̂ are the projection matrices for V and V̂",
      "author" : [ "V̂ ) = PV" ],
      "venue" : "(Vershynin,",
      "citeRegEx" : "−,? \\Q2010\\E",
      "shortCiteRegEx" : "−",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Topic modeling is an area with significant recent work in the intersection of algorithms and machine learning (Arora et al., 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014).",
      "startOffset" : 110,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al.",
      "startOffset" : 17,
      "endOffset" : 892
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al.",
      "startOffset" : 17,
      "endOffset" : 918
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al.",
      "startOffset" : 17,
      "endOffset" : 947
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999).",
      "startOffset" : 17,
      "endOffset" : 1071
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution.",
      "startOffset" : 17,
      "endOffset" : 1087
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents—that is, a function that given a document, predicts its mixture over topics—without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D− over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al.",
      "startOffset" : 17,
      "endOffset" : 3847
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents—that is, a function that given a document, predicts its mixture over topics—without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D− over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al.",
      "startOffset" : 17,
      "endOffset" : 3871
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents—that is, a function that given a document, predicts its mixture over topics—without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D− over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al.",
      "startOffset" : 17,
      "endOffset" : 3895
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents—that is, a function that given a document, predicts its mixture over topics—without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D− over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al. (2004); Sun (2013).",
      "startOffset" : 17,
      "endOffset" : 3917
    }, {
      "referenceID" : 0,
      "context" : ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents—that is, a function that given a document, predicts its mixture over topics—without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D− over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al. (2004); Sun (2013). We then describe several natural assumptions under which we can indeed efficiently solve the problem, learning accurate topic mixture predictors.",
      "startOffset" : 17,
      "endOffset" : 3929
    }, {
      "referenceID" : 0,
      "context" : "Existing work in topic modeling, such as Arora et al. (2012b); Anandkumar et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "(2012b); Anandkumar et al. (2014), provide elegant procedures for handling large noise that is caused by drawing only 2 or 3 words according to the distribution induced by x.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "Phase 2 is similar to that of Arora et al. (2012b). Algorithm 1 formalizes the details of this approach.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "data sets (Bansal et al., 2014).",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "Now, using the Davis and Kahan (1970) or Wedin (1972) sin θ theorem (see Proposition B.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "Now, using the Davis and Kahan (1970) or Wedin (1972) sin θ theorem (see Proposition B.",
      "startOffset" : 15,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : "We use the Matrix Bernstein inequality (Tropp, 2015), described in Appendix B, to demonstrate the concentration of ‖ 1 mDE > + 1 mED ‖2.",
      "startOffset" : 39,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "For the problem of recovering a kdimensional nullspace, Hardt and Moitra (2013) introduced an efficient algorithm that tolerates agnostic noise up to = O(k/n).",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Can we learn ai’s within error of using polynomially many samples? Note that when D is only supported on the corners of ∆, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).",
      "startOffset" : 269,
      "endOffset" : 338
    }, {
      "referenceID" : 14,
      "context" : "Can we learn ai’s within error of using polynomially many samples? Note that when D is only supported on the corners of ∆, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).",
      "startOffset" : 269,
      "endOffset" : 338
    }, {
      "referenceID" : 15,
      "context" : "Can we learn ai’s within error of using polynomially many samples? Note that when D is only supported on the corners of ∆, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).",
      "startOffset" : 269,
      "endOffset" : 338
    } ],
    "year" : 2016,
    "abstractText" : "Recently there has been significant activity in developing algorithms with provable guarantees for topic modeling. In standard topic models, a topic (such as sports, business, or politics) is viewed as a probability distribution ai over words, and a document is generated by first selecting a mixture w over topics, and then generating words i.i.d. from the associated mixture Aw. Given a large collection of such documents, the goal is to recover the topic vectors and then to correctly classify new documents according to their topic mixture. In this work we consider a broad generalization of this framework in which words are no longer assumed to be drawn i.i.d. and instead a topic is a complex distribution over sequences of paragraphs. Since one could not hope to even represent such a distribution in general (even if paragraphs are given using some natural feature representation), we aim instead to directly learn a document classifier. That is, we aim to learn a predictor that given a new document, accurately predicts its topic mixture, without learning the distributions explicitly. We present several natural conditions under which one can do this efficiently and discuss issues such as noise tolerance and sample complexity in this model. More generally, our model can be viewed as a generalization of the multi-view or co-training setting in machine learning. ∗Supported in part by National Science Foundation grants CCF-1525971 and CCF-1535967. †Supported in part by National Science Foundation grant CCF-1525971 and by a Microsoft Research Graduate Fellowship and an IBM Ph.D Fellowship. ar X iv :1 61 1. 01 25 9v 1 [ cs .L G ] 4 N ov 2 01 6",
    "creator" : "LaTeX with hyperref package"
  }
}