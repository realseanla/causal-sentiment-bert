{
  "name" : "1603.09460.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "System Combination for Short Utterance Speaker Recognition",
    "authors" : [ "Lantian Li", "Dong Wang", "Thomas Fang Zheng" ],
    "emails" : [ "lilt@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn", "fzheng@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "text-independent speaker recognition with short test utterances. This paper presents a combination approach to improve short utterance speaker recognition (SUSR), where two phonetic-aware systems are combined together: one is the DNN-based i-vector system and the other is the subregion-based GMM-UBM system proposed by us recently. The former employs phone posteriors to construct an i-vector model in which the shared statistics offer stronger robustness against limited test data. The latter establishes a phone-dependent GMM-UBM system which represents speaker characteristics with more details. A scorelevel system combination approach is proposed to integrate the respective advantages of the two systems. Experimental results confirm that on the text-independent SUSR task, both the DNNbased i-vector system and the subregion-based GMM-UBM system outperform their respective baselines, and the scorelevel system combination delivers significant performance improvement. Index Terms: subregion model, DNN-ivector, system combination, speaker recognition"
    }, {
      "heading" : "1. Introduction",
      "text" : "After decades of research, current text-independent speaker recognition (SRE) systems can obtain rather good performance, if the test utterances are sufficiently long [1, 2, 3]. However, if the utterances are short, serious performance degradation is often observed. For instance, Vogt et al. [4] reported that when the test speech was shortened from 20 seconds to 2 seconds, the performance degraded sharply in terms of equal error rate (EER) from 6.34% to 23.89% on a NIST SRE task. Mak et al. [5] showed that when the length of the test speech was less than 2 seconds, the EER was raised to 35%. The performance degradation seriously limits the application of SRE in practice, since long-duration test would impact user experience significantly, and in many situations it is very difficult, if not possible, to collect sufficient test data, for example in forensic applications. How to improve performance of speaker recognition on short utterances (SUSR) is an open research topic.\nA multitude of studies have been conducted in SUSR. For example, in [6], the authors showed that the performance on short utterances can be improved by JFA. This work was extended in [7] which reported that the i-vector model can dis-\nThis work was supported by the National Natural Science Foundation of China under Grant No. 61371136 and No. 61271389, it was also supported by the National Basic Research Program (973 Program) of China under Grant No. 2013CB329302. The authors are with Division of Technical Innovation and Development of Tsinghua National Laboratory for Information Science and Technology and Research Institute of Information Technology (RIIT) of Tsinghua University.\ntill speaker information in a more effective way so it is more suitable for SUSR. In addition, a score-based segment selection technique was proposed in [8]. A relative EER reduction of 22% was reported by the authors on a recognition task where the test utterances were shorter than 15 seconds in length.\nWe argue that the difficulty associated with textindependent SUSR can be largely attributed to the mismatched distributions of speech data between enrollment and test. Assuming that the enrollment speech is sufficiently long, so the speaker model can be well trained. If the test speech is sufficient as well, the distribution of the test data tends to match the distribution represented by the speaker model; however, if the test speech is short, then only a part of the probability mass represented by the speaker model can be covered by the test speech. For a GMM-UBM system, this is equal to say that only a few Gaussian components of the model are covered by the test data, and therefore the likelihood evaluation is biased. For the ivector model, since the Gaussian components share some statistics via a single latent variable, the impact of short test speech is partly alleviated. However, the limited data anyway leads to insufficient evaluation of the Baum-Welch statistics, resulting in a less reliable i-vector inference.\nA possible solution for the text-independent SUSR problem is to identify the phone content of the speech signal, and then model and evaluate speakers on individual phones. We call this ‘phonetic-aware’ approach. This approach can be regarded as a transfer from a text-independent task to a text-dependent task. The latter is certainly more resilient to short utterances, as has been demonstrated in [9].\nTwo phonetic-aware approaches have been proposed. One is the subregion model based on the GMM-UBM architecture [10], and the other is the DNN-based i-vector model [11, 12]. Both the two approaches employ an automatic speech recognition (ASR) system to generate phone transcriptions or posteriors for enrollment speech, and then establish a phoneticaware speaker model based on the transcriptions or posteriors. These two approaches, however, are significantly different in model structure and implementation. The subregion modeling approach builds multiple phone-dependent UBMs and speaker GMMs, and evaluates test speech on the phone-dependent models. The DNN-based i-vector approach, in contrast, keeps the single UBM/GMM framework, but relates each Gaussian component to a phone or a phone state. The former tends to be more flexible when learning speaker characteristics, while the latter is more robust against limited test data, due to the lowdimensional latent variable that is shared among all the Gaussian components. We therefore argue that the two approaches can be combined, so that the respective advantages of the two methods can be integrated.\nThe rest of the paper is organized as follows: Section 2 discusses some related work, Section 3 presents the subregion\nar X\niv :1\n60 3.\n09 46\n0v 1\n[ cs\n.C L\n] 3\n1 M\nar 2\n01 6\nmodel, and Section 4 describes the combination approach. Section 5 presents the experiments, and the entire paper is concluded in Section 6."
    }, {
      "heading" : "2. Related work",
      "text" : "The idea of employing phonetic information in speaker recognition has been investigated by previous research studies. For instance, Omar et al. [13] proposed to derive UBMs from Gaussian components of a GMM-based ASR system, with a Kmeans clustering approach based on the symmetric KL distance. The DNN-based i-vector method was proposed in [11, 12]. In the work, posteriors of senones (context-dependent states) generated by a DNN trained for ASR were used for model training as well as i-vector inference. Note that all these studies focus on relatively long utterances (5-10 seconds), whereas our study in this paper focuses on utterances as short as 0.5 seconds."
    }, {
      "heading" : "3. Subregion modeling",
      "text" : "We briefly describe the subregion model presented by us recently [10]. The basic idea is firstly presented, and then the implementation details are described."
    }, {
      "heading" : "3.1. Acoustic subregions",
      "text" : "The conventional GMM-UBM system treats the entire acoustic space as a whole probabilistic space, and computes the probability of an input speech signal by a GMM model, formulated as follows:\np(x; s) = ∏ t ∑ c P (c)N (xt;µsc,Σc)\nwhere x denotes the speech signal, and N (x;µ,Σ) represents a Gaussian distribution with µ as the mean and Σ as the covariance matrix. Further more, c indexes the Gaussian component, and s indexes the speaker. P (c) is a prior distribution on the c-th component. Roughly speaking, this model splits the acoustic space into a number of subregions, and each subregion is modelled by a Gaussian distribution.\nThere are at least three potential problems with this model: (1) the subregion splitting is based on unsupervised clustering (via the EM algorithm [14]), so it is not necessarily meaningful in phonetic; (2) each subregion is modeled by a Gaussian, which seems too simple; (3) the priors over the subregions are fixed, independent of the speech signal xt.\nThe subregion model was proposed to solve these problems. Firstly, the acoustic space is split into subregions that roughly correspond to phonetic units (e.g., phones); secondly, each subregion is modelled by a GMM instead of a single Gaussian; thirdly, the weight for each subregion is based on the posterior P (c|xt) instead of the prior P (c). This is formulated as follows:\np(x; s) = ∏ t ∑ c P (c|xt) ∑ k πc,kN (xt;µsc,k,Σc,k)\nwhere k indexes the Gaussians within a subregion GMM. A key component of this model is the posterior probability P (c|xt), which is not a pre-trained constant value, but an assignment of each signal xt to the subregions. In our study, this quantity is generated by an ASR system."
    }, {
      "heading" : "3.2. Speech units",
      "text" : "The inventory of speech units varies for different languages. In Chinese, the language focused in this paper, Initials and Finals (IF) are the most commonly used [15]. Roughly speaking, Initials correspond to consonants, and Finals correspond to vowels and nasals. Among the IFs, Finals are recognized to convey more speaker related information [16, 17], and therefore are used as the speech units in this study.\nUsing Finals to train the subregion model is not very practical, because there is a large number of Finals, and most Finals can only find limited data in both training and test. A possible solution is to cluster similar units together and build subregion models based on the resulting speech unit classes. In this study, we develop a vector quantization (VQ) method based on the kmeans algorithm [18] to conduct the clustering."
    }, {
      "heading" : "3.3. Subregion modeling based on speech unit classes",
      "text" : "Denote the speech unit classes (Final clusters) by {SUC-c,c= 1, ..., C}, a subregion UBM can be trained for each SUC-c with the training data that are aligned to the Finals in SUC-c by the ASR system. The subregion UBM of class SUC-c is denoted by λUBMc . The speaker-dependent subregion GMMs can be trained based on the subregion UBMs, using the enrollment data that have been aligned to the Finals of each cluster. The entire training process of subregion models is illustrated below.\n• Global UBM training, denoted by λUBM . A global UBM is trained with the entire training database by employing the EM algorithm.\n• Subregion UBM training. The ASR system is used to align the above training speech (acoustic features) to the Finals. The aligned speech data are then assigned to the speech unit classes according to the definition of {SUCc}. A subregion UBM λUBMc is then trained for the cth speech unit class based on the global UBM, by employing the MAP algorithm [19] with the speech data assigned to SUC-c.\n• Subregion speaker model training. For a speaker s, first segment the enrollment speech data into Finals and assign the speech data to {SUC-c}, by the same way as in the subregion UBM training. Then for each speech unit class SUC-c, a subregion speaker-dependent GMM λsc is trained by MAP adaptation from the subregion UBM λUBMc with the assigned enrollment data.\nOnce the speaker-dependent subregion GMMs are trained, a test utterance can be scored on each subregion. Suppose a test utterance contains L Finals according to the decoding result of speech recognition, and denote the speech unit class of the l-th Final by c(l). Further denote the speech segment of this unit by Xl, and its length is Tl. The score of Xl is measured by the log likelihood ratio between the subregion speaker-dependent GMM λsc(l) and the subregion UBM λ UBM c(l) , where s denotes the speaker. This is formulated by:\nϕs,l = log p(Xl|λsc(l))− log p(Xl|λubmc(l) )\nThe score of the entire utterance is computed as the average of the subregion-based scores:\nϕs = ∑L l=1 ϕs,l∑L l=1 Tl ."
    }, {
      "heading" : "4. System combination",
      "text" : "In this section, we first describe the difference between the subregion model and another phonetic-aware method: the DNN based i-vector model. Then the combination system is presented."
    }, {
      "heading" : "4.1. DNN-ivector and subregion model",
      "text" : "The DNN-based i-vector approach proposed by Lei and colleagues [11] replaces GMM-based posteriors by DNNgenerated posteriors when computing the Baum-Welch statistics for model training and i-vector inference. The DNN model is trained for speech recognition, so the output targets correspond to phones or states. This essentially builds a UBM and speaker GMMs where the Gaussian components correspond to phones or states. This is quite similar as the subregion model, though the model structures of the two models are different. On one hand, the subregion model builds GMMs for each subregion, while the DNN-based i-vector approach still assumes Gaussian for each subregion. From this aspect, the subregion model tends to be more flexible and represents speaker characteristics with more details. On the other hand, the subregions in the subregion modeling are relatively independent, whereas the subregions in the DNN-ivector model share statistics via the latent variable (i-vector). This sharing may lead to more strong robustness against limited test data."
    }, {
      "heading" : "4.2. Score-level system combination",
      "text" : "Due to the difference of the two phonetic-aware models and their perspective advantages, it is reasonable to combine them together. The combination system involves three components. Firstly, a DNN model for ASR is trained and used to generate the phonetic information: phone posteriors and phone alignments. Secondly, the phone posteriors are used to train the DNN-based i-vector model, and the phone alignments are used to build the subregion model. Thirdly, when scoring a test speech, the scores derived from the DNN-ivector system and the subregion GMM-UBM system are averaged to make the final decision. Figure 1 illustrates the system framework."
    }, {
      "heading" : "5. Experiments",
      "text" : ""
    }, {
      "heading" : "5.1. Database",
      "text" : ""
    }, {
      "heading" : "5.1.1. Database for evaluation (SUD12)",
      "text" : "There is not a standard database for performance evaluation on text-independent SUSR tasks. Therefore, we firstly designed and recorded a database that is suitable for SUSR research and published it for research usage1. The database was named as “SUD12” [20], and was designed in the principle to guarantee sufficient IF coverage. In order to focus on short utterances and exclude other factors such as the channel and emotion, the recording was conducted in the same room and with the same microphone, and the reading style was neutral. The database consists of 28 male speakers and 28 female speakers, and all the utterances are in standard Chinese. For each speaker, there are 100 Chinese sentences, each of which contains 15 ∼ 30 Chinese characters. The sampling rate is 16 kHz with 16-bits precision.\nThe enrollment database involves all the 56 speakers. For each speaker, after removing silence segments, the effective speech signals for enrollment is about 35 seconds. The test database consists of 56 speakers, and each speaker involves 62- 63 short utterances that cover all the Finals. The length of each utterance is not more than 2 seconds and mostly as short as 0.5 seconds. With the test database, 3, 523 target trials and 197, 293 non-target trials are defined for performance evaluation."
    }, {
      "heading" : "5.1.2. Database for UBM training (863DB)",
      "text" : "The speech data used to train the UBMs and subregion UBMs were chosen from the 863 Chinese speech corpus [21]. The 863 database was well designed to cover all the Chinese IFs, so it is particularly suitable to train subregion UBMs based on Final classes. All the recordings are at a sampling rate of 16 kHz, and the sample precision is 16 bits. In this study, we choose 17 hours of speech data and denote the database by 863DB."
    }, {
      "heading" : "5.2. Experimental conditions",
      "text" : "The Kaldi toolkit [22] was used to conduct the experiments. Following the standard recipe of SRE08, the acoustic feature was the conventional 60-dimensional Mel frequency cepstral coefficients (MFCCs), which involved 20-dimensional static components plus the first and second order derivatives. The frame size was 25 ms and the frame shift was 10 ms. Besides, a simple energy-based voice activity detection (VAD) was performed before the feature extraction.\nThe ASR system used to generate the phone alignment was a large-scale DNN-HMM hybrid system. The system was trained using Kaldi following the WSJ S5 recipe. The feature used was 40-dimensional Fbanks. The basic features were spliced by a window of 11 frames, and an LDA (linear discriminative analysis) transform was applied to reduce the dimensionality to 200. The DNN structure involved 4 hidden layers, each containing 1, 200 hidden units. The output layer contained 6, 761 units, corresponding to the number of GMM senones. The DNN was trained with 6, 000 hours of speech signals, and the decoding employed a powerful 5-gram language model trained on 2 TB text data.\nWe chose the conventional GMM-UBM approach to construct the baseline SUSR system. The UBM consisted of 1, 024 Gaussian components and was trained with the 863DB. The\n1http://www.cslt.org/resources.php?Public%20data\nSUD12 was employed to conduct the evaluation. With the enrollment data, the speaker GMMs were derived from the UBM by MAP, where the MAP adaptation factor was optimized so that the EER on the test set was the best. For comparison, a GMM-based i-vector system was also constructed. The training was based on the same UBM model as the GMM-UBM system, and the dimensionality of the i-vector was 400.\nFor the DNN-based i-vector system, the DNN model was trained following the same procedure as the one used for the ASR system, but with less number of senones. In our experiments, the number was 928, comparable to the number of Gaussian components of the GMM-UBM system. The dimensionality of the DNN-based i-vectors was set to 400."
    }, {
      "heading" : "5.3. Basic results",
      "text" : "We first investigated the subregion model based on speech unit classes. For this model, the number of speech unit classes need to be defined before hand. In our experiments, we observed that either too small or too large clustering numbers lead to suboptimal performance, and the optimal setting in our experiment was C=6 [10]. Table 1 shows the derived unit classes. It can be seen that the resultant clusters are intuitively reasonable.\nThe results in terms of EER are presented in Table 2, where ‘GMM-UBM’ is the GMM-UBM baseline system, ‘SBM-DD’ denotes the subregion modeling system (C=6). ‘GMM i-vector’ denotes the traditional GMM-based i-vector system, and ‘DNN i-vector’ denotes the DNN-based i-vector system.\nWe first observe that both the subregion modeling system and the DNN-based i-vector system outperform their relative baselines (‘GMM-UBM’ and ‘GMM-based i-vector’) in a significant way. This confirms the effectiveness of the two phonetic-aware methods. Besides, it can be seen that the GMMUBM baseline outperforms the two i-vector systems, but after the probabilistic linear discriminant analysis (PLDA) [23] is employed, the i-vector system is significantly improved and outperforms the GMM-UBM system.\nFor a better understanding of the performance of different systems on various operation points, the DET curves are presented in Figure 2, where the horizontal axis represents the false acceptance rate (FAR) and the vertical axis represents the false rejection rate (FRR) [24]. The advantage of the two phoneticaware models can be clearly observed."
    }, {
      "heading" : "5.4. System combination",
      "text" : "We combine the ‘DNN i-vector + PLDA’ system and the ‘SBMDD’ system by a linear score fusion: αsplda + (1 − α)ssbm, where α is the interpolation factor. Figure 3 presents the performance with various α. It clearly shows that the system combination leads to better performance than each individual system. Figure 3 shows that α=0.94 is a good choice. Table 2 and Figure 2 have shown the results of the combination system with this configuration."
    }, {
      "heading" : "6. Conclusions",
      "text" : "This paper presents a combination system to deal with short utterances in text-independent speaker recognition. This system combines two phonetic-aware methods: one is the DNN-based i-vector system and the other is the subregion-based GMMUBM system. The experimental results show that both the DNN-based i-vector system and the subregion-based GMMUBM system outperforms their respective baselines, and a simple score fusion leads to the best performance we have obtained so far. Future work involves combination of feature-based and model-based compensations for short utterances, and investigation on phone-discriminative methods."
    }, {
      "heading" : "7. References",
      "text" : "[1] F. Bimbot, J.-F. Bonastre, C. Fredouille, G. Gravier, I. Magrin-\nChagnolleau, S. Meignier, T. Merlin, J. Ortega-Garcı́a, D. Petrovska-Delacrétaz, and D. A. Reynolds, “A tutorial on textindependent speaker verification,” EURASIP Journal on Applied Signal Processing, vol. 2004, pp. 430–451, 2004.\n[2] T. Kinnunen and H. Li, “An overview of text-independent speaker recognition: From features to supervectors,” Speech communication, vol. 52, no. 1, pp. 12–40, 2010.\n[3] C. S. Greenberg, V. M. Stanford, A. F. Martin, M. Yadagiri, G. R. Doddington, J. J. Godfrey, and J. Hernandez-Cordero, “The 2012 nist speaker recognition evaluation,” in Proc. INTERSPEECH’13, 2013, pp. 1971–1975.\n[4] R. Vogt, S. Sridharan, and M. Mason, “Making confident speaker verification decisions with minimal speech,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1182–1192, 2010.\n[5] M.-W. Mak, R. Hsiao, and B. Mak, “A comparison of various adaptation methods for speaker verification with limited enrollment data,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2008, vol. 1. IEEE, 2006, pp. I–I.\n[6] R. J. Vogt, C. J. Lustri, and S. Sridharan, “Factor analysis modelling for speaker verification with short utterances,” in The Speaker and Language Recognition Workshop. IEEE, 2008.\n[7] A. Kanagasundaram, R. Vogt, D. B. Dean, S. Sridharan, and M. W. Mason, “i-vector based speaker recognition on short utterances,” in Proceedings of the 12th Annual Conference of the International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341– 2344.\n[8] M. Nosratighods, E. Ambikairajah, J. Epps, and M. J. Carey, “A segment selection technique for speaker verification,” Speech Communication, vol. 52, no. 9, pp. 753–761, 2010.\n[9] A. Larcher, K.-A. Lee, B. Ma, and H. Li, “Rsr2015: Database for text-dependent speaker verification using multiple pass-phrases,” in Proc. INTERSPEECH’12, 2012.\n[10] L. Li, D. Wang, C. Zhang, and T. Z. Zheng, “Improving short utterance speaker recognition by modeling speech unit classes,” IEEE Transactions on Audio, Speech, and Language Processing, vol. DOI: 10.1109/TASLP.2016.2544660, 2016.\n[11] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, “A novel scheme for speaker recognition using a phonetically-aware deep neural network,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014. IEEE, 2014, pp. 1695– 1699.\n[12] P. Kenny, V. Gupta, T. Stafylakis, P. Ouellet, and J. Alam, “Deep neural networks for extracting baum-welch statistics for speaker recognition,” in Odyseey’2014. Odyssey, 2014.\n[13] M. K. Omar and J. W. Pelecanos, “Training universal background models for speaker recognition.” in Odyssey’2010. Odyssey, 2010.\n[14] T. K. Moon, “The expectation-maximization algorithm,” Signal processing magazine, IEEE, vol. 13, no. 6, pp. 47–60, 1996.\n[15] J.-Y. Zhang, T. F. Zheng, J. Li, C.-H. Luo, and G.-L. Zhang, “Improved context-dependent acoustic modeling for continuous chinese speech recognition.” in Proc. INTERSPEECH’01, 2001, pp. 1617–1620.\n[16] H. Beigi, Fundamentals of speaker recognition. Springer, 2011.\n[17] C. Gong, Research on Highly Distinguishable Speech Selection Methods in Speaker Recognition. Tsinghua University, 2014.\n[18] A. Hall, “Methods for demonstrating resemblance in taxonomy and ecology,” Nature, vol. 214, pp. 830–831, 1967.\n[19] J.-L. Gauvain and C.-H. Lee, “Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains,” IEEE Transactions on Speech and audio processing, vol. 2, no. 2, pp. 291–298, 1994.\n[20] C. Zhang, X.-J. Wu, T. F. Zheng, L.-L. Wang, and C. Yin, “A k-phoneme-class based multi-model method for short utterance speaker recognition,” in Asia-Pacific Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC), vol. 20, no. 12, 2012, pp. 1–4.\n[21] D. Wang, X.-Y. Zhu, and Y. Liu, “Multi-layer channel normalization for frequency-dynamic feature extraction,” Journal of Software, vol. 12, no. 9, pp. p1523–1529, 2005.\n[22] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, “The kaldi speech recognition toolkit,” in IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, IEEE Catalog No.: CFP11SRW-USB.\n[23] S. J. Prince and J. H. Elder, “Probabilistic linear discriminant analysis for inferences about identity,” in ICCV’07. IEEE, 2007, pp. 1–8.\n[24] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki, “The det curve in assessment of detection task performance,” DTIC Document, Tech. Rep., 1997."
    } ],
    "references" : [ {
      "title" : "A tutorial on textindependent speaker verification",
      "author" : [ "F. Bimbot", "J.-F. Bonastre", "C. Fredouille", "G. Gravier", "I. Magrin- Chagnolleau", "S. Meignier", "T. Merlin", "J. Ortega-Garcı́a", "D. Petrovska-Delacrétaz", "D.A. Reynolds" ],
      "venue" : "EURASIP Journal on Applied Signal Processing, vol. 2004, pp. 430–451, 2004.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "An overview of text-independent speaker recognition: From features to supervectors",
      "author" : [ "T. Kinnunen", "H. Li" ],
      "venue" : "Speech communication, vol. 52, no. 1, pp. 12–40, 2010.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The 2012 nist speaker recognition evaluation",
      "author" : [ "C.S. Greenberg", "V.M. Stanford", "A.F. Martin", "M. Yadagiri", "G.R. Doddington", "J.J. Godfrey", "J. Hernandez-Cordero" ],
      "venue" : "Proc. INTERSPEECH’13, 2013, pp. 1971–1975.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Making confident speaker verification decisions with minimal speech",
      "author" : [ "R. Vogt", "S. Sridharan", "M. Mason" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1182–1192, 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A comparison of various adaptation methods for speaker verification with limited enrollment data",
      "author" : [ "M.-W. Mak", "R. Hsiao", "B. Mak" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2008, vol. 1. IEEE, 2006, pp. I–I.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Factor analysis modelling for speaker verification with short utterances",
      "author" : [ "R.J. Vogt", "C.J. Lustri", "S. Sridharan" ],
      "venue" : "The Speaker and Language Recognition Workshop. IEEE, 2008.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "i-vector based speaker recognition on short utterances",
      "author" : [ "A. Kanagasundaram", "R. Vogt", "D.B. Dean", "S. Sridharan", "M.W. Mason" ],
      "venue" : "Proceedings of the 12th Annual Conference of the International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341– 2344.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A segment selection technique for speaker verification",
      "author" : [ "M. Nosratighods", "E. Ambikairajah", "J. Epps", "M.J. Carey" ],
      "venue" : "Speech Communication, vol. 52, no. 9, pp. 753–761, 2010.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Rsr2015: Database for text-dependent speaker verification using multiple pass-phrases",
      "author" : [ "A. Larcher", "K.-A. Lee", "B. Ma", "H. Li" ],
      "venue" : "Proc. INTERSPEECH’12, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Improving short utterance speaker recognition by modeling speech unit classes",
      "author" : [ "L. Li", "D. Wang", "C. Zhang", "T.Z. Zheng" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. DOI: 10.1109/TASLP.2016.2544660, 2016.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A novel scheme for speaker recognition using a phonetically-aware deep neural network",
      "author" : [ "Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014. IEEE, 2014, pp. 1695– 1699.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep neural networks for extracting baum-welch statistics for speaker recognition",
      "author" : [ "P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam" ],
      "venue" : "Odyseey’2014. Odyssey, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The expectation-maximization algorithm",
      "author" : [ "T.K. Moon" ],
      "venue" : "Signal processing magazine, IEEE, vol. 13, no. 6, pp. 47–60, 1996.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Improved context-dependent acoustic modeling for continuous chinese speech recognition.",
      "author" : [ "J.-Y. Zhang", "T.F. Zheng", "J. Li", "C.-H. Luo", "G.-L. Zhang" ],
      "venue" : "in Proc. INTERSPEECH’01,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2001
    }, {
      "title" : "Fundamentals of speaker recognition",
      "author" : [ "H. Beigi" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Research on Highly Distinguishable Speech Selection Methods in Speaker Recognition",
      "author" : [ "C. Gong" ],
      "venue" : "Tsinghua University,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Methods for demonstrating resemblance in taxonomy and ecology",
      "author" : [ "A. Hall" ],
      "venue" : "Nature, vol. 214, pp. 830–831, 1967.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains",
      "author" : [ "J.-L. Gauvain", "C.-H. Lee" ],
      "venue" : "IEEE Transactions on Speech and audio processing, vol. 2, no. 2, pp. 291–298, 1994.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "A k-phoneme-class based multi-model method for short utterance speaker recognition",
      "author" : [ "C. Zhang", "X.-J. Wu", "T.F. Zheng", "L.-L. Wang", "C. Yin" ],
      "venue" : "Asia-Pacific Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC), vol. 20, no. 12, 2012, pp. 1–4.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-layer channel normalization for frequency-dynamic feature extraction",
      "author" : [ "D. Wang", "X.-Y. Zhu", "Y. Liu" ],
      "venue" : "Journal of Software, vol. 12, no. 9, pp. p1523–1529, 2005.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The kaldi speech recognition toolkit",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely" ],
      "venue" : "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, IEEE Catalog No.: CFP11SRW-USB.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic linear discriminant analysis for inferences about identity",
      "author" : [ "S.J. Prince", "J.H. Elder" ],
      "venue" : "ICCV’07. IEEE, 2007, pp. 1–8.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The det curve in assessment of detection task performance",
      "author" : [ "A. Martin", "G. Doddington", "T. Kamm", "M. Ordowski", "M. Przybocki" ],
      "venue" : "DTIC Document, Tech. Rep., 1997.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "After decades of research, current text-independent speaker recognition (SRE) systems can obtain rather good performance, if the test utterances are sufficiently long [1, 2, 3].",
      "startOffset" : 167,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "After decades of research, current text-independent speaker recognition (SRE) systems can obtain rather good performance, if the test utterances are sufficiently long [1, 2, 3].",
      "startOffset" : 167,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : "After decades of research, current text-independent speaker recognition (SRE) systems can obtain rather good performance, if the test utterances are sufficiently long [1, 2, 3].",
      "startOffset" : 167,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "[4] reported that when the test speech was shortened from 20 seconds to 2 seconds, the performance degraded sharply in terms of equal error rate (EER) from 6.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] showed that when the length of the test speech was less than 2 seconds, the EER was raised to 35%.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "For example, in [6], the authors showed that the performance on short utterances can be improved by JFA.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "This work was extended in [7] which reported that the i-vector model can dis-",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "In addition, a score-based segment selection technique was proposed in [8].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "The latter is certainly more resilient to short utterances, as has been demonstrated in [9].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "One is the subregion model based on the GMM-UBM architecture [10], and the other is the DNN-based i-vector model [11, 12].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "One is the subregion model based on the GMM-UBM architecture [10], and the other is the DNN-based i-vector model [11, 12].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "One is the subregion model based on the GMM-UBM architecture [10], and the other is the DNN-based i-vector model [11, 12].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "The DNN-based i-vector method was proposed in [11, 12].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "The DNN-based i-vector method was proposed in [11, 12].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "We briefly describe the subregion model presented by us recently [10].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "There are at least three potential problems with this model: (1) the subregion splitting is based on unsupervised clustering (via the EM algorithm [14]), so it is not necessarily meaningful in phonetic; (2) each subregion is modeled by a Gaussian, which seems too simple; (3) the priors over the subregions are fixed, independent of the speech signal xt.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : "In Chinese, the language focused in this paper, Initials and Finals (IF) are the most commonly used [15].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "Among the IFs, Finals are recognized to convey more speaker related information [16, 17], and therefore are used as the speech units in this study.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "Among the IFs, Finals are recognized to convey more speaker related information [16, 17], and therefore are used as the speech units in this study.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "In this study, we develop a vector quantization (VQ) method based on the kmeans algorithm [18] to conduct the clustering.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "A subregion UBM λ c is then trained for the cth speech unit class based on the global UBM, by employing the MAP algorithm [19] with the speech data assigned to SUC-c.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : "The DNN-based i-vector approach proposed by Lei and colleagues [11] replaces GMM-based posteriors by DNNgenerated posteriors when computing the Baum-Welch statistics for model training and i-vector inference.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "The database was named as “SUD12” [20], and was designed in the principle to guarantee sufficient IF coverage.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "The speech data used to train the UBMs and subregion UBMs were chosen from the 863 Chinese speech corpus [21].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "The Kaldi toolkit [22] was used to conduct the experiments.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "In our experiments, we observed that either too small or too large clustering numbers lead to suboptimal performance, and the optimal setting in our experiment was C=6 [10].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "Besides, it can be seen that the GMMUBM baseline outperforms the two i-vector systems, but after the probabilistic linear discriminant analysis (PLDA) [23] is employed, the i-vector system is significantly improved and outperforms the GMM-UBM system.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 22,
      "context" : "For a better understanding of the performance of different systems on various operation points, the DET curves are presented in Figure 2, where the horizontal axis represents the false acceptance rate (FAR) and the vertical axis represents the false rejection rate (FRR) [24].",
      "startOffset" : 271,
      "endOffset" : 275
    } ],
    "year" : 2017,
    "abstractText" : "Noticeable performance degradation is often observed in text-independent speaker recognition with short test utterances. This paper presents a combination approach to improve short utterance speaker recognition (SUSR), where two phonetic-aware systems are combined together: one is the DNN-based i-vector system and the other is the subregion-based GMM-UBM system proposed by us recently. The former employs phone posteriors to construct an i-vector model in which the shared statistics offer stronger robustness against limited test data. The latter establishes a phone-dependent GMM-UBM system which represents speaker characteristics with more details. A scorelevel system combination approach is proposed to integrate the respective advantages of the two systems. Experimental results confirm that on the text-independent SUSR task, both the DNNbased i-vector system and the subregion-based GMM-UBM system outperform their respective baselines, and the scorelevel system combination delivers significant performance improvement.",
    "creator" : "LaTeX with hyperref package"
  }
}