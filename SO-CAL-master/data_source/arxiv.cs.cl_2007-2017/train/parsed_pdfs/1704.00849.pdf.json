{
  "name" : "1704.00849.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks",
    "authors" : [ "Chin-Cheng Hsu", "Hsin-Te Hwang", "Yi-Chiao Wu", "Yu Tsao", "Hsin-Min Wang" ],
    "emails" : [ "whm}@iis.sinica.edu.tw,", "yu.tsao@citi.sinica.edu.tw" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The primary goal of voice conversion (VC) is to convert the speech from a source speaker to that of a target, without changing the linguistic or phonetic content. However, consider the case of converting one’s voice into that of another who speaks a different language. Traditional VC techniques would have trouble dealing with such cases because most of them require parallel training data in which many pairs of speakers uttered the same texts. In this paper, we are devoted to bridging the gap between parallel and non-parallel VC systems.\nWe pursue a unified generative model for speech that naturally accommodates VC (Sec. 2). In this framework, we do not have to align any frames or to cluster phones or frames explicitly. The idea is skeletonized in the probabilistic graphical model (PGM) in Fig. 1. In this model, our attention is directed away from seeking alignment. Rather, what we are concerned about are 1) finding a good inference model (Sec. 2.1) for the latent variable z and 2) building a good synthesizer whose outputs match the distribution of the real speech of the target (Sec. 2.2 through 2.3). In this paper, we present a specific implementation in which a variational autoencoder (VAE [1]) assumes the inference task and a Wasserstein generative adversarial network (W-GAN [2]) undertakes speech synthesis. Our contribution is two-fold:\n• We introduce the W-GAN to non-parallel voice conversion and elucidate the reason why it fits this task (Sec. 2).\n• We demonstrate the ability of W-GAN to synthesize more realistic spectra (Sec. 3)."
    }, {
      "heading" : "2. Non-parallel voice conversion via deep generative models",
      "text" : "Given spectral frames Xs = {xs,n}Nsn=1 from the source speaker and those Xt = {xt,n′}Ntn′=1 from the target, assume that the real data distributions of the source and the target respectively admit a density p∗s and p∗t . Let f be a voice conversion function that induces a conditional distribution pt|s. The goal of VC is to estimate f so that pt|s best approximates the real data distribution p∗t :\npt|s(f(xs,n)) ≈ p∗t (xt,n′). (1)\nWe can decompose the VC function f into two stages according to the PGM in Fig. 1b. In the first stage, a speakerindependent encoder Eφ infers a latent content zn. In the second stage, a speaker dependent decoder Gθ mixes zn with a speaker-specific variable y to reconstruct the input. The problem of VC is then reformulated as:\nxt,n′ ≈ f(xs,n) = Gθ(Eφ(xs,n),y). (2)\nIn short, this model explains the observation x using two latent variables y and z. We will drop the frame indices whenever readability is unharmed. We refer to y as the speaker representation vector because it is determined solely by the speaker identity. We refer to z as the phonetic content vector because with a fixed y, we can generate that speaker’s voice by varying\nar X\niv :1\n70 4.\n00 84\n9v 3\n[ cs\n.C L\n] 8\nJ un\n2 01\n7\nz. Note that the term phonetic content is only valid in the context of our experimental settings where the speech is natural, noise-free, and non-emotional.\nThis encoder-decoder architecture facilitates VC from unaligned or non-parallel corpora. The function of the encoder is similar to a phone recognizer whereas the decoder operates as a synthesizer. The architecture enables voice conversion for the following reasons. The speaker representation y can be obtained from training. The encoder can infer the phonetic content z. The synthesizer can reconstruct any spectral frame x with the corresponding y and z. Combining these elements, we can build a non-parallel VC system via optimizing the encoder, the decoder (synthesizer), and the speaker representation. With the encoder, frame-wise alignment is no longer needed; frames that belong to the same phoneme class now hinge on a similar z. With this conditional synthesizer, VC becomes as simple as replacing the speaker representation y. (as illustrated in Fig. 1c).\nWe delineate our proposed method incrementally in three subsections: a conditional variational autoencoder (C-VAE) in Sec. 2.1, a generative adversarial nets (GAN) applied to improve the over-simplified C-VAE model in Sec. 2.2, and a Wasserstein GAN (W-GAN) that explicitly considers VC in the training objectives in Sec. 2.3."
    }, {
      "heading" : "2.1. Modeling speech with a C-VAE",
      "text" : "Recent works have proven the viability of speech modeling with VAEs [3, 4]. A C-VAE that realizes the PGM in Fig. 1a maximizes a variational lower bound of the log-likelihood:\nlog pθ(x|y) ≤ −Jvae(x|y) = −(Jobs(x|y)+Jlat(x)), (3) Jlat(φ;x) = DKL ( qφ(z|x)‖pθ(z) ) , (4)\nJobs(φ, θ;x,y) = −Eqφ(z|x) [ log pθ(x|z,y) ] , (5)\nwhere x ∈ Xs ∪Xt, DKL is the Kullback-Leibler divergence, pθ(z) is our prior distribution model of z, pθ(x|z,y) is our synthesis model, and qφ(z|x) is our inference model. Note that the synthesis is conditioned on an extra input y, thus the name conditional VAE.\nIn order to train the C-VAE, we have to simplify the model in several aspects. First, we choose pθ(x|z,y) to be a normal distribution whose covariance is an identity matrix. Second, we choose pθ(z) to be a standard normal distribution. Third, the expectation over z is approximated by sampling methods. With these simplifications, we can avoid intractability and focus on modeling the statistics of the Gaussian distributions.\nFor the phonetic content z, we have:\nqφ(z|x) = N (z | Eφ1(x), diag(Eφ2(x))), (6)\nwhere φ1 ∪ φ2 = φ are the parameters of the encoder, and Eφ1 and Eφ2 are the inference models of mean and variance. For the reconstructed or converted spectral frames, we have:\npθ(xm,n|zn,ym) = N (xm,n | Gθ(zn,ym), I). (7)\nTraining this C-VAE means maximizing (3). For every input (xn,yn), we can sample the latent variable zn using the re-parameterization trick described in [1]. With zn and yn, the model can reconstruct the input, and by replacing yn, it can convert voice. This means that we are building virtually multiple models in one. Conceptually, the speaker switch lies in the speaker representation y because the synthesis is conditioned on y."
    }, {
      "heading" : "2.2. Improving speech models with GANs",
      "text" : "Despite the effectiveness of C-VAE, the simplification induces inaccuracy in the synthesis model. This defect originates from the fallible assumption that the observed data is normally distributed and uncorrelated across dimensions. This assumption gave us a defective learning objective, leading to muffled converted voices. Therefore, we are motivated to resort to models that side-step this defect.\nWe can improve the C-VAE by incorporating a GAN objective [5] into the decoder. A vanilla GAN [6] consists of two components: a generator (synthesizer) Gθ that produces realistic spectrum and a discriminatorDψ that judges whether an input is a true spectrum or a generated one. These two components seek an equilibrium in a min-max game with the Jensen-Shannon divergence DJS as the objective, which is defined as follows:\nJgan(θ, ψ;x) = 2 DJS ( p∗t ‖pt|s ) + 2 log 2.\n= Ex∼p∗t [ log p∗t\np∗t + pt|s\n] + Ex∼pt|s [ log\npt|s p∗t + pt|s ] = Ex∼p∗t [ logD∗ψ(x) ] + Ez∼qφ [ log ( 1−D∗ψ(Gθ(z)) )] ,\n(8)\nwhere D∗ψ denotes the optimal discriminator, which is the density ratio in the second equality in (8). We can view this as a density ratio estimation problem without explicit specification of distributions.\nPresumably, GANs produce sharper spectra because they optimize a loss function between two distributions in a more direct fashion. We can combine the objectives of VAE and GAN by assigning VAE’s decoder as GAN’s generator to form a VAEGAN [5]. However, the VAE-GAN does not consider VC explicitly. Therefore, we propose our final model: variational autoencoding Wasserstein GAN (VAW-GAN)."
    }, {
      "heading" : "2.3. Direct consideration of voice conversion with W-GAN",
      "text" : "A deficiency in the VAE-GAN formulations is that it treats VC indirectly. We simply assume that when the model is welltrained, it naturally equips itself with the ability to convert voices. In contrast, we can directly optimize a non-parallel VC loss by renovating DJS with a Wasserstein objective [2].\n2.3.1. Wasserstein distance\nThe Wasserstein-1 distance is defined as follows:\nW (p∗t , pt|s) = inf γ∈Π(p∗t ,pt|s)\nE(x,x̂)∼γ [ ‖x− x̂‖ ] (9)\nwhere Π(p, q) denotes the set of all joint distributions γ(x, x̂) whose marginals are respectively p∗t and pt|s. According to the definition, the Wasserstein distance is calculated from the optimal transport, or the best frame alignment. Note that (9) is thus suitable for parallel VC.\nOn the other hand, the Kantorovich-Rubinstein duality [7] of (9) allows us to explicitly approach non-parallel VC:\nW (p∗t , pt|s) = sup ‖D‖L≤1\nEx∼p∗t [ D(x) ] − Ex∼pt|s [ D(x) ] ,\n(10) where the supremum is over all 1-Lipschitz functionsD : X → R. If we have a parameterized family of functions Dψ∈Ψ that are all K-Lipschitz for some K, we could consider solving the problem:\nmax ψ∈Ψ Jwgan, (11)\nAlgorithm 1 VAE-WGAN training function AUTOENCODE(X, y)\nZµ ← Eφ1(X) Zσ ← Eφ2(X) Z ← sample fromN (Zµ, Zσ) X ′ ← Gθ(Z, y) return X ′, Z\nφ, θ, ψ← initialization while not converged do\nXs ← mini-batch of random samples from source Xt ← mini-batch of random samples from target X ′ s, Zs ← AUTOENCODE(Xs, ys) X ′ t , Zt ← AUTOENCODE(Xt, yt) Xt|s ← Gθ(Zs, yt) Jobs ← Jobs(Xs) + Jobs(Xt) Jlat ← Jlat(Zs) + Jlat(Zt) Jwgan ← Jwgan(Xt, Xs)\n// Update the encoder, generator, and discriminator while not converged do ψ\nupdate←−−−− −∇ψ(−Jwgan) φ\nupdate←−−−− −∇φ(Jobs + Jlat) θ update←−−−− −∇θ(Jobs + αJwgan)\nwhere Jwgan is defined as Ex∼p∗t [ Dψ(x) ] − Ez∼qφ(z|x) [ Dψ(Gθ(z),yt) ] (12)\nAlignment is not required in this formulation because of the respective expectations. What we need now is a batch of real frames from the target speaker, another batch of synthetic frames converted from the source into the target speaker, and a good discriminator Dψ ."
    }, {
      "heading" : "2.3.2. VAW-GAN",
      "text" : "Incorporating the W-GAN loss (12) with (3) yields our final objective:\nJvawgan =−DKL ( qφ(zn|xn)‖p(zn) ) + Ez∼qφ(z|x) [ log pθ(x|z,y)\n] + α Ex∼p∗t [ Dψ(x)\n] − α Ez∼qφ(z|x) [ Dψ(Gθ(z,yt))\n] (13) where α is a coefficient which emphasizes the W-GAN loss. This objective is shared across all three components: the encoder, the synthesizer, and the discriminator. The synthesizer minimizes this loss whereas the discriminator maximizes it; consequently the two components have to be optimized in alternating order. For clarity, we summarize the training procedures in Alg. 1. Note that we actually use an update schedule for Dψ instead of training it to real optimality."
    }, {
      "heading" : "3. Experiments",
      "text" : ""
    }, {
      "heading" : "3.1. The dataset",
      "text" : "The proposed VC system was evaluated on the Voice Conversion Challenge 2016 dataset [8]. The dataset was a parallel speech corpus; however, frame alignment was not performed in the following experiments.\nWe conducted experiments on a subset of 3 speakers. In the inter-gender experiment, we chose SF1 as the source and TM3 as the target. In the intra-gender experiment, we chose TF2 as the target. We used the first 150 utterances (around 10 minutes) per speaker for training, the succeeding 12 for validation, and 25 (out of 54) utterances in the official testing set for subjective evaluations."
    }, {
      "heading" : "3.2. The feature set",
      "text" : "We used the STRAIGHT toolkit [9] to extract speech parameters, including the STRAIGHT spectra (SP for short), aperiodicity (AP), and pitch contours (F0). The rest of the experimental settings were the same as in [3], except that we rescaled log energy-normalized SP (denoted by logSP en) to the range of [−1, 1] dimension-wise. Note that our system performed frameby-frame conversion without post-filtering and that we utilized neither contextual nor dynamic features in our experiments."
    }, {
      "heading" : "3.3. Configurations and hyper-parameters",
      "text" : "The baseline system was the C-VAE system (denoted simply as VAE) [3] because its performance had been proven to be on par with another simple parallel baseline. In our proposed system, the encoder, the synthesizer, and the discriminator were convolutional neural networks. The phonetic space was 64- dimensional and assumed to have a standard normal distribution. The speaker representation were one-hot coded, and their embeddings were optimized as part of the generator parameters1."
    }, {
      "heading" : "3.4. The training and conversion procedures",
      "text" : "We first set α to 0 to exclude W-GAN, and trained the VAE till convergence to get the baseline model. Then, we proceeded on training the whole VAW-GAN via setting α to 50.\nConversion was conducted on a frame-by-frame basis as shown in Fig. 1c. First, Eφ inferred the phonetic content zn from xs,n Then, we specified a speaker identity (integer, the subscript t in yt) that retrieved the speaker representation vector y. The synthesizer Gθ then generated a conditional output frame x̂ using zn and yt."
    }, {
      "heading" : "3.5. Subjective evaluations",
      "text" : "Five-point mean opinion score (MOS) tests were conducted in a pairwise manner. Each of the 10 listeners graded the pairs of outputs from the VAW-GAN and the VAE. Inter-gender and intra-gender VC were evaluated respectively.\nThe MOS results on naturalness shown in Fig. 2 demonstrate that VAW-GAN significantly outperforms the VAE baseline (p-value 0.01 in paired t-tests). The results are in accordance with the converted spectra shown in Fig. 3, where the output spectra from VAW-GAN express richer variability across the frequency axis, hence reflecting clearer voices and enhanced intelligibility.\nWe did not report objective evaluations such as mean melcepstral coefficients because we found inconsistent results with the subjective evaluations. However, similar inconsistency is common in the VC literature because it is highly likely that those evaluations are inconsistent with human auditory systems [10, 11, 12]. The performance of speaker similarity was also\n1 Due to space limitations, the rest of the specification of hyper-parameters and audio samples can be found on-line: https://github.com/JeremyCCHsu/vc-vawgan\nunreported because we found that it remained about the same as that of [13] (System B in [14])."
    }, {
      "heading" : "4. Discussions",
      "text" : ""
    }, {
      "heading" : "4.1. W-GAN improved spectrum modeling",
      "text" : "As we can see in Fig. 3, the spectral envelopes of the synthetic speech from VAW-GAN are more structured, with more observable peaks and troughs. Spectral structures are key to the speech intelligibility, indirectly contributing to the elevated MOS. In addition, the more detailed spectral shapes in the highfrequency region reflect clearer (non-muffled) voice of the synthetic speech."
    }, {
      "heading" : "4.2. W-GAN as a variance modeling alternative",
      "text" : "The Wasserstein objective in (13) is minimized when the distribution of the converted spectrum pt|s is closest to the true data distribution p∗t . Unlike VAE that assumes a Gaussian distribution on the observation, W-GAN models the observation implicitly through a series of stochastic procedures, without prescribing any density forms. In Fig. 4, we can observe that the output spectra of the VAW-GAN system have larger variance compared to those of the VAE system. The global variance (GV) of the VAW-GAN output may not be as good as that of the data but the higher values indicate that VAW-GAN does not centralize predicted values at the mean too severely. Since speech has a highly diverse distribution, it requires more sophisticated analysis on this phenomenon."
    }, {
      "heading" : "4.3. Imperfect speaker modeling in VAW-GAN",
      "text" : "The reason that the speaker similarity of the converted voice is not improved reminds us of the fact that both VAE and VAWGAN optimize the same PGM, thus the same speaker model. Therefore, modeling speaker with one global variable might be insufficient. As modeling speaker with a frame-wise variable may conflict with the phonetic vector z, we may have to resort to other PGMs. We will investigate this problem in the future."
    }, {
      "heading" : "5. Related work",
      "text" : "To handle non-parallel VC, many researchers resort to framebased, segment-based, or cluster-based alignment schemes. One of the most intuitive ways is to apply an automatic speech recognition (ASR) module to the utterances, and proceed with explicit alignment or model adaptation [15, 16]. The ASR module provides every frame with a phonetic label (usually the phonemic states). It is particularly suitable for text-to-speech (TTS) systems because they can readily utilize these labeled frames [17]. A shortcoming with these approaches is that they require an extra mapping to realize cross-lingual VC. To this end, the INCA-based algorithms [18, 19] were proposed to iteratively seek frame-wise correspondence using converted surrogate frames. Another attempt is to separately build frame clusters for the source and the target, and then set up a mapping between them [20].\nRecent advances include [21], in which the authors exploited i-vectors to represent speakers. Their work differed from ours in that they adopted explicit alignment during training. In [22], the authors represented the phonetic space with senone probabilities outputted from an ASR module, and then generated voice by means of a TTS module. Despite differences in realization, our models do share some similarity ideally."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We have presented a voice conversion framework that is able to directly incorporate a non-parallel VC criterion into the objective function. The proposed VAW-GAN framework improves the outputs with more realistic spectral shapes. Experimental results demonstrate significantly improved performance over the baseline system."
    }, {
      "heading" : "7. Acknowledgements",
      "text" : "This work was supported in part by the Ministry of Science and Technology of Taiwan under Grant: MOST 105-2221-E-001012-MY3."
    }, {
      "heading" : "8. References",
      "text" : "[1] D. P. Kingma and M. Welling, “Auto-encoding variational\nbayes,” CoRR, vol. abs/1312.6114, 2013. [Online]. Available: http://arxiv.org/abs/1312.6114\n[2] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” CoRR, vol. abs/1701.07875, 2017. [Online]. Available: http://arxiv.org/abs/1701.07875\n[3] C. Hsu, H. Hwang, Y. Wu, Y. Tsao, and H. Wang, “Voice conversion from non-parallel corpora using variational autoencoder,” in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016, Jeju, South Korea, December 13-16, 2016. IEEE, 2016, pp. 1– 6. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2016. 7820786\n[4] M. Blaauw and J. Bonada, “Modeling and transforming speech using variational autoencoders,” in Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1770–1774. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2016-1183\n[5] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther, “Autoencoding beyond pixels using a learned similarity metric,” in Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, ser. JMLR Workshop and Conference Proceedings, M. Balcan and K. Q. Weinberger, Eds., vol. 48. JMLR.org, 2016, pp. 1558–1566. [Online]. Available: http://jmlr.org/proceedings/papers/v48/larsen16.html\n[6] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. C. Courville, and Y. Bengio, “Generative adversarial networks,” CoRR, vol. abs/1406.2661, 2014. [Online]. Available: http://arxiv.org/abs/1406.2661\n[7] C. Villani, Optimal Transport: Old and New, ser. Grundlehren der mathematischen Wissenschaften. Berlin: Springer, 2009.\n[8] T. Toda, L. Chen, D. Saito, F. Villavicencio, M. Wester, Z. Wu, and J. Yamagishi, “The voice conversion challenge 2016,” in Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1632–1636. [Online]. Available: http: //dx.doi.org/10.21437/Interspeech.2016-1066\n[9] H. Kawahara, I. Masuda-Katsuse, and A. de Cheveigné, “Restructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds,” Speech Commun., no. 3-4, pp. 187–207, 1999.\n[10] T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory,” IEEE Transactions on Audio, Speech, and Language Processing, 2007.\n[11] L.-H. Chen, Z.-H. Ling, L.-J. Liu, and L.-R. Dai, “Voice conversion using deep neural networks with layer-wise generative training,” IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 22, pp. 1506–1521, 2014.\n[12] H.-T. Hwang, Y. Tsao, H.-M. Wang, Y.-R. Wang, and S.-H. Chen, “A probabilistic interpretation for artificial neural network-based voice conversion,” Proc. APSIPA, 2015.\n[13] Y.-C. Wu, H.-T. Hwang, C.-C. Hsu, Y. Tsao, and H.-M. Wang, “Locally linear embedding for exemplar-based spectral conversion,” Proc. INTERSPEECH, in press.\n[14] M. Wester, Z. Wu, and J. Yamagishi, “Analysis of the voice conversion challenge 2016 evaluation results,” in Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1637– 1641. [Online]. Available: https://doi.org/10.21437/Interspeech. 2016-1331\n[15] M. Dong, C. Yang, Y. Lu, J. W. Ehnes, D. Huang, H. Ming, R. Tong, S. W. Lee, and H. Li, “Mapping frames with DNN-HMM recognizer for non-parallel voice conversion,” in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2015, Hong Kong, December 16-19, 2015. IEEE, 2015, pp. 488–494. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2015.7415320\n[16] M. Zhang, J. Tao, J. Tian, and X. Wang, “Text-independent voice conversion based on state mapped codebook,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2008, March 30 - April 4, 2008, Caesars Palace, Las Vegas, Nevada, USA. IEEE, 2008, pp. 4605–4608. [Online]. Available: http://dx.doi.org/10.1109/ ICASSP.2008.4518682\n[17] P. Song, W. Zheng, and L. Zhao, “Non-parallel training for voice conversion based on adaptation method,” in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26- 31, 2013. IEEE, 2013, pp. 6905–6909. [Online]. Available: http://dx.doi.org/10.1109/ICASSP.2013.6639000\n[18] D. Erro, A. Moreno, and A. Bonafonte, “INCA algorithm for training voice conversion systems from nonparallel corpora,” IEEE Trans. Audio, Speech & Language Processing, vol. 18, no. 5, pp. 944–953, 2010. [Online]. Available: http://dx.doi.org/ 10.1109/TASL.2009.2038669\n[19] Y. Agiomyrgiannakis, “The matching-minimization algorithm, the INCA algorithm and a mathematical framework for voice conversion with unaligned corpora,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2016, Shanghai, China, March 20-25, 2016. IEEE, 2016, pp. 5645–5649. [Online]. Available: http://dx.doi.org/10.1109/ICASSP.2016.7472758\n[20] H. Ney, D. Sündermann, A. Bonafonte, and H. Höge, “A first step towards text-independent voice conversion,” in INTERSPEECH 2004 - ICSLP, 8th International Conference on Spoken Language Processing, Jeju Island, Korea, October 4-8, 2004. ISCA, 2004.\n[21] J. Wu, Z. Wu, and L. Xie, “On the use of i-vectors and average voice model for voice conversion without parallel data,” in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016, Jeju, South Korea, December 13-16, 2016. IEEE, 2016, pp. 1–6. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2016.7820901\n[22] F. Xie, F. K. Soong, and H. Li, “A KL divergence and dnn-based approach to voice conversion without parallel training sentences,” in Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 287–291. [Online]. Available: http: //dx.doi.org/10.21437/Interspeech.2016-116\n[23] Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016, Jeju, South Korea, December 13-16, 2016. IEEE, 2016. [Online]. Available: http://ieeexplore.ieee.org/xpl/mostRecentIssue. jsp?punumber=7803478\n[24] N. Morgan, Ed., Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016. ISCA, 2016. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2016"
    } ],
    "references" : [ {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "CoRR, vol. abs/1312.6114, 2013. [Online]. Available: http://arxiv.org/abs/1312.6114",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Wasserstein GAN",
      "author" : [ "M. Arjovsky", "S. Chintala", "L. Bottou" ],
      "venue" : "CoRR, vol. abs/1701.07875, 2017. [Online]. Available: http://arxiv.org/abs/1701.07875",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Voice conversion from non-parallel corpora using variational autoencoder",
      "author" : [ "C. Hsu", "H. Hwang", "Y. Wu", "Y. Tsao", "H. Wang" ],
      "venue" : "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016, Jeju, South Korea, December 13-16, 2016. IEEE, 2016, pp. 1– 6. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2016. 7820786",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Modeling and transforming speech using variational autoencoders",
      "author" : [ "M. Blaauw", "J. Bonada" ],
      "venue" : "Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1770–1774. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2016-1183",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity metric",
      "author" : [ "A.B.L. Larsen", "S.K. Sønderby", "H. Larochelle", "O. Winther" ],
      "venue" : "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, ser. JMLR Workshop and Conference Proceedings, M. Balcan and K. Q. Weinberger, Eds., vol. 48. JMLR.org, 2016, pp. 1558–1566. [Online]. Available: http://jmlr.org/proceedings/papers/v48/larsen16.html",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Generative adversarial networks",
      "author" : [ "I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde- Farley", "S. Ozair", "A.C. Courville", "Y. Bengio" ],
      "venue" : "CoRR, vol. abs/1406.2661, 2014. [Online]. Available: http://arxiv.org/abs/1406.2661",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Optimal Transport: Old and New, ser",
      "author" : [ "C. Villani" ],
      "venue" : "Grundlehren der mathematischen Wissenschaften. Berlin: Springer,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "The voice conversion challenge 2016",
      "author" : [ "T. Toda", "L. Chen", "D. Saito", "F. Villavicencio", "M. Wester", "Z. Wu", "J. Yamagishi" ],
      "venue" : "Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1632–1636. [Online]. Available: http: //dx.doi.org/10.21437/Interspeech.2016-1066",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Restructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds",
      "author" : [ "H. Kawahara", "I. Masuda-Katsuse", "A. de Cheveigné" ],
      "venue" : "Speech Commun., no. 3-4, pp. 187–207, 1999.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory",
      "author" : [ "T. Toda", "A.W. Black", "K. Tokuda" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, 2007.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Voice conversion using deep neural networks with layer-wise generative training",
      "author" : [ "L.-H. Chen", "Z.-H. Ling", "L.-J. Liu", "L.-R. Dai" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 22, pp. 1506–1521, 2014.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A probabilistic interpretation for artificial neural network-based voice conversion",
      "author" : [ "H.-T. Hwang", "Y. Tsao", "H.-M. Wang", "Y.-R. Wang", "S.-H. Chen" ],
      "venue" : "Proc. APSIPA, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Locally linear embedding for exemplar-based spectral conversion",
      "author" : [ "Y.-C. Wu", "H.-T. Hwang", "C.-C. Hsu", "Y. Tsao", "H.-M. Wang" ],
      "venue" : "Proc. INTERSPEECH, in press.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Analysis of the voice conversion challenge 2016 evaluation results",
      "author" : [ "M. Wester", "Z. Wu", "J. Yamagishi" ],
      "venue" : "Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1637– 1641. [Online]. Available: https://doi.org/10.21437/Interspeech. 2016-1331",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Mapping frames with DNN-HMM recognizer for non-parallel voice conversion",
      "author" : [ "M. Dong", "C. Yang", "Y. Lu", "J.W. Ehnes", "D. Huang", "H. Ming", "R. Tong", "S.W. Lee", "H. Li" ],
      "venue" : "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2015, Hong Kong, December 16-19, 2015. IEEE, 2015, pp. 488–494. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2015.7415320",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Text-independent voice conversion based on state mapped codebook",
      "author" : [ "M. Zhang", "J. Tao", "J. Tian", "X. Wang" ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2008, March 30 - April 4, 2008, Caesars Palace, Las Vegas, Nevada, USA. IEEE, 2008, pp. 4605–4608. [Online]. Available: http://dx.doi.org/10.1109/ ICASSP.2008.4518682",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Non-parallel training for voice conversion based on adaptation method",
      "author" : [ "P. Song", "W. Zheng", "L. Zhao" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26- 31, 2013. IEEE, 2013, pp. 6905–6909. [Online]. Available: http://dx.doi.org/10.1109/ICASSP.2013.6639000",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "INCA algorithm for training voice conversion systems from nonparallel corpora",
      "author" : [ "D. Erro", "A. Moreno", "A. Bonafonte" ],
      "venue" : "IEEE Trans. Audio, Speech & Language Processing, vol. 18, no. 5, pp. 944–953, 2010. [Online]. Available: http://dx.doi.org/ 10.1109/TASL.2009.2038669",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The matching-minimization algorithm, the INCA algorithm and a mathematical framework for voice conversion with unaligned corpora",
      "author" : [ "Y. Agiomyrgiannakis" ],
      "venue" : "2016 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2016, Shanghai, China, March 20-25, 2016. IEEE, 2016, pp. 5645–5649. [Online]. Available: http://dx.doi.org/10.1109/ICASSP.2016.7472758",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A first step towards text-independent voice conversion",
      "author" : [ "H. Ney", "D. Sündermann", "A. Bonafonte", "H. Höge" ],
      "venue" : "INTERSPEECH 2004 - ICSLP, 8th International Conference on Spoken Language Processing, Jeju Island, Korea, October 4-8, 2004. ISCA, 2004.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "On the use of i-vectors and average voice model for voice conversion without parallel data",
      "author" : [ "J. Wu", "Z. Wu", "L. Xie" ],
      "venue" : "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016, Jeju, South Korea, December 13-16, 2016. IEEE, 2016, pp. 1–6. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2016.7820901",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A KL divergence and dnn-based approach to voice conversion without parallel training sentences",
      "author" : [ "F. Xie", "F.K. Soong", "H. Li" ],
      "venue" : "Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 287–291. [Online]. Available: http: //dx.doi.org/10.21437/Interspeech.2016-116",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In this paper, we present a specific implementation in which a variational autoencoder (VAE [1]) assumes the inference task and a Wasserstein generative adversarial network (W-GAN [2]) undertakes speech synthesis.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "In this paper, we present a specific implementation in which a variational autoencoder (VAE [1]) assumes the inference task and a Wasserstein generative adversarial network (W-GAN [2]) undertakes speech synthesis.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 2,
      "context" : "Recent works have proven the viability of speech modeling with VAEs [3, 4].",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Recent works have proven the viability of speech modeling with VAEs [3, 4].",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "For every input (xn,yn), we can sample the latent variable zn using the re-parameterization trick described in [1].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "We can improve the C-VAE by incorporating a GAN objective [5] into the decoder.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "A vanilla GAN [6] consists of two components: a generator (synthesizer) Gθ that produces realistic spectrum and a discriminatorDψ that judges whether an input is a true spectrum or a generated one.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "We can combine the objectives of VAE and GAN by assigning VAE’s decoder as GAN’s generator to form a VAEGAN [5].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "In contrast, we can directly optimize a non-parallel VC loss by renovating DJS with a Wasserstein objective [2].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "On the other hand, the Kantorovich-Rubinstein duality [7] of (9) allows us to explicitly approach non-parallel VC:",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "The proposed VC system was evaluated on the Voice Conversion Challenge 2016 dataset [8].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "We used the STRAIGHT toolkit [9] to extract speech parameters, including the STRAIGHT spectra (SP for short), aperiodicity (AP), and pitch contours (F0).",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "The rest of the experimental settings were the same as in [3], except that we rescaled log energy-normalized SP (denoted by logSP en) to the range of [−1, 1] dimension-wise.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "The baseline system was the C-VAE system (denoted simply as VAE) [3] because its performance had been proven to be on par with another simple parallel baseline.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "However, similar inconsistency is common in the VC literature because it is highly likely that those evaluations are inconsistent with human auditory systems [10, 11, 12].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "However, similar inconsistency is common in the VC literature because it is highly likely that those evaluations are inconsistent with human auditory systems [10, 11, 12].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 11,
      "context" : "However, similar inconsistency is common in the VC literature because it is highly likely that those evaluations are inconsistent with human auditory systems [10, 11, 12].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "unreported because we found that it remained about the same as that of [13] (System B in [14]).",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "unreported because we found that it remained about the same as that of [13] (System B in [14]).",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "One of the most intuitive ways is to apply an automatic speech recognition (ASR) module to the utterances, and proceed with explicit alignment or model adaptation [15, 16].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 15,
      "context" : "One of the most intuitive ways is to apply an automatic speech recognition (ASR) module to the utterances, and proceed with explicit alignment or model adaptation [15, 16].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "It is particularly suitable for text-to-speech (TTS) systems because they can readily utilize these labeled frames [17].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "To this end, the INCA-based algorithms [18, 19] were proposed to iteratively seek frame-wise correspondence using converted surrogate frames.",
      "startOffset" : 39,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "To this end, the INCA-based algorithms [18, 19] were proposed to iteratively seek frame-wise correspondence using converted surrogate frames.",
      "startOffset" : 39,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "Another attempt is to separately build frame clusters for the source and the target, and then set up a mapping between them [20].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 20,
      "context" : "Recent advances include [21], in which the authors exploited i-vectors to represent speakers.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : "In [22], the authors represented the phonetic space with senone probabilities outputted from an ASR module, and then generated voice by means of a TTS module.",
      "startOffset" : 3,
      "endOffset" : 7
    } ],
    "year" : 2017,
    "abstractText" : "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.",
    "creator" : "LaTeX with hyperref package"
  }
}