{
  "name" : "1602.05772.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure",
    "authors" : [ "Stefan Gerdjikov", "Klaus U. Schulz" ],
    "emails" : [ "gerdjikov@abv.bg", "schulz@cis.uni-muenchen.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "While there is no complete agreement on what the structural subparts of natural language exactly are, there is a general understanding that natural language is built ∗st gerdjikov@abv.bg †schulz@cis.uni-muenchen.de\nar X\niv :1\n60 2.\n05 77\n2v 1\n[ cs\nin a compositional way using characteristic classes of substrings (or utterances) such as phrases, words, and morphems. In natural language processing (NLP) and related fields, all these units play an important practical role. Phrases are used, e.g., in Statistical Machine Translation (SMT) [ON04, Koe10], Text Classification [Seb01, DIK11], and Information Retrieval [MRS08, Lui07, ZLC+13]. Words (“tokens” and “types”) represent the basis for text indexing approaches in Information Retrieval, and since centuries linguists collect words in general and specific lexica, including typical contexts and collocations. Morphems carry information on part-of-speech, syntax and semantic roles [Lui07].\nIn this paper we present results from an ongoing research project which is centered around the problem of how to detect structural subparts of natural (or other) language and is characterized by the following methodological principles.\n1. Try to detect meaningful units of any length, as opposed to, e.g., fixed length n-grams of words or letters.\n2. Only use corpora (or sequence data) as an empirical basis to find these units.\n3. When analyzing corpora, do not use any kind of prior linguistic knowledge and no knowledge on the nature/functionality of a particular symbol (such as blank, hyphen, brackets, uppercase vs. lowercase etc.) but rather simple mathematical assumptions.\n4. Proceed in a completely unsupervised way.\nWhile we acknowledge the success of NLP approaches based on n-gram statistics, it is clear that using units of fixed length does not naturally lead to meaningful subparts. In anatomic terms, our aim here is to find the “muscles, organs, and nerves” of a given language corpus. The other above principles are related in the sense that they stress purely empirical evidence. Corpora, from our point, are just raw sequences of symbols. For analyzing corpora we do not use prior linguistic knowledge since we want to see what “the corpus structure itself tells us about language”. This might help to compare existing linguistic knowledge and hypotheses with “purely empirical” corpus-driven evidence. Furthermore, principles and methods obtained from this puristic approach can be applied to any (not necessarily natural) language and might contribute to the analysis of other kind of sequence data such as biological sequences, sequence of gestures, animal language, music annotation, and others. We avoid any kind of supervision since we are not primarily interested in the path from prior human linguistic knowledge to the analysis of data, but rather on the question how structure can be detected and then “destilled” to knowledge. The focus of this paper is the mining of phrases, subphrase\nstructure, and words. From a practical side, one vision is to automatically and “online” create a kind of “general dictionary and grammar of a given corpus” that collects, connects and relates all interesting language units of the corpus, with a direct reference to all occurrences. In the Conclusion we add a remark on possible interaction of our approach with unsupervised morphology induction approaches, such as [Gol01, Zem08, CL05, SO15].\nAs a matter of fact, this programmatic approach is by no means new. As an early influential advocate, Zellig Harris [Har54, Har91] has stressed that corpus analysis based on mathematical principles should be used to derive empirical evidence for linguistic hypotheses. Later, Maurice Gross [Gro93, Gro99], Franz Guenthner [Gue05], Johannes Goller [Gol10] and others followed this line. Corpus analysis is often used to find lexico-semantic relations [Sto10]. Further related work is described in Section 6. One algorithmic contribution of our work is the use of a special index structure for corpora as a basis: symmetric directed acyclic word graphs (SDAWGs) [IHS+01] represent a generalization of suffix trees [Ukk95]. Using the SDAWG index of a corpus, for any substring of the corpus we have immediate access to all left and right contexts of arbitrary length. In this way we see “how” a substring occurs in the corpus (i.e., in which contexts and larger sequences, how often), which provides an excellent basis for recognizing meaningful units and connected tasks.\nWhen asking for the composition of natural language in terms of meaningful subparts, a fundamental question is how small parts are combined to form larger units. Most grammar approaches (e.g., context-free grammars) are “concatenation based” in the sense that always disjoint, non-overlapping parts are glued together to build larger units. In contrast, our base assumption is that in general “phrases” can overlap, and combination principles go beyond plain concatenation.\nOur base view of language composition is briefly explained in Section 2. Using this view, Section 3 introduces mathematical principles and an algorithm for recognizing meaningful units and decomposing sentences into “phrases”. We illustrate the sentence decompositions and “phrases” obtained for a variety of distinct languages (English, German, French, Spanish, Dutch, Chinese) and corpora (including Wikipedia, Europarl, Medline). Section 4 introduces principles for further decomposing “phrases”. The base units found in this way (in most cases) are “words”. Recall that we start from a point where we do not have the concept of a “word”. Again we presents results obtained for distinct corpora and languages. In Section 5 we discuss some possible applications of the phrase detection mechanisms: first we show how subphrase structure can be used to find a ranked spectrum of “most characteristic words” of the given corpus, and to extract paradigmatic word networks based on co-occurrence of terms in phrases. We also look at stylis-\ntic corpus analysis, terminology extraction, and automated query expansion for search engines. In Section 6 we briefly comment on related work. Section 7 gives a conclusion and comments on future work."
    }, {
      "heading" : "2 Phrase model - intuitions and motivation",
      "text" : "As a starting point we look at the notion of a “phrase”. When we want to recognize “phrases” in a corpus-driven way we need as a starting point some primitive base assumptions how “meaningful phrases” can be recognized and distinguished from arbitrary, “meaningless subsegments” of language. In our view, recognition of phrases should proceed in parallel with the recognition of those language units that connect and interlink phrases. Our main motivation comes from psycholinguistics [HW03, SWC06, Tra07]: Young children recognise function words but do not use them in the beginning. This suggests that there are some natural principles that govern the structure of natural language that are strongly related to the use of function words. Children are able to detect these regularities, and to distinguish between language parts that serve to combine and connect meaningful parts - function words - on the one-hand side, and meaningful parts - content words and phrases - on the other side.\nFunction words, or closed-class words, alone bear little sense, [Fri52]. Their main function is to bind other words or expressions in a grammatically correct way to form more complex meaningful language units. To suit these needs of the language, only a few function words, e.g. prepositions, articles, suffice. Thus, although some specific word may act as a function word in a special domain, generally the set of function words is stable in every particular language.\nIn our situation we do not assume that we know the set of function words, and the notion of a “word” is partially misleading: we do not assume that “phrase connectives” are always words, we even do not have a notion of “word” at the beginning. Yet, for convenience, we say “function word” when we mean “phrase connective”. We assume that the corpus consists of sentences1 which make sense. The remaining assumptions we make about phrases and function words in the corpus are:\n(A1) Phrases occur in different contexts.\n(A2) Sentences are built of (overlapping) phrases.\n(A3) Phrases overlap on function words. 1Other small meaningful text units like paragraphs also conform with our model.\nThe first assumption means that the corpus provides structural evidence that helps to recognize phrases. The circumstance that the corpus consists of sentences that make sense is important for the second and third assumptions2. Assumption (A2) follows the intuition that the sense of a sentence is determined by the sense of the individual phrases and the way they are composed, whereas the final assumption reflects the fact that function words bind smaller meaningful language units to build larger ones. In our case, the smaller meaningful language units are the phrases we are looking for, and the larger ones are the sentences in the corpus.\nSince function words alone, to a large extent, are lacking sense, we can fairly approximate the sense of the sentence with the sense accumulated by the individual phrases. Thus, we do not need to bother of modelling what sense is, rather we will be concerned with the issue how regular w.r.t. the given corpus the bindings between the phrases are. We try to summarise this intuition in the following two principles:\n(P1) The function words occur regularly as the boundaries of phrases in the corpus.\n(P2) A particular sentence should exhibit these regularities as good as its structure w.r.t. the corpus allows."
    }, {
      "heading" : "3 Computing phrases and function words",
      "text" : "Following the considerations of the previous section, we first look at the problem of how to compute the phrases and function words of a corpus. In the first subsection we develop an algorithm to solve this task. Afterwards we present the results obtained for distinct languages and corpora."
    }, {
      "heading" : "3.1 Algorithmic principles",
      "text" : "We consider a corpus, C, consisting of a finite number of sentences. The individual sentences are merely nonempty strings Sn ∈ Σ+ over a finite alphabet Σ. In practice, the alphabet Σ is the set of all the symbols, i.e. letters, punctuation, white space, etc., that were used to create the corpus. According to Assumption (A1), the phrases are among those strings S ∈ Σ∗ that occur in different contexts within the corpus C. Formally, for a phrase S the following two properties must hold:\n2However, this is not a prerequisite for the mathematical approach described in the next section.\n1. there are distinct a, b ∈ Σ such that aS and bS occur in C, or S is a prefix of some of the sentences Sn, and\n2. there are distinct c, d ∈ Σ such that Sc and Sd occur in C, or S is an suffix of some of the sentences Sn.\nStrings with the above two properties are called general phrase candidates of the corpus, the set of general phrase candidates is denoted G(C). For example, if the strings unfortunate and fortunately occur as part of some sentences in the corpus3, then they witness that fortunate can be followed by at least two distinct letters, l and , and similarly it can be preceded by at least two distinct letters, n and . Therefore fortunate is a general phrase candidate in this case. However, if it turned out that the string ortuna is always preceded by the letter f in the corpus, then the string ortuna will be not a member of G(C). The same would be true if ortuna was always followed by a t in the corpus.\nAccording to (A2), a sentence Sn in the corpus can be decomposed into a sequence of nonempty strings (P1, P2, . . . , Pkn) with kn > 1 such that Pi ∈ G(C). For instance, if a sentence starts like\nWe are not making the connection ...\nit may be the case that P1 = We are not , P2 = are not making the , P3 = the connection. In this case P1 and P2 overlap on the string F1 = are not , whereas P2 and P3 overlap on the string F2 = the . In general, Pi and Pi+1 overlap on a common possibly empty string Fi ∈ Σ∗, i.e. Pi = P ′i ◦ Fi and Pi+1 = Fi ◦ P ′′i+1, such that:\nSn = P1 ◦ F−11 ◦ P2 ◦ F −1 2 · · · ◦ F −1 kn−1 ◦ Pkn .\nWe call the Fi’s function strings. Upon termination of our approach will declare some of these strings to be function words. Recall that if Pi, Pi+1 are in the set of general phrase candidates G(C), then each suffix of Pi satisfies Property 2 and each prefix of Pi+1 satisfies Property 1. In particular, since Fi is a suffix of Pi and Fi is a prefix of Pi+1, it follows that each function string Fi is always a general phrase candidate.\nIn general, there can be many decompositions, (P1, P2, . . . , Pkn), for a given sentence Sn. According to Principle (P2) we should select the one(s) that exhibit(s) the most regular sequence of function strings, (F1, F2, . . . , Fkn−1). Further, according to Principle (P1), the function strings should occur regularly as the boundaries of phrases. We will incorporate these two principles in terms of appropriate probability and likelihood measures.\n3We use the symbol “ ” to make a white space explicit.\nTo do so, let us assume for a moment that we have already determined a multiset of phrases P ⊆ G(C). Based on (P1) we are going to assess the property of a function string F to be a function word in the corpus. Next, based on (P2), we are going to determine the best decomposition of each sentence in the corpus as a sequence of general phrase candidates. This second step will allow us to refine (redefine) the multiset of phrases we have started with. We can then iterate this process until the multiset of phrases stabilizes. Based on the obtained probabilities and the eventual sentence decompositions we define the ultimate set of function words and phrases.\nFormally, to assess the property of F to be a function string w.r.t. the current multiset of phrases P , we simply consider how often F occurs as a proper prefix, suffix or proper infix in P:\npref(F |P) = |{F ◦ S ∈ P |S ∈ Σ+}| suff(F |P) = |{S ◦ F ∈ P |S ∈ Σ+}|\ninf(F |P) = |{S1FS2 ∈ P \\ {F} | (S1, S2) ∈ (Σ∗)2}|.\nThis statistics accounts for multiplicities. For example, if F = the and the phrase P = the connection occurs 12 times in P , then P will contribute with 12 units to pref( the |P). Now, we define the probability of a string F ∈ G(C) to be a function word w.r.t. to a given multiset P ⊆ G(C) as the empirical probability that F is both a proper prefix and a proper suffix of a string in P:\npfw(F |P) = ppref (F |P) · psuf (F |P) (1)\nwhere\nppref (F |P) = pref(F |P) inf(F |P) , psuf (F |P) = suff(F |P) inf(F |P) .\nTo model (P2) we optimize the likelihood w.r.t. pfw(.|P). This means, that given a sentence Sn ∈ C we search for those sequences of strings (P1, P2, . . . , Pkn) (kn > 1) whose sequence of overlaps F = (F1, F2, . . . , Fkn−1) maximises the function:\n`(F |Sn,P, C) = kn−1∏ i=1 pfw(Fi|P). (2)\nDetermining the maxima of Equation 2 supplies us with (an) optimal decomposition(s) (P1, P2, . . . , Pkn) for each sentence Sn in the corpus C. We call a substring Sn[i..j] of Sn an optimal phrase candidate for Sn if it belongs to some optimal decomposition for Sn. We call the overlap of two consecutive strings in an optimal decomposition for Sn an optimal overlap. Now, we are in a position to reconsider\nthe multiset of phrases P we have started with. The intuition is, that if we have made a reasonable even if not perfect guess for the set of phrases in the first step, then we should have obtained good scores for the typical function words. Thus, in the second step, optimising the likelihood across the sentences the function words will guide us to optimal strings which better describe the proper phrases in C. Following these considerations we define the new multisets\nF(P) := {Sn[j1..j2] |Sn[j1..j2] optimal overlap in a sentence Sn} P ′ := {Sn[j1..j2] |Sn[j1..j2] optimal phrase candidate for a sentence Sn}.\nAlthough the approach described above reflects principles (A1) - (A3) as well as (P1) and (P2), there is one subtle point that it overlooks. The optimisation of the likelihood implicitly struggles for shorter sequences of phrases. For instance, if\nP1 = are not making the\nP2 = the connections"
    }, {
      "heading" : "P = are not making the connections",
      "text" : "are three general phrase candidates, the optimisation of Equation 2 in situations like We are not making the connections... will always prefer the longer phrase, P , rather than the two shorter overlapping phrases P1 and P2. Thus, P = are not making the connections contributes to the count inf( the |P), but it will not contribute to the counts pref( the |P) and suff( the |P). This is a problem, because if are not making the and the connections are indeed phrases, then the presence of the phrase\nare not making the connections\nwitnesses that the acts as binding element in this case. However, this example will contribute to decreasing instead of increasing the probability pfw( the |P).\nTo remedy this inconsistency in the model, we take additional care for the cases described above and reflect them in the probability measure for a function word. Specifically, we define the set of stable prefix (suffix) phrases of a phrase: let P ∈ P . A prefix (suffix) P ′ of P is a stable prefix (suffix) phrase of P w.r.t. the the set of phrases P iff two conditions are satisfied:\n1. P ′ is a phrase in P ,\n2. every prefix (suffix) P ′′ of P of length |P ′′| > |P ′| which is a general phrase candidate is again a phrase in P .\nBy SPref(P |P) and SSuff(P |P) we respectively denote the set of all stable prefix (suffix) phrases of phrase P . Now, we account for the inconsistency of the model described above in the following way. We consider phrases P ∈ P which can be represented as two overlapping phrases P1, P2 ∈ P but at least one of them should be stable. This means that either P1 is a stable prefix phrase of P , or P2 is a stable suffix phrase of P . The overlap value ov(F |P) of a function string F w.r.t. the set of phrases P is defined as the number of phrases, P , in the from P1 ◦F−1 ◦P2 ∈ P where (P1, P2) ∈ P2 such that P1 ∈ SPref(P ) or P2 ∈ SSuff(P ). As before we account for possible multiplicities P in the set P . The overlap value ov(F |P) is used to boost the empirical probabilities ppref (F |P) and psuf (F |P):\np̃pref (F |P) = ppref (F |P) + ηpref (F |P) ov(F |P) inf(F |P)\np̃suf (F |P) = psuf (F |P) + ηsuf (F |P) ov(F |P) inf(F |P)\nwhere ηi(F |P) = pi(F |P)/(ppref (F |P) + psuf (F |P)) for i ∈ {pref,suf}. Essentially, the above formulae assume that the counts pref(F |P) and suff(F |P) are representative and thus the ratio pref(F |P)/suff(F |P) reliably represents the true ratio of prefix-suffix property. Yet, due to cases accounted for in ov(F |P), they are underestimated with respect to the total number of occurrences of F as an infix. This is why we distribute the amount ov(F |P) as to maximise the entropy w.r.t. the empirical pref(F |P)/suff(F |P). In the final model instead of ppref and psuf in Equation 1 we use p̃pref and p̃suf , respectively.\nAlgorithm for detecting phrases and function words. Our complete algorithm for detecting phrases and function words proceeds in the following way. 1. Initialisation. We start with the multiset of phrases P0 := G(C). The multiplicity of each string is simply the total number of occurrences of the string in the corpus. 2. Iteration. Afterwards we follow the spirit of the Maximum A Posteriori Principle, [BTS87, GJ08], and compute p̃(i)pref = p̃pref (.|Pi) and p̃ (i) suf = p̃suf (.|Pi) and define the next set of phrases as Pi+1 = P ′i (see above). 3. Termination. As in other similar approaches the final goal is to obtain convergence. In our case we want to arrive at a state where the parameters Pi, p̃pref and p̃suf stabilize. Since the probability functions p̃pref and p̃suf are entirely determined by the current set of phrases Pi it is enough to define a convergence criterium for the multisets Pi.\nIn order to assess the similarity between the multisets Pi and Pi+1, we use a pure set theoretical measure. Let mi(P ) denote the multiplicity of P in the set Pi,\ni.e. the number of times the general phrase candidate P is assigned to Pi. We can express the size of the symmetric difference Pi∆Pi+1 of the two multisets Pi and Pi+1 as\n‖Pi∆Pi+1‖ = ∑ P |mi(P )−mi+1(P )|.\nSimilarly we compute the size of the union Pi ∪ Pi+1 of the two multisets as ‖Pi ∪ Pi+1‖ = ∑ P max(mi(P ),mi+1(P )).\nAs a similarity measure between the multisets Pi and Pi+1 we use the relative size\nρi = ρ(Pi,Pi+1) = ‖Pi∆Pi+1‖ ‖Pi ∪ Pi+1‖ .\nWe iterate Step 2 until:\n[Halting Criterion] ρi ≤ ρi+1 + θ, where θ = 10−6. (3)\nWe will comment on this halting criterion at the end of this section. 4. Final model. Assume that the algorithm terminates after n+1 iterations of Step 2 when for the first time ρn+1 ≥ ρn − θ. We define:\nP = Pn p̃pref = p̃ (n) pref\np̃suf = p̃ (n) suf\npfw(F | P) = p̃pref (F |P) · p̃suf (F |P).\nWe define the set of function words of the corpus, F(C), as the set of strings F ∈ Fn that satisfy the following properties:\n• F has multiplicity at least |C|/103 in F(P) = F(Pn),\n• p̃pref (F |P) + p̃suf (F |P) > 0.4 and\n• p̃pref (F |P)/p̃suf (F |P) ∈ (1/4; 4).\nThe first condition expresses that F not only occurs often in the corpus, but also serves as an overlap in many different cases. The second and third conditions4 capture the intuition that F is both a typical prefix and suffix boundary of a phrase, and not only, say, a typical prefix.\n4Note that the inequality p̃pref (F |P) + p̃suf (F |P) ≤ 1 is always fulfilled.\nOur experiments have shown that the particular choice of these parameters is not significant. Particularly, among the strings satisfying the above conditions almost all achieve p̃pref (F |P)+p̃suf (F |P) > 0.5 whereas the ratio between the prefix and suffix property varies in spans (1/3; 3). Additionally, typically the strings meeting these constraints have total probability at rates:\np̃pref (F |P)p̃suf (F |P) > 9/100\nwhere the possible maximum is 25/100 (attained for p̃pref (F |P) = p̃suf (F |P) = 0.5). At the same the strings which are pruned have probabilities of one magnitude less.\nWe define the set of phrases of the corpus, P(C) as those P ∈ P that (i) start with some element of F(C) or are the beginning of a sentence and at the same time (ii) end with some element of F(C) or are the end of a sentence.\nRemarks on the Halting Criterion. Naively, one could expect that the similarity ρi (normalized size of symmetric difference) between two consecutive sets of phrases converges to 0. However, in practice this does not happen The reason for this phenomenon are rare strings which in particular sentences can serve both as overlaps in an ambiguous sequence of phrases. Thus, when they achieve good scores as overlaps the number of phrases they participate increases and this makes their scores drop in the next phase. However, when they are not active the part of the phrases where they are suffixes and prefixes become optimal and they boost the probabilities of these rare strings implying a vicious circle.\nThis is confirmed in our experiments, see Figure 1, where we depict in logarithmic scale the behaviour of ρi for distinct datasets. Multiset similarities ρi start with values around 1 and drop very quickly to values below 0.01. Afterwards values ρi slowly decrease to stabilise at rates 0.2× 10−3 and 0.5× 10−2, but they do not continue to converge to 0. Nevertheless, the multisets Pi exhibit a rather stable behaviour. In fact, more than 90% of the value ρi is due to cases where the multiplicity of a phrase P is increased or decreased by just 1, |mi(P )−mi+1(P )| = 1. Further the weighted signed symmetric difference between Pi and Pi+1:\nδi = ∑ P mi(P )−mi+1(P ) ‖Pi ∪ Pi+1‖ .\nranges at rates |δi| ∈ [10−6; 10−4] and below, which shows that typically a “small gain” for one phrase is compensated by a “small loss” of another phrase. This means, that on average, the changes between p(i+1)pref and p (i) pref (resp. p (i+1) suf and p (i) suf ) are small for function words with high probabilities. This shows that function words with high probabilities p(i)fw do not change significantly their probabilities to\np (i+1) fw . Whenever such a function word is admissible in a particular decomposition, it will be selected for a split. “Persistent turbulences” are caused by words occurring only once, or words occurring in a very specific syntactical contexts.\nTechnical realization. Technically, the set of all general phrase candidates G(C) can be represented after a linear preprocessing in linear space as a compressed acyclic word graph, [BBH+87, IHS+01], which generalises suffix trees, [Ukk95]. In this structure the required statistics, pref , suff , inf and ov can be easily collected by a straightforward traversal. Meanwhile, the optimisation required in Equation 2 reduces to a standard dynamic programming scheme. The graph necessary for this computation can be constructed by the means of an Aho-Corasicklike algorithm, [AC75]. Further speed-up can be achieved by an A-star algorithm, [HNR68, HNR72], built on top of the shortest path problem algorithm in proper interval graphs, [ACL93]."
    }, {
      "heading" : "3.2 Experimental basis - languages and corpora",
      "text" : "We considered corpora of different language, domain and size, see Table 1. The corpora cover six languages: English, German, French, Spanish, Dutch, and Chinese. Domains covered vary from philosophy through scientific medical abstracts, politics and journalistic articles, to general mixed topics. The size of the corpora scaled from several thousand to several Million sentences.\nWittgenstein Corpus. The Wittgenstein Corpus consists of 3871 original remarks of the German philosopher Ludwig Wittgenstein. Although the general language is German, the corpus also contains a few remarks in English.\nMedline Corpus. The Medline Corpus is a collection of about 15 Million medical abstracts in English. Each abstract consists of about 10 sentences. We considered two subsets of this corpus. They were obtained by randomly selecting 0.5% and 10%, respectively, of the abstracts and gathering all the sentences in the resulting abstracts. The 0.5%-Medline Corpus consists of 800 K sentences, whereas the 10%-Medline Corpus consists of about 15 M sentences.\nEU-Parliament Corpus. The EU-Parliament Corpus is a collection of the political statements in EU-Parliament from 1997 through 2012. The statements are maintained in all the languages of the EU-members at the date of the statement. Thus the entire corpus amounts to ca. 2 Million sentences for the languages of the long-lasting EU-members, English, German, etc. and to several hundreds of thousands for the more recent EU-members.\nFrom this corpus we selected excerpts from the 2000-2001 sessions of the EUParliament in English, German, French, Spanish and Dutch. The size of the se-\nlected corpora amounts to 20 K sentences. We also processed the entire French and Spanish corpora, amounting at about 2 Million sentences, each.\nAccountant Corpus. Accountant Corpus is a collection of the issues of the Dutch newspaper “Accountant”. The texts are from the period of World War I. The digital version of the corpus was obtained through OCR. We considered as sentences sequences of characters separated by at least two new lines. Single new lines were replaced by white spaces. In this way we tried to roughly reflect the natural reading order of paper articles. The obtained corpus, Accountant, contains about 500 K different sentences (of total 780 K) most of which have the structure of a paragraph or article.\nWikipedia. We considered also dumps of the English and German Wikipedias from May 14, 2014. In the English version we selected the pages AA – AG in their raw form. In the German Corpus we selected the pages AA – AH. We used WIKIPEDIAEXTRACTOR [AF13] to remove the hyperlinks. In both cases the sentences were defined as sequences of characters terminated by a new line. In this way we roughly reflect the paragraph structure of these electronic resources. The size of the English corpus amounted to 1,1 Million paragraphs of which more than 890 K different and the German Corpus resulted in about 650 K paragraphs, more than 510 K of which were distinct.\nChinese Corpus. “保镖天下” (”Bodyguard Legend”) is a Chinese electronic fiction which is freely available online for research, [htt]. It contains about 24K of small paragraphs or single sentences. The total number of characters is about 2M."
    }, {
      "heading" : "3.3 Results on function words",
      "text" : "In this section we comment on the function words, F(C), that the algorithm described in the previous sections detected in the various corpora. Table 2 gives an overview for the European languages and distinct corpora. For each corpus we present the number of different strings in |F(C)|, and we present a top segment of the function words. At this point, the order of the function words is determined by the number of times a particular string has occurred as an overlap in an optimal decomposition. The corresponding results for the Chinese corpus are illustrated at the end of this subsection.\nSpeaking in general terms, the number of the detected function words varies between 100–300. The results in Table 2 suggest that the actual number of function words does not depend on the size of the corpus, rather on its domain. For instance, the two Medline excerpts have a similar number of function words, although the size of one corpus is 20 times the size of the other. It is expected that in corpora with paragraph-like structure the number of function words increases. This is due\nto binding words occurring in the beginning of sentences which in a sentence-based corpus could not be used as overlaps. The ratio of the corpus size to the number of corpus units also exerts certain role.\nFurther, in a domain-specific corpus, such as EU-Parliament or Medline, we may expect that certain words usually considered as content words will often occur at the boundaries of phrases. For example5, this is the case with patients with in the larger Medline corpus and with patients and cell in the smaller one. A manual inspection of the results reveals that there are 10-20 such strings in the domain-specific corpora which have the above property, yet only one or two in the Wikipedia corpora, i.e. million and John .\nBesides the particular cases mentioned above, the obtained lists comprise of white space, punctuation symbols, articles, prepositions, pronouns, auxiliary verbs. A significant number of function “words” are multi-tokens such as “to the”. All the function words in the Medline corpora and in most EU-Parliament corpora (EN, DE, NL and the smaller ES and FR) are delimited by white-spaces or punctuation symbols. There is a minimal number of exceptions in the other corpora (two in\n5Here and in what follows we use the symbol to highlight an occurrence of the white-space symbol.\nthe English Wikipedia, one in the larger Spanish, and three exceptions in the larger French corpus, five in the Dutch Accountant Corpus and German Wikipedia and two in the Wittgenstein Corpus). The reason for these exceptions, e.g. “e. ”, “es. ”, “s. ” in French or “en. ” in German (Wittgenstein Corpus) and Dutch (Accountant), “s\" ” in the English Wikipedia, can be explained in the following way. Some classes of words are underrepresented in the context of specific punctuation symbols. For instance, in German relative clauses often end with a verb with suffix en. Thus, we often observe “en. ” at the end of the sentence. Yet, considering a particular verb form it is by no means evident that it will occur twice at the end of a sentence.\nFunction words obtained for the “保镖天下” corpus. Since in Chinese the words of a sentence are not delimited it was interesting to see how the approach would work in this situation. Remarkably, results do not strongly differ from the other cases. For our Chinese corpus, “保镖天下”, which represents fiction, we obtained a total of 291 function words. We present the top 50 function words most often used in the decompositions with accompanying English equivalent. “” (empty string); “，” (comma white space); “的” (of); “了” (over); “是” (is/are); “你” (you); “我” (I/me); “这” (this); “人” (people/human); “着” (no specific meaning in Chinese); “在” (at/in); “就” (no specific meaning in Chinese); “”” (apostrophs); “黎箫” (Li Xiao – a person’s name in this fiction); “他” (he); “也” (also/too); “到” (arrive); “？” (?); “都” (both/all); “去” (go); “！” (!); “来” (come); “还” (still/again); “看着” (look/stare); “！” ( !); “！”” ( !); “那” (that); “一个” (one); “出” (go/get out); “上” (up/above); “要” (need); “和” (and); “：“” (:apostrophs) “自己” (myself); “将” (will/shall); “没有” (no/didn’t);\n“能” (can/be able to); “后” (behind/later); “被” (be–used in passive tense, before a verb); “会” (will/can); “向” (towards); “已经” (already); “让” (let); “对” (write/correct); “却” (but/and yet/however); “给” (give/provide); “竟然” (unexpectedly/to one’s surprise); “我们” (we/us); “一脸” (the whole face); “现 在” (now/currently);"
    }, {
      "heading" : "3.4 Quantitative results on phrases",
      "text" : "In this section we provide some quantitative statistics on the multiset of phrases P extracted in the final step of our algorithm. From our point of view it is interesting to assess how complex the resulting phrases are and how often they have been selected by the algorithm to obtain an optimal decomposition of a sentence.\nTo assess the first characteristic of P(C) we counted in a postprocessing step the number of words, word(P ), contained in each of the strings P ∈ P(C). This\nwas done by counting the number of white spaces, ws(P ), in each of the strings P ∈ P(C) and subtracting one, unless the string itself turned out to be a beginning or an end of a sentences in the corpus. Thus, ws(P ) takes on values 0, 1, . . . and word(P ) takes on values −1, 0, . . . . The values word(P ) = −1 and word(P ) = 0 mean that the string contains no word. We found out that strings of this form are unavoidable, details are left out.\nFigure 2 shows the distribution of the number of phrases in P(C) w.r.t. their word(P )-lengths for word(P ) ≤ 30. The variety of longer phrases word(P ) = 2, 3, 4 is larger than the variety of single-word phrases. It is worth to note that number of cases word(P ) = −1, 0 is very small. Manual inspection of such cases shows that these strings result from some particular features of the corpus, e.g. special symbols such as “ˆ” in the OCR-ed corpus “Accountant”, special nomenclature and indices of the remarks in the Wittgenstein Corpus, chemical formulas in the Medline Corpus etc.\nIn order to estimate phrases of what length were actually preferred by the algorithm we proceeded in the following way. For w = −1, 0, . . . we computed the total number of multiplicities, M(w), of a string P ∈ P(C) with word(P ) = w, i.e.:\nM(w) = ∑\nP∈C:word(P )=w\nmn(P ).\nWe also computed the total number of occurrences, O(w), in the corpus of the strings P ∈ P with word(P ) = w, i.e.:\nO(w) = ∑\nP∈C:word(P )=w,mn(P )≥1\nocc(P ).\nThus, the ratio M(w)/O(w) provides an average on which percentage of the occurrences of a phrase P ∈ P with word(P ) = w has been used in the final decompositions determined by P . In Figure 3 we depict the relationship between w and M(w)/O(w) for w ≤ 30 for the corpora w.r.t. their size. Results confirm that the algorithm selects longer phrases with higher preference than shorter ones."
    }, {
      "heading" : "3.5 Qualitative results on phrases",
      "text" : "In our approach, the set of phrases represents a subset of the set of general phrase candidates, G(C). The above conditions imposed on general phrase candidates imply that a string with only one occurrence in C that does not represent a full sentence cannot belong to G(C). This means that only those proper phrases can be found that have at least two occurrences (with distinct immediate contexts) in the given corpus C. Since long substrings often have just one occurrence, many long substrings\nthat we would treat - from a linguistic point of view - as phrases are not found in P(C). As a matter of fact, we cannot obtain another result when we demand that phrases must occur in distinct contexts in the given corpus. From a qualitative point of view, the essential characteristics of the set of extracted phrases P(C) can be summarized in the following way. Examples for all cases are presented below.\n1. Ignoring a small percentage of exceptions, elements P ∈ P(C) are sequences of “words” delimited by blanks or special sentence symbols (,.;:). At this point it should be kept in mind that our algorithm does not know the notion of a word, nor the role of distinct letters in the alphabet. Exceptions where elements P ∈ P(C) are not delimited by blanks (or sentence symbols) are caused by strings with only one occurrence in the corpus.\n2. In most cases, elements P ∈ P(C) extracted are meaningful units in the sense that they represent phrases, or phrases extended on the left/right border by characteristic overlapping units, function words. In addition we often find units that would be interesting for language learners, showing how kernel sequences of words are typically connected with neighboured phrases. For the sake of reference, such units will be called phrases extended by typical connectors.\n3. When ignoring delimiting functions words, linguistic phrases in the corpus in most cases are representable as elements in P(C) or sequences of such elements.\nExample 3.1 We present the decomposition of the Europarl-sentence You have requested a debate on this subject in the course of the next few days , during this part-session . (en) and the parallel sentences for the languages French (fr), Spanish (es), Dutch (du), and German (de). In the following sentence decompositions, phrases P ∈ P(C) extracted are the substrings marked by upper brackets ( . . .) or lower brackets ( . . .), and overlaps of decompositions have the form ( . . .) or ( . . .), function “words” in overlaps highlighted in bold.\n(en) (You have( )requested a( )debate( on this )subject( in the )course of the next few days( ), during\nthis( )part-session . )\n(fr) (Vous avez( )souhaité( )un débat à ce sujet( )dans les prochains jours( , )au cours( de cette )période de session( ). )\n(es) (Sus Señorı́as han( )solicitado un( )debate sobre el\n( )tema para( )los próximos dı́as ,( )en el curso( de ) este perı́odo( )de sesiones .)\n(du) (U heeft aangegeven dat u( )deze vergaderperiode( ) een debat( )wilt( )over deze( )rampen .)\n(de) (Im Parlament( )besteht der( )Wunsch nach( einer )\nAussprache ( )im Verlauf( )dieser Sitzungsperiode( ) in den nächsten Tagen( ).)\nSeveral long phrases with not just one occurrences in the corpus are recognized. Examples are\nin the course of the next few days\nun débat à ce sujet\nin den nächsten Tagen\nLooking at the sentence beginnings we find “phrases extended by typical connectors” in the above sense:\nYou have\nVous avez\nU heeft aangegeven dat u\nSus Señorı́as han\nExamples where long phrases can be represented as sequence of units in P(C) (possibly deleting overlaps on the two sides) are\nYou have requested a debate on this subject\nVous avez souhaité un débat à ce sujet\nau cours de cette période de session\nde este perı́odo de sesiones .\nim Verlauf dieser Sitzungsperiode\nExample 3.2 We present a typical decomposition of a sentence of the Medline Corpus.\n(A( viral )etiology,( )eg, the( )congenital rubella syndrome, ( )was considered most likely,(\n)but detailed( ) investigations( proved to be )negative.)\nIn this sentence, ignoring extensions caused by the comma “,”, terminological expressions such as\nviral etiology\ncongenital rubella syndrome\nare found as phrases. In fact, since terminological expressions often appear in distinct contexts, one strength of the method is the ability to find terminological phrases (see Section 5.2). The noun phrase detailed investigations is split, at this point corpus characteristics cause the effect that but detailed and investigations proved to be are prefered. Phrases found also include\nwas considered most likely,\nproved to be negative.\nPhrases that can be obtained by combining recognized units are, e.g.,\nbut detailed investigations proved to be negative.\nAlso note that viral and proved to be occur as an overlap, a characteristic feature of the Medline corpus.\nExample 3.3 We present a typical decomposition of a sentence of the English Wikipedia.\n(She further( )demonstrated that( )much of the other( )\nresearch in the book( )arguing for( )a link( between ) Sir( John )Williams and( )the Ripper crimes( )was flawed.)\nHere between and John occur as an overlap. Phrases found are\nresearch in the book\nthe Ripper crimes\nInteresting units that language learners would consider as important expressions, but not representing linguistic phrases (“phrases extended by typical connectors”\nin the above sense), are\narguing for\na link between\nExample 3.4 We present a typical decomposition of a sentence of the German Wikipedia.\n(vom( )Orchestre de la Suisse Romande) (, den( )Berliner\nPhilharmoniker,( )dem Tonhalle-Orchester Zürich( )mit Werken ( )klass(ischer Komponisten), (wie )auch( )Komponisten (\n) Polens( von )Frédéric Chopin( )bis( )Witold Lutoslawski (\n)über( )Krzysztof Penderecki.)\nDue to particularities of the corpus, ischer Komponisten occurs as an overlap. Phrases directly found are\nOrchestre de la Suisse Romande\nBerliner Philharmoniker,\ndem Tonhalle-Orchester Zürich\nPhrases corresponding to combinations of recognized units are\nmit Werken klassischer Komponisten\nvon Frédéric Chopin bis Witold Lutoslawski\nAlso note that person names are well recognized. This is again due to the effect that these person names appear in distinct contexts in the corpus.\nExample 3.5 We present a typical decomposition of a sentence of the Chinese corpus. The sentence is\n“老者顺手递过来一份文件，黎箫打开一看，眼睛立刻就直 了，没看别的，就看那张不大的照片，美女啊，绝对的美女 啊，我们黎箫口水“飞流直下三千尺””\nthe English translation is\n“The old man handed over a document, LiXiao stared at it with motionless eyes as soon as he opened it, looking at nothing else, just the small picture, beautiful girl, a really beautiful girl, the saliva of our Lixiao has flowed three thousand feet.”\nThe sentence decomposition determined by our algorithm is the following.\n“(老者)(顺手 ( )递过(来 )一份 ( )文件(， )黎箫打开 ( )一看，眼睛立\n刻 ) (就直了 (，)没看 ) (别的 (，)就看 ) (那张 ( )不大(的 )照片(，)美女 ) (啊， 绝对的 ( )美女啊(， )我们黎箫 ( )口水 ) (“飞 ( )流 ) (直下 ( )三千 ) (尺”)”\nIt has the following (partially overlapping) phrases:\n“(老者old man)(顺手smoothly ( )递过来hand (something to somebody) )\n(来一份one piece ( )文件document, or paper， ) (, Lixiao opens黎箫打 开 ( )一看，眼睛立刻looks, eyes immediately ) (就直了motionless/steady，) (, 没看did not look)(别的others， ( ), 就看only look ) (那张that piece of()不大的not big, small ) (的照片picture， ( ),美女beautiful girl ) (啊a， 绝对的absolute()美女啊beautiful girl， ) (,我们黎箫Lixiao or our Lixiao ( )口 水saliva) (“飞fly ( )流flow ) (直下straightly down ( )三千three thousand ) (尺”feet)”"
    }, {
      "heading" : "3.6 Computing extended sets of phrases",
      "text" : "An evident drawback of the approach proposed above is its incapability to detect phrases occurring only once: since these phrases do not appear in distinct contexts they are not contained in the initial set of general phrase candidates G(C). Obviously, many substrings occurring only once represent interesting language units. The problem is challenging because we do not have immediate statistical evidence that helps to detect such substrings. We looked at two ways to address this problem, in both cases using a dynamic approach.\n1. Extended island phrases. We consider the sentences, Sn, in the natural order of their appearance in the corpus C. At each time step, t, we let Ct be the set of sentences observed up to moment t. Using a dynamic interpretation of assumptions (A1)-(A3), the set of phrase candidates G(Ct) w.r.t. the restricted corpus Ct only contains those strings that occur in different left and right contexts within Ct. Now, when processing sentence St+1 we compute “island” substrings V characterized by the following property:\n1. V is a maximal infix of St+1 which does not contain as a substring any phrase candidate in G(Ct) bounded with function words, F(P).\n“Island substrings” V with these properties are extended on both sides until we reach the sentence border or a delimiting function word. Extended island strings\nV̂ are treated as candidates for phrases. For example, if sentence S = St+1 has the form\nThe Commission proposal is a start but it is not enough.\nit might be the case “The ”, “ is a ”, “ but ”, “ it is ”, and “ not ” are already elements of G(C)t, but none of the remaining words in the sentence has this property. The strings “Commission proposal”, “start” and “enough.” are islands not covered by these phrase candidates:\nThe Commission proposal is a start but it is not enough.\nAssume that when decomposing the sentence these strings are decomposed in a brute force way into substrings like “C”, “ommission”, “pro”, “po”, “osal” and so on. None of these smaller strings is bounded by function words on both sides. In this situation we guess that\nCommission proposal , start and enough. are phrases. Applying this simple procedure we extend the set of phrases, P(C) with these extended islands. Results obtained are illustrated in Table 3.\n2. Instances of island schemes. In a second step we extended this idea. For each sentence St in the original corpus a copy Sat was introduced where the longest infixes which cannot be covered by phrases in Pt are substituted by a new character “UNK”. For example, the above sentence would be transformed to\nThe UNK is a UNK but it is not UNK\nIn this way we obtain a new corpus Ca = {San}Nn=1 consisting of abstracted sentence transformations of the original corpus. Abstract phrases detected in this corpus can be pulled back to C, in this way recognising patterns that we were unable to detect originally. For example, if The UNK is a UNK is a phrase in Ca we may conclude that The Commission proposal is a start could be treated also as a phrase in the original corpus, C. For phrase detection in Ca we apply the algorithm described in Section 3.1. Afterwards we can simply take the resulting phrases containing UNK in Pa - called island schemes - and replace the special symbol by the original island. Table 4 represents some island schemes and instances obtained by applying this procedure to the excerpts of English and German EU Parliament Corpus."
    }, {
      "heading" : "4 Computing subphrase structure and content words",
      "text" : "In this section we further analyse the structure of phrases. As a side result we obtain a method for finding content words (non-function words) of the corpus."
    }, {
      "heading" : "4.1 Algorithmic principles",
      "text" : "Now that we have determined a set of function words F(C) and a multiset of phrases P(C) we proceed to structure phrases in a hierarchical way. We address this issue in two steps. In the first step we explain how to generate the constituents of the phrases in P(C), which we call subphrases. As a by-product of this step we will be able to derive a notion corresponding to the common “words”. Recall that our algorithm just analyzes plain sequences of symbols without having any notion of word “built in”. In a second step, we continue to structure the resulting subphrases in a hierarchical way, obtaining a form of decomposition tree.\n1. Splitting phrases and subphrases. Following the discussion in Section 2 and the model that we developed in Section 3, given a phrase P ∈ P(C) we look for a decomposition into substrings of G(C) and determine those that exhibit the most regular function words. Formally, we search for a decomposition of P into a sequence P1, P2, . . . , Pk of general phrase candidates Pi such that the sequence of induced function words F = (F1, F2, . . . , Fk−1) optimises the likelihood:\n`(F |P) = k−1∏ i=1 p̃pref (Fi|P) · p̃suf (Fi|P).\nThe only difference to the situation above is that we now have fixed the set of function words, requiring that Fi ∈ F(C) for all 1 ≤ i < k. Strings Pi only need to be general phrase candidates in G(C) but may fail to be elements of P(C). In this way, a larger spectrum of meaningful units in the corpus is obtained, beyond the multiset of (full) phrases in P(C). We call each Pi a subphrase in C. Using the same principles again, each subphrase P can be further decomposed into finer subphrases until we arrive at atomic subphrases that cannot be further partitioned. By SP(C) we denote the union of P(C) with all subphrases obtained from iterated decomposition of phrases and subphrases. SP(C) is called the set of subphrases of the corpus C. The set of atomic subphrases of C is denoted ASP(C).\nIt is easy to see that each subphrase P that is not a sentence prefix (suffix) has a prefix (suffix) which is a function word in F(C). The kernel of P is obtained from P by deleting the prefix (suffix) of P in F(C) which has the maximal probability pfw(.|P) as a function word. If P does not have such a prefix (suffix), we “delete” from P the empty prefix (suffix). In this way, from the set of phrases P(C) we obtain the set of phrase kernels PK(C), from the set of subphrases we obtain the set of subphrase kernels SPK(C), and from the set of atomic subphrases we obtain the set of atomic subphrase kernelsASPK(C). It turns out that the atomic subphrase kernels of a phrase P typically are the “content words” contained in P , i.e., words that are not in the set of F(C).\n2. Decomposition trees and functional schemes. For a phrase P we may select an optimal decomposition. When we continue to decompose the subphrases obtained, always selecting an optimal decomposition, we obtain a decomposition tree t for P . Each node of t represent a subphrase of P . The children of a node P ′ in t are obtained using the set of function words F(C) and the above optimization principle, selecting one optimal decomposition. A parallel kernel decomposition tree tK is obtained replacing each node in t by its kernel. Using the two trees and looking at the leaves, each phrase P has a representation as a sequence of function words and atomic kernels. For phrases P that do not correspond to a sentence prefix or suffix, P is described as\nP = F0A1F1 . . . Fn−1AnFn\nwhere Fi ∈ F(C) and the strings Ai represent the leaves of tK . In this situation, the sequence F0|F1| . . . |Fn is called a functional scheme for P . For sentence prefixes (suffixes) function word F0 (resp. Fn) has to be omitted. A functional scheme can be considered as a rudimentary form of grammar rule. In general a (sub)phrase may have several optimal decompositions. Hence, for a given phrase P we obtain a set of decomposition trees T (P ), a parallel set of kernel decomposition trees TK(P ), and a set of functional schemesFS(P ). However, we found that the functional scheme for a phrase and the leaf representation P = F0A1F1 . . . Fn−1AnFn is unique in most cases. From the multiset of phrases of the corpus C we obtain the multiset FS(C) of all functional schemes in C, which may be ordered by the number of occurrences."
    }, {
      "heading" : "4.2 Results for distinct languages and corpora",
      "text" : "As an illustration, in Figure 4 we present a decomposition tree and the parallel kernel decomposition tree for the German phrase\nüber den Vorschlag für eine Richtlinie des Europäischen Parlaments und des Rates\nEmpty nodes in the kernel decomposition tree are caused by phrases that become empty when removing the function word borders. The functional scheme for the phrase has the form\n| den | für eine | des || und des |.\nAtomic subphrase kernels obtained are über, Vorschlag, Richtlinie, Europäischen, Parlaments, and Rates.\nThe results for phrase decomposition in European languages are similar. As mentioned above, in almost each case, the functional scheme for a phrase turned out to be unique, even if there are several decomposition trees. This shows that the difference between distinct decompositions is mainly an issue of distinct orders in which to decompose (sub)phrases, not an issue of distinct final decomposition parts.\nFor all European languages and corpora considered, the set of atomic subphrase kernels obtained is essentially a lexicon of the content words of the corpus. Hence, despite of the fact that we do make any distinction between the distinct symbols in the textual alphabet, words are recognized as the “atomic” units from phrase decomposition. For Chinese, iterated subphrase splitting often leads to single symbols. Though multi-symbol words in general occur at one level of the decomposition of phrases, we yet do not have good principles to exactly find this point.\nAn obvious question is if the functional schemes obtained can be considered as a kind of induced grammar. However, there is no direct correspondence between traditional linguistic categories and the spectrum of functional schemes. Many of the functional schemes capture small sequence of words of a similar form with many instances, but the distinct instances do not always have a common linguistic functionality. In Table 5 we show the most frequent functional schemes for the English and German Wikipedia Corpora, respectively.\nOn the other hand we also found a considerable number of interesting patterns that point to a special form of grammatical construction. In Table 6 we add a selection of interesting complex schemes with several instances."
    }, {
      "heading" : "5 Applications",
      "text" : "In this section we consider a selection of possible applications of the above methods. In each case the main point is to show that the automated computation of phrases and subphrases of a corpus and the bidirectional structure of the index structure used offer interesting possibilities to approach known problems on new paths."
    }, {
      "heading" : "5.1 Corpus lexicology",
      "text" : "The predominant perspective in lexicology and corpus linguistics is to use a repository of corpora for collecting and verifying knowledge about a language or language variety. Under this perspective, language is the central object of study, and\ncorpora are just a means to obtain insights about language, e.g., by collecting the vocabulary found in the corpora. There is second perspective, which in general finds less attention: here a given corpus is considered as an independent object of study and the goal is to obtain insights about the “language of the corpus”. In this subsection we follow the second perspective. The methods described below, as those discussed above, can be applied in an unsupervised and fully automated way. They can be considered as a preliminary step to dynamically generate in an “online” manner views that characterize the vocabulary and language of a given corpus in distinct ways. The long-term objective is to enable “travels” in the language of the corpus, to compare the languages of distinct corpora etc. We start with an study where we try to find the most characteristic vocabulary of a given corpus. Afterwards we look at connections between words, semantic fields and phrase nets and at language style.\nCharacteristic words of a corpus. The problem of how to find “important” terms of a corpus or document collection has found considerable attention (see Section 6). Since both very rare and very frequent terms in general are inappropriate, there is no simple solution. Here we suggest a method that takes the composition of sentences and phrases into account. In order to find characteristic words of the corpus C we first try to find the most “characteristic” or “specific” child Pi of a phrase P ∈ P(C) in the decomposition trees t ∈ T (P ). As we see below, at this step “rare” language units are prefered. Afterwards, looking at those kernels in ASPK(C) that appear as the kernels of many “most characteristic subphrases”, we derive a ranked set of “characteristic words” or “key words” of the corpus C. At this step, terms that are too rare obtain a low score. In order to formalise the intuitive notion of “characteristic”, we argue probabilistically. The intuition is that we want to decrease the uncertainty to obtain a longer phrase P starting from a shorter subphrase Pi. Probabilistically, this means that we need to maximise the conditional probability p(P |Pi) where Pi ranges over all subphrases of P . Now, clearly:\np(P |Pi) = p(P, Pi)\np(Pi)\nand if we look at the probability for these events as the event that the particular strings occur, then it is obvious that p(P, Pi) = p(P ) because Pi is a substring of P . Thus, if we naively consider the probability p as empirical probability, then the probability p(P |Pi) is simply the ratio of the occurrences of Pi that imply an occurrence of P , i.e.:\np(P |Pi) = occ(P )\nocc(Pi) .\nPractically, the maximal value is simply obtained by selecting the child Pi of P with the smallest number of occurrences occ(Pi) in the corpus. In our experiments, if two children have the same number of occurrences we simply selected the leftmost child.\nUsing the above measure for specificity we assign to each subphrase P ∈ SP(C) its most specific child P ′. We write P ′ = msc(P ). Since a given subphrase P ′ in general acts as the most specific child of several superphrases P we obtain a forest\nMSSC = (SP(C),msc)\ncalled the forest of most specific subphrases. The roots ofMSSC are the “characteristic” elements in the set of atomic subphrases,ASP(C). The nodes ofMSSC are the elements of SP(C), leaves representing maximal phrases. As before, as an alternative view we may look at the modified forest MSSKC where we replace each subphase by its kernel. The roots of this forest, which represent “characteristic” content words, are called characteristic atomic kernels.\nAs a measure for the importance of an atomic phrase P in ASP(C) we may use the number ρC(P ) of distinct phrases in P(C) that are descendants of P in the forestMSSC . In the experiments described below we found that selecting atomic phrases (or their kernels) with high ρC-score in fact yields a characteristic profile for a given corpus.\nIn our experiments we computed a ranked list of characteristic atomic kernels for our fragment of the Medline corpus, following the above method. The 60 topranked kernels are shown in Table 7. For the sake of comparison, similar ranking list were computed for the English Europarl corpus and for 2% of the English Wikipedia. In Table 7, pairs (n/m) present Europarl ranks (n) and Wikipedia ranks (m). Entries “-” mean that the entry did not occur in the ranked list for the corpus. The ranked lists for the corpora, despite of the large variety of word types in the lists, are radically distinct. When taking the top 60 kernels from Medline, only three (“only”, “one”, “time”) occur among the top 60 words in the ranking list for the Europarl corpus, and only seven (“used”, “only”, “found”, “one”, “developed”, “use”, “time”) occur among the top 60 words in the ranking list for the Wikipedia corpus. This shows that top segments of the ranking lists yield an interesting “profile” for a given corpus, which might also be useful for classification tasks and for stylistic studies (see below).\nConnections between words, semantic fields and phrase nets. Automated approaches for exploring the paradigmatics of lexical units typically look at the cooccurrence of terms in documents, paragraphs, sentences or fixed size neighbourhoods [SP93, Sto10]. Another, more “syntactic” view is obtained when looking\nat the co-occurrence of terms in phrases. To this end, the forest of most specific subphrases MSSC can be used. We compute the network G = (ASP(C), E) where E is the set of all edges of the form e = (A1, P,A2) where P ∈ P(C) is a descendant of the root nodes Ai inMSSC (i = 1, 2). In the graph G, two atomic phrases A1 and A2 are strongly interlinked if they co-occur in many full phrases. Note that phrases P can have variable length and we do not ask if A1 and A2 are direct neighbours in P . Figures 5 and 6 show some of these graphs. Networks of this kind, when visualized in an appropriate way, offer interesting possibilities for mining and exploring corpora and related corpus vocabulary. Figure 5 starts from four characteristic words from Table 7, “blood”, “children”, “risk”, and “tumor”. Presented are atomic phrases that co-occur with these words in phrases, and the phrases where the co-occurrence appeared. Figure 6 is obtained in a similar way from the Wittgenstein corpus, starting from “Erklärung”, “Bedeutung” and “Gebrauch”.\nMining language style. It is a well-known phenomenon that the style even of official language differs when looking at distinct communities, fields or domains. When entering a new community, even native speakers often have difficulties to “adapt” language style and to meet the typical phrases and constructions of the\ndomain. For foreign language learners this task is even more complex. Many phrases automatically extracted from corpora using the above methods reflect the typical style of the corpus domain. For mining “stylistic” phrases we may start from subphrases such as “ we ”, “ my ”, and “it is ” and then look at the full phrases containing these strings. These phrases exhibit another specific “face” for each corpus. Below we present phrases including the strings “ my ”, and “it is ” derived (a) from the Europarl corpus, and (b) from Medline. It is important to note that the phrases extracted may contain the given string at any position, such as in “as a conclusion it is suggested that”, or in “express my regret at the”.\n“my” in Europarl: my wholehearted support - my compliments to - extend my special thanks to - my congratulations to - my colleague , Commissioner Wallström - In my opinion , this is the only way - In my view , this - close to my heart - to thank my colleagues on the committee for - I expressed my views on - It is my belief that - On behalf of my - my honour to present to you - my group welcomes this - express my regret at the ...\n“my” in Medline: to my knowledge this is the first - my own experience - my personal experience - studies in my laboratory - my professional life - of my research career - my assessment - my findings suggest that - answer to my question - my colleagues and i have developed ... .\n“it is” in Europarl: it is extremely important - it is therefore necessary to - it is a question of - it is , however , - it is in this spirit that - it is especially important - it is high time that - it is indeed the case that - it is my belief that - it is worth pointing out - it is unacceptable that - it is not enough to ... “it is” in Medline: from these results it is concluded that - it is suggested that - it is shown that - it is argued that - it is postulated that - however it is not known whether - therefore it is suggested that - it is tempting to speculate that - indicating that it is - it is important to understand - furthermore it is shown that - as a conclusion it is suggested that ..."
    }, {
      "heading" : "5.2 Terminology extraction",
      "text" : "Automated terminology extraction helps to build up terminological dictionaries and thesauri, it has often been studied in the literature (Section 6). Most methods suggested use characteristic sequences of POS, word association measures such as log likelihood, mutual information etc., and corpus comparison to find terminological expressions in a given corpus. Here we suggest a three step approach only based on the techniques described above and using distinct corpora. In Step 1 we compute the characteristic atomic kernels of phrases for the given corpus, only relying on the input corpus. In Step 2, given the set of characteristic atomic kernels we try to single out “terminological” kernels (e.g., “cancer”) using the ranked lists of characteristic atomic kernels obtained from another corpora for comparison. In Step 3, given a terminological kernel (“cancer”) we try to find terminological (multi-word)\nexpressions that contain this kernel as substring (“female lung cancer”, “prostate cancer”).\nStep 1 - characteristic atomic kernels. Our starting point for searching kernels is the decomposition of phrases described in Section 4. We compute the ranked list of characteristic atomic kernels. In general there are much more truly “characteristic” kernels than “terminological” kernels. Even if there is hardly a general agreement of what “terminological” exactly means, typically, mainly noun phrases are considered as terminological expressions. The list in Table 7 show that words of many distinct types can be “characteristic” for a given corpus.\nStep 2 - terminological kernels. When trying to filter out “terminological” kernels from the list of “characteristic” kernels we could of course use POS or other type of linguistic information. However, for the sake of consistency of the approach we do not use external or human linguistic knowledge here. The numbers found in Table 7 suggest that “terminological” kernels typically do not appear in the top segment of the list obtained for the Wikipedia corpus. If we ignore kernels with a rank below 1,000 in one of the two corpora used for comparison, from the Medline top 60 segment we obtain the following list of kernels\ntreatment (2223/1106), obtained (3743/1141), patients (4104/1897), patient (-/3414), cancer (-/2935), expression (1333/1666),\ndemonstrated (1167/1900), disease (3185/1674), cells (-/1123), decreased (5109/3881), factors (1920/1089), tumor (-/10737),\nexamined (4705/5826), normal (3079/1211), blood (-/1376), subjects (1068/2110), decrease (3735/2992), protein (7516/1941),\ncompared (2692/1199), investigated (3658/5008), valuated (8548/7615)\nWe still find entries that are usually not considered as “terminological”, but clearly a much better focus is obtained. With larger bounds, the percentage of non-terminological kernels can be further reduced, a suitable compromise between precision and recall needs to be defined for each application.\nStep 3 - kernel expansion. For each of the remaining top-ranked phrase kernels we consider the list of all phrases that contain this kernel as a substring. In this way, kernels are expanded to the left, to the right or in both directions. Since our phrases typically come with function word delimiters we clean each set by eliminating left and right “borders” that appear in our list of function words (see above). Cleaned substrings are ranked by frequency of occurrence. As a result we obtain a set of multi-word expressions. The characteristic features of the terminological expression found in this way are the following\n1. the number of words in these expressions is not fixed,\n2. the kernels used may appear at any position in such a multi-word expression.\nAs an illustration, in Appendix A we present upper parts of the ranked lists obtained for the kernels “disease”, “syndrome”, and “inflammatory”. In most cases, “disease” occupies the last position of the phrase. Expressions can be long - some examples from the full list are “risk factor for cerebrovascular disease”, or “intestinal failure-associated liver disease”. Examples where “disease” is not at the last position are “disease-related modification of exposure” and “disease is known to be caused”. For “syndrom”, an example is “metabolic syndrome components”. In contrast, “inflammatory” typically is found at the beginning of an expression, as expected. Examples not following this rule are “proinflammatory cytokine” and “nonsteroidal anti-inflammatory drugs”. We did do not use any kind of additional cosmetics (which would easily be possible in practice). From our point, this third step is working extremely well."
    }, {
      "heading" : "5.3 Automated query expansion for search engines and free context size answer browsing",
      "text" : "Querying a large corpus can be a difficult task since it is not simple to estimate in advance the number of answers to a search query. If the query is too specific, the number of answers might be small or empty, in other cases the answer set becomes extremely large and a manual inspection is not possible. In order to support users, search engines in the Internet often present a selection of possible right expansions to a unspecific string. For example, when typing “John” in the search window of the English Wikipedia, some expansions such as “John McCain”, “John Lennon”, and others (s.b.) are suggested.\nThe index-based technology presented above can be used to automatically produce interesting expansions. A given short query is just treated as a kernel in the sense of Section 5.2. Just using Step 3 “kernel expansion” of the above procedure, interesting right and left expansions for a query are found. In order to judge how well this approach works we indexed the complete English Wikipedia7 lists of possible expansions for a number of short queries. Possible expansions were ranked by the number of occurrences. Topmost Expansion suggestions for “John”, “Peter”, “Blue” and “Brown” are presented in Appendix B. We then compared this with Wikipedia’s expansion suggestions. Main insights obtained are:\n1. Wikipedia only suggests a small number of expansions. The list of possible expansions produced by our method is much larger.\n7The corpus, Wikipedia dumps May 2014, used for these experiments was split into sentences by PUNKT, [KS06], and afterwards processed by our approach from Section 3.1.\n2. Wikipedia only produces right expansions for a query. Our lists also include left or two-sided expansion suggestions.\n3. As to recall, most of Wikipedia’s suggestions are also found in our lists. In most exceptional cases, variants of Wikipedia’s suggestions are found.\nThe third point is illustrated in Table 8 where we show the full list of Wikipedia’s expansion suggestions (June 18, 2015) for “John”, “Peter”, “Blue”, “Brown”, “Italian”, and “French”. We show which suggestions are also found using our indexbased “kernel expansion” method, and where we found variants of Wikipedia’s suggestions. The recall is excellent. In many cases, distinct variant phrases of Wikipedia’s suggestions were found in addition. Some of these are mentioned in Table 8.\nFree context size answer browsing. Once the user has selected one of the expansion suggestions (“John McCain”), Wikipedia leads her to the Wikipedia page for the expansion (https://en.wikipedia.org/wiki/John McCain). The symmetric index technology can be used for another type of interaction, which we call “free context size answer browsing”. We present to the user a two-sided list of concordances that includes all occurrences of the expansion in the indexed corpus. Since in the index left and right contexts of arbitrary length are directly accessible, the size of contexts shown can be selected and varied by the user. In Figure 7 we see a user interface with this functionality embedded. In this search interface, after typing any string (here: “agen”), the user may select the number of hits and the size of the concordances, which includes both left and right contexts for a hit. Using the lower button, the size of contexts presented can be changed in real-time. This kind of search interface is used at a project at CIS, LMU, where we collaborate with philosophers to find new and innovative ways of how to access corpora in Digital Humanities. See http://www.cis.uni-muenchen.de/dighum/research-groupco/index.html."
    }, {
      "heading" : "6 Related work",
      "text" : "The discussion of related work is separated into two parts. We first consider contributions that are related when looking at the general characteristics of the approach presented above. Afterwards we take a more application oriented view.\nGeneral characteristics of approach\nFinding phrases. When looking for phrases, we do not use linguistic knowledge and only consider structural properties of the corpus. Our approach is completely language independent. In many application fields, as a common standard approach phrases for simplicity are replaced by n-grams of words, [ON04, Koe10]. Additional grammatical features, e.g. part of speech [Lui07], syntactic [ZLC+13], or semantic markers [EWVN11] may be applied to find n-grams that exhibit phrase characteristics. For larger values of n, statistical uncertainty is growing. Deep learning approaches [CWB+11, LST13] try to resolve this issue by deriving regularities in smaller dimension spaces. However, the embedding of data in a smaller dimension space is achieved by extracting features from a fixed length context or from the entire sentence, [CWB+11], with no attempt to reflect the general structure of the corpus. Recent approaches in this area try to account for this drawback by more advanced network topology, [TSM15]. Due to the fixed size of n-grams considered at one time, and since n-grams start at any point in the sentence, the subphrase structure of sentences is not analyzed.\nGrammar development and induction. When trying to analyze sentence structure, grammar-based parsing algorithms can be used [Tom13]. The sentence and phrase decomposition strategies described in this paper can be considered as a preliminary step towards corpus based, automated, and language independent grammar induction. While our approach leads from corpus structure to grammatical rules, in parsing, fixed grammar rules impose a “predefined” structure on the corpus. For most languages, a full and correct syntactic analysis of arbitrary sentences in unrestricted texts is not possible. For searching phrases of a particular type, partial/local grammars [Gro97] can achieve high precision and recall. Until today, the development of (full, partial, or local) grammmars with high coverage and precision is an ambitious and difficult task that needs a high amount of human work. Furthermore, most methods we are aware of are language dependent. Language independent and automated construction steps are found in probabilistic context-free grammars [Sak10] and learning regular expressions [DLT04]. The latter problem is mainly of theoretical interest.\nSequence analysis without linguistic knowledge. The problem of detecting meaningful units in a text corpus can be considered also as a kind of segmentation problem. In [LK09] and in [GGJ09] the authors consider the problem of segmenting sequences of letters without blanks and punctuation into words. [GGJ09] is focused on the segmentation of children’s utterances transcriptions into words. The authors present a Bayesian approach combined with the Maximum A Posteriori Principle in order to detect the most probable boundaries of words within\na sequence. In particular, they use Dirichlet distribution to model the number of different words contained in the corpus and a probability for an end of a word.\nThe approach presented in [LK09] is a generic one and the authors apply it also to Chinese word segmentation, Machine Translation alignment and others. The model defines only a probability for a sequence of letters to belong together. In this measure a key role is played by a fine-tuned parameter that controls the length of sequences. Other parameters of the model are trained via Expectation Maximisation Principle.\nAn extensive work has been also done on supervised word segmentation in Chinese. The methods range from statistical approaches based on CRFS [PFM04] to Recurrent Neural Networks [CQZH15] and others.\nThere are two main differences between the works cited above and the current paper. Firstly, the problem of segmentation requires that each letter is assigned to exactly one word. In contrast, we allow overlaps between these units which represents the possibility of sharing small pieces of information which however have high frequency and can provide us with reliable statistics. Secondly, it does not seem that previous approaches reflect the structure of the corpus, rather it is the statistics which aggregates the statistics obtained from the different entries of the corpus. In the presented approach we use this structure in a two-fold manner: (i) to constrain the number of available strings and thus define their possible boundaries in terms of other strings; and (ii) to find the best fit of these shorter strings within a particular corpus’ entry. This considerations allow us to avoid the explicit modelling of phrase/word types and phrase/word length.\nIndex-based analysis of corpora. In [Gol10] (see also [Gue05]), suffix arrays are used as a text index structure for mining local grammars. POS-info stored in a synchronized index structure enables a rich set of queries including textual and grammatical conditions. In many fields, suffix trees/arrays are used for various tasks, applications ranging from sequence analysis in bioinformatics [Gus97] to text-reuse in digital humanities. The index structure used for our approach, symmetric directed acyclic word graphs (SDAWGs) [IHS+01], has yet not found much attention. The power of this index relies on the fact that it gives access to left and right contexts of any length. This feature is used in many of the above algorithms.\nApplications\nWe mentioned several possible applications of the methods developed. In all those fields there exists a large literature, only a rudimentary impression can be given. Finding characteristic terms of a document collection is studied, for example, in Information Retrieval. “Global” term weights are, e.g., Poisson overestimation, dis-\ncrimination value, and inverse document frequency [MRS08]. Specialized methods have been developed for the related problem of automated keyword extraction [RECC10]. Terminology extraction is studied, for example, in [PL05]. Methods for automated query expansion are discussed, e.g., in [LSM12]. In corpus linguistics and lexicology, the problem of finding syntagmatic and paradigmatic relations is a central topic. [SP93, Sto10] present contributions to finding lexical-semantic relations, collocation, word maps, and paradigmatics of a lexical unit. In all cases, the methods suggested above are unique by using information on the subphrase structure of sentences inferred from general structural properties of the corpus. For achieving optimal results it would be interesting to combine distinct approaches."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we described results of an ongoing research project where we analyze corpora in a completely language independent and unsupervised way without any prior linguistic knowledge. We introduced an algorithm for detecting function words and phrases. In many cases the phrases (combinations of phrases) obtained are linguistic phrases or extensions with “adjuncts” (words connecting linguistic phrases). We then showed how phrases can be split into subphrases and how to determine content words. For Chinese, detection of phrases yields satisfactory results, as to word segmentation it is difficult to detect the best level of phrase decomposition.\nA weakness of the main algorithm for partitioning sentences into phrases presented in Sections 3.1-3.5 is that only strings can qualify as phrases that occur within distinct contexts in the corpus. All sequences with just one occurrence in the corpus cannot become phrases. To avoid this problem, and to come closer to phrases in a proper linguistic sense we need to abstract from plain symbol sequences in some way. The extensions of the main algorithm presented in Section 3.6 represent one promising path for abstraction that deserves further investigation. The detection of phrase patterns such as those in Section 3.6 and Section 5 combined with the pure character-based structure underlying the approach may be also useful for recognising the morphological structure of the language and its dependence on the local phrase context. Some preliminary experiments in this direction showed that our index-structure provides structural indicators that can be used to extract reliable sets of morphemes and to segment words. We assume that unsupervised statistical methods for morphology induction, e.g. [Gol01, Zem08, CL05], can be combined with methods for mining the phrase structure of the corpus, providing another form of abstraction. This might provide a solid alternative to the deep neural network approach taken by [SO15] where embeddings account for the\nlocal context of the words and their morphological properties. The applications discussed show that the unsupervised analysis of (sub)phrase structure and the two-directional symmetric index used for corpus representation offer many interesting options for finding new solutions to prominent problems in distinct fields.\nIn this paper, when analyzing corpora we were radical in the sense that we do not use any kind of explicit linguistic knowledge/resource and no knowledge on the nature/functionality of a symbol (blank, hyphen,...). As a matter of fact, from a practical perspective it is interesting to give up this rigid principle. We intend to adapt the methods obtained to “non-raw” texts. For example, text annotated with POS info just represents another kind of interesting source data [Gol10] for “language mining”. The active use of prior knowledge is one point of future work.\nAcknowledgements. We express our gratitude to: Max Hadersbeck for the sentence segmentation of the Accountant Corpus; Thomas Müller for providing us with sentence segmentation of the German and English dumps of Wikipedia from May 2014. Wenpeng Yin for the translations from Chinese to English. Stoyan Mihov, Petar Mitankin and Tinko Tinchev for comments and discussions on the ongoing research. Hinrich Schütze for a discussion on function words. The second co-author would like to explicitly acknowledge the technical lead of the first coauthor: in the wake of many joint discussions, it was the first co-author who found and implemented the algorithms suggested in this paper. This research was carried out within the ISALTeC Project, Grant Agreement 625160 of Marie Curie Intra Fellowship within the 7th European Community Framework Programme."
    }, {
      "heading" : "Appendix A",
      "text" : "We present top segments of the ranked lists of terminological expressions obtained for some kernels using the method described in Section 5.2. No filtering or cleansing steps were applied to improve the lists.\nExample 7.1 Terminological expressions obtained for kernel “disease”: disease/8418 disease./832 diseases/526 disease,/390 diseases./348 diseases,/158 this disease./153 coronary artery disease/85 infectious dis-\neases/85 diseased/82 Alzheimer’s disease./82 this disease/77 coronary artery disease./73 liver disease./68 cardiovascular disease./63 liver disease/53\nAlzheimer’s disease (AD)/52 Alzheimer’s disease,/52 heart disease/52 Parkinson’s disease./51 disease-specific/51 heart disease./50 disease)/47 Crohn’s\ndisease/46 disease-free survival/46 coronary heart disease./45 autoimmune diseases./45 metastatic disease./45 cardiovascular disease/43 Alzheimer’s\ndisease/42 diseases such/42 cardiovascular disease,/42 The disease/41 autoimmune diseases/41 vascular disease/40 cardiovascular diseases/40 Alzheimer’s\ndisease (AD)./39 coronary heart disease (CHD/39 coronary artery disease,/38 Parkinson’s disease/37 cardiovascular diseases./37 disease progres-\nsion/36 coronary heart disease/35 renal disease./34 metastatic disease/33 disease-related/33 liver disease,/31 infectious diseases./31 Parkinson’s dis-\nease (PD)/31 Crohn’s disease./30 disease-free/30 Parkinson’s disease,/30 this disease,/29 course of the disease./29 inflammatory bowel disease/29\nautoimmune disease/28 coronary heart disease,/28 vascular disease./28 diseases, including/28 inflammatory bowel disease./28 coronary artery dis-\nease (CAD)/28 Alzheimer’s disease (AD/28 renal disease/27 disease-/27 Hodgkin’s disease/26 cardiovascular disease (CVD/26 neurodegenerative\ndiseases/26 Hodgkin’s disease./25 periodontal disease./25 Crohn’s disease,/25 neurodegenerative diseases./25 kidney disease./25 inflammatory dis-\neases/25 disease)./24 obstructive pulmonary disease (COPD)./24 diseases, such/23 lung disease./23 Parkinson’s disease (PD)./23 and disease./23 dis-\nease severity/23 lung disease/22 periodontal disease/22 liver diseases/22 inflammatory diseases./22 ischemic heart disease/22 disease, including/21\nobstructive pulmonary disease (COPD/21 heart disease,/21 coeliac disease./20 ischaemic heart disease./20 disease;/20 autoimmune disease./20 coro-\nnary artery disease (CAD)./20 Graves’ disease./20 cardiovascular diseases,/20 Parkinson’s disease (PD/20 diseases in/19 congenital heart disease/19\nvarious diseases/19 liver diseases./19 sickle cell disease./19 renal disease,/19 sexually transmitted diseases/19 advanced disease./19 end-stage renal dis-\nease (ESRD/19 respiratory diseases/19 diseases associated/19 and diseased/18 vascular diseases/18 inflammatory bowel disease,/18 graft-versus-host\ndisease (GVHD/18 genetic diseases/18 kidney disease/18 kidney disease (CKD/18 chronic obstructive pulmonary disease (COPD/18 lung diseases/17\nunderlying disease./17 Paget’s disease/17 allergic diseases/17 stable disease./17 cerebrovascular disease./17 infectious disease/17 disease-associated/17\nend-stage renal disease./17 cerebrovascular disease,/17 pulmonary disease./17 infectious disease./17 malignant diseases/16 ischemic heart disease./16\ndisease, especially/16 malignant disease./16 disease, particularly/16 coronary disease./16 progressive disease/16 other diseases/16 kidney disease,/16\ndisease activity/16 course of the disease/15 vascular disease,/15 a disease/15 Hodgkin’s disease,/15 cardiac disease/15 Graves’ disease/15 congenital\nheart disease./15 periodontal diseases/15 chronic diseases/15 infectious diseases,/15 recurrent disease./15 the disease/15 severity of the disease./15 in-\nflammatory bowel disease (IBD)/15 coronary heart disease (CHD)./15 various diseases./15 obstructive pulmonary disease./15 disease, and/15 recurrent\ndisease/15\nExample 7.2 Terminological expressions obtained for kernel “syndrome”: syndrome/549 syndrome./287 syndrome,/173 syndromes/157 metabolic syndrome/100 syndromes./82 this syndrome/62 syndromes,/60 metabolic\nsyndrome./47 nephrotic syndrome/44 this syndrome./33 acquired immunodeficiency syndrome (AIDS/33 metabolic syndrome,/31 Down syndrome/30\nsyndrome)/27 Down’s syndrome/26 Sjögren’s syndrome/26 The syndrome/22 irritable bowel syndrome/20 syndrome)./20 syndrome and/20 carpal\ntunnel syndrome/19 Cushing’s syndrome/19 syndrome characterized/19 respiratory distress syndrome/17 a syndrome/16 acquired immune deficiency\nsyndrome (AIDS)/16 Cushing’s syndrome./16 compartment syndrome/16 This syndrome/15 irritable bowel syndrome (IBS)/14 long QT syndrome/13\nDown syndrome./13 toxic shock syndrome/13 nephrotic syndrome./12 Cushing’s syndrome,/12 Rett syndrome/12 myelodysplastic syndrome (MDS/12\nmeconium aspiration syndrome/11 clinical syndrome/11 syndrome associated/11 -syndrome/11 Budd-Chiari syndrome/11 Turner’s syndrome/11 Down\nsyndrome,/11 coronary syndromes/11 polycystic ovary syndrome (PCOS)/11 syndrome&quot/11 Tourette syndrome/11 this syndrome,/10 Down’s\nsyndrome,/10 syndrome is/10 The metabolic syndrome/10 antiphospholipid syndrome/10 metabolic syndrome (MetS)/10 ’s syndrome/9 syndrome\nis presented./9 Wolff-Parkinson-White syndrome/9 Down syndrome (DS)/9 respiratory distress syndrome,/9 acute coronary syndrome/9 hepatopul-\nmonary syndrome/9 acquired immunodeficiency syndrome/9 pain syndromes/8 Marfan’s syndrome/8 adult respiratory distress syndrome/8 respiratory\ndistress syndrome (RDS/8 nephrotic syndrome,/8 Reiter’s syndrome/8 syndrome in/8 syndrome, including/8 Turner syndrome/8 Tourette’s syndrome,/8\nMarfan syndrome/8 myelodysplastic syndrome/8 coronary syndrome/8 Metabolic syndrome/8 pain syndrome/8 pain syndrome./8 Marfan syndrome./8\nLennox-Gastaut syndrome/8 sicca syndrome/8 Cockayne syndrome/8 Sheehan’s syndrome/7 Goodpasture’s syndrome/7 Sjögren’s syndrome./7 res-\npiratory distress syndrome./7 pain syndromes./7 acquired immunodeficiency syndrome (AIDS)./7 syndrome is characterized/7 Felty’s syndrome/7\npolycystic ovary syndrome/7 withdrawal syndrome/7 sleep apnoea syndrome/7 Terson syndrome/7 coronary syndromes./7 Williams syndrome/7 ac-\nquired immunodeficiency syndrome./7 coronary syndrome./7 fatigue syndrome (CFS/7 Rett syndrome./7 Behçet’s syndrome/7 SVC syndrome/6 Bart-\nter’s syndrome/6 ) syndrome/6 Zollinger-Ellison syndrome/6 Down’s syndrome./6 syndrome occurred/6 adult respiratory distress syndrome (ARDS)/6\nTourette’s syndrome/6 Wiskott-Aldrich syndrome/6 fragile X syndrome,/6 obstructive sleep apnea syndrome/6 sudden infant death syndrome (SIDS)./6\nGuillain-Barré syndrome/6 compartment syndrome,/6 syndrome&quot;./6 obstructive sleep apnea syndrome (OSAS)/6 syndromes such/6 respiratory\ndistress syndrome (ARDS)./6 systemic inflammatory response syndrome/6 wasting syndrome/6 Lowe syndrome/6 metabolic syndrome (MS)/6 Bru-\ngada syndrome/6 Apert syndrome/6 Beckwith-Wiedemann syndrome/6 Ogilvie’s syndrome/6 syndrome patients./6 fragile X-associated tremor/ataxia\nsyndrome/6 Gilbert’s syndrome/5 syndrome due/5 ’s syndrome,/5 deficiency syndrome/5 fat embolism syndrome/5 -like syndrome/5 syndrome (P/5\npain syndromes,/5 Reye’s syndrome/5 adult respiratory distress syndrome./5 syndrome’/5 sick sinus syndrome/5 abstinence syndrome/5 Raynaud’s\nsyndrome/5 acquired immune deficiency syndrome (AIDS)./5 myelodysplastic syndromes/5 heart-asthenia syndrome/5 Sjögren’s syndrome,/5 Mar-\nfan’s syndrome./5 syndrome caused/5 syndromes, including/5 HELLP syndrome/5 The clinical syndrome/5 carpal tunnel syndrome./5 sudden infant\ndeath syndrome (SIDS)/5 short bowel syndrome./5 syndrome (/5 post-polio syndrome/5 Turner syndrome./5 antiphospholipid syndrome./5 Zollinger-\nEllison syndrome (ZES)/5 fragile X syndrome./5 sudden infant death syndrome/5 Guillain-Barré syndrome,/5 syndrome (HUS)/5 syndrome are de-\nscribed./5 Angelman syndrome/5 Kallmann syndrome/5 systemic inflammatory response syndrome (SIRS)/5 acquired immune deficiency syndrome/5\nsyndrome occurs/5 Stevens-Johnson syndrome/5 Klinefelter’s syndrome/5 Bartter syndrome/5 the metabolic syndrome/5 Panayiotopoulos syndrome/5\nNoonan syndrome/5 polycystic ovary syndrome (PCOS)./5 polycystic ovary syndrome./5 Sweet’s syndrome/5 metabolic syndrome and/5 acute coro-\nnary syndromes (ACS/5 syndrome was diagnosed/5 syndrome was diagnosed./5 syndrome-associated/5 WAGR syndrome/5 Churg-Strauss syndrome\n(CSS)/5 Sjogren’s syndrome/5 post-Q-fever fatigue syndrome/5 coronary syndrome,/5 coronary syndromes (ACS/5 Hunter syndrome./5 Eisenmenger\nsyndrome./5 syndrome, especially/5 scimitar syndrome/5 5q- syndrome/5\nExample 7.3 Terminological expressions obtained for kernel “inflammatory”: inflammatory/381 anti-inflammatory/117 inflammatory response/111 proinflammatory/63 proinflammatory cytokines/52 inflammatory response./46\nantiinflammatory/41 pro-inflammatory/40 inflammatory reaction/37 inflammatory process/35 inflammatory cells/34 inflammatory mediators/33 inflam-\nmatory cytokines/33 inflammatory bowel/32 anti-inflammatory effects/31 inflammatory responses/31 pro-inflammatory cytokines/30 inflammatory\nbowel disease/29 inflammatory bowel disease./28 inflammatory cells./28 inflammatory markers/27 an inflammatory/26 inflammatory diseases/25 in-\nflammatory lesions/24 proinflammatory cytokine/24 pro-inflammatory cytokine/23 inflammatory processes/23 chronic inflammatory/23 anti-inflammatory\neffect/22 inflammatory diseases./22 inflammatory processes./21 inflammatory responses./21 inflammatory changes/21 inflammatory response,/19 in-\nflammatory process./19 inflammatory bowel disease,/18 inflammatory,/17 inflammatory reaction./17 inflammatory reactions/17 anti-inflammatory\ndrugs/16 inflammatory disorders/15 inflammatory conditions./15 inflammatory bowel disease (IBD)/15 the inflammatory/15 inflammatory infiltrate/14\ninflammatory cell infiltration/14 non-steroidal anti-inflammatory drugs (NSAIDs)/14 systemic inflammatory response/14 nonsteroidal anti-inflammatory\ndrugs/13 inflammatory disease./13 nonsteroidal anti-inflammatory drugs (NSAIDs/13 inflammatory conditions/13 anti-inflammatory activity/12 in-\nflammatory cytokine/12 anti-inflammatory properties/12 proinflammatory mediators/12 anti-inflammatory activity./12 inflammatory markers,/12 The\ninflammatory response/11 inflammatory mediators,/11 systemic inflammatory/11 inflammatory bowel disease (IBD)./11 inflammatory markers./11\ninflammatory bowel disease (IBD/11 The inflammatory/10 inflammatory disease/10 proinflammatory cytokines./10 inflammatory process,/10 anti-\ninflammatory agents/9 inflammatory diseases,/9 inflammatory disorders./9 inflammatory bowel diseases/9 The anti-inflammatory activity/9 inflamma-\ntory cytokines./9 inflammatory cytokines,/9 pro- and anti-inflammatory/9 pelvic inflammatory/8 anti-inflammatory drugs./8 inflammatory demyeli-\nnating/8 noninflammatory/8 anti-inflammatory properties./8 anti-inflammatory effects./8 proinflammatory cytokines,/8 neuroinflammatory/8 inflam-\nmatory cell/7 inflammatory changes./7 inflammatory infiltrate./7 inflammatory mediator/7 The anti-inflammatory/7 inflammatory parameters/7 in-\nflammatory diseases such/7 infiltration of inflammatory/7 nonsteroidal anti-inflammatory drug/7 inflammatory reactions./7 inflammatory cascade/7\nanti-inflammatory cytokine IL-10/7 inflammatory disease,/7 inflammatory state/7 inflammatory activity/6 nonsteroidal anti-inflammatory drugs./6 in-\nflammatory infiltrates/6 non-inflammatory/6 Anti-inflammatory/6 various inflammatory/6 anti-inflammatory cytokine/6 inflammatory pseudotumor/6\ninflammatory pain/6 inflammatory events/6 inflammatory conditions,/6 inflammatory infiltration/6 inflammatory mediators such/6 systemic inflamma-\ntory response syndrome/6 inflammatory mediators./6 pelvic inflammatory disease,/6 non-steroidal anti-inflammatory drug (NSAID)/6 macrophage in-\nflammatory protein-1alpha/6 of inflammatory/6 inflammatory states./6 pro-inflammatory cytokines,/6 pro-inflammatory cytokines./6 Pro-inflammatory\ncytokines/6 inflammatory responses,/6 non-steroidal anti-inflammatory drugs/6 macrophage inflammatory/6 inflammatory changes,/6 Proinflamma-\ntory cytokines/6 anti-inflammatory agents./5 fibro-inflammatory/5 pelvic inflammatory disease (PID)/5 inflammatory pain./5 nonsteroidal antiinflam-\nmatory/5 inflammatory reactions,/5 necroinflammatory/5 inflammatory and neoplastic/5 inflammatory skin/5 inflammatory lung/5 An inflammatory/5\nproinflammatory mediators,/5 non-specific inflammatory/5 systemic inflammatory response syndrome (SIRS)/5 inflammatory reaction,/5 proinflam-\nmatory cytokine,/5 active inflammatory bowel/5 nonsteroidal anti-inflammatory drugs,/5 inflammatory disorder/5 inflammatory stimuli/5 postinflam-\nmatory/5 non-steroidal anti-inflammatory drugs,/5 production of proinflammatory/5 inflammatory component./5 systemic inflammatory response./5\nseveral inflammatory/5 inflammatory status/5 inflammatory cell infiltration./5 inflammatory signaling/5 anti-inflammatory activities./5 inflammatory\nstimuli./5 non-steroidal anti-inflammatory drug/5 nonsteroidal antiinflammatory drugs (NSAIDs)/5 release of inflammatory mediators/5"
    } ],
    "references" : [ {
      "title" : "Efficient string mathing: An aid to bibliographic search",
      "author" : [ "Alfred Aho", "Margaret Corasick" ],
      "venue" : "Communication of the Association for Computing Machinery,",
      "citeRegEx" : "Aho and Corasick.,? \\Q1975\\E",
      "shortCiteRegEx" : "Aho and Corasick.",
      "year" : 1975
    }, {
      "title" : "An optimal algorithm for shortest paths on weighted interval and circular-arc graphs with applications",
      "author" : [ "Mikhail Atallah", "Danny Chen", "D. Lee" ],
      "venue" : "Technical report, Purdue University,",
      "citeRegEx" : "Atallah et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Atallah et al\\.",
      "year" : 1993
    }, {
      "title" : "Compete inverted files for effixient text retrieval",
      "author" : [ "A. Blumer", "J. Blumer", "D. Haussler", "R. McConnell", "A. Ehrenfeucht" ],
      "venue" : "Journal of the Association for Computing Machinery,",
      "citeRegEx" : "Blumer et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Blumer et al\\.",
      "year" : 1987
    }, {
      "title" : "Posterior Convergence under Incomplete Information",
      "author" : [ "D.E. Ben-Tal", "A. abd Brown", "R.L. Smith" ],
      "venue" : "Technical report, Department of Industrial and Operations Engineering,",
      "citeRegEx" : "Ben.Tal et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Ben.Tal et al\\.",
      "year" : 1987
    }, {
      "title" : "Gated recursive neural network for chinese word segmentation",
      "author" : [ "Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Micahel Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Empirical studies on machine learning based text classification algorithms",
      "author" : [ "Shweta C. Dharmadhikari", "Maya Ingle", "Parag Kulakrni" ],
      "venue" : "Advanced Computing: An International Journal,",
      "citeRegEx" : "Dharmadhikari et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dharmadhikari et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning regular languages using rfsas",
      "author" : [ "François Denis", "Aurélien Lemay", "Alain Terlutte" ],
      "venue" : "Theoretical computer science,",
      "citeRegEx" : "Denis et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Denis et al\\.",
      "year" : 2004
    }, {
      "title" : "Combining semantic and syntactic generalization in example-based machine translation",
      "author" : [ "Sarah Ebling", "Andy Way", "Martin Volk", "Sudip Kumar Naskar" ],
      "venue" : "In Proceedings of the 15th Conference of the European Association for Machine Translation,",
      "citeRegEx" : "Ebling et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ebling et al\\.",
      "year" : 2011
    }, {
      "title" : "The Structure of English. An Introduction to the Construction of English Sentences",
      "author" : [ "Charles Fries" ],
      "venue" : "New York: Harcourt, Brace and Company,",
      "citeRegEx" : "Fries.,? \\Q1952\\E",
      "shortCiteRegEx" : "Fries.",
      "year" : 1952
    }, {
      "title" : "A bayesian framework for word segmentation: Exploring effects of context",
      "author" : [ "Sharon Goldwater", "Thomas L. Griffiths", "Mark Johnson" ],
      "venue" : null,
      "citeRegEx" : "Goldwater et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Goldwater et al\\.",
      "year" : 2009
    }, {
      "title" : "Consistency of Empirical Likelihood and Maximum A-Posteriori Probability under Misspecification",
      "author" : [ "Marian Grendar", "George G. Judge" ],
      "venue" : "Technical report, Department of Agricultural & Resource Economics,",
      "citeRegEx" : "Grendar and Judge.,? \\Q2008\\E",
      "shortCiteRegEx" : "Grendar and Judge.",
      "year" : 2008
    }, {
      "title" : "Unsupervised learning of the morphology of a natural language",
      "author" : [ "John A. Goldsmith" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Goldsmith.,? \\Q2001\\E",
      "shortCiteRegEx" : "Goldsmith.",
      "year" : 2001
    }, {
      "title" : "Exploring the Grammar of Natural Language Using Index Structures. PhD thesis, Center for language and information processing (CIS)",
      "author" : [ "Johannes Goller" ],
      "venue" : "University of Munich,",
      "citeRegEx" : "Goller.,? \\Q2010\\E",
      "shortCiteRegEx" : "Goller.",
      "year" : 2010
    }, {
      "title" : "The construction of local grammars",
      "author" : [ "Maurice Gross" ],
      "venue" : "Finite-State Language Processing,",
      "citeRegEx" : "Gross.,? \\Q1997\\E",
      "shortCiteRegEx" : "Gross.",
      "year" : 1997
    }, {
      "title" : "A bootstrap method for constructing local grammars",
      "author" : [ "Maurice Gross" ],
      "venue" : "In Contemporary Mathematics: Proceedings of the Symposium, University of Belgrad,",
      "citeRegEx" : "Gross.,? \\Q1999\\E",
      "shortCiteRegEx" : "Gross.",
      "year" : 1999
    }, {
      "title" : "Algorithms on strings, trees and sequences: computer science and computational biology",
      "author" : [ "Dan Gusfield" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Gusfield.,? \\Q1997\\E",
      "shortCiteRegEx" : "Gusfield.",
      "year" : 1997
    }, {
      "title" : "A theory of language and information. A mathematical approach",
      "author" : [ "S. Harris" ],
      "venue" : null,
      "citeRegEx" : "Harris.,? \\Q1991\\E",
      "shortCiteRegEx" : "Harris.",
      "year" : 1991
    }, {
      "title" : "A formal basis for the heuristic determination of minimal cost paths. Systems Science and Cybernetics",
      "author" : [ "Peter Hart", "Nils Nillson", "Bertram Raphael" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "Hart et al\\.,? \\Q1968\\E",
      "shortCiteRegEx" : "Hart et al\\.",
      "year" : 1968
    }, {
      "title" : "German-learning infants‘ ability to detect unstressed closed-class elements in continuous speech",
      "author" : [ "Barbara Höhle", "Jürgen Weissenborn" ],
      "venue" : "Developmental Science,",
      "citeRegEx" : "Höhle and Weissenborn.,? \\Q2003\\E",
      "shortCiteRegEx" : "Höhle and Weissenborn.",
      "year" : 2003
    }, {
      "title" : "On-line construction of symmetric compact directed acyclic word graphs",
      "author" : [ "Shunsuke Inenaga", "Hiromasa Hoshino", "Ayumi Shinohara", "Masayuki Takeda", "Setsuo Arikawa" ],
      "venue" : "In Proc. of 8th International Symposium on String Processing and Information Retrieval",
      "citeRegEx" : "Inenaga et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Inenaga et al\\.",
      "year" : 2001
    }, {
      "title" : "Statistical Machine Translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : null,
      "citeRegEx" : "Koehn.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2010
    }, {
      "title" : "Unsupervised multilingual sentence boundary detection",
      "author" : [ "Tibor Kiss", "Jan Strunk" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Kiss and Strunk.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kiss and Strunk.",
      "year" : 2006
    }, {
      "title" : "Online em for unsupervised models",
      "author" : [ "Percy Liang", "Dan Klein" ],
      "venue" : "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Liang and Klein.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liang and Klein.",
      "year" : 2009
    }, {
      "title" : "Improving retrieval results with discipline-specific query expansion",
      "author" : [ "Thomas Lüke", "Philipp Schaer", "Philipp Mayr" ],
      "venue" : "In Theory and Practice of Digital Libraries,",
      "citeRegEx" : "Lüke et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lüke et al\\.",
      "year" : 2012
    }, {
      "title" : "Extraction of significant phrases from text",
      "author" : [ "Yuan Lui" ],
      "venue" : "International Journal of Computer, Control, Quantum and Information Engineering,",
      "citeRegEx" : "Lui.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lui.",
      "year" : 2007
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Manning et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2008
    }, {
      "title" : "The alignment template approach to statistical machine translation",
      "author" : [ "Franz Och", "Hermann Ney" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Och and Ney.,? \\Q2004\\E",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2004
    }, {
      "title" : "Chinese segmentation and new word detection using conditional random fields",
      "author" : [ "Fuchun Peng", "Fangfang Feng", "Andre McCallum" ],
      "venue" : "Technical report, University of Massachusetts - Amherst,",
      "citeRegEx" : "Peng et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2004
    }, {
      "title" : "Corpus-based terminology extraction",
      "author" : [ "Alexandre Patry", "Philippe Langlais" ],
      "venue" : "In Proceedings of the 7th International Conference on Terminology and Knowledge Engineering,",
      "citeRegEx" : "Patry and Langlais.,? \\Q2005\\E",
      "shortCiteRegEx" : "Patry and Langlais.",
      "year" : 2005
    }, {
      "title" : "Automatic keyword extraction from individual documents",
      "author" : [ "Stuart Rose", "Dave Engel", "Nick Cramer", "Wendy Cowley" ],
      "venue" : "Text Mining,",
      "citeRegEx" : "Rose et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rose et al\\.",
      "year" : 2010
    }, {
      "title" : "Probabilistic context-free grammars",
      "author" : [ "Yasubumi Sakakibara" ],
      "venue" : "Encyclopedia of Machine Learning,",
      "citeRegEx" : "Sakakibara.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sakakibara.",
      "year" : 2010
    }, {
      "title" : "Machine learning in automated text categorization",
      "author" : [ "Fabrizio Sebastiani" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Sebastiani.,? \\Q2001\\E",
      "shortCiteRegEx" : "Sebastiani.",
      "year" : 2001
    }, {
      "title" : "Unsupervised morphology induction using word embeddings",
      "author" : [ "Radu Soricut", "Franz Och" ],
      "venue" : "In Proc. NAACL,",
      "citeRegEx" : "Soricut and Och.,? \\Q2015\\E",
      "shortCiteRegEx" : "Soricut and Och.",
      "year" : 2015
    }, {
      "title" : "A vector model for syntagmatic and paradigmatic relatedness",
      "author" : [ "Hinrich Schütze", "Jan Pedersen" ],
      "venue" : "In Proc. of Ninth Annual Conference ofthe UW Centre for the New OED and Text Research,",
      "citeRegEx" : "Schütze and Pedersen.,? \\Q1993\\E",
      "shortCiteRegEx" : "Schütze and Pedersen.",
      "year" : 1993
    }, {
      "title" : "Lexical-semantic Relations: Theoretical and Practical Perspectives. Linguisticae investigationes: Supplementa : LIS : studies in French and general linguistics",
      "author" : [ "Petra Storjohann" ],
      "venue" : null,
      "citeRegEx" : "Storjohann.,? \\Q2010\\E",
      "shortCiteRegEx" : "Storjohann.",
      "year" : 2010
    }, {
      "title" : "Recognition and representation of function words in english learning infants",
      "author" : [ "Rushen Shi", "Janet Werker", "Anne Cutler" ],
      "venue" : "Infancy, 10(2):187–198,",
      "citeRegEx" : "Shi et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2006
    }, {
      "title" : "Efficient parsing for natural language: A fast algorithm for practical systems, volume 8",
      "author" : [ "Masaru Tomita" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Tomita.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tomita.",
      "year" : 2013
    }, {
      "title" : "Wie Kinder Sprachen Lernen: Und wie wir sie dabei unterstützen können",
      "author" : [ "Rosemarie Tracy" ],
      "venue" : "Nar Francke Attempto Verlag,",
      "citeRegEx" : "Tracy.,? \\Q2007\\E",
      "shortCiteRegEx" : "Tracy.",
      "year" : 2007
    }, {
      "title" : "Improved semantic representations form tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Tai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "On-line construction of suffix-trees",
      "author" : [ "Esko Ukkonen" ],
      "venue" : null,
      "citeRegEx" : "Ukkonen.,? \\Q1995\\E",
      "shortCiteRegEx" : "Ukkonen.",
      "year" : 1995
    }, {
      "title" : "Unsupervised acquiring of morphological paradigms from tokenized text. In Advances in Multilingual and Multimodal Information, pages 892–899",
      "author" : [ "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Zeman.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zeman.",
      "year" : 2008
    }, {
      "title" : "Combination of unsupervised keyphrase extraction algorithms",
      "author" : [ "Zede Zhu", "Miao LI", "Lei Chen", "Zhenxin Yang", "Sheng Chen" ],
      "venue" : "In 2013 International Conference on Asian Language Processing,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "When looking at the structure of natural language, “phrases” and ”words” are central notions. We consider the problem of identifying such “meaningful subparts” of language of any length and underlying composition principles in a completely corpus-based and language-independent way without using any kind of prior linguistic knowledge. Unsupervised methods for identifying “phrases”, mining subphrase structure and finding words in a fully automated way are described. This can be considered as a step towards automatically computing a “general dictionary and grammar of the corpus”. We hope that in the long run variants of our approach turn out to be useful for other kind of sequence data as well, such as, e.g., speech, genom sequences, or music annotation. Even if we are not primarily interested in immediate applications, results obtained for a variety of languages show that our methods are interesting for many practical tasks in text mining, terminology extraction and lexicography, search engine technology, and related fields.",
    "creator" : "LaTeX with hyperref package"
  }
}