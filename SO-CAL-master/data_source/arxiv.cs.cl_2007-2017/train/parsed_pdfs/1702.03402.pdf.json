{
  "name" : "1702.03402.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Mohamed Bouaziz", "Mohamed Morchid", "Richard Dufour", "Georges Linarès", "Renato De Mori" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n03 40\n2v 1\n[ cs\n.L G\n] 1\n1 Fe\nb 20\nRecently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-theart architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach. Index Terms: long short-term memory, sequence classification, stream structuring"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Recently, automatic sequence classification became an ubiquitous problem, having then encountered a high research interest [1, 2, 3]. This is due to the need to structure knowledge as a set of dependent localized information alongside with the new computer capabilities to efficiently process large amount of data. Among the recent methods employed to structure these sequences, the machine learning domain provides a set of high-level representations well adapted to automatic sequence classification based on Deep Neural Networks (DNN) such as Convolutional Neural Networks (CNN) [4] or Recurrent Neural Networks (RNN) [5].\nRNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have\ngained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing. In speech recognition [10, 11, 12], these LSTM models exploit the contextual information whenever speech production or perception is influenced by emotion, strong accents, or background noise. The most effective use of RNNs for sequence classification is to combine the RNNs with Hidden Markov Models (HMMs) in a hybrid approach [13, 14]. Nonetheless, RNNs or RNN-HMM could not be directly employed for sequence classification using multiple inputs from synchronous streams such as TV shows coming from different channels. Indeed, RNNs can only be trained to make a set of elements labeled in a single stream of input information.\nIn this paper, we introduce an original multistream neural network architecture, called Parallel LSTM (PLSTM), that simultaneously takes into account different synchronous streams in order to automatically classify this multistream sequence. To evaluate the effectiveness of the proposed PLSTM multistream neural network architecture, experiments are carried out on the LIA’s Electronic ProgramGuide (EPG) dataset containing 3 years of TV programs from 4 different channels. The PLSTM performance is compared with the LSTM stateof-the-art approach as well as a classic n-gram approach considered as the baseline. Our PLSTM approach is an important step for sequence classification since it can be applied to any set of synchronous sequences.\nSection 2 proposes an overview of a couple of RNN architectures. Section 3 presents the proposed PLSTM. The experimental protocol and the discussion on the results are presented in Section 4 and 5 respectively. Finally, Section 6 concludes this work and gives some interesting perspectives."
    }, {
      "heading" : "2. RECURRENT NEURAL NETWORKS",
      "text" : "This section introduces the state-of-the art concepts of two recurrent neural networks: LSTM and BLSTM."
    }, {
      "heading" : "2.1. Long Short-Term Memory (LSTM)",
      "text" : "Long Short-Term Memory (LSTM) [6] networks are a special case of Recurrent Neural Networks (RNNs) [5]. The goal of this architecture is to create an internal cell state of the\nnetwork which allows it to exhibit dynamic temporal behavior. This internal state allows the RNN to process arbitrary sequences of inputs such as sequences of words [8] for language modeling, time series [1]. . . The RNN takes as input a sequence x = (x1, x2, . . . , xT ) and computes the hidden sequence h = (h1, h2, . . . , hT ) as well as the output vector y = (y1, y2, . . . , yT ) by iterating from t = 1 to T :\nht = H(Wxhxt + Whhht−1 + bh) (1)\nyt = Whyht + by (2)\nwhere T is the total number of sequences; Wxh are the weight matrices between the input layers x and h and so on; b is a bias vector, and H is the composite function. [6] shows that LSTM networks outperformRNNs for finding long range context and dependencies. The LSTM composite functionH forming the LSTM cell with peephole connections [15] is presented in Figure 1 and defined as:\nit = σ(Wxixt + Whiht−1 + Wcict−1 + bi) (3)\nft = σ(Wxfxt + Whfht−1 + Wcfct−1 + bf ) (4)\nct = ftct−1 + it tanh(Wxcxt + Whcht−1 + bc) (5)\not = σ(Wxoxt + Whoht−1 + Wcoct + bo) (6)\nht = ot tanh(ct) (7)\nwhere i, f and o, are respectively the input, forget and output gates, and c the cell activation vector with the same size than the hidden vector h. The weight matrices W from cell c to gates i, f and o, are diagonal, and thus, an element e in each gate vector receives only the element e from the cell vector. Finally, σ is the logistic sigmoid function."
    }, {
      "heading" : "2.2. Bidirectional Long Short-Term Memory (BLSTM)",
      "text" : "LSTM networks use only the previous context to predict the next segment for a given sequence. Bidirectional RNN (BRNN) [16], presented in Figure 2, can process both directions with two separate hidden layers (one for each direction). This type of RNN feeds to a same output layer fed forwarded\ninputs through the two hidden layers. Therefore, the BRNN computes both forward hidden sequence −→ h and backward sequence ←− h as well as the output vector y, by iterating −→ h from t = 1 to T , and ←− h from t = T to 1:\n−→ h t = H(Wx−→h xt + W−→h−→h −→ h t−1 + b−→h ) (8) ←− h t = H(Wx←−h xt + W←−h←−h ←− h t+1 + b←−h ) (9)\nyt = W−→h y −→ h t + W←−h y ←− h t + by (10)\nBy replacing the BRNN cells with LSTM cells, the Bidirectionnal LSTM (BLSTM) [7] is obtained. The BLSTM allows to exhibit long range context dependencies and takes advantage from the two directions structure. The output vector y is processed by evaluating simultaneously the two directions hidden sequences by computing the composite functionH in the forward ( −→ h ) and backward ( ←− h ) directions."
    }, {
      "heading" : "3. PARALLEL LONG SHORT-TERM MEMORY (PLSTM)",
      "text" : "The BRNN neural architecture uses the same sequence x as an input for both forward and backward directions, which is useful for information from a single stream. The paper proposes an original neural network, called Parallel RNN (PRNN) and presented in Figure 3, that takes advantage from the BRNN structure in a multistream context. By replacing the PRNN cells with LSTM cells, the proposed Parallel LSTM (PLSTM) is obtained.\nThe original PLSTM architecture corresponds to the PRNN description by replacing the H function with the LSTM composite function. PLSTM differs from the classical BLSTM by feeding forward, not a shared sequence, but different input vectors through a dedicated hidden layer hn for each input vector xn. Moreover, BLSTM employs only 2 hidden layers due to its bidirectional concept while PLSTM can use multiple ones. The input sequences are considered independent and require to be mapped in homogeneous separate subspaces (W matrix from input x to hidden h spaces). Therefore, a single LSTM containing concatenated inputs from different independent sequences is not theoretically suitable for finding out a common homogeneous subspace to map heterogeneous input representation of parallel sequences.\nThus, for each nth stream (1 ≤ n ≤ N ), the PLSTM takes the input sequence xn = (xn1 , x n 2 , . . . , x N T ) and computes the hidden sequence hn = (hn1 , h n 2 , . . . , h N T ) and the output vector y by iterating from t = 1 to T .\nhNt = H(WxNhNx N t + WhNhNh N t−1 + b N h ) (11)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (12)\nh2t = H(Wx2h2x 2 t + Wh2h2h 2 t−1 + b 2 h) (13) h1t = H(Wx1h1x 1 t + Wh1h1h 1 t−1 + b 1 h) (14) yt = N∑\nn=1\nWhnyh n t + by (15)\nwhere N is the number of streams. In our experiments, the output vector y takes advantage of the N channels to predict the telecast’s genre for one given channeln (1 ≤ n ≤ N ). Therefore, PLSTM feeds forward separate sequences in order to predict a label and codes internal hidden structures between the parallel hidden sequences. [7] introduces the BLSTM with Back Propagation Trough Time (BPTT) algorithm [17] for training. For our proposed PLSTM architecture, the training takes place overN input sequences: Forward Pass: feeds all input data for the sequences into the PLSTM and determines the predicted outputs.\n• Do forward pass for the forward states of each of theN\nlayers.\n• Do forward pass for output layer.\nBackward Pass: processes the error function derivative for the sequences used in the forward pass.\n• Do backward pass for output neurons.\n• Do backward pass for forward states.\nUpdating Weights"
    }, {
      "heading" : "4. EXPERIMENTAL PROTOCOL",
      "text" : "Multistream sequence classification is evaluated with the proposed PLSTM architecture (2 and 4 parallel sequences) as well as the classic LSTM network on an automatic TV show genre labeling task. Two n-gram based models (baseline) are also considered for fair comparison. Next sections describe the dataset, the genre sequence classification as well as the neural networks settings."
    }, {
      "heading" : "4.1. Multichannel EPG dataset",
      "text" : "The Electronic Program Guide (EPG) dataset is extracted from 4 French TV channels (M6, TF1, France 5 and TV5 Monde) for 3 years, from January 2013 to December 2015. M6 channel is used in our experiments as the output stream. Data from 2013 and 2014 are merged and split into the training (70%) and validation (30%) datasets using a stratified shuffle split [18] in order to preserve the same percentage of samples of each class in the output of both folds, while the 2015 dataset is kept for testing. In order to guarantee a clean experimental environment, labels (i.e. genres) that are absent at least in one of the three folds were removed. Doing so allows us to have equivalent datasets in terms of labels vocabulary. Table 1 shows the genres distribution for M6, the chosen output channel."
    }, {
      "heading" : "4.2. Genre Prediction Experiments",
      "text" : "For a given input history sequence (composed of the n previous telecast genres), a genre label representing the next M6’s\ntelecast is output. The size of the genre sequences (n) varies from 1 to 4. Then, three input configurations are employed. Mono-channel input: only M6 history sequences for a baseline n-gram experiment (with a statistical language model from the SRILM toolkit [19]) and a straightforward LSTM model. Bi-channel input: both M6 and TF1 channel histories are employed as input for P2LSTM (PLSTM with two parallel streams as a BLSTMwith forward-forward directions and separate inputs). The aim of this experiment is to move up the context’s information using a similar and rival channel, the two being generalist channels. Multichannel input: History of each of the 4 streams (i.e. channels) is used as input for 4n-gram and P4LSTM experiments (PLSTM with 4 parallel streams)."
    }, {
      "heading" : "4.3. Neural Networks Setup",
      "text" : "The classical LSTM, and the proposed P2LSTM and P4LSTM, are composed with 3 layers: input layer x of size varying from 1 to 4, a hidden layer h of size 80 for all LSTM-based models and an output layer y with a size equals to the number of different possible TV genres (11). The Keras library [20], based on Theano [21] for fast tensor manipulation and CUDAbased GPU acceleration, has been employed to train neural networks on an Nvidia GeForce GTX TITAN X GPU card. The training times, detailed in Table 2 for all models, match with the sequence size of all models. Indeed, even with the most time-consuming configuration, namely P4LSTM with 4 elements history, the training does not last more than 25 minutes."
    }, {
      "heading" : "5. RESULTS AND DISCUSSION",
      "text" : "Table 3 shows the overall results, in terms of the standard F1 metric related to the genre prediction task outputs, using each method and for different stream sequence sizes from 1 to 4."
    }, {
      "heading" : "5.1. N-gram based models",
      "text" : "The multi-channel 4n-gram model outperforms the simple ngram one for each of the different 4 genre sequence configurations except for 3 sized history. 4n-gram method reaches around 60% of F-score using 1 sized sequences against near 57% for mono-channel n-gram using its best history configuration. The observed results confirm the interest of using multiple streams to predict the next telecast’s genre for a specified channel."
    }, {
      "heading" : "5.2. LSTM and PLSTM",
      "text" : "One can figure out from Table 3 that mono-channel LSTM performances gradually become closer and closer to the multichannel n-gram model ones when the size of sequences moves up and overtakes it with an F1 score of 58% using 4 sized sequences. Therefore, LSTM-based models require longer sequences to learn long term dependencies than the ngram based methods. P4LSTM obtains the best result with an F1 score close of 66% using a sequence of size 4. In order to analyze these results, the Error Rates (ER) are also presented in Table 4. The overall F1 scores are different from those related to the ER. For example, at its best configuration of a 4 sized sequence, P4LSTM error rate reaches about 21.5%, which corresponds to a correct rate of 78.5% against an F1measure of only 66%. The reason of this is that the F1-metric may be not suitable for the task due to the labels imbalance with different numbers of genre occurrences varying from 14 to 1,683 in the test set."
    }, {
      "heading" : "5.3. Discussion",
      "text" : "Confusion matrices of 4n-gram and P4LSTM experiments using sequences of 4 telecasts are shown in Tables 5 and 6 to point out benefits of the proposed PLSTM model.\nIt is worth emphasizing that most of the missed instances in all systems are wrongly labeled as one of the two most frequent classes, Weather and Fiction, as well as the Other Magazine genre (some examples are in green cells). False positives are more recurrent in Other Magazine than in News, the relatively more frequent class. The reason is that News is a well defined genre occurring mostly at the same time each day unlike Other Magazine genre that encompasses various telecasts that are broadcast at several and irregular daytime. Teleshopping shows are often broadcast at nearly the same\ntime of morning than Cartoons and affects dramatically the performance of the 4n-gram model in this context (cf. underlined italic cell in Table 5). Finally, the confusion matrix of P4LSTM experiment shows that this system fails more dramatically to predict the least frequent genres Others, Reality TV, andDocumentary. For example, for the two least frequent genres, Reality TV and Documentary, respectively no more than one of the 76 and the 14 instances was correctly found. This leads to a precision and a recall of 0 which penalizes their averages respectively and then the overall F-score.\nIn order to evaluate the impact of the least frequent genres on the F1 measure, especially on the PLSTM systems, we also reported on Table 7 the F1 results on the same outputs of the experiments of Table 3 by excluding the two least frequent genres from the averages of precision and recall (Reality TV and Documentary).\nOverall, the results of the PLSTM detailed in Table 7 and Figure 4, demonstrate the benefits obtained at least for history sequences longer than 2 genres with an F1 score greater than\n71%.\nRegarding multichannel P4LSTM approach, the highest performance reaches an F1-measure of about 76% using 4 sized sequences with a gain of about 2 and 5 points compared respectively to P2LSTM and 4n-gram model best performances."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "The paper proposes an original Long Short-Term Memory (LSTM) based neural network architecture for automatic classification of multistream sequences called PLSTM. PLSTM is evaluated during a telecast genre prediction task and the observed results show that the proposed PLSTM is efficient when the size of sequences is large enough with a gain of more than 10 points of error rate compared to classical ngram model, and about 7 and 4 points respectively compared to LSTM and P2LSTM. Future works will apply this promising multistream neural network architecture to Spoken Language Understanding tasks such as topic extraction, keyword spotting and Part-of-Speech tagging."
    }, {
      "heading" : "7. REFERENCES",
      "text" : "[1] F. A. Gers, D. Eck, and J. Schmidhuber, “Applying\nlstm to time series predictable through time-window approaches,” in Artificial Neural NetworksICANN 2001. Springer, 2001, pp. 669–676.\n[2] A. Severyn and A. Moschitti, “Twitter sentiment anal-\nysis with deep convolutional neural networks,” in Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015, pp. 959–962.\n[3] M. Huang, Y. Cao, and C. Dong, “Modeling rich con-\ntexts for sentiment classification with lstm,” CoRR, vol. abs/1605.01478, 2016.\n[4] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,\n“Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n[5] J. L. Elman, “Finding structure in time,” Cognitive sci-\nence, vol. 14, no. 2, pp. 179–211, 1990.\n[6] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural computation, vol. 9, no. 8, pp. 1735– 1780, 1997.\n[7] A. Graves and J. Schmidhuber, “Framewise phoneme\nclassification with bidirectional lstm and other neural network architectures,” Neural Networks, vol. 18, no. 5, pp. 602–610, 2005.\n[8] M. Sundermeyer, R. Schlüter, and H. Ney, “Lstm neural\nnetworks for language modeling.” in INTERSPEECH, 2012, pp. 194–197.\n[9] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show\nand tell: A neural image caption generator,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3156–3164.\n[10] A. Graves, S. Fernández, and J. Schmidhuber, “Bidi-\nrectional lstm networks for improved phoneme classification and recognition,” in Artificial Neural Networks: Formal Models and Their Applications–ICANN 2005. Springer, 2005, pp. 799–804.\n[11] S. Fernández, A. Graves, and J. Schmidhuber, “An ap-\nplication of recurrent neural networks to discriminative keyword spotting,” in Artificial Neural Networks– ICANN 2007. Springer, 2007, pp. 220–229.\n[12] M. Wöllmer, F. Eyben, S. Reiter, B. Schuller, C. Cox,\nE. Douglas-Cowie, and R. Cowie, “Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies.” in INTERSPEECH, vol. 2008. Citeseer, 2008, pp. 597–600.\n[13] H. A. Bourlard and N. Morgan, Connectionist speech\nrecognition: a hybrid approach. Springer Science & Business Media, 2012, vol. 247.\n[14] Y. Bengio, “Markovian models for sequential data,”\nNeural computing surveys, vol. 2, no. 1049, pp. 129– 162, 1999.\n[15] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber,\n“Learning precise timing with lstm recurrent networks,” The Journal of Machine Learning Research, vol. 3, pp. 115–143, 2003.\n[16] M. Schuster and K. K. Paliwal, “Bidirectional recurrent\nneural networks,” Signal Processing, IEEE Transactions on, vol. 45, no. 11, pp. 2673–2681, 1997.\n[17] M. Schuster, “On supervised learning from sequential\ndata with applications for speech recognition,” Daktaro disertacija, Nara Institute of Science and Technology, 1999.\n[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al., “Scikit-learn: Machine learning in python,” The Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.\n[19] A. Stolcke et al., “Srilm-an extensible language mod-\neling toolkit.” in INTERSPEECH, vol. 2002, 2002, p. 2002.\n[20] F. Chollet, “keras,” https://github.com/fchollet/keras,\n2015.\n[21] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J.\nGoodfellow, A. Bergeron, N. Bouchard, and Y. Bengio, “Theano: new features and speed improvements,” Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012."
    } ],
    "references" : [ {
      "title" : "Applying lstm to time series predictable through time-window approaches",
      "author" : [ "F.A. Gers", "D. Eck", "J. Schmidhuber" ],
      "venue" : "Artificial Neural NetworksICANN 2001. Springer, 2001, pp. 669–676.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Twitter sentiment analysis with deep convolutional neural networks",
      "author" : [ "A. Severyn", "A. Moschitti" ],
      "venue" : "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015, pp. 959–962.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Modeling rich contexts for sentiment classification with lstm",
      "author" : [ "M. Huang", "Y. Cao", "C. Dong" ],
      "venue" : "CoRR, vol. abs/1605.01478, 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Finding structure in time",
      "author" : [ "J.L. Elman" ],
      "venue" : "Cognitive science, vol. 14, no. 2, pp. 179–211, 1990.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735– 1780, 1997.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "author" : [ "A. Graves", "J. Schmidhuber" ],
      "venue" : "Neural Networks, vol. 18, no. 5, pp. 602–610, 2005.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Lstm neural networks for language modeling.",
      "author" : [ "M. Sundermeyer", "R. Schlüter", "H. Ney" ],
      "venue" : "in INTERSPEECH,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3156–3164.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bidirectional lstm networks for improved phoneme classification and recognition",
      "author" : [ "A. Graves", "S. Fernández", "J. Schmidhuber" ],
      "venue" : "Artificial Neural Networks: Formal Models and Their Applications–ICANN 2005. Springer, 2005, pp. 799–804.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "An application of recurrent neural networks to discriminative keyword spotting",
      "author" : [ "S. Fernández", "A. Graves", "J. Schmidhuber" ],
      "venue" : "Artificial Neural Networks– ICANN 2007. Springer, 2007, pp. 220–229.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies.",
      "author" : [ "M. Wöllmer", "F. Eyben", "S. Reiter", "B. Schuller", "C. Cox", "E. Douglas-Cowie", "R. Cowie" ],
      "venue" : "in INTER- SPEECH,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Connectionist speech recognition: a hybrid approach",
      "author" : [ "H.A. Bourlard", "N. Morgan" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Markovian models for sequential data",
      "author" : [ "Y. Bengio" ],
      "venue" : "Neural computing surveys, vol. 2, no. 1049, pp. 129– 162, 1999.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Learning precise timing with lstm recurrent networks",
      "author" : [ "F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber" ],
      "venue" : "The Journal of Machine Learning Research, vol. 3, pp. 115–143, 2003.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "M. Schuster", "K.K. Paliwal" ],
      "venue" : "Signal Processing, IEEE Transactions on, vol. 45, no. 11, pp. 2673–2681, 1997.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "On supervised learning from sequential data with applications for speech recognition",
      "author" : [ "M. Schuster" ],
      "venue" : "Daktaro disertacija, Nara Institute of Science and Technology, 1999.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg" ],
      "venue" : "The Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Srilm-an extensible language modeling toolkit.",
      "author" : [ "A. Stolcke" ],
      "venue" : "in INTERSPEECH,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "keras",
      "author" : [ "F. Chollet" ],
      "venue" : "https://github.com/fchollet/keras, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recently, automatic sequence classification became an ubiquitous problem, having then encountered a high research interest [1, 2, 3].",
      "startOffset" : 123,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "Recently, automatic sequence classification became an ubiquitous problem, having then encountered a high research interest [1, 2, 3].",
      "startOffset" : 123,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "Recently, automatic sequence classification became an ubiquitous problem, having then encountered a high research interest [1, 2, 3].",
      "startOffset" : 123,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "Among the recent methods employed to structure these sequences, the machine learning domain provides a set of high-level representations well adapted to automatic sequence classification based on Deep Neural Networks (DNN) such as Convolutional Neural Networks (CNN) [4] or Recurrent Neural Networks (RNN) [5].",
      "startOffset" : 267,
      "endOffset" : 270
    }, {
      "referenceID" : 4,
      "context" : "Among the recent methods employed to structure these sequences, the machine learning domain provides a set of high-level representations well adapted to automatic sequence classification based on Deep Neural Networks (DNN) such as Convolutional Neural Networks (CNN) [4] or Recurrent Neural Networks (RNN) [5].",
      "startOffset" : 306,
      "endOffset" : 309
    }, {
      "referenceID" : 5,
      "context" : "RNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have gained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "RNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have gained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "RNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have gained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "RNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have gained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing.",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "In speech recognition [10, 11, 12], these LSTM models exploit the contextual information whenever speech production or perception is influenced by emotion, strong accents, or background noise.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "In speech recognition [10, 11, 12], these LSTM models exploit the contextual information whenever speech production or perception is influenced by emotion, strong accents, or background noise.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "In speech recognition [10, 11, 12], these LSTM models exploit the contextual information whenever speech production or perception is influenced by emotion, strong accents, or background noise.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "The most effective use of RNNs for sequence classification is to combine the RNNs with Hidden Markov Models (HMMs) in a hybrid approach [13, 14].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "The most effective use of RNNs for sequence classification is to combine the RNNs with Hidden Markov Models (HMMs) in a hybrid approach [13, 14].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "Long Short-Term Memory (LSTM) [6] networks are a special case of Recurrent Neural Networks (RNNs) [5].",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "Long Short-Term Memory (LSTM) [6] networks are a special case of Recurrent Neural Networks (RNNs) [5].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "This internal state allows the RNN to process arbitrary sequences of inputs such as sequences of words [8] for language modeling, time series [1].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "This internal state allows the RNN to process arbitrary sequences of inputs such as sequences of words [8] for language modeling, time series [1].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "[6] shows that LSTM networks outperformRNNs for finding long range context and dependencies.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "The LSTM composite functionH forming the LSTM cell with peephole connections [15] is presented in Figure 1 and defined as:",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "Bidirectional RNN (BRNN) [16], presented in Figure 2, can process both directions with two separate hidden layers (one for each direction).",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "By replacing the BRNN cells with LSTM cells, the Bidirectionnal LSTM (BLSTM) [7] is obtained.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "[7] introduces the BLSTM with Back Propagation Trough Time (BPTT) algorithm [17] for training.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "[7] introduces the BLSTM with Back Propagation Trough Time (BPTT) algorithm [17] for training.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : "Data from 2013 and 2014 are merged and split into the training (70%) and validation (30%) datasets using a stratified shuffle split [18] in order to preserve the same percentage of samples of each class in the output of both folds, while the 2015 dataset is kept for testing.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "Mono-channel input: only M6 history sequences for a baseline n-gram experiment (with a statistical language model from the SRILM toolkit [19]) and a straightforward LSTM model.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "The Keras library [20], based on Theano [21] for fast tensor manipulation and CUDAbased GPU acceleration, has been employed to train neural networks on an Nvidia GeForce GTX TITAN X GPU card.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 20,
      "context" : "The Keras library [20], based on Theano [21] for fast tensor manipulation and CUDAbased GPU acceleration, has been employed to train neural networks on an Nvidia GeForce GTX TITAN X GPU card.",
      "startOffset" : 40,
      "endOffset" : 44
    } ],
    "year" : 2017,
    "abstractText" : "Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-theart architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach.",
    "creator" : "LaTeX with hyperref package"
  }
}