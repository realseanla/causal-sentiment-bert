{
  "name" : "1601.06755.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Utility of Hedged Assertions in the Emergence of Shared Categorical Labels",
    "authors" : [ "Martha Lewis", "Jonathan Lawry" ],
    "emails" : [ "martha.lewis@bristol.ac.uk,", "j.lawry@bristol.ac.uk" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "An evolutionary approach to semantics enables the development in robots and autonomous agents of flexible, mutable concepts that could be learnt through interaction and can change over time [13]. This approach is investigated by Eyre and Lawry in [2], in which they develop a model of language evolution based in the label semantics framework. They show that using a mixture of positive and negated assertions enables the development of languages that are both shared, and discriminate effectively between elements within the environment. We extend this work to include assertions modified by the words ‘very’ and ‘quite’, and show that doing so improves performance in two ways. Use of the hedge ‘very’ improves levels of convergence attained. Using the hedge ‘quite’ reduces the amount of overlap within an agent’s label set. We describe in detail the theoretical approach to concepts taken and linguistic hedges in the remainder of this section. Section 2 gives details of the mathematical and computational model used in the simulations. Section 3 gives results of the simulations which are discussed in section 4. Lastly, section 5 gives conclusions and further avenues of research."
    }, {
      "heading" : "1.1 A representation model for concepts",
      "text" : "We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3]. Prototype theory offers an alternative to the classical theory of concepts, basing categorization on proximity to a prototype. This approach is based on experimental results where human subjects were found to view membership in a concept as a matter of degree, with some objects having higher membership than others [11]. Fuzzy set theory [15], in which an object x has a graded membership µL(x) in a concept L, was proposed as a formalism for prototype theory. However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].\n1 University of Bristol, England, email: martha.lewis@bristol.ac.uk, j.lawry@bristol.ac.uk\nConceptual spaces theory renders concepts as convex regions of a conceptual space - a geometrical structure with quality dimensions and a distance metric. Examples are: the RGB colour cube, pictured in figure 1; physical dimensions of height, breadth and depth; or the taste tetrahedron. Since concepts are convex regions of such spaces, the centroid of such a region can naturally be viewed as the prototype of the concept.\nLabel semantics [7] is a random set approach to concepts which quantifies an agent’s uncertainty about the extent of application of a concept. We refer to this as subjective uncertainty [8] to emphasise that it concerns the definition of concepts and categories, in contrast to stochastic uncertainty which concerns the state of the world. Lawry and Tang [8] combine the label semantics approach with conceptual spaces and prototype theory, to give a formalisation of concepts as based on a prototype and a threshold, located in a conceptual space.\nWithin this framework, agents use sets of labels LA = {L1, L2, ..., Ln} to describe an underlying conceptual space Ω with distance metric d(x, y) between points. The conceptual space could be, as mentioned, the RGB colour space. Labels Li would then be concepts such as ‘red’, ‘blue’, ‘purple’, ‘orange’ and so on. These labels are viewed as regions of the conceptual space. So the concept ‘blue’ is represented by the blue region in the colour cube. Within label semantics, these regions are specified by prototypes Pi and thresholds εi. This is in contrast to Gärdenfors’ original approach which is to view the space as partitioned by a Voronoi tessellation. If this latter approach is taken, each individual point in the conceptual space is allocated to exactly one label. With a prototype-threshold approach, it is easy to accommodate the idea of an object being accurately described by more than one concept, or conversely, some points within the space not being assigned to any concept. This difar X iv :1\n60 1.\n06 75\n5v 1\n[ cs\n.A I]\n2 5\nJa n\n20 16\nference is illustrated in figures 2 and 3.\nIn this model, however, agents are uncertain as to exactly where the thresholds lie. To illustrate this, consider the concept ‘tall’. It is easy to point out a tall person, and to point out a person who is not tall, but it is difficult to specify the exact threshold between ‘tall’ and ‘not tall’. This uncertainty concerning where the threshold lies is represented in the label semantics framework by saying that a threshold εi is drawn from a probability distribution δi. Labels Li are associated with neighbourhoods N εiLi = {~x ∈ Ω : d(~x, Pi) ≤ εi}, i.e. the region within the threshold. These ideas are represented in figure 4.\nThe threshold εi is uncertain, however, so there is some probability that εi in figure 4 is actually wide enough to include the object b, i.e. that Li is appropriate to describe b. The appropriateness µLi(x) of a\nlabel Li to describe an element x is then given by the probability that x lies within the neighbourhood N εiLi , i.e. that the distance d(x, Pi) is less than εi. So:\nµLi(x) = P (d(x, Pi) ≤ εi) = ∫ ∞ d(x,Pi) δi(εi)dεi\nFigure 5 shows how this appropriateness measure works in a setup similar to that in figure 4.\nThis appropriateness measure is similar to Zadeh’s description of fuzzy membership in a concept [15]."
    }, {
      "heading" : "1.2 Linguistic hedges",
      "text" : "Hedges are words or phrases such as ‘very’, ‘quite’, ‘strictly speaking’ which modify the domain of application of a concept. In particular, ‘very’, and ‘quite’ respectively contract or expand the domain of application of a concept, so that, for example, ‘very tall’ applies to fewer people than does ‘tall’, whereas ‘quite tall’ applies to more. Within fuzzy set theory, we expect that µveryL(x) ≤ µL(x) and µquiteL(x) ≥ µL(x). Applying this to the concept ‘tall’, again, this means that membership in the concept ‘very tall’ should always be less than membership in ‘tall’. So anyone who can be described as ‘very tall’ can also be described as ‘tall’. Zadeh [16] uses operations of concentration and dilation to render these ideas. Concentration is described as CON(µLi(x)) = (µLi(x))\n2 and dilation is often rendered as DIL(µLi(x)) = (µLi(x))\n1/2. However, we argue, as do [1], that Zadeh’s formulae are, to an extent, arbitrary, since the notion of taking a power of a membership value does not correspond to anything that language users might do. Rather, it simply has some of the right effects. As with [1], we take a semantic approach.\nIn [9], we propose that a concept ‘very L’ or ‘quite L’ be rendered by considering that the prototype of ‘very/quite L’ is equal to that of the base concept L, but that the threshold of the hedged concept ‘very/quite L’ is respectively smaller or larger than that of the base concept. This approach is grounded in the idea that ‘very/quite L’ should apply to respectively fewer or more objects than L. Narrowing or widening the threshold achieves this in a natural way. This is illustrated in figure 6.\nOur model of the hedges ‘very’ and ‘quite’ therefore requires simply that vεi ≤ εi and that qεi ≥ εi. We implement this model in a version of the multi-agent simulation created in [2] in order to investigate how the use of these hedges in a model of language helps\nthe emergence of shared categories across a community of language users."
    }, {
      "heading" : "2 METHODS",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "To investigate the utility of hedged assertions we implement a multiagent simulation of a version of the category game [14], following [2], in which shared categories develop over time as a result of the interactions of the category users. An overview of the game is as follows. Agents use labels to describe a conceptual space Ω. At each timestep, agents are randomly paired into speakers and listeners, and each pair is shown a distinct element x ∈ Ω. The speaker makes an assertion θ about the element based on its label set. The listener then updates its own label set to be more similar to that of the speaker, based on this assertion and a parameter w which can be thought of as the age of the speaker. The update made by the speaker is a combination of shifting the prototype of the relevant label and changing the size of the threshold. The aim is that after a number of timesteps, label sets across the population have converged to a common set of shared categories."
    }, {
      "heading" : "2.2 Conceptual models",
      "text" : "Each agent is equipped with the same number n of labels Li, with point prototypes Pi ∈ Ω, where Ω = [0, 1]3. At the start of the simulations the Pi are uniformly distributed around the space. Thresholds εi are also randomly initiated, and considered to be some multiple of a base threshold ε. Each threshold εi ∼ U(0, bi), where again, the bi can be considered to be a multiple of some common b, and the bi are taken from U [0.5, 2]. The distance metric is Euclidean.\nEach agent therefore has a label set LA = {L1, L2, ...Ln}. These labels can be hedged to form a set LA+ = LA∪{very Li, quite Li : i = 1, ..., n}. Hedged concepts have the same prototype Pi as basic labels, but a scaled threshold vεi or qεi where v < 1 and q > 1. Agents can assert positive or negated, hedged or basic labels, giving an assertion set AS = {kLi,¬kLi : i = 1, ..., n; k = very, quite, basic}, where k = basic means that the label is not hedged."
    }, {
      "heading" : "2.3 Assertion model",
      "text" : "At each timestep, half the agents are designated speaker agents and make assertions, determined by the assertion model used.The assertion model is based on the probability of making a particular assertion θ, given that the object being described is x. Following methods in [2, 8], we calculate the posterior probability of each θ ∈ AS, given an element x ∈ Ω. The assertion made by a speaker agent is the assertion with the highest probability. The posterior probability of each θ, given x, is determined by the appropriateness of the assertion θ to describe x, i.e. µθ(x), and the prior probability P (θ) of asserting θ.\nWe first consider which sets of labels that are appropriate to describe x ∈ Ω. The probability that any particular set of labels F ⊆ LA are appropriate to describe x is given by a probability mass function mx : 2LA → [0, 1]. One way of determining mx is via the consonant selection function introduced in [8]. This states:\nDefinition 1 (Consonant selection function) Given non-zero appropriateness measures on basic labels µLi(x) : i = 1, ..., n ordered such that µLi(x) ≥ µLi+1 for i = 1, ..., n, the consonant selection function identifies the mass function\nmx({L1, ..., Ln}) = µLn(x) mx({L1, ..., Li}) = µLi(x)− µLi+1(x) for i = 1, ..n− 1 mx(∅) = 1− µL1(x) mx(F ) = 0 if F 6= {L1, L2, ..., Lk} for some k ≤ n\nBecause we have ordered the labels by µLi(x) ≥ µLi+1 , if the label Li is appropriate to describe x, all labels Lj : j < i must also be appropriate to describe x. The quantity µLi(x) − µLi+1(x) corresponds to the idea that x in some sense lies between the thresholds εi+1 and εi, so that Li is appropriate to describe x, but Li+1 is not. We extend this definition to the case of hedged labels simply by considering all hedged labels as basic labels, explained in the example below.\nExample 2 (Determining the mass function) Suppose we are determining the mass function for subsets F ⊆ {kL1, kL2 : k = very, quite, basic}, given the point a ∈ x1 × x2, as illustrated in figure 7.\nSuppose that µquiteL2(a) = 0.9, µL2(a) = 0.7, µquiteL1(a) = 0.3, µveryL2(a) = 0.1, µL1(a) = 0, µveryL1(a) = 0, giving us the order µquiteL2(a) ≥ µL2(a) ≥ µquiteL1(a) ≥ µveryL2(a) ≥ µL1(a) ≥ µveryL1(a). We may then assign probabilities to subsets of labels according to the consonant selection function:\nmx(F6) = mx({quite L2, L2, quite L1, very L2, L1, very L1}) = µveryL1(a) = 0 mx(F5) = mx({quite L2, L2, quite L1, very L2, L1}) = µL1(a)− µveryL1(a) = 0 mx(F4) = mx({quite L2, L2, quite L1, very L2}) = µveryL2(a)− µL1(a) = 0.1 mx(F3) = mx({quite L2, L2, quite L1}) = µquiteL1(a)− µveryL2(a) = 0.2 mx(F2) = mx({quite L2, L2}) = µL2(a)− µquiteL1(a) = 0.4 mx(F1) = mx({quite L2}) = µquiteL2(a)− µL2(a) = 0.2 mx(∅) = 1− µquiteL2(a) = 0.1\nHaving determined the probability mass function on sets of labels, a mass assignment on sets of assertions is then defined.\nDefinition 3 (Mass assignment on assertions) max : 2AS → [0, 1] is defined such that:\nmax(G) = ∑\nF⊆LA+:C(F )=G\nmx(F )\nwhere C (F ) = {θ ∈ AS : F ∈ λ(θ)}, and λ(θ) is defined recursively by\nλ(kLi) = {F ⊆ LA+ : kLi ∈ F} λ(¬θ) = (λ(θ))c\nλ(θ ∧ φ) = λ(θ) ∩ λ(φ) λ(θ ∨ φ) = λ(θ) ∪ λ(φ)\nThis definition has the implication that for Gi = Fi ∪ {¬kLj : kLj ∈ LA+\\Fi}, max(Gi) = mx(Fi).\nThen the probability of an assertion θ being made, given that an object x is being described, can be calculated by summing over G ⊆ AS that contain θ.\nDefinition 4 Given a prior distribution on AS, a posterior distribution given an object x can be calculated by:\nP (A = θ|x) = ∑\nG⊆AS:θ∈G\nmax(G)P (A = θ|A ∈ G)\n= P (θ) ∑\nG⊆AS:θ∈G\nmax(G)\nP (G)\nHere, P (G) = ∑ ϕ∈G P (ϕ)\nThe value of P (θ) for one particular label L is a product of two elements: the prior probability pp of making a positive assertion (or 1 − pp for a negated assertion); and the prior probability of making a hedged assertion, given by pv for making an assertion hedged with the word ‘very’, pq for making an assertion hedged with ‘quite’ or 1− pv − pq for making a basic assertion, summarised in table 1.\nThe prior probability of asserting any particular label Li ∈ LA is uniform acrossLA. Hence the value ofP (θ) calculated above should be divided by n, giving, for example,\nP (¬vL2) = pn ∗ pv n\nExample 5 (Determining the posterior probability of assertion) Suppose, for an easy example, we want to calculate the probability of asserting ‘very L1’, given object a, as in example 2. We need to calculate\nP (A = ‘very L1’|a) = P (θ) ∑\nG⊆AS:‘veryL1’∈G\nmax(G)\nP (G)\nwhere Gi = Fi ∪ {¬kLj : kLj ∈ LA+\\Fi}. However, the only subset Gi 3 ‘very L1’ is G6, so\nP (A = ‘very L1’|x) = P (‘very L1’) max(G6)\nP (G6)\n= 0\nSuppose, for a more involved example, the label set LA+ is as in example 2, with pp = 0.7, pv = 0.7, pq = 0.2, and we want to determine P (A = quite L1|a). The prior probability P (quite L1) = 0.7∗0.2\n2 = 0.07. So we have:\nP (A = quite L1|a) = 0.07 ∑\nGi:quiteL1∈Gi\nmax(Gi)\nP (Gi)\n= 0.07( max(G6) P (G6) + max(G5) P (G5) + max(G4) P (G4) + max(G3) P (G3) ) = 0.07(0 + 0 + 0.1∑\nϕ∈G4 P (ϕ) + 0.2∑ ϕ∈G3 P (ϕ) )\n= 0.07( 0.1\n0.54 +\n0.2 0.4 )\n= 0.048\nHaving calculated the probability of each assertion, the speaker agent makes the most probable assertion θ ∈ AS."
    }, {
      "heading" : "2.4 Updating algorithms",
      "text" : "Once the speaker agent has made assertion θ, the listener agent computes µθ(x) based on its current label set. If µθ(x) < w, where w is a parameter that can be thought of as the age of the speaker agent, the listener agent updates its label set LA by moving the prototype and/or changing the threshold of the concept, until µθ(x) = w. Formulae for these updates are again based on [2]. A label defined by Pi and εi is updated to P ′i = Pi − λ(x− Pi) and ε′i = αεi. Values for λ and α are sought, such that µ′θ(x) = w.\n2.4.1 Case 1: θ = kLi\nRecall that εi ∼ U(0, bi), so that for x ∈ Ω,\nµkLi(x) = 1− ||x− Pi|| kbi < w by assumption.\nThe label Li is updated to L′i, where P ′ i = Pi − λ(x − Pi) and ε′i = αεi, such that µkL′i(x) ≥ w, and minimising the distance between the interpretations as measured by the Haussdorff distance between the two neighbourhoods,\nH (NLi ,NL′i) = ||Pi − P ′ i ||+ |εi − ε′i| (1)\n= |λ|||x− Pi||+ εbi b |1− α| (*)\nTo minimise the update, we set µkL′i(x) = w, so:\nw = µkL′i(x) = 1− ||x− P ′i || kb′i = 1− |1− λ|||x− Pi|| αkbi\nwhich gives\nα = |1− λ|||x− Pi||\n(1− w)kbi\n= (1− λ)||x− Pi||\n(1− w)kbi since λ = 1→ P ′i = x\nTo updateLi we will always want λ ≥ 0, α ≥ 1, as we are dealing with a positive label.\nSubstituting α into equation (*), we obtain\nH (NLi ,N ′ Li) = |λ|||x− Pi||+ εbi b ( (1− |λ|)||x− Pi|| (1− w)kbi − 1)\n= |λ|||x− Pi||(1− ε b(1− w)k ) + ε||x− Pi|| b(1− w)k − εbi b\n(2)\nThen if 1 − ε b(1−w)k > 0, i.e. ε < b(1 − w)k, the quantity (2)\ncan be minimised by setting λ = 0 so α = ||x−Pi|| (1−w)kbi . Otherwise, we have α = 1, λ = 1− (1−w)kbi||x−Pi|| . Since ε is a random variable, so is the choice between λ and α. We therefore need a concrete updating rule. We update Pi and bi with the expected values of λ and α respectively. ε ∼ Uniform[0, b], so\nP (ε < b(1− w)k) = { (1− w)k if (1− w)k < 1 1 otherwise\nWe can therefore calculate\nE(α) =\n{ ||x−Pi||\nbi + 1− (1− w)k if (1− w)k < 1\n||x−Pi|| (1−w)kbi\notherwise\nand\nE(λ) = { (1− (1− w)k)(1− (1−w)kbi||x−Pi|| ) if (1− w)k < 1 0 otherwise"
    }, {
      "heading" : "2.4.2 Case 2: θ = ¬kLi",
      "text" : "By an entirely similar argument, we obtain\nE(α) =\n{ ||x−Pi||\nbi + 1− wk if wk < 1\n||x−Pi|| wkbi\notherwise\nand\nE(λ) = { (1− wk)(1− wkbi||x−Pi|| ) if wq < 1 0 otherwise\nSo at each timestep, each listener agent, for whom µθ(x) < w, updates the relevant label using the the quantities E(α), E(λ)."
    }, {
      "heading" : "2.5 Performance metrics",
      "text" : "Performance metrics from [2] are used, measuring the Average Pairwise Distance between label sets (APD) and the Average Label Overlap (ALO). APD measures the difference in label sets in the community, and ALO indicates the extent to which an agent’s concepts overlap. We seek low values for each metric.\nAPD is calculated using the Haussdorff distance between two neighbourhoods as given in equation 1. The difference between the label sets of any one pair of agents is given by\nIPD = n∑ i=1 H (N jLi ,N k Li)\nwhere n is the number of labels each agent has and j and k refer to distinct agents.\nThis is averaged over pairs of agents. There are N agents, therefore ( N 2 ) pairs, giving:\nAPD = 2\nN(N − 1) N∑ k=j+1 N∑ j=1 IPDjk\nALO is the extent to which labels overlap. To calculate this, we take the maximum value of the intersection of a pair of labels, as measured by a min rule. We average this value over pairs of labels. The overlap within an individual’s label set is therefore\nILO = 2\nn(n− 1) n∑ j=i+1 n∑ i=1 max{min{µLi(x), µLj (x) : x ∈ Ω}}\nAveraged across the population this is:\nALO = 1\nN N∑ k=1 ILOk\nwhere ILOk siginifies agent k’s label overlap."
    }, {
      "heading" : "2.6 Simulation process",
      "text" : "Simulations with n = 100 agents were run for T = 104 timesteps. Agent weights w ∈ [0.2, 0.8] were updated at each timestep in increments of 1/T . Whenw ≥ 0.8, agents are reborn with randomised labels and w = 0.2. 20 simulations are run for each reported combination of parameters.\n[2] show that if pp ∈ [0.5, 0.6] then performance of the system changes from low ALO and high APD to vice versa at approximately pp = 0.56. We ran simulations in a slightly extended range for comparison, varying the prior probabilities pv, pb and pq of asserting the different hedges ‘very’, ‘basic’, and ‘quite’. We present results from three sets of parameters. As a baseline we run simulations with no hedges, i.e. pv = 0, pb = 1, pq = 0. To investigate the effects of using contraction hedges, we run simulations with parameters pv = 0.7, pb = 0.2, pq = 0.1. For expansion hedges, we use parameters pv = 0.1, pb = 0.2, pq = 0.7."
    }, {
      "heading" : "3 RESULTS",
      "text" : "The results presented show performance against the two metrics after 104 simulation timesteps. By this point, the population has generally reached a steady state in which performance does not greatly change.\nFigure 8 shows the steady state of APD achieved after 104 timesteps for a range of values pp ∈ [0.4, 0.6]. Three sets of results are presented: results using unhedged assertions; results with a high prior probability of using contraction hedges; and results from simulations with a high prior probability of asserting expansion hedges, where these prior probabilities are as stated in 2.6.\nA high prior of asserting contracted labels reduces minimum APD achieved from 0.38 when pp = 0.56 or pp = 0.6 to 0.29 when pp = 0.57 (figure 8). Performing a paired t-test across the 20 simulations gives the mean difference between these values as 0.097. This difference is statistically significant with p < 0.001 and with 95% confidence interval [0.084, 0.110]. The median and range of results are given in figure 9. At pp = 0.57, ALO decreases, from 0.92 to 0.89 (figure 11). The mean value of this difference across the 20 simulations is 0.032. Again, this is statistically significant with p < 0.001 and 95% confidence interval of [0.029, 0.035], further illustrated in figure 10. These results imply that a high prior probability of asserting contraction hedges enables us to improve convergence between agents’ label sets as well as reducing overlap within label sets slightly.\nWith a high prior probability of asserting expanded labels, lower values of ALO can be achieved when the probability of asserting positive labels is 0.45 , decreasing to 0.02 compared to 0.1, figure 11. The mean difference between these values across the 20 simulations is 0.083, which is statistically significant with p < 0.001 and a 95% confidence interval of [0.078, 0.089]. The data is represented in figure 13. At this value of pp, APD achieved is 0.73 compared to 0.85 for unhedged assertions, figure 8. The mean value of this difference across the 20 simulations is 0.12. This figure is statistically significant with p < 0.001 and 95% confidence interval [0.116, 0.126]. The data is again represented in figure 12. A high prior probability of asserting expansion hedges therefore enables minimal overlap to be maintained at low pp whilst improving convergence.\nWe can also examine how fast the community of agents arrives at a\nsteady state. Figure 14 shows that at short timescales (t < 2000), better convergence may be achieved allowing only unhedged assertions. In a more extreme case, figure 15 shows that for pp = 0.5, better performance on the ALO metric is only achieved after 7500 timesteps. Although this improvement takes a longer time to achieve, it goes together with improved performance on APD which is achieved in a similar timescale to the unhedged model 16."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "These results show that, in a model of language development across a population, hedged assertions can improve both the level of convergence to shared language as measured by average pairwise difference between label sets (APD) and, to an extent, the discriminatory power of individuals’ label sets, as measured by average label\noverlap (ALO). The two different types of hedges improve performance in distinct ways. If overall convergence is important, a high prior probability of asserting contraction hedges should be used to improve performance on the APD metric. Conversely, if the ability of the agents to discriminate precisely between objects in the environment is more important, then expansion hedges, together with lower probabilities of asserting positive labels, should be used to maintain low levels of ALO whilst still improving performance on APD.\nThe improved performance against the two metrics is tempered by the fact that the speed at which the steady state is achieved is somewhat slower than when using simply unhedged assertions. However, the improvement in APD is seen relatively quickly at pp = 0.56, soon after the unhedged model has reached its steady state. The improvement in ALO when using expansion hedges, for pp = 0.5, does not occur until after 7, 500 timesteps, well after the unhedged model has reached its steady state. However, the improvement in perfor-\nmance on ALO goes together with improved performance on APD which is attained at the same speed as the in the unhedged model.\nIf the speed of development of shared categories is not important, the two types of hedges would be useful in different types of situation, depending whether convergence or discriminatory power is more important. This might be dependent on, for example, the structure of the underlying environment. In the current simulation, objects are presented uniformly across the space. If objects were distributed non-uniformly, perhaps clumping in various regions of the space, then perhaps the ability to discriminate precisely between different labels would be less important, since the environment provides that distinction naturally. Convergence to shared labels would then be more important.\nIf speed is important, using contraction hedges can still improve levels of convergence in a relatively short timeframe.\nThere are many parameters in the simulation that bear further investigation. The distribution of objects in the environment, as mentioned above, is likely to have an effect on performance again the two metrics. In the current simulations, hedge values of v = 0.5 and h = 2 are used. Increasing and decreasing these values could have an impact on performance, as would, perhaps, allowing agents to have difference values of v and h. The range of w allowed also affects performance. When w = [0.01, 0.99], agents no longer achieve high levels of convergence at pp > 0.55 (results not shown). Other weight ranges may positively affect performance, however."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "We have investigated the utility of hedged assertions in the development of a shared language, and shown that allowing agents to make hedged assertions improves the ability to develop common categories\nin two distinct ways. Firstly, using contraction hedges, i.e. words like ‘very’, allows improved levels of convergence to shared categories, whilst slightly improving the extent to which labels overlap. Secondly, using expansion hedges, or words like ‘quite’, enables the development of label sets that are more discriminatory of the environment and also have better levels of convergence. However, both these improvements come with a slower speed of development of shared labels. It may be possible to improve these speeds by tuning other parameters such as the age range of agents or the values of hedges used."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "Martha Lewis gratefully acknowledges support from EPSRC Grant No. EP/E501214/1"
    } ],
    "references" : [ {
      "title" : "Adjusting the core and/or the support of a fuzzy set-a new approach to fuzzy modifiers",
      "author" : [ "P. Bosc", "D. Dubois", "A. HadjAli", "O. Pivert", "H. Prade" ],
      "venue" : "Fuzzy Systems Conference, 2007. FUZZ-IEEE 2007. IEEE International, pp. 1–6. IEEE, ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Language games with vague categories and negations",
      "author" : [ "Henrietta Eyre", "Jonathan Lawry" ],
      "venue" : "Adaptive Behavior,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Conceptual spaces: The geometry of thought",
      "author" : [ "P. Gärdenfors" ],
      "venue" : "The MIT Press",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Inheritance of attributes in natural concept conjunctions",
      "author" : [ "J. Hampton" ],
      "venue" : "Memory & Cognition, 15(1), 55–71, ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Conceptual combinations and fuzzy logic",
      "author" : [ "J. Hampton" ],
      "venue" : "Concepts and Fuzzy Logic, eds., R. Belohlavek and G. J. Klir, The MIT Press, ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Prototype theory and compositionality",
      "author" : [ "H. Kamp", "B. Partee" ],
      "venue" : "Cognition, 57(2), 129–191, ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A framework for linguistic modelling",
      "author" : [ "J. Lawry" ],
      "venue" : "Artificial Intelligence, 155(1-2), 1–39, ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Uncertainty modelling for vague concepts: A prototype theory approach",
      "author" : [ "J. Lawry", "Y. Tang" ],
      "venue" : "Artificial Intelligence, 173(18), 1539– 1558, ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A label semantics approach to linguistic hedges",
      "author" : [ "Martha Lewis", "Jonathan Lawry" ],
      "venue" : "International Journal of Approximate Reasoning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "On the adequacy of prototype theory as a theory of concepts",
      "author" : [ "D.N. Osherson", "E.E. Smith" ],
      "venue" : "Cognition, 9(1), 35–58, ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Cognitive representations of semantic categories.",
      "author" : [ "E. Rosch" ],
      "venue" : "Journal of Experimental Psychology: General,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1975
    }, {
      "title" : "Conceptual combination with prototype concepts",
      "author" : [ "E.E. Smith", "D.N. Osherson" ],
      "venue" : "Cognitive Science, 8(4), 337–361, ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Why we need evolutionary semantics",
      "author" : [ "Luc Steels" ],
      "venue" : "KI 2011: Advances in Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Coordinating perceptually grounded categories through language: A case study for colour",
      "author" : [ "Luc Steels", "Tony Belpaeme" ],
      "venue" : "Behavioral and brain sciences,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "Fuzzy sets",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Information and Control, 8(3), 338–353, ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "A fuzzy-set-theoretic interpretation of linguistic hedges",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Journal of Cybernetics, ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1972
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "1 INTRODUCTION An evolutionary approach to semantics enables the development in robots and autonomous agents of flexible, mutable concepts that could be learnt through interaction and can change over time [13].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 1,
      "context" : "This approach is investigated by Eyre and Lawry in [2], in which they develop a model of language evolution based in the label semantics framework.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "1 A representation model for concepts We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3].",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "1 A representation model for concepts We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3].",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "1 A representation model for concepts We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "1 A representation model for concepts We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : "This approach is based on experimental results where human subjects were found to view membership in a concept as a matter of degree, with some objects having higher membership than others [11].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 14,
      "context" : "Fuzzy set theory [15], in which an object x has a graded membership μL(x) in a concept L, was proposed as a formalism for prototype theory.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "Label semantics [7] is a random set approach to concepts which quantifies an agent’s uncertainty about the extent of application of a concept.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "We refer to this as subjective uncertainty [8] to emphasise that it concerns the definition of concepts and categories, in contrast to stochastic uncertainty which concerns the state of the world.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 7,
      "context" : "Lawry and Tang [8] combine the label semantics approach with conceptual spaces and prototype theory, to give a formalisation of concepts as based on a prototype and a threshold, located in a conceptual space.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "This appropriateness measure is similar to Zadeh’s description of fuzzy membership in a concept [15].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "Zadeh [16] uses operations of concentration and dilation to render these ideas.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "However, we argue, as do [1], that Zadeh’s formulae are, to an extent, arbitrary, since the notion of taking a power of a membership value does not correspond to anything that language users might do.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "As with [1], we take a semantic approach.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "In [9], we propose that a concept ‘very L’ or ‘quite L’ be rendered by considering that the prototype of ‘very/quite L’ is equal to that of the base concept L, but that the threshold of the hedged concept ‘very/quite L’ is respectively smaller or larger than that of the base concept.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "We implement this model in a version of the multi-agent simulation created in [2] in order to investigate how the use of these hedges in a model of language helps",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "To investigate the utility of hedged assertions we implement a multiagent simulation of a version of the category game [14], following [2], in which shared categories develop over time as a result of the interactions of the category users.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "To investigate the utility of hedged assertions we implement a multiagent simulation of a version of the category game [14], following [2], in which shared categories develop over time as a result of the interactions of the category users.",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "Each agent is equipped with the same number n of labels Li, with point prototypes Pi ∈ Ω, where Ω = [0, 1].",
      "startOffset" : 100,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "Following methods in [2, 8], we calculate the posterior probability of each θ ∈ AS, given an element x ∈ Ω.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : "Following methods in [2, 8], we calculate the posterior probability of each θ ∈ AS, given an element x ∈ Ω.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "The probability that any particular set of labels F ⊆ LA are appropriate to describe x is given by a probability mass function mx : 2 → [0, 1].",
      "startOffset" : 136,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : "One way of determining mx is via the consonant selection function introduced in [8].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "Definition 3 (Mass assignment on assertions) max : 2 → [0, 1] is defined such that:",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "Formulae for these updates are again based on [2].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "5 Performance metrics Performance metrics from [2] are used, measuring the Average Pairwise Distance between label sets (APD) and the Average Label Overlap (ALO).",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "[2] show that if pp ∈ [0.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "We investigate the emergence of shared concepts in a community of language users using a multi-agent simulation. We extend results showing that negated assertions are of use in developing shared categories, to include assertions modified by linguistic hedges. Results show that using hedged assertions positively affects the emergence of shared categories in two distinct ways. Firstly, using contraction hedges like ‘very’ gives better convergence over time. Secondly, using expansion hedges such as ‘quite’ reduces concept overlap. However, both these improvements come at a cost of slower speed of development.",
    "creator" : "LaTeX with hyperref package"
  }
}