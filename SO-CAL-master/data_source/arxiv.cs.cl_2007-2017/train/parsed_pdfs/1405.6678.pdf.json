{
  "name" : "1405.6678.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hybrid Type-Logical Grammars, First-Order Linear Logic and the Descriptive Inadequacy of Lambda Grammars",
    "authors" : [ "Richard Moot" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n66 78\nv1 [\ncs .L\nO ]\n2 6\nM ay\n2 01\n4"
    }, {
      "heading" : "Hybrid Type-Logical Grammars,",
      "text" : ""
    }, {
      "heading" : "First-Order Linear Logic and the Descriptive",
      "text" : "Inadequacy of Lambda Grammars∗"
    }, {
      "heading" : "Richard Moot",
      "text" : "May 27, 2014"
    }, {
      "heading" : "1 Introduction",
      "text" : "Hybrid type-logical grammars (Kubota & Levine 2012, Kubota & Levine 2013c, Kubota & Levine 2013a) are a relatively new framework in computational linguistics, which combines insights from the Lambek calculus (Lambek 1958) and lambda grammars (Oehrle 1994, Muskens 2001, Muskens 2003)— lambda grammars are also called, depending on the authors, abstract categorial grammars (de Groote 2001) and linear grammars (Pollard 2011), though with somewhat different notational conventions1. The resulting combined system solves some know problems of both the Lambek calculus and of lambda grammars and the additional expressiveness of hybrid type-logical grammars permits the treatment of linguistic phenomena such as gapping which have no satisfactory solution in either subsystem.\nThe goal of this paper is to prove that hybrid type-logical grammars are a fragment of first-order linear logic. This embedding result has several important consequences: it not only provides a simple new proof theory for the calculus, thereby clarifying the proof-theoretic foundations of hybrid type-logical grammars, but, since the translation is simple and direct, it also provides several new parsing strategies for hybrid type-logical grammars. Second, NP-completeness of hybrid type-logical grammars follows immediately.\nThe main embedding result also sheds new light on problems with lambda grammars, which are a subsystem of hybrid type-logical grammars and hence a special case of the translation into first-order linear logic. Abstract categorial grammars are attractive both because of their simplicity — they use the simply\n∗This work has benefitted from the generous support of the French agency Agence Nationale de la Recherche as part of the project Polymnie (ANR-12-CORD-0004).\n1I prefer the term lambda grammars, since I think it most clearly describes the system. Though the term abstract categorial grammars appears to be more common, and I use it from time to time in this article, I will argue in Section 7 that abstract categorial grammars/lambda grammars are unlike all other versions of categorial grammars in important ways.\n1\ntyped lambda calculus, one of the most widely used tools in formal semantics, to compute surface structure (strings) as well as to compute logical form (meanings) — and because of the fact that they provide a natural account of quantifier scope and extraction; for both, the analysis is superior to the Lambek calculus analysis. So it is easy to get the impression that lambda grammars are an unequivocal improvement over Lambek grammars.\nIn reality, the picture is much more nuanced: while lambda grammars have some often discussed advantages over Lambek grammars, there are several cases — notably coordination, but we will see in Section 7 that this is true for any analysis where the Lambek calculus uses non-atomic arguments — where the Lambek grammar analysis is clearly superior. Many key examples illustrating the elegance of categorial grammars with respect to the syntax-semantics interface fail to have a satisfactory treatment in abstract categorial grammars.\nHowever, whether or not lambda grammars are an improvement over the Lambek calculus is ultimately not the most important question. Since there is a large number of formal systems which improve upon the Lambek calculus, it makes much more sense to compare lambda grammars to these extensions, which include, among many others, Hybrid Type-Logical Grammars, the Displacement calculus (Morrill, Valent́ın & Fadda 2011) and multimodal typelogical grammars (Moortgat 1996b, Moortgat 1997). These extended Lambek calculi all keep the things that worked in the Lambek calculus but improve on the analysis in ways which allow the treatment of more complex phenomena in syntax and especially in the syntax-semantics interface. Compared to these systems, the inadequacies of lambda grammars are evident: even for the things lambda grammars do right (quantifier scope and extraction), there are phenomena, such as reflexives and gapping, which are handled by the same mechanisms as quantifier scope and extraction in alternative theories, yet which cannot be adequately handled by lambda grammars. The abstract categorial grammar treatment suffers from problems of overgeneration and problems at the syntaxsemantics interface unlike any other categorial grammar. I will discuss some possible solutions for lambda grammars, but it is clear that a major redesign of the theory is necessary. The most painless solution seems to be a move either to hybrid type-logical grammars or directly to first-order linear logic: both are simple, conservative extensions which solve the many problems of lambda grammars while staying close to the spirit of lambda grammars.\nThis paper is structured as follows. Section 2 will introduce first-order linear logic and Section 3 will provide some background about the simply typed lambda calculus. These two introductory sections can be skimmed by people familiar with first-order linear logic and the simply typed lambda calculus respectively. Section 4 will introduce hybrid type-logical grammars and in Section 5 we will give a translation of hybrid type-logical grammars into first-order linear logic and prove its correctness. Section 6 will then compare the Lambek calculus and several of its extensions through their translations in first-order linear logic. This comparison points to a number of potential problems for lambda grammars. We will discuss these problems, as well as some potential solutions in Section 7. Finally, the last section will contain some concluding remarks.\n2"
    }, {
      "heading" : "2 First-order Linear Logic",
      "text" : "Linear logic was introduced by Girard (1987) as a logic which restricts the structural rules which apply freely in classical logic. The multiplicative, intuitionistic fragment of first-order linear logic (which in the following, I will call either MILL1 or simply first-order linear logic), can be seen as a resource-conscious version of first-order intuitionistic logic. Linear implication, written A ⊸ B, is a variant of intuitionistic implication A ⇒ B with the additional constraint that the A argument formula is used exactly once. So, looking at linear logic from the context of grammatical analysis, we would assign an intransitive verb the formula np ⊸ s, indicating it is a formula which combines with a single np (noun phrase) to form an s (a sentence).\nLinear logic is a commutative logic. In the context of language modelling, this means our languages are closed under permutations of the input string, which does not make for a good linguistic principle (at least not a good universal one and a principle which is at least debatable even in languages which allow relatively free word order). We need some way to restrict or control commutativity. The Lambek calculus (Lambek 1958) has the simplest such restriction: we drop the structural rule of commutativity altogether. This means linear implication A ⊸ B splits into two implications: A\\B, which looks for an A to its left to form a B, and B/A, which looks for an A to its right to form a B. In the Lambek calculus, we would therefore refine the assignment to intransitive verbs from np ⊸ s to np\\s, indicating the intransitive verb is looking for the subject to its left.\nIn first-order linear logic, we can choose a more versatile solution, namely using first-order variables to encode word order. We assign atomic formulas a pair of string positions: np becomes np(0, 1), meaning it is a noun phrase spanning position 0 (its leftmost position) to 1 (its rightmost position). Using pairs of (integer) variables to represent strings is standard in parsing algorithms. The addition of quantifiers makes things more interesting. For example, we can assign the formula ∀x.n(3, x) ⊸ np(2, x) to a determiner “the” which spans positions 2, 3. This means it is looking for a noun which starts at its right (that is the leftmost position of this noun is the rightmost position of the determiner, 3) but ends at any position x to produce a noun phrase which starts at position 2 (the leftmost position of the determiner) and ends at position x (the rightmost position of the noun). Combined with a noun n(3, 4), this would allow us to instantiate x to 4 and produce np(2, 4). In other words, the formula given to the determiner indicates it is looking for a noun to its right in order to produce a noun phrase, using a form of “concatenation by instantiation of variables” which should be familiar to anyone who has done some logic programming or who has a basic familiarity with parsing in general (Pereira & Shieber 1987, Shieber, Schabes & Pereira 1995). Similarly, we can assign an intransitive verb at position 1,2 the formula ∀y.np(y, 1) ⊸ s(y, 2) to indicate it is looking for a noun phrase to its left to form a sentence, as the Lambek calculus formula np\\s for intransitive verbs does — this correspondence between first-order linear logic and the Lambek calculus is fully general and discussed fully in (Moot &\n3\nPiazza 2001) and briefly in the next section."
    }, {
      "heading" : "2.1 MILL1",
      "text" : "After this informal introduction to first-order linear logic, it is time to be a bit more precise. We will not need function symbols in the current paper, so terms are either variables denoted x, y, z, . . . (a countably infinite number) or constants, for which I will normally use integers 0, 1, . . ., giving an m-word string m + 1 string positions, from 0 to m. The atomic formulas are of the form a(t1, . . . , tm) with ti terms, a a predicate symbol (we only need a finite, typically smal, number of predicate symbols, often only the following four: n for noun, np for noun phrase, s for sentence, pp for predicate phrase) and m its arity. Our language does not contain the identity relation symbol “=”. Given this set of atomic formulas A and the set of variables V , the set of formulas is defined as follows2."
    }, {
      "heading" : "F ::= A | F ⊸ V | ∀V .F",
      "text" : "We treat formulas as syntactically equivalent up to renaming of bound variables, so substituting ∀y.A[x := y] (where A does not contain y before this substitution is made) for ∀x.A inside a formula B will produce an equivalent formula, for example ∀x.a(x) ≡ ∀y.a(y).\nTable 1 shows the natural deduction rules for first-order linear logic. The variable x in the ∀E and ∀I rules is called the eigenvariable of the rule. The ∀I rule has the condition that the variable y which is replaced by the eigenvariable does not occur in undischarged hypotheses of the proof and that x does not occur in A before the substitution is made3. Throughout this paper, we will use the standard convention in first-order (linear) logic (Girard 1991, Bellin & van de Wiele 1995, Troelstra & Schwichtenberg 2000) that every occurrence of a quantifier ∀, ∃ in a sequent uses a distinct variable and in addition that no variable occurs both free and bound in a sequent.\nAs shown in (Moot & Piazza 2001), we can translate Lambek calculus sequents and formulas into first-order linear logic as follows.\nA1, . . . , An ⊢ B =\n‖A1‖ 0,1, . . . ‖An‖ n−1,n ⊢ ‖B‖0,n\n2We need neither the multiplicative conjunction ⊗ nor the existential quantifier ∃ in this paper, though adding them to the logic poses no problems. The natural deduction rules for ∃ and ⊗ are slightly more complicated than those for ∀ and ⊸ but the basic proof net building blocks don’t change, see for example (Girard 1991, Moot & Piazza 2001, Moot 2014).\n3It is sometimes more convenient to use the following ∀I rule\nA\n∀x.A ∀I∗\nwith the condition there are no free occurrence of x in open hypotheses. The rule of Table 1 is more convient in the following section when we use meta-variables, where it becomes “replace all occurrences of a (meta-)variable by x, then quantify over x”.\n4\n‖a‖x,y = a(x, y)\n‖A/B‖x,y = ∀z.‖B‖y,z ⊸ ‖A‖x,z\n‖B\\A‖y,z = ∀x‖B‖x,y ⊸ ‖A‖x,z\nThe integers 0 to n represent the positions of the formulas in the sequent and the translations for complex formulas introduce universally quantified variables. The translation for A/B states that if we have a formula A/B at positions x, y then for any z if we find a formula B at positions y, z (that is, to the immediate right of our A/B formula) then we have an A at positions x, z, starting at the left position of the A/B formula and ending at the right position of the B argument. In other words, a formula A/B is something which combines with a B to its right to form an A, just like its Lambek calculus counterpart.\nUsing this translation, we can see that the first-order linear logic formulas used for the determiner and the intransitive verb in the previous section correspond to the translations of np/n at position 2, 3 and np\\s at position 1, 2 respectively.\nTo give a simple example of a first-order linear logic proof, we shown a derivation of “every student ran”, corresponding to the Lambek calculus sequent.\n(s/(np\\s))/n, n, np\\s ⊢ s\nWe first translate the sequent into first-order linear logic.\n‖(s/(np\\s))/n‖0,1, ‖n‖1,2, ‖np\\s‖2,3 ⊢ ‖s‖0,3\nThen translate the formulas as follows.\n∀y.[n(1, y) ⊸ ∀z.[∀x.[np(x, y) ⊸ s(x, z)] ⊸ s(0, z)]], n(1, 2), ∀v.[np(v, 2) ⊸ s(v, 3) ⊢ s(0, 3)]\nWe can then show that “every student ran” is a grammatical sentence under these formula assignments as follows.\n5\n∀y.[n(1, y) ⊸ ∀z.[∀x.[np(x, y) ⊸ s(x, z)] ⊸ s(0, z)]]\nn(1, 2) ⊸ ∀z.[∀x.[np(x, 2) ⊸ s(x, z)] ⊸ s(0, z)] ∀E n(1, 2)\n∀z.[∀x.[np(x, 2) ⊸ s(x, z)] ⊸ s(0, z)] ⊸ E\n∀x.[np(x, 2) ⊸ s(x, 3)] ⊸ s(0, 3) ∀E\n∀v.[np(v, 2) ⊸ s(v, 3)]\ns(0, 3) ⊸ E\nThe application of the final⊸ E rule is valid, since ∀x.[np(x, 2) ⊸ s(x, 3)] ≡ ∀v.[np(v, 2) ⊸ s(v, 3)].\nDefinition 2.1 (Universal closure) If A is a formula we denote the set of free variables of A by FV(A).\nFor an antecedent Γ = A1, . . . , An, FV(Γ) = FV(A1) ∪ · · · ∪ FV(An). The universal closure of a formula A with FV(A) = {x1, . . . , xn}, denoted Cl(A), is the formula ∀x1 . . .∀xn.A. The universal closure of a formula A modulo antecedent Γ, written ClΓ(A), is defined by universally quantifying over the free variables in A which do not occur in Γ. If FV(A) \\ FV(Γ) = {x1, . . . , xn}, then ClΓ(A) = ∀x1 . . .∀xn.A.\nProposition 2.2 Γ ⊢ A iff Γ ⊢ ClΓ(A).\nProof If the closure modulo Γ prefixes n universal quantifiers to A, we can go from Γ ⊢ A to Γ ⊢ ClΓ(A) by using the ∀I rule n times (the quantified variables added for the closure have been chosen to respect the condition on the rule) and in the opposite direction by using the ∀E rule n times. ✷"
    }, {
      "heading" : "2.2 MILL1 with focusing and unification",
      "text" : "The ∀E rule, as formulated in the previous section, has the disadvantage that it requires us to choose a term t with which to replace x and that making the right choice for t requires some insight into how the resulting formula will be used in the rest of the proof. In the example of the preceding section we need to make two such “educated guesses”: we instantiate y to 2 to allow the elimination rule with minor premiss n(1, 2) and we instantiate z to 3 to produce the desired conclusion s(0, 3).\nThe standard solution to automate this process in first-order logic theorem proving is to change the ∀E rule: instead of directly replacing the quantified variable by the “right” choice, we replace it by a meta-variable (I will use the Prolog-like notation A, B, . . . for these variables, or, when confusion with the notation A and B for arbitrary formulas is possible C, D, E, . . ., V , W , X , . . .). These meta-variables will represent our current knowledge about the term with which we will replace a given quantified variable. The MGU we compute for the endsequent will correspond to the most general instantiations of these variables in the given proof (that is, all other instantiations can be obtained from this final MGU by means of additional substitutions).\n6\nThe ⊸ E rule unifies the B formulas of the argument and minor premiss of the rule (so the two occurrences of B need only be unifiable instead of identical). Remember that the unification of two atomic formulas a(x1, . . . , xm) and b(y1, . . . , yn) is only defined when a = b and m = n and that unification tries to find the most general instantiation of all free variables such that xi = yi (for all 1 ≤ i ≤ n = m) and fails if no such instantiation exists. The presence of an explicit quantifier presents a complication, but only a minor one: bound variables are treated just like constants which, in addition, must respect the variable condition.\nMore precisely, the unification of two formulas is defined as follows.\nunify(a(x1, . . . , xn), a(y1, . . . , yn)) = unify(xi, yi) for all 1 ≤ i ≤ n\nunify(A1 ⊸ B1, A2 ⊸ B2) = unify(A2, A1), unify(B1, B2)\nunify(∀x.A, ∀y.B) = unify(A,B[y := x])\nThe ∀ case assumes there are no free occurrences of x in B before substitution. It is defined in such a way that it is independent of the actual variable names used for the quantifier (as mentioned, we use a different variable for each occurrence of a quantifier) and bound occurrences of xi and yi are treated as constants in the unify(xi, yi) clause, subject to the following condition: if we compute a substitution D := x for a formula A and x is not free for D in A then unification fails. In other words, the substitution cannot introduce new bound variables, so for example ∀y.a(D, y) and ∀z.a(z, z) fail to unify, since D is not free for y in ∀y.a(D, y), and therefore we cannot legally substitute y for D since it would result in an “accidental capture”, creating a new bound occurrence of y.4\nAs second problem with natural deduction proof search is that we can have subproofs like the following.\na(y)\n∀x.a(x) ∀I\na(y) ∀E\n[A]i .... B\nA ⊸ B ⊸ Ii A B ⊸ E\nIn both cases, we introduce a connective and then immediately eliminate it. A natural deduction proof is called normal if is does not contain any subproof of the forms shown above. One of the classic results for natural deduction is normalization which states that we can eliminate such detours (Girard, Lafont & Taylor 1988, Troelstra & Schwichtenberg 2000). In the case of linear logic, removing such detours is even guaranteed to decrease the size of the proof.\nWe use a form of focalized natural deduction (Andreoli 1992, Brock-Nannestad & Schürmann 2010), which is a syntactic variant of natural deduction guaranteed to generate only normal natural deduction proofs. We use two turnstiles,\n4In such cases, substitution succeeds but does nothing and subsequent unification fails, since the formulas are not alphabetic variants after substitution.\n7"
    }, {
      "heading" : "Lexicon",
      "text" : ""
    }, {
      "heading" : "Axiom/Hypothesis",
      "text" : ""
    }, {
      "heading" : "Shift Focus",
      "text" : ""
    }, {
      "heading" : "Logical Rules",
      "text" : "the negative ⊢n and the positive ⊢p (for the reader familiar with focused proofs, Γ ⊢ C ⇓ corresponds to Γ ⊢n C and Γ ⊢ C ⇑ to Γ ⊢p C).\nWe will call a sequent Γ ⊢p C a positive sequent (and C a positive formula) and a sequent Γ ⊢n C a negative sequent (and C a negative formula).\nTable 2 shows the rules of first-order linear logic in this format. For the lexicon rule, we require that the formula A is closed. The formula A of the hypothesis rule can contain free variables.\nFor the ∀I rule, y is either a variable or a meta-variable which has no free occurrences in any undischarged hypothesis.\nFor the ⊸ E rule, s is the most general unifier of 〈Γ, B〉 and 〈∆, B〉. That is, we unify the two occurrences of B in their respective contexts, using unification for complex formulas as defined above. The resulting most general unifier is then applied to the two contexts and to A (replacing, if necessary, any variables\n8\nshared between A and B in the formula A). We can see from the rules that axioms start negative and stay negative as long as they are the major premiss of a ⊸ E rule or the premiss of a ∀E rule. We must switch to positive sequents to use the introduction rules or to use the sequent as the minor premiss of a ⊸ E rule.\nThe “detour” subproofs we have seen above cannot receive a consistent labeling: the formula A ⊸ B is the conclusion of a ⊸ I rule and must therefore be on the right-hand side of a positive sequent, however, it is also the major premiss of a ⊸ E rule and must therefore be on the right-hand side of a negative sequent (it is easily verified there is no way to transform a positive sequent into a negative sequent, however the point is that the original detour receives an inconsistent labeling).\n⊢p a(y)\n⊢p ∀x.a(x) ∀I ⊢n ∀x.a(x) ???\n⊢n a(y) ∀E\n[⊢n A]i.... ⊢p B\n⊢p A ⊸ B ⊸ Ii ⊢n A ⊸ B ??? ⊢p A\n⊢n B ⊸ E\nDefinition 2.3 A principal branch is a sequence of negative sequents which starts at a hypothesis, then follows all elimination rules from (major) premiss to conclusion ending at a focus shift rule (this corresponds to the normal notion of principal branch from e.g. (Girard et al. 1988); a sequence of negative sequents can only pass through the major premiss of a ⊸ E rule and through the single premiss of a ∀E rule).\nA track is a path of negative sequents followed by a focus shift followed by a path of positive sequents. A track ends either in the conclusion of the proof or in the minor premiss of a ⊸ E rule.\nThe main track of a proof is the track which ends in its conclusion (these definitions corresponds to the standard notion of track and main track in normal proofs, see e.g. (Troelstra & Schwichtenberg 2000)).\nThis suggests a relation between focused proofs and normal natural deduction proofs, which is made explicit in the following two propositions.\nProposition 2.4 For every natural deduction proof of Γ ⊢ B, there is a focused natural deduction proof with unification of Γ ⊢p B.\nProof We first transform the natural deduction proof of Γ ⊢ B into a normal natural deduction proof, then proceed by induction on the length of the proof and show that we can create both a proof of Γ ⊢p B and a substitution s. We proceed by induction on the depth of the proof.\nIf d = 1, we have an axiom or hypothesis rule, which we translate as follows.\nA ⊢n A A ⊢p A ±\n9\nIf d > 1 we proceed by case analysis on the last rule. The only case which requires some attention is the ⊸ E case. Given that the proof is normal, we have a normal (sub)proof which ends in a ⊸ E rule. We are therefore on the principal branch of this subproof and we know that a principal branch starts with an axiom/lexicon rule then passes only ∀E rules and ⊸ E rules through their major premiss. Hence, the last rule producing the major premiss in the original proof must either have been an axiom/lexicon rule or an elimination rule for ⊸ or ∀.\nNow induction hypothesis gives us a proof δ1 of Γ ⊢p B ⊸ A and a proof δ2 of ∆ ⊢p B. However, given that the last rule of the proof which produces δ1 was either axiom/lexicon, the ∀E rule or the ⊸ E rule — all of which have negative sequents as their conclusion — the last rule of δ1 must have been the focus shift rule. Removing this focus shift rule produces a valid proof δ1′ of Γ ⊢n B ⊸ A, which we can combine with the proof δ2 of ∆ ⊢p B as follows.\n.... δ1′\nΓ ⊢n B ⊸ A .... δ2 ∆ ⊢p B\nΓ,∆ ⊢n A Γ,∆ ⊢p A ±\nNote that this is again a proof which ends with a focus shift rule. Since the original proof uses the stricter notion of identity (instead of unifiability) for the B formulas, we need not change the substitution we have computed so far and therefore leave Γ, ∆ and A unchanged.\nFor the ∀E rule, induction hypothesis gives us a proof δ of Γ ⊢p ∀x.A, by reasoning similar to the case for ⊸ E, we know the last rule of δ was a focus shift rule, which we can remove, then extend the proof as follows.\n.... δ\nΓ ⊢n ∀x.A\nΓ ⊢n A[x := D] ∀E Γ ⊢p A[x := D] ±\nAdding the substitution D := t (where t is the term used for the in the original ∀E rule) to the unifier.\nThe cases for ∀I and ⊸ I are trivial, since we can extend the proof with the same rule. ✷\nProposition 2.5 For every focused natural deduction proof, there is a natural deduction proof.\nProof If we remove the focus shift rule and replace both ⊢n and ⊢p by ⊢ then we only need to give specific instantiations for the ∀E rules. The most general unifier s computed for the complete proof gives us such values for each (negatively) quantified variable (if wanted, remaining meta-variables can be replaced by free variables). ✷\n10\nThe following is a standard property of normal natural deduction proofs (and therefore of focused natural deduction proofs).\nProposition 2.6 Focused proofs satisfy the subformula property. That is, any formula occurring in a proof of Γ ⊢p B (or Γ ⊢n B) is a subformula either of Γ or of B.\nThe following proposition is easily verified by induction on A and using the correspondence between natural deduction proofs and λ-terms.\nProposition 2.7 We can restrict the focus shift rule to atomic formulas A. When we do so, we only produce long normal form proofs (which correspond to beta normal eta long lambda terms).\nThe proof from the previous section looks as follows in the unification-based version of first-order linear logic, though we use a form with implicit antecedents to economize on horizontal space and to make comparison with the proof of the previous section easier. This proof produces the most general unifier Y = 2, Z = 3, corresponding to the explicit instantiations for y and z at the ∀E rules in the previous proof.\n⊢n ∀y.[n(1, y) ⊸ ∀z.[∀x.[np(x, y) ⊸ s(x, z)] ⊸ s(0, z)]]\n⊢n n(1, Y ) ⊸ ∀z.[∀x.[np(x, Y ) ⊸ s(x, z)] ⊸ s(0, z)] ∀E\n⊢n n(1, 2) ⊢p n(1, 2) ±\n⊢n ∀z.[∀x.[np(x, 2) ⊸ s(x, z)] ⊸ s(0, z)] ⊸ E\n⊢n ∀x.[np(x, 2) ⊸ s(x, Z)] ⊸ s(0, Z) ∀E\n⊢p ∀v.[np(v, 2) ⊸ s(v, 3)] ⊢p ∀v.[np(v, 2) ⊸ s(v, 3)] ±\n⊢n s(0, 3) ⊸ E ⊢p s(0, 3) ±\nRestricting focus shift (±) to atomic formulas, produces the following proof in long normal form. Remark that our hypothesis in this proof is not np(V, 2) but np(U,W ) which unifies with np(V, 2) at the ⊸ E rule immediately below it.\n⊢n ∀y.[n(1, y) ⊸ ∀z.[∀x.[np(x, y) ⊸ s(x, z)] ⊸ s(0, z)]]\n⊢n n(1, Y ) ⊸ ∀z.[∀x.[np(x, Y ) ⊸ s(x, z)] ⊸ s(0, z)] ∀E\n⊢n n(1, 2) ⊢p n(1, 2) ±\n⊢n ∀z.[∀x.[np(x, 2) ⊸ s(x, z)] ⊸ s(0, z)] ⊸ E\n⊢n ∀x.[np(x, 2) ⊸ s(x, Z)] ⊸ s(0, Z) ∀E\n⊢n np(U,W ) Hyp1 ⊢p np(U,W ) ± ⊢n ∀v.np(v, 2) ⊸ s(v, 3) ⊢n np(V, 2) ⊸ s(V, 3) ∀E\n⊢n s(V, 3) ⊸ E ⊢p s(V, 3) ±\n⊢p np(V, 2) ⊸ s(V, 3) ⊸ I1 ⊢p ∀w.[np(w, 2) ⊸ s(w, 3)] ∀I\n⊢n s(0, 3) ⊸ E ⊢p s(0, 3) ±"
    }, {
      "heading" : "2.3 Proof Nets",
      "text" : "Proof nets are an elegant alternative to natural deduction and an important research topic in their own right; for reasons of space we provide only an informal introduction — the reader interested in more detail is referred to (Girard 1995) for an introduction and to (Danos & Regnier 1989, Bellin & van de Wiele 1995) for detailed proofs in the context of linear logic and to (Lamarche & Retoré 1996,\n11\nMoot 2002, Moot & Retoré 2012) for introductions in the context of categorial grammars and the Lambek calculus. Though proof nets shine especially for the ∃ and ⊗ rules (where the natural deduction formulation requires commutative conversions to decide proof equivalence), they are a useful alternative in the ∀ and ⊸ case as well since they provide an easy combinatorial way to do proof search and therefore make arguments about non-derivability of statements and serve to count the number of readings.\nGirard (1991) shows that the proof nets of multiplicative linear logic (Girard 1987, Danos & Regnier 1989) have a simple extension to the first-order case. Essentially, a proof net is a graph labeled with (polarized occurrences of) the (sub)formulas of a sequent Γ ⊢ C, subject to some conditions we will discuss below. Obviously, not all graphs labeled with formulas correspond to derivable statements. However, we can characterize the proof nets among the larger class of proof structures (graphs labeled with formulas which, contrary to proof nets, do not necessarily correspond to proofs) by means of simple graph-theoretic properties.\nThe basic building blocks of proof structures are links, as shown in Figure 1. We will call the formulas displayed below the link their conclusion and the formulas displayed above it their premisses. The axiom link (top left) has no premisses and two conclusions, the cut link has no conclusions and two premisses, the binary logical links have two premisses (A and B) and one conclusion A ⊸ B and the unary logical links have one premiss A and one conclusion ∀x.A. We will call x the eigenvariable of the link and require that all links use distinct variables.\nGiven a statement A1, . . . , An ⊢ C we can unfold the formulas using the logical links of the figure, using the negative links for the Ai and the positive link for C. Since there is only one type of link for each combination of connective/polarity, we unfold our formulas deterministically5, until we end up at the atomic formulas and have produced a “formula forest”, a sequence of formula decomposition trees labeled with some additional information (polarity labels and dashed lines), which is sometimes called a proof frame.\nWe turn this proof frame into a proof structure by connecting atomic formulas of opposite polarity in such a way there is a perfect matching between the positive and negative atoms. This step can already fail, for example if the number of positive and negative occurrences of an atomic formula differ but also because of incompatible atomic formulas like a(0, 1) and a(x, 1), with x the eigenvariable of a ∀+ link. More generally, it can be the case that there is no coherent substitution which allows us to perform a complete matching of the atomic formulas using axiom links. These restrictions on the instantiations of variables are a powerful tool for proof search (Moot 2007, Moot 2014).\nProof structures are essentially graphs where some of the links are drawn with dashed lines; the binary dashed lines are paired, as indicated by the connecting arc. We will call the dashed logical links (∀+ and ⊸+) the positive links\n5For the negative ∀ this is not immediately obvious, since we need to choose a suitable term t. We will discuss this case below but we will essentially use meta-variables and unification just like we did for natural deduction in Section 2.2.\n12\nand the solid logical links (∀− and ⊸−) the negative links. The terms positive and negative links only apply to the logical links; the axiom and cut link are neither positive nor negative. A proof structure containing only negative logical links is just a graph labeled with polarized formulas.\nFigure 2 shows the proof net which corresponds to the natural deduction proof of Section 2.1. To save space, we have noted only the main connective at each link; the full formula can be obtained unambiguously from the context. We have also been free in the way we ordered the premisses of the ⊸ links, which allows us to give a planar presentation of the axiom links, much like Lambek calculus proof nets. However, there is no planarity requirement in the proof net calculus; the first-order variables offer more flexibility than simple planarity. For the ∀− links, we have annotated the substitutions next to the link. If we use a unification-based presentation, as we did for natural deduction in Section 2.2, we can “read off” these substitutions from the most general unifier computed for the axioms (as opposed to natural deduction, the axioms and not the ⊸ E rule, which corresponds to the ⊸− link, are responsible for the unification of variables).\nA proof structure is a proof net if the statement A1, . . . , An ⊢ C is derivable, that is, given the proof of Section 2.1, we know the proof structure of Figure 2 is a proof net. However, this definition is not very useful, since it depends on finding a proof in some other proof system; we would like to use the proof structure\n13\nitself to directly decide whether or not the statement is derivable. However, it is possible to distinguish the proof nets from the other proof structures by simple graph-theoretic properties. To do so, we first introduce some auxiliary notions, which turn the graph-like proof structures into standard graphs. Since axiom, cut and the negative links already produce normal graphs (⊸− corresponds to two edges, all other links to a single edge in the graph), we only need a way to remove the positive links.\nDefinition 2.8 A switching is a choice for each positive link as follows.\n• For each ⊸+ link, we choose one its premisses (A or B).\n• For each ∀+ link, we choose either its premiss A or any of the formulas in the proof structure containing a free occurrence of the eigenvariable of the link.\nA given a switching s, a correction graph is a proof structure where we replace all dashed links by a link from the conclusion of the link to the formula chosen by the switching s.\n14\nTheorem 2.9 (Girard 1991) A proof structure is a proof net iff all its correction graphs are acyclic and connected.\nDefined like this, it would seem that deciding whether or not a proof structure is a proof net is rather complicated: there are potentially many correction graphs — we have two independent possibilities for each ⊸+ link and generally at least two subformulas containing the eigenvariable of each ∀+ link, giving 2n correction graphs for n positive links — and we need verify all of them. Fortunately, there are very efficient alternatives: linear time in the quantifier-free case (Murawski & Ong 2000, Guerrini 1999) and at most squared time, though possibly better, in the case with quantifiers (Moot 2014).\nGoing back to the example shown in Figure 2, we can see that there are two positive links and twelve correction graphs: there are six free occurrences of x — four in atomic formulas and two additional occurrences in the conclusions (⊸+ and ⊸−) which combine these atomic formulas into np(x, 2) ⊸ s(x, 2) — times the two independent possibilities for switching ⊸+ left or right. We can verify that all twelve possibilities produce acyclic, connected graphs. Removing the positive links splits the graph into three connected components: the single node labeled ⊸+ (representing (np(x, 2) ⊸ s(x, 2))+), a component containing the intransitive verb ending at the axioms to s(x, 3)+ and np(x, 2)− and a final component containing the rest of the graph, ending at the conclusion of the ∀+ link (which has been disconnected from its premiss). Now, any switching for the ⊸+ link will connect its isolated conclusion node to the component containing s(x, 3)+ and np(x, 2)− (via one or the other of these nodes), leaving two connected components. Finally, all free occurrences of the variable x occur in this newly created component, therefore any choice for a switching of the ∀+ link will join these disconnected components into a single, connected component. Since each choice connected two disjoint components, we have not generated any cycles.\nWe can also show that this is the only possible proof structure for the given logical statement: there is only one choice for the n formulas, one choice for the np formulas though two choices for the s formulas. However, the alternative proof structure would link s(0, z) to s(x, z) (for some value of z), which fails because x, being the eigenvariable of a ∀+ link, cannot be instantiated to 0.\nAs a second example, let’s show how we can use correction graphs to show underivability. Though it is clear that the switching for the universal quantifier must refer to free occurrences of its eigenvariable somewhere (as do its counterparts in natural deduction and sequent calculus), it is not so easy to find a small example in the ∀,⊸ fragment where this condition is necessary to show underivability, since finding a global instantiation of the variables is already a powerful constraint on proof structures. However, the existential quantifier and the universal quantifier differ only in the labeling of formulas for the links and we need the formula labeling only for determining the free variables.\nA proof structure of the underivable sequent (∀x.a(x)) ⊸ b 0 ∃y.[a(y) ⊸ b] is shown in Figure 3. It is easy to verify this is the unique proof structure corresponding to this sequent. This sequent is used for computing the prenex\n15\nnormal form of a formula in classical logic (replacing ⊸ by ⇒), but it is invalid in intuitionistic logic and linear logic since it depends on the structural rule of right contraction.\nIn order to show the sequent is invalid in linear logic, it suffices to find a switching such that the corresponding correction graph either contains a cycle or is disconnected. Figure 4 shows a correction graph for the proof structure of Figure 3 which is both cyclic and disconnected: the axiom a(x) ⊢ a(x) is not connected to the rest of the structure and the connection between ∀x.a(x) and a(x) ⊸ b produces a cycle, since there is a second path to these two formulas through the axiom b ⊢ b.\nThis concludes our brief introduction to proof nets for first-order linear logic. We refer the reader to Appendix A of (Girard et al. 1988) for discussion about the relation between proof nets and natural deduction.\n16"
    }, {
      "heading" : "3 Basic Properties of the Simply Typed Lambda",
      "text" : "Calculus\nBefore introducing hybrid type-logical grammars, we will first review some basic properties of the simply typed lambda calculus which will prove useful in what follows. This section is not intended as a general introduction to the simply typed lambda calculus: we will assume the reader has at least some basic knowledge such as can be found in Chapter 3 of (Girard et al. 1988) or other textbooks and some knowledge about substitution and most general unifiers. For more detail, and for proofs of the lemmas and propositions of this section, the reader is referred to (Hindley 2008).\nA remark on notation: we will use → exclusively as a type constructor (also when we know we are using it to type a linear lambda term) and ⊸ exclusively as a logical connective.\nDefinition 3.1 A lambda term M is a linear lambda term iff\n1. for every subterm λx.N of M , x has exactly one occurrence in N (in other words, each abstraction binds exactly one variable occurrence),\n2. all free variables of M occur exactly once.\nTable 3 lists the Curry-style typing rules for the linear lambda calculus. For the → E rule, Γ and ∆ cannot share term variables; for the → I rule, Γ cannot contain x (ie. Γ, xα must be a valid context).\nProposition 3.2 For linear lambda terms, we have the following:\n1. When M is a linear lambda term and Γ ⊢ M : α a deduction of M , then the variables occurring in Γ are exactly the free variables of M .\n2. If M , N are linear lambda terms which do not share free variables then (M N) is a linear lambda term.\n3. If M is a linear lambda term with a free occurrence of x then λx.M is a linear lambda term.\n17\n4. If M is a linear lambda term and M ։βη N then N is a linear lambda term.\nLemma 3.3 (Substitution) If Γ, x : α ⊢ M : β, ∆ ⊢ N : α and Γ and ∆ are compatible (ie. there are no conflicting variable assignments and therefore Γ,∆ is a valid context), then Γ,∆ ⊢ M [x := N ] : β.\nThe following two results are rather standard, we can find them in (Hindley 2008) as Lemmas 2C1 and 2C2.\nLemma 3.4 (Subject Reduction) Let M ։βη N , then Γ ⊢ M : α ⇒ Γ ⊢ N : α\nLemma 3.5 (Subject Expansion) Let M ։βη N with M a linear lambda term, then Γ ⊢ N : α ⇒ Γ ⊢ M : α"
    }, {
      "heading" : "3.1 Principal types",
      "text" : "The main notions from Chapter 3 of (Hindley 2008) are the following.\nDefinition 3.6 (Principal type) A principal type of a term M is a type α such that\n1. for some context Γ we have Γ ⊢ M : α\n2. if Γ′ ⊢ M : β, then there is a substitution s such that s(α) = β.\nDefinition 3.7 (Principal pair) A principal pair for a term M is a pair 〈Γ, α〉 such that Γ ⊢ M : α and for all β such that Γ ⊢ M : β there is a substitution s with s(α) = β\nDefinition 3.8 (Principal deduction) A principal deduction for a term M is a derivation δ of a statement Γ ⊢ M : α such that every other derivation with term M is an instance of δ (ie. obtained by globally applying a substitution s to all types in the proof).\nFrom the definitions above, it is clear that if δ is a principal deduction for Γ ⊢ M : α then Γ, α is a principal pair and α a principal type of M .\nIf M contains free variables x1, . . . xn we can compute the principal type α1 → . . . (αn → β) of the closed term ⊢ λx1, . . . xn.M which is the same as the principal type for xα11 , . . . , x αn n ⊢ M β."
    }, {
      "heading" : "3.2 The principal type algorithm",
      "text" : "The principal type algorithm of Hindley (2008) is defined as follows. It is slightly more general and computes principal deductions. It takes as input a lambda term M and outputs either its principal type α or fails in case M is untypable. We closely follow Hindley’s presentation, keeping his numbering but restricting ourselves to linear lambda terms; we omit his correctness proof of the algorithm.\nWe proceed by induction on the construction of M .\n18\nI. If M is a variable, say x, then we take an unused type variable α and return xα ⊢ x : α as principal deduction.\nII. If M is of the form λx.N and x occurs in N then we look at the principal deduction δ ofN by induction hypothesis : if we fail to compute a principal deduction for N then there is no principal deduction for λx.N either. If such a deduction δ does exist, then we can extend it as follows.\n.... δ\nxα,Γ ⊢ N : β\nΓ ⊢ λx.N : α → β ⊸ I\nIII. M is of the form λx.N and x does not occur in N ; this case cannot occur since it violates the condition on linear lambda terms (we must bind exactly one occurrence of x in N), so we fail.\nIV. M is of the form (N P ). If the algorithm fails for either N or P , then M is untypable and we fail. If not, induction hypothesis gives us a principal proof δ1 for Γ ⊢ N : γ\n′ and a principal proof δ2 for ∆ ⊢ P : γ. If necessary, we rename type variables if Γ and ∆ such that Γ and ∆ have no type variables in common. Since M is linear, N and P cannot share term variables.\n(a) If γ′ is of the form α → β then we compute the most general unifier s of 〈Γ, α〉 and 〈∆, γ〉. If this fails the term is untypable; if not we combine the proofs as follows.\n.... s(δ1)\ns(Γ) ⊢ N : s(α) → s(β)\n.... s(δ2)\ns(∆) ⊢ P : s(γ)\ns(Γ), s(∆) ⊢ (N P ) : s(β) → E\n(b) If γ′ is a type variable, then we compute the most general unifier s of 〈Γ, γ′〉 and 〈∆, γ → β〉 (with β a fresh type variable). If this succeeds and the term is typable, we can produce its principal proof as follows.\n.... s(δ1)\ns(Γ) ⊢ N : s(γ) → s(β)\n.... s(δ2)\ns(∆) ⊢ P : s(γ)\ns(Γ), s(∆) ⊢ (N P ) : s(β) → E\nThe main utility of principal types in the current paper is given by the coherence theorem.\nTheorem 3.9 (Coherence) Suppose Γ ⊢ N : α and let α be a principal type of N then ∀P ∀Γ′ ⊆ Γ Γ′ ⊢ P : α =⇒ P ≡βη N\n19\nThe coherence theorem states that a principal type determines a lambda term uniquely (up to βη equivalence). Since we work in a linear system, where weakening is not allowed, we only need the special case Γ′ = Γ. This special case of Theorem 3.9 is the following: if Γ ⊢ N : α with α a principal type of N then for any P such that Γ ⊢ P : α we have that P ≡βη N .\nIn brief, the principal type algorithm allows us to compute the principal type of a given typable lambda term, whereas the coherence theorem allows us to reconstruct a lambda term (up to βη equivalence) from a principal type.\nDefinition 3.10 We say a sequent Γ ⊢ C is balanced if all atomic types occurring in the sequent occur exactly twice.\nThe following lemmas are easy consequences of 1) the Curry-Howard isomorphism between linear lambda terms and Intuitionistic Linear Logic (ILL), which allows us to interpret the linear type constructor “→” as the logical connective “⊸” 2) the correspondence between (normal) natural deduction proofs and (cut-free) proof nets and 3) the fact that renaming the conclusions of the axiom links in a proof net gives another proof net.\nLemma 3.11 If M is a linear lambda term with free variables x1, . . . , xn then the principal type α1 → . . . (α1 → β) of λx1 . . . λxn.M is balanced. Hence the principal type of xα11 , . . . x αn n ⊢ M β is balanced.\nProof Compute the natural deduction proof of M and convert it to a ILL proof net. By subject reduction (Lemma 3.4), normalization/cut elimination keeps the type α invariant. Let P be the cut-free proof net which corresponds to the natural deduction proof of M and which has the same type as M . We obtain a balanced proof net by using a different atomic formula for all axiom links. From this proof net, we can obtain all other types of M by renaming the axiom links (allowing for non-atomic axiom links), hence it is a principal type and it is balanced by construction. ✷\nLemma 3.12 If M is a beta-normal lambda term with free variables x1, . . . , xn and if λx1, . . . , λxn.M has a balanced typing then M is linear.\nProof If λx1, . . . , λxn.M has a balanced typing, then from this typing we can construct a unique cut-free ILL proof net of λx1, . . . , λxn.M . Since it is an ILL proof net, this lambda term must be linear and therefore M as well. ✷"
    }, {
      "heading" : "3.3 Examples",
      "text" : "To illustrate the principal type algorithm, we give two examples in this section. As a first example, we compute the principal proof of C ≡ λf.λx.λy.((f y)x) as follows.\n20\nfγ0 ⊢ f : γ0 yβ ⊢ y : β\nfβ→γ1 , yβ ⊢ (f y) : γ1 → E xα ⊢ x : α\nfβ→α→γ , yβ, xα ⊢ ((f y)x) : γ → E\nfβ→α→γ , xα ⊢ λy.((f y)x) : β → γ → I\nfβ→α→γ ⊢ λx.λy.((f y)x) : α → β → γ → I\n⊢ λf.λx.λy.((f y)x) : (β → α → γ) → α → β → γ → I\nThe substitutions γ0 := β → γ1 (for the topmost → E rule) and γ1 := α → γ (for the bottom → E rule) have been left implicit in the proof.\nAs a second example, the principal proof of l2→1 ⊢ λO.λS.λz.(S (l (O z))) is the following.\nSα2 ⊢ S : α2\nl2→1 ⊢ l : 2 → 1\nOα0 ⊢ O : α0 z β ⊢ z : β\nOβ→α1 , zβ ⊢ (O z) : α1 → E\nl2→1, Oβ→2, zβ ⊢ (l (O z)) : 1 → E\nS1→α, l2→1, Oβ→2, zβ ⊢ (S (l (O z))) : α → E\nS1→α, l2→1, Oβ→2 ⊢ λz.(S (l (O z))) : β → α → I\nl2→1, Oβ→2 ⊢ λS.λz.(S (l (O z))) : (1 → α) → β → α → I\nl2→1 ⊢ λO.λS.λz.(S (l (O z))) : (β → 2) → (1 → α) → β → α → I\nThe substitutions α0 := β → α1, α1 := 2, α2 := 1 → α (of the three → E rules, from top to bottom) have again been left implicit."
    }, {
      "heading" : "4 Hybrid Type-Logical Grammars",
      "text" : "Hybrid type-logical grammars have been introduced in (Kubota & Levine 2012) as an extension of lambda grammars which combines insights from the Lambek calculus into lambda grammars. Depending on authors, lambda grammars (Muskens 2003) are also called abstract categorial grammars (de Groote 2001) or linear grammars (Pollard 2011).\nFormulas of hybrid type-logical grammars are defined as follows, where F2 are the formulas of hybrid type-logical grammars and F1 the formulas of Lambek grammars. A denotes the atomic formulas of the Lambek calculus — we will call these formulas simple atomic formulas, since their denotations are strings — B signifies complex atomic formulas, whose denotations are not simple strings, but string tuples.\nF0 ::= A\nF1 ::= F0 | F1/F1 | F1\\F1\nF2 ::= B | F1 | F2|F2\n21\nAs is clear from the recursive definition of formulas above, hybrid type-logical grammars are a sort of layered or fibred logic. Such logics have been studied before as extensions of the Lambek calculus by replacing the atomic formulas in F0 by feature logic formulas (Bayer & Johnson 1995, Dörre & Manandhar 1995).\nLambek grammars are obtained by not allowing connectives or complex atoms in F2. From hybrid type-logical grammars, we obtain lambda grammars by not allowing connectives in F1. Inversely, we can see hybrid type-logical grammars as lambda grammars where simple atomic formulas have been replaced by Lambek formulas.\nBefore presenting the rules of hybrid type-logical grammars, we’ll introduce some notational conventions: A and B range over arbitrary formulas; C, D and E denote type variables or type constants; n and n − 1 denote type constants corresponding to string positions; α and β denote arbitrary types. Types are written as superscripts to the terms; x, y and z denote term variables; M and N denote arbitrary terms.\nTable 4 shows the rules of Hybrid Type-Logical Grammars. The rules are presented in such a way that they compute principal types in addition to the terms. We obtain the Church-typed version — equivalent to the calculus presented in (Kubota & Levine 2012) — by replacing all type variables and constants by the type constant σ. For the principal types, we use the Curry-typed version, though for readability, we often write the types of subterms as superscripts as well.\nThe subsystem containing only the rules for | is simply lambda grammar. The subsystem containing only the rules for / and \\ is a notational variant of the Lambek calculus.\nFor the Lexicon rule, 〈xn→n−1, α〉 is a principal pair for M or, equivalently, with λx.M a β-normal η-long linear lambda term and (n → n − 1) → α its principal type). For the Axiom/Hypothesis rule, M is the eta-expansion of x : A.\nFor the Lambek calculus elimination rule /E and \\E, s is the most general unifier of 〈Γ;F 〉 and 〈∆;D〉 (this generally just replaces F by D but takes care of the cases where C = D or E = F as well). The concatenation operation of the Lambek calculus corresponds to function composition on terms and to unification of string positions on types (much like we have seen in Section 2).\nFor the Lambek calculus introduction rules /I and \\I, s is the most general unifier of 〈Γ;C → D〉 (resp. 〈Γ;D → C〉) and 〈∅;F → F 〉 (ie. we simply identify C and D and replace x by the identity function on string positions — the empty string).\nIn the |E rule, s is the most general unifier of 〈Γ;β〉 and 〈∆; γ〉. For convenience, we will often tacitly apply the following rule.\nΓ ⊢ Mα : A M =βη N\nΓ ⊢ Nα : A =βη\nThough the above rule is not strictly necessary, we use it to simplify the lambda terms we compute, performing on-the-fly β-normalization (ie. we replace\n22"
    }, {
      "heading" : "Lexicon",
      "text" : "xn→n−1 : A ⊢ Mα : A"
    }, {
      "heading" : "Axiom/Hypothesis",
      "text" : "xα : A ⊢ Mα : A"
    }, {
      "heading" : "Logical rules – Lambek",
      "text" : "Γ ⊢ MF→C : A/B ∆ ⊢ NE→D : B\ns(Γ), s(∆) ⊢ (λzs(E).M (N z))s(E)→s(C) : A /E\nΓ ⊢ MF→C : B ∆ ⊢ NE→D : B\\A\ns(Γ), s(∆) ⊢ (λzs(E).M (N z))s(E)→s(C) : A \\E\nΓ, xD→C : B ⊢ MD→E : A\ns(Γ) ⊢ ((λxs(D)→s(C).M) (λzs(F ).z))s(C)→s(E) : A/B /I\nΓ, xC→D : B ⊢ ME→D : A\ns(Γ) ⊢ ((λxs(C)→s(D).M) (λzs(F ).z))s(E)→s(C) : B\\A \\I\nLogical rules – lambda grammars\nM by its beta-normal, or beta-normal-eta-long, form N). Since we have both subject reduction and subject expansion, M and N are guaranteed to have the same type α.\nApart from the types, the system presented in Table 4 is a notational variant of hybrid type-logical grammars as presented by Kubota and Levine (2012, 2013c). We have replaced strings as basic types by string positions with Church type σ → σ. This is a standard strategy in abstract categorial grammars, akin to the difference lists in Prolog, which allows us to do without an explicit concatenation operation: concatenation is simply treated as function\n23\ncomposition, as can be seen from the term assignments for the /E and \\E rules. The introduction rules /I and \\I are presented somewhat differently than the Kubota and Levine version, who present rules requiring (in our notation) premisses with term assignments M ≡βη λz.N [(x z)] and M ≡βη (xN) respectively. The present formulation has the advantage that it is more robust in the sense that it does not require us to test that M is βη equivalent to the given terms. Though it may appear a bit strange that the /I and \\I rules require the identity of the type variable D between x and M , it is clear that this follows from the intended interpretation, which requires the string variable x to occur at the beginning (resp. end) of the string denoted by M , and this solution seems preferable to interleaving normalization and pattern matching in our rules.\nThe types, at least for the | rules, are exactly those computed using the principal type algorithm of Hindley (2008) discussed in Section 3.1. We will see how the types for the Lambek connectives and the lexicon rule correspond to principal type computations in the next section."
    }, {
      "heading" : "4.1 Justification of the principal types for the new rules",
      "text" : "For /E and \\E, their principal types are justified as follows; s1 is the most general unifier of 〈zG;G〉 and 〈∆;E〉 — since G is a type variable not occurring elsewhere, we can assume without loss of generality that s1 just replaces G with E — and s2 is the most general unifier of 〈s1(∆), s1(zG); s1(D)〉 and 〈Γ;F 〉. The important type unification is of D and F (the unification of E and G affects only a discharged axiom).\nAt the level of the types, the two rules are the same: both correspond to concatenation.\nzG ⊢ z : G ∆ ⊢ N : E → D s1(∆), s1(z G) ⊢ N z : s1(D) → E Γ ⊢ M : F → C\ns2(Γ), s2(s1(∆)), s2(s1(z G)) ⊢ M (N z) : s2(s1(C))\n→ E\ns2(Γ), s2(s1(∆)) ⊢ λz.M (N z) : s2(s1(E)) → s2(s1(C)) → I\nTaking s = s1 ∪ s2, which is possible since Γ, ∆ and z are disjoint, gives us the following proof.\nzG ⊢ z : G ∆ ⊢ N : E → D\ns(∆), s(zG) ⊢ N z : s(D) → E Γ ⊢ M : F → C\ns(Γ), s(∆), s(zG) ⊢ M (N z) : s(C) → E\ns(Γ), s(∆) ⊢ λz.M (N z) : s(E) → s(C) → I\nSince s1 only replaced G by E and G no longer appears in the conclusion of the proof (the corresponding hypothesis z has been withdrawn) we can treat s as the most general unifier of 〈∆;D〉 and 〈Γ;F 〉.\nWe compute the principal type for the /I rule as follows.\n24\nΓ, xD→C ⊢ M : D → E\nΓ ⊢ λx.M : (D → C) → D → E → I ⊢ zF ⊢ z : F λz.z : F → F → I\nΓ ⊢ (λx.M)λz.z : C → E → E\nAnd symmetrically for \\I.\nΓ, xC→D ⊢ M : E → D\nΓ ⊢ λx.M : (C → D) → E → D → I zF ⊢ z : F ⊢ λz.z : F → F → I\nΓ ⊢ (λx.M)λz.z : E → C → E\nFrom the point of view of the principal type computation, we identify the C and D variables, essentially replacing x by the empty string.\nLemma 4.1 The proof rules for Hybrid type-logical grammars of Table 4 compute principal types for the lambda terms corresponding to their proofs.\nProof We essentially use the same algorithm as Hindley (2008), which is somewhat simplified by the restriction to linear lambda terms which are eta-long.\nThe principal types for /E, \\E, /I and \\I rules are justified as shown above. The lexicon rule is justified by the Substitution Lemma (Lemma 3.3): given a principal type α for a lexical entry, we replace a hypothesis of the form α ⊢ α by a hypothesis of the form n → n− 1 ⊢ α, where we know this second sequent has a linear proof. ✷\nCorollary 4.2 Given a principal type derived by the rules of hybrid type-logical grammar shown above, we can compute the corresponding lambda term up to βη equivalence.\nProof Since the principal types computed are balanced by Lemma 3.11, by the Coherence theorem (Theorem 3.9), we can compute the corresponding lambda term up to βη equivalence. An easy way to do so is to construct the proof net corresponding to the principal type (which is unique because of balance) and to compute its lambda term; this lambda term is the unique beta-normal eta-long term corresponding to the principal type. ✷"
    }, {
      "heading" : "4.2 Example",
      "text" : "As an example of how to compute the principal derivation corresponding to a hybrid derivation, we look at the following hybrid derivation.\n[x : np] [y : np\\s]\nλz.(x (y z)) : s \\E\nλxλz.(x (y z)) : s|np |I λPλv.((P e) v) : s|(s|np)\nλv.(e (y v)) : s |E\nλw.(ew) : s/(np\\s) /I\n25\nThe corresponding principal derivation looks as follows (for reasons of vertical space, the lexical entry for e1→0 has not been eta-expanded to λPλw.(P e)w as it should to obtain the given principal type instead of ((1 → 0) → G) → G; though either type will end up being instantiated to the same result type, the eta-expanded principal type ((1 → 0) → H → G) → H → G has the important advantage that it can be obtained without instantiating type variables to complex types; similarly, xA→B and yC→D appear in eta-short form).\nxA→B ⊢ xA→B yC→D ⊢ yC→D zE ⊢ zE yC→D, zC ⊢ (y z)D →E\nxD→B , yC→D, zC ⊢ (x (y z))B →E\nxD→B , yC→D ⊢ (λz.x (y z))C→B →I\nyC→D ⊢ (λx.λz.x (y z))(D→B)→C→B →I e1→0 ⊢ (λP.P e)((1→0)→H→G)→H→G\ne1→0, yC→1 ⊢ (λz.e (y z))C→0 →E\ne1→0 ⊢ (λy.λz.e (y z))(C→1)→C→0 →I\nvJ ⊢ vJ\n⊢ (λv.v)J→J →I\ne1→0 ⊢ (λz.e z)1→0\nThe \\E and the /I rules correspond to three rules each in this principal derivation (the derivation of λz.x (y z) for \\E and the part of the derivation from λz.e (y z) to λz.e z for /I, this last rule satisfies the constraint for the application of the rule, with y appearing at the last position)\nIn principle, the computation of the principal type can fail because of the constants (even though there might be a proof using variables). However, this failure would mean the final term fails to respect the word order of the input string. Principal types using distinct variables for string positions would seem a useful tool for computing all possible word orders for a given set of lexical entries, though."
    }, {
      "heading" : "4.3 Semantics",
      "text" : "One of the attractive points of categorial grammars is that we have a very simple and elegant syntax-semantics interface by means of the Curry-Howard isomorphism between intuitionistic proofs and lambda terms (or, in our case between linear intuitionistic proofs and linear lambda terms). By interpreting the logical connectives for the implications “/”, “\\”, “|” and “⊸” as the type constructor “→”— the formulas as types interpretation— our derivations in the Lambek calculus, in lambda grammars, in hybrid type-logical grammars and in first-order linear logic (where we treat the quantifier as being semantically inert, that is, quantifier rules are “invisible” to the meaning) correspond to λ-terms — the proofs as terms interpretation. Using the Curry-Howard isomorphism, we can obtain semantics in the tradition of Montague simply by giving lexical substitutions in the lexicon, using essentially the rules of Table 3 (though we typically use the Church-style typing) to assign a derivational meaning to a proof.\nThe semantic version of the proof from the previous section looks as follows.\n26\n[x : np] [Q : np\\s]\n(Qx) : s \\E\nλx.(Qx) : s|np |I\nλP.∀z.(P z) : s|(s|np)\n∀z.(Qz) : s |E\nλQ.∀z.(Qz) : s/(np\\s) /I\nThough syntactically, the Lambek elimination rule corresponds to function composition (concatenation), semantically it corresponds to simple application and the introduction rule to abstraction. Given the standard Montegovian semantics for “everyone” as λP.∀z.(P z) (the set of properties P such that all z have this property), the previous proof actually produces an equivalent term as the semantics for s/(np\\s), so the generalized quantifier can function as a Lambek calculus subject quantifier while keeping the same semantics.\nMore detail about the syntax-semantics interface in categorial grammars can be found in (Moortgat 1997, Moot & Retoré 2012)."
    }, {
      "heading" : "5 Equivalence",
      "text" : "For the main result, we only need to show that a hybrid principal type proof corresponds to a MILL1 proof, since we can reconstruct the lambda term from the principal type.\nThe basic idea which makes the correspondence work is that there is a 1-1 mapping between the atomic terms of a predicate in MILL1 and the principal type which is assigned to the corresponding term in a hybrid derivation. So from the term assigned to a hybrid derivation, we compute the principal type using the principal type algorithm (PTA) and this gives us the first-order variables and from the first-order variables of a MILL1 derivation we obtain the principal type and a hybrid lambda term thanks to the coherence theorem, as shown schematically below.\nHybrid lambda term Principal type First-order variables PTA\nCoherence"
    }, {
      "heading" : "5.1 String positions, types and formulas",
      "text" : "We need an auxiliary function f (for flatten) which reduces a complex type to a list of atomic types. Following Kanazawa (2011), we compute this list by first taking the yield of the type tree and then reversing this list, which is convenient for induction since it has f(β → α) = f(α)⌢f(β) ( “⌢” denotes list concatenation, [A] the singleton list containing element A and [A1, . . . , An] the n-element list with ith element Ai).\n27\nDefinition 5.1 Let α be a type, the list f(α) is defined as follows.\nf(A) = [A] when A atomic\nf(β → α) = f(α)⌢f(β)\nFor example, we have the following.\nf((B → 2) → (1 → A) → B → A) = f((1 → A) → B → A)⌢f(B → 2) = f(B → A)⌢f(1 → A)⌢f(B → 2) = [A,B,A, 1, 2, B]\nDefinition 5.2 Let A be a formula in Hybrid Type-Logical Grammar, α its principal type and L = f(α) the flattened list of atomic types obtained from α according to Definition 5.1. The translation of A into first-order linear logic is defined as follows.\n‖p‖[C1,...,Cn] = p(C1, . . . , Cn) ‖(A|B)‖f(β→α) = ‖B‖f(β) ⊸ ‖A‖f(α) ‖(A/B)‖[C,D] = ∀x.‖B‖[D,x] ⊸ ‖A‖[C,x] ‖(B\\A)‖[C,D] = ∀x.‖B‖[x,C] ⊸ ‖A‖[x,D]\nWe can obtain a closed formula by universally quantifying over all variables in the list of arguments replacing all of them with quantified variables using the universal closure operation (Definition 2.1).\n‖A‖Lc = Cl(‖A‖ L)\nProposition 5.3 Let A be a formula in first-order linear logic and H a formula in hybrid type-logical grammar and A ≡ ‖H‖f(α). The free meta-variables of A are exactly the type variables of α (and of f(α)).\nProof Immediate by induction on H using the translation. All new variables introduced during the translation are bound. ✷\nLemma 5.4 Let A1 and A2 be first-order linear logic formulas obtained by the translation function from Hybrid Type-Logical Grammar formulas H1 and H2 with γ1 and γ2 as their respective principal types. In other words, A1 ≡ ‖H1‖f(γ1) and A2 ≡ ‖H2‖f(γ2).\nA1 unifies with A2 with MGU s if and only if H1 ≡ H2 and γ1 unifies with γ2 with this same MGU s.\nProof Suppose A1 and A2 unify with MGU s. We must show that H1 ≡ H2 and that s is an MGU for γ1 and γ2. Showing H1 ≡ H2 is an easy induction (exploiting the fact that A|B does not have a quantifier prefix and therefore cannot unify with a Lambek connective and that A/B and B\\A cannot unify\n28\nwith each other because of the condition preventing accidental capture of variables). Given that H1 and H2 are identical, we know that A1 and A2 differ only in the free variables (the bound variables are equivalent up to renaming) and that the free variables for A1 and A2 are exactly the type variables of γ1 and γ2 (by Proposition 5.3). Therefore any substitution that makes A1 and A2 equal (up to renaming of bound variables) makes γ1 and γ2 equal.\nFor the other direction, suppose that H1 ≡ H2 and that s is the MGU of γ1 and γ2. Since s is a MGU s(γ1) ≡ s(γ2) and therefore given that the translation function uses identical hybrid formulas and identical principal types we have that A1 ≡ A2. ✷\nIt is insightful to compare the translation of (np\\s)/np (with principal type 2 → 1) to that of (s|np)|np with principal type (B → 2) → (1 → A) → B → A. Though the two end results are formulas which are equivalent to each other (after universal closure of the meta-variables), there is a difference in the string position list for the non-atomic subformulas: the Lambek formula only ever has a pair of string positions, whereas the linear formula starts with a full list of string positions which decreases at each step. In other words, for the Lambek formula, we compute the string positions step-by-step whereas the lambda grammar version of the same formula precomputes all string positions then divides them among the subformulas.\n‖(np\\s)/np‖[1,2] = ∀y.‖np‖[2,y] ⊸ ‖np\\s‖[1,y] = ∀y.np(2, y) ⊸ ‖np\\s‖[1,y] = ∀y.np(2, y) ⊸ ∀x.‖np‖[x,1] ⊸ ‖s‖[x,y] = ∀y.np(2, y) ⊸ ∀x.[np(x, 1) ⊸ s(x, y)]\n‖(s|np)|np‖[A,B,A,1,2,B] = ‖np‖[2,B] ⊸ ‖s|np‖[A,B,A,1] = np(2, B) ⊸ ‖s|np‖[A,B,A,1] = np(2, B) ⊸ ‖np‖[A,1] ⊸ ‖s‖[A,B] = np(2, B) ⊸ np(A, 1) ⊸ s(A,B)\n‖(s|np)|np‖ [A,B,A,1,2,B] c = ∀x.∀y.{np(2, B) ⊸ np(A, 1) ⊸ s(A,B)}[A := x,B := y] = ∀x.∀y.[np(2, y) ⊸ np(x, 1) ⊸ s(x, y)]\nRemember that sequents in hybrid type-logical grammar are of the form xα11 : A1, . . . , x αn n : An ⊢ M\nβ : B with M a linear lambda term containing exactly the free variables x1, . . . , xn and that the principal type of λx1, . . . xn.M is balanced and of the form α1 → . . . → an → β. For the translation, we separate lexical axioms from other axioms: lexical axioms correspond to closed formulas, whereas the other axioms typically have free variables. With this in mind, we translate sequents as ‖A1‖f(α1), . . . , ‖An‖f(αn) ⊢ ‖B‖f(β), where the translation ‖.‖c is used for hypotheses which start at a lexicon rule and ‖.‖ for hypothesis which start at the axiom rule. For the right-hand side B, we use\n29\nthe universal closure of all free variables in f(β) minus the free variables on the left hand side of the sequent (the only free variables are those used in the translation of hypothesis rules), this is the universal closure of B modulo Γ of Definition 2.1.\nIn order not to overburden our notation, when the types are understood from the context, we will often abbreviate this translation as ‖Γ‖ ⊢ ‖B‖ (or even as Γ ⊢ ‖B‖, leaving the translation of Γ implicit). As a special case of this translation, the sequent w1→01 : A1, . . . w n→n−1 n : An ⊢ M\nn→0 : B, which is the endsequent corresponding to a sentence in a hybrid type-logical grammars, is translated as ‖A1‖ [0,1] c , . . . , ‖An‖ [n−1,n] c ⊢ ‖B‖ [0,n] c .\nExample: gapping To give an example, the gapping lexical entry for “and” of (Kubota & Levine 2012) looks as follows in our notation.\n((s|tv)|(s|tv))|(s|tv) : λSTV2λSTV1λTVλz.(STV1 TV)(and (STV2 λx.x))\nwhere tv is short for (np\\s)/np. The principal type for this lambda term would be the following (the corresponding formulas have been annotated above for ease of comparison).\n((\ntv ︷ ︸︸ ︷ E → E) → s ︷ ︸︸ ︷ D → 4) → (( tv ︷ ︸︸ ︷ B → A) → s ︷ ︸︸ ︷ 3 → C) → ( tv ︷ ︸︸ ︷ B → A) → s ︷ ︸︸ ︷ D → C\nIf tv were an atomic formula, the first-order linear logic formula would look as shown below on the first line, the complete formula (for the positive translation) is shown just below it.\n(tv(E,E) ⊸ s(4, D)) ⊸ (tv(A,B) ⊸ s(C, 3)) ⊸ tv(A,B) ⊸ s(C,D) ≡\n(∀v.[np(v, E) ⊸ ∀w.[np(E,w) ⊸ s(v, w)]] ⊸ s(4, D)) ⊸\n(∀x′[np(x,A) ⊸ ∀y′[np(B, y′) ⊸ s(x′, y′)]] ⊸ s(C, 3)) ⊸\n∀x.[np(x,A) ⊸ ∀y.[np(B, y) ⊸ s(x, y)]] ⊸ s(C,D)\nThough the formula above looks intimidating (even before universal closure), it is easy to verify that it is equivalent (up to variable names) to the firstorder linear logic formula which corresponds to the analysis of gapping for the Displacement calculus from Section 3.2.6 of (Morrill et al. 2011), using the translation given in (Moot 2014)."
    }, {
      "heading" : "5.2 Proof-theoretic properties of the translation into MILL1",
      "text" : "Before proving the main theorem, stating that for every hybrid proof there is a first-order linear logic proof of its translation, we will spend some time on the structure of normal/focused natural deduction proofs and the consequences of the translation function. Given that in hybrid type-logical grammars, the lambda-grammar connective “|” always outscopes the Lambek connectives “/” and “\\”, proofs using the translated formulas into focused first-order linear logic look schematically as shown in Figure 5.\n30\nThe figure shows the main track of a proof, which starts either with a hypothesis/axioms, then has an elimination part, followed by a focus shift followed by an introduction part ending in the conclusion of the proof — this is just the definition of a main track (Definition 2.3). The definition of formulas guarantees that the elimination part starts with any number of [|E] rules (possibly zero, like all other parts, as indicated by the ∗ superscript in the figure) followed by any combination of [/E] and [\\E] rules. The order is inverse in the introduction part of the track, with [/I] and [\\I] preceding [|I]. For the translation of these rules into first-order linear logic, the quantifiers corresponding to the rules for the lambda grammar connective “|” are obtained by universal closure, so if they are present, it must be as a prefix at the beginning of the proof or as a postfix at the end of the proof — the subpaths labeled (1)-(2) and (6)-(7) in Figure 5 — and the Lambek connectives correspond to a combination of a ∀ and a ⊸ rule upon translation.\nProposition 5.5 a. The main track of a focused proof of a translated hybrid sequent looks as shown in Figure 5.\nb. The subproofs Γj ⊢p ‖F2‖ contain the sequence of proof steps in (1)-(6), that is they do not end with any ∀I rules corresponding to a hybrid connective.\nc. The subproofs in ∆i ⊢p ‖F1‖ contain the sequence of proof steps in (1)-(5), that is they do not end with any lambda grammar introduction rules.\nProof These are immediate consequences of the translation function and the structure of normal proofs.\n31\na. follows from the way the translation function is defined and the standard structure of a main track.\nb. since normal proofs satisfy the subformula property and since hybrid connectives are translated into prenex formulas, we do not produce subformulas of the form (∀x1, . . . , xn.[A ⊸ B]) ⊸ C (for n ≥ 1).\nc. would contradict the definition of hybrid formulas, since it would have a Lambek connective outscope a lambda grammar connective. ✷\nAn immediate corollary of Proposition 5.5 is that ∀I rules corresponding to lambda grammar connectives occur only at the end of the main track of a proof, just like ∀E rules corresponding to lambda grammar connective occur only at the start of any track in which they occur.\nDefinition 5.6 We say a first-order linear logic proof obtained by translating a hybrid proof is in quantifier-reduced form, when all ∀E and ∀I rules obtained by universal closure of lambda-grammar connectives have been removed from the proof.\nMore precisely, the translation is kept as before with the following two exceptions:\n• the Lexicon rule is translated as ‖A‖ f(α) c ⊢ ‖A‖f(α) (with the closure op-\neration applied only to the translation of the antecedent)\n• the endsequent is translated as Γ ⊢ ‖C‖f(β) (without the usual closure modulo Γ).\nProposition 5.7 A sequent Γ ⊢ A produced by the translation function is derivable if and only if its quantifier-reduced form is.\nProof Immediate by Proposition 2.2. ✷ Quantifier-reduced form is a way of “compiling” away the predictable prefixes of ∀E rules (for each of the lexical leaves of the proof) and the equally predictable postfix of ∀I rules introduced by the universal closure operation. This simplifies the structure of the proof, as is clear from Proposition 5.8 below and from Figure 5 — we keep only the subpath (2)-(6). It also simplifies the correctness proof of the translation in the following sections, since we avoid having to start each inductive step by a number of ∀E rules and end it with a number of ∀I rules.\nThe quantifier-reduced form of a proof is sensitive to the way we have obtained the formula: the Lexicon rule for the Lambek formula np\\s has quantifierreduced form ∀x.np(x, 1) ⊸ s(x, 2) ⊢n ∀y.np(y, 1) ⊸ s(y, 2) whereas Lexicon rule for the formula s|np with principal type (1 → A) → (2 → A), which would normally be assigned the same axiom, has quantifier-reduced form ∀x.np(x, 1) ⊸ s(x, 2) ⊢n np(A, 1) ⊸ s(A, 2) (which we can obtain from the previous sequent by a single application of ∀E).\nProposition 5.8 Let δ be first-order linear logic proof in long normal form which has the translation of a hybrid sequent as its conclusion. All occurrences\n32\nof ∀E and ∀I of the quantifier-reduced from δ′ of δ occur respectively in the following contexts.\nΓ ⊢n ∀x.[A ⊸ B]\nΓ ⊢n A ⊸ B ∀E ∆ ⊢p A\nΓ,∆ ⊢n B ⊸ E\nΓ, A ⊢p B\nΓ ⊢p A ⊸ B ⊸ I Γ ⊢p ∀x.[A ⊸ B] ∀I\nProof Given Proposition 5.5 and the fact that δ′ is quantifier-reduced, all quantifiers occur in (sub)formulas of the form ∀x.[A ⊸ B], which corresponds to the translation of a Lambek formula. Given that δ is in long normal form, meaning that the focus shift rule is applied only to atomic formulas, so is its quantifier-reduced form δ′.\nLook at an arbitrary application of the ∀E rule. We show it must be part of a subproof of the form shown above on the left. After application of the ∀E rule, we have the sequent Γ ⊢n A ⊸ B. The focus shift rule cannot apply, since A ⊸ B is not atomic and δ′ is in long normal form. Therefore, by inspection of the available rules ⊸ E is the only rule available and we are in the case shown above.\nThe case for the ∀I rule is similar. To obtain a formula Γ ⊢p A ⊸ B as the premiss of the ∀I rule, focus shift is excluded because we have a complex formula. The only available alternative removes the main connective as shown above on the right. ✷"
    }, {
      "heading" : "5.3 Hybrid proof to MILL1 proof",
      "text" : "After this long setup, everything is in place to prove the main theorem. Thanks to the way we have defined our basic notions and translations, the proof is rather simple. We show that under the given translation, the proof rules of hybrid type-logical grammar are derived rules of MILL1. In the next section, we show the converse: that MILL1 proofs using formulas obtained from the translation correspond to proofs in hybrid type-logical grammars.\nThe proof is actually stronger: we show that proofs in the two systems generate the same semantics. This is easily verified since, as discussed in Section 4.3, the elimination (resp. introduction) rules for /, \\ and | correspond to the elimination (resp. introduction) rule for ⊸. The elimination rules (for /, \\, | and ⊸) correspond to application and the introduction rule correspond to abstraction. The quantifier ∀ is treated as semantically inert.\nLemma 5.9 Let δ be a hybrid proof of Γ ⊢ A, then there is an MILL1 proof δ∗ of its translation ‖Γ‖ ⊢ ‖A‖.\nProof We produce a unfocused proof with unification (that is, we do not distinguish between ⊢p and ⊢n). If desired, we can transform the proof obtained by this lemma into a focused proof by Proposition 2.4). We also produce a proof in quantifier-reduced form.\n33\nSince the lexicon/axioms rules are in beta-normal eta-long form by definition, we know from Lemma 3.22 of Kanazawa (2011) that substitution is only of type variables/atoms for type variables and never of a complex type for a type variable, so the arity of our predicate symbols in first-order linear logic is fixed.\nInduction on the depth d of the proof. If d = 1 we either have an axiom rule or a lexical hypothesis. In both cases, we have a sequent x : A ⊢ Mα : A, with α the principal type of M and with x of type α in the axiom case and of type i → i − 1 (for the ith word) in the lexicon case. We translate the axiom by ‖A‖f(α) ⊢ ‖A‖f(α) (letting the free variables of f(α) become free meta-variables) and the lexical hypothesis by the axiom ‖A‖ f(α) c ⊢ ‖A‖f(α), replacing the meta-variables in f(α) on the left with variables and quantifying over them, making the formula on the left-hand side of the turnstile closed. Since we produce a proof in quantifier-reduced form, we do not perform the closure on the right-hand side of the turnstile (or, if you prefer, we perform the closure but immediately follow it by ∀E rules for all quantifiers introduced by the closure operation).\nIf d > 1, induction hypothesis gives us proofs of the premisses of the rule and we proceed by case analysis on the last rule in the hybrid proof.\n[\\E] By induction hypothesis, we have a proof δ1 of Γ ⊢ ‖B‖[C,D] and a proof δ2 of ∆ ⊢ ‖B\\A‖[F,E]. In addition, we know by induction hypothesis that a MGU s of 〈Γ;D〉 and 〈∆;F 〉 exists. Therefore, we can construct a proof of the conclusion of the /E rule as follows. Since G is fresh, unifying it with C is possible and produces a new substitution s′.\n.... δ1\nΓ ⊢ ‖B‖[C,D]\n.... δ2\n∆ ⊢ ‖B\\A‖[F,E]\n∆ ⊢ ∀x.‖B‖[x,F ] ⊸ ‖A‖[x,E] =def\n∆ ⊢ ‖B‖[G,F ] ⊸ ‖A‖[G,E] ∀E\ns ′(Γ), s′(∆) ⊢ ‖A‖[s\n′(C),s′(E)] ⊸ E\n[/E] Symmetric.\n[|E] By induction hypothesis, we have a proof δ1 of Γ ⊢ ‖A|B‖f(β→α) and a proof δ2 of ∆ ⊢ ‖B‖f(γ). We also know there is an MGU s of 〈Γ;β〉 and 〈∆; γ〉. Therefore, we can combine these two proofs using ⊸ E and (by Lemma 5.4) this same unification, as follows.\n.... δ1\nΓ ⊢ ‖A|B‖f(β→α)\nΓ ⊢ ‖B‖f(β) ⊸ ‖A‖f(α) =def\n.... δ2\n∆ ⊢ ‖B‖f(γ)\ns(Γ), s(∆) ⊢ ‖A‖f(s(α)) ⊸ E\n34\n[\\I] By induction hypothesis, we have a proof δ1 of Γ, ‖B‖[D,C] ⊢ ‖A‖[D,E]. In addition, since all principal types are balanced and the two occurrences of D occur in the translations of B and A respectively, we know there are no occurrences of D in Γ. Hence, after the ⊸ I rule, we satisfy the condition for the ∀I rule and can extend the proof as follows.\n.... δ1\nΓ, ‖B‖[D,C] ⊢ ‖A‖[D,E]\nΓ ⊢ ‖B‖[D,C] ⊸ ‖A‖[D,E] ⊸ I\nΓ ⊢ ∀x.‖B‖[x,C] ⊸ ‖A‖[x,E] ∀I\nΓ ⊢ ‖B\\A‖[C,E] =def\n[/I] Symmetric.\n[|I] Induction hypothesis gives us a proof δ1 of Γ, ‖B‖f(β) ⊢ ‖A‖f(α), which we can extend as follows.\n.... δ1\nΓ, ‖B‖f(β) ⊢ ‖A‖f(α)\nΓ ⊢ ‖B‖f(β) ⊸ ‖A‖f(α) ⊸ I\nΓ ⊢ ‖A|B‖f(β→α) =def\n✷"
    }, {
      "heading" : "5.4 MILL1 proof to hybrid proof",
      "text" : "Lemma 5.10 Let δ be the MILL1 derivation of the translation of a hybrid sequent, that is, of ‖A1‖[0,1], . . . , ‖An‖[n−1,n] ⊢ ‖B‖[0,n]. Then there is a hybrid proof δ∗ of x1→01 : A1, . . . , x n→n−1 n : An ⊢ M\nn→0 : B, where M ≡βη λz.(x1 . . . (xn z)).\nProof The fact that M ≡βη λz.(x1 . . . (xn z)) follows immediately from the balanced occurrences of the type constants 0, . . . , n.\nLet δ be the focused MILL1 derivation of ‖Γ‖ ⊢p ‖A‖, or, the case being, of ‖Γ‖ ⊢n ‖A‖. We assume δ to be in quantifier-reduced form.\nWe proceed by induction on the depth d of the proof. If d = 1, then there are two cases.\nLexicon If the rule was a lexical hypothesis, then it is a proof of ‖Ai‖ [i−1,i]\nfor one of the Ai of the endsequent of the proof. By construction, we can recover the principal type α and (by Coherence) a unique β-normal η-long lambda term M of type α. Therefore, we have a hybrid proof xi−1→ii : Ai ⊢ M α : Ai, with α the principal type by construction.\n35\nAxiom If the rule was an axiom then the formula A does not appear in the endsequent. We again recover the principal type α and the (eta-expanded) lambda term M from the translation function and we return the hybrid proof xα : A ⊢ Mα : A, with M the eta-expansion of x to produce a valid Axiom rule.\nIf d > 1, then we proceed by case analysis of the last rule of the proof.\n[±] Induction hypothesis gives us the proof corresponding to the negative premiss of the rule. We return the same proof.\n[∀E/ ⊸ E] For the combination of a ∀E/ ⊸ E rule, there are two cases to consider, depending on whether the translated formula had / or \\ as main connective. In case it was /, our translation unfolds as shown below. The MGU s unifies D with F (it doesn’t matter here if the ∀E step has been done separately: in that case x is replaced by a fresh metavariable G and the MGU unifies G with E).\n‖Γ‖ ⊢n ‖A/B‖[C,D]\n‖Γ‖ ⊢n ∀x.‖B‖[D,x] ⊸ ‖A‖[C,x] =def\n‖Γ‖ ⊢n ‖B‖[D,E] ⊸ ‖A‖[C,E] ∀E\n‖∆‖ ⊢p ‖B‖[F,E]\ns(‖Γ‖), s(‖∆‖) ⊢n ‖A‖[s(C),s(E)] ⊸ E\nLemma 5.4 guarantees that the two hybrid formulas B are indeed identical and induction hypothesis gives us a proof δ1 of Γ ⊢ MD→C : A/B and a proof δ2 of ∆ ⊢ NE→F : B, which we can combine by the /E rule, using the same substitution s, to produce a proof of Γ,∆ ⊢ As(E)→s(C) as required.\n.... δ1\nΓ ⊢ MD→C : A/B\n.... δ2\n∆ ⊢ NE→F : B\ns(Γ), s(∆) ⊢ (λz.M(N z))s(E)→s(C) : A /E\nAccording to Lemma 4.1, we have also computed the corresponding principal type s(E) → s(C).\nThe case for \\ is symmetric.\n[⊸ E] In a quantifier reduced proof, a solitary ⊸ E (without preceding ∀E producing the major premiss of the rule, which was treated in the previous case) originated from a formula A|B. We are in the following case.\nΓ ⊢n ‖A|B‖f(β→α)\nΓ ⊢n ‖B‖f(β) ⊸ ‖A‖f(α) ≡def ∆ ⊢p ‖B‖f(γ)\ns(Γ), s(∆) ⊢n ‖A‖f(s(α)) ⊸ E\n36\nBy induction hypothesis there is a proof of δ1 of Γ ⊢ A|B (where the term M of A|B has principal type β → α) and a proof δ2 of ∆ ⊢ B (where the term N assigned to B has principal type γ). By Lemma 5.4, the two hybrid B formulas are identical and we can use the MGU s as the most general unifier of γ and β. We can therefore combine these proofs using the |E rule and s as follows.\n.... δ1\nΓ ⊢ Mβ→α : A|B .... δ2 ∆ ⊢ Nγ : B\ns(Γ), s(∆) ⊢ (M N)s(α) : A |E\nProducing principal type s(α) for this derivation.\n[⊸ I/∀I] If it results from a translation with a pair of string formulas, we treat the combination of the ⊸ I and a ∀I rule as a single step. By Proposition 5.8, we can do so without loss of generality. Such a combination can only result from the translation of a positive formula with main connective / or . We treat only /I; the case for \\I is symmetric.\nΓ, ‖B‖[D,E] ⊢p ‖A‖[C,E]\nΓ ⊢p ‖B‖[D,E] ⊸ ‖A‖[C,E] ⊸ I\nΓ ⊢p ∀x.‖B‖[D,x] ⊸ ‖A‖[C,x] ∀I\nΓ ⊢p ‖A/B‖[C,D] =def\nWe can simply extend the proof δ1 from the induction hypothesis as follows.\n.... δ1\nΓ, xE→D : B ⊢ ME→C : A\nΓ ⊢ ((λx.M)(λz.z))D→C : A/B /I\n[⊸ I] Finally, the case where the ⊸ I is not followed by a ∀I corresponds to the |I rule. We are in the following situation.\nΓ, ‖B‖f(β) ⊢p ‖A‖f(α)\nΓ ⊢p ‖B‖f(β) ⊸ ‖A‖f(α) ⊸ I\nΓ ⊢p ‖B|A‖f(β→α) =def\nWe can simply extend the proof δ1 of Γ, x β : B ⊢ Mα : A given by the induction hypothesis as follows.\n.... δ1\nΓ, xβ : B ⊢ Mα : A\nΓ ⊢ (λx.M)β→α : A|B |I\n37\n✷"
    }, {
      "heading" : "5.5 Main Theorem",
      "text" : "Theorem 5.11 Derivability of hybrid type-logical grammars and their translation into first-order linear logic coincides. Moreover, proofs in the two systems produce the same semantic lambda terms.\nProof Immediate from Lemma 5.9 and Lemma 5.10 and the observation that /E, \\E and |E, like ⊸ E to which they correspond by translation, are all translated as application on the meaning level and similarly for the different introduction rules and abstraction. ✷\nThanks to Theorem 5.11, we can use the well-understood proof theory of first-order linear logic for parsing/theorem proving hybrid type-logical grammars. Besides (focused) natural deduction and proof nets, discussed in Section 2, the work on sequent proof search of Lincoln & Shankar (1994), which includes a treatment of the additives, can also directly be applied. These proof systems all have their strengths and inconveniences, but, since they are all equivalent we can choose the most appropriate tool for the job. For example, focused natural deduction and proof nets simplify the work of enumerating readings for a given statement, and, as shown in Figure 4, proof nets provide an easy way to show underivability of a statement. In addition, the main theorem has the following immediate consequence.\nCorollary 5.12 Hybrid type-logical grammars are NP-complete\nProof Hardness follows from the fact that hybrid type-logical grammars contain the Lambek calculus (the implicational fragment of the Lambek calculus was shown to be NP-complete by Savateev (2009)) — or alternatively from the fact that they contain lexicalized abstract categorial grammars (de Groote 2001). Since first-order linear logic is NP-complete, by Lemma 5.9 and the fact that the translation is linear in the size of the formulas, hybrid type-logical grammars are in NP. ✷\nTo compare hybrid type-logical grammars with lambda grammars, we first define an interesting subclass of hybrid type-logical grammars which we will show to be equivalent to lambda grammars.\nDefinition 5.13 A hybrid proof is strictly separated iff for every /I and \\I rule, the subproof leading to the premiss of this introduction rule consists only of Lambek elimination rules and premisses A ⊢ A with A a Lambek formula (ie. a member of F1, containing only /, \\ and simple atomic formulas).\nWe can enforce strict separation directly in the proof theory by splitting the ⊢ symbol into ⊢L and ⊢λ, subscripting by ⊢L the premisses and conclusions of the /E, \\E, /I, \\I and axiom/hypothesis for Lambek formulas as ⊢L, subscripting\n38\nby ⊢λ the |E, |I and axiom/hypothesis for formulas not in F1 and adding the inclusion rule.\nΓ ⊢L ME→D : B Γ ⊢λ ME→D : B L, λ\nNot all proofs in hybrid type-logical grammars are strictly separated, as shown by the example in Section 4.2 on page 25, where the final /I rule is preceded by both |E and |I.\nLemma 5.14 Strictly separated hybrid type-logical grammars generate the same string languages and the same string-meaning relations as lambda grammars.\nProof (sketch) The main idea from (Buszkowski 1996), who uses a variant of the proof from (Pentus 1995, Pentus 1997), is that we can replace Lambek calculus formulas by sets of atomic formulas (CFG nonterminals) which behave combinatorially like AB formulas — the CFG nonterminals are essentially the names for AB formulas — in such a way that these sets generate the same lambda term semantics. Here, we do the same for all Lambek sub-formulas of a given hybrid type-logical grammar.\nBy the definition of strict separation, we know that all Lambek rules occur in subproofs where these rules are not intermingled with the lambda grammar rules. Hence, Buszkowski’s construction translates these proofs of Γ ⊢L B into proofs of Γ′ ⊢L B where only the /E and \\E rules are used. Then, by treating all Lambek formulas as CFG nonterminals and all instantiations of the /E and \\E rules in the grammar as CFG rules. That is, the instantiation of the the \\E rule for specific formulas A and B\nA A\\B\nB \\E\nbecomes a non-logical rule\nD E C\n(or, if we prefer to write it as a CFG rule: D,E −→ C), where C is the non-terminal corresponding to formula B, D corresponds to formula A and E corresponds to the formula A\\B.\n✷\nLemma 5.15 Parsing lambda grammars which are the translation of strictly separated hybrid type-logical grammars is NP-complete.\nProof The construction of Lemma 5.14 generates, by means of the Buszkowski (1996) proof, many non-logical grammar rules. Given that such a system may not be decidable, we need to be careful. However, by the construction of (Buszkowski 1996), all non-lexicalized rules are of the form D,E −→ F with D,\n39\nE and F atomic formulas. Moreover, these atomic formulas correspond to AB formulas, such that either D = A/B, E = B and F = A or D = B, F = B\\A and F = A (for some Lambek formulas A and B). Therefore, we can start our proof by computing the closure of these AB subproofs in O(n3), then continue the normal lambda grammar proof, which is NP-complete. ✷\nIt should be obvious from the proof sketch of Lemmas 5.14 and 5.15 that though strictly separated hybrid type-logical grammars generate the same string languages and string-meaning pairs as lambda grammars, hybrid type-logical grammars allow a much more compact specification of such grammars since we avoid a brute-force explosion of the size of the lexicon and of the number of lexical entries per word. Though I don’t believe that the NP-complete problems we encounter in computational linguistics are necessarily intractable — Matsuzaki, Miyao & Tsujii (2007) show that some NP-complete problems in computational linguistics can be solved much more efficiently than O(n6) problems — having an exponential explosion of grammar size followed by an NP-complete problem is profoundly worrying for those interested in actually parsing the formalism.\nIt is unclear whether we can generalize the proof of Lemma 5.14 to dispense with the strict separation requirement on hybrid grammars. Allowing interleaving of the Lambek grammar and lambda grammar rules seems to require a generalization of the results of (Buszkowski 1996) to the hybrid type-logical grammar case and, unless we change the proof of the theorem considerably, this would require a type of interpolation proof for hybrid type-logical grammars, which, as we will see in Section 6.1, seems problematic for the lambda grammar part of the system. For example, looking back to the proof in Section 4.2, it is unclear how to replace the final /I rule by the elimination rule for either / or \\, besides adding s/(np\\s) directly as an additional lexical entry for the quantifier.\nAlso, though it is certainly a desirable property of the hybrid system to derive s|(s|np) ⊢ s/(np\\s) (for the given lexical lambda term), since it relates the generalized quantifier formulas to one of its standard Lambek calculus formulas, it is unclear if we actually need this type of derivation to give a natural account of the linguistic data. So the following question remains open: are there any examples of hybrid type-logical grammar analyses where there is no corresponding lambda grammar analysis? Having to resort to lexical duplication is already a problem, both from a conceptual point of view and from the point of view of parsing, but are there cases where even this doesn’t suffice?\nThough we will leave this question unresolved, we investigate the descriptive inadequacy of lambda grammars in Section 7."
    }, {
      "heading" : "6 Comparison",
      "text" : "The proof nets discussed in Section 2.3 provide an insightful way to compare the different calculi discussed in this article in terms of their basic “building blocks”, seen from the point of view of first-order linear logic.\nWe need to be careful, since this comparison only gives necessary conditions to be in a certain fragment of first-order linear logic, and as such, we can use it\n40\nonly as a diagnostic for showing that possibilities are absent from a logic. We can directly use the different translation functions to give sufficient conditions.\nThe conditions on the variables in the different fragments are also absent from the visual representation. Nevertheless, we will see that this comparison is insightful."
    }, {
      "heading" : "6.1 A visual comparison of the different calculi",
      "text" : "Figure 6 shows the Lambek calculus connectives as links for first-order linear logic proof nets. Curry’s (1961) criticism of the Lambek calculus connectives, seen from the current perspective, is that they combine subcategorization information (functor-argument structure) and string operations. Though from a modern proof-theoretical point of view (Andreoli 1992) it is perfectly valid to combine multiple positive and multiple negative rules into a single rule, separating the two gives more freedom (that is, it allows us to express more relations between the string positions and go beyond simple concatenation — the prefix and postfix of the Lambek calculus).\nAs shown in Figure 7, the first-order linear logic solution decomposes the\n41\nLambek connectives into separate subcategorization and string position components. In a sense, this decomposition answers Curry’s critique in a very simple way.\nCurry’s own solution is different and causes a loss of symmetry: as Figure 8 makes clear, the positive universal link is missing! This loss of symmetry is easy to miss in a unification-based presentation of the logic where, in addition, the quantifiers occur only as an implicit prefix of the formula. For a logician/proof theorist, this is worrying since many classical results and desirable properties of the system (restriction to atomic axioms, cut elimination, interpolation6) depend on this symmetry. However, it is also the cause of empirical inadequacy: positive A/B and B\\A can no longer be represented, hence no satisfactory treatment of adverbs, coordination, gapping etc.; we will elaborate this point in detail in Section 7.\nAnother way to look at this is that lambda grammars require all formulas to be expressed in prenex normal form — something we exploit in the translation function. However, since we are using linear logic, not all formulas have a prenex normal form. The following are all underivable (assuming no occurrences of x in B). Refer back to Figure 4 to see why the first statement is underivable.\n(∀x.A) ⊸ B 0 ∃x(A ⊸ B)\n∃x(A ⊸ B) 0 (∀x.A) ⊸ B\nB ⊸ ∃x.A 0 ∃x.(B ⊸ A)\n∃x.(B ⊸ A) 0 B ⊸ ∃x.A\nThe hybrid solution to this problem is shown in Figure 9: reintroduce the positive Lambek connectives directly. There are now two ways of coding the negative Lambek connectives. The resulting system is also greater than the sum\n6Interpolation, proved first for the Lambek calculus in (Roorda 1991) is a key component of the context-freeness proof for the Lambek calculus of Pentus (1997) and is likely to play a similar role in proofs about the generative capacity of these alternative and extended systems.\n42\nof its parts, since gapping, which has a satisfactory neither in Lambek grammars nor in lambda grammars, can be elegantly treated in hybrid categorial grammar (Kubota & Levine 2012, Kubota & Levine 2013c).\nSymmetry is still lost7, but empirically the system seems comparable to the Displacement calculus (Morrill et al. 2011): the Displacement calculus has the full symmetry absent from hybrid type-logical grammars. In spite of this, as we have seen at the end of Section 5.1, in many cases, the analyses proposed for the two formalisms basically agree, as is made especially clear by their translation into MILL1.\nThe differences between the two systems seems to be that hybrid type-logical grammars can, like lambda grammars, generate non-well-nested string languages and that Displacement grammars (seen from the point of view of hybrid typelogical grammars) allow the Lambek connectives to outscope the discontinuous connectives. Further analysis is necessary to decide which of these two systems has the better empirical coverage.\nD grammars (Morrill et al. 2011) have a different perspective, which is shown in Figure 10. Functor argument structure and string positions are still joined, but a greater number of combinations are possible (from 0 to n quantifiers, for a small value of n determined by the grammar). Lambek grammars are now the restriction to a single quantifier for each binary connective.\nD grammars enriched with bridge, left projection and right projection, shown in Figure 11, permit combinations of string position/subcategorization which are not of the same polarity. These uses are rather restricted compared to the visually similar quantifier link of first-order linear logic: essentially, they enable us to require that a pair of positions spans the empty string.\nSumming up, first-order linear logic decomposes the connectives of different\n7Neither full logical symmetry nor having the Lambek calculus as a subsystems is of course necessary to have an empirically valid formal system, as shown, for example by CCG (Steedman 2001). However it calls for further investigation as to what exactly is absent from the system and if this absence is important from a descriptive point of view. For lambda grammars, we will do this in detail in Section 7.\n43\ngrammatical frameworks — the Lambek calculus, lambda grammars, Hybrid Type-Logical Grammars and the Displacement calculus — in a natural way into its four types of links. This visual comparison both highlights the differences between this calculi and opens the way for a more detailed comparison of the descriptive limitations of one calculus compared to another.\nGiven that it is a decomposition of connectives, the MILL1 translation is slightly bigger in terms of the total number of connectives in the lexical entries. However, the basic operation are simple and well-understood and the first-order variables actually function as powerful constraints during proof search. Thanks to the embedding results of this paper and of (Moot 2014), we can import the large range of linguistic phenomena treated by Displacement grammars and Hybrid Type-Logical Grammar directly into MILL1.\nFrom the point of view of first-order linear logic, the connectives of the other calculi are synthetic connectives, combined connectives of the same polarity. We can mix and match these synthetic connectives as we see fit. We can also exploit the symmetry of first-order linear logic and use lambda grammar lexical entries as arguments, restoring the symmetry of lambda grammars (and of Hybrid\n44\nType-Logical Grammars). In addition, we can add the product ⊗ and quantifier ∃ to our calculus essentially for free. Moreover, as discussed in (Moot & Piazza 2001, Moot 2014) we can use the quantifiers of first-order linear logic to give an account of agreement and island constraints as well. So we can improve upon Displacement grammar analyses by adding agreement and island constraints and improve upon Hybrid Type-Logical Grammar analyses by adding symmetry, agreement and island constraints, all with the same logical primitives."
    }, {
      "heading" : "7 Descriptive Inadequacy of Lambda Grammars",
      "text" : "As already alluded to in Section 6.1, the asymmetry of lambda grammars is the cause of descriptive inadequacy. Researcher in lambda grammars have been aware of problems with coordination at least since Muskens (2001), who briefly mentions an apparent incompatibility between lambda grammars and the categorial grammar treatment of coordination, but the problem can be traced back to (Curry 1961) where the analysis of the coordination “both . . . and . . . ” in §5-6 is problematic. Kubota & Levine (2013a, 2013c) show how catastrophic the predictions of lambda grammars are; we will repeat their observations below while adding several additional troublesome cases. This problem has been little noted and little discussed8. Indeed, one can find several claims in the literature which deny there is a problem: Muskens claims elsewhere (Muskens 2003) that “Since word order is now completely encoded in the phrase structure term, there is no longer any need for a directionality of the calculus” and that “The availability of syntactic λ-terms reins in the overgeneration of the traditional undirected calculi.”. However, as we will show below, using lambda terms to limit the overgeneration of undirected calculi is only partially successful and it is exactly for this reason that a satisfactory treatment of coordination has remained elusive. Worse, the problem of overgeneration is not limited to coordination, but a problem with any higher-order type of the Lambek calculus. The standard higher-order lambda grammar treatments for generalized quantifiers and for non-peripheral are the only cases we know of where lambda grammars make the right predictions. But even here, the lambda grammar analysis does not generalize: generalized quantifiers can be see as instances of Moortgat’s (1996a) q(A,B,C) operator, and the lambda grammar treatment only works when B is atomic and therefore for quantifiers, of type q(np, s, s), but not for reflexives, of type q(np, np\\s, np\\s). For non-peripheral extraction, the lambda grammar analysis again presupposes the extracted element is an atomic formula and therefore the treatment does not generalize to gapping (for more on gapping see Section 7.2).\nTo give an idea of how widespread and serious the problems are, the following is a non-exhaustive list of problems for lambda grammars.\n(1) John deliberately hit Mary. (adverbs)\n8At least in the lambda grammar and abstract categorial grammar literature, the problem is discussed in the context of linear grammar in (Worth 2014).\n45\n(2) John bought a sandwich and ran to the train. (VP coordination)\n(3) John caught and ate a fish. (TV coordination)\n(4) John likes both black and gray t-shirts. (adjective coordination, after Curry, 1961)\n(5) John loves but Mary hates Noam. (right-node raising)\n(6) John bought himself a present. (reflexives)\n(7) John studies logic and Charles, phonetics. (gapping)\n(8) John left before Mary did. (ellipsis)\n(9) John ate more donuts than Mary bought bagels. (comparative subdeletion)\nThese problems range from the mundane to the more involved, but the important point is that, taken together, these problems occur very frequently and that all cases listed above have a simple and elegant treatment in the Displacement calculus (Morrill et al. 2011), in Hybrid type-logical grammars (Kubota & Levine 2012, Kubota & Levine 2013b) and in multimodal type-logical grammars (Hendriks 1995, Kurtonina & Moortgat 1997). Sentence (1) to (5) are simply and correctly handled by Lambek grammars and Sentence (1) to (4) even by AB grammars.\nLet me be precise about what I mean by descriptive inadequacy in this context, since some authors use the term with a slightly different meaning. A theory suffers from descriptive inadequacy if it fails to capture linguistic generalizations and instead has to resort to enumerating the linguistic data. In a lexicalized formalism like categorial grammars, this means we want to avoid multiplying the number of lexical entries for the words in our grammar as much as possible9. So in the context of the examples above, we would like Sentence (1) to use the same lexical entries as the sentence “John hit Mary”, with the lexical assignment to “deliberately” being to only addition and we would like Sentence (3) to use the same lexical entries as the sentences “John caught a fish” and “John ate a fish”, with the lexical assignment to “and” being the only difference. When I say that lambda grammars suffer from descriptive inadequacy, this does not mean that they are fundamentally unable to handle Sentences (1) to (9), since Lemma 5.14 guarantees that they can (given that the phenomena listed above all have strictly separated hybrid proofs). I mean that they cannot treat the sentences above without introducing otherwise unmotivated additional lexical entries — in fact, not without an exponential blowup of the size of the lexicon, as is clear from Lemma 5.14.\nIn Section 7.3 we will discuss the consequences of these problems in detail, as\n9Maybe a more reasonable measure would prefer the sum of the size for all entries assigned to a word to be as small as possible, since a single entry A1 ⊕ . . . ⊕ An is not really simpler that n distinct entries A1, . . . , An.\nIt should also be noted as the size of our grammar increases (in terms of the number of words and constructions it is able to handle), so does the size of our lexicon. So this is a relative measure rather than an absolute one.\n46\nwell as some possible modifications to lambda grammars which may solve these problems, chiefly among those are extensions to hybrid type-logical grammar and to first-order linear logic."
    }, {
      "heading" : "7.1 Inhabitation machines",
      "text" : "To show the main results, we need some additional notions of the typed lambda calculus. An inhabitation machine (see (Barendregt, Dekkers & Statman 2013)) is a type of grammar which, given a type, enumerates all possible terms of this type. Their use for categorial grammars has been pioneered by van Benthem (1995).\nFrom page 33 of (Barendregt et al. 2013), the following two-level grammar (defined on type-context pairs) enumerates all closed inhabitants in beta-normal eta-long form of a given type.\nΓ is a context, Γ, xα denotes Γ∪{xα} (where x is distinct from the terms in Γ, so the result is again a valid context), A is an atomic type, α, β are arbitrary types and ~α → β is short for α1 → . . . → αn → β.\nL(A; Γ) =⇒ xL(α1; Γ) . . . L(αn; Γ) if x : ~α → A ∈ Γ L(α → β; Γ) =⇒ λxα.L(β; Γ, xα)\nThe lambda grammar case is considerably more restricted: the lexical lambda terms must be linear and contain, for a given word w with corresponding variable ws, a single occurrence of ws. That is, we start with Γ = {wσ→σs } and for the application rule, we partition Γ− {x~α→A} into jointly exhaustive, pairwise disjoint subsets and divide these over the different subterms. In addition, we want our lexical term to produce the correct word order and to be compatible with the syntactic lambda grammar derivation."
    }, {
      "heading" : "7.2 Problems for lambda grammars",
      "text" : "In this next section, we will show several problematic cases for lambda grammars, using inhabitation machines to exhaust all possible solutions and find all of them inadequate."
    }, {
      "heading" : "Adverbs",
      "text" : "As a first problem for lambda grammars, the Lambek calculus formula of an adverb such as “deliberately”, as it occurs in a sentence like “Eduardo deliberately fell”, is (np\\s)/(np\\s) — it modifies a verb having taken all arguments except its subject and this verb phrase is on the immediate right of the adverb. If we translate this formula to a first-order formula and move (where possible) the quantifiers to the prefix and eliminate them, we obtain the formula (∀c.np(c, 2) ⊸ s(c,D)) ⊸ np(E, 1) ⊸ s(E,D) but we cannot use the principal type ((2 → c) → D → c) → (1 → E) → D → E (with c a fresh type constant) since it is uninhabited.\n47\nThe lambda grammar syntactic type (s|np)|(s|np) translates to the prosodic type ((σ → σ) → σ → σ) → (σ → σ) → σ → σ and produces the inhabitation machine shown in Figure 12. We use the variable d (of type σ → σ) to stand for the occurrence of the string “deliberately”. We can see that the VP node in the figure requires first an argument of type σ → σ (the downward arrow) then an argument of type σ (the upward arrow) to produce a term of type σ. Valid linear paths through the machine must pass each term label exactly once, and must pass the λy-label (on the curved arrow upwards to σ) before the y variable.\nFigure 13 spits the σ node in two, making the scope of the y variable clearer. The word order of the sentence constrains the paths we can take. We must take an NP arc before we take a d arc, since “deliberately” occurs after the subjet noun phrase. So from the top σ node, we can only take three possible paths, as shown below. For comparison, the uninhabited type corresponding most closely to the first-order formula is shown as item 4. We can see that the three other types are obtained by replacing the c constant by a C variable and exchanging one of the occurrences of C with another atomic type in such a way that the resulting type is inhabited.\nλVPλNPλz.NP (d ((VP λy.y) z)) :(1)\n((C → C) → D → 2) → (1 → E) → D → E\nλVPλNPλz.NP ((VP λy.d y) z) :(2)\n((2 → 1) → D → C) → (C → E) → D → E\nλVPλNPλz.((VP λy.NP (d y)) z) :(3)\n((2 → C) → D → E) → (1 → C) → D → E\nUninhabited:(4)\n((2 → c) → D → c) → (1 → E) → D → E\n48\nWe investigate the three possibilities in turn. Lambda term 1 comes closest to the first-order linear logic formula, but it is a lambda term modeled after those used for extraction and, as such, it takes a sentence missing a noun phrase anywhere as its argument, instead of a verb phrase. Therefore, it incorrectly predicts that the three following sentences are all grammatical.\n(10) John deliberately Mary hit.\n(11) John deliberately Mary insinuates likes Susan.\n(12) John deliberately Mary hit the sister of.\nPredicting that sentence (10) means “It was deliberate on the part of John that Mary hit him”, with sentence (11) meaning approximately “John made Mary insinuate that he likes Susan” and sentence (12) meaning something like “Mary hit the sister of John and this was deliberate on the part of John”. It seems very difficult to block this example without also blocking the noun “boy which Mary likes the sister of” (not super-natural, but we want to allow these kinds of extractions which are essentially indistinguishable from the current formula).\nLambda term 2 shifts from the extraction-like lambda term and its corresponding overgeneration to a lambda term similar to those used for in situ\n49\nbinding/quantifying in10, where we require a sentence missing a noun phrase at the position of “deliberately” as argument. Though this analysis again allows us to derive the correct word order, it also makes the dubious claim that there is an np constituent at the position of the adverb. In addition, it overgenerates as follows.\n(13) Mary John hit deliberately.\n(14) Mary the friend of deliberately left.\n(15) Mary John gave the friend of deliberately a book.\nThough it is possible to argue that sentence (13) is a sort of topicalization (with stress on Mary), it is problematic that this topicalization is triggered by the adverb, since topicalization is independent of the presence or absence of adverbs. Moreover, we generate the semantics “It was deliberate on the part of Mary that John hit her” for sentence (13). We generate the semantics “It was deliberate on the part of Mary that her friend left” for sentence (14) and similarly “Mary incited John to give her friend a book” for sentence (15).\nFinally, lambda term 3 selects for a sentence missing a noun phrase with the only condition that this noun phrase occurs directly before the adverb. Here, we make the odd claim that the noun phrase and the adverb together span the position of an np: that is, it claims that an adverb is a post-modifier of an np. In addition, it is again an in situ binding/quantifying in analysis, but this time with the complex string “np deliberately” (where lambda term 2 used an in situ binding analysis of just the word “deliberately”).\n(16) John hit Mary deliberately.\n(17) The friend of Mary deliberately left.\n(18) The friend of Mary deliberately who lives in Paris left.\nThough sentences (16) and (17) are syntactically correct, the problem is that we generate the semantics “It was deliberate on the part of Mary that John hit her” for sentence (16) and a reading “It was deliberate on the part of Mary that her friend left” for sentence (17) and (18).\nIn sum, we cannot capture the essence of the Lambek calculus formula (np\\s)/(np\\s) in lambda grammars. Other adverb formulas — (np\\s)\\(np\\s) (an adverb occurring after the verb phrase) and (n/n)/(n/n) (for adverbs such as “very”), etc. — suffer from the same problem. The best approximations that we can obtain all suffer from overgeneration because non-commutativity is insufficiently enforced.\nThere is, of course, a solution which replaces the complex np\\s argument by a new atomic formula, say vp and then, for all lexical items of the form ((np\\s)/An) . . . /A1, adds an additional formula (vp/An) . . . /A1. This would essentially double the number of lexical formulas for verbs, adverbs and prepo-\n10As we have seen, a generalized quantifier like “everyone” is assigned the lambda term λP.P (e) with e being the string constant corresponding to the word “everyone”.\n50\nsitions — syntactic categories which already have a high number of lexical formulas — for just a single type of problematic example... More such examples will follow.\nWe will discuss this potential solution in a bit more detail in Section 7.3, but it should already be clear that this is not a particularly attractive option, since it is a prototypical example of descriptive inadequacy, the reasons for doubling the lexicon are purely theory-internal: no other categorial grammar, not even AB grammars, have this type of overgeneration for the simple cases we’ve shown."
    }, {
      "heading" : "Coordination",
      "text" : "As noted by Kubota & Levine (2013a, 2013c), we can play a similar game for “John caught and ate a fish”, which looks as shown in Figure 14; for the sake of space, we do not show the prefix λTV2.λTV1.λNP2.λNP1.λz, where TV2 is the transitive verb to the right of “and” (“ate” in the current example), TV1 is the transitive verb to the left of it (“caught”), NP1 is the subject, NP2 is the object and z is the end of the complete string.\nRemark that “and” takes all constituents as argument: the two transitive verbs, the subject noun phrase and the object noun phrase, so it would seem that we should be able to generate the right string.\nAs before, we have split the σ and σ → σ nodes for readability; the actual graph merges all σ and all σ → σ nodes. The implausible analyses with TV1 and subject of TV2 and with TV2 as object of TV1 are not shown in the figure, but they fail for the same reasons discussed below.\n51\nThe graph of Figure 14 shows that the TV1 node takes first its subjet (down and to the left of it), then its object (directly below) and finally an argument of type σ (the upward arrow back to σ) and similarly for TV2. The TV1 node (optionally) takes NP1 as its subjet and TV2 (optionally) takes NP2 as its object.\nTwo combinations are fairly limited: the second argument of TV1 is either NP1 or the empty string and the first argument of TV2 is either NP2 or the empty string. However, if the lexical entry contains the subterm ((TV1M)NP1) (for someM at the place of the object), then we are essentially using a quantifyingin analysis for the subject: (TV1M) is a sentence missing a noun phrase anywhere and applying this term to an argument puts this argument back at the place of the missing noun phrase. Consequently, it would allow the derivation of “caught John and ate a fish”. Similar overgeneration occurs for “ate” and “a fish” if we use the quantifying-in combination (TV2NP2) for the object.\nIf we want to avoid both types of overgeneration (subject quantifying-in and object quantifying in), the only remaining analysis consists of choosing λx.x, λy.y, λv.v and λw.w as arguments for the two transitive verbs.11 This solution is shown in full below.\nλTV2.λTV1.λNP2.λNP1.λz.\nNP1 ((TV1 λx.x λy.y)(and ((TV2 λv.v λw.w)(NP2 z))))\nAs we can see from the proof in Figure 15, this lexical type allows us to derive “John caught and ate a fish” with the correct semantics. The proof has been slightly simplified by using distinct variables for the words instead of complex lambda terms (ie. we have not done lexical substitution). This has the advantage that we can use the resulting lambda term for computing the semantics as well, for which we use the following (standard) semantic substitutions12. We can obtain the prosodic lambda terms from the principal types and the string positions (eg. (2 → 1) ⊢ (B → 2) → (1 → A) → B → A for “caught”, which is the standard transitive verb principal type we have seen before). The semantic terms below are all standard.\nand = λTV1λTV2λyλx.((TV1 y)x) ∧ ((TV2 y)x)\nj = john’\nf = a fish’\nc = caught’\na = ate’\n11This solution still overgenerates because it equates transitive verb with “sentence missing two noun phrases” and therefore incorrectly predicts that “John likes []np ’s friend from []np” can felicitously fill this role as follows.\n(i) Mary went to and John likes ’s friend from Paris.\nMeaning “Mary went to Paris and John likes Mary’s friend from there”. 12To keep this example simple, we have treated “a fish” as an individual constant instead of a quantified noun phrase, since quantification is irrelevant for this example.\n52\nJohn\nnp1→0\ncaught\nc(B→2)→(1→A)→B→A\n(s|np)|np\nand and((G→G)→(H→H)→J→3)→((E→E)→(F→F )→2→I)→(L→J)→(I→K)→L→K\n(((s|np)|np) | ((s|np)|np)) | ((s|np)|np)\nate a(D→4)→(3→C)→D→C\n(s|np)|np\n(and a)((E→E)→(F→F)→2→I)→(L→4)→(I→K)→L→K\n((s|np)|np) | ((s|np)|np)\n((and a) c)(L→4)→(1→K)→L→K\n(s|np)|np\na fish\nf5→4\nnp\n(((and a) c) f)(1→K)→5→K\ns|np\n((((and a) c) f) j)5→0\ns\nF ig u re 1 5 : P ro o f o f “ J o h n ca u g h t a n d a te a fi sh ” (sim p lifi ed )\nUnfortunately, this analysis of “and” also make the (rather catastrophic) prediction that “John caught and ate a fish” has a second reading which can be paraphrased as “John caught a fish and a fish ate John”. This reading is easy to miss when we look only at eta-short proofs, since the key point of this second derivation involves switching the two arguments of the transitive verb, as shown in Figure 16.13 The crux of this second proof is that swapping the two arguments of “ate” is a purely local operation which has no visible effects on the word order: the only difference between the proof in Figure 15 and the proof in Figure 16 is in the subproof with undischarged hypothesis “ate” (with term a resp. λx.λy.(a y)x).\nAs shown in the figure, the second proof computes the following “deep structure”.\n(((and (Ca)) c) f) j = (((and λx.λy.((a y)x) c) f) j\nIn a similar way, we can obtain a third and a fourth reading, corresponding the string “John caught and ate a fish” but to the meanings “A fish caught John and John ate a fish” and “A fish caught and ate John” respectively, as follows.\n(((and a) (Cc)) f) j = (((and a)λv.λw.(cw) v) f) j\n(((and (Ca)) (Cc)) f) j = (((and λx.λy.((a y)x)λv.λw.(cw) v) f) j\nThe problem is that though we would want the two (s|np)|np arguments of “and” to be transitive verbs, they mean “a sentence missing two np arguments anywhere”, which is what causes the problems with commutativity.\nWe can again remedy this by adding new lexical entries, for example choosing tv for the two transitive verbs and (tv\\((np\\s)/np))/tv for the conjunction, but this would mean adding several other lexical entries to analyse sentences like “John has understood and will probably implement Dijkstra’s algorithm”, which are handled by the Lambek calculus analysis — since “has understood” and “will probably implement” can both be analysed as (np\\s)/np — but not by the new atomic tv analysis. So adding lexical entries is not only inelegant and an admittance of descriptive inadequacy, but such additions can cascade throughout the grammar.\nI would seem that another simple potential solution would be to add case to lambda grammars. While adding case to first-order linear logic is something we can do essentially for free using extra arguments, adding case to lambda grammars at least complicates either the grammars or the types. In addition, though case would exclude the subject-object swaps we have seen in this section, is is easy to see this would not be a real solution, because the sentences in (19) below are all sentences missing a subject/nominative np, those in (20) sentences missing an object/accusative np and those in (21) sentences missing both a subject and an object (for clarity, the missing subjects and objects have been shown as []s and []o respectively). So while adding case excludes some\n13The term λf.λx.λy.((f y)x) which switches subject and object is of course the C combinator we have already seen in Section 3.3. It commutes the two arguments of a function f , and the proof shown in Figure 16 has a subproof which computes Ca ≡ λx.λy.(a y)x.\n54\nJohn\nnp1→0\ncaught\nc(B→2)→(1→A)→B→A\n(s|np)|np\nand and((G→G)→(H→H)→J→3)→((E→E)→(F→F )→2→I)→(L→J)→(I→K)→L→K\n(((s|np)|np) | ((s|np)|np)) | ((s|np)|np)\nate a(D→4)→(3→C)→D→C\n(s|np)|np yD→4 np\n(a y)(3→C)→D→C\ns|np x3→C np\n((a y)x)D→C\ns (λy.((a y)x))(D→4)→D→C\ns|np\n(λx.λy.((a y)x))(3→C)→(D→4)→D→C\n(s|np)|np\n(and a)((E→E)→(F→F )→2→I)→(L→4)→(I→K)→L→K\n((s|np)|np) | ((s|np)|np)\n((and a) c)(L→4)→(1→K)→L→K\n(s|np)|np\na fish\nf5→4\nnp\n(((and a) c) f)(1→K)→5→K\ns|np\n((((and a) c) f) j)5→0\ns\nF ig u re 1 6 : P ro o f o f “ J o h n ca u g h t a n d a te a fi sh ” w ith sem a n tics “ J o h n ca u g h t a fi sh a n d a fi sh a te J o h n ” (sim p lifi ed ).\nbad derivations, we would still predict sentences like “*Sue likes Mary and John saw the man whom likes” is grammatical (with meaning “Sue likes Mary and John saw the man whom Sue likes.”), that “ *John saw the friend of who lives in Paris and Ted likes Sue” is grammatical and means “John saw the friend of Sue who lives in Paris and Ted likes Sue” and that “*Sue John believes avoids but Ted saw whom kissed Peter” is grammatical and means “John believes Sue avoids Peter but Ted saw Peter whom Sue kissed”.\n(19) a. []s likes Mary. b. John believes []s left. c. John saw the man whom []s likes.\n(20) a. John likes []o. b. John saw the friend of []o who lives in Paris. c. Ted gave []o flowers.\n(21) a. []s gave []o flowers. b. John believes []s avoids []o. c. John saw []o whom []s kissed.\nWhile it would certainly be possible to appeal to island constraints or other independently motivated mechanisms to exclude coordination of the phrases listed above, it seems that use case for this purpose is inherently on the wrong track: it uses lambda terms to encode word order for negative implications and a cascade of stop-gap solutions to constrain word order for positive implications. As the examples above make clear, for coordination, we don’t coordinate (partial) constituents which have the same case marking, but rather those which have the same structure and Lambek calculus formulas, such as (np\\s)/np for transitive verb conjunction, are a good proxy for this notion of the same structure.\nThough we have given an in-depth analysis only of transitive verb conjunction, other conjunctions of complex types (adjectives, intransitive verbs, etc.) suffer similar problems. In all cases, the lambda grammar analysis is between a rock and a hard place, suffering either from overgeneration (as the adverb case) or from bizarre readings (as in the transitive verb conjunction case)."
    }, {
      "heading" : "Gapping",
      "text" : "As a last problem case, the standard (multimodal) categorial grammar analysis of gapping (Hendriks 1995), of which we have seen the hybrid version in Section 5.1, does not fare any better when we try to translate it into lambda grammar. The analysis of a sentence like\n(22) John studies logic and Charles phonetics.\nwould assign “and” the formula.\n((s|((s|np)|np)) | (s|((s|np)|np))) | (s|((s|np)|np))\nThe idea behind the analysis of Hendriks (1995) is that “and” takes first two sentences missing a transitive verb as its arguments, then a transitive verb\n56\nto produce a sentence by placing the transitive verb back to its normal place in the first argument (which is the sentences to its left missing a transitive verb) and using the empty string instead of the transitive verb in the second sentence. In short, it uses a quantifying-in analysis for the transitive verb in the sentence to the left and an extraction analysis for the “missing” transitive in the sentence to the right. What is nice about this analysis, is that we use a normal coordination formula for “and”, an instance of the schema (X |X)|X with (in this case) X = (s|np)|np. Since the analysis of Hendriks (1995) uses a combination of quantifying in and extraction, it is tempting to think that the lambda grammar analysis is unproblematic. However, they key point of the analysis is that we need both extraction and in situ binding for a complex formula, a transitive verb, though unlike for the coordination case it occurs in a negative position in the gapping coordination type. Let’s investigate the possibilities.\nThe formula for transitive verb gapping produces the (simplified and reduced) inhabitation machine shown in Figure 17.\nAs before, the prefix λSTV2.λSTV1.λTV.λz. has been remove from the figure; STV1 denotes the sentence missing a transitive verb to the left of “and” (“John logic” in our case) and STV2 denotes the sentence missing a transitive verb to the right of “and” (“Charles phonetics” in our case), TV the transitive verb (here: “studies”) and z the end of the string.\nWe can obtain the full combinatorics by identifying all nodes with the same type. The current reduced graph emphasizes the reasonable lambda terms: for example, TV can only be an argument of STV1, corresponding to the the quantifying-in analysis producing the desired word order “John studies logic”, similarly, the first argument of the transitive verb has been restricted to the subject and the second argument to the object. In fact, getting the word order and semantics right leaves a unique lambda term — this is just the term from (Bourreau 2013), where the subterm (STV1TV) has been eta-expanded.14\nλSTV2.λSTV1.λTV.λz.((STV1 λO1λS1λx.(((TV λw.O1w)λv.S1 v))x)\n(and (STV2λO2λS2λy.S2 (O2 y)) z))\nThe principal type of this term is.\n(((L → K) → (K → M) → L → M) → J → 4) →\n(((D → C) → (F → E) → H → G) → 3 → I) →\n((D → C) → (F → E) → H → G) → J → I\nGiven this lambda term and principal type, the we can derive the correct word order and semantics as shown in Figure 18. We have again abbreviated the\n14The eta-short term looks as follows.\nλSTV2.λSTV1.λTV.λz.((STV1TV)\n(and (STV2 λO2λS2λy.S2 (O2 y)) z))\n57\nproof, using the following abbreviations for readability.\nX = s|((s|np)|np)\nα = ((L → K) → (K → M) → L → M) → J → 4\nβ = (D → C) → (F → E) → H → G\nWe have also performed the substitutions necessary for the → E rules directly on the hypotheses of the proof. We obtain the semantics by substituting the\n58\nfollowing terms for the constants in the lambda term computed for this proof.\nand = λSTV1λSTV2λTV.(STV1TV) ∧ (STV2TV)\nj = john’\nl = logic’\nc = charles’\np = phonetics’\ns = studies’\nBut again, there is an alternative proof, shown in Figure 19. This proof swaps both the arguments of P and the two abstractions of s (“studies”). The net result is that the left conjunct stays as before, syntactically and semantically, since the two swaps cancel out against each other. Now the right conjunct has its arguments swapped in the semantics only, giving the absurd reading “John studies logic and phonetics studies Charles”.\nSince this article is already rather long, we cannot treat the other problem cases mentioned at the start of Section 7. However, the examples which have been treated in detail serve as a blueprint to constructing similar problems for the additional listed problem cases. In all cases, the fundamental asymmetry of lambda grammars means we have insufficient tools at our disposal to constrain the word order for positive implications and that the best possible approximations are inadequate both syntactically and semantically."
    }, {
      "heading" : "7.3 Solutions for lambda grammars",
      "text" : "Given the descriptive challenges for lambda grammars, it seems natural to ask how lambda grammars could evolve to rise to these challenges.\n1. Stasis. Keeping the formalism and the analyses as they are is a possible, if not a very attractive solution, since it would require us to significantly tone down the ambitions of the syntax-semantics interface of the formalism, thereby losing one of the attractive aspects of categorial grammars.\nWe can also choose to embrace descriptive inadequacy and use the result from (Buszkowski 1996), which, as discussed in Section 5.5, translates Lambek grammars into AB grammars while preserving the semantics, though at the price of an explosion in lexicon size (as Pentus’, 1995 original proof) — to obtain at least the most of the coverage of hybrid type-logical grammars directly within lambda grammars (though this presupposes strict separation, as required for the application of Lemma 5.14). This would save lambda grammars empirically, incorporating the syntaxsemantics interface of the hybrid system, but we would then have a combinatorial explosion followed by an NP-complete problem (according to Lemma 5.15). Given that the original Pentus proof, with O(|G|n3) complexity for some colossal |G|, never resulted in fast, practical parsers for\n59\nJohn\nj1→0\nnp\nP (3→2)→(1→0)→B ′ →A′\n(s|np)|np\nlogic\nl3→2\nnp\n(P l)(1→0)→B ′ →A′\ns|np\n((P l) j)B ′ →A′\ns\n(λP.(P l) j)((3→2)→(1→0)→B ′ →A′)→B′→A′\ns|((s|np)|np)\nand andα→(β→3→I)→β→J→I\n(X |X)|X\nCharles c5→4\nnp\nQ(6→5)→(5→4)→D ′ →C′\n(s|np)|np\nphonetics\np6→5\nnp\n(Qp)(5→4)→D ′ →C′\ns|np\n((Qp) c)D ′ →C′\ns\n(λQ.(Qp) c)((6→5)→(5→4)→D ′ →C′)→D′→C′\ns|((s|np)|np)\n(and (λQ.(Qp) c))(β→3→I)→β→6→I\nX |X\n((and (λQ.(Qp) c)) (λP.(P l) j))((3→2)→(1→0)→3→I)→6→I\ns|((s|np)|np)\nx1→A\nnp\nstudies s(B→2)→(1→A)→B→A\n(s|np)|np yB→2 np\n(s y)(1→A)→B→A\ns|np\n((s y)x)B→A\ns\n(λx.(s y)x)(1→A)→B→A\ns|np\n(λyx.(s y)x)(B→2)→(1→A)→B→A\n(s|np)|np\n(((and (λQ.(Qp) c)) (λP.(P l) j)) (λyx.(s y)x))6→0\ns\nF ig u re 1 8 : P ro o f o f ” J o h n stu d ies lo g ic a n d C h a rles p h o n etics” .\nJohn j1→0\nnp P (1→0)→(3→2)→B\n′ →A′\n(s|np)|np\n(P j)(3→2)→B ′ →A′\ns|np\nlogic\nl3→2\nnp\n((P j) l)B ′ →A′\ns\n(λP.(P j) l)((1→0)→(3→2)→B ′ →A′)→B′→A′\ns|((s|np)|np)\nand andα→(β→3→I)→β→J→I\n(X |X)|X\nCharles c5→4\nnp\nQ(6→5)→(5→4)→D ′ →C′\n(s|np)|np\nphonetics\np6→5\nnp\n(Qp)(5→4)→D ′ →C′\ns|np\n((Qp) c)D ′ →C′\ns\n(λQ.(Qp) c)((6→5)→(5→4)→D ′ →C′)→D′→C′\ns|((s|np)|np)\n(and (λQ.(Qp) c))(β→3→I)→β→6→I\nX |X\n((and (λQ.(Qp) c)) (λP.(P j) l))((1→0)→(3→2)→3→I)→6→I\ns|((s|np)|np)\nx1→A\nnp\nstudies s(B→2)→(1→A)→B→A\n(s|np)|np yB→2 np\n(s y)(1→A)→B→A\ns|np\n((s y)x)B→A\ns (λy.(s y)x)(B→2)→B→A\ns|np\n(λxy.(s y)x)(1→A)→(B→2)→B→A\n(s|np)|np\n(((and (λQ.(Qp) c)) (λP.(P j) l)) (λxy.(s y)x))6→0\ns\nF ig u re 1 9 : P ro o f o f ” J o h n stu d ies lo g ic a n d C h a rles p h o n etics” w ith sem a n tics “ J o h n stu d ies lo g ic a n d p h o n etics stu d ies C h a rles” .\nthe Lambek calculus because of the grammar size constant, having a similar constant for an NP-complete problem does not bode well for parsing the resulting grammar.\nSo it seems we have two unappealing options here: give up — or significantly reduce the ambition of — the syntax-semantics interface or give up actually parsing lambda grammars.\n2. Change the terms and/or their interpretation. The lambda grammars discussed above produce strings. Several authors have looked at lambda grammars which generate different types of structures, such as trees (Muskens 2001, de Groote 2002). When we generate trees, we can add a separate yield algebra which tells us how to interpret the possible word orders generated by a given tree. This “multiple transduction” approach has several other instances and though it is conceivable that such multiple transductions may help alleviate some of the symptoms, it does not address their root cause, which is the asymmetry of the system.\nWhat we need is a system which can reject “candidate derivations” which have been computed at the previous level. Such a solution can be found in (Muskens 2007), who produces first-order logic formulas and adds a separate theorem-prover component (which is essentially a model-builder for multimodal categorial grammars). Muskens’ solution would solve the problems with lambda grammars by essentially generating potential derivations and asking a multimodal grammar if these derivations are valid. However, this setup does not seem to have any benefits over a direct multimodal implementation and suffers from the same complexity problems as multimodal categorial grammars. Pogodalla & Pompigne (2012), discussed below since they change the types in addition to the terms, also fall into this category.\nAnother potential solution in this family would be to add term equations (and corresponding reductions) to the lambda calculus. However, it is unclear what sort of form such a solution would take.\nAs we have seen in the treatment of transitive verb conjunctions, adding case offers (at best) a partial solution by attacking the symptoms rather than the underlying cause of the problem. It should also be noted that the only solution of this kind which has been worked out in any detail uses dependent types and this complicates the types as well as the terms (Pogodalla & Pompigne 2012, Pompigne 2013), which moves us to the next point.\n3. Change the types. Various authors (de Groote & Maarek 2007, Pogodalla & Pompigne 2012) have looked at extending the type theory of lambda grammars beyond the simply typed lambda calculus. Of these extensions, dependent types seem well-suited to the challenges posed in this paper, though they would need to be added on a much larger scale than previously assumed and they would complicate the type/term calculus and its mathematical properties considerably.\n62\nAn alternative solution, proposed in the context of linear grammars (Worth 2014) uses subtyping combined with restrictions on the form of subterm cooccurrences. It is unclear to me at the moment whether this type of treatment is equivalent to other proposals (eg. those of hybrid type-logical grammars) and whether it corresponds to a natural fragment of first-order linear logic.\nIs seems that the easiest way to fix the inadequacies of lambda grammars would by to extend the system to hybrid type-logical grammar: existing linguistic analyses in lambda grammars can be preserved and/or corrected while the system keeps much of the flavor of lambda grammars.\nAn alternative, especially for those convinced of the need to extend lambda grammars to handle linguistic features and island constraints (Pogodalla & Pompigne 2012, Pompigne 2013), is to move to first-order linear logic, which also allows us to preserves the things that work in lambda grammars but incorporate a simple treatment of both features and island constraints without having to change the underlying logical theory. Those particularly attached to dependent types can obtain them from MILL1 proofs by means of the Curry-Howard isomorphism; first-order logic is a fairly weak fragment of the lambda calculus with dependent types (Sørensen & Urzyczyn 2006) so we need to verify whether it is expressive enough. Since smart parsing algorithms for lambda grammars (de Groote 2007) already use first-order (linear) logic to drive proof search, this solution stays close to the computational core of lambda grammars: it remedies the severe problems but also allows us to include treatments for which much more complicated analyses have been proposed. However, it is not clear in such a setup what the typed lambda terms actually contribute and we would have a much simpler system if we simply removed the typed lambda terms from the surface structure component and handle all of the surface structure in first-order linear logic."
    }, {
      "heading" : "8 Conclusions",
      "text" : "In this paper, we have shown that Hybrid Type-Logical Grammars (Kubota & Levine 2013a) (and by extension lambda grammars/abstract categorial grammars) can be embedded in first-order linear logic by means of a simple translation, formula to formula and proof to proof. This provides cleaner prooftheoretic foundations for Hybrid Type-Logical Grammars but also suggests new ways of parsing these grammars. As an immediate corollary, we have also shown that Hybrid Type-Logical Grammars are NP-complete (like lambda grammars and the Lambek calculus).\nWe have also seen how this translation provides a new perspective of the known (but often ignored) problems of lambda grammars with coordination and shown that the lack of left-right symmetry (or, at the very least, the absence of a way to emulate the Lambek calculus introduction rules) results in overgeneration and descriptive inadequacy problems for a much larger class of cases than previously assumed.\n63\nCombined with the results from (Moot & Piazza 2001) and (Moot 2014), this means that the Lambek calculus, the Displacement calculus, lambda grammars and Hybrid Type-Logical Grammars can all be translated into first-order linear logic by means of simple translations and that, moreover, many of the analyses of linguistic phenomena in these different systems converge upon translation into first-order linear logic.\nFirst-order linear logic can thus be seen as a way to decompose the connectives of all these logics, separating the functor/argument structure from the word order operations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This paper is deeply indebted to Yusuke Kubota and Robert Levine, whose ESSLLI 2013 course awoke my curiosity both about the proof theoretic aspects of hybrid type-logical grammar and about the descriptive inadequacies of lambda grammars/abstract categorial grammars — the two principal themes of the current paper.\nEarly versions of these ideas were presented at the LIX Colloquium on the Theory and Application of Formal Proofs (Palaiseau, November 2013), Computational Linguistics in the Netherlands (Leiden, January 2014) and the Polymnie workshop (Toulouse, March 2014). I would like all the people present there for their questions and constructive comments, notably Crit Cremers, Philippe de Groote, Dominic Hughes and Dale Miller.\nLast, but certainly not least, I would like to thank Michael Moortgat, Carl Pollard and Christian Retoré for their discussion about the themes of this paper.\nAll remaining errors are of course my own. This work has benefitted from the generous support of the French agency Agence Nationale de la Recherche as part of the project Polymnie (ANR-12CORD-0004)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Hybrid type-logical grammars (Kubota & Levine 2012, Kubota & Levine 2013c, Kubota & Levine 2013a) are a relatively new framework in computational linguistics, which combines insights from the Lambek calculus (Lambek 1958) and lambda grammars (Oehrle 1994, Muskens 2001, Muskens 2003)— lambda grammars are also called, depending on the authors, abstract categorial grammars (de Groote 2001) and linear grammars (Pollard 2011), though with somewhat different notational conventions. The resulting combined system solves some know problems of both the Lambek calculus and of lambda grammars and the additional expressiveness of hybrid type-logical grammars permits the treatment of linguistic phenomena such as gapping which have no satisfactory solution in either subsystem. The goal of this paper is to prove that hybrid type-logical grammars are a fragment of first-order linear logic. This embedding result has several important consequences: it not only provides a simple new proof theory for the calculus, thereby clarifying the proof-theoretic foundations of hybrid type-logical grammars, but, since the translation is simple and direct, it also provides several new parsing strategies for hybrid type-logical grammars. Second, NP-completeness of hybrid type-logical grammars follows immediately. The main embedding result also sheds new light on problems with lambda grammars, which are a subsystem of hybrid type-logical grammars and hence a special case of the translation into first-order linear logic. Abstract categorial grammars are attractive both because of their simplicity — they use the simply",
    "creator" : "LaTeX with hyperref package"
  }
}