{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing", "abstract": "In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs $mt$ and $src$ in a single neural architecture, modeling $\\{mt, src\\} \\rightarrow pe$ directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT 2016 shared task on automatic post-editing and can demonstrate that double-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Double-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.", "histories": [["v1", "Tue, 13 Jun 2017 15:55:02 GMT  (204kb,D)", "http://arxiv.org/abs/1706.04138v1", null], ["v2", "Sat, 30 Sep 2017 13:03:33 GMT  (233kb,D)", "http://arxiv.org/abs/1706.04138v2", "Accepted for presentation at IJCNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "roman grundkiewicz"], "accepted": false, "id": "1706.04138"}
