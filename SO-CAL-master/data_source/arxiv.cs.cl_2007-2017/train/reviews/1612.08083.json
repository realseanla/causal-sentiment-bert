{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2016", "title": "Language Modeling with Gated Convolutional Networks", "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. In this paper we present a convolutional approach to language modeling. We introduce a novel gating mechanism that eases gradient propagation and which performs better than the LSTM-style gating of (Oord et al, 2016) despite being simpler. We achieve a new state of the art on WikiText-103 as well as a new best single-GPU result on the Google Billion Word benchmark. In settings where latency is important, our model achieves an order of magnitude speed-up compared to a recurrent baseline since computation can be parallelized over time. To our knowledge, this is the first time a non-recurrent approach outperforms strong recurrent models on these tasks.", "histories": [["v1", "Fri, 23 Dec 2016 20:32:33 GMT  (221kb,D)", "http://arxiv.org/abs/1612.08083v1", null], ["v2", "Mon, 12 Jun 2017 22:34:00 GMT  (434kb,D)", "http://arxiv.org/abs/1612.08083v2", null], ["v3", "Fri, 8 Sep 2017 22:26:49 GMT  (434kb,D)", "http://arxiv.org/abs/1612.08083v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yann n dauphin", "angela fan", "michael auli", "david grangier"], "accepted": true, "id": "1612.08083"}
