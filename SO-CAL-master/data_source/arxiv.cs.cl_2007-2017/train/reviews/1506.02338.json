{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Modeling Order in Neural Word Embeddings at Scale", "abstract": "Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network.", "histories": [["v1", "Mon, 8 Jun 2015 02:21:46 GMT  (446kb,D)", "https://arxiv.org/abs/1506.02338v1", null], ["v2", "Wed, 10 Jun 2015 15:42:42 GMT  (446kb,D)", "http://arxiv.org/abs/1506.02338v2", null], ["v3", "Thu, 11 Jun 2015 03:00:29 GMT  (445kb,D)", "http://arxiv.org/abs/1506.02338v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andrew trask", "david gilmore", "matthew russell"], "accepted": true, "id": "1506.02338"}
