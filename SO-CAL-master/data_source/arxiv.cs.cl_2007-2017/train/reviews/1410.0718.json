{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2014", "title": "Not All Neural Embeddings are Born Equal", "abstract": "Neural language models learn word representations that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models. We show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and/or syntactic role. The findings suggest that, while monolingual models learn information about how concepts are related, neural-translation models better capture their true ontological status.", "histories": [["v1", "Thu, 2 Oct 2014 21:35:35 GMT  (23kb,D)", "http://arxiv.org/abs/1410.0718v1", "4 pages plus 1 page of references"], ["v2", "Thu, 13 Nov 2014 15:58:35 GMT  (83kb,D)", "http://arxiv.org/abs/1410.0718v2", "4 pages plus 1 page of references"]], "COMMENTS": "4 pages plus 1 page of references", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix hill", "kyunghyun cho", "sebastien jean", "coline devin", "yoshua bengio"], "accepted": false, "id": "1410.0718"}
