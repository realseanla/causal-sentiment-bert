{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural Networks", "abstract": "Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling unseen words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different clock rates. Despite the multi-clock structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%.", "histories": [["v1", "Tue, 13 Sep 2016 11:41:48 GMT  (174kb)", "http://arxiv.org/abs/1609.03777v1", "Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016) on May 20, 2016"], ["v2", "Thu, 2 Feb 2017 13:49:41 GMT  (178kb)", "http://arxiv.org/abs/1609.03777v2", "Submitted to NIPS 2016 on May 20, 2016 (v1), accepted to ICASSP 2017 (v2)"]], "COMMENTS": "Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016) on May 20, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1609.03777"}
