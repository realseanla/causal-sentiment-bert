{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?", "abstract": "Recurrent neural network grammars (RNNG) are a recently proposed probabilistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model's latent attention largely agreeing with predictions made by hand-crafted rules, albeit with some important differences). By training grammars without non-terminal labels, we find that phrasal representations depend minimally on non-terminals, providing support for the endocentricity hypothesis.", "histories": [["v1", "Thu, 17 Nov 2016 16:41:41 GMT  (638kb,D)", "http://arxiv.org/abs/1611.05774v1", null], ["v2", "Tue, 10 Jan 2017 19:15:08 GMT  (526kb,D)", "http://arxiv.org/abs/1611.05774v2", "10 pages. To appear in EACL 2017, Valencia, Spain"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adhiguna kuncoro", "miguel ballesteros", "lingpeng kong", "chris dyer", "graham neubig", "noah a smith"], "accepted": false, "id": "1611.05774"}
