{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung &amp; Zisserman, 2016a). All existing works, however, perform only word classification, not sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton &amp; Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level, using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 93.4% accuracy, outperforming experienced human lipreaders and the previous 79.6% state-of-the-art accuracy.", "histories": [["v1", "Sat, 5 Nov 2016 04:05:18 GMT  (3950kb,D)", "http://arxiv.org/abs/1611.01599v1", null], ["v2", "Fri, 16 Dec 2016 16:09:34 GMT  (1926kb,D)", "http://arxiv.org/abs/1611.01599v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["yannis m assael", "brendan shillingford", "shimon whiteson", "nando de freitas"], "accepted": false, "id": "1611.01599"}
