{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2013", "title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 74.4. A combination of techniques leads to 37% reduction in perplexity, or 11% reduction in cross-entropy (bits), over that baseline.", "histories": [["v1", "Wed, 11 Dec 2013 00:25:57 GMT  (13kb)", "https://arxiv.org/abs/1312.3005v1", "Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper"], ["v2", "Fri, 28 Feb 2014 22:26:59 GMT  (13kb)", "http://arxiv.org/abs/1312.3005v2", "Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper"], ["v3", "Tue, 4 Mar 2014 18:30:26 GMT  (13kb)", "http://arxiv.org/abs/1312.3005v3", "Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper"]], "COMMENTS": "Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ciprian chelba", "tomas mikolov", "mike schuster", "qi ge", "thorsten brants", "phillipp koehn", "tony robinson"], "accepted": false, "id": "1312.3005"}
