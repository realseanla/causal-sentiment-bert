{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Learning When to Attend for Neural Machine Translation", "abstract": "In the past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. However, previous attention models always refer to some source words when predicting a target word, which contradicts with the fact that some target words have no corresponding source words. Motivated by this observation, we propose a novel attention model that has the capability of determining when a decoder should attend to source words and when it should not. Experimental results on NIST Chinese-English translation tasks show that the new model achieves an improvement of 0.8 BLEU score over a state-of-the-art baseline.", "histories": [["v1", "Wed, 31 May 2017 16:05:49 GMT  (124kb)", "http://arxiv.org/abs/1705.11160v1", "5 pages, 2 figures"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["junhui li", "muhua zhu"], "accepted": false, "id": "1705.11160"}
