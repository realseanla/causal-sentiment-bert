{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Coverage Embedding Models for Neural Machine Translation", "abstract": "In this paper, we enhance the attention-based neural machine translation by adding an explicit coverage embedding model to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector, and then keeps updating it with a gated recurrent unit as the translation goes. All the initialized coverage embeddings and updating matrix are learned in the training procedure. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.", "histories": [["v1", "Tue, 10 May 2016 18:44:34 GMT  (913kb,D)", "http://arxiv.org/abs/1605.03148v1", "6 pages"], ["v2", "Mon, 29 Aug 2016 15:10:34 GMT  (1106kb,D)", "http://arxiv.org/abs/1605.03148v2", "6 pages; In Proceddings of EMNLP 2016"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haitao mi", "baskaran sankaran", "zhiguo wang", "abe ittycheriah"], "accepted": true, "id": "1605.03148"}
