{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction", "abstract": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observable data using a feature-rich conditional random field. Then a reconstruction of the input is (re)generated, conditional on the latent structure, using models for which maximum likelihood estimation has a closed-form. Our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. We show competitive results with instantiations of the model for two canonical NLP tasks: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.", "histories": [["v1", "Wed, 5 Nov 2014 04:49:38 GMT  (557kb,D)", "http://arxiv.org/abs/1411.1147v1", null], ["v2", "Mon, 10 Nov 2014 05:58:04 GMT  (556kb,D)", "http://arxiv.org/abs/1411.1147v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["waleed ammar", "chris dyer", "noah a smith"], "accepted": true, "id": "1411.1147"}
