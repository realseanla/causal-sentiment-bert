{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution", "abstract": "Most existing approaches for zero pronoun resolution are supervised approaches, where annotated data are released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in zero pronoun resolution task. The existing approaches mainly face the challenge of costing manpower on labeling the extended data for better training performance and domain adaption. To alleviate the problem above, in this paper we propose a simple but novel approach to automatically produce large-scale pseudo training data for zero pronoun resolution. Furthermore, to avoid the drawbacks of the feature engineering based approaches, we proposed an attention-based LSTM model for this task. Experimental results show that our proposed approach outperforms the state-of-the-art methods significantly with an absolute improvement of 5.1% F-score in OntoNotes 5.0 corpus.", "histories": [["v1", "Mon, 6 Jun 2016 02:45:47 GMT  (534kb,D)", "https://arxiv.org/abs/1606.01603v1", null], ["v2", "Fri, 17 Jun 2016 09:18:35 GMT  (267kb,D)", "http://arxiv.org/abs/1606.01603v2", "10pages, under review at EMNLP2016, some typos have been fixed"], ["v3", "Tue, 6 Jun 2017 02:47:41 GMT  (249kb,D)", "http://arxiv.org/abs/1606.01603v3", "8+2 pages, published as a conference paper at ACL2017 (long paper)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ting liu", "yiming cui", "qingyu yin", "weinan zhang", "shijin wang", "guoping hu"], "accepted": true, "id": "1606.01603"}
