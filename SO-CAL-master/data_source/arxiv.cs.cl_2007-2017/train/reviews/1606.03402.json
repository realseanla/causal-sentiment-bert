{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning", "abstract": "Encoder-decoder networks are popular for probabilistic modeling sequences in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables and are not subject to label bias of locally conditioned models that assume partial conditional independence. However in practice they exhibit a bias towards short sequences even when using a beam search to find the optimal sequence. Surprisingly, sometimes there is even a decline in accuracy with increasing the beam size.", "histories": [["v1", "Fri, 10 Jun 2016 17:30:46 GMT  (240kb,D)", "https://arxiv.org/abs/1606.03402v1", null], ["v2", "Wed, 21 Sep 2016 17:33:58 GMT  (305kb,D)", "http://arxiv.org/abs/1606.03402v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["pavel sountsov", "sunita sarawagi"], "accepted": true, "id": "1606.03402"}
