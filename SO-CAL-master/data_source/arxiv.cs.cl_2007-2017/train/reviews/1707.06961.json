{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "Mimicking Word Embeddings using Subword RNNs", "abstract": "Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised character-based model in low-resource settings.", "histories": [["v1", "Fri, 21 Jul 2017 16:18:10 GMT  (315kb,D)", "http://arxiv.org/abs/1707.06961v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuval pinter", "robert guthrie", "jacob eisenstein"], "accepted": true, "id": "1707.06961"}
