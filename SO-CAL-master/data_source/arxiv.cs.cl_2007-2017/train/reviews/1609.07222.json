{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "Deep Multi-Task Learning with Shared Memory", "abstract": "Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks.", "histories": [["v1", "Fri, 23 Sep 2016 03:35:27 GMT  (350kb,D)", "http://arxiv.org/abs/1609.07222v1", "accepted by emnlp2016. arXiv admin note: text overlap witharXiv:1605.05101"]], "COMMENTS": "accepted by emnlp2016. arXiv admin note: text overlap witharXiv:1605.05101", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pengfei liu", "xipeng qiu", "xuanjing huang"], "accepted": false, "id": "1609.07222"}
