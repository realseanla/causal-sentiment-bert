{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Future Word Contexts in Neural Network Language Models", "abstract": "Recently, bidirectional recurrent network language models (bi-RNNLMs) have been shown to outperform standard, unidirectional, recurrent neural network language models (uni-RNNLMs) on a range of speech recognition tasks. This indicates that future word context information beyond the word history can be useful. However, bi-RNNLMs pose a number of challenges as they make use of the complete previous and future word context information. This impacts both training efficiency and their use within a lattice rescoring framework. In this paper these issues are addressed by proposing a novel neural network structure, succeeding word RNNLMs (su-RNNLMs). Instead of using a recurrent unit to capture the complete future word contexts, a feedforward unit is used to model a finite number of succeeding, future, words. This model can be trained much more efficiently than bi-RNNLMs and can also be used for lattice rescoring. Experimental results on a meeting transcription task (AMI) show the proposed model consistently outperformed uni-RNNLMs and yield only a slight degradation compared to bi-RNNLMs in N-best rescoring. Additionally, performance improvements can be obtained using lattice rescoring and subsequent confusion network decoding.", "histories": [["v1", "Fri, 18 Aug 2017 13:11:22 GMT  (617kb,D)", "http://arxiv.org/abs/1708.05592v1", "Submitted to ASRU2017"]], "COMMENTS": "Submitted to ASRU2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xie chen", "xunying liu", "anton ragni", "yu wang", "mark gales"], "accepted": false, "id": "1708.05592"}
