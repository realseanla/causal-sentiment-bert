{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2015", "title": "Very Deep Multilingual Convolutional Neural Networks for LVCSR", "abstract": "Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.78% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 12.2% after cross-entropy training, a 1.6% WER improvement over our own baseline and a 1.0% WER improvement over the best published CNN result so far.", "histories": [["v1", "Tue, 29 Sep 2015 22:28:11 GMT  (66kb,D)", "http://arxiv.org/abs/1509.08967v1", "Manuscript submitted to ICASSP 2016"], ["v2", "Sat, 23 Jan 2016 18:18:58 GMT  (66kb,D)", "http://arxiv.org/abs/1509.08967v2", "Accepted for publication at ICASSP 2016"]], "COMMENTS": "Manuscript submitted to ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["tom sercu", "christian puhrsch", "brian kingsbury", "yann lecun"], "accepted": false, "id": "1509.08967"}
