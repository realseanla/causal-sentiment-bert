{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization", "abstract": "A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.", "histories": [["v1", "Wed, 8 Feb 2017 17:30:51 GMT  (60kb,D)", "http://arxiv.org/abs/1702.02535v1", null], ["v2", "Wed, 19 Apr 2017 20:40:43 GMT  (61kb,D)", "http://arxiv.org/abs/1702.02535v2", "This paper is accepted by ACL 2017"], ["v3", "Tue, 25 Apr 2017 16:33:52 GMT  (61kb,D)", "http://arxiv.org/abs/1702.02535v3", "This paper is accepted by ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ye zhang", "matthew lease", "byron c wallace"], "accepted": true, "id": "1702.02535"}
