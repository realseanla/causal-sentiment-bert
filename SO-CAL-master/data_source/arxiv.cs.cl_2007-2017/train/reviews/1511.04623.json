{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2015", "title": "Learning to Represent Words in Context with Multilingual Supervision", "abstract": "We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.", "histories": [["v1", "Sat, 14 Nov 2015 21:36:38 GMT  (109kb)", "https://arxiv.org/abs/1511.04623v1", null], ["v2", "Thu, 19 Nov 2015 23:35:42 GMT  (190kb)", "http://arxiv.org/abs/1511.04623v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kazuya kawakami", "chris dyer"], "accepted": false, "id": "1511.04623"}
