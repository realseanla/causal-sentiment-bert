{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Data Distillation for Controlling Specificity in Dialogue Generation", "abstract": "People speak at different levels of specificity in different situations. Depending on their knowledge, interlocutors, mood, etc.} A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network--based conversational agent this ability. Our approach involves alternating between \\emph{data distillation} and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity.", "histories": [["v1", "Wed, 22 Feb 2017 08:32:47 GMT  (156kb,D)", "http://arxiv.org/abs/1702.06703v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "will monroe", "dan jurafsky"], "accepted": false, "id": "1702.06703"}
