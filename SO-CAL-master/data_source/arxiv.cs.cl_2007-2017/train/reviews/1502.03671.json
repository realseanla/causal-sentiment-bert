{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Phrase-based Image Captioning", "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.", "histories": [["v1", "Thu, 12 Feb 2015 14:17:15 GMT  (730kb,D)", "http://arxiv.org/abs/1502.03671v1", null], ["v2", "Thu, 9 Apr 2015 09:48:52 GMT  (572kb,D)", "http://arxiv.org/abs/1502.03671v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\u00e9mi lebret", "pedro h o pinheiro", "ronan collobert"], "accepted": true, "id": "1502.03671"}
