{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Automatic Annotation of Structured Facts in Images", "abstract": "Motivated by the application of fact-level image understanding, we present an automatic method for data collection of structured visual facts from images with captions. Example structured facts include attributed objects (e.g., &lt;flower, red&gt;), actions (e.g., &lt;baby, smile&gt;), interactions (e.g., &lt;man, walking, dog&gt;), and positional information (e.g., &lt;vase, on, table&gt;). The collected annotations are in the form of fact-image pairs (e.g.,&lt;man, walking, dog&gt; and an image region containing this fact). With a language approach, the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83\\% according to human judgment. %that we obtained using both Amazon Mechanical Turk and volunteer scientists in our laboratory. Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms. %Our work enable large-scale fact-level understanding of images. %Based on human j evaluation shows that more than 83\\% of the automatically collected annotation %We focus on collecting higher order visual facts annotations which include attributed objects (e.g. &lt;car, black&gt;, &lt;flower, red&gt;, actions &lt;baby, smile&gt;, and interactions &lt;dog, riding, wave&gt;.", "histories": [["v1", "Sat, 2 Apr 2016 06:35:45 GMT  (8911kb,D)", "https://arxiv.org/abs/1604.00466v1", null], ["v2", "Tue, 5 Apr 2016 18:58:10 GMT  (8911kb,D)", "http://arxiv.org/abs/1604.00466v2", null], ["v3", "Fri, 8 Apr 2016 00:04:22 GMT  (8911kb,D)", "http://arxiv.org/abs/1604.00466v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["mohamed elhoseiny", "scott cohen", "walter chang", "brian price", "ahmed elgammal"], "accepted": false, "id": "1604.00466"}
