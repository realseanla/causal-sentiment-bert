{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Variational Neural Machine Translation", "abstract": "Models of neural machine translation are often from a discriminative family of encoder-decoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoder-decoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform an efficient posterior inference, we build a neural posterior approximator that is conditioned only on the source side. Additionally, we employ a reparameterization technique to estimate the variational lower bound so as to enable standard stochastic gradient optimization and large-scale training for the variational model. Experiments on NIST Chinese-English translation tasks show that the proposed variational neural machine translation achieves significant improvements over both state-of-the-art statistical and neural machine translation baselines.", "histories": [["v1", "Wed, 25 May 2016 13:18:57 GMT  (239kb,D)", "http://arxiv.org/abs/1605.07869v1", "10 pages"], ["v2", "Sun, 25 Sep 2016 23:37:14 GMT  (206kb,D)", "http://arxiv.org/abs/1605.07869v2", "10 pages, accepted at emnlp 2016"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su", "hong duan", "min zhang"], "accepted": true, "id": "1605.07869"}
