{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2015", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space", "abstract": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.", "histories": [["v1", "Fri, 24 Apr 2015 22:12:14 GMT  (203kb,D)", "http://arxiv.org/abs/1504.06654v1", "In Conference on Empirical Methods in Natural Language Processing, 2014"]], "COMMENTS": "In Conference on Empirical Methods in Natural Language Processing, 2014", "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["arvind neelakantan", "jeevan shankar", "alexandre passos", "andrew mccallum"], "accepted": true, "id": "1504.06654"}
