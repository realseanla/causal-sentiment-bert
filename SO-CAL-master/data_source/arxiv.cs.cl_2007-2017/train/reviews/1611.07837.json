{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Adaptive Feature Abstraction for Translating Video to Language", "abstract": "A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. We consider both \"hard\" and \"soft\" attention mechanisms, to adaptively and sequentially focus on different layers of features (levels of feature \"abstraction\"), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.", "histories": [["v1", "Wed, 23 Nov 2016 15:21:48 GMT  (4421kb,D)", "http://arxiv.org/abs/1611.07837v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["yunchen pu", "martin renqiang min", "zhe gan", "lawrence carin"], "accepted": false, "id": "1611.07837"}
