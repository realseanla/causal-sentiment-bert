{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Building an Evaluation Scale using Item Response Theory", "abstract": "We introduce Item Response Theory (IRT) from psychometrics as an alternative to majority voting to create an IRT gold standard ($GS_{IRT}$). IRT describes characteristics of individual items in $GS_{IRT}$ - their difficulty and discriminating power - and is able to account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we evaluated IRT's model-fitting of a majority vote gold standard designed for Recognizing Textual Entailment (RTE), denoted as $GS_{RTE}$. By collecting human responses and fitting our IRT model, we found that up to 31% of $GS_{RTE}$ were not useful in building $GS_{IRT}$ for RTE. In addition, we found low inter-annotator agreement for some items in $GS_{RTE}$ suggesting that more work is needed for creating intelligent gold-standards.", "histories": [["v1", "Sat, 28 May 2016 13:19:15 GMT  (70kb,D)", "http://arxiv.org/abs/1605.08889v1", null], ["v2", "Fri, 23 Sep 2016 16:35:16 GMT  (86kb,D)", "http://arxiv.org/abs/1605.08889v2", "To appear in the proceedings of EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john p lalor", "hao wu", "hong yu"], "accepted": true, "id": "1605.08889"}
