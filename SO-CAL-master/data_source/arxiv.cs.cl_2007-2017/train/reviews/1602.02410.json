{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2016", "title": "Exploring the Limits of Language Modeling", "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.", "histories": [["v1", "Sun, 7 Feb 2016 19:11:17 GMT  (76kb,D)", "http://arxiv.org/abs/1602.02410v1", null], ["v2", "Thu, 11 Feb 2016 23:01:48 GMT  (77kb,D)", "http://arxiv.org/abs/1602.02410v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rafal jozefowicz", "oriol vinyals", "mike schuster", "noam shazeer", "yonghui wu"], "accepted": false, "id": "1602.02410"}
