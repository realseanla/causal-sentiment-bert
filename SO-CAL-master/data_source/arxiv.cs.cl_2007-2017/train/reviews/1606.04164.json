{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation", "abstract": "In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.", "histories": [["v1", "Mon, 13 Jun 2016 22:40:33 GMT  (25kb)", "http://arxiv.org/abs/1606.04164v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["orhan firat", "baskaran sankaran", "yaser al-onaizan", "fatos t yarman-vural", "kyunghyun cho"], "accepted": true, "id": "1606.04164"}
