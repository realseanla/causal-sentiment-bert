{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Learning to Embed Words in Context for Syntactic Tasks", "abstract": "We present models for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data. We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.", "histories": [["v1", "Fri, 9 Jun 2017 01:39:12 GMT  (1172kb)", "https://arxiv.org/abs/1706.02807v1", null], ["v2", "Mon, 12 Jun 2017 01:42:12 GMT  (1172kb)", "http://arxiv.org/abs/1706.02807v2", "Accepted by ACL 2017 Repl4NLP workshop"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lifu tu", "kevin gimpel", "karen livescu"], "accepted": false, "id": "1706.02807"}
