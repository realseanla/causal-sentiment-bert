{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Neural Machine Transliteration: Preliminary Results", "abstract": "Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoder-decoder model is able to achieve significantly higher transliteration quality over traditional statistical models.", "histories": [["v1", "Wed, 14 Sep 2016 13:12:12 GMT  (63kb,D)", "http://arxiv.org/abs/1609.04253v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amir h jadidinejad"], "accepted": false, "id": "1609.04253"}
