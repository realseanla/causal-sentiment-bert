{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "histories": [["v1", "Thu, 19 Dec 2013 14:18:14 GMT  (15kb)", "https://arxiv.org/abs/1312.5559v1", "4 pages, 1 table, ICLR Workshop"], ["v2", "Tue, 14 Jan 2014 17:33:49 GMT  (15kb)", "http://arxiv.org/abs/1312.5559v2", "4 pages, 1 table, ICLR Workshop; main experimental table was extended with more experimental results"], ["v3", "Tue, 18 Feb 2014 14:17:46 GMT  (15kb)", "http://arxiv.org/abs/1312.5559v3", "4 pages, 1 table, ICLR Workshop; main experimental table was extended with more experimental results; related word added"]], "COMMENTS": "4 pages, 1 table, ICLR Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["irina sergienya", "hinrich sch\\\"utze"], "accepted": false, "id": "1312.5559"}
