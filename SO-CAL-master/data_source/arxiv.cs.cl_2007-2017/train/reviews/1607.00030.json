{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "HUME: Human UCCA-Based Evaluation of Machine Translation", "abstract": "Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, providing a more fine-grained analysis of translation quality, and enables the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.", "histories": [["v1", "Thu, 30 Jun 2016 20:35:47 GMT  (1078kb,D)", "https://arxiv.org/abs/1607.00030v1", null], ["v2", "Tue, 27 Sep 2016 13:39:42 GMT  (1126kb,D)", "http://arxiv.org/abs/1607.00030v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandra birch", "omri abend", "ondrej bojar", "barry haddow"], "accepted": true, "id": "1607.00030"}
