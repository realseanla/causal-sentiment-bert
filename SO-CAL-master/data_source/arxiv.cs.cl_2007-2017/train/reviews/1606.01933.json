{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "A Decomposable Attention Model for Natural Language Inference", "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "histories": [["v1", "Mon, 6 Jun 2016 20:30:57 GMT  (61kb,D)", "http://arxiv.org/abs/1606.01933v1", "5 pages, 1 figure"], ["v2", "Sun, 25 Sep 2016 23:52:45 GMT  (63kb,D)", "http://arxiv.org/abs/1606.01933v2", "7 pages, 1 figure, Proceeedings of EMNLP 2016"]], "COMMENTS": "5 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ankur p parikh", "oscar t\u00e4ckstr\u00f6m", "dipanjan das 0001", "jakob uszkoreit"], "accepted": true, "id": "1606.01933"}
