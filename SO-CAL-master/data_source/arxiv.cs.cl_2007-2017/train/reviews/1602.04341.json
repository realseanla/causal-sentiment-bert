{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "abstract": "Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system's ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of passage, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.", "histories": [["v1", "Sat, 13 Feb 2016 14:38:47 GMT  (439kb,D)", "http://arxiv.org/abs/1602.04341v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "sebastian ebert", "hinrich sch\\\"utze"], "accepted": false, "id": "1602.04341"}
