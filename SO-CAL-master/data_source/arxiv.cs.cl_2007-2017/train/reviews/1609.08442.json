{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Collaborative Learning for Language and Speaker Recognition", "abstract": "This paper presents a unified model to perform language and speaker recognition simultaneously and altogether. The model is based on a multi-task recurrent neural network where the output of one task is fed as the input of the other, leading to a collaborative learning framework that can improve both language and speaker recognition by borrowing information from each other. Our experiments demonstrated that the multi-task model outperforms the task-specific models on both tasks.", "histories": [["v1", "Tue, 27 Sep 2016 13:48:01 GMT  (237kb,D)", "https://arxiv.org/abs/1609.08442v1", "Submitted to ICASSP 2017"], ["v2", "Tue, 23 May 2017 09:56:54 GMT  (622kb,D)", "http://arxiv.org/abs/1609.08442v2", null]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["lantian li", "zhiyuan tang", "dong wang", "rew abel", "yang feng", "shiyue zhang"], "accepted": false, "id": "1609.08442"}
