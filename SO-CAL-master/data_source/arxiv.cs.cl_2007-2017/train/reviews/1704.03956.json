{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Incremental Skip-gram Model with Negative Sampling", "abstract": "This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SNGS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SNGS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.", "histories": [["v1", "Thu, 13 Apr 2017 00:36:33 GMT  (167kb,D)", "http://arxiv.org/abs/1704.03956v1", null], ["v2", "Sat, 15 Apr 2017 07:15:00 GMT  (167kb,D)", "http://arxiv.org/abs/1704.03956v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nobuhiro kaji", "hayato kobayashi"], "accepted": true, "id": "1704.03956"}
