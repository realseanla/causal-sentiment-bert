{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "histories": [["v1", "Fri, 1 Apr 2016 20:06:46 GMT  (33kb,D)", "http://arxiv.org/abs/1604.00400v1", "LREC 2016"]], "COMMENTS": "LREC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["arman cohan", "nazli goharian"], "accepted": false, "id": "1604.00400"}
