{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples", "abstract": "Machine Learning has been a big success story during the AI resurgence. One particular stand out success relates to unsupervised learning from a massive amount of data, albeit much of it relates to one modality/type of data at a time. In spite of early assertions of the unreasonable effectiveness of data, there is increasing recognition of utilizing knowledge whenever it is available or can be created purposefully. In this paper, we focus on discussing the indispensable role of knowledge for deeper understanding of complex text and multimodal data in situations where (i) large amounts of training data (labeled/unlabeled) are not available or labor intensive to create, (ii) the objects (particularly text) to be recognized are complex (i.e., beyond simple entity-person/location/organization names), such as implicit entities and highly subjective content, and (iii) applications need to use complementary or related data in multiple modalities/media. What brings us to the cusp of rapid progress is our ability to (a) create knowledge, varying from comprehensive or cross domain to domain or application specific, and (b) carefully exploit the knowledge to further empower or extend the applications of ML/NLP techniques. Using the early results in several diverse situations - both in data types and applications - we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data.", "histories": [["v1", "Tue, 25 Oct 2016 02:13:53 GMT  (1114kb,D)", "http://arxiv.org/abs/1610.07708v1", "15 pages, 5 figures"]], "COMMENTS": "15 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["amit sheth", "sujan perera", "sanjaya wijeratne"], "accepted": false, "id": "1610.07708"}
