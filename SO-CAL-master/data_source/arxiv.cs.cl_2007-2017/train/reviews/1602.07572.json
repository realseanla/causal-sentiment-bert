{"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Ultradense Word Embeddings by Orthogonal Transformation", "abstract": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information - sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space.", "histories": [["v1", "Wed, 24 Feb 2016 16:06:25 GMT  (54kb,D)", "http://arxiv.org/abs/1602.07572v1", null], ["v2", "Sun, 8 May 2016 08:50:11 GMT  (53kb,D)", "http://arxiv.org/abs/1602.07572v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sascha rothe", "sebastian ebert", "hinrich sch\u00fctze"], "accepted": true, "id": "1602.07572"}
