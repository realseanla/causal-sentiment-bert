{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "A Compositional Approach to Language Modeling", "abstract": "Traditional language models treat language as a finite state automaton on a probability space over words. This is a very strong assumption when modeling something inherently complex such as language. In this paper, we challenge this by showing how the linear chain assumption inherent in previous work can be translated into a sequential composition tree. We then propose a new model that marginalizes over all possible composition trees thereby removing any underlying structural assumptions. As the partition function of this new model is intractable, we use a recently proposed sentence level evaluation metric Contrastive Entropy to evaluate our model. Given this new evaluation metric, we report more than 100% improvement across distortion levels over current state of the art recurrent neural network based language models.", "histories": [["v1", "Fri, 1 Apr 2016 01:51:34 GMT  (300kb,D)", "http://arxiv.org/abs/1604.00100v1", "submitted to ACL 2016"]], "COMMENTS": "submitted to ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kushal arora", "anand rangarajan"], "accepted": false, "id": "1604.00100"}
