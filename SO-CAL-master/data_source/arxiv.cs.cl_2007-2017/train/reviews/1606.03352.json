{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems", "abstract": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.", "histories": [["v1", "Fri, 10 Jun 2016 14:56:19 GMT  (1169kb,D)", "http://arxiv.org/abs/1606.03352v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.NE stat.ML", "authors": ["tsung-hsien wen", "milica gasic", "nikola mrksic", "lina maria rojas-barahona", "pei-hao su", "stefan ultes", "david vandyke", "steve j young"], "accepted": true, "id": "1606.03352"}
