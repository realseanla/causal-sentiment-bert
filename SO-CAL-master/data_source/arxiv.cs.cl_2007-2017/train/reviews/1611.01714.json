{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Beyond Fine Tuning: A Modular Approach to Learning on Small Data", "abstract": "In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.", "histories": [["v1", "Sun, 6 Nov 2016 01:32:39 GMT  (1096kb,D)", "http://arxiv.org/abs/1611.01714v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["ark", "erson", "kyle shaffer", "artem yankov", "court d corley", "nathan o hodas"], "accepted": false, "id": "1611.01714"}
