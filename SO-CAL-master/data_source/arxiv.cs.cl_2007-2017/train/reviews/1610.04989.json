{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification", "abstract": "Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets.", "histories": [["v1", "Mon, 17 Oct 2016 07:28:06 GMT  (139kb,D)", "http://arxiv.org/abs/1610.04989v1", "Published as long paper of EMNLP2016"]], "COMMENTS": "Published as long paper of EMNLP2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jiacheng xu", "danlu chen", "xipeng qiu", "xuanjing huang"], "accepted": true, "id": "1610.04989"}
