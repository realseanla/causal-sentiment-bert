{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Deep Reinforcement Learning for Mention-Ranking Coreference Models", "abstract": "Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.", "histories": [["v1", "Tue, 27 Sep 2016 21:00:26 GMT  (924kb,D)", "http://arxiv.org/abs/1609.08667v1", "To appear in EMNLP 2016"], ["v2", "Thu, 20 Oct 2016 21:58:34 GMT  (924kb,D)", "http://arxiv.org/abs/1609.08667v2", "To appear in EMNLP 2016"], ["v3", "Mon, 31 Oct 2016 20:30:15 GMT  (924kb,D)", "http://arxiv.org/abs/1609.08667v3", "To appear in EMNLP 2016"]], "COMMENTS": "To appear in EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kevin clark", "christopher d manning"], "accepted": true, "id": "1609.08667"}
