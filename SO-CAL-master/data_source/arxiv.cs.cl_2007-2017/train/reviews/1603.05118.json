{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Recurrent Dropout without Memory Loss", "abstract": "This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.", "histories": [["v1", "Wed, 16 Mar 2016 14:33:47 GMT  (47kb)", "http://arxiv.org/abs/1603.05118v1", null], ["v2", "Fri, 5 Aug 2016 09:59:25 GMT  (95kb)", "http://arxiv.org/abs/1603.05118v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stanislau semeniuta", "aliaksei severyn", "erhardt barth"], "accepted": false, "id": "1603.05118"}
