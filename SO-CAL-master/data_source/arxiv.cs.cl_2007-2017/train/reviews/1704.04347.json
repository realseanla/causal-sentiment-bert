{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Exploiting Cross-Sentence Context for Neural Machine Translation", "abstract": "In translation, considering the document as a whole allows certain ambiguities and inconsistencies to be resolved. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.", "histories": [["v1", "Fri, 14 Apr 2017 04:56:36 GMT  (166kb,D)", "https://arxiv.org/abs/1704.04347v1", null], ["v2", "Mon, 17 Apr 2017 00:21:19 GMT  (166kb,D)", "http://arxiv.org/abs/1704.04347v2", null], ["v3", "Sun, 23 Jul 2017 05:19:42 GMT  (170kb,D)", "http://arxiv.org/abs/1704.04347v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["longyue wang", "zhaopeng tu", "andy way", "qun liu"], "accepted": true, "id": "1704.04347"}
