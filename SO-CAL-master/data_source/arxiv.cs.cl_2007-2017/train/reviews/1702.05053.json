{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Addressing the Data Sparsity Issue in Neural AMR Parsing", "abstract": "Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources.", "histories": [["v1", "Thu, 16 Feb 2017 17:09:12 GMT  (209kb)", "http://arxiv.org/abs/1702.05053v1", "Accepted by EACL-17"]], "COMMENTS": "Accepted by EACL-17", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiaochang peng", "chuan wang", "daniel gildea", "nianwen xue"], "accepted": false, "id": "1702.05053"}
