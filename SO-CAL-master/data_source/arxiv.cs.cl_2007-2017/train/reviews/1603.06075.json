{"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Tree-to-Sequence Attentional Neural Machine Translation", "abstract": "Most of the existing neural machine translation (NMT) models focus on the conversion of sequential data and do not directly take syntax into consideration. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.", "histories": [["v1", "Sat, 19 Mar 2016 10:08:40 GMT  (437kb)", "https://arxiv.org/abs/1603.06075v1", null], ["v2", "Tue, 22 Mar 2016 09:55:39 GMT  (340kb,D)", "http://arxiv.org/abs/1603.06075v2", null], ["v3", "Wed, 8 Jun 2016 08:39:11 GMT  (371kb,D)", "http://arxiv.org/abs/1603.06075v3", "Accepted as a full paper at the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["akiko eriguchi", "kazuma hashimoto", "yoshimasa tsuruoka"], "accepted": true, "id": "1603.06075"}
