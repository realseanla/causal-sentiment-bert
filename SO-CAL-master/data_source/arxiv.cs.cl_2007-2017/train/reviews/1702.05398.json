{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2017", "title": "Experiment Segmentation in Scientific Discourse as Clause-level Structured Prediction using Recurrent Neural Networks", "abstract": "We propose a deep learning model for identifying structure within experiment narratives in scientific literature. We take a sequence labeling approach to this problem, and label clauses within experiment narratives to identify the different parts of the experiment. Our dataset consists of paragraphs taken from open access PubMed papers labeled with rhetorical information as a result of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells that labels clauses. The clause representations are computed by combining word representations using a novel attention mechanism that involves a separate RNN. We compare this model against LSTMs where the input layer has simple or no attention and a feature rich CRF model. Furthermore, we describe how our work could be useful for information extraction from scientific literature.", "histories": [["v1", "Fri, 17 Feb 2017 15:39:21 GMT  (222kb,D)", "http://arxiv.org/abs/1702.05398v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pradeep dasigi", "gully a p c burns", "eduard hovy", "anita de waard"], "accepted": false, "id": "1702.05398"}
