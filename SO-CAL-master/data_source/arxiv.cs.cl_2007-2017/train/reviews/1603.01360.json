{"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Neural Architectures for Named Entity Recognition", "abstract": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.", "histories": [["v1", "Fri, 4 Mar 2016 06:36:29 GMT  (123kb,D)", "http://arxiv.org/abs/1603.01360v1", "Proceedings of NAACL 2016"], ["v2", "Wed, 6 Apr 2016 03:11:58 GMT  (124kb,D)", "http://arxiv.org/abs/1603.01360v2", "Proceedings of NAACL 2016"], ["v3", "Thu, 7 Apr 2016 15:09:36 GMT  (124kb,D)", "http://arxiv.org/abs/1603.01360v3", "Proceedings of NAACL 2016"]], "COMMENTS": "Proceedings of NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guillaume lample", "miguel ballesteros", "sandeep subramanian", "kazuya kawakami", "chris dyer"], "accepted": true, "id": "1603.01360"}
