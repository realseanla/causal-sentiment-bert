{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Iterative Alternating Neural Attention for Machine Reading", "abstract": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.", "histories": [["v1", "Tue, 7 Jun 2016 18:25:48 GMT  (278kb,D)", "http://arxiv.org/abs/1606.02245v1", "Under review for EMNLP 2016"], ["v2", "Wed, 8 Jun 2016 18:17:03 GMT  (278kb,D)", "http://arxiv.org/abs/1606.02245v2", "Under review for EMNLP 2016"], ["v3", "Fri, 10 Jun 2016 21:16:56 GMT  (278kb,D)", "http://arxiv.org/abs/1606.02245v3", "Under review for EMNLP 2016"], ["v4", "Wed, 9 Nov 2016 18:11:09 GMT  (280kb,D)", "http://arxiv.org/abs/1606.02245v4", null]], "COMMENTS": "Under review for EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["alessandro sordoni", "philip bachman", "adam trischler", "yoshua bengio"], "accepted": false, "id": "1606.02245"}
