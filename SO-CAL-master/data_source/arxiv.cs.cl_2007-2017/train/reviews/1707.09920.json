{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2017", "title": "Regularization techniques for fine-tuning in neural machine translation", "abstract": "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English-&gt;German and English-&gt;Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.", "histories": [["v1", "Mon, 31 Jul 2017 15:31:12 GMT  (34kb,D)", "http://arxiv.org/abs/1707.09920v1", "EMNLP 2017 short paper; for bibtex, seethis http URL"]], "COMMENTS": "EMNLP 2017 short paper; for bibtex, seethis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["antonio valerio miceli barone", "barry haddow", "ulrich germann", "rico sennrich"], "accepted": true, "id": "1707.09920"}
