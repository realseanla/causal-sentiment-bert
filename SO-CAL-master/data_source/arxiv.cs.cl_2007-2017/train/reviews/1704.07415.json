{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "abstract": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems.", "histories": [["v1", "Mon, 24 Apr 2017 18:49:38 GMT  (498kb,D)", "http://arxiv.org/abs/1704.07415v1", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yichen gong", "samuel r bowman"], "accepted": false, "id": "1704.07415"}
