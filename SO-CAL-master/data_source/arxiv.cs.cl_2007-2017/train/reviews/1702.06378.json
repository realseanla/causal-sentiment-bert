{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition", "abstract": "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling objectives used for end-to-end training of speech recognition models. Both models define the transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a \"continuation\" of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder used for feature extraction for both outputs. We find that this multi-task objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model.", "histories": [["v1", "Tue, 21 Feb 2017 13:39:35 GMT  (144kb,D)", "http://arxiv.org/abs/1702.06378v1", "5 pages, 2 figures, submitted to Interspeech 2017"], ["v2", "Mon, 6 Mar 2017 02:40:45 GMT  (144kb,D)", "http://arxiv.org/abs/1702.06378v2", "5 pages, 2 figures, submitted to Interspeech 2017"], ["v3", "Thu, 23 Mar 2017 20:42:54 GMT  (144kb,D)", "http://arxiv.org/abs/1702.06378v3", "5 pages, 2 figures, submitted to Interspeech 2017"], ["v4", "Mon, 5 Jun 2017 18:19:34 GMT  (144kb,D)", "http://arxiv.org/abs/1702.06378v4", "5 pages, 2 figures, camera ready version at Interspeech 2017"]], "COMMENTS": "5 pages, 2 figures, submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["liang lu", "lingpeng kong", "chris dyer", "noah a smith"], "accepted": false, "id": "1702.06378"}
