{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Vocabulary Manipulation for Neural Machine Translation", "abstract": "In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).", "histories": [["v1", "Tue, 10 May 2016 20:50:56 GMT  (421kb,D)", "http://arxiv.org/abs/1605.03209v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haitao mi", "zhiguo wang", "abe ittycheriah"], "accepted": true, "id": "1605.03209"}
