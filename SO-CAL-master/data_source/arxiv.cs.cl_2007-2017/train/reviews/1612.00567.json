{"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features", "abstract": "Transition-based models can be fast and accurate for constituent parsing. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which spans over non-local constituents. On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shift-reduce parsing. To address this limitation, we leverage a fast neural model to extract lookahead features. In particular, we build a bidirectional LSTM model, which leverages the full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as lookahead features. The resulting parser gives 1.3% absolute improvement in WSJ and 2.3% in CTB compared to the baseline, given the highest reported accuracies for fully-supervised parsing.", "histories": [["v1", "Fri, 2 Dec 2016 04:55:24 GMT  (196kb,D)", "http://arxiv.org/abs/1612.00567v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiangming liu", "yue zhang"], "accepted": true, "id": "1612.00567"}
