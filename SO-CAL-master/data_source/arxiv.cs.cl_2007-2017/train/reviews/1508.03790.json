{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "Depth-Gated LSTM", "abstract": "In this short note, we present an extension of LSTM to use a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper recurrent units. Importantly, the linear dependence is gated through a gating function, which we call forget gate. This gate is a function of lower layer memory cell, its input, and its past memory. We conducted experiments and verified that this new architecture of LSTMs is able to improve machine translation and language modeling performances.", "histories": [["v1", "Sun, 16 Aug 2015 04:31:37 GMT  (44kb,D)", "https://arxiv.org/abs/1508.03790v1", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v2", "Wed, 19 Aug 2015 19:38:58 GMT  (73kb,D)", "http://arxiv.org/abs/1508.03790v2", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v3", "Thu, 20 Aug 2015 07:13:04 GMT  (73kb,D)", "http://arxiv.org/abs/1508.03790v3", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v4", "Tue, 25 Aug 2015 04:24:20 GMT  (101kb,D)", "http://arxiv.org/abs/1508.03790v4", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"]], "COMMENTS": "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015", "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["kaisheng yao", "trevor cohn", "katerina vylomova", "kevin duh", "chris dyer"], "accepted": false, "id": "1508.03790"}
