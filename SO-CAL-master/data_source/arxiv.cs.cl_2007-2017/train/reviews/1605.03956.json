{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "On the Convergent Properties of Word Embedding Methods", "abstract": "Do word embeddings converge to learn similar things over different initializations? How repeatable are experiments with word embeddings? Are all word embedding techniques equally reliable? In this paper we propose evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness -- an important property to consider when developing new word embedding methods.", "histories": [["v1", "Thu, 12 May 2016 19:59:43 GMT  (293kb,D)", "http://arxiv.org/abs/1605.03956v1", "RepEval @ ACL 2016"]], "COMMENTS": "RepEval @ ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yingtao tian", "vivek kulkarni", "bryan perozzi", "steven skiena"], "accepted": false, "id": "1605.03956"}
