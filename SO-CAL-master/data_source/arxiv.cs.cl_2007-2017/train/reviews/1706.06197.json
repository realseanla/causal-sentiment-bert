{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2017", "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting", "abstract": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1--4\\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.", "histories": [["v1", "Mon, 19 Jun 2017 22:36:33 GMT  (153kb)", "http://arxiv.org/abs/1706.06197v1", "ICML 2017"], ["v2", "Wed, 5 Jul 2017 01:34:50 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v2", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"], ["v3", "Mon, 30 Oct 2017 09:48:41 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v3", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"], ["v4", "Tue, 31 Oct 2017 02:04:52 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v4", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.CV", "authors": ["xu sun", "xuancheng ren", "shuming ma", "houfeng wang"], "accepted": true, "id": "1706.06197"}
