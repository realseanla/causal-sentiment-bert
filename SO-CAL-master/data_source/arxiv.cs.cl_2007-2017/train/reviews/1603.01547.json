{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Text Understanding with the Attention Sum Reader Network", "abstract": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Our model outperforms models previously proposed for these tasks by a large margin.", "histories": [["v1", "Fri, 4 Mar 2016 17:32:42 GMT  (366kb,D)", "http://arxiv.org/abs/1603.01547v1", null], ["v2", "Fri, 24 Jun 2016 13:04:47 GMT  (825kb,D)", "http://arxiv.org/abs/1603.01547v2", "Presented at ACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rudolf kadlec", "martin schmid", "ondrej bajgar", "jan kleindienst"], "accepted": true, "id": "1603.01547"}
