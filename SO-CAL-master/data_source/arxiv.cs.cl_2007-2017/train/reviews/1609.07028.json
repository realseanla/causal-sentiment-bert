{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Image-embodied Knowledge Representation Learning", "abstract": "Entity images provide significant visual information that helps the construction of knowledge representations. Most conventional methods learn knowledge representations solely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model, where knowledge representations are learned with both triples and images. More specifically, for each image of an entity, we construct image-based representations via a neural image encoder, and these representations with respect to multiple image instances are then integrated via an attention-based method. We evaluate our models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.", "histories": [["v1", "Thu, 22 Sep 2016 15:37:45 GMT  (348kb,D)", "http://arxiv.org/abs/1609.07028v1", "7 pages"], ["v2", "Mon, 22 May 2017 08:14:27 GMT  (343kb,D)", "http://arxiv.org/abs/1609.07028v2", "7 pages; Accepted by IJCAI-2017"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["ruobing xie", "zhiyuan liu", "huanbo luan", "maosong sun"], "accepted": false, "id": "1609.07028"}
