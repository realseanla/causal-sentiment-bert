{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Plan, Attend, Generate: Character-level Neural Machine Translation with Planning in the Decoder", "abstract": "We investigate the integration of a planning mechanism into an encoder-decoder architecture with attention for character-level machine translation. We develop a model that plans ahead when it computes alignments between the source and target sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the strategic attentive reader and writer (STRAW) model. Our proposed model is end-to-end trainable with fully differentiable operations. We show that it outperforms a strong baseline on three character-level decoder neural machine translation on WMT'15 corpus. Our analysis demonstrates that our model can compute qualitatively intuitive alignments and achieves superior performance with fewer parameters.", "histories": [["v1", "Tue, 13 Jun 2017 23:11:04 GMT  (401kb,D)", "https://arxiv.org/abs/1706.05087v1", "Accepted to Rep4NLP 2017 Workshop at ACL 2017 Conference"], ["v2", "Fri, 23 Jun 2017 06:31:05 GMT  (401kb,D)", "http://arxiv.org/abs/1706.05087v2", "Accepted to Rep4NLP 2017 Workshop at ACL 2017 Conference"]], "COMMENTS": "Accepted to Rep4NLP 2017 Workshop at ACL 2017 Conference", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["caglar gulcehre", "francis dutil", "adam trischler", "yoshua bengio"], "accepted": false, "id": "1706.05087"}
