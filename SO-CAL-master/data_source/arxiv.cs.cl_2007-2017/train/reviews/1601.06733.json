{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Long Short-Term Memory-Networks for Machine Reading", "abstract": "Teaching machines to process text with psycholinguistics insights is a challenging task. We propose an attentive machine reader that reads text from left to right, whilst linking the word at the current fixation point to previous words stored in the memory as a kind of implicit information parsing to facilitate understanding. The reader is equipped with a Long Short-Term Memory architecture, which, different from previous work, has a memory tape (instead of a memory cell) to store the past information and adaptively use them without severe information compression. We demonstrate the excellent performance of the machine reader in language modeling as well as the downstream sentiment analysis and natural language inference.", "histories": [["v1", "Mon, 25 Jan 2016 19:25:48 GMT  (737kb,D)", "http://arxiv.org/abs/1601.06733v1", null], ["v2", "Tue, 26 Jan 2016 20:48:02 GMT  (675kb,D)", "http://arxiv.org/abs/1601.06733v2", null], ["v3", "Mon, 1 Feb 2016 14:29:04 GMT  (675kb,D)", "http://arxiv.org/abs/1601.06733v3", null], ["v4", "Thu, 17 Mar 2016 13:28:16 GMT  (1354kb,D)", "http://arxiv.org/abs/1601.06733v4", null], ["v5", "Thu, 7 Apr 2016 09:53:49 GMT  (1354kb,D)", "http://arxiv.org/abs/1601.06733v5", null], ["v6", "Wed, 1 Jun 2016 12:27:42 GMT  (1623kb,D)", "http://arxiv.org/abs/1601.06733v6", "Fixed the incomparable parameters in experiments to previous work; fixed a few typos"], ["v7", "Tue, 20 Sep 2016 21:20:09 GMT  (1618kb,D)", "http://arxiv.org/abs/1601.06733v7", "Published as a conference paper at EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jianpeng cheng 0001", "li dong", "mirella lapata"], "accepted": true, "id": "1601.06733"}
