{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Cynical Selection of Language Model Training Data", "abstract": "The Moore-Lewis method of \"intelligent selection of language model training data\" is very effective, cheap, efficient... and also has structural problems. (1) The method defines relevance by playing language models trained on the in-domain and the out-of-domain (or data pool) corpora against each other. This powerful idea-- which we set out to preserve-- treats the two corpora as the opposing ends of a single spectrum. This lack of nuance does not allow for the two corpora to be very similar. In the extreme case where the come from the same distribution, all of the sentences have a Moore-Lewis score of zero, so there is no resulting ranking. (2) The selected sentences are not guaranteed to be able to model the in-domain data, nor to even cover the in-domain data. They are simply well-liked by the in-domain model; this is necessary, but not sufficient. (3) There is no way to tell what is the optimal number of sentences to select, short of picking various thresholds and building the systems.", "histories": [["v1", "Thu, 7 Sep 2017 14:30:50 GMT  (16kb)", "http://arxiv.org/abs/1709.02279v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amittai axelrod"], "accepted": false, "id": "1709.02279"}
