{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Syllable-level Neural Language Model for Agglutinative Language", "abstract": "Language models for agglutinative languages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes. We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property. Our model outperforms character-level embedding in perplexity by 16.87 with 9.50M parameters. Proposed method achieves state of the art performance over existing input prediction methods in terms of Key Stroke Saving and has been commercialized.", "histories": [["v1", "Fri, 18 Aug 2017 06:02:16 GMT  (1451kb)", "http://arxiv.org/abs/1708.05515v1", "Accepted at EMNLP 2017 workshop on Subword and Character level models in NLP (SCLeM)"]], "COMMENTS": "Accepted at EMNLP 2017 workshop on Subword and Character level models in NLP (SCLeM)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["seunghak yu", "nilesh kulkarni", "haejun lee", "jihie kim"], "accepted": false, "id": "1708.05515"}
