{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "Structured Training for Neural Network Transition-Based Parsing", "abstract": "We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.", "histories": [["v1", "Fri, 19 Jun 2015 21:05:01 GMT  (2714kb,D)", "http://arxiv.org/abs/1506.06158v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david weiss", "chris alberti", "michael collins", "slav petrov"], "accepted": true, "id": "1506.06158"}
