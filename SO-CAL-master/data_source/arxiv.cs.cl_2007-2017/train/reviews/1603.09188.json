{"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings", "abstract": "We introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, or multimodal embeddings. We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambiguation task. VerSe is made publicly available and can be downloaded at:", "histories": [["v1", "Wed, 30 Mar 2016 13:43:38 GMT  (1468kb,D)", "http://arxiv.org/abs/1603.09188v1", "11 pages, NAACL-HLT 2016"]], "COMMENTS": "11 pages, NAACL-HLT 2016", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["spandana gella", "mirella lapata", "frank keller"], "accepted": true, "id": "1603.09188"}
