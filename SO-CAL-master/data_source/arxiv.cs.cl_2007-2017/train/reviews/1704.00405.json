{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling", "abstract": "Traditional approaches to Semantic Role Labeling (SRL) depend heavily on manual feature engineering. Recurrent neural network (RNN) with long-short-term memory (LSTM) only treats sentence as sequence data and can not utilize higher level syntactic information. In this paper, we propose Syntax Aware LSTM (SA-LSTM) which gives RNN-LSTM ability to utilize higher level syntactic information gained from dependency relationship information. SA-LSTM also assigns different trainable weights to different types of dependency relationship automatically. Experiment results on Chinese Proposition Bank (CPB) show that, even without pre-training or introducing any other extra semantically annotated resources, our SA-LSTM model still outperforms the state of the art significantly base on Student's t-test ($p&lt;0.05$). Trained weights of types of dependency relationship form a stable and self-explanatory pattern.", "histories": [["v1", "Mon, 3 Apr 2017 02:10:19 GMT  (986kb,D)", "https://arxiv.org/abs/1704.00405v1", null], ["v2", "Thu, 20 Apr 2017 01:55:26 GMT  (268kb,D)", "http://arxiv.org/abs/1704.00405v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["feng qian", "lei sha", "baobao chang", "lu-chen liu", "ming zhang"], "accepted": false, "id": "1704.00405"}
