{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2017", "title": "Parallel Long Short-Term Memory for Multi-stream Classification", "abstract": "Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-the-art architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach.", "histories": [["v1", "Sat, 11 Feb 2017 09:50:40 GMT  (99kb)", "http://arxiv.org/abs/1702.03402v1", "2016 IEEE Workshop on Spoken Language Technology"]], "COMMENTS": "2016 IEEE Workshop on Spoken Language Technology", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["mohamed bouaziz", "mohamed morchid", "richard dufour", "georges linar\\`es", "renato de mori"], "accepted": false, "id": "1702.03402"}
