{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "abstract": "In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects.", "histories": [["v1", "Fri, 13 Jan 2017 07:26:00 GMT  (36kb)", "http://arxiv.org/abs/1701.03578v1", "AAAI workshop on Crowdsourcing, Deep Learning and Artificial Intelligence Agents, Feb 2017, San Francisco CA, USA"]], "COMMENTS": "AAAI workshop on Crowdsourcing, Deep Learning and Artificial Intelligence Agents, Feb 2017, San Francisco CA, USA", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["seunghyun yoon", "hyeongu yun", "yuna kim", "gyu-tae park", "kyomin jung"], "accepted": false, "id": "1701.03578"}
