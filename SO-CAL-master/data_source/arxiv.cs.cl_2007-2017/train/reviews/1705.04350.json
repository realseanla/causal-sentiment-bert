{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Imagination improves Multimodal Translation", "abstract": "Multimodal machine translation is the task of translating sentences in a visual context. We decompose this problem into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.", "histories": [["v1", "Thu, 11 May 2017 18:49:17 GMT  (1023kb,D)", "http://arxiv.org/abs/1705.04350v1", "Under review"], ["v2", "Fri, 7 Jul 2017 09:18:55 GMT  (1023kb,D)", "http://arxiv.org/abs/1705.04350v2", "Clarified main contributions, minor correction to Equation 8, additional comparisons in Table 2, added more related work"]], "COMMENTS": "Under review", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["desmond elliott", "\\'akos k\\'ad\\'ar"], "accepted": false, "id": "1705.04350"}
