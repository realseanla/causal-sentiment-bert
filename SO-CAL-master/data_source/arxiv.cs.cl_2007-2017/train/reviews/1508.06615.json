{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2015", "title": "Character-Aware Neural Language Models", "abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline (by 30-35%) and a word-level LSTM baseline (by 15-25%), again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.", "histories": [["v1", "Wed, 26 Aug 2015 19:25:34 GMT  (211kb,D)", "http://arxiv.org/abs/1508.06615v1", null], ["v2", "Thu, 17 Sep 2015 23:18:00 GMT  (209kb,D)", "http://arxiv.org/abs/1508.06615v2", null], ["v3", "Fri, 16 Oct 2015 03:18:13 GMT  (209kb,D)", "http://arxiv.org/abs/1508.06615v3", null], ["v4", "Tue, 1 Dec 2015 22:59:24 GMT  (209kb,D)", "http://arxiv.org/abs/1508.06615v4", "AAAI 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE stat.ML", "authors": ["yoon kim", "yacine jernite", "david sontag", "alexander m rush"], "accepted": true, "id": "1508.06615"}
