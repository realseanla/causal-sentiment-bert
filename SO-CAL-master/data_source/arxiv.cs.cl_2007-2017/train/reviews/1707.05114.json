{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2017", "title": "Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation", "abstract": "This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.", "histories": [["v1", "Mon, 17 Jul 2017 12:09:08 GMT  (205kb)", "http://arxiv.org/abs/1707.05114v1", "Accepted for publication at EMNLP 2017"]], "COMMENTS": "Accepted for publication at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["baosong yang", "derek f wong", "tong xiao", "lidia s chao", "jingbo zhu"], "accepted": true, "id": "1707.05114"}
