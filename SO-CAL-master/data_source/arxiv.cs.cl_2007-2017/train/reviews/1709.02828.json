{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Globally Normalized Reader", "abstract": "Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answer's sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type. This method improves the performance of all models considered in this work and is of independent interest for a variety of NLP tasks.", "histories": [["v1", "Fri, 8 Sep 2017 18:27:50 GMT  (332kb,D)", "http://arxiv.org/abs/1709.02828v1", "Presented at EMNLP 2017"]], "COMMENTS": "Presented at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jonathan raiman", "john miller"], "accepted": true, "id": "1709.02828"}
