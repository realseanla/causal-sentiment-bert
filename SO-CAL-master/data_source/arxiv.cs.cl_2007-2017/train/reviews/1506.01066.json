{"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Visualizing and Understanding Neural Models in NLP", "abstract": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,", "histories": [["v1", "Tue, 2 Jun 2015 21:17:31 GMT  (1681kb,D)", "http://arxiv.org/abs/1506.01066v1", null], ["v2", "Fri, 8 Jan 2016 18:10:22 GMT  (2467kb,D)", "http://arxiv.org/abs/1506.01066v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "xinlei chen", "eduard h hovy", "dan jurafsky"], "accepted": true, "id": "1506.01066"}
