{"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "Why We Need New Evaluation Metrics for NLG", "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.", "histories": [["v1", "Fri, 21 Jul 2017 12:47:03 GMT  (241kb,D)", "http://arxiv.org/abs/1707.06875v1", "accepted to EMNLP 2017"]], "COMMENTS": "accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jekaterina novikova", "ond\\v{r}ej du\\v{s}ek", "amanda cercas curry", "verena rieser"], "accepted": true, "id": "1707.06875"}
