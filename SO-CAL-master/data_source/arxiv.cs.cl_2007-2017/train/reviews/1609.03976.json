{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Multimodal Attention for Neural Machine Translation", "abstract": "The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.", "histories": [["v1", "Tue, 13 Sep 2016 18:46:03 GMT  (782kb,D)", "http://arxiv.org/abs/1609.03976v1", "10 pages, under review COLING 2016"]], "COMMENTS": "10 pages, under review COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["ozan caglayan", "lo\\\"ic barrault", "fethi bougares"], "accepted": false, "id": "1609.03976"}
