{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "One Representation per Word - Does it make Sense for Composition?", "abstract": "In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through composition alone. We evaluate the performance of off-the-shelf single-vector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as pointwise addition are able to recover sense specific information from a single-sense vector model remarkably well.", "histories": [["v1", "Wed, 22 Feb 2017 07:41:08 GMT  (363kb,D)", "http://arxiv.org/abs/1702.06696v1", "to appear at the EACL 2017 workshop on Sense, Concept and Entity Representations and their Applications"]], "COMMENTS": "to appear at the EACL 2017 workshop on Sense, Concept and Entity Representations and their Applications", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thomas kober", "julie weeds", "john wilkie", "jeremy reffin", "david weir"], "accepted": false, "id": "1702.06696"}
