{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Neural Word Segmentation with Rich Pretraining", "abstract": "Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.", "histories": [["v1", "Fri, 28 Apr 2017 14:46:25 GMT  (336kb,D)", "http://arxiv.org/abs/1704.08960v1", "Accepted by ACL 2017"]], "COMMENTS": "Accepted by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jie yang", "yue zhang", "fei dong"], "accepted": true, "id": "1704.08960"}
