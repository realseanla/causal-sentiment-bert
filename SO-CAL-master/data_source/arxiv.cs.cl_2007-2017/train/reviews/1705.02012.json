{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2017", "title": "Machine Comprehension by Text-to-Text Neural Question Generation", "abstract": "We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD.", "histories": [["v1", "Thu, 4 May 2017 20:58:06 GMT  (142kb)", "http://arxiv.org/abs/1705.02012v1", null], ["v2", "Mon, 15 May 2017 14:47:05 GMT  (51kb)", "http://arxiv.org/abs/1705.02012v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xingdi yuan", "tong wang", "caglar gulcehre", "alessandro sordoni", "philip bachman", "sandeep subramanian", "saizheng zhang", "adam trischler"], "accepted": false, "id": "1705.02012"}
