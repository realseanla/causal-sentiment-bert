{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Neural Text Generation from Structured Data with Application to the Biography Domain", "abstract": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magni- tude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocab- ulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text genera- tion. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that trans- fer sample-specific words from the in- put database to the generated output sen- tence. Our neural model significantly out- performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.", "histories": [["v1", "Thu, 24 Mar 2016 22:40:00 GMT  (282kb,D)", "http://arxiv.org/abs/1603.07771v1", null], ["v2", "Thu, 22 Sep 2016 14:47:44 GMT  (341kb,D)", "http://arxiv.org/abs/1603.07771v2", "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016"], ["v3", "Fri, 23 Sep 2016 15:16:46 GMT  (341kb,D)", "http://arxiv.org/abs/1603.07771v3", "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\u00e9mi lebret", "david grangier", "michael auli"], "accepted": true, "id": "1603.07771"}
