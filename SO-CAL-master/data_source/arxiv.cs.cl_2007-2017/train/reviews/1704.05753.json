{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection", "abstract": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.", "histories": [["v1", "Wed, 19 Apr 2017 14:41:21 GMT  (91kb,A)", "http://arxiv.org/abs/1704.05753v1", "Accepted for publication at ACL 2017"], ["v2", "Thu, 19 Oct 2017 23:04:39 GMT  (91kb,A)", "http://arxiv.org/abs/1704.05753v2", "Published at ACL 2017"]], "COMMENTS": "Accepted for publication at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["youxuan jiang", "jonathan k kummerfeld", "walter s lasecki"], "accepted": true, "id": "1704.05753"}
