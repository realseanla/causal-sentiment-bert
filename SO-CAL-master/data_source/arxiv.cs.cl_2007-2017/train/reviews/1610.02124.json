{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "There's No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction", "abstract": "Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics.", "histories": [["v1", "Fri, 7 Oct 2016 02:17:17 GMT  (49kb,D)", "http://arxiv.org/abs/1610.02124v1", "to appear in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)"]], "COMMENTS": "to appear in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["courtney napoles", "keisuke sakaguchi", "joel r tetreault"], "accepted": true, "id": "1610.02124"}
