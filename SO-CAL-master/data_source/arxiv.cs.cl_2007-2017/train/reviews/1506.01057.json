{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "abstract": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization\\footnote{Code for the three models described in this paper can be found at www.stanford.edu/~jiweil/ .", "histories": [["v1", "Tue, 2 Jun 2015 20:53:53 GMT  (306kb,D)", "http://arxiv.org/abs/1506.01057v1", null], ["v2", "Sat, 6 Jun 2015 01:47:34 GMT  (302kb,D)", "http://arxiv.org/abs/1506.01057v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "minh-thang luong", "dan jurafsky"], "accepted": true, "id": "1506.01057"}
