{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Predicting Target Language CCG Supertags Improves Neural Machine Translation", "abstract": "Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English-German, a high-resource pair, and for English-Romanian, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.", "histories": [["v1", "Fri, 3 Feb 2017 20:31:34 GMT  (141kb,D)", "http://arxiv.org/abs/1702.01147v1", null], ["v2", "Tue, 18 Jul 2017 12:07:45 GMT  (226kb,D)", "http://arxiv.org/abs/1702.01147v2", "Accepted at the Second Conference on Machine Translation (WMT17). This version includes more results regarding target syntax for Romanian-&gt;English and reports fewer results regarding source syntax"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["maria nadejde", "siva reddy", "rico sennrich", "tomasz dwojak", "marcin junczys-dowmunt", "philipp koehn", "alexandra birch"], "accepted": false, "id": "1702.01147"}
