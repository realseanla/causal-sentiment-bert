{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Learning Simpler Language Models with the Differential State Framework", "abstract": "Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks like language modeling. Existing architectures that address the issue are often complex and costly to train. The Delta Recurrent Neural Network (Delta-RNN) framework is a simple and high-performing design that unifies previously proposed gated neural models. The Delta-RNN models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. This requires hardly any more parameters than a classical simple recurrent network. The models outperform popular complex architectures, such as the Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU) and achieve state-of-the art performance in language modeling at character and word levels and yield comparable performance at the subword level.", "histories": [["v1", "Sun, 26 Mar 2017 20:02:44 GMT  (120kb,D)", "http://arxiv.org/abs/1703.08864v1", null], ["v2", "Thu, 1 Jun 2017 21:19:31 GMT  (125kb,D)", "http://arxiv.org/abs/1703.08864v2", "Updated benchmark experimental results (as well as content). Regularized versions of the Delta-RNN described and evaluated"], ["v3", "Mon, 5 Jun 2017 14:40:47 GMT  (124kb,D)", "http://arxiv.org/abs/1703.08864v3", "Now distinguishing framework from its implementation (the Delta-RNN). Updated benchmark experimental results (as well as content). Regularized versions of the Delta-RNN described and evaluated"], ["v4", "Sun, 16 Jul 2017 22:27:29 GMT  (115kb)", "http://arxiv.org/abs/1703.08864v4", "Edits/revisions applied throughout document"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander g ororbia ii", "tomas mikolov", "david reitter"], "accepted": false, "id": "1703.08864"}
