{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2014", "title": "Kaggle LSHTC4 Winning Solution", "abstract": "Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores. This document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package. A package omitting precomputed result files is also distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies.", "histories": [["v1", "Sat, 3 May 2014 01:41:27 GMT  (243kb,D)", "http://arxiv.org/abs/1405.0546v1", "Kaggle LSHTC winning solution description"], ["v2", "Fri, 9 May 2014 04:57:19 GMT  (243kb,D)", "http://arxiv.org/abs/1405.0546v2", "Kaggle LSHTC winning solution description"]], "COMMENTS": "Kaggle LSHTC winning solution description", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["antti puurula", "jesse read", "albert bifet"], "accepted": false, "id": "1405.0546"}
