{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Neural Machine Translation Training in a Multi-Domain Scenario", "abstract": "In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and weighted ensemble. We evaluate these methods based on three criteria: i) translation quality, ii) training time, and iii) robustness towards out-of-domain tests. Our findings on Arabic-English and German-English language pairs show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning.", "histories": [["v1", "Tue, 29 Aug 2017 11:56:41 GMT  (1738kb,D)", "http://arxiv.org/abs/1708.08712v1", "7 pages"], ["v2", "Sun, 3 Sep 2017 13:01:59 GMT  (1739kb,D)", "http://arxiv.org/abs/1708.08712v2", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hassan sajjad", "nadir durrani", "fahim dalvi", "yonatan belinkov", "stephan vogel"], "accepted": false, "id": "1708.08712"}
