{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Can Active Memory Replace Attention?", "abstract": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation.", "histories": [["v1", "Thu, 27 Oct 2016 04:28:29 GMT  (18kb)", "http://arxiv.org/abs/1610.08613v1", null], ["v2", "Tue, 7 Mar 2017 04:04:33 GMT  (18kb)", "http://arxiv.org/abs/1610.08613v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["lukasz kaiser", "samy bengio"], "accepted": true, "id": "1610.08613"}
