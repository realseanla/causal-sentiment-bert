{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2016", "title": "Investigating gated recurrent neural networks for speech synthesis", "abstract": "Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.", "histories": [["v1", "Mon, 11 Jan 2016 17:54:53 GMT  (26kb,D)", "http://arxiv.org/abs/1601.02539v1", "Accepted by ICASSP 2016"]], "COMMENTS": "Accepted by ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["zhizheng wu", "simon king"], "accepted": false, "id": "1601.02539"}
