{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Speech-Based Visual Question Answering", "abstract": "This paper introduces the task of speech-based visual question answering (VQA), that is, to generate an answer given an image and an associated spoken question. Our work is the first study of speech-based VQA with the intention of providing insights for applications such as speech-based virtual assistants. Two methods are studied: an end to end, deep neural network that directly uses audio waveforms as input versus a pipelined approach that performs ASR (Automatic Speech Recognition) on the question, followed by text-based visual question answering. Our main findings are 1) speech-based VQA achieves slightly worse results than the extensively-studied VQA with noise-free text and 2) the end-to-end model is competitive even though it has a simple architecture. Furthermore, we investigate the robustness of both methods by injecting various levels of noise into the spoken question and find speech-based VQA to be tolerant of noise at reasonable levels. The speech dataset, code, and supplementary material will be released to the public.", "histories": [["v1", "Mon, 1 May 2017 10:43:28 GMT  (2640kb,D)", "http://arxiv.org/abs/1705.00464v1", "In review"], ["v2", "Sat, 16 Sep 2017 03:43:20 GMT  (2690kb,D)", "http://arxiv.org/abs/1705.00464v2", null]], "COMMENTS": "In review", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["ted zhang", "dengxin dai", "tinne tuytelaars", "marie-francine moens", "luc van gool"], "accepted": false, "id": "1705.00464"}
