{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Deep Multimodal Semantic Embeddings for Speech and Images", "abstract": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.", "histories": [["v1", "Wed, 11 Nov 2015 21:30:10 GMT  (2105kb,D)", "http://arxiv.org/abs/1511.03690v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["david harwath", "james glass"], "accepted": false, "id": "1511.03690"}
