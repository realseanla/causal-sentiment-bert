{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jul-2016", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "abstract": "Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this work, we present a novel model for Cloze-style reading comprehension tasks, called attention-over-attention reader. Our model aims to place another attention mechanism over the document-level attention, and induces \"attended attention\" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.", "histories": [["v1", "Fri, 15 Jul 2016 09:10:11 GMT  (69kb,D)", "http://arxiv.org/abs/1607.04423v1", "8+1 pages. arXiv admin note: substantial text overlap witharXiv:1607.02250"], ["v2", "Mon, 18 Jul 2016 09:46:02 GMT  (68kb,D)", "http://arxiv.org/abs/1607.04423v2", "8+1 pages. some typos and descriptions fixed, closely related toarXiv:1607.02250"], ["v3", "Thu, 4 Aug 2016 06:17:42 GMT  (72kb,D)", "http://arxiv.org/abs/1607.04423v3", "8+1 pages. fixed errors in Fig.1 and Table.3, closely related toarXiv:1607.02250"], ["v4", "Tue, 6 Jun 2017 02:51:54 GMT  (251kb,D)", "http://arxiv.org/abs/1607.04423v4", "8+2 pages. accepted as a conference paper at ACL2017 (long paper)"]], "COMMENTS": "8+1 pages. arXiv admin note: substantial text overlap witharXiv:1607.02250", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["yiming cui", "zhipeng chen", "si wei", "shijin wang", "ting liu", "guoping hu"], "accepted": true, "id": "1607.04423"}
