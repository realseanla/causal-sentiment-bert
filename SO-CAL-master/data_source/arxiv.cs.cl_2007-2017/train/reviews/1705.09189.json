{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs", "abstract": "We introduce a neural network that represents sentences by composing their words according to induced binary parse trees. We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser. Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees. As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation. We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task.", "histories": [["v1", "Thu, 25 May 2017 14:09:48 GMT  (113kb,D)", "http://arxiv.org/abs/1705.09189v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jean maillard", "stephen clark", "dani yogatama"], "accepted": false, "id": "1705.09189"}
