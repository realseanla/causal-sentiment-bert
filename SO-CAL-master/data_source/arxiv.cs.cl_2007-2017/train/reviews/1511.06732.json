{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Sequence Level Training with Recurrent Neural Networks", "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the BLEU score: a popular metric to compare a sequence to a reference. On three different tasks, our approach outperforms several strong baselines for greedy generation, and it matches their performance with beam search, while being several times faster.", "histories": [["v1", "Fri, 20 Nov 2015 19:25:54 GMT  (1935kb,D)", "http://arxiv.org/abs/1511.06732v1", null], ["v2", "Mon, 14 Dec 2015 16:11:27 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06732v2", null], ["v3", "Tue, 15 Dec 2015 16:51:31 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06732v3", null], ["v4", "Wed, 6 Jan 2016 06:24:58 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v4", null], ["v5", "Fri, 12 Feb 2016 16:05:32 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v5", null], ["v6", "Wed, 4 May 2016 13:43:39 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v6", null], ["v7", "Fri, 6 May 2016 21:18:46 GMT  (1995kb,D)", "http://arxiv.org/abs/1511.06732v7", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["marc'aurelio ranzato", "sumit chopra", "michael auli", "wojciech zaremba"], "accepted": true, "id": "1511.06732"}
