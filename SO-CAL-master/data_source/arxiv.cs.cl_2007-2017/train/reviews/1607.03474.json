{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2016", "title": "Recurrent Highway Networks", "abstract": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with such 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory networks. We introduce a novel theoretical analysis of recurrent networks based on Ger\\v{s}gorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks (RHN), which are long not only in time but also in space, generalizing LSTMs to larger step-to-step depths. Experiments indicate that the proposed architecture results in complex but efficient models, beating previous models for character prediction on the Hutter Prize dataset with less than half of the parameters.", "histories": [["v1", "Tue, 12 Jul 2016 19:36:50 GMT  (126kb,D)", "http://arxiv.org/abs/1607.03474v1", "9 pages, 5 figures. Submitted to NIPS conference 2016"], ["v2", "Thu, 11 Aug 2016 17:07:42 GMT  (136kb,D)", "http://arxiv.org/abs/1607.03474v2", "11 pages, 5 figures, 3 tables. Submitted to NIPS conference 2016"], ["v3", "Thu, 27 Oct 2016 19:39:22 GMT  (133kb,D)", "http://arxiv.org/abs/1607.03474v3", "13 pages, 6 figures, 2 tables"], ["v4", "Fri, 3 Mar 2017 21:10:42 GMT  (145kb,D)", "http://arxiv.org/abs/1607.03474v4", "12 pages, 6 figures, 3 tables"], ["v5", "Tue, 4 Jul 2017 19:29:23 GMT  (145kb,D)", "http://arxiv.org/abs/1607.03474v5", "12 pages, 6 figures, 3 tables"]], "COMMENTS": "9 pages, 5 figures. Submitted to NIPS conference 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["julian georg zilly", "rupesh kumar srivastava", "jan koutn\u00edk", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1607.03474"}
