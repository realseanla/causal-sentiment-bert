{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Language Models with Pre-Trained (GloVe) Word Embeddings", "abstract": "In this work we implement a training of a Language Model (LM), using Recurrent Neural Network (RNN) and GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2], but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].", "histories": [["v1", "Wed, 12 Oct 2016 15:53:02 GMT  (115kb,D)", "https://arxiv.org/abs/1610.03759v1", null], ["v2", "Sun, 5 Feb 2017 11:24:05 GMT  (119kb,D)", "http://arxiv.org/abs/1610.03759v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["victor makarenkov", "bracha shapira", "lior rokach"], "accepted": false, "id": "1610.03759"}
