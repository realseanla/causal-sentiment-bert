{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Training RNNs as Fast as CNNs", "abstract": "Recurrent neural networks scale poorly due to the intrinsic difficulty in parallelizing their state computations. For instance, the forward pass computation of $h_t$ is blocked until the entire computation of $h_{t-1}$ finishes, which is a major bottleneck for parallel computing. In this work, we propose an alternative RNN implementation by deliberately simplifying the state computation and exposing more parallelism. The proposed recurrent unit operates as fast as a convolutional layer and 5-10x faster than cuDNN-optimized LSTM. We demonstrate the unit's effectiveness across a wide range of applications including classification, question answering, language modeling, translation and speech recognition. We open source our implementation in PyTorch and CNTK.", "histories": [["v1", "Fri, 8 Sep 2017 16:02:30 GMT  (129kb,D)", "http://arxiv.org/abs/1709.02755v1", null], ["v2", "Tue, 12 Sep 2017 20:13:56 GMT  (130kb,D)", "http://arxiv.org/abs/1709.02755v2", "address related work in the new version"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["tao lei", "yu zhang"], "accepted": false, "id": "1709.02755"}
