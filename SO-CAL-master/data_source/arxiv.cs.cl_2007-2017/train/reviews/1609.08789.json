{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Memory Visualization for Gated Recurrent Neural Networks in Speech Recognition", "abstract": "Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks.", "histories": [["v1", "Wed, 28 Sep 2016 06:26:16 GMT  (995kb,D)", "http://arxiv.org/abs/1609.08789v1", "Submitted to ICASSP 2017"], ["v2", "Mon, 26 Dec 2016 09:25:14 GMT  (995kb,D)", "http://arxiv.org/abs/1609.08789v2", "ICASSP 2017"], ["v3", "Mon, 27 Feb 2017 02:07:34 GMT  (2135kb,D)", "http://arxiv.org/abs/1609.08789v3", "ICASSP 2017"]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["zhiyuan tang", "ying shi", "dong wang", "yang feng", "shiyue zhang"], "accepted": false, "id": "1609.08789"}
