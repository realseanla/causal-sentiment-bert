{
  "name" : "1702.05821.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Post-edit Analysis of Collective Biography Generation",
    "authors" : [ "Bo Han", "Will Radford", "Anaïs Cadilhac", "Art Harol", "Andrew Chisholm", "Ben Hachey" ],
    "emails" : [ "bhan@hugo.ai", "wradford@hugo.ai", "acadilhac@hugo.ai", "aharol@hugo.ai", "achisholm@hugo.ai", "bhachey@hugo.ai", "@id_aa_carmack", "@inc" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords Text Generation, Collective Intelligence, Evaluation"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Natural language generation applications often use human postediting to ensure quality of final output. Understanding the kind and amount of manual effort is critical to prioritising how generation can be improved and what editorial guidelines should be implemented. Recent work also suggests that some post-editing can be automated, with substantial improvements in a machine translation shared task (+5.5 BLEU score) [2].\nPost-edit analysis also has a long history in the evaluation of text generation tasks, in particular machine translation [1, 3]. The closest work to ours [4] is a post-edit evaluation of 2,728 pairs of system-edited texts from a system for generating weather forecasts, reporting a range of edits including individual style preferences as well as corrections. We build on this work to perform a post-edit analysis complementary to existing evaluation metrics like ROUGE and BLEU. We introduce a visualisation that helps identify and prioritise improvements, and present a case study using data from a commercial biography generation system, reporting a detailed analysis of edit flow and characteristic edit actions.\nc©2017 International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. WWW’17 Companion, April 3–7, 2017, Perth, Australia. ACM 978-1-4503-4914-7/17/04. http://dx.doi.org/10.1145/3041021.3054264\n."
    }, {
      "heading" : "2. SYSTEM OVERVIEW",
      "text" : "Our case study evaluates an collective biography generation system that is part of a larger commercial tool for finding and summarising information about a person on the web. Input comprises facts and events derived from social and news sources (e.g., name, affiliations, education, skills, investments). Facts and events are obtained from a search tool, which guides internal crowd users through the search process to select a range of relevant and diverse sources. The core generation capability uses 26 templates to produce an average of 2.8 suggested sentences per biography, e.g.: Suggested: Philip has been the Co-Founder at High Fidelity, Inc. since January 2013 and an Investor at Milk, Sunglass.io, Akili Interactive, Crowdfunder and more, and specializes in virtual worlds, start-ups and software development. He received a BS degree in Physics from The University of California, San Diego in 1992. Philip frequently tweets the hashtags #vr, #highfidelity and #avatars. Philip recently interacted with @id_aa_carmack and @inc on Twitter. Philip’s favorite movie is Meditate and Destroy. Philip is a member of Facebook groups “Virtual Blogging”, “San Francisco Bay Area Free Yoga and Meditation Events” and “BVHS Class of 1986”.\nInternal crowd users select, edit and add suggested sentences, e.g.: Selected: Philip has been the Co-Founder at High Fidelity, Inc. since January 2013 and an Investor at Milk, Sunglass.io, Akili Interactive, Crowdfunder and more, and specializes in virtual worlds, start-ups and software development. Edited: He ‘received’7→‘holds’ a BS degree in Physics from The University of California, San Diego in 1992. Selected: Philip frequently tweets the hashtags #vr, #highfidelity and #avatars. Not Selected: Philip recently interacted with @id_aa_carmack and @inc on Twitter. Not Selected: Philip’s favorite movie is Meditate and Destroy. Philip is a member of Facebook groups “Virtual Blogging”, “San Francisco Bay Area Free Yoga and Meditation Events” and “BVHS Class of 1986”. New: He is married to Yvette Forte Rosedale. New: In 2007, Philip was listed in Time Magazine’s 100 Most Influential People in The World.\nThe result is a concise and informative text for end users, e.g.: Final: Philip has been the Co-Founder at High Fidelity, Inc. since January 2013 and an Investor at Milk, Sunglass.io, Akili Interactive, Crowdfunder and more, and specializes in virtual worlds, start-ups and software development. He received a BS degree in Physics from The University of California, San Diego in 1992. Philip frequently tweets the hashtags #vr, #highfidelity and #avatars. He is married to Yvette Forte Rosedale. In 2007, Philip was listed in Time Magazine’s 100 Most Influential People in The World.\nThe goal of the analysis here is twofold: (1) identify areas for improving system coverage and precision to focus human effort on high-value cognitively demanding tasks and improve end-toend efficiency; (2) identify strategies for improving consistency and quality of biographies produced from noisy web data. The analysis uses 10,320 final biographies for which we also have the original suggested sentences from the generation system. This allows a unique and detailed characterisation of collective biography generation, tracking the editorial process from automatically generated sentences to final sentences after human post-editing.\nar X\niv :1\n70 2.\n05 82\n1v 1\n[ cs\n.C L\n] 2\n0 Fe\nb 20\n17"
    }, {
      "heading" : "3. VISUALISING EDIT FLOW",
      "text" : "Sentence alignment We first align suggested with final sentences using a minimum Levenshtein ratio of 0.8, defined as (l(a+b) − d(a, b))/l(a+b) where l(a+b) is the combined length of the input strings and d(a, b) is the Levenshtein edit distance between the input strings. Suggested biographies have a total 28,842 sentences and 525,555 tokens, while final biographies have a total of 33,238 sentences and 580,521 tokens. So, final biographies tend to have more sentences (3.2 versus 2.8), but each sentence is shorter on average (17.5 versus 18.2).\nEdit flow Figure 1 visualises the flow of tokens from suggestions through to the final biography using a Sankey diagram. The total number of tokens in suggested sentences breaks down into two categories: 231,347 (44%) are selected by users and 220,793 (56%) are not selected. Looking at tokens in final sentences: 349,793 (60%) are new sentences written by human editors. This suggests that work on improving end-to-end efficiency should prioritise extending fact collection and generation to provide more useful suggestions (more detail in Section 4). Trimming unused suggestions is less valuable as ignoring these is relatively easy.\nToken alignment We refine the edit flow analysis by identifying phrase substitutions within the selected sentences aligned in the first step. We leverage the standard diff library in Python to collect token-level edit operations. This provides more detailed phraselevel substitution, deletion and insertion statistics corresponding to the middle of the edit flow diagram. Interestingly, we find that the vast majority (95%) of tokens in aligned sentences are copied directly from selected to final sentences. Phrase edits collectively represent only 5% of tokens in selected sentences, suggesting that generation obtains high accuracy where facts and templates are available."
    }, {
      "heading" : "4. CHARACTERISING EDITS",
      "text" : "Selected From token alignments in selected sentences, we observe a total of 1,833 substitution, 621 deletion and 468 insertion phrase pairs and 4,568, 1,343 and 1,284 instances respectively. Overall, selection edits tend to result in shorter sentences with an average of 1.8 tokens before editing compared to 1.6 post-editing. Table 1 lists some of the most common edit actions. We observe that the top edits are generally grammatical (e.g., ‘the Engineer’7→‘an Engineer’) or stylistic (e.g., ‘received a BFA’ 7→‘holds a BFA’). Other edits address capitalisation consistency or errors from collected facts (e.g., ‘The University’7→‘the University’, ‘microsoft office’7→‘Microsoft Office’).\nNot Selected Ignoring the sentence alignment threshold, we find that 67% of suggested sentences that were not selected have some overlap with sentences in the final biography. The remaining 33% were not directly useful for their biographies. By inspection of a random 100 of these, we observe that most talk about hobbies (e.g., interests, travel, books), social media interactions, or a monolingual person’s sole language.\nNew Sentences Looking from the other side, we find that 53% of new sentences have some overlap with suggestions. These comprise sentences where editors deleted/introduced many tokens, or split/merged suggested sentences. The remaining 47% of new sentences are due primarily to coverage issues in search or fact collection feeding into generation."
    }, {
      "heading" : "5. DISCUSSION",
      "text" : "We introduced an edit flow visualisation for analysis of human post-editing of automatically generated text. This helps to identify and prioritise work for improving end-to-end efficiency. It is encouraging that most new sentences include suggested content, but these sentences still represent significant human cost. This suggests that end-to-end efficiency can be improved most by expanding generation templates to provide editors more style and content options. The next priority is then to automate post-edits where possible, learnt from data here or external resources. Another issue discovered during analysis is the lack of clear editorial policy, with users sometimes making conflicting edits. Editorial style policy is challenging to implement with crowd-sourced editing, but we expect that gradual introduction of high-precision rules should actively guide editors toward a more consistent style. In current work, we are implementing the changes suggested above, developing a virtuous circle to improve both efficiency and quality over time."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] W. Aziz, S. Castilho, and L. Specia. PET: a tool for\npost-editing and assessing machine translation. In LREC, pages 3982–3987, 2012.\n[2] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow, M. Huck, A. Jimeno Yepes, P. Koehn, V. Logacheva, C. Monz, M. Negri, A. Neveol, M. Neves, M. Popel, M. Post, R. Rubino, C. Scarton, L. Specia, M. Turchi, K. Verspoor, and M. Zampieri. Findings of the 2016 conference on machine translation. In WMT, pages 131–198, 2016.\n[3] C. Scarton. Discourse and document-level information for evaluating language output tasks. In NAACL Student Research Workshop, pages 118–125, 2015.\n[4] S. G. Sripada, E. Reiter, and L. Hawizy. Evaluating an NLG system using post-editing. In IJCAI, pages 1700–1701, 2005."
    } ],
    "references" : [ {
      "title" : "PET: a tool for post-editing and assessing machine translation",
      "author" : [ "W. Aziz", "S. Castilho", "L. Specia" ],
      "venue" : "LREC, pages 3982–3987",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Findings of the 2016 conference on machine translation",
      "author" : [ "O. Bojar", "R. Chatterjee", "C. Federmann", "Y. Graham", "B. Haddow", "M. Huck", "A. Jimeno Yepes", "P. Koehn", "V. Logacheva", "C. Monz", "M. Negri", "A. Neveol", "M. Neves", "M. Popel", "M. Post", "R. Rubino", "C. Scarton", "L. Specia", "M. Turchi", "K. Verspoor", "M. Zampieri" ],
      "venue" : "WMT, pages 131–198",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Discourse and document-level information for evaluating language output tasks",
      "author" : [ "C. Scarton" ],
      "venue" : "NAACL Student Research Workshop, pages 118–125",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Evaluating an NLG system using post-editing",
      "author" : [ "S.G. Sripada", "E. Reiter", "L. Hawizy" ],
      "venue" : "IJCAI, pages 1700–1701",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "5 BLEU score) [2].",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "Post-edit analysis also has a long history in the evaluation of text generation tasks, in particular machine translation [1, 3].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "Post-edit analysis also has a long history in the evaluation of text generation tasks, in particular machine translation [1, 3].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : "The closest work to ours [4] is a post-edit evaluation of 2,728 pairs of system-edited texts from a system for generating weather forecasts, reporting a range of edits including individual style preferences as well as corrections.",
      "startOffset" : 25,
      "endOffset" : 28
    } ],
    "year" : 2017,
    "abstractText" : "Text generation is increasingly common but often requires manual post-editing where high precision is critical to end users. However, manual editing is expensive so we want to ensure this effort is focused on high-value tasks. And we want to maintain stylistic consistency, a particular challenge in crowd settings. We present a case study, analysing human post-editing in the context of a templatebased biography generation system. An edit flow visualisation combined with manual characterisation of edits helps identify and prioritise work for improving end-to-end efficiency and accuracy.",
    "creator" : "LaTeX with hyperref package"
  }
}