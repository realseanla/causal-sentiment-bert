{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2015", "title": "End-To-End Memory Networks", "abstract": "In this paper we introduce a variant of Memory Networks that needs significantly less supervision to perform question and answering tasks. The original model requires that the sentences supporting the answer be explicitly indicated during training. In contrast, our approach only requires the answer to the question during training. We apply the model to the synthetic bAbI tasks, showing that our approach is competitive with the supervised approach, particularly when trained on a sufficiently large amount of data. Furthermore, it decisively beats other weakly supervised approaches based on LSTMs. The approach is quite general and can potentially be applied to many other tasks that require capturing long-term dependencies.", "histories": [["v1", "Tue, 31 Mar 2015 03:05:37 GMT  (119kb,D)", "http://arxiv.org/abs/1503.08895v1", null], ["v2", "Fri, 3 Apr 2015 02:23:20 GMT  (120kb,D)", "http://arxiv.org/abs/1503.08895v2", null], ["v3", "Sun, 12 Apr 2015 04:19:33 GMT  (120kb,D)", "http://arxiv.org/abs/1503.08895v3", null], ["v4", "Mon, 8 Jun 2015 21:42:20 GMT  (331kb,D)", "http://arxiv.org/abs/1503.08895v4", null], ["v5", "Tue, 24 Nov 2015 19:41:57 GMT  (332kb,D)", "http://arxiv.org/abs/1503.08895v5", "Accepted to NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["sainbayar sukhbaatar", "arthur szlam", "jason weston", "rob fergus"], "accepted": true, "id": "1503.08895"}
