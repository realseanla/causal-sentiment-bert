{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Weakly Supervised PLDA Training", "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labelled development data, which is highly expensive in most cases. We present a cheap PLDA training approach, which assumes that speakers in the same session can be easily separated, and speakers in different sessions are simply different. This results in `weak labels' which are not fully accurate but cheap, leading to a weak PLDA training.", "histories": [["v1", "Tue, 27 Sep 2016 13:46:55 GMT  (311kb,D)", "http://arxiv.org/abs/1609.08441v1", "Submitted to ICASSP 2017"], ["v2", "Tue, 23 May 2017 10:19:15 GMT  (693kb,D)", "http://arxiv.org/abs/1609.08441v2", null]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.SD", "authors": ["lantian li", "yixiang chen", "dong wang", "chenghui zhao"], "accepted": false, "id": "1609.08441"}
