{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "The word entropy of natural languages", "abstract": "The average uncertainty associated with words is an information-theoretic concept at the heart of quantitative and computational linguistics. The entropy has been established as a measure of this average uncertainty - also called average information content. We here use parallel texts of 21 languages to establish the number of tokens at which word entropies converge to stable values. These convergence points are then used to select texts from a massively parallel corpus, and to estimate word entropies across more than 1000 languages. Our results help to establish quantitative language comparisons, to understand the performance of multilingual translation systems, and to normalize semantic similarity measures.", "histories": [["v1", "Wed, 22 Jun 2016 16:00:52 GMT  (141kb,D)", "http://arxiv.org/abs/1606.06996v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christian bentz", "dimitrios alikaniotis"], "accepted": false, "id": "1606.06996"}
