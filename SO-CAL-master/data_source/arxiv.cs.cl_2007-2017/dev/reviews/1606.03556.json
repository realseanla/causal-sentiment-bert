{"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2016", "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?", "abstract": "We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). We find that depending on the implementation used, machine-generated attention maps are either \\emph{negatively correlated} with human attention or have positive correlation worse than task-independent saliency. Overall, our experiments paint a bleak picture for the current generation of attention models in VQA.", "histories": [["v1", "Sat, 11 Jun 2016 05:41:10 GMT  (8091kb,D)", "http://arxiv.org/abs/1606.03556v1", "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016"], ["v2", "Fri, 17 Jun 2016 04:39:01 GMT  (9100kb,D)", "http://arxiv.org/abs/1606.03556v2", "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016"]], "COMMENTS": "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["abhishek das", "harsh agrawal", "c lawrence zitnick", "devi parikh", "dhruv batra"], "accepted": true, "id": "1606.03556"}
