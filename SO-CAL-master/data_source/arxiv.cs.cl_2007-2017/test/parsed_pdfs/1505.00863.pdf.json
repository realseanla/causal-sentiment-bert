{
  "name" : "1505.00863.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Feature-based Classification Technique for Answering Multi-choice World History Questions: FRDC_QA at NTCIR-11 QA-Lab Task",
    "authors" : [ "Shuangyong Song", "Yao Meng", "Zhongguang Zheng", "Jun Sun", "Huan Zhong" ],
    "emails" : [ "sunjun}@cn.fujitsu.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.\nTeam Name FRDC_QA\nSubtask QA-Lab English Subtask\nKeywords Question Answering, The National Center Test for University Admissions, World History, Feature Extraction, Classification Model."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Question Answering (QA) is a specialized area in Information Retrieval. QA systems are concerned with providing relevant answers in response to questions proposed in natural language. QA is therefore composed of three distinct modules: question classification, information retrieval, and answer extraction, each of which has a core component beside other supplementary components [7]. Question classification plays an essential role in QA systems by classifying the submitted question according to its type.\nIn particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10]. NTCIR-11 QALab task aims to provide a module-based platform for system performance evaluations and comparisons for solving real-world university entrance exam questions, which are selected from The National Center Test for University Admissions and from secondary exams at 4 universities in Japan.\nFRDC_QA take part in the English subtask. We design a system and the details of it are given as follows: In section 2, we introduce the external resource and some convenient storage ways. The framework for questions with multiple sentence\nchoices is proposed in section 3. In section 4, the frameworks for other types of questions are described. The evaluation results of our system on world history exam B in 2007 Japan University Admissions are given in section 5. Finally we make a conclusion and discuss our plans for future work in section 6."
    }, {
      "heading" : "2. HASH MAP & LUCENE INDEXES OF EXTERNAL RESOURCE",
      "text" : ""
    }, {
      "heading" : "2.1 External Resource",
      "text" : "We utilize Wikipedia as external resource for our QA-Lab task. Wikipedia is a well-known free content, multilingual encyclopedia written collaboratively by contributors around the world [6]. In this paper, for the English subtask, we download the Wikipedia dataset with the version of ‘enwiki dump progress on 20140502’ from Wikimedia Downloads 1 . Downloaded dataset contains ‘enwiki-20140502-all-titles’ as the list of all the Wiki-items, and ‘enwiki-20140502-pages-articles’ with articles of all the Wiki-items. All those data will be processed to be more formal and then be stored in hash map or Lucene2 indexes, for realizing convenient and quick search in our QA system. The details are given below."
    }, {
      "heading" : "2.2 Hash Map of Item Title",
      "text" : "For quickly checking if a word or word group is a Wikipedia item, we put all Wikipedia item titles into a hash map. The title list dataset contains 32,877,103 titles of Wikipedia items in total, and we convert all characters of them to be lowercase. Word or word groups will also be converted to be characters in lowercase when they are checked, for realizing an exact matching. When we detect items contained in a sentence, we adopt a Maximum Matching Method. For example, for a sentence with N words, we first check if this whole sentence is a Wiki item, and then check all sub-sentences with N-1 continuous words, then subsentences with length of N-2, and so on. In particular, if a detected item consist of another detected item, the latter one will be removed and the longer one will be reserved."
    }, {
      "heading" : "2.3 Lucene Index of Item Page",
      "text" : "Each item has its related Wiki article, describing the details of this item. We put the title and page content into a Lucene index file as two word string fields, and then we can easily get the description of a Wiki item with simple Lucene search. This index file is used to search the relationship between items, since two related items will show in the Wiki article of each other, vice versa.\n1 http://download.wikipedia.com/enwiki/20140502 2 http://lucene.apache.org"
    }, {
      "heading" : "2.4 Lucene Index of Item Redirection",
      "text" : "Different Wiki items may have same meanings, such as ‘AccessibleComputing’ and ‘Computer accessibility’. For those items with same meanings, Wikipedia utilize ‘redirection’ tag to link one of them to another. Therefore, just one of them has a Wiki article with detailed description, and the Wiki article of another item just contains one sentence with redirection declaration. Take the ‘AccessibleComputing’ and ‘Computer accessibility’ for example, the Wiki article of ‘Computer accessibility’ contains detailed description of this item, but the Wiki article of ‘AccessibleComputing’ just contains one sentence “Redirect page  Computer accessibility”. We put those ‘AccessibleComputing’ like redirected Wiki item titles into a Lucene index file as one word string field, and take the related item titles as another word string field, then we can easily search the real description of those redirected Wiki items."
    }, {
      "heading" : "2.5 Lucene Index of Item Time",
      "text" : "For answering some questions about items’ occurrence time, we extract the time information of each item from the Wiki articles and put those information into a Lucene index. There are two different types of time information we should extract, one is the exact time of this item, and another is the period of this item. Such as the time of ‘Independence Day (United States)’ is ‘The Fourth of July’, and the period of ‘French and Indian War’ is ‘1754–1763’. For the period type, we respectively record the front part as ‘start time’ and latter part as ‘end time’ since lots of questions may ask them separately."
    }, {
      "heading" : "3. FRAMEWORK FOR QUESTIONS WITH MULTIPLE SENTENCE CHOICES",
      "text" : ""
    }, {
      "heading" : "3.1 Brief Description of Framework",
      "text" : "We first introduce the type of questions with multiple sentence choices, and an example is given below (italic characters):\nBackground text: … … (8) In India in the latter half of the 19th century, a large-scale rebellion against colonial rule took place; one of the things that triggered this was the fact that Muslim soldiers revolted due to a rumor that pork fat had been used on the cartridges in their guns. … …\nQuestion: From 1-4 below, choose the most appropriate sentence concerning the underlined portion (8).\nChoices: 1. This rebellion is also called the Sipahi (Sepoy) Mutiny. 2. The Ever Victorious Army was actively involved in suppressing\nthis rebellion.\n3. The Ever Victorious Army was actively involved in suppressing\nthis rebellion.\n4. After this rebellion, Queen Victoria also became the empress of\nthe Mughal Empire.\nWe take this type of questions as accuracy probability ranking problem for all the choices, and we utilize classification models to handle this problem. By separating right choices and wrong choices in the training dataset, binary classification models can be trained. Seven features, such as semantic relationship between background text and choices, semantic relationship between question and choices, etc., are extracted for training classification models, and eleven classifiers are selected to calculate the accuracy probability of each choice. Then the results of those classifiers are combined together to get the final ranking of choices. Details of this framework are given in following subsections."
    }, {
      "heading" : "3.2 Features",
      "text" : "We extracted seven features of each choice in total for classifiers, and the details of them are given below:\n1) Internal Item Relativity:\nWe first detect all items contained in a choice sentence with the Maximum Matching Method described in section 2.2, and then detect relationships among those items. The method for judging if or not two items are related is detecting if an item shows in the Wiki article of another item. For an choice sentence consisting of N items, we can get N(N-1)/2 ‘item couple’, and each related ‘item couple’ will contribute 1 point to this feature. Therefore, the value of this feature ‘Internal Item Relativity’ will be from 0 to N(N-1)/2.\n2) Item Relativity between Text and Choice:\nAll the items contained in the ‘text portion’ and choice sentence need to be detected first, and relationships between Text items and Choice items will be detected. For a text sentence consisting of M items and a choice sentence consisting of N items, we can get M*N ‘item couple’, and each related ‘item couple’ will contribute 1 point to this feature. Therefore, the value of this feature ‘Item Relativity between Text and Choice’ will be from 0 to M*N.\n3) Item Relativity between Question and Choice:\nAll the items contained in the question sentence and choice sentence need to be detected first, and relationships between Question items and Choice items will be detected. For a question sentence consisting of Q items and a choice sentence consisting of N items, we can get Q*N ‘item couple’, and each related ‘item couple’ will contribute 1 point to this feature. Therefore, the value of this feature ‘Item Relativity between Question and Choice’ will be from 0 to Q*N.\n4) Minimum Distance with Negative Sentences:\nWe assume that one choice ‘more similar with a negative sentence, more likely to be a wrong answer’. For getting this feature, we firstly need to exact all the negative sentences in Wiki articles, which contains ‘is not’, ‘are not’, ‘did not’ or other negative expressions. After removing stop words in choice sentences and those negative sentences, all of them can be represented as word vectors. Distance between two word vectors V1 and V2 is calculated with the formula below:\n1 2( ) 2\n1 2 1 2\n1\n( , ) ( ) L V V\ni i\ni D V V w w \n  (1)\nin which, the D(V1,V2) means distance between vector V1 and vector V2, and the V1∪V2 means the union of V1 and V2, and the L(V1∪V2) means the length of V1∪V2, and the w1i means the value of V1 on the ith dimension of V1∪V2, and the w2i means the value of V2 on the ith dimension of V1∪V2. This formula is modified from the Euclidean Distance [2], without firstly creating the vector of words in all Wiki articles and choices, which is very time consuming and with low robustness.\n5) Number of Related Wiki Articles:\nWith the ‘Lucene Index of Item Page’ described in section 2.3, we take a choice as a query and search all the possible related Wiki articles from this index file. The number of returned Wiki articles will be taken as the value of this feature.\n6) Similarity with Top 1 Related Wiki Article:\nThe search method is same as the above feature, but the value of this feature is the value of the semantic similarity between the choice sentence and the top 1 returned Wiki article, which is very easy to get with a ready-made function in Lucene system.\n7) Similarity with Top 3 Related Wiki Articles:\nThe search method is same as the above feature, but the value of this feature is the average value of the semantic similarity between the choice sentence and the top 3 returned Wiki articles."
    }, {
      "heading" : "3.3 Classifiers",
      "text" : "In our system, we in total utilize eleven classifiers to training different classification models respectively. Simple description of them are given below:\nRandom Forest: Random forest is an ensemble learning method for classification that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees [3].\nLogitBoost: LogitBoost is a boosting algorithm that casts the AdaBoost algorithm into a statistical framework. Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm [4].\nLogistic Model Trees: Logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning [11].\nAdaBoost M1: AdaBoost M1 is an improved version of traditional AdaBoost algorithm, which can be used to classify both binary and polynominal label with numerical, binominal and polynominal (and weighted) attributes [12].\nBagging: Bagging, also called bootstrap aggregating, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid over-fitting [13].\nMultiBoostAB: MultiBoosting is an extension to the highly successful AdaBoost technique for forming decision committees. MultiBoosting can be viewed as combining AdaBoost with wagging. It is able to harness both AdaBoost's high bias and variance reduction with wagging's superior variance reduction [14].\nLocally Weighted Learning: Locally weighted learning uses an instance-based algorithm to assign instance weights which are then used by a specified Weighted Instances Handler. Can do classification (e.g. using naive Bayes) or regression (e.g. using linear regression) [15].\nLogistic Regression: Logistic regression is a probabilistic statistical classification model, which can be used to predict a binary response from a binary predictor, used for predicting the outcome of a categorical dependent variable based on one or more predictor variables. [16].\nSimple Naïve Bayes: Naive Bayes classifier, in which the numeric attributes are modelled by a normal distribution [17].\nNaïve Bayes: An improved Naive Bayes classifier using estimator classes. Numeric estimator precision values are chosen based on analysis of the training data [18].\nUpdateable Naïve Bayes: An updateable version of Naïve Bayes model. This classifier will use a default precision of 0.1 for numeric attributes when build Classifier is called with zero training instances [18].\nAll of the classification model training procedures are realized with WEKA [5], which is a collection of machine learning algorithms for data mining tasks and contains tools for data preprocessing, classification, regression, clustering, association rules, and visualization."
    }, {
      "heading" : "3.4 Choice Selection",
      "text" : "Each classifier can get an accuracy probability for each choice, and the average value of the accuracy probability from all classifiers will be taken as the final accuracy probability of a choice. Then, if the question is asking us to choose the right choice with the keywords ‘correct’, ‘correctly’ or ‘appropriate’, we choose the choice with highest accuracy probability as the final answer. If the question is asking us to choose the wrong choice with the keywords ‘incorrect’, ‘incorrectly’ or ‘mistake’, we choose the choice with lowest accuracy probability as the final answer."
    }, {
      "heading" : "4. FRAMEWORKS FOR OTHER TYPES",
      "text" : ""
    }, {
      "heading" : "OF QUESTIONS",
      "text" : "In this section, we give some description of our frameworks for problems besides the type with multiple sentence choices, such as questions with chronological sequence choices, questions with term choices, etc."
    }, {
      "heading" : "4.1 Framework for Questions with Chronological Sequence Choices (without images)",
      "text" : "We first give an example of this type of question (italic characters):\nBackground text: … … (7) A Cold War began between the US and the Soviet Union, and the world faced another serious conflict. In relation to this … …\nQuestion: In regard to the underlined portion (7), from (1)-(4) below, choose the correct chronological sequence of events relating to the Cold War.\nChoices: 1. Warsaw Treaty Organization formed - Berlin blockade - Cuban\nmissile crisis - Japan-US Security Treaty signed (1951)\n2. Berlin blockade - Japan-US Security Treaty signed (1951) -\nWarsaw Treaty Organization formed - Cuban missile crisis\n3. Japan-US Security Treaty signed (1951) - Cuban missile crisis -\nBerlin blockade - Warsaw Treaty Organization formed\n4. Berlin blockade - Warsaw Treaty Organization formed - Japan-\nUS Security Treaty signed (1951) - Cuban missile crisis\nFor this type of questions, we utilize the ‘Lucene Index of Item Time’ to search timestamp of each event in the choices, and rank them with the chronological order, then we can choose the right answer according to this order easily."
    }, {
      "heading" : "4.2 Framework for Questions with Term Choices (without images)",
      "text" : "An example of this type of question is given below (italic characters):\nBackground text: … … (1) Nomadic tribes on horseback emerged on the Eurasian continent. Their elusive character became a major threat to sedentary agricultural societies, so troops mounted on horseback were organized to counteract them. Rulers who sought good horses also emerged, such as … …\nQuestion: In regard to the underlined portion (1), from 1-4 below, choose the one name that correctly describes the nomadic tribe on horseback that came to prominence in the 6th century and built up a nation.\nChoices: 1. Scythians 2. Göktürks 3. Yuezhi 4. Xiongnu\nWe detect items contained in the background text and the question with the Maximum Matching Method, then calculate the relativity between those items and the choice item, with using the same method described in section 3.2. Finally, the choice with highest relativity with the background text and the question will be chosen as the final answer."
    }, {
      "heading" : "4.3 Framework for Questions with Judging True or False Sentences (without images)",
      "text" : "An example of this type of question is given below (italic characters):\nBackground text: … … (3) founder of the kingdom - is believed to be the Chumo who appears in the \"Book of Wei (Weishu)\", which is a record of the Northern Wei dynasty… …\nQuestion: In regard to the underlined portion (3), from 1-4 below, choose the correct combination of \"correct\" and \"incorrect\" in regard to the following sentences taa and b concerning the historic founder of the kingdom.\nQuestion text: a Liu Bang defeated Xiang Yu and made Chang'an the capital. b Yelü Dashi built the Kara-Khitan Khanate.\nChoices: 1. a - Correct b - Correct 2. a - Correct b - Incorrect 3. a - Incorrect b - Correct 4. a - Incorrect b - Incorrect\nWe use the same training data with same features as described in section 3.2 to train Support Vector Machine classification model (SVM) to handle this type of questions by directly output the ‘true of false’ result of each choice instead of the accuracy probability. Then we can easily choose the right choice according to the output of the SVM model."
    }, {
      "heading" : "4.4 Framework for Other types of Questions",
      "text" : "We choose the final answer with the random selection method for other types of questions, which usually need image analysis technology. In particular, we set a specified random seed to keep the stability of the results given by our system."
    }, {
      "heading" : "5. EVALUATION RESULTS",
      "text" : "Table 1 gives the evaluation results of our system on the phase 1 contest data - world history exam B in 2007 Japan University Admissions.\nFor types ‘Questions with multiple sentence choices’ and ‘Questions with term choices (without images)’, we achieve a precision of about 45% on both ‘Number of correct answer’ and ‘Score of correct answer’, which shows the much better effectiveness than random method, since we think the precision of random method should be 25% on four-choice questions. However, the real result of random method on ‘other types of questions’ is not as good as our thought. We got wrong answers on all the seven ‘other types of questions’ with the random method, which makes our total result getting a precision of 37%, far below the 45%."
    }, {
      "heading" : "6. CONCLUSIONS AND FUTURE WORK",
      "text" : "In our work of NTCIR-11 QA-Lab task, we design a system for solving real-world university entrance exam questions, which are related to world history. We utilize Wikipedia as the main external resource for our system, since nearly all of world history knowledge can be found in Wikipedia. In addition, we design different solution frameworks for different types of questions, such as questions with multiple sentence choices, questions with temporal term choices, questions with nontemporal term choices, etc. Although our system performs much better than random methods, it is still far from meeting actual demand. Several attempts can be tried to improve the system performance in our future work, e.g., (1) more useful external resources can be utilized, such as query results from Google like search engines, electronic history books, etc. (2) more reasonable and intelligent combination way for different classification models should be tried; (3) different writing styles for timestamps, locations and personal names should be considered. Furthermore, a unified domain insensitive system for choosing wrong/right answer from multiple sentence choice will be a trial in our future work."
    }, {
      "heading" : "7. ACKNOWLEDGMENTS",
      "text" : "We sincerely thank our colleagues in Fujitsu Laboratories Ltd., Takuya Makino, Hiroko Suzuki, Tomoya Iwakura and Tetsuro Takahashi, for their help on providing us external training dataset and valuable discussions with us about feature extraction and machine learning methods."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Kano, Y. 2014. Solving History Problems of the National\nCenter Test for University Admissions. In Proceedings of\nthe 28th Annual Conference of the Japanese Society for Artificial Intelligence.\n[2] Song, S., Li, Q. and Bao, H. 2012. Detecting Dynamic\nAssociation among Twitter Topics. In Proceedings of the 21st International World Wide Web Conference (Lyon, France, April 16-20, 2012), pages 605-606.\n[3] Breiman, L. 2001. Random forests. Machine Learning.\n45(1): 5–32.\n[4] Friedman, J., Hastie, T. and Tibshirani, R. 2000. Additive\nlogistic regression: a statistical view of boosting. Annals of Statistics. 28(2): 337–407.\n[5] Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann,\nP., Witten, I. H. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations. Volume 11, Issue 1.\n[6] Bhole, A., Fortuna, B., Grobelnik, M. and Mladenić, D.\n2007. Extracting Named Entities and Relating Them over Time Based on Wikipedia. Informatica (Slovenia). 31(4): 463-468.\n[7] Allam, A. M. N., Haggag, M. H. 2012. The Question\nAnswering Systems: A Survey. International Journal of Research and Reviews in Information Sciences. Vol. 2, No. 3, September 2012.\n[8] Martin R, F., Sibyl, H., Veronika, K. 2005. Answering\nmultiple-choice questions in high-stakes medical examinations. Medical Education, Vol. 39, No. 9, September 2005, pp. 890-894.\n[9] Awadallah, R., Rauber, A. 2006. Web-Based multiple\nchoice question answering for english and arabic questions, In Proceeding of the 28th European conference on Advances in Information Retrieval, pp. 515-518.\n[10] Sorger, B., Dahmen, B., Reithler, J., Gosseries, O.,\nMaudoux, A., Laureys, S., Goebel, R. 2009. Another kind of ‘BOLD Response’: answering multiple-choice questions via online decoded single-trial brain signals. Progress in Brain Research, Vol. 177, 2009, pp. 275–292.\n[11] Landwehr, N., Hall, M. A., Frank, E. 2003. Logistic Model\nTrees. In Proceeding of the 14th European Conference on Machine Learning. pp. 241-252.\n[12] Eibl, G., Pfeiffer, K. P. 2002. How to Make AdaBoost.M1\nWork for Weak Base Classifiers by Changing Only One Line of the Code. In Proceeding of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases. pp. 72-83.\n[13] Breiman, L. 1996. Bagging predictors. Machine Learning,\nvol. 24, no. 2, pp. 123-140.\n[14] Webb, G. I. 2000. MultiBoosting: A Technique for\nCombining Boosting and Wagging. Machine Learning. Vol.40, no. 2, pp. 159-196.\n[15] Atkeson, C. G., Moore, A. W., Schaal, S. 1997. Locally\nWeighted Learning. Artificial Intelligence Review, vol. 11, no. 1, pp. 11-73.\n[16] Bishop, C. M., Nasrabadi, N. M. 2007. Pattern Recognition\nand Machine Learning. Journal of Electronic Imaging, vol. 16, no. 4.\n[17] Duda R., Hart, P. 1973. Pattern Classification and Scene\nAnalysis. Wiley, New York.\n[18] John, G. H., Langley, P. 1995. Estimating Continuous\nDistributions in Bayesian Classifiers. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence. pp. 338-345."
    } ],
    "references" : [ {
      "title" : "Solving History Problems of the National Center Test for University Admissions",
      "author" : [ "Y. Kano" ],
      "venue" : "In Proceedings of  the 28th Annual Conference of the Japanese Society for Artificial Intelligence",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Detecting Dynamic Association among Twitter Topics",
      "author" : [ "S. Song", "Q. Li", "H. Bao" ],
      "venue" : "In Proceedings of the 21st International World Wide Web Conference",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Additive logistic regression: a statistical view of boosting",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "The WEKA Data Mining Software: An Update",
      "author" : [ "M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten" ],
      "venue" : "SIGKDD Explorations. Volume",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Extracting Named Entities and Relating Them over Time Based on Wikipedia",
      "author" : [ "A. Bhole", "B. Fortuna", "M. Grobelnik", "D. Mladenić" ],
      "venue" : "Informatica (Slovenia)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "The Question Answering Systems: A Survey",
      "author" : [ "A.M.N. Allam", "M.H. Haggag" ],
      "venue" : "International Journal of Research and Reviews in Information Sciences",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Answering multiple-choice questions in high-stakes medical examinations",
      "author" : [ "F. Martin R", "H. Sibyl", "K. Veronika" ],
      "venue" : "Medical Education,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Web-Based multiple choice question answering for english and arabic questions",
      "author" : [ "R. Awadallah", "A. Rauber" ],
      "venue" : "In Proceeding of the 28th European conference on Advances in Information Retrieval,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Another kind of ‘BOLD Response’: answering multiple-choice questions via online decoded single-trial brain signals",
      "author" : [ "B. Sorger", "B. Dahmen", "J. Reithler", "O. Gosseries", "A. Maudoux", "S. Laureys", "R. Goebel" ],
      "venue" : "Progress in Brain Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Logistic Model Trees",
      "author" : [ "N. Landwehr", "M.A. Hall", "E. Frank" ],
      "venue" : "In Proceeding of the 14th European Conference on Machine Learning",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "How to Make AdaBoost.M1 Work for Weak Base Classifiers by Changing Only One Line of the Code",
      "author" : [ "G. Eibl", "K.P. Pfeiffer" ],
      "venue" : "In Proceeding of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "MultiBoosting: A Technique for Combining Boosting and Wagging",
      "author" : [ "G.I. Webb" ],
      "venue" : "Machine Learning. Vol.40,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2000
    }, {
      "title" : "Locally Weighted Learning",
      "author" : [ "C.G. Atkeson", "A.W. Moore", "S. Schaal" ],
      "venue" : "Artificial Intelligence Review,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1997
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "C.M. Bishop", "N.M. Nasrabadi" ],
      "venue" : "Journal of Electronic Imaging,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Pattern Classification and Scene Analysis",
      "author" : [ "Duda R", "P. Hart" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1973
    }, {
      "title" : "Estimating Continuous Distributions in Bayesian Classifiers",
      "author" : [ "G.H. John", "P. Langley" ],
      "venue" : "In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "QA is therefore composed of three distinct modules: question classification, information retrieval, and answer extraction, each of which has a core component beside other supplementary components [7].",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 0,
      "context" : "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].",
      "startOffset" : 155,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].",
      "startOffset" : 155,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].",
      "startOffset" : 155,
      "endOffset" : 164
    }, {
      "referenceID" : 8,
      "context" : "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].",
      "startOffset" : 155,
      "endOffset" : 164
    }, {
      "referenceID" : 4,
      "context" : "Wikipedia is a well-known free content, multilingual encyclopedia written collaboratively by contributors around the world [6].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "This formula is modified from the Euclidean Distance [2], without firstly creating the vector of words in all Wiki articles and choices, which is very time consuming and with low robustness.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm [4].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 9,
      "context" : "Logistic Model Trees: Logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning [11].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 10,
      "context" : "AdaBoost M1: AdaBoost M1 is an improved version of traditional AdaBoost algorithm, which can be used to classify both binary and polynominal label with numerical, binominal and polynominal (and weighted) attributes [12].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 11,
      "context" : "It is able to harness both AdaBoost's high bias and variance reduction with wagging's superior variance reduction [14].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "using linear regression) [15].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "[16].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "Simple Naïve Bayes: Naive Bayes classifier, in which the numeric attributes are modelled by a normal distribution [17].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "Numeric estimator precision values are chosen based on analysis of the training data [18].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "1 for numeric attributes when build Classifier is called with zero training instances [18].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "All of the classification model training procedures are realized with WEKA [5], which is a collection of machine learning algorithms for data mining tasks and contains tools for data preprocessing, classification, regression, clustering, association rules, and visualization.",
      "startOffset" : 75,
      "endOffset" : 78
    } ],
    "year" : 2015,
    "abstractText" : "Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.",
    "creator" : "Microsoft® Word 2013"
  }
}