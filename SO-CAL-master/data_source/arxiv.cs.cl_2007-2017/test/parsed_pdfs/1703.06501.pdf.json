{
  "name" : "1703.06501.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Métodos de Otimização Combinatória Aplicados ao Problema de Compressão MultiFrases∗",
    "authors" : [ "Elvys Linhares Pontes", "Thiago Gouveia da Silva", "Andréa Carneiro Linhares", "Juan-Manuel Torres-Moreno", "Stéphane Huet" ],
    "emails" : [ "elvys.linhares-pontes@alumni.univ-avignon.com,", "thiago.gouveia@ifpb.edu.br" ],
    "sections" : [ {
      "heading" : null,
      "text" : "this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for MultiSentences Compression (MSC), method which reduces the sentence length without removing core information, the insertion of optimization methods requires further study to improve the performance of MSC. This article describes a method for MSC using Combinatorial Optimization and Graph Theory to generate more informative sentences while maintaining their grammaticality. An experiment led on a corpus of 40 clusters of sentences shows that our system has achieved a very good quality and is better than the state-of-the-art.\nKEYWORDS. Combinatorial Optimization, Multi-Sentences Compression, Word Graph.\n∗Preprint of XLVIII Simpósio Brasileiro de Pesquisa Operacional, 2016.\nar X\niv :1\n1. Introdução\nO aumento da quantidade de dispositivos eletrônicos (smartphones, tablets, etc) e da Internet móvel tornaram o acesso à informação fácil e rápido. Através da Internet é possı́vel ter acesso aos acontecimentos de todo o mundo a partir de diferentes sites, blogs e portais. Páginas como a Wikipédia e portais de notı́cias fornecem informações detalhadas sobre diversas temáticas, entretanto os textos são longos e possuem muitas informações irrelevantes. Uma solução para esse problema é a geração de resumos contendo as principais informações do documento e sem redundâncias (Linhares Pontes et al., 2014). Vista a vasta quantidade e o fácil acesso às informações, é possı́vel automatizar a análise e geração de resumos a partir da análise estatı́stica, morfológica e sintática das frases (Torres-Moreno, 2014).\nO Processamento da Linguagem Natural (PLN) concerne à aplicação de sistemas e técnicas de informática para analisar a linguagem humana. Dentre as diversas aplicações do PLN (tradução automática, compressão textual, etc.), a Sumarização Automática de Textos (SAT) consiste em resumir um ou mais textos automaticamente. O sistema sumarizador identifica os dados relevantes e cria um resumo a partir das principais informações (Linhares Pontes et al., 2015). A CMF é um dos métodos utilizados na SAT para gerar resumos, que utiliza um conjunto de frases para gerar uma única frase de tamanho reduzido gramaticalmente correta e informativa (Filippova, 2010; Boudin e Morin, 2013).\nNeste artigo, apresentamos um método baseado na Teoria dos Grafos e na Otimização Combinatória para modelar um documento como um Grafo de Palavras (GP) (Filippova, 2010) e gerar a CMF com uma melhor qualidade informativa.\nA seção 2 descreve o problema e os trabalhos relacionados à CMF. Detalhamos a abordagem e a modelagem matemática nas seções 3 e 4, respectivamente. O corpus, as ferramentas utilizadas e os resultados obtidos são discutidos na seção 5. Finalmente, as conclusões e os comentários finais são expostos na seção 6."
    }, {
      "heading" : "2. Compressão MultiFrases",
      "text" : "A Compressão MultiFrases (CMF) consiste em produzir uma frase de tamanho reduzido gramaticalmente correta a partir de um conjunto de frases oriundas de um documento, preservando-se as principais informações desse conjunto. Uma compressão pode ter diferentes valores de Taxa de Compressão (TC), entretanto quanto menor a TC maior será a redução das informações nele contidas. Seja D o documento analisado composto pelas frases {f1, f2, . . . , fn} e fraseCMF a compressão desse documento, a TC é definida por:\nTC = ||fraseCMF ||∑n\ni=1 ||fi|| n\n. (1)\nonde ||fi|| é o tamanho da frase fi (quantidade de palavras). Dessa forma, os principais desafios da CMF são a seleção dos conteúdos informativos e a legibilidade da frase produzida.\nDentre as diversas abordagens feitas sobre a CMF, algumas baseiam-se em analisadores sintáticos para a produção de compressões gramaticais. Por exemplo, Barzilay e McKeown (2005) desenvolveram uma técnica de geração text-to-text em que cada frase\ndo texto é representada como uma árvore de dependência. De forma geral, essa técnica alinha e combina estas árvores para gerar a fusão das frases analisadas. Outra abordagem possı́vel é descrita por Filippova (2010), que gerou compressões de frases de boa qualidade utilizando uma simples modelagem baseada na Teoria dos Grafos e uma lista de stopwords1. Boudin e Morin (2013) geraram a CMF mais informativas a partir da análise da relevância das frases geradas pelo método de Filippova.\nVisto que os trabalhos apresentados utilizaram uma modelagem simples e obtiveram resultados de boa qualidade, este trabalho baseia-se na mesma modelagem utilizada por Filippova e métodos de otimização combinatória para aumentar a informatividade da CMF. As subseções 2.1 e 2.2 descrevem os métodos utilizados por Filippova (2010) e Boudin e Morin (2013), respectivamente."
    }, {
      "heading" : "2.1. Filippova",
      "text" : "Filippova (2010) modelou um documento D composto por frases similares como um Grafo de Palavras (GP). O GP é um grafo direcionadoGP = (V,A), onde V é o conjunto de vértices (palavras) e A é o conjunto de arcos (relação de adjacência). Dessa forma, dado um documento D de frases similares {f1, f2, . . . , fn}, o GP é construı́do a partir da adição dessas frases no grafo. A Figura 1 ilustra o GP descrito por Filippova das seguintes frases:\na) George Solitário, a última tartaruga gigante Pinta Island do mundo, faleceu. b) A tartaruga gigante conhecida como George Solitário morreu na segunda no Par-\nque Nacional de Galapagos, Equador. c) Ele tinha apenas cem anos de vida, mas a última tartaruga gigante Pinta conhecida,\nGeorge Solitário, faleceu. d) George Solitário, a última tartaruga gigante da sua espécie, morreu.\nInicialmente, o GP é composto pela primeira frase (a) e pelos vértices -começoe -fim-. Uma palavra é representada por um vértice existente somente se ela possuir a\n1Stopwords são palavras comuns sem relevância informativa para uma frase. Ex: artigos, preposições, etc.\nmesma forma minúscula, mesma Part-Of-Speech (POS)2, e se não existir outra palavra dessa mesma frase que já tenha sido mapeada nesse vértice. Um novo vértice é criado caso não seja encontrado um vértice com suas caracterı́sticas no GP. Dessa forma, cada frase representa um caminho simples entre os vértices -começo- e -fim-.\nAs frases são analisadas e adicionadas individualmente ao GP. Para cada frase analisada, as palavras são inseridas na seguinte ordem:\n1. Palavras que não sejam stopwords e para os quais não existam nenhum candidato no grafo ou mapeamento não ambı́guo; 2. Palavras que não sejam stopwords e para os quais existam vários candidatos possı́veis no grafo ou que ocorram mais de uma vez na mesma frase; 3. Stopwords.\nNos grupos 2 e 3, o mapeamento das palavras é ambı́guo, pois há mais de uma palavra no grafo que referencia a mesma palavra/POS. Nesse caso, as palavras predecessoras e posterioras são analisadas para verificar o contexto da palavra e escolher o mapeamento correto. Caso uma dessas palavras não possua o mesmo contexto das existentes no grafo, um novo vértice é criado para representá-la.\nTendo adicionado os vértices, os pesos dos arcos representam o nı́vel de coesão entre as palavras de dois vértices a partir da frequência e da posição dessas palavras nas frases, conforme as Equações 2 e 3:\nw(ei,j) = coesão(ei,j)\nfreq(i)× freq(j) , (2)\ncoesão(ei,j) = freq(i) + freq(j)∑ f∈D dist(f, i, j) −1 , (3)\ndist(f, i, j) = { pos(f, i)− pos(f, j) se pos(f, i) < pos(f, j) 0 caso contrário (4)\nonde freq(i) é a frequência da palavra mapeada no vértice i e a função pos(f, i) retorna a posição da palavra i na frase f .\nA partir do GP, o sistema calcula os 50 menores caminhos3 que tenham no mı́nimo oito palavras e ao menos um verbo. Por fim, o sistema normaliza os scores (distâncias do caminhos) das frases geradas a partir do comprimento das mesmas e seleciona a frase com o menor score normalizado como a melhor CMF."
    }, {
      "heading" : "2.2. Boudin e Morin",
      "text" : "Boudin e Morin (2013) (BM) propuseram um método para melhor avaliar a qualidade de uma frase e gerar compressões mais informativas a partir da abordagem descrita por Filippova (seção 2.1). BM utilizaram a mesma metodologia de Filippova para gerar os 200 menores caminhos, que tenham no mı́nimo oito palavras e ao menos um verbo, do\n2POS é a classe gramatical de uma palavra numa frase. 3Ressaltando que cada caminho no GP representa uma frase.\nGP. Ao invés de realizar uma simples normalização dos valores de cada frase como Filippova, BM mensuraram a relevância da frase gerada (caminho c no GP) a partir das keyphrases4 e o comprimento das frases, conforme Equações 5 e 6:\nscore(c) =\n∑ i,j∈caminho(c)w(i,j)\n||c|| × ∑ k∈c scorekp(k) , (5)\nscorekp(k) = ∑ w∈k TextRank(w) ||k||+ 1 , (6)\nonde w(i,j) é o score entre os vértices i e j descrito na Equação 2, o algoritmo TextRank (Mihalcea e Tarau, 2004) que calcula a relevância de uma palavra w no GP a partir das suas palavras predecessoras e posteriores e scorekp(k) é a relevância da keyphrase k presente no caminho c. Por fim, a frase com o menor score é a escolhida para a compressão do texto."
    }, {
      "heading" : "3. Nova modelagem do problema",
      "text" : "Os métodos de Filippova e BM calculam os menores caminhos do GP analisando somente o nı́vel de coesão entre duas palavras vizinhas no texto. Após a geração dos caminhos, os scores de cada caminho são normalizados para escolher o “menor” deles. Entretanto, duas palavras possuindo uma forte coesão não significa que as mesmas possuam uma boa informatividade. Por mais que a normalização ou reanálise das frases seja eficiente, esses métodos estão sempre limitados às frases geradas pela análise do nı́vel de coesão. Portanto, a geração de 50 ou 200 dos menores caminhos (frases) não garante a existência de uma frase com boa informatividade. Por isso, propomos um método para analisar concomitantemente a coesão e a relevância das palavras a fim de gerar uma compressão mais informativa de um documento.\nO método aqui exposto visa calcular o caminho mais curto analisando a coesão das palavras e bonificando os caminhos que possuam palavras-chaves e 3-grams5 frequentes do texto. Inicialmente, utiliza-se a mesma abordagem de Filippova (seção 2.1) para modelar um documento D como um GP e calcular a coesão das palavras. Além de considerar a coesão, analisamos as palavras-chaves e os 3-grams do documento para gerar uma CMF mais informativa. As palavras-chaves auxiliam a geração de caminhos com as principais informações descritas no texto. Como o documento D é composto por frases similares, consideramos que o documento possui somente uma temática. A Latent Dirichlet Allocation (LDA) é um método para analisar as frases de um texto e identificar o conjunto de palavras que representam as temáticas nele abordadas (Blei et al., 2003). Configura-se o método LDA para identificar o conjunto de palavras que representa uma única temática do documento. Finalmente, esse conjunto de palavras constitui as palavras-chaves do documento D.\nUma outra consideração sobre o documento analisado é que a presença de uma palavra em diferentes frases aumenta sua relevância para a CMF (vale salientar que consideramos a relevância dos stopwords igual a zero). A partir da ponderação dos 2-grams\n4Keyphrases são as palavras que representam o conteúdo principal do texto. 53-gram é formado por 3 palavras vizinhas.\n(Equação 2), consideramos que a relevância de um 3-gram é baseado na relevância dos dois 2-grams que o formam, como descrito na Equação 7:\n3-gram(i, j, k) = qt3(i, j, k) maxa,b,c∈GP qt3(a, b, c) × w(ei,j) + w(ej,k) 2 , (7)\nonde qt3(i, j, k) é quantidade de 3-grams composto pelas palavras dos vértices i, j e k no documento. Os 3-grams auxiliam a geração de CMF com estruturas importantes para o texto e incrementam a qualidade gramatical das frases geradas.\nO nosso sistema calcula os 50 menores caminhos do GP que possuam ao menos 8 palavras, baseado na coesão, palavras-chaves e 3-grams (Equação 9). Contrariamente ao método de Filippova, as frases podem ter score negativo, pois reduzimos o valor do caminho composto por palavras-chaves e 3-grams. Dessa forma, normalizamos os scores dos caminhos (frases) baseado na função exponencial para obter um score maior que zero, conforme a Equação 8:\nscorenorm(f) = escoreopt(f)\n||f || , (8)\nonde scoreopt(f) é o valor do caminho para gerar a frase f a partir da Equação 9. Finalmente, selecionamos a frase com menor score normalizado e contendo, ao menos, um verbo como a melhor compressão das frases do documento.\nPara exemplificar nosso método, simplificamos sua análise e utilizamos o texto modelado na Figura 1. Nessa figura, existem diversos caminhos possı́veis entre os vértices -começo- e -fim-. A partir das palavras-chaves “George”, “gigante”, “solitário” “tartaruga” e “última”, nosso método gerou a compressão “a tartaruga gigante conhecida george solitário morreu”. Dentre as 5 palavras-chaves analisadas, foi gerada uma compressão contendo 4 delas e com as principais informações das frases."
    }, {
      "heading" : "4. Modelo Matemático Proposto",
      "text" : "Formalmente, o GP utilizado pode ser representado como segue: seja GP = (V,A) um grafo orientado simples no qual V é o conjunto de vértices (palavras), A o conjunto de arcos (2-grams) e bij é o peso do arco (i, j) ∈ A (coesão das palavras dos vértices i e j, Equação 2). Sem perda de generalidade, considere v0 como o vértice -começo- e adicione um arco auxiliar do vértice -fim- para v0. Adicionalmente, cada vértice possui uma cor indicando se o mesmo é uma palavra-chave. Denotamos K como o conjunto de cores em que cada palavra-chave do documento representa uma cor diferente. A cor 0 (não palavras-chaves) possui o custo c0 = 0 e as palavras-chaves possuem o mesmo custo ck = 1 (para k > 0 e k ∈ K). O conjunto T é composto pelos 3-grams do documento com uma frequência maior que 1. Cada 3-gram t = (a, b, c) ∈ T possui o custo dt = 3-gram(a, b, c) (Equação 7) normalizados entre 0 e 1.\nExistem vários algoritmos com complexidade polinomial para encontrar o menor caminho em um grafo. Contudo, a restrição de que o caminho deve possuir um número mı́nimo Pmin de vértices (o número mı́nimo de palavras da compressão) torna o problema NP-Hard. De fato, encontrar o menor caminho no GP descrito implica encontrar um\nciclo com inı́cio e fim em v0, e caso Pmin seja igual a |V |, o problema corresponde ao Problema do Caixeiro Viajante (PCV). Nesse caso, como o PCV é um caso especial do problema descrito, ele também será NP-Hard.\nO modelo matemático proposto para resolução do problema apresentado define cinco grupos de variáveis de decisão:\n• xij , ∀(i, j) ∈ A, indicando se o arco (i, j) faz parte da solução; • yv, ∀v ∈ V , indicando se o vértice (a palavra) v faz parte da solução; • zt, ∀t ∈ T , indicando se o 3-gram t faz parte da solução; • wk, ∀k ∈ K, indicando que alguma palavra com a cor (palavra-chave) k foi uti-\nlizada na solução; e • uv, ∀v ∈ V , variáveis auxiliares para eliminação de sub-ciclos da solução.\nO processo de encontrar as 50 melhores soluções se deu pela proibição das soluções encontradas e reexecução do modelo. Optamos por essa estratégia em virtude da simetria gerada pela técnica de eliminação de sub-ciclos que utilizamos. A formulação é apresentada nas expressões (9) a (22).\nMinimize ( α ∑\n(i,j)∈A bi,j · xi,j − β ∑ k∈K ck · wk − γ ∑ t∈T dk · zt )\n(9)\ns.a. ∑ v∈V\nyv ≥ Pmin, (10)∑ v∈V (k) yv ≥ wk, ∀k ∈ K, (11)\n2zt ≤ xij + xjl, ∀t = (i, j, l) ∈ T, (12)∑ i∈δ−(v)\nxiv = yv ∀v ∈ V, (13)∑ i∈δ+(v) xvi = yv ∀v ∈ V, (14)\ny0 = 1, (15)\nu0 = 1, (16)\nui − uj + 1 ≤M −M · xij ∀(i, j) ∈ A, j 6= 0, (17)\nxij ∈ {0, 1}, ∀(i, j) ∈ A, (18)\nzl ∈ {0, 1}, ∀t ∈ T, (19)\nyv ∈ {0, 1}, ∀v ∈ V, (20)\nwk ∈ [0, 1], ∀k ∈ K, (21)\nuv ∈ [1, |V |], ∀v ∈ V. (22)\nA função objetiva do programa (9) maximiza a qualidade da compressão gerada. As variáveis α, β e γ controlam, respectivamente, a relevância da coesão, das palavraschaves e dos 3-grams na geração da compressão. A restrição (10) limita o número de vértices (palavras) utilizadas na solução. O conjunto de restrições (11) faz a correspondência entre as variáveis de cores (palavras-chaves) e de vértices (palavras), sendo V (k) o conjunto de todos os vértices com a cor k (uma palavra-chave pode ser representada por mais de um vértice). O conjunto de restrições (12) faz a correspondência entre as variáveis de 3-grams e de arcos (2-grams). As igualdades (13) e (14) obrigam que para cada palavra usada na solução exista um arco ativo interior (entrando) e um exterior (saindo), respectivamente. A igualdade (15) força que o vértice zero seja usado na solução. Por fim, as restrições (16) e (17) são responsáveis pela eliminação de sub-ciclos enquanto as expressões (18)-(22) definem o domı́nio das variáveis.\nComo discutido em Pataki (2003), existem duas formas clássicas de evitar ciclos em problemas derivados do PCV. A primeira consiste na criação de um conjunto exponencial de cortes garantindo que para todo subconjunto de vértices S ⊂ V , S 6= ∅, haja exatamente |S| − 1 arcos ativos (mais detalhes em Lenstra et al. (1985)). A segunda, conhecida como formulação Miller–Tucker–Zemlin (MTZ) utiliza um conjunto auxiliar de variáveis, uma para cada vértice, de modo a evitar que um vértice seja visitado mais de uma vez no ciclo e um conjunto de arcos-restrições. Mais informações sobre a formulação MTZ podem ser obtidas em Öncan et al. (2009).\nNeste trabalho, optamos por eliminar sub-ciclos utilizando o método MTZ, uma vez que sua implementação é mais simples. Para tal, utilizamos uma variável auxiliar uv para cada vértice v ∈ V , e o conjunto de arcos-restrições definido em (17). Nesse grupo de restrições, M representa um número grande o suficiente, podendo ser utilizado o valor M = |V |."
    }, {
      "heading" : "5. Experimentos computacionais",
      "text" : "O desempenho do sistema proposto foi analisado a partir de diversos valores dos parâmetros (β e γ) associados à função objetivo. Os testes foram realizados num computador com processador i5 2.6 GHz e 6 GB de memoria RAM no sistema operacional Ubuntu 14.04 de 64 bits. Os algoritmos foram implementados utilizando a linguagem de programação Python e as bibliotecas takahe6 e gensim7. O modelo matemático foi implementado na linguagem C++ com a biblioteca Concert e o solver utilizado foi o CPLEX 12.6."
    }, {
      "heading" : "5.1. Corpus e ferramentas utilizadas",
      "text" : "Para avaliar a qualidade dos sistemas, utilizamos o corpus publicado por Boudin e Morin (2013). Esse corpus contém 618 frases (média de 33 palavras por frase) divididas em 40 clusters de notı́cias em Francês extraı́dos do Google News8. A taxa de redundância de um corpus é obtida pela divisão da quantidade de palavras únicas pela quantidade de palavras de cada cluster. A taxa de redundância do corpus que utilizamos é 38,8%. Cada palavra do corpus é acompanhada por sua POS. Para cada cluster, há 3 frases comprimidas por profissionais. Dividimos o corpus em duas partes de 20 clusters. A primeira parte é utilizada como corpus de aprendizado e a outra parte como corpus de teste. As frases do\n6Site: http://www.florianboudin.org/publications.html 7Site: https://radimrehurek.com/gensim/models/ldamodel.html 8Site: https://news.google.fr\ncorpus de aprendizado tem o tamanho médio de 34,1 palavras e as frases do corpus de teste tem um tamanho médio de 31,6 palavras.\nAs caracterı́sticas mais importante da CMF são a informatividade e gramaticalidade das frases. A informatividade representa a porcentagem das principais informações transmitidas no texto. Como consideramos que as compressões de referência possuem as informações mais importantes, avaliamos a informatividade de uma compressão baseada nas informações em comum entre a mesma e as compressões de referência usando o sistema ROUGE (Lin, 2004). Utilizamos as métricas de cobertura ROUGE-1 e ROUGE-2, que analisam os 1-grams e 2-grams, respectivamente, das compressões de referências presentes nas compressões geradas pelos sistemas, para estimar a informatividade das compressões geradas.\nDevido a complexidade da análise gramatical de uma frase, foi utilizado uma avaliação manual para estimar a qualidade das compressões propostas por nosso sistema. Como a avaliação humana é lenta, utilizamos essa técnica somente para o corpus de teste. Para o corpus de aprendizado, decidimos avaliar somente a qualidade informativa (coberturas ROUGE-1 e ROUGE-2) e a TC devido ser inviável a análise manual da quantidade de testes do nosso sistema."
    }, {
      "heading" : "5.2. Resultados",
      "text" : "Nomeamos nosso sistema como GP+OPT e utilizamos os sistemas de Filippova e de BM como baselines. Testamos o GP+OPT utilizando 1, 3, 5, 7 e 9 palavras-chaves9 (PC) obtidas a partir do método LDA. Como o GP+OPT utiliza como base o método de Filippova, tornamos fixo o α = 1.0 (priorizando a coesão das compressões geradas) e variamos β e γ de tal forma que:\nβ + γ < 1.0; β, γ = 0.0, 0.1, ..., 0.8, 0.9. (23)\nTodos os sistemas geraram a compressão de um documento em tempo viável (menos de 6 segundos). Devido à grande quantidade de testes gerados para o corpus de aprendizado, selecionamos os resultados que generalizam o funcionamento do GP+OPT. A Tabela 1 descreve a qualidade e a TC das compressões. Essa tabela é dividida em 4 partes. A primeira descreve os resultados das baselines e as demais partes descrevem os resultados do nosso sistema. A primeira parte da tabela comprova que o pós-tratamento utilizado por BM (análise da relevância das keyphrases) é melhor que a simples normalização dos scores das frases realizada por Filippova. O aumento da relevância dos 3-grams na nossa modelagem melhora a informatividade da compressão sem aumentar bruscamente a TC, pois os 3-grams favorecem a utilização de 2-grams frequentes no texto (segunda parte da Tabela 1). Além disso, os 3-grams podem melhorar a qualidade gramatical, pois eles adicionam conjuntos de palavras gramaticalmente corretos à compressão.\nApesar do aumento da relevância das palavras-chaves gerar compressões com uma maior TC, as compressões são mais informativas (a terceira parte da tabela) e proporcionam as melhores compressões (linhas em negrito da Tabela 1). Dentre os melhores resultados (última parte da Tabela), escolhemos a versão do nosso sistema com PC=9,\n9Visto que o texto é composto de frases similares sobre um mesmo tópico, consideramos que 9 palavras é a quantidade máxima de palavras-chaves para representar um tópico.\nβ=0.8 e γ=0.1, pois essa configuração prioriza as palavras-chaves e tenta adicionar 3- grams às compressões.\nSelecionado a melhor configuração do nosso sistema, validamos a qualidade dos sistemas utilizando o corpus de teste (Tabela 2). Similar aos resultados do corpus de aprendizado, o método de BM foi melhor que o método de Filippova para as métricas ROUGE-1 e ROUGE-2. GP+OPT obteve resultados bem superiores que as baselines comprovando que a análise da coesão juntamente com as palavras-chaves e 3-grams auxiliam a geração de melhores compressões. Apesar dos valores da TC do nosso sistema terem sido maiores que os valores da TC das baselines10, a TC do sistema GP+OPT ficou próxima da TC das compressões dos profissionais (TC = 59%).\nCom o intuito de melhor analisar a qualidade informativa e gramatical das compressões, 5 franceses avaliaram as compressões geradas por cada sistema e notificaram a qualidade gramatical e informativa para o corpus de teste (Tabela 3). Nosso sistema gerou estatisticamente compressões mais informativas que as baselines. Apesar da média da gramaticalidade do nosso sistema ter sido inferior a dos demais sistemas, não podemos confirmar qual sistema é estatisticamente melhor para a gramaticalidade devido ao fato dos intervalos de confiança da gramaticalidade dos sistemas se cruzarem. Portanto, nosso\n10A diferença do tamanho médio das frases entre os sistemas GP+OPT e Filippova foi 3,7 palavras.\nsistema pode gerar compressões com qualidade gramatical igual às compressões geradas pelos métodos de Filippova ou de BM.\nDesse modo, pode-se afirmar que o GP+OPT apresentou melhores resultados que as baselines gerando compressões mais informativas e com uma boa qualidade gramatical."
    }, {
      "heading" : "6. Considerações Finais e Proposta de Trabalhos Futuros",
      "text" : "A CMF gera frases de boa qualidade sendo uma ferramenta interessante para a SAT. A análise concomitante da coesão, palavras-chaves e 3-grams identificaram as informações principais do documento. Apesar do nosso sistema ter gerado compressões com uma TC maior que as baselines, a informatividade foi consideravelmente melhor. A análise manual dos franceses comprovou que nosso método gerou compressões mais informativas e mantendo uma boa qualidade gramatical.\nOs próximos trabalhos visam criar um corpus similar ao de BM para o idioma Português e testar o desempenho do nosso sistema para diferentes idiomas. Além disso, pretende-se adaptar o sistema para escolher a relevância das palavras-chaves e dos 3- grams baseados no tamanho e no vocabulário do documento. Finalmente, objetiva-se implementar diferentes métodos para a obtenção de palavras-chaves, a fim de avaliar o impacto de cada um na qualidade da geração da CMF.\nAgradecimentos\nEste trabalho foi financiado parcialmente pelo projeto europeu CHISTERA-AMIS ANR15-CHR2-0001."
    } ],
    "references" : [ {
      "title" : "Sentence fusion for multidocument news summarization",
      "author" : [ "Barzilay", "K.R.R. e McKeown" ],
      "venue" : "Computational Linguistics, v. 31,",
      "citeRegEx" : "Barzilay and McKeown,? \\Q2005\\E",
      "shortCiteRegEx" : "Barzilay and McKeown",
      "year" : 2005
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "Ng", "M.I.A.Y. e Jordan" ],
      "venue" : "Journal Machine Learning Research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Keyphrase extraction for n-best reranking in multi-sentence compression",
      "author" : [ "Boudin", "E.F. e Morin" ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Boudin and Morin,? \\Q2013\\E",
      "shortCiteRegEx" : "Boudin and Morin",
      "year" : 2013
    }, {
      "title" : "Multi-sentence compression: Finding shortest paths in word",
      "author" : [ "K. Filippova" ],
      "venue" : "COLING, p",
      "citeRegEx" : "Filippova,? \\Q2010\\E",
      "shortCiteRegEx" : "Filippova",
      "year" : 2010
    }, {
      "title" : "The traveling salesman problem: a guided tour of combinatorial optimization",
      "author" : [ "J.K. Lenstra", "A.R. Kan", "Lawler", "D.E.L. e Shmoys" ],
      "venue" : null,
      "citeRegEx" : "Lenstra et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lenstra et al\\.",
      "year" : 1985
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Lin", "C.-Y" ],
      "venue" : "ACL workshop on Text Summarization Branches Out,",
      "citeRegEx" : "Lin and C..Y.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lin and C..Y.",
      "year" : 2004
    }, {
      "title" : "Sasi: sumarizador automático de documentos baseado no problema do subconjunto independente de vértices",
      "author" : [ "E. Linhares Pontes", "Linhares", "A.C. e Torres-Moreno", "J.-M" ],
      "venue" : "Anais do Simpósio Brasileiro de Pesquisa Operacional,",
      "citeRegEx" : "Pontes et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pontes et al\\.",
      "year" : 2014
    }, {
      "title" : "Lia-rag: a system based on graphs and divergence of probabilities applied to speech-to-text summarization",
      "author" : [ "E. Linhares Pontes", "Linhares", "A.C. e Torres-Moreno", "J.-M" ],
      "venue" : "CCCS (Call Centre Conversation Summarization) Multiling 2015 Workshop,",
      "citeRegEx" : "Pontes et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pontes et al\\.",
      "year" : 2015
    }, {
      "title" : "TextRank: Bringing order into texts",
      "author" : [ "Mihalcea", "P.R. e Tarau" ],
      "venue" : "Proceedings of EMNLP04and the 2004 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Mihalcea and Tarau,? \\Q2004\\E",
      "shortCiteRegEx" : "Mihalcea and Tarau",
      "year" : 2004
    }, {
      "title" : "A comparative analysis of several asymmetric traveling salesman problem formulations",
      "author" : [ "T. Öncan", "Altınel", "G.İ.K. e Laporte" ],
      "venue" : "Computers & Operations Research, v. 36,",
      "citeRegEx" : "Öncan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Öncan et al\\.",
      "year" : 2009
    }, {
      "title" : "Teaching integer programming formulations using the traveling salesman problem",
      "author" : [ "G. Pataki" ],
      "venue" : "SIAM review, v. 45,",
      "citeRegEx" : "Pataki,? \\Q2003\\E",
      "shortCiteRegEx" : "Pataki",
      "year" : 2003
    }, {
      "title" : "Automatic Text Summarization",
      "author" : [ "Torres-Moreno", "J.-M" ],
      "venue" : "John Wiley & Sons. ISBN 9781-84821-668-6,",
      "citeRegEx" : "Torres.Moreno and J..M.,? \\Q2014\\E",
      "shortCiteRegEx" : "Torres.Moreno and J..M.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "A CMF é um dos métodos utilizados na SAT para gerar resumos, que utiliza um conjunto de frases para gerar uma única frase de tamanho reduzido gramaticalmente correta e informativa (Filippova, 2010; Boudin e Morin, 2013).",
      "startOffset" : 180,
      "endOffset" : 219
    }, {
      "referenceID" : 3,
      "context" : "Neste artigo, apresentamos um método baseado na Teoria dos Grafos e na Otimização Combinatória para modelar um documento como um Grafo de Palavras (GP) (Filippova, 2010) e gerar a CMF com uma melhor qualidade informativa.",
      "startOffset" : 152,
      "endOffset" : 169
    }, {
      "referenceID" : 3,
      "context" : "Outra abordagem possı́vel é descrita por Filippova (2010), que gerou compressões de frases de boa qualidade utilizando uma simples modelagem baseada na Teoria dos Grafos e uma lista de stopwords1.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "Outra abordagem possı́vel é descrita por Filippova (2010), que gerou compressões de frases de boa qualidade utilizando uma simples modelagem baseada na Teoria dos Grafos e uma lista de stopwords1. Boudin e Morin (2013) geraram a CMF mais informativas a partir da análise da relevância das frases geradas pelo método de Filippova.",
      "startOffset" : 41,
      "endOffset" : 219
    }, {
      "referenceID" : 3,
      "context" : "Visto que os trabalhos apresentados utilizaram uma modelagem simples e obtiveram resultados de boa qualidade, este trabalho baseia-se na mesma modelagem utilizada por Filippova e métodos de otimização combinatória para aumentar a informatividade da CMF. As subseções 2.1 e 2.2 descrevem os métodos utilizados por Filippova (2010) e Boudin e Morin (2013), respectivamente.",
      "startOffset" : 167,
      "endOffset" : 330
    }, {
      "referenceID" : 3,
      "context" : "Visto que os trabalhos apresentados utilizaram uma modelagem simples e obtiveram resultados de boa qualidade, este trabalho baseia-se na mesma modelagem utilizada por Filippova e métodos de otimização combinatória para aumentar a informatividade da CMF. As subseções 2.1 e 2.2 descrevem os métodos utilizados por Filippova (2010) e Boudin e Morin (2013), respectivamente.",
      "startOffset" : 167,
      "endOffset" : 354
    }, {
      "referenceID" : 3,
      "context" : "Grafo de palavras gerado a partir das frases a-d e um possı́vel caminho representando a compressão (Filippova, 2010).",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "A Latent Dirichlet Allocation (LDA) é um método para analisar as frases de um texto e identificar o conjunto de palavras que representam as temáticas nele abordadas (Blei et al., 2003).",
      "startOffset" : 165,
      "endOffset" : 184
    }, {
      "referenceID" : 8,
      "context" : "Como discutido em Pataki (2003), existem duas formas clássicas de evitar ciclos em problemas derivados do PCV.",
      "startOffset" : 18,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "A primeira consiste na criação de um conjunto exponencial de cortes garantindo que para todo subconjunto de vértices S ⊂ V , S 6= ∅, haja exatamente |S| − 1 arcos ativos (mais detalhes em Lenstra et al. (1985)).",
      "startOffset" : 188,
      "endOffset" : 210
    }, {
      "referenceID" : 4,
      "context" : "A primeira consiste na criação de um conjunto exponencial de cortes garantindo que para todo subconjunto de vértices S ⊂ V , S 6= ∅, haja exatamente |S| − 1 arcos ativos (mais detalhes em Lenstra et al. (1985)). A segunda, conhecida como formulação Miller–Tucker–Zemlin (MTZ) utiliza um conjunto auxiliar de variáveis, uma para cada vértice, de modo a evitar que um vértice seja visitado mais de uma vez no ciclo e um conjunto de arcos-restrições. Mais informações sobre a formulação MTZ podem ser obtidas em Öncan et al. (2009).",
      "startOffset" : 188,
      "endOffset" : 529
    }, {
      "referenceID" : 3,
      "context" : "Sistemas ROUGE-1 ROUGE-2 TC Filippova (2010) 0,58769 0,43063 51,9% Boudin e Morin (2013) 0,62364 0,45467 55,8% GP+OPT PC=9 β=0.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Sistemas ROUGE-1 ROUGE-2 TC Filippova (2010) 0,58769 0,43063 51,9% Boudin e Morin (2013) 0,62364 0,45467 55,8% GP+OPT PC=9 β=0.",
      "startOffset" : 28,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "Sistemas ROUGE-1 ROUGE-2 TC Filippova (2010) 0,58455 0,43939 51,1% Boudin e Morin (2013) 0,62116 0,45734 55,2% GP+OPT PC=9 β=0.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Sistemas ROUGE-1 ROUGE-2 TC Filippova (2010) 0,58455 0,43939 51,1% Boudin e Morin (2013) 0,62116 0,45734 55,2% GP+OPT PC=9 β=0.",
      "startOffset" : 28,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "Sistemas Gramaticalidade Informatividade Filippova (2010) 4,2 ± 0,18 2,86 ± 0,32 Boudin e Morin (2013) 3,99 ± 0,21 3,31 ± 0,32 GP+OPT PC=9 β=0.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "Sistemas Gramaticalidade Informatividade Filippova (2010) 4,2 ± 0,18 2,86 ± 0,32 Boudin e Morin (2013) 3,99 ± 0,21 3,31 ± 0,32 GP+OPT PC=9 β=0.",
      "startOffset" : 41,
      "endOffset" : 103
    } ],
    "year" : 2017,
    "abstractText" : "The Internet has led to a dramatic increase in the amount of available information. In this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for MultiSentences Compression (MSC), method which reduces the sentence length without removing core information, the insertion of optimization methods requires further study to improve the performance of MSC. This article describes a method for MSC using Combinatorial Optimization and Graph Theory to generate more informative sentences while maintaining their grammaticality. An experiment led on a corpus of 40 clusters of sentences shows that our system has achieved a very good quality and is better than the state-of-the-art.",
    "creator" : "LaTeX with hyperref package"
  }
}