{
  "name" : "1611.05962.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Siwei Lai" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "密级\n博士学位论文\n基于神经网络的词和文档语义向量表示方法研究\n作者姓名 来斯惟\n指导教师 赵军研究员\n中国科学院自动化研究所\n学位类别 工学博士\n学科专业 模式识别与智能系统\n培养单位 中国科学院自动化研究所\n2016年 1月\nWord and Document Embeddings based on Neural Network Approaches\nBy\nSiwei Lai\nA Dissertation Submitted to The University of Chinese Academy of Sciences\nIn partial fulfillment of the requirement For the degree of\nDoctor of Engineering\nInstitute of Automation Chinese Academy of Sciences\nJanuary, 2016\n独创性声明 本人声明所递交的论文是我个人在导师指导下进行的研究工作及取得的研究成果。尽\n我所知，除了文中特别加以标注和致谢的地方外，论文中不包含其他人已经发表或撰写过 的研究成果。与我一同工作的同志对本研究所做的任何贡献均已在论文中作了明确地说明 并表示了谢意。\n签名: 日期：\n关于论文使用授权的说明 本人完全了解中国科学院自动化研究所有关保留、使用学位论文的规定，即：中国科\n学院自动化研究所有权保留送交论文的复印件，允许论文被查阅和借阅；可以公布论文的 全部或部分内容，可以采用影印、缩印或其他复制手段保存论文。\n（保密的论文在解密后应遵守此规定）\n签名: 导师签名: 日期：\n摘 要\n数据表示是机器学习中的基础工作，数据表示的好坏直接影响到整个系统\n的性能。传统机器学习思路下，对数据的表示主要通过人工设计特征来完成，在 很长一段时间里，文本、语音、图像领域中的各项任务均通过人工设计更好的 特征来实现性能的提升。近年来，随着深度学习和表示学习的兴起，基于神经 网络的数据表示技术在各个领域崭露头角。\n在自然语言处理领域，最常用的语义表示方法是词袋子模型，该方法存在\n数据稀疏问题，并且不能保留词序信息。早期方法中提出的词性、句法结构等 复杂特征，往往只能对特定的任务带来性能提升。本文从词和文档两个层次对 文本的语义表示技术进行系统的总结分析，并提出了自己的表示技术，具体如 下。 一、词向量表示技术的理论及实验分析。在这一部分中，本文对现有的词 向量表示技术进行了系统的理论对比及实验分析。理论方面，本文阐述了现有 各种模型之间的联系，从模型的结构与目标等方面对模型进行了比较，并证明 了其中最重要的两个模型 Skip-gram与 GloVe之间的关系。实验方面，本文从 模型、语料和训练参数三个角度分析了训练词向量的关键技术。本文选取了三 大类一共八个指标对词向量进行评价，这三大类指标涵盖了现有的词向量用法。 本工作为首个对词向量进行系统评价的工作，通过理论和实验的比较分析，文 章提出了一些对生成词向量的参考建议。 二、基于字词联合训练的中文表示及应用。现有的中文表示技术往往沿用 了英文的思路，直接从词的层面对文本表示进行构建。本文根据中文的特点，提 出了基于字词联合训练的表示技术。该方法在字的上下文空间中融入了词，利 用词的语义空间，更好地对汉字建模；同时利用字的平滑效果，更好地对词建 模。文章在分词任务、词义相似度任务和文本分类任务上对字和词的表示进行 了评价，实验表明字词联合训练得到的字词向量，相比单独训练字向量或词向 量，有显著的提升。 三、基于循环卷积网络的文档表示及应用。在这一部分中，本文分析了现 有的文档表示技术：基于循环网络的表示技术、基于递归网络的表示技术和基 于卷积网络的表示技术。并且，针对现有的三种表示技术的不足，本文提出了\nii 基于神经网络的词和文档语义向量表示方法研究\n基于卷积循环网络的文档表示技术。该方法克服了此前递归网络的复杂度过高 的问题，循环网络的语义偏置问题，以及卷积网络窗口较难选择的问题。文章 在文本分类任务上对新提出的表示技术进行了对比分析，实验表明基于循环卷 积网络的文本表示技术比现有的表示技术能取得更好的性能。\n关键词：自然语言处理，词向量，神经网络，表示学习，分布表示\nAbstract\nData representation is a fundamental task in machine learning. The representation of data affects the performance of the whole machine learning system. In a long history, the representation of data is done by feature engineering, and researchers aim at designing better features for specific tasks. Recently, the rapid development of deep learning and representation learning has brought new inspiration to various domains. In natural language processing, the most widely used feature representation is the Bag-of-Words model. This model has the data sparsity problem and cannot keep the word order information. Other features such as part-of-speech tagging or more complex syntax features can only fit for specific tasks inmost cases. This thesis focuses onword representation and document representation. We compare the existing systems and present our new model. First, for generating word embeddings, we make comprehensive comparisons among existing word embedding models. In terms of theory, we figure out the relationship between the two most important models, i.e., Skip-gram and GloVe. In our experiments, we analyze three key points in generating word embeddings, including the model construction, the training corpus and parameter design. We evaluate word embeddings with three types of tasks, and we argue that they cover the existing use of word embeddings. Through theory and practical experiments, we present some guidelines for how to generate a good word embedding. Second, in Chinese character or word representation, we find that the existing models always use theword embeddingmodels directly. We introduce the joint training of Chinese character and word. This method incorporates the context words into the representation space of a Chinese character, which leads to a better representation of Chinese characters and words. In the tasks of Chinese character segmentation and document classification, the joint training outperforms the existing methods that train characters or words with traditional word embedding algorithms. Third, for document representation, we analyze the existing document representation models, including recursive neural networks, recurrent neural networks and con-\niv 基于神经网络的词和文档语义向量表示方法研究\nvolutional neural networks. We point out the drawbacks of these models and present our newmodel, the recurrent convolutional neural networks. In text classification task, the experimental results show that our model outperforms the existing models.\nKeywords: Natural Language Processing, Word Embedding, Neural Network, Representation Learning, Distributional Representation\n目 录\n摘 要 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · i\nAbstract · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · iii\n目 录 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · v\n术语与符号 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · xiii\n0.1 术语 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · xiii\n0.2 符号 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · xiv\n0.3 图例 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · xv\n第一章 绪论 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 1\n1.1 研究背景 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 1\n1.2 论文结构 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 3\n第二章 现有词的分布表示技术 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 5\n2.1 分布表示 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 5\n2.1.1 基于矩阵的分布表示 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 6\n2.1.2 基于聚类的分布表示（分布聚类） · · · · · · · · · · · · · · · · · · · · · · · 7\n2.1.3 基于神经网络的分布表示（词向量） · · · · · · · · · · · · · · · · · · · · · 8\n2.2 神经网络词向量表示技术 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 8\n2.2.1 语言模型简介 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 9\n2.2.2 神经网络语言模型（NNLM） · · · · · · · · · · · · · · · · · · · · · · · · · · · · 10\n2.2.3 log双线性语言模型（LBL） · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 13\n2.2.4 循环神经网络语言模型（RNNLM） · · · · · · · · · · · · · · · · · · · · · · 14\n2.2.5 C&W模型 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 15\nvi 基于神经网络的词和文档语义向量表示方法研究\n2.2.6 CBOW模型和 Skip-gram模型 · · · · · · · · · · · · · · · · · · · · · · · · · · · · 17 2.2.7 Order模型 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 20 2.2.8 词向量模型的理论比较 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 20\n2.3 相关工作 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 24 2.4 模型总结 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 25\n第三章 词向量表示技术的实验分析 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 27\n3.1 引言 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 27 3.2 评价方法 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 29\n3.2.1 词向量的语言学特性 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 29 3.2.2 词向量用作特征 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 31 3.2.3 词向量用做神经网络初始值 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 31\n3.3 实验及分析 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 32\n3.3.1 性能增益率 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 33 3.3.2 模型比较 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 35 3.3.3 语料影响 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 40 3.3.4 参数选择 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 45\n3.4 相关工作 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 48\n3.4.1 模型比较 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 48 3.4.2 语料影响 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 50\n3.5 本章小结 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 50\n第四章 基于字词联合训练的中文表示及应用 · · · · · · · · · · · · · · · · · · · · · · · · · · · · 53\n4.1 引言 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 53 4.2 相关工作 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 54\n4.2.1 表示学习 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 54 4.2.2 中文分词 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 56\n4.3 基于字词联合训练的中文表示技术 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 57 4.4 基于字表示的分词模型 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 58\n目 录 vii\n4.5 实验及分析 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 60\n4.5.1 字词联合训练实验设置 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 60 4.5.2 字表示的实验 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 60 4.5.3 词表示的实验 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 63 4.5.4 上下文加入字的影响 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 69\n4.6 本章小结 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 71\n第五章 基于循环卷积网络的文档表示及应用 · · · · · · · · · · · · · · · · · · · · · · · · · · · · 73\n5.1 引言 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 73 5.2 相关工作 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 75\n5.2.1 组合语义 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 75 5.2.2 递归神经网络 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 76 5.2.3 循环神经网络 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 77 5.2.4 卷积神经网络 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 79 5.2.5 文本分类 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 80\n5.3 模型 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 80\n5.3.1 词表示 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 81 5.3.2 文本表示 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 82 5.3.3 模型训练 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 83\n5.4 实验设计 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 84\n5.4.1 实验设置 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 85 5.4.2 对比方法 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 85\n5.5 实验及分析 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 86\n5.5.1 20Newsgroups · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 87 5.5.2 复旦文本分类 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 89 5.5.3 ACL论文集 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 90 5.5.4 斯坦福情感树库 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 91 5.5.5 实验总结 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 94\n5.6 本章小结 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 95\nviii 基于神经网络的词和文档语义向量表示方法研究\n第六章 总结与展望 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 97\n附录 A Skip-gram模型与“词-词”矩阵分解模型关系的证明 · · · · · · · · · · · · 99\n参考文献 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 103\n发表文章目录 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 117\n简 历 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 119\n致 谢 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 121\n表 格\n2-1 分布表示模型的概要信息 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 25\n3-1 各词向量模型在目标词与上下文建模上的异同 · · · · · · · · · · · · · · · · · · · 27 3-2 词向量对比实验设置总表 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 33 3-3 各模型在完整W&N语料下的最佳性能（百分比） · · · · · · · · · · · · · · · 37 3-4 各模型在不同规模语料下性能增益率超过 95%的次数 · · · · · · · · · · · 38 3-5 若干词及其用不同模型得到的最近邻对比表 · · · · · · · · · · · · · · · · · · · · · 39 3-6 训练词向量的各语料集概要信息 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 41 3-7 使用不同语料训练 CBOW模型时各任务的性能 · · · · · · · · · · · · · · · · · · 42 3-8 若干词在 IMDB和W&N语料下的最近邻 · · · · · · · · · · · · · · · · · · · · · · · 44 3-9 混合语料训练的词向量，在 avg任务上的效果 · · · · · · · · · · · · · · · · · · · 44 3-10 不同迭代停止条件下，词向量足够好的实验数量（共 144组） · · · 47\n4-1 各模型在中文分词任务上的表现 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 62 4-2 各模型在中文语义相关性任务上的表现（×100） · · · · · · · · · · · · · · · · 65 4-3 不同字词模型得到的最近邻对比表 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 68 4-4 各模型在中文文本分类任务上的表现 · · · · · · · · · · · · · · · · · · · · · · · · · · · · 69 4-5 字词联合训练时，加入字作为词的上下文的实验结果（×100） · · · 70 4-6 字词联合训练中加入字作为词的上下文，得到的最近邻对比表 · · · 70\n5-1 循环卷积网络参数列表 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 84 5-2 文本分类数据集概要信息 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 85 5-3 各模型在 20Newsgroups数据集上的表现 · · · · · · · · · · · · · · · · · · · · · · · · · 88 5-4 各模型在复旦文本分类数据集上的表现 · · · · · · · · · · · · · · · · · · · · · · · · · · 90 5-5 各模型在 ACL论文集数据集上的表现 · · · · · · · · · · · · · · · · · · · · · · · · · · · 91 5-6 各模型在斯坦福情感树库数据集上的表现 · · · · · · · · · · · · · · · · · · · · · · · 92 5-7 循环卷积网络与递归张量网络抽取的正负情感关键短语 · · · · · · · · · · 93\n插 图\n0-1 神经网络模型结构示意图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · xv\n2-1 神经网络语言模型（NNLM）模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · 10 2-2 循环神经网络语言模型（RNNLM）模型结构图 · · · · · · · · · · · · · · · · · · 14 2-3 C&W模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 15 2-4 CBOW模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 17 2-5 Skip-gram模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 18 2-6 Order模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 20 2-7 神经网络词向量模型复杂程度对比图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · 21\n3-1 验证集损失及各指标的性能增益率随着迭代次数的变化曲线 · · · · · 46 3-2 tfl和 pos任务的性能随词向量维度的变化曲线 · · · · · · · · · · · · · · · · · · · 49\n4-1 CWE+P模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 55 4-2 SEING模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 56 4-3 字词联合训练模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 58 4-4 分词算法基本网络结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 59 4-5 各模型汉字的建模比例对词义的影响 · · · · · · · · · · · · · · · · · · · · · · · · · · · · 66\n5-1 递归神经网络模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 76 5-2 循环神经网络模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 78 5-3 卷积神经网络模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 79 5-4 循环卷积网络模型结构图 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · 81 5-5 窗口大小对卷积神经网络等模型的性能影响 · · · · · · · · · · · · · · · · · · · · · 89\n术语与符号\n0.1 术语\n• 分布假说（distributional hypothesis）：上下文相似的词，其语义也相似。该 假说由 Harris在 1954年提出 [35]，并由 Firth在 1957年进一步明确和完 善 [29]。 • 分布表示（distributional representation）：分布（distributional）描述的是上 下文的概率分布，因此用上下文描述语义的表示方法（基于分布假说的方\n法）都可以称作分布表示。与之相对的是形式语义表示。\n• 分布式表示（distributed representation）：分布式（distributed）描述的是把 信息分布式地存储在向量的各个维度中，与之相对的是局部表示（local representation），如词的独热表示（one-hot representation），在高维向量中 只有一个维度描述了词的语义。一般来说，通过矩阵降维或神经网络降维\n可以将语义分散存储到向量的各个维度中，因此，这类方法得到的低维向 量一般都可以称作分布式表示。\nxiv 基于神经网络的词和文档语义向量表示方法研究\n0.2 符号\n为了更一致地描述词和文档表示中各项技术，全文的符号系统统一如下：\n• 粗体小写字母表示列向量，如 h、p、q、e等。其中 e(w)特指词 w的词 向量，e′(w)特指词 w的辅助词向量（具体作用在模型中会有介绍）。 • 大写字母表示矩阵，常用的符号有H、W、U、A、B等。需要注意的是， 为了与常用的数学及神经网络符号统一，O 仍然为复杂度渐近上限记号，\nE 表示能量函数。\n• 双线体大写字母表示集合，具体包括：D表示数据集（包括训练词向量的 语料、训练文本分类的数据集以及训练分词模型的数据集）；R表示实数 集，Ra 表示 a维实数向量集合，Ra×b 表示 a行 b列的实数矩阵集合；V 表示词表（单词的集合）。 • 正体字表示数学函数，如 exp、max、tanh等。 • ϕ表示非线性激活函数，可能为 tanh、sigmoid（Logistic函数）、ReLU [32] 等。 • 绝对值符号 |x|对于集合表示集合的大小，如 |V|表示词表中词的总个数； 对于向量表示向量的维度，如 |e|表示词向量的维度。 • [p1;p2; . . . ;pn]表示向量 p1,p2, . . . ,pn的拼接。 • 词、词序列的描述方法：\n– vi和wi均特指词，其中 vi表示词表中的第 i个词（满足 1 ≤ i ≤ |V|）； wi表示文本序列（句子或文档）中的第 i个词。 – 词序列有两种描述方法，列举式描述w2, w3, . . . , w8与起止点描述w2:8 等价。 – c特指词 w 的上文或上下文。在语言模型中，词 wi 对应的上文 c为 w1:i−1，或 n元语言模型中的 wi−(n−1):i−1。在其它词向量模型中，词\nwi对应的上下文 c为 wi−(n−1)/2, . . . , wi−1, wi+1, . . . , wi+(n−1)/2。\n术语与符号 xv\n0.3 图例\n为了更清晰且一致地描述神经网络的结构，本文使用以下图案体系：\n表示神经网络节点（一个实数值）。\n表示向量。其中节点个数仅作示意用，不是实际的向量维度。\n表示线性变换。箭头尾部向量与矩阵相乘，得到箭头头部向量。\nw\n表示词 w的词向量。w是词的独热表示，与词向量矩阵相乘之后， 得到矩阵中的一行，也就是 w的词向量。词向量也可以简单地表 示成 w 。两种方法含义一致，仅根据排版需要而选择。\n原始文本\n输入层\n隐藏层\n输出层\nw1 w2 w3\n图 0-1 神经网络模型结构示意图\n图 0-1展示了一个三层前馈网络的图。其中第一层由 3个词向量拼接而成， 经过线性变换，得到隐藏层。输入层到隐藏层可以看作三个向量与三个矩阵相 乘，求和得到隐藏层；也可以理解为三个向量拼接成一个更长的向量，与一个 大矩阵相乘，得到隐藏层。一般而言，神经网络的隐藏层都需要通过一个非线 性的激活函数 ϕ，因此在图中省去这部分的表示。最后隐藏层通过线性变换得 到输出层，输出层一共有 4个节点。\n第一章 绪论\n1.1 研究背景\n数据表示是机器学习中的基础工作，数据表示的好坏直接影响到整个机器\n学习系统的性能 [5]。因此，人们投入了大量精力去研究如何针对具体任务，设 计一种合适的数据表示方法，以提升机器学习系统的性能，这一环节也被称作 特征工程。传统机器学习方法不能直接从数据中自动挖掘出有判别力的信息， 而特征工程正是通过人类的智慧、知识和灵感来弥补机器学习方法的这一缺陷。 在自然语言处理领域，最常用的文本表示方法是词袋子表示 [128]，该方法会面 临数据稀疏问题，并且不能保留词序信息。研究人员针对这些缺陷，还提出了 词法特征、句法特征等复杂特征。借助这些人工精心设计的特征，机器学习在 自然语言领域逐步取代了以往基于规则的方法，成为自然语言处理中的主流方 法。\n特征工程在传统机器学习算法中，有着不可替代的地位，但是由于需要大\n量人力和专业知识，反而成为了机器学习系统性能提升的瓶颈。为了让机器学 习算法有更好的扩展性，研究人员希望可以减少对特征工程的依赖。这样，当 把机器学习算法推广到新的领域中时，就可以省去大量专家在新领域上的分析 和探索，加快应用的进程，使得系统更为智能。从人工智能的角度看，算法直 接从原始的感知数据中自动分辨出有效的信息，是机器走向智能的重要一步。\n近年来，随着Web2.0的兴起，互联网上的数据急剧膨胀。根据国际数据公 司（IDC）的统计和预测，2011年全球网络数据量已经达到 1.8ZB，到 2020年， 全球数据总量预计还将增长 50倍。大量无标注数据的出现，也让研究人员开始 考虑，如何利用算法从这些大规模无标注的数据中自动挖掘规律，得到有用的 信息。2006年 Hinton提出的深度学习 [40]，为解决这一问题带来了新的思路。 在之后的发展中，基于神经网络的表示学习技术开始在各个领域崭露头角。尤 其在图像和语音领域的多个任务上，基于表示学习的方法在性能上均超过了传 统方法。\n但是，在自然语言处理领域，深度学习技术并没有产生类似图像和语音领\n域那样的突破。其中一个主要的原因是，在图像和语音领域，最基本的数据是\n2 基于神经网络的词和文档语义向量表示方法研究\n信号数据，我们可以通过一些距离度量，判断信号是否相似。而文本是符号数 据，两个词只要字面不同，就难以刻画它们之间的联系，即使是“麦克风”和\n“话筒”这样的同义词，从字面上也难以看出这两者意思相同（语义鸿沟现象）。\n正因为这样，在判断两幅图片是否相似时，只需通过观察图片本身就能给出回 答；而判断两个词是否相似时，还需要更多的背景知识才能做出回答。\n我们希望计算机可以从大规模无标注的文本数据中自动学习得到文本表\n示，这种表示需要包含对应语言单元（词或文档）的语义信息，同时可以直接 通过这种表示度量文本之间的语义相似度。\n1954年，Harris提出分布假说（distributional hypothesis），即“上下文相似的 词，其语义也相似”[35]，为词的分布表示提供了理论基础。基于分布假说，研究 人员提出了多种词表示模型：如基于矩阵的 LSA模型 [56]、基于聚类的 Brown clustering模型 [12]以及本文关注的神经网络词表示模型，本文第二章对这些模 型进行了综述。在分布假说中，需要关注的对象有两个：词和上下文，其中最 关键的是上下文的表示。在前两个模型中，上下文只能使用传统的词袋子表示， 如果需要表示复杂的上下文，会遇到维数灾难问题。而神经网络模型可以使用 组合方式对上下文进行建模，只需线性复杂度即可对复杂的 n元短语进行建模。 神经网络模型生成的词表示通常被称为词向量（word embedding），是一个低维 的实数向量表示，通过这种表示，可以直接对词之间的相似度进行刻画。相比 传统的词袋子表示方法以及矩阵、聚类等衍生方法，词向量可以缓解维数灾难 的问题。从广义上讲，传统的词袋子模型也是用向量描述文本，也应当被称作 词的向量表示，但是这种向量是高维稀疏的。在本文中，“词向量”特指由神经 网络模型得到的低维实数向量表示。\n对于文本分类、信息检索等实际需求而言，仅使用词级别的语义表示不足以\n有效地完成这些任务，因此还需要通过模型，得到句子和文档级别的语义表示。 但是，由于文档的多样性，直接使用分布假说构建文档的语义向量表示时，会遇 到严重的数据稀疏问题；同时由于分布假说是针对词义的假说，这种通过上下 文获取语义的方式对句子和文档是否有效，还有待讨论。为了获得句子和文档 的语义表示，研究人员一般采用语义组合的方式。德国数学家弗雷格（Gottlob Frege）在 1892年提出：一段话的语义由其各组成部分的语义以及它们之间的 组合方法所确定 [30]。现有的句子或者文档表示也通常以该思路为基础，通过 语义组合的方式获得。主流的神经网络语义组合方法包括递归神经网络、循环\n第一章 绪论 3\n神经网络和卷积神经网络，这些方法采用了不同的组合方式从词级别的语义组 合到句子和文档级别。\n1.2 论文结构\n本文着眼于基于神经网络的词和文档表示技术，通过理论分析和实验比较，\n探索现有方法的联系和区别，比较其优劣，并提出自己的文本表示技术。\n本文一共分为六章，后续章节安排如下：\n• 第二章介绍了以分布假说为基础的分布表示的体系，系统地总结对比了基 于矩阵、基于聚类和基于神经网络的分布表示方法，并详细介绍了基于神\n经网络的词向量表示技术。\n• 第三章对现有的词向量表示技术进行了系统实验分析。具体从模型、语料 和训练参数三个角度分析训练词向量的关键技术。本文选取了三大类一共\n八个指标对词向量进行评价，这三大类指标涵盖了现有的词向量用法。在 此基础上，通过理论和实验的比较分析，提出了一系列针对生成词向量的 参考建议。\n• 第四章提出了基于字词联合训练的中文字词表示方法。现有的中文表示技 术往往沿用了英文的思路，直接从词的层面对文本表示进行构建。本文根\n据中文的特点，提出了基于字词联合训练的中文表示技术。该方法在字的 上下文空间中融入了词，利用词的语义空间，更好地对汉字建模。同时也 利用字，加深了对词义的建模。实验表明，字词联合训练对字表示和词表 示均有一定的提升。\n• 第五章综述了现有的神经网络文档向量表示技术,并针对现有的三种表示 技术的不足，提出了基于循环卷积网络的文档表示技术。该方法克服了此\n前递归网络复杂度过高的问题，循环网络的语义偏置问题，以及卷积网络 窗口较难选择的问题。文章在文本分类任务上对新提出的表示技术进行了 对比分析，实验表明，基于循环卷积网络的文本表示技术比现有的表示技 术有更好的性能。\n• 第六章对前面的工作进行了总结，并提出了进一步研究的展望。\n第二章 现有词的分布表示技术\n本章首先介绍以分布假说为基础的分布表示方法体系，然后详细介绍其中\n基于神经网络的分布表示方法（词向量），最后总结以上所有方法，并阐述它们 之间的联系和区别。\n2.1 分布表示\n词是承载语义的最基本的单元 [1]，而传统的独热表示（one-hot representation）仅仅将词符号化，不包含任何语义信息。如何将语义融入到词表示中？ Harris在 1954年提出的分布假说（distributional hypothesis）为这一设想提供了 理论基础：上下文相似的词，其语义也相似 [35]。Firth在 1957年对分布假说进 行了进一步阐述和明确：词的语义由其上下文决定（a word is characterized by the company it keeps）[29]。二十世纪 90年代初期，统计方法在自然语言处理中逐渐 成为主流，分布假说也再次被人关注。Dagan和 Schütze等人总结完善了利用上 下文分布表示词义的方法，并将这种表示用于词义消歧等任务 [20, 21, 100, 101]， 这类方法在当时被成为词空间模型（word space model）。在此后的发展中，这 类方法逐渐演化成为基于矩阵的分布表示方法，期间的十多年时间里，这类方 法得到的词表示都被直接称为分布表示（distributional representation）。1992年， Brown等人同样基于分布假说，构造了一个上下文聚类模型，开创了基于聚类 的分布表示方法 [12]。2006年之后，随着硬件性能的提升以及优化算法的突破， 神经网络模型逐渐在各个领域中发挥出自己的优势，使用神经网络构造词表示 的方法可以更灵活地对上下文进行建模，这类方法开始逐渐成为了词分布表示 的主流方法。\n到目前为止，基于分布假说的词表示方法，根据建模的不同，主要可以分\n为三类：基于矩阵的分布表示（2.1.1小节）、基于聚类的分布表示（2.1.2小节） 和基于神经网络的分布表示（2.1.3 小节）。从广义上看，所有基于分布假说得 到的表示均可称为分布表示（distributional representation），如上述的三种。而狭 义的分布表示通常指基于矩阵的分布表示 [116]。本文以 Christopher Manning的 观点为基准1，文中出现的“分布表示”均指广义的分布表示，其它观点参考本\n1Manning在 2015年深度学习暑期学校（蒙特利尔）中澄清，分布（distributional）意为使用上下文表\n6 基于神经网络的词和文档语义向量表示方法研究\n章 2.3小节的论述。\n尽管这些不同的分布表示方法使用了不同的技术手段获取词表示，但由于\n这些方法均基于分布假说，它们的核心思想也都由两部分组成：一、选择一种 方式描述上下文；二、选择一种模型刻画某个词（下文称“目标词”）与其上下 文之间的关系。上文介绍的矩阵、聚类和神经网络三种方法，采用了不同的方 式对上下文和目标词之间的关系进行建模。以下三个小节将简要介绍这三类模 型。\n2.1.1 基于矩阵的分布表示\n基于矩阵的分布表示通常又称为分布语义模型（distributional semantic models）[3]，一些文献中也直接将其称作分布表示（distributional representation）[116]。 这类方法需要构建一个“词-上下文”矩阵，从矩阵中获取词的表示。在“词-上 下文”矩阵中，每行对应一个词，每列表示一种不同的上下文，矩阵中的每个 元素对应相关词和上下文的共现次数。在这种表示下，矩阵中的一行，就成为 了对应词的表示，这种表示描述了该词的上下文的分布。由于分布假说认为上 下文相似的词，其语义也相似，因此在这种表示下，两个词的语义相似度可以 直接转化为两个向量的空间距离。这类方法具体可以分为三个步骤：\n一、选取上下文。最常见的有三种方法：第一种，将词所在的文档作为上\n下文，形成“词-文档”矩阵 [56]；第二种，将词附近上下文中的各个词（如上 下文窗口中的 5个词）作为上下文，形成“词-词”矩阵 [70, 90]；第三种，将 词附近上下文各词组成的 n元词组（n-gram）作为上下文 [45]。在这三种方法 中，“词-文档”矩阵非常稀疏，而“词-词”矩阵相对较为稠密，效果一般好于 前者。“词-n元词组”相对“词-词”矩阵保留了词序信息，建模更精确，但由于 比前者更稀疏，实际效果不一定能超越前者。 二、确定矩阵中各元素的值。“词-上下文”共现矩阵根据其定义，里面各元 素的值应为词与对应的上下文的共现次数。然而直接使用原始共现次数作为矩 阵的值在大多数情况下效果并不好 [3]，因此研究人员提出了多种加权和平滑方 法，最常用的有 tf-idf、PMI和直接取 log。 三、矩阵分解（可选）。在原始的“词-上下文”矩阵中，每个词表示为一个 非常高维（维度是不同上下文的总个数）且非常稀疏的向量，使用降维技术可以\n达语义。http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/\n第二章 现有词的分布表示技术 7\n将这一高维稀疏向量压缩成低维稠密向量。降维技术可以减少噪声带来的影响， 但也可能损失一部分信息。最常用的分解技术包括奇异值分解（SVD）、非负矩 阵分解（NMF）、典型关联分析（Canonical Correlation Analysis，CCA）[22, 23]、 Hellinger PCA（HPCA）[59]。\n基于矩阵的分布表示在这些步骤的基础上，衍生出了若干不同方法，如经\n典的 LSA [56]就是使用“词-文档”矩阵，tf-idf作为矩阵元素的值，并使用 SVD 分解，得到词的低维向量表示。在这类方法中，最新的为 GloVe模型 [90]，下 文简单介绍这一模型。\nGlobal Vector模型（GloVe）\n总体上看，GloVe模型是一种对“词-词”矩阵进行分解从而得到词表示的 方法。矩阵第 i 行第 j 列的值为词 vi 与词 vj 在语料中的共现次数 xij 的对数。 在矩阵分解步骤，GloVe模型借鉴了推荐系统中基于隐因子分解（Latent Factor Model）的方法 [4, 88]，在计算重构误差时，只考虑共现次数非零的矩阵元素， 同时对矩阵中的行和列加入了偏移项。具体为最小化下式：\n∑ i,j∈V,xij ̸=0 f (xij) ( log(xij)− pTi qj + b (1) i + b (2) j )2 (2.1)\n其中 pi为词 vi作为目标词时的词向量，qj为词 vj作为上下文时的词向量，b(1)、 b(2) 为针对词表中各词的偏移向量，f(x)是一个加权函数，对低频的共现词对 进行衰减，减少低频噪声带来的误差，定义为：\nf(x) = (x/xmax)α 如果 x < xmax1 其它情况 (2.2) 2.1.2 基于聚类的分布表示（分布聚类）\n基于聚类的分布表示又称作分布聚类（distributional clustering）[91]，这类 方法通过聚类手段构建词与其上下文之间的关系。其中最经典的方法是布朗聚 类（Brown clustering）[12]。布朗聚类是一种层级聚类方法，聚类结果为每个词 的多层类别体系。因此可以根据两个词的公共类别判断这两个词的语义相似度。\n8 基于神经网络的词和文档语义向量表示方法研究\n具体而言，布朗聚类需要最大化以下似然，其中 ci为词 wi对应的类别：\nP (wi|wi−1) = P (wi|ci)P (ci|ci−1)\n布朗聚类只考虑了相邻词之间的关系，也就是说，每个词只使用它的上一个词， 作为上下文信息。\n除了布朗聚类以外，还有若干基于聚类的表示方法 [69, 91]。由于这类方法 不是本文的重点，在此不再赘述。\n2.1.3 基于神经网络的分布表示（词向量）\n基于神经网络的分布表示一般称为词向量、词嵌入（word embedding）或分 布式表示（distributed representation）[116]。神经网络词向量表示技术通过神经 网络技术对上下文，以及上下文与目标词之间的关系进行建模。由于神经网络 较为灵活，这类方法的最大优势在于可以表示复杂的上下文。在前面基于矩阵 的分布表示方法中，最常用的上下文是词。如果使用包含词序信息的 n-gram作 为上下文，当 n增加时，n-gram的总数会呈指数级增长，此时会遇到维数灾难 问题。而神经网络在表示 n-gram 时，可以通过一些组合方式对 n 个词进行组 合，参数个数仅以线性速度增长。有了这一优势，神经网络模型可以对更复杂 的上下文进行建模，在词向量中包含更丰富的语义信息。下一节将详细介绍不 同神经网络模型是如何对上下文以及上下文与目标词之间的关系进行建模的。\n2.2 神经网络词向量表示技术\n神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依\n然是上下文的表示以及上下文与目标词之间的关系的建模。构建上下文与目标 词之间的关系，最自然的一种思路就是使用语言模型。从历史上看，早期的词 向量只是神经网络语言模型的副产品。同时，神经网络语言模型对后期词向量 的发展方向有着决定性的作用。因此，本节首先依照词向量模型的发展顺序介 绍各模型的来龙去脉，其中 2.2.1小节简述语言模型，2.2.2到 2.2.4小节介绍三 种经典的神经网络语言模型，2.2.5到 2.2.7小节介绍专门用于生成词向量的神 经网络模型。最后 2.2.8小节从上下文的表示、上下文与目标词之间的关系归纳 分析这些模型。\n第二章 现有词的分布表示技术 9\n2.2.1 语言模型简介\n语言模型可以对一段文本的概率进行估计，对信息检索、机器翻译、语音\n识别等任务有着重要的作用。\n形式化讲，统计语言模型的作用是为一个长度为m的字符串确定一个概率\n分布 P (w1, w2, ..., wm)，表示其存在的可能性，其中 w1 到 wm 依次表示这段文 本中的各个词。一般在实际求解过程中，通常采用下式计算其概率值：\nP (w1, w2, ..., wm) = P (w1) P (w2|w1) P (w3|w1, w2)\n. . . P (wi | w1, w2, ..., wi−1) . . . P (wm | w1, w2, ..., wm−1) (2.3)\n在实践中，如果文本的长度较长，公式 2.3右部 P (wi | w1, w2, . . . , wi−1)的 估算会非常困难。因此，研究者们提出使用一个简化模型：n 元模型（n-gram model）。在 n 元模型中估算条件概率时，距离大于等于 n 的上文词会被忽略， 也就是对上述条件概率做了以下近似：\nP (wi | w1, w2, ..., wi−1) ≈ P (wi | wi−(n−1), . . . , wi−1) (2.4)\n当 n = 1时又称一元模型（unigram model），公式 2.4右部会退化成 P (wi)， 此时，整个句子的概率为：P (w1, w2, ..., wm) = P (w1)P (w2) . . . P (wm)。从式中 可以知道，一元语言模型中，文本的概率为其中各词概率的乘积。也就是说，模 型假设了各个词之间都是相互独立的，文本中的词序信息完全丢失。因此，该 模型虽然估算方便，但性能有限。\n当 n = 2 时又称二元模型（bigram model），将 n 代入公式 2.4 中，右部 为 P (wi|wi−1)。常用的还有 n = 3时的三元模型（trigram model），使用 P (wi | wi−2, wi−1)作为近似。这些方法均可以保留一定的词序信息。\n在 n元模型中，传统的方法一般采用频率计数的比例来估算 n元条件概率：\nP (wi | wi−(n−1), . . . , wi−1) = count(wi−(n−1), . . . , wi−1, wi) count(wi−(n−1), . . . , wi−1)\n(2.5)\n其中，count(wi−(n−1), . . . , wi−1)表示文本序列wi−(n−1), . . . , wi−1在语料中出现的\n10 基于神经网络的词和文档语义向量表示方法研究\n次数。\n为了更好地保留词序信息，构建更有效的语言模型，我们希望在 n元模型\n中选用更大的 n。但是，当 n较大时，长度为 n序列出现的次数就会非常少，在 按照公式 2.5估计 n元条件概率时，就会遇到数据稀疏问题，导致估算结果不 准确。因此，一般在百万词级别的语料中，三元模型是比较常用的选择 [63]，同 时也需要配合相应的平滑算法，进一步降低数据稀疏带来的影响 [33, 51]。\n为了更好地解决 n元模型估算概率时遇到的数据稀疏问题，神经网络语言\n模型应运而生。\n2.2.2 神经网络语言模型（NNLM）\nXu 等人在 2000 年首次尝试使用神经网络求解二元语言模型 [124]。2001 年，Bengio等人正式提出神经网络语言模型（Neural Network Language Model， NNLM）[6, 7]。该模型在学习语言模型的同时，也得到了词向量。 NNLM同样也是对 n元语言模型进行建模，估算 P (wi | wi−(n−1), . . . , wi−1) 的值。但与传统方法不同的是，NNLM 不通过计数的方法对 n 元条件概率进 行估计，而是直接通过一个神经网络结构，对其进行建模求解。图 2-1展示了 NNLM的基本结构。\n原始文本\n输入层\n隐藏层\n输出层\nx\nwi−(n−1) wi−2 wi−1\nh\ny\n|V|\nwi\ne(wi−(n−1))\n. . .\ne(wi−2) e(wi−1)\n图 2-1 神经网络语言模型（NNLM）模型结构图\n具体而言，对语料中一段长度为 n的序列 wi−(n−1), . . . , wi−1, wi，n元语言\n第二章 现有词的分布表示技术 11\n模型需要最大化以下似然：\nP (wi | wi−(n−1), . . . , wi−1) (2.6)\n其中，wi为需要通过语言模型预测的词（目标词）。对于整个模型而言，输入为 条件部分的整个词序列：wi−(n−1), . . . , wi−1，输出为目标词的分布。\n神经网络语言模型采用普通的三层前馈神经网络结构，其中第一层为输入\n层。Bengio提出使用各词的词向量作为输入以解决数据稀疏问题，因此输入层 为词 wi−(n−1), . . . , wi−1的词向量的顺序拼接：\nx = [e(wi−(n−1)); . . . ; e(wi−2); e(wi−1)] (2.7)\n当输入层完成对上文的表示 x之后，模型将其送入剩下两层神经网络，依\n次得到隐藏层 h和输出层 y：\nh = tanh(b(1) +Hx) (2.8)\ny = b(2) +Wx+ Uh (2.9)\n其中 H ∈ R|h|×(n−1)|e| 为输入层到隐藏层的权重矩阵，U ∈ R|V|×|h| 为隐藏层到 输出层的权重矩阵，|V|表示词表的大小，|e|表示词向量的维度，|h|为隐藏层 的维度。b(1)、b(2)均为模型中的偏置项。矩阵W ∈ R|V|×(n−1)|e|表示从输入层到 输出层的直连边权重矩阵。由于W 的存在，该模型可能会从非线性的神经网络 退化成为线性分类器。Bengio等人在文中指出，如果使用该直连边，可以减少 一半的迭代次数；但如果没有直连边，可以生成性能更好的语言模型。因此在 后续工作中，很少有使用输入层到输出层直连边的工作，下文也直接忽略这一 项。如果不考虑W 矩阵，整个模型计算量最大的操作，就是从隐藏层到输出层 的矩阵运算 Uh，后续的模型均有对这一操作的优化（2.2.3节到 2.2.6节）。\n输出层一共有 |V|个元素，依次对应下一个词为词表中某个词的可能性。这 里将其中对应词 w 的元素记作 y(w)。由于神经网络的输出层并不直接保证各 元素之和为 1，输出层的 y 并不是概率值。因此，在输出层 y 之后，需要加入\n12 基于神经网络的词和文档语义向量表示方法研究\nsoftmax函数，将 y转成对应的概率值：\nP (wi | wi−(n−1), . . . , wi−1) = exp (y(wi))∑|V| k=1 exp (y(vk))\n(2.10)\n神经网络语言模型之所以能对 n元条件概率进行更好的建模，缓解数据稀\n疏问题，是由于它使用词序列的词向量对上文进行表示；而传统语言模型使用 的是各词的独热表示作为上文的表示。词的独热表示（one-hot representation）是 一个 |V|维向量。对于词 vi，其独热表示向量中，只有第 i维是 1，其余各维均 是 0。在长度为 n− 1的上文序列中，如果采用独热表示，则是一个 |V|n−1维的 0/1向量，空间非常稀疏；而使用词向量表示时，则是一个 (n− 1)|e|维的实数 向量。利用这种低维的实数表示，可以使相似的上文预测出相似的目标词，而 传统模型中只能通过相同的上下文预测出相同的目标词。\n神经网络语言模型中的词向量出现在两个地方。在输入层中，各词的词向量\n存于一个 |e|× |V|维的实数矩阵中。词 w到其词向量 e(w)的转化就是从该矩阵 中取出一列。值得注意的是，隐藏层到输出层的权重矩阵 U 的维度为 |V| × |h|， 可以将其看做 |V|个 |h|维的行向量，其中的每一个向量，均可以看做某个词在 模型中的另一个表示 e′。在不考虑 W 的情况下，每个词在模型中有两套词向 量，其中 e(w)为词 w作为上下文时的表示，而 e′(w)为词 w作为目标词时的表 示。由于 Bengio 等人的工作只考虑对语言模型的建模，词向量只是其副产品， 因此他们并没有指出哪一套向量作为词向量效果更好。在其他关注词向量的工 作中，通常只使用 e作为词向量，具体在后文中会有讨论。\n如上文所述，输出层的分量 y(wi)描述的是在上文为 wi−(n−1), . . . , wi−1 的\n条件下，下一个词为wi的可能性，该分量体现了上文序列与目标词之间的关系。 通常，y(wi)又被称作能量函数，记作 E(wi;wi−(n−1):i−1)：\ny(wi) = b (2) + e′(wi) T tanh ( b(1) +H [ e(wi−(n−1)); . . . ; e(wi−1) ]) = E(wi;wi−(n−1):i−1)\n(2.11)\n对于整个语料而言，语言模型需要最大化：\n∑ wi−(n−1):i∈D logP (wi | wi−(n−1), . . . , wi−1) (2.12)\n第二章 现有词的分布表示技术 13\n训练时，神经网络语言模型使用随机梯度下降法 [10]来优化上述训练目标。 每次迭代，随机从语料 D中选取一段文本 wi−(n−1), . . . , wi作为训练样本，使用 下式进行一次梯度迭代：\nθ ← θ + α ∂ logP (wi | wi−(n−1), . . . , wi−1)\n∂θ (2.13)\n式中，α是学习速率；θ为模型中的所有参数，包括词向量和网络结构中的权重 U、H、b(1)、b(2)。\n2.2.3 log双线性语言模型（LBL）\n2007年，Mnih和Hinton在神经网络语言模型（NNLM）的基础上提出了 log 双线性语言模型（Log-Bilinear Language Model，LBL）[79]。LBL与 NNLM的 区别正如它们的名字所示，LBL的模型结构是一个 log双线性结构；而 NNLM 的模型结构为神经网络结构。具体来讲，LBL模型的能量函数为：\nE(wi;wi−(n−1):i−1) = b (2) + e(wi) Tb(1)+\ne(wi) TH [ e(wi−(n−1)); . . . ; e(wi−1) ] (2.14) LBL模型的能量函数（公式 2.14）与 NNLM的能量函数（公式 2.11）主要\n有两个区别。一、LBL模型中，没有非线性的激活函数 tanh，而由于 NNLM是 非线性的神经网络结构，激活函数必不可少；二、LBL模型中，只有一份词向 量 e，也就是说，无论一个词是作为上下文，还是作为目标词，使用的是同一份 词向量。其中第二点（只有一份词向量），只在原版的 LBL模型中存在，后续的 改进工作均不包含这一特点。 之后的几年中，Mnih等人在 LBL模型的基础上做了一系列改进工作。其 中最重要的模型有两个：层级 log双线性语言模型（Hierarchical LBL，HLBL） [80]和基于向量的逆语言模型（inverse vector LBL，ivLBL）[81]。以下分别介 绍这两个模型所用的技术。\n层级 softmax\nHLBL模型 [80]采用了 Bengio在 2005年提出的层级 softmax函数 [82]，加 速了目标层的求解。传统的 softmax函数如公式 2.10，分母所示的归一化项，由\n14 基于神经网络的词和文档语义向量表示方法研究\n于需要得到 y 中的每个分量的值，计算非常耗时。而层级 softmax函数通过构 造一个树形结构，使求解某个分量对应的概率值时，只需要O(log(|V|))次运算， 而不需要此前的 O(|V|)次运算，极大地降低了模型的时间复杂度。\n噪声对比估算\n2013年提出的基于向量的逆语言模型（ivLBL）[81]虽然名字上仍然继承了 log双线性语言模型（LBL），实际上已经抛弃了 log双线性结构，转而采用了向 量点积结构。该模型在 Skip-gram模型（见 2.2.6节）的基础上，使用噪声对比 估算（noise-contrastive estimation，NCE）[34]加速 softmax的估计，将 softmax 的复杂度降低到常数级别。\n2.2.4 循环神经网络语言模型（RNNLM）\n2.2.2节提到的 NNLM以及 2.2.3提到的 LBL模型均为 n元模型。Mikolov 等人提出的循环神经网络语言模型（Recurrent Neural Network based Language Model，RNNLM）则直接对P (wi | w1, w2, ..., wi−1)进行建模，而不使用公式 2.4对 其进行简化 [72, 74]。因此，RNNLM可以利用所有的上文信息，预测下一个词， 其模型结构如图 2-2所示。\n|V|\nh(i)\n原始文本\n输入层\n隐藏层\n输出层 y\nh(i− 1) wi e(wi)\nwi+1\n+ W\n图 2-2 循环神经网络语言模型（RNNLM）模型结构图\nRNNLM的核心在于其隐藏层的算法：\nh(i) = ϕ(e(wi) +Wh(i− 1)) (2.15)\n第二章 现有词的分布表示技术 15\n其中，ϕ 为非线性激活函数。该式对应 NNLM 的公式 2.8。但与 NNLM 不同， RNNLM并不采用 n元近似，而是使用迭代的方式直接对所有上文进行建模。在 公式 2.15中，h(i)表示文本中第 i个词 wi所对应的隐藏层，该隐藏层由当前词 的词向量 e(wi)以及上一个词对应的隐藏层 h(i− 1)结合得到。\n隐藏层的初始状态为 h(0)，随着模型逐个读入语料中的词 w1, w2, . . .，隐\n藏层不断地更新为 h(1),h(2), . . .。根据公式 2.15，每一个隐藏层包含了当前词 的信息以及上一个隐藏层的信息。通过这种迭代推进的方式，每个隐藏层实际 上包含了此前所有上文的信息，相比 NNLM只能采用上文 n元短语作为近似， RNNLM包含了更丰富的上文信息，也有潜力达到更好的效果。\nRNNLM的输出层计算方法与 NNLM的输出层一致，可见公式 2.9。\n2.2.5 C&W模型\n与前面的三个基于语言模型的词向量生成方法不同，Collobert和Weston在 2008年提出的 C&W模型 [17]，是第一个直接以生成词向量为目标的模型。\n原始文本\n输入层\n隐藏层\n输出层\nx\nh\ny\nwi−(n−1)/2\ne(wi−(n−1)/2)\nwi−1\ne(wi−1)\n. . .\nwi+1\ne(wi+1)\nwi+(n−1)/2\ne(wi+(n−1)/2)\nscore\n. . .\ne(∗)\n∗\n图 2-3 C&W模型结构图\n语言模型的目标为求解 P (wi | w1, w2, ..., wi−1)，其中隐藏层到输出层的矩 阵运算是最耗费时间的部分。因此，前面的各个词向量模型中，几乎都有对这 一部分做优化的步骤，如层级 softmax、分组 softmax和噪声对比估算。C&W模 型的目标是更快速地生成词向量，因此它并没有采取语言模型的方式，去求解 上述条件概率，转而采用了另一种更高效的方法，直接对 n元短语打分。对于 语料中出现过的 n元短语，模型会对其打高分；而对于语料中没有出现的随机 短语，模型会对其打低分。通过这种方式，C&W模型可以更直接地学习得到符 合分布假说的词向量。\n16 基于神经网络的词和文档语义向量表示方法研究\n具体而言，对于整个语料，C&W模型需要最小化：\n∑ (w,c)∈D ∑ w′∈V max(0, 1− score(w, c) + score(w′, c)) (2.16)\n其中，(w, c)为从语料中选出的一个 n元短语 wi−(n−1)/2, . . . , wi+(n−1)/2，一般 n 为奇数，以保证上文和下文的词数一致；w为序列中的中间词，即 wi，在该模 型中为目标词；c表示目标词 w的上下文；w′ 为字典中的某一个词。模型采用 pairwise的方式 [16]对文本片段进行优化，希望正样本的打分要比负样本的打 分至少高 1分。正样本 (w, c)来自语料，而负样本 (w′, c)则是将正样本序列中 的中间词替换成其它词。形式化地，目标词 w和上下文 c分别为：\nw = wi\nc = wi−(n−1)/2, . . . , wi−1, wi+1, . . . , wi+(n−1)/2 (2.17)\n代入公式 2.16中，正负样本分别为：\n(w, c) = wi−(n−1)/2, . . . , wi+(n−1)/2\n(w′, c) = wi−(n−1)/2, . . . , wi−1, w ′, wi+1, . . . , wi+(n−1)/2\n(2.18)\n在大多数情况下，把一个普通短语的中间词随机替换成其它词，得到的都是不 正确的短语，所以这样构造的负样本是有效的（多数情况下确实是负样本，极 少数情况下把正确的短语当作负样本，但是不会影响模型整体的效果）。同时， 由于负样本仅仅是修改了正样本中的一个词，也不会让分类面距离负样本太远 而影响分类效果。\nC&W模型（图 2-3）与NNLM（图 2-1）相比，主要的不同点在于 C&W模型 将目标词放到了输入层，同时输出层也从语言模型的 |V|个节点变为一个节点， 这个节点的数值表示对这组 n元短语的打分。打分只有高低之分，没有概率的 特性，因此无需复杂的归一化操作。C&W模型使用这种方式把 NNLM模型在 最后一层的 |V| × |h|次运算降为 |h|次运算，极大地降低了模型的时间复杂度。 这个区别使得 C&W模型成为神经网络词向量模型中最为特殊的一个，其它模 型的目标词均在输出层，只有 C&W模型的目标词在输入层。由这一改变带来 的影响，可以参见第三章 3.3.2节的分析。\n第二章 现有词的分布表示技术 17\n+\n原始文本\n输入层\n输出层\nx\ny\n|V|\nwi\nwi−(n−1)/2\ne(wi−(n−1)/2)\nwi−1\ne(wi−1)\nwi+1\ne(wi+1)\nwi+(n−1)/2\ne(wi+(n−1)/2)\n. . . . . .\n图 2-4 CBOW模型结构图\n2.2.6 CBOW模型和 Skip-gram模型\nMikolov等人在 2013年的文献 [73]中，同时提出了CBOW（Continuous Bagof-Words）和 Skip-gram模型。他们设计两个模型的主要目的是希望用更高效的 方法获取词向量。因此，他们根据前人在 NNLM、RNNLM和 C&W模型上的 经验，简化现有模型，保留核心部分，得到了这两个模型。\nCBOW模型\nCBOW模型的结构如图 2-4，该模型一方面根据 C&W模型的经验，使用一 段文本的中间词作为目标词；另一方面，又以 NNLM作为蓝本，并在其基础上 做了两个简化。一、CBOW没有隐藏层，去掉隐藏层之后，模型从神经网络结 构直接转化为 log线性结构，与 Logistic回归一致。log线性结构比三层神经网 络结构少了一个矩阵运算，大幅度地提升了模型的训练速度。二、CBOW去除 了上下文各词的词序信息，使用上下文各词词向量的平均值2，代替神经网络语 言模型使用的上文各词词向量的拼接。形式化地，CBOW模型对于一段训练样 本 wi−(n−1), . . . , wi，输入为：\nx = 1 n− 1 ∑ wj∈c e(wj) (2.19)\n2Mokolov在文献 [73]中使用的是求和，但是在后来 word2vec的工具包中将求和改进为平均值。\n18 基于神经网络的词和文档语义向量表示方法研究\n由于没有隐藏层，CBOW模型的输入层直接就是上下文的表示。CBOW模型根 据上下文的表示，直接对目标词进行预测：\nP (w|c) = exp (e ′(w)Tx)∑ w′∈V exp ( e′(w′)Tx ) (2.20) 上述二式中，目标词 w 和上下文 c 的定义与 C&W 模型（2.2.5 节）中的公 式 2.17 一致。对于整个语料而言，与神经网络语言模型类似，CBOW 的优化 目标为最大化： ∑\n(w,c)∈D\nlogP (w|c) (2.21)\n原始文本\n输入层\n输出层\nx\ny\nwi−(n−1)/2\ne(wi−(n−1)/2)\nwi−1\ne(wi−1)\nwi+1\ne(wi+1)\nwi+(n−1)/2\ne(wi+(n−1)/2)\nwi\n|V|\n. . . . . .\n图 2-5 Skip-gram模型结构图\nSkip-gram模型\nSkip-gram模型的结构如图 2-5，与 CBOW模型一样，Skip-gram模型中也没 有隐藏层。和 CBOW模型不同的是，Skip-gram模型每次从目标词 w的上下文 c 中选择一个词，将其词向量作为模型的输入 x，也就是上下文的表示。Skip-gram 模型同样通过上下文预测目标词3，对于整个语料的优化目标为最大化：\n∑ (w,c)∈D ∑ wj∈c logP (w|wj) (2.22)\n3本文为了各模型之间的一致性，将 Skip-gram模型描述成通过上下文预测目标词，而在 Skip-gram的 论文 [73]中将模型描述成通过目标词预测上下文。由于模型需要遍历整个语料，任意一个窗口中的两个 词 wa, wb 都需要计算 P (wa|wb) + P (wb|wa)，因此这两种描述方式是等价的。\n第二章 现有词的分布表示技术 19\n其中，\nP (w|wj) = exp (e′(w)Te(wj))∑ w′∈V exp ( e′(w′)Te(wj) ) (2.23) 负采样技术\nMikolov等人在 2013年提出了负采样技术（negative sampling），进一步提 升了最后一层的效率 [75]。负采样技术借鉴了 C&W模型采用的构造负样本的 方法，还参考了 ivLBL模型所用的 NCE方法，最后构造出了一个优化目标，最 大化正样本的似然，同时最小化负样本的似然。 负采样技术与 C&W 模型中相应部分的区别主要是，负采样技术不采用 pairwise 的方式训练，因此，一个正样本可以对应多个负样本，Mikolov 等人 在实验中论述了使用多个负样本（一般选 5）能有效提升模型的性能。 负采样技术与 NCE技术的主要区别是，负采样技术仅仅是优化正负样本的 似然，而不对输出层做概率归一化。NCE技术则是通过噪声样本对概率进行估 计。在实验中，Mikolov等人也论述了负采样技术相比 NCE技术少了一些约束， 对于生成词向量，是有帮助的 [75]。\n二次采样技术\n在大规模语料中，高频词通常就是停用词（如英语中的“the”、汉语中的 “的”）。一方面，这些高频词只能带来非常少量的语义信息，比如几乎所有的词\n都会和“的”共同出现，但是并不能说明这些词的语义都相似。另一方面，训 练高频词本身占据了大量的时间，但在迭代过程中，这些高频词的词向量变化 并不大。Mikolov等人为了解决这一问题，提出了二次采样技术（subsampling） [75]，具体而言，如果词 w在语料中的出现频率 f(w)大于阈值 t，则有 P (w)的 概率在训练时跳过这个词。\nP (w) = 1−\n√ t\nf(w) (2.24)\nword2vec工具包的实现与论文 [75]中的公式稍有不同：\nP (w) = f(w)− t f(w) −\n√ t\nf(w) (2.25)\n20 基于神经网络的词和文档语义向量表示方法研究\n无论采用论文中的公式，还是工具包实现的公式，词出现的越频繁，就越有可 能在训练中被跳过。这种二次采样技术不仅可以提升词向量的训练速度，大多 数情况下也能提升词向量的性能 [65, 75]。\n2.2.7 Order模型\n2.2.6 节提到的 CBOW 模型和 Skip-gram 模型为了有更高的性能，在神经 网络语言模型或者 log双线性语言模型的基础上，同时去掉了隐藏层和词序信 息。为了更好地分析词序信息对词向量性能的影响，这里提出一个新模型，名 为“Order”，意为保留了词序信息。该模型在保留词序信息的同时，去除了隐藏 层，其结构如图 2-6所示。\n原始文本\n输入层\n输出层\nx\ny\nwi−(n−1)/2\ne(wi−(n−1)/2)\nwi−1\ne(wi−1)\nwi+1\ne(wi+1)\nwi+(n−1)/2\ne(wi+(n−1)/2)\nwi\n|V|\n. . . . . .\n图 2-6 Order模型结构图\n相对 CBOW 模型，Order 模型使用上下文词向量的拼接作为模型的输入， 形式化地：\nx = [ e(wi−(n−1)); . . . ; e(wi−(n−1)/2−1); e(wi−(n−1)/2+1); . . . ; e(wi) ] (2.26)\n相对 log双线性语言模型，Order模型采用了公式 2.20（与 CBOW模型一 致），直接从上下文的表示预测目标词。\n2.2.8 词向量模型的理论比较\n本节从上下文表示以及上下文与目标词之间的关系两个角度分析各个神经\n网络词向量模型。\n第二章 现有词的分布表示技术 21\n上下文表示\n上文介绍的各种神经网络词向量模型中，除了 Skip-gram 模型使用词作为 上下文表示之外，其它模型均使用 n-gram作为上下文表示，而这些表示使用不 同的组合策略构造 n-gram的表示。如 CBOW模型使用 n-gram中各词词向量的 平均值作为上下文表示；Order模型使用 n-gram中各词词向量的拼接作为上下 文表示，这种方法可以看做词向量的线性组合；LBL模型则是直接对 n-gram中 各词的词向量做了线性变换；NNLM和 C&W模型更是做了非线性变换。这些 不同的策略可以从复杂度的角度进行分析，本文具体从三个角度来观察这些模 型：模型结构的复杂度、模型参数的个数以及模型求解的时间复杂度。\nSkip-gram CBOW\nOrder\nLBL NNLM\nC&W\n模型的结构复杂度\n参数个数\n图 2-7 神经网络词向量模型复杂程度对比图\n图 2-7展示了各词向量模型复杂度的关系图。图中箭头方向表示各模型复 杂程度的拓扑序，从简单的模型指向复杂的模型。水平方向的相对位置表示模 型结构的复杂程度，从简单到复杂。垂直方向的相对位置表示模型的参数个数， 越靠上的模型参数越多。\n模型结构的复杂度\n从模型结构上看，CBOW模型与 Skip-gram模型相比，采取了更复杂的上下 文表示方法，用上下文词向量的线性叠加代替了随机选取其中一个词的词向量。 Order相比 CBOW模型在上下文表示时，保留了词序信息。LBL在保留词序信 息的同时，还进一步使用线性变换，使模型具有上下文的语义组合能力。NNLM\n22 基于神经网络的词和文档语义向量表示方法研究\n与 C&W模型进一步采用了非线性激活函数，使得整个模型为神经网络结构，表 达能力强于 LBL的双线形结构。 这些模型中，虽然从模型结构上看，在图 2-7中有从左往右的绝对顺序，但 是如果看模型整体的复杂度，并不能直接断言 LBL模型比 Order更复杂，或者 C&W模型比 LBL更复杂。这是因为这些模型在保证词向量维度相同时，使用 的参数个数有很大的差异，更复杂的结构或者更多的参数都有可能导致模型的 复杂度增加。\n参数个数\n从参数个数上看，参数最少的模型是 C&W。在神经网络词向量模型中，参 数个数主要包含词向量和网络结构中的其它参数这两部分。C&W 模型只有一 份词向量，而其它模型均维护了两份词向量，因此 C&W模型的参数个数最少， 为 |e| × |V|+(win+1)|h|。其中 |e|为词向量的维度，|V|为词表大小，|h|为隐 藏层的维度，win为上下文窗口的大小。Skip-gram与 CBOW模型只使用了两 份词向量，而没有其它额外的模型参数，参数个数为 2|e| × |V|。LBL和 NNLM 这两个模型在 CBOW等模型的基础上加入了隐藏层，因此增加了一个输入层到 隐藏层的权重矩阵，其参数个数为 (|e|+ |h|)× |V|+ (win− 1)|e| × |h|。参数最 多的 Order模型由于既保持了上下文的词序信息，又采用了线性结构，因此在 模型中，词当作目标词时的词向量 e′，其维度需要与作为上下文时的词向量的 维度和一致，因此总参数个数为 win|e| × |V|。\n时间复杂度\n从模型的时间复杂度上看，文献 [73]对早期的若干模型均有分析，因此这 里只做简要介绍。基于神经网络的词向量模型均通过扫描语料中的每一个词， 取该词以及其周围的上下文作为一个样本。因此对比这些模型时，可以只分析 训练一个样本的时间复杂度。对于原版的 NNLM和 LBL模型，训练一个样本 需要的计算为输入层到隐藏层，隐藏层到输出层这两个矩阵运算，其复杂度为 O((win−1)|e|×|h|+ |h|×|V|)。对于Order模型，由于省略了隐藏层，其复杂度 为 O((win− 1)|e| × |V|)。对于 CBOW和 Skip-gram模型，由于进一步忽略了词 序信息，其复杂度为 O(|e| × |V|)。C&W模型在结构中省去了对目标词的预测， 其复杂度仅为O((win+1)|e|× |h|)。对于上述各个模型，如果采用层级 softmax 函数做输出层的优化，式子中的 |V|可以加速到 log(|V|)，而如果使用噪声对比\n第二章 现有词的分布表示技术 23\n估算（noise-contrastive estimation）或者负采样技术（negative sampling），|V|可 以进一步优化到常数。因此，如果这些模型都使用先进的负采样技术预估输出 层，则这些模型在时间复杂度上的排序，与模型结构的复杂度一致，从图 2-7中 看，从左往右时间复杂度依次递增。\n效率和性能\nCBOW、Skip-gram和 Order模型相对于其它神经网络模型，均去除了隐藏 层。如果有隐藏层，输入层的上下文表示可以通过一个线性变换或者非线性变 换得到隐藏层，这种操作属于语义组合操作 [36]。如果没有隐藏层，上下文词 之间的关系为普通的线性叠加关系，会丢失部分语义信息。\nCBOW模型和 Skip-gram模型还通过不同的方法去掉了其它神经网络模型 中保留的词序信息。虽然这两个模型根据上下文各词与目标词之间的距离采用 了加权策略，可以少量保留词序信息，但是这种策略相对于词向量顺序拼接的 方式，可以认为几乎忽略了词序信息。\n这些模型采取的简化策略，使其有更高的运行效率，可以在更大规模的语\n料上训练词向量，但是模型本身对语义捕获的能力也有一些降低。这些简化究 竟对词向量的性能有多少影响，需要通过实验来说明。\n目标词与上下文之间的关系\n现有词向量模型的目标词与上下文之间主要有两种关系。从神经网络的目\n标上看，C&W模型与众不同。C&W模型的目标函数是求目标词 w与其上下文 c的联合打分，而其他模型均为根据上下文 c，预测目标词 w。从各个模型的结 构，以及对目标词与上下文的处理方式看，C&W模型将上下文和目标词同时放 在输入层，通过神经网络模型优化它们之间的关系。而其它的神经网络模型只 把上下文放在输入层，把目标词放在输出层。C&W模型使用神经网络模型构建 目标词和上下文的组合关系，而其它神经网络模型的上下文和目标词之间呈现 预测关系。\n如果从能量函数来看，C&W模型和其它模型之间的区别会更为清晰。比如 LBL模型的能量函数（公式 2.14）中，主要部分为：\nE(w; c) = e(w)THe(c) (2.27)\n24 基于神经网络的词和文档语义向量表示方法研究\n而根据公式 2.23可得，Skip-gram模型的能量函数为：\nE(w; c) = e′(w)Te(c) (2.28)\n在这些模型中，上下文对应的向量与目标词对应的向量，通过点积或双线\n性计算，均有“交互”关系，而在 C&W模型中，上下文与目标词的向量仅为加 法“组合”关系：\nE(w; c) = Ae(w) +Be(c) (2.29)\n无论是上下文与目标词呈组合关系的 C&W模型，还是预测目标词，目标词 与上下文之间有交互关系的其它神经网络模型，均符合分布假说。但是由于这 两种方式的表达能力不同，因此对于语义的捕获能力可能也会有差异。第三章 实验部分会对这两种不同方法做进一步的分析。\n2.3 相关工作\n本文将分布表示归类为基于矩阵的分布表示、基于聚类的分布表示和基于\n神经网络的分布表示。这种分类方式基本沿用了 Turian 等人的分类 [116]。在 Turian的分类中，这三种方法分别被称作“distributional representation”、“clusteringbased word representation”和“distributed representation”。本文和 Turian的区别 在于，本文将这三种方法统称为“distributional representation”，而将其中的第一 种方法称作基于矩阵的分布表示。 Baroni 等人的文献 [3] 分析了语义向量模型，因此没有考虑聚类模型。文 中，Baroni将基于矩阵的分布表示称作计数（count）方法，而将神经网络模型 称作预测（predict）方法。 Sahlgren的博士论文 [98]研究了词空间模型（word space model），具体分析 了基于矩阵的分布表示中的“词-文档”矩阵和“词-词”矩阵，并得到结论：“词文档”矩阵主要构建了词的组合关系（syntagmatic），而“词-词”矩阵主要构建 了词的替换关系（paradigmatic）。 Turney和 Pantel的工作 [118]总结了向量空间模型（vector space model），他 们将向量空间模型分为“词-文档”矩阵、“词-上下文”矩阵和“词对-模板”矩 阵这三种，认为“词-文档”矩阵适合用来表示文档，“词-词”矩阵适合表示词， 而“词对-模板”矩阵用来表示词对之间的关系。\n第二章 现有词的分布表示技术 25\n2.4 模型总结\n本章介绍了三类不同的分布表示方法：基于矩阵的分布表示、基于聚类的\n分布表示和基于神经网络的分布表示。其中的经典模型总结于表 2-1中。这三 类模型中，基于聚类的模型较为特别，将词表示为聚类的类标。如果采用层级 聚类方法，可以根据聚类类别的公共前缀衡量词之间的相似度。而另外两类模 型得到的都是向量表示，可以直接使用余弦距离、欧氏距离等向量空间距离衡 量指标来检测词义的相似度。\n名称 上下文 上下文与目标 词之间的建模 （技术手段） LSA/LSI [56] 文档 矩阵 HAL [70] 词 GloVe [90] 词 Jones & Mewhort [45] n-gram Brown Clustering [12] 词 聚类 Skip-gram [73] 词\n神经网络 CBOW [73] n-gram（加权） Order（2.2.7小节） n-gram（线性组合） LBL [79] n-gram（线性组合） NNLM [7] n-gram（非线性组合） C&W [17] n-gram（非线性组合）\n表 2-1 分布表示模型的概要信息\n基于矩阵的模型和基于神经网络的模型最终都可以得到词的低维向量表\n示，这两类方法是否存在一定的联系？答案是肯定的。Levy 和 Goldberg 证明 了对“词-词”矩阵做 SVD分解与 Skip-gram模型配合负采样技术优化具有相同 的最优解 [64]。本文证明了“词-词”矩阵分解与 Skip-gram模型的原始形式具 有相同的最优解，具体证明过程见附录 A。遗憾的是，这两个等价关系需要一 个前提条件：最优解可以取到。在实际优化过程中，一般的优化方法只能不断 逼近最优解，而很难达到最优解。尽管理论上这两者的最终优化结果应当一致， 但是在实践中还是有较大的差异，文献 [3, 64, 65]比较了这些模型在实践中的 差异。2015年 Li等人在文献 [67]中证明了“词-词”矩阵分解与 Skip-gram模型 可以完全等价。这项工作抛弃了 SVD所用的均方根误差来作为矩阵分解的重构\n26 基于神经网络的词和文档语义向量表示方法研究\n误差，而是采用了一种对数概率误差。他们的实验也证明，这两者可以达到相 同的效果。 这些证明建立了“词-词”矩阵与 Skip-gram模型之间的联系，这两个模型只 是矩阵和神经网络这两种技术手段下的特例，它们都选用了词作为上下文。而 神经网络相对矩阵表示的优势正是可以通过组合手段，对上下文进行更为复杂 的建模，同时避免维数灾难问题。下一章将会详细分析各个神经网络词向量模 型在实践中的差别。\n第三章 词向量表示技术的实验分析\n在使用深度学习技术解决自然语言处理任务时，最基础的问题是词的表示。\n本章从理论和实验两个角度分析和探讨了构建词向量表示时的关键点：模型、语 料以及参数的选择，并给出了若干条生成优质词向量的参考建议。\n3.1 引言\n词向量模型可以从大规模无标注语料中自动学习到句法和语义信息 [76]。 近年来，大量研究者投身到设计新型的词向量模型中，基于词向量的神经网络 模型也为多项自然语言处理任务带来了性能的提升，甚至在多项任务中达到了 目前最好的效果。现有的词向量模型在提出时，作者均声称他们的方法比前人 的方法好，然而这些工作在评价词向量时，会挑选比较局限的评价指标，有时 候甚至使用了不同的训练语料，因此其评价结果可能会缺乏借鉴意义。现有的 词向量模型纷繁复杂，这些模型究竟哪个更好，或者在什么情况下更适合用哪 个模型，现有的研究工作仍然缺乏相应的比较分析。本章主要分析词向量生成 中的几个关键点，包括词向量模型的选择、语料的选择以及训练参数的选取，对 各现有的词向量模型进行全面的比较。\n模型 上下文的表示 目标词与其上下文关系\nSkip-gram [73] 上下文中某一个词的词向量 上下文预测目标词 CBOW [73] 上下文各词词向量的平均值 上下文预测目标词\nOrder 上下文各词词向量的拼接 上下文预测目标词 LBL [79] 上下文各词的语义组合 上下文预测目标词 NNLM [7] 上下文各词的语义组合 上下文预测目标词 C&W [17] 上下文各词与目标词的语义组合1 上下文和目标词联合打分\n表 3-1 各词向量模型在目标词与上下文建模上的异同\n为了选择最合适的词向量模型，首先需要对现有词向量模型的结构进行剖\n析。根据第二章的分析，可以从上下文的表示以及目标词与上下文的关系这两 个角度进行对比分析。表 3-1列举了神经网络词向量模型在这两方面的异同。例 1该模型联合处理上下文和目标词，不存在一个独立的上下文表示。\n28 基于神经网络的词和文档语义向量表示方法研究\n如，Skip-gram模型 [73]选取了目标词 w上下文中的某一个词的词向量，作为其 上下文表示。CBOW模型采用上下文各词词向量的平均值作为上下文表示。这 两个模型为了加速，均忽略了词序信息。然而，Landauer在文献 [54]中曾分析， 文本中大约有 20%的语义来自于词序，而剩下部分来自词的选择。因此，这两 个模型可能会丢失一些重要信息。与之相对的，LBL模型 [79]、NNLM模型 [7]、 C&W 模型均使用上下文窗口中各词词向量的拼接作为上下文的表示，可以保 留词序信息。本文希望知道，使用哪个模型效果更好？具体而言，在多种不同 的上下文表示之中，以及在上下文与目标词的两种不同的关系之间，应当如何 选择合适的模型？（问题一）\n同时，词向量的精度非常依赖于训练语料的选择，不同大小、不同领域的\n语料会极大地影响词向量的性能。因此，本文还希望知道，训练语料的大小及 领域对词向量有什么样的影响？（问题二）\n除了模型和语料的选择，现有的词向量算法也非常依赖于参数的选择。最\n主要的是模型的迭代次数，以及词向量的维度。对于迭代次数，如果迭代次数 过少，词向量就会训练不充分，所包含的信息不足；如果迭代次数过多，模型 很可能过拟合。对于词向量的维度，本文也希望找到合适的维度。因此，本文 尝试分析在迭代训练中，选择什么样的迭代次数可以获得足够好的词向量，同 时避免过拟合？（问题三）以及多少维的词向量效果最理想？（问题四）\n为了更客观地回答上述四个问题，本文选取了三大类指标，共八个具体的\n任务评价这些词向量。本文认为这些评价指标涵盖了现有词向量的所有用法。 第一类指标为评价词向量的语言学特性。本文使用标准的 WordSim353数据集 [28]以及 TOEFL数据集 [55]评价词向量的空间距离是否和人的直观感受一致。 第二类指标中，本文将词向量作为现有自然语言任务中的特征，看其所能达到 的性能。具体而言，本文选择了文本分类任务和命名实体识别任务。第三类指 标中，本文将词向量作为神经网络模型的初始值，并使用卷积神经网络做情感 分类任务，以及使用文献 [18]的模型做词性标注任务。通过使用这些不同的评 价指标对词向量模型进行评价，本文尝试分析出应该怎么选择模型（问题一）和 参数（问题三和四）。本文进一步通过使用不同规模和不同领域的语料对词向量 进行训练尝试回答第二个问题。\n本章的主要贡献为系统化地整理现有的词向量模型，并且通过多种评价指\n标全面地比较分析各词向量模型与语料的选取。通过这些实验比较和理论分析，\n第三章 词向量表示技术的实验分析 29\n本文给出了若干条生成词向量的参考建议。\n1. 选择一个合适领域的语料，在此前提下，语料规模越大越好。使用大规模 的语料进行训练，可以普遍提升词向量的性能，如果使用领域内的语料，\n对同领域的任务会有显著的提升。\n2. 选择一个合适的模型。复杂的模型相比简单的模型，在较大的语料中才 有优势。简单的模型在绝大多数情况下已经足够好。预测目标词的模型\n（表 3-1中除了 C&W以外的所有模型）比目标词与上下文呈组合关系的模 型（C&W模型）在多个任务中有更好的性能。\n3. 训练时，迭代优化的终止条件最好根据具体任务的验证集来判断，或者近 似地选取其它类似的任务作为指标，但是不应该选用训练词向量时的损失\n函数。\n4. 词向量的维度一般需要选择 50维及以上，特别当衡量词向量的语言学特 性时，词向量的维度越大，效果越好。\n3.2 评价方法\n为了更全面地对各种不同的词向量模型进行评价，本文考察了各种词向量\n的用法，并将这些用法分成三大类，分别为：一、利用词向量的语言学特性完 成任务；二、将词向量作为特征，提高自然语言处理任务的性能；三、将词向 量作为神经网络的初始值，提升神经网络模型的优化效果。本文在这三大类用 法的基础上，选取了八个有代表性的具体任务，作为词向量的评价指标。\n3.2.1 词向量的语言学特性\n各词向量模型均基于分布假说设计而成，因此无论哪种词向量模型，都会\n符合分布假说所提出的性质：具有相似上下文的词，会拥有相似的语义，并且 其词向量的空间距离更接近。文献 [3]通过语义相关性、同义词判别、概念分类 和类比等实验论述了词向量具有各种不同的语言学特性。本文从中选取了三个 代表性任务。\n30 基于神经网络的词和文档语义向量表示方法研究\n语义相关性 (ws)\n衡量语义相关性最经典的是 WordSim353数据集，该数据集包含了 353个 词对，其中每一个词对有至少十位标注者对其进行 0到 10之间的打分，分数越 高表示标注人员认为这两个词的语义更相关或者更相似。例如，词对“student, professor”的平均打分为 6.81，而词对“professor, cucumber”的打分为 0.31。评 价时，对于每个词对，本文使用所有标注者打分的平均值作为参考得分 X，以 词对的两个词向量的余弦距离作为模型得到的相关性得分 Y，并衡量这两组数 值之间的皮尔逊相关系数。皮尔逊相关系数衡量了两个变量之间的线性相关性， 值在 −1到 1之间，如果模型得到的打分与人工标注的打分一致，得分就越高。 具体而言，X 和 Y 之间的皮尔逊相关系数定义为 X 和 Y 之间的协方差与它们 标准差的商：\nρX,Y = cov(X,Y ) σXσY\n(3.1)\n同义词检测 (tfl)\n托福考试（TOEFL）数据集 [55]包含 80个单选题，每个题目包含一个问题 词以及四个选项，要求从四个选项中选出一个与问题词同义的词语。例如：问 题“levied”，选项“imposed”、“believed”、“requested”、“correlated”，正确答案 为“imposed”。对于每一个问题，需要计算问题词与选项词对应词向量之间的 余弦距离，并选用距离最近的选项词，作为答案。在评价词向量时，本文直接 使用 80个问题的准确率。\n单词类比 (sem、syn)\n英文单词类比数据集由Mikolov等人于 2013年 [73]提出，该数据集包含了 9000 个语义类比问题以及 1 万个句法类比问题。语义类比问题包括国家首都、 家庭成员称谓、国家货币等五类问题，如，“‘king’对‘queen’如同‘man’对什 么？”，答案为“woman”。句法类比问题有比较级、最高级、名词单复数等九类 问题，如“‘dance’对‘dancing’如同‘predict’对什么？”，答案为“predicting”。\n为了回答这类类比问题，Mikolov等人 [73]根据相似关系词对的词向量之 差也相似的特点，提出使用词向量的加减法来完成这一任务。例如，对于问题\n第三章 词向量表示技术的实验分析 31\n“‘king’对‘queen’如同‘man’对什么？”，该方法直接从词表中寻找与−−−→queen− −−→ king +−−→man最相似的词，作为答案。评价时使用回答问题的准确率。\n单词类比任务的数据集相对前两个任务规模较大，因此在实验中，结果较\n为稳定，该指标也成为评价词向量的经典指标。\n3.2.2 词向量用作特征\n词向量可以从无标注文本中学习到句法和词法的特征，很多现有工作直接\n使用词向量作为机器学习系统的特征，并以此提高系统的性能。\n本文选用两个有代表性的任务，一、将词向量作为唯一特征，完成文本分\n类任务；二、将词向量作为现有系统的额外特征，完成命名实体识别任务。选 用词向量作为唯一特征可以从一个侧面看出词向量的表达能力，而选用词向量 作为现有系统的额外特征可以看出词向量所含的信息与现有人工设计特征的区 别。\n基于平均词向量的文本分类 (avg)\n该任务直接以文本中各词词向量的加权平均值作为文档的表示，以此为特\n征，利用 Logistic回归完成文本分类任务。其中权重为文档中各词的词频。本文 选用了 IMDB数据集 [71]做文本分类实验。该数据集包含三部分，其中训练集 和测试集各 2.5万篇文档，用来做文本分类的训练和测试；无标注部分共 5万 篇文档，用于训练词向量。任务的评价指标为文本分类的准确率。\n命名实体识别 (ner)\n命名实体识别（Named entity recognition，NER）在机器学习框架下，通常作 为一个序列标注问题处理。在这一评价指标中，本文将词向量作为现有命名实 体识别系统 [97]的额外特征，该系统的性能接近现有系统的最好性能。实验设 置与 Turian等人实现的方式一致 [116]。任务的评价指标为命名实体识别的 F1 值，测试集是 CoNLL03多任务数据集的测试集。\n3.2.3 词向量用做神经网络初始值\nErhan等人在文献 [27]中论述了，恰当地选取神经网络的初始值，可以让 神经网络收敛到更好的局部最优解。在自然语言处理任务中，基于神经网络模 型的方法一般都会使用词向量作为其输入层的初始值。\n32 基于神经网络的词和文档语义向量表示方法研究\n在上一类词向量的用法（将词向量作为特征）中，词向量是模型的固定输\n入值，在模型的训练过程中，输入值不会改变，只有模型中的参数会改变。然 而，将神经网络的初始值赋值为词向量之后，神经网络在训练过程中会改变设 置的初始值。因此这两类词向量的用法表面上看非常相似，实质上却是不同的。\n基于卷积神经网络的文本分类 (cnn)\n卷积神经网络（Convolutional neural networks，CNN）是表示文本的有效模 型。2014年，Lebret等人 [59]以及 Kim等人 [48]同时提出用于文本分类任务的 卷积神经网络。在这一评价指标中，本文选用了这一经典的卷积神经网络。网 络结构在 5.2.4节有详细介绍。 本文选取斯坦福情感树库（Stanford Sentiment Treebank）数据集作为文本分 类的训练集、验证集和测试集 [110]。由于该数据集规模较小，文本分类的效果 受网络初始值的影响较大，导致了评价指标的不稳定。为了更客观地评价卷积 网络中，不同词向量对文本分类性能的影响，本文对每一份词向量重复做 5次 实验。在每次实验中，输入层词表示均初始化为这份词向量，网络结构中的其 它参数则初始化为不同的随机值。对于每一次实验，本文在训练集上训练卷积 神经网络，取验证集上准确率最高的点，并报告其在测试集上的准确率。最后 将 5组实验的测试集准确率的平均值作为最终的评价指标。\n词性标注 (pos)\n词性标注（part-of-speech tagging）是一个经典的序列标注问题。在这个任 务中，本文使用 Collobert等人提出的网络 [18]，对句子中的每个词做序列标注。 该任务选用华尔街日报数据集 [115]。评价指标为模型在验证集上达到最佳效果 时，测试集上的准确率。\n3.3 实验及分析\n在这一节中，本文针对不同模型、不同语料、不同任务做了大量实验，并\n回答前面提出的四个问题。表 3-2列举了主要实验设置，包括实验中选取的模 型、语料，以及实验中使用的主要参数。在具体的实验中，本文并没有穷举所 有的设置组合，而是根据实验需要选取其中的一部分参数进行评测。具体设置 将在各实验中介绍。\n第三章 词向量表示技术的实验分析 33\n类别 实验设置 模型 Skip-gram, CBOW, Order, LBL, NNLM, C&W\n语料 维基百科（Wiki）：100M, 1.6B 纽约时报（NYT）：100M, 1.2B Wiki+NYT（W&N）：10M, 100M, 1B, 2.8B IMDB电影评论：13M 参数 维度：10, 20, 50, 100, 200 固定窗口大小：5\n表 3-2 词向量对比实验设置总表\n3.3.1 性能增益率\n在使用 3.2节中提到的 8个不同的指标评价词向量性能时，可能会遇到两 个问题。为了更直观地感受这两个问题，可以参考表 3-3a（第 37页）中的实验 结果。\n一、不同评价指标的绝对数值差异较大。如 syn、sem 任务的性能一般在 40%左右，ws任务的性能集中在 50%到 60%，而 tfl的性能大约在 70%多，pos 的性能都在 95%以上。由于指标之间的差异，我们只能在同一个指标内对不同 的模型进行纵向的比较，而较难对一个模型在不同指标中的表现做横向的比较。\n二、不同评价指标内的相对差异变化较大。如 ws任务的性能中，最好的模 型达到了 63.89%，然而最差的模型只有 46.17%，差距有约 18个百分点；但是 在 avg任务中各模型的性能差异相对较小，最差的模型能到 73.26%，而最好的 模型也只能到 74.94%，差距不到 2个百分点。\n正是因为有这两个问题的存在，在评价词向量时，如果两个模型在性能数\n值上非常接近，我们会很难定量地判断孰优孰劣。例如，在 pos中，Order模型 与 LBL模型分别可以达到 96.76%和 97.77%的准确率。这里很难说 LBL模型 比 Order模型更适合做 pos任务。这种性能上微小的差别可能并不是由于模型 的优劣产生的，更可能是由于测试集样本数较少（ws任务和 tfl任务）或者二次 训练带来的误差（avg、ner、cnn和 pos任务）所导致的。\n为了解决第一个问题，本文考虑用“性能增益”代替各项任务性能的绝对\n数值。性能增益是指一个词向量在某任务上的性能比随机词向量在该任务上性 能的相对增幅。随机词向量与其他模型生成的词向量一致，也使用 50维的实数\n34 基于神经网络的词和文档语义向量表示方法研究\n向量，但是其中各个维度的值，由−1到 1之间的均匀分布随机生成。使用随机 词向量在各任务中得到的性能数值表示，哪怕词向量中不包含任何有用的信息， 该任务也能达到的性能。因此，某个词向量相对随机词向量的性能增益表示该 词向量所包含的信息对这一任务带来的贡献。\n单纯使用性能增益并不能解决上面的第二个问题。为了解决第二个问题，\n本文进一步提出使用“性能增益率”（Performance Gain Ratio）这一评价指标来 代替性能增益。性能增益率的思想借鉴了文献 [24]，每个词向量只与同等条件 下最好的词向量做对比。本文根据词向量的特殊性质，将词向量 a相对词向量 b的性能增益率定义为：\nPGR(a, b) = pa − prand pb − prand\n(3.2)\n词向量 a对同等条件下最好的词向量 best的性能增益率 PGR(a, best)可以简写 作词向量 a的性能增益率 PGR(a)：\nPGR(a) = pa − prand pbest − prand\n(3.3)\n上述二式中 px 表示词向量 x在某项任务上的性能，prand 表示随机向量在这项 任务上的性能。性能增益 pa − prand 体现的是词向量 a相比随机词向量可以带 来的性能的提升。类似地，性能增益率体现的是，词向量 a与词向量 best相比， 所带来性能提升的比例。\n由于本文设定词向量的参考标准为同等条件下效果最好的词向量，因此，\n性能增益率是一个小于等于 1的数字。如果值为 1，则表示该词向量已经达到 同等条件下的最佳效果；如果为 0，则说明与随机词向量的效果一样，没有带来 任何收益；如果为负数，说明该词向量不仅没有对任务带来提升，而且还对任 务产生了负面的效果。\n本文需要在 8个不同的指标下，对各词向量进行评测。使用性能增益率可 以让所有的评测结果都有一套统一的指标描述性能，并且一般情况下性能的数 值大小均在 0到 1之间。这一特点一方面可以让我们以最直观的方式了解到每 份词向量对任务带来的实际提升有多少；另一方面，这也为多指标联合分析带 来了可能，使得我们可以直接分析各个词向量在多个任务下的综合表现。\n第三章 词向量表示技术的实验分析 35\n3.3.2 模型比较\n为了公平地比较各个不同的模型，本文需要对各模型采取相同的实现，同\n时使用同样的语料训练。\n实验概览\n在模型实现方面，本文使用的 Skip-gram 模型和 CBOW 模型的实现基于 word2vec开源工具包2。其余模型均在 word2vec工具包中 CBOW实现的基础上 修改得到。具体来说，在 CBOW模型中，上下文的表示为上下文若干词的平均 词向量；在 Order模型中，本文将其替换为上下文若干词的词向量的拼接；在 LBL模型的实现中，本文在 Order模型实现的基础上加入了一个隐藏层，使得 上下文表示先通过一次线性变换进入隐藏层，再对其预测；在 NNLM模型的实 现中，本文在 LBL模型的线性变换之后加入 tanh激活函数，使其成为一个非线 性变换，整个模型是一个三层的神经网络结构；在 C&W模型的实现中，本文 基于 NNLM的实现，将预测的目标词从输出层移动到输入层，这样输入层就是 目标词与其上下文词的词向量的拼接，而输出层则只保留一个节点，用于表示 这组上下文、目标词组合的评分。\n对于所有的模型，本文将中间词作为目标词（一般来说，以语言模型为基\n础的模型，如 NNLM和 LBL，都使用最后一个词作为目标词，这里也将其改成 中间词），目标词上下文各两个词作为其对应的上下文，即 win = 5。对于所有 模型，本文使用二次采样（subsampling）技术 [75]，并设置 t = 10−4。二次采样 技术的细节可见第二章第 2.2.6小节。word2vec工具包与文献 [75]中所描述的 二次采样公式略有差别，本文在实验中使用word2vec工具包所用的公式。同时， 为了提高实验效率，本文也对 word2vec工具包做了适当的改动。word2vec工具 包采用梯度下降法作为其优化算法，同时也采用了学习速率下降的策略。在训 练的初始阶段，其学习速率为一个设定的初始学习速率（如 CBOW模型默认为 0.05）；在训练过程中，学习速率均匀下降，下降幅度与已学习的样本个数呈正 比；到训练的最后阶段，学习速率降为 0。在这种学习速率下降策略下，如果想 分析迭代 1次到 n次对结果的影响，由于迭代的中间结果不能直接使用，程序 需要扫描语料 1+2+ · · ·+n = n(n+1)/2次才能真正生成各种迭代次数的训练 结果。为了方便分析迭代次数带来的影响，本文使用 AdaGrad [25]替代原先的\n2https://code.google.com/p/word2vec/\n36 基于神经网络的词和文档语义向量表示方法研究\n方法，更改后，与普通的梯度下降算法一致，只需要扫描 n次语料，即可完成 对迭代次数影响的分析。本文设定 AdaGrad的学习速率为 0.1，使用该学习速率 时，与原方法的学习效果最为类似。实验表明，将原优化方法改成 AdaGrad之 后，效果基本保持不变，有时会有微弱的提升。\n在语料方面，本小节使用统一的训练语料：W&N数据集（维基百科与纽约 时报的混合语料，共 28亿单词，详见 3.3.3小节）。同时，由于这些模型都采用 迭代算法优化，为保证每个模型都能训练到最佳状态，在每一组实验中，本文 对每个模型反复迭代，直到该模型在所有的任务上性能均已经收敛或达到过峰 值，并取迭代过程中的最佳值作为模型在该任务上的性能。因此，对于各项任 务，可能会选取不同的迭代次数的词向量。\n表 3-3展示了上述实验结果。其中表 3-3a展示了各任务中性能的绝对数值， 表 3-3b 则是转换后的各模型在各任务中的性能增益率。表格中模型为“随机” 一行表示，如果采用随机的词向量，各项任务可以达到的性能。从实验结果中 可以看出：一、与随机词向量相比，无论哪个模型，在哪个任务上，效果均有显 著的提升。也就是说，这些词向量模型均可以在一定程度上捕获句法和语义的 特征；并且可以用来提升自然语言处理任务的效果。二、从总体上看，各个任 务的最佳结果（加粗部分）由不同的模型得到。从 Skip-gram到 LBL，均有取得 若干最佳结果。\n上下文的表示\n分析上下文的表示时，我们主要考虑除了 C&W以外的模型，这些模型的 唯一差异就是上下文的表示方式不同。\n为了说明不同上下文表示之间的差别，本文在不同规模的语料下对这些模\n型进行比较。具体而言，本文加入了W&N数据集的三个子集，分别包含 1000 万（10M）单词、1亿（100M）单词和 10亿（1B）单词。这三个子集与完整集 合 28亿（2.8B）单词一起，用于分析不同种类上下文表示的影响。\n在这个实验中，需要从模型、语料规模、任务这三个维度中发现规律，为\n了把注意力放在模型和语料规模上，本文利用性能增益率将各个任务的性能归 一化。如果一个模型在某任务上达到 95%的性能增益率，本文认为这个模型在 该任务上足够好。选取 95%可以在一定程度上消除小规模评价数据集以及二次 训练带来的误差。比如，在表 3-3b中，NNLM模型在 cnn任务和 pos任务上均\n第三章 词向量表示技术的实验分析 37\n模型 syn sem ws tfl avg ner cnn pos 随机 0.00 0.00 0.00 25.00 64.38 84.39 36.60 95.41 Skip-gram 51.78 44.80 63.89 76.25 74.94 88.90 43.84 96.57 CBOW 55.83 44.43 62.21 77.50 74.68 88.47 43.75 96.63 Order 55.57 36.38 62.44 77.50 74.93 88.41 44.77 96.76 LBL 45.74 29.12 57.86 75.00 74.32 88.69 43.98 96.77 NNLM 41.41 23.51 59.25 71.25 73.70 88.36 44.40 96.73 C&W 3.13 2.20 46.17 47.50 73.26 88.15 41.86 96.66\n(a)性能实际值\n模型 syn sem ws tfl avg ner cnn pos Skip-gram 93 100 100 98 100 100 89 85 CBOW 100 99 97 100 98 90 88 90 Order 100 81 98 100 100 89 100 99 LBL 82 65 91 95 94 95 90 100 NNLM 74 52 93 88 88 88 95 97 C&W 6 5 72 43 84 83 64 92\n(b)性能增益率\n表 3-3 各模型在完整W&N语料下的最佳性能（百分比）\n达到或超过了 95%的性能增益率，因此 NNLM在使用完整W&N语料训练时， 在两个任务上性能足够好。在评估性能增益率时，各模型只与同样配置的模型 做比较，用 10M语料训练得到的词向量，只与 10M语料训练得到的最好词向 量进行比较，其它规模的语料也类似。\n根据第二章 2.2.8小节中的分析，本文从模型复杂度的角度分析了不同的上 下文表示。基于这种视角，可以更好地解释实验结果。表 3-4展示了在四种不 同规模的语料下训练，各模型在多少任务中表现足够好。每个单元格中 a+b表 示某模型（行）使用某语料（列）训练得到的词向量，在前四个评价语言学特 性的任务中，有 a个任务表现足够好；在后四个评价其用于特征或用于神经网 络初始值效果的任务中，有 b个任务表现足够好。根据表格中的数据，本文做 了以下分析。\n一、简单模型在小语料上整体表现更好，而复杂的模型需要更大的语料作 支撑。首先从语料大小的角度观察，在 10M规模的训练语料下，结构最简单的\n38 基于神经网络的词和文档语义向量表示方法研究\n模型 10M 100M 1B 2.8B\nSkip-gram 4+2 4+2 2+2 3+2 CBOW 1+1 3+3 4+1 4+1 Order 0+2 1+2 2+3 3+3 LBL 0+2 0+2 0+2 1+2 NNLM 0+2 0+3 0+3 0+2\n表 3-4 各模型在不同规模语料下性能增益率超过 95%的次数\nSkip-gram模型有 6个任务足够好，是最佳的选择；当语料扩大到 100M时，稍 微复杂一些的 CBOW模型开始体现出优势；如果继续扩大语料，能保留词序信 息的 Order模型效果超过了 Skip-gram和 CBOW模型。然后观察结构最复杂的 LBL 模型与 NNLM 模型，从中可以大致看出，随着语料规模逐渐变大，这两 个模型的相对效果在逐渐变好。实验中，即使在最大规模的语料下，这两个模 型也没有达到所有模型中的最好效果，但是从趋势看，这两个保留语义组合关 系的模型，仍然有潜力在更大规模的语料下超越其它更简单的模型。这一部分 的实验只是探讨了语料规模与模型复杂度之间的关系，实际上从语料的角度来 看，语料领域的影响比语料规模或者模型的影响更大，后文 3.3.3小节会有具体 探讨。\n二、对于实际的自然语言处理任务，各模型的差异不大。各模型在实际的 自然语言处理任务中（加号后面的数字）表现均比较类似。结合表 3-3中的结 果，在完整W&N语料下训练时，简单模型和复杂模型的性能差距也非常的小。 因此，一般情况下，将 Skip-gram和 CBOW等简单模型生成的词向量，用于自 然语言处理任务，就可以得到一个令人满意的效果。\n上下文和目标词的关系\n现有词向量模型中，目标词与上下文之间主要有两种关系。C&W模型对目 标词与其上下文联合打分；而其他模型均为根据上下文，预测目标词。因此，在 分析上下文和目标词之间的关系时，我们主要关注 C&W模型与其它模型的差 异。实验结果显示，C&W模型在语言学任务上（syn、sem、ws和 tfl），效果均 不如其它模型。尤其在类比任务 syn和 sem中，C&W模型的性能接近于随机词 向量，可以看出 C&W的词向量几乎不存在线性减法关系。与此同时，C&W模 型在其它任务上表现并不差。为了深入分析 C&W模型与其它模型的区别，本\n第三章 词向量表示技术的实验分析 39\n文选取了若干词，并列举了用 CBOW和 C&W模型得到的词向量中，与这些词 最相似的词（词向量空间距离最近的词，后文简称“最近邻”）。表 3-5展示了对 比结果。\n模型 Monday commonly reddish\nCBOW\nThursday generically greenish Friday colloquially reddish-brown\nWednesday popularly yellowish Tuesday variously purplish Saturday Commonly brownish\nC&W\n8:30 often purplish 12:50 generally pendulous 1PM previously brownish 4:15 have orange-brown\nmid-afternoon are grayish\n表 3-5 若干词及其用不同模型得到的最近邻对比表\n从表中可以发现，用 CBOW模型训练时，“Monday”的最近邻是一星期中 的其它几天，而C&W模型得到的的却是一天中的时刻。类似地，“commonly”的 最近邻用 CBOW得到的都是一些语义类似的词，使用时可以替换“commonly”， 而 C&W模型得到的有些是和 commonly搭配使用的词。“reddish”的最近邻多 为颜色，除了 C&W模型得到的“pendulous”，该词一般与“reddish”一起使用， 形容花朵。 根据文献 [98] 中的论述，CBOW 这类通过上下文预测目标词的模型直接 对二阶关系进行建模，而一般认为二阶关系就是替换关系（paradigmatic）[57]。 简单地说，这些模型通过相似的目标词，构建出上下文之间的相似关系。然而， C&W模型没有显式地对二阶关系进行建模。实验中也可以看出，C&W的最近 邻并不全是替换关系的词，而 CBOW 模型得到的几乎全是替换关系的词。本 文的结果初看与 C&W 模型原始论文 [18] 的结果有差异，在该论文的表 7 中， C&W模型的最近邻均为替换关系。实际上，文献 [18]的结果并不是纯 C&W得 到的结果，而是在 C&W模型之后，进一步使用了四个有监督任务（词性标注、 命名实体识别等）调整参数得到的词向量。有监督任务与 CBOW这类预测目标 词的方法类似，也会使相似的词向量具有替换关系。因此，本文认为，C&W之 所以与其它模型的效果有较大的差异，主要是因为 C&W模型将目标词放在了\n40 基于神经网络的词和文档语义向量表示方法研究\n输入层，这种结构对语义关系（主要是替换关系）的建模能力不如 CBOW这类 预测目标词的结构。\n结论\n经过两个实验的比较，现在可以回答引言中提出的第一个问题：使用哪个 模型效果更好？具体而言，在多种不同的上下文表示之中，以及在上下文与目 标词的两种不同的关系之间，应当如何选择合适的模型？\n一、对于小语料，像 Skip-gram这样的简单的模型会取得更好的效果。对于 更大的语料，CBOW和 Order这样对上下文有更复杂建模的模型，会有更好的 效果。\n二、对于实际的自然语言处理任务（比如将词向量用作现有任务的特征，或 者用于神经网络模型的初始值），使用 Skip-gram、CBOW和 Order这样的简单 模型就已经足够好。\n三、对于评价语言学特性的任务（比如词汇相似度），通过上下文预测目标 词的模型，比上下文与目标词联合打分的 C&W模型效果更好。其中比较特别的 是，C&W模型得到的词向量完全不包含线性平移关系。\n3.3.3 语料影响\n本小节从语料规模和语料的领域两个角度着手，分析语料对词向量的影响。\n实验概览\n为了充分研究语料领域的影响，本文选取了三份不同领域的语料，其中包\n括 16亿词的维基百科语料（Wikipedia）3、12亿词的纽约时报语料（New York Times）4以及 1300万词的 IMDB电影评论语料。同时，为了探索混合领域语料 的效果，本文还将维基百科与纽约时报语料合并，称作W&N语料，该语料总 共包含 28亿词，是本次实验中最大的语料。表 3-6展示了上述各语料的概要信 息。总体而言，纽约时报语料的文档长度较长，而维基百科语料由于涉及的知 识面更为广泛，其唯一词数也较多。\n3本文使用了 2013年 9月 7日的维基百科全站存档，使用更新的存档效果应当会更好。https://dumps. wikimedia.org/enwiki/\n4包含 1987年 1月到 2007年 7月的纽约时报新闻。https://catalog.ldc.upenn.edu/LDC2008T19\n第三章 词向量表示技术的实验分析 41\n语料 词数 文档数 唯一词数 词表内词数\n维基百科（Wiki） 1,705,736,997 4,335,623 8,387,089 1,643,119,281 纽约时报（NYT） 1,207,480,927 1,855,658 3,321,810 1,190,382,040 IMDB电影评论 13,419,330 50,000 85,092 13,243,538\n表 3-6 训练词向量的各语料集概要信息\n为了研究语料规模的影响，本文对三个较大的语料（维基百科、纽约时报、\nW&N混合语料）选取其不同规模的子集进行分析。对于W&N语料，本文选取 了 10亿（1B）词、1亿（100M）词、1000万（10M）词三个子集。对于另外两 个语料，本文各选取了 1000万（10M）词的子集。表 3-2（第 33页）中列举了 各语料及其子集的设置。对于同一个语料而言，小子集是大子集的子集。也就 是说，W&N语料中，1000万词子集的内容在 1亿词的子集中均存在，1亿词的 子集的内容也可以在 10亿词的子集中找到。另外两个语料也类似。\n为了尽可能客观地分析真正由语料带来的影响，本文还对各语料做了以下\n两个约束。\n一、为了消除词表选择对结果带来的影响，本文对所有实验设置了固定的\n词表。词表中的词保证在维基百科语料与纽约时报语料中至少各出现 23次，使 得最终词表大小约为 20万词（201369）。对于任何一份语料，词表外的词会被 直接忽略。表 3-6中“词表内词数”一列即为忽略词表外的词之后，语料中剩余 词的数量。\n二、打乱语料的顺序。word2vec这类工具包采用从前往后的顺序依次学习 语料中的各篇文档，这种方式训练得到的词向量会偏向语料中靠后部分的语义。 如果语料中各篇文档的顺序以默认顺序排列，如纽约时报以时间顺序排列，就 可能导致某些词的词向量所包含的语义更接近现代的语义，而不是几十年前的 语义。这可能会带来一些潜在的不公平。为了尽可能降低这种情况的发生，本 文对所有的语料进行文档级的随机打乱。只保证一篇文档中的词按照原来的顺 序书写，而文档和文档之间的顺序是随机的。\n使用不同语料训练得到的词向量，在各任务上的效果如表 3-7。根据 3.3.2小 节中的结论，在本小节所用的语料规模下，CBOW模型的平均表现最好。因此 本实验选取 CBOW模型训练所有词向量，使用其它模型也能得到一致的结论。 表中各任务的性能使用性能增益率衡量，其参考值为不同语料下能达到的最高\n42 基于神经网络的词和文档语义向量表示方法研究\n语料 规模 syn sem ws tfl avg ner cnn pos\n纽约时报 1.2B 93 52 90 98 50 76 85 96 100M 76 30 88 93 46 77 83 86 维基百科 1.6B 92 100 100 93 51 100 86 94 100M 74 65 98 93 47 88 90 83\nW&N\n2.8B 100 89 95 93 50 97 91 100 1B 98 87 95 100 48 98 90 98 100M 79 63 97 96 51 85 92 86 10M 29 27 76 60 42 49 77 42\nIMDB 13M 32 21 55 82 100 26 100 -13\n表 3-7 使用不同语料训练 CBOW模型时各任务的性能\n值。因此每个评价指标的最大性能增益率就是 100%，对应表格中的每一列的最 大值都是 100。以下分别分析语料的规模和领域对各任务的影响。\n语料规模\n同领域的语料，语料越大效果越好。对比纽约时报数据集与其 1亿（100M） 词的子集，或者对比维基百科语料与其子集，又或者W&N语料与其不同规模 的子集，可以发现在几乎所有情况下，同领域的大语料比小语料效果要好。表 格中有少量例外，可以看出即使在例外的情况下，小语料与其对应的大语料相 比，效果的优势也非常微弱。这些例外很可能是因为训练词向量时的波动或者 评价指标的不稳定导致的。\n值得注意的是，在 syn（句法问题的单词类比任务）中，语料的规模直接决定 了最终的性能，而语料领域几乎对结果没有影响。例如，三个语料的 100M词子 集效果非常类似；纽约时报、维基百科的完整语料以及W&N混合语料的 1B词 子集也有类似的性能；W&N语料的 10M词子集与 13M词的 IMDB语料也有类 似的性能。syn任务中的句法问题主要为类似“year:years law:__”、“good:better rough:__”的词形变换问题。尽管不同的语料所涉及的领域，甚至使用的体裁差 异较大，但它们均遵循基本的英语语法。因此，不同领域的语料训练得到的词 向量拥有相似的句法特征，在评价句法特性的任务上，表现也非常接近。\n第三章 词向量表示技术的实验分析 43\n语料领域\n对于大多数任务（除了 syn以外的任务），语料领域对词向量在各任务中的 表现起了主导作用。对于不同的任务，语料的领域有不同的影响。\n一、在评价词向量语义特征的任务中，使用维基百科会比纽约时报有更好 的效果，如 sem（语义问题的类比任务）和 ws（语义相关性任务）。在这两个任 务中，维基百科语料的 100M 子集，性能也已经超过了 1.2B 的纽约时报语料。 本文认为维基百科语料包含了更全面的知识，语义信息更为丰富，因此用维基 百科语料训练得到的词向量，包含的语义特征也更为丰富，在评价语义特性的 任务中，效果也更好。\n二、领域内的语料对相似领域任务的效果提升非常明显，但在领域不契合 时甚至会有负面作用。使用 IMDB语料训练的词向量，在 avg和 cnn任务上表 现优于其它语料，但是在 ner和 pos任务上表现很差。IMDB语料包含了 IMDB 网站上的 50万条电影评论，这与 avg和 cnn两个任务所用的训练集和测试集的 数据源相同，均来自 IMDB电影评论网站。在这两个任务中，IMDB语料的效果 甚至超过规模大两个数量级的领域外语料。尤其是在 avg任务中，词向量作为 文本分类的唯一特征，IMDB训练的词向量，性能增益率大约是其它语料的两 倍。但是另一方面，在 ner和 pos任务中，IMDB训练的词向量效果非常差，在 pos任务上甚至对最终性能有负面影响。本文认为 IMDB语料在 ner和 pos任务 中效果差主要是因为该语料为网络评论语料，与维基百科和纽约时报相比，书 写相对不规范，可能只包含少量对命名实体识别（ner）和词性标注（pos）有用 的信息。\n从上述实验中可以看出语料的领域对任务性能的影响非常明显，那么为什\n么领域内的语料会对性能有这么大的提升呢？为了更直观地感受这一原因，本 文列举了若干词及其在不同语料下的最近邻（词向量最相似的词），结果见表 3- 8。“movie”一词的最近邻在 IMDB语料下主要是“this”、“it”、“thing”这类词， 也就是说，“movie”一词在 IMDB语料中甚至可以看做是停用词。“Sci-Fi”在 IMDB语料下的最近邻均为“科幻电影”其他形式的缩写，而在W&N语料下， 则是其他类型的电影。“season”在 IMDB 下的含义表示电视剧的“季”，而在 W&N下主要表示比赛的赛季。从这些例子中可以看出，用领域内语料训练的词 向量之所以能提升对应领域的任务，其中一个主要原因是通过领域内语料训练 的词向量，包含的语意与词在该领域内的含义更为一致。\n44 基于神经网络的词和文档语义向量表示方法研究\nCorpus movie Sci-Fi season\nIMDB\nfilm SciFi episode this sci-fi seasons it fi installment\nthing Sci episodes miniseries SF series\nW&N\nfilm Nickelodeon half-season big-budget Cartoon seasons movies PBS homestand live-action SciFi playoffs low-budget TV game\n表 3-8 若干词在 IMDB和W&N语料下的最近邻\n神经网络词向量模型的训练均基于分布假说，因此，词的语义直接由其上\n下文决定。在不同领域的语料中，针对同一个词的上下文分布差异可能会非常 大，特别是多义词。领域内的语料往往只刻画出某个词在领域内的含义，而这 对于同领域的任务是恰好合适的。\n规模和领域的权衡\nW&N\nIMDB 20% 40% 60% 80% 100%\n+0% 91 94 100 100 100 +20% 79 87 91 96 99 +40% 68 86 88 92 98 +60% 65 79 85 88 93 +80% 64 75 84 87 92 +100% 64 70 83 86 88\n表 3-9 混合语料训练的词向量，在 avg任务上的效果\n前面两个小节的实验表明，领域内的数据越多，对于同领域任务的效果也\n就更好。然而在实际情况下，对于一个特定领域的任务，领域内的语料往往数 量有限。如果需要有更大规模的语料，则不得不引入领域外的语料。也就是说， 语料的领域纯度与语料规模是互相冲突的。在这种情况下，应当尽可能保持语 料的领域纯度，还是增加领域外的语料来扩大语料的规模？为了回答这一问题，\n第三章 词向量表示技术的实验分析 45\n本文设计了如下实验：选取 13M词的 IMDB数据集以及W&N语料的 13M词 子集，通过这两个数据集的不同比例的融合，探索语料领域和规模哪个更重要。 实验结果如表 3-9所示，表格中的结果为混合语料用 CBOW模型训练，在 avg 任务下的效果。表格中的每一列表示使用了多少比例的 IMDB语料，每一行则 表示加入了多少比例的W&N的 13M子集。表格中的每一列从上到下，语料规 模都在增大，而语料的领域纯度都在降低。实验结果表明无论选取多大规模的 IMDB语料，增加W&N语料都会一致地降低 avg任务的性能。这一实验中，语 料的领域纯度比语料规模更重要。 除了这一实验以外，表 3-7中的结果以及前面两个小节的分析也可以看出 语料的领域更为重要。如 sem、ws、ner任务中，100M的维基百科语料效果超 过了 1.2B的纽约时报语料；而且在这三个任务中，维基百科和纽约时报的混合 语料，均不如单纯使用维基百科语料。\n结论\n经过上述实验及分析，现在可以回答引言中的第二个问题训练语料的大小 及领域对词向量有什么样的影响？本文认为语料领域比语料规模更重要，具体 而言，可以分为以下几点：\n一、使用领域内的语料，对同领域的任务有明显的帮助。领域内的语料可 以让词向量拥有领域内的语义，对同领域任务的促进是最明显的。而且，使用 纯领域内语料比混合领域外的大规模语料的效果更好。\n二、如果选择了不合适的语料，很可能没有办法从语料中得到与任务相匹 配的语义或者用法，会对相应的任务起到负面效果。如使用网络评论语料训练 的词向量，用于词性标注任务时，性能下降。\n三、对于同领域的语料，语料规模越大，词向量性能越好。\n3.3.4 参数选择\n迭代次数\n现有神经网络词向量模型均采用迭代方式训练。与其它迭代训练的机器学\n习方法类似，词向量模型的训练结果受迭代次数的影响较大。一般来说，如果 迭代次数较少，会导致训练不充分；然而如果迭代次数过多，模型可能会过拟 合。机器学习领域应用最广泛的迭代终止条件是，当验证集损失（损失函数的\n46 基于神经网络的词和文档语义向量表示方法研究\n值）达到最低值时，迭代终止 [95]。在词向量训练过程中，损失函数刻画了模型 预测“目标词”的精度（或者上下文与目标词的匹配程度）。然而真实的任务并 不要求词向量能够多么精确地预测目标词，而是希望词向量具有更好的语言学 特性，以及能更好地辅助其它自然语言处理任务。因此，损失函数仅仅是实际 任务的一个代理，在某些情况下，损失函数与实际任务的性能可能不一致。本 小节从这一经典方法开始，探索适合词向量训练的迭代停止条件。\n本小节的实验选取 95%的语料作为训练集，剩下 5%作为验证集。图 3-1描 绘了 3个具有代表性的训练过程，分别为使用 CBOW模型在W&N语料的三个 子集（10M单词、100M单词、1B单词）上的训练过程。横坐标表示迭代次数， 纵坐标为各任务的性能以及验证集损失。\n0.300\n0.350\n0.400\n0.450\n0.500\n60%\n70%\n80%\n90%\n100%\n1 10 100 1000 10000\n0.280\n0.285\n0.290\n0.295\n0.300\n80%\n85%\n90%\n95%\n100%\n1 10 100 1000\n0.270\n0.275\n0.280\n80%\n85%\n90%\n95%\n100%\n001011\nsyn sem ws tfl avg ner cnn pos loss\n10M\n100M\n1B\n图 3-1 验证集损失及各指标的性能增益率随着迭代次数的变化曲线\n从这些例子中可以发现，验证集损失的确与实际任务的性能不一致。在\n100M词的子集中，验证集损失在第 20次迭代达到峰值，也就是说，在 20次迭 代之后，模型已经过拟合了训练数据，但是各任务的性能在之后的几次迭代中\n第三章 词向量表示技术的实验分析 47\n仍然持续上升。与之相对的，在 1B词的子集中，模型直到最后都没有过拟合训 练集，但是 ner 与 pos 任务的性能在若干次迭代之后就开始下降。这些例子说 明，在训练词向量时，使用验证集损失作为迭代停止条件，可能不是一个合适 的选择。\n值得注意的是，相比任务性能与验证集损失之间的差距，各个任务之间的\n差异反而更小。该结果表明，可以使用一个简单的任务去检测词向量是否在其 它任务上迭代到最佳状态。为了验证这一方法的可行性，本文遍历了 6种模型， 在 3 个 W&N 语料的子集上训练，用 8 个任务检验了效果，总共 144 种搭配。 表 3-10报告了选取各任务以及验证集损失作为参考指标时，在多少种情况下词 向量可以训练得足够好（达到迭代中峰值效果的 95%以上）。如果使用经典的 验证集损失作为指标，最后只有 89组实验足够好，然而使用 tfl任务（这些任 务中最简单的任务）的峰值作为迭代停止条件时，总共有 117组实验足够好。\n验证集损失 syn sem ws tfl avg ner cnn pos\n89 105 111 103 117 104 91 103 101\n表 3-10 不同迭代停止条件下，词向量足够好的实验数量（共 144组）\n当针对某个任务训练词向量时，使用任务对应的验证集作为迭代终止条件\n是最好的选择，因为这一目标与最终目标是一致的。然而在某些实际情况下，测 试一遍任务的验证集可能会非常耗时，比如 cnn和 pos这类任务，评估性能需 要数十分钟时间（文本分类或者词性标注的训练过程非常耗时），而 tfl任务只 需要几秒即可完成。因此，这一策略为性能的峰值提供了一个比较不错的近似， 尤其在目标任务非常耗时的情况下较为有用。\n另外值得一提的是，尽管众所周知，迭代算法一般需要多次迭代才能达到\n最佳效果，本小节的实验结果也支持这一结论。但是在 word2vec工具包的早期 版本中5，训练 Skip-gram 模型和 CBOW 模型均只使用一次迭代，而由于该工 具包是目前最常用的训练词向量的方法，因此大量基于 word2vec的工作并没能 达到其最佳性能。特别是在一些比较工作中（如文献 [90]），在比较其它模型与 Skip-gram 和 CBOW 模型时，对 Skip-gram 模型和 CBOW 模型只迭代了一次， 得到的结果并不准确。\n5该问题仅存在于最早的版本中，在 2014年 9月的更新中，word2vec工具包已支持多次迭代。\n48 基于神经网络的词和文档语义向量表示方法研究\n现在可以回答引言中提出的第三个问题，在迭代训练中，选择什么样的迭 代次数可以获得足够好的词向量，同时避免过拟合？本文认为在大多数情况下， 可以选取一个简单任务的性能峰值作为训练词向量的迭代终止条件。在条件允 许的情况下，选择目标任务的验证集性能作为参考标准，是最合适的选择。\n词向量维度\n本小节通过选择不同的模型和任务，分析词向量维度对性能的影响。实验\n发现，对于分析词向量语言学特性的任务，有着一致的结论：维度越大，效果 越好。图 3-2(a) 以 tfl 任务为例，绘制了性能随着维度的变化曲线图。这一结 论在文献 [73]的实验中证实，当词向量维度到达 600维时，其语义特性仍然在 变好。由于训练更高维度的词向量非常耗时，目前尚不能确定这一结果成立的 上界。与分析词向量语言学特性的任务不同，在使用词向量提升自然语言处理 任务的指标中，词向量维度到达 50之后，效果提升非常微弱。图 3-2(b)以 pos 任务为例，展示了这类实验结果。词向量的维度选择多少维比较合适？本文认 为，对于分析词向量语言学特性的任务，维度越大效果越好（除 C&W模型以 外，在 3.3.2节中已有解释）；对于提升自然语言处理任务而言，50维词向量通 常就足够好。\n3.4 相关工作\n本节将列举词向量比较方面的相关工作，并分析这些工作得到的结论与本\n章实验结论的异同。\n3.4.1 模型比较\n在比较词向量的各项工作中，与本文最类似的工作是 Turian等人在 2010年 发表的文献 [116]。他们在文中比较了 HLBL模型与 C&W模型在命名实体识别 （ner）任务和短语识别（chunking）任务中的表现，这两个任务均在现有自然语 言工具的基础上加入词向量作为额外的特征，以此评估词向量对系统性能的提\n升。他们的实验中，使用了一个 6300万（63M）词的小规模语料，并发现 HLBL 模型与 C&W模型在两个任务中表现相当。 Baroni等人在 2014年发表的文献 [3]中比较了“计数”模型与“预测”模 型在若干语义相似度任务中的表现。他们在文中将基于统计“词-上下文”共现\n第三章 词向量表示技术的实验分析 49\n40%\n50%\n60%\n70%\n80%\n90%\n10 20 50 100 200\n95.9%\n96.1%\n96.3%\n96.5%\n96.7%\n10 20 50 100 200\nSkip-gram CBOW Order LBL NNLM C&W\n(a) tfl\n(b) pos\n图 3-2 tfl和 pos任务的性能随词向量维度的变化曲线\n矩阵，以及在其基础上进行矩阵分解的方法，统称为计数模型；并将基于神经 网络的词向量模型统称为预测模型。Baroni等人在实验中使用“词-上下文”共 现矩阵的原始形式、SVD分解、NMF分解（非负矩阵分解）[62, 68]作为计数 模型的代表，使用 CBOW模型作为预测模型的代表。他们对这两类模型进行多 种语义特性的评价，包括语义相关性、同义词判别、概念分类、类比等。其实 验表明，预测模型对在各项指标中比计数模型有显著的优势。 然而，Milajevs等人在 2014年发表的文献 [77]中，给出了相反的结论。文 中指出，他们在实验中，尝试了向量逐元素相加、逐元素相乘等基本的语义组 合方式，使用这些组合方式表示短语以及句子的语义，并通过短语语义相似度 等指标进行评测。实验结果表明，基于共现矩阵的词表示方法相比神经网络的 词向量模型，其基本的语义组合能力更强。 Levy等人在 2015年发表的文献 [65]中尝试了多种不同的模型参数，包括 动态窗口或者固定窗口的选择、重新采样技术的效果、低频词的处理方式、上 下文的平滑方案等。实验发现，大部分参数设置的技巧，对基于共现矩阵的模 型以及基于神经网络的模型同时有效。\n50 基于神经网络的词和文档语义向量表示方法研究\n3.4.2 语料影响\n语料规模方面，Mikolov等人在文献 [73]中发现，语料规模越大，CBOW 模型在类比任务（本文中的 syn任务和 sem任务）中效果更好。Pennington在文 献 [90]中指出，对于 GloVe而言，语料规模越大，句法问题的类比任务（syn） 效果越好，但是语义问题的类比任务（sem）却不一定。\n语料领域方面，Stenetorp等人在 2012年 [111]发现，使用领域内的语料训 练得到的词表示，相比使用新闻语料训练得到的词向量，在进行生物医药领域 的命名实体识别任务时，有明显的优势。他们在实验中使用的词表示为布朗聚 类（Brown Clustering），与基于神经网络的模型不同，布朗聚类得到的词表示并 非是一个低维的实数向量。但是类似的是，无论是基于聚类的词表示方法还是 基于神经网络的词向量表示方法，他们都是根据语料统计建模得到的，因此语 料的影响也是类似的。\n3.5 本章小结\n词向量包含了丰富的词义信息，对词义分析以及各项自然语言处理任务均\n有一定的帮助。长期以来人们通过增加语料、改进模型等手段，试图寻找一种 通用有效的词向量。然而本文通过大量实验，发现对于不同的任务，最好的词 向量所用的模型、语料、参数均各不相同。也就是说，对于所有任务都有效的通 用词向量，可能是不存在的。从模型的角度看，不同的模型建模了不同的语义 关系，如 C&W模型可以对组合关系进行建模，而 CBOW等模型可以更好地对 替换关系进行建模。从语料的角度看，特定领域的任务需要词在该领域中的含 义，而往往只有领域内语料，才能将多义词训练成为领域内的语义。从任务的 角度看，不同的任务需要词向量具有不同的性质，这些性质之间可能存在矛盾， 如 ner、pos任务希望词向量包含规范的语法信息，而 cnn任务需要词向量具有 网络语言的灵活表示。因此无论从模型、语料还是任务的角度看，通用有效的 词向量，都是几乎不可能获得的。尽管不存在对所有任务有效的词向量，对于 特定任务，依然可以使用本章得到的结论，生成一份有效的词向量。\n1. 选择一个合适领域的语料，在此前提下，语料规模越大越好。使用大规模 的语料进行训练，可以普遍提升词向量的性能，如果使用领域内的语料，\n对同领域的任务会有显著的提升。\n第三章 词向量表示技术的实验分析 51\n2. 选择一个合适的模型。复杂的模型相比简单的模型，在较大的语料中才有 优势。简单的模型在绝大多数情况下已经足够好。预测目标词的模型比目\n标词与上下文呈组合关系的模型（C&W模型）在多个任务中有更好的性 能。\n3. 训练时，迭代优化的终止条件最好根据具体任务的验证集来判断，或者近 似地选取其它类似的任务作为指标，但是不应该选用训练词向量时的损失\n函数。\n4. 词向量的维度一般需要选择 50维及以上，特别当衡量词向量的语言学特 性时，词向量的维度越大，效果越好。\n第四章 基于字词联合训练的中文表示及应用\n不同于英文，中文中最自然的语言单位是“字”。现有工作在学习中文文本\n的向量表示时往往直接沿用了英文的处理方式，而忽略了中文的特殊性。本章 根据中文特点，提出了基于字词联合训练的中文字、词表示学习方法。本文还 提出了一个基于神经网络的中文分词模型，探索了神经网络在中文分词中的效 果，也在该任务上评价字词联合训练得到的字表示效果。\n4.1 引言\n词是“最小的能独立运用的语言单位”[1]，由于中文具有大字符集连续书 写的特点，如果不进行分析，计算机则无法得知中文词的确切边界，从而很难 理解文本中所包含的语义信息。因此，中文分词是自然语言处理中的一个关键 基础技术，是其他中文应用，例如命名实体识别、句法分析、语义分析等任务 中前期处理的关键环节，其性能的优劣对于中文信息处理尤为重要。\n中文分词的研究在近二十年来取得了丰富的成果。早期采用的是基于词典\n的匹配方法，如：最大正向匹配、最大逆向匹配、双向匹配等。然而，由于语言 的复杂性，中文文本中存在大量的词边界歧义与未登录词 (OOV)。仅仅使用基 于词典的匹配方法无法有效地解决以上两个中文分词中的关键难点问题。所以 越来越多的方法关注基于字的中文分词。基于字的中文分词方法的基本假设是 一个词语内部字符高内聚，而词语边界与外部汉字低耦合。每一个词都可以通 过其所在的上下文特征进行表示，通过统计模型可以很好的判别当前字在构词 过程中的作用（词的开始、中间、结束或是单字词）。大量实验表明，这种基于 字的中文分词方法要明显优于基于词典匹配的分词方法。然而，现有基于字标 注的分词方法通常使用词（unigram）、二元词组（bigram）等特征，这些特征都 是相互孤立的，且本身并不具有语义信息。比如“江”、“河”、“湖”、“海”在 表示地名时用法非常类似，但是现有模型会分别对这四个字进行建模，不能很 好地利用字之间的联系。\n近些年随着深度学习的兴起，基于神经网络的特征学习方法为自然语言处\n理带来了新的思路。如本文其它章节所介绍的，现有基于神经网络的词向量和 文档向量表示技术已经为命名实体识别、词性标注、文本分类等多项任务带来\n54 基于神经网络的词和文档语义向量表示方法研究\n了性能上的提升。本文基于这一想法，提出了基于字表示的分词模型。该模型 利用神经网络字表示对字与字之间的关系进行建模，在较少参数的情况下，依 旧保持了较好的分词精度。\n在基于字表示的分词模型中，获得有效的字表示是其中非常关键的一步。\n但是现有的中文字表示工作中，大多数方法直接沿用了第二章中所介绍的英文 词向量的生成方法，将字当做模型的处理单元，建立每个字与其上下文字之间 的关系 [131]。但是，在中文中，如果想要通过词向量模型，直接获取字的“语 义”，可能会遇到一些障碍。如文献 [1]所述，词是能独立运用的最小语言单位， 语义是词或者词组与他们之间含义的关系，直接从字的层面分析语义，可能意 义并不大。由于这一原因，本文设计了基于字词联合训练的中文表示技术，该 方法使用词的语义空间对字进行建模，为字带来更好的表示。\n另一方面，对于中文的词表示，大多数工作同样直接使用了处理英文的方\n法。然而中文的符号系统是汉字，如果可以有效利用字与字之间的关系，以及 字与词之间的关系，势必能为中文词表示带来提升。Chen等人的工作 [15]通过 假设词的语义由其中字的意思以及词特有的意思融合而成，构造了 CWE模型。 该模型已经超越了直接使用英文词向量模型的效果。本文提出的字词联合训练 模型通过对汉字的有效建模，让这些汉字建立了一些词之间的关系，使得词的 上下文更丰富，从而提升了词表示的语义。实验结果表明，该模型的效果超过 了现有利用字信息提升词义的方法。 本章后续内容安排如下：第 4.2节介绍了分词和表示学习的相关工作；第 4.3节 介绍了基于字词联合训练的表示方法；第 4.4节介绍了基于字表示的分词算法 框架；第 4.5节为实验及分析；最后对本章工作进行总结。\n4.2 相关工作\n4.2.1 表示学习\n传统机器学习方法中，特征的选取是其中最耗时的一个环节。这一环节需\n要耗费领域专家大量的精力，针对具体任务进行分析，从而设计出有效的特征。 表示学习的提出缓解了这一严峻的问题。表示学习的目标是通过算法自动学习 得到特征表示，使得机器学习算法可以更高效地运转。\n在自然语言中，最基础的语义单元为词。研究人员已经提出了大量词表示\n的学习方法，如：Skip-gram [73]、CBOW [73]、NNLM [7]等模型，这些模型在\n第四章 基于字词联合训练的中文表示及应用 55\n本文第二章进行了综述，并在第三章做了详细的实验比较。其中无论哪个模型， 均基于分布假说，对目标词与其上下文词之间的关系进行建模。因此这类模型 的最终效果受到分布假说的制约。\n原始文本\n输入层\n输出层 与 中国 科研\n+\n教育 教B 育E\n|V|\n图 4-1 CWE+P模型结构图\n与本文同期，已经有两项工作跳出了分布假说的框架，将词的内部元素考\n虑到词表示中。在中文词表示方面，Chen等人在 2015年发表的文献 [15]中提 出了 CWE+P等模型（结构如图 4-1），这一模型改进自 Skip-gram模型（也适用 于 CBOW模型），将一个词拆分成两部分：词本身和组成这个词的汉字。训练 过程中，使用词本身的向量以及组成这个词的各个字向量的平均值表示这个词 的语义。\n在英文词表示方面，Sun等人在 2016年发表的文献 [112]中提出了 SEING 模型（结构如图 4-2所示）。该模型认为英文中具有相同语素（morpheme）的单 词具有相似的语义，因此在建模时使用 Skip-gram的思路，对于目标词不仅预测 上下文的词，也预测目标词的所有语素。这套方法也可以近似沿用到中文处理， 将英文单词的语素类比为中文的汉字。\n文献 [15]与 [112]均为对词表示的改进。然而在中文分词、词性标注等任务 中，对汉字的表示也是一个必不可少的环节。现有的字表示方法，都直接沿用 了英文中对单词的建模方法，将训练语料拆分成字级别，建立语料中每个字与 其上下文其它字之间的关系 [131]。对于字表示的方法，需要有进一步的探索。\n56 基于神经网络的词和文档语义向量表示方法研究\n原始文本\n输入层\n输出层 与 中国 科研\n教育\n教 育\n|V| |C|\n图 4-2 SEING模型结构图\n4.2.2 中文分词\n传统中文分词方法依赖词典匹配，并通过贪心算法截取可能的最大长度词\n进行有限的歧义消除。常用的贪心策略有正向最大匹配法、逆向最大匹配法和 双向匹配法等。然而，基于词典方法存在两个明显的缺陷，即不能很好地处理 词边界歧义和未登录词（OOV）。为了解决中文分词的这两个关键问题，许多研 究工作集中到了基于字标注的机器学习中文分词方法。\n基于字标注的中文分词方法基本假设是一个词语内部文本高内聚，而词语\n边界与外部文字低耦合。通过统计机器学习方法学习判断词界是当前中文分词 的主流做法。现有工作大多使用序列标注模型执行 BMES标注。Xue等人提出 了基于 HMM 模型的字标注中文分词方法 [125]。刘群等提出一种基于层叠隐 马模型的汉语词法分析方法 [132]。该方法引入角色 HMM识别未登录词，使用 Viterbi算法标注出全局最优的角色序列。同时，该方法还提出了一种基于 N-最 短路径的策略进行切分排歧。Wang等人使用基于字分类的 CRF模型进行中文 词法分析 [89]。对基于字标注中文分词方法的改进包括引入更多的标签和设计 更有效的特征 [114, 129]、联合使用产生式模型和判别式模型以融合两者的优点 [119]以及将无监督方法中使用的特征引入有监督方法中 [130]等。这些传统统 计机器学习方法依赖于人工设计的特征，设计特征需要大量的人工参与，设计 有效的特征非常费时费力。而将表示学习方法引入机器学习中，可以将特征表 示这一步交给算法完成，在一定程度上减少人工，提升效率。\n第四章 基于字词联合训练的中文表示及应用 57\n4.3 基于字词联合训练的中文表示技术\n分布假说认为，词的语义由其上下文决定。根据分布假说构造的词向量模型\n也在各项任务中取得了一定的成果。但是在中文里，最自然的语言单位是“字”。 不同于富含语义信息的词，字仅为记录汉语用的符号系统，本身不具备语义 [1]。 在一些现有的中文向量表示的工作中，直接将分布语义推广到字表示中，使用 上下文中各个字的分布作为当前字的表示 [131]。但由于字本身不具备语义，这 种方式的效果会受到一定的约束。\n为了让字的表示具有更丰富的语义信息，本文借鉴了分布假说的思想，提\n出利用某个字上下文中各个词的分布，作为这个字的表示。虽然字本身仍然不 具备语义信息，但是利用这种表示，把字放入词的语义空间中，通过字词联合 训练，可以更有效地对字进行建模。\n本文第二章和第三章对若干现有的词向量模型进行了阐述和分析，这里本\n文选取其中一个对上下文建模最为直接的模型，Skip-gram模型，并在此基础上 实现字词联合训练的想法。如公式 4.1（同公式 2.22）所示，Skip-gram模型的 优化目标为，w的上下文中的某个词 wj 对词 w的条件概率：∑\n(w,c)∈D ∑ wj∈c logP (w | wj) (4.1)\n为了实现字词联合训练，本文提出同时优化上下文中某个词 wj对目标词 w\n的条件概率，以及上下文词中各个的汉字 chk 对目标词 w的条件概率：\n∑ (w,c)∈D ∑ wj∈c (1− β) logP (w | wj) + β 1|wj| ∑ chk∈wj logP (w | chk)  (4.2) 式中，chk表示词 wj 中的汉字，|wj|表示词 wj 的字数。其中归一化项 1|wj | 用于 使不同字数的词在训练中拥有同样的地位。模型结构如图 4-3。\n在字词联合训练中，不仅每个词具有对应的词向量，每个字也具有对应的\n字向量。词向量和字向量的维度相同，并且根据目标函数（公式 4.2），字和词 的向量表示在同一个语义空间中。\n模型的训练方法与第二章中介绍的各个词向量模型相同，采用随机梯度下\n降法求解各个词以及各个字的向量表示。\n58 基于神经网络的词和文档语义向量表示方法研究\n原始文本\n输入层\n输出层 与 中国 科研\n教育 教 育\n|V|\n图 4-3 字词联合训练模型结构图\n字词联合训练模型相比单独训练字向量，在时间复杂度上只有常数的增加，\n增幅大约为语料中词的平均字数，一般在 3左右。因此该方法仍然可以推广到 大规模语料中使用。\n4.4 基于字表示的分词模型\n与其它基于字标注的分词方法相似，本文提出的分词模型也采用 BMES体 系对汉字进行标注。对于单字词，其标签为 S；对于多字词，词中的第一个汉 字标签为 B，最后一个汉字标签为 E，中间字的标签为 M。对训练数据的每个 字进行标注后，本文采用一种 3层神经网络结构对每个字进行训练，其结构如 图 4-4。\n对于句子中的每个字的标签分类任务，本文选取当前字以及上下文窗口中，\n共 win个字作为特征。其中上文和下文均为 (win − 1)/2个字。图中最下方为 这 win个字的原始文本 w1, w2, . . . , wwin，经过第一层，将每个字转换成其字向 量表示 e(wi)，并把 win个字连接成一个 win × |e|维的向量 x。该向量是神经 网络的输入层：\nx = [e(w1); . . . ; e(wwin)] (4.3)\n隐藏层 h的设计与普通的前馈神经网络一致，输入层的各个节点与隐藏层\n的 |h|个节点之间两两均有边连接。隐藏层选用 tanh函数作为激活函数：\nh = tanh(b(1) +Hx) (4.4)\n第四章 基于字词联合训练的中文表示及应用 59\n教 育 与 科 研\nB M E S\n原始文本\n输入层\n隐藏层\n输出层\nx\nh\ny\n图 4-4 分词算法基本网络结构图\n其中 H 为输入层到隐藏层的权重，b(1) 为偏移向量。使用类似的方法，可以将 隐藏层转为输出层：\ny = b(2) + Uh (4.5)\n输出层一共有 4个节点，使用 softmax[11]归一化后，分别表示这个字被打上 B、 M、E、S标签的概率：\nP (i | w, θ) = exp (yi)∑ k=1...4 exp (yk)\n(4.6)\n其中参数 θ包含各个字的字向量 e，以及网络中的参数 H、U、b(1)、b(2)。\n对于整个训练语料，本文使用最大似然估计法，即求出一组参数 θ，最大\n化：\nθ 7→ ∑\n(w,tagw)∈D\nlogP (tagw | w, θ) (4.7)\n其中 D是训练语料集，tagw 是字 w的正确标签。\n本文使用随机梯度下降法 [10]来优化上述训练目标。每次迭代，随机选取 一个样本 w, tagw，使用下式进行一次梯度迭代。式中，α是学习速率。\nθ ← θ + α∂ logP (tagw|w, θ) ∂θ\n(4.8)\n60 基于神经网络的词和文档语义向量表示方法研究\n4.5 实验及分析\n为了充分说明字词联合训练的有效性，本文从字的表示和词的表示两方面\n分别评价字词联合训练方法。下文 4.5.1小节介绍了字词联合训练的实验设置， 包括训练语料与参数；4.5.2小节介绍了对其中字向量部分的评价；4.5.3小节介 绍了对其中词向量部分的评价；4.5.4小节探讨了如果在字词联合训练的基础上 额外加入上下文的字，会对模型产生什么影响。\n4.5.1 字词联合训练实验设置\n在训练字词向量时，本文使用了两个语料：“小语料”为北京大学标注的\nSIGHAN 2005分词数据的训练集，也是 4.5.2小节中分词实验的训练集，共 179 万字。该语料中的词为人工标注的标准词，与分词实验中的词一致。“大语料” 为维基百科的中文语料，共 1.6亿字。本文使用 ICTCLAS1工具包对其进行分词。 由于使用工具包进行分词，语料中存在一定的分词错误现象。\n本文选取这两个数据集主要出于以下两方面考虑。一、对于评价字表示的\n实验，“小语料”属于领域内的小规模标准数据集，而“大语料”属于大规模少 量噪声的领域外数据集。本文希望通过这两个数据集分析有少量噪声的大规模 语料对字向量性能的影响。二、对于评价词表示的实验，本文只使用“大语料” 训练字词联合表示。根据第三章的实验结果，使用维基百科语料训练的词向量 对于语义类任务的效果最理想，因此在评价字词联合训练的词表示时，也使用 通过维基百科训练的词向量。\n根据第三章中的经验，本章训练字词向量时，所有字、词向量的维度均为\n50；上下文窗口大小为 5；同时，也采用负采样技术对模型进行优化。\n4.5.2 字表示的实验\n根据第三章中的分析，在自然语言处理任务中，如果使用好的词向量作为\n神经网络模型的初始值，能使模型收敛到更好的局部最优解。因此本文将字向 量作为神经网络分词模型（第 4.4节）的初始值，对不同字向量的效果进行评 价。\n1http://ictclas.nlpir.org\n第四章 基于字词联合训练的中文表示及应用 61\n实验设置\n本节采用分词任务对字向量进行评价。分词数据集采用 SIGHAN 2005 bakeoff评测中，北京大学标注的语料。原始语料只包含了训练集与测试集，在实验 前，本文将原始语料的训练集前 90% 当作实际的训练集，最后 10% 当作验证 集。测试集保持不变。最后训练集共有 1626187个字，验证集包含了 160898个 字，测试集有 168973字。\n在分词语料中，英文与数字的出现次数较少（甚至有可能 26个英文字母中 有的字母未在训练集中出现过）。为了简化处理流程，本文使用了一个简单的数 据预处理步骤，将所有的连续数字字符替换成一个专用的数字标记“NUMBER”， 将所有连续的英文字母替换成一个专用的英文单词标记“WORD”。如训练语 料“中国/教育/与/科研/计算机网/（/ＣＥＲＮＥＴ/）/已/连接/了/２００/多/所/大 学”经过预处理步骤将会变成“中国/教育/与/科研/计算机网/（/WORD/）/已/连 接/了/NUMBER/多/所/大学”。其中 NUMBER和WORD在训练时都当作一个字 符来考虑。这种方法在一定程度上丢失了部分语义信息，会对分词精度产生负 面的影响。但是在训练语料不充分的情况下，该预处理可以简化后续步骤，将 实验重心放在处理汉字上。\n基于字标注的分词模型需要确定上下文窗口的大小，即设定上下文窗口中\n共多少个字会对当前字的标签产生主要影响。黄昌宁和赵海在文献 [135] 中通 过大量实验表明窗口 5个字可以覆盖真实文本中 99%以上的情况。因此本文也 取上下文窗口为 5，即使用上文 2个字、下文 2个字与当前字。这一参数不仅用 于神经网络分词模型，同时也用于对比方法中的其它模型。基于字标注的分词 模型得到的结果为对各个字打各个标签的概率，当模型对文本中每个字都算出 标签概率后，本文使用 Viterbi算法搜索最优路径，得到最终的分词结果。\n对比方法\n本节实验主要为了证明字词联合训练对字表示的促进作用。因此主要对比\n模型为不使用字词联合训练的 Skip-gram 模型。在这一对比实验中，首先需要 将语料拆分成字，然后直接用 Skip-gram 模型在汉字级别对字进行建模。另一 方面，为了说明分词模型的效果，本文同时选用了分词中较常用的最大熵模型， 特征选用一元及二元特征。对于语料中的某个字 chk，其特征向量具体包括：\n62 基于神经网络的词和文档语义向量表示方法研究\n• 一元特征 chi，其中 i为 {k − 2, k − 1, k, k + 1, k + 2}，如果 chi 超出了句 子的边界，则使用一个特殊的符号“PADDING”来代替。 • 二元特征 chi_chi+1，其中 i为 {k− 2, k− 1, k, k+ 1}，如果 ci或 ci+1超出 了句子的边界，则忽略这个特征。\n以上各特征的权重均为 1。具体而言，本文设计了两个基准实验，第一个实验只 使用了上述的一元特征，在后文中称作“最大熵一元特征”；第二个实验同时使 用了一元特征和二元特征，在后文中称作“最大熵二元特征”。\n实验结果及分析\n模型 准确率 召回率 F值 Sighan2005最佳成绩（封闭） 94.6 95.3 95.0 Sighan2005最佳成绩（开放） 96.8 96.9 96.9 最大熵一元特征 86.8 86.6 86.7 最大熵二元特征 95.3 94.5 94.9 随机字向量 93.43 92.91 93.17 小语料字向量 93.76 93.17 93.46 大语料字向量 93.66 93.29 93.48 小语料字词向量 93.96 93.21 93.58 大语料字词向量 94.02 93.28 93.65\n表 4-1 各模型在中文分词任务上的表现\n为了展示不同字向量对分词系统的性能影响，本文设计了多组对比实验：\n• 随机字向量：使用随机赋值的字向量，对 4.4节中的模型进行初始化。 • 小语料字向量：使用 Skip-gram模型直接在小语料上训练字向量，作为网 络的初始值。 • 大语料字向量：使用 Skip-gram模型直接在大语料上训练字向量，作为网 络的初始值。 • 小语料字词向量：使用字词联合训练在小语料上训练字向量，作为网络的 初始值。 • 大语料字词向量：使用字词联合训练在大语料上训练字向量，作为网络的 初始值。\n第四章 基于字词联合训练的中文表示及应用 63\n表 4-1中列举了本文所做的 7组实验。其中“最大熵一元特征”和“最大熵 二元特征”为上一小节中描述的两个基准实验，后面各实验为不同的字向量对 分词性能的影响。\n通过这些实验，可以发现，在神经网络模型中，无论使用哪种技术生成的\n字向量，均能有效提升模型的性能。为了更直观地评估各种字向量对分词模型 的帮助，本文使用第三章第 3.3.1小节（第 33页）中提出的性能增益率来评价 这几种不同的字向量。以随机字向量作为基准，四种字向量中 F值最高的大语 料字词向量作为对比，各字向量的性能增益率为：小语料字向量 60%，大语料 字向量 65%，小语料字词向量 85%，大语料字词向量 100%。\n本文对比在两种语料下，字词联合训练与单独训练字表示。实验表明，字词\n联合训练在两种语料下，在性能增益率上分别有 25到 35个百分点的提升，充 分说明了字词联合训练对于中文字表示有非常显著的提升。\n对比不同语料下，字向量带来的性能提升，可以发现，在单独对字进行训\n练时，大语料能对字向量带来微弱的提升；而如果使用字词联合训练，大语料相 对小语料能明显提高字向量的性能。本文认为之所以使用字词联合训练时，大 语料更能体现出其效果，是因为如果只针对字进行训练，字向量的上下文空间 局限在字符层面，相比而言，字词联合训练时，字向量的上下文空间为词的语 义空间，信息更为丰富。而维基百科大语料中本身拥有了大量的语义信息，只 有通过表达能力强的表示才能充分捕获到其中的信息。\n综合对比基于字向量的神经网络分词模型与传统的机器学习分词方法（基\n准实验），神经网络分词模型虽然也只使用了各个词中的字的表示作为输入，相 比使用一元特征的传统分词方法，有非常显著的提升。然而不可否认，神经网 络分词模型相比二元特征仍然有一定的差距。这些差距可能需要更有效的字表 示技术以及更好的神经网络模型来弥补。Collobert在文献 [18]中也得到了类似 的结论，在词性标注、命名实体识别等序列标注任务中，他们的神经网络方法 与传统基于特征工程的机器学习方法仍然有一些差距。他们在实验中指出，神 经网络模型配合少量人工的先验知识，就可以达到以往通过人工精心设计特征 才能达到的性能。\n4.5.3 词表示的实验\n为了说明字词联合训练对词表示的作用，本节使用语义相关性任务和文本\n分类任务对联合训练中得到的词表示进行评价，并与 Chen等人 [15]与 Sun等\n64 基于神经网络的词和文档语义向量表示方法研究\n人 [112]的模型进行对比分析。\n评价方法\n与第三章中对英文词向量的评价方法一致，本节对中文词向量的评价同样\n从三个方面进行：一、利用词向量的语言学特性完成任务；二、将词向量作为特 征，提高自然语言处理任务的性能；三、将词向量作为神经网络的初始值，提 升神经网络模型的优化效果。本文从这三类评价方法中，各选取一个任务评价 不同模型得到的词向量。\n在评价词向量的语言学特性方面，本节选取了语义相关性任务。该任务与\n第三章 3.2小节提到的 ws任务类似，这里选用文献 [15]提供的两个中文语义相 关性数据集进行评价。这两个数据集分别含有 297个词对以及 240个词对，在 下文中简写为 ws297和 ws240。数据集中的每个词对都有若干位标注者对其进 行打分（两个数据集的打分范围分别为 0到 10以及 0到 5），分数越高表示标 注人员认为这两个词的语义更相关或者更相似。例如，词对“饮料、汽车”的 平均打分为 1.23，而词对“学校、学生”的打分为 8.71。评价时，对于每个词 对，本文使用所有标注者打分的平均值作为参考得分 X，以词对中两个词的词 向量的余弦距离作为模型得到的相关性得分 Y，并衡量 X, Y 这两组数值之间 的皮尔逊相关系数。具体定义可见 3.2小节中的公式。\n在将词向量作为特征提升系统性能方面，本节选用基于平均词向量的文本\n分类方法。该方法与第三章 3.2小节提到的 avg任务一致，后文同样简写为 avg。 实验选取复旦文本分类语料2，使用 Logistic回归模型完成文本分类任务。\n在将词向量作为初始值提升神经网络模型性能方面，本节选取卷积神经网\n络和循环卷积网络完成文本分类任务。其中卷积神经网络与第三章 3.2小节提 到的 cnn任务一致，而循环卷积网络则是本文第五章提出的神经网络文档表示 模型，这两个任务在后文中简写为 cnn和 rcnn。实验选用的语料同样是复旦文 本分类语料，模型参数与第五章中所用参数一致。\n对比方法\n本节实验希望验证字词联合训练对于词的语义有提升。为了说明这一点，\n实验的主要对比方法为传统的 Skip-gram模型，该模型直接对词进行建模，而不 2http://www.datatang.com/data/44139，http://www.datatang.com/data/43543\n第四章 基于字词联合训练的中文表示及应用 65\n考虑字在其中的作用。本文提出的字词联合训练在 Skip-gram 模型的基础上加 入了对字的建模，因此对比 Skip-gram模型的提升，就是字词联合训练对词表示 带来的实际提升。 进一步地，为了说明字词联合训练的有效性，本文还将Chen等人的CWE+P 模型 [15]与 Sun等人的 SEING模型 [112]纳入对比。Chen等人的模型同样也 是对 Skip-gram模型的改进，这种方法将一个词拆分成两部分：词和组成这个词 的各个汉字，在表示这个词的语义时，使用词本身的向量以及组成这个词的各 个字向量的平均值。Sun等人的方法原本只对英文进行建模，该模型认为英文 中具有相同语素的单词具有相似的语义，因此在建模时使用 Skip-gram的思路， 对于目标词不仅预测上下文的词，也预测目标词的所有语素。这套方法也可以 近似沿用到中文处理，将英文单词的语素类比为中文的汉字。\n实验结果及分析\n模型 ws240 ws297 CWE+P [15] 44.07 53.43 SEING [112] 42.84 49.80 Skip-gram 43.51 52.53 字词联合训练 46.68 54.45\n表 4-2 各模型在中文语义相关性任务上的表现（×100）\n表 4-2展示了不同方法在词类比任务中的效果。实验中，设定字词联合训 练的参数 β = 0.5，即各模型对字和词的建模比例为 1 : 1。对于 CWE+P模型与 SEING模型，文献 [15]和 [112]并没有对字和词的建模比例进行考虑，也可以 认为这两个模型对字词的建模比例为 1 : 1。 为了更进一步说明本文提出的字词联合训练的有效性，这里调节公式 4.2中 的 β，分析在不同的 β 下，字词联合训练的效果。作为对比，实验中同样调节 了 CWE+P与 SEING模型的字词建模比例。对于 CWE+P模型，可以直接使用 CWE开源工具包3，设定 char-rate参数为 β/(1− β)；对于 SEING模型，计算每 个样本时，设定有 β 的比例对词中的字进行预测，有 1 − β 的比例对词的上下 文进行预测。图 4-5展示了实验结果。参数 β 的含义为在字词联合训练中，对\n3https://github.com/Leonard-Xu/CWE\n66 基于神经网络的词和文档语义向量表示方法研究\n0.46\n0.48\n0.5\n0.52\n0.54\n0.56\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nw s2\n97 任 务 性 能\n汉字的建模比例\nCWE+P SEING 字词联合训练\n图 4-5 各模型汉字的建模比例对词义的影响\n字的建模占多大的比例。如果 β = 0则只对词进行建模，三个模型均等价于原 始的 Skip-gram模型；如果 β = 1说明模型只对字进行建模。实验中尝试了 β从 0到 0.9中的各个值。从整体上看，逐渐增加汉字建模比例时，字词联合训练对 词义的建模呈现出先上升后下降的趋势；CWE+P模型也有类似的趋势，峰值处 汉字的建模比例为 10%（性能为 0.5486，略低于字词联合训练）；而 SEING模 型几乎随着对汉字建模比例的增加，效果一直变差。 本文提出的字词联合训练与 CWE+P模型或者 SEING模型均为对字（或者 语素）和词同时建模，虽然建模方式有所不同，但是为什么这三种方法在实验 中呈现出较大的差异？尤其是 SEING 模型，对于英文词义表示有显著的提升 [112]，然而在中文词义相似度任务中却起到了负面效果。为了回答这一问题， 本文首先分析这三种模型对字词建模的区别。\nCWE+P模型的基础假设是词的语义可以拆解成两部分：词中的各个汉字 （模型名称中的 +P表示汉字需加上前后缀的标记）的语义，以及这个词特有的 语义。在文献 [15]中，作者将这两部分组合成实际词向量时，使用了向量加法。 在这种设定下，字词所占语义比例需要仔细调节，如果比例过高，则词的语义\n几乎直接由字决定，在现代汉语中这一点并不成立。从前面的实验中也看出，字 的比例在 10%左右时可以让词义达到最佳效果，原文中设定的 50% 很可能不 是最佳参数。从另一个角度看，由于目前 CWE模型只使用了朴素的向量加法作 为词义合成，如果选用更合理的组合函数，应当能达到更好的效果。\n第四章 基于字词联合训练的中文表示及应用 67\nSEING模型的基础假设是，具有相似语素的单词，有相似的含义，类比到 中文即具有相似汉字的词，有相似的含义。从前面的实验中可以看出，这一假设 从英文沿用到中文时，效果并不理想。造成中英文区别的原因可能有两点：一、 中文的语素（morpheme）并非直接为汉字。虽然大多数情况下中文汉字就是语 素，但是也存在一些多个字组成的语素（多音节语素），如“苏打”、“巧克力”。 二、英文中的语素多为表义的词根、固定的前缀后缀等（如 unbreakable可以分 解成 un、break、able三个语素），而汉字在组词时，相比英文更为灵活，大多数 汉字既可以作为前缀，又可以作为后缀（如星期天、天空），即使同作为前缀或 后缀，意思也不尽相同（如夏天、晴天）。综合分析这两点，从统计上看，第一 点中提到的多音节语素在中文语料中的比例较低，而第二点区别在中英文中普 遍存在，更可能是影响 SEING模型性能的核心原因。因此，将 SEING模型的 基础假设直接沿用到中文时，即具有相同汉字的词拥有相似的语义，或许并不 合适。\n本文提出的字词联合训练从模型上看，只增加了对汉字上下文的建模，并 没有修改词的建模。但是实际上，由于汉字的出现，一些原本没有联系的上下 文，被联系了起来。也就是说，汉字对词起到了平滑作用，在一定程度上扩充 了上下文，从而提升了词表示。与此同时，由于汉字的上下文也是根据分布假 说构造得到的，因此即使汉字的建模比例不合理，一般情况下也不会产生负面 作用。\n综合分析这三个模型，CWE+P模型与本文的字词联合训练模型，遵循了分 布假说，词义由其上下文决定；而 SEING模型要求每个词有两个分布，不仅需 要符合上下文的分布，还需要符合内部语素的分布，某种程度上说，这种方法 破坏了分布假说，对于中文词表示效果不理想。对比 CWE+P模型和字词联合 训练模型，CWE+P模型利用构词法的思想认为词的意思部分有字决定，而字词 联合训练模型扩展了分布假说，对字的上下文词直接进行建模。尽管从本文的 实验中看，字词联合训练略胜一筹，但是 CWE+P模型通过调节参数，或者变 化构词方法，仍然很有潜力。\n为了更直观地感受这些模型之间的差异，本文选取若干词，以及通过这些\n模型得到的最近邻。结果列于表 4-3。\n从第一个例子“巡逻”中可以看出，三种对汉字建模的模型均可以在 Skipgram的基础上，让字面上更相似的词在空间距离中的距离更短。如“巡逻队”、“巡\n68 基于神经网络的词和文档语义向量表示方法研究\n模型 Skip-gram CWE+P SEING 字词联合训练\n巡逻\n值勤 巡逻队 巡逻队 巡逻队 巡逻队 巡逻艇 巡逻艇 值勤 侦察 执勤 巡逻舰 巡逻艇 护航 护航 侦察 侦察 执勤 巡逻员 防空 蛙人\n星期天\n周日 星期二 星期二 周日 周末 星期三 周日 星期六 星期四 星期四 星期日 星期二 礼拜天 星期五 星期五 星期日 星期六 星期六 星期六 礼拜天\n风光旖旎\n湖光山色 湖光山色 旖旎 湖光山色 叠翠 山清水秀 风光 葱郁 草泽 风动石 湖光山色 丰茂 满陇桂 茶园 碧波 山清水秀 丰茂 风光 风景点 叠翠\n表 4-3 不同字词模型得到的最近邻对比表\n逻艇”这些词，在 Skip-gram 模型中排名比较靠后，而融入了对汉字的建模之 后，排名均有提升。第二个例子“星期天”可以看出，CWE+P和 SEING模型更 多地考虑了字面上的相似性（“星”、“期”二字），最近邻中主要为星期中的其 它日子。而字词联合训练更多地保留了语义信息，不仅得到了 Skip-gram模型原 本就能得到的“周日”和“礼拜天”，再配合字对词的平滑，可以发现“星期日” 这个字面上更相似的同义词。第三个例子本文选取了低频词“风光旖旎”，所有 模型均可以发现近义词“湖光山色”。SEING模型对字面相似有单独的建模，因 此最相似的两个词是“旖旎”和“风光”；CWE+P模型中字对词义的影响也比 较大，因此“风动石”、“风光”也排在了靠前的位置；字词联合训练通过平滑 得到的词多为形容风景秀丽的词，相比 Skip-gram模型得到的“草泽”等词，与 原词的意思更匹配。\n尽管在词义相似度任务中，不同的模型得到的词向量展现出了不同的语义\n特性，但是在文本分类任务中，这些模型相比 Skip-gram模型，都能一致地提升 性能，具体结果列于表 4-4中。从这些结果中可以看出，Skip-gram模型生成的 词向量，比随机词向量的性能有显著的提升，进一步地，融入了字信息的三种模\n第四章 基于字词联合训练的中文表示及应用 69\n模型 avg cnn rcnn 随机词向量 80.75 93.57 94.89 Skip-gram 85.53 94.04 95.20 CWE+P 86.15 94.17 95.27 SEING 85.66 94.15 95.32 字词联合训练 86.23 94.22 95.30\n表 4-4 各模型在中文文本分类任务上的表现\n型得到的词向量，比 Skip-gram均有一定程度的提升。这三种文本分类模型中， avg使用了结构最简单的 Logistic回归，cnn使用了相对较为复杂的卷积神经网 络，而 rcnn使用的是本文提出的循环卷积网络。从实验结果中可以看出，在相 对简单的模型中（如 avg），使用更好的词向量，可以带来较大幅度的提升；而 在相对复杂的模型中（如 rcnn），词向量带来的提升较少。纵观这三种模型，以 及五种不同的词向量，可以很明显地看出，对于文本表示，模型结构的影响远 远大于初始值（或者特征）的选择。这也是后文深入探索文本表示模型的一个 出发点。\n根据上述实验及分析，本文认为字词联合训练通过字对词的平滑，将更多\n的词和上下文建立起了联系。这种方式在保留分布假说的基础上，可以使用更 丰富的上下文信息，也得到了更有效的词表示，在词义相似度任务以及文本分 类任务中，相比此前的方法均有一定的提升。\n4.5.4 上下文加入字的影响\n在本文提出的字词联合训练模型中，将词引入到字的上下文空间中，前面\n的实验已经说明，这种方式无论对于字表示还是词表示均有一定的提升。\n对应于将词引入到字的上下文空间中，本文也同时尝试了将字引入到词的\n上下文空间中。具体方法为，通过将上下文各词拆分成字，将上下文词以及对 应的字全部作为目标词的上下文。通过这种改造之后，同样使用分词和词义相 似度任务进行评价。结果列于表 4-5。表中，分词使用 F值作为评价指标，ws240 和 ws297使用皮尔逊相关系数作为指标。\n从实验结果中看，加入字作为词的上下文，对分词实验有小幅提升；然而\n对于词的语义相似度任务，性能有非常严重的下降。为了更直观地说明造成这 一现象的原因，这里同样展示若干词的最近邻，列于表 4-6。\n70 基于神经网络的词和文档语义向量表示方法研究\n模型 分词 ws240 ws297 Skip-gram字向量 93.48 - - Skip-gram词向量 - 43.51 52.53 字词联合训练 93.65 46.68 54.45 字词联合训练 +字上下文 93.69 37.11 43.16\n表 4-5 字词联合训练时，加入字作为词的上下文的实验结果（×100）\n模型 Skip-gram 字词联合训练 字词联合训练 +字上下文\n江\n泾 浙 浙 江口 奉化 陕 青弋 江阴 泗 西江 宁海 潭 泗 太仓 湘\n星期天\n周日 周日 周日 周末 星期六 热天 星期四 星期二 月姬 礼拜天 星期日 周末 星期六 礼拜天 周六\n表 4-6 字词联合训练中加入字作为词的上下文，得到的最近邻对比表\n由于在词的上下文中加入字之后，在字、词评价任务上有不同的结论，因\n此这里分别从字和词两个角度进行分析。对于字“江”，加入字作为上下文的模 型得到的最近邻，均为含义上比较相关的汉字；而 Skip-gram 模型得到的最近 邻主要是一些江的名字；字词联合训练得到的最近邻主要是浙江和江苏的城市 名。单独从最近邻的相关性上分析，这三个模型从不同的角度描述了“江”所 包含的意思，但是对于分词任务而言，由于字是其中最主要的特征，一份准确 刻画不同字之间的关系的字向量，应当对于分词有促进作用。对于词“星期天”， 加入了字作为上下文的模型，得到的最近邻里出现了一些语义上不太相关的词， 如“热天”。本文认为这是因为字本身并没有明确的语义，一个字在不同的词中 很可能有完全不同的意思。因此，在上下文中融入了汉字，反而让上下文分布 变得更模糊，从而导致弱化了词的语义。\n综上所述，本文认为在上下文中加入汉字，可以直接建立起汉字之间的关\n第四章 基于字词联合训练的中文表示及应用 71\n系，对于生成的字向量有一定的提升，通过分词任务评价，也可以看出相比单 纯的字词联合训练有小幅提升。但是当上下文中混入汉字之后，由于这些字并 不直接拥有明确的语义信息，反而干扰了词的上下文分布，对词表示起到了负 面作用。因此，在使用字表示时，可以考虑在词中加入字作为上下文，而在使 用词表示时，只使用字词联合训练模型则更合适。\n4.6 本章小结\n本章提出字词联合训练方法，在字的上下文中引入了词，使用词的语义空\n间对字进行建模，获得更好的字表示。相比直接沿用英文词向量的训练方法，字 词联合训练在中文分词任务上，性能增益率有 35个百分点的提升。另一方面， 由于在字词联合训练中，增加了对汉字的建模，这些汉字建立了一些词之间的 关系，使得词的上下文更丰富，提升了词表示的语义。在中文词义相似度任务 中，字词联合训练比现有两种借助汉字提升词表示语义的模型更为有效。在文 本分类任务中，使用字词联合训练得到的词向量在用做特征以及神经网络初始 值时，相比此前方法也有一定的提升。此外，实验结果还指出，大规模少量噪 声的语料相比小规模几乎无噪声的语料依然有较大的优势，因此本文提出的字 词联合训练方法，有潜力在更大规模的语料上发挥出自己的优势。\n第五章 基于循环卷积网络的文档表示及应用\n在文本分类、信息检索等实际任务中，不仅需要词级别的语义表示，更需\n要句子和文档级别的语义表示。本章首先总结了现有神经网络模型对文档的表 示方法，然后提出了基于循环卷积网络的文档表示方法。该方法可以在线性时 间复杂度下，对不同距离的上下文进行自适应建模，得到有效的文本表示，并 在文本分类任务上，相对现有方法有显著的提升。\n5.1 引言\n文本分类在网页检索、信息筛选、情感分析等任务中是一个至关重要的步\n骤 [2]。文本分类中的关键问题在于文本表示，在传统机器学习方法中，通常以 特征表示的形式出现。文本分类中最常用的特征表示方法是词袋子模型。词袋 子模型中，最常用的特征是词、二元词组、多元词组（n-gram）以及一些人工抽 取的模板特征。在以特征的形式表示文本之后，传统模型往往使用词频、互信 息 [19]、pLSA [13]、LDA [38]等方法筛选出最有效的特征。然而，传统方法在 表示文本时，会忽略上下文信息，同时也会丢失词序信息。比如以下例子：\nA sunset stroll along the South Bank affords an array of stunning vantage points.\n分析句子中的“Bank”一词，如果孤立地看这个词，我们并不知道这是表示“河 岸”还是“银行”，这时就需要通过上下文对词义进行消歧。当纳入“Bank”的 前一个词，看二元词组“South Bank”时，可以发现两个单词的首字母都是大写 的，这对于不了解伦敦的人来说，很可能会以为是“南方银行”。当看了足够的 上下文“stroll along the South Bank”，我们才可以肯定这里说的是南岸，和银行 无关。尽管传统特征中诸如多元词组以及更复杂的特征（如树核 [94]）也能捕 获词序信息，但是这些特征往往会遇到数据稀疏问题，影响到文本分类的精度。\n近年来，预训练词向量以及深度神经网络模型为自然语言处理带来了新的\n思路。本文的第二章和第三章已经介绍了词向量可以从无标注的文本中自动学 习得到语义和语法信息，并且能对各项自然语言处理任务带来性能上显著的提 升。在词向量的帮助下，有人提出一些组合语义的方法来表示文本的语义。\n74 基于神经网络的词和文档语义向量表示方法研究\nSocher等人在 2011年 [105, 109]、2013年 [110]发表了递归神经网络（Recursive Neural Network）的相关工作。该方法被证实在构建句子级语义时较为有 效。然而，递归神经网络需要按照一个树形结构来构建句子的语义，其性能依 赖于构建文本树的精度。而且，构建这棵树需要至少 O(n2)的时间复杂度，其 中 n表示句子的长度。当模型在处理长句子或者文档时，所花费的时间往往是 不可接受的。更进一步地，在做文档表示时，两个句子之间的关系不一定能构 成树形结构。因此递归神经网络可能不适合构建长句子或者文档的语义。\n循环神经网络（Recurrent Neural Network）可以在 O(n)时间内构建文本的 语义 [26]。该模型逐词处理整个文档，并把所有上文的语义保存到一个固定大 小的隐藏层中。循环神经网络的优势在于它可以更好地捕捉上下文信息，对长 距离的上下文信息进行建模。然而，循环神经网络是一个有偏的模型，如对于 正向的循环神经网络而言，文本中靠后的词相对靠前的词占据了更主导的地位。 由于这一语义偏置的特性，循环神经网络在构建整个文本的语义时，会更多地 包含文本后面部分的信息。但是实际上并非所有文本的重点都放在最后，这可 能会影响其生成的语义表示的精确度。\n为了解决语义偏置的问题，有人提出用卷积神经网络（Convolutional Neural Network）来构建文本语义 [18]。卷积神经网络利用最大池化技术能从文本中找 出最有用的文本片段，其复杂度也是 O(n)。因此卷积神经网络在构建文本语义 时有更大的潜力。然而，现有卷积神经网络的模型总是使用比较简单的卷积核， 如固定窗口 [18, 46]。在使用这类模型时，如何确定窗口大小是一个关键问题。 当窗口太小时，可能导致上下文信息保留不足，难以对词进行精确刻画；而当 窗口太大时，会导致参数过多，增加模型优化难度。因此，需要考虑，如何构 建模型，才能更好地捕获上下文信息，减少选择窗口大小带来的困难。并以此 为基础来更好地完成文本分类的任务。\n为了解决上述模型的缺陷，本文提出了循环卷积网络（Recurrent Convolutional Neural Network），并将其用到文本分类中。首先，本文使用一个双向循环 结构对上下文进行建模。对比基于窗口的上下文建模方法，循环结构的参数更 少，在引入较少噪声的前提下，可以捕获尽可能远的上下文信息，从而保留长 距离的词序信息。第二，本文使用最大池化技术自动判断对文本分类最重要的 特征。结合这两者，循环卷积网络同时拥有了循环神经网络和卷积神经网络的 优势，既能很好地刻画上下文信息，又能无偏地描述整个文本的内容，并且其\n第五章 基于循环卷积网络的文档表示及应用 75\n复杂度仅为 O(n)。\n本文在四个不同的数据集上，对比了循环卷积网络与此前最好方法的性能。\n这四个数据集包含了中英文的文本分类任务，分类体系包括学科分类、作者母 语分类和情感分类。实验结果表明，本文的方法相对此前的方法有明显的优势。\n5.2 相关工作\n本节首先介绍现有神经网络文档向量表示技术，这类技术均均基于组合语\n义的思路，从词的语义组合得到文档的语义。5.2.1小节介绍了组合语义的思想及 起源，5.2.2小节到 5.2.4小节介绍了三种现有的文档向量表示方法。最后 5.2.5小 节简述了传统文本分类方法。\n5.2.1 组合语义\n在获取句子和文档的语义表示时，很容易想到直接沿用词的分布假说，对\n文档进行建模。然而，如果采用分布假说直接生成句子或者文档的向量表示，会 遇到极大的数据稀疏问题。Koehn在 2005年发表的文献 [53]中曾对 Europarl语 料进行了统计，结果表明，在该语料中，总共有 47,889,787个词，其中有 304,786 个不同的词，出现至少 10次的词一共有 58,552个；而语料中一共有 1,920,209 个句子，其中不同的句子有 1,860,118个，至少出现 10次的句子更是只有 597 句。因此，如果将句子看成一个整体，用词向量模型来训练句子的表示，由于 绝大多数句子因为只出现过一次，训练的结果将毫无统计意义。另一方面，分 布假说是针对词义的假说，这种通过上下文获取语义的方式对句子和文档是否 有效，还有待讨论。因此需要寻求新的思路对句子和文档进行建模。\n德国数学家弗雷格（Gottlob Frege）在 1892年就曾提出：一段话的语义由其 各组成部分的语义以及它们之间的组合方法所确定 [30]。现有的句子或者文档 表示也通常以该思路为基础，通过语义组合的方式获得。常用的组合语义组合 函数，如线性加权、矩阵乘法、张量乘法等，在 Hermann的文献 [36]中有详细 的总结。近年来基于神经网络的语义组合技术为文档表示带来了新的思路。从 神经网络的结构上看，主要可以分为三种方式：递归神经网络（5.2.2小节）、循 环神经网络（5.2.3小节）和卷积神经网络（5.2.4小节）。\n76 基于神经网络的词和文档语义向量表示方法研究\n5.2.2 递归神经网络\n递归神经网络（Recursive Neural Network）的结构如图 5-1，其核心为通过 一个树形结构，从词开始逐步合成各短语的语义，最后得到整句话的语义。\nw1 x1 w2 x2\nw3\nx3 w4 x4\nw5\nx5\ny1 = g(x1,x2) y2 = g(x4,x5)\ny3 = g(y1,x3)\ny4 = g(y3,y2)\n图 5-1 递归神经网络模型结构图\n递归神经网络使用的树形结构一般为二叉树，在某些特殊情况下（如依存\n句法分析树 [107]）也使用多叉树。本文主要从树的构建方式和子节点到父节点 的组合函数，这两方面介绍介绍递归神经网络。 树形结构有两种方式生成：一、使用句法分析器构建句法树 [105, 110]；二、 使用贪心方法选择重建误差最小的相邻子树，逐层合并 [109]。这两种方法各有 优劣，使用句法分析器的方法可以保证生成的树形结构是一棵句法树，树中各 个节点均对应句子中的短语，通过网络合并生成的各个节点的语义表示也对应 各短语的语义。使用贪心方法构建树形结构则可以通过自动挖掘大量数据中的 规律，无监督地完成这一过程，但是树中的各个节点不能保证有实际的句法成 分。\n子节点到父节点的组合函数 y = f(a, b)主要有三种：\n一、句法组合。这种方式下，子节点的表示为向量 a, b，父节点可以通过矩\n阵运算得到：\ny = ϕ (H [a; b]) (5.1)\n其中 ϕ为非线性的激活函数，权重矩阵 H 可能固定 [108]，也可能根据子树对 应的句法结构不同，而选用不同的矩阵 [103]。该方法一般用于句法分析中。\n第五章 基于循环卷积网络的文档表示及应用 77\n二、矩阵向量法 [78]。在这种表示下，每个节点由两部分表示组成，一个矩 阵和一个向量，对于 A,a子节点和 B, b子节点，其组合函数为：\ny = ϕ (H [Ba;Ab]) (5.2)\nY = WM\n[ A\nB\n] (5.3)\n其中 WM ∈ R|a|×2|a|，保证父节点对应的语义变换矩阵 Y ∈ R|a|×|a|，与叶节点 的 A、B 矩阵维度一致。使用这种方法，每个词均有一个语义变换矩阵，对于 否定词等对句法结构另一部分有类似影响的词而言，普通的句法组合方式没法 很好地对其建模，而这种矩阵向量表示则可以解决这一问题。Socher等人将该 方法用于关系分类中 [106]。\n三、张量组合。张量组合方式使用张量中的每一个矩阵，将子节点组合生\n成父节点表示中的一维。\ny = ϕ ( [a; b]TW [1:d] [a; b] +H [a; b] ) (5.4)\n其中W [1:d]表示张量W 中的第 1到 d个切片矩阵。不同的切片用于生成父节点 y中不同的维度。该方法是句法组合方法的泛化形式，有更强的语义组合能力， Socher等人将其用于情感分析任务中 [104]。\n递归神经网络在构建文本表示时，其精度依赖于文本树的精度。无论使用\n哪种构建方式，哪种组合函数，构建文本树均需要至少O(n2)的时间复杂度，其 中 n表示句子的长度。当模型在处理长句子或者文档时，所花费的时间往往是 不可接受的。更进一步地，在做文档表示时，两个句子之间的关系不一定能构 成树形结构。因此递归神经网络在大量句子级任务中表现出色，但可能不适合 构建长句子或者文档级别的语义。\n5.2.3 循环神经网络\n循环神经网络（Recurrent Neural Network）由 Elman等人在 1990年首次提 出 [26]。该模型的核心是通过循环方式逐个输入文本中的各个词，并维护一个 隐藏层，保留所有的上文信息。\n循环神经网络是递归神经网络的一个特例，可以认为它对应的是一棵任何\n78 基于神经网络的词和文档语义向量表示方法研究\nh(i)\nh(i− 1) wi e(wi)\n(a)循环结构 w1\nw2\nw3\nh(0)\nh(1)\nh(2)\nh(3)\n(b)展开循环的结构\n图 5-2 循环神经网络模型结构图\n一个非叶结点的右子树均为叶结点的树。这种特殊结构使得循环神经网络具有 两个特点：一、由于固定了网络结构，模型只需在 O(n)时间内即可构建文本的 语义。这使得循环神经网络可以更高效地对文本的语义进行建模。二、从网络结 构上看，循环神经网络的层数非常深，句子中有几个词，网络就有几层。因此， 使用传统方法训练循环神经网络时，会遇到梯度衰减或梯度爆炸的问题，这需 要模型使用更特别的方法来实现优化过程 [8, 42]。\n循环神经网络对文本语义的构建过程与 2.2.4 小节中介绍的循环神经网络 语言模型类似。每个词与代表所有上文的隐藏层组合成新的隐藏层（结构如图 5- 2a，算法如式 5.5），从文本的第一个词循环计算到最后一个词。当模型输入所 有的词之后，最后一个词对应的隐藏层代表了整个文本的语义。\nh(i) = ϕ (H [e(wi);h(i− 1)]) (5.5)\n优化方式上，循环神经网络与其它网络结构也略有差异。在普通的神经网\n络中，反向传播算法可以利用导数的链式法则直接推算得到。但是在循环神经网 络中，由于其隐藏层到下一个隐藏层的权重矩阵H 是复用的，直接对权重矩阵 求导非常困难。循环神经网络最朴素的优化方式为沿时间反向传播技术（Backpropagation Through Time，BPTT）。该方法首先将网络展开成图 5-2b的形式，对 于每一个标注样本，模型通过普通网络的反向传播技术对隐藏层逐个更新，并 反复更新其中的权重矩阵 H。由于梯度衰减的问题，使用 BPTT优化循环神经 网络时，只传播固定的层数（比如五层）。为了解决梯度衰减问题，Hochreiter和\n第五章 基于循环卷积网络的文档表示及应用 79\nSchmidhuber在 1997年提出了 LSTM（Long Short-Term Memory）模型 [43]。该 模型引入了记忆单元，可以保存长距离信息，是循环神经网络的一种常用的优 化方案。\n无论采用哪种优化方式，循环神经网络的语义都会偏向文本中靠后的词。因\n此，循环神经网络很少直接用来表示整个文本的语义。但由于其能有效表示上 下文信息，因此被广泛用于序列标注任务，如 2.2.4小节中提到的神经网络语言 模型。\n5.2.4 卷积神经网络\n卷积神经网络（Convolutional Neural Network）最早由 Fukushima在 1980年 提出 [31]，此后，LeCun等人对其做了重要改进 [61]。\nw1\nw2\nw3\nw4\nh (1) 1\nh (1) 2\nh (1) 3\nh (1) 4\nh(2)\n图 5-3 卷积神经网络模型结构图\n卷积神经网络的结构如图 5-3，其核心是局部感知和权值共享。在一般的前 馈神经网络中，隐藏层的每个节点都与输入层的各个节点有全连接；而在卷积 神经网络中，隐藏层的每个节点只与输入层的一个固定大小的区域（win个词， 对应图中 win = 3）有连接。从固定区域到隐藏层的这个子网络，对于输入层的 所有区域是权值共享的。输入层到隐藏层的公式，形式化为：\nxi = [e(wi−⌊win/2⌋); . . . ; e(wi); . . . ; e(wi+⌊win/2⌋)] (5.6)\nh (1) i = tanh (Wxi + b) (5.7)\n80 基于神经网络的词和文档语义向量表示方法研究\n在得到若干个隐藏层之后，卷积神经网络通常会采用池化技术，将不定长\n度的隐藏层压缩到固定长度的隐藏层中。常用的有均值池化和最大池化 [18]。最 大池化的公式为：\nh(2) = nmax\ni=1 h\n(1) i (5.8)\n卷积神经网络通过其卷积核，可以对文本中的每个部分的局部信息进行建\n模；通过其池化层，可以从各个局部信息中整合出全文语义，模型的整体复杂 度为 O(n)。 卷积神经网络应用非常广泛。在自然语言领域，Collobert等人首次将其用 于处理语义角色标注任务，有效提升了系统的性能 [18]。2014年，Kalchbrenner 等人与 Kim分别发表了利用卷积神经网络做文本分类的论文 [47, 48]。Zeng等 人提出使用卷积神经网络做关系分类任务，取得了一定的成功 [127]。\n5.2.5 文本分类\n传统文本分类方法主要着眼于三个问题：特征表示、特征筛选和合适机器\n学习算法的选择。\n特征表示方面，最常用的是词袋子特征，复杂一些的有词性标签、名词短\n语 [66]以及树核 [94]等特征。不同的特征从不同的角度对数据进行刻画，通常 需要通过各种特征的组合，才能更好地描述文本。然而这些特征都存在数据稀 疏问题。\n特征选择希望通过删除噪声特征来提高文本分类的性能。最常用的方法是\n删除停用词（比如“的”），更高级的方法包括利用信息增益、互信息 [19]等指 标来筛选特征。近年来，Ng等人提出的在优化目标中加入 L1 正则化 [85]，自 动学习出稀疏特征，在文本分类的大规模应用中起到了重要作用。\n在选择机器学习算法方面，几乎所有的分类器算法都在文本分类中有应用，\n如最近邻分类器，决策树分类器。在面对大规模文本分类任务时，高效的线性 分类器应用的最为广泛，如 Logistic回归（LR）、朴素贝叶斯（NB）和支持向量 机（SVM）。\n5.3 模型\n本文提出一个深度模型来构建文本的语义，图 5-4展示了该模型的网络结 构。网络的输入是文档 D，由词序列 w1, w2 . . . wn 组成。网络的输出节点数与\n第五章 基于循环卷积网络的文档表示及应用 81\n分类类别数相同。本文使用 P (k|D, θ)来表示文档属于类别 k的概率，其中 θ是 网络的参数。\n5.3.1 词表示\n本文将词和其上下文组合在一起，用来表示一个词。上下文可以帮助词进\n行消歧，从而获得更精确的语义。本文使用一个双向循环结构来捕捉上下文。\n定义 cl(wi) 为词 wi 的上文表示，cr(wi) 为词 wi 的下文表示。cl(wi) 和\ncr(wi) 均为稠密实数向量，向量的维度是 |c|。cl(wi) 的计算公式见式 5.9，其 中 e(wi−1) 是词 wi−1 的词向量。词向量也是一个低维实数向量，维度为 |e|。 cl(wi−1) 是词 wi−1 的上文表示。所有文档的第一个词的上文表示均使用同样 的参数 cl(w1)。W (l) 是一个矩阵，用于把表示上文的隐藏层转移到下一个词的 上文表示中。W (sl)也是矩阵，用于将当前词的语义合成到下一个词的上文表示 中。ϕ是非线性激活函数。wi的下文表示 cr(wi)也有相同的计算方式，具体如 式 5.10。同样地，所有文档的最后一个词的下文表示也使用同样的参数 cr(wn)。\ncl(wi) = ϕ(W (l)cl(wi−1) +W (sl)e(wi−1)) (5.9)\ncr(wi) = ϕ(W (r)cr(wi+1) +W (sr)e(wi+1)) (5.10)\n...\n...\nx3\nx4\nx5\nx6\nx7\ncl(w3)\ncl(w4)\ncl(w5)\ncl(w6)\ncl(w7)\ncr(w3)\ncr(w4)\ncr(w5)\ncr(w6)\ncr(w7)\ny (2) 3\ny (2) 4\ny (2) 5\ny (2) 6\ny (2) 7\ny(3)\n循环结构（卷积层） 最大池化层 输出层\n上文表示 下文表示词向量\nstroll\nalong\nthe\nSouth\nBank\n图 5-4 循环卷积网络模型结构图\n图 5-4为循环卷积网络的整体结构。图中展示了例句“A sunset stroll along the South Bank affords an array of stunning vantage points.”的中间部分。其中各元\n82 基于神经网络的词和文档语义向量表示方法研究\n素的下标表示对应各词在句子中的位置，如“Bank”是句子中的第 7个词，在 图中记为 w7。 根据公式 5.9、5.10所示，上下文向量可以分别捕捉到上文和下文的语义信 息。例如，在图 5-4中，cl(w7)包含了词“Bank”的上文“A sunset stroll along the South”以及之前的所有词所包含的信息。而 cr(w7)则包含了“Bank”的下 文“affords an array of stunning vantage points”所包含的信息。\n在得到词 wi 的上下文信息之后，本文利用这些上下文信息对其进行消歧。\n本文将词 wi的表示 xi定义为：词 wi的上文表示 cl(wi)、下文表示 cr(wi)以及 其词向量 e(wi)的拼接。\nxi = [cl(wi); e(wi); cr(wi)] (5.11)\n使用循环结构，只需要对文本进行一次正向（从左往右）扫描，就可以获\n得所有的上文表示 cl；同样地，只需要一次反向（从右往左）扫描，就可以获 得所有的下文表示 cr。因此，整个过程的复杂度是 O(n)。与此同时，使用这种 循环结构捕获到的上下文信息，会比使用窗口表示的上下文有更好的效果。原 因是，固定窗口的上下文表示方法只包含了部分上下文信息。\n当得到词 wi 对应的表示 xi 之后，本文对其作一个线性变换，再通过一个\n非线性的激活函数，将值送入神经网络的下一层。\ny (2) i = tanh ( W (2)xi + b (2) )\n(5.12)\n其中 y(2)i 是隐含语义向量，在下一步中，本文会对其进行进一步分析，找出对 文本分类最有用的隐含语义因子。\n5.3.2 文本表示\n本文使用卷积神经网络来构建整个文本的语义。从卷积网络的角度来看，\n上文介绍的循环结构可以看做卷积核。当得到所有的词表示之后，本文在此加 入一个最大池化层（max-pooling layer）。\ny(3) = nmax i=1 y (2) i (5.13)\n第五章 基于循环卷积网络的文档表示及应用 83\n这里，max操作是逐个元素计算的，也就是说，y(3)的第 k维就是各 y(2)i 向 量的第 k 维的最大值。借助最大池化层，可以将不同长度的文本转成固定长度 的向量，从而表示整个文本。除了最大池化层，还有均值池化层 [18]等其它池 化技术。本文选用最大池化技术的主要出发点是，对于文本分类而言，最具决 定性的词或者短语往往只有几处，而不是均匀散在文本各处。最大池化正好可 以找出其中最有判别力的语言片段。同时，最大池化层的复杂度也是 O(n)。循 环卷积网络中，循环结构和卷积结构是串联的，池化层使用的是循环结构的输 出，因此整个模型的复杂度也是 O(n)。\n模型的最后一部分是输出层，输出层定义如下。\ny(4) = W (4)y(3) + b(4) (5.14)\n最后，使用 softmax函数将输出值转为概率值。\nP (i|D, θ) = exp\n( y (4) i ) ∑n\nk=1 exp ( y (4) k ) (5.15) 5.3.3 模型训练\n训练模型参数\n网络中的所有参数 θ见表 5-1。\n训练目标为最大化以下似然，其中 D是训练文档集，classD是文档D的正 确分类：\nθ 7→ ∑ D∈D logP (classD|D, θ) (5.16)\n本文使用随机梯度下降法 [10]来优化上述训练目标。每次迭代，随机选取一个 样本 (D, classD)，使用下式进行一次梯度迭代。式中，α是学习速率。\nθ ← θ + α∂ logP (classD|D, θ) ∂θ\n(5.17)\n对于循环结构的优化本文采用沿时间反向传播技术（backpropagation through time），该技术在本文第二章第 5.2.3小节中介绍循环神经网络时已经提到，这\n84 基于神经网络的词和文档语义向量表示方法研究\n参数符号 维度 含义\ne(V) |e| × |V| 所有词向量 b(2) H 隐藏层偏置项 b(4) O 输出层偏置项\ncl(w1) |c| 初始上文参数 cr(wn) |c| 初始下文参数 W (2) H × (|e|+ 2|c|) 隐藏层转移矩阵 W (4) O ×H 输出层转移矩阵 W (l) |c| × |c| 上文语义转移矩阵 W (r) |c| × |c| 下文语义转移矩阵 W (sl) |e| × |c| 当前词融入上文语义转移矩阵 W (sr) |e| × |c| 当前词融入下文语义转移矩阵\n表 5-1 循环卷积网络参数列表\n里不再赘述。 在训练中，本文参考了 Plaut和 Hinton的建议 [93]，使用了一个神经网络训 练中常用的优化技巧。所有的参数在初始化时均使用均匀分布，其中随机数的 最大绝对值为该元素入节点个数的平方根。入节点个数也就是神经网络中上一 层的节点个数。对应的学习速率同样也除以入节点个数。\n词向量预训练\n词向量是一种词的分布表示，这种表示更适合作为神经网络的输入。最近\n的研究 [27, 40]表明，如果选择一个好的初始值，神经网络可以收敛到更好的局 部最优解。本章根据第三章中得到的经验，选取了适合本任务而且效率较高的 Skip-gram模型，来优化词向量。Skip-gram模型的介绍可见第二章第 2.2.6小节。\n5.4 实验设计\n为了证明循环卷积网络的有效性，本文在四个不同种类的数据集上做对比\n实验。这些数据集包含了中文和英文两种语言，分类体系包括学科分类、文档 作者母语分类和情感分类，文本长度包括句子级和篇章级。四个数据集的统计 信息见表 5-2。 下文 5.4.1小节详细描述了实验设置，包括数据预处理、参数设置等；5.4.2小 节描述了所有实验中均用到的对比模型。\n第五章 基于循环卷积网络的文档表示及应用 85\n数据集 类别数 训练/验证/测试集划分 平均文档长度 语言 20Newsgroups 4 7520/836/5563 429 英文 复旦文本分类 20 8823/981/9832 2981 中文 ACL论文集 5 146257/28565/28157 25 英文 斯坦福情感树库 5 8544/1101/2210 19 英文\n表 5-2 文本分类数据集概要信息\n5.4.1 实验设置\n对于英文数据集，本文使用 Stanford Tokenizer1对语料进行分词。对于中文 数据集，使用 ICTCLAS2分词。文本中的停用词和特殊符号均当作普通单词保 留。这四个数据集均已分好训练集和测试集，其中 ACL论文集和斯坦福情感树 库已经分好了训练、验证、测试集。对于另两个数据集，本文随机选取训练集 的 10%作为验证集，剩下部分作为真实的训练集。\n网络超参数的选择一般需要根据数据的不同而有所调节。在实验中，本文\n参考了 Collobert等人 [18]和 Turian等人 [116]的工作，只使用了一组最常用的 超参数。具体来说，学习速率 α = 0.01，隐藏层大小 H = 100，词向量维度 |e| = 50，上下文向量维度 |c| = 50。中英文的词向量使用中英文维基百科进行 训练，训练工具包为 word2vec3。公式 5.9和公式 5.10中的激活函数 ϕ，本文使 用双曲正切函数 tanh。如果使用更适合深层网络的 ReLU激活函数 [32]能对系 统性能有小幅度的提升，但由于对比方法同样使用 tanh作为激活函数，本实验 中依然选择使用 tanh。\n5.4.2 对比方法\n词袋子、二元词组 + Logistic回归、支持向量机\nWang和 Manning在 2012年发表的文章 [120]中展示了若干种实现简便的 文本分类基准实验。这些实验中主要使用词和二元词组作为特征，并用常规分 类器算法进行分类。本文在实验中选择了词以及二元词组作为特征，特征的权 重为各词在文档中的词频。同时，本文选择 Logistic回归和支持向量机作为分\n1http://nlp.stanford.edu/software/tokenizer.shtml 2http://ictclas.nlpir.org 3http://code.google.com/p/word2vec\n86 基于神经网络的词和文档语义向量表示方法研究\n类器，具体使用了 LIBLINEAR工具包4，并使用其默认参数作为分类器的设置。\n平均词向量 + Logistic回归\n该方法使用文档中各词词向量的加权平均值作为文档的表示，然后使用Logistic回归分类器对其进行分类。其中各个词的权重为词在文档中的词频。该方 法可能是利用词向量表示文本的一种最简单的方法，通常基于神经网络的分类 模型均会选用该方法作对比 [58, 110]。\n作为一种文本表示，平均词向量除了用在单语文本分类中，还出现在其它\n场景下，如 Huang等人在其模型中，使用平均词向量作为文档的全局表示 [44]， 辅助提升词向量的语义。Klementiev等人也使用平均词向量作为文本表示，并 基于此进行跨语言的文本分类 [50]。\n卷积神经网络\nCollobert等人在 2011年将卷积神经网络引入自然语言处理中。尽管他们使 用卷积神经网络做的是语义角色标注任务，但是该模型可以非常容易地移植到 文本分类任务中。只需要将其输出层修改为和循环卷积网络中一样的分类器即 可。卷积神经网络使用固定的窗口作为上下文表示，对比循环卷积网络，其卷 积核是窗口中若干词的拼接，也就是将循环卷积网络模型中对词 wi 的表示 xi 替换为：\nxi = [e(wi−⌊win/2⌋); . . . ; e(wi); . . . ; e(wi+⌊win/2⌋)]\n与本文同期，也有若干其它工作使用卷积神经网络直接做文本分类任务，\n大体思想与上述模型类似，如文献 [47, 48, 59]。\n其他对比方法\n会在具体实验中介绍。\n5.5 实验及分析\n本节前四小节详细介绍了各数据集上的实验结果，并对各个实验进行深入\n分析。最后 5.5.5小节总结了所有的实验。 4http://www.csie.ntu.edu.tw/~cjlin/liblinear/\n第五章 基于循环卷积网络的文档表示及应用 87\n5.5.1 20Newsgroups5\n20Newsgroups数据集包括了 20个新闻组的共约 2万封邮件，是目前使用 最广泛的英文文本分类数据集之一。该数据集有若干不同的版本，本文选用了 bydate版本，因为该版本已经将数据集分成了训练集和测试集，方便和现有工 作做对比。\n本文按照 Hingmire等人的工作 [38]将数据集划分成 comp、politics、rec和 religion一共四个大类，各类别分别包括：\n• comp：comp.os.ms-windows.misc、comp.graphics、comp.sys.mac.hardware、 comp.windows.x、comp.sys.ibm.pc.hardware\n• politics：talk.politics.misc、talk.politics.guns、talk.politics.mideast\n• rec：rec.autos、rec.motorcycles、rec.sport.baseball、rec.sport.hockey\n• religion：talk.religion.misc、alt.atheism、soc.religion.christian\n20Newsgroups数据集是一个类别不均衡的文本分类数据集，即使按照上述方法 分成四个大类，各类别样本数仍然大约呈 6:3:5:3的比例。\n该数据集在这种划分下的最佳结果由文献 [38]所得，文中提出了一种结合 了 LDA、EM算法和朴素贝叶斯分类器的文本分类模型，并命名为 ClassifyLDAEM。本文按照这项工作的设置，使用宏平均 F1值（Macro F-measure）评价各 模型在 20Newsgroups数据集上的表现。宏平均 F1值定义为各分类 F1值的宏平 均数。\n表 5-3展示了各模型在 20Newsgroups数据集上的表现。其中表格最上面一 部分区域表示传统文本分类算法的效果，中间部分为此前该数据集上最好的模 型 ClassifyLDA-EM 的表现，下面部分分别为本文实现的卷积神经网络和循环 卷积网络的性能。根据实验结果，可以得到下述结论：\n一、无论是卷积神经网络还是循环卷积网络，它们都比传统基于分类器的\n方法要有明显的优势。基于词特征及二元词组特征的传统分类方法，最多只有 93.12%的性能，相比而言，循环卷积网络达到了 96.49%的性能，分类误差减\n5http://qwone.com/~jason/20Newsgroups/\n88 基于神经网络的词和文档语义向量表示方法研究\n模型 宏平均 F1 词袋子 + Logistic回归 92.81 二元词组 + Logistic回归 93.12 词袋子 +支持向量机 92.43 二元词组 +支持向量机 92.32 平均词向量 + Logistic回归 89.39 ClassifyLDA-EM [38] 93.60 卷积神经网络 94.79 循环卷积网络 96.49\n表 5-3 各模型在 20Newsgroups数据集上的表现\n少了 49%。此前最好的方法 ClassifyLDA-EM也只达到了 93.6%的性能，循环 卷积网络在其基础上降低了 45%的分类错误。\n二、对比卷积神经网络和循环卷积网络，可以发现循环卷积网络在文本分\n类性能上有明显的优势。循环卷积网络相对卷积神经网络的分类误差，减少了 33%。对此，本文设计了一个实验，更仔细地对比分析这两个模型。\n对比循环卷积网络和卷积神经网络\n卷积神经网络和循环卷积网络最主要的不同点在于对上下文的表示。卷积\n网络使用一个固定大小的窗口来表示上下文信息，而循环卷积网络使用循环结 构来构建任意距离的上下文信息。卷积网络的性能受到窗口大小的影响，窗口 太小会丢失一些长距离的关系；窗口太大会引入数据稀疏问题，而且参数过多 会使得模型更难训练。 为了更好地分析卷积神经网络的性能，本文尝试了从 1到 19之间的所有奇 数大小的窗口，作为卷积网络中上下文信息的数据源。比如说，窗口为 1时，xi = [e(wi)]，词的表示就是词向量。当窗口为 3时，xi = [e(wi−1); e(wi); e(wi+1)]，也 就是说，词表示为当前词、前一个词以及下一个词的词向量的组合。 图 5-5展示了在 20Newsgroups数据集上，卷积神经网络在不同大小的窗口 下，在测试集上取得的评估结果。图中的横坐标表示卷积神经网络选择的不同 窗口大小，纵坐标为模型在测试集上的性能，以宏平均 F1值为指标。可以看到， 卷积网络（蓝线）和设想的一样，随着窗口的变大，测试效果先变好，再变差。 当窗口大小取 11个词时（前五个词，后五个词），模型的性能达到最佳状态。而\n第五章 基于循环卷积网络的文档表示及应用 89\n93.26\n94.28 94.69\n94.75\n94.76\n94.79\n94.76 94.50\n94.45\n94.42\n93.6\n96.49\n93\n94\n95\n96\n97\n1 3 5 7 9 11 13 15 17 19\n模 型 性 能 （ 宏 平 均 F\n1）\nCNN ClassifyLDA-EM RCNN 上下文窗口大小（词）\n图 5-5 窗口大小对卷积神经网络等模型的性能影响\n循环卷积网络（绿线）的性能不依赖于窗口大小，比卷积神经网络选最佳窗口 大小时的效果更好。\n由于在实验中，本文使用完全相同的参数和实现方法，卷积神经网络与循\n环卷积网络的仅有差别在于对上下文的表示，因此本文认为循环卷积网络对于 卷积神经网络的优势来自于循环卷积网络中的循环结构可以保留更长的上下文 信息；同时相比大窗口的卷积神经网络，循环卷积网络也会引入更少的噪声。\n5.5.2 复旦文本分类6\n复旦文本分类数据集由复旦大学李荣陆 [134] 提供。该数据集是一个中文 文本分类语料，其中包括艺术、教育、能源等一共 20个类别。复旦文本分类数 据集同样也是一个类别分布不均衡的数据集，其中较大的类别，如经济类，有 3200篇文档；而较小的类别，如文学类，只有 70篇文档。数据集中训练样本与 测试样本大致呈 1:1的比例。\n该数据集上此前的最佳结果由李文波等人的 Labeled-LDA模型 [133]所得。\n表 5-4展示了各模型在复旦文本分类数据集上的表现。与 20Newsgroups的 实验结果类似，循环卷积网络和卷积神经网络比其它对比模型有明显的优势。 同时，在该数据集上循环卷积网络比卷积神经网络的分类误差少 19%。\n6http://www.datatang.com/data/44139，http://www.datatang.com/data/43543\n90 基于神经网络的词和文档语义向量表示方法研究\n模型 准确率 词袋子 + Logistic回归 92.08 二元词组 + Logistic回归 92.97 词袋子 +支持向量机 93.02 二元词组 +支持向量机 93.03 平均词向量 + Logistic回归 86.89 Labeled-LDA [133] 90.80 卷积神经网络 94.04 循环卷积网络 95.20\n表 5-4 各模型在复旦文本分类数据集上的表现\n通过这个实验，可以发现循环卷积网络对于中文的文本表示依然有效。对\n于传统基于特征的文本分类方法而言，特征抽取是至关重要的一个步骤。相比 而言，英文有较多较好的文本特征抽取工具，而中文的文本特征抽取目前尚无 法达到英文的水准，比如英文句法分析工具的性能一般比中文句法分析工具的 性能好 10个百分点。\n本文提出的循环卷积网络不依赖于除了分词工具外的自然语言处理工具，\n直接从词出发，构建文本的语义表示，并且分类效果较传统方法有较大的优势。 因此利用循环卷积网络做文本表示以及完成文本分类任务，对于处理缺乏自然 语言处理工具的语种，是一种值得考虑的方法。\n5.5.3 ACL论文集7\nACL 论文集（ACL Anthology Network）[96] 包含了国际计算语言学协会 （ACL，The Association for Computational Linguistics）旗下的多个会议和期刊的 论文。Post和 Bergsma对论文集中从 2001年到 2009年的论文进行了作者母语 标注。标注过程主要依据作者的国籍，标注了五种最常见的作者母语：英语、日\n语、德语、汉语和法语 [94]。为了保证数据集的有效性，Post等人抛弃了模棱两 可的文章，只保留有把握的部分，一共对于 8483篇论文中的 1959篇进行了标 注。 为了充分调研模型在不同长度文本下的性能，本文按照文献 [94]中的实验 设置，对各篇论文中的每个句子进行分类。在这个分类数据集上，目前最佳的\n7http://old-site.clsp.jhu.edu/~sbergsma/Stylo/\n第五章 基于循环卷积网络的文档表示及应用 91\n结果为 Post等人 [94]所得，他们在实验中对比分析了若干种不同的树核（Tree kernel）特征。本文在对比实验中列举了其中最有代表性的两种特征，分别为： 一、使用 Berkeley parser [92]抽取得到的深度为 1的上下文无关文法（CFG）的 个数，作为特征集合；二、由 Charniak和 Johnson提出的，对上述特征重排序 得到的特征集合（C&J）[14]。\n模型 准确率 词袋子 + Logistic回归 46.67 二元词组 + Logistic回归 47.00 词袋子 +支持向量机 45.24 二元词组 +支持向量机 46.14 平均词向量 + Logistic回归 41.32 CFG [94] 39.20 C&J [94] 49.20 卷积神经网络 47.47 循环卷积网络 49.19\n表 5-5 各模型在 ACL论文集数据集上的表现\n表 5-5展示了各模型在ACL论文集数据集上的表现。与前两个数据集类似， 卷积神经网络和循环卷积网络依然有较好的效果。 在该数据集上，两个基于树核特征的方法，分别获得了 39.2%和 49.2%的 性能。由此可见，人工设计特征达到的性能，对特征的依赖程度非常大。对比 循环卷积网络和这两个人工精心设计的特征，实验表明本文的方法比直接使用 CFG特征有显著的优势，并且和 C&J特征有相似的效果。本文认为循环卷积网 络有这样的效果是因为它能捕获长距离的上下文信息。尽管树核的特征也能捕 获长距离的句法依赖关系，但是循环卷积网络不需要人工设计特征就能达到相 似的效果。对于一些缺乏句法分析工具的语言，循环卷积网络会有较大的优势。\n5.5.4 斯坦福情感树库8\n斯坦福情感树库（Stanford Sentiment Treebank）由 Socher等人标注并发布。 2005年，Pang和 Lee抓取了 IMDB网站9上的若干条电影评论数据，得到了电\n8http://nlp.stanford.edu/sentiment/ 9http://www.imdb.com/\n92 基于神经网络的词和文档语义向量表示方法研究\n影评论数据集 [87]。2013年，Socher等人 [110]在电影评论数据集的基础上，使 用 Stanford Parser [49]对其中的每个句子进行句法分析，并对得到的每个句法子 树所对应的短语进行情感倾向打分，每个短语由三位标注者打分，并取其平均 值作为标注的短语情感。 该数据集一共包括 11855条电影评论，和此前最好的工作 [110]一致，本文 也使用五分类（非常正面、正面、中立、负面、非常负面）标签体系。在文献 [110]中，Socher等人利用了对每条评论中所有短语的标注信息，训练了递归神 经网络。然而在训练循环卷积网络时，只使用了对于整个句子的情感倾向标注， 而没有使用其中对于短语和子句的标注。 与本文同期，另有一项基于卷积网络的文本分类工作发表 [48]，下文会针 对这些方法进行详细的比较。\n模型 准确率 词袋子 + Logistic回归 40.86 二元词组 + Logistic回归 36.24 词袋子 +支持向量机 40.70 二元词组 +支持向量机 36.61 平均词向量 + Logistic回归 32.70 递归神经网络 [109] 43.20 递归张量网络 [110] 45.70 卷积神经网络（Kim）[48] 48.00 卷积神经网络 46.35 循环卷积网络 47.21\n表 5-6 各模型在斯坦福情感树库数据集上的表现\n表 5-6展示了各模型在斯坦福情感树库数据集上的表现。与前三个数据集 类似，卷积神经网络和循环卷积网络依然有较好的效果。 实验结果中，文献 [48]与本文实现的卷积神经网络在模型结构上完全一致。 这两个同样的模型得到不同的结果主要是因为文献 [48]用了较多的神经网络优 化技巧，主要包括 Dropout[41, 121]、AdaDelta[126]等先进的归一化方法和优化 方法。这些最新提出的优化方法对所有的神经网络模型都有效果。但是由于本 文的主要对比模型递归神经网络 [109, 110]并没有使用这些先进的优化方法，为 了更明确地对比模型结构带来的性能提升，本文实现的模型中也没有使用文献\n第五章 基于循环卷积网络的文档表示及应用 93\n[48]所用的优化方法。 在这个实验中，可以发现卷积神经网络或者循环卷积网络在句子级分类任 务上，相对递归神经网络仍然有较大的提升。基于卷积的网络在表示文本语义 上相对此前基于递归结构的神经网络模型具有更大的优势。本文认为这是因为 卷积神经网络可以更好地捕获上下文信息，并从中选择出最重要的特征；而递 归网络需要按照树形结构构建文本语义，这依赖于所构建的树的精度。另一方 面，递归网络的时间复杂度是 O(n2)，而循环卷积网络只需要 O(n)的时间复杂 度。在实践中，递归张量网络按照文献 [110]中的报告，需要 3到 5小时的训练 时间。而循环卷积网络同样在斯坦福情感树库数据集上用单线程训练，只需要 若干分钟。\n关键短语\n正面情感 负面情感\n循环卷积网络 well worth the a wonderful movie even stinging at and invigorating film and ingenious entertainment and enjoy . ’s sweetest movie\nA dreadful live-action Extremely boring . Extremely dumb . an awfully derivative ’s weaker than incredibly dull . very bad sign\n递归张量网络 an amazing performance most visually stunning wonderful all-ages triumph a wonderful movie\nfor worst movie A lousy movie a complete failure very bad sign\n表 5-7 循环卷积网络与递归张量网络抽取的正负情感关键短语\n为了直观地观察循环卷积网络与递归张量网络在文档构建时的表现，本文\n在此列举了这两个模型抽取得到的若干“关键短语”。在循环卷积网络中，关键 短语是指模型在最大池化层提取出来次数最多的短语片段。由于在循环卷积网 络中，每个词实际上是带有整个文档的语义的，因此本文只列举了中心词附近 的多词短语。在递归张量网络中 [110]，关键的短语为情感最强烈的短语。 表 5-7列举了两个模型在训练后，从测试集中抽取的关键短语，这里仅列举 三元短语。其中递归张量网络抽取得到的关键短语直接引自文献 [110]。\n94 基于神经网络的词和文档语义向量表示方法研究\n从表中可以发现，循环卷积网络与递归张量网络会同时将一些短语判断为\n关键短语，如“a wonderful movie”、“very bad sign”。而对于大多数关键短语而 言，这两个模型抽取的结果是不同的。递归张量网络由于使用了句法分析器，对 句法树中的每一个节点进行标注，因此得到的关键短语均为句法树上的某个子 树。而循环卷积网络不依赖于句法分析器，得到的短语并不是普通句法意义上 的短语，而是一个中心词与其上下文的组合。\n根据循环卷积网络的特点，在分析其抽取得到的关键短语时，应该更多地\n关注其中心词，并且以上下文作为参考，确保其意义正确。从表中观察，对于 正面情感最重要的词是“worth”、“sweetest”、“wonderful”之类，而对于负面情 感最重要的词是“awfully”、“bad”、“boring”等，这与人的直观感受一致。因 此本文认为循环卷积网络不仅能有效地对文本进行建模，而且在建模过程中的 步骤也是有意义的。\n5.5.5 实验总结\n根据上述四个数据集上的结果，本文主要得出以下结论：\n• 对比神经网络方法（卷积、递归、循环卷积）和传统文本分类算法（词袋 子 +LR），实验结果表明神经网络方法在四个数据集上均有优势。这说明 神经网络可以有效地构建文本语义。相比传统词袋子的方法，神经网络模\n型可以更有效地对上下文进行建模，保留词序信息，同时减少了数据稀疏 问题。\n• 在斯坦福情感树库数据集上，本文对比了卷积网络、循环卷积网络和递归 网络，并发现基于卷积的网络（卷积网络与循环卷积网络）有更好的分类\n性能。本文认为卷积网络在表示文本语义上相对此前的递归神经网络模型 具有更大的优势。基于卷积的网络相对递归网络的主要优势在于：一、递 归神经网络需要按照树形结构构建文本语义，这依赖于所构建的树的精 度；而卷积网络直接从纯文本中对上下文建模，并从中选择出最重要的特 征，对上下文的建模更有效，同时也不依赖于其它的自然语言处理工具， 避免了误差传递。二、卷积网络可以在线性时间内得到文本的表示，而递 归网络需要句子长度的平方级时间，对于长句子而言，卷积网络会有更高 的效率。\n第五章 基于循环卷积网络的文档表示及应用 95\n• 在除了 ACL论文集以外的数据集中，循环卷积网络都达到了目前最好的 成绩。在 ACL论文集数据集中，循环卷积网络也取得了和此前最好几乎 一样的成绩。在 20Newsgroups和复旦文本分类数据集中，本文分别将错 误率降低了 33%和 19%。这说明循环卷积网络能有效地表示文本，并且 能在文本分类任务中体现出其优势。\n• 在 ACL数据集上，本文对比循环卷积网络和人工精心设计的特征。实验 表明循环卷积网络比直接使用 CFG特征有显著的优势，并且和 C&J特征 有相似的效果。本文认为循环卷积网络有这样的效果是因为它能捕获长距\n离的上下文信息。尽管树核的特征也能捕获长距离的句法依赖关系，但是 循环卷积网络不需要人工设计特征就能达到相似的效果。对于一些缺乏句 法分析工具的语言，循环卷积网络会有较大的优势。\n• 对比卷积网络和循环卷积网络，本文发现循环卷积网络在每个数据集上都 比基于窗口的卷积网络的效果要好。这说明循环结构相比窗口模式，可以\n更有效地对上下文进行建模。\n纵观四个数据集上的效果，本文认为利用循环卷积网络做文本表示，以及\n以此为基础进行文本分类，是一种有效的方法。该方法不依赖于现有的自然语 言处理工具，对文本的语言、分类体系不敏感，并且相比人工精心设计的特征 也有非常大的竞争力。\n5.6 本章小结\n本章提出使用循环卷积网络做文本表示，并用其完成了文本分类任务。该\n模型通过循环结构捕获上下文信息，并通过卷积网络构建整个文本的表示。相 比现有的递归神经网络，循环卷积网络不依赖于现有的自然语言处理工具，并 且在更小的时间复杂度内对文本进行建模。相比现有的卷积神经网络，循环卷 积网络可以捕获长距离的依赖关系，对上下文更有效地建模。实验结果表明循 环卷积网络在多个数据集上均取得了最优的成绩。\n第六章 总结与展望\n文本表示是自然语言处理的基础工作，具有重要的理论意义和广阔的应用\n前景。本文对基于神经网络的文本表示技术中，最重要的两个问题，词和文档 的表示，进行深入分析，比较现有方法的优劣，并提出自己的方法。\n本文的主要工作和贡献总结如下： 在基于神经网络的词向量表示方法中，本文对现有的词向量表示技术进行\n了系统的理论对比及实验分析。理论方面，本文阐述了各种现有模型的联系，从 上下文的表示、上下文与目标词之间的关系两方面对模型进行了分类整理，并 证明了其中最重要的两个模型 Skip-gram与 GloVe之间的关系。实验方面，本 文从模型、语料和训练参数三个角度分析训练词向量的关键技术。本文选取了 三大类一共八个指标对词向量进行评价，这三大类指标涵盖了现有的词向量用 法。通过实验的比较，本文发现，训练词向量首先需要选择一个与任务相匹配 语料，选择领域内的语料训练词向量，能对系统性能有巨大的提升。对于确定 类型的语料，语料规模越大，词向量的性能越好。其次，训练词向量需要选择 一个合适的模型，当语料较小时，应当选用模型结构最简单的 Skip-gram模型， 而当语料较大时，选用 CBOW或者本文提出的 Order模型会有更好的效果。最 后，在训练词向量模型时，需要利用任务验证集或类似任务，找到词向量训练 的最佳迭代次数。\n在中文表示技术中，本文提出了基于字词联合训练的中文表示方法。现有\n的中文表示技术往往沿用了英文的思路，直接从词的层面对文本表示进行构建。 本文根据中文的特点，提出了基于字词联合训练的表示技术。该方法在字表示 的空间中融入了上下文的词，利用词的语义空间，更好地对汉字建模；同时在 词表示中借助字的平滑作用，更好地对词进行建模。对于字表示，本文在分词 任务上对比了字词联合训练与单独训练的字表示，实验表明字词联合训练得到 的字向量，在中文分词任务上有显著的优势；对于词表示，本文采用词义相似 度任务以及文本分类任务对比字词联合训练与单独训练的词表示，实验结果同 样展示了字词联合训练的有效性。\n在基于神经网络的文档表示技术中，本文分析了现有的文档表示技术：基\n于循环网络的表示技术、基于递归网络的表示技术和基于卷积网络的表示技术。\n98 基于神经网络的词和文档语义向量表示方法研究\n并针对现有的三种表示技术的不足，提出了基于卷积循环网络的文档表示技术。 该方法克服了此前递归网络的复杂度过高的问题，循环网络的语义偏置问题， 以及卷积网络窗口较难选择的问题。本文在文本分类任务上对新提出的表示技 术进行了对比分析，实验表明基于循环卷积网络的文本表示技术比现有的表示 技术在文本分类任务上，有更好的性能。\n在机器学习领域有一个公认的观点是，模型选用的特征决定了机器学习算\n法所能达到的上界；而具体模型的选择，则决定了对上界的逼近程度。因此，特 征的表示在机器学习中是至关重要的一个步骤。在基于神经网络的词向量表示 技术中，尽管不同的模型有着不同的性能，但这些模型均基于分布假说，语义 由其上下文决定。这些模型所能表达出的语义，受到语料中各词上下文分布的 约束。\n最近一两年，已经有人尝试跳出分布假说的框架，使用更广泛的信息，对\n语义进行建模。Weston等人设计了一个模型，从知识库中获取语义表示，并用 于提升关系抽取任务的性能 [123]。Wang等人指出，利用知识库中的知识，可 以进行词义消歧，进一步提升词向量的性能 [122]。\n未来的工作需要考虑，如何利用海量的多源异构的数据，从中挖掘出有用\n的信息，更好地对数据和知识进行表示。\n附录 A Skip-gram模型与“词-词”矩阵分解模型关系的证明\n基于“词-词”矩阵分解的模型与神经网络词向量模型中最简单的 Skip-gram 模型有同样的最优解。下面给出简要证明。\n首先列出第二章中 Skip-gram模型的整体优化目标，公式 2.22：\n∑ (wi,c)∈D ∑ wj∈c logP (wi|wj) (2.22)\n该式的两层求和分别为遍历语料中的每一个词，以及遍历每个词的上下文。而 每次参与求和的元素 logP (wi|wj)，只与 (wi, wj)这一组词对有关。词对中的两 个词均来自词表，因此对于一个固定的词表来说，不同类型的求和项最多只有 |V|2个。因此，原式通过调整求和顺序，合并相同的 (wi, wj)词对，等价于：∑ vi∈V ∑ wj∈V xij logP (vi|vj) (A.1)\n其中，V为词表，vi和 vj均为词表中的某个词，xij表示词对 (vi, vj)在公式 2.22中， 各个求和项里的出现次数。\n根据第二章中 Skip-gram模型的条件概率公式 2.23：\nP (vi|vj) = exp (e′(vi)Te(vj))∑ vk∈V exp ( e′(vk) Te(vj) ) (2.23)\n将其代入公式 A.1，得到：\n∑ vi∈V ∑ vj∈V xij log exp (e′(vi)Te(vj))∑ vk∈V exp ( e′(vk) Te(vj) ) (A.2)\n为了方便记述，这里将公式 A.2中的符号做一定的简化。令 pk = e′(wk)，qk =\n100 基于神经网络的词和文档语义向量表示方法研究\ne(wk)，则公式 A.2简化为：\n|V|∑ i=1 |V|∑ j=1 xij log exp (pTi qj)∑|V| k=1 exp (pTkqj)\n(A.3)\n假设向量 p和 q的维度足够大，则 pTi qj 可以拟合任意矩阵。记：\naij = p T i qj bij = exp(aij)∑|V| k=1 exp(akj)\n(A.4)\n则根据 softmax函数的性质，对于某个固定的 j 存在约束：\n|V|∑ k=1 bkj = 1 (A.5)\n将公式A.4代入公式A.3的训练目标中，并且交换两个求和的顺序，可得，Skipgram的目标为最大化：\n|V|∑ j=1 |V|∑ i=1 xij log(bij) (A.6)\n由于存在约束A.5，在对xij做归一化之后，公式A.6第二个求和符号部分 ∑|V|\ni=1 xij log(bij) 可以看做交叉熵，根据交叉熵的性质，公式 A.6的最优解为：\nbij = xij∑|V| k=1 xkj\n(A.7)\n根据公式 A.4中的定义，可得：\nexp(aij)∑|V| k=1 exp(akj) = xij∑|V| k=1 xkj\n(A.8)\n其中，如果 aij = log(xij) + tj，其中 tj 为只与 j 有关的变量，则公式 A.8成立。\n因此，当且仅当 pTi qj = log(xij)可以满足时，最大化公式 2.22（求解 Skip-\n附录 A SKIP-GRAM模型与“词-词”矩阵分解模型关系的证明 101\ngram模型）与最小化以下二式，具有相同的最优解：\n|V|∑ j=1 |V|∑ i=1 ( log(xij)− pTi qj )2 (A.9) |V|∑ j=1 |V|∑ i=1 ( log xij∑|V| k=1 xkj − pTi qj )2 (A.10)\n由于上述各式中 xij 的定义与 GloVe中 xij 的计算方法相同，因此在最优解 可以取到的情况下，Skip-gram模型与“词-词”矩阵分解的模型具有相同的最优 解。其中公式 A.9是对 log共现次数进行分解，与 GloVe的优化目标（公式 2.1） 一致；而公式 A.10是对平移后的 PMI（Pointwise Mutual Information）共现矩阵 进行分解。\n参考文献\n[1] 汉语信息处理词汇 01部分: 基本术语（GB12200.1-90）6. 中国标准出版 社, 1991.\n[2] Charu C Aggarwal and ChengXiang Zhai. A survey of text classification algorithms. In Mining text data, pages 163–222. Springer, 2012.\n[3] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), volume 1, pages 238–247, 2014.\n[4] Robert M. Bell and Yehuda Koren. Lessons from the netflix prize challenge. SIGKDD Explor. Newsl., 9:75–79, 2007.\n[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE TPAMI, 35(8):1798–1828, 2013.\n[6] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. In Advances in Neural Information Processing Systems, pages 932–938, 2001.\n[7] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3:1137–1155, 2003.\n[8] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.\n[9] Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. Joint learning of words and meaning representations for open-text semantic parsing. In\n104 基于神经网络的词和文档语义向量表示方法研究\nInternational Conference on Artificial Intelligence and Statistics, pages 127– 135, 2012.\n[10] Léon Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nımes, volume 91, 1991.\n[11] John S Bridle. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. InNeurocomputing: Algorithms, Architectures and Applications, pages 227–236. Springer, 1990.\n[12] Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479, 1992.\n[13] Lijuan Cai and ThomasHofmann. Text categorization by boosting automatically extracted concepts. In SIGIR, pages 182–189, 2003.\n[14] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In ACL, pages 173–180, 2005.\n[15] Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. Joint learning of character and word embeddings. In International Joint Conference on Artificial Intelligence, 2015.\n[16] William W Cohen, Robert E Schapire, and Yoram Singer. Learning to order things. Journal of Artificial Intelligence Research, 10:243–270, 1998.\n[17] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. International Conference on Machine Learning, 2008.\n[18] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537, 2011.\n[19] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.\n参考文献 105\n[20] Ido Dagan, Lillian Lee, and Fernando CN Pereira. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34(1-3):43–69, 1999.\n[21] Ido Dagan, Shaul Marcus, and Shaul Markovitch. Contextual word similarity and estimation from sparse data. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 164–171, 1993.\n[22] Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Ungar. Multi-view learning of word embeddings via cca. In Advances in Neural Information Processing Systems, pages 199–207, 2011.\n[23] Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Ungar. Eigenwords: Spectral word embeddings. The Journal of Machine Learning Research, 16, 2015.\n[24] Elizabeth D Dolan and Jorge J Moré. Benchmarking optimization software with performance profiles. Mathematical programming, 91(2):201–213, 2002.\n[25] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.\n[26] Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.\n[27] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660, 2010.\n[28] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131, 2002.\n[29] John R Firth. A synopsis of linguistic theory, 1930-1955. Studies in Linguistic Analysis, 1957.\n[30] Gottlob Frege. Über sinn und bedeutung. Funktion - Begriff - Bedeutung, 1892.\n106 基于神经网络的词和文档语义向量表示方法研究\n[31] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for amechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193–202, 1980.\n[32] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume, volume 15, pages 315–323, 2011.\n[33] Irving J Good. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3-4):237–264, 1953.\n[34] Michael U Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. The Journal of Machine Learning Research, 13(1):307–361, 2012.\n[35] Zellig S Harris. Distributional structure. Word, 1954.\n[36] Karl Moritz Hermann. Distributed Representations for Compositional Semantics. PhD thesis, University of Oxford, 2014.\n[37] Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio. Not all neural embeddings are born equal. arXiv preprint arXiv:1410.0718, 2014.\n[38] Swapnil Hingmire, Sandeep Chougule, Girish K Palshikar, and Sutanu Chakraborti. Document classification by topic labeling. In SIGIR, pages 877– 880, 2013.\n[39] Geoffrey E Hinton. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, volume 1, page 12, 1986.\n[40] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.\n参考文献 107\n[41] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.\n[42] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107–116, 1998.\n[43] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n[44] Eric H Huang, Richard Socher, Christopher DManning, and Andrew YNg. Improving word representations via global context and multiple word prototypes. In ACL, pages 873–882, 2012.\n[45] Michael N Jones and Douglas JK Mewhort. Representing word meaning and order information in a composite holographic lexicon. Psychological review, 114(1):1, 2007.\n[46] Nal Kalchbrenner and Phil Blunsom. Recurrent convolutional neural networks for discourse compositionality. InWorkshop on CVSC, pages 119–126, 2013.\n[47] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655–665, 2014.\n[48] Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, 2014.\n[49] Dan Klein and Christopher D. Manning. Accurate unlexicalized parsing. In ACL, pages 423–430, 2003.\n[50] Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. Inducing crosslingual distributed representations of words. In Coling, pages 1459–1474, 2012.\n108 基于神经网络的词和文档语义向量表示方法研究\n[51] Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP95., 1995 International Conference on, volume 1, pages 181–184, 1995.\n[52] Youngjoong Ko. A study of term weighting schemes using class information for text classification. In SIGIR, pages 1029–1030, 2012.\n[53] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. InMT summit, volume 5, pages 79–86, 2005.\n[54] Thomas K Landauer. On the computational basis of learning and cognition: Arguments from lsa. Psychology of learning and motivation, 41:43–84, 2002.\n[55] Thomas K Landauer and Susan T Dumais. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211, 1997.\n[56] Thomas K Landauer, Peter W Foltz, and Darrell Laham. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259–284, 1998.\n[57] Gabriella Lapesa, Stefan Evert, and Sabine Schulte im Walde. Contrasting syntagmatic and paradigmatic relations: Insights from distributional semantic models. In Proceedings of the Third Joint Conference on Lexical and Computational Semantics (SEM 2014), pages 160–170, 2014.\n[58] Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, 2014.\n[59] Rémi Lebret and Ronan Collobert. Word embeddings through hellinger pca. EACL 2014, page 482, 2014.\n[60] Rémi Lebret and Ronan Collobert. Word embeddings through hellinger pca. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–490, 2014.\n参考文献 109\n[61] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[62] Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances in neural information processing systems, pages 556– 562, 2001.\n[63] Gregory W Lesher, Bryan J Moulton, D Jeffery Higginbotham, et al. Effects of ngram order and training text size on word prediction. In Proceedings of the RESNA’99 Annual Conference, pages 52–54, 1999.\n[64] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177–2185, 2014.\n[65] Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 2015.\n[66] David D Lewis. An evaluation of phrasal and clustered representations on a text categorization task. In SIGIR, pages 37–50, 1992.\n[67] Yitan Li, Linli Xu, Fei Tian, Liang Jiang, Xiaowei Zhong, and Enhong Chen. Word embedding revisited: A new representation learning and explicit matrix factorization perspective. In International Joint Conference on Artificial Intelligence, 2015.\n[68] Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. Neural computation, 19(10):2756–2779, 2007.\n[69] Dekang Lin and Xiaoyun Wu. Phrase clustering for discriminative learning. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing, pages 1030–1038, 2009.\n110 基于神经网络的词和文档语义向量表示方法研究\n[70] Kevin Lund, Curt Burgess, and Ruth Ann Atchley. Semantic and associative priming in high-dimensional semantic space. In Proceedings of the 17th annual conference of the Cognitive Science Society, volume 17, pages 660–665, 1995.\n[71] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 142–150, 2011.\n[72] Tomáš Mikolov. Statistical language models based on neural networks. PhD thesis, Brno University of Technology, 2012.\n[73] TomasMikolov, Kai Chen, GregCorrado, and JeffreyDean. Efficient estimation of word representations in vector space. International Conference on Learning Representations Workshop Track, 2013.\n[74] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, pages 1045–1048, 2010.\n[75] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n[76] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In NAACL-HLT, pages 746–751, 2013.\n[77] Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Matthew Purver. Evaluating neural word representations in tensor-based compositional settings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719, 2014.\n[78] Jeff Mitchell and Mirella Lapata. Composition in distributional models of semantics. Cognitive science, 34(8):1388–1429, 2010.\n参考文献 111\n[79] Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641–648, 2007.\n[80] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pages 1081–1088, 2008.\n[81] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in Neural Information Processing Systems, pages 2265–2273, 2013.\n[82] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252, 2005.\n[83] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In AISTATS, pages 246–252, 2005.\n[84] Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. Dependency tree-based sentiment classification using crfs with hidden variables. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 786–794, 2010.\n[85] Andrew Y Ng. Feature selection, l1 vs. l2 regularization, and rotational invariance. In ICML, page 78, 2004.\n[86] Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271, 2004.\n[87] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124, 2005.\n[88] Arkadiusz Paterek. Improving regularized singular value decomposition for collaborative filtering. In Proc. KDDCupWorkshop at SIGKDD’07, 13th ACM Int. Conf. on Knowledge Discovery and Data Mining, pages 39–42, 2007.\n112 基于神经网络的词和文档语义向量表示方法研究\n[89] Fuchun Peng, Fangfang Feng, and Andrew McCallum. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the 20th international conference on Computational Linguistics, page 562, 2004.\n[90] Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe : Global Vectors forWord Representation. InProceedings of the EmpiricialMethods in Natural Language Processing, 2014.\n[91] Fernando Pereira, Naftali Tishby, and Lillian Lee. Distributional clustering of english words. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 183–190, 1993.\n[92] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. InColing-ACL, pages 433–440, 2006.\n[93] David C Plaut and Geoffrey E Hinton. Learning sets of filters using backpropagation. Computer Speech & Language, 2(1):35–61, 1987.\n[94] Matt Post and Shane Bergsma. Explicit and implicit syntactic features for text classification. In ACL, pages 866–872, 2013.\n[95] Lutz Prechelt. Early stopping-but when? Neural Networks: Tricks of the trade, pages 55–69, 1998.\n[96] Dragomir R Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. The acl anthology network corpus. In NLPIR4DL, pages 54–61, 2009.\n[97] Lev Ratinov and Dan Roth. Design challenges andmisconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147–155, 2009.\n[98] Magnus Sahlgren. The Word-Space Model. PhD thesis, Gothenburg University, 2006.\n参考文献 113\n[99] Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information processing & management, 24(5):513–523, 1988.\n[100] Hinrich Schütze. Context space. In AAAI fall symposium on probabilistic approaches to natural language, pages 113–120, 1992.\n[101] Hinrich Schütze. Dimensions of meaning. In Supercomputing’92., Proceedings, pages 787–796, 1992.\n[102] Richard Socher. Recursive Deep Learning for Natural Language Processing and Computer Vision. PhD thesis, Stanford University, 2014.\n[103] Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. Parsing with compositional vector grammars. In ACL, pages 455–465, 2013.\n[104] Richard Socher, Danqi Chen, Christopher DManning, and AndrewNg. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pages 926–934, 2013.\n[105] Richard Socher, Eric H Huang, Jeffrey Pennington, Andrew Y Ng, and Christopher D Manning. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In NIPS, volume 24, pages 801–809, 2011.\n[106] Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. Semantic compositionality through recursive matrix-vector spaces. In EMNLPCoNLL, pages 1201–1211, 2012.\n[107] Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218, 2014.\n[108] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 129– 136, 2011.\n114 基于神经网络的词和文档语义向量表示方法研究\n[109] Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP11), pages 151–161, 2011.\n[110] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP13), pages 1631–1642, 2013.\n[111] Pontus Stenetorp, Hubert Soyer, Sampo Pyysalo, Sophia Ananiadou, and Takashi Chikayama. Size (and domain) matters: Evaluating semantic word space representations for biomedical text. In Proceedings of the 5th International Symposium on Semantic Mining in Biomedicine, 2012.\n[112] Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi Cheng. Inside out: Two jointly predictive models for word representations and phrase representations. In Proceedings of the 30th AAAI conference, 2016.\n[113] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024, 2011.\n[114] Buzhou Tang, Xuan Wang, and Xiaohong Wang. Chinese word segmentation based on large margin methods. International Journal of Asian Language Processing, 19(2):55–68, 2009.\n[115] Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 173–180, 2003.\n[116] Joseph Turian, LevRatinov, andYoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th\n参考文献 115\nannual meeting of the association for computational linguistics (ACL), pages 384–394, 2010.\n[117] Peter D Turney. Similarity of semantic relations. Computational Linguistics, 32(3):379–416, 2006.\n[118] Peter D Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37:141–188, 2010.\n[119] Kun Wang, Chengqing Zong, and Keh-Yih Su. A character-based joint model for chinese word segmentation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1173–1181, 2010.\n[120] Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL, pages 90–94, 2012.\n[121] Sida I. Wang and Christopher D. Manning. Fast dropout training. In In Proceedings of the ICML, 2013.\n[122] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly embedding. InProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1591–1601, 2014.\n[123] Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1366–1371, 2013.\n[124] Wei Xu and Alex Rudnicky. Can artificial neural networks learn language models? In Sixth International Conference on Spoken Language Processing, 2000.\n[125] Nianwen Xue et al. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48, 2003.\n[126] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\n116 基于神经网络的词和文档语义向量表示方法研究\n[127] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classification via convolutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335–2344, 2014.\n[128] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. Understanding bag-of-words model: a statistical framework. International Journal of Machine Learning and Cybernetics, 1(1-4):43–52, 2010.\n[129] Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. Effective tag set selection in chinese word segmentation via conditional random field modeling. In Proceedings of PACLIC, volume 20, pages 87–94, 2006.\n[130] Hai Zhao and Chunyu Kit. Integrating unsupervised and supervised word segmentation: The role of goodness measures. Information Sciences, 181(1):163– 183, 2011.\n[131] Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. Deep learning for Chinese word segmentation and POS tagging. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647–657, 2013.\n[132] 刘群,张华平,俞鸿魁,程学旗. 基于层叠隐马模型的汉语词法分析. 计算机 研究与发展, 41(8):1421–1429, 2004.\n[133] 李文波,孙乐,张大鲲. 基于 Labeled-LDA模型的文本分类新算法. 计算机 学报, 31(4):620–627, 2008.\n[134] 李荣陆. 文本分类及其相关技术研究. PhD thesis,上海: 复旦大学计算机与 信息技术系, 2005.\n[135] 黄昌宁,赵海. 中文分词十年回顾. 中文信息学报, 21(3):8–19, 2007.\n发表文章目录\n[1] Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. Recurrent Convolutional Neural Networks for Text Classification. In Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI’15), pages 2267–2273. 2015.\n[2] 来斯惟，徐立恒，陈玉博，刘康，赵军.基于表示学习的中文分词算法探索. 中文信息学报. 27(5):8–14. 2013.\n[3] Siwei Lai, Yang Liu, Huxiang Gu, Liheng Xu, Kang Liu, Shiming Xiang, Jun Zhao, Rui Diao, Liang Xiang, Hang Li, and Dong Wang. Hybrid Recommendation Models for Binary User Preference Prediction Problem. The Journal of Machine Learning Research Workshop and Conference Proceedings (JMLR W&CP). 18:137–151. 2012.\n[4] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation Classification via Convolutional Deep Neural Network. In Proceedings of the 25th International Conference on Computational Linguistics (COLING’14), pages 2335–2344. 2014. (Best Papers Award).\n[5] Liheng Xu, Kang Liu, Siwei Lai and Jun Zhao. Product Feature Mining: Semantic Clues versus Syntactic Constituents. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL’14), pages 336– 346. 2014.\n[6] Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao. How to Generate a Good Word Embedding? arXiv:1507.05523. (Under review).\n简 历\n基本情况\n来斯惟，男，1987年 3月出生，中国科学院自动化研究所在读博士研究生。\n教育状况\n2006年 9月至 2010年 7月，北京化工大学，获得工学学士学位， 专业：计算机科学与技术。\n2010年 9月至 2015年 12月，中国科学院自动化研究所，硕博连读研究生， 专业：模式识别与智能系统。\n研究兴趣\n自然语言处理，推荐系统\n联系方式\n通讯地址：北京市中关村东路 95号\n邮编：100190\nE-mail: swlai@nlpr.ia.ac.cn\n致 谢\n文章接近尾声，人工神经网络还在学习更好的词和文档的表示，大脑中的\n神经网络也正试图用最合适的方式表示出五年以来从迷茫到收获的一幕幕场 景。其中的困惑、挫折、喜悦、兴奋不便表示成向量列于此，谨以此简单的文 字对所有关心、支持、帮助我的人表示感谢。\n首先衷心感谢我的导师赵军研究员，是他带领我走进自然语言处理这个充\n满趣味和挑战的研究领域。在学术培养中，赵老师采用了“有监督”和“无监 督”相结合的方式：既给予我恰当的引导，又提供了充足的自由探索空间。赵 老师用严谨的治学态度指引我一次次提高能力，以谦逊的学者风范熏陶我一步 步迈向成熟。在生活方面，赵老师也给予了我无微不至的关心和帮助。借此机 会向赵老师表示我最崇高的敬意和最诚挚的感谢！同时也感谢实验室的宗成庆 老师、刘文举老师、陶建华老师，在开题和中期考核中，对我研究工作的指导 和宝贵建议。\n感谢实验室的刘康师兄、周光有师兄和徐立恒师兄，他们犀利的视角让我\n少走了许多弯路。同时要感谢实验室的其他兄弟姐妹，他们是：蔡黎、齐振宇、 张涛、刘洋、刘芳、郑培祥、曾道建、张元哲、王雪鹏、何世柱、许家铭、陈玉 博、刘树林、纪国良、魏琢钰、王炳宁、郝彦超、曾祥荣、刘操、左新宇、郭尚 敏，感谢他们的无私帮助。大家共同努力，打造出了同时具有活跃的学术交流 氛围和温暖的生活、羊毛气息的实验室，共同寻找“最优路径”快速逼近目标。 感谢李嫣然、孙飞、尹文鹏、李亦锬、Omer Levy、Yoav Goldberg、Tomáš Mikolov和所有审稿人，对本文及相关论文提出的宝贵建议。同时感谢 ACM竞 赛工作组的成员，为本文大规模实验的实施提供了支持。\n最后，特别感谢我的家人。感谢我的父母，他们的关怀和无私的爱伴我走过\n了二十多年的学习历程。感谢我的妻子朱微溪，这些年默默地付出了很多很多， 她的鼓励和支持使我有信心和毅力完成学业。同时感谢我的女儿来书瑶，在我 最艰难的时候给了我动力，使我渡过了难关。\n只言片语不足以表示我的感激之情，谨以此文献给所有关心、支持、帮助\n我的人。"
    } ],
    "references" : [ {
      "title" : "A survey of text classification algorithms",
      "author" : [ "Charu C Aggarwal", "ChengXiang Zhai" ],
      "venue" : "In Mining text data,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors",
      "author" : [ "Marco Baroni", "Georgiana Dinu", "Germán Kruszewski" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Lessons from the netflix prize challenge",
      "author" : [ "Robert M. Bell", "Yehuda Koren" ],
      "venue" : "SIGKDD Explor. Newsl.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Yoshua Bengio", "Aaron Courville", "Pascal Vincent" ],
      "venue" : "IEEE TPAMI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "A Neural Probabilistic Language Model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1994
    }, {
      "title" : "Joint learning of words and meaning representations for open-text semantic parsing",
      "author" : [ "Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Stochastic gradient learning in neural networks",
      "author" : [ "Léon Bottou" ],
      "venue" : "In Proceedings of Neuro-Nımes,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1991
    }, {
      "title" : "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. InNeurocomputing: Algorithms, Architectures and Applications, pages 227–236",
      "author" : [ "John S Bridle" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1990
    }, {
      "title" : "Class-based n-gram models of natural language",
      "author" : [ "Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1992
    }, {
      "title" : "Text categorization by boosting automatically extracted concepts",
      "author" : [ "Lijuan Cai", "ThomasHofmann" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2003
    }, {
      "title" : "Coarse-to-fine n-best parsing and maxent discriminative reranking",
      "author" : [ "Eugene Charniak", "Mark Johnson" ],
      "venue" : "In ACL,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "Joint learning of character and word embeddings",
      "author" : [ "Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huanbo Luan" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Learning to order things",
      "author" : [ "William W Cohen", "Robert E Schapire", "Yoram Singer" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1998
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Elements of information theory",
      "author" : [ "Thomas M Cover", "Joy A Thomas" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Similarity-based models of word cooccurrence probabilities",
      "author" : [ "Ido Dagan", "Lillian Lee", "Fernando CN Pereira" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "Contextual word similarity and estimation from sparse data",
      "author" : [ "Ido Dagan", "Shaul Marcus", "Shaul Markovitch" ],
      "venue" : "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1993
    }, {
      "title" : "Multi-view learning of word embeddings via cca",
      "author" : [ "Paramveer S. Dhillon", "Dean P. Foster", "Lyle H. Ungar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Eigenwords: Spectral word embeddings",
      "author" : [ "Paramveer S. Dhillon", "Dean P. Foster", "Lyle H. Ungar" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Benchmarking optimization software with performance profiles",
      "author" : [ "Elizabeth D Dolan", "Jorge J Moré" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2002
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1990
    }, {
      "title" : "Why does unsupervised pre-training help deep learning",
      "author" : [ "Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin" ],
      "venue" : "ACM Transactions on Information Systems,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2002
    }, {
      "title" : "A synopsis of linguistic theory, 1930-1955",
      "author" : [ "John R Firth" ],
      "venue" : "Studies in Linguistic Analysis,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1957
    }, {
      "title" : "Neocognitron: A self-organizing neural network model for amechanism of pattern recognition unaffected by shift in position",
      "author" : [ "Kunihiko Fukushima" ],
      "venue" : "Biological cybernetics,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1980
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2011
    }, {
      "title" : "The population frequencies of species and the estimation of population",
      "author" : [ "Irving J Good" ],
      "venue" : "parameters. Biometrika,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1953
    }, {
      "title" : "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
      "author" : [ "Michael U Gutmann", "Aapo Hyvärinen" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2012
    }, {
      "title" : "Distributed Representations for Compositional Semantics",
      "author" : [ "Karl Moritz Hermann" ],
      "venue" : "PhD thesis, University of Oxford,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Not all neural embeddings are born equal",
      "author" : [ "Felix Hill", "KyungHyun Cho", "Sebastien Jean", "Coline Devin", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1410.0718,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2014
    }, {
      "title" : "Document classification by topic labeling",
      "author" : [ "Swapnil Hingmire", "Sandeep Chougule", "Girish K Palshikar", "Sutanu Chakraborti" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2013
    }, {
      "title" : "Learning distributed representations of concepts",
      "author" : [ "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the eighth annual conference of the cognitive science society,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1986
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Geoffrey E Hinton", "Ruslan R Salakhutdinov" ],
      "venue" : "Science, 313(5786):504–507,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2006
    }, {
      "title" : "Improving neural networks by preventing coadaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2012
    }, {
      "title" : "The vanishing gradient problem during learning recurrent neural nets and problem solutions",
      "author" : [ "Sepp Hochreiter" ],
      "venue" : "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1998
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1997
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Eric H Huang", "Richard Socher", "Christopher DManning", "Andrew YNg" ],
      "venue" : "In ACL,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2012
    }, {
      "title" : "Representing word meaning and order information in a composite holographic lexicon",
      "author" : [ "Michael N Jones", "Douglas JK Mewhort" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2007
    }, {
      "title" : "Recurrent convolutional neural networks for discourse compositionality",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom" ],
      "venue" : "InWorkshop on CVSC,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2013
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2014
    }, {
      "title" : "Accurate unlexicalized parsing",
      "author" : [ "Dan Klein", "Christopher D. Manning" ],
      "venue" : "In ACL, pages 423–430,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2003
    }, {
      "title" : "Inducing crosslingual distributed representations of words",
      "author" : [ "Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai" ],
      "venue" : "In Coling,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2012
    }, {
      "title" : "Improved backing-off for m-gram language modeling",
      "author" : [ "Reinhard Kneser", "Hermann Ney" ],
      "venue" : "In Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1995
    }, {
      "title" : "A study of term weighting schemes using class information for text classification",
      "author" : [ "Youngjoong Ko" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2012
    }, {
      "title" : "Europarl: A parallel corpus for statistical machine translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : "InMT summit,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2005
    }, {
      "title" : "On the computational basis of learning and cognition: Arguments from lsa",
      "author" : [ "Thomas K Landauer" ],
      "venue" : "Psychology of learning and motivation,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2002
    }, {
      "title" : "A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
      "author" : [ "Thomas K Landauer", "Susan T Dumais" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 1997
    }, {
      "title" : "An introduction to latent semantic analysis",
      "author" : [ "Thomas K Landauer", "Peter W Foltz", "Darrell Laham" ],
      "venue" : "Discourse processes,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 1998
    }, {
      "title" : "Contrasting syntagmatic and paradigmatic relations: Insights from distributional semantic models",
      "author" : [ "Gabriella Lapesa", "Stefan Evert", "Sabine Schulte im Walde" ],
      "venue" : "In Proceedings of the Third Joint Conference on Lexical and Computational Semantics (SEM",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2014
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V Le", "Tomas Mikolov" ],
      "venue" : "In ICML,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2014
    }, {
      "title" : "Word embeddings through hellinger pca",
      "author" : [ "Rémi Lebret", "Ronan Collobert" ],
      "venue" : "EACL",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2014
    }, {
      "title" : "Word embeddings through hellinger pca",
      "author" : [ "Rémi Lebret", "Ronan Collobert" ],
      "venue" : "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2014
    }, {
      "title" : "Gradientbased learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 1998
    }, {
      "title" : "Algorithms for non-negative matrix factorization",
      "author" : [ "Daniel D Lee", "H Sebastian Seung" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2001
    }, {
      "title" : "Effects of ngram order and training text size on word prediction",
      "author" : [ "Gregory W Lesher", "Bryan J Moulton", "D Jeffery Higginbotham" ],
      "venue" : "In Proceedings of the RESNA’99 Annual Conference,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 1999
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2014
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2015
    }, {
      "title" : "An evaluation of phrasal and clustered representations on a text categorization task",
      "author" : [ "David D Lewis" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 1992
    }, {
      "title" : "Word embedding revisited: A new representation learning and explicit matrix factorization perspective",
      "author" : [ "Yitan Li", "Linli Xu", "Fei Tian", "Liang Jiang", "Xiaowei Zhong", "Enhong Chen" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2015
    }, {
      "title" : "Projected gradient methods for nonnegative matrix factorization",
      "author" : [ "Chih-Jen Lin" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 2007
    }, {
      "title" : "Phrase clustering for discriminative learning",
      "author" : [ "Dekang Lin", "Xiaoyun Wu" ],
      "venue" : "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing,",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2009
    }, {
      "title" : "Semantic and associative priming in high-dimensional semantic space",
      "author" : [ "Kevin Lund", "Curt Burgess", "Ruth Ann Atchley" ],
      "venue" : "In Proceedings of the 17th annual conference of the Cognitive Science Society,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 1995
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "71",
      "shortCiteRegEx" : "71",
      "year" : 2011
    }, {
      "title" : "Statistical language models based on neural networks",
      "author" : [ "Tomáš Mikolov" ],
      "venue" : "PhD thesis, Brno University of Technology,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : "72",
      "year" : 2012
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "TomasMikolov", "Kai Chen", "GregCorrado", "JeffreyDean" ],
      "venue" : "International Conference on Learning Representations Workshop Track,",
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur" ],
      "venue" : "In INTER- SPEECH 2010, 11th Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "74",
      "shortCiteRegEx" : "74",
      "year" : 2010
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "75",
      "shortCiteRegEx" : "75",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "76",
      "shortCiteRegEx" : "76",
      "year" : 2013
    }, {
      "title" : "Evaluating neural word representations in tensor-based compositional settings",
      "author" : [ "Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "77",
      "shortCiteRegEx" : "77",
      "year" : 2014
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "Jeff Mitchell", "Mirella Lapata" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "78",
      "shortCiteRegEx" : "78",
      "year" : 2010
    }, {
      "title" : "Three new graphical models for statistical language modelling",
      "author" : [ "Andriy Mnih", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "79",
      "shortCiteRegEx" : "79",
      "year" : 2007
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "Andriy Mnih", "Geoffrey E Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "80",
      "shortCiteRegEx" : "80",
      "year" : 2008
    }, {
      "title" : "Learning word embeddings efficiently with noise-contrastive estimation",
      "author" : [ "Andriy Mnih", "Koray Kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "81",
      "shortCiteRegEx" : "81",
      "year" : 2013
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "Frederic Morin", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the international workshop on artificial intelligence and statistics,",
      "citeRegEx" : "82",
      "shortCiteRegEx" : "82",
      "year" : 2005
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "Frederic Morin", "Yoshua Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "83",
      "shortCiteRegEx" : "83",
      "year" : 2005
    }, {
      "title" : "Dependency tree-based sentiment classification using crfs with hidden variables",
      "author" : [ "Tetsuji Nakagawa", "Kentaro Inui", "Sadao Kurohashi" ],
      "venue" : "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "84",
      "shortCiteRegEx" : "84",
      "year" : 2010
    }, {
      "title" : "Feature selection, l1 vs. l2 regularization, and rotational invariance",
      "author" : [ "Andrew Y Ng" ],
      "venue" : "In ICML, page",
      "citeRegEx" : "85",
      "shortCiteRegEx" : "85",
      "year" : 2004
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "86",
      "shortCiteRegEx" : "86",
      "year" : 2004
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "In ACL,",
      "citeRegEx" : "87",
      "shortCiteRegEx" : "87",
      "year" : 2005
    }, {
      "title" : "Improving regularized singular value decomposition for collaborative filtering",
      "author" : [ "Arkadiusz Paterek" ],
      "venue" : "In Proc. KDDCupWorkshop at SIGKDD’07,",
      "citeRegEx" : "88",
      "shortCiteRegEx" : "88",
      "year" : 2007
    }, {
      "title" : "Chinese segmentation and new word detection using conditional random fields",
      "author" : [ "Fuchun Peng", "Fangfang Feng", "Andrew McCallum" ],
      "venue" : "In Proceedings of the 20th international conference on Computational Linguistics,",
      "citeRegEx" : "89",
      "shortCiteRegEx" : "89",
      "year" : 2004
    }, {
      "title" : "Distributional clustering of english words",
      "author" : [ "Fernando Pereira", "Naftali Tishby", "Lillian Lee" ],
      "venue" : "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "91",
      "shortCiteRegEx" : "91",
      "year" : 1993
    }, {
      "title" : "Learning accurate, compact, and interpretable tree annotation",
      "author" : [ "Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein" ],
      "venue" : "InColing-ACL, pages 433–440,",
      "citeRegEx" : "92",
      "shortCiteRegEx" : "92",
      "year" : 2006
    }, {
      "title" : "Learning sets of filters using backpropagation",
      "author" : [ "David C Plaut", "Geoffrey E Hinton" ],
      "venue" : "Computer Speech & Language,",
      "citeRegEx" : "93",
      "shortCiteRegEx" : "93",
      "year" : 1987
    }, {
      "title" : "Explicit and implicit syntactic features for text classification",
      "author" : [ "Matt Post", "Shane Bergsma" ],
      "venue" : "In ACL,",
      "citeRegEx" : "94",
      "shortCiteRegEx" : "94",
      "year" : 2013
    }, {
      "title" : "Early stopping-but when",
      "author" : [ "Lutz Prechelt" ],
      "venue" : "Neural Networks: Tricks of the trade,",
      "citeRegEx" : "95",
      "shortCiteRegEx" : "95",
      "year" : 1998
    }, {
      "title" : "The acl anthology network corpus",
      "author" : [ "Dragomir R Radev", "Pradeep Muthukrishnan", "Vahed Qazvinian" ],
      "venue" : "In NLPIR4DL,",
      "citeRegEx" : "96",
      "shortCiteRegEx" : "96",
      "year" : 2009
    }, {
      "title" : "Design challenges andmisconceptions in named entity recognition",
      "author" : [ "Lev Ratinov", "Dan Roth" ],
      "venue" : "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,",
      "citeRegEx" : "97",
      "shortCiteRegEx" : "97",
      "year" : 2009
    }, {
      "title" : "The Word-Space Model",
      "author" : [ "Magnus Sahlgren" ],
      "venue" : "PhD thesis, Gothenburg University,",
      "citeRegEx" : "98",
      "shortCiteRegEx" : "98",
      "year" : 2006
    }, {
      "title" : "Term-weighting approaches in automatic text retrieval",
      "author" : [ "Gerard Salton", "Christopher Buckley" ],
      "venue" : "Information processing & management,",
      "citeRegEx" : "99",
      "shortCiteRegEx" : "99",
      "year" : 1988
    }, {
      "title" : "Context space. In AAAI fall symposium on probabilistic approaches to natural language, pages",
      "author" : [ "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "100",
      "shortCiteRegEx" : "100",
      "year" : 1992
    }, {
      "title" : "Dimensions of meaning",
      "author" : [ "Hinrich Schütze" ],
      "venue" : "In Supercomputing’92.,",
      "citeRegEx" : "101",
      "shortCiteRegEx" : "101",
      "year" : 1992
    }, {
      "title" : "Recursive Deep Learning for Natural Language Processing and Computer Vision",
      "author" : [ "Richard Socher" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "102",
      "shortCiteRegEx" : "102",
      "year" : 2014
    }, {
      "title" : "Parsing with compositional vector grammars",
      "author" : [ "Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In ACL,",
      "citeRegEx" : "103",
      "shortCiteRegEx" : "103",
      "year" : 2013
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher DManning", "AndrewNg" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "104",
      "shortCiteRegEx" : "104",
      "year" : 2013
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "105",
      "shortCiteRegEx" : "105",
      "year" : 2011
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In EMNLP- CoNLL,",
      "citeRegEx" : "106",
      "shortCiteRegEx" : "106",
      "year" : 2012
    }, {
      "title" : "Grounded compositional semantics for finding and describing images with sentences",
      "author" : [ "Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "107",
      "shortCiteRegEx" : "107",
      "year" : 2014
    }, {
      "title" : "Parsing natural scenes and natural language with recursive neural networks",
      "author" : [ "Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 28th international conference on machine learning",
      "citeRegEx" : "108",
      "shortCiteRegEx" : "108",
      "year" : 2011
    }, {
      "title" : "Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "author" : [ "Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "109",
      "shortCiteRegEx" : "109",
      "year" : 2011
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "110",
      "shortCiteRegEx" : "110",
      "year" : 2013
    }, {
      "title" : "Size (and domain) matters: Evaluating semantic word space representations for biomedical text",
      "author" : [ "Pontus Stenetorp", "Hubert Soyer", "Sampo Pyysalo", "Sophia Ananiadou", "Takashi Chikayama" ],
      "venue" : "In Proceedings of the 5th International Symposium on Semantic Mining in Biomedicine,",
      "citeRegEx" : "111",
      "shortCiteRegEx" : "111",
      "year" : 2012
    }, {
      "title" : "Inside out: Two jointly predictive models for word representations and phrase representations",
      "author" : [ "Fei Sun", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng" ],
      "venue" : "In Proceedings of the 30th AAAI conference,",
      "citeRegEx" : "112",
      "shortCiteRegEx" : "112",
      "year" : 2016
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "113",
      "shortCiteRegEx" : "113",
      "year" : 2011
    }, {
      "title" : "Chinese word segmentation based on large margin methods",
      "author" : [ "Buzhou Tang", "Xuan Wang", "Xiaohong Wang" ],
      "venue" : "International Journal of Asian Language Processing,",
      "citeRegEx" : "114",
      "shortCiteRegEx" : "114",
      "year" : 2009
    }, {
      "title" : "Feature-rich part-of-speech tagging with a cyclic dependency network",
      "author" : [ "Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer" ],
      "venue" : "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume",
      "citeRegEx" : "115",
      "shortCiteRegEx" : "115",
      "year" : 2003
    }, {
      "title" : "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th  参考文献  115 annual meeting of the association for computational linguistics",
      "author" : [ "Joseph Turian", "LevRatinov", "andYoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "116",
      "shortCiteRegEx" : "116",
      "year" : 2010
    }, {
      "title" : "Similarity of semantic relations",
      "author" : [ "Peter D Turney" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "117",
      "shortCiteRegEx" : "117",
      "year" : 2006
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Peter D Turney", "Patrick Pantel" ],
      "venue" : "Journal of artificial intelligence research,",
      "citeRegEx" : "118",
      "shortCiteRegEx" : "118",
      "year" : 2010
    }, {
      "title" : "A character-based joint model for chinese word segmentation",
      "author" : [ "Kun Wang", "Chengqing Zong", "Keh-Yih Su" ],
      "venue" : "In Proceedings of the 23rd International Conference on Computational Linguistics,",
      "citeRegEx" : "119",
      "shortCiteRegEx" : "119",
      "year" : 2010
    }, {
      "title" : "Baselines and bigrams: Simple, good sentiment and topic classification",
      "author" : [ "Sida Wang", "Christopher D Manning" ],
      "venue" : "In ACL,",
      "citeRegEx" : "120",
      "shortCiteRegEx" : "120",
      "year" : 2012
    }, {
      "title" : "Fast dropout training",
      "author" : [ "Sida I. Wang", "Christopher D. Manning" ],
      "venue" : "Proceedings of the ICML,",
      "citeRegEx" : "121",
      "shortCiteRegEx" : "121",
      "year" : 2013
    }, {
      "title" : "Knowledge graph and text jointly embedding",
      "author" : [ "Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen" ],
      "venue" : "InProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "122",
      "shortCiteRegEx" : "122",
      "year" : 2014
    }, {
      "title" : "Connecting language and knowledge bases with embedding models for relation extraction",
      "author" : [ "Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "123",
      "shortCiteRegEx" : "123",
      "year" : 2013
    }, {
      "title" : "Can artificial neural networks learn language models",
      "author" : [ "Wei Xu", "Alex Rudnicky" ],
      "venue" : "In Sixth International Conference on Spoken Language Processing,",
      "citeRegEx" : "124",
      "shortCiteRegEx" : "124",
      "year" : 2000
    }, {
      "title" : "Chinese word segmentation as character tagging",
      "author" : [ "Nianwen Xue" ],
      "venue" : "Computational Linguistics and Chinese Language Processing,",
      "citeRegEx" : "125",
      "shortCiteRegEx" : "125",
      "year" : 2003
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "126",
      "shortCiteRegEx" : "126",
      "year" : 2012
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao" ],
      "venue" : "In Proceedings of COL- ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,",
      "citeRegEx" : "127",
      "shortCiteRegEx" : "127",
      "year" : 2014
    }, {
      "title" : "Understanding bag-of-words model: a statistical framework",
      "author" : [ "Yin Zhang", "Rong Jin", "Zhi-Hua Zhou" ],
      "venue" : "International Journal of Machine Learning and Cybernetics,",
      "citeRegEx" : "128",
      "shortCiteRegEx" : "128",
      "year" : 2010
    }, {
      "title" : "Effective tag set selection in chinese word segmentation via conditional random field modeling",
      "author" : [ "Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu" ],
      "venue" : "In Proceedings of PACLIC,",
      "citeRegEx" : "129",
      "shortCiteRegEx" : "129",
      "year" : 2006
    }, {
      "title" : "Integrating unsupervised and supervised word segmentation: The role of goodness measures",
      "author" : [ "Hai Zhao", "Chunyu Kit" ],
      "venue" : "Information Sciences,",
      "citeRegEx" : "130",
      "shortCiteRegEx" : "130",
      "year" : 2011
    }, {
      "title" : "Deep learning for Chinese word segmentation and POS tagging",
      "author" : [ "Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "131",
      "shortCiteRegEx" : "131",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "1 术语 • 分布假说(distributional hypothesis):上下文相似的词,其语义也相似。该 假说由 Harris在 1954年提出 [35],并由 Firth在 1957年进一步明确和完 善 [29]。 • 分布表示(distributional representation):分布(distributional)描述的是上 下文的概率分布,因此用上下文描述语义的表示方法(基于分布假说的方 法)都可以称作分布表示。与之相对的是形式语义表示。 • 分布式表示(distributed representation):分布式(distributed)描述的是把 信息分布式地存储在向量的各个维度中,与之相对的是局部表示(local representation),如词的独热表示(one-hot representation),在高维向量中 只有一个维度描述了词的语义。一般来说,通过矩阵降维或神经网络降维 可以将语义分散存储到向量的各个维度中,因此,这类方法得到的低维向 量一般都可以称作分布式表示。",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 29,
      "context" : "• 粗体小写字母表示列向量,如 h、p、q、e等。其中 e(w)特指词 w的词 向量,e′(w)特指词 w的辅助词向量(具体作用在模型中会有介绍)。 • 大写字母表示矩阵,常用的符号有H、W、U、A、B等。需要注意的是, 为了与常用的数学及神经网络符号统一,O 仍然为复杂度渐近上限记号, E 表示能量函数。 • 双线体大写字母表示集合,具体包括:D表示数据集(包括训练词向量的 语料、训练文本分类的数据集以及训练分词模型的数据集);R表示实数 集,R 表示 a维实数向量集合,Ra×b 表示 a行 b列的实数矩阵集合;V 表示词表(单词的集合)。 • 正体字表示数学函数,如 exp、max、tanh等。 • φ表示非线性激活函数,可能为 tanh、sigmoid(Logistic函数)、ReLU [32] 等。 • 绝对值符号 |x|对于集合表示集合的大小,如 |V|表示词表中词的总个数; 对于向量表示向量的维度,如 |e|表示词向量的维度。 • [p1;p2; .",
      "startOffset" : 351,
      "endOffset" : 355
    }, {
      "referenceID" : 3,
      "context" : "数据表示是机器学习中的基础工作,数据表示的好坏直接影响到整个机器 学习系统的性能 [5]。因此,人们投入了大量精力去研究如何针对具体任务,设 计一种合适的数据表示方法,以提升机器学习系统的性能,这一环节也被称作 特征工程。传统机器学习方法不能直接从数据中自动挖掘出有判别力的信息, 而特征工程正是通过人类的智慧、知识和灵感来弥补机器学习方法的这一缺陷。 在自然语言处理领域,最常用的文本表示方法是词袋子表示 [128],该方法会面 临数据稀疏问题,并且不能保留词序信息。研究人员针对这些缺陷,还提出了 词法特征、句法特征等复杂特征。借助这些人工精心设计的特征,机器学习在 自然语言领域逐步取代了以往基于规则的方法,成为自然语言处理中的主流方 法。 特征工程在传统机器学习算法中,有着不可替代的地位,但是由于需要大 量人力和专业知识,反而成为了机器学习系统性能提升的瓶颈。为了让机器学 习算法有更好的扩展性,研究人员希望可以减少对特征工程的依赖。这样,当 把机器学习算法推广到新的领域中时,就可以省去大量专家在新领域上的分析 和探索,加快应用的进程,使得系统更为智能。从人工智能的角度看,算法直 接从原始的感知数据中自动分辨出有效的信息,是机器走向智能的重要一步。 近年来,随着Web2.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 123,
      "context" : "数据表示是机器学习中的基础工作,数据表示的好坏直接影响到整个机器 学习系统的性能 [5]。因此,人们投入了大量精力去研究如何针对具体任务,设 计一种合适的数据表示方法,以提升机器学习系统的性能,这一环节也被称作 特征工程。传统机器学习方法不能直接从数据中自动挖掘出有判别力的信息, 而特征工程正是通过人类的智慧、知识和灵感来弥补机器学习方法的这一缺陷。 在自然语言处理领域,最常用的文本表示方法是词袋子表示 [128],该方法会面 临数据稀疏问题,并且不能保留词序信息。研究人员针对这些缺陷,还提出了 词法特征、句法特征等复杂特征。借助这些人工精心设计的特征,机器学习在 自然语言领域逐步取代了以往基于规则的方法,成为自然语言处理中的主流方 法。 特征工程在传统机器学习算法中,有着不可替代的地位,但是由于需要大 量人力和专业知识,反而成为了机器学习系统性能提升的瓶颈。为了让机器学 习算法有更好的扩展性,研究人员希望可以减少对特征工程的依赖。这样,当 把机器学习算法推广到新的领域中时,就可以省去大量专家在新领域上的分析 和探索,加快应用的进程,使得系统更为智能。从人工智能的角度看,算法直 接从原始的感知数据中自动分辨出有效的信息,是机器走向智能的重要一步。 近年来,随着Web2.",
      "startOffset" : 204,
      "endOffset" : 209
    }, {
      "referenceID" : 36,
      "context" : "8ZB,到 2020年, 全球数据总量预计还将增长 50倍。大量无标注数据的出现,也让研究人员开始 考虑,如何利用算法从这些大规模无标注的数据中自动挖掘规律,得到有用的 信息。2006年 Hinton提出的深度学习 [40],为解决这一问题带来了新的思路。 在之后的发展中,基于神经网络的表示学习技术开始在各个领域崭露头角。尤 其在图像和语音领域的多个任务上,基于表示学习的方法在性能上均超过了传 统方法。 但是,在自然语言处理领域,深度学习技术并没有产生类似图像和语音领 域那样的突破。其中一个主要的原因是,在图像和语音领域,最基本的数据是",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 52,
      "context" : "信号数据,我们可以通过一些距离度量,判断信号是否相似。而文本是符号数 据,两个词只要字面不同,就难以刻画它们之间的联系,即使是“麦克风”和 “话筒”这样的同义词,从字面上也难以看出这两者意思相同(语义鸿沟现象)。 正因为这样,在判断两幅图片是否相似时,只需通过观察图片本身就能给出回 答;而判断两个词是否相似时,还需要更多的背景知识才能做出回答。 我们希望计算机可以从大规模无标注的文本数据中自动学习得到文本表 示,这种表示需要包含对应语言单元(词或文档)的语义信息,同时可以直接 通过这种表示度量文本之间的语义相似度。 1954年,Harris提出分布假说(distributional hypothesis),即“上下文相似的 词,其语义也相似”[35],为词的分布表示提供了理论基础。基于分布假说,研究 人员提出了多种词表示模型:如基于矩阵的 LSA模型 [56]、基于聚类的 Brown clustering模型 [12]以及本文关注的神经网络词表示模型,本文第二章对这些模 型进行了综述。在分布假说中,需要关注的对象有两个:词和上下文,其中最 关键的是上下文的表示。在前两个模型中,上下文只能使用传统的词袋子表示, 如果需要表示复杂的上下文,会遇到维数灾难问题。而神经网络模型可以使用 组合方式对上下文进行建模,只需线性复杂度即可对复杂的 n元短语进行建模。 神经网络模型生成的词表示通常被称为词向量(word embedding),是一个低维 的实数向量表示,通过这种表示,可以直接对词之间的相似度进行刻画。相比 传统的词袋子表示方法以及矩阵、聚类等衍生方法,词向量可以缓解维数灾难 的问题。从广义上讲,传统的词袋子模型也是用向量描述文本,也应当被称作 词的向量表示,但是这种向量是高维稀疏的。在本文中,“词向量”特指由神经 网络模型得到的低维实数向量表示。 对于文本分类、信息检索等实际需求而言,仅使用词级别的语义表示不足以 有效地完成这些任务,因此还需要通过模型,得到句子和文档级别的语义表示。 但是,由于文档的多样性,直接使用分布假说构建文档的语义向量表示时,会遇 到严重的数据稀疏问题;同时由于分布假说是针对词义的假说,这种通过上下 文获取语义的方式对句子和文档是否有效,还有待讨论。为了获得句子和文档 的语义表示,研究人员一般采用语义组合的方式。德国数学家弗雷格(Gottlob Frege)在 1892年提出:一段话的语义由其各组成部分的语义以及它们之间的 组合方法所确定 [30]。现有的句子或者文档表示也通常以该思路为基础,通过 语义组合的方式获得。主流的神经网络语义组合方法包括递归神经网络、循环",
      "startOffset" : 381,
      "endOffset" : 385
    }, {
      "referenceID" : 10,
      "context" : "信号数据,我们可以通过一些距离度量,判断信号是否相似。而文本是符号数 据,两个词只要字面不同,就难以刻画它们之间的联系,即使是“麦克风”和 “话筒”这样的同义词,从字面上也难以看出这两者意思相同(语义鸿沟现象)。 正因为这样,在判断两幅图片是否相似时,只需通过观察图片本身就能给出回 答;而判断两个词是否相似时,还需要更多的背景知识才能做出回答。 我们希望计算机可以从大规模无标注的文本数据中自动学习得到文本表 示,这种表示需要包含对应语言单元(词或文档)的语义信息,同时可以直接 通过这种表示度量文本之间的语义相似度。 1954年,Harris提出分布假说(distributional hypothesis),即“上下文相似的 词,其语义也相似”[35],为词的分布表示提供了理论基础。基于分布假说,研究 人员提出了多种词表示模型:如基于矩阵的 LSA模型 [56]、基于聚类的 Brown clustering模型 [12]以及本文关注的神经网络词表示模型,本文第二章对这些模 型进行了综述。在分布假说中,需要关注的对象有两个:词和上下文,其中最 关键的是上下文的表示。在前两个模型中,上下文只能使用传统的词袋子表示, 如果需要表示复杂的上下文,会遇到维数灾难问题。而神经网络模型可以使用 组合方式对上下文进行建模,只需线性复杂度即可对复杂的 n元短语进行建模。 神经网络模型生成的词表示通常被称为词向量(word embedding),是一个低维 的实数向量表示,通过这种表示,可以直接对词之间的相似度进行刻画。相比 传统的词袋子表示方法以及矩阵、聚类等衍生方法,词向量可以缓解维数灾难 的问题。从广义上讲,传统的词袋子模型也是用向量描述文本,也应当被称作 词的向量表示,但是这种向量是高维稀疏的。在本文中,“词向量”特指由神经 网络模型得到的低维实数向量表示。 对于文本分类、信息检索等实际需求而言,仅使用词级别的语义表示不足以 有效地完成这些任务,因此还需要通过模型,得到句子和文档级别的语义表示。 但是,由于文档的多样性,直接使用分布假说构建文档的语义向量表示时,会遇 到严重的数据稀疏问题;同时由于分布假说是针对词义的假说,这种通过上下 文获取语义的方式对句子和文档是否有效,还有待讨论。为了获得句子和文档 的语义表示,研究人员一般采用语义组合的方式。德国数学家弗雷格(Gottlob Frege)在 1892年提出:一段话的语义由其各组成部分的语义以及它们之间的 组合方法所确定 [30]。现有的句子或者文档表示也通常以该思路为基础,通过 语义组合的方式获得。主流的神经网络语义组合方法包括递归神经网络、循环",
      "startOffset" : 411,
      "endOffset" : 415
    }, {
      "referenceID" : 27,
      "context" : "1 分布表示 词是承载语义的最基本的单元 [1],而传统的独热表示(one-hot representation)仅仅将词符号化,不包含任何语义信息。如何将语义融入到词表示中? Harris在 1954年提出的分布假说(distributional hypothesis)为这一设想提供了 理论基础:上下文相似的词,其语义也相似 [35]。Firth在 1957年对分布假说进 行了进一步阐述和明确:词的语义由其上下文决定(a word is characterized by the company it keeps)[29]。二十世纪 90年代初期,统计方法在自然语言处理中逐渐 成为主流,分布假说也再次被人关注。Dagan和 Schütze等人总结完善了利用上 下文分布表示词义的方法,并将这种表示用于词义消歧等任务 [20, 21, 100, 101], 这类方法在当时被成为词空间模型(word space model)。在此后的发展中,这 类方法逐渐演化成为基于矩阵的分布表示方法,期间的十多年时间里,这类方 法得到的词表示都被直接称为分布表示(distributional representation)。1992年, Brown等人同样基于分布假说,构造了一个上下文聚类模型,开创了基于聚类 的分布表示方法 [12]。2006年之后,随着硬件性能的提升以及优化算法的突破, 神经网络模型逐渐在各个领域中发挥出自己的优势,使用神经网络构造词表示 的方法可以更灵活地对上下文进行建模,这类方法开始逐渐成为了词分布表示 的主流方法。 到目前为止,基于分布假说的词表示方法,根据建模的不同,主要可以分 为三类:基于矩阵的分布表示(2.",
      "startOffset" : 260,
      "endOffset" : 264
    }, {
      "referenceID" : 18,
      "context" : "1 分布表示 词是承载语义的最基本的单元 [1],而传统的独热表示(one-hot representation)仅仅将词符号化,不包含任何语义信息。如何将语义融入到词表示中? Harris在 1954年提出的分布假说(distributional hypothesis)为这一设想提供了 理论基础:上下文相似的词,其语义也相似 [35]。Firth在 1957年对分布假说进 行了进一步阐述和明确:词的语义由其上下文决定(a word is characterized by the company it keeps)[29]。二十世纪 90年代初期,统计方法在自然语言处理中逐渐 成为主流,分布假说也再次被人关注。Dagan和 Schütze等人总结完善了利用上 下文分布表示词义的方法,并将这种表示用于词义消歧等任务 [20, 21, 100, 101], 这类方法在当时被成为词空间模型(word space model)。在此后的发展中,这 类方法逐渐演化成为基于矩阵的分布表示方法,期间的十多年时间里,这类方 法得到的词表示都被直接称为分布表示(distributional representation)。1992年, Brown等人同样基于分布假说,构造了一个上下文聚类模型,开创了基于聚类 的分布表示方法 [12]。2006年之后,随着硬件性能的提升以及优化算法的突破, 神经网络模型逐渐在各个领域中发挥出自己的优势,使用神经网络构造词表示 的方法可以更灵活地对上下文进行建模,这类方法开始逐渐成为了词分布表示 的主流方法。 到目前为止,基于分布假说的词表示方法,根据建模的不同,主要可以分 为三类:基于矩阵的分布表示(2.",
      "startOffset" : 362,
      "endOffset" : 380
    }, {
      "referenceID" : 19,
      "context" : "1 分布表示 词是承载语义的最基本的单元 [1],而传统的独热表示(one-hot representation)仅仅将词符号化,不包含任何语义信息。如何将语义融入到词表示中? Harris在 1954年提出的分布假说(distributional hypothesis)为这一设想提供了 理论基础:上下文相似的词,其语义也相似 [35]。Firth在 1957年对分布假说进 行了进一步阐述和明确:词的语义由其上下文决定(a word is characterized by the company it keeps)[29]。二十世纪 90年代初期,统计方法在自然语言处理中逐渐 成为主流,分布假说也再次被人关注。Dagan和 Schütze等人总结完善了利用上 下文分布表示词义的方法,并将这种表示用于词义消歧等任务 [20, 21, 100, 101], 这类方法在当时被成为词空间模型(word space model)。在此后的发展中,这 类方法逐渐演化成为基于矩阵的分布表示方法,期间的十多年时间里,这类方 法得到的词表示都被直接称为分布表示(distributional representation)。1992年, Brown等人同样基于分布假说,构造了一个上下文聚类模型,开创了基于聚类 的分布表示方法 [12]。2006年之后,随着硬件性能的提升以及优化算法的突破, 神经网络模型逐渐在各个领域中发挥出自己的优势,使用神经网络构造词表示 的方法可以更灵活地对上下文进行建模,这类方法开始逐渐成为了词分布表示 的主流方法。 到目前为止,基于分布假说的词表示方法,根据建模的不同,主要可以分 为三类:基于矩阵的分布表示(2.",
      "startOffset" : 362,
      "endOffset" : 380
    }, {
      "referenceID" : 95,
      "context" : "1 分布表示 词是承载语义的最基本的单元 [1],而传统的独热表示(one-hot representation)仅仅将词符号化,不包含任何语义信息。如何将语义融入到词表示中? Harris在 1954年提出的分布假说(distributional hypothesis)为这一设想提供了 理论基础:上下文相似的词,其语义也相似 [35]。Firth在 1957年对分布假说进 行了进一步阐述和明确:词的语义由其上下文决定(a word is characterized by the company it keeps)[29]。二十世纪 90年代初期,统计方法在自然语言处理中逐渐 成为主流,分布假说也再次被人关注。Dagan和 Schütze等人总结完善了利用上 下文分布表示词义的方法,并将这种表示用于词义消歧等任务 [20, 21, 100, 101], 这类方法在当时被成为词空间模型(word space model)。在此后的发展中,这 类方法逐渐演化成为基于矩阵的分布表示方法,期间的十多年时间里,这类方 法得到的词表示都被直接称为分布表示(distributional representation)。1992年, Brown等人同样基于分布假说,构造了一个上下文聚类模型,开创了基于聚类 的分布表示方法 [12]。2006年之后,随着硬件性能的提升以及优化算法的突破, 神经网络模型逐渐在各个领域中发挥出自己的优势,使用神经网络构造词表示 的方法可以更灵活地对上下文进行建模,这类方法开始逐渐成为了词分布表示 的主流方法。 到目前为止,基于分布假说的词表示方法,根据建模的不同,主要可以分 为三类:基于矩阵的分布表示(2.",
      "startOffset" : 362,
      "endOffset" : 380
    }, {
      "referenceID" : 96,
      "context" : "1 分布表示 词是承载语义的最基本的单元 [1],而传统的独热表示(one-hot representation)仅仅将词符号化,不包含任何语义信息。如何将语义融入到词表示中? Harris在 1954年提出的分布假说(distributional hypothesis)为这一设想提供了 理论基础:上下文相似的词,其语义也相似 [35]。Firth在 1957年对分布假说进 行了进一步阐述和明确:词的语义由其上下文决定(a word is characterized by the company it keeps)[29]。二十世纪 90年代初期,统计方法在自然语言处理中逐渐 成为主流,分布假说也再次被人关注。Dagan和 Schütze等人总结完善了利用上 下文分布表示词义的方法,并将这种表示用于词义消歧等任务 [20, 21, 100, 101], 这类方法在当时被成为词空间模型(word space model)。在此后的发展中,这 类方法逐渐演化成为基于矩阵的分布表示方法,期间的十多年时间里,这类方 法得到的词表示都被直接称为分布表示(distributional representation)。1992年, Brown等人同样基于分布假说,构造了一个上下文聚类模型,开创了基于聚类 的分布表示方法 [12]。2006年之后,随着硬件性能的提升以及优化算法的突破, 神经网络模型逐渐在各个领域中发挥出自己的优势,使用神经网络构造词表示 的方法可以更灵活地对上下文进行建模,这类方法开始逐渐成为了词分布表示 的主流方法。 到目前为止,基于分布假说的词表示方法,根据建模的不同,主要可以分 为三类:基于矩阵的分布表示(2.",
      "startOffset" : 362,
      "endOffset" : 380
    }, {
      "referenceID" : 10,
      "context" : "1 分布表示 词是承载语义的最基本的单元 [1],而传统的独热表示(one-hot representation)仅仅将词符号化,不包含任何语义信息。如何将语义融入到词表示中? Harris在 1954年提出的分布假说(distributional hypothesis)为这一设想提供了 理论基础:上下文相似的词,其语义也相似 [35]。Firth在 1957年对分布假说进 行了进一步阐述和明确:词的语义由其上下文决定(a word is characterized by the company it keeps)[29]。二十世纪 90年代初期,统计方法在自然语言处理中逐渐 成为主流,分布假说也再次被人关注。Dagan和 Schütze等人总结完善了利用上 下文分布表示词义的方法,并将这种表示用于词义消歧等任务 [20, 21, 100, 101], 这类方法在当时被成为词空间模型(word space model)。在此后的发展中,这 类方法逐渐演化成为基于矩阵的分布表示方法,期间的十多年时间里,这类方 法得到的词表示都被直接称为分布表示(distributional representation)。1992年, Brown等人同样基于分布假说,构造了一个上下文聚类模型,开创了基于聚类 的分布表示方法 [12]。2006年之后,随着硬件性能的提升以及优化算法的突破, 神经网络模型逐渐在各个领域中发挥出自己的优势,使用神经网络构造词表示 的方法可以更灵活地对上下文进行建模,这类方法开始逐渐成为了词分布表示 的主流方法。 到目前为止,基于分布假说的词表示方法,根据建模的不同,主要可以分 为三类:基于矩阵的分布表示(2.",
      "startOffset" : 562,
      "endOffset" : 566
    }, {
      "referenceID" : 111,
      "context" : "3 小节)。从广义上看,所有基于分布假说得 到的表示均可称为分布表示(distributional representation),如上述的三种。而狭 义的分布表示通常指基于矩阵的分布表示 [116]。本文以 Christopher Manning的 观点为基准1,文中出现的“分布表示”均指广义的分布表示,其它观点参考本 1Manning在 2015年深度学习暑期学校(蒙特利尔)中澄清,分布(distributional)意为使用上下文表",
      "startOffset" : 95,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "1 基于矩阵的分布表示 基于矩阵的分布表示通常又称为分布语义模型(distributional semantic models)[3],一些文献中也直接将其称作分布表示(distributional representation)[116]。 这类方法需要构建一个“词-上下文”矩阵,从矩阵中获取词的表示。在“词-上 下文”矩阵中,每行对应一个词,每列表示一种不同的上下文,矩阵中的每个 元素对应相关词和上下文的共现次数。在这种表示下,矩阵中的一行,就成为 了对应词的表示,这种表示描述了该词的上下文的分布。由于分布假说认为上 下文相似的词,其语义也相似,因此在这种表示下,两个词的语义相似度可以 直接转化为两个向量的空间距离。这类方法具体可以分为三个步骤: 一、选取上下文。最常见的有三种方法:第一种,将词所在的文档作为上 下文,形成“词-文档”矩阵 [56];第二种,将词附近上下文中的各个词(如上 下文窗口中的 5个词)作为上下文,形成“词-词”矩阵 [70, 90];第三种,将 词附近上下文各词组成的 n元词组(n-gram)作为上下文 [45]。在这三种方法 中,“词-文档”矩阵非常稀疏,而“词-词”矩阵相对较为稠密,效果一般好于 前者。“词-n元词组”相对“词-词”矩阵保留了词序信息,建模更精确,但由于 比前者更稀疏,实际效果不一定能超越前者。 二、确定矩阵中各元素的值。“词-上下文”共现矩阵根据其定义,里面各元 素的值应为词与对应的上下文的共现次数。然而直接使用原始共现次数作为矩 阵的值在大多数情况下效果并不好 [3],因此研究人员提出了多种加权和平滑方 法,最常用的有 tf-idf、PMI和直接取 log。 三、矩阵分解(可选)。在原始的“词-上下文”矩阵中,每个词表示为一个 非常高维(维度是不同上下文的总个数)且非常稀疏的向量,使用降维技术可以 达语义。http://www.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 111,
      "context" : "1 基于矩阵的分布表示 基于矩阵的分布表示通常又称为分布语义模型(distributional semantic models)[3],一些文献中也直接将其称作分布表示(distributional representation)[116]。 这类方法需要构建一个“词-上下文”矩阵,从矩阵中获取词的表示。在“词-上 下文”矩阵中,每行对应一个词,每列表示一种不同的上下文,矩阵中的每个 元素对应相关词和上下文的共现次数。在这种表示下,矩阵中的一行,就成为 了对应词的表示,这种表示描述了该词的上下文的分布。由于分布假说认为上 下文相似的词,其语义也相似,因此在这种表示下,两个词的语义相似度可以 直接转化为两个向量的空间距离。这类方法具体可以分为三个步骤: 一、选取上下文。最常见的有三种方法:第一种,将词所在的文档作为上 下文,形成“词-文档”矩阵 [56];第二种,将词附近上下文中的各个词(如上 下文窗口中的 5个词)作为上下文,形成“词-词”矩阵 [70, 90];第三种,将 词附近上下文各词组成的 n元词组(n-gram)作为上下文 [45]。在这三种方法 中,“词-文档”矩阵非常稀疏,而“词-词”矩阵相对较为稠密,效果一般好于 前者。“词-n元词组”相对“词-词”矩阵保留了词序信息,建模更精确,但由于 比前者更稀疏,实际效果不一定能超越前者。 二、确定矩阵中各元素的值。“词-上下文”共现矩阵根据其定义,里面各元 素的值应为词与对应的上下文的共现次数。然而直接使用原始共现次数作为矩 阵的值在大多数情况下效果并不好 [3],因此研究人员提出了多种加权和平滑方 法,最常用的有 tf-idf、PMI和直接取 log。 三、矩阵分解(可选)。在原始的“词-上下文”矩阵中,每个词表示为一个 非常高维(维度是不同上下文的总个数)且非常稀疏的向量,使用降维技术可以 达语义。http://www.",
      "startOffset" : 115,
      "endOffset" : 120
    }, {
      "referenceID" : 52,
      "context" : "1 基于矩阵的分布表示 基于矩阵的分布表示通常又称为分布语义模型(distributional semantic models)[3],一些文献中也直接将其称作分布表示(distributional representation)[116]。 这类方法需要构建一个“词-上下文”矩阵,从矩阵中获取词的表示。在“词-上 下文”矩阵中,每行对应一个词,每列表示一种不同的上下文,矩阵中的每个 元素对应相关词和上下文的共现次数。在这种表示下,矩阵中的一行,就成为 了对应词的表示,这种表示描述了该词的上下文的分布。由于分布假说认为上 下文相似的词,其语义也相似,因此在这种表示下,两个词的语义相似度可以 直接转化为两个向量的空间距离。这类方法具体可以分为三个步骤: 一、选取上下文。最常见的有三种方法:第一种,将词所在的文档作为上 下文,形成“词-文档”矩阵 [56];第二种,将词附近上下文中的各个词(如上 下文窗口中的 5个词)作为上下文,形成“词-词”矩阵 [70, 90];第三种,将 词附近上下文各词组成的 n元词组(n-gram)作为上下文 [45]。在这三种方法 中,“词-文档”矩阵非常稀疏,而“词-词”矩阵相对较为稠密,效果一般好于 前者。“词-n元词组”相对“词-词”矩阵保留了词序信息,建模更精确,但由于 比前者更稀疏,实际效果不一定能超越前者。 二、确定矩阵中各元素的值。“词-上下文”共现矩阵根据其定义,里面各元 素的值应为词与对应的上下文的共现次数。然而直接使用原始共现次数作为矩 阵的值在大多数情况下效果并不好 [3],因此研究人员提出了多种加权和平滑方 法,最常用的有 tf-idf、PMI和直接取 log。 三、矩阵分解(可选)。在原始的“词-上下文”矩阵中,每个词表示为一个 非常高维(维度是不同上下文的总个数)且非常稀疏的向量,使用降维技术可以 达语义。http://www.",
      "startOffset" : 377,
      "endOffset" : 381
    }, {
      "referenceID" : 66,
      "context" : "1 基于矩阵的分布表示 基于矩阵的分布表示通常又称为分布语义模型(distributional semantic models)[3],一些文献中也直接将其称作分布表示(distributional representation)[116]。 这类方法需要构建一个“词-上下文”矩阵,从矩阵中获取词的表示。在“词-上 下文”矩阵中,每行对应一个词,每列表示一种不同的上下文,矩阵中的每个 元素对应相关词和上下文的共现次数。在这种表示下,矩阵中的一行,就成为 了对应词的表示,这种表示描述了该词的上下文的分布。由于分布假说认为上 下文相似的词,其语义也相似,因此在这种表示下,两个词的语义相似度可以 直接转化为两个向量的空间距离。这类方法具体可以分为三个步骤: 一、选取上下文。最常见的有三种方法:第一种,将词所在的文档作为上 下文,形成“词-文档”矩阵 [56];第二种,将词附近上下文中的各个词(如上 下文窗口中的 5个词)作为上下文,形成“词-词”矩阵 [70, 90];第三种,将 词附近上下文各词组成的 n元词组(n-gram)作为上下文 [45]。在这三种方法 中,“词-文档”矩阵非常稀疏,而“词-词”矩阵相对较为稠密,效果一般好于 前者。“词-n元词组”相对“词-词”矩阵保留了词序信息,建模更精确,但由于 比前者更稀疏,实际效果不一定能超越前者。 二、确定矩阵中各元素的值。“词-上下文”共现矩阵根据其定义,里面各元 素的值应为词与对应的上下文的共现次数。然而直接使用原始共现次数作为矩 阵的值在大多数情况下效果并不好 [3],因此研究人员提出了多种加权和平滑方 法,最常用的有 tf-idf、PMI和直接取 log。 三、矩阵分解(可选)。在原始的“词-上下文”矩阵中,每个词表示为一个 非常高维(维度是不同上下文的总个数)且非常稀疏的向量,使用降维技术可以 达语义。http://www.",
      "startOffset" : 429,
      "endOffset" : 437
    }, {
      "referenceID" : 41,
      "context" : "1 基于矩阵的分布表示 基于矩阵的分布表示通常又称为分布语义模型(distributional semantic models)[3],一些文献中也直接将其称作分布表示(distributional representation)[116]。 这类方法需要构建一个“词-上下文”矩阵,从矩阵中获取词的表示。在“词-上 下文”矩阵中,每行对应一个词,每列表示一种不同的上下文,矩阵中的每个 元素对应相关词和上下文的共现次数。在这种表示下,矩阵中的一行,就成为 了对应词的表示,这种表示描述了该词的上下文的分布。由于分布假说认为上 下文相似的词,其语义也相似,因此在这种表示下,两个词的语义相似度可以 直接转化为两个向量的空间距离。这类方法具体可以分为三个步骤: 一、选取上下文。最常见的有三种方法:第一种,将词所在的文档作为上 下文,形成“词-文档”矩阵 [56];第二种,将词附近上下文中的各个词(如上 下文窗口中的 5个词)作为上下文,形成“词-词”矩阵 [70, 90];第三种,将 词附近上下文各词组成的 n元词组(n-gram)作为上下文 [45]。在这三种方法 中,“词-文档”矩阵非常稀疏,而“词-词”矩阵相对较为稠密,效果一般好于 前者。“词-n元词组”相对“词-词”矩阵保留了词序信息,建模更精确,但由于 比前者更稀疏,实际效果不一定能超越前者。 二、确定矩阵中各元素的值。“词-上下文”共现矩阵根据其定义,里面各元 素的值应为词与对应的上下文的共现次数。然而直接使用原始共现次数作为矩 阵的值在大多数情况下效果并不好 [3],因此研究人员提出了多种加权和平滑方 法,最常用的有 tf-idf、PMI和直接取 log。 三、矩阵分解(可选)。在原始的“词-上下文”矩阵中,每个词表示为一个 非常高维(维度是不同上下文的总个数)且非常稀疏的向量,使用降维技术可以 达语义。http://www.",
      "startOffset" : 474,
      "endOffset" : 478
    }, {
      "referenceID" : 1,
      "context" : "1 基于矩阵的分布表示 基于矩阵的分布表示通常又称为分布语义模型(distributional semantic models)[3],一些文献中也直接将其称作分布表示(distributional representation)[116]。 这类方法需要构建一个“词-上下文”矩阵,从矩阵中获取词的表示。在“词-上 下文”矩阵中,每行对应一个词,每列表示一种不同的上下文,矩阵中的每个 元素对应相关词和上下文的共现次数。在这种表示下,矩阵中的一行,就成为 了对应词的表示,这种表示描述了该词的上下文的分布。由于分布假说认为上 下文相似的词,其语义也相似,因此在这种表示下,两个词的语义相似度可以 直接转化为两个向量的空间距离。这类方法具体可以分为三个步骤: 一、选取上下文。最常见的有三种方法:第一种,将词所在的文档作为上 下文,形成“词-文档”矩阵 [56];第二种,将词附近上下文中的各个词(如上 下文窗口中的 5个词)作为上下文,形成“词-词”矩阵 [70, 90];第三种,将 词附近上下文各词组成的 n元词组(n-gram)作为上下文 [45]。在这三种方法 中,“词-文档”矩阵非常稀疏,而“词-词”矩阵相对较为稠密,效果一般好于 前者。“词-n元词组”相对“词-词”矩阵保留了词序信息,建模更精确,但由于 比前者更稀疏,实际效果不一定能超越前者。 二、确定矩阵中各元素的值。“词-上下文”共现矩阵根据其定义,里面各元 素的值应为词与对应的上下文的共现次数。然而直接使用原始共现次数作为矩 阵的值在大多数情况下效果并不好 [3],因此研究人员提出了多种加权和平滑方 法,最常用的有 tf-idf、PMI和直接取 log。 三、矩阵分解(可选)。在原始的“词-上下文”矩阵中,每个词表示为一个 非常高维(维度是不同上下文的总个数)且非常稀疏的向量,使用降维技术可以 达语义。http://www.",
      "startOffset" : 668,
      "endOffset" : 671
    }, {
      "referenceID" : 20,
      "context" : "将这一高维稀疏向量压缩成低维稠密向量。降维技术可以减少噪声带来的影响, 但也可能损失一部分信息。最常用的分解技术包括奇异值分解(SVD)、非负矩 阵分解(NMF)、典型关联分析(Canonical Correlation Analysis,CCA)[22, 23]、 Hellinger PCA(HPCA)[59]。 基于矩阵的分布表示在这些步骤的基础上,衍生出了若干不同方法,如经 典的 LSA [56]就是使用“词-文档”矩阵,tf-idf作为矩阵元素的值,并使用 SVD 分解,得到词的低维向量表示。在这类方法中,最新的为 GloVe模型 [90],下 文简单介绍这一模型。",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "将这一高维稀疏向量压缩成低维稠密向量。降维技术可以减少噪声带来的影响, 但也可能损失一部分信息。最常用的分解技术包括奇异值分解(SVD)、非负矩 阵分解(NMF)、典型关联分析(Canonical Correlation Analysis,CCA)[22, 23]、 Hellinger PCA(HPCA)[59]。 基于矩阵的分布表示在这些步骤的基础上,衍生出了若干不同方法,如经 典的 LSA [56]就是使用“词-文档”矩阵,tf-idf作为矩阵元素的值,并使用 SVD 分解,得到词的低维向量表示。在这类方法中,最新的为 GloVe模型 [90],下 文简单介绍这一模型。",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 55,
      "context" : "将这一高维稀疏向量压缩成低维稠密向量。降维技术可以减少噪声带来的影响, 但也可能损失一部分信息。最常用的分解技术包括奇异值分解(SVD)、非负矩 阵分解(NMF)、典型关联分析(Canonical Correlation Analysis,CCA)[22, 23]、 Hellinger PCA(HPCA)[59]。 基于矩阵的分布表示在这些步骤的基础上,衍生出了若干不同方法,如经 典的 LSA [56]就是使用“词-文档”矩阵,tf-idf作为矩阵元素的值,并使用 SVD 分解,得到词的低维向量表示。在这类方法中,最新的为 GloVe模型 [90],下 文简单介绍这一模型。",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 52,
      "context" : "将这一高维稀疏向量压缩成低维稠密向量。降维技术可以减少噪声带来的影响, 但也可能损失一部分信息。最常用的分解技术包括奇异值分解(SVD)、非负矩 阵分解(NMF)、典型关联分析(Canonical Correlation Analysis,CCA)[22, 23]、 Hellinger PCA(HPCA)[59]。 基于矩阵的分布表示在这些步骤的基础上,衍生出了若干不同方法,如经 典的 LSA [56]就是使用“词-文档”矩阵,tf-idf作为矩阵元素的值,并使用 SVD 分解,得到词的低维向量表示。在这类方法中,最新的为 GloVe模型 [90],下 文简单介绍这一模型。",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "总体上看,GloVe模型是一种对“词-词”矩阵进行分解从而得到词表示的 方法。矩阵第 i 行第 j 列的值为词 vi 与词 vj 在语料中的共现次数 xij 的对数。 在矩阵分解步骤,GloVe模型借鉴了推荐系统中基于隐因子分解(Latent Factor Model)的方法 [4, 88],在计算重构误差时,只考虑共现次数非零的矩阵元素, 同时对矩阵中的行和列加入了偏移项。具体为最小化下式: ∑",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 84,
      "context" : "总体上看,GloVe模型是一种对“词-词”矩阵进行分解从而得到词表示的 方法。矩阵第 i 行第 j 列的值为词 vi 与词 vj 在语料中的共现次数 xij 的对数。 在矩阵分解步骤,GloVe模型借鉴了推荐系统中基于隐因子分解(Latent Factor Model)的方法 [4, 88],在计算重构误差时,只考虑共现次数非零的矩阵元素, 同时对矩阵中的行和列加入了偏移项。具体为最小化下式: ∑",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 86,
      "context" : "基于聚类的分布表示又称作分布聚类(distributional clustering)[91],这类 方法通过聚类手段构建词与其上下文之间的关系。其中最经典的方法是布朗聚 类(Brown clustering)[12]。布朗聚类是一种层级聚类方法,聚类结果为每个词 的多层类别体系。因此可以根据两个词的公共类别判断这两个词的语义相似度。",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "基于聚类的分布表示又称作分布聚类(distributional clustering)[91],这类 方法通过聚类手段构建词与其上下文之间的关系。其中最经典的方法是布朗聚 类(Brown clustering)[12]。布朗聚类是一种层级聚类方法,聚类结果为每个词 的多层类别体系。因此可以根据两个词的公共类别判断这两个词的语义相似度。",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 65,
      "context" : "布朗聚类只考虑了相邻词之间的关系,也就是说,每个词只使用它的上一个词, 作为上下文信息。 除了布朗聚类以外,还有若干基于聚类的表示方法 [69, 91]。由于这类方法 不是本文的重点,在此不再赘述。",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 86,
      "context" : "布朗聚类只考虑了相邻词之间的关系,也就是说,每个词只使用它的上一个词, 作为上下文信息。 除了布朗聚类以外,还有若干基于聚类的表示方法 [69, 91]。由于这类方法 不是本文的重点,在此不再赘述。",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 111,
      "context" : "3 基于神经网络的分布表示(词向量) 基于神经网络的分布表示一般称为词向量、词嵌入(word embedding)或分 布式表示(distributed representation)[116]。神经网络词向量表示技术通过神经 网络技术对上下文,以及上下文与目标词之间的关系进行建模。由于神经网络 较为灵活,这类方法的最大优势在于可以表示复杂的上下文。在前面基于矩阵 的分布表示方法中,最常用的上下文是词。如果使用包含词序信息的 n-gram作 为上下文,当 n增加时,n-gram的总数会呈指数级增长,此时会遇到维数灾难 问题。而神经网络在表示 n-gram 时,可以通过一些组合方式对 n 个词进行组 合,参数个数仅以线性速度增长。有了这一优势,神经网络模型可以对更复杂 的上下文进行建模,在词向量中包含更丰富的语义信息。下一节将详细介绍不 同神经网络模型是如何对上下文以及上下文与目标词之间的关系进行建模的。",
      "startOffset" : 92,
      "endOffset" : 97
    }, {
      "referenceID" : 59,
      "context" : "5估计 n元条件概率时,就会遇到数据稀疏问题,导致估算结果不 准确。因此,一般在百万词级别的语料中,三元模型是比较常用的选择 [63],同 时也需要配合相应的平滑算法,进一步降低数据稀疏带来的影响 [33, 51]。 为了更好地解决 n元模型估算概率时遇到的数据稀疏问题,神经网络语言 模型应运而生。",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 30,
      "context" : "5估计 n元条件概率时,就会遇到数据稀疏问题,导致估算结果不 准确。因此,一般在百万词级别的语料中,三元模型是比较常用的选择 [63],同 时也需要配合相应的平滑算法,进一步降低数据稀疏带来的影响 [33, 51]。 为了更好地解决 n元模型估算概率时遇到的数据稀疏问题,神经网络语言 模型应运而生。",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 47,
      "context" : "5估计 n元条件概率时,就会遇到数据稀疏问题,导致估算结果不 准确。因此,一般在百万词级别的语料中,三元模型是比较常用的选择 [63],同 时也需要配合相应的平滑算法,进一步降低数据稀疏带来的影响 [33, 51]。 为了更好地解决 n元模型估算概率时遇到的数据稀疏问题,神经网络语言 模型应运而生。",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 119,
      "context" : "2 神经网络语言模型(NNLM) Xu 等人在 2000 年首次尝试使用神经网络求解二元语言模型 [124]。2001 年,Bengio等人正式提出神经网络语言模型(Neural Network Language Model, NNLM)[6, 7]。该模型在学习语言模型的同时,也得到了词向量。 NNLM同样也是对 n元语言模型进行建模,估算 P (wi | wi−(n−1), .",
      "startOffset" : 49,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "2 神经网络语言模型(NNLM) Xu 等人在 2000 年首次尝试使用神经网络求解二元语言模型 [124]。2001 年,Bengio等人正式提出神经网络语言模型(Neural Network Language Model, NNLM)[6, 7]。该模型在学习语言模型的同时,也得到了词向量。 NNLM同样也是对 n元语言模型进行建模,估算 P (wi | wi−(n−1), .",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "2 神经网络语言模型(NNLM) Xu 等人在 2000 年首次尝试使用神经网络求解二元语言模型 [124]。2001 年,Bengio等人正式提出神经网络语言模型(Neural Network Language Model, NNLM)[6, 7]。该模型在学习语言模型的同时,也得到了词向量。 NNLM同样也是对 n元语言模型进行建模,估算 P (wi | wi−(n−1), .",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "训练时,神经网络语言模型使用随机梯度下降法 [10]来优化上述训练目标。 每次迭代,随机从语料 D中选取一段文本 wi−(n−1), .",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 75,
      "context" : "3 log双线性语言模型(LBL) 2007年,Mnih和Hinton在神经网络语言模型(NNLM)的基础上提出了 log 双线性语言模型(Log-Bilinear Language Model,LBL)[79]。LBL与 NNLM的 区别正如它们的名字所示,LBL的模型结构是一个 log双线性结构;而 NNLM 的模型结构为神经网络结构。具体来讲,LBL模型的能量函数为: E(wi;wi−(n−1):i−1) = b (2) + e(wi) Tb(1)+ e(wi) TH [ e(wi−(n−1)); .",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 76,
      "context" : "11)主要 有两个区别。一、LBL模型中,没有非线性的激活函数 tanh,而由于 NNLM是 非线性的神经网络结构,激活函数必不可少;二、LBL模型中,只有一份词向 量 e,也就是说,无论一个词是作为上下文,还是作为目标词,使用的是同一份 词向量。其中第二点(只有一份词向量),只在原版的 LBL模型中存在,后续的 改进工作均不包含这一特点。 之后的几年中,Mnih等人在 LBL模型的基础上做了一系列改进工作。其 中最重要的模型有两个:层级 log双线性语言模型(Hierarchical LBL,HLBL) [80]和基于向量的逆语言模型(inverse vector LBL,ivLBL)[81]。以下分别介 绍这两个模型所用的技术。",
      "startOffset" : 256,
      "endOffset" : 260
    }, {
      "referenceID" : 77,
      "context" : "11)主要 有两个区别。一、LBL模型中,没有非线性的激活函数 tanh,而由于 NNLM是 非线性的神经网络结构,激活函数必不可少;二、LBL模型中,只有一份词向 量 e,也就是说,无论一个词是作为上下文,还是作为目标词,使用的是同一份 词向量。其中第二点(只有一份词向量),只在原版的 LBL模型中存在,后续的 改进工作均不包含这一特点。 之后的几年中,Mnih等人在 LBL模型的基础上做了一系列改进工作。其 中最重要的模型有两个:层级 log双线性语言模型(Hierarchical LBL,HLBL) [80]和基于向量的逆语言模型(inverse vector LBL,ivLBL)[81]。以下分别介 绍这两个模型所用的技术。",
      "startOffset" : 297,
      "endOffset" : 301
    }, {
      "referenceID" : 76,
      "context" : "层级 softmax HLBL模型 [80]采用了 Bengio在 2005年提出的层级 softmax函数 [82],加 速了目标层的求解。传统的 softmax函数如公式 2.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 78,
      "context" : "层级 softmax HLBL模型 [80]采用了 Bengio在 2005年提出的层级 softmax函数 [82],加 速了目标层的求解。传统的 softmax函数如公式 2.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 77,
      "context" : "2013年提出的基于向量的逆语言模型(ivLBL)[81]虽然名字上仍然继承了 log双线性语言模型(LBL),实际上已经抛弃了 log双线性结构,转而采用了向 量点积结构。该模型在 Skip-gram模型(见 2.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 31,
      "context" : "6节)的基础上,使用噪声对比 估算(noise-contrastive estimation,NCE)[34]加速 softmax的估计,将 softmax 的复杂度降低到常数级别。",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 68,
      "context" : "4对 其进行简化 [72, 74]。因此,RNNLM可以利用所有的上文信息,预测下一个词, 其模型结构如图 2-2所示。",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 70,
      "context" : "4对 其进行简化 [72, 74]。因此,RNNLM可以利用所有的上文信息,预测下一个词, 其模型结构如图 2-2所示。",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "5 C&W模型 与前面的三个基于语言模型的词向量生成方法不同,Collobert和Weston在 2008年提出的 C&W模型 [17],是第一个直接以生成词向量为目标的模型。",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : ", wi+(n−1)/2,一般 n 为奇数,以保证上文和下文的词数一致;w为序列中的中间词,即 wi,在该模 型中为目标词;c表示目标词 w的上下文;w′ 为字典中的某一个词。模型采用 pairwise的方式 [16]对文本片段进行优化,希望正样本的打分要比负样本的打 分至少高 1分。正样本 (w, c)来自语料,而负样本 (w′, c)则是将正样本序列中 的中间词替换成其它词。形式化地,目标词 w和上下文 c分别为:",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 69,
      "context" : "Mikolov等人在 2013年的文献 [73]中,同时提出了CBOW(Continuous Bagof-Words)和 Skip-gram模型。他们设计两个模型的主要目的是希望用更高效的 方法获取词向量。因此,他们根据前人在 NNLM、RNNLM和 C&W模型上的 经验,简化现有模型,保留核心部分,得到了这两个模型。",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 69,
      "context" : "2Mokolov在文献 [73]中使用的是求和,但是在后来 word2vec的工具包中将求和改进为平均值。",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 69,
      "context" : "3本文为了各模型之间的一致性,将 Skip-gram模型描述成通过上下文预测目标词,而在 Skip-gram的 论文 [73]中将模型描述成通过目标词预测上下文。由于模型需要遍历整个语料,任意一个窗口中的两个 词 wa, wb 都需要计算 P (wa|wb) + P (wb|wa),因此这两种描述方式是等价的。",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 71,
      "context" : "Mikolov等人在 2013年提出了负采样技术(negative sampling),进一步提 升了最后一层的效率 [75]。负采样技术借鉴了 C&W模型采用的构造负样本的 方法,还参考了 ivLBL模型所用的 NCE方法,最后构造出了一个优化目标,最 大化正样本的似然,同时最小化负样本的似然。 负采样技术与 C&W 模型中相应部分的区别主要是,负采样技术不采用 pairwise 的方式训练,因此,一个正样本可以对应多个负样本,Mikolov 等人 在实验中论述了使用多个负样本(一般选 5)能有效提升模型的性能。 负采样技术与 NCE技术的主要区别是,负采样技术仅仅是优化正负样本的 似然,而不对输出层做概率归一化。NCE技术则是通过噪声样本对概率进行估 计。在实验中,Mikolov等人也论述了负采样技术相比 NCE技术少了一些约束, 对于生成词向量,是有帮助的 [75]。",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 71,
      "context" : "Mikolov等人在 2013年提出了负采样技术(negative sampling),进一步提 升了最后一层的效率 [75]。负采样技术借鉴了 C&W模型采用的构造负样本的 方法,还参考了 ivLBL模型所用的 NCE方法,最后构造出了一个优化目标,最 大化正样本的似然,同时最小化负样本的似然。 负采样技术与 C&W 模型中相应部分的区别主要是,负采样技术不采用 pairwise 的方式训练,因此,一个正样本可以对应多个负样本,Mikolov 等人 在实验中论述了使用多个负样本(一般选 5)能有效提升模型的性能。 负采样技术与 NCE技术的主要区别是,负采样技术仅仅是优化正负样本的 似然,而不对输出层做概率归一化。NCE技术则是通过噪声样本对概率进行估 计。在实验中,Mikolov等人也论述了负采样技术相比 NCE技术少了一些约束, 对于生成词向量,是有帮助的 [75]。",
      "startOffset" : 387,
      "endOffset" : 391
    }, {
      "referenceID" : 71,
      "context" : "在大规模语料中,高频词通常就是停用词(如英语中的“the”、汉语中的 “的”)。一方面,这些高频词只能带来非常少量的语义信息,比如几乎所有的词 都会和“的”共同出现,但是并不能说明这些词的语义都相似。另一方面,训 练高频词本身占据了大量的时间,但在迭代过程中,这些高频词的词向量变化 并不大。Mikolov等人为了解决这一问题,提出了二次采样技术(subsampling) [75],具体而言,如果词 w在语料中的出现频率 f(w)大于阈值 t,则有 P (w)的 概率在训练时跳过这个词。",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 71,
      "context" : "word2vec工具包的实现与论文 [75]中的公式稍有不同:",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 61,
      "context" : "无论采用论文中的公式,还是工具包实现的公式,词出现的越频繁,就越有可 能在训练中被跳过。这种二次采样技术不仅可以提升词向量的训练速度,大多 数情况下也能提升词向量的性能 [65, 75]。",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 71,
      "context" : "无论采用论文中的公式,还是工具包实现的公式,词出现的越频繁,就越有可 能在训练中被跳过。这种二次采样技术不仅可以提升词向量的训练速度,大多 数情况下也能提升词向量的性能 [65, 75]。",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 69,
      "context" : "从模型的时间复杂度上看,文献 [73]对早期的若干模型均有分析,因此这 里只做简要介绍。基于神经网络的词向量模型均通过扫描语料中的每一个词, 取该词以及其周围的上下文作为一个样本。因此对比这些模型时,可以只分析 训练一个样本的时间复杂度。对于原版的 NNLM和 LBL模型,训练一个样本 需要的计算为输入层到隐藏层,隐藏层到输出层这两个矩阵运算,其复杂度为 O((win−1)|e|×|h|+ |h|×|V|)。对于Order模型,由于省略了隐藏层,其复杂度 为 O((win− 1)|e| × |V|)。对于 CBOW和 Skip-gram模型,由于进一步忽略了词 序信息,其复杂度为 O(|e| × |V|)。C&W模型在结构中省去了对目标词的预测, 其复杂度仅为O((win+1)|e|× |h|)。对于上述各个模型,如果采用层级 softmax 函数做输出层的优化,式子中的 |V|可以加速到 log(|V|),而如果使用噪声对比",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 32,
      "context" : "CBOW、Skip-gram和 Order模型相对于其它神经网络模型,均去除了隐藏 层。如果有隐藏层,输入层的上下文表示可以通过一个线性变换或者非线性变 换得到隐藏层,这种操作属于语义组合操作 [36]。如果没有隐藏层,上下文词 之间的关系为普通的线性叠加关系,会丢失部分语义信息。 CBOW模型和 Skip-gram模型还通过不同的方法去掉了其它神经网络模型 中保留的词序信息。虽然这两个模型根据上下文各词与目标词之间的距离采用 了加权策略,可以少量保留词序信息,但是这种策略相对于词向量顺序拼接的 方式,可以认为几乎忽略了词序信息。 这些模型采取的简化策略,使其有更高的运行效率,可以在更大规模的语 料上训练词向量,但是模型本身对语义捕获的能力也有一些降低。这些简化究 竟对词向量的性能有多少影响,需要通过实验来说明。",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 111,
      "context" : "本文将分布表示归类为基于矩阵的分布表示、基于聚类的分布表示和基于 神经网络的分布表示。这种分类方式基本沿用了 Turian 等人的分类 [116]。在 Turian的分类中,这三种方法分别被称作“distributional representation”、“clusteringbased word representation”和“distributed representation”。本文和 Turian的区别 在于,本文将这三种方法统称为“distributional representation”,而将其中的第一 种方法称作基于矩阵的分布表示。 Baroni 等人的文献 [3] 分析了语义向量模型,因此没有考虑聚类模型。文 中,Baroni将基于矩阵的分布表示称作计数(count)方法,而将神经网络模型 称作预测(predict)方法。 Sahlgren的博士论文 [98]研究了词空间模型(word space model),具体分析 了基于矩阵的分布表示中的“词-文档”矩阵和“词-词”矩阵,并得到结论:“词文档”矩阵主要构建了词的组合关系(syntagmatic),而“词-词”矩阵主要构建 了词的替换关系(paradigmatic)。 Turney和 Pantel的工作 [118]总结了向量空间模型(vector space model),他 们将向量空间模型分为“词-文档”矩阵、“词-上下文”矩阵和“词对-模板”矩 阵这三种,认为“词-文档”矩阵适合用来表示文档,“词-词”矩阵适合表示词, 而“词对-模板”矩阵用来表示词对之间的关系。",
      "startOffset" : 68,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "本文将分布表示归类为基于矩阵的分布表示、基于聚类的分布表示和基于 神经网络的分布表示。这种分类方式基本沿用了 Turian 等人的分类 [116]。在 Turian的分类中,这三种方法分别被称作“distributional representation”、“clusteringbased word representation”和“distributed representation”。本文和 Turian的区别 在于,本文将这三种方法统称为“distributional representation”,而将其中的第一 种方法称作基于矩阵的分布表示。 Baroni 等人的文献 [3] 分析了语义向量模型,因此没有考虑聚类模型。文 中,Baroni将基于矩阵的分布表示称作计数(count)方法,而将神经网络模型 称作预测(predict)方法。 Sahlgren的博士论文 [98]研究了词空间模型(word space model),具体分析 了基于矩阵的分布表示中的“词-文档”矩阵和“词-词”矩阵,并得到结论:“词文档”矩阵主要构建了词的组合关系(syntagmatic),而“词-词”矩阵主要构建 了词的替换关系(paradigmatic)。 Turney和 Pantel的工作 [118]总结了向量空间模型(vector space model),他 们将向量空间模型分为“词-文档”矩阵、“词-上下文”矩阵和“词对-模板”矩 阵这三种,认为“词-文档”矩阵适合用来表示文档,“词-词”矩阵适合表示词, 而“词对-模板”矩阵用来表示词对之间的关系。",
      "startOffset" : 293,
      "endOffset" : 296
    }, {
      "referenceID" : 93,
      "context" : "本文将分布表示归类为基于矩阵的分布表示、基于聚类的分布表示和基于 神经网络的分布表示。这种分类方式基本沿用了 Turian 等人的分类 [116]。在 Turian的分类中,这三种方法分别被称作“distributional representation”、“clusteringbased word representation”和“distributed representation”。本文和 Turian的区别 在于,本文将这三种方法统称为“distributional representation”,而将其中的第一 种方法称作基于矩阵的分布表示。 Baroni 等人的文献 [3] 分析了语义向量模型,因此没有考虑聚类模型。文 中,Baroni将基于矩阵的分布表示称作计数(count)方法,而将神经网络模型 称作预测(predict)方法。 Sahlgren的博士论文 [98]研究了词空间模型(word space model),具体分析 了基于矩阵的分布表示中的“词-文档”矩阵和“词-词”矩阵,并得到结论:“词文档”矩阵主要构建了词的组合关系(syntagmatic),而“词-词”矩阵主要构建 了词的替换关系(paradigmatic)。 Turney和 Pantel的工作 [118]总结了向量空间模型(vector space model),他 们将向量空间模型分为“词-文档”矩阵、“词-上下文”矩阵和“词对-模板”矩 阵这三种,认为“词-文档”矩阵适合用来表示文档,“词-词”矩阵适合表示词, 而“词对-模板”矩阵用来表示词对之间的关系。",
      "startOffset" : 392,
      "endOffset" : 396
    }, {
      "referenceID" : 113,
      "context" : "本文将分布表示归类为基于矩阵的分布表示、基于聚类的分布表示和基于 神经网络的分布表示。这种分类方式基本沿用了 Turian 等人的分类 [116]。在 Turian的分类中,这三种方法分别被称作“distributional representation”、“clusteringbased word representation”和“distributed representation”。本文和 Turian的区别 在于,本文将这三种方法统称为“distributional representation”,而将其中的第一 种方法称作基于矩阵的分布表示。 Baroni 等人的文献 [3] 分析了语义向量模型,因此没有考虑聚类模型。文 中,Baroni将基于矩阵的分布表示称作计数(count)方法,而将神经网络模型 称作预测(predict)方法。 Sahlgren的博士论文 [98]研究了词空间模型(word space model),具体分析 了基于矩阵的分布表示中的“词-文档”矩阵和“词-词”矩阵,并得到结论:“词文档”矩阵主要构建了词的组合关系(syntagmatic),而“词-词”矩阵主要构建 了词的替换关系(paradigmatic)。 Turney和 Pantel的工作 [118]总结了向量空间模型(vector space model),他 们将向量空间模型分为“词-文档”矩阵、“词-上下文”矩阵和“词对-模板”矩 阵这三种,认为“词-文档”矩阵适合用来表示文档,“词-词”矩阵适合表示词, 而“词对-模板”矩阵用来表示词对之间的关系。",
      "startOffset" : 549,
      "endOffset" : 554
    }, {
      "referenceID" : 52,
      "context" : "名称 上下文 上下文与目标 词之间的建模 (技术手段) LSA/LSI [56] 文档 矩阵 HAL [70] 词 GloVe [90] 词 Jones & Mewhort [45] n-gram Brown Clustering [12] 词 聚类 Skip-gram [73] 词",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 66,
      "context" : "名称 上下文 上下文与目标 词之间的建模 (技术手段) LSA/LSI [56] 文档 矩阵 HAL [70] 词 GloVe [90] 词 Jones & Mewhort [45] n-gram Brown Clustering [12] 词 聚类 Skip-gram [73] 词",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 41,
      "context" : "名称 上下文 上下文与目标 词之间的建模 (技术手段) LSA/LSI [56] 文档 矩阵 HAL [70] 词 GloVe [90] 词 Jones & Mewhort [45] n-gram Brown Clustering [12] 词 聚类 Skip-gram [73] 词",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "名称 上下文 上下文与目标 词之间的建模 (技术手段) LSA/LSI [56] 文档 矩阵 HAL [70] 词 GloVe [90] 词 Jones & Mewhort [45] n-gram Brown Clustering [12] 词 聚类 Skip-gram [73] 词",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 69,
      "context" : "名称 上下文 上下文与目标 词之间的建模 (技术手段) LSA/LSI [56] 文档 矩阵 HAL [70] 词 GloVe [90] 词 Jones & Mewhort [45] n-gram Brown Clustering [12] 词 聚类 Skip-gram [73] 词",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 69,
      "context" : "神经网络 CBOW [73] n-gram(加权) Order(2.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 75,
      "context" : "7小节) n-gram(线性组合) LBL [79] n-gram(线性组合) NNLM [7] n-gram(非线性组合) C&W [17] n-gram(非线性组合)",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "7小节) n-gram(线性组合) LBL [79] n-gram(线性组合) NNLM [7] n-gram(非线性组合) C&W [17] n-gram(非线性组合)",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "7小节) n-gram(线性组合) LBL [79] n-gram(线性组合) NNLM [7] n-gram(非线性组合) C&W [17] n-gram(非线性组合)",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 60,
      "context" : "基于矩阵的模型和基于神经网络的模型最终都可以得到词的低维向量表 示,这两类方法是否存在一定的联系?答案是肯定的。Levy 和 Goldberg 证明 了对“词-词”矩阵做 SVD分解与 Skip-gram模型配合负采样技术优化具有相同 的最优解 [64]。本文证明了“词-词”矩阵分解与 Skip-gram模型的原始形式具 有相同的最优解,具体证明过程见附录 A。遗憾的是,这两个等价关系需要一 个前提条件:最优解可以取到。在实际优化过程中,一般的优化方法只能不断 逼近最优解,而很难达到最优解。尽管理论上这两者的最终优化结果应当一致, 但是在实践中还是有较大的差异,文献 [3, 64, 65]比较了这些模型在实践中的 差异。2015年 Li等人在文献 [67]中证明了“词-词”矩阵分解与 Skip-gram模型 可以完全等价。这项工作抛弃了 SVD所用的均方根误差来作为矩阵分解的重构",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "基于矩阵的模型和基于神经网络的模型最终都可以得到词的低维向量表 示,这两类方法是否存在一定的联系?答案是肯定的。Levy 和 Goldberg 证明 了对“词-词”矩阵做 SVD分解与 Skip-gram模型配合负采样技术优化具有相同 的最优解 [64]。本文证明了“词-词”矩阵分解与 Skip-gram模型的原始形式具 有相同的最优解,具体证明过程见附录 A。遗憾的是,这两个等价关系需要一 个前提条件:最优解可以取到。在实际优化过程中,一般的优化方法只能不断 逼近最优解,而很难达到最优解。尽管理论上这两者的最终优化结果应当一致, 但是在实践中还是有较大的差异,文献 [3, 64, 65]比较了这些模型在实践中的 差异。2015年 Li等人在文献 [67]中证明了“词-词”矩阵分解与 Skip-gram模型 可以完全等价。这项工作抛弃了 SVD所用的均方根误差来作为矩阵分解的重构",
      "startOffset" : 287,
      "endOffset" : 298
    }, {
      "referenceID" : 60,
      "context" : "基于矩阵的模型和基于神经网络的模型最终都可以得到词的低维向量表 示,这两类方法是否存在一定的联系?答案是肯定的。Levy 和 Goldberg 证明 了对“词-词”矩阵做 SVD分解与 Skip-gram模型配合负采样技术优化具有相同 的最优解 [64]。本文证明了“词-词”矩阵分解与 Skip-gram模型的原始形式具 有相同的最优解,具体证明过程见附录 A。遗憾的是,这两个等价关系需要一 个前提条件:最优解可以取到。在实际优化过程中,一般的优化方法只能不断 逼近最优解,而很难达到最优解。尽管理论上这两者的最终优化结果应当一致, 但是在实践中还是有较大的差异,文献 [3, 64, 65]比较了这些模型在实践中的 差异。2015年 Li等人在文献 [67]中证明了“词-词”矩阵分解与 Skip-gram模型 可以完全等价。这项工作抛弃了 SVD所用的均方根误差来作为矩阵分解的重构",
      "startOffset" : 287,
      "endOffset" : 298
    }, {
      "referenceID" : 61,
      "context" : "基于矩阵的模型和基于神经网络的模型最终都可以得到词的低维向量表 示,这两类方法是否存在一定的联系?答案是肯定的。Levy 和 Goldberg 证明 了对“词-词”矩阵做 SVD分解与 Skip-gram模型配合负采样技术优化具有相同 的最优解 [64]。本文证明了“词-词”矩阵分解与 Skip-gram模型的原始形式具 有相同的最优解,具体证明过程见附录 A。遗憾的是,这两个等价关系需要一 个前提条件:最优解可以取到。在实际优化过程中,一般的优化方法只能不断 逼近最优解,而很难达到最优解。尽管理论上这两者的最终优化结果应当一致, 但是在实践中还是有较大的差异,文献 [3, 64, 65]比较了这些模型在实践中的 差异。2015年 Li等人在文献 [67]中证明了“词-词”矩阵分解与 Skip-gram模型 可以完全等价。这项工作抛弃了 SVD所用的均方根误差来作为矩阵分解的重构",
      "startOffset" : 287,
      "endOffset" : 298
    }, {
      "referenceID" : 63,
      "context" : "基于矩阵的模型和基于神经网络的模型最终都可以得到词的低维向量表 示,这两类方法是否存在一定的联系?答案是肯定的。Levy 和 Goldberg 证明 了对“词-词”矩阵做 SVD分解与 Skip-gram模型配合负采样技术优化具有相同 的最优解 [64]。本文证明了“词-词”矩阵分解与 Skip-gram模型的原始形式具 有相同的最优解,具体证明过程见附录 A。遗憾的是,这两个等价关系需要一 个前提条件:最优解可以取到。在实际优化过程中,一般的优化方法只能不断 逼近最优解,而很难达到最优解。尽管理论上这两者的最终优化结果应当一致, 但是在实践中还是有较大的差异,文献 [3, 64, 65]比较了这些模型在实践中的 差异。2015年 Li等人在文献 [67]中证明了“词-词”矩阵分解与 Skip-gram模型 可以完全等价。这项工作抛弃了 SVD所用的均方根误差来作为矩阵分解的重构",
      "startOffset" : 328,
      "endOffset" : 332
    }, {
      "referenceID" : 72,
      "context" : "1 引言 词向量模型可以从大规模无标注语料中自动学习到句法和语义信息 [76]。 近年来,大量研究者投身到设计新型的词向量模型中,基于词向量的神经网络 模型也为多项自然语言处理任务带来了性能的提升,甚至在多项任务中达到了 目前最好的效果。现有的词向量模型在提出时,作者均声称他们的方法比前人 的方法好,然而这些工作在评价词向量时,会挑选比较局限的评价指标,有时 候甚至使用了不同的训练语料,因此其评价结果可能会缺乏借鉴意义。现有的 词向量模型纷繁复杂,这些模型究竟哪个更好,或者在什么情况下更适合用哪 个模型,现有的研究工作仍然缺乏相应的比较分析。本章主要分析词向量生成 中的几个关键点,包括词向量模型的选择、语料的选择以及训练参数的选取,对 各现有的词向量模型进行全面的比较。",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 69,
      "context" : "模型 上下文的表示 目标词与其上下文关系 Skip-gram [73] 上下文中某一个词的词向量 上下文预测目标词 CBOW [73] 上下文各词词向量的平均值 上下文预测目标词 Order 上下文各词词向量的拼接 上下文预测目标词 LBL [79] 上下文各词的语义组合 上下文预测目标词 NNLM [7] 上下文各词的语义组合 上下文预测目标词 C&W [17] 上下文各词与目标词的语义组合1 上下文和目标词联合打分",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 69,
      "context" : "模型 上下文的表示 目标词与其上下文关系 Skip-gram [73] 上下文中某一个词的词向量 上下文预测目标词 CBOW [73] 上下文各词词向量的平均值 上下文预测目标词 Order 上下文各词词向量的拼接 上下文预测目标词 LBL [79] 上下文各词的语义组合 上下文预测目标词 NNLM [7] 上下文各词的语义组合 上下文预测目标词 C&W [17] 上下文各词与目标词的语义组合1 上下文和目标词联合打分",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 75,
      "context" : "模型 上下文的表示 目标词与其上下文关系 Skip-gram [73] 上下文中某一个词的词向量 上下文预测目标词 CBOW [73] 上下文各词词向量的平均值 上下文预测目标词 Order 上下文各词词向量的拼接 上下文预测目标词 LBL [79] 上下文各词的语义组合 上下文预测目标词 NNLM [7] 上下文各词的语义组合 上下文预测目标词 C&W [17] 上下文各词与目标词的语义组合1 上下文和目标词联合打分",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "模型 上下文的表示 目标词与其上下文关系 Skip-gram [73] 上下文中某一个词的词向量 上下文预测目标词 CBOW [73] 上下文各词词向量的平均值 上下文预测目标词 Order 上下文各词词向量的拼接 上下文预测目标词 LBL [79] 上下文各词的语义组合 上下文预测目标词 NNLM [7] 上下文各词的语义组合 上下文预测目标词 C&W [17] 上下文各词与目标词的语义组合1 上下文和目标词联合打分",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 15,
      "context" : "模型 上下文的表示 目标词与其上下文关系 Skip-gram [73] 上下文中某一个词的词向量 上下文预测目标词 CBOW [73] 上下文各词词向量的平均值 上下文预测目标词 Order 上下文各词词向量的拼接 上下文预测目标词 LBL [79] 上下文各词的语义组合 上下文预测目标词 NNLM [7] 上下文各词的语义组合 上下文预测目标词 C&W [17] 上下文各词与目标词的语义组合1 上下文和目标词联合打分",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 69,
      "context" : "如,Skip-gram模型 [73]选取了目标词 w上下文中的某一个词的词向量,作为其 上下文表示。CBOW模型采用上下文各词词向量的平均值作为上下文表示。这 两个模型为了加速,均忽略了词序信息。然而,Landauer在文献 [54]中曾分析, 文本中大约有 20%的语义来自于词序,而剩下部分来自词的选择。因此,这两 个模型可能会丢失一些重要信息。与之相对的,LBL模型 [79]、NNLM模型 [7]、 C&W 模型均使用上下文窗口中各词词向量的拼接作为上下文的表示,可以保 留词序信息。本文希望知道,使用哪个模型效果更好?具体而言,在多种不同 的上下文表示之中,以及在上下文与目标词的两种不同的关系之间,应当如何 选择合适的模型?(问题一) 同时,词向量的精度非常依赖于训练语料的选择,不同大小、不同领域的 语料会极大地影响词向量的性能。因此,本文还希望知道,训练语料的大小及 领域对词向量有什么样的影响?(问题二) 除了模型和语料的选择,现有的词向量算法也非常依赖于参数的选择。最 主要的是模型的迭代次数,以及词向量的维度。对于迭代次数,如果迭代次数 过少,词向量就会训练不充分,所包含的信息不足;如果迭代次数过多,模型 很可能过拟合。对于词向量的维度,本文也希望找到合适的维度。因此,本文 尝试分析在迭代训练中,选择什么样的迭代次数可以获得足够好的词向量,同 时避免过拟合?(问题三)以及多少维的词向量效果最理想?(问题四) 为了更客观地回答上述四个问题,本文选取了三大类指标,共八个具体的 任务评价这些词向量。本文认为这些评价指标涵盖了现有词向量的所有用法。 第一类指标为评价词向量的语言学特性。本文使用标准的 WordSim353数据集 [28]以及 TOEFL数据集 [55]评价词向量的空间距离是否和人的直观感受一致。 第二类指标中,本文将词向量作为现有自然语言任务中的特征,看其所能达到 的性能。具体而言,本文选择了文本分类任务和命名实体识别任务。第三类指 标中,本文将词向量作为神经网络模型的初始值,并使用卷积神经网络做情感 分类任务,以及使用文献 [18]的模型做词性标注任务。通过使用这些不同的评 价指标对词向量模型进行评价,本文尝试分析出应该怎么选择模型(问题一)和 参数(问题三和四)。本文进一步通过使用不同规模和不同领域的语料对词向量 进行训练尝试回答第二个问题。 本章的主要贡献为系统化地整理现有的词向量模型,并且通过多种评价指 标全面地比较分析各词向量模型与语料的选取。通过这些实验比较和理论分析,",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 50,
      "context" : "如,Skip-gram模型 [73]选取了目标词 w上下文中的某一个词的词向量,作为其 上下文表示。CBOW模型采用上下文各词词向量的平均值作为上下文表示。这 两个模型为了加速,均忽略了词序信息。然而,Landauer在文献 [54]中曾分析, 文本中大约有 20%的语义来自于词序,而剩下部分来自词的选择。因此,这两 个模型可能会丢失一些重要信息。与之相对的,LBL模型 [79]、NNLM模型 [7]、 C&W 模型均使用上下文窗口中各词词向量的拼接作为上下文的表示,可以保 留词序信息。本文希望知道,使用哪个模型效果更好?具体而言,在多种不同 的上下文表示之中,以及在上下文与目标词的两种不同的关系之间,应当如何 选择合适的模型?(问题一) 同时,词向量的精度非常依赖于训练语料的选择,不同大小、不同领域的 语料会极大地影响词向量的性能。因此,本文还希望知道,训练语料的大小及 领域对词向量有什么样的影响?(问题二) 除了模型和语料的选择,现有的词向量算法也非常依赖于参数的选择。最 主要的是模型的迭代次数,以及词向量的维度。对于迭代次数,如果迭代次数 过少,词向量就会训练不充分,所包含的信息不足;如果迭代次数过多,模型 很可能过拟合。对于词向量的维度,本文也希望找到合适的维度。因此,本文 尝试分析在迭代训练中,选择什么样的迭代次数可以获得足够好的词向量,同 时避免过拟合?(问题三)以及多少维的词向量效果最理想?(问题四) 为了更客观地回答上述四个问题,本文选取了三大类指标,共八个具体的 任务评价这些词向量。本文认为这些评价指标涵盖了现有词向量的所有用法。 第一类指标为评价词向量的语言学特性。本文使用标准的 WordSim353数据集 [28]以及 TOEFL数据集 [55]评价词向量的空间距离是否和人的直观感受一致。 第二类指标中,本文将词向量作为现有自然语言任务中的特征,看其所能达到 的性能。具体而言,本文选择了文本分类任务和命名实体识别任务。第三类指 标中,本文将词向量作为神经网络模型的初始值,并使用卷积神经网络做情感 分类任务,以及使用文献 [18]的模型做词性标注任务。通过使用这些不同的评 价指标对词向量模型进行评价,本文尝试分析出应该怎么选择模型(问题一)和 参数(问题三和四)。本文进一步通过使用不同规模和不同领域的语料对词向量 进行训练尝试回答第二个问题。 本章的主要贡献为系统化地整理现有的词向量模型,并且通过多种评价指 标全面地比较分析各词向量模型与语料的选取。通过这些实验比较和理论分析,",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 75,
      "context" : "如,Skip-gram模型 [73]选取了目标词 w上下文中的某一个词的词向量,作为其 上下文表示。CBOW模型采用上下文各词词向量的平均值作为上下文表示。这 两个模型为了加速,均忽略了词序信息。然而,Landauer在文献 [54]中曾分析, 文本中大约有 20%的语义来自于词序,而剩下部分来自词的选择。因此,这两 个模型可能会丢失一些重要信息。与之相对的,LBL模型 [79]、NNLM模型 [7]、 C&W 模型均使用上下文窗口中各词词向量的拼接作为上下文的表示,可以保 留词序信息。本文希望知道,使用哪个模型效果更好?具体而言,在多种不同 的上下文表示之中,以及在上下文与目标词的两种不同的关系之间,应当如何 选择合适的模型?(问题一) 同时,词向量的精度非常依赖于训练语料的选择,不同大小、不同领域的 语料会极大地影响词向量的性能。因此,本文还希望知道,训练语料的大小及 领域对词向量有什么样的影响?(问题二) 除了模型和语料的选择,现有的词向量算法也非常依赖于参数的选择。最 主要的是模型的迭代次数,以及词向量的维度。对于迭代次数,如果迭代次数 过少,词向量就会训练不充分,所包含的信息不足;如果迭代次数过多,模型 很可能过拟合。对于词向量的维度,本文也希望找到合适的维度。因此,本文 尝试分析在迭代训练中,选择什么样的迭代次数可以获得足够好的词向量,同 时避免过拟合?(问题三)以及多少维的词向量效果最理想?(问题四) 为了更客观地回答上述四个问题,本文选取了三大类指标,共八个具体的 任务评价这些词向量。本文认为这些评价指标涵盖了现有词向量的所有用法。 第一类指标为评价词向量的语言学特性。本文使用标准的 WordSim353数据集 [28]以及 TOEFL数据集 [55]评价词向量的空间距离是否和人的直观感受一致。 第二类指标中,本文将词向量作为现有自然语言任务中的特征,看其所能达到 的性能。具体而言,本文选择了文本分类任务和命名实体识别任务。第三类指 标中,本文将词向量作为神经网络模型的初始值,并使用卷积神经网络做情感 分类任务,以及使用文献 [18]的模型做词性标注任务。通过使用这些不同的评 价指标对词向量模型进行评价,本文尝试分析出应该怎么选择模型(问题一)和 参数(问题三和四)。本文进一步通过使用不同规模和不同领域的语料对词向量 进行训练尝试回答第二个问题。 本章的主要贡献为系统化地整理现有的词向量模型,并且通过多种评价指 标全面地比较分析各词向量模型与语料的选取。通过这些实验比较和理论分析,",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 5,
      "context" : "如,Skip-gram模型 [73]选取了目标词 w上下文中的某一个词的词向量,作为其 上下文表示。CBOW模型采用上下文各词词向量的平均值作为上下文表示。这 两个模型为了加速,均忽略了词序信息。然而,Landauer在文献 [54]中曾分析, 文本中大约有 20%的语义来自于词序,而剩下部分来自词的选择。因此,这两 个模型可能会丢失一些重要信息。与之相对的,LBL模型 [79]、NNLM模型 [7]、 C&W 模型均使用上下文窗口中各词词向量的拼接作为上下文的表示,可以保 留词序信息。本文希望知道,使用哪个模型效果更好?具体而言,在多种不同 的上下文表示之中,以及在上下文与目标词的两种不同的关系之间,应当如何 选择合适的模型?(问题一) 同时,词向量的精度非常依赖于训练语料的选择,不同大小、不同领域的 语料会极大地影响词向量的性能。因此,本文还希望知道,训练语料的大小及 领域对词向量有什么样的影响?(问题二) 除了模型和语料的选择,现有的词向量算法也非常依赖于参数的选择。最 主要的是模型的迭代次数,以及词向量的维度。对于迭代次数,如果迭代次数 过少,词向量就会训练不充分,所包含的信息不足;如果迭代次数过多,模型 很可能过拟合。对于词向量的维度,本文也希望找到合适的维度。因此,本文 尝试分析在迭代训练中,选择什么样的迭代次数可以获得足够好的词向量,同 时避免过拟合?(问题三)以及多少维的词向量效果最理想?(问题四) 为了更客观地回答上述四个问题,本文选取了三大类指标,共八个具体的 任务评价这些词向量。本文认为这些评价指标涵盖了现有词向量的所有用法。 第一类指标为评价词向量的语言学特性。本文使用标准的 WordSim353数据集 [28]以及 TOEFL数据集 [55]评价词向量的空间距离是否和人的直观感受一致。 第二类指标中,本文将词向量作为现有自然语言任务中的特征,看其所能达到 的性能。具体而言,本文选择了文本分类任务和命名实体识别任务。第三类指 标中,本文将词向量作为神经网络模型的初始值,并使用卷积神经网络做情感 分类任务,以及使用文献 [18]的模型做词性标注任务。通过使用这些不同的评 价指标对词向量模型进行评价,本文尝试分析出应该怎么选择模型(问题一)和 参数(问题三和四)。本文进一步通过使用不同规模和不同领域的语料对词向量 进行训练尝试回答第二个问题。 本章的主要贡献为系统化地整理现有的词向量模型,并且通过多种评价指 标全面地比较分析各词向量模型与语料的选取。通过这些实验比较和理论分析,",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 26,
      "context" : "如,Skip-gram模型 [73]选取了目标词 w上下文中的某一个词的词向量,作为其 上下文表示。CBOW模型采用上下文各词词向量的平均值作为上下文表示。这 两个模型为了加速,均忽略了词序信息。然而,Landauer在文献 [54]中曾分析, 文本中大约有 20%的语义来自于词序,而剩下部分来自词的选择。因此,这两 个模型可能会丢失一些重要信息。与之相对的,LBL模型 [79]、NNLM模型 [7]、 C&W 模型均使用上下文窗口中各词词向量的拼接作为上下文的表示,可以保 留词序信息。本文希望知道,使用哪个模型效果更好?具体而言,在多种不同 的上下文表示之中,以及在上下文与目标词的两种不同的关系之间,应当如何 选择合适的模型?(问题一) 同时,词向量的精度非常依赖于训练语料的选择,不同大小、不同领域的 语料会极大地影响词向量的性能。因此,本文还希望知道,训练语料的大小及 领域对词向量有什么样的影响?(问题二) 除了模型和语料的选择,现有的词向量算法也非常依赖于参数的选择。最 主要的是模型的迭代次数,以及词向量的维度。对于迭代次数,如果迭代次数 过少,词向量就会训练不充分,所包含的信息不足;如果迭代次数过多,模型 很可能过拟合。对于词向量的维度,本文也希望找到合适的维度。因此,本文 尝试分析在迭代训练中,选择什么样的迭代次数可以获得足够好的词向量,同 时避免过拟合?(问题三)以及多少维的词向量效果最理想?(问题四) 为了更客观地回答上述四个问题,本文选取了三大类指标,共八个具体的 任务评价这些词向量。本文认为这些评价指标涵盖了现有词向量的所有用法。 第一类指标为评价词向量的语言学特性。本文使用标准的 WordSim353数据集 [28]以及 TOEFL数据集 [55]评价词向量的空间距离是否和人的直观感受一致。 第二类指标中,本文将词向量作为现有自然语言任务中的特征,看其所能达到 的性能。具体而言,本文选择了文本分类任务和命名实体识别任务。第三类指 标中,本文将词向量作为神经网络模型的初始值,并使用卷积神经网络做情感 分类任务,以及使用文献 [18]的模型做词性标注任务。通过使用这些不同的评 价指标对词向量模型进行评价,本文尝试分析出应该怎么选择模型(问题一)和 参数(问题三和四)。本文进一步通过使用不同规模和不同领域的语料对词向量 进行训练尝试回答第二个问题。 本章的主要贡献为系统化地整理现有的词向量模型,并且通过多种评价指 标全面地比较分析各词向量模型与语料的选取。通过这些实验比较和理论分析,",
      "startOffset" : 726,
      "endOffset" : 730
    }, {
      "referenceID" : 51,
      "context" : "如,Skip-gram模型 [73]选取了目标词 w上下文中的某一个词的词向量,作为其 上下文表示。CBOW模型采用上下文各词词向量的平均值作为上下文表示。这 两个模型为了加速,均忽略了词序信息。然而,Landauer在文献 [54]中曾分析, 文本中大约有 20%的语义来自于词序,而剩下部分来自词的选择。因此,这两 个模型可能会丢失一些重要信息。与之相对的,LBL模型 [79]、NNLM模型 [7]、 C&W 模型均使用上下文窗口中各词词向量的拼接作为上下文的表示,可以保 留词序信息。本文希望知道,使用哪个模型效果更好?具体而言,在多种不同 的上下文表示之中,以及在上下文与目标词的两种不同的关系之间,应当如何 选择合适的模型?(问题一) 同时,词向量的精度非常依赖于训练语料的选择,不同大小、不同领域的 语料会极大地影响词向量的性能。因此,本文还希望知道,训练语料的大小及 领域对词向量有什么样的影响?(问题二) 除了模型和语料的选择,现有的词向量算法也非常依赖于参数的选择。最 主要的是模型的迭代次数,以及词向量的维度。对于迭代次数,如果迭代次数 过少,词向量就会训练不充分,所包含的信息不足;如果迭代次数过多,模型 很可能过拟合。对于词向量的维度,本文也希望找到合适的维度。因此,本文 尝试分析在迭代训练中,选择什么样的迭代次数可以获得足够好的词向量,同 时避免过拟合?(问题三)以及多少维的词向量效果最理想?(问题四) 为了更客观地回答上述四个问题,本文选取了三大类指标,共八个具体的 任务评价这些词向量。本文认为这些评价指标涵盖了现有词向量的所有用法。 第一类指标为评价词向量的语言学特性。本文使用标准的 WordSim353数据集 [28]以及 TOEFL数据集 [55]评价词向量的空间距离是否和人的直观感受一致。 第二类指标中,本文将词向量作为现有自然语言任务中的特征,看其所能达到 的性能。具体而言,本文选择了文本分类任务和命名实体识别任务。第三类指 标中,本文将词向量作为神经网络模型的初始值,并使用卷积神经网络做情感 分类任务,以及使用文献 [18]的模型做词性标注任务。通过使用这些不同的评 价指标对词向量模型进行评价,本文尝试分析出应该怎么选择模型(问题一)和 参数(问题三和四)。本文进一步通过使用不同规模和不同领域的语料对词向量 进行训练尝试回答第二个问题。 本章的主要贡献为系统化地整理现有的词向量模型,并且通过多种评价指 标全面地比较分析各词向量模型与语料的选取。通过这些实验比较和理论分析,",
      "startOffset" : 742,
      "endOffset" : 746
    }, {
      "referenceID" : 16,
      "context" : "如,Skip-gram模型 [73]选取了目标词 w上下文中的某一个词的词向量,作为其 上下文表示。CBOW模型采用上下文各词词向量的平均值作为上下文表示。这 两个模型为了加速,均忽略了词序信息。然而,Landauer在文献 [54]中曾分析, 文本中大约有 20%的语义来自于词序,而剩下部分来自词的选择。因此,这两 个模型可能会丢失一些重要信息。与之相对的,LBL模型 [79]、NNLM模型 [7]、 C&W 模型均使用上下文窗口中各词词向量的拼接作为上下文的表示,可以保 留词序信息。本文希望知道,使用哪个模型效果更好?具体而言,在多种不同 的上下文表示之中,以及在上下文与目标词的两种不同的关系之间,应当如何 选择合适的模型?(问题一) 同时,词向量的精度非常依赖于训练语料的选择,不同大小、不同领域的 语料会极大地影响词向量的性能。因此,本文还希望知道,训练语料的大小及 领域对词向量有什么样的影响?(问题二) 除了模型和语料的选择,现有的词向量算法也非常依赖于参数的选择。最 主要的是模型的迭代次数,以及词向量的维度。对于迭代次数,如果迭代次数 过少,词向量就会训练不充分,所包含的信息不足;如果迭代次数过多,模型 很可能过拟合。对于词向量的维度,本文也希望找到合适的维度。因此,本文 尝试分析在迭代训练中,选择什么样的迭代次数可以获得足够好的词向量,同 时避免过拟合?(问题三)以及多少维的词向量效果最理想?(问题四) 为了更客观地回答上述四个问题,本文选取了三大类指标,共八个具体的 任务评价这些词向量。本文认为这些评价指标涵盖了现有词向量的所有用法。 第一类指标为评价词向量的语言学特性。本文使用标准的 WordSim353数据集 [28]以及 TOEFL数据集 [55]评价词向量的空间距离是否和人的直观感受一致。 第二类指标中,本文将词向量作为现有自然语言任务中的特征,看其所能达到 的性能。具体而言,本文选择了文本分类任务和命名实体识别任务。第三类指 标中,本文将词向量作为神经网络模型的初始值,并使用卷积神经网络做情感 分类任务,以及使用文献 [18]的模型做词性标注任务。通过使用这些不同的评 价指标对词向量模型进行评价,本文尝试分析出应该怎么选择模型(问题一)和 参数(问题三和四)。本文进一步通过使用不同规模和不同领域的语料对词向量 进行训练尝试回答第二个问题。 本章的主要贡献为系统化地整理现有的词向量模型,并且通过多种评价指 标全面地比较分析各词向量模型与语料的选取。通过这些实验比较和理论分析,",
      "startOffset" : 886,
      "endOffset" : 890
    }, {
      "referenceID" : 1,
      "context" : "各词向量模型均基于分布假说设计而成,因此无论哪种词向量模型,都会 符合分布假说所提出的性质:具有相似上下文的词,会拥有相似的语义,并且 其词向量的空间距离更接近。文献 [3]通过语义相关性、同义词判别、概念分类 和类比等实验论述了词向量具有各种不同的语言学特性。本文从中选取了三个 代表性任务。",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 51,
      "context" : "同义词检测 (tfl) 托福考试(TOEFL)数据集 [55]包含 80个单选题,每个题目包含一个问题 词以及四个选项,要求从四个选项中选出一个与问题词同义的词语。例如:问 题“levied”,选项“imposed”、“believed”、“requested”、“correlated”,正确答案 为“imposed”。对于每一个问题,需要计算问题词与选项词对应词向量之间的 余弦距离,并选用距离最近的选项词,作为答案。在评价词向量时,本文直接 使用 80个问题的准确率。",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 69,
      "context" : "单词类比 (sem、syn) 英文单词类比数据集由Mikolov等人于 2013年 [73]提出,该数据集包含了 9000 个语义类比问题以及 1 万个句法类比问题。语义类比问题包括国家首都、 家庭成员称谓、国家货币等五类问题,如,“‘king’对‘queen’如同‘man’对什 么?”,答案为“woman”。句法类比问题有比较级、最高级、名词单复数等九类 问题,如“‘dance’对‘dancing’如同‘predict’对什么?”,答案为“predicting”。 为了回答这类类比问题,Mikolov等人 [73]根据相似关系词对的词向量之 差也相似的特点,提出使用词向量的加减法来完成这一任务。例如,对于问题",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 69,
      "context" : "单词类比 (sem、syn) 英文单词类比数据集由Mikolov等人于 2013年 [73]提出,该数据集包含了 9000 个语义类比问题以及 1 万个句法类比问题。语义类比问题包括国家首都、 家庭成员称谓、国家货币等五类问题,如,“‘king’对‘queen’如同‘man’对什 么?”,答案为“woman”。句法类比问题有比较级、最高级、名词单复数等九类 问题,如“‘dance’对‘dancing’如同‘predict’对什么?”,答案为“predicting”。 为了回答这类类比问题,Mikolov等人 [73]根据相似关系词对的词向量之 差也相似的特点,提出使用词向量的加减法来完成这一任务。例如,对于问题",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 67,
      "context" : "基于平均词向量的文本分类 (avg) 该任务直接以文本中各词词向量的加权平均值作为文档的表示,以此为特 征,利用 Logistic回归完成文本分类任务。其中权重为文档中各词的词频。本文 选用了 IMDB数据集 [71]做文本分类实验。该数据集包含三部分,其中训练集 和测试集各 2.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 92,
      "context" : "命名实体识别 (ner) 命名实体识别(Named entity recognition,NER)在机器学习框架下,通常作 为一个序列标注问题处理。在这一评价指标中,本文将词向量作为现有命名实 体识别系统 [97]的额外特征,该系统的性能接近现有系统的最好性能。实验设 置与 Turian等人实现的方式一致 [116]。任务的评价指标为命名实体识别的 F1 值,测试集是 CoNLL03多任务数据集的测试集。 3.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 111,
      "context" : "命名实体识别 (ner) 命名实体识别(Named entity recognition,NER)在机器学习框架下,通常作 为一个序列标注问题处理。在这一评价指标中,本文将词向量作为现有命名实 体识别系统 [97]的额外特征,该系统的性能接近现有系统的最好性能。实验设 置与 Turian等人实现的方式一致 [116]。任务的评价指标为命名实体识别的 F1 值,测试集是 CoNLL03多任务数据集的测试集。 3.",
      "startOffset" : 154,
      "endOffset" : 159
    }, {
      "referenceID" : 25,
      "context" : "3 词向量用做神经网络初始值 Erhan等人在文献 [27]中论述了,恰当地选取神经网络的初始值,可以让 神经网络收敛到更好的局部最优解。在自然语言处理任务中,基于神经网络模 型的方法一般都会使用词向量作为其输入层的初始值。",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 55,
      "context" : "基于卷积神经网络的文本分类 (cnn) 卷积神经网络(Convolutional neural networks,CNN)是表示文本的有效模 型。2014年,Lebret等人 [59]以及 Kim等人 [48]同时提出用于文本分类任务的 卷积神经网络。在这一评价指标中,本文选用了这一经典的卷积神经网络。网 络结构在 5.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 44,
      "context" : "基于卷积神经网络的文本分类 (cnn) 卷积神经网络(Convolutional neural networks,CNN)是表示文本的有效模 型。2014年,Lebret等人 [59]以及 Kim等人 [48]同时提出用于文本分类任务的 卷积神经网络。在这一评价指标中,本文选用了这一经典的卷积神经网络。网 络结构在 5.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 105,
      "context" : "4节有详细介绍。 本文选取斯坦福情感树库(Stanford Sentiment Treebank)数据集作为文本分 类的训练集、验证集和测试集 [110]。由于该数据集规模较小,文本分类的效果 受网络初始值的影响较大,导致了评价指标的不稳定。为了更客观地评价卷积 网络中,不同词向量对文本分类性能的影响,本文对每一份词向量重复做 5次 实验。在每次实验中,输入层词表示均初始化为这份词向量,网络结构中的其 它参数则初始化为不同的随机值。对于每一次实验,本文在训练集上训练卷积 神经网络,取验证集上准确率最高的点,并报告其在测试集上的准确率。最后 将 5组实验的测试集准确率的平均值作为最终的评价指标。",
      "startOffset" : 72,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "词性标注 (pos) 词性标注(part-of-speech tagging)是一个经典的序列标注问题。在这个任 务中,本文使用 Collobert等人提出的网络 [18],对句子中的每个词做序列标注。 该任务选用华尔街日报数据集 [115]。评价指标为模型在验证集上达到最佳效果 时,测试集上的准确率。",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 110,
      "context" : "词性标注 (pos) 词性标注(part-of-speech tagging)是一个经典的序列标注问题。在这个任 务中,本文使用 Collobert等人提出的网络 [18],对句子中的每个词做序列标注。 该任务选用华尔街日报数据集 [115]。评价指标为模型在验证集上达到最佳效果 时,测试集上的准确率。",
      "startOffset" : 116,
      "endOffset" : 121
    }, {
      "referenceID" : 22,
      "context" : "单纯使用性能增益并不能解决上面的第二个问题。为了解决第二个问题, 本文进一步提出使用“性能增益率”(Performance Gain Ratio)这一评价指标来 代替性能增益。性能增益率的思想借鉴了文献 [24],每个词向量只与同等条件 下最好的词向量做对比。本文根据词向量的特殊性质,将词向量 a相对词向量 b的性能增益率定义为: PGR(a, b) = pa − prand pb − prand (3.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 71,
      "context" : "在模型实现方面,本文使用的 Skip-gram 模型和 CBOW 模型的实现基于 word2vec开源工具包2。其余模型均在 word2vec工具包中 CBOW实现的基础上 修改得到。具体来说,在 CBOW模型中,上下文的表示为上下文若干词的平均 词向量;在 Order模型中,本文将其替换为上下文若干词的词向量的拼接;在 LBL模型的实现中,本文在 Order模型实现的基础上加入了一个隐藏层,使得 上下文表示先通过一次线性变换进入隐藏层,再对其预测;在 NNLM模型的实 现中,本文在 LBL模型的线性变换之后加入 tanh激活函数,使其成为一个非线 性变换,整个模型是一个三层的神经网络结构;在 C&W模型的实现中,本文 基于 NNLM的实现,将预测的目标词从输出层移动到输入层,这样输入层就是 目标词与其上下文词的词向量的拼接,而输出层则只保留一个节点,用于表示 这组上下文、目标词组合的评分。 对于所有的模型,本文将中间词作为目标词(一般来说,以语言模型为基 础的模型,如 NNLM和 LBL,都使用最后一个词作为目标词,这里也将其改成 中间词),目标词上下文各两个词作为其对应的上下文,即 win = 5。对于所有 模型,本文使用二次采样(subsampling)技术 [75],并设置 t = 10−4。二次采样 技术的细节可见第二章第 2.",
      "startOffset" : 541,
      "endOffset" : 545
    }, {
      "referenceID" : 71,
      "context" : "6小节。word2vec工具包与文献 [75]中所描述的 二次采样公式略有差别,本文在实验中使用word2vec工具包所用的公式。同时, 为了提高实验效率,本文也对 word2vec工具包做了适当的改动。word2vec工具 包采用梯度下降法作为其优化算法,同时也采用了学习速率下降的策略。在训 练的初始阶段,其学习速率为一个设定的初始学习速率(如 CBOW模型默认为 0.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 23,
      "context" : "05);在训练过程中,学习速率均匀下降,下降幅度与已学习的样本个数呈正 比;到训练的最后阶段,学习速率降为 0。在这种学习速率下降策略下,如果想 分析迭代 1次到 n次对结果的影响,由于迭代的中间结果不能直接使用,程序 需要扫描语料 1+2+ · · ·+n = n(n+1)/2次才能真正生成各种迭代次数的训练 结果。为了方便分析迭代次数带来的影响,本文使用 AdaGrad [25]替代原先的 2https://code.",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 93,
      "context" : "从表中可以发现,用 CBOW模型训练时,“Monday”的最近邻是一星期中 的其它几天,而C&W模型得到的的却是一天中的时刻。类似地,“commonly”的 最近邻用 CBOW得到的都是一些语义类似的词,使用时可以替换“commonly”, 而 C&W模型得到的有些是和 commonly搭配使用的词。“reddish”的最近邻多 为颜色,除了 C&W模型得到的“pendulous”,该词一般与“reddish”一起使用, 形容花朵。 根据文献 [98] 中的论述,CBOW 这类通过上下文预测目标词的模型直接 对二阶关系进行建模,而一般认为二阶关系就是替换关系(paradigmatic)[57]。 简单地说,这些模型通过相似的目标词,构建出上下文之间的相似关系。然而, C&W模型没有显式地对二阶关系进行建模。实验中也可以看出,C&W的最近 邻并不全是替换关系的词,而 CBOW 模型得到的几乎全是替换关系的词。本 文的结果初看与 C&W 模型原始论文 [18] 的结果有差异,在该论文的表 7 中, C&W模型的最近邻均为替换关系。实际上,文献 [18]的结果并不是纯 C&W得 到的结果,而是在 C&W模型之后,进一步使用了四个有监督任务(词性标注、 命名实体识别等)调整参数得到的词向量。有监督任务与 CBOW这类预测目标 词的方法类似,也会使相似的词向量具有替换关系。因此,本文认为,C&W之 所以与其它模型的效果有较大的差异,主要是因为 C&W模型将目标词放在了",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 53,
      "context" : "从表中可以发现,用 CBOW模型训练时,“Monday”的最近邻是一星期中 的其它几天,而C&W模型得到的的却是一天中的时刻。类似地,“commonly”的 最近邻用 CBOW得到的都是一些语义类似的词,使用时可以替换“commonly”, 而 C&W模型得到的有些是和 commonly搭配使用的词。“reddish”的最近邻多 为颜色,除了 C&W模型得到的“pendulous”,该词一般与“reddish”一起使用, 形容花朵。 根据文献 [98] 中的论述,CBOW 这类通过上下文预测目标词的模型直接 对二阶关系进行建模,而一般认为二阶关系就是替换关系(paradigmatic)[57]。 简单地说,这些模型通过相似的目标词,构建出上下文之间的相似关系。然而, C&W模型没有显式地对二阶关系进行建模。实验中也可以看出,C&W的最近 邻并不全是替换关系的词,而 CBOW 模型得到的几乎全是替换关系的词。本 文的结果初看与 C&W 模型原始论文 [18] 的结果有差异,在该论文的表 7 中, C&W模型的最近邻均为替换关系。实际上,文献 [18]的结果并不是纯 C&W得 到的结果,而是在 C&W模型之后,进一步使用了四个有监督任务(词性标注、 命名实体识别等)调整参数得到的词向量。有监督任务与 CBOW这类预测目标 词的方法类似,也会使相似的词向量具有替换关系。因此,本文认为,C&W之 所以与其它模型的效果有较大的差异,主要是因为 C&W模型将目标词放在了",
      "startOffset" : 296,
      "endOffset" : 300
    }, {
      "referenceID" : 16,
      "context" : "从表中可以发现,用 CBOW模型训练时,“Monday”的最近邻是一星期中 的其它几天,而C&W模型得到的的却是一天中的时刻。类似地,“commonly”的 最近邻用 CBOW得到的都是一些语义类似的词,使用时可以替换“commonly”, 而 C&W模型得到的有些是和 commonly搭配使用的词。“reddish”的最近邻多 为颜色,除了 C&W模型得到的“pendulous”,该词一般与“reddish”一起使用, 形容花朵。 根据文献 [98] 中的论述,CBOW 这类通过上下文预测目标词的模型直接 对二阶关系进行建模,而一般认为二阶关系就是替换关系(paradigmatic)[57]。 简单地说,这些模型通过相似的目标词,构建出上下文之间的相似关系。然而, C&W模型没有显式地对二阶关系进行建模。实验中也可以看出,C&W的最近 邻并不全是替换关系的词,而 CBOW 模型得到的几乎全是替换关系的词。本 文的结果初看与 C&W 模型原始论文 [18] 的结果有差异,在该论文的表 7 中, C&W模型的最近邻均为替换关系。实际上,文献 [18]的结果并不是纯 C&W得 到的结果,而是在 C&W模型之后,进一步使用了四个有监督任务(词性标注、 命名实体识别等)调整参数得到的词向量。有监督任务与 CBOW这类预测目标 词的方法类似,也会使相似的词向量具有替换关系。因此,本文认为,C&W之 所以与其它模型的效果有较大的差异,主要是因为 C&W模型将目标词放在了",
      "startOffset" : 430,
      "endOffset" : 434
    }, {
      "referenceID" : 16,
      "context" : "从表中可以发现,用 CBOW模型训练时,“Monday”的最近邻是一星期中 的其它几天,而C&W模型得到的的却是一天中的时刻。类似地,“commonly”的 最近邻用 CBOW得到的都是一些语义类似的词,使用时可以替换“commonly”, 而 C&W模型得到的有些是和 commonly搭配使用的词。“reddish”的最近邻多 为颜色,除了 C&W模型得到的“pendulous”,该词一般与“reddish”一起使用, 形容花朵。 根据文献 [98] 中的论述,CBOW 这类通过上下文预测目标词的模型直接 对二阶关系进行建模,而一般认为二阶关系就是替换关系(paradigmatic)[57]。 简单地说,这些模型通过相似的目标词,构建出上下文之间的相似关系。然而, C&W模型没有显式地对二阶关系进行建模。实验中也可以看出,C&W的最近 邻并不全是替换关系的词,而 CBOW 模型得到的几乎全是替换关系的词。本 文的结果初看与 C&W 模型原始论文 [18] 的结果有差异,在该论文的表 7 中, C&W模型的最近邻均为替换关系。实际上,文献 [18]的结果并不是纯 C&W得 到的结果,而是在 C&W模型之后,进一步使用了四个有监督任务(词性标注、 命名实体识别等)调整参数得到的词向量。有监督任务与 CBOW这类预测目标 词的方法类似,也会使相似的词向量具有替换关系。因此,本文认为,C&W之 所以与其它模型的效果有较大的差异,主要是因为 C&W模型将目标词放在了",
      "startOffset" : 477,
      "endOffset" : 481
    }, {
      "referenceID" : 90,
      "context" : "值)达到最低值时,迭代终止 [95]。在词向量训练过程中,损失函数刻画了模型 预测“目标词”的精度(或者上下文与目标词的匹配程度)。然而真实的任务并 不要求词向量能够多么精确地预测目标词,而是希望词向量具有更好的语言学 特性,以及能更好地辅助其它自然语言处理任务。因此,损失函数仅仅是实际 任务的一个代理,在某些情况下,损失函数与实际任务的性能可能不一致。本 小节从这一经典方法开始,探索适合词向量训练的迭代停止条件。 本小节的实验选取 95%的语料作为训练集,剩下 5%作为验证集。图 3-1描 绘了 3个具有代表性的训练过程,分别为使用 CBOW模型在W&N语料的三个 子集(10M单词、100M单词、1B单词)上的训练过程。横坐标表示迭代次数, 纵坐标为各任务的性能以及验证集损失。",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 69,
      "context" : "本小节通过选择不同的模型和任务,分析词向量维度对性能的影响。实验 发现,对于分析词向量语言学特性的任务,有着一致的结论:维度越大,效果 越好。图 3-2(a) 以 tfl 任务为例,绘制了性能随着维度的变化曲线图。这一结 论在文献 [73]的实验中证实,当词向量维度到达 600维时,其语义特性仍然在 变好。由于训练更高维度的词向量非常耗时,目前尚不能确定这一结果成立的 上界。与分析词向量语言学特性的任务不同,在使用词向量提升自然语言处理 任务的指标中,词向量维度到达 50之后,效果提升非常微弱。图 3-2(b)以 pos 任务为例,展示了这类实验结果。词向量的维度选择多少维比较合适?本文认 为,对于分析词向量语言学特性的任务,维度越大效果越好(除 C&W模型以 外,在 3.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 111,
      "context" : "1 模型比较 在比较词向量的各项工作中,与本文最类似的工作是 Turian等人在 2010年 发表的文献 [116]。他们在文中比较了 HLBL模型与 C&W模型在命名实体识别 (ner)任务和短语识别(chunking)任务中的表现,这两个任务均在现有自然语 言工具的基础上加入词向量作为额外的特征,以此评估词向量对系统性能的提 升。他们的实验中,使用了一个 6300万(63M)词的小规模语料,并发现 HLBL 模型与 C&W模型在两个任务中表现相当。 Baroni等人在 2014年发表的文献 [3]中比较了“计数”模型与“预测”模 型在若干语义相似度任务中的表现。他们在文中将基于统计“词-上下文”共现",
      "startOffset" : 53,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "1 模型比较 在比较词向量的各项工作中,与本文最类似的工作是 Turian等人在 2010年 发表的文献 [116]。他们在文中比较了 HLBL模型与 C&W模型在命名实体识别 (ner)任务和短语识别(chunking)任务中的表现,这两个任务均在现有自然语 言工具的基础上加入词向量作为额外的特征,以此评估词向量对系统性能的提 升。他们的实验中,使用了一个 6300万(63M)词的小规模语料,并发现 HLBL 模型与 C&W模型在两个任务中表现相当。 Baroni等人在 2014年发表的文献 [3]中比较了“计数”模型与“预测”模 型在若干语义相似度任务中的表现。他们在文中将基于统计“词-上下文”共现",
      "startOffset" : 250,
      "endOffset" : 253
    }, {
      "referenceID" : 58,
      "context" : "矩阵,以及在其基础上进行矩阵分解的方法,统称为计数模型;并将基于神经 网络的词向量模型统称为预测模型。Baroni等人在实验中使用“词-上下文”共 现矩阵的原始形式、SVD分解、NMF分解(非负矩阵分解)[62, 68]作为计数 模型的代表,使用 CBOW模型作为预测模型的代表。他们对这两类模型进行多 种语义特性的评价,包括语义相关性、同义词判别、概念分类、类比等。其实 验表明,预测模型对在各项指标中比计数模型有显著的优势。 然而,Milajevs等人在 2014年发表的文献 [77]中,给出了相反的结论。文 中指出,他们在实验中,尝试了向量逐元素相加、逐元素相乘等基本的语义组 合方式,使用这些组合方式表示短语以及句子的语义,并通过短语语义相似度 等指标进行评测。实验结果表明,基于共现矩阵的词表示方法相比神经网络的 词向量模型,其基本的语义组合能力更强。 Levy等人在 2015年发表的文献 [65]中尝试了多种不同的模型参数,包括 动态窗口或者固定窗口的选择、重新采样技术的效果、低频词的处理方式、上 下文的平滑方案等。实验发现,大部分参数设置的技巧,对基于共现矩阵的模 型以及基于神经网络的模型同时有效。",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 64,
      "context" : "矩阵,以及在其基础上进行矩阵分解的方法,统称为计数模型;并将基于神经 网络的词向量模型统称为预测模型。Baroni等人在实验中使用“词-上下文”共 现矩阵的原始形式、SVD分解、NMF分解(非负矩阵分解)[62, 68]作为计数 模型的代表,使用 CBOW模型作为预测模型的代表。他们对这两类模型进行多 种语义特性的评价,包括语义相关性、同义词判别、概念分类、类比等。其实 验表明,预测模型对在各项指标中比计数模型有显著的优势。 然而,Milajevs等人在 2014年发表的文献 [77]中,给出了相反的结论。文 中指出,他们在实验中,尝试了向量逐元素相加、逐元素相乘等基本的语义组 合方式,使用这些组合方式表示短语以及句子的语义,并通过短语语义相似度 等指标进行评测。实验结果表明,基于共现矩阵的词表示方法相比神经网络的 词向量模型,其基本的语义组合能力更强。 Levy等人在 2015年发表的文献 [65]中尝试了多种不同的模型参数,包括 动态窗口或者固定窗口的选择、重新采样技术的效果、低频词的处理方式、上 下文的平滑方案等。实验发现,大部分参数设置的技巧,对基于共现矩阵的模 型以及基于神经网络的模型同时有效。",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 73,
      "context" : "矩阵,以及在其基础上进行矩阵分解的方法,统称为计数模型;并将基于神经 网络的词向量模型统称为预测模型。Baroni等人在实验中使用“词-上下文”共 现矩阵的原始形式、SVD分解、NMF分解(非负矩阵分解)[62, 68]作为计数 模型的代表,使用 CBOW模型作为预测模型的代表。他们对这两类模型进行多 种语义特性的评价,包括语义相关性、同义词判别、概念分类、类比等。其实 验表明,预测模型对在各项指标中比计数模型有显著的优势。 然而,Milajevs等人在 2014年发表的文献 [77]中,给出了相反的结论。文 中指出,他们在实验中,尝试了向量逐元素相加、逐元素相乘等基本的语义组 合方式,使用这些组合方式表示短语以及句子的语义,并通过短语语义相似度 等指标进行评测。实验结果表明,基于共现矩阵的词表示方法相比神经网络的 词向量模型,其基本的语义组合能力更强。 Levy等人在 2015年发表的文献 [65]中尝试了多种不同的模型参数,包括 动态窗口或者固定窗口的选择、重新采样技术的效果、低频词的处理方式、上 下文的平滑方案等。实验发现,大部分参数设置的技巧,对基于共现矩阵的模 型以及基于神经网络的模型同时有效。",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 61,
      "context" : "矩阵,以及在其基础上进行矩阵分解的方法,统称为计数模型;并将基于神经 网络的词向量模型统称为预测模型。Baroni等人在实验中使用“词-上下文”共 现矩阵的原始形式、SVD分解、NMF分解(非负矩阵分解)[62, 68]作为计数 模型的代表,使用 CBOW模型作为预测模型的代表。他们对这两类模型进行多 种语义特性的评价,包括语义相关性、同义词判别、概念分类、类比等。其实 验表明,预测模型对在各项指标中比计数模型有显著的优势。 然而,Milajevs等人在 2014年发表的文献 [77]中,给出了相反的结论。文 中指出,他们在实验中,尝试了向量逐元素相加、逐元素相乘等基本的语义组 合方式,使用这些组合方式表示短语以及句子的语义,并通过短语语义相似度 等指标进行评测。实验结果表明,基于共现矩阵的词表示方法相比神经网络的 词向量模型,其基本的语义组合能力更强。 Levy等人在 2015年发表的文献 [65]中尝试了多种不同的模型参数,包括 动态窗口或者固定窗口的选择、重新采样技术的效果、低频词的处理方式、上 下文的平滑方案等。实验发现,大部分参数设置的技巧,对基于共现矩阵的模 型以及基于神经网络的模型同时有效。",
      "startOffset" : 402,
      "endOffset" : 406
    }, {
      "referenceID" : 69,
      "context" : "2 语料影响 语料规模方面,Mikolov等人在文献 [73]中发现,语料规模越大,CBOW 模型在类比任务(本文中的 syn任务和 sem任务)中效果更好。Pennington在文 献 [90]中指出,对于 GloVe而言,语料规模越大,句法问题的类比任务(syn) 效果越好,但是语义问题的类比任务(sem)却不一定。 语料领域方面,Stenetorp等人在 2012年 [111]发现,使用领域内的语料训 练得到的词表示,相比使用新闻语料训练得到的词向量,在进行生物医药领域 的命名实体识别任务时,有明显的优势。他们在实验中使用的词表示为布朗聚 类(Brown Clustering),与基于神经网络的模型不同,布朗聚类得到的词表示并 非是一个低维的实数向量。但是类似的是,无论是基于聚类的词表示方法还是 基于神经网络的词向量表示方法,他们都是根据语料统计建模得到的,因此语 料的影响也是类似的。",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 106,
      "context" : "2 语料影响 语料规模方面,Mikolov等人在文献 [73]中发现,语料规模越大,CBOW 模型在类比任务(本文中的 syn任务和 sem任务)中效果更好。Pennington在文 献 [90]中指出,对于 GloVe而言,语料规模越大,句法问题的类比任务(syn) 效果越好,但是语义问题的类比任务(sem)却不一定。 语料领域方面,Stenetorp等人在 2012年 [111]发现,使用领域内的语料训 练得到的词表示,相比使用新闻语料训练得到的词向量,在进行生物医药领域 的命名实体识别任务时,有明显的优势。他们在实验中使用的词表示为布朗聚 类(Brown Clustering),与基于神经网络的模型不同,布朗聚类得到的词表示并 非是一个低维的实数向量。但是类似的是,无论是基于聚类的词表示方法还是 基于神经网络的词向量表示方法,他们都是根据语料统计建模得到的,因此语 料的影响也是类似的。",
      "startOffset" : 188,
      "endOffset" : 193
    }, {
      "referenceID" : 126,
      "context" : "了性能上的提升。本文基于这一想法,提出了基于字表示的分词模型。该模型 利用神经网络字表示对字与字之间的关系进行建模,在较少参数的情况下,依 旧保持了较好的分词精度。 在基于字表示的分词模型中,获得有效的字表示是其中非常关键的一步。 但是现有的中文字表示工作中,大多数方法直接沿用了第二章中所介绍的英文 词向量的生成方法,将字当做模型的处理单元,建立每个字与其上下文字之间 的关系 [131]。但是,在中文中,如果想要通过词向量模型,直接获取字的“语 义”,可能会遇到一些障碍。如文献 [1]所述,词是能独立运用的最小语言单位, 语义是词或者词组与他们之间含义的关系,直接从字的层面分析语义,可能意 义并不大。由于这一原因,本文设计了基于字词联合训练的中文表示技术,该 方法使用词的语义空间对字进行建模,为字带来更好的表示。 另一方面,对于中文的词表示,大多数工作同样直接使用了处理英文的方 法。然而中文的符号系统是汉字,如果可以有效利用字与字之间的关系,以及 字与词之间的关系,势必能为中文词表示带来提升。Chen等人的工作 [15]通过 假设词的语义由其中字的意思以及词特有的意思融合而成,构造了 CWE模型。 该模型已经超越了直接使用英文词向量模型的效果。本文提出的字词联合训练 模型通过对汉字的有效建模,让这些汉字建立了一些词之间的关系,使得词的 上下文更丰富,从而提升了词表示的语义。实验结果表明,该模型的效果超过 了现有利用字信息提升词义的方法。 本章后续内容安排如下:第 4.",
      "startOffset" : 190,
      "endOffset" : 195
    }, {
      "referenceID" : 13,
      "context" : "了性能上的提升。本文基于这一想法,提出了基于字表示的分词模型。该模型 利用神经网络字表示对字与字之间的关系进行建模,在较少参数的情况下,依 旧保持了较好的分词精度。 在基于字表示的分词模型中,获得有效的字表示是其中非常关键的一步。 但是现有的中文字表示工作中,大多数方法直接沿用了第二章中所介绍的英文 词向量的生成方法,将字当做模型的处理单元,建立每个字与其上下文字之间 的关系 [131]。但是,在中文中,如果想要通过词向量模型,直接获取字的“语 义”,可能会遇到一些障碍。如文献 [1]所述,词是能独立运用的最小语言单位, 语义是词或者词组与他们之间含义的关系,直接从字的层面分析语义,可能意 义并不大。由于这一原因,本文设计了基于字词联合训练的中文表示技术,该 方法使用词的语义空间对字进行建模,为字带来更好的表示。 另一方面,对于中文的词表示,大多数工作同样直接使用了处理英文的方 法。然而中文的符号系统是汉字,如果可以有效利用字与字之间的关系,以及 字与词之间的关系,势必能为中文词表示带来提升。Chen等人的工作 [15]通过 假设词的语义由其中字的意思以及词特有的意思融合而成,构造了 CWE模型。 该模型已经超越了直接使用英文词向量模型的效果。本文提出的字词联合训练 模型通过对汉字的有效建模,让这些汉字建立了一些词之间的关系,使得词的 上下文更丰富,从而提升了词表示的语义。实验结果表明,该模型的效果超过 了现有利用字信息提升词义的方法。 本章后续内容安排如下:第 4.",
      "startOffset" : 463,
      "endOffset" : 467
    }, {
      "referenceID" : 69,
      "context" : "1 表示学习 传统机器学习方法中,特征的选取是其中最耗时的一个环节。这一环节需 要耗费领域专家大量的精力,针对具体任务进行分析,从而设计出有效的特征。 表示学习的提出缓解了这一严峻的问题。表示学习的目标是通过算法自动学习 得到特征表示,使得机器学习算法可以更高效地运转。 在自然语言中,最基础的语义单元为词。研究人员已经提出了大量词表示 的学习方法,如:Skip-gram [73]、CBOW [73]、NNLM [7]等模型,这些模型在",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 69,
      "context" : "1 表示学习 传统机器学习方法中,特征的选取是其中最耗时的一个环节。这一环节需 要耗费领域专家大量的精力,针对具体任务进行分析,从而设计出有效的特征。 表示学习的提出缓解了这一严峻的问题。表示学习的目标是通过算法自动学习 得到特征表示,使得机器学习算法可以更高效地运转。 在自然语言中,最基础的语义单元为词。研究人员已经提出了大量词表示 的学习方法,如:Skip-gram [73]、CBOW [73]、NNLM [7]等模型,这些模型在",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 5,
      "context" : "1 表示学习 传统机器学习方法中,特征的选取是其中最耗时的一个环节。这一环节需 要耗费领域专家大量的精力,针对具体任务进行分析,从而设计出有效的特征。 表示学习的提出缓解了这一严峻的问题。表示学习的目标是通过算法自动学习 得到特征表示,使得机器学习算法可以更高效地运转。 在自然语言中,最基础的语义单元为词。研究人员已经提出了大量词表示 的学习方法,如:Skip-gram [73]、CBOW [73]、NNLM [7]等模型,这些模型在",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 13,
      "context" : "与本文同期,已经有两项工作跳出了分布假说的框架,将词的内部元素考 虑到词表示中。在中文词表示方面,Chen等人在 2015年发表的文献 [15]中提 出了 CWE+P等模型(结构如图 4-1),这一模型改进自 Skip-gram模型(也适用 于 CBOW模型),将一个词拆分成两部分:词本身和组成这个词的汉字。训练 过程中,使用词本身的向量以及组成这个词的各个字向量的平均值表示这个词 的语义。 在英文词表示方面,Sun等人在 2016年发表的文献 [112]中提出了 SEING 模型(结构如图 4-2所示)。该模型认为英文中具有相同语素(morpheme)的单 词具有相似的语义,因此在建模时使用 Skip-gram的思路,对于目标词不仅预测 上下文的词,也预测目标词的所有语素。这套方法也可以近似沿用到中文处理, 将英文单词的语素类比为中文的汉字。 文献 [15]与 [112]均为对词表示的改进。然而在中文分词、词性标注等任务 中,对汉字的表示也是一个必不可少的环节。现有的字表示方法,都直接沿用 了英文中对单词的建模方法,将训练语料拆分成字级别,建立语料中每个字与 其上下文其它字之间的关系 [131]。对于字表示的方法,需要有进一步的探索。",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 107,
      "context" : "与本文同期,已经有两项工作跳出了分布假说的框架,将词的内部元素考 虑到词表示中。在中文词表示方面,Chen等人在 2015年发表的文献 [15]中提 出了 CWE+P等模型(结构如图 4-1),这一模型改进自 Skip-gram模型(也适用 于 CBOW模型),将一个词拆分成两部分:词本身和组成这个词的汉字。训练 过程中,使用词本身的向量以及组成这个词的各个字向量的平均值表示这个词 的语义。 在英文词表示方面,Sun等人在 2016年发表的文献 [112]中提出了 SEING 模型(结构如图 4-2所示)。该模型认为英文中具有相同语素(morpheme)的单 词具有相似的语义,因此在建模时使用 Skip-gram的思路,对于目标词不仅预测 上下文的词,也预测目标词的所有语素。这套方法也可以近似沿用到中文处理, 将英文单词的语素类比为中文的汉字。 文献 [15]与 [112]均为对词表示的改进。然而在中文分词、词性标注等任务 中,对汉字的表示也是一个必不可少的环节。现有的字表示方法,都直接沿用 了英文中对单词的建模方法,将训练语料拆分成字级别,建立语料中每个字与 其上下文其它字之间的关系 [131]。对于字表示的方法,需要有进一步的探索。",
      "startOffset" : 225,
      "endOffset" : 230
    }, {
      "referenceID" : 13,
      "context" : "与本文同期,已经有两项工作跳出了分布假说的框架,将词的内部元素考 虑到词表示中。在中文词表示方面,Chen等人在 2015年发表的文献 [15]中提 出了 CWE+P等模型(结构如图 4-1),这一模型改进自 Skip-gram模型(也适用 于 CBOW模型),将一个词拆分成两部分:词本身和组成这个词的汉字。训练 过程中,使用词本身的向量以及组成这个词的各个字向量的平均值表示这个词 的语义。 在英文词表示方面,Sun等人在 2016年发表的文献 [112]中提出了 SEING 模型(结构如图 4-2所示)。该模型认为英文中具有相同语素(morpheme)的单 词具有相似的语义,因此在建模时使用 Skip-gram的思路,对于目标词不仅预测 上下文的词,也预测目标词的所有语素。这套方法也可以近似沿用到中文处理, 将英文单词的语素类比为中文的汉字。 文献 [15]与 [112]均为对词表示的改进。然而在中文分词、词性标注等任务 中,对汉字的表示也是一个必不可少的环节。现有的字表示方法,都直接沿用 了英文中对单词的建模方法,将训练语料拆分成字级别,建立语料中每个字与 其上下文其它字之间的关系 [131]。对于字表示的方法,需要有进一步的探索。",
      "startOffset" : 381,
      "endOffset" : 385
    }, {
      "referenceID" : 107,
      "context" : "与本文同期,已经有两项工作跳出了分布假说的框架,将词的内部元素考 虑到词表示中。在中文词表示方面,Chen等人在 2015年发表的文献 [15]中提 出了 CWE+P等模型(结构如图 4-1),这一模型改进自 Skip-gram模型(也适用 于 CBOW模型),将一个词拆分成两部分:词本身和组成这个词的汉字。训练 过程中,使用词本身的向量以及组成这个词的各个字向量的平均值表示这个词 的语义。 在英文词表示方面,Sun等人在 2016年发表的文献 [112]中提出了 SEING 模型(结构如图 4-2所示)。该模型认为英文中具有相同语素(morpheme)的单 词具有相似的语义,因此在建模时使用 Skip-gram的思路,对于目标词不仅预测 上下文的词,也预测目标词的所有语素。这套方法也可以近似沿用到中文处理, 将英文单词的语素类比为中文的汉字。 文献 [15]与 [112]均为对词表示的改进。然而在中文分词、词性标注等任务 中,对汉字的表示也是一个必不可少的环节。现有的字表示方法,都直接沿用 了英文中对单词的建模方法,将训练语料拆分成字级别,建立语料中每个字与 其上下文其它字之间的关系 [131]。对于字表示的方法,需要有进一步的探索。",
      "startOffset" : 387,
      "endOffset" : 392
    }, {
      "referenceID" : 126,
      "context" : "与本文同期,已经有两项工作跳出了分布假说的框架,将词的内部元素考 虑到词表示中。在中文词表示方面,Chen等人在 2015年发表的文献 [15]中提 出了 CWE+P等模型(结构如图 4-1),这一模型改进自 Skip-gram模型(也适用 于 CBOW模型),将一个词拆分成两部分:词本身和组成这个词的汉字。训练 过程中,使用词本身的向量以及组成这个词的各个字向量的平均值表示这个词 的语义。 在英文词表示方面,Sun等人在 2016年发表的文献 [112]中提出了 SEING 模型(结构如图 4-2所示)。该模型认为英文中具有相同语素(morpheme)的单 词具有相似的语义,因此在建模时使用 Skip-gram的思路,对于目标词不仅预测 上下文的词,也预测目标词的所有语素。这套方法也可以近似沿用到中文处理, 将英文单词的语素类比为中文的汉字。 文献 [15]与 [112]均为对词表示的改进。然而在中文分词、词性标注等任务 中,对汉字的表示也是一个必不可少的环节。现有的字表示方法,都直接沿用 了英文中对单词的建模方法,将训练语料拆分成字级别,建立语料中每个字与 其上下文其它字之间的关系 [131]。对于字表示的方法,需要有进一步的探索。",
      "startOffset" : 501,
      "endOffset" : 506
    }, {
      "referenceID" : 120,
      "context" : "传统中文分词方法依赖词典匹配,并通过贪心算法截取可能的最大长度词 进行有限的歧义消除。常用的贪心策略有正向最大匹配法、逆向最大匹配法和 双向匹配法等。然而,基于词典方法存在两个明显的缺陷,即不能很好地处理 词边界歧义和未登录词(OOV)。为了解决中文分词的这两个关键问题,许多研 究工作集中到了基于字标注的机器学习中文分词方法。 基于字标注的中文分词方法基本假设是一个词语内部文本高内聚,而词语 边界与外部文字低耦合。通过统计机器学习方法学习判断词界是当前中文分词 的主流做法。现有工作大多使用序列标注模型执行 BMES标注。Xue等人提出 了基于 HMM 模型的字标注中文分词方法 [125]。刘群等提出一种基于层叠隐 马模型的汉语词法分析方法 [132]。该方法引入角色 HMM识别未登录词,使用 Viterbi算法标注出全局最优的角色序列。同时,该方法还提出了一种基于 N-最 短路径的策略进行切分排歧。Wang等人使用基于字分类的 CRF模型进行中文 词法分析 [89]。对基于字标注中文分词方法的改进包括引入更多的标签和设计 更有效的特征 [114, 129]、联合使用产生式模型和判别式模型以融合两者的优点 [119]以及将无监督方法中使用的特征引入有监督方法中 [130]等。这些传统统 计机器学习方法依赖于人工设计的特征,设计特征需要大量的人工参与,设计 有效的特征非常费时费力。而将表示学习方法引入机器学习中,可以将特征表 示这一步交给算法完成,在一定程度上减少人工,提升效率。",
      "startOffset" : 292,
      "endOffset" : 297
    }, {
      "referenceID" : 85,
      "context" : "传统中文分词方法依赖词典匹配,并通过贪心算法截取可能的最大长度词 进行有限的歧义消除。常用的贪心策略有正向最大匹配法、逆向最大匹配法和 双向匹配法等。然而,基于词典方法存在两个明显的缺陷,即不能很好地处理 词边界歧义和未登录词(OOV)。为了解决中文分词的这两个关键问题,许多研 究工作集中到了基于字标注的机器学习中文分词方法。 基于字标注的中文分词方法基本假设是一个词语内部文本高内聚,而词语 边界与外部文字低耦合。通过统计机器学习方法学习判断词界是当前中文分词 的主流做法。现有工作大多使用序列标注模型执行 BMES标注。Xue等人提出 了基于 HMM 模型的字标注中文分词方法 [125]。刘群等提出一种基于层叠隐 马模型的汉语词法分析方法 [132]。该方法引入角色 HMM识别未登录词,使用 Viterbi算法标注出全局最优的角色序列。同时,该方法还提出了一种基于 N-最 短路径的策略进行切分排歧。Wang等人使用基于字分类的 CRF模型进行中文 词法分析 [89]。对基于字标注中文分词方法的改进包括引入更多的标签和设计 更有效的特征 [114, 129]、联合使用产生式模型和判别式模型以融合两者的优点 [119]以及将无监督方法中使用的特征引入有监督方法中 [130]等。这些传统统 计机器学习方法依赖于人工设计的特征,设计特征需要大量的人工参与,设计 有效的特征非常费时费力。而将表示学习方法引入机器学习中,可以将特征表 示这一步交给算法完成,在一定程度上减少人工,提升效率。",
      "startOffset" : 435,
      "endOffset" : 439
    }, {
      "referenceID" : 109,
      "context" : "传统中文分词方法依赖词典匹配,并通过贪心算法截取可能的最大长度词 进行有限的歧义消除。常用的贪心策略有正向最大匹配法、逆向最大匹配法和 双向匹配法等。然而,基于词典方法存在两个明显的缺陷,即不能很好地处理 词边界歧义和未登录词(OOV)。为了解决中文分词的这两个关键问题,许多研 究工作集中到了基于字标注的机器学习中文分词方法。 基于字标注的中文分词方法基本假设是一个词语内部文本高内聚,而词语 边界与外部文字低耦合。通过统计机器学习方法学习判断词界是当前中文分词 的主流做法。现有工作大多使用序列标注模型执行 BMES标注。Xue等人提出 了基于 HMM 模型的字标注中文分词方法 [125]。刘群等提出一种基于层叠隐 马模型的汉语词法分析方法 [132]。该方法引入角色 HMM识别未登录词,使用 Viterbi算法标注出全局最优的角色序列。同时,该方法还提出了一种基于 N-最 短路径的策略进行切分排歧。Wang等人使用基于字分类的 CRF模型进行中文 词法分析 [89]。对基于字标注中文分词方法的改进包括引入更多的标签和设计 更有效的特征 [114, 129]、联合使用产生式模型和判别式模型以融合两者的优点 [119]以及将无监督方法中使用的特征引入有监督方法中 [130]等。这些传统统 计机器学习方法依赖于人工设计的特征,设计特征需要大量的人工参与,设计 有效的特征非常费时费力。而将表示学习方法引入机器学习中,可以将特征表 示这一步交给算法完成,在一定程度上减少人工,提升效率。",
      "startOffset" : 475,
      "endOffset" : 485
    }, {
      "referenceID" : 124,
      "context" : "传统中文分词方法依赖词典匹配,并通过贪心算法截取可能的最大长度词 进行有限的歧义消除。常用的贪心策略有正向最大匹配法、逆向最大匹配法和 双向匹配法等。然而,基于词典方法存在两个明显的缺陷,即不能很好地处理 词边界歧义和未登录词(OOV)。为了解决中文分词的这两个关键问题,许多研 究工作集中到了基于字标注的机器学习中文分词方法。 基于字标注的中文分词方法基本假设是一个词语内部文本高内聚,而词语 边界与外部文字低耦合。通过统计机器学习方法学习判断词界是当前中文分词 的主流做法。现有工作大多使用序列标注模型执行 BMES标注。Xue等人提出 了基于 HMM 模型的字标注中文分词方法 [125]。刘群等提出一种基于层叠隐 马模型的汉语词法分析方法 [132]。该方法引入角色 HMM识别未登录词,使用 Viterbi算法标注出全局最优的角色序列。同时,该方法还提出了一种基于 N-最 短路径的策略进行切分排歧。Wang等人使用基于字分类的 CRF模型进行中文 词法分析 [89]。对基于字标注中文分词方法的改进包括引入更多的标签和设计 更有效的特征 [114, 129]、联合使用产生式模型和判别式模型以融合两者的优点 [119]以及将无监督方法中使用的特征引入有监督方法中 [130]等。这些传统统 计机器学习方法依赖于人工设计的特征,设计特征需要大量的人工参与,设计 有效的特征非常费时费力。而将表示学习方法引入机器学习中,可以将特征表 示这一步交给算法完成,在一定程度上减少人工,提升效率。",
      "startOffset" : 475,
      "endOffset" : 485
    }, {
      "referenceID" : 114,
      "context" : "传统中文分词方法依赖词典匹配,并通过贪心算法截取可能的最大长度词 进行有限的歧义消除。常用的贪心策略有正向最大匹配法、逆向最大匹配法和 双向匹配法等。然而,基于词典方法存在两个明显的缺陷,即不能很好地处理 词边界歧义和未登录词(OOV)。为了解决中文分词的这两个关键问题,许多研 究工作集中到了基于字标注的机器学习中文分词方法。 基于字标注的中文分词方法基本假设是一个词语内部文本高内聚,而词语 边界与外部文字低耦合。通过统计机器学习方法学习判断词界是当前中文分词 的主流做法。现有工作大多使用序列标注模型执行 BMES标注。Xue等人提出 了基于 HMM 模型的字标注中文分词方法 [125]。刘群等提出一种基于层叠隐 马模型的汉语词法分析方法 [132]。该方法引入角色 HMM识别未登录词,使用 Viterbi算法标注出全局最优的角色序列。同时,该方法还提出了一种基于 N-最 短路径的策略进行切分排歧。Wang等人使用基于字分类的 CRF模型进行中文 词法分析 [89]。对基于字标注中文分词方法的改进包括引入更多的标签和设计 更有效的特征 [114, 129]、联合使用产生式模型和判别式模型以融合两者的优点 [119]以及将无监督方法中使用的特征引入有监督方法中 [130]等。这些传统统 计机器学习方法依赖于人工设计的特征,设计特征需要大量的人工参与,设计 有效的特征非常费时费力。而将表示学习方法引入机器学习中,可以将特征表 示这一步交给算法完成,在一定程度上减少人工,提升效率。",
      "startOffset" : 510,
      "endOffset" : 515
    }, {
      "referenceID" : 125,
      "context" : "传统中文分词方法依赖词典匹配,并通过贪心算法截取可能的最大长度词 进行有限的歧义消除。常用的贪心策略有正向最大匹配法、逆向最大匹配法和 双向匹配法等。然而,基于词典方法存在两个明显的缺陷,即不能很好地处理 词边界歧义和未登录词(OOV)。为了解决中文分词的这两个关键问题,许多研 究工作集中到了基于字标注的机器学习中文分词方法。 基于字标注的中文分词方法基本假设是一个词语内部文本高内聚,而词语 边界与外部文字低耦合。通过统计机器学习方法学习判断词界是当前中文分词 的主流做法。现有工作大多使用序列标注模型执行 BMES标注。Xue等人提出 了基于 HMM 模型的字标注中文分词方法 [125]。刘群等提出一种基于层叠隐 马模型的汉语词法分析方法 [132]。该方法引入角色 HMM识别未登录词,使用 Viterbi算法标注出全局最优的角色序列。同时,该方法还提出了一种基于 N-最 短路径的策略进行切分排歧。Wang等人使用基于字分类的 CRF模型进行中文 词法分析 [89]。对基于字标注中文分词方法的改进包括引入更多的标签和设计 更有效的特征 [114, 129]、联合使用产生式模型和判别式模型以融合两者的优点 [119]以及将无监督方法中使用的特征引入有监督方法中 [130]等。这些传统统 计机器学习方法依赖于人工设计的特征,设计特征需要大量的人工参与,设计 有效的特征非常费时费力。而将表示学习方法引入机器学习中,可以将特征表 示这一步交给算法完成,在一定程度上减少人工,提升效率。",
      "startOffset" : 538,
      "endOffset" : 543
    }, {
      "referenceID" : 126,
      "context" : "分布假说认为,词的语义由其上下文决定。根据分布假说构造的词向量模型 也在各项任务中取得了一定的成果。但是在中文里,最自然的语言单位是“字”。 不同于富含语义信息的词,字仅为记录汉语用的符号系统,本身不具备语义 [1]。 在一些现有的中文向量表示的工作中,直接将分布语义推广到字表示中,使用 上下文中各个字的分布作为当前字的表示 [131]。但由于字本身不具备语义,这 种方式的效果会受到一定的约束。 为了让字的表示具有更丰富的语义信息,本文借鉴了分布假说的思想,提 出利用某个字上下文中各个词的分布,作为这个字的表示。虽然字本身仍然不 具备语义信息,但是利用这种表示,把字放入词的语义空间中,通过字词联合 训练,可以更有效地对字进行建模。 本文第二章和第三章对若干现有的词向量模型进行了阐述和分析,这里本 文选取其中一个对上下文建模最为直接的模型,Skip-gram模型,并在此基础上 实现字词联合训练的想法。如公式 4.",
      "startOffset" : 164,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "5) 输出层一共有 4个节点,使用 softmax[11]归一化后,分别表示这个字被打上 B、 M、E、S标签的概率:",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "本文使用随机梯度下降法 [10]来优化上述训练目标。每次迭代,随机选取 一个样本 w, tagw,使用下式进行一次梯度迭代。式中,α是学习速率。 θ ← θ + α logP (tagw|w, θ) ∂θ (4.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 16,
      "context" : "1小节(第 33页)中提出的性能增益率来评价 这几种不同的字向量。以随机字向量作为基准,四种字向量中 F值最高的大语 料字词向量作为对比,各字向量的性能增益率为:小语料字向量 60%,大语料 字向量 65%,小语料字词向量 85%,大语料字词向量 100%。 本文对比在两种语料下,字词联合训练与单独训练字表示。实验表明,字词 联合训练在两种语料下,在性能增益率上分别有 25到 35个百分点的提升,充 分说明了字词联合训练对于中文字表示有非常显著的提升。 对比不同语料下,字向量带来的性能提升,可以发现,在单独对字进行训 练时,大语料能对字向量带来微弱的提升;而如果使用字词联合训练,大语料相 对小语料能明显提高字向量的性能。本文认为之所以使用字词联合训练时,大 语料更能体现出其效果,是因为如果只针对字进行训练,字向量的上下文空间 局限在字符层面,相比而言,字词联合训练时,字向量的上下文空间为词的语 义空间,信息更为丰富。而维基百科大语料中本身拥有了大量的语义信息,只 有通过表达能力强的表示才能充分捕获到其中的信息。 综合对比基于字向量的神经网络分词模型与传统的机器学习分词方法(基 准实验),神经网络分词模型虽然也只使用了各个词中的字的表示作为输入,相 比使用一元特征的传统分词方法,有非常显著的提升。然而不可否认,神经网 络分词模型相比二元特征仍然有一定的差距。这些差距可能需要更有效的字表 示技术以及更好的神经网络模型来弥补。Collobert在文献 [18]中也得到了类似 的结论,在词性标注、命名实体识别等序列标注任务中,他们的神经网络方法 与传统基于特征工程的机器学习方法仍然有一些差距。他们在实验中指出,神 经网络模型配合少量人工的先验知识,就可以达到以往通过人工精心设计特征 才能达到的性能。",
      "startOffset" : 633,
      "endOffset" : 637
    }, {
      "referenceID" : 13,
      "context" : "3 词表示的实验 为了说明字词联合训练对词表示的作用,本节使用语义相关性任务和文本 分类任务对联合训练中得到的词表示进行评价,并与 Chen等人 [15]与 Sun等",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 107,
      "context" : "人 [112]的模型进行对比分析。",
      "startOffset" : 2,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "2小节提到的 ws任务类似,这里选用文献 [15]提供的两个中文语义相 关性数据集进行评价。这两个数据集分别含有 297个词对以及 240个词对,在 下文中简写为 ws297和 ws240。数据集中的每个词对都有若干位标注者对其进 行打分(两个数据集的打分范围分别为 0到 10以及 0到 5),分数越高表示标 注人员认为这两个词的语义更相关或者更相似。例如,词对“饮料、汽车”的 平均打分为 1.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "考虑字在其中的作用。本文提出的字词联合训练在 Skip-gram 模型的基础上加 入了对字的建模,因此对比 Skip-gram模型的提升,就是字词联合训练对词表示 带来的实际提升。 进一步地,为了说明字词联合训练的有效性,本文还将Chen等人的CWE+P 模型 [15]与 Sun等人的 SEING模型 [112]纳入对比。Chen等人的模型同样也 是对 Skip-gram模型的改进,这种方法将一个词拆分成两部分:词和组成这个词 的各个汉字,在表示这个词的语义时,使用词本身的向量以及组成这个词的各 个字向量的平均值。Sun等人的方法原本只对英文进行建模,该模型认为英文 中具有相同语素的单词具有相似的语义,因此在建模时使用 Skip-gram的思路, 对于目标词不仅预测上下文的词,也预测目标词的所有语素。这套方法也可以 近似沿用到中文处理,将英文单词的语素类比为中文的汉字。",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 107,
      "context" : "考虑字在其中的作用。本文提出的字词联合训练在 Skip-gram 模型的基础上加 入了对字的建模,因此对比 Skip-gram模型的提升,就是字词联合训练对词表示 带来的实际提升。 进一步地,为了说明字词联合训练的有效性,本文还将Chen等人的CWE+P 模型 [15]与 Sun等人的 SEING模型 [112]纳入对比。Chen等人的模型同样也 是对 Skip-gram模型的改进,这种方法将一个词拆分成两部分:词和组成这个词 的各个汉字,在表示这个词的语义时,使用词本身的向量以及组成这个词的各 个字向量的平均值。Sun等人的方法原本只对英文进行建模,该模型认为英文 中具有相同语素的单词具有相似的语义,因此在建模时使用 Skip-gram的思路, 对于目标词不仅预测上下文的词,也预测目标词的所有语素。这套方法也可以 近似沿用到中文处理,将英文单词的语素类比为中文的汉字。",
      "startOffset" : 152,
      "endOffset" : 157
    }, {
      "referenceID" : 13,
      "context" : "模型 ws240 ws297 CWE+P [15] 44.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 107,
      "context" : "43 SEING [112] 42.",
      "startOffset" : 9,
      "endOffset" : 14
    }, {
      "referenceID" : 13,
      "context" : "5,即各模型对字和词的建模比例为 1 : 1。对于 CWE+P模型与 SEING模型,文献 [15]和 [112]并没有对字和词的建模比例进行考虑,也可以 认为这两个模型对字词的建模比例为 1 : 1。 为了更进一步说明本文提出的字词联合训练的有效性,这里调节公式 4.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 107,
      "context" : "5,即各模型对字和词的建模比例为 1 : 1。对于 CWE+P模型与 SEING模型,文献 [15]和 [112]并没有对字和词的建模比例进行考虑,也可以 认为这两个模型对字词的建模比例为 1 : 1。 为了更进一步说明本文提出的字词联合训练的有效性,这里调节公式 4.",
      "startOffset" : 52,
      "endOffset" : 57
    }, {
      "referenceID" : 107,
      "context" : "5486,略低于字词联合训练);而 SEING模 型几乎随着对汉字建模比例的增加,效果一直变差。 本文提出的字词联合训练与 CWE+P模型或者 SEING模型均为对字(或者 语素)和词同时建模,虽然建模方式有所不同,但是为什么这三种方法在实验 中呈现出较大的差异?尤其是 SEING 模型,对于英文词义表示有显著的提升 [112],然而在中文词义相似度任务中却起到了负面效果。为了回答这一问题, 本文首先分析这三种模型对字词建模的区别。 CWE+P模型的基础假设是词的语义可以拆解成两部分:词中的各个汉字 (模型名称中的 +P表示汉字需加上前后缀的标记)的语义,以及这个词特有的 语义。在文献 [15]中,作者将这两部分组合成实际词向量时,使用了向量加法。 在这种设定下,字词所占语义比例需要仔细调节,如果比例过高,则词的语义 几乎直接由字决定,在现代汉语中这一点并不成立。从前面的实验中也看出,字 的比例在 10%左右时可以让词义达到最佳效果,原文中设定的 50% 很可能不 是最佳参数。从另一个角度看,由于目前 CWE模型只使用了朴素的向量加法作 为词义合成,如果选用更合理的组合函数,应当能达到更好的效果。",
      "startOffset" : 160,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "5486,略低于字词联合训练);而 SEING模 型几乎随着对汉字建模比例的增加,效果一直变差。 本文提出的字词联合训练与 CWE+P模型或者 SEING模型均为对字(或者 语素)和词同时建模,虽然建模方式有所不同,但是为什么这三种方法在实验 中呈现出较大的差异?尤其是 SEING 模型,对于英文词义表示有显著的提升 [112],然而在中文词义相似度任务中却起到了负面效果。为了回答这一问题, 本文首先分析这三种模型对字词建模的区别。 CWE+P模型的基础假设是词的语义可以拆解成两部分:词中的各个汉字 (模型名称中的 +P表示汉字需加上前后缀的标记)的语义,以及这个词特有的 语义。在文献 [15]中,作者将这两部分组合成实际词向量时,使用了向量加法。 在这种设定下,字词所占语义比例需要仔细调节,如果比例过高,则词的语义 几乎直接由字决定,在现代汉语中这一点并不成立。从前面的实验中也看出,字 的比例在 10%左右时可以让词义达到最佳效果,原文中设定的 50% 很可能不 是最佳参数。从另一个角度看,由于目前 CWE模型只使用了朴素的向量加法作 为词义合成,如果选用更合理的组合函数,应当能达到更好的效果。",
      "startOffset" : 297,
      "endOffset" : 301
    }, {
      "referenceID" : 0,
      "context" : "1 引言 文本分类在网页检索、信息筛选、情感分析等任务中是一个至关重要的步 骤 [2]。文本分类中的关键问题在于文本表示,在传统机器学习方法中,通常以 特征表示的形式出现。文本分类中最常用的特征表示方法是词袋子模型。词袋 子模型中,最常用的特征是词、二元词组、多元词组(n-gram)以及一些人工抽 取的模板特征。在以特征的形式表示文本之后,传统模型往往使用词频、互信 息 [19]、pLSA [13]、LDA [38]等方法筛选出最有效的特征。然而,传统方法在 表示文本时,会忽略上下文信息,同时也会丢失词序信息。比如以下例子:",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "1 引言 文本分类在网页检索、信息筛选、情感分析等任务中是一个至关重要的步 骤 [2]。文本分类中的关键问题在于文本表示,在传统机器学习方法中,通常以 特征表示的形式出现。文本分类中最常用的特征表示方法是词袋子模型。词袋 子模型中,最常用的特征是词、二元词组、多元词组(n-gram)以及一些人工抽 取的模板特征。在以特征的形式表示文本之后,传统模型往往使用词频、互信 息 [19]、pLSA [13]、LDA [38]等方法筛选出最有效的特征。然而,传统方法在 表示文本时,会忽略上下文信息,同时也会丢失词序信息。比如以下例子:",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "1 引言 文本分类在网页检索、信息筛选、情感分析等任务中是一个至关重要的步 骤 [2]。文本分类中的关键问题在于文本表示,在传统机器学习方法中,通常以 特征表示的形式出现。文本分类中最常用的特征表示方法是词袋子模型。词袋 子模型中,最常用的特征是词、二元词组、多元词组(n-gram)以及一些人工抽 取的模板特征。在以特征的形式表示文本之后,传统模型往往使用词频、互信 息 [19]、pLSA [13]、LDA [38]等方法筛选出最有效的特征。然而,传统方法在 表示文本时,会忽略上下文信息,同时也会丢失词序信息。比如以下例子:",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 34,
      "context" : "1 引言 文本分类在网页检索、信息筛选、情感分析等任务中是一个至关重要的步 骤 [2]。文本分类中的关键问题在于文本表示,在传统机器学习方法中,通常以 特征表示的形式出现。文本分类中最常用的特征表示方法是词袋子模型。词袋 子模型中,最常用的特征是词、二元词组、多元词组(n-gram)以及一些人工抽 取的模板特征。在以特征的形式表示文本之后,传统模型往往使用词频、互信 息 [19]、pLSA [13]、LDA [38]等方法筛选出最有效的特征。然而,传统方法在 表示文本时,会忽略上下文信息,同时也会丢失词序信息。比如以下例子:",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 89,
      "context" : "分析句子中的“Bank”一词,如果孤立地看这个词,我们并不知道这是表示“河 岸”还是“银行”,这时就需要通过上下文对词义进行消歧。当纳入“Bank”的 前一个词,看二元词组“South Bank”时,可以发现两个单词的首字母都是大写 的,这对于不了解伦敦的人来说,很可能会以为是“南方银行”。当看了足够的 上下文“stroll along the South Bank”,我们才可以肯定这里说的是南岸,和银行 无关。尽管传统特征中诸如多元词组以及更复杂的特征(如树核 [94])也能捕 获词序信息,但是这些特征往往会遇到数据稀疏问题,影响到文本分类的精度。 近年来,预训练词向量以及深度神经网络模型为自然语言处理带来了新的 思路。本文的第二章和第三章已经介绍了词向量可以从无标注的文本中自动学 习得到语义和语法信息,并且能对各项自然语言处理任务带来性能上显著的提 升。在词向量的帮助下,有人提出一些组合语义的方法来表示文本的语义。",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 100,
      "context" : "Socher等人在 2011年 [105, 109]、2013年 [110]发表了递归神经网络(Recursive Neural Network)的相关工作。该方法被证实在构建句子级语义时较为有 效。然而,递归神经网络需要按照一个树形结构来构建句子的语义,其性能依 赖于构建文本树的精度。而且,构建这棵树需要至少 O(n)的时间复杂度,其 中 n表示句子的长度。当模型在处理长句子或者文档时,所花费的时间往往是 不可接受的。更进一步地,在做文档表示时,两个句子之间的关系不一定能构 成树形结构。因此递归神经网络可能不适合构建长句子或者文档的语义。 循环神经网络(Recurrent Neural Network)可以在 O(n)时间内构建文本的 语义 [26]。该模型逐词处理整个文档,并把所有上文的语义保存到一个固定大 小的隐藏层中。循环神经网络的优势在于它可以更好地捕捉上下文信息,对长 距离的上下文信息进行建模。然而,循环神经网络是一个有偏的模型,如对于 正向的循环神经网络而言,文本中靠后的词相对靠前的词占据了更主导的地位。 由于这一语义偏置的特性,循环神经网络在构建整个文本的语义时,会更多地 包含文本后面部分的信息。但是实际上并非所有文本的重点都放在最后,这可 能会影响其生成的语义表示的精确度。 为了解决语义偏置的问题,有人提出用卷积神经网络(Convolutional Neural Network)来构建文本语义 [18]。卷积神经网络利用最大池化技术能从文本中找 出最有用的文本片段,其复杂度也是 O(n)。因此卷积神经网络在构建文本语义 时有更大的潜力。然而,现有卷积神经网络的模型总是使用比较简单的卷积核, 如固定窗口 [18, 46]。在使用这类模型时,如何确定窗口大小是一个关键问题。 当窗口太小时,可能导致上下文信息保留不足,难以对词进行精确刻画;而当 窗口太大时,会导致参数过多,增加模型优化难度。因此,需要考虑,如何构 建模型,才能更好地捕获上下文信息,减少选择窗口大小带来的困难。并以此 为基础来更好地完成文本分类的任务。 为了解决上述模型的缺陷,本文提出了循环卷积网络(Recurrent Convolutional Neural Network),并将其用到文本分类中。首先,本文使用一个双向循环 结构对上下文进行建模。对比基于窗口的上下文建模方法,循环结构的参数更 少,在引入较少噪声的前提下,可以捕获尽可能远的上下文信息,从而保留长 距离的词序信息。第二,本文使用最大池化技术自动判断对文本分类最重要的 特征。结合这两者,循环卷积网络同时拥有了循环神经网络和卷积神经网络的 优势,既能很好地刻画上下文信息,又能无偏地描述整个文本的内容,并且其",
      "startOffset" : 16,
      "endOffset" : 26
    }, {
      "referenceID" : 104,
      "context" : "Socher等人在 2011年 [105, 109]、2013年 [110]发表了递归神经网络(Recursive Neural Network)的相关工作。该方法被证实在构建句子级语义时较为有 效。然而,递归神经网络需要按照一个树形结构来构建句子的语义,其性能依 赖于构建文本树的精度。而且,构建这棵树需要至少 O(n)的时间复杂度,其 中 n表示句子的长度。当模型在处理长句子或者文档时,所花费的时间往往是 不可接受的。更进一步地,在做文档表示时,两个句子之间的关系不一定能构 成树形结构。因此递归神经网络可能不适合构建长句子或者文档的语义。 循环神经网络(Recurrent Neural Network)可以在 O(n)时间内构建文本的 语义 [26]。该模型逐词处理整个文档,并把所有上文的语义保存到一个固定大 小的隐藏层中。循环神经网络的优势在于它可以更好地捕捉上下文信息,对长 距离的上下文信息进行建模。然而,循环神经网络是一个有偏的模型,如对于 正向的循环神经网络而言,文本中靠后的词相对靠前的词占据了更主导的地位。 由于这一语义偏置的特性,循环神经网络在构建整个文本的语义时,会更多地 包含文本后面部分的信息。但是实际上并非所有文本的重点都放在最后,这可 能会影响其生成的语义表示的精确度。 为了解决语义偏置的问题,有人提出用卷积神经网络(Convolutional Neural Network)来构建文本语义 [18]。卷积神经网络利用最大池化技术能从文本中找 出最有用的文本片段,其复杂度也是 O(n)。因此卷积神经网络在构建文本语义 时有更大的潜力。然而,现有卷积神经网络的模型总是使用比较简单的卷积核, 如固定窗口 [18, 46]。在使用这类模型时,如何确定窗口大小是一个关键问题。 当窗口太小时,可能导致上下文信息保留不足,难以对词进行精确刻画;而当 窗口太大时,会导致参数过多,增加模型优化难度。因此,需要考虑,如何构 建模型,才能更好地捕获上下文信息,减少选择窗口大小带来的困难。并以此 为基础来更好地完成文本分类的任务。 为了解决上述模型的缺陷,本文提出了循环卷积网络(Recurrent Convolutional Neural Network),并将其用到文本分类中。首先,本文使用一个双向循环 结构对上下文进行建模。对比基于窗口的上下文建模方法,循环结构的参数更 少,在引入较少噪声的前提下,可以捕获尽可能远的上下文信息,从而保留长 距离的词序信息。第二,本文使用最大池化技术自动判断对文本分类最重要的 特征。结合这两者,循环卷积网络同时拥有了循环神经网络和卷积神经网络的 优势,既能很好地刻画上下文信息,又能无偏地描述整个文本的内容,并且其",
      "startOffset" : 16,
      "endOffset" : 26
    }, {
      "referenceID" : 105,
      "context" : "Socher等人在 2011年 [105, 109]、2013年 [110]发表了递归神经网络(Recursive Neural Network)的相关工作。该方法被证实在构建句子级语义时较为有 效。然而,递归神经网络需要按照一个树形结构来构建句子的语义,其性能依 赖于构建文本树的精度。而且,构建这棵树需要至少 O(n)的时间复杂度,其 中 n表示句子的长度。当模型在处理长句子或者文档时,所花费的时间往往是 不可接受的。更进一步地,在做文档表示时,两个句子之间的关系不一定能构 成树形结构。因此递归神经网络可能不适合构建长句子或者文档的语义。 循环神经网络(Recurrent Neural Network)可以在 O(n)时间内构建文本的 语义 [26]。该模型逐词处理整个文档,并把所有上文的语义保存到一个固定大 小的隐藏层中。循环神经网络的优势在于它可以更好地捕捉上下文信息,对长 距离的上下文信息进行建模。然而,循环神经网络是一个有偏的模型,如对于 正向的循环神经网络而言,文本中靠后的词相对靠前的词占据了更主导的地位。 由于这一语义偏置的特性,循环神经网络在构建整个文本的语义时,会更多地 包含文本后面部分的信息。但是实际上并非所有文本的重点都放在最后,这可 能会影响其生成的语义表示的精确度。 为了解决语义偏置的问题,有人提出用卷积神经网络(Convolutional Neural Network)来构建文本语义 [18]。卷积神经网络利用最大池化技术能从文本中找 出最有用的文本片段,其复杂度也是 O(n)。因此卷积神经网络在构建文本语义 时有更大的潜力。然而,现有卷积神经网络的模型总是使用比较简单的卷积核, 如固定窗口 [18, 46]。在使用这类模型时,如何确定窗口大小是一个关键问题。 当窗口太小时,可能导致上下文信息保留不足,难以对词进行精确刻画;而当 窗口太大时,会导致参数过多,增加模型优化难度。因此,需要考虑,如何构 建模型,才能更好地捕获上下文信息,减少选择窗口大小带来的困难。并以此 为基础来更好地完成文本分类的任务。 为了解决上述模型的缺陷,本文提出了循环卷积网络(Recurrent Convolutional Neural Network),并将其用到文本分类中。首先,本文使用一个双向循环 结构对上下文进行建模。对比基于窗口的上下文建模方法,循环结构的参数更 少,在引入较少噪声的前提下,可以捕获尽可能远的上下文信息,从而保留长 距离的词序信息。第二,本文使用最大池化技术自动判断对文本分类最重要的 特征。结合这两者,循环卷积网络同时拥有了循环神经网络和卷积神经网络的 优势,既能很好地刻画上下文信息,又能无偏地描述整个文本的内容,并且其",
      "startOffset" : 33,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : "Socher等人在 2011年 [105, 109]、2013年 [110]发表了递归神经网络(Recursive Neural Network)的相关工作。该方法被证实在构建句子级语义时较为有 效。然而,递归神经网络需要按照一个树形结构来构建句子的语义,其性能依 赖于构建文本树的精度。而且,构建这棵树需要至少 O(n)的时间复杂度,其 中 n表示句子的长度。当模型在处理长句子或者文档时,所花费的时间往往是 不可接受的。更进一步地,在做文档表示时,两个句子之间的关系不一定能构 成树形结构。因此递归神经网络可能不适合构建长句子或者文档的语义。 循环神经网络(Recurrent Neural Network)可以在 O(n)时间内构建文本的 语义 [26]。该模型逐词处理整个文档,并把所有上文的语义保存到一个固定大 小的隐藏层中。循环神经网络的优势在于它可以更好地捕捉上下文信息,对长 距离的上下文信息进行建模。然而,循环神经网络是一个有偏的模型,如对于 正向的循环神经网络而言,文本中靠后的词相对靠前的词占据了更主导的地位。 由于这一语义偏置的特性,循环神经网络在构建整个文本的语义时,会更多地 包含文本后面部分的信息。但是实际上并非所有文本的重点都放在最后,这可 能会影响其生成的语义表示的精确度。 为了解决语义偏置的问题,有人提出用卷积神经网络(Convolutional Neural Network)来构建文本语义 [18]。卷积神经网络利用最大池化技术能从文本中找 出最有用的文本片段,其复杂度也是 O(n)。因此卷积神经网络在构建文本语义 时有更大的潜力。然而,现有卷积神经网络的模型总是使用比较简单的卷积核, 如固定窗口 [18, 46]。在使用这类模型时,如何确定窗口大小是一个关键问题。 当窗口太小时,可能导致上下文信息保留不足,难以对词进行精确刻画;而当 窗口太大时,会导致参数过多,增加模型优化难度。因此,需要考虑,如何构 建模型,才能更好地捕获上下文信息,减少选择窗口大小带来的困难。并以此 为基础来更好地完成文本分类的任务。 为了解决上述模型的缺陷,本文提出了循环卷积网络(Recurrent Convolutional Neural Network),并将其用到文本分类中。首先,本文使用一个双向循环 结构对上下文进行建模。对比基于窗口的上下文建模方法,循环结构的参数更 少,在引入较少噪声的前提下,可以捕获尽可能远的上下文信息,从而保留长 距离的词序信息。第二,本文使用最大池化技术自动判断对文本分类最重要的 特征。结合这两者,循环卷积网络同时拥有了循环神经网络和卷积神经网络的 优势,既能很好地刻画上下文信息,又能无偏地描述整个文本的内容,并且其",
      "startOffset" : 326,
      "endOffset" : 330
    }, {
      "referenceID" : 16,
      "context" : "Socher等人在 2011年 [105, 109]、2013年 [110]发表了递归神经网络(Recursive Neural Network)的相关工作。该方法被证实在构建句子级语义时较为有 效。然而,递归神经网络需要按照一个树形结构来构建句子的语义,其性能依 赖于构建文本树的精度。而且,构建这棵树需要至少 O(n)的时间复杂度,其 中 n表示句子的长度。当模型在处理长句子或者文档时,所花费的时间往往是 不可接受的。更进一步地,在做文档表示时,两个句子之间的关系不一定能构 成树形结构。因此递归神经网络可能不适合构建长句子或者文档的语义。 循环神经网络(Recurrent Neural Network)可以在 O(n)时间内构建文本的 语义 [26]。该模型逐词处理整个文档,并把所有上文的语义保存到一个固定大 小的隐藏层中。循环神经网络的优势在于它可以更好地捕捉上下文信息,对长 距离的上下文信息进行建模。然而,循环神经网络是一个有偏的模型,如对于 正向的循环神经网络而言,文本中靠后的词相对靠前的词占据了更主导的地位。 由于这一语义偏置的特性,循环神经网络在构建整个文本的语义时,会更多地 包含文本后面部分的信息。但是实际上并非所有文本的重点都放在最后,这可 能会影响其生成的语义表示的精确度。 为了解决语义偏置的问题,有人提出用卷积神经网络(Convolutional Neural Network)来构建文本语义 [18]。卷积神经网络利用最大池化技术能从文本中找 出最有用的文本片段,其复杂度也是 O(n)。因此卷积神经网络在构建文本语义 时有更大的潜力。然而,现有卷积神经网络的模型总是使用比较简单的卷积核, 如固定窗口 [18, 46]。在使用这类模型时,如何确定窗口大小是一个关键问题。 当窗口太小时,可能导致上下文信息保留不足,难以对词进行精确刻画;而当 窗口太大时,会导致参数过多,增加模型优化难度。因此,需要考虑,如何构 建模型,才能更好地捕获上下文信息,减少选择窗口大小带来的困难。并以此 为基础来更好地完成文本分类的任务。 为了解决上述模型的缺陷,本文提出了循环卷积网络(Recurrent Convolutional Neural Network),并将其用到文本分类中。首先,本文使用一个双向循环 结构对上下文进行建模。对比基于窗口的上下文建模方法,循环结构的参数更 少,在引入较少噪声的前提下,可以捕获尽可能远的上下文信息,从而保留长 距离的词序信息。第二,本文使用最大池化技术自动判断对文本分类最重要的 特征。结合这两者,循环卷积网络同时拥有了循环神经网络和卷积神经网络的 优势,既能很好地刻画上下文信息,又能无偏地描述整个文本的内容,并且其",
      "startOffset" : 616,
      "endOffset" : 620
    }, {
      "referenceID" : 16,
      "context" : "Socher等人在 2011年 [105, 109]、2013年 [110]发表了递归神经网络(Recursive Neural Network)的相关工作。该方法被证实在构建句子级语义时较为有 效。然而,递归神经网络需要按照一个树形结构来构建句子的语义,其性能依 赖于构建文本树的精度。而且,构建这棵树需要至少 O(n)的时间复杂度,其 中 n表示句子的长度。当模型在处理长句子或者文档时,所花费的时间往往是 不可接受的。更进一步地,在做文档表示时,两个句子之间的关系不一定能构 成树形结构。因此递归神经网络可能不适合构建长句子或者文档的语义。 循环神经网络(Recurrent Neural Network)可以在 O(n)时间内构建文本的 语义 [26]。该模型逐词处理整个文档,并把所有上文的语义保存到一个固定大 小的隐藏层中。循环神经网络的优势在于它可以更好地捕捉上下文信息,对长 距离的上下文信息进行建模。然而,循环神经网络是一个有偏的模型,如对于 正向的循环神经网络而言,文本中靠后的词相对靠前的词占据了更主导的地位。 由于这一语义偏置的特性,循环神经网络在构建整个文本的语义时,会更多地 包含文本后面部分的信息。但是实际上并非所有文本的重点都放在最后,这可 能会影响其生成的语义表示的精确度。 为了解决语义偏置的问题,有人提出用卷积神经网络(Convolutional Neural Network)来构建文本语义 [18]。卷积神经网络利用最大池化技术能从文本中找 出最有用的文本片段,其复杂度也是 O(n)。因此卷积神经网络在构建文本语义 时有更大的潜力。然而,现有卷积神经网络的模型总是使用比较简单的卷积核, 如固定窗口 [18, 46]。在使用这类模型时,如何确定窗口大小是一个关键问题。 当窗口太小时,可能导致上下文信息保留不足,难以对词进行精确刻画;而当 窗口太大时,会导致参数过多,增加模型优化难度。因此,需要考虑,如何构 建模型,才能更好地捕获上下文信息,减少选择窗口大小带来的困难。并以此 为基础来更好地完成文本分类的任务。 为了解决上述模型的缺陷,本文提出了循环卷积网络(Recurrent Convolutional Neural Network),并将其用到文本分类中。首先,本文使用一个双向循环 结构对上下文进行建模。对比基于窗口的上下文建模方法,循环结构的参数更 少,在引入较少噪声的前提下,可以捕获尽可能远的上下文信息,从而保留长 距离的词序信息。第二,本文使用最大池化技术自动判断对文本分类最重要的 特征。结合这两者,循环卷积网络同时拥有了循环神经网络和卷积神经网络的 优势,既能很好地刻画上下文信息,又能无偏地描述整个文本的内容,并且其",
      "startOffset" : 722,
      "endOffset" : 730
    }, {
      "referenceID" : 42,
      "context" : "Socher等人在 2011年 [105, 109]、2013年 [110]发表了递归神经网络(Recursive Neural Network)的相关工作。该方法被证实在构建句子级语义时较为有 效。然而,递归神经网络需要按照一个树形结构来构建句子的语义,其性能依 赖于构建文本树的精度。而且,构建这棵树需要至少 O(n)的时间复杂度,其 中 n表示句子的长度。当模型在处理长句子或者文档时,所花费的时间往往是 不可接受的。更进一步地,在做文档表示时,两个句子之间的关系不一定能构 成树形结构。因此递归神经网络可能不适合构建长句子或者文档的语义。 循环神经网络(Recurrent Neural Network)可以在 O(n)时间内构建文本的 语义 [26]。该模型逐词处理整个文档,并把所有上文的语义保存到一个固定大 小的隐藏层中。循环神经网络的优势在于它可以更好地捕捉上下文信息,对长 距离的上下文信息进行建模。然而,循环神经网络是一个有偏的模型,如对于 正向的循环神经网络而言,文本中靠后的词相对靠前的词占据了更主导的地位。 由于这一语义偏置的特性,循环神经网络在构建整个文本的语义时,会更多地 包含文本后面部分的信息。但是实际上并非所有文本的重点都放在最后,这可 能会影响其生成的语义表示的精确度。 为了解决语义偏置的问题,有人提出用卷积神经网络(Convolutional Neural Network)来构建文本语义 [18]。卷积神经网络利用最大池化技术能从文本中找 出最有用的文本片段,其复杂度也是 O(n)。因此卷积神经网络在构建文本语义 时有更大的潜力。然而,现有卷积神经网络的模型总是使用比较简单的卷积核, 如固定窗口 [18, 46]。在使用这类模型时,如何确定窗口大小是一个关键问题。 当窗口太小时,可能导致上下文信息保留不足,难以对词进行精确刻画;而当 窗口太大时,会导致参数过多,增加模型优化难度。因此,需要考虑,如何构 建模型,才能更好地捕获上下文信息,减少选择窗口大小带来的困难。并以此 为基础来更好地完成文本分类的任务。 为了解决上述模型的缺陷,本文提出了循环卷积网络(Recurrent Convolutional Neural Network),并将其用到文本分类中。首先,本文使用一个双向循环 结构对上下文进行建模。对比基于窗口的上下文建模方法,循环结构的参数更 少,在引入较少噪声的前提下,可以捕获尽可能远的上下文信息,从而保留长 距离的词序信息。第二,本文使用最大池化技术自动判断对文本分类最重要的 特征。结合这两者,循环卷积网络同时拥有了循环神经网络和卷积神经网络的 优势,既能很好地刻画上下文信息,又能无偏地描述整个文本的内容,并且其",
      "startOffset" : 722,
      "endOffset" : 730
    }, {
      "referenceID" : 49,
      "context" : "在获取句子和文档的语义表示时,很容易想到直接沿用词的分布假说,对 文档进行建模。然而,如果采用分布假说直接生成句子或者文档的向量表示,会 遇到极大的数据稀疏问题。Koehn在 2005年发表的文献 [53]中曾对 Europarl语 料进行了统计,结果表明,在该语料中,总共有 47,889,787个词,其中有 304,786 个不同的词,出现至少 10次的词一共有 58,552个;而语料中一共有 1,920,209 个句子,其中不同的句子有 1,860,118个,至少出现 10次的句子更是只有 597 句。因此,如果将句子看成一个整体,用词向量模型来训练句子的表示,由于 绝大多数句子因为只出现过一次,训练的结果将毫无统计意义。另一方面,分 布假说是针对词义的假说,这种通过上下文获取语义的方式对句子和文档是否 有效,还有待讨论。因此需要寻求新的思路对句子和文档进行建模。 德国数学家弗雷格(Gottlob Frege)在 1892年就曾提出:一段话的语义由其 各组成部分的语义以及它们之间的组合方法所确定 [30]。现有的句子或者文档 表示也通常以该思路为基础,通过语义组合的方式获得。常用的组合语义组合 函数,如线性加权、矩阵乘法、张量乘法等,在 Hermann的文献 [36]中有详细 的总结。近年来基于神经网络的语义组合技术为文档表示带来了新的思路。从 神经网络的结构上看,主要可以分为三种方式:递归神经网络(5.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 32,
      "context" : "在获取句子和文档的语义表示时,很容易想到直接沿用词的分布假说,对 文档进行建模。然而,如果采用分布假说直接生成句子或者文档的向量表示,会 遇到极大的数据稀疏问题。Koehn在 2005年发表的文献 [53]中曾对 Europarl语 料进行了统计,结果表明,在该语料中,总共有 47,889,787个词,其中有 304,786 个不同的词,出现至少 10次的词一共有 58,552个;而语料中一共有 1,920,209 个句子,其中不同的句子有 1,860,118个,至少出现 10次的句子更是只有 597 句。因此,如果将句子看成一个整体,用词向量模型来训练句子的表示,由于 绝大多数句子因为只出现过一次,训练的结果将毫无统计意义。另一方面,分 布假说是针对词义的假说,这种通过上下文获取语义的方式对句子和文档是否 有效,还有待讨论。因此需要寻求新的思路对句子和文档进行建模。 德国数学家弗雷格(Gottlob Frege)在 1892年就曾提出:一段话的语义由其 各组成部分的语义以及它们之间的组合方法所确定 [30]。现有的句子或者文档 表示也通常以该思路为基础,通过语义组合的方式获得。常用的组合语义组合 函数,如线性加权、矩阵乘法、张量乘法等,在 Hermann的文献 [36]中有详细 的总结。近年来基于神经网络的语义组合技术为文档表示带来了新的思路。从 神经网络的结构上看,主要可以分为三种方式:递归神经网络(5.",
      "startOffset" : 540,
      "endOffset" : 544
    }, {
      "referenceID" : 102,
      "context" : "递归神经网络使用的树形结构一般为二叉树,在某些特殊情况下(如依存 句法分析树 [107])也使用多叉树。本文主要从树的构建方式和子节点到父节点 的组合函数,这两方面介绍介绍递归神经网络。 树形结构有两种方式生成:一、使用句法分析器构建句法树 [105, 110];二、 使用贪心方法选择重建误差最小的相邻子树,逐层合并 [109]。这两种方法各有 优劣,使用句法分析器的方法可以保证生成的树形结构是一棵句法树,树中各 个节点均对应句子中的短语,通过网络合并生成的各个节点的语义表示也对应 各短语的语义。使用贪心方法构建树形结构则可以通过自动挖掘大量数据中的 规律,无监督地完成这一过程,但是树中的各个节点不能保证有实际的句法成 分。 子节点到父节点的组合函数 y = f(a, b)主要有三种: 一、句法组合。这种方式下,子节点的表示为向量 a, b,父节点可以通过矩 阵运算得到: y = φ (H [a; b]) (5.",
      "startOffset" : 39,
      "endOffset" : 44
    }, {
      "referenceID" : 100,
      "context" : "递归神经网络使用的树形结构一般为二叉树,在某些特殊情况下(如依存 句法分析树 [107])也使用多叉树。本文主要从树的构建方式和子节点到父节点 的组合函数,这两方面介绍介绍递归神经网络。 树形结构有两种方式生成:一、使用句法分析器构建句法树 [105, 110];二、 使用贪心方法选择重建误差最小的相邻子树,逐层合并 [109]。这两种方法各有 优劣,使用句法分析器的方法可以保证生成的树形结构是一棵句法树,树中各 个节点均对应句子中的短语,通过网络合并生成的各个节点的语义表示也对应 各短语的语义。使用贪心方法构建树形结构则可以通过自动挖掘大量数据中的 规律,无监督地完成这一过程,但是树中的各个节点不能保证有实际的句法成 分。 子节点到父节点的组合函数 y = f(a, b)主要有三种: 一、句法组合。这种方式下,子节点的表示为向量 a, b,父节点可以通过矩 阵运算得到: y = φ (H [a; b]) (5.",
      "startOffset" : 121,
      "endOffset" : 131
    }, {
      "referenceID" : 105,
      "context" : "递归神经网络使用的树形结构一般为二叉树,在某些特殊情况下(如依存 句法分析树 [107])也使用多叉树。本文主要从树的构建方式和子节点到父节点 的组合函数,这两方面介绍介绍递归神经网络。 树形结构有两种方式生成:一、使用句法分析器构建句法树 [105, 110];二、 使用贪心方法选择重建误差最小的相邻子树,逐层合并 [109]。这两种方法各有 优劣,使用句法分析器的方法可以保证生成的树形结构是一棵句法树,树中各 个节点均对应句子中的短语,通过网络合并生成的各个节点的语义表示也对应 各短语的语义。使用贪心方法构建树形结构则可以通过自动挖掘大量数据中的 规律,无监督地完成这一过程,但是树中的各个节点不能保证有实际的句法成 分。 子节点到父节点的组合函数 y = f(a, b)主要有三种: 一、句法组合。这种方式下,子节点的表示为向量 a, b,父节点可以通过矩 阵运算得到: y = φ (H [a; b]) (5.",
      "startOffset" : 121,
      "endOffset" : 131
    }, {
      "referenceID" : 104,
      "context" : "递归神经网络使用的树形结构一般为二叉树,在某些特殊情况下(如依存 句法分析树 [107])也使用多叉树。本文主要从树的构建方式和子节点到父节点 的组合函数,这两方面介绍介绍递归神经网络。 树形结构有两种方式生成:一、使用句法分析器构建句法树 [105, 110];二、 使用贪心方法选择重建误差最小的相邻子树,逐层合并 [109]。这两种方法各有 优劣,使用句法分析器的方法可以保证生成的树形结构是一棵句法树,树中各 个节点均对应句子中的短语,通过网络合并生成的各个节点的语义表示也对应 各短语的语义。使用贪心方法构建树形结构则可以通过自动挖掘大量数据中的 规律,无监督地完成这一过程,但是树中的各个节点不能保证有实际的句法成 分。 子节点到父节点的组合函数 y = f(a, b)主要有三种: 一、句法组合。这种方式下,子节点的表示为向量 a, b,父节点可以通过矩 阵运算得到: y = φ (H [a; b]) (5.",
      "startOffset" : 160,
      "endOffset" : 165
    }, {
      "referenceID" : 103,
      "context" : "1) 其中 φ为非线性的激活函数,权重矩阵 H 可能固定 [108],也可能根据子树对 应的句法结构不同,而选用不同的矩阵 [103]。该方法一般用于句法分析中。",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 98,
      "context" : "1) 其中 φ为非线性的激活函数,权重矩阵 H 可能固定 [108],也可能根据子树对 应的句法结构不同,而选用不同的矩阵 [103]。该方法一般用于句法分析中。",
      "startOffset" : 62,
      "endOffset" : 67
    }, {
      "referenceID" : 74,
      "context" : "二、矩阵向量法 [78]。在这种表示下,每个节点由两部分表示组成,一个矩 阵和一个向量,对于 A,a子节点和 B, b子节点,其组合函数为:",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 101,
      "context" : "其中 WM ∈ R|a|×2|a|,保证父节点对应的语义变换矩阵 Y ∈ R|a|×|a|,与叶节点 的 A、B 矩阵维度一致。使用这种方法,每个词均有一个语义变换矩阵,对于 否定词等对句法结构另一部分有类似影响的词而言,普通的句法组合方式没法 很好地对其建模,而这种矩阵向量表示则可以解决这一问题。Socher等人将该 方法用于关系分类中 [106]。 三、张量组合。张量组合方式使用张量中的每一个矩阵,将子节点组合生 成父节点表示中的一维。",
      "startOffset" : 171,
      "endOffset" : 176
    }, {
      "referenceID" : 99,
      "context" : "其中W 表示张量W 中的第 1到 d个切片矩阵。不同的切片用于生成父节点 y中不同的维度。该方法是句法组合方法的泛化形式,有更强的语义组合能力, Socher等人将其用于情感分析任务中 [104]。 递归神经网络在构建文本表示时,其精度依赖于文本树的精度。无论使用 哪种构建方式,哪种组合函数,构建文本树均需要至少O(n)的时间复杂度,其 中 n表示句子的长度。当模型在处理长句子或者文档时,所花费的时间往往是 不可接受的。更进一步地,在做文档表示时,两个句子之间的关系不一定能构 成树形结构。因此递归神经网络在大量句子级任务中表现出色,但可能不适合 构建长句子或者文档级别的语义。",
      "startOffset" : 93,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : "3 循环神经网络 循环神经网络(Recurrent Neural Network)由 Elman等人在 1990年首次提 出 [26]。该模型的核心是通过循环方式逐个输入文本中的各个词,并维护一个 隐藏层,保留所有的上文信息。 循环神经网络是递归神经网络的一个特例,可以认为它对应的是一棵任何",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "一个非叶结点的右子树均为叶结点的树。这种特殊结构使得循环神经网络具有 两个特点:一、由于固定了网络结构,模型只需在 O(n)时间内即可构建文本的 语义。这使得循环神经网络可以更高效地对文本的语义进行建模。二、从网络结 构上看,循环神经网络的层数非常深,句子中有几个词,网络就有几层。因此, 使用传统方法训练循环神经网络时,会遇到梯度衰减或梯度爆炸的问题,这需 要模型使用更特别的方法来实现优化过程 [8, 42]。 循环神经网络对文本语义的构建过程与 2.",
      "startOffset" : 199,
      "endOffset" : 206
    }, {
      "referenceID" : 38,
      "context" : "一个非叶结点的右子树均为叶结点的树。这种特殊结构使得循环神经网络具有 两个特点:一、由于固定了网络结构,模型只需在 O(n)时间内即可构建文本的 语义。这使得循环神经网络可以更高效地对文本的语义进行建模。二、从网络结 构上看,循环神经网络的层数非常深,句子中有几个词,网络就有几层。因此, 使用传统方法训练循环神经网络时,会遇到梯度衰减或梯度爆炸的问题,这需 要模型使用更特别的方法来实现优化过程 [8, 42]。 循环神经网络对文本语义的构建过程与 2.",
      "startOffset" : 199,
      "endOffset" : 206
    }, {
      "referenceID" : 39,
      "context" : "Schmidhuber在 1997年提出了 LSTM(Long Short-Term Memory)模型 [43]。该 模型引入了记忆单元,可以保存长距离信息,是循环神经网络的一种常用的优 化方案。 无论采用哪种优化方式,循环神经网络的语义都会偏向文本中靠后的词。因 此,循环神经网络很少直接用来表示整个文本的语义。但由于其能有效表示上 下文信息,因此被广泛用于序列标注任务,如 2.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 28,
      "context" : "4 卷积神经网络 卷积神经网络(Convolutional Neural Network)最早由 Fukushima在 1980年 提出 [31],此后,LeCun等人对其做了重要改进 [61]。",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 57,
      "context" : "4 卷积神经网络 卷积神经网络(Convolutional Neural Network)最早由 Fukushima在 1980年 提出 [31],此后,LeCun等人对其做了重要改进 [61]。",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "在得到若干个隐藏层之后,卷积神经网络通常会采用池化技术,将不定长 度的隐藏层压缩到固定长度的隐藏层中。常用的有均值池化和最大池化 [18]。最 大池化的公式为: h = n max i=1 h (1) i (5.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : "卷积神经网络通过其卷积核,可以对文本中的每个部分的局部信息进行建 模;通过其池化层,可以从各个局部信息中整合出全文语义,模型的整体复杂 度为 O(n)。 卷积神经网络应用非常广泛。在自然语言领域,Collobert等人首次将其用 于处理语义角色标注任务,有效提升了系统的性能 [18]。2014年,Kalchbrenner 等人与 Kim分别发表了利用卷积神经网络做文本分类的论文 [47, 48]。Zeng等 人提出使用卷积神经网络做关系分类任务,取得了一定的成功 [127]。 5.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 43,
      "context" : "卷积神经网络通过其卷积核,可以对文本中的每个部分的局部信息进行建 模;通过其池化层,可以从各个局部信息中整合出全文语义,模型的整体复杂 度为 O(n)。 卷积神经网络应用非常广泛。在自然语言领域,Collobert等人首次将其用 于处理语义角色标注任务,有效提升了系统的性能 [18]。2014年,Kalchbrenner 等人与 Kim分别发表了利用卷积神经网络做文本分类的论文 [47, 48]。Zeng等 人提出使用卷积神经网络做关系分类任务,取得了一定的成功 [127]。 5.",
      "startOffset" : 191,
      "endOffset" : 199
    }, {
      "referenceID" : 44,
      "context" : "卷积神经网络通过其卷积核,可以对文本中的每个部分的局部信息进行建 模;通过其池化层,可以从各个局部信息中整合出全文语义,模型的整体复杂 度为 O(n)。 卷积神经网络应用非常广泛。在自然语言领域,Collobert等人首次将其用 于处理语义角色标注任务,有效提升了系统的性能 [18]。2014年,Kalchbrenner 等人与 Kim分别发表了利用卷积神经网络做文本分类的论文 [47, 48]。Zeng等 人提出使用卷积神经网络做关系分类任务,取得了一定的成功 [127]。 5.",
      "startOffset" : 191,
      "endOffset" : 199
    }, {
      "referenceID" : 122,
      "context" : "卷积神经网络通过其卷积核,可以对文本中的每个部分的局部信息进行建 模;通过其池化层,可以从各个局部信息中整合出全文语义,模型的整体复杂 度为 O(n)。 卷积神经网络应用非常广泛。在自然语言领域,Collobert等人首次将其用 于处理语义角色标注任务,有效提升了系统的性能 [18]。2014年,Kalchbrenner 等人与 Kim分别发表了利用卷积神经网络做文本分类的论文 [47, 48]。Zeng等 人提出使用卷积神经网络做关系分类任务,取得了一定的成功 [127]。 5.",
      "startOffset" : 234,
      "endOffset" : 239
    }, {
      "referenceID" : 62,
      "context" : "5 文本分类 传统文本分类方法主要着眼于三个问题:特征表示、特征筛选和合适机器 学习算法的选择。 特征表示方面,最常用的是词袋子特征,复杂一些的有词性标签、名词短 语 [66]以及树核 [94]等特征。不同的特征从不同的角度对数据进行刻画,通常 需要通过各种特征的组合,才能更好地描述文本。然而这些特征都存在数据稀 疏问题。 特征选择希望通过删除噪声特征来提高文本分类的性能。最常用的方法是 删除停用词(比如“的”),更高级的方法包括利用信息增益、互信息 [19]等指 标来筛选特征。近年来,Ng等人提出的在优化目标中加入 L1 正则化 [85],自 动学习出稀疏特征,在文本分类的大规模应用中起到了重要作用。 在选择机器学习算法方面,几乎所有的分类器算法都在文本分类中有应用, 如最近邻分类器,决策树分类器。在面对大规模文本分类任务时,高效的线性 分类器应用的最为广泛,如 Logistic回归(LR)、朴素贝叶斯(NB)和支持向量 机(SVM)。",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 89,
      "context" : "5 文本分类 传统文本分类方法主要着眼于三个问题:特征表示、特征筛选和合适机器 学习算法的选择。 特征表示方面,最常用的是词袋子特征,复杂一些的有词性标签、名词短 语 [66]以及树核 [94]等特征。不同的特征从不同的角度对数据进行刻画,通常 需要通过各种特征的组合,才能更好地描述文本。然而这些特征都存在数据稀 疏问题。 特征选择希望通过删除噪声特征来提高文本分类的性能。最常用的方法是 删除停用词(比如“的”),更高级的方法包括利用信息增益、互信息 [19]等指 标来筛选特征。近年来,Ng等人提出的在优化目标中加入 L1 正则化 [85],自 动学习出稀疏特征,在文本分类的大规模应用中起到了重要作用。 在选择机器学习算法方面,几乎所有的分类器算法都在文本分类中有应用, 如最近邻分类器,决策树分类器。在面对大规模文本分类任务时,高效的线性 分类器应用的最为广泛,如 Logistic回归(LR)、朴素贝叶斯(NB)和支持向量 机(SVM)。",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 17,
      "context" : "5 文本分类 传统文本分类方法主要着眼于三个问题:特征表示、特征筛选和合适机器 学习算法的选择。 特征表示方面,最常用的是词袋子特征,复杂一些的有词性标签、名词短 语 [66]以及树核 [94]等特征。不同的特征从不同的角度对数据进行刻画,通常 需要通过各种特征的组合,才能更好地描述文本。然而这些特征都存在数据稀 疏问题。 特征选择希望通过删除噪声特征来提高文本分类的性能。最常用的方法是 删除停用词(比如“的”),更高级的方法包括利用信息增益、互信息 [19]等指 标来筛选特征。近年来,Ng等人提出的在优化目标中加入 L1 正则化 [85],自 动学习出稀疏特征,在文本分类的大规模应用中起到了重要作用。 在选择机器学习算法方面,几乎所有的分类器算法都在文本分类中有应用, 如最近邻分类器,决策树分类器。在面对大规模文本分类任务时,高效的线性 分类器应用的最为广泛,如 Logistic回归(LR)、朴素贝叶斯(NB)和支持向量 机(SVM)。",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 81,
      "context" : "5 文本分类 传统文本分类方法主要着眼于三个问题:特征表示、特征筛选和合适机器 学习算法的选择。 特征表示方面,最常用的是词袋子特征,复杂一些的有词性标签、名词短 语 [66]以及树核 [94]等特征。不同的特征从不同的角度对数据进行刻画,通常 需要通过各种特征的组合,才能更好地描述文本。然而这些特征都存在数据稀 疏问题。 特征选择希望通过删除噪声特征来提高文本分类的性能。最常用的方法是 删除停用词(比如“的”),更高级的方法包括利用信息增益、互信息 [19]等指 标来筛选特征。近年来,Ng等人提出的在优化目标中加入 L1 正则化 [85],自 动学习出稀疏特征,在文本分类的大规模应用中起到了重要作用。 在选择机器学习算法方面,几乎所有的分类器算法都在文本分类中有应用, 如最近邻分类器,决策树分类器。在面对大规模文本分类任务时,高效的线性 分类器应用的最为广泛,如 Logistic回归(LR)、朴素贝叶斯(NB)和支持向量 机(SVM)。",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 16,
      "context" : "这里,max操作是逐个元素计算的,也就是说,y的第 k维就是各 y i 向 量的第 k 维的最大值。借助最大池化层,可以将不同长度的文本转成固定长度 的向量,从而表示整个文本。除了最大池化层,还有均值池化层 [18]等其它池 化技术。本文选用最大池化技术的主要出发点是,对于文本分类而言,最具决 定性的词或者短语往往只有几处,而不是均匀散在文本各处。最大池化正好可 以找出其中最有判别力的语言片段。同时,最大池化层的复杂度也是 O(n)。循 环卷积网络中,循环结构和卷积结构是串联的,池化层使用的是循环结构的输 出,因此整个模型的复杂度也是 O(n)。 模型的最后一部分是输出层,输出层定义如下。",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "本文使用随机梯度下降法 [10]来优化上述训练目标。每次迭代,随机选取一个 样本 (D, classD),使用下式进行一次梯度迭代。式中,α是学习速率。 θ ← θ + α logP (classD|D, θ) ∂θ (5.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 88,
      "context" : "里不再赘述。 在训练中,本文参考了 Plaut和 Hinton的建议 [93],使用了一个神经网络训 练中常用的优化技巧。所有的参数在初始化时均使用均匀分布,其中随机数的 最大绝对值为该元素入节点个数的平方根。入节点个数也就是神经网络中上一 层的节点个数。对应的学习速率同样也除以入节点个数。",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 25,
      "context" : "词向量是一种词的分布表示,这种表示更适合作为神经网络的输入。最近 的研究 [27, 40]表明,如果选择一个好的初始值,神经网络可以收敛到更好的局 部最优解。本章根据第三章中得到的经验,选取了适合本任务而且效率较高的 Skip-gram模型,来优化词向量。Skip-gram模型的介绍可见第二章第 2.",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 36,
      "context" : "词向量是一种词的分布表示,这种表示更适合作为神经网络的输入。最近 的研究 [27, 40]表明,如果选择一个好的初始值,神经网络可以收敛到更好的局 部最优解。本章根据第三章中得到的经验,选取了适合本任务而且效率较高的 Skip-gram模型,来优化词向量。Skip-gram模型的介绍可见第二章第 2.",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : "1 实验设置 对于英文数据集,本文使用 Stanford Tokenizer1对语料进行分词。对于中文 数据集,使用 ICTCLAS2分词。文本中的停用词和特殊符号均当作普通单词保 留。这四个数据集均已分好训练集和测试集,其中 ACL论文集和斯坦福情感树 库已经分好了训练、验证、测试集。对于另两个数据集,本文随机选取训练集 的 10%作为验证集,剩下部分作为真实的训练集。 网络超参数的选择一般需要根据数据的不同而有所调节。在实验中,本文 参考了 Collobert等人 [18]和 Turian等人 [116]的工作,只使用了一组最常用的 超参数。具体来说,学习速率 α = 0.",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 111,
      "context" : "1 实验设置 对于英文数据集,本文使用 Stanford Tokenizer1对语料进行分词。对于中文 数据集,使用 ICTCLAS2分词。文本中的停用词和特殊符号均当作普通单词保 留。这四个数据集均已分好训练集和测试集,其中 ACL论文集和斯坦福情感树 库已经分好了训练、验证、测试集。对于另两个数据集,本文随机选取训练集 的 10%作为验证集,剩下部分作为真实的训练集。 网络超参数的选择一般需要根据数据的不同而有所调节。在实验中,本文 参考了 Collobert等人 [18]和 Turian等人 [116]的工作,只使用了一组最常用的 超参数。具体来说,学习速率 α = 0.",
      "startOffset" : 252,
      "endOffset" : 257
    }, {
      "referenceID" : 29,
      "context" : "10中的激活函数 φ,本文使 用双曲正切函数 tanh。如果使用更适合深层网络的 ReLU激活函数 [32]能对系 统性能有小幅度的提升,但由于对比方法同样使用 tanh作为激活函数,本实验 中依然选择使用 tanh。",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 115,
      "context" : "2 对比方法 词袋子、二元词组 + Logistic回归、支持向量机 Wang和 Manning在 2012年发表的文章 [120]中展示了若干种实现简便的 文本分类基准实验。这些实验中主要使用词和二元词组作为特征,并用常规分 类器算法进行分类。本文在实验中选择了词以及二元词组作为特征,特征的权 重为各词在文档中的词频。同时,本文选择 Logistic回归和支持向量机作为分 1http://nlp.",
      "startOffset" : 61,
      "endOffset" : 66
    }, {
      "referenceID" : 54,
      "context" : "平均词向量 + Logistic回归 该方法使用文档中各词词向量的加权平均值作为文档的表示,然后使用Logistic回归分类器对其进行分类。其中各个词的权重为词在文档中的词频。该方 法可能是利用词向量表示文本的一种最简单的方法,通常基于神经网络的分类 模型均会选用该方法作对比 [58, 110]。 作为一种文本表示,平均词向量除了用在单语文本分类中,还出现在其它 场景下,如 Huang等人在其模型中,使用平均词向量作为文档的全局表示 [44], 辅助提升词向量的语义。Klementiev等人也使用平均词向量作为文本表示,并 基于此进行跨语言的文本分类 [50]。",
      "startOffset" : 139,
      "endOffset" : 148
    }, {
      "referenceID" : 105,
      "context" : "平均词向量 + Logistic回归 该方法使用文档中各词词向量的加权平均值作为文档的表示,然后使用Logistic回归分类器对其进行分类。其中各个词的权重为词在文档中的词频。该方 法可能是利用词向量表示文本的一种最简单的方法,通常基于神经网络的分类 模型均会选用该方法作对比 [58, 110]。 作为一种文本表示,平均词向量除了用在单语文本分类中,还出现在其它 场景下,如 Huang等人在其模型中,使用平均词向量作为文档的全局表示 [44], 辅助提升词向量的语义。Klementiev等人也使用平均词向量作为文本表示,并 基于此进行跨语言的文本分类 [50]。",
      "startOffset" : 139,
      "endOffset" : 148
    }, {
      "referenceID" : 40,
      "context" : "平均词向量 + Logistic回归 该方法使用文档中各词词向量的加权平均值作为文档的表示,然后使用Logistic回归分类器对其进行分类。其中各个词的权重为词在文档中的词频。该方 法可能是利用词向量表示文本的一种最简单的方法,通常基于神经网络的分类 模型均会选用该方法作对比 [58, 110]。 作为一种文本表示,平均词向量除了用在单语文本分类中,还出现在其它 场景下,如 Huang等人在其模型中,使用平均词向量作为文档的全局表示 [44], 辅助提升词向量的语义。Klementiev等人也使用平均词向量作为文本表示,并 基于此进行跨语言的文本分类 [50]。",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 46,
      "context" : "平均词向量 + Logistic回归 该方法使用文档中各词词向量的加权平均值作为文档的表示,然后使用Logistic回归分类器对其进行分类。其中各个词的权重为词在文档中的词频。该方 法可能是利用词向量表示文本的一种最简单的方法,通常基于神经网络的分类 模型均会选用该方法作对比 [58, 110]。 作为一种文本表示,平均词向量除了用在单语文本分类中,还出现在其它 场景下,如 Huang等人在其模型中,使用平均词向量作为文档的全局表示 [44], 辅助提升词向量的语义。Klementiev等人也使用平均词向量作为文本表示,并 基于此进行跨语言的文本分类 [50]。",
      "startOffset" : 279,
      "endOffset" : 283
    }, {
      "referenceID" : 43,
      "context" : "与本文同期,也有若干其它工作使用卷积神经网络直接做文本分类任务, 大体思想与上述模型类似,如文献 [47, 48, 59]。",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 44,
      "context" : "与本文同期,也有若干其它工作使用卷积神经网络直接做文本分类任务, 大体思想与上述模型类似,如文献 [47, 48, 59]。",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 55,
      "context" : "与本文同期,也有若干其它工作使用卷积神经网络直接做文本分类任务, 大体思想与上述模型类似,如文献 [47, 48, 59]。",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 34,
      "context" : "1 20Newsgroups5 20Newsgroups数据集包括了 20个新闻组的共约 2万封邮件,是目前使用 最广泛的英文文本分类数据集之一。该数据集有若干不同的版本,本文选用了 bydate版本,因为该版本已经将数据集分成了训练集和测试集,方便和现有工 作做对比。 本文按照 Hingmire等人的工作 [38]将数据集划分成 comp、politics、rec和 religion一共四个大类,各类别分别包括:",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 34,
      "context" : "20Newsgroups数据集是一个类别不均衡的文本分类数据集,即使按照上述方法 分成四个大类,各类别样本数仍然大约呈 6:3:5:3的比例。 该数据集在这种划分下的最佳结果由文献 [38]所得,文中提出了一种结合 了 LDA、EM算法和朴素贝叶斯分类器的文本分类模型,并命名为 ClassifyLDAEM。本文按照这项工作的设置,使用宏平均 F1值(Macro F-measure)评价各 模型在 20Newsgroups数据集上的表现。宏平均 F1值定义为各分类 F1值的宏平 均数。 表 5-3展示了各模型在 20Newsgroups数据集上的表现。其中表格最上面一 部分区域表示传统文本分类算法的效果,中间部分为此前该数据集上最好的模 型 ClassifyLDA-EM 的表现,下面部分分别为本文实现的卷积神经网络和循环 卷积网络的性能。根据实验结果,可以得到下述结论: 一、无论是卷积神经网络还是循环卷积网络,它们都比传统基于分类器的 方法要有明显的优势。基于词特征及二元词组特征的传统分类方法,最多只有 93.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 34,
      "context" : "39 ClassifyLDA-EM [38] 93.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 91,
      "context" : "3 ACL论文集7 ACL 论文集(ACL Anthology Network)[96] 包含了国际计算语言学协会 (ACL,The Association for Computational Linguistics)旗下的多个会议和期刊的 论文。Post和 Bergsma对论文集中从 2001年到 2009年的论文进行了作者母语 标注。标注过程主要依据作者的国籍,标注了五种最常见的作者母语:英语、日 语、德语、汉语和法语 [94]。为了保证数据集的有效性,Post等人抛弃了模棱两 可的文章,只保留有把握的部分,一共对于 8483篇论文中的 1959篇进行了标 注。 为了充分调研模型在不同长度文本下的性能,本文按照文献 [94]中的实验 设置,对各篇论文中的每个句子进行分类。在这个分类数据集上,目前最佳的 7http://old-site.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 89,
      "context" : "3 ACL论文集7 ACL 论文集(ACL Anthology Network)[96] 包含了国际计算语言学协会 (ACL,The Association for Computational Linguistics)旗下的多个会议和期刊的 论文。Post和 Bergsma对论文集中从 2001年到 2009年的论文进行了作者母语 标注。标注过程主要依据作者的国籍,标注了五种最常见的作者母语:英语、日 语、德语、汉语和法语 [94]。为了保证数据集的有效性,Post等人抛弃了模棱两 可的文章,只保留有把握的部分,一共对于 8483篇论文中的 1959篇进行了标 注。 为了充分调研模型在不同长度文本下的性能,本文按照文献 [94]中的实验 设置,对各篇论文中的每个句子进行分类。在这个分类数据集上,目前最佳的 7http://old-site.",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 89,
      "context" : "3 ACL论文集7 ACL 论文集(ACL Anthology Network)[96] 包含了国际计算语言学协会 (ACL,The Association for Computational Linguistics)旗下的多个会议和期刊的 论文。Post和 Bergsma对论文集中从 2001年到 2009年的论文进行了作者母语 标注。标注过程主要依据作者的国籍,标注了五种最常见的作者母语:英语、日 语、德语、汉语和法语 [94]。为了保证数据集的有效性,Post等人抛弃了模棱两 可的文章,只保留有把握的部分,一共对于 8483篇论文中的 1959篇进行了标 注。 为了充分调研模型在不同长度文本下的性能,本文按照文献 [94]中的实验 设置,对各篇论文中的每个句子进行分类。在这个分类数据集上,目前最佳的 7http://old-site.",
      "startOffset" : 314,
      "endOffset" : 318
    }, {
      "referenceID" : 89,
      "context" : "结果为 Post等人 [94]所得,他们在实验中对比分析了若干种不同的树核(Tree kernel)特征。本文在对比实验中列举了其中最有代表性的两种特征,分别为: 一、使用 Berkeley parser [92]抽取得到的深度为 1的上下文无关文法(CFG)的 个数,作为特征集合;二、由 Charniak和 Johnson提出的,对上述特征重排序 得到的特征集合(C&J)[14]。",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 87,
      "context" : "结果为 Post等人 [94]所得,他们在实验中对比分析了若干种不同的树核(Tree kernel)特征。本文在对比实验中列举了其中最有代表性的两种特征,分别为: 一、使用 Berkeley parser [92]抽取得到的深度为 1的上下文无关文法(CFG)的 个数,作为特征集合;二、由 Charniak和 Johnson提出的,对上述特征重排序 得到的特征集合(C&J)[14]。",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "结果为 Post等人 [94]所得,他们在实验中对比分析了若干种不同的树核(Tree kernel)特征。本文在对比实验中列举了其中最有代表性的两种特征,分别为: 一、使用 Berkeley parser [92]抽取得到的深度为 1的上下文无关文法(CFG)的 个数,作为特征集合;二、由 Charniak和 Johnson提出的,对上述特征重排序 得到的特征集合(C&J)[14]。",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 89,
      "context" : "32 CFG [94] 39.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 89,
      "context" : "20 C&J [94] 49.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 83,
      "context" : "影评论数据集 [87]。2013年,Socher等人 [110]在电影评论数据集的基础上,使 用 Stanford Parser [49]对其中的每个句子进行句法分析,并对得到的每个句法子 树所对应的短语进行情感倾向打分,每个短语由三位标注者打分,并取其平均 值作为标注的短语情感。 该数据集一共包括 11855条电影评论,和此前最好的工作 [110]一致,本文 也使用五分类(非常正面、正面、中立、负面、非常负面)标签体系。在文献 [110]中,Socher等人利用了对每条评论中所有短语的标注信息,训练了递归神 经网络。然而在训练循环卷积网络时,只使用了对于整个句子的情感倾向标注, 而没有使用其中对于短语和子句的标注。 与本文同期,另有一项基于卷积网络的文本分类工作发表 [48],下文会针 对这些方法进行详细的比较。",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 105,
      "context" : "影评论数据集 [87]。2013年,Socher等人 [110]在电影评论数据集的基础上,使 用 Stanford Parser [49]对其中的每个句子进行句法分析,并对得到的每个句法子 树所对应的短语进行情感倾向打分,每个短语由三位标注者打分,并取其平均 值作为标注的短语情感。 该数据集一共包括 11855条电影评论,和此前最好的工作 [110]一致,本文 也使用五分类(非常正面、正面、中立、负面、非常负面)标签体系。在文献 [110]中,Socher等人利用了对每条评论中所有短语的标注信息,训练了递归神 经网络。然而在训练循环卷积网络时,只使用了对于整个句子的情感倾向标注, 而没有使用其中对于短语和子句的标注。 与本文同期,另有一项基于卷积网络的文本分类工作发表 [48],下文会针 对这些方法进行详细的比较。",
      "startOffset" : 27,
      "endOffset" : 32
    }, {
      "referenceID" : 45,
      "context" : "影评论数据集 [87]。2013年,Socher等人 [110]在电影评论数据集的基础上,使 用 Stanford Parser [49]对其中的每个句子进行句法分析,并对得到的每个句法子 树所对应的短语进行情感倾向打分,每个短语由三位标注者打分,并取其平均 值作为标注的短语情感。 该数据集一共包括 11855条电影评论,和此前最好的工作 [110]一致,本文 也使用五分类(非常正面、正面、中立、负面、非常负面)标签体系。在文献 [110]中,Socher等人利用了对每条评论中所有短语的标注信息,训练了递归神 经网络。然而在训练循环卷积网络时,只使用了对于整个句子的情感倾向标注, 而没有使用其中对于短语和子句的标注。 与本文同期,另有一项基于卷积网络的文本分类工作发表 [48],下文会针 对这些方法进行详细的比较。",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 105,
      "context" : "影评论数据集 [87]。2013年,Socher等人 [110]在电影评论数据集的基础上,使 用 Stanford Parser [49]对其中的每个句子进行句法分析,并对得到的每个句法子 树所对应的短语进行情感倾向打分,每个短语由三位标注者打分,并取其平均 值作为标注的短语情感。 该数据集一共包括 11855条电影评论,和此前最好的工作 [110]一致,本文 也使用五分类(非常正面、正面、中立、负面、非常负面)标签体系。在文献 [110]中,Socher等人利用了对每条评论中所有短语的标注信息,训练了递归神 经网络。然而在训练循环卷积网络时,只使用了对于整个句子的情感倾向标注, 而没有使用其中对于短语和子句的标注。 与本文同期,另有一项基于卷积网络的文本分类工作发表 [48],下文会针 对这些方法进行详细的比较。",
      "startOffset" : 171,
      "endOffset" : 176
    }, {
      "referenceID" : 105,
      "context" : "影评论数据集 [87]。2013年,Socher等人 [110]在电影评论数据集的基础上,使 用 Stanford Parser [49]对其中的每个句子进行句法分析,并对得到的每个句法子 树所对应的短语进行情感倾向打分,每个短语由三位标注者打分,并取其平均 值作为标注的短语情感。 该数据集一共包括 11855条电影评论,和此前最好的工作 [110]一致,本文 也使用五分类(非常正面、正面、中立、负面、非常负面)标签体系。在文献 [110]中,Socher等人利用了对每条评论中所有短语的标注信息,训练了递归神 经网络。然而在训练循环卷积网络时,只使用了对于整个句子的情感倾向标注, 而没有使用其中对于短语和子句的标注。 与本文同期,另有一项基于卷积网络的文本分类工作发表 [48],下文会针 对这些方法进行详细的比较。",
      "startOffset" : 217,
      "endOffset" : 222
    }, {
      "referenceID" : 44,
      "context" : "影评论数据集 [87]。2013年,Socher等人 [110]在电影评论数据集的基础上,使 用 Stanford Parser [49]对其中的每个句子进行句法分析,并对得到的每个句法子 树所对应的短语进行情感倾向打分,每个短语由三位标注者打分,并取其平均 值作为标注的短语情感。 该数据集一共包括 11855条电影评论,和此前最好的工作 [110]一致,本文 也使用五分类(非常正面、正面、中立、负面、非常负面)标签体系。在文献 [110]中,Socher等人利用了对每条评论中所有短语的标注信息,训练了递归神 经网络。然而在训练循环卷积网络时,只使用了对于整个句子的情感倾向标注, 而没有使用其中对于短语和子句的标注。 与本文同期,另有一项基于卷积网络的文本分类工作发表 [48],下文会针 对这些方法进行详细的比较。",
      "startOffset" : 339,
      "endOffset" : 343
    }, {
      "referenceID" : 104,
      "context" : "70 递归神经网络 [109] 43.",
      "startOffset" : 10,
      "endOffset" : 15
    }, {
      "referenceID" : 105,
      "context" : "20 递归张量网络 [110] 45.",
      "startOffset" : 10,
      "endOffset" : 15
    }, {
      "referenceID" : 44,
      "context" : "70 卷积神经网络(Kim)[48] 48.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 44,
      "context" : "表 5-6展示了各模型在斯坦福情感树库数据集上的表现。与前三个数据集 类似,卷积神经网络和循环卷积网络依然有较好的效果。 实验结果中,文献 [48]与本文实现的卷积神经网络在模型结构上完全一致。 这两个同样的模型得到不同的结果主要是因为文献 [48]用了较多的神经网络优 化技巧,主要包括 Dropout[41, 121]、AdaDelta[126]等先进的归一化方法和优化 方法。这些最新提出的优化方法对所有的神经网络模型都有效果。但是由于本 文的主要对比模型递归神经网络 [109, 110]并没有使用这些先进的优化方法,为 了更明确地对比模型结构带来的性能提升,本文实现的模型中也没有使用文献",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 44,
      "context" : "表 5-6展示了各模型在斯坦福情感树库数据集上的表现。与前三个数据集 类似,卷积神经网络和循环卷积网络依然有较好的效果。 实验结果中,文献 [48]与本文实现的卷积神经网络在模型结构上完全一致。 这两个同样的模型得到不同的结果主要是因为文献 [48]用了较多的神经网络优 化技巧,主要包括 Dropout[41, 121]、AdaDelta[126]等先进的归一化方法和优化 方法。这些最新提出的优化方法对所有的神经网络模型都有效果。但是由于本 文的主要对比模型递归神经网络 [109, 110]并没有使用这些先进的优化方法,为 了更明确地对比模型结构带来的性能提升,本文实现的模型中也没有使用文献",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 37,
      "context" : "表 5-6展示了各模型在斯坦福情感树库数据集上的表现。与前三个数据集 类似,卷积神经网络和循环卷积网络依然有较好的效果。 实验结果中,文献 [48]与本文实现的卷积神经网络在模型结构上完全一致。 这两个同样的模型得到不同的结果主要是因为文献 [48]用了较多的神经网络优 化技巧,主要包括 Dropout[41, 121]、AdaDelta[126]等先进的归一化方法和优化 方法。这些最新提出的优化方法对所有的神经网络模型都有效果。但是由于本 文的主要对比模型递归神经网络 [109, 110]并没有使用这些先进的优化方法,为 了更明确地对比模型结构带来的性能提升,本文实现的模型中也没有使用文献",
      "startOffset" : 152,
      "endOffset" : 161
    }, {
      "referenceID" : 116,
      "context" : "表 5-6展示了各模型在斯坦福情感树库数据集上的表现。与前三个数据集 类似,卷积神经网络和循环卷积网络依然有较好的效果。 实验结果中,文献 [48]与本文实现的卷积神经网络在模型结构上完全一致。 这两个同样的模型得到不同的结果主要是因为文献 [48]用了较多的神经网络优 化技巧,主要包括 Dropout[41, 121]、AdaDelta[126]等先进的归一化方法和优化 方法。这些最新提出的优化方法对所有的神经网络模型都有效果。但是由于本 文的主要对比模型递归神经网络 [109, 110]并没有使用这些先进的优化方法,为 了更明确地对比模型结构带来的性能提升,本文实现的模型中也没有使用文献",
      "startOffset" : 152,
      "endOffset" : 161
    }, {
      "referenceID" : 121,
      "context" : "表 5-6展示了各模型在斯坦福情感树库数据集上的表现。与前三个数据集 类似,卷积神经网络和循环卷积网络依然有较好的效果。 实验结果中,文献 [48]与本文实现的卷积神经网络在模型结构上完全一致。 这两个同样的模型得到不同的结果主要是因为文献 [48]用了较多的神经网络优 化技巧,主要包括 Dropout[41, 121]、AdaDelta[126]等先进的归一化方法和优化 方法。这些最新提出的优化方法对所有的神经网络模型都有效果。但是由于本 文的主要对比模型递归神经网络 [109, 110]并没有使用这些先进的优化方法,为 了更明确地对比模型结构带来的性能提升,本文实现的模型中也没有使用文献",
      "startOffset" : 170,
      "endOffset" : 175
    }, {
      "referenceID" : 104,
      "context" : "表 5-6展示了各模型在斯坦福情感树库数据集上的表现。与前三个数据集 类似,卷积神经网络和循环卷积网络依然有较好的效果。 实验结果中,文献 [48]与本文实现的卷积神经网络在模型结构上完全一致。 这两个同样的模型得到不同的结果主要是因为文献 [48]用了较多的神经网络优 化技巧,主要包括 Dropout[41, 121]、AdaDelta[126]等先进的归一化方法和优化 方法。这些最新提出的优化方法对所有的神经网络模型都有效果。但是由于本 文的主要对比模型递归神经网络 [109, 110]并没有使用这些先进的优化方法,为 了更明确地对比模型结构带来的性能提升,本文实现的模型中也没有使用文献",
      "startOffset" : 238,
      "endOffset" : 248
    }, {
      "referenceID" : 105,
      "context" : "表 5-6展示了各模型在斯坦福情感树库数据集上的表现。与前三个数据集 类似,卷积神经网络和循环卷积网络依然有较好的效果。 实验结果中,文献 [48]与本文实现的卷积神经网络在模型结构上完全一致。 这两个同样的模型得到不同的结果主要是因为文献 [48]用了较多的神经网络优 化技巧,主要包括 Dropout[41, 121]、AdaDelta[126]等先进的归一化方法和优化 方法。这些最新提出的优化方法对所有的神经网络模型都有效果。但是由于本 文的主要对比模型递归神经网络 [109, 110]并没有使用这些先进的优化方法,为 了更明确地对比模型结构带来的性能提升,本文实现的模型中也没有使用文献",
      "startOffset" : 238,
      "endOffset" : 248
    }, {
      "referenceID" : 44,
      "context" : "[48]所用的优化方法。 在这个实验中,可以发现卷积神经网络或者循环卷积网络在句子级分类任 务上,相对递归神经网络仍然有较大的提升。基于卷积的网络在表示文本语义 上相对此前基于递归结构的神经网络模型具有更大的优势。本文认为这是因为 卷积神经网络可以更好地捕获上下文信息,并从中选择出最重要的特征;而递 归网络需要按照树形结构构建文本语义,这依赖于所构建的树的精度。另一方 面,递归网络的时间复杂度是 O(n),而循环卷积网络只需要 O(n)的时间复杂 度。在实践中,递归张量网络按照文献 [110]中的报告,需要 3到 5小时的训练 时间。而循环卷积网络同样在斯坦福情感树库数据集上用单线程训练,只需要 若干分钟。",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 105,
      "context" : "[48]所用的优化方法。 在这个实验中,可以发现卷积神经网络或者循环卷积网络在句子级分类任 务上,相对递归神经网络仍然有较大的提升。基于卷积的网络在表示文本语义 上相对此前基于递归结构的神经网络模型具有更大的优势。本文认为这是因为 卷积神经网络可以更好地捕获上下文信息,并从中选择出最重要的特征;而递 归网络需要按照树形结构构建文本语义,这依赖于所构建的树的精度。另一方 面,递归网络的时间复杂度是 O(n),而循环卷积网络只需要 O(n)的时间复杂 度。在实践中,递归张量网络按照文献 [110]中的报告,需要 3到 5小时的训练 时间。而循环卷积网络同样在斯坦福情感树库数据集上用单线程训练,只需要 若干分钟。",
      "startOffset" : 244,
      "endOffset" : 249
    }, {
      "referenceID" : 105,
      "context" : "为了直观地观察循环卷积网络与递归张量网络在文档构建时的表现,本文 在此列举了这两个模型抽取得到的若干“关键短语”。在循环卷积网络中,关键 短语是指模型在最大池化层提取出来次数最多的短语片段。由于在循环卷积网 络中,每个词实际上是带有整个文档的语义的,因此本文只列举了中心词附近 的多词短语。在递归张量网络中 [110],关键的短语为情感最强烈的短语。 表 5-7列举了两个模型在训练后,从测试集中抽取的关键短语,这里仅列举 三元短语。其中递归张量网络抽取得到的关键短语直接引自文献 [110]。",
      "startOffset" : 154,
      "endOffset" : 159
    }, {
      "referenceID" : 105,
      "context" : "为了直观地观察循环卷积网络与递归张量网络在文档构建时的表现,本文 在此列举了这两个模型抽取得到的若干“关键短语”。在循环卷积网络中,关键 短语是指模型在最大池化层提取出来次数最多的短语片段。由于在循环卷积网 络中,每个词实际上是带有整个文档的语义的,因此本文只列举了中心词附近 的多词短语。在递归张量网络中 [110],关键的短语为情感最强烈的短语。 表 5-7列举了两个模型在训练后,从测试集中抽取的关键短语,这里仅列举 三元短语。其中递归张量网络抽取得到的关键短语直接引自文献 [110]。",
      "startOffset" : 241,
      "endOffset" : 246
    }, {
      "referenceID" : 118,
      "context" : "并针对现有的三种表示技术的不足,提出了基于卷积循环网络的文档表示技术。 该方法克服了此前递归网络的复杂度过高的问题,循环网络的语义偏置问题, 以及卷积网络窗口较难选择的问题。本文在文本分类任务上对新提出的表示技 术进行了对比分析,实验表明基于循环卷积网络的文本表示技术比现有的表示 技术在文本分类任务上,有更好的性能。 在机器学习领域有一个公认的观点是,模型选用的特征决定了机器学习算 法所能达到的上界;而具体模型的选择,则决定了对上界的逼近程度。因此,特 征的表示在机器学习中是至关重要的一个步骤。在基于神经网络的词向量表示 技术中,尽管不同的模型有着不同的性能,但这些模型均基于分布假说,语义 由其上下文决定。这些模型所能表达出的语义,受到语料中各词上下文分布的 约束。 最近一两年,已经有人尝试跳出分布假说的框架,使用更广泛的信息,对 语义进行建模。Weston等人设计了一个模型,从知识库中获取语义表示,并用 于提升关系抽取任务的性能 [123]。Wang等人指出,利用知识库中的知识,可 以进行词义消歧,进一步提升词向量的性能 [122]。 未来的工作需要考虑,如何利用海量的多源异构的数据,从中挖掘出有用 的信息,更好地对数据和知识进行表示。",
      "startOffset" : 422,
      "endOffset" : 427
    }, {
      "referenceID" : 117,
      "context" : "并针对现有的三种表示技术的不足,提出了基于卷积循环网络的文档表示技术。 该方法克服了此前递归网络的复杂度过高的问题,循环网络的语义偏置问题, 以及卷积网络窗口较难选择的问题。本文在文本分类任务上对新提出的表示技 术进行了对比分析,实验表明基于循环卷积网络的文本表示技术比现有的表示 技术在文本分类任务上,有更好的性能。 在机器学习领域有一个公认的观点是,模型选用的特征决定了机器学习算 法所能达到的上界;而具体模型的选择,则决定了对上界的逼近程度。因此,特 征的表示在机器学习中是至关重要的一个步骤。在基于神经网络的词向量表示 技术中,尽管不同的模型有着不同的性能,但这些模型均基于分布假说,语义 由其上下文决定。这些模型所能表达出的语义,受到语料中各词上下文分布的 约束。 最近一两年,已经有人尝试跳出分布假说的框架,使用更广泛的信息,对 语义进行建模。Weston等人设计了一个模型,从知识库中获取语义表示,并用 于提升关系抽取任务的性能 [123]。Wang等人指出,利用知识库中的知识,可 以进行词义消歧,进一步提升词向量的性能 [122]。 未来的工作需要考虑,如何利用海量的多源异构的数据,从中挖掘出有用 的信息,更好地对数据和知识进行表示。",
      "startOffset" : 469,
      "endOffset" : 474
    }, {
      "referenceID" : 0,
      "context" : "[2] Charu C Aggarwal and ChengXiang Zhai.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[3] Marco Baroni, Georgiana Dinu, and Germán Kruszewski.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[4] Robert M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[8] Yoshua Bengio, Patrice Simard, and Paolo Frasconi.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[9] Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[10] Léon Bottou.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[11] John S Bridle.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] Lijuan Cai and ThomasHofmann.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] Eugene Charniak and Mark Johnson.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] William W Cohen, Robert E Schapire, and Yoram Singer.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] Ronan Collobert and Jason Weston.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[19] Thomas M Cover and Joy A Thomas.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[20] Ido Dagan, Lillian Lee, and Fernando CN Pereira.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] Ido Dagan, Shaul Marcus, and Shaul Markovitch.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] Paramveer S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] Paramveer S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] Elizabeth D Dolan and Jorge J Moré.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] John Duchi, Elad Hazan, and Yoram Singer.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] Jeffrey L Elman.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[27] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[28] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[29] John R Firth.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[31] Kunihiko Fukushima.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[32] Xavier Glorot, Antoine Bordes, and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[33] Irving J Good.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[34] Michael U Gutmann and Aapo Hyvärinen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[36] Karl Moritz Hermann.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[37] Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[38] Swapnil Hingmire, Sandeep Chougule, Girish K Palshikar, and Sutanu Chakraborti.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[39] Geoffrey E Hinton.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[40] Geoffrey E Hinton and Ruslan R Salakhutdinov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[41] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[42] Sepp Hochreiter.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[43] Sepp Hochreiter and Jürgen Schmidhuber.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "[44] Eric H Huang, Richard Socher, Christopher DManning, and Andrew YNg.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "[45] Michael N Jones and Douglas JK Mewhort.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 42,
      "context" : "[46] Nal Kalchbrenner and Phil Blunsom.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 43,
      "context" : "[47] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : "[48] Yoon Kim.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "[49] Dan Klein and Christopher D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 46,
      "context" : "[50] Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 47,
      "context" : "[51] Reinhard Kneser and Hermann Ney.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "[52] Youngjoong Ko.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 49,
      "context" : "[53] Philipp Koehn.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 50,
      "context" : "[54] Thomas K Landauer.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 51,
      "context" : "[55] Thomas K Landauer and Susan T Dumais.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 52,
      "context" : "[56] Thomas K Landauer, Peter W Foltz, and Darrell Laham.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 53,
      "context" : "[57] Gabriella Lapesa, Stefan Evert, and Sabine Schulte im Walde.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 54,
      "context" : "[58] Quoc V Le and Tomas Mikolov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 55,
      "context" : "[59] Rémi Lebret and Ronan Collobert.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 56,
      "context" : "[60] Rémi Lebret and Ronan Collobert.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 57,
      "context" : "[61] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 58,
      "context" : "[62] Daniel D Lee and H Sebastian Seung.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 59,
      "context" : "[63] Gregory W Lesher, Bryan J Moulton, D Jeffery Higginbotham, et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 60,
      "context" : "[64] Omer Levy and Yoav Goldberg.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 61,
      "context" : "[65] Omer Levy, Yoav Goldberg, and Ido Dagan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 62,
      "context" : "[66] David D Lewis.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 63,
      "context" : "[67] Yitan Li, Linli Xu, Fei Tian, Liang Jiang, Xiaowei Zhong, and Enhong Chen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 64,
      "context" : "[68] Chih-Jen Lin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 65,
      "context" : "[69] Dekang Lin and Xiaoyun Wu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 66,
      "context" : "[70] Kevin Lund, Curt Burgess, and Ruth Ann Atchley.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 67,
      "context" : "[71] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 68,
      "context" : "[72] Tomáš Mikolov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 69,
      "context" : "[73] TomasMikolov, Kai Chen, GregCorrado, and JeffreyDean.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 70,
      "context" : "[74] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 71,
      "context" : "[75] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 72,
      "context" : "[76] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 73,
      "context" : "[77] Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Matthew Purver.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 74,
      "context" : "[78] Jeff Mitchell and Mirella Lapata.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 75,
      "context" : "[79] Andriy Mnih and Geoffrey Hinton.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 76,
      "context" : "[80] Andriy Mnih and Geoffrey E Hinton.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 77,
      "context" : "[81] Andriy Mnih and Koray Kavukcuoglu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 78,
      "context" : "[82] Frederic Morin and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 79,
      "context" : "[83] Frederic Morin and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 80,
      "context" : "[84] Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 81,
      "context" : "[85] Andrew Y Ng.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 82,
      "context" : "[86] Bo Pang and Lillian Lee.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 83,
      "context" : "[87] Bo Pang and Lillian Lee.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 84,
      "context" : "[88] Arkadiusz Paterek.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 85,
      "context" : "[89] Fuchun Peng, Fangfang Feng, and Andrew McCallum.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 86,
      "context" : "[91] Fernando Pereira, Naftali Tishby, and Lillian Lee.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 87,
      "context" : "[92] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 88,
      "context" : "[93] David C Plaut and Geoffrey E Hinton.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 89,
      "context" : "[94] Matt Post and Shane Bergsma.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 90,
      "context" : "[95] Lutz Prechelt.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 91,
      "context" : "[96] Dragomir R Radev, Pradeep Muthukrishnan, and Vahed Qazvinian.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 92,
      "context" : "[97] Lev Ratinov and Dan Roth.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 93,
      "context" : "[98] Magnus Sahlgren.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 94,
      "context" : "[99] Gerard Salton and Christopher Buckley.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 95,
      "context" : "[100] Hinrich Schütze.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 96,
      "context" : "[101] Hinrich Schütze.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 97,
      "context" : "[102] Richard Socher.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 98,
      "context" : "[103] Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 99,
      "context" : "[104] Richard Socher, Danqi Chen, Christopher DManning, and AndrewNg.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 100,
      "context" : "[105] Richard Socher, Eric H Huang, Jeffrey Pennington, Andrew Y Ng, and Christopher D Manning.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 101,
      "context" : "[106] Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 102,
      "context" : "[107] Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 103,
      "context" : "[108] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 104,
      "context" : "[109] Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 105,
      "context" : "[110] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 106,
      "context" : "[111] Pontus Stenetorp, Hubert Soyer, Sampo Pyysalo, Sophia Ananiadou, and Takashi Chikayama.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 107,
      "context" : "[112] Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi Cheng.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 108,
      "context" : "[113] Ilya Sutskever, James Martens, and Geoffrey E Hinton.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 109,
      "context" : "[114] Buzhou Tang, Xuan Wang, and Xiaohong Wang.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 110,
      "context" : "[115] Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 111,
      "context" : "[116] Joseph Turian, LevRatinov, andYoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 112,
      "context" : "[117] Peter D Turney.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 113,
      "context" : "[118] Peter D Turney and Patrick Pantel.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 114,
      "context" : "[119] Kun Wang, Chengqing Zong, and Keh-Yih Su.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 115,
      "context" : "[120] Sida Wang and Christopher D Manning.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 116,
      "context" : "[121] Sida I.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 117,
      "context" : "[122] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 118,
      "context" : "[123] Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 119,
      "context" : "[124] Wei Xu and Alex Rudnicky.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 120,
      "context" : "[125] Nianwen Xue et al.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 121,
      "context" : "[126] Matthew D Zeiler.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 122,
      "context" : "[127] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 123,
      "context" : "[128] Yin Zhang, Rong Jin, and Zhi-Hua Zhou.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 124,
      "context" : "[129] Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 125,
      "context" : "[130] Hai Zhao and Chunyu Kit.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 126,
      "context" : "[131] Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 0,
      "context" : "[2] 来斯惟,徐立恒,陈玉博,刘康,赵军.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[3] Siwei Lai, Yang Liu, Huxiang Gu, Liheng Xu, Kang Liu, Shiming Xiang, Jun Zhao, Rui Diao, Liang Xiang, Hang Li, and Dong Wang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[4] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] Liheng Xu, Kang Liu, Siwei Lai and Jun Zhao.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "Data representation is a fundamental task in machine learning. The representation of data affects the performance of the whole machine learning system. In a long history, the representation of data is done by feature engineering, and researchers aim at designing better features for specific tasks. Recently, the rapid development of deep learning and representation learning has brought new inspiration to various domains. In natural language processing, the most widely used feature representation is the Bag-of-Words model. This model has the data sparsity problem and cannot keep the word order information. Other features such as part-of-speech tagging or more complex syntax features can only fit for specific tasks inmost cases. This thesis focuses onword representation and document representation. We compare the existing systems and present our new model. First, for generating word embeddings, we make comprehensive comparisons among existing word embedding models. In terms of theory, we figure out the relationship between the two most important models, i.e., Skip-gram and GloVe. In our experiments, we analyze three key points in generating word embeddings, including the model construction, the training corpus and parameter design. We evaluate word embeddings with three types of tasks, and we argue that they cover the existing use of word embeddings. Through theory and practical experiments, we present some guidelines for how to generate a good word embedding. Second, in Chinese character or word representation, we find that the existing models always use theword embeddingmodels directly. We introduce the joint training of Chinese character and word. This method incorporates the context words into the representation space of a Chinese character, which leads to a better representation of Chinese characters and words. In the tasks of Chinese character segmentation and document classification, the joint training outperforms the existing methods that train characters or words with traditional word embedding algorithms. Third, for document representation, we analyze the existing document representation models, including recursive neural networks, recurrent neural networks and coniv 基于神经网络的词和文档语义向量表示方法研究 volutional neural networks. We point out the drawbacks of these models and present our newmodel, the recurrent convolutional neural networks. In text classification task, the experimental results show that our model outperforms the existing models.",
    "creator" : "LaTeX with hyperref package"
  }
}