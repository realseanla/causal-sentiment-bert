{
  "name" : "1703.10356.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "End-to-End MAP Training of a Hybrid HMM-DNN Model",
    "authors" : [ "Lior Fritz", "David Burshtein" ],
    "emails" : [ "lior.fritz@gmail.com,", "burstyn@eng.tau.ac.il" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "A major advantage of connectionist temporal classification (CTC) [1] over a hybrid of a hidden Markov model (HMM) and a deep neural network (DNN) [2] lies in the simplicity of the training and its scalability. However, there are some issues with CTC. First, exact decoding is computationally intractable, and one needs to use some approximation [1]. In addition, CTC does not excel in providing a good alignment between the input and output sequences, posing challenges in some applications [3, 4].\nWe suggest a novel training method of an HMM-DNN hybrid. Similarly to CTC, we use end-to-end training. However, unlike CTC, our derivation is formulated using the maximum a-posteriori (MAP) approach with a simple language model (LM) in the training stage. The resulting objective function is similar to [5] with some differences. First, we derive our method from first principles as a MAP estimate. Second, we allow the integration of a scalable LM into the objective function. Decoding can be performed using a standard HMM decoder, without approximations. We use the weighted finite state transducer (WFST) approach [6] and discuss some implementation issues. Speech recognition results on the Wall Street Journal (WSJ) corpus [7] compare favorably with CTC. One of the advantages of the method is its robustness and the fact that it can be used to provide reliable alignment between the input and output sequences."
    }, {
      "heading" : "2. The Model",
      "text" : "Consider a sentence where each word is represented by a left to right hidden Markov model (HMM) [8] whose states are the basic elements of the word (termed labels), such as the character spelling or the phonetic transcription of the word as in [9, 10]. A blank label is inserted between words, and we reserve two states for sentence start and end. The transcript (template),\nΓ, of the sentence is its sequence of labels. As an example, consider the sentence “is he” and assume a character-based modeling. Then the transcript is Γ = {start, blank, i, s, blank, h, e, blank, end}.\nIn fact, the sentence transcript, Γ, defines a left to right HMM, where transitions are only allowed between neighboring states in the transcript, or from a state to itself (self transition). That is,\nP (Γk | Γj) = { pΓj (0) if k = j pΓj (1) if k = j + 1 0 otherwise\n(1)\nand pj(0) + pj(1) = 1 for all j. Figure 1 describes the character-based HMM defined by the sentence “is he”.\nTo formulate a MAP estimation scheme we need to define a probabilistic LM for strings s = (s0, s1, . . . , sT ). This model also defines the distribution of the transcript random variable, Γ. As an optimal solution, one would use the ground-truth LM used in the decoding stage, yet this would be computationally infeasible. As an alternative solution, we suggest a degenerated model of labels bi-grams. We have found that this model is sufficient for obtaining state-of-the-art results. Given that at time t−1 the process is at state c, we remain at c with probability pc(0), and make a transition to any other state ĉ with probability pc(1)q(c, ĉ). That is,\npc,ĉ =P (St = ĉ | St−1 = c) = {\npc(0) if ĉ = c pc(1) · q(c, ĉ) otherwise (2)\nWe assume q(c, c) = 0. For a characters based labeling this constraint can be dealt with either by using the solution suggested in [5], or by defining additional characters that represent consecutive identical characters (e.g. ‘ll’, ‘mm’). For phonetic based labeling we add a blank label between consecutive identical phonemes.\nLet {ot} be the sequence of acoustic feature vectors representing some sentence. Define a time-extended feature vector, õt\n∆= (ot−F , . . . ,ot, . . . ,ot+F ), for some integer F . We assume a left to right HMM for the sequence {õt}. The underlying hidden state (label) sequence is {st}, where st ∈ {0, 1, . . . , L−1} and L is the total number of possible states. The joint probability of õ ∆={õt}Tt=1\nar X\niv :1\n70 3.\n10 35\n6v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\n01 7\nand s ∆= {st}Tt=0 is given by\nP (õ, s) = T∏ t=1 pst−1,st T∏ t=1 P (õt | st) (3)\nSuppose also that some neural network (NN) recognizer produces the values yt,st ≡ yt,st,θ, which are estimates of 1 logP (st | õt), for t = 1, 2, . . . , T . Here θ are the parameters of the NN. Note that ∑L−1 l=0 e\nyt,l = 1 for all t. By Bayes’ law, (3) can be rewritten as,\nP (õ, s) = A ( T∏ t=1 pst−1,st ) · exp { T∑ t=1 (yt,st − ωst ) } (4) where ωl ∆= logP (l) is the logarithm of the prior proba-\nbility of the state l, and A = ∏T t=1 P (õt).\nWe train our model using a database of N pairs (Γn, õn), n = 1, 2, . . . , N , where õn is the n-th observation sequence, and Γn is its associated transcript. First, the values of the probability transition matrix q(c, ĉ) can be estimated from a training text using\nq(c, ĉ) = Nc,ĉ/Nc (5)\nwhere Nc,ĉ (Nc, respectively) is the number of occurrences of label pairs c, ĉ (labels, c) in the text.\nTo train the remaining model parameters we use the MAP criterion. The goal is to maximize the loglikelihood of the transcript given its observation sequence. That is, the MAP estimate, ΨMAP, to the parameter vector, Ψ = { θ, {pj(0), pj(1), ωj}j∈{0,1,...,L−1} } , is given by\nΨMAP = argmax Ψ N∑ n=1 logPΨ (Γn | õn) (6)\nlogP (Γ | õ) = logP (õ,Γ)− logP (õ) (7) Since the term A in (4) cancels out in (7), in the sequel we set A = 1 without loss of generality.\nOur criterion (6)-(7) is similar to the one in [5]. However, our criterion is derived as a MAP estimate. Also, in [5] the values of the transition probabilities between all labels have to be trained, whereas in our model, the probabilities q(c, ĉ) can be pre-trained, so that only pc(0) for all labels c need to be trained from the acoustics. This also allows convenient integration with higher-level LM.\nOnce the model has been trained, Decoding is preformed with a standard LM instead of the simple bi-gram LM used for training. Thus we can apply a standard HMM decoder as we explain later in more detail."
    }, {
      "heading" : "2.1. Training Procedure",
      "text" : "To apply (6), we need to compute P (õ) and P (õ,Γ). By (4) and since, as explained, we may set A = 1, the first term, P (õ), is computed by summing over all possible state sequences, as follows,\nP (õ) = ∑\ns ( T∏ t=1 pst−1,st ) · exp { T∑ t=1 yt,st } (8)\n1The base of all the logarithms in this paper is e\nwhere we denote yt,st ∆= yt,st − ωst . The transition probabilities are given by (2). To compute P (õ,Γ) we use,\nP (õ,Γ) = ∑\ns∈S(Γ) ( T∏ t=1 pst−1,st ) · exp { T∑ t=1 yt,st } (9)\nwhere S(Γ) are all state sequences (s0, s1, . . . , sT ) which are consistent with the given transcript, Γ.\nTo compute P (õ,Γ) and its derivatives efficiently, first denote\nαt(k) ∆= P (T (S0, . . . , St) ∈ (Γ0, . . . ,Γk) (10)\n, õ1, . . . , õt)\nβt(k) ∆= P (T (St, . . . , ST ) ∈ (Γk, . . . ,ΓK) (11)\n, õt+1, . . . , õT | St = Γk)\nwhere T (S0, . . . , St) is the transcript of the state sequence up to time t and where Γ0, . . . ,Γk is the k + 1 prefix of Γ. T (St, . . . , ST ) is the transcript of the state sequence from t to T and Γk, . . . ,ΓK is the K − k + 1\nsuffix of Γ. Then P (õ,Γ) = K∑ k=0 αt(k)βt(k) for any t.\nNow, αt(k) and βt(k) can be computed efficiently using the following time recursions,\nαt(k) = (αt−1(k)pΓk (0) (12) +αt−1(k − 1)pΓk−1(1)q(Γk−1,Γk) ) eyt,Γk\nβt(k) =βt+1(k)pΓk (0)e yt+1,Γk (13)\n+ βt+1(k + 1)pΓk (1)q(Γk,Γk+1)e yt+1,Γk+1\nOur training algorithm is a stochastic gradient descent (SGD) that operates on mini-batches. To compute the gradient with respect to any element, θi, of the parameter vector, θ, we use\n∂P (õ,Γ) ∂θi = ∑ t,k ∂P (õ,Γ) ∂yt,k ∂yt,k ∂θi\n(14)\nWe calculate ∂yt,k ∂θi using the back-propagation algorithm. In addition, by (9),\n∂P (õ,Γ) ∂yt,r\n= ∑\nk : Γk=r\nαt(k)βt(k) (15)\n∂P (õ,Γ) ∂ωr = − T∑ t=1 ∑ k : Γk=r αt(k)βt(k) (16)\n∂P (õ,Γ) ∂pr(0) = T∑ t=1 ∑ k : Γk=r αt−1(k)βt(k) exp { yt,Γk } (17)\n∂P (õ,Γ) ∂pr(1) = T∑ t=1 ∑ k : Γk=r αt−1(k)q(Γk,Γk+1) (18)\n· βt(k + 1) exp { yt,Γk+1 } Note that the values eyt,l are constrained to be valid probabilities. To satisfy this constraint we use the standard approach of a softmax layer at the output of the\nNN. Similarly, pj(0), pj(1) and ωj are constrained to be valid probabilities. To satisfy this constraint, we first define unconstrained transition probabilities and priors and pass them through a softmax function that produces valid transition probabilities and priors. The derivatives of the unconstrained parameters can be calculated using the chain rule, taking into account the softmax derivatives and (15)-(18).\nTo compute P (õ) and its derivatives efficiently, first denote\nα̃t(c) ∆= P (õ1, õ2, . . . , õt, St = c) (19)\nβ̃t(c) ∆= P (õt+1, õt+2, . . . , õT | St = c) (20) Then P (õ) = ∑ c α̃t(c)β̃t(c) for any t. Now, α̃t(c) and β̃t(c) can be computed efficiently using,\nα̃t(c) = ( α̃t−1(c)pc(0) (21)\n+ ∑ ĉ6=c α̃t−1(ĉ)pĉ(1)q(ĉ, c) ) eyt,c\nβ̃t(c) =β̃t+1(c)pc(0)eyt+1,c (22) + ∑ ĉ6=c β̃t+1(ĉ)pc(1)q(c, ĉ)eyt+1,ĉ\nAlso,\n∂P (õ) ∂yt,r = α̃t(r)β̃t(r) (23)\n∂P (õ) ∂ωr = − T∑ t=1 α̃t(r)β̃t(r) (24)\n∂P (õ) ∂pr(0) = T∑ t=1 α̃t−1(r)β̃t(r) exp { yt,r }\n(25)\n∂P (õ) ∂pr(1) = T∑ t=1 ∑ c6=r α̃t−1(r)q(r, c)β̃t(c) exp { yt,c } (26)"
    }, {
      "heading" : "2.2. Decoding",
      "text" : "To decode we can use standard HMM decoding techniques without approximations. To integrate our model with a LM, we use the WFST approach. A WFST is a finite-state machine, in which each transition has an input symbol, an output symbol and a weight. Our WFST is implemented with the FST library OpenFST [11], and is based on the decoding method of EESEN [10].\nWe first build separate WFSTs for the LM (grammar), the lexicon and the HMM labels. An example for the grammar WFST, G, with two possible sentences is shown in Figure 2a. The lexicon WFST, L, encodes sequences of lexicon units (e.g. phonemes or characters) to words. It enforces the occurrence of the blank label between words, and, for phoneme labeling, also between identical labels. An example is shown in Figure 2b. The HMM WFST, H, maps a sequence of frame-level labels into a single lexicon unit. It allows occurrences of repetitive labels with the proper weighting. An example is shown in Figure 2c. Finally, we compose these graphs into a single graph. The inputs to this graph are the outputs of the neural network, normalized by the priors.\n0 1cats:cats/1.0\n2 like:lik e/0.5\n4\nplaying:playing/1.0\n3\nhate:hate/0.5 water: water/ 1.0\n(a) The G WFST exemplified on the sentences: “cats like playing” and “cats hate water”. The caption on each arc represents: input symbol : output symbol / weight.\n0 1 bl: 2 K:cats 3 AE: 4 T: 5 S:\nbl:\n(b) The L WFST exemplified on the word “cats” using phonemes. The symbol represents no input/output."
    }, {
      "heading" : "2.3. Numerical Issues",
      "text" : "For HMMs and CTC, the calculations of the forward and backward variables are usually performed in log-scale, as in [9,10]. Another possibility is to use the normalization technique in [12]. In our implementation it was necessary to combine both approaches, with some modification of the method in [12], as we now describe. For each time frame, t, we first compute the forward (backward, respectively) variables according to the original recursions in log-scale, and then subtract the maximal value of the forward (backward) variables at t from each forward (backward) variable. A simpler variant of the above is to perform the subtraction only every certain time units.\nFor simplicity, we describe our method in the linear scale, for P (õ) and its derivatives. The adaptation to the calculation of P (õ,Γ) and its derivatives is straightforward. Denote α̃∗t\n∆= max c α̃t(c) and β̃∗t ∆= max c β̃t(c).\nEffectively, the normalized forward and backward variables are given by,\nαt(c) = α̃t(c) t∏\nu=1 α̃∗u , βt(c) = β̃t(c) T∏ u=t β̃∗u\n(27)\nDefining P t(õ) , ∑\nc αt(c)βt(c), we have P t(õ) = ∑ c α̃t(c) t∏\nu=1 α̃∗u β̃t(c) T∏ u=t β̃∗u = P (õ) t∏ u=1 α̃∗u T∏ u=t β̃∗u\n(28)\nUsing (23)-(24) yields (the derivatives with respect to pr(0) and pr(1) can be obtained similarly using (25)- (26)),\n∂ logP (õ) ∂yt,r = αt(r)βt(r) P t(õ)\n(29)\n∂ logP (õ) ∂ωr = − T∑ t=1 αt(r)βt(r) P t(õ)\n(30)"
    }, {
      "heading" : "2.4. Implementation",
      "text" : "We implemented our model in Tensorflow [13]. Our loss function is implemented in C++, where the samples in the mini-batch are calculated in parallel on different cores of the CPU. We have integrated the feature extraction procedures from EESEN (which uses Kaldi’s scripts [14]) and [15, 16], the NN procedures from Tensorflow and the WFSTs decoding procedures from EESEN. Since the computation time required by the NN is dominant, the training process takes only 5%-8% more time when using our loss function in comparison to CTC."
    }, {
      "heading" : "3. Experiments",
      "text" : "We conducted experiments on the Wall Street Journal (WSJ) corpus [7]. The training data consists of 81 hours of transcribed speech. We use almost the same training process and NN architecture as in [10]. We extract 95% of the training set for training, and 5% for cross validation. The inputs of the NN are 40-dimensional filterbank with delta and delta-delta coefficients. The features are normalized by mean and variance on the speaker basis. We operate on a phonemes-based system, with 72 labels. The utterances in the training set are sorted by their lengths. The mini-batch size is set to 30. We use the ADAM optimizer [17] with an initial learning rate of 0.001. We use gradient clipping [18] with a value of 50. The learning rate decay and the stopping criteria are determined based on the validation WER (optimizing for each method). We test our model on the eval92 and dev93 test sets. The acoustic model scores are scaled down by a factor of 0.5-0.9, and the optimal value is chosen. We use the standard pruned trigram LM of WSJ, as in [10]. The bi-gram LM used in training is obtained using the training set text, and the large WSJ LM training set text. We use a RNN architecture of 4 layers of 320 bi-directional LSTM cells [19] without peephole connections [20].\nTable 1 shows the obtained WER on the validation, eval92 and dev93 sets. We compare our model (HMMMAP) to the CTC model under the same conditions. Decoding with CTC is performed using the TLG WFST and the prior normalization method in [10]. Our CTC model obtains comparable results with the ones reported in [10]. We see a consistent improvement in WER of HMM-MAP compared to CTC. Figure 3 shows the validation WER vs. epoch of CTC, and our model using three different LMs during training: bi-grams, uni-grams and uniform. It demonstrates the importance of using a reasonable LM during training. Figure 4 shows the obtained alignments on some utterance from the training set. It demonstrates a significantly improved alignment of HMM-MAP compared to CTC, which is due to the peaky output distributions of CTC."
    }, {
      "heading" : "4. Conclusions",
      "text" : "We have presented a novel end-to-end MAP training method of a HMM-DNN hybrid. The model compares favorably to CTC on the WSJ corpus. We also believe our method is more robust in the following sense. In [21], using CTC, it was found that the network’s prediction of the blank label was too dominant during decoding. The authors suggested a blank-prediction-diminishing method, by subtracting a high prior from the network’s blank prediction. In [10] a different priors normalization method was proposed, which also diminishes the blank predictions. However, the formulation of the CTC model does not justify priors normalization. In [22] it has been shown that the priors subtraction method in [10] might even degrade the WER in some cases.\nIn future work, we plan to further examine the effect of the LM used in the training process, and apply our model to other sequence-to-sequence tasks, where it may be important to provide good alignment between the input and output sequences."
    }, {
      "heading" : "5. Acknowledgements",
      "text" : "This research was supported by the Yitzhak and Chaya Weinstein Research Institute for Signal Processing. A Tesla K40c GPU used in this research was donated by the NVIDIA Corporation."
    }, {
      "heading" : "6. References",
      "text" : "[1] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber,\n“Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.\n[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.\n[3] Y. Wang, X. Deng, S. Pu, and Z. Huang, “Residual convolutional CTC networks for automatic speech recognition,” arXiv preprint arXiv:1702.07793, 2017.\n[4] R. Sanabria, F. Metze, and F. De La Torre, “Robust end-to-end deep audiovisual speech recognition,” arXiv preprint arXiv:1611.06986, 2016.\n[5] R. Collobert, C. Puhrsch, and G. Synnaeve, “Wav2letter: an end-to-end convnet-based speech recognition system,” Neural Information Processing Systems (NIPS). [Online]. Available: https://arxiv.org/abs/1609.03193\n[6] M. Mohri, F. Pereira, and M. Riley, “Weighted finitestate transducers in speech recognition,” Computer Speech & Language, vol. 16, no. 1, pp. 69–88, 2002.\n[7] D. B. Paul and J. M. Baker, “The design for the Wall Street Journal-based CSR corpus,” in Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 357–362.\n[8] R. L. Rabiner, “A tutorial on hidden Markov models and selected applications in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2, pp. 257–286, 1989.\n[9] A. Y. Hannun, C. Case, J. Casper, B. C. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng, “Deep speech: scaling up end-to-end speech recognition,” CoRR, vol. abs/1412.5567, 2014. [Online]. Available: http://arxiv.org/abs/1412.5567\n[10] Y. Miao, M. Gowayyed, and F. Metze, “EESEN: Endto-end speech recognition using deep RNN models and WFST-based decoding,” in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), December 2015.\n[11] C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri, “OpenFst: A general and efficient weighted finite-state transducer library. Implementation and Application of Automata,” 2007.\n[12] L. R. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition. PTR Prentice Hall, 1993.\n[13] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin et al., “Tensorflow: Large-scale machine learning on heterogeneous distributed systems,” arXiv preprint arXiv:1603.04467, 2016.\n[14] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The Kaldi speech recognition toolkit,” in IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584. IEEE Signal Processing Society, 2011.\n[15] Y. Miao, “Kaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn,” arXiv preprint arXiv:1401.6984, 2014.\n[16] V. Renkens, “Kaldi with TensorFlow Neural Net,” https: //github.com/vrenkens/tfkaldi, 2016.\n[17] D. Kingma and J. Ba, “ADAM: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.\n[18] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difficulty of training recurrent neural networks,” International Conference on Machine Learning (ICML), vol. 28, pp. 1310–1318, 2013.\n[19] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735– 1780, 1997.\n[20] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber, “Learning precise timing with LSTM recurrent networks,” Journal of machine learning research, vol. 3, no. Aug, pp. 115–143, 2002.\n[21] H. Sak, A. Senior, K. Rao, O. Irsoy, A. Graves, F. Beaufays, and J. Schalkwyk, “Learning acoustic frame labeling for speech recognition with recurrent neural networks,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4280–4284.\n[22] N. Kanda, X. Lu, and H. Kawai, “Maximum a posteriori based decoding for CTC acoustic models,” Interspeech 2016, pp. 1868–1872, 2016."
    } ],
    "references" : [ {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,",
      "author" : [ "A. Graves", "S. Fernández", "F. Gomez", "J. Schmidhuber" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Residual convolutional CTC networks for automatic speech recognition,",
      "author" : [ "Y. Wang", "X. Deng", "S. Pu", "Z. Huang" ],
      "venue" : "arXiv preprint arXiv:1702.07793,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "Robust end-to-end deep audiovisual speech recognition,",
      "author" : [ "R. Sanabria", "F. Metze", "F. De La Torre" ],
      "venue" : "arXiv preprint arXiv:1611.06986,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Weighted finitestate transducers in speech recognition,",
      "author" : [ "M. Mohri", "F. Pereira", "M. Riley" ],
      "venue" : "Computer Speech & Language,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "The design for the Wall Street Journal-based CSR corpus,",
      "author" : [ "D.B. Paul", "J.M. Baker" ],
      "venue" : "Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1992
    }, {
      "title" : "A tutorial on hidden Markov models and selected applications in speech recognition,",
      "author" : [ "R.L. Rabiner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1989
    }, {
      "title" : "Deep speech: scaling up end-to-end speech recognition,",
      "author" : [ "A.Y. Hannun", "C. Case", "J. Casper", "B.C. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng" ],
      "venue" : "CoRR, vol. abs/1412.5567,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "EESEN: Endto-end speech recognition using deep RNN models and WFST-based decoding,",
      "author" : [ "Y. Miao", "M. Gowayyed", "F. Metze" ],
      "venue" : "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "OpenFst: A general and efficient weighted finite-state transducer library",
      "author" : [ "C. Allauzen", "M. Riley", "J. Schalkwyk", "W. Skut", "M. Mohri" ],
      "venue" : "Implementation and Application of Automata,”",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Fundamentals of Speech Recognition",
      "author" : [ "L.R. Rabiner", "B.-H. Juang" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1993
    }, {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous distributed systems,",
      "author" : [ "M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin" ],
      "venue" : "arXiv preprint arXiv:1603.04467,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "The Kaldi speech recognition toolkit,",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz" ],
      "venue" : "IEEE 2011 workshop on automatic speech recognition and understanding,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1925
    }, {
      "title" : "Kaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn,",
      "author" : [ "Y. Miao" ],
      "venue" : "arXiv preprint arXiv:1401.6984,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Kaldi with TensorFlow Neural Net,",
      "author" : [ "V. Renkens" ],
      "venue" : "https: //github.com/vrenkens/tfkaldi,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "ADAM: A method for stochastic optimization,",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "On the difficulty of training recurrent neural networks,",
      "author" : [ "R. Pascanu", "T. Mikolov", "Y. Bengio" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Long short-term memory,",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1997
    }, {
      "title" : "Learning precise timing with LSTM recurrent networks,",
      "author" : [ "F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber" ],
      "venue" : "Journal of machine learning research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Learning acoustic frame labeling for speech recognition with recurrent neural networks,",
      "author" : [ "H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Maximum a posteriori based decoding for CTC acoustic models,",
      "author" : [ "N. Kanda", "X. Lu", "H. Kawai" ],
      "venue" : "Interspeech 2016,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Introduction A major advantage of connectionist temporal classification (CTC) [1] over a hybrid of a hidden Markov model (HMM) and a deep neural network (DNN) [2] lies in the simplicity of the training and its scalability.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Introduction A major advantage of connectionist temporal classification (CTC) [1] over a hybrid of a hidden Markov model (HMM) and a deep neural network (DNN) [2] lies in the simplicity of the training and its scalability.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "First, exact decoding is computationally intractable, and one needs to use some approximation [1].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "In addition, CTC does not excel in providing a good alignment between the input and output sequences, posing challenges in some applications [3, 4].",
      "startOffset" : 141,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "In addition, CTC does not excel in providing a good alignment between the input and output sequences, posing challenges in some applications [3, 4].",
      "startOffset" : 141,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "We use the weighted finite state transducer (WFST) approach [6] and discuss some implementation issues.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Speech recognition results on the Wall Street Journal (WSJ) corpus [7] compare favorably with CTC.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "The Model Consider a sentence where each word is represented by a left to right hidden Markov model (HMM) [8] whose states are the basic elements of the word (termed labels), such as the character spelling or the phonetic transcription of the word as in [9, 10].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "The Model Consider a sentence where each word is represented by a left to right hidden Markov model (HMM) [8] whose states are the basic elements of the word (termed labels), such as the character spelling or the phonetic transcription of the word as in [9, 10].",
      "startOffset" : 254,
      "endOffset" : 261
    }, {
      "referenceID" : 8,
      "context" : "The Model Consider a sentence where each word is represented by a left to right hidden Markov model (HMM) [8] whose states are the basic elements of the word (termed labels), such as the character spelling or the phonetic transcription of the word as in [9, 10].",
      "startOffset" : 254,
      "endOffset" : 261
    }, {
      "referenceID" : 9,
      "context" : "Our WFST is implemented with the FST library OpenFST [11], and is based on the decoding method of EESEN [10].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "Our WFST is implemented with the FST library OpenFST [11], and is based on the decoding method of EESEN [10].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "Numerical Issues For HMMs and CTC, the calculations of the forward and backward variables are usually performed in log-scale, as in [9,10].",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Numerical Issues For HMMs and CTC, the calculations of the forward and backward variables are usually performed in log-scale, as in [9,10].",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 10,
      "context" : "Another possibility is to use the normalization technique in [12].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "In our implementation it was necessary to combine both approaches, with some modification of the method in [12], as we now describe.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "Model WER% validation eval92 dev93 CTC, EESEN [10] 7.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "87 Hybrid HMM-DNN [10] 7.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "We implemented our model in Tensorflow [13].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "We have integrated the feature extraction procedures from EESEN (which uses Kaldi’s scripts [14]) and [15, 16], the NN procedures from Tensorflow and the WFSTs decoding procedures from EESEN.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "We have integrated the feature extraction procedures from EESEN (which uses Kaldi’s scripts [14]) and [15, 16], the NN procedures from Tensorflow and the WFSTs decoding procedures from EESEN.",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "We have integrated the feature extraction procedures from EESEN (which uses Kaldi’s scripts [14]) and [15, 16], the NN procedures from Tensorflow and the WFSTs decoding procedures from EESEN.",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "We conducted experiments on the Wall Street Journal (WSJ) corpus [7].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "We use almost the same training process and NN architecture as in [10].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : "We use the ADAM optimizer [17] with an initial learning rate of 0.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "We use gradient clipping [18] with a value of 50.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "We use the standard pruned trigram LM of WSJ, as in [10].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "We use a RNN architecture of 4 layers of 320 bi-directional LSTM cells [19] without peephole connections [20].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : "We use a RNN architecture of 4 layers of 320 bi-directional LSTM cells [19] without peephole connections [20].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "Decoding with CTC is performed using the TLG WFST and the prior normalization method in [10].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "Our CTC model obtains comparable results with the ones reported in [10].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "In [21], using CTC, it was found that the network’s prediction of the blank label was too dominant during decoding.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "In [10] a different priors normalization method was proposed, which also diminishes the blank predictions.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "In [22] it has been shown that the priors subtraction method in [10] might even degrade the WER in some cases.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "In [22] it has been shown that the priors subtraction method in [10] might even degrade the WER in some cases.",
      "startOffset" : 64,
      "endOffset" : 68
    } ],
    "year" : 2017,
    "abstractText" : "An hybrid of a hidden Markov model (HMM) and a deep neural network (DNN) is considered. End-to-end training using gradient descent is suggested, similarly to the training of connectionist temporal classification (CTC). We use a maximum a-posteriori (MAP) criterion with a simple language model in the training stage, and a standard HMM decoder without approximations. Recognition results are presented using speech databases. Our method compares favorably to CTC in terms of performance, robustness and quality of alignments.",
    "creator" : "LaTeX with hyperref package"
  }
}