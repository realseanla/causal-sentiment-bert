{
  "name" : "1611.00448.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Natural-Parameter Networks: A Class of Probabilistic Neural Networks",
    "authors" : [ "Hao Wang", "Xingjian Shi", "Dit-Yan Yeung" ],
    "emails" : [ "hwangaz@cse.ust.hk", "xshiab@cse.ust.hk", "dyyeung@cse.ust.hk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently neural networks (NN) have achieved state-of-the-art performance in various applications ranging from computer vision [12] to natural language processing [20]. However, NN trained by stochastic gradient descent (SGD) or its variants is known to suffer from overfitting especially when training data is insufficient. Besides overfitting, another problem of NN comes from the underestimated uncertainty, which could lead to poor performance in applications like active learning.\nBayesian neural networks (BNN) offer the promise of tackling these problems in a principled way. Early BNN works include methods based on Laplace approximation [16], variational inference (VI) [11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of scalability. Some recent advances in this direction seem to shed light on the practical adoption of BNN. [8] proposed a method based on VI in which a Monte Carlo estimate of a lower bound on the marginal likelihood is used to infer the weights. Recently, [10] used an online version of expectation propagation (EP), called ‘probabilistic back propagation’ (PBP), for the Bayesian learning of NN, and [4] proposed ‘Bayes by Backprop’ (BBB), which can be viewed as an extension of [8] based on the ‘reparameterization trick’ [13]. More recently, an interesting Bayesian treatment called ‘Bayesian dark knowledge’ (BDK) was designed to approximate a teacher network with a simpler student network based on stochastic gradient Langevin dynamics (SGLD) [1].\nAlthough these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a ‘vanilla’ NN; (2) as mentioned in [1], methods\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n00 44\n8v 1\n[ cs\n.L G\n] 2\nN ov\n2 01\nbased on online EP or VI do not involve sampling, but they need to compute the predictive density by integrating out the parameters, which is computationally inefficient; (3) these methods assume Gaussian distributions for the weights and neurons, allowing no flexibility to customize different distributions according to the data as is done in probabilistic graphical models (PGM).\nTo address the problems, we propose natural-parameter networks (NPN) as a class of probabilistic neural networks where the input, target output, weights, and neurons can all be modeled by arbitrary exponential-family distributions (e.g., Poisson distributions for word counts) instead of being limited to Gaussian distributions. Input distributions go through layers of linear and nonlinear transformation deterministically before producing distributions to match the target output distributions (previous work [21] shows that providing distributions as input by corrupting the data with noise plays the role of regularization). As byproducts, output distributions of intermediate layers may be used as second-order representations for the associated tasks. Thanks to the properties of the exponential family [3, 19], distributions in NPN are defined by the corresponding natural parameters which can be learned efficiently by backpropagation. Unlike [4, 1], NPN explicitly propagates the estimates of uncertainty back and forth in deep networks. This way the uncertainty estimates for each layer of neurons are readily available for the associated tasks. Our experiments show that such information is helpful when neurons of intermediate layers are used as representations like in autoencoders (AE). In summary, our main contributions are:\n• We propose NPN as a class of probabilistic neural networks. Our model combines the merits of NN and PGM in terms of computational efficiency and flexibility to customize the types of distributions for different types of data. • Leveraging the properties of the exponential family, some sampling-free backpropagationcompatible algorithms are designed to efficiently learn the distributions over weights by learning the natural parameters. • Unlike most probabilistic NN models, NPN obtains the uncertainty of intermediate-layer neurons as byproducts, which provide valuable information to the learned representations. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance on classification, regression, and unsupervised representation learning tasks."
    }, {
      "heading" : "2 Natural-Parameter Networks",
      "text" : "The exponential family refers to an important class of distributions with useful algebraic properties. Distributions in the exponential family have the form p(x|η) = h(x)g(η) exp{ηTu(x)}, where x is the random variable, η denotes the natural parameters, u(x) is a vector of sufficient statistics, and g(η) is the normalizer. For a given type of distributions, different choices of η lead to different shapes. For example, a univariate Gaussian distribution with η = (c, d)T corresponds to N (− c2d ,− 1 2d ).\nMotivated by this observation, in NPN, only the natural parameters need to be learned to model the distributions over the weights and neurons. Consider an NPN which takes a vector random distribution (e.g., a multivariate Gaussian distribution) as input, multiplies it by a matrix random distribution, goes through nonlinear transformation, and outputs another distribution. Since all three distributions in the process can be specified by their natural parameters (given the types of distributions), learning and prediction of the network can actually operate in the space of natural parameters. For example, if we use element-wise (factorized) gamma distributions for both the weights and neurons, the NPN counterpart of a vanilla network only needs twice the number of free parameters (weights) and neurons since there are two natural parameters for each univariate gamma distribution."
    }, {
      "heading" : "2.1 Notation and Conventions",
      "text" : "We use boldface uppercase letters like W to denote matrices and boldface lowercase letters like b for vectors. Similarly, a boldface number (e.g., 1 or 0) represents a row vector or a matrix with identical entries. In NPN, o(l) is used to denote the values of neurons in layer l before nonlinear transformation and a(l) is for the values after nonlinear transformation. As mentioned above, NPN tries to learn distributions over variables rather than variables themselves. Hence we use letters without subscripts c, d, m, and s (e.g., o(l) and a(l)) to denote ‘random variables’ with corresponding distributions. Subscripts c and d are used to denote natural parameter pairs, such as Wc and Wd. Similarly, subscripts m and s are for mean-variance pairs. Note that for clarity, many operations used below are implicitly element-wise, for example, the square z2, division zb , partial derivative ∂z ∂b , the\ngamma function Γ(z), logarithm log z, factorial z!, 1 + z, and 1z . For the data D = {(xi,yi)} N i=1, we set a(0)m = xi,a (0) s = 0 (Input distributions with a (0) s 6= 0 resemble AE’s denoising effect.) as input of the network and yi denotes the output targets (e.g., labels and word counts). In the following text we drop the subscript i (and sometimes the superscript (l)) for clarity. The bracket (·, ·) denotes concatenation or pairs of vectors."
    }, {
      "heading" : "2.2 Linear Transformation in NPN",
      "text" : "Here we first introduce the linear form of a general NPN. For simplicity, we assume distributions with two natural parameters (e.g., gamma distributions, beta distributions, and Gaussian distributions), η = (c, d)T , in this section. Specifically, we have factorized distributions on the weight matrices, p(W(l)|W(l)c ,W(l)d ) = ∏ i,j p(W (l) ij |W (l) c,ij ,W (l) d,ij), where the pair (W (l) c,ij ,W (l) d,ij) is the corresponding natural parameters. For b(l), o(l), and a(l) we assume similar factorized distributions.\nIn a traditional NN, the linear transformation follows o(l) = a(l−1)W(l) + b(l) where a(l−1) is the output from the previous layer. In NN a(l−1), W(l), and b(l) are deterministic variables while in NPN they are exponential-family distributions, meaning that the result o(l) is also a distribution. For convenience of subsequent computation it is desirable to approximate o(l) using another exponentialfamily distribution. We can do this by matching the mean and variance. Specifically, after computing (W (l) m ,W (l) s ) = f(W (l) c ,W (l) d ) and (b (l) m ,b (l) s ) = f(b (l) c ,b (l) d ), we can get o (l) c and o (l) d through the mean o(l)m and variance o (l) s of o(l) as follows:\n(a(l−1)m ,a (l−1) s ) = f(a (l−1) c ,a (l−1) d ), o (l) m = a (l−1) m W (l) m + b (l) m , (1)\no(l)s = a (l−1) s W (l) s + a (l−1) s (W (l) m ◦W(l)m ) + (a(l−1)m ◦ a(l−1)m )W(l)s + b(l)s , (2)\n(o(l)c ,o (l) d ) = f −1(o(l)m ,o (l) s ), (3)\nwhere ◦ denotes the element-wise product and the bijective function f(·, ·) maps the natural parameters of a distribution into its mean and variance (e.g., f(c, d) = ( c+1−d , c+1 d2 ) in gamma distributions). Similarly we use f−1(·, ·) to denote the inverse transformation. W(l)m , W(l)s , b(l)m , and b(l)s are the mean and variance of W(l) and b(l) obtained from the natural parameters. The computed o(l)m and o (l) s can then be used to recover o (l) c and o (l) d , which will subsequently facilitate the feedforward computation of the nonlinear transformation described in Section 2.3."
    }, {
      "heading" : "2.3 Nonlinear Transformation in NPN",
      "text" : "After we obtain the linearly transformed distribution over o(l) defined by natural parameters o(l)c and o\n(l) d , an element-wise nonlinear transformation v(·) (with a well defined inverse function v−1(·)) will be imposed. The resulting activation distribution is pa(a(l)) = po(v−1(a(l)))|v−1 ′ (a(l))|, where po is the factorized distribution over o(l) defined by (o(l)c ,o (l) d ).\nThough pa(a(l)) may not be an exponential-family distribution, we can approximate it with one, p(a(l)|a(l)c ,a(l)d ), by matching the first two moments. Once the mean a (l) m and variance a (l) s of pa(a(l)) are obtained, we can compute corresponding natural parameters with f−1(·, ·) (approximation accuracy is sufficient according to preliminary experiments). The feedforward computation is:\nam = ∫ po(o|oc,od)v(o)do, as = ∫ po(o|oc,od)v(o)2do− a2m, (ac,ad) = f−1(am,as). (4)\nHere the key computational challenge is computing the integrals in Equation (4). Closed-form solutions are needed for their efficient computation. If po(o|oc,od) is a Gaussian distribution, closedform solutions exist for common activation functions like tanh(x) and max(0, x) (details are in Section B.3). Unfortunately this is not the case for other distributions. Leveraging the convenient form of the exponential family, we find that it is possible to design activation functions so that the integrals for non-Gaussian distributions can also be expressed in closed form. Theorem 1. Assume an exponential-family distribution po(x|η) = h(x)g(η) exp{ηTu(x)}, where the vector u(x) = (u1(x), u2(x), . . . , uM (x))T (M is the number of natural parameters). If activation function v(x) = r − q exp(−τui(x)) is used, the first two moments of v(x), ∫ po(x|η)v(x)dx\nSimilarly the second moment can be computed as E(v(x)2) = r2 + q2 g(η) g(η̂) − 2rq g(η) g(η̃) .\nA more detailed proof is provided in the supplementary material. With Theorem 2, what remains is to find the constants that make v(x) strictly increasing and bounded (Table 1 shows some exponentialfamily distributions and their possible activation functions). For example in Equation (4), if v(x) = r − q exp(−τx), am = r − q( odod+τ ) oc for the gamma distribution.\nIn the backpropagation, for distributions with two natural parameters the gradient consists of two terms. For example, ∂E∂oc = ∂E ∂am ◦ ∂am∂oc + ∂E ∂as ◦ ∂as∂oc , where E is the error term of the network.\nAlgorithm 1 Deep Nonlinear NPN\n1: Input: Data D = {(xi,yi)}Ni=1, number of iterations T , learning rate ρt, number of layers L. 2: for t = 1 : T do 3: for l = 1 : L do 4: Apply Equation (1)-(4) to compute the linear and nonlinear transformation in layer l. 5: end for 6: Compute the error E from (o(L)c ,o (L) d ) or (a (L) c ,a (L) d ). 7: for l = L : 1 do 8: Compute ∂E\n∂W (l) m , ∂E ∂W (l) s , ∂E ∂b (l) m , and ∂E ∂b (l) s . Compute ∂E ∂W (l) c , ∂E ∂W (l) d , ∂E ∂b (l) c , and ∂E ∂b (l) d .\n9: end for 10: Update W(l)c , W (l) d , b (l) c , and b (l) d in all layers. 11: end for"
    }, {
      "heading" : "2.4 Deep Nonlinear NPN",
      "text" : "Naturally layers of nonlinear NPN can be stacked to form a deep NPN1, as shown in Algorithm 22.\nA deep NPN is in some sense similar to a PGM with a chain structure. Unlike PGM in general, however, NPN does not need costly inference algorithms like variational inference or Markov chain Monte Carlo. For some chain-structured PGM (e.g, hidden Markov models), efficient inference algorithms also exist due to their special structure. Similarly, the Markov property enables NPN to be efficiently trained in an end-to-end backpropagation learning fashion in the space of natural parameters.\n1Although the approximation accuracy may decrease as NPN gets deeper during feedforward computation, it can be automatically adjusted according to data during backpropagation.\n2Note that since the first part of Equation (1) and the last part of Equation (4) are canceled out, we can directly use (a(l)m ,a (l) s ) without computing (a (l) c ,a (l) d ) here.\nPGM is known to be more flexible than NN in the sense that it can choose different distributions to depict different relationships among variables. A major drawback of PGM is its scalability especially when the PGM is deep. Different from PGM, NN stacks relatively simple computational layers and learns the parameters using backpropagation, which is computationally more efficient than most algorithms for PGM. NPN has the potential to get the best of both worlds. In terms of flexibility, different types of exponential-family distributions can be chosen for the weights and neurons. Using gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version of nonnegative matrix factorization [14] while an NPN with the Bernoulli distribution and sigmoid activation resembles a Bayesian treatment of sigmoid belief networks [17]. If Poisson distributions are chosen for the neurons, NPN becomes a neural analogue of deep Poisson factor analysis [26, 9].\nNote that similar to the weight decay in NN, we may add the KL divergence between the prior distributions and the learned distributions on the weights to the error E for regularization (we use isotropic Gaussian priors in the experiments). In NPN, the chosen prior distributions correspond to priors in Bayesian models and the learned distributions correspond to the approximation of posterior distributions on weights. Note that the generative story assumed here is that weights are sampled from the prior, and then output is generated (given all data) from these weights."
    }, {
      "heading" : "3 Variants of NPN",
      "text" : "In this section, we introduce three NPN variants with different properties to demonstrate the flexibility and effectiveness of NPN. Note that in practice we use a transformed version of the natural parameters, referred to as proxy natural parameters here, instead of the original ones for computational efficiency. For example, in gamma distributions p(x|c, d) = Γ(c)−1dcxc−1 exp(−dx), we use proxy natural parameters (c, d) during computation rather than the natural parameters (c− 1,−d)."
    }, {
      "heading" : "3.1 Gamma NPN",
      "text" : "The gamma distribution with support over positive values is an important member of the exponential family. The corresponding probability density function is p(x|c, d) = Γ(c)−1dcxc−1 exp(−dx) with (c − 1,−d) as its natural parameters (we use (c, d) as proxy natural parameters). If we assume gamma distributions for W(l), b(l), o(l), and a(l), an AE formed by NPN becomes a deep and nonlinear version of nonnegative matrix factorization [14]. To see this, note that this AE with activation v(x) = x and zero biases b(l) is equivalent to finding a factorization of matrix X such that X = H ∏L l=L2 W(l) where H denotes the middle-layer neurons and W(l) has nonnegative entries from gamma distributions. In this gamma NPN, parameters W(l)c , W (l) d , b (l) c , and b (l) d can be learned following Algorithm 2. We detail the algorithm as follows:\nLinear Transformation: Since gamma distributions are assumed here, we can use the function f(c, d) = ( cd , c d2 ) to compute (W (l) m ,W (l) s ) = f(W (l) c ,W (l) d ), (b (l) m ,b (l) s ) = f(b (l) c ,b (l) d ), and (o (l) c ,o (l) d ) = f −1(o (l) m ,o (l) s ) during the probabilistic linear transformation in Equation (1)-(3).\nNonlinear Transformation: With the proxy natural parameters for the gamma distributions over o(l), the mean a(l)m and variance a (l) s for the nonlinearly transformed distribution over a(l) would be obtained with Equation (4). Following Theorem 2, closed-form solutions are possible with v(x) = r(1 − exp(−τx)) (r = q and ui(x) = x) where r and τ are constants. Using this new activation function, we have (see Section B.1 and F.1 of the supplementary material for details on the function and derivation):\nam = ∫ po(o|oc,od)v(o)do = r(1−\noocd Γ(oc) ◦ Γ(oc) ◦ (od + τ)−oc) = r(1− ( od od + τ )oc),\nas = r 2(( od od + 2τ )oc − ( od od + τ )2oc).\nError: With o(L)c and o(L)d , we can compute the regression error E as the negative log-likelihood:\nE = (log Γ(o(L)c )− o(L)c ◦ log o (L) d − (o (L) c − 1) ◦ logy + o (L) d ◦ y)1 T ,\nwhere y is the observed output corresponding to x. For classification, cross-entropy loss can be used as E. Following the computation flow above, BP can be used to learn W(l)c , W (l) d , b (l) c , and b (l) d ."
    }, {
      "heading" : "3.2 Gaussian NPN",
      "text" : "Different from the gamma distribution which has support over positive values only, the Gaussian distribution, also an exponential-family distribution, can describe real-valued random variables. This makes it a natural choice for NPN. We refer to this NPN variant with Gaussian distributions over both the weights and neurons as Gaussian NPN. Details of Algorithm 2 for Gaussian NPN are as follows:\nLinear Transformation: Besides support over real values, another property of Gaussian distributions is that the mean and variance can be used as proxy natural parameters, leading to an identity mapping function f(c, d) = (c, d) which cuts the computation cost. We can use this function to compute (W (l) m ,W (l) s ) = f(W (l) c ,W (l) d ), (b (l) m ,b (l) s ) = f(b (l) c ,b (l) d ), and (o (l) c ,o (l) d ) = f −1(o (l) m ,o (l) s ) during the probabilistic linear transformation in Equation (1)-(3).\nNonlinear Transformation: If the sigmoid activation v(x) = σ(x) = 11+exp(−x) is used, am in Equation (4) would be (convolution of Gaussian with sigmoid is approximated by another sigmoid):\nam = ∫ N (o|oc, diag(od)) ◦ σ(o)do ≈ σ(\noc\n(1 + ζ2od) 1 2\n), (5)\nas = ∫ N (o|oc, diag(od)) ◦ σ(o)2do− a2m ≈ σ( α(oc + β)\n(1 + ζ2α2od)1/2 )− a2m, (6)\nwhere α = 4− 2 √ 2, β = − log( √\n2 + 1), and ζ2 = π/8. Similar approximation can be applied for activation v(x) = tanh(x) since tanh(x) = 2σ(2x)− 1. If the ReLU activation v(x) = max(0, x) is used, we can use the techniques in [6] to obtain the first two moments of max(z1, z2) where z1 and z2 are Gaussian random variables. Full derivation for v(x) = σ(x), v(x) = tanh(x), and v(x) = max(0, x) is left to the supplementary material.\nError: With o(L)c and o(L)d in the last layer, we can then compute the error E as the KL divergence KL(N (o(L)c , diag(o(L)d )) ‖N (ym, diag( ))), where is a vector with all entries equal to a small value . Hence the error E = 12 (\no (L) d\n1T + ( 1 o\n(L) d\n)(o (L) c − y)T −K + (log o(L)d )1T −K log ).\nFor classification tasks, cross-entropy loss is used. Following the computation flow above, BP can be used to learn W(l)c , W (l) d , b (l) c , and b (l) d ."
    }, {
      "heading" : "3.3 Poisson NPN",
      "text" : "The Poisson distribution, as another member of the exponential family, is often used to model counts (e.g., counts of words, topics, or super topics in documents). Hence for text modeling, it is natural to assume Poisson distributions for neurons in NPN. Interestingly, this design of Poisson NPN can be seen as a neural analogue of some Poisson factor analysis models [26].\nBesides closed-form nonlinear transformation, another challenge of Poisson NPN is to map the pair (o (l) m ,o (l) s ) to the single parameter o (l) c of Poisson distributions. According to the central limit theorem,\nwe have o(l)c = 14 (2o (l) m − 1 +\n√ (2o (l) m − 1)2 + 8o(l)s ) (see Section C and F.3 of the supplementary\nmaterial for proofs, justifications, and detailed derivation of Poisson NPN)."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section we evaluate variants of NPN and other state-of-the-art methods on four real-world datasets. We use Matlab (with GPU) to implement NPN, AE variants, and the ‘vanilla’ NN trained with dropout SGD (dropout NN). For other baselines, we use the Theano library [2] and MXNet [5]."
    }, {
      "heading" : "4.1 Toy Regression Task",
      "text" : "To gain some insights into NPN, we start with a toy 1d regression task so that the predicted mean and variance can be visualized. Following [1], we generate 20 points in one dimension from a uniform distribution in the interval [−4, 4]. The target outputs are sampled from the function y = x3 + n, where n ∼ N (0, 9). We fit the data with the Gaussian NPN, BDK, and PBP (see the supplementary material for detailed hyperparameters). Figure 1 shows the predicted mean and variance of NPN, BDK, and PBP along with the mean provided by the dropout NN (for larger versions of figures please refer to the end of the supplementary materials). As we can see, the variance of PBP, BDK, and NPN diverges as x is farther away from the training data. Both NPN’s and BDK’s predictive distributions are accurate enough to keep most of the y = x3 curve inside the shaded regions with relatively low variance. An interesting observation is that the training data points become more scattered when x > 0. Ideally, the variance should start diverging from x = 0, which is what happens in NPN. However, PBP and BDK are not sensitive enough to capture this dispersion change. In another dataset, Boston Housing, the root mean square error for PBP, BDK, and NPN is 3.01, 2.82, and 2.57."
    }, {
      "heading" : "4.2 MNIST Classification",
      "text" : "The MNIST digit dataset consists of 60,000 training images and 10,000 test images. All images are labeled as one of the 10 digits. We train the models with 50,000 images and use 10,000 images for validation. Networks with a structure of 784-800-800-10 are used for all methods, since 800 works best for the dropout NN (denoted as Dropout1 in Table 2) and BDK (BDK with a structure of 784-400-400-10 achieves an error rate of 1.41%). We also try the dropout NN with twice the number of hidden neurons (Dropout2 in Table 2) for fair comparison. For BBB, we directly quote their results from [4]. We implement BDK and NPN using the same hyperparameters as in [1] whenever possible. Gaussian priors are used for NPN (see the supplementary material for detailed hyperparameters).\nAs shown in Table 2, BDK and BBB achieve comparable performance with dropout NN (similar to [1], PBP is not included in the comparison since it supports regression only), and gamma NPN slightly outperforms dropout NN. Gaussian NPN is able to achieve a lower error rate of 1.25%. Note that BBB with Gaussian priors can only achieve an error rate of 1.82%; 1.34% is the result of using Gaussian mixture priors. For reference, the error rate for dropout NN with 1600 neurons in each hidden layer is 1.40%. The time cost per epoch is 18.3s, 16.2s, and 6.4s for NPN, BDK, NN respectively. Note that BDK is in C++ and NPN is in Matlab.\nTo evaluate NPN’s ability as a Bayesian treatment to avoid overfitting, we vary the size of the training set (from 100 to 10,000 data points) and compare the test error rates. As shown in Table 3, the margin between the Gaussian NPN and dropout NN increases as the training set shrinks. Besides, to verify the effectiveness of the estimated uncertainty, we split the test set into 9 subsets according NPN’s estimated variance (uncertainty) a(L)s 1T for each sample and show the accuracy for each subset in Figure 2. We can find that the more uncertain NPN is, the lower the accuracy, indicating that the estimated uncertainty is well calibrated."
    }, {
      "heading" : "4.3 Second-Order Representation Learning",
      "text" : "Besides classification and regression, we also consider the problem of unsupervised representation learning with a subsequent link prediction task. Three real-world datasets, Citeulike-a, Citeulike-t, and arXiv, are used. The first two datasets are from [22, 23], collected separately from CiteULike in different ways to mimic different real-world settings. The third one is from arXiv as one of the SNAP datasets [15]. Citeulike-a consists of 16,980 documents, 8,000 terms, and 44,709 links (citations). Citeulike-t consists of 25,975 documents, 20,000 terms, and 32,565 links. The last dataset, arXiv, consists of 27,770 documents, 8,000 terms, and 352,807 links.\nThe task is to perform unsupervised representation learning before feeding the extracted representations (middle-layer neurons) into a Bayesian LR algorithm [3]. We use the stacked autoencoder (SAE) [7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines (hyperparameters like weight decay and dropout rate are chosen by cross validation). As in SAE, we use different variants of NPN to form autoencoders where both the input and output targets are bag-of-words (BOW) vectors for the documents. The network structure for all models is B-100-50 (B is the number of terms). Please refer to the supplementary material for detailed hyperparameters.\nOne major advantage of NPN over SAE and SDAE is that the learned representations are distributions instead of point estimates. Since representations from NPN contain both the mean and variance, we call them secondorder representations. Note that although VAE also produces second-order representations, the variance part is simply parameterized by multilayer perceptrons while NPN’s variance is naturally computed through propagation of distributions. These 50-dimensional representations with both mean and variance are fed into a Bayesian LR algorithm for link prediction (for deterministic AE the variance is set to 0).\nWe use links among 80% of the nodes (documents) to train the Bayesian LR and use other links as the test set. link rank and AUC (area under the ROC curve) are used as evaluation metrics. The link rank is the average rank of the observed links from test nodes to training nodes. We compute the AUC for every test node and report the average values. By definition, lower link rank and higher AUC indicate better predictive performance and imply more powerful representations.\nTable 4 shows the link rank for different models. For fair comparison we also try all baselines with double budget (a structure of B-200-50) and report whichever has higher accuracy. As we can see, by treating representations as distributions rather than points in a vector space, NPN is able to achieve much lower link rank than all baselines, including VAE with variance information. The numbers in the brackets show the link rank of NPN if we discard the variance information. The performance gain from variance information verifies the effectiveness of the variance (uncertainty) estimated by NPN. Among different variants of NPN, the Gaussian NPN seems to perform better in datasets with fewer words like Citeulike-t (only 18.8 words per document). The Poisson NPN, as a more natural choice to model text, achieves the best performance in datasets with more words (Citeulike-a and arXiv). The performance in AUC is consistent with that in terms of the link rank (see Section D of the supplementary material). To further verify the effectiveness of the estimated uncertainty, we plot the reconstruction error and the variance o(L)s 1T for each data point of Citeulike-a in Figure 3. As we can see, higher uncertainty often indicates not only higher reconstruction error E but also higher variance in E."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have introduced a family of models, called natural-parameter networks, as a novel class of probabilistic NN to combine the merits of NN and PGM. NPN regards the weights and neurons as arbitrary exponential-family distributions rather than just point estimates or factorized Gaussian distributions. Such flexibility enables richer descriptions of hierarchical relationships among latent variables and adds another degree of freedom to customize NN for different types of data. Efficient sampling-free backpropagation-compatible algorithms are designed for the learning of NPN. Experiments show that\nNPN achieves state-of-the-art performance on classification, regression, and representation learning tasks. As possible extensions of NPN, it would be interesting to connect NPN to arbitrary PGM to form fully Bayesian deep learning models [24, 25], allowing even richer descriptions of relationships among latent variables. It is also worth noting that NPN cannot be defined as generative models and, unlike PGM, the same NPN model cannot be used to support multiple types of inference (with different observed and hidden variables). We will try to address these limitations in our future work."
    }, {
      "heading" : "A Proof of Theorem 2",
      "text" : "Theorem 2. Assume an exponential-family distribution po(x|η) = h(x)g(η) exp{ηTu(x)}, where the vector u(x) = (u1(x), u2(x), . . . , uM (x))T (M is the number of natural parameters). If activation function v(x) = r − q exp(−τui(x)) is used, the first two moments of v(x), ∫ po(x|η)v(x)dx\nand ∫ po(x|η)v(x)2dx, can be expressed in closed form. Here i ∈ {1, 2, . . . ,M} and r, q, and τ are constants.\nProof. We first let η = (η1, η2, . . . , ηM ), η̃ = (η1, η2, . . . , ηi − τ, . . . , ηM ), and η̂ = (η1, η2, . . . , ηi − 2τ, . . . , ηM ). The first moment of v(x) is\nE(v(x)) = ∫ po(x|η)(r − q exp(−τui(x)))dx\n= r − q ∫ h(x)g(η) exp{ηTu(x)− τui(x)}dx\n= r − q ∫ h(x) g(η)\ng(η̃) g(η̃) exp{η̃Tu(x)}dx\n= r − q g(η) g(η̃)\n∫ h(x)g(η̃) exp{η̃Tu(x)}dx\n= r − q g(η) g(η̃) .\nSimilarly the second moment\nE(v(x)2) = ∫ po(x|η)(r − q exp(−τui(x)))2dx\n= ∫ po(x|η)(r2 + q2 exp(−2τui(x))− 2rq exp(−τui(x)))dx\n= r2 ∫ po(x|η)dx+ q2 ∫ h(x)g(η) exp{ηTu(x)− 2τui(x)}dx\n− 2rq ∫ h(x)g(η) exp{ηTu(x)− τui(x)}dx\n= r2 + q2 ∫ h(x)g(η) exp{η̂Tu(x)}dx− 2rq ∫ h(x)g(η) exp{η̃Tu(x)}dx\n= r2 + q2 ∫ h(x) g(η)\ng(η̂) g(η̂) exp{η̃Tu(x)}dx− 2rq\n∫ h(x) g(η)\ng(η̃) g(η̃) exp{η̃Tu(x)}dx\n= r2 + q2 g(η)\ng(η̂)\n∫ h(x)g(η̂) exp{η̃Tu(x)}dx− 2rq g(η)\ng(η̃)\n∫ h(x)g(η̃) exp{η̃Tu(x)}dx\n= r2 + q2 g(η) g(η̂) − 2rq g(η) g(η̃) ."
    }, {
      "heading" : "B Exponential-Family Distributions and Activation Functions",
      "text" : "In this section we provide a list of exponential-family distributions with corresponding activation functions that could lead to close-form expressions of the first two moments of v(x), namely\nE(v(x)) and E(v(x)2). With Theorem 2, we only need to find the constants (r, q, and τ ) that make v(x) = r − q exp(−τui(x)) monotonically increasing and bounded. As mentioned in the paper, we use the activation function v(x) = r(1− exp(−τx)) for the gamma NPN and the Poisson NPN. Figure 4(left) plots this function with different τ when r = 1. As we can see, this function has a similar shape with the positive half of v(x) = tanh(x) (the negative part is irrelevant because both the gamma distribution and the Poisson distribution have support over positive values only). Note that the activation function v(x) = 1 − exp(−1.5x) is very similar to v(x) = tanh(x).\nFor beta distributions, since the support set is (0, 1) the domain of the activation function is also (0, 1). In this case v(x) = qxτ is a reasonable activation function when τ ∈ (0, 1) and q = 1. Figure 4(middle) shows this function with differnt τ when q = 1. Since we expect the nonlinearly transformed distribution to be another beta distribution, the domain of the function should be (0, 1) and the field should be [0, 1]. With these criteria, v(x) = 1.3 tanh(x) might be a better activation function than v(x) = tanh(x). As shown in the figure, different τ leads to different shapes of the function.\nFor Rayleigh distributions with support over positive reals, v(x) = r − qe−τx2 is a proper activation function with the domain x ∈ R+. Figure 4(right) plots this function with different τ when r = q = 1. We can see that this function also has a similar shape with the positive half of v(x) = tanh(x)."
    }, {
      "heading" : "B.1 Gamma Distributions",
      "text" : "For gamma distributions with (v(x) = r(1− exp(−τx)), as mentioned in the paper,\nam = ∫ po(oj |oc,od)v(o)do = r ∫ +∞ 0 1 Γ(oc) oocd ◦ o oc−1e−od◦o(1− e−τo)do\n= r(1− oocd\nΓ(oc) ∫ +∞ 0 ooc−1e−(od+τ)◦odo)\n= r(1− oocd\nΓ(oc) ◦ Γ(oc) ◦ (od + τ)−oc)\n= r(1− ( od od + τ )oc).\nSimilarly we have\nas = ∫ po(oj |oc,od)v(o)2do− a2m\n= r2 ∫ +∞\n0\n1\nΓ(oc) oocd ◦ o oc−1e−od◦o(1− 2e−τo + e−2τo)do− a2m\n= r2(1− 2 oocd\nΓ(oc) ◦ Γ(oc) ◦ (od + τ)−oc + oocd Γ(oc) ◦ Γ(oc) ◦ (od + 2τ)−oc)− a2m\n= r2(1− 2( od od + τ )oc + ( od od + 2τ )oc)− a2m = r2(( od\nod + 2τ )oc − ( od od + τ )2oc).\nEquivalently we can obtain the same am and as by following Theorem 2. For the gamma distribution\np(x|c, d) = d c\nΓ(c) exp{(c− 1) log x+ (−b)x}.\nThus we have η = (c − 1,−d)T , u(x) = (log x, x)T , and g(η) = d c\nΓ(c) . Using v(x) = r(1 − exp(−τx)) implies g(η̃) = (d+τ) c\nΓ(c) and g(η̂) = (d+2τ)c Γ(c) . Hence we have\nam = r − r g(η) g(η̃) = r(1− ( od od + τ )oc),\nand the variance\nas = r 2 + r2\ng(η) g(η̂) − 2r2 g(η) g(η̃) − r2(1− g(η) g(η̃) )2\n= r2(( od od + 2τ )oc − ( od od + τ )2oc)."
    }, {
      "heading" : "B.2 Poisson Distributions",
      "text" : "For Poisson distributions with v(x) = r(1 − exp(−τx)), using the Taylor expansion of exp(exp(−τ)λ) with respect to λ,\nexp(exp(−τ)λ) = +∞∑ x=0 λx exp(−τx) x! ,\nwe have\nam = r +∞∑ x=0 oxc exp(−oc) x! (1− exp(−τx))\n= r( +∞∑ x=0 oxc exp(−oc) x! − +∞∑ x=0 oxc exp(−oc) x! exp(−τx))\n= r(1− exp(−oc) +∞∑ x=0 oxc exp(−τx) x! )\n= r(1− exp(−oc) exp(exp(−τ)oc)) = r(1− exp((exp(−τ)− 1)oc)).\nSimilarly, we have\nas = r 2 +∞∑ x=0 oxc exp(−oc) x! (1− exp(−τx))2 − a2m\n= r2 +∞∑ x=0 oxc exp(−oc) x! (1− 2 exp(−τx) + exp(−2τx))− a2m\n= r2( +∞∑ x=0 oxc exp(−oc) x! − 2 +∞∑ x=0 oxc exp(−oc) x! exp(−τx) + +∞∑ x=0 oxc exp(−oc) x! exp(−2τx))− a2m\n= r2(exp((exp(−2τ)− 1)oc)− exp(2(exp(−τ)− 1)oc). Equivalently we can follow Theorem 2 to obtain am and as. For the Poisson distribution\np(x|c) = 1 x! exp(−c) exp{x log c}\nThus we have η = log c, u(x) = x, and g(η) = exp(−c). Using v(x) = r(1− exp(−τx)) implies g(η̃) = exp(− exp(−τ)c) and g(η̂) = exp(− exp(−2τ)c). Hence we have\nam = r − r g(η)\ng(η̃)\n= r(1− exp((exp(−τ)− 1)oc)),\nand the variance\nas = r 2 + r2\ng(η) g(η̂) − 2r2 g(η) g(η̃) − r2(1− g(η) g(η̃) )2\n= r2(exp((exp(−2τ)− 1)oc)− exp(2(exp(−τ)− 1)oc))."
    }, {
      "heading" : "B.3 Gaussian Distributions",
      "text" : "In this subsection, we provide detailed derivation of (am,as) for Gaussian distributions."
    }, {
      "heading" : "B.3.1 Sigmoid Activation",
      "text" : "We start by proving the following theorem: Theorem 3. Consider a univariate Gaussian distributionN (x|µ, σ2) and the probit function Φ(x) =∫ x −∞N (θ|0, 1)dθ. If ζ\n2 = π8 , for any constants a and b, the following equation holds:∫ Φ(ζa(x+ b)N (x|µ, σ2)dx = Φ( ζa(µ+ b)\n(1 + ζ2a2σ2) 1 2\n). (7)\nProof. Making the change of variable x = µ+ σz, we have I = ∫ Φ(ζa(x+ b)N (x|µ, σ2)dx\n= ∫ Φ(ζa(µ+ σz + b)) 1\n(2πσ) 1 2 exp{−1 2 z2}σdz.\nTaking the derivative with respect to µ, ∂I ∂µ = ζa 2π ∫ exp{−1 2 z2 − 1 2 ζ2a2(µ+ σz + b)2}dz\n= ζa\n2π\n∫ exp{−1\n2 z2 − 1 2 ζ2a2(µ2 + σ2z2 + b2 + 2µσz + 2µb+ 2σzb)}dz\n= ζa\n2π\n∫ exp{−1\n2 (1 + ζ2a2σ2)(z2 +\n2ζ2a2σ(µ+ b)\n1 + ζ2a2σ2 z +\n(µ2 + b2 + 2µb)ζ2a2\n1 + ζ2a2σ2 )}dz\n= ζa\n2π\n∫ exp{−1\n2 (1 + ζ2a2σ2)((z +\nζ2a2σ(µ+ b)\n1 + ζ2a2σ2 )2 − ζ\n4a4σ2(µ+ b)2\n(1 + ζ2a2σ2)2 +\n(µ+ b)2ζ2a2\n1 + ζ2a2σ2 )}dz\n= ζa\n2π\n∫ exp{−1\n2 (1 + ζ2a2σ2)((z +\nζ2a2σ(µ+ b)\n1 + ζ2a2σ2 )2 +\n1\n2\nζ4a4σ2(µ+ b)2\n1 + ζ2a2σ2 − 1 2 (µ+ b)2ζ2a2)}dz\n= ζa\n2π\n∫ exp{−1\n2 (1 + ζ2a2σ2)((z +\nζ2a2σ(µ+ b)\n1 + ζ2a2σ2 )2 − 1 2\n(µ+ b)2ζ2a2\n1 + ζ2a2σ2 )}dz\n= 1\n(2π) 1 2\nζa\n(1 + ζ2a2σ2) 1 2 exp{−1 2\n(µ+ b)2ζ2a2\n1 + ζ2a2σ2 }.\nTaking derivative of the right-hand side of Equation (7) also gives 1\n(2π) 1 2\nζa\n(1 + ζ2a2σ2) 1 2 exp{−1 2\n(µ+ b)2ζ2a2\n1 + ζ2a2σ2 },\nwhich means the derivatives of the left and right hand sides of Equation (7) with respect to µ are equal. When µ approaches negative infinity, the derivatives go to zero, which implies that the constant of the integration is zero. Hence Equation (7) holds.\nAs mentioned in the paper (with a slight abuse of notation on σ), if the sigmoid activation v(x) = σ(x) = 11+exp(−x) is used,\nam = ∫ N (o|oc, diag(od)) ◦\n1\n1 + exp(−o) do ≈ ∫ N (o|oc, diag(od)) ◦ Φ(ζo)do. (8)\nFollowing Theorem 3 with a = 1 and b = 0, we have\nam ≈ Φ( oc\n(ζ−2 + od) 1 2\n)\n= σ( oc\n(1 + ζ2od) 1 2\n).\nFor the variance,\nas ≈ ∫ N (o|oc, diag(od)) ◦ Φ(ζα(o + β))do− a2m\n= σ( α(om + β)\n(1 + ζ2α2os)1/2 )− a2m. (9)\nEquation (9) holds due to Theorem 3 with a = α = 4− 2 √ 2 and b = β = − log( √ 2 + 1)."
    }, {
      "heading" : "B.3.2 Hyperbolic Tangent Activation",
      "text" : "If the tanh activation v(x) = tanh(x) is used, since tanh(x) = 2σ(2x)− 1, we have\nam = ∫ N (o|oc, diag(od)) ◦ (2σ(2o)− 1)do\n= 2 ∫ N (o|oc, diag(od)) ◦ σ(2o)do− 1\n≈ 2 ∫ N (o|oc, diag(od)) ◦ Φ(2ζo)do− 1\n= 2Φ( 2ζoc\n(1 + 4ζ2od) 1 2\n)− 1 (10)\n= 2σ( oc\n(0.25 + ζ2od) 1 2\n)− 1,\nwhere Equation (10) is due to Theorem 3 with a = 2 and b = 0. For the variance of a:\nas = ∫ N (o|oc, diag(od)) ◦ (2σ(2o)− 1)2do− a2m\n= ∫ N (o|oc, diag(od)) ◦ (4σ(2o)2 − 4σ(2o) + 1)do− a2m\n≈ ∫ N (o|oc, diag(od)) ◦ (4Φ(ζα(o + β))− 4σ(2o) + 1)do− a2m\n= 4σ( α(oc + β)\n(1 + ζ2α2od) 1 2\n)− a2m − 2am − 1, (11)\nwhere Equation (11) follows from Theorem 3 with a = α = 8−4 √ 2 and b = β = −0.5 log( √ 2+1)."
    }, {
      "heading" : "B.3.3 ReLU Activation",
      "text" : "If the ReLU activation v(x) = max(0, x) is used, we can use the techniques in [6] to obtain the first two moments of z = max(z1, z2) where z1 ∼ N (µ1, σ21) and z2 ∼ N (µ2, σ22). Specifically,\nE(z) = µ1Φ(γ) + µ2Φ(−γ) + νφ(γ) E(z2) = (µ21 + σ 2 1)Φ(γ) + (µ 2 2 + σ 2 2)Φ(−γ) + (µ1 + µ2)νφ(γ),\nwhere Φ(x) = ∫ x −∞N (θ|0, 1)dθ, φ(x) = N (x|0, 1), ν = √ σ21 + σ 2 2 , and γ = µ1−µ2 ν . If N (µ1, σ21) = N (c, d) and N (µ2, σ22) = N (0, 0), we recover the probabilistic version of ReLU. In this case,\nE(z) = Φ( c√ d )c+\n√ d\n2π exp{−1 2\nc2 d }\nD(z) = E(z2)− E(z)2 = Φ( c√ d\n)(c2 + d) + c √ d√\n2π exp{−1 2\nc2 d } − c2.\nHence we have the following equations as in the main text:\na(l)m = Φ(om ◦ o(l)s − 12 ) ◦ om + √ os√ 2π ◦ exp(−om 2 2os )\na(l)s = Φ(om ◦ o(l)s − 12 ) ◦ (om2 + os) + o (l) m ◦ √ os√\n2π ◦ exp(−om\n2\n2os )− a2m."
    }, {
      "heading" : "C Mapping Function for Poisson Distributions",
      "text" : "Since the mapping function involves Gaussian approximation to a Poisson distribution, we start with proving the connection between Gaussian distributions and Poisson distributions. Lemma 1. Assume Y is a Poisson random variable with mean c and variance c. If X1, X2, . . . , Xc are independent Poisson random variables with mean 1, we have:\nY = c∑ i=1 Xi\nProof. We can use the concept of moment generating functions (i.e., two distributions are identical if they have exactly the same moment generating function), which is defined as M(t) = E(exp(tZ)) for a random variable Z, to prove the lemma. The moment generating function for a Poisson random variable with mean c and variance c is:\nM1(t) = exp(c(exp(t)− 1)).\nOn the other hand, the moment generating function for c∑ i=1 Xi is:\nM2(t) = E(exp(t c∑ i=1 Xi))\n= E( c∏ i=1 exp(tXi))\n= c∏ i=1 E(exp(tXi)) (12)\n= c∏ i=1 exp(exp(t)− 1) (13)\n= exp(c(exp(t)− 1)) = M1(t),\nwhere Equation (12) is due to the fact that X1, X2, . . . , Xc are independent. Equation (13) is the result of using the moment generating functions of Poisson distributions. Since c∑ i=1 Xi has exactly the same moment generating function as a Poisson random variable with mean c and variance c, by definition of Y , we have:\nY = c∑ i=1 Xi\nTheorem 4. A Poisson distribution with mean c and variance c can be approximated by a Gaussian distribution N (c, c) if c is sufficiently large.\nProof. We first use Y to denote the random variable corresponding to the Poisson distribution with mean c and variance c. According to Lemma 1, we have Y = c∑ i=1 Xi where X1, X2, . . . , Xc are\nindependent Poisson random variables with mean 1. Hence,\nY − c√ c =\nc∑ i=1\nXi − c √ c\n= √ c( 1\nc c∑ i=1 Xi − 1),\nwhere 1c c∑ i=1 Xi is the sample mean. By the central limit theorem, we know that if c is sufficiently large, √ c( 1c c∑ i=1 Xi − 1) can be approximated by the Gaussian distribution N (0, 1). Thus Y can be approximated by the Gaussian distribution N (c, c).\nNote that although c is a nonnegative integer above, the proof can be easily generalized to the case in which c is a nonnegative real value.\nDuring the feedforward computation of the Poisson NPN, after obtaining the mean o(l)m and variance o (l) s of the linearly transformed distribution over o(l), we map them back to the proxy natural parameters o(l)c . Unfortunately the mean and variance of a Poisson are the same, which is obviously not the case for o(l)m and o (l) s . Here we propose to find o (l) c by minimizing the KL divergence of the factorized Poisson distribution p(o(l)|o(l)c ) and the Gaussian distribution N (o(l)m , diag(o(l)s ))3. Since the direct KL divergence involves the computation of an infinite series in the entropy term of the Poisson distribution, closed-form solutions are not available. To address the problem, we use a Gaussian distribution N (o(l)c , diag(o(l)c )) as a proxy of the Poisson distribution with the mean o(l)c (which is justified by Theorem 4)4. Specifically, we aim to find a Gaussian distribution N (o(l)c , diag(o(l)c )) to best approximateN (o(l)m , diag(o(l)s )) and directly use o(l)c in the new Gaussian as the result of mapping.\nFor simplicity, we consider the univariate case where we aim to find a Gaussian distribution N (c, c) to approximate N (m, s). The KL divergence between N (c, c) and N (m, s)\nDKL(N (c, c)‖N (m, s)) = 1 2 ( c s +\n(c−m)2\ns − 1 + log s− log c),\nwhich is convex with respect to c > 0. We set the gradient of DKL(N (c, c)‖N (m, s)) with respect to c as 0 and solve for c, giving\nc = 2m− 1± √ (2m− 1)2 + 8s 4 .\nSince in Poisson distributions, c is always positive, there is only one solution for c:\nc = 2m− 1 + √ (2m− 1)2 + 8s 4 .\nThus the mapping is\no(l)c = 1\n4 (2o(l)m − 1 +\n√ (2o (l) m − 1)2 + 8o(l)s ).\n3The relationships between Poisson distributions and Gaussian distributions are described in Theorem 4. The theorem, however, cannot be directly used here since o(l)m and o (l) s are not identical. This is why we have to resort to the KL divergence. 4Note that for Theorem 4 to be valid, c has to be sufficiently large, which is why we do not normalize the word counts as preprocessing and why we use a large r for the activation v(x) = r(1− exp(−τx))."
    }, {
      "heading" : "D AUC for Link Prediction and Different Data Splitting",
      "text" : "In this section, we show the AUC for different models on the link prediction task. As we can see in Table 5 above, the result in AUC is consistent with that in link rank (as shown in Table 3 of the paper). NPN is able to achieve much higher AUC than SAE, SDAE, and VAE. Among different variants of NPN, the Gaussian NPN seems to perform better in datasets with fewer words like Citeulike-t (18.8 words per document). The Poisson NPN, as a more natural choice to model text, achieves the best performance in datasets with more words (Citeulike-a with 66.6 words per document and arXiv with 88.8 words per document).\nFor the link prediction task, we also try to split the data in a different way and compare the performance of different models. Specifically, we randomly select 80% of the observed links (rather than nodes) as the training set and use the others as the test set. The results are consistent with those for the original data-splitting method."
    }, {
      "heading" : "E Hyperparameters and Preprocessing",
      "text" : "In this section we provide details on the hyperparameters and preprocessing of the experiments as mentioned in the paper."
    }, {
      "heading" : "E.1 Toy Regression Task",
      "text" : "For the toy 1d regression task, we use networks with one hidden layer containing 100 neurons and ReLU activation, as in [1, 10]. For the Gaussian NPN, we use the KL divergence loss and isotropic Gaussian priors with precision 10−4 for the weights (and biases). The same priors are used in other experiments."
    }, {
      "heading" : "E.2 MNIST Classification",
      "text" : "For preprocessing, following [4, 1], pixel values are normalized to the range [0, 1]. For the NPN variants, we use these hyperparameters: minibatch size 128, number of epochs 2000 (the same as BDK). For the learning rate, AdaDelta is used. Note that since NPN is dropout-compatible, we can use dropout (with nearly no additional cost) for effective regularization. The training and testing of dropout NPN are similar to those of the vanilla dropout NN."
    }, {
      "heading" : "E.3 Second-Order Representation Learning",
      "text" : "For all models, we preprocess the BOW vectors by normalizing them into the range [0, 1]. Although theoretically Poisson NPN does not need any preprocessing since Poisson distributions naturally model word counts, in practice, we find normalizing the BOW vectors will increase both stability during training and the predictive performance. For simplicity, in the Poisson NPN, r is set to 1 and τ = 0.1 (these two hyperparameters can be tuned to further improve performance). For the Gaussian NPN, sigmoid activation is used. The other hyperparameters of NPN are the same as in the MNIST experiments."
    }, {
      "heading" : "F Details on Variants of NPN",
      "text" : ""
    }, {
      "heading" : "F.1 Gamma NPN",
      "text" : "In gamma NPN, parameters W(l)c , W (l) d , b (l) c , and b (l) d can be learned following Algorithm 2. Specifically, during the feedforward phase, we will compute the error E given the input a(0)m = x\nAlgorithm 2 Deep Nonlinear NPN\n1: Input: The data {(xi,yi)}Ni=1, number of iterations T , learning rate ρt, number of layers L. 2: for t = 1 : T do 3: for l = 1 : L do 4: Compute (o(l)m ,o (l) s ) from (a (l−1) m ,a (l−1) s ). (o (l) c ,o (l) d ) = f −1(o (l) m ,o (l) s ). 5: Compute (a(l)m ,a (l) s ) from (o (l) c ,o (l) d ). 6: end for 7: Compute the error E. 8: for l = L : 1 do 9: Compute ∂E\n∂W (l) m , ∂E ∂W (l) s , ∂E ∂b (l) m , and ∂E ∂b (l) s . Compute ∂E ∂W (l) c , ∂E ∂W (l) d , ∂E ∂b (l) c , and ∂E ∂b (l) d .\n10: end for 11: Update W(l)c , W (l) d , b (l) c , and b (l) d in all layers. 12: end for\n(a(0)s = 0) and the parameters (W (l) c , W (l) d , b (l) c , and b (l) d ). With the mean a (l−1) m and variance a (l−1) s from the previous layer, o (l) m and o (l) s can be computed according to equations in Section 2.2 of the paper, where\n(W(l)m ,W (l) s ) = (W (l) c ◦W (l) d −1 ,W(l)c ◦W (l) d −2 ), (b(l)m ,b (l) s ) = (b (l) c ◦ b (l) d −1 ,b(l)c ◦ b (l) d −2 ). (14)\nAfter that we can get the proxy natural parameters using (o(l)c ,o (l) d ) = (o (l) m ◦ o(l)s\n−1 ,o (l) m 2 ◦ o(l)s ).\nWith the proxy natural parameters for the gamma distributions over o(l), the mean a(l)m and variance a (l) s for the nonlinearly transformed distribution over a(l) would be obtained. As mentioned before, using traditional activation functions like tanh v(x) = tanh(x) and ReLU v(x) = max(0, x) could not give us closed-form solutions for the integrals. Following Theorem 2, closed-form solutions are possible with v(x) = r(1 − exp(−τx)) (r = q and ui(x) = x) where r and τ are constants. This function has a similar shape with the positive half of v(x) = tanh(x) with r as the saturation point and τ controlling the slope.\nWith the computation procedure for the feedforward phase, the gradients of E with respect to parameters W(l)c , W (l) d , b (l) c , and b (l) d can be derived and used for backpropagation. Note that to ensure positive entries in the parameters we can use the function k(x) = log(1 + exp(x)) or k(x) = exp(x− h). For example, we can let b(l)c = log(1 + exp(b(l)c′ )) and treat b (l) c′ as parameters to learn instead of b(l)c .\nWe can add the KL divergence between the learned distribution and the prior distribution on weights to the objective function to regularize gamma NPN. If we use an isotropic Gaussian prior N (0, λ−1s ) for each entry of the weights, we can compute the KL divergence for each entry (between Gam(c, d) and N (0, λ−1s )) as:\nKL(Gam(x|c, d)‖N (x|0, λ−1s ))\n= ∫ Gam(x|c, d) logGam(x|c, d)dx− ∫ Gam(x|c, d) logN (x|0, λ−1s )\n= − log Γ(c) + (c− 1)ψ(c) + log d− c+ 1 2 log(2π)− 1 2 log λs + 1 2 λs\n∫ dc\nΓ(c) xc+2−1 exp(−dx)dx\n= − log Γ(c) + (c− 1)ψ(c) + log d− c+ 1 2 log(2π)− 1 2 log λs + 1 2 λs Γ(c+ 2) Γ(c)\n= − log Γ(c) + (c− 1)ψ(c) + log d− c+ 1 2 log(2π)− 1 2 log λs + 1 2 λsc(c+ 1), (15)\nwhere ψ(x) = d dx log Γ(x) is the digamma function."
    }, {
      "heading" : "F.2 Gaussian NPN",
      "text" : "For details on the Bayesian nonlinear transformation, please refer to Section B.3 above. For the KL divergence between the learned distribution and the prior distribution on weights, we can compute it\nas:\nKL(N (x|c, d)‖N (x|0, λ−1s )) = 1\n2 (λs + λsc\n2 − 1− log λs − log d), (16)\nAs we can see, the term − 12 log d will help to prevent the learned variance d from collapsing to 0 (in practice we can use 12λd(d− h)\n2, where λd and h are hyperparameters, to approximate this term for better numerical stability) and the term 12c\n2 is equivalent to L2 regularization. Similar to BDK, we can use a mixture of Gaussians as the prior distribution."
    }, {
      "heading" : "F.3 Poisson NPN",
      "text" : "The Poisson distribution, as another member of the exponential family, is often used to model counts (e.g., number of events happened or number of words in a document). Different from the previous distributions, it has support over nonnegative integers. The Poisson distribution takes the form p(x|c) = c\nx exp(−c) x! with one single natural parameter log c (we use c as the proxy natural parameter).\nIt is this single natural parameter that makes the learning of a Poisson NPN trickier. For text modeling, assuming Poisson distributions for neurons is natural because they can model the counts of words and topics (even super topics) in documents. Here we assume a factorized Poisson distribution p(o(l)|o(l)c ) = ∏ j p(o (l) j |o (l) c,j) and do the same for a\n(l). To ensure having positive natural parameters we use gamma distributions for the weights. Interestingly, this design of Poisson NPN can be seen as a neural analogue of some Poisson factor analysis models [26].\nFollowing Algorithm 2, we need to compute E during the feedforward phase given the input a(0)m = x (a(0)s = 0) and the parameters (W (l) c , W (l) d , b (l) c , and b (l) d ), the first step being to compute the mean o (l) m and variance o (l) s . Since gamma distributions are assumed for the weights, we can compute the mean and variance of the weights as follows:\n(W(l)m ,W (l) s ) = (W (l) c ◦W (l) d −1 ,W(l)c ◦W (l) d −2 ), (b(l)m ,b (l) s ) = (b (l) c ◦ b (l) d −1 ,b(l)c ◦ b (l) d −2 ). (17)\nHaving computed the mean o(l)m and variance o (l) s of the linearly transformed distribution over o(l), we map them back to the proxy natural parameters o(l)c . Unfortunately the mean and variance of a Poisson are the same, which is obviously not the case for o(l)m and o (l) s . Hence we propose to find o (l) c by minimizing the KL divergence of the factorized Poisson distribution p(o(l)|o(l)c ) and the Gaussian distribution N (o(l)m , diag(o(l)s )), resulting in the mapping (see Section C for proofs and justifications):\no(l)c = 1\n4 (2o(l)m − 1 +\n√ (2o (l) m − 1)2 + 8o(l)s ). (18)\nAfter finding o(l)c , the next step in Algorithm 2 is to get the mean a (l) m and variance a (l) s of the nonlinearly transformed distribution. As is the case for gamma NPN, traditional activation functions will not give us closed-form solutions. Fortunately, the activation v(x) = r(1 − exp(−τx)) also works for Poisson NPN. Specifically,\nam = r +∞∑ x=0 oxc exp(−oc) x! (1− exp(−τx)) = r(1− exp((exp(−τ)− 1)oc)),\nwhere the superscript (l) is dropped. Similarly, we have\nas = r 2(exp((exp(−2τ)− 1)oc)− exp(2(exp(−τ)− 1)oc)).\nFull derivation is provided in Section B.2.\nOnce we go through L layers to get the proxy natural parameters o(L)c for the distribution over o(L), the error E can be computed as the negative log-likelihood. Assuming that the target output y has nonnegative integers as entries,\nE = −1T (y ◦ log o(L)c − o(L)c − log(y!)).\nFor y with real-valued entries, the L2 loss could be used as the error E. Note that if we use the normalized BOW as the target output, the same error E can be used as the Gaussian NPN. Besides this loss term, we can add the KL divergence term in Equation (15) to regularize Poisson NPN.\nDuring backpropagation, the gradients are computed to update the parameters W(l)c , W (l) d , b (l) c , and b(l)d . Interestingly, since o (l) c is guaranteed to be nonnegative, the model still works even if we directly use W(l)m and W (l) s as parameters, though the resulting models are not exactly the same. In the experiments, we use this Poisson NPN for a Bayesian autoencoder and feed the extracted second-order representations into a Bayesian LR algorithm for link prediction."
    }, {
      "heading" : "G Derivation of Gradients",
      "text" : "In this section we list the gradients used in backpropagation to update the NPN parameters."
    }, {
      "heading" : "G.1 Gamma NPN",
      "text" : "In the following we assume an activation function of v(x) = r(1 − exp(−τx)) and use ψ(x) = d dx log Γ(x) to denote the digamma function. E is the error we want to minimize.\nE → o(L):\n∂E\n∂o (L) c\n= ψ(o(L)c )− log o (L) d − logy\n∂E\n∂o (L) d\n= −o (L) c\no (L) d\n+ y.\no(l) → a(l−1):\n∂E\n∂a (l−1) m\n= ∂E\n∂o (l) m\nW(l)m T + ( ∂E\n∂o (l) s\nW(l)s T ) ◦ 2a(l−1)m\n∂E\n∂a (l−1) s\n= ∂E\n∂o (l) s\nW(l)s T + ∂E\n∂o (l) s\n(W(l)m ◦W(l)m ) T\na(l) → o(l):\n∂E\n∂o (l) c\n= ∂E\n∂a (l) m\n◦ (−r( o\n(l) d\no (l) d + τ\n)o (l) c ◦ log(\no (l) d\no (l) d + τ\n))\n+ r2 ∂E\n∂a (l) s\n(( o\n(l) d\no (l) d + 2τ\n)o (l) c ◦ log(\no (l) d\no (l) d + 2τ\n)− 2( o\n(l) d\no (l) d + 2τ\n)2o (l) c ◦ log(\no (l) d\no (l) d + 2τ\n))\n∂E\n∂o (l) c\n= ∂E\n∂a (l) m\n◦ (−ro(l)c ◦ ( o\n(l) d\no (l) d + τ\n)o (l) c −1 ◦ τ\n(o (l) d + τ)\n2 )\n+ r2 ∂E\n∂a (l) s\n◦ (o(l)c ◦ ( o\n(l) d\no (l) d + 2τ\n)o (l) c −1 ◦ 2τ\n(o (l) d + 2τ) 2\n− 2o(l)c ◦ ( o\n(l) d\no (l) d + τ\n)2o (l) c −1 ◦ τ\n(o (l) d + τ)\n2 ).\no(l) →W(l),o(l) → b(l):\nThe gradients with respect to the mean-variance pairs:\n∂E\n∂W (l) m\n= a(l−1)m T ∂E\n∂o (l) m\n+ (a(l−1)s T ∂E\n∂o (l) s\n) ◦ 2W(l)m\n∂E\n∂W (l) s\n= a(l−1)s T ∂E\n∂o (l) s\n+ (a(l−1)m ◦ a(l−1)m )T ∂E\n∂o (l) s\n∂E\n∂b (l) m\n= ∂E\n∂o (l) m\n∂E\n∂b (l) s\n= ∂E\n∂o (l) s\nThe gradients with respect to the proxy natural parameters:\n∂E\n∂W (l) c\n= ∂E\n∂W (l) m ◦ 1 W (l) d + ∂E ∂W (l) s ◦ 1 W (l) d 2\n∂E\n∂W (l) d\n= − ∂E ∂W (l) m\n◦ W (l) c\nW (l) d\n2 − 2 ∂E\n∂W (l) s\n◦ W (l) c\nW (l) d\n3\n∂E\n∂b (l) c\n= ∂E\n∂b (l) m ◦ 1 b (l) d + ∂E ∂b (l) s ◦ 1 b (l) d 2\n∂E\n∂b (l) d\n= − ∂E ∂b (l) m\n◦ b (l) c\nb (l) d\n2 − 2 ∂E\n∂b (l) s\n◦ b (l) c\nb (l) d\n3"
    }, {
      "heading" : "G.2 Gaussian NPN",
      "text" : "In the following we assume the sigmoid activation function and use cross-entropy loss. Other activation functions and loss could be derived similarly. For the equations below, α = 4 − 2 √ 2, β = − log( √\n2 + 1), ζ2 = π8 , and κ(x) = (1 + ζ 2x)− 1 2 .\nE → o(L):\n∂E\n∂o (L) m\n= (σ(κ(o(L)s ) ◦ o(L)m )− y) ◦ κ(o(L)s )\n∂E\n∂o (L) s\n= (σ(κ(o(L)s ) ◦ o(L)m )− y) ◦ o(L)m ◦ (− π\n16 (1 + πo(L)s /8) −3/2).\no(l) → a(l−1):\n∂E\n∂a (l−1) m\n= ∂E\n∂o (l) m\nW(l)m T + ( ∂E\n∂o (l) s\nW(l)s T ) ◦ 2a(l−1)m\n∂E\n∂a (l−1) s\n= ∂E\n∂o (l) s\nW(l)s T + ∂E\n∂o (l) s\n(W(l)m ◦W(l)m ) T .\na(l) → o(l): ∂E\n∂o (l) m\n= ∂E\n∂a (l) m\n◦ dsigmoid(κ(o(l)s ) ◦ o(l)m ) ◦ κ(o(l)s )\n+ α ∂E\n∂a (l) s\n◦ dsigmoid( α(o (l) m + β)\n(1 + ζ2α2o (l) s )1/2\n) ◦ (1 + ζ2α2o(l)s )−1/2\n− 2a(l)m ◦ ∂E\n∂a (l) s\n◦ dsigmoid(κ(o(l)s ) ◦ o(l)m ) ◦ κ(o(l)s )\n∂E\n∂o (l) s\n= ∂E\n∂a (l) m\n◦ dsigmoid(κ(o(l)s ) ◦ o(l)m ) ◦ o(l)m ◦ (− 1\n2 ζ2(1 + ζ2o(l)s ) −3/2)\n+ ∂E\n∂a (l) s\n◦ dsigmoid( α(o (l) m + β)\n(1 + ζ2α2o (l) s )1/2\n) ◦ (α(o(l)m + β)) ◦ (− 1\n2 ζ2α2(1 + ζ2α2o(l)s ) −3/2)\n− 2a(l)m ◦ ∂E\n∂a (l) s\n◦ dsigmoid(κ(o(l)s ) ◦ o(l)m ) ◦ o(l)m ◦ (− 1\n2 ζ2(1 + ζ2o(l)s ) −3/2),\nwhere dsigmoid(x) is the gradient of σ(x).\no(l) →W(l),o(l) → b(l):\n∂E\n∂W (l) c\n= a(l−1)m T ∂E\n∂o (l) m\n+ (a(l−1)s T ∂E\n∂o (l) s\n) ◦ 2W(l)c\n∂E\n∂W (l) d\n= a(l−1)s T ∂E\n∂o (l) s\n+ (a(l−1)m ◦ a(l−1)m )T ∂E\n∂o (l) s\n∂E\n∂b (l) c\n= ∂E\n∂o (l) m\n∂E\n∂b (l) d\n= ∂E\n∂o (l) s\nNote that we directly use the mean and variance as proxy natural parameters here."
    }, {
      "heading" : "G.3 Poisson NPN",
      "text" : "In the following we assume the activation function v(x) = r(1−exp(τx)) and use Poisson regression loss E = 1T (y ◦ log o(L)c − o(L)c − log(y!)) (the target output y is a vector with nonnegative integer entries). Gamma distributions are used on weights.\nE → o(L)c : ∂E\n∂o (L) c\n= y\no (L) c\n− 1.\no (l) c → o(l)m ,o(l)c → o(l)s :\n∂E\n∂o (l) m\n= ∂E\no (l) c\n◦ (1 2 + 1 2 ((2o(l)m − 1)2 + 8o(l)s )− 1 2 ◦ (2o(l)m − 1))\n∂E\n∂o (l) s\n= ∂E\no (l) c\n◦ ((2o(l)m − 1)2 + 8o(l)s )− 1 2 .\no(l) → a(l−1): ∂E\n∂a (l−1) m\n= ∂E\n∂o (l) m\nW(l)m T + ( ∂E\n∂o (l) s\nW(l)s T ) ◦ 2a(l−1)m\n∂E\n∂a (l−1) s\n= ∂E\n∂o (l) s\nW(l)s T + ∂E\n∂o (l) s\n(W(l)m ◦W(l)m ) T .\na(l) → o(l)c : ∂E\n∂o (l) c = −r(exp(−τ)− 1) ∂E ∂a (l) m ◦ exp((exp(−τ)− 1)o(l)c )\n+ r2 ∂E\n∂a (l) s\n◦ ((exp(−2τ)− 1) exp((exp(−2τ)− 1)o(l)c )\n− 2(exp(−τ)− 1) exp(2(exp(−τ)− 1)o(l)c ))\no(l) →W(l),o(l) → b(l): The gradients with respect to the mean-variance pairs:\n∂E\n∂W (l) m\n= a(l−1)m T ∂E\n∂o (l) m\n+ (a(l−1)s T ∂E\n∂o (l) s\n) ◦ 2W(l)m\n∂E\n∂W (l) s\n= a(l−1)s T ∂E\n∂o (l) s\n+ (a(l−1)m ◦ a(l−1)m )T ∂E\n∂o (l) s\n∂E\n∂b (l) m\n= ∂E\n∂o (l) m\n∂E\n∂b (l) s\n= ∂E\n∂o (l) s\nThe gradients with respect to the proxy natural parameters:\n∂E\n∂W (l) c\n= ∂E\n∂W (l) m ◦ 1 W (l) d + ∂E ∂W (l) s ◦ 1 W (l) d 2\n∂E\n∂W (l) d\n= − ∂E ∂W (l) m\n◦ W (l) c\nW (l) d\n2 − 2 ∂E\n∂W (l) s\n◦ W (l) c\nW (l) d\n3\n∂E\n∂b (l) c\n= ∂E\n∂b (l) m ◦ 1 b (l) d + ∂E ∂b (l) s ◦ 1 b (l) d 2\n∂E\n∂b (l) d\n= − ∂E ∂b (l) m\n◦ b (l) c\nb (l) d\n2 − 2 ∂E\n∂b (l) s\n◦ b (l) c\nb (l) d\n3"
    } ],
    "references" : [ {
      "title" : "Bayesian dark knowledge",
      "author" : [ "A.K. Balan", "V. Rathod", "K.P. Murphy", "M. Welling" ],
      "venue" : "NIPS",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Springer-Verlag New York, Inc., Secaucus, NJ, USA",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Weight uncertainty in neural network",
      "author" : [ "C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra" ],
      "venue" : "ICML",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems",
      "author" : [ "T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu", "C. Zhang", "Z. Zhang" ],
      "venue" : "CoRR, abs/1512.01274",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The greatest of a finite set of random variables",
      "author" : [ "C.E. Clark" ],
      "venue" : "Operations Research, 9(2):145–162",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "Deep Learning",
      "author" : [ "I. Goodfellow", "Y. Bengio", "A. Courville" ],
      "venue" : "Book in preparation for MIT Press",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "NIPS",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep poisson factor modeling",
      "author" : [ "R. Henao", "Z. Gan", "J. Lu", "L. Carin" ],
      "venue" : "NIPS",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Probabilistic backpropagation for scalable learning of Bayesian neural networks",
      "author" : [ "J.M. Hernández-Lobato", "R. Adams" ],
      "venue" : "ICML",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Keeping the neural networks simple by minimizing the description length of the weights",
      "author" : [ "G.E. Hinton", "D. Van Camp" ],
      "venue" : "COLT",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "F. Li" ],
      "venue" : "CVPR",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "CoRR, abs/1312.6114",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Algorithms for non-negative matrix factorization",
      "author" : [ "D.D. Lee", "H.S. Seung" ],
      "venue" : "NIPS",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Krevl. SNAP Datasets: Stanford large network dataset collection",
      "author" : [ "A.J. Leskovec" ],
      "venue" : "http://snap. stanford.edu/data,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "A practical Bayesian framework for backprop networks",
      "author" : [ "J. MacKay David" ],
      "venue" : "Neural computation",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Learning stochastic feedforward networks",
      "author" : [ "R.M. Neal" ],
      "venue" : "Department of Computer Science, University of Toronto",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Bayesian learning for neural networks",
      "author" : [ "R.M. Neal" ],
      "venue" : "PhD thesis, University of Toronto",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Deep exponential families",
      "author" : [ "R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei" ],
      "venue" : "AISTATS",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Semantic hashing",
      "author" : [ "R. Salakhutdinov", "G.E. Hinton" ],
      "venue" : "Int. J. Approx. Reasoning, 50(7):969–978",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol" ],
      "venue" : "JMLR, 11:3371–3408",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Collaborative topic modeling for recommending scientific articles",
      "author" : [ "C. Wang", "D.M. Blei" ],
      "venue" : "KDD",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Collaborative topic regression with social regularization for tag recommendation",
      "author" : [ "H. Wang", "B. Chen", "W.-J. Li" ],
      "venue" : "IJCAI",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Collaborative deep learning for recommender systems",
      "author" : [ "H. Wang", "N. Wang", "D. Yeung" ],
      "venue" : "KDD",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Towards Bayesian deep learning: A framework and some existing methods",
      "author" : [ "H. Wang", "D. Yeung" ],
      "venue" : "TKDE",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Beta-negative binomial process and poisson factor analysis",
      "author" : [ "M. Zhou", "L. Hannah", "D.B. Dunson", "L. Carin" ],
      "venue" : "AISTATS",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Recently neural networks (NN) have achieved state-of-the-art performance in various applications ranging from computer vision [12] to natural language processing [20].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 19,
      "context" : "Recently neural networks (NN) have achieved state-of-the-art performance in various applications ranging from computer vision [12] to natural language processing [20].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "Early BNN works include methods based on Laplace approximation [16], variational inference (VI) [11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of scalability.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "Early BNN works include methods based on Laplace approximation [16], variational inference (VI) [11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of scalability.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "Early BNN works include methods based on Laplace approximation [16], variational inference (VI) [11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of scalability.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "[8] proposed a method based on VI in which a Monte Carlo estimate of a lower bound on the marginal likelihood is used to infer the weights.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "Recently, [10] used an online version of expectation propagation (EP), called ‘probabilistic back propagation’ (PBP), for the Bayesian learning of NN, and [4] proposed ‘Bayes by Backprop’ (BBB), which can be viewed as an extension of [8] based on the ‘reparameterization trick’ [13].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "Recently, [10] used an online version of expectation propagation (EP), called ‘probabilistic back propagation’ (PBP), for the Bayesian learning of NN, and [4] proposed ‘Bayes by Backprop’ (BBB), which can be viewed as an extension of [8] based on the ‘reparameterization trick’ [13].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "Recently, [10] used an online version of expectation propagation (EP), called ‘probabilistic back propagation’ (PBP), for the Bayesian learning of NN, and [4] proposed ‘Bayes by Backprop’ (BBB), which can be viewed as an extension of [8] based on the ‘reparameterization trick’ [13].",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 12,
      "context" : "Recently, [10] used an online version of expectation propagation (EP), called ‘probabilistic back propagation’ (PBP), for the Bayesian learning of NN, and [4] proposed ‘Bayes by Backprop’ (BBB), which can be viewed as an extension of [8] based on the ‘reparameterization trick’ [13].",
      "startOffset" : 278,
      "endOffset" : 282
    }, {
      "referenceID" : 0,
      "context" : "More recently, an interesting Bayesian treatment called ‘Bayesian dark knowledge’ (BDK) was designed to approximate a teacher network with a simpler student network based on stochastic gradient Langevin dynamics (SGLD) [1].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 7,
      "context" : "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a ‘vanilla’ NN; (2) as mentioned in [1], methods",
      "startOffset" : 188,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a ‘vanilla’ NN; (2) as mentioned in [1], methods",
      "startOffset" : 188,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a ‘vanilla’ NN; (2) as mentioned in [1], methods",
      "startOffset" : 188,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a ‘vanilla’ NN; (2) as mentioned in [1], methods",
      "startOffset" : 214,
      "endOffset" : 217
    }, {
      "referenceID" : 0,
      "context" : "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a ‘vanilla’ NN; (2) as mentioned in [1], methods",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 20,
      "context" : "Input distributions go through layers of linear and nonlinear transformation deterministically before producing distributions to match the target output distributions (previous work [21] shows that providing distributions as input by corrupting the data with noise plays the role of regularization).",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 2,
      "context" : "Thanks to the properties of the exponential family [3, 19], distributions in NPN are defined by the corresponding natural parameters which can be learned efficiently by backpropagation.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "Thanks to the properties of the exponential family [3, 19], distributions in NPN are defined by the corresponding natural parameters which can be learned efficiently by backpropagation.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "Unlike [4, 1], NPN explicitly propagates the estimates of uncertainty back and forth in deep networks.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Unlike [4, 1], NPN explicitly propagates the estimates of uncertainty back and forth in deep networks.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Table 1: Activation Functions for Exponential-Family Distributions Distribution Probability Density Function Activation Function Support Beta Distribution p(x) = Γ(c+d) Γ(c)Γ(d) xc−1(1− x)d−1 qx , τ ∈ (0, 1) [0, 1] Rayleigh Distribution p(x) = x σ2 exp{− x2 2σ2 } r − q exp{−τx} (0,+∞) Gamma Distribution p(x) = 1 Γ(c) dcxc−1 exp{−dx} r − q exp{−τx} (0,+∞) Poisson Distribution p(x) = c x exp{−c} x! r − q exp{−τx} Nonnegative interger Gaussian Distribution p(x) = (2πσ2)− 1 2 exp{− 1 2σ2 (x− μ)} ReLU, tanh, and sigmoid (−∞,+∞) and ∫ po(x|η)v(x)dx, can be expressed in closed form.",
      "startOffset" : 208,
      "endOffset" : 214
    }, {
      "referenceID" : 13,
      "context" : "Using gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version of nonnegative matrix factorization [14] while an NPN with the Bernoulli distribution and sigmoid activation resembles a Bayesian treatment of sigmoid belief networks [17].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "Using gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version of nonnegative matrix factorization [14] while an NPN with the Bernoulli distribution and sigmoid activation resembles a Bayesian treatment of sigmoid belief networks [17].",
      "startOffset" : 271,
      "endOffset" : 275
    }, {
      "referenceID" : 25,
      "context" : "If Poisson distributions are chosen for the neurons, NPN becomes a neural analogue of deep Poisson factor analysis [26, 9].",
      "startOffset" : 115,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "If Poisson distributions are chosen for the neurons, NPN becomes a neural analogue of deep Poisson factor analysis [26, 9].",
      "startOffset" : 115,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "If we assume gamma distributions for W, b, o, and a, an AE formed by NPN becomes a deep and nonlinear version of nonnegative matrix factorization [14].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "If the ReLU activation v(x) = max(0, x) is used, we can use the techniques in [6] to obtain the first two moments of max(z1, z2) where z1 and z2 are Gaussian random variables.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : "Interestingly, this design of Poisson NPN can be seen as a neural analogue of some Poisson factor analysis models [26].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "For other baselines, we use the Theano library [2] and MXNet [5].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "For other baselines, we use the Theano library [2] and MXNet [5].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Following [1], we generate 20 points in one dimension from a uniform distribution in the interval [−4, 4].",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "For BBB, we directly quote their results from [4].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "We implement BDK and NPN using the same hyperparameters as in [1] whenever possible.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "As shown in Table 2, BDK and BBB achieve comparable performance with dropout NN (similar to [1], PBP is not included in the comparison since it supports regression only), and gamma NPN slightly outperforms dropout NN.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "The first two datasets are from [22, 23], collected separately from CiteULike in different ways to mimic different real-world settings.",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "The first two datasets are from [22, 23], collected separately from CiteULike in different ways to mimic different real-world settings.",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "The third one is from arXiv as one of the SNAP datasets [15].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "The task is to perform unsupervised representation learning before feeding the extracted representations (middle-layer neurons) into a Bayesian LR algorithm [3].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "We use the stacked autoencoder (SAE) [7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines (hyperparameters like weight decay and dropout rate are chosen by cross validation).",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "We use the stacked autoencoder (SAE) [7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines (hyperparameters like weight decay and dropout rate are chosen by cross validation).",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "We use the stacked autoencoder (SAE) [7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines (hyperparameters like weight decay and dropout rate are chosen by cross validation).",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "As possible extensions of NPN, it would be interesting to connect NPN to arbitrary PGM to form fully Bayesian deep learning models [24, 25], allowing even richer descriptions of relationships among latent variables.",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 24,
      "context" : "As possible extensions of NPN, it would be interesting to connect NPN to arbitrary PGM to form fully Bayesian deep learning models [24, 25], allowing even richer descriptions of relationships among latent variables.",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "Since we expect the nonlinearly transformed distribution to be another beta distribution, the domain of the function should be (0, 1) and the field should be [0, 1].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "3 ReLU Activation If the ReLU activation v(x) = max(0, x) is used, we can use the techniques in [6] to obtain the first two moments of z = max(z1, z2) where z1 ∼ N (μ1, σ 1) and z2 ∼ N (μ2, σ 2).",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "For the toy 1d regression task, we use networks with one hidden layer containing 100 neurons and ReLU activation, as in [1, 10].",
      "startOffset" : 120,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "For the toy 1d regression task, we use networks with one hidden layer containing 100 neurons and ReLU activation, as in [1, 10].",
      "startOffset" : 120,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : "For preprocessing, following [4, 1], pixel values are normalized to the range [0, 1].",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "For preprocessing, following [4, 1], pixel values are normalized to the range [0, 1].",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "For preprocessing, following [4, 1], pixel values are normalized to the range [0, 1].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "For all models, we preprocess the BOW vectors by normalizing them into the range [0, 1].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 25,
      "context" : "Interestingly, this design of Poisson NPN can be seen as a neural analogue of some Poisson factor analysis models [26].",
      "startOffset" : 114,
      "endOffset" : 118
    } ],
    "year" : 2016,
    "abstractText" : "Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.",
    "creator" : "LaTeX with hyperref package"
  }
}