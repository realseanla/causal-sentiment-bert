{"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2015", "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision", "abstract": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.", "histories": [["v1", "Thu, 5 Mar 2015 07:07:48 GMT  (1602kb,D)", "https://arxiv.org/abs/1503.01558v1", null], ["v2", "Sun, 8 Mar 2015 04:11:49 GMT  (1602kb,D)", "http://arxiv.org/abs/1503.01558v2", "To appear in NAACL 2015"], ["v3", "Fri, 13 Mar 2015 18:55:22 GMT  (1602kb,D)", "http://arxiv.org/abs/1503.01558v3", "To appear in NAACL 2015"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.IR", "authors": ["jonathan malmaud", "jonathan huang", "vivek rathod", "nicholas johnston", "andrew rabinovich", "kevin murphy 0002"], "accepted": true, "id": "1503.01558"}
