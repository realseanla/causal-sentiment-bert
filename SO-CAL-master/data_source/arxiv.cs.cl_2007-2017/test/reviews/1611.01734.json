{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Deep Biaffine Attention for Neural Dependency Parsing", "abstract": "While deep learning parsing approaches have proven very successful at finding the structure of sentences, most neural dependency parsers use neural networks only for feature extraction, and then use those features in traditional parsing algorithms. In contrast, this paper builds off recent work using general-purpose neural network components, training an attention mechanism over an LSTM to attend to the head of the phrase. We get state-of-the-art results for standard dependency parsing benchmarks, achieving 95.44% UAS and 93.76% LAS on the PTB dataset, 0.8% and 1.0% improvement, respectively, over Andor et al. (2016). In addition to proposing a new parsing architecture using dimensionality reduction and biaffine interactions, we examine simple hyperparameter choices that had a profound influence on the model's performance, such as reducing the value of beta2 in the Adam optimization algorithm.", "histories": [["v1", "Sun, 6 Nov 2016 07:26:38 GMT  (22kb)", "http://arxiv.org/abs/1611.01734v1", null], ["v2", "Tue, 22 Nov 2016 02:01:39 GMT  (22kb)", "http://arxiv.org/abs/1611.01734v2", "Under review for ICLR 2017; fixed typos and clarified prediction process"], ["v3", "Fri, 10 Mar 2017 04:37:03 GMT  (19kb)", "http://arxiv.org/abs/1611.01734v3", "Accepted to ICLR 2017; updated with new results and comparison to more recent models, including current state-of-the-art"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["timothy dozat", "christopher d manning"], "accepted": true, "id": "1611.01734"}
