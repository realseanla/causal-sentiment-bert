{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2017", "title": "Natural Language Processing with Small Feed-Forward Networks", "abstract": "We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.", "histories": [["v1", "Tue, 1 Aug 2017 09:13:44 GMT  (107kb,D)", "http://arxiv.org/abs/1708.00214v1", "EMNLP 2017 short paper"]], "COMMENTS": "EMNLP 2017 short paper", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jan a botha", "emily pitler", "ji ma", "anton bakalov", "alex salcianu", "david weiss", "ryan t mcdonald", "slav petrov"], "accepted": true, "id": "1708.00214"}
