{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2013", "title": "Programming with Personalized PageRank: A Locally Groundable First-Order Probabilistic Logic", "abstract": "In many probabilistic first-order representation systems, inference is performed by \"grounding\"---i.e., mapping it to a propositional representation, and then performing propositional inference. With a large database of facts, groundings can be very large, making inference and learning computationally expensive. Here we present a first-order probabilistic language which is well-suited to approximate \"local\" grounding: every query $Q$ can be approximately grounded with a small graph. The language is an extension of stochastic logic programs where inference is performed by a variant of personalized PageRank. Experimentally, we show that the approach performs well without weight learning on an entity resolution task; that supervised weight-learning improves accuracy; and that grounding time is independent of DB size. We also show that order-of-magnitude speedups are possible by parallelizing learning.", "histories": [["v1", "Fri, 10 May 2013 04:16:15 GMT  (321kb,D)", "http://arxiv.org/abs/1305.2254v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["william yang wang", "kathryn mazaitis", "william w cohen"], "accepted": false, "id": "1305.2254"}
