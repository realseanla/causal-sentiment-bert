{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "On Minimax Optimal Offline Policy Evaluation", "abstract": "This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a minimax risk lower bound, and analyze the risk of two standard estimators. It is shown, and verified in simulation, that one is minimax optimal up to a constant, while another can be arbitrarily worse, despite its empirical success and popularity. The results are applied to related problems in contextual bandits and fixed-horizon Markov decision processes, and are also related to semi-supervised learning.", "histories": [["v1", "Fri, 12 Sep 2014 06:10:15 GMT  (521kb,D)", "http://arxiv.org/abs/1409.3653v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["lihong li", "remi munos", "csaba szepesvari"], "accepted": false, "id": "1409.3653"}
