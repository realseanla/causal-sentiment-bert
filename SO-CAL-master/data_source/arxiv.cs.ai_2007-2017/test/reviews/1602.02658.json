{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Graying the black box: Understanding DQNs", "abstract": "In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize of deep neural networks in Reinforcement Learning.", "histories": [["v1", "Mon, 8 Feb 2016 17:27:31 GMT  (7148kb,D)", "http://arxiv.org/abs/1602.02658v1", null], ["v2", "Tue, 9 Feb 2016 16:13:00 GMT  (7148kb,D)", "http://arxiv.org/abs/1602.02658v2", null], ["v3", "Wed, 17 Feb 2016 19:15:55 GMT  (7149kb,D)", "http://arxiv.org/abs/1602.02658v3", null], ["v4", "Mon, 24 Apr 2017 09:57:21 GMT  (7660kb,D)", "http://arxiv.org/abs/1602.02658v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["tom zahavy", "nir ben-zrihem", "shie mannor"], "accepted": true, "id": "1602.02658"}
