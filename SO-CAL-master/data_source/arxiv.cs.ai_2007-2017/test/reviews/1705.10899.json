{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Propositional Knowledge Representation in Restricted Boltzmann Machines", "abstract": "Representing symbolic knowledge into a connectionist network is the key element for the integration of scalable learning and sound reasoning. Most of the previous studies focus on discriminative neural networks which unnecessarily require a separation of input/output variables. Recent development of generative neural networks such as restricted Boltzmann machines (RBMs) has shown a capability of learning semantic abstractions directly from data, posing a promise for general symbolic learning and reasoning. Previous work on Penalty logic show a link between propositional logic and symmetric connectionist networks, however it is not applicable to RBMs. This paper proposes a novel method to represent propositional formulas into RBMs/stack of RBMs where Gibbs sampling can be seen as MaxSAT. It also shows a promising use of RBMs to learn symbolic knowledge through maximum likelihood estimation.", "histories": [["v1", "Wed, 31 May 2017 00:24:16 GMT  (52kb,D)", "http://arxiv.org/abs/1705.10899v1", null], ["v2", "Thu, 1 Jun 2017 00:19:24 GMT  (52kb,D)", "http://arxiv.org/abs/1705.10899v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["son n tran"], "accepted": false, "id": "1705.10899"}
