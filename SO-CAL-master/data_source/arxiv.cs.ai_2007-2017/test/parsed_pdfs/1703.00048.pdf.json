{
  "name" : "1703.00048.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Provable Optimal Algorithms for Generalized Linear Contextual Bandits",
    "authors" : [ "Lihong Li", "Yu Lu", "Dengyong Zhou" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n00 04\n8v 1\n[ cs\n.L G\n] 2\n8 Fe\nb 20\n17\nservices from news recommendation to advertising. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contex-\ntual bandits, which achieves an Õ( √ dT ) regret over T rounds with d dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on\nthe best previous result by a √ d factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximumlikelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for the certain cases."
    }, {
      "heading" : "1. Introduction",
      "text" : "Contextual bandit problems are originally motivated by applications in clinical trials (Woodroofe, 1979). When a standard treatment and a new treatment are available for a certain disease, the doctor needs to decide, in a sequetial manner, which of them to use based on the patient’s profiles such as age, general physical status or medicine history. With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search (Agarwal et al., 2009; Li et al., 2010; 2012). In the problem of personalized news recommendation, the website must recommend news articles that are most interesting to users that visit the website. The problem is especially challenging for breaking news, as little data are available\n1Microsoft Research, Redmond, WA 98052 2Department of Statistics, Yale University, New Haven, CT, USA.\nto make good prediction about user interest. A trade-off naturally occurs in this kind of sequential decision making problems. One needs to balance exploitation—choosing actions that performed well in the past—and exploration— choosing actions that may potentially give better outcomes.\nIn this paper, we study the following stochastic, Karmed contextual bandit problem. Suppose at each of the T rounds, an agent is presented with a set of K actions, each of which is associated with a context (a d-dimensional feature vector). By choosing an action based on the rewards obtained from previous rounds and on the contexts, the agent will receive a stochastic reward generated from some unknown distribution conditioned on the context and the chosen action. The goal of the agent is to maximize the expected cumulative rewards over T rounds. The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector. The linear model is theoretically convenient to work on. However, in practice, we usually have binary rewards (click or not, treatment working or not). Logistic regression model based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012). Hence, we consider generalized linear models (GLM) in the contextual bandit setting, in which linear, logistic and probit regression serve as three important special cases.\nThe celebrated work of Lai & Robbins (1985) first introduces the upper confidence bound (UCB) approach to efficient exploration. Later, the idea of confidence bound has been successfully applied to many stochastic bandits problems, from K-arm bandits problems (Auer et al., 2002a; Bubeck & Cesa-Bianchi, 2012) to linear bandits (Auer, 2003; Abbasi-Yadkori et al., 2011). UCB-type algorithms are both efficient and provable optimal in K-arm bandits andK-armed linear bandits. However, most study are limited to the linear case. While some UCB-type algorithms using GLMs perform well empirically (Li et al., 2012), there is little theoretical study of them. A natural question arises: can we find an efficient algorithm to achieve the optimal convergence rate for generalized linear bandits?\nOur Contributions In this paper, we propose a GLM version of the UCB algorithm called SupCB-GLM that\nachieves a regret overT rounds of order Õ( √ dT ). This rate improves the state-of-the-art results of Filippi et al. (2010)\nby a √ d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem’s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits. A key observation in proving this result is that the ℓ2 confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in all directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To our best knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice.\nRelated Work The study of GLM bandits problem goes back at least to Sarkar (1991), who considered discounted regrets rather than cumulative regerts. They prove that a myopic rule without exploration is asymptotically optimal. Recently, Filippi et al. (2010) study the same stochastic GLM bandit problem considered here. They propose the GLM-UCB algorithm, similar to our Algorithm 1, which\nachieves a regret of Õ(d √ T ) after T rounds. However, as we believe the optimal regret for stochastic GLM bandits should be the same as linear case when the number of arms\nis small, their rates misses a √ d term than the optimal rates.\nAnother line of research focuses on using EXP-type algorithms, which can be applied to almost any model classes (Auer et al., 2002b). These algorithms, which choose actions using a carefully randomized policy, use importance sampling to reduce a bandit problem to its fullinformation analogue. Later variants of the EXP4 algorithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give\nan Õ( √ dKT ) regret that is near-optimal with respect to\nT . However, these regret bounds have a √ K dependence. Moreover, these algorithms can be expensive to run: they either have a computational complexity exponential in d for our GLM case, or need to make a large number of calls to a nontrivial optimization oracle.\nOrganization The rest of this paper is organized as follows. Section 2 introduceS the generalized linear bandit problem. Section 3 gives a brief review of the statistical properties of generalized linear model, and gives a sharp\nnon-asymptotic normality-type result for GLM parameter estimation which can be of independent value. With this tool, Section 4 presents our algorithms and the main theoretical results. Section 5 concludes the paper with further discussions, including several open problems. All proofs are given in the supplementary materials.\nNotations For a vector x ∈ Rd, we use ‖x‖ to denote its ℓ2- norm and x\n′ its transpose. Denote the d-dimensional unit ball centered at the origin byBd := {x ∈ Rd : ‖x‖ ≤ 1}. The weighted ℓ2-norm associated with a positivedefinite matrix A is defined by ‖x‖A := √ x′Ax. The minimum and maximum singular values of a matrix A are written as λmin(A) and ‖A‖, respectively. The trace of a matrixA is tr (A). For two symmetric matricesA andB of the same dimensions, A B means that A−B is positive semi-definite. For a real-valued function f , we use ḟ and f̈ to denote its first and second derivatives. Finally, the set {1, 2, . . . , n} is abbreviated as [n]."
    }, {
      "heading" : "2. Problem Setting",
      "text" : "We considered the stochastic K-armed contextual bandit problem. Let T be the number of total rounds. At round t, the agent observes a context consisting of a set of K feature vectors, {xt,a | a ∈ [K]} ⊂ Rd, which is drawn IID from an unknown distribution ν, with ‖xt,a‖ ≤ 1. Each feature vector xt,a is associated with an unknown stochastic reward yt,a ∈ [0, 1]. The agent selects one action, denoted at, and observes the corresponding reward yt,at . Finally, we make a regularity assumption about the distribution ν: there exists a constant σ0 > 0 such that λmin(E[ 1 K ∑ a∈[K] xt,ax ′ t,a]) ≥ σ20 for all t.\nIn this paper, we consider the generalized linear model, or GLM, in which there is an unknown parameter θ∗ ∈ Rd and a fixed, strictly increasing link function µ : R → R such that E[Y | X ] = µ(X ′θ∗), where X is the chosen action’s feature and Y the corresponding reward. One can verify that linear and logistic models are special cases of GLMwith µ(x) = x and µ(x) = 1/(1+e−x), respectively.\nThe agent’s goal is to maximize the cumulative expected rewards over T rounds. Suppose the agent takes action at at round t. Then the agent’s strategy can be evaluated by comparing its expected reward to the best expected reward. To do so, define the optimal action at round t by a∗t = argmaxa∈[K] µ(x ′ t,aθ\n∗). Then, the agent’s total regret of following strategy π can be expressed as follows\nRT (π) :=\nT ∑\nt=1\n(\nµ(x′t,a∗t θ ∗)− µ(x′t,atθ∗)\n)\n.\nNote that RT (π) is in general a random variable due to the possible randomness in π. Denote by Xt = xt,at , Yt =\nyt,at , and our model can be written as\nYt = µ(X ′ tθ ∗) + ǫt , (1)\nwhere {ǫt, t ∈ [T ]} are independent zero-mean noise. Here, Xt is a random variable because the agent chooses current action based on previous rewards. Formally, we assume there is an increasing sequence of sigma fields {Fn} such that ǫt is Ft-measurable with E [ ǫt | Ft−1 ] = 0. An example of Fn will be the sigma-field generated by {X1, Y1, . . . , Xn, Yn}. Also, we assume the noise ǫt is sub-Gaussian with parameter σ, where σ is some positive, universal constant; that is, for all t,\nE [ eλǫt | Ft−1 ] ≤ eλ2σ2/2. (2)\nIn practice, when we have bounded reward Yt ∈ [0, 1], the noise ǫt is also bounded and hence satisfies (2) with some appropriate σ value. In addition to the boundedness assumption on the rewards and feature vectors, we also need the following assumption on the link function µ.\nAssumption 1. κ := inf{‖x‖≤1, ‖θ−θ∗‖≤1} µ̇(x ′θ) > 0.\nAs we shall see in Section 3, the asymptotic normality of maximum-likelihood estimates implies the necessity of this assumption. Note that this assumption is weaker than Assumption 1 in Filippi et al. (2010), as it only requires to control the local behavior of µ̇(x′θ) near θ∗.\nAssumption 2. µ is twice differentiable. Its first and second order derivatives are upper-bounded by Lµ and Mµ, respectively.\nIt can be verified that Assumption 2 holds for the logistic link function, where we may choose Lµ = Mµ = 1/4."
    }, {
      "heading" : "3. Generalized Linear Models",
      "text" : "To motivate the algorithms proposed in this paper, we first briefly review the classical likelihood theory of generalized linear models. In the canonical generalized linear model (McCullagh & Nelder, 1989), the conditional distribution of Y given X is from the exponential family, and its density, parameterized by θ ∈ Θ, can be written as\nP(Y | X ) = exp\n{\nY X ′θ∗ −m(X ′θ∗)\ng(η) + h(Y, η)\n}\n. (3)\nHere, η ∈ R+ is a known scale parameter; m, g and h are three normalization functions mapping from R to R. The exponential family (3) is a very broad family of distributions including the Gaussian, binomial, Poisson, gamma and inverse-Gaussian distributions. It follows from standard properties of exponential families (Brown, 1986) that m is infinitely differentiable satisfying ṁ(X ′θ∗) = E[Y | X ] = µ(X ′θ∗) and m̈(X ′θ∗) = V(Y | X). It can be checked that the data generated from (3) automatically satisfies the sub-Gaussian condition (2).\nSuppose we have independent samples of Y1, Y2, . . . , Yn condition onX1, X2, . . . , Xn. The log-likelihood function of θ under model (3) is\nlog ℓ(θ) =\nn ∑\nt=1\n[ YtX ′ tθ −m(X ′tθ) v(η) + c(Yt, η) ]\n= 1\nv(η)\nn ∑\nt=1\n[YtX ′ tθ −m(X ′tθ)] + constant .\nConsequently, the maximum likelihood estimate (MLE) may be defined by\nθ̂n ∈ argmax θ∈Θ\nn ∑\nt=1\n[YtX ′ tθ −m(X ′tθ)] .\nFrom classical likelihood theory (Lehmann & Casella, 1998), we know that when the sample size n goes to infinity, theMLE θ̂n is asymptotically normal, that is, θ̂n−θ∗ → N (0, I−1θ∗ ), where Iθ = ∑n t=1 µ̇(X ′ tθ)XtX ′ t is the Fisher InformationMatrix. Note that if µ̇(X ′tθ ∗) → 0, the asymptotic variance of x′θ̂ can go to infinity for some x. This suggests the necessity of Assumption 1.\nAs we will see later, the normality result is crucial in our regret analysis of GLM bandits. However, to the best of our knowledge, there is no non-asymptotic normality results of the MLE for GLM. In the following, we present a finite-sample version of the classical asymptotic normality results, which can be of independent interest. Theorem 1. Define Vn = ∑n t=1 XtX ′ t, and let δ > 0 be given. Furthermore, assume that\nλmin(Vn) ≥ 512M2µσ 2\nκ4\n(\nd2 + log 1\nδ\n)\n. (4)\nThen, with probability at least 1 − 3δ, the maximumlikelihood estimator satisfies, for any x ∈ Rd, that\n|x′(θ̂n − θ∗)| ≤ 3σ\nκ\n√\nlog(1/δ) ‖x‖V −1n . (5)\nThis theorem characterizes the behavior of MLE on every direction. It implies that x′(θ̂n − θ∗) has a sub-Gaussian tail bound for any x ∈ Rd. It also provides a rigorous justification of the asymptotic upper confidence bound derived heuristically by Filippi et al. (2010, Section 4.2).\nThe proof of the theorem is given in the appendix. It consists of two main steps, as is typical for proving normalitytype results of MLEs (Van der Vaart, 2000). We first show the n−1/2-consistency of θ̂ to θ∗. Then, by using a secondorder Taylor expansion or Newton-step, we can prove the desired normality of θ̂.\nThe condition (4) on λmin(Vn) is necessary for the consistency of estimating linear models (Lai & Wei,\n1982; Bickel et al., 2009) and generalized linear models (Fahrmeir & Kaufmann, 1985; Chen et al., 1999). It can be satisfied under mild conditions such as the proposition below, which will be useful for our analysis.\nProposition 1. Define Vn = ∑n t=1 XtX ′ t, where Xt is drawn iid from some distribution ν with support in the unit ball, Bd. Furthermore, let Σ := E[XtX ′ t] be the second moment matrix, and B and δ > 0 be two positive constants. Then, there exist positive, universal constants C1 and C2 such that λmin(Vn) ≥ B with probability at least 1− δ, as long as\nn ≥ (\nC1 √ d+ C2 √ log(1/δ)\nλmin(Σ)\n)2\n+ 2B\nλmin(Σ) .\nProof Sketch. We give a proof sketch here, and the full proof is found in the appendix. In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore,Vn is denotedV and so on. We will need a technical lemma, which is an existing result in randommatrix theory. The version we presented here is adapted from Equation (5.23) of Theorem 5.39 from Vershynin (2012).\nLemma 1. Let A ∈ Rn×d be a matrix whose rows Ai are independent sub-Gaussian isotropic random vectors in R d with parameter σ, namely, E exp(x′(Ai − EAi)) ≤ exp(σ2 ‖x‖2 /2) for any x ∈ Rd. Then, there exist positive, universal constants C1 and C2 such that, for every t ≥ 0, the following holds with probability at least 1 − 2 exp(−C2t2), where ε = σ2(C1 √ d/n + t/ √ n): ∥ ∥\n1 nA ′A− Id ∥ ∥ ≤ max{ε, ε2}.\nLet X be a random vector drawn from the distribution ν. Define Z := Σ−1/2X . Then Z is isotropic, namely, E[ZZ ′] = Id. Define U = ∑n t=1 ZtZ ′ t = Σ\n−1/2V Σ−1/2. From Lemma 1, we have that, for any t, with probability at least 1 − 2 exp(−C2t2), λmin(U) ≥ n − C1σ2 √ nd −\nσ2t √ n, where σ is the sub-Gaussian parameter ofZ , which is upper-bounded by ∥ ∥Σ−1/2 ∥\n∥ = λ −1/2 min (Σ) (see, e.g.,\nVershynin (2012)). We thus can rewrite the above inequality (which holds with probability 1− δ as\nλmin(U) ≥ n− λ−1min(Σ) ( C1σ 2 √ nd+ t √ n ) .\nThis implies the following lower bound:\nλmin(V ) ≥ λmin(Σ)n− C1 √ nd− C2 √ n log(1/δ) .\nFinally, simple calculations show that the last expression above is no less than B as long as n is no smaller than the expression stated in the proposition, finishing the proof."
    }, {
      "heading" : "4. Algorithms and Main Results",
      "text" : "In this section, we are going to present two algorithms. While the first algorithm is computationally more efficient, the second algorithm has a provable optimal regret bound."
    }, {
      "heading" : "4.1. Algorithm UCB-GLM",
      "text" : "The idea of upper confidence bounds (UCB) is highly effective in dealing with the exploration and exploitation trade-off in many parametric bandit problems, including K-arm bandits (Auer et al., 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008). For the generalized linear model considered here, since µ is a strictly increasing function, our goal is equivalent to choosing a ∈ [K] to maximize x′t,aθ ∗ at round t. Suppose θ̂t is our current estimator of θ ∗ after round t. An exploitation action is to take the action that maximizes the estimated mean value, while an exploration action is to choose the one that has the largest variance. Thus, to balance exploitation and exploration, we can simply choose the action that maximizes the sum of estimated mean and variance, which can be interpreted as an upper confidence bound of x′t,aθ̂t. This leads to the algorithm UCB-GLM (Algorithm 1).\nAlgorithm 1 UCB-GLM\nInput: the total rounds T , tuning parameter τ and α. Initialization: randomly choose at ∈ [K] for t ∈ [τ ], set Vτ+1 = ∑τ i=1 XtX ′ t\nFor t = τ + 1, τ + 2, . . . , T do\n1. Calculate the maximum-likelihood estimator θ̂t by solving the equation\nt−1 ∑\ni=1\n(Yi − µ(X ′iθ))Xi = 0 (6)\n2. Choose at = argmaxa∈[K]\n( X ′t,aθ̂t + α ‖Xt,a‖V −1t )\n3. Observe Yt, let Xt ← Xt,at , Vt+1 ← Vt +XtX ′t End For\nUCB-GLM take two parameters. At the initialization stage, we randomly choose actions to ensure a unique solution of (6). The choice of τ in the theorem statement follows from Proposition 1 with B = 1. It should be noted that the IID assumption about contextual (i.e., the distribution ν) is only needed to ensure Vτ+1 is invertable (similar to the first phase in the algorithm of Filippi et al. (2010)); the rest of our analysis does not depend on this stochastic assumption. The same may be achieved by using regularization (see, e.g., Abbasi-Yadkori et al. (2011)). Another tuning parameter α is used to control the amount of exploration. The larger the α is, the more exploration will be used.\nAs mentioned earlier, the feature vectors Xt depend\non the previous rewards. Consequently, the rewards {Yi, i ∈ [t]} may not be independent given {Xi, i ∈ [t]}. We instead use results on self-normalized martingales (Abbasi-Yadkori et al., 2011), together with a finitetime normality result like Theorem 1, to prove the next theorem.\nTheorem 2. Fix any δ > 0. There exists a universal constant C > 0, such that if we run UCB-GLM with\nα = σκ\n√\nd 2 log(1 + 2T/d) + log(1/δ) and τ = Cσ −2 0 (d+\nlog(1/δ)), then, with probability at least 1 − 2δ, the regret of the algorithm is upper bounded by\nRT ≤ τ + 2Lµσd\nκ log\n(\nT\ndδ\n)√ T .\nThe theorem shows an Õ(d √ T ) regret bound that is independent of K . Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008). By choosing δ = 1/T and using the fact that RT ≤ T , this highprobability result implies a bound on the expected regret:\nE[RT ] = Õ(d √ T ). Our result improves the previous\nregret bound of Filippi et al. (2010) by a √ logT factor. Moreover, the algorithm proposed in Filippi et al. (2010) involves a projection step, which is computationally more expensive comparing to UCB-GLM. Finally, this algorithm works well in practice. We give a heuristic argument for its strong performance in Section 5, under a specific condition that sometimes are satisfied.\nProof of Theorem 2. We first bound the one-step regret. To do so, fix t and let X∗t = xt,a∗t and ∆t = θ̂t − θ∗, where a∗t ∈ argmaxa∈[K] µ(x′t,aθ∗) is an optimal action at round t. The selection of at in UCB-GLM implies\n〈X∗t , θ̂t〉+ α ‖X∗t ‖V −1t ≤ 〈Xt, θ̂t〉+ α ‖Xt‖V −1t . (7) Then, we have\n〈X∗t , θ∗〉 − 〈Xt, θ∗〉 = 〈X∗t −Xt, θ̂t〉 − 〈X∗t −Xt, θ̂t − θ∗〉 ≤ α ‖Xt‖V −1t − α ‖X ∗ t ‖V −1t − 〈X ∗ t −Xt,∆t〉\n≤ α ( ‖Xt‖V −1t − ‖X ∗ t ‖V −1t ) + ‖X∗t −Xt‖V −1t ‖∆t‖Vt , where the last inequality is due to Cauchy-Schwartz inequality. We have the following two lemmas to bound ‖∆t‖Vt and ‖Xt‖V −1t , respectively. Their proofs are deferred to the appendix.\nLemma 2. Let {Xt}∞t=1 be a sequence in Rd satisfying ‖Xt‖ ≤ 1. Define X0 = 0 and Vt = ∑t−1 s=0 XsX ′ s. Suppose there is an integerm such that λmin(Vm+1) ≥ 1, then for all n > 0,\nm+n ∑\nt=m+1\n‖Xt‖V −1t ≤ √ 2nd log ( n+m\nd\n)\n.\nLemma 3. Suppose λmin(Vτ+1) ≥ 1. For any δ ∈ [1/T, 1), define event\nE∆ := { ‖∆t‖Vt ≤ σ\nκ\n√\nd 2 log(1 + 2t/d) + log(1/δ)\n}\n.\nThen, event E∆ holds for all t ≥ τ with probability at least 1− δ.\nWe now choose α = σκ\n√\nd 2 log(1 + 2T/d) + log(1/δ). If\nevent E∆ holds for all t ≥ τ , then,\n〈X∗t , θ∗〉 − 〈Xt, θ∗〉 ≤ α (\n‖Xt‖V −1t − ‖X ∗ t ‖V −1t + ‖X ∗ t −Xt‖V −1t\n)\n≤ 2α ‖Xt‖V −1t .\nCombining the above with Lemma 2 yields\nT ∑\nt=τ+1\n( 〈X∗t , θ∗〉 − 〈Xt, θ∗〉 )\n≤ 2α √ 2Td log ( T\nd\n)\n≤ 2dσ κ log\n(\nT\ndδ\n)√ T . (8)\nNote that µ is an increasing Lipschitz function with Lipschitz constant Lµ and the µ function is bounded between 0 and 1. The regret of algorithm UCB-GLM can be upper bounded as\nRT =\nτ ∑\nt=1\n( µ (〈X∗t , θ∗〉)− µ (〈Xt, θ∗〉) )\n+ T ∑\nt=τ+1\n( µ (〈X∗t , θ∗〉)− µ (〈Xt, θ∗〉) )\n≤ τ + Lµ T ∑\nt=τ+1\n( 〈X∗t , θ∗〉 − 〈Xt, θ∗〉 ) .\nThe proof can be finished by applying (8) and the specified value of τ to the bound above."
    }, {
      "heading" : "4.2. Algorithm SupCB-GLM",
      "text" : "While the algorithm UCB-GLM performs sufficiently well in practice (Li et al., 2012), it is unclear whether it can achieve the optimal rates of O( √ dT logK), when K is small. As mentioned in Section 4.1, the key technical difficulty in analyzing UCB-GLM is the dependence between samples. Inspired by a technique developed by Auer (2003) to create independent samples for linear contextual bandits, we propose another algorithm SupCB-GLM (Algorithm 3), which uses algorithm CB-GLM (Algorithm 2) as a subroutine.\nAlgorithm 2 CB-GLM\nInput: parameter α, index set Ψ(t), and candidate set A.\n1. Let θ̂t be the solution of ∑ i∈Ψ(t) [Yi − µ(X ′iθ)]Xi = 0\n2. Vt = ∑ i∈Ψ(t) XiX ′ i\n3. For a ∈ A, do\nwt,a = α ‖xt,a‖V −1t , mt,a = 〈xt,a, θ̂t〉\nEnd For\nThis algorithm also relies on the idea of confidence bound to do exploration. At round t, the algorithm screens the candidate actions based on the value of w (s) t,a through S stages until an action is chosen. At stage s, we set the confidence level at stage s to be 2−s. If w (s) t,a > 2 −s for some a, we need to do more exploration on xt,a and thus we choose this action. Otherwise, the actions are filtered in step 2d such that the actions passed to the next stage are close enough to the optimal action. Since all the widths are smaller than 2−s, if m (s) t,a < m (s) t,j − 2 · 2−s for some j ∈ As, the action a can not be the optimal action. The filter process terminates when we have already got accurate estimate of all x′t,aθ ∗ up to the 1/ √ T level and we do not need to do exploration. Thus in step 2c we just choose the action that maximizes the estimated mean value.\nOur algorithm is different from the algorithm SupLinRel in Auer (2003) that we directly maximize the mean, rather than the upper confidence bound, in steps c and d. This modification leads to a simpler algorithm and a cleaner regret analysis. Also, we would like to point out that, unlike SpectralEliminator (Valko et al., 2014), the algorithm can easily handle a changing action set.\nThe following result, adapted fromAuer (2003, lemma 14), shows how the algorithm SupCB-GLM will give us independent samples. For the sake of completeness, we also present its proof here.\nLemma 4. For all s ∈ [S] and t ∈ [T ], given {xi,ai , i ∈ Ψs(t)}, the rewards {yi,ai , i ∈ Ψs(t)} are independent random variables.\nProof of Lemma 4. Since a trial t can only be added to Ψs(t) in step 2b of algorithm SupCB-GLM, the event {t ∈ Ψs} only depends on the results of trials τ ∈ ∪σ<sΨσ(t) and on w\n(s) t,a . From the definition of w (s) t,a , we know it only\ndepends on the feature vectors xi,ai , i ∈ Ψs(t) and on xt,i. This implies the lemma.\nWith Lemma 4, we are able to apply the non-asymptotic\nAlgorithm 3 SupCB-GLM\nInput: tuning parameter α, τ , the number of trials T .\nInitialization:\nfor t ∈ [τ ], randomly choose at ∈ [K]. Set S = ⌊log2 T ⌋, F = {a1, · · · , aτ} and Ψ0 = Ψ1 =\n· · · = ΨS = ∅. For t = τ + 1, τ + 2, · · · , T do\n1. Initialize A1 = [K] and s = 1.\n2. While at =Null\na. Run CB-GLM with α and Ψs ∪ F to calculate m\n(s) t,a and w (s) t,a for all a ∈ As.\nb. If w (s) t,a > 2 −s for some a ∈ As,\nset at = a, updateΨs = Ψs ∪ {t}\nc. Else if w (s) t,a ≤ 1/ √ T for all a ∈ As,\nset at = argmax a∈As\nm (s) t,a, updateΨ0 = Ψ0 ∪ {t}\nd. Else if w (s) t,a ≤ 2−s for all a ∈ As,\nAs+1 = {a ∈ As,m(s)t,a ≥ max j∈As m (s) t,j − 2 · 2−s},\ns ← s+ 1 ."
    }, {
      "heading" : "End For",
      "text" : "normality result (5) and thus to prove our regret bound of Algorithm SupCB-GLM.\nTheorem 3. For any 0 < δ < 1, if we run the SupCB-GLM algorithm with τ = √ dT and α = 3σκ √\n2 log(TK/δ) for"
    }, {
      "heading" : "T ≥ T0 rounds, where",
      "text" : "T0 = Ω\n(\nσ2 κ4 max\n{\nd3, log(TK/δ)\nd\n})\n, (9)\nthe regret of the algorithm is bounded as\nRT ≤ 45(σLµ/κ) √ logT log(TK/δ) log(T/d) √ dT ,\nwith probability at least 1− δ. With δ = 1/T , we obtain\nE[RT ] = O ( (log T )1.5 √ dT logK ) .\nThe theorem demonstrates an Õ( √ dT logK) regret bound for the algorithm SupCB-GLM. It has been proved in\nChu et al. (2011, Theorem 2) that √ dT is the minimax lower bound of the expected regret forK-armed linear bandits, a special of the GLM bandits considered here. Therefore, the regret of our SupCB-GLM algorithm is optimal up\nto logarithm terms of T and K . To the best of our knowledge, this is the first algorithm which achieves the (near)optimal rate of GLM bandits.\nIt is worthwhile to compare Theorem 3 with the result in Theorem 2. When K = o(2d) is small, the rate of SupCBGLM is faster, and we improve the previous rates by a√ d factor. Here, we give a briefly illustration of how we\nget rid of the extra √ d factor. Both in Theorem 2 and in Filippi et al. (2010), |x′(θ̂n − θ∗)| is upper bounded by using the Cauchy-Schwartz inequality,\n|x′(θ̂n − θ∗)| ≤ ‖x‖V −1n ∥ ∥ ∥ θ̂n − θ∗ ∥ ∥ ∥\nVn . (10)\nLemma 3 in the supplementary material establishes that\n∥ ∥ ∥ θ̂n − θ∗ ∥ ∥ ∥\nVn ≤ C2\n√\nd log(T/δ).\nThis will lead to an extra √ d factor compared to (5). By using Cauchy-Schwartz (10), we only make use of the fact that θ̂n is close to θ ∗ in the ℓ2 sense. However, (5) tells us that actually θ̂n is close to θ ∗ in every direction. This is\nthe reason why we are able to remove the extra √ d factor to achieve a near-optimal regret. It also explains why the bound in Theorem 2 is tight when K is large. As K goes large, it is likely there is a direction x for which (10) is tight.\nProof of Theorem 3. To facilitate our proof, we first present two technical lemmas. Lemma 4, Theorem 1, Theorem 5.39 of Vershynin (2012) together with a union bound yield Lemma 5. The proof of Lemma 6 is deferred to the appendix. Lemma 5. Fix δ > 0. Choose in SupCB-GLM τ = √ dT and α = 3σκ √\n2 log(TK/δ). Suppose T satisfies condition (9). Define the following event:\nEX := {|m(s)t,a−x′t,aθ∗| ≤ w(s)t,a , ∀t ∈ [τ+1, T ], s ∈ [S], a ∈ [K]} . (11) Then, event EX holds with probability at least 1− δ.\nProof of Lemma 5. By Lemma 4, we have independent samples now. Then to apply Theorem 1, the key is to lower bound the minimum eigenvalue of Vt. Note that we randomly select the feature vectors at the first τ = √ dT rounds, that is, they are independent. Moreover, the feature vectors are bounded. Thus, X1, X2, . . . , Xτ are independent sub-Gaussian with parameter 1. By Proposition 1, we have\nλmin(Vt) ≥ λmin(Vτ ) ≥ c √ dT\nfor some constant c with probability at least 1 − exp(− √ dT ). By Theorem 1 and union bound, we have the desired result under condition (9).\nLemma 6. Suppose that event EX holds, and that in round t, the action at is chosen at stage st. Then, a ∗ t ∈ As for all s ≤ st. Furthermore, we have\nµ(x′t,a∗t θ ∗)− µ(x′t,atθ∗)\n≤ { (8Lµ)/2 st if at is selected in step 2b\n(2Lµ)/ √ T if at is selected in step 2c .\nDefine Vs,t = ∑\nt∈Ψs(T ) XtX ′ t, then by Lemma 2,\n∑\nt∈Ψs(T )\nw (s) t,at =\n∑\nt∈Ψs(T )\nα(δ)‖xt,at‖V −1s,t\n≤ α(δ) √ 2d log(T/d)|Ψs(n)| .\nOn the other hand, by the step 2b of SupCB-GLM,\n∑\nt∈Ψs(T )\nw (s) t,at ≥ 2−s|Ψs(T )|.\nCombining the above two inequalities gives us\n|Ψs(T )| ≤ 2sα(δ) √ 2d log (T/d)|Ψs(T )|. (12)\nLet Ψ0 be the collection of trials such that at is chosen in step 2c. Since we have chose S = log2 T , each t ∈ [τ + 1, T ]must be in one of Ψs and hence, {τ, τ + 1, . . . , T } = Ψ0 ∪ ( ∪Ss=1Ψs(T ) ) . If we set τ = √ dT , we have\nRT =\nτ ∑\nt=1\n(\nµ(x′t,a∗t θ ∗)− µ(x′t,atθ∗)\n)\n+\nT ∑\nt=τ+1\n(\nµ(x′t,a∗t θ ∗)− µ(x′t,atθ∗)\n)\n≤ τ + ∑\nt∈Ψ0\n(\nµ(x′t,a∗t θ ∗)− µ(x′t,atθ∗)\n)\n+\nS ∑\ns=1\n∑\nt∈Ψs(T )\n(\nµ(x′t,a∗t θ ∗)− µ(x′t,atθ∗)\n)\n≤ √ dT + T · 2Lµ√\nT +\nS ∑\ns=1\nLµ · 23−s · |Ψs(T )|\n≤ √ dT + 2Lµ √ T + 8Lµα(δ) S ∑\ns=1\n√\n2d log T\nd |Ψs(T )|\n≤ √ dT + 2Lµ √ T + 8Lµα(δ) √ 2d log(T/d) √ ST ≤ 45(σLµ/κ) √ logT log(TK/δ) log(T/d) √ dT ,\nwith probability at least 1 − δ. Here, the first inequality is due to the assumption that 0 ≤ µ ≤ 1. The second inequality is Lemma 6. The third inequality is the inequality (12) and the fourth inequality is implied by Cauchy-Schwartz. This completes the proof of the high-probability result."
    }, {
      "heading" : "5. Discussions",
      "text" : "In this paper, we propose two algorithms forK-armed bandits with generalized linear models. While the first algorithm, UCB-GLM, achieves the optimal rate for the case of infinite number of actions, the second algorithm SupCBGLM is provable optimal for the case of finite number actions at each round. However, it remains open whether UCB-GLM can achieve the optimal rate for small K ."
    }, {
      "heading" : "5.1. A better regret bound for UCB-GLM",
      "text" : "A key quantity in determine the regret of algorithm UCBGLM are the minimum eigenvalue of Vt. If we make an addition assumption on the minimum eigenvalue of Vt, we will be able to prove an O( √ dT ) regret bound for UCB-"
    }, {
      "heading" : "GLM.",
      "text" : "Theorem 4. We run algorithm UCB-GLM with τ = 8σ2\nκ2 d logT and α ≤ Lµσ/κ. For any δ ∈ [1/T, 1), suppose there is an universal constant c such that\nT ∑\nt=τ+1\nλ −1/2 min (Vt) ≤ c\n√ T . (13)\nholds with probability at least 1− δ, and\nT = Ω\n(\nσR\nκLµ d log2 T\n)\n. (14)\nThen, the regret of the algorithm is bounded by\nRT ≤ CLµσ\nκ\n√\ndT log(T/δ)\nwith probability at least 1 − 2δ, where C is an universal constant.\nThis theorem provides some insights of why UCB-GLM performs well in practice. Although the condition in (13) is hard to check and may be violated in some cases, for example, in K-armed bandits, we provide a heuristic argument to justify this assumption in a range of problems. When t is large enough, our estimator θ̂t is very close to θ ∗. If we assume there is a positive gap between 〈xt,a∗t , θ∗〉 and 〈xt,a, θ∗〉 for all a 6= a∗t , we will have at = a∗t after, for example, √ T steps. Since {xt,a, a ∈ [K]} are independent for t ∈ [T ], {xt,a∗t } are also independent samples. Then Vt/twill be well-approximated by the covariancematrix of xt,a∗t , which we denote by Σ0. In many problem in practice, especially when features are dense, it is unlikely the feature vector xt,a∗t lies in a low-dimensional subspace of R d. It implies that Σ0 has full rank, and that we will have λmin(Vt) = Θ(t · λmin(Σ0)) when t is large enough. It follows that,\nT ∑\nt=τ+1\n1 √\nλmin(Vt) =\nT ∑\nt=τ+1\nΘ(t−1/2) = O( √ T ).\nIt should be cautioned that, since we do not know the distribution of our feature vectors, we cannot assume the above gap exists. It is therefore challenging to make the above arguments rigorous. In fact, when studying the ARIMA model in time series, Lai & Wei (1982, Example 1) provide an example such that λmin(Vt) = O(log t)."
    }, {
      "heading" : "5.2. Open Questions",
      "text" : "Computational efficient algorithms. While UCB-GLM and SupCB-GLM enjoy good theoretical properties, they can be expensive in some application domains. First, they require inverting a d × d matrix in every step, a costly operation when d is large. Second, at step t, the MLE is computed using Θ(t) samples, meaning that the per-step complexity grows at least linearly with t for a straightforward implementation of the algorithms. It is therefore interesting to investigate more scalable alternatives. It is possible to use a first-order, iterative optimization procedure to amortize the cost, analogous to the approach taken by Agarwal et al. (2014).\nK-dependent lower bound. Currently, all the lower bound results on (generalized) linear bandits have no dependence on the number of actionsK . The minimax lower bound will be of particularly interest because all current lower bound results assume that K ≤ d. Although it will at most be a logarithm dependence on K , it is still a theoretically interesting question.\nRandomized algorithms with optimal regret rate. As opposed to the deterministic, UCB-style algorithms studied in this paper, randomized algorithms like EXP4 (Auer et al., 2002b) and Thompson Sampling (Thompson, 1933) have advantages in certain situations, for example, when reward observations are delayed (Chapelle & Li, 2012). Recently developed techniques for analyzing Bayes regret in BLM bandits (Russo & Van Roy, 2014) may be useful to analyze the cumulative regret considered here."
    }, {
      "heading" : "A. Proof of Theorem 1",
      "text" : "In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore, Vn is denoted V and so on.\nTo prove normality-type results of the maximum likelihood estimator θ̂, typically we first show the n−1/2-consistency of θ̂ to θ∗. Then, by using a second-order Taylor expansion or Newton-step, we can prove the desired normality of θ̂. More details can be found in standard textbooks such as Van der Vaart (2000).\nSince m is twice differentiable with m̈ ≥ 0, the maximum-likelihood estimation can be written as the solution to the following equation\nn ∑\ni=1\n(Yi − µ(X ′iθ))Xi = 0 . (15)\nDefine G(θ) := ∑n i=1 (µ(X ′ iθ)− µ(X ′iθ∗))Xi, and we have\nG(θ∗) = 0 and G(θ̂) =\nn ∑\ni=1\nǫiXi , (16)\nwhere the noise ǫi is defined in (1). For convenience, define Z := G(θ̂) = ∑n i=1 ǫiXi.\nStep 1: Consistency of θ̂. We first prove the consistency of θ̂. For any θ1, θ2 ∈ Rd, mean value theorem implies that there exists some θ̄ = vθ1 + (1− v)θ2 with 0 < v < 1, such that\nG(θ1)−G(θ2) = [ n ∑\ni=1\nµ̇(X ′i θ̄)XiX ′ i\n]\n(θ1 − θ2) := F (θ̄)(θ1 − θ2) (17)\nSince µ̇ > 0 and λmin(V ) > 0, we have\n(θ1 − θ2)′(G(θ1)−G(θ2)) ≥ (θ1 − θ2)′(κV )(θ1 − θ2) > 0\nfor any θ1 6= θ2. Hence, G(θ) is an injection from Rd to Rd, and so G−1 is a well-defined function. Consequently, (15) has a unique solution θ̂ = G−1(Z).\nLet us consider an η-neighborhood of θ∗, Bη := {θ : ‖θ − θ∗‖ ≤ η}, where η > 0 is a constant that will be specified later. Note that Bη is a convex set, thus θ̄ ∈ Bη as long as θ1, θ2 ∈ Bη . Define κη := infθ∈Bη µ̇(x′θ) > 0. From (17), for any θ ∈ Bη,\n‖G(θ)‖2V −1 = ‖G(θ) −G(θ∗)‖ 2 V −1\n= (θ − θ∗)′F (θ̄)V −1F (θ̄)(θ − θ∗) ≥ κ2ηλmin(V ) ‖θ − θ∗‖2 ,\nwhere the last inequality is due to the fact that F (θ̄) κηV . On the other hand, Lemma A of Chen et al. (1999) implies that\n{\nθ : ‖G(θ)‖V −1 ≤ κηη √ λmin(V ) } ⊂ Bη .\nNow it remains to upper bound ‖Z‖V −1 = ∥ ∥ ∥ G(θ̂) ∥ ∥ ∥\nV −1 to ensure θ̂ ∈ Bη . To do so, we need the following technical\nlemma, whose proof is deferred to Section C.\nLemma 7. Recall σ which is the constant in (2). For any δ > 0, define the following event:\nEG := { ‖Z‖V −1 ≤ 4σ √ d+ log(1/δ) } .\nThen, EG holds with probability at least 1− δ.\nSuppose EG holds for the rest of the proof. Then, η ≥ 4σκη √ d+log(1/δ) λmin(V ) implies ∥ ∥ ∥ θ̂t − θ ∥ ∥ ∥ ≤ η. Since κ = κ1, we have κη ≥ κ as long as η ≤ 1. Thus, we have\n∥ ∥ ∥ θ̂ − θ ∥ ∥ ∥ ≤ 4σ\nκ\n√\nd+ log(1/δ)\nλmin(V ) ≤ 1 , (18)\nwhen λmin(V ) ≥ 16σ2 [d+ log(1/δ)] /κ2.\nStep 2: Normality of θ̂. Now, we are ready to precede to prove the normality result. The following assumes EG holds (which is high-probability event, according to Lemma 7).\nDefine∆ := θ̂ − θ∗. It follows from (17) that there exists a v ∈ [0, 1] such that\nZ = G(θ̂)−G(θ∗) = (H + E)∆ ,\nwhere θ̃ := vθ∗ + (1− v)θ̂, H := F (θ∗) =∑ni=1 µ̇(X ′iθ∗)XiX ′i and E := F (θ̃)− F (θ∗). Intuitively, when θ̂ and θ∗ are close, elements in E are small. By the mean value theorem,\nE =\nn ∑\ni=1\n( µ̇(X ′i θ̃)− µ̇(X ′iθ∗) ) XiX ′ i =\nn ∑\ni=1\nµ̈(ri)X ′ i∆XiX ′ i\nfor some ri ∈ R. Since µ̈ ≤ Mµ and v ∈ [0, 1], for any x ∈ Rd \\ {0}, we have\nx′H−1/2EH−1/2x = (1 − v) n ∑\ni=1\nµ̈(ri)X ′ i∆ ∥ ∥ ∥ x′H−1/2Xi ∥ ∥ ∥ 2\n≤ n ∑\ni=1\nMµ ‖Xi‖ ‖∆‖ ∥ ∥ ∥ x′H−1/2Xi ∥ ∥ ∥ 2\n≤ Mµ ‖∆‖ ( x′H−1/2 ( n ∑\ni=1\nXiX ′ i\n)\nH−1/2x\n)\n≤ Mµ κ ‖∆‖ ‖x‖2 ,\nwhere we have used the assumption that ‖Xi‖ ≤ 1 for the second inequality. Therefore,\n∥ ∥ ∥ H−1/2EH−1/2 ∥ ∥ ∥ ≤ Mµ κ ‖∆‖ ≤ 4Mµσ κ2\n√\nd+ log(1/δ)\nλmin(V ) . (19)\nWhen λmin(V ) ≥ 64M2µσ2(d+ log(1/δ))/κ4, we have ∥\n∥ ∥ H−1/2EH−1/2\n∥ ∥ ∥ ≤ 1/2 . (20)\nNow we are ready to prove the theorem. For any x ∈ Rd,\nx′(θ̂ − θ∗) = x′(H + E)−1Z = x′H−1Z − x′H−1E(H + E)−1Z . (21)\nNote that the matrix (H + E) is nonsingular, so its inversion exists.\nFor the first term, {ǫi} are sub-Gaussian random variables with sub-Gaussian parameter σ. Define\nD := [X1, X2, . . . , Xn] ′ ∈ Rn×d\nto be the design matrix. Hoeffding inequality gives\nP{|x′H−1Z| ≥ t} ≤ 2 exp { − t 2\n2σ2 ‖x′H−1D′‖2\n}\n. (22)\nSinceH κV = κD′D, we have\n∥ ∥x′H−1D′ ∥ ∥ 2 = x′H−1D′DH−1x ≤ 1\nκ2 x′V −1x =\n1\nκ2 ‖x‖2V −1 ,\nso (22) implies\nP{|x′H−1Z| ≥ t} ≤ 2 exp { − t 2κ2\n2σ2 ‖x‖2V −1\n}\n.\nLet the right-hand side be 2δ and solve for t, we obtain that with probability at least 1− 2δ,\n|x′H−1Z| ≤ √ 2σ\nκ\n√\nlog(1/δ) ‖x‖V −1 . (23)\nFor the second term,\n|x′H−1E(H + E)−1Z| ≤ ‖x‖H−1 ∥ ∥ ∥ H−1/2E(H + E)−1Z ∥ ∥ ∥\n≤ ‖x‖H−1 ∥ ∥ ∥ H−1/2E(H + E)−1H1/2 ∥ ∥ ∥ ‖Z‖H−1\n≤ 1 κ ‖x‖V −1\n∥ ∥ ∥ H−1/2E(H + E)−1H1/2 ∥ ∥\n∥ ‖Z‖V −1 , (24)\nwhere the last inequality is due to the fact that H κV . Since (H + E)−1 = H−1 −H−1E(H + E)−1, we have ∥\n∥ ∥ H−1/2E(H + E)−1H1/2\n∥ ∥ ∥ = ∥ ∥ ∥ H−1/2E ( H−1 −H−1E(H + E)−1 ) H1/2 ∥ ∥ ∥\n= ∥ ∥ ∥ H−1/2EH−1/2 +H−1/2EH−1E(H + E)−1H1/2 ∥ ∥ ∥\n≤ ∥ ∥ ∥ H−1/2EH−1/2 ∥ ∥ ∥ + ∥ ∥ ∥ H−1/2EH−1/2 ∥ ∥ ∥ ∥ ∥ ∥ H−1/2E(H + E)−1H1/2 ∥ ∥ ∥ .\nBy solving this inequality, we get\n∥ ∥ ∥ H−1/2E(H + E)−1H1/2 ∥ ∥ ∥ ≤\n∥ ∥H−1/2EH−1/2 ∥ ∥\n1− ∥ ∥H−1/2EH−1/2 ∥ ∥\n≤ 2 ∥ ∥ ∥ H−1/2EH−1/2 ∥ ∥ ∥ ≤ 8Mµσ\nκ2\n√\nd+ log(1/δ)\nλmin(V ) ,\nwhere we have used (20) and (19) in the second and third inequalities, respectively. Combining it with (24) and the bound in EG, we have\n|x′H−1E(H + E)−1Z| ≤ 32Mµσ 2 κ3 d+ log(1/δ) √ λmin(V ) ‖x‖V −1 . (25)\nFrom (21), (23) and (25), one can see that (5) holds as long as the lower bound (4) for λmin(V ) holds. Finally, an application of a union bound on two small-probability events (given in Lemma 7 and (23), respectively) asserts that (5) holds with probability at least 1− 3δ."
    }, {
      "heading" : "B. Proof of Proposition 1",
      "text" : "In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore, Vn is denoted V and so on.\nLet X be a random vector drawn from the distribution ν. Define Z := Σ−1/2X . Then Z is isotropic, namely, E[ZZ ′] = Id. Define U = ∑n t=1 ZtZ ′ t = Σ\n−1/2VΣ−1/2. From Lemma 1, we have that, for any t, with probability at least 1− 2 exp(−C2t2),\nλmin(U) ≥ n− C1σ2 √ nd− σ2t√n .\nwhere σ is the sub-Gaussian parameter of Z , which is upper-bounded by ∥ ∥Σ−1/2 ∥ ∥ = λ −1/2 min (Σ) (see, e.g., Vershynin (2012)). We thus can rewrite the above inequality (which holds with probability 1− δ as\nλmin(U) ≥ n− λ−1min(Σ) ( C1σ 2 √ nd+ t √ n ) .\nWe now bound the minimum eigenvalue of V , as follows:\nλmin(V ) = min x∈Bd\nx′V x\n= min x∈Bd\nx′Σ1/2UΣ1/2x\n≥ λmin(U) min x∈Bd x′Σx\n= λmin(U)λmin(Σ) ≥ λmin(Σ) ( n− λ−1min(Σ)(C1σ2 √ nd+ t √ n) ) = λmin(Σ)n− C1 √ nd− C2 √ n log(1/δ) .\nFinally, it can be verified (Lemma 9) that the last expression above is no less than B as long as\nn ≥ (\nC1 √ d+ C2 √ log(1/δ)\nλmin(Σ)\n)2\n+ 2B\nλmin(Σ) ,\nfinishing the proof."
    }, {
      "heading" : "C. Technical Lemmas and Proofs",
      "text" : ""
    }, {
      "heading" : "C.1. Proof of Lemma 7",
      "text" : "Noting that\n‖Z‖V −1 = ‖V −1/2Z‖2 = sup ‖a‖2≤1 〈a, V −1/2Z〉,\nlet B̂ be a 1/2-net of the unit ball Bd. Then |B̂| ≤ 6d (Pollard, 1990, Lemma 4.1), and for any x ∈ Bd, there is a x̂ ∈ B̂ such that ‖x− x̂‖ ≤ 1/2. Consequently,\n〈x, V −1/2Z〉 = 〈x̂, V −1/2Z〉+ 〈x− x̂, V −1/2Z〉\n= 〈x̂, V −1/2Z〉+ ‖x− x̂‖ 〈 x− x̂‖x− x̂‖ , V −1/2Z〉\n≤ 〈x̂, V −1/2Z〉+ 1 2 sup z∈Bd 〈z, V −1/2Z〉.\nTaking supremum on both sides, we get\nsup x∈Bd 〈x, V −1/2Z〉 ≤ 2max x̂∈B̂ 〈x̂, V −1/2Z〉 .\nThen a union bound argument implies\nP {‖Z‖V −1 > t} ≤ P {\nmax x̂∈B̂\n〈x̂, V −1/2Z〉 > t/2 }\n≤ ∑\nx̂∈B̂\nP\n{ 〈x̂, V −1/2Z〉 > t/2 }\n≤ ∑\nx̂∈B̂\nexp\n{\n− t 2\n8σ2 ∥ ∥x̂′V −1/2X ′ ∥ ∥ 2\n}\n≤ exp { −t2/(8σ2) + d log 6 } ,\nwhere we have used Hoeffding’s inequality for the third inequality and |B̂| ≤ 6d for the last inequality. A choice of t = 4σ √ d+ log(1/δ) completes the proof."
    }, {
      "heading" : "C.2. Proof of Lemma 2",
      "text" : "By Abbasi-Yadkori et al. (2011, Lemma 11), we have\nm+n ∑\nt=m+1\n‖Xt‖2V −1t ≤ 2 log detVm+n+1 detVm+1 ≤ 2d log ( tr (Vm+1) + n d ) − 2 log detVm+1 .\nNote that tr (Vm+1) = ∑m t=1 tr (XtX ′ t) = ∑m t=1 ‖Xt‖2 ≤ m and that det Vm+1 = ∏d i=1 λi ≥ λdmin(Vm+1) ≥ 1, where {λi} are the eigenvalues of Vm+1. Applying Cauchy-Schwartz inequality yields m+n ∑\nt=m+1\n‖Xt‖V −1t ≤\n√ √ √ √n m+n ∑\nt=m+1\n‖Xt‖2V −1t ≤ √ 2nd log ( n+m\nd\n)\n."
    }, {
      "heading" : "C.3. Proof of Lemma 3",
      "text" : "Define Gt(θ) = ∑t−1 i=1(µ(X ′ iθ) − µ(X ′iθ∗))Xi and Zt = ∑t−1 i=1 ǫiXi. Following the same argument as in the proof of Theorem 1, we have Gt(θ̂t) = Zt and ‖Gt(θ)‖2V −1t ≥ κ 2‖θ − θ∗‖2Vt (26) for any θ ∈ {θ : ‖θ − θ∗‖ ≤ 1}. Combining (26) with the following lemma and the equality Zt = Gt(θ̂t) completes the proof. Lemma 8. Suppose there is an integerm such that λmin(Vm) ≥ 1, then for any δ ∈ (0, 1), with probability at least 1− δ, for all t > m,\n‖Zt‖2V −1t ≤ 4σ 2\n(\nd 2 log(1 + 2t/d) + log(1/δ)\n)\n.\nProof. For convenience, fix t such that t > m, and denote Vt and Zt by V and Z , respectively. Furthermore, define V̄ := V + λI and let 1 be the vector of all 1s. It is easy to observe that\n‖Z‖2V −1 = ‖Z‖ 2 V̄ −1 + Z ′(V −1 − V̄ −1)Z . (27) We start with bounding the second term. The ShermanMorrison formula gives\nV̄ −1 = V −1 − λV −2\n1 + λ1′V −11 .\nSince 1′V −11 ≥ 0, the above implies that 0 ≤ Z ′(V −1 − V̄ −1)Z\n≤ λZ ′V −2Z ≤ λ ∥ ∥V −1 ∥\n∥ ‖Z‖2V −1\n= λ\nλmin(V ) ‖Z‖2V −1 .\nSince λmin(V ) ≥ λmin(Vm) ≥ 1, we now have\n0 ≤ Z ′(V −1 − V̄ −1)Z ≤ λ ‖Z‖2V −1 .\nThe above inequality together with (27) implies that\n‖Z‖2V −1 ≤ (1− λ)−1 ‖Z‖ 2 V̄ −1 .\nThe proof can be finished by applying Theorem 1 and Lemma 10 from Abbasi-Yadkori et al. (2011) to bound ‖Z‖2V̄ −1 , using λ = 1/2."
    }, {
      "heading" : "C.4. Proof of Lemma 6",
      "text" : "We will prove the first part of the lemma by induction. It is easy to check the lemma holds for s = 1. Suppose we have a∗t ∈ As and we want to prove a∗t ∈ As+1. Since the algorithm proceeds to stage s+ 1, we know from step 2b that\n|m(s)t,a − x′t,aθ∗| ≤ w(s)t,a ≤ 2−s\nfor all a ∈ As. Specially, it holds for a = a∗t because a∗t ∈ As by our induction step. Then the optimality of a∗t implies\nm (s) t,a∗t ≥ x′t,a∗t θ ∗ − 2−s ≥ x′t,aθ∗ − 2−s ≥ m(s)t,a − 2 · 2−s\nfor all a ∈ As. Thus we have a∗t ∈ As+1 according to step 2d. Suppose at is selected at stage st in step 2b. If st = 1, obviously the lemma holds because 0 ≤ µ(x) ≤ 1 for all x. If st > 1, since we have proved a ∗ t ∈ Ast , again step 2b at stage st − 1 implies\n|m(st−1)t,a − x′t,aθ∗| ≤ 2−st+1\nfor a = at and a = a ∗ t . Step 2d at stage st − 1 implies\nm (st−1) t,a∗t −m(st−1)t,at ≤ 2 · 2−st+1 .\nCombining above two inequalities, we get\nx′t,atθ ∗ ≥ m(st−1)t,at − 2−st+1 ≥ m (st−1) t,a∗t − 3 · 2−st+1 ≥ x′t,a∗t θ ∗ − 4 · 2−st+1 .\nWhen at is selected in step 2c, sincem (st) t,at ≥ m (st) t,a∗t , we have\nx′t,atθ ∗ ≥ m(st)t,at − 1/ √ T ≥ m(st)t,a∗t − 1/ √ T ≥ x′t,a∗t θ ∗ − 2/ √ T .\nUsing the fact that µ(x1)− µ(x2) ≤ Lµ(x1 − x2) for x1 ≥ x2, we will get the desired result."
    }, {
      "heading" : "C.5. Proof of Lemma 9",
      "text" : "Lemma 9. Let a and b be two positive constants. Ifm ≥ a2 + 2b, thenm− a√m− b ≥ 0.\nProof. Supposem ≥ a2 + 2b, then\nm− a√m− b = a2 + b− a √ a2 + 2b\n≥ a2 + b− a √ a2 + 2b+ b2/a2 = a2 + b− a √ (a+ b/a)2 = a2 + b− a(a+ b/a) = 0 ."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Abbasi-Yadkori", "Yasin", "Pál", "Dávid", "Szepesvári", "Csaba" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2011
    }, {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Agarwal", "Alekh", "Hsu", "Daniel", "Kale", "Satyen", "Langford", "John", "Li", "Lihong", "Schapire", "Robert E" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Online models for content optimization",
      "author" : [ "Agarwal", "Deepak", "Chen", "Bee-Chung", "Elango", "Pradheep", "Motgi", "Nitin", "Park", "Seung-Taek", "Ramakrishnan", "Raghu", "Roy", "Scott", "Zachariah", "Joe" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2009
    }, {
      "title" : "Using confidence bounds for exploitation-exploration trade-offs",
      "author" : [ "Auer", "Peter" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Auer and Peter.,? \\Q2003\\E",
      "shortCiteRegEx" : "Auer and Peter.",
      "year" : 2003
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Contextual bandit algorithms with supervised learning guarantees",
      "author" : [ "Beygelzimer", "Alina", "Langford", "John", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2010
    }, {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "Bickel", "Peter J", "Ritov", "Ya’acov", "Tsybakov", "Alexandre B" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Bickel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel et al\\.",
      "year" : 2009
    }, {
      "title" : "Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory, volume 9 of Lecture Notes-Monograph Series",
      "author" : [ "Brown", "Lawrence D" ],
      "venue" : "Institute of Mathematical Statistics,",
      "citeRegEx" : "Brown and D.,? \\Q1986\\E",
      "shortCiteRegEx" : "Brown and D.",
      "year" : 1986
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Bubeck", "Sébastien", "Cesa-Bianchi", "Nicolo" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2012
    }, {
      "title" : "An empirical evaluation of Thompson sampling",
      "author" : [ "Chapelle", "Olivier", "Li", "Lihong" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2012
    }, {
      "title" : "Strong consistency of maximum quasi-likelihood estimators in generalized linear models with fixed and adaptive designs",
      "author" : [ "Chen", "Kani", "Hu", "Inchi", "Ying", "Zhiliang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Chen et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 1999
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "Chu", "Wei", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Chu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "Dani", "Varsha", "Hayes", "Thomas P", "Kakade", "ShamM" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Consistency and asymptotic normality of the maximum likelihood estimator in generalized linear models",
      "author" : [ "Fahrmeir", "Ludwig", "Kaufmann", "Heinz" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Fahrmeir et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Fahrmeir et al\\.",
      "year" : 1985
    }, {
      "title" : "Parametric bandits: The generalized linear case",
      "author" : [ "Filippi", "Sarah", "Cappe", "Olivier", "Garivier", "Aurélien", "Szepesvári", "Csaba" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Filippi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Filippi et al\\.",
      "year" : 2010
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "Lai", "Tze Leung", "Robbins", "Herbert" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 1985
    }, {
      "title" : "Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems",
      "author" : [ "Lai", "Tze Leung", "Wei", "Ching Zong" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Lai et al\\.,? \\Q1982\\E",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 1982
    }, {
      "title" : "Theory of Point Estimation, volume 31 of Springer Texts in Statistics",
      "author" : [ "Lehmann", "Erich Leo", "Casella", "George" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Lehmann et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Lehmann et al\\.",
      "year" : 1998
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "Li", "Lihong", "Chu", "Wei", "Langford", "John", "Schapire", "Robert E" ],
      "venue" : "In Proceedings of the 19th International Conference on World Wide Web (WWW),",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "An unbiased offline evaluation of contextual bandit algorithms with generalized linear models",
      "author" : [ "Li", "Lihong", "Chu", "Wei", "Langford", "John", "Moon", "Taesup", "Wang", "Xuanhui" ],
      "venue" : "JMLR Workshop and Conference Proceedings,",
      "citeRegEx" : "Li et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2012
    }, {
      "title" : "Generalized Linear Models, volume 37",
      "author" : [ "McCullagh", "Peter", "Nelder", "John A" ],
      "venue" : "CRC press,",
      "citeRegEx" : "McCullagh et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "McCullagh et al\\.",
      "year" : 1989
    }, {
      "title" : "Empirical processes: Theory and applications. In NSF-CBMS regional conference series in probability and statistics, pp. i–86",
      "author" : [ "Pollard", "David" ],
      "venue" : "JSTOR,",
      "citeRegEx" : "Pollard and David.,? \\Q1990\\E",
      "shortCiteRegEx" : "Pollard and David.",
      "year" : 1990
    }, {
      "title" : "Linearly parameterized bandits",
      "author" : [ "Rusmevichientong", "Paat", "Tsitsiklis", "John N" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Rusmevichientong et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rusmevichientong et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning to optimize via posterior sampling",
      "author" : [ "Russo", "Daniel", "Van Roy", "Benjamin" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Russo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russo et al\\.",
      "year" : 2014
    }, {
      "title" : "One-armed bandit problems with covariates",
      "author" : [ "Sarkar", "Jyotirmoy" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Sarkar and Jyotirmoy.,? \\Q1991\\E",
      "shortCiteRegEx" : "Sarkar and Jyotirmoy.",
      "year" : 1991
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "Thompson", "William R" ],
      "venue" : null,
      "citeRegEx" : "Thompson and R.,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson and R.",
      "year" : 1933
    }, {
      "title" : "Spectral bandits for smooth graph functions",
      "author" : [ "Valko", "Michal", "Munos", "Rémi", "Kveton", "Branislav", "Kocák", "Tomáš" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Valko et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Valko et al\\.",
      "year" : 2014
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "Vershynin", "Roman" ],
      "venue" : "Compressed Sensing: Theory and Applications,",
      "citeRegEx" : "Vershynin and Roman.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vershynin and Roman.",
      "year" : 2012
    }, {
      "title" : "A one-armed bandit problem with a concomitant variable",
      "author" : [ "Woodroofe", "Michael" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Woodroofe and Michael.,? \\Q1979\\E",
      "shortCiteRegEx" : "Woodroofe and Michael.",
      "year" : 1979
    }, {
      "title" : "θ∗‖ , where the last inequality is due to the fact that F (θ̄",
      "author" : [ "≥ κηλmin(V ) ‖θ" ],
      "venue" : "κηV . On the other hand, Lemma A of Chen et al",
      "citeRegEx" : "−,? \\Q1999\\E",
      "shortCiteRegEx" : "−",
      "year" : 1999
    }, {
      "title" : "Z〉, let B̂ be a 1/2-net of the unit ball B. Then |B̂| ≤ 6 (Pollard, 1990, Lemma 4.1), and for any x ∈ B, there is a x̂ ∈ B̂ such that ‖x− x̂",
      "author" : [ "V 〈a" ],
      "venue" : null,
      "citeRegEx" : ".a,? \\Q1990\\E",
      "shortCiteRegEx" : ".a",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search (Agarwal et al., 2009; Li et al., 2010; 2012).",
      "startOffset" : 159,
      "endOffset" : 204
    }, {
      "referenceID" : 19,
      "context" : "With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search (Agarwal et al., 2009; Li et al., 2010; 2012).",
      "startOffset" : 159,
      "endOffset" : 204
    }, {
      "referenceID" : 13,
      "context" : "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.",
      "startOffset" : 76,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.",
      "startOffset" : 76,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.",
      "startOffset" : 76,
      "endOffset" : 191
    }, {
      "referenceID" : 20,
      "context" : "Logistic regression model based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012).",
      "startOffset" : 111,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : ", 2002a; Bubeck & Cesa-Bianchi, 2012) to linear bandits (Auer, 2003; Abbasi-Yadkori et al., 2011).",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 20,
      "context" : "While some UCB-type algorithms using GLMs perform well empirically (Li et al., 2012), there is little theoretical study of them.",
      "startOffset" : 67,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : ", 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector. The linear model is theoretically convenient to work on. However, in practice, we usually have binary rewards (click or not, treatment working or not). Logistic regression model based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012). Hence, we consider generalized linear models (GLM) in the contextual bandit setting, in which linear, logistic and probit regression serve as three important special cases. The celebrated work of Lai & Robbins (1985) first introduces the upper confidence bound (UCB) approach to efficient exploration.",
      "startOffset" : 8,
      "endOffset" : 640
    }, {
      "referenceID" : 6,
      "context" : "Later variants of the EXP4 algorithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give an Õ( √ dKT ) regret that is near-optimal with respect to T .",
      "startOffset" : 37,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "Later variants of the EXP4 algorithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give an Õ( √ dKT ) regret that is near-optimal with respect to T .",
      "startOffset" : 37,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "This rate improves the state-of-the-art results of Filippi et al. (2010) by a √ d factor, assuming the number of arms is fixed.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "This rate improves the state-of-the-art results of Filippi et al. (2010) by a √ d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem’s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits.",
      "startOffset" : 51,
      "endOffset" : 313
    }, {
      "referenceID" : 10,
      "context" : "This rate improves the state-of-the-art results of Filippi et al. (2010) by a √ d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem’s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits. A key observation in proving this result is that the l2 confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in all directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To our best knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice. Related Work The study of GLM bandits problem goes back at least to Sarkar (1991), who considered discounted regrets rather than cumulative regerts.",
      "startOffset" : 51,
      "endOffset" : 1229
    }, {
      "referenceID" : 10,
      "context" : "This rate improves the state-of-the-art results of Filippi et al. (2010) by a √ d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem’s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits. A key observation in proving this result is that the l2 confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in all directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To our best knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice. Related Work The study of GLM bandits problem goes back at least to Sarkar (1991), who considered discounted regrets rather than cumulative regerts. They prove that a myopic rule without exploration is asymptotically optimal. Recently, Filippi et al. (2010) study the same stochastic GLM bandit problem considered here.",
      "startOffset" : 51,
      "endOffset" : 1405
    }, {
      "referenceID" : 15,
      "context" : "Note that this assumption is weaker than Assumption 1 in Filippi et al. (2010), as it only requires to control the local behavior of μ̇(xθ) near θ.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : ", 2009) and generalized linear models (Fahrmeir & Kaufmann, 1985; Chen et al., 1999).",
      "startOffset" : 38,
      "endOffset" : 84
    }, {
      "referenceID" : 30,
      "context" : "We give a proof sketch here, and the full proof is found in the appendix. In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore,Vn is denotedV and so on. We will need a technical lemma, which is an existing result in randommatrix theory. The version we presented here is adapted from Equation (5.23) of Theorem 5.39 from Vershynin (2012). Lemma 1.",
      "startOffset" : 7,
      "endOffset" : 385
    }, {
      "referenceID" : 30,
      "context" : "∥ = λ −1/2 min (Σ) (see, e.g., Vershynin (2012)).",
      "startOffset" : 6,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).",
      "startOffset" : 28,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).",
      "startOffset" : 28,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).",
      "startOffset" : 28,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : ", the distribution ν) is only needed to ensure Vτ+1 is invertable (similar to the first phase in the algorithm of Filippi et al. (2010)); the rest of our analysis does not depend on this stochastic assumption.",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : ", Abbasi-Yadkori et al. (2011)).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "We instead use results on self-normalized martingales (Abbasi-Yadkori et al., 2011), together with a finitetime normality result like Theorem 1, to prove the next theorem.",
      "startOffset" : 54,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008).",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008). By choosing δ = 1/T and using the fact that RT ≤ T , this highprobability result implies a bound on the expected regret: E[RT ] = Õ(d √ T ). Our result improves the previous regret bound of Filippi et al. (2010) by a √ logT factor.",
      "startOffset" : 126,
      "endOffset" : 358
    }, {
      "referenceID" : 13,
      "context" : "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008). By choosing δ = 1/T and using the fact that RT ≤ T , this highprobability result implies a bound on the expected regret: E[RT ] = Õ(d √ T ). Our result improves the previous regret bound of Filippi et al. (2010) by a √ logT factor. Moreover, the algorithm proposed in Filippi et al. (2010) involves a projection step, which is computationally more expensive comparing to UCB-GLM.",
      "startOffset" : 126,
      "endOffset" : 436
    }, {
      "referenceID" : 20,
      "context" : "Algorithm SupCB-GLM While the algorithm UCB-GLM performs sufficiently well in practice (Li et al., 2012), it is unclear whether it can achieve the optimal rates of O( √ dT logK), when K is small.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "Algorithm SupCB-GLM While the algorithm UCB-GLM performs sufficiently well in practice (Li et al., 2012), it is unclear whether it can achieve the optimal rates of O( √ dT logK), when K is small. As mentioned in Section 4.1, the key technical difficulty in analyzing UCB-GLM is the dependence between samples. Inspired by a technique developed by Auer (2003) to create independent samples for linear contextual bandits, we propose another algorithm SupCB-GLM (Algorithm 3), which uses algorithm CB-GLM (Algorithm 2) as a subroutine.",
      "startOffset" : 88,
      "endOffset" : 359
    }, {
      "referenceID" : 27,
      "context" : "Also, we would like to point out that, unlike SpectralEliminator (Valko et al., 2014), the algorithm can easily handle a changing action set.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 29,
      "context" : "If w (s) t,a > 2 −s for some a, we need to do more exploration on xt,a and thus we choose this action. Otherwise, the actions are filtered in step 2d such that the actions passed to the next stage are close enough to the optimal action. Since all the widths are smaller than 2, if m (s) t,a < m (s) t,j − 2 · 2 for some j ∈ As, the action a can not be the optimal action. The filter process terminates when we have already got accurate estimate of all xt,aθ ∗ up to the 1/ √ T level and we do not need to do exploration. Thus in step 2c we just choose the action that maximizes the estimated mean value. Our algorithm is different from the algorithm SupLinRel in Auer (2003) that we directly maximize the mean, rather than the upper confidence bound, in steps c and d.",
      "startOffset" : 17,
      "endOffset" : 675
    }, {
      "referenceID" : 15,
      "context" : "Both in Theorem 2 and in Filippi et al. (2010), |x(θ̂n − θ∗)| is upper bounded by using the Cauchy-Schwartz inequality, |x(θ̂n − θ)| ≤ ‖x‖V −1 n ∥",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 31,
      "context" : "This will lead to an extra √ d factor compared to (5). By using Cauchy-Schwartz (10), we only make use of the fact that θ̂n is close to θ ∗ in the l2 sense. However, (5) tells us that actually θ̂n is close to θ ∗ in every direction. This is the reason why we are able to remove the extra √ d factor to achieve a near-optimal regret. It also explains why the bound in Theorem 2 is tight when K is large. As K goes large, it is likely there is a direction x for which (10) is tight. Proof of Theorem 3. To facilitate our proof, we first present two technical lemmas. Lemma 4, Theorem 1, Theorem 5.39 of Vershynin (2012) together with a union bound yield Lemma 5.",
      "startOffset" : 11,
      "endOffset" : 618
    } ],
    "year" : 2017,
    "abstractText" : "Contextual bandits are widely used in Internet services from news recommendation to advertising. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an Õ( √ dT ) regret over T rounds with d dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a √ d factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximumlikelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for the certain cases.",
    "creator" : "LaTeX with hyperref package"
  }
}