{
  "name" : "1703.01697.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Principles and Examples of Plausible Reasoning and Propositional Plausible Logic",
    "authors" : [ "David Billington" ],
    "emails" : [ "d.billington@griffith.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n01 69\n7v 2\n[ cs\n.A I]\n3 A\npr 2\n01 7"
    }, {
      "heading" : "1. Introduction",
      "text" : "We are interested in reasoning about situations that (a) have imprecisely defined parts, and (b) this lack of precision is not quantified. That is, there are no degrees or layers or levels of precision, and in particular there are no numbers like probabilities, that would quantify the lack of precision. These situations are often indicated by the ordinary, rather than technical, use of words such as ‘mostly’, ‘usually’, ‘typically’, ‘normally’, ‘probably’, ‘likely’, ‘plausible’, ‘believable’, and ‘reasonable’. Although these words are not synonymous, they share a common property, which may be expressed by using either frequency of occurrence or weight of evidence. In frequency terms the property is that something is true more often than not; in evidence terms the property is that the evidence for something outweighs the evidence against it. An example is ‘Mammals usually are non-venomous’.\nWe shall call these situations plausible-reasoning situations because we shall call the reasoning used in such situations plausible reasoning.\nThis article has two aims. The first is to introduce principles that give a much clearer understanding of what it means for a formal logic to do plausible reasoning; that is the kind of reasoning indicated above. The hope is that this set of principles is comprehensive. Whether it is or not, this seems to be the first such set of principles even though plausible reasoning has been used for at least 2500 years (Walton, Tindale, & Gordon, 2014). However on page 114 of (Walton et al., 2014) there is a list of 11 characteristics of plausible reasoning, rather than characteristics of formal logics that do plausible reasoning.\nThe second aim is to define a propositional logic, called Propositional Plausible Logic (PPL), that satisfies all these principles of plausible reasoning. This shows that all the principles together are consistent; that is, there is no principle whose negation is implied by all the other principles. A pruned version of PPL is presented in (Billington, 2015).\nIn this paper we shall be considering only propositional situations; that is, situations that can be fully represented by a propositionally adequate language. That is, by a language\nwhich has an adequate set of propositional connectives. The most common adequate sets of connectives contain negation and at least one of conjunction, or disjunction, or material implication. The connectives we shall use are negation ¬, conjunction ∧ , and disjunction ∨ . A propositionally adequate logic is a logic based on a propositionally adequate language. Moreover we shall consider only those plausible-reasoning situations that can be specified by a plausible-structure S = (Fact(S),Plaus(S)) where Fact(S) is a set of propositional formulas representing the factual part of S, and Plaus(S) is a set representing the plausible part of S. The elements of Plaus(S) can have a variety of forms; for example: defaults are used in Reiter’s Default Logic (Reiter, 1980), defeasible rules are used in ASPIC (Caminada & Amgoud, 2007) and ASPIC+ (Modgil & Prakken, 2013), and defeasible and warning rules are used in Defeasible Logic (Billington, 2008). The plausible-structure syntax is very general, while being specific enough to permit the definition of concepts needed later.\nThis article is organised into the following sections. The next section defines some ideas and notation from classical propositional logic that are needed in Sections 3 and 5. Section 3 presents the principles of plausible reasoning. Section 4 contains a survey of various nonnumeric non-monotonic logics. The definition of PPL is in Section 5. In Section 6 we apply PPL to some examples. In Section 7 we state and discuss some important properties of PPL, and show that PPL satisfies all the principles in Section 3. Section 8 is the conclusion. All the proofs are in the appendices."
    }, {
      "heading" : "2. A Classical Propositional Logic using Resolution",
      "text" : "Formulas in classical propositional logics are usually defined using sequences, for example (a ∨ (b ∨ a)). Sequences make unwanted distinctions which are often best removed, for example neither order nor repetitions are needed. So the above example is more clearly written as ∨ {a, b}. We shall define classical propositional formulas that are based on sets rather than sequences. Such set-based formulas simplify the definition of resolution. The classical notions of truth value, valuation, satisfaction, semantic consequence |=, tautology, contradiction, equivalence of formulas, and resolution are as usual.\nLet us start by agreeing on some notation. As usual ‘iff’ abbreviates ‘if and only if’. X is a subset of Y is denoted by X⊆Y ; the notation X⊂Y means X⊆Y and X 6=Y , and denotes that X is a strict subset of Y . The empty set is denoted by {}, and the set of all integers by Z. The cardinality of a set S is denoted by |S|. If m and n are integers then we define the integer interval [m..n] by [m..n] = {i∈Z : m ≤ i ≤ n}.\nOur alphabet is the union of the following pairwise disjoint sets of symbols: a nonempty countable set, Atm, of (propositional) atoms; the set {¬, ∧ , ∨ } of connectives with ¬, ∧ , ∨ denoting negation, conjunction, and disjunction respectively; and the set of punctuation marks consisting of the comma and both braces. We now define a formula.\nDefinition 2.1. 1) If a is an atom then a is a formula. 2) If f is a formula then ¬f is a formula. 3) If F is a finite set of formulas then ∧ F is a formula and ∨ F is a formula. 4) Every formula can be built by a finite number of applications of (1), (2), and (3). The set of all formulas is denoted by Fml .\nIt is convenient to write ∧ F and ∨ F even though the set F of formulas may be infinite.\nThe next three definitions define some special formulas, the set of all literals in a clause or dual-clause, and the complement of a literal.\nDefinition 2.2. 1) The set, Lit , of all literals is defined by Lit = Atm ∪ {¬a : a∈Atm}. 2) A clause is either a literal or the disjunction, ∨ L, of a finite set, L, of literals. 3) ∨ {} is the empty clause or falsum. 4) A dual-clause is either a literal or the conjunction, ∧ L, of a finite set, L, of literals. 5) ∧ {} is the empty dual-clause or verum. 6) A formula is contingent iff it is not a tautology and it is not a contradiction.\nDefinition 2.3. 1) If l is a literal then Lit(l) = {l}. 2) If L is a finite set of literals then Lit( ∨ L) = L = Lit( ∧ L).\nDefinition 2.4. Let a be any atom and L be any set of literals. 1) The complement of a, ∼a, is defined by ∼a = ¬a. 2) The complement of ¬a, ∼¬a, is defined by ∼¬a = a. 3) The complement of L, ∼L, is defined by ∼L = {∼l : l∈L}.\nLet C be any set of clauses. We want to remove from C all the clauses that, when removed, will not change the truth value of ∧ C. By Lemma A.1(1) (See Appendix A) this means removing all tautologies and all clauses that have a strict subclause in C. We also want to simplify C by replacing any clause ∨ {l} in C by l. The result will be the core of C.\nDually, let D be any set of dual-clauses. We want to remove from D all the dualclauses that, when removed, will not change the truth value of ∨ D. By Lemma A.1(2) (See Appendix A) this means removing all contradictions and all dual-clauses that have a strict sub-dual-clause in D. We also want to simplify D by replacing any dual-clause ∧ {l} in D by l. The result will be the core of D. The following definition does both of these.\nDefinition 2.5. Let G be either a set of clauses or a set of dual-clauses. 1) The set of elements of G that are contingent or empty, Ctge(G), is defined by\nCtge(G) = {g∈G : g is contingent or empty}. 2) The set of minimal elements of G, Min(G), is defined by\nMin(G) = {g∈G : if g′∈G then Lit(g′) 6⊂Lit(g)}. 3) The simplification of the formula f , smp(f), is defined as follows.\nIf f ∈{ ∧ {g}, ∨ {g}}, where g is a formula, then smp(f) = smp(g); else smp(f) = f .\n4) The simplification of G, Smp(G), is defined by Smp(G) = {smp(g) : g∈G}. 5) The core of G, Cor(G), is defined by Cor (G) = Smp(Min(Ctge(G))).\nLet C be any set of clauses. The set of all clauses derivable by resolution from clauses in C is denoted by Res(C). Also we usually abbreviate Cor (Res(C)) to CorRes(C), and Smp(Res(C)) to SmpRes(C).\nWe shall need to convert a formula f into a set Claus(f) of clauses, such that ∧ Claus(f) is equivalent to f . Unfortunately there are many such sets of clauses, so we shall follow Sections 2 and 3 of Chapter I of (Nerode & Shore, 1997) to define which set we mean.\nWe shall denote the true truth value by T and the false truth value by F. Let Val denote the set of all valuations, and Atm(f) denote the set of atoms in the formula f .\nDefinition 2.6. If A is a set of atoms then define Val(A), the set of valuations which are false outside A, by Val(A) = {v∈Val : for all a in Atm−A, v(a) = F}.\nSo if |A| = n then |Val(A)| = 2n.\nDefinition 2.7. Let f be any formula, F any set of formulas, and v any valuation. 1) Define L(f, v) = {a : a∈Atm(f) and v(a) = T} ∪ {¬a : a∈Atm(f) and v(a) = F}. 2) Define Claus(f) = { ∨ ∼L(f, v) : v∈Val(Atm(f)) and v(f) = F}. 3) Define Claus(F ) = ⋃ {Claus(f) : f ∈F}.\nIf a set F of formulas is unsatisfiable then classical propositional logic can prove any formula from F ; that is, classical propositional logic is explosive. Explosive logics are not ideal because, for unsatisfiable sets of formulas, the idea of ‘proof’ becomes worthless. For example, let F1 = {a,¬a, b}. Then from F1 an explosive logic can prove ¬b, which does not seem sensible. Logics that are not explosive are called paraconsistent logics. But we want more than mere paraconsistency. In the example above, either a or ¬a seems to be a mistake, so it would be reasonable that from F1 we can conclude only b, and what follows from b.\nLet F be an unsatisfiable set of formulas. There are several ways of getting a satisfiable subset of F (see the literature on Belief Revision and Paraconsistent Logics). One way is to take the intersection of all the maximal satisfiable subsets of F , (this is the full meet contraction of F by a contradiction). An equivalent way is to remove all the minimal unsatisfiable subsets of F . For example, let F2 = {a,¬a, ∨ {a, b}}. Then the (only) minimal unsatisfiable subset of F2 is {a,¬a}. Removing this from F2 leaves { ∨ {a, b}}. Alternatively the intersection of all the maximal satisfiable subsets of F2 is also { ∨ {a, b}}.\nHowever, another way to get a satisfiable subset of F is to remove all the formulas of F that are ‘contaminated’ by potential errors, as follows. First we convert F to the set of clauses Claus(F ). Then F is unsatisfiable iff Claus(F ) is unsatisfiable iff Res(Claus(F )) contains a literal, say l, and its complement, ∼l. At least one of l and ∼l is an error. Certainly both l and ∼l are potential errors. Potential errors contaminate any clause containing them, making the clause unreliable. We then remove all the contaminated clauses from Claus(F ) to get the result Sat(Claus(F )).\nApplying this to F2 we see that Claus(F2) = { ∨ {a}, ∨ {¬a}, ∨ {a, b}} and the set, Err(Claus(F2)), of potential error literals of Claus(F2) is Err(Claus(F2)) = {a,¬a}. Hence very clause in Claus(F2) is contaminated by a potential error, and so Sat(Claus(F2)) = {}.\nThe formal definition of the functions Err(.) and Sat(.) follows.\nDefinition 2.8. Let C be any set of clauses. Err(C) = {l∈Lit : {l,∼l}⊆SmpRes(C)}. Sat(C) = {c∈C : c 6= ∨ {} and Lit(c)∩Err (C) = {}}.\nOf course if C is satisfiable then Sat(C) = C. If C is any set of clauses then Sat(C) is satisfiable, and so Sat(Sat(C)) = Sat(C).\nLet C be a set of clauses. It can be shown that every literal in every clause in every minimal unsatisfiable subset of C is a potential error literal and so is in Err(C). But Sat(C)\nremoves all the clauses that contain any potential error literal, not just the clauses that are composed entirely of potential error literals. Hence the clauses in Sat(C) may be regarded as at least as reliable as the clauses in the intersection of all the maximal satisfiable subsets of C, as some of these clauses may contain potential error literals, as happens with F2.\nWe can now define our paraconsistent propositional logic by mimicking the standard definition of proof by resolution, which we shall also define.\nDefinition 2.9. Let F be any set of formulas and f be any formula. As usual, define F proves f (by resolution), denoted by F ⊢ f , as follows. F ⊢ f iff ∨ {} ∈ Res(Claus({¬f}∪F )). Define F judiciously proves f , denoted by F f , as follows. F f iff ∨ {} ∈ Res(Claus(¬f)∪Sat(Claus(F ))).\nExplosiveness is a symptom of the fact that, from a set F of formulas, classical logic proves formulas that, arguably, do not follow from F . For example, if a and b are different atoms then from the contradiction ∧ {a,¬a} we can prove b. Because b has nothing to do with a or ¬a, our intuition is that b does not follow from ∧ {a,¬a}. Although ‘follows from’ is an intuitive concept rather than a formal one, we shall attempt to formally define the concept. To refine our intuition let us consider tautologies.\nLet F be a set of formulas, f and g be formulas, L and M be finite sets of literals, and t be a tautology.\nA) We could argue that tautologies stand on their own, they do not depend on any other formula. So if t /∈F then t does not follow from F . B) We would like ‘follows from’ to be syntax independent. That is, if f follows from F and f is equivalent to g then g follows from F . C) If f ∈F then it seems reasonable that f follows from F . D) If ∨ L follows from F and L ⊆M then it seems reasonable that ∨ M follows from F . E) If a and b are different atoms then it seems reasonable that ∨ {a,¬a} does not follow\nfrom { ∨ {b,¬b}}. By (C), ∨ {b,¬b} follows from { ∨ {b,¬b}}. But this would make ‘follows from’ syntax dependent, contrary to (B). So tautologies present difficulties for any definition of ‘follows from’. However, in Section 3 we do not want to force all tautologies to be provable, so we shall declare that tautologies do not follow from any set of formulas. Thus we arrive at the following definition.\nDefinition 2.10. Let F be any set of formulas. Define Taut to be the set of all tautologies. Define the set of formulas that follow from F , From(F ), by From(F ) = {f ∈Fml : F f} − Taut .\nSince Sat(Claus(F )) ⊆ Claus(F ) we have From(F ) ⊆ {f ∈Fml : F f} ⊆ {f ∈Fml : F ⊢ f}."
    }, {
      "heading" : "3. Principles of Plausible Reasoning",
      "text" : "Lists of postulates, properties, or principles that concern special types of reasoning are useful for at least the following reasons. 1) They help characterise the intended special type of reasoning.\n2) They provide a means of evaluating existing reasoning systems to see how well they perform the intended special type of reasoning. 3) They provide guidelines for creating new reasoning systems for the intended special type of reasoning. 4) They explicitly show a difference between the intended special type of reasoning and an existing form of reasoning. Notable examples of such lists are the following. The AGM postulates for belief change (Alchourròn, Gärdenfors, & Makinson, 1985; Gärdenfors, 1988), various properties of nonmonotonic consequence relations (Makinson, 1988; Kraus, Lehmann, & Magidor, 1990), and the postulates that a rule-based argumentation system should satisfy (Caminada & Amgoud, 2007).\nWe shall state the principles of this section by referring to the logic or proof algorithm directly; rather than by referring to consequence relations. A consequence relation, say |∼, relates a set F of formulas to a formula f ; where F |∼ f means that f is a consequence of F . Consequence relations are appropriate if the reasoning situations under consideration can be characterised by a set of formulas. But the plausible-reasoning situations we consider are specified by a plausible-structure S = (Fact(S),Plaus(S)) where the elements of Plaus(S) may be very different from the formulas in Fact(S). For these situations consequence relations are much less appropriate. For example consider two fundamental properties that consequences relations may have; namely cut and cautious monotonicity, which together are equivalent to cumulativity, also called lemma addition. If F and G are sets of formulas then let F |∼ G mean for all g in G, F |∼ g. Then cumulativity is the following property. If F |∼ G then for all formulas h, F |∼ h iff F ∪G |∼ h. A straightforward translation of F |∼ f into our situation is S |∼ f , where S is a plausible-structure. But then it is really hard to know what F ∪G might mean. Essentially we are trying to add proved formulas to S. But the only set of formulas in S is Fact(S). So we could try letting F ∪G be (Fact(S)∪G,Plaus(S)). But this is only sensible when the formulas in G have been proved using only Fact(S). When the formulas in G have been proved using Plaus(S) then it is no longer sensible to treat the formulas in G as facts; because they are not facts, they are only plausible conclusions.\nSome of the principles of plausible reasoning are regarded as necessary and so use the word ‘must’; the other principles are regarded as desirable and so use the word ‘should’.\nAs well as the principles of plausible reasoning, we shall present several plausiblereasoning examples. Some of these examples are based on an n-lottery, that is, randomly selecting a number from the finite integer interval [1..n]. We shall use si to denote that the number i was selected. Four examples will guide the development of some of the principles, and so we shall call these examples signpost examples. Our first signpost example is the 3-lottery example.\nExample 3.1 (The 3-lottery example). Consider a 3-lottery. Then we have the following. 1) Exactly one element of {s1, s2, s3} is true. 2) Each element of {s1, s2, s3} is probably false. 3) The disjunction of any 2 elements of {s1, s2, s3} is probably true.\nThis example illustrates some important properties of plausible reasoning that will be considered in several of the following subsections.\nThe following notation will be convenient. Let Thm(L, α,S) denote the set of all formulas derivable from the plausible-structure S by using the proof algorithm α of the logic L. If F is a set of propositional formulas then Thm(F ) denotes all the formulas derivable from F by (the proof algorithm of) any classical propositional logic. This simpler notation is unambiguous because Thm(F ) is independent of the logic (for example Hilbert systems, natural deduction, or resolution systems) and its proof algorithm."
    }, {
      "heading" : "3.1 Representation",
      "text" : "Plausible-reasoning situations may contain facts as well as plausible information; for instance statement (1) of Example 3.1 is a factual statement, unlike the other two statements which are plausible. Hence the first part of our first principle of plausible reasoning.\nAlthough the inherent lack of precision of plausible-reasoning situations is not quantified, a logic could represent this lack of precision with undue accuracy, for instance by using probabilities. Forbidding this is too restrictive, as the logic may deduce a conclusion using the probabilities but then present that conclusion without using probabilities. All we need is that the conclusions are not unduly precise. In particular if a formula is proved by using plausible information then it should not be regard as a fact. Hence the second part of our first principle of plausible reasoning.\nPrinciple 3.1 (The Representation Principle). 1) A logic for plausible reasoning must be able to represent, and distinguish between, factual\nand plausible statements. 2) The formulas proved by a logic for plausible reasoning must not be more precise than\nthe information used to derive them.\nWe note that when a situation is precisely described, perhaps using probabilities, a logic for plausible reasoning should be able to reason with the corresponding imprecisely defined situation; Example 3.1 is such a situation.\nWe infer from Principle 3.1(1) that we should be able to distinguish between conclusions that are factual and those that are merely plausible. One way of making this distinction is to have a factual proof algorithm that only uses facts and deduces only facts, and also a plausible proof algorithm that may use plausible statements and facts and deduces formulas that are only plausible. Of course if a plausible proof algorithm deduces only facts when given just facts then it can be regarded as both a factual and a plausible proof algorithm. The need for multiple proof algorithms is discussed further in Subsection 3.8."
    }, {
      "heading" : "3.2 Evidence and Non-Monotonicity",
      "text" : "Let us now see if we can establish some general guidelines concerning the provability of a given formula f . A plausible-reasoning situation will have evidence for and against f . So it seems reasonable to determine whether f is provable or not by just comparing these two sets of evidence, and declaring f provable iff the preponderance of evidence is for f .\nA consequence of the evidence criterion needs the following definitions. If S1 and S2 are plausible-structures then S1⊆S2 means Fact(S1)⊆Fact(S2) and Plaus(S1)⊆Plaus(S2). A proof algorithm α of a logic L is said to bemonotonic iff for any two plausible-structures, S1 and S2, if S1⊆S2 then Thm(L, α,S1)⊆Thm(L, α,S2). For example, the proof algorithm\nof a classical propositional logic is monotonic. A proof algorithm is non-monotonic iff it is not monotonic. A plausible proof algorithm is non-monotonic because the addition of evidence against a previously provable formula can cause it to be unprovable, as shown in our second signpost example.\nExample 3.2 (The Non-Monotonicity example). Consider the following two statements. The first is plausible and the second is factual. 1) a is probably true. 2) ¬a is (definitely) true. From (1) the conclusion is ‘a is plausible’. From (1) and (2), ‘a is plausible’ cannot be deduced, but ‘¬a is true’ can be.\nThe discussion above justifies our next principle.\nPrinciple 3.2. 3.2.1) The Evidence Principle. A plausible proof algorithm can prove a formula f iff all the evidence for f sufficiently outweighs all the evidence against f . 3.2.2) The Non-Monotonicity Principle. A plausible proof algorithm must be non-monotonic.\nExactly what constitutes evidence for or against f can only be determined when the particular logic for plausible reasoning is known. Also ‘sufficiently outweighs’ depends on the intuition that is being modelled, as well as the particular logic.\nA proof algorithm that fails the Evidence Principle seems to be seriously flawed. So the Evidence Principle may be a principle that any sensible proof algorithm should satisfy."
    }, {
      "heading" : "3.3 Conjunction",
      "text" : "We shall say a proof algorithm α of a logic L is conjunctive iff for any plausible-structure, S, and any two formulas f and g, if {f, g} ⊆Thm(L, α,S) then ∧ {f, g} ∈Thm(L, α,S). For example, the proof algorithm of any classical propositional logic is conjunctive. A proof algorithm is non-conjunctive iff it is not conjunctive.\nConjunctions of plausible formulas behave very differently from conjunctions of formulas that are certain. In Example 3.1, ∧ {¬s1,¬s2} is equivalent to s3. So although ¬s1 is plausible and ¬s2 is plausible, ∧ {¬s1,¬s2} is not plausible. Clearly plausible proof algorithms are not conjunctive.\nAlthough the conjunction of two plausible formulas is not necessarily plausible, the conjunction of two facts is a fact. So what about the conjunction of a fact and a plausible formula? Clearly it cannot be a fact, but is it always plausible? Intuitively, a fact f is always true, and a plausible formula g is true more often that not. So it seems reasonable that their conjunction be true whenever g is true, and hence it is reasonable that the conjunction is plausible. After we account for explosiveness and the problem of tautologies we get the following definition. We shall say a proof algorithm α of a logic L is plausibly conjunctive iff for any plausible-structure, S, and any two formulas f and g, if f ∈From(Fact(S)) and g ∈ Thm(L, α,S) then ∧ {f, g} ∈ Thm(L, α,S). For example, the proof algorithm of any classical propositional logic is plausibly conjunctive.\nPrinciple 3.3 (Conjunction). 3.3.1) The Non-Conjunction Principle. A plausible proof algorithm must not be conjunctive. 3.3.2) The Plausible Conjunction Principle.\nA plausible proof algorithm should be plausibly conjunctive.\nThe Non-Conjunction Principle is supported by the fact that the ‘And’ rule of (Kraus et al., 1990), (If a |∼ x and a |∼ y then a |∼ ∧ {x, y}.), is not probabilistically sound, see (Makinson & Hawthorne, 2014)(Section 2.1) where they call the ‘And’ rule the ‘Right∧+’ rule. Also Definition 2.4 of (Hawthorne & Makinson, 2007) defines an ‘And’ rule that is probabilistically sound and has a similar intuition to our Plausible Conjunction Principle."
    }, {
      "heading" : "3.4 Disjunction",
      "text" : "We shall say a proof algorithm α of a logic L is disjunctive iff for any plausible-structure, S, and any two formulas f and g, if ∨ {f, g}∈Thm(L, α,S) then either f ∈Thm(L, α,S) or g ∈Thm(L, α,S). A proof algorithm is non-disjunctive iff it is not disjunctive. The proof algorithm of any classical propositional logic is non-disjunctive.\nThe 3-lottery example (Example 3.1) shows that, although s1 and s2 are both unlikely their disjunction ∨ {s1, s2} is likely. Hence our next principle is necessary.\nPrinciple 3.4 (The Non-Disjunction Principle). A plausible proof algorithm must not be disjunctive."
    }, {
      "heading" : "3.5 Supraclassicality",
      "text" : "Consider a plausible-structure S. Let α be a plausible proof algorithm of the logic L. Then it is tempting to suggest that Thm(Fact(S)) ⊆ Thm(L, α,S). This is called supraclassicality, and could be phrased as ‘what is true is usually true’.\nAs we saw in Section 2 after Definition 2.7, classical propositional logic is explosive and proves all tautologies. But we do not want to force logics for plausible reasoning to be explosive or to prove all tautologies.\nDefinition 3.1. A proof algorithm α of a logic L has the plausible supraclassicality property and is said to be plausibly supraclassical iff for any plausible-structure, S, From(Fact(S)) ⊆ Thm(L, α,S).\nPrinciple 3.5 (The Plausible Supraclassicality Principle). Factual and plausible proof algorithms should be plausibly supraclassical.\nSince From(Fact(S)) ⊆ Thm(Fact(S)), if α is supraclassical (that is, Thm(Fact(S)) ⊆ Thm(L, α,S)) then it is plausibly supraclassical."
    }, {
      "heading" : "3.6 Right Weakening",
      "text" : "Right Weakening can be thought of as closure under classical inference. More precisely, a proof algorithm α has the right weakening property iff for any plausible-structure, S, and any formula f , if f ∈Thm(L, α,S) and f |= g then g∈Thm(L, α,S). By replacing g with any tautology, we see that a consequence of the right weakening property is Taut ⊆\nThm(L, α,S). But we do not want to force logics for plausible reasoning to prove all tautologies. We say a proof algorithm α has the weak right weakening property iff for any plausible-structure, S, and any formula f , if f ∈Thm(L, α,S) then From({f}) ⊆ Thm(L, α,S).\nHowever, suppose that whenever the facts of the plausible-structure S and a formula f are true then the formula g is also true; in symbols Fact(S)∪{f} |= g. Then in the situation defined by S, g is true at least as often as f . So if f is usually true then g should also be usually true. We say a proof algorithm α has the strong right weakening property iff for any plausible-structure, S, and any formula f , if f ∈Thm(L, α,S) and Fact(S)∪{f} |= g then g∈Thm(L, α,S).\nCombining the ideas in the preceding two paragraphs produces the following definition and corresponding principle. A proof algorithm α of a logic L has the plausible right weakening property iff for any plausible-structure, S, and any formula f , if f ∈Thm(L, α,S) then From(Fact(S)∪{f}) ⊆ Thm(L, α,S).\nPrinciple 3.6 (The Plausible Right Weakening Principle). A plausible proof algorithm should have the plausible right weakening property.\nWe note that strong right weakening implies all the other right weakening properties, and weak right weakening is implied by all the other right weakening properties."
    }, {
      "heading" : "3.7 Consistency",
      "text" : "Of the 11 characteristics of plausible reasoning given on page 114 of (Walton et al., 2014), characteristic 8 is ‘stability’; which seems to mean (bottom of page 97 of (Walton et al., 2014)) that plausible statements are consistent. However, as we shall show, where consistency is concerned the number of plausible statements is important.\nWe say a proof algorithm α of a logic L is n-consistent iff for any plausible-structure, S, and any set of formulas, F , if Fact(S) is satisfiable, and F ⊆Thm(L, α,S), and |F | ≤ n then F is satisfiable. Also a proof algorithm α of a logic L is strongly n-consistent iff for any plausible-structure, S, and any set of formulas, F , if Fact(S) is satisfiable, and F ⊆Thm(L, α,S), and |F | ≤ n then Fact(S)∪F is satisfiable.\nSo if a proof algorithm is strongly n-consistent then it is n-consistent. If Fact(S) is satisfiable then Thm(Fact(S)) is satisfiable; else Thm(Fact(S)) contains all formulas.\nContradictions are not plausible, so plausible proof algorithms must be 1-consistent. Hence Principle 3.7.1 below.\nSuppose S is a plausible-structure such that Fact(S) is satisfiable. If f ∈Thm(L, α,S) then in the situation defined by S, f is more likely to be true than not. Hence we should expect Fact(S)∪{f} to be satisfiable. That is, strong 1-consistency should hold.\nNow consider strong 2-consistency. So suppose f and g are formulas such that {f, g}⊆ Thm(L, α,S). By strong 1-consistency, both Fact(S)∪{f} and Fact(S)∪{g} should be satisfiable. If Fact(S)∪{f, g} is unsatisfiable then Fact(S)∪{g} |= ¬f . If f and g are contingent then the strong right weakening property is reasonable, and so we should expect that ¬f ∈Thm(L, α,S). Thus we have {f,¬f}⊆Thm(L, α,S). But, a reasonable property of ‘likely’ is that for any formula f , at most one of f and ¬f is likely. Therefore we should not have {f,¬f} ⊆ Thm(L, α,S). This unsatisfactory situation can be avoided if\nFact(S)∪{f, g} is satisfiable. So plausible proof algorithms should be strongly 2-consistent. Hence Principle 3.7.2 below.\nConsider the 3-lottery example (Example 3.1) and let U = {¬s1,¬s2, ∨ {s1, s2}}. For each x in U , x is likely; and ¬x is not likely. But U is (classically) unsatisfiable. The set U shows the necessity of Principle 3.7.3 below.\nPrinciple 3.7 (Consistency).\n3.7.1) The 1-Consistency Principle. A plausible proof algorithm must be 1-consistent. 3.7.2) The Strong 2-Consistency Principle. A plausible proof algorithm should be strongly 2-consistent. 3.7.3) The Non-3-Consistency Principle. A plausible proof algorithm that can prove disjunctions must not be 3-consistent."
    }, {
      "heading" : "3.8 Multiple Intuitions: Ambiguity",
      "text" : "With the possible exception of tautologies, classical propositional logic captures our intuition about what follows from a satisfiable set of facts. But there are different wellinformed intuitions about what follows from a plausible-reasoning situation. For example, as early as 1987 (Section 4.1 of (Touretzky, Horty, & Thomason, 1987)) it was recognised that a plausible-reasoning situation could elicit different sensible conclusions, depending on whether ambiguity was blocked or propagated. The essence of Figure 3 in (Touretzky et al., 1987) is our third signpost example.\nExample 3.3 (The Ambiguity Puzzle).\n1) There is evidence that a is likely. 2) There is evidence that ¬a is likely. 3) There is evidence that b is likely. 4) If a then ¬b is likely.\nWhat can be concluded about b? The evidence for b is (3). The evidence against b comes from (1) and (4). If we knew that a was definitely true then the evidence for b and against b would be equal. Ignoring (2), a is only likely by (1), so the evidence against b is weaker than the evidence for b. But (2) means that a is even less likely, and so the evidence against b has been further weakened. Thus b is more likely than ¬b. Hence many people think that it is reasonable to be able to conclude b. Such reasoning might be called ‘best bet’ or ‘most likely’ or ‘balance of probabilities’ reasoning.\nA formula f is said to be ambiguous iff there is evidence for f and there is evidence against f and neither f nor ¬f can be proved. Since (1) and (2) give equal evidence for and against a, a is ambiguous.\nIf the evidence against b has been weakened sufficiently to allow b to be concluded, then b is not ambiguous. So the ambiguity of a has been blocked from propagating to b. An algorithm that can prove b (but not ¬b) is said to be ambiguity blocking. This level of reasoning is appropriate if the benefit of being right outweighs the penalty for being wrong.\nIf the evidence against b has not been weakened sufficiently to allow b to be concluded, then b is ambiguous. So the ambiguity of a has been propagated to b. An algorithm that\ncannot prove b (or ¬b) is said to be ambiguity propagating. This more cautious level of reasoning is appropriate if the penalty for being wrong outweighs the benefit of being right.\nIt is well-known that the Anglo-American legal system uses a hierarchy of proof levels, two of which are the ‘balance of probabilities’ or ‘preponderance of the evidence’ (used in civil cases) which is ambiguity blocking, and ‘beyond reasonable doubt’ (used in criminal cases) which is ambiguity propagating. So there is a need for a proof algorithm that blocks ambiguity and one that propagates ambiguity.\nTo avoid confusion, one should know which algorithm is used; unless it is irrelevant to the point being made. This, and our observation at the beginning of this section that a logic for plausible reasoning should have a factual proof algorithm, leads to our next principle.\nPrinciple 3.8 (The Many Proof Algorithms Principle). A logic for plausible reasoning should have at least 1) a factual proof algorithm, 2) an ambiguity blocking plausible proof algorithm, and 3) an ambiguity propagating plausible proof algorithm. Also, the proof algorithm used to prove a formula should be explicit or irrelevant.\nClearly the algorithms in (2) and (3) must be different. But, as indicated after Principle 3.1, the factual algorithm could be the same as a plausible algorithm."
    }, {
      "heading" : "3.9 Decisiveness",
      "text" : "For a formula, f , a proof algorithm, α, will satisfy exactly one of the following conditions. i) α does not terminate. ii) α terminates in a state indicating that f is proved, iii) α terminates in a state indicating that f is not provable, iv) α terminates in some other state. A proof algorithm α is said to be decisive iff for every formula f , α terminates in either a state indicating that f is proved, or a state indicating that f is not provable.\nOur next principle is clearly desirable.\nPrinciple 3.9 (The Decisiveness Principle). Factual and plausible proof algorithms should be decisive."
    }, {
      "heading" : "3.10 Truth Values",
      "text" : "Let us change our focus from deduction to the more semantic notion of assigning truth values to statements. For classical propositional logic there are exactly two truth values: T for true and F for false. If v is a valuation (that is a function from the set of formulas to the set of truth values) and f and g are formulas then 1) Either v(f) = T or v(¬f) = T but not both, (the Excluded Middle property) and 2) v( ∧ {f, g}) = T iff v(f) = T = v(g), and 3) v( ∨ {f, g}) = T iff v(f) = T or v(g) = T.\nThe 3-lottery example (Example 3.1) shows that the closest plausible reasoning can get to (2) and (3) is (4) and (5) below. 4) If v( ∧ {f, g}) = T then v(f) = T = v(g). 5) If v(f) = T or v(g) = T then v( ∨ {f, g}) = T.\nMoreover consider our fourth signpost example.\nExample 3.4 (The 4-lottery example). Consider a 4-lottery. Then we have the following.\n1) Exactly one element of {s1, s2, s3, s4} is true. 2) Each element of {s1, s2, s3, s4} is probably false. 3) The disjunction of any 2 of elements of {s1, s2, s3, s4} is not probably true and not\nprobably false. 4) The disjunction of any 3 elements of {s1, s2, s3, s4} is probably true.\nIntuitively some formulas concerning Example 3.4 have different truth values; for example ∨ {s1, s2, s3, s4} is definitely true, ¬ ∨ {s1, s2, s3, s4} is definitely false, ¬s1 is probably true, s1 is probably false, and ∨ {s1, s2} is as likely to be true as false. So plausible reasoning appears to need at least 3 truth values: one indicating that a formula is more likely to be true than false, one indicating that a formula is as likely to be true as false, and one indicating that a formula is more likely to be false than true.\nHence our last principle of plausible reasoning.\nPrinciple 3.10 (The Included Middle Principle). A logic for plausible reasoning should have at least 3 truth values.\nBut what happens if we insist on there being exactly two truth values? Suppose a logic L for plausible reasoning has exactly 2 truth values, T and F. Also suppose that for any formulas f , g, and h, the following truth conditions hold.\nTC1) If f is more likely to be true than false then f and ¬f have different truth values. TC2) If f is as likely to be true as false then f and ¬f have the same truth value. TC3) The truth value of ∨ {f, g, h} is T iff the truth value of at least one of f , g, or h is T. TC4) ¬ ∨ {f, g, h} and ∧ {¬f,¬g,¬h} have the same truth value. TC5) The truth value of ∧ {f, g, h} is T iff the truth value of each one of f , g, and h is T.\nNow apply L to Example 3.4. By TC1, for each i in {1, 2, 3}, si and ¬si have different truth values. By TC2, ∨ {s1, s2, s3} and ¬ ∨ {s1, s2, s3} have the same truth value, which by TC4 is the same as ∧ {¬s1,¬s2,¬s3}.\nIf the truth value of ∨ {s1, s2, s3} is T then by TC3, for some i, the truth value of si is T; hence the truth value of ¬si is F and so by TC5 the truth value of ∧ {¬s1,¬s2,¬s3} is F. On the other hand if the truth value of ∨ {s1, s2, s3} is F then by TC3, for each i, the truth value of si is F; hence the truth value of each ¬si is T and so by TC5 the truth value of ∧ {¬s1,¬s2,¬s3} is T.\nSo in both cases ∨ {s1, s2, s3} and ∧ {¬s1,¬s2,¬s3} have different truth values which\ncontradicts what we had before.\nThe conditions TC1, TC2, TC3, TC4, and TC5 are so closely related to the meaning of ‘true’, ‘false’, ‘conjunction’, ‘disjunction’, and ‘negation’, that it is hard to reject any of them. Therefore it seems that having only two truth values is an over-simplification."
    }, {
      "heading" : "3.11 Correctness",
      "text" : "A logic that satisfies all the previous principles could nonetheless have a fatal flaw. It could give an unsatisfactory answer to a particular example. Some examples may well have no set of answers that are generally agreed upon. But some examples do have a set of answers that are generally agreed upon. We might call these answers the correct answers. So it is tempting to state a principle of correctness similar to “When correct answers exist, a logic must give all the correct answers, and no incorrect answers.”.\nThe problem with such a principle is that it is impossible to show that any logic satisfies it. The most that can be done is to produce an counter-example that shows a logic fails the principle, or demonstrate that for a chosen set of examples the logic gets the correct answers. But there might exist a counter-example that shows the logic fails the principle of correctness.\nThus we shall refrain from trying to formally state a Correctness Principle."
    }, {
      "heading" : "4. Some Non-Monotonic Logics",
      "text" : "We shall consider the relationship between some (non-numeric) non-monotonic logics and the principles and examples of Section 3.\nThere are three well-known non-monotonic logics, namely Default Logic, Circumscription, and Autoepistemic Logic; see (Antoniou, 1997) for an introduction. Answer Set Programming (ASP) (Baral, 2003) is a well-known Knowledge Representation system.\nEach of the proof algorithms of these four well-known systems is conjunctive and so fails the Non-Conjunction Principle (Principle 3.3.1). Also for each of these four proof algorithms, the set of all provable formulas is either satisfiable or contains all formulas. So all four proof algorithms fail the Non-3-Consistency Principle (Principle 3.7.3). Hence none of these logics reasons correctly about the 3-lottery example (Example 3.1). Finally all four of these proof algorithms are ambiguity propagating but not ambiguity blocking. So each of these logics fails the Many Proof Algorithms Principle (Principle 3.8). Hence when ambiguity blocking is required — for instance in civil cases — these logics do not get the right answers.\nLogics that deal with only literals are incapable of the reasoning required by Example 3.1. Logics in this category include inheritance networks (Horty, Thomason, & Touretzky, 1990), the DeLP system of (Garcia & Simari, 2004), the ASPIC system mentioned in (Caminada & Amgoud, 2007), the logic in (Prakken & Sartor, 1997), Ordered logic (Geerts, Vermeir, & Nute, 1994), and most Defeasible Logics (Billington, 2008).\nPropositional Plausible Logic (PPL), which is defined in the next section, is a member of the family of Defeasible Logics. The only Defeasible Logics that deal with conjunction and disjunction, besides PPL, are the logic in (Billington & Rock, 2001), let’s call it DL1, and the logic in (Billington, 2008), let’s call it DL8. But the plausible proof algorithms of both DL1 and DL8 are conjunctive and so do not satisfy the Non-Conjunction Principle (Principle 3.3.1). Also the Decisiveness Principle (Principle 3.9) fails for the plausible proof algorithms that define the Defeasible Logics in: (Billington, 1993), (Billington & Rock, 2001), (Maier & Nute, 2006), (Billington, 2008), and (Billington, 2011). Since all Defeasible Logics apart from PPL are closely related to a Defeasible Logic in these five citations, all Defeasible Logics apart from PPL fail the Decisiveness Principle. So PPL is\nthe only Defeasible Logic that satisfies all the principles in Section 3. Also PPL is more expressive than previous Defeasible Logics because the non-strict rules in PPL use formulas whereas previous Defeasible Logics only used literals and clauses. This is significant because a finite set of clauses is very different to the conjunction of those clauses, see the 3-lottery example (Example 3.1).\nArgumentation systems, (Dung, 1995), are well-known non-monotonic reasoning systems that can use rules, for example ASPIC (Caminada & Amgoud, 2007) and ASPIC+ (Modgil & Prakken, 2013). Let E ∈{admissible, complete, preferred, grounded, ideal, semi-stable, stable}. Then the semantics of ASPIC+ defined by intersecting all E-extensions is ambiguity propagating and so fails the Many Proof Algorithms Principle (Principle 3.8(2)).\nAn early argumentation system is given in (Simari & Loui, 1992) and it also is ambiguity propagating and so fails the Many Proof Algorithms Principle (Principle 3.8(2)). It also has other problems mentioned in (Geerts, Laenens, & Vermier, 1998).\nThree postulates that a rule-based argumentation system should satisfy are given in (Caminada & Amgoud, 2007). Postulate 1 is closure under strict rules; that is Modus Ponens for strict rules (Theorem 7.3(3)). It is a kind of right weakening property (Subsection 3.6). Postulate 2 requires the set of all proved literals to be consistent. If only literals can be proved, as in (Caminada & Amgoud, 2007), then this is implied by the Strong 2-Consistency Principle (Principle 2). Postulates 1 and 2 jointly imply Postulate 3.\nIt is not surprising that Conditional Logics (Nute & Cross, 2001; Arlo-Costa & Egré, 2016) have been used to analyse non-monotonic reasoning. Let 99K denote a weak conditional; so that for formulas f and g, f 99K g means ‘if f then ... g’ where ‘...’ could be ‘normally’, ‘typically’, ‘probably’, or any other similar word or phrase. A set of such weak conditionals is called a ‘conditional knowledge base’. The following two rules are particularly important for differentiating our plausible reasoning from other kinds of reasoning. And: If f 99K g and f 99K h then f 99K ∧ {g, h}. Or: If f 99K h and g 99K h then ∨ {f, g} 99K h.\nThe And-rule is also called the CC-rule, and the Or-rule is also called the CA-rule.\nLet Ax3 = ∧ { ∨ {s1, s2, s3}, ¬ ∧ {s1, s2}, ¬ ∧ {s1, s3}, ¬ ∧ {s2, s3}} be the formula that characterises the 3-lottery example, Example 3.1(1) As noted in Subsection 3.3, we have Ax3 99K ¬s1 and Ax3 99K ¬s2 but not Ax3 99K ∧ {¬s1,¬s2}. So reasoning systems that satisfy the And-rule do not do plausible reasoning.\nThe formula, Ax7, that characterises a 7-lottery is the conjunction of the following 22 formulas: ∨ {s1, s2, s3, s4, s5, s6, s7} and ¬\n∧ {si, sj}, where 1 ≤ i < j ≤ 7. Let f be∧\n{Ax7,¬s1,¬s2}, let g be ∧ {Ax7,¬s3,¬s4}, and let h be ∨ {s5, s6, s7}. Then f is equivalent to exactly one of s3 or s4 or s5 or s6 or s7, and g is equivalent to exactly one of s1 or s2 or s5 or s6 or s7. So f 99K h and g 99K h. But ∨ {f, g} does not restrict the selected number at all, and h is not the usual result of a 7-lottery. So we do not have ∨ {f, g} 99K h. Therefore reasoning systems that satisfy the Or-rule do not do plausible reasoning.\nIn (Delgrande, 2007) it is observed that the following reasoning systems satisfy both the And-rule and the Or-rule and hence do not do our plausible reasoning: systems based on intuitions from probability theory such as (Adams, 1975) and (Pearl, 1988), and from qualitative possibilistic logic (Dubois, Lang, & Prade, 1994), those based on C4 (Lamarre, 1991), CT4 (Boutilier, 1994a), and S (Burgess, 1981).\nGeffner and Pearl (Geffner & Pearl, 1992) define a logic called ‘conditional entailment’. The second paragraph on page 235 of (Geffner & Pearl, 1992) contains the following sentence. “In the propositional case, the only difference between conditional entailment and prioritized circumscription is the source of the priorities: while prioritized circumscription relies on the user, conditional entailment extracts the priorities from the knowledge base itself.” As noted near the beginning of this section, circumscription fails the Non-Conjunction Principle (Principle 3.3.1), the Non-3-Consistency Principle (Principle 3.7.3), and the Many Proof Algorithms Principle (Principle 3.8). Hence conditional entailment also fails these principles.\nThe consequence function of (Makinson, 1988) and the cumulative conditional knowledge bases of (Kraus et al., 1990) satisfy both the And-rule and the Or-rule. Preferential conditional knowledge bases (Kraus et al., 1990) are cumulative. Rational conditional knowledge bases (Lehmann & Magidor, 1992) are preferential. Hence both the preferential and rational closure of a conditional knowledge base satisfies both the And-rule and the Or-rule; and so does not do the plausible reasoning we are trying to characterise.\nAs noted in (Delgrande, 2007) the following systems are ‘essentially the same as’ rational closure and hence do not do our plausible reasoning: System Z (Pearl, 1990), systems based on conditional logic (Crocco & Lamarre, 1992), on modal logic (Boutilier, 1994b), on possibilistic logic (Benferhat, Dubois, & Prade, 1992), and on conditional objects (Dubois & Prade, 1991).\nThe Propositional Typicality Logic (PTL) of (Booth, Meyer, & Varzinczak, 2013) and (Booth, Casini, Meyer, & Varzinczak, 2015) has several semantics. Each semantics is at least preferential and so satisfies both the And-rule and the Or-rule. Hence PTL does not do the plausible reasoning we are trying to characterise.\nThe conditional logic C of (Delgrande, 2007) does not satisfy the Plausible Right Weakening Principle (Principle 3.6). Also C and the extensions of C considered in (Delgrande, 2007) have only one proof algorithm and so fail the Many Proof Algorithms Principle (Principle 3.8).\nApart from the problems mentioned in Section 5 of (Goldszmidt & Pearl, 1991), System Z (Pearl, 1990) and System Z+ (Goldszmidt & Pearl, 1991) are ambiguity propagating but not ambiguity blocking. Hence they fail the Many Proof Algorithms Principle (Principle 3.8). Moreover, although they can represent the 3-lottery example (Example 3.1), they cannot prove anything about the example because the set of rules is not ‘consistent’ as defined in (Pearl, 1990; Goldszmidt & Pearl, 1991).\nThe logic implemented by theorist (Poole, 1988) and the Preferred Subtheories logic in (Brewka, 1989) both generate consistent extensions and so fail the Non-3-Consistency Principle (Principle 3.7.3).\nEvery logic reviewed above fails at least one of the following principles: the NonConjunction Principle (Principle 3.3.1), the Non-3-Consistency Principle (Principle 3.7.3), the Many Proof Algorithms Principle (Principle 3.8), and the correctness principle as instanced by the 3-lottery example (Example 3.1). So these principles seem to be central to the difference between the plausible reasoning characterised in Section 3 and other kinds of non-numeric non-monotonic reasoning. As far as we know, Propositional Plausible Logic is the only non-numeric non-monotonic propositionally adequate logic that satisfies all the principles in Section 3."
    }, {
      "heading" : "5. Propositional Plausible Logic (PPL)",
      "text" : "The purpose of this section is to define a propositional logic, called Propositional Plausible Logic (PPL), that satisfies all the principles in Section 3. The plausible-structure used in PPL is defined in Subsection 5.1. The proof algorithms are defined in Subsection 5.2. The notions of ‘proof’ and ‘truth’ are developed in Subsection 5.3 and Subsection 5.4, respectively.\nAs well as the notation introduced in Section 2, we shall use the following notation concerning sequences. The empty sequence is denoted by (). Let S be a sequence. If S is finite then S+e denotes the sequence formed by just adding e onto the right end of S. Define e∈S to mean e is an element of S, and e /∈S to mean e is not an element of S."
    }, {
      "heading" : "5.1 Plausible Descriptions",
      "text" : "Propositional Plausible Logic (PPL) reasons about plausible-reasoning situations that may contain facts, like definitions and membership of categories. These facts are represented by formulas that are converted into clauses called axioms and these axioms are then converted into strict rules. The plausible information is represented by defeasible rules, warning rules, and a priority relation, >, on rules.\nIntuitively the various kinds of rules have the following meanings. The strict rule A→c means if every formula in A is true then c is true. So strict rules are like material implication except that A is a finite set of formulas rather than a single formula. (We have already seen that A and ∧ A behave differently.) For example, ‘nautiluses are cephalopods’ could be written as {n}→c, and ‘cephalopods are molluscs’ could be written as {c}→m. Roughly, the defeasible rule A⇒ c means if every formula in A is true then c is usually true. For example, ‘molluscs usually have shells’ could be written as {m} ⇒ s, and ‘cephalopods usually have no shells’ could be written as {c}⇒¬s.\nThe warning rule A❀ c roughly means if every formula in A is true then c might be true. So A❀¬c warns against concluding usually c, but does not support usually ¬c. For example, ‘objects that look red in red light might not be red’ could be written as {looks-redin-red-light}❀¬r. Warning rules can be used to prevent unwanted chaining. For example, suppose we have ‘if a then usually b’ ({a} ⇒ b) and ‘if b then usually c’ ({b}⇒ c). Then it may be too risky to conclude ‘usually c’ from a. Without introducing evidence for ¬c, the conclusion of ‘usually c’ from a can be prevented by the warning rule {a}❀ ¬c. An instance of this example can be created by letting a be x ∈ {1, 2, 3, 4}, b be x ∈ {2, 3, 4}, and c be x∈{3, 4, 5}. Warning rules have also been called ‘defeaters’ and ‘interfering rules’. The formal definition of a rule and its associated terms follows.\nDefinition 5.1. A rule, r, is any triple (A(r), arrow (r), c(r)) such that A(r), called the set of antecedents of r, is a finite (possibly empty) set of formulas; arrow (r)∈{→,⇒,❀}; and c(r), called the consequent of r, depends on arrow (r). If arrow (r) is the strict arrow, →, then c(r) is either a formula or the conjunction of a countable set of formulas, and r is written A(r) → c(r) and called a strict rule. If arrow (r) is the defeasible arrow, ⇒, then c(r) is a formula, and r is written A(r) ⇒ c(r) and called a defeasible rule. If arrow (r) is the warning arrow, ❀, then c(r) is a formula, and r is written A(r) ❀ c(r) and called a warning rule.\nA priority relation, >, on rules is used to indicate the more relevant of two rules. For instance, the specific rule ‘cephalopods usually have no shells’, ({c}⇒¬s), is more relevant than the general rule ‘molluscs usually have shells’, ({m}⇒ s), when reasoning about the external appearance of cephalopods. Hence {c}⇒¬s > {m}⇒ s. More generally, some common policies for defining > are the following. Prefer specific rules over general rules; prefer authoritative rules, (for instance national laws override state laws); prefer recent rules (because they are more up-to-date); and prefer more reliable rules. If r and s are rules and r > s then we often say r is superior to s and s is inferior to r.\nAlthough the priority relation does not have to be transitive, it does have to be acyclic.\nDefinition 5.2. Let R be any set of rules. A binary relation, >, on R is cyclic iff there exists a finite sequence, (r1, r2, ..., rn) where n ≥ 1, of elements of R such that r1 > r2 > ... > rn > r1; that is, rn > r1 and for all i in [1 .. n−1], ri > ri+1. A binary relation, >, is acyclic iff it is not cyclic.\nLet us now consider the conversion of the facts of a plausible-reasoning situation represented by a set F of formulas into strict rules. First we form Claus(F ) which is the set of clauses formed from F . Next we generate the set of axioms, Ax by defining Ax = CorRes(Sat(Claus(F ))). Finally we convert a contingent clause with n literals into 2n−1 strict rules. The conversion is done by the function Rul(.) in the usual way as shown by the following example. Rul( ∨ {a, b, c}) = { {}→ ∨ {a, b, c}, { ∧ {¬b,¬c}}→a, { ∧ {¬a,¬c}}→b, { ∧ {¬a,¬b}}→c, {¬a}→ ∨ {b, c}, {¬b}→ ∨ {a, c}, {¬c}→ ∨ {a, b} }.\nThe full definition of Rul(.) and Rul(., .), as well as some useful notation, is given in the next definition.\nDefinition 5.3. Let R be a set of rules, F be a finite set of formulas, and C be a set of contingent clauses. 1) Rs is the set of strict rules in R. That is, Rs = {r∈R : r is a strict rule}. 2) Rd is the set of defeasible rules in R. That is, Rd = {r∈R : r is a defeasible rule}. 3) c(R) is the set of consequents of the rules in R. That is, c(R) = {c(r) : r∈R}. 4) If c∈C then Rul(c) = { {}→c} ∪ { { ∧ ∼(L−K)} → ∨ K : c = ∨ L and {}⊂K⊂L}. 5) Rul(C) = ⋃ {Rul(c) : c∈C}. 6) Rul(C,F ) is the set of rules in Rul(C) whose set of antecedents is F . That is, Rul(C,F ) = {r∈Rul(C) : A(r) = F}.\nWe note that the set of antecedents of any strict rule formed by Rul(.) has at most one element.\nAlthough Rul(Ax ) gives us the strict rules that characterise the set F of facts we started with, we can reduce the number of these strict rules by ‘anding’ all those that have the same antecedent. For example, the ‘anding’ of {a}→c1, {a}→c2, and {a}→c3 is {a}→∧ {c1, c2, c3}. We now have the set of strict rules that we want. This set is formally defined by PD2 below. The formal structure used for describing plausible-reasoning situations is called a plausible description and is defined below.\nDefinition 5.4. If R is a set of rules then (R,>) is a plausible description iff PD1, PD2, PD3, and PD4 all hold.\nPD1) There is a set F of formulas such that Ax (R) = CorRes(Sat(Claus(F ))). Ax(R) is called the set of axioms of R and is usually denoted by Ax . PD2) Rs = {A→ smp( ∧ c(Rul(Ax , A))) : A∈{A(r) : r∈Rul(Ax )}}. PD3) If Ax 6={} then r denotes the strict rule {} → ∧ Ax . PD4) > is a priority relation on R; that is, > ⊆ R×(R−{r}) and > is not cyclic.\nSuppose (R,>) is a plausible description. Then Ax is empty iff Rs is empty. If Ax 6={} then r∈Rs. If Rs is not empty we can extract Ax from the consequent of r. This shows that Ax (R) is indeed dependent on R. Different strict rules in R have different sets of antecedents, and no rule is superior (>) to r.\nFor PPL the plausible-structure is a plausible description (R,>) and the factual part is Ax (R), which by PD2 is equivalent to the strict rules in R, Rs. The plausible part consists of the non-strict rules in R and the priority relation >.\nBy Lemma A.2(6) (See Appendix A) and PD1, Ax is satisfiable. So in PPL we extend the meaning of ‘fact’ from just being an element of Ax to a formula that is implied by Ax . Explicitly, a formula f is said to be a fact iff Ax |=f ."
    }, {
      "heading" : "5.2 The Proof Relation and the Proof Algorithms",
      "text" : "In this subsection we define what it means for a formula to be proved from a plausible description. We shall do this by defining a proof relation, |− , and various proof algorithms. This complex task will be done by giving the overall strategy, and then progressively refining this general plan until all the terms used have been defined.\nAny method of demonstrating that Ax |= f will do as an algorithm for proving facts; so there is no need to specify a particular one. Let our top level general plan for proving a formula be the following.\nDistinguish between proving facts and proving formulas that are not facts.\nLower case Greek letters will be used to denote the proof algorithms that will eventually be defined. A general proof algorithm will be denoted by α (a for alpha and algorithm). We shall use ϕ (f for phi and fact) to denote our factual proof algorithm. Until a further refinement is needed we shall use the notation α |−f to denote that a formula f is proved by the proof algorithm α.\nSince facts are always true they are (at least) probably true. So we shall decree that facts are provable by all proof algorithms. Thus we have the following. All algorithms prove all facts. In symbols, if Ax |= f then α |− f . The factual algorithm proves a formula iff it is a fact. In symbols, ϕ |− f iff Ax |= f .\nNow consider formulas f that are not facts, that is, Ax 6|= f . To (plausibly) prove f we need to do two things. First, establish some evidence for f . Second, defeat all the evidence against f . This will satisfy the requirements of the Evidence Principle, Principle 3.2.1. So our first refinement of the general plan is the following.\nRefinement 5.1. Suppose (R,>) is a plausible description, Ax = Ax (R), and f is a formula. 1) If Ax |= f then α |− f . Also ϕ |− f iff Ax |= f . 2) If Ax 6|= f and α 6= ϕ then α |− f iff (2.1) and (2.2) hold.\n2.1) Establish some evidence for f .\n2.2) Defeat all the evidence against f .\nIn accordance with the intuitive meaning of the three kinds of rules given at the beginning of this subsection, the evidence for f consists of strict or defeasible rules that have a consequent that implies f . However since the axioms are always true, we can weaken this to requiring that Ax∪{c(r)} implies f , provided that Ax∪{c(r)} is satisfiable. So if R′⊆R it will be convenient to let R′[f ] = {r∈R′ : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= f}. In the case of Refinement 5.1(2.1) we have Ax 6|= f and so r cannot support f . Hence the following notation is convenient. Rsd = (Rs∪Rd)− {r}. So the evidence for f is all the rules in R that support f , that is Rsd[f ]. To establish some evidence for f we must prove the set of antecedents of a rule supporting f . So we need to find a rule r in Rsd[f ] and prove A(r).\nBut A(r) is a set of formulas, not a formula. By proving a finite set F of formulas we shall mean proving every formula in F . In symbols, α |−F iff ∀f ∈F , α |− f . So if F is empty we have α |− {}.\nCollecting these ideas together gives our next refinement.\nRefinement 5.2. Suppose (R,>) is a plausible description, Ax = Ax (R), and f is a formula. 1) If F is a finite set of formulas then α |−F iff ∀f ∈F , α |− f . 2) If Ax |= f then α |− f . Also ϕ |− f iff Ax |= f . 3) If Ax 6|= f and α 6= ϕ then α |− f iff ∃r∈Rsd[f ] such that (3.1) and (3.2) hold.\n3.1) α |−A(r). 3.2) Defeat all the evidence against f .\nEach rule whose consequent implies ¬f is evidence against f . The set of such rules is R[¬f ]. In Refinement 5.2(3) we have a rule r in Rsd[f ]. So any rule in R[¬f ] that is inferior to r has already been defeated by r and hence need not be explicitly considered. This reduces the set of evidence against f that must be considered to the set of rules in R[¬f ] that are not inferior to r; in symbols, {s∈R[¬f ] : s 6< r}.\nA rule s in {s∈R[¬f ] : s 6< r} is defeated either by team defeat or by disabling s. The team of rules for f is Rsd[f ]. The rule s is defeated by team defeat iff there is a rule t in the team of rules for f , Rsd[f ], such that t is superior to s, t > s, and the set of antecedents of t, A(t), is proved α |−A(t). So if R′⊆R it will be convenient to let R′[f ; s] denote the set of all rules in R′[f ] that are superior to s. In symbols, R′[f ; s] = {t∈R′[f ] : t > s}. Alternatively s is disabled iff the set of antecedents of s, A(s), cannot be proved, α 6|−A(s).\nThree notations for useful sets of rules have been introduced, so their formal definition is appropriate before our third refinement.\nDefinition 5.5. Suppose (R,>) is a plausible description, Ax = Ax (R), R′⊆R, f is a formula, and s∈R. 1) Rsd = (Rs∪Rd)− {r}. 2) R′[f ] = {r∈R′ : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= f}. 3) R′[f ; s] = {t∈R′[f ] : t > s}.\nRefinement 5.3. Suppose (R,>) is a plausible description, Ax = Ax (R), and f is a formula. 1) If F is a finite set of formulas then α |−F iff ∀f ∈F , α |− f . 2) If Ax |= f then α |− f . Also ϕ |− f iff Ax |= f . 3) If Ax 6|= f and α 6= ϕ then α |− f iff ∃r∈Rsd[f ] such that (3.1) and (3.2) hold.\n3.1) α |−A(r). 3.2) ∀s∈{s∈R[¬f ] : s 6< r} either\n3.2.1) ∃t∈Rsd[f ; s] such that α |−A(t); or 3.2.2) α 6|−A(s).\nRefinement 5.3 has no undefined terms, but unfortunately it has two failings. There is only one plausible proof algorithm (denoted by α), and so the Many Proof Algorithms Principle, Principle 3.8, fails. Also, a proof may get into a loop, and hence the Decisiveness Principle, Principle 3.9, fails.\nBefore we consider looping, let us invent the other proof algorithms. The α in Refinement 5.3(3.2.2) evaluates evidence against f ; and this need not be the same α as in (3.1) and (3.2.1) which evaluates evidence for f . To avoid confusion let us call the α in (3.2.2), α′. Replacing α by α′ in (3.2.2) creates the need to decide what (α′)′ is. Let us simplify (α′)′ to α′′. Some obvious choices are: α′′ = α, or α′′ = α′, or α′′ is some other proof algorithm. The third choice postpones and complicates the choice that must eventually be made. Experimentation shows that the second choice has some properties that we would rather avoid. So we let α′′ = α.\nAnother change that can be made is to the set {s ∈ R[¬f ] : s 6< r} of rules that a proof algorithm regards as evidence against f . Let Foe(α, f, r) denote the set of rules that α regards as the evidence against f that is not inferior to r.\nThese ideas gives us our fourth refinement.\nRefinement 5.4. Suppose (R,>) is a plausible description, Ax = Ax (R), and f is a formula. 1) If F is a finite set of formulas then α |−F iff ∀f ∈F , α |− f . 2) If Ax |= f then α |− f . Also ϕ |− f iff Ax |= f . 3) If Ax 6|= f and α 6= ϕ then α |− f iff ∃r∈Rsd[f ] such that (3.1) and (3.2) hold.\n3.1) α |−A(r). 3.2) ∀s∈Foe(α, f, r) either\n3.2.1) ∃t∈Rsd[f ; s] such that α |−A(t); or 3.2.2) α′ 6|−A(s).\nLet us create our first non-factual proof algorithm, β, by changing Refinement 5.4 as little as possible. So let Foe(β, f, r) = {s∈R[¬f ] : s 6< r}. Let β be defined by replacing each α in Refinement 5.4 with β. Of course now β′ must be defined. First let Foe(β′, f, r) = {s∈R[¬f ] : s 6< r}. Then let β′ be defined by replacing each α in Refinement 5.4 with β′. (Recall that β′′ = β.) Later we shall show that β is ambiguity blocking (b for beta and blocking). We are not really concerned with any primed algorithm as they only assist with the definition of their non-primed co-algorithm. But later we shall show that β and β′ prove exactly the same formulas. So why is β′ needed? Without β′ it is exceedingly difficult to prove the relationship between β and the other algorithms we are about to define.\nOur next algorithm, π, will be shown to be ambiguity propagating (p for pi and propagating). We want to make π as strong as possible; that is, π proves f if there is no evidence against f . This can be done by making its co-algorithm π′ as weak as possible; that is, π′ ignores all evidence against f ; hence Foe(π′, f, r) = {}. This is the only change we make to Refinement 5.4. Explicitly, let Foe(π, f, r) = {s ∈ R[¬f ] : s 6< r}. Let π be defined by replacing each α in Refinement 5.4 with π. Let π′ be defined by replacing each α in Refinement 5.4 with π′. (Recall that π′′ = π.)\nOur last algorithm, ψ, will also be shown to be ambiguity propagating (p for psi and propagating). We want to make ψ weaker than π. This can be done by making its coalgorithm ψ′ regard those rules that imply ¬f and are superior to r as evidence against f ; hence Foe(ψ′, f, r) = {s∈R[¬f ] : s > r}. This is the only change we make to Refinement 5.4. Explicitly, let Foe(ψ, f, r) = {s∈R[¬f ] : s 6< r}. Let ψ be defined by replacing each α in Refinement 5.4 with ψ. Let ψ′ be defined by replacing each α in Refinement 5.4 with ψ′. (Recall that ψ′′ = ψ.)\nTo emphasise that we are only interested in the non-primed proof algorithms we note that there are examples in which both π′ and ψ′ can prove both f and ¬f . This is fine as both π′ and ψ′ only assess the evidence against f , rather than try to defeasibly justify accepting f , as the non-primed algorithms do.\nThe following two formal definitions collect together for easy reference the above notations concerning algorithms and Foe(., ., .).\nDefinition 5.6. Define the set, Alg, of names of the proof algorithms by Alg = {ϕ, π, ψ, β, β′, ψ′, π′}. Define ϕ′ = ϕ. If α∈{π, ψ, β} then define (α′)′ = α′′ = α. If α∈Alg then the co-algorithm of α is α′.\nDefinition 5.7. Suppose (R,>) is a plausible description, f is a formula, and r∈R. 1) If α∈{π, ψ, β, β′} and r 6= r then Foe(α, f, r) = {s∈R[¬f ] : s 6< r}. 2) If α∈{ϕ, π′} or r = r then Foe(α, f, r) = {}. 3) Foe(ψ′, f, r) = {s∈R[¬f ] : s > r} = R[¬f ; r].\nFinally, let us consider looping. To prove f we use α and a rule r. While proving f we may have to prove other formulas. During a proof of one of these other formulas, if we choose to use α and r again then we will be in a loop and so this choice should fail. To prevent such a looping choice we need to record that α and r have been used previously. We shall call such a record of used algorithms and rules a history. Its formal definition follows.\nDefinition 5.8. Suppose (R,>) is a plausible description and α∈Alg. Define αR = {αr : r∈R}. Then H is an α-history iff H is a finite sequence of elements of αR ∪ α′R that has no repeated elements.\nUnfortunately using a history complicates Refinement 5.4 because we now no longer have just an algorithm proving a formula, but an algorithm and a history proving a formula. Therefore in (1), (2), and (3), α |−x becomes (α,H) |−x. In (3.1) α and r have now been used so H must be updated to H+αr and hence α |−A(r) becomes (α,H+αr) |−A(r). Also in (3.2.1) α and t have been used so H must be updated to H+αt and hence α |−A(t) becomes (α,H+αt) |−A(t). Similarly in (3.2.2) α′ and s have been used and so H must be updated to H+α′s and hence α′ 6|−A(s) becomes (α′,H+α′s) 6|−A(s). Finally to prevent looping we must be sure that αr /∈H in (3.1), αt /∈H in (3.2.1), and α′s /∈H in (3.2.2).\nIncorporating these changes into Refinement 5.4 gives our formal definition of the proof algorithms and proof relation |− . The letter I is attached to these final inference conditions.\nDefinition 5.9. Suppose P = (R,>) is a plausible description, Ax = Ax (R), α∈Alg , H is an α-history, and f is a formula. The proof relation for P, |− , and the proof algorithms are defined by I1 to I3. I1) If F is a finite set of formulas then (α,H) |−F iff ∀f ∈F , (α,H) |− f . I2) If Ax |= f then (α,H) |− f . Also (ϕ,H) |− f iff Ax |= f . I3) If Ax 6|= f and α 6= ϕ then (α,H) |− f iff ∃r∈Rsd[f ] such that I3.1 and I3.2 hold.\nI3.1) αr /∈H and (α,H+αr) |−A(r). I3.2) ∀s∈Foe(α, f, r) either\nI3.2.1) ∃t∈Rsd[f ; s] such that αt /∈H and (α,H+αt) |−A(t); or I3.2.2) α′s /∈H and (α′,H+α′s) 6|−A(s).\nThe following notation is useful.\nDefinition 5.10. If P is a plausible description, α∈Alg, and x is either a formula or a finite set of formulas then define x is α-provable iff α |−x iff (α, ()) |−x, and P(α) = {f ∈Fml : α |− f} to be the set of all α-provable formulas."
    }, {
      "heading" : "A semantic aside",
      "text" : "Subsection 5.2, and its culmination in Definition 5.9, can be given a semantic interpretation. By Theorem 7.3(3), the meaning of the strict rule A→f is that for any proof algorithm α, if α |−A then α |− f . The meaning of the defeasible rule A⇒f is that for any non-factual proof algorithm α, if α |−A and the evidence against f is defeated then α |− f . Exactly what the evidence against f is and how it is defeated is given by I3.2. By I3.2 the warning rule s = A❀¬f can only be used as evidence against f ; and exactly how s can be defeated is also given by I3.2. Thus Definition 5.9 can be seen as giving a meaning to each of the three kinds of rules.\nSimilarly Definition 5.9, and the explanations preceding it, can be seen as giving a meaning to each of the proof algorithms. By I2, we see that ϕ |− f means Ax |= f . Each of the non-factual proof algorithms, α, regards Rsd[f ] as the set of potential evidence for f ; and how α establishes that there is actual evidence for f is given by I3.1. Given some actual evidence r for f , the set that α regards as evidence against f is Foe(α, f, r). Exactly how this evidence can be defeated is given by I3.2."
    }, {
      "heading" : "5.3 A Proof Theory",
      "text" : "Definition 5.9 is recursive, however it can be iterated to yield a rooted tree — defined in Definition 5.13 — that could be regarded as the structure of a proof in PPL. The nodes of this tree will have special labels called tags which we now define.\nDefinition 5.11. Suppose (R,>) is a plausible description, α∈Alg , F is a finite set of formulas, H is an α-history, f is a formula, r∈Rsd[f ], s∈R[¬f ], and p is a node of a tree. The tag, t(p), of p is a triple t(p) = (Subj (p), op(p), pv (p)). The subject of p, Subj (p), has one of the following forms: (α,H,F ), −(α′,H, F ),\n(α,H, f), (α,H, f, r), or (α,H, f, r, s). The operation of p, op(p), is either min (for minimum), max (for maximum), or −. If op(p) is min [resp. max, −] then p is referred to as a min [resp. max, minus] node. The proof value of p, pv(p), is either +1 or −1.\nThe arithmetic properties of the proof values are defined below. These are as expected, but note that max{} = −1 and min{} = +1.\nDefinition 5.12. Suppose S⊆{+1,−1}. 1) minS = −1 iff −1∈S. (3) maxS = +1 iff +1∈S. (5) −−1 = +1. 2) minS = +1 iff −1 /∈S. (4) maxS = −1 iff +1 /∈S. (6) −+1 = −1.\nSo min and max act like quantifiers when applied to a set of proof values. That is, minS = −1 iff there exists v in S such that v = −1; maxS = +1 iff there exists v in S such that v = +1; minS = +1 iff for all v in S, v = +1; and maxS = −1 iff for all v in S, v = −1.\nDefinition 5.13. Let P = (R,>) be a plausible description. Then T is an evaluation tree of P iff T is a rooted tree constructed as follows. Each node, p, of T has exactly one tag, t(p). For each node p of T there is exactly one number, #p, in [1..6] such that p satisfies T#p and T7. T1) Subj (p) = (α,H,F ), α∈Alg , H is an α-history, and F is a finite set of formulas.\nDefine S(p) = {(α,H, f) : f ∈F}. Then op(p) = min, p has |S(p)| children, and each element of S(p) is the subject of exactly one child of p. If S(p) = {} then pv(p) = +1.\nT2) Subj (p) = (α,H, f), α∈Alg , H is an α-history, f is a formula, and Ax |= f . Then p has no children and t(p) = ((α,H, f),min,+1). T3) Subj (p) = (α,H, f), α ∈ Alg−{ϕ}, H is an α-history, f is a formula, and Ax 6|= f . Define S(p) = {(α,H, f, r) : αr /∈H and r∈Rsd[f ]}. Then op(p) = max, p has |S(p)| children, and each element of S(p) is the subject of exactly one child of p. If S(p) = {} then pv(p) = −1. T4) Subj (p) = (α,H, f, r), α ∈ Alg−{ϕ}, H is an α-history, f is a formula, Ax 6|= f , αr /∈H, and r∈Rsd[f ]. Define S(p) = {(α,H+αr,A(r))} ∪ {(α,H, f, r, s) : s∈Foe(α, f, r)}. Then op(p) = min, p has |S(p)| children, and each element of S(p) is the subject of exactly one child of p. T5) Subj (p) = (α,H, f, r, s), α ∈ Alg−{ϕ, π′}, H is an α-history, f is a formula, Ax 6|= f , αr /∈H, r∈Rsd[f ], and s∈Foe(α, f, r). Define S(p) = {(α,H+αt,A(t)) : αt /∈H and t∈Rsd[f ; s]} ∪ {−(α\n′,H+α′s,A(s)) : α′s /∈H}. Then op(p) = max, p has |S(p)| children, and each element of S(p) is the subject of exactly one child of p. If S(p) = {} then pv(p) = −1.\nT6) Subj (p) = −(α′,H, F ), α ∈ {π, ψ, β, β′}∪{ψ′ : > is not empty}, H is an α-history, and F is a finite set of formulas. Then op(p) = −; p has exactly one child, say p1; and Subj (p1) = (α\n′,H, F ). T7) If op(p) = min then pv(p) = min{pv(c) : c is a child of p}.\nIf op(p) = max then pv(p) = max{pv (c) : c is a child of p}. If op(p) = − and c is the child of p then pv(p) = −pv(c).\nIt is possible for an evaluation tree to be infinite. Although this can be prevented by insisting that the set of rules in a plausible description is finite, it is not necessary.\nDefinition 5.14. A plausible description P is a plausible theory iff every evaluation tree of P is finite. A Propositional Plausible Logic consists of a plausible theory and its proof relation.\nBoth the proof relation |− and evaluation trees are cumbersome to use for derivations by hand. So we shall define a proof function P , which is easier to use and is a straightforward translation of the proof relation |− of Definition 5.9 into the function P such that (α,H) |−x iff P (α,H, x) = +1 and (α,H) 6|−x iff P (α,H, x) = −1. The auxiliary functions: For (evidence for), and Dftd (defeated), are used in the definition of P .\nDefinition 5.15. Suppose P = (R,>) is a plausible theory, α∈Alg , H is an α-history, and f is a formula. The proof function for P, P , and its auxiliary functions For and Dftd are defined by P1 to P5. P1) If F is a finite set of formulas, then P (α,H,F ) = min{P (α,H, f) : f ∈F}. P2) If Ax |= f then P (α,H, f) = +1. Also P (ϕ,H, f) = +1 iff Ax |= f . P3) If Ax 6|= f and α 6= ϕ then P (α,H, f) = max{For (α,H, f, r) : αr /∈H and r∈Rsd[f ]}. P4) If Ax 6|= f and α 6= ϕ and αr /∈H and r∈Rsd[f ] then For (α,H, f, r) = min[{P (α,H+αr,A(r))} ∪ {Dftd(α,H, f, r, s) : s∈Foe(α, f, r)}]. P5) If Ax 6|= f and α ∈ Alg−{ϕ, π′} and αr /∈H and r∈Rsd[f ] and s∈Foe(α, f, r) then\nDftd(α,H, f, r, s) = max[{P (α,H+αt,A(t)) : αt /∈H and t∈Rsd[f ; s]} ∪ {−P (α′,H+α′s,A(s)) : α′s /∈H}].\nWe end this subsection by stating the relationship between proof relations (Definition 5.9), evaluation trees (Definition 5.13), and proof functions (Definition 5.15). But before we can do this we need the following notation.\nDefinition 5.16. Suppose P = (R,>) is a plausible theory, α∈Alg , H is an α-history, and x is either a formula or finite set of formulas. Let T [α,H, x] denote the evaluation tree of P whose root node has the subject (α,H, x). Let T (α,H, x) denote the proof value of the root node of T [α,H, x].\nTheorem 5.17 (Notational Equivalence). Suppose P is a plausible theory, α∈Alg , H is an α-history, and x is either a formula or a finite set of formulas. Then P (α,H, x) = +1 iff (α,H) |−x iff T (α,H, x) = +1.\nThe idea of ‘logical consequence’ in PPL is defined and most easily understood by considering the proof relation |− of Definition 5.9. The evaluation trees of Definition 5.13 are mainly used to prove results about PPL. Proof functions (Definition 5.15) make hand evaluations easier. So the equivalences expressed in Theorem 5.17 are essential."
    }, {
      "heading" : "5.4 A Truth Theory",
      "text" : "Logics often have a function from the set of all formulas to a set of truth values such that (a) the truth value of a formula is related to its proof value, and\n(b) the truth value of a formula is related to the truth values of its parts. Subsection 3.10 deals with (b), while this subsection is concerned with (a).\nConsider the possibilities that could occur when the proof algorithm α evaluates the evidence for and against the formula f . If there is sufficient evidence for both f and ¬f then, as far as α is concerned, f and ¬f are ambiguous, and so both should be assigned the ambiguous truth value a. If there is sufficient evidence for f but insufficient evidence for ¬f then, as far as α is concerned, f is usually true and ¬f is usually false, so f should be assigned the usually true truth value t and ¬f should be assigned the usually false truth value f. If there is insufficient evidence for both f and ¬f then α does not know enough about f or about ¬f , and so both should be assigned the undetermined truth value u.\nSince the truth value of a formula, f , depends on the proof algorithm, α, evaluating its evidence, we need a veracity (or truth) function V such that V (α, f) is in the set of plausible truth values {a, t, f,u}.\nDefinition 5.18. Suppose P = (R,>) is a plausible theory, α∈Alg , and f is any formula. The truth function for P, V , from Alg×Fml to the set of plausible truth values {a, t, f,u} is defined by V1 to V4. V1) V (α, f) = a iff α |− f and α |−¬f . V2) V (α, f) = t iff α |− f and α 6|− ¬f . V3) V (α, f) = f iff α 6|− f and α |−¬f . V4) V (α, f) = u iff α 6|− f and α 6|− ¬f .\nNow that PPL is defined we need to show that it is well-behaved and satisfies all the principles in Section 3. But before we do that it is worthwhile to get a better understanding of the logic by applying it to some examples."
    }, {
      "heading" : "6. Examples",
      "text" : "We shall show how PPL represents and reasons with the first three signpost examples in Section 3. To save space and effort we shall use some of the theorems in Section 7; this will also illustrate some of the utility of these theorems. In some of the following examples we shall use the following equations denoted by † and ✷. †) P (α,H, {f}) = P (α,H, f), by P1. ✷) P (α,H, {}) = min{} = +1, by P1."
    }, {
      "heading" : "6.1 The Non-Monotonicity Example",
      "text" : "Recall the following from Example 3.2. 1) a is probably true. 2) ¬a is (definitely) true. We show that from (1) the conclusion is ‘a is plausible’; and from (1) and (2), that ‘a is plausible’ cannot be deduced, but ‘¬a is true’ can be.\nThe plausible theory (R,>) which models (1) is defined as follows. The priority relation > is empty, and R = {ra}, where ra is {}⇒a. So Rs = {} = Ax (R) = Ax , R[a] = {ra}, R[¬a] = {}. Also if l∈{a,¬a} and s∈R then R[l; s] = {}.\nEvaluation 6.1.1. α∈{π, ψ, β} and α |− a\n1α) P (α, (), a) = For (α, (), a, ra), by P3 2α) = P (α, (αra), {}), by P4, and Foe(α, a, ra) = {} 3α) = +1, by ✷\nSome evaluations can be parameterised by the proof algorithm. The range of such a parameter is given after the number of the evaluation. If an evaluation proves or disproves something then this is given after the number of the evaluation.\nEvaluation 6.1.1 and Theorem 5.17(Notational Equivalence), shows that π, ψ, and β can prove a using only (1).\nThe plausible theory (R,>) which models (1) and (2) is defined as follows. The priority relation > is empty, and R = {ra, r s na}, where ra is {}⇒ a, and r s na is {}→¬a. Since Rs = {r s na}, Ax (R) = Ax = {¬a}. So by P2, if α∈{ϕ, π, ψ, β} then P (α, (),¬a) = +1.\nHence using only (1) and (2), ¬a is certain, and by Theorem 7.4(1)(Consistency), π, ψ, and β cannot prove a."
    }, {
      "heading" : "6.2 The Ambiguity Puzzle",
      "text" : "We show that the π and ψ proof algorithms are ambiguity propagating and that the β proof algorithm is ambiguity blocking.\nThe plausible theory (R,>) which models the Ambiguity Puzzle (Example 3.3) is defined as follows. The priority relation > is empty, and R = {ra, rna, rb, ranb}, where ra is {}⇒a, rna is {}⇒¬a, rb is {}⇒b, and ranb is {a}⇒¬b.\nSince Rs = {}, Ax (R) = Ax = {}. So R[a] = {ra}, R[b] = {rb}, R[¬a] = {rna}, and R[¬b] = {ranb}. If l∈{a,¬a, b,¬b} and s∈R then R[l; s] = {}.\nEvaluation 6.2.1. α∈{π, ψ, β} 1α) P (α, (), b) = For (α, (), b, rb), by P3 2α) = min{P (α, (αrb), {}), Dftd(α, (), b, rb, ranb)}, by P4 3α) = Dftd(α, (), b, rb, ranb), by ✷ 4α) = −P (α′, (α′ranb), a), by P5, † 5α) = −For(α′, (α′ranb), a, ra), by P3\nEvaluation 6.2.2. α∈{π, ψ} and α 6|− b 5α) P (α, (), b) = −For(α′, (α′ranb), a, ra), by Evaluation 6.2.1 6α) = −P (α′, (α′ranb, α\n′ra), {}), by P4 7α) = −1, by ✷.\nEvaluation 6.2.3. β |− b 5β) P (β, (), b) = −For (β′, (β′ranb), a, ra), by Evaluation 6.2.1 6β) = −min{P (β′, (β′ranb, β ′ra), {}), Dftd(β ′, (β′ranb), a, ra, rna)}, by P4 7β) = −Dftd(β′, (β′ranb), a, ra, rna), by ✷ 8β) = −− P (β, (β′ranb, βrna), {}), by P5 9β) = +1, by ✷.\nBy Evaluation 6.2.2 and Theorems 5.17(Notational Equivalence) and 7.1(Decisiveness), π and ψ cannot prove b and so they are ambiguity propagating. By Evaluation 6.2.3 and Theorem 5.17(Notational Equivalence), β proves b and so is ambiguity blocking."
    }, {
      "heading" : "6.3 The 3-lottery Example",
      "text" : "Recall the following from Example 3.1. 1) Exactly one element of {s1, s2, s3} is true. 2) Each element of {¬s1,¬s2,¬s3} is usually true. 3) The disjunction of any pair of elements of {s1, s2, s3} is usually true. From (2) we get r11 to r13 below. From (3) we get r14 to r16 below. From (1) we have∨ {s1, s2, s3}, ¬ ∧ {s1, s2}, ¬ ∧ {s1, s3}, and ¬ ∧ {s2, s3}. Converting these facts to clauses gives: Ax = { ∨ {s1, s2, s3}, ∨ {¬s1,¬s2}, ∨ {¬s1,¬s3}, ∨ {¬s2,¬s3}}.\nThe plausible theory (R,>) which models this situation is defined as follows. The priority relation > is empty, and R = {r1, r2, ..., r16}, where r1: {} → ∧ Ax , r2: {¬s1} → ∨ {s2, s3}, r5: { ∧ {¬s2,¬s3}} → s1, r8: {s1} → ∧ {¬s2,¬s3}, r3: {¬s2} → ∨ {s1, s3}, r6: { ∧ {¬s1,¬s3}} → s2, r9: {s2} → ∧ {¬s1,¬s3}, r4: {¬s3} → ∨ {s1, s2}, r7: { ∧ {¬s1,¬s2}} → s3, r10: {s3} → ∧ {¬s1,¬s2}, r11: {} ⇒ ¬s1, r14: {} ⇒ ∨ {s1, s2}, r12: {} ⇒ ¬s2, r15: {} ⇒ ∨ {s1, s3}, r13: {} ⇒ ¬s3, r16: {} ⇒ ∨ {s2, s3}.\nLet U = {¬s1,¬s2, ∨ {s1, s2}}. If α∈{π, ψ, β} then we show α proves each element of\nU , α cannot prove the negation of each element of U , and α cannot prove ∧ {¬s1,¬s2}.\nNote Rsd[¬s1] = {r2, r6, r7, r9, r10, r11, r16}, and R s d[s1] = {r5, r8} = R s d[ ∧ {¬s2,¬s3}].\nEvaluation 6.3.1. π |−¬s1 1) P (π, (),¬s1) = max{For (π, (),¬s1, ri) : i∈{2, 6, 7, 9, 10, 11, 16}}, by P3 2) For (π, (),¬s1, r11) = min{P (π, (πr11), {}), Dftd(π, (),¬s1, r11, r5),\nDftd(π, (),¬s1, r11, r8)}, by P4 3) = min{−P (π′, (π′r5), ∧ {¬s2,¬s3}), −P (π\n′, (π′r8), s1)}, by ✷, P5, † 4) P (π′, (π′r5), ∧ {¬s2,¬s3}) = For (π ′, (π′r5), ∧ {¬s2,¬s3}, r8), by P3 5) = P (π′, (π′r5, π ′r8), s1), by P4, † 6) = max{}, by P3 7) = −1. 8) ∴ For (π, (),¬s1, r11) = −P (π\n′, (π′r8), s1), by (7) to (2) 9) = −For(π′, (π′r8), s1, r5), by P3\n10) = −P (π′, (π′r8, π ′r5), ∧ {¬s2,¬s3}), by P4, † 11) = −max{}, by P3 12) = +1. 13) ∴ P (π, (),¬s1) = +1, by (12) to (8), and (1).\nBecause the 3-lottery example is symmetric in s1, s2, and s3, a very similar evaluation gives P (π, (),¬s2) = +1 and P (π, (),¬s3) = +1. Hence by †, P (π, (), {¬s3}) = +1. By Theorem 5.17(Notational Equivalence), π |−¬s1, π |−¬s2, and π |− {¬s3}. By Theorem 7.3(3)(Modus Ponens for strict rules), using r4, we get π |− ∨ {s1, s2}. Thus π proves each element in U = {¬s1,¬s2, ∨ {s1, s2}}.\nSuppose α∈{π, ψ, β}. Then by Theorem 7.6(The proof algorithm hierarchy), α proves each element of U . By Theorem 7.4(1)(Consistency), the negation of each element of U cannot be proved by α. Hence by Theorem 7.3(2)(Right Weakening), α cannot prove ∧ {¬s1,¬s2}."
    }, {
      "heading" : "7. Properties of Propositional Plausible Logic (PPL)",
      "text" : "We shall show that PPL is well-behaved and satisfies all the principles in Section 3.\nTheorem 7.1 (Decisiveness). Suppose P is a plausible theory, α∈Alg, H is an α-history, and x is either a formula or a finite set of formulas. Then either T (α,H, x) = +1 or T (α,H, x) = −1, but not both.\nKnowing that every evaluation will terminate is very comforting. So at the end of each evaluation either we will have a proof or we will not. If we do not have a proof then maybe there is a proof but we missed it. Fortunately decisiveness assures us that an evaluation will always terminate, and when it does we will have either a proof or a disproof — that is a demonstration that there is no proof.\nTheorem 7.2 (Plausible Conjunction). Suppose (R,>) is a plausible description, Ax = Ax (R), α∈Alg, H is an α-history, and f and g are both formulas. If Ax |= f and (α,H) |− g then (α,H) |− ∧ {f, g}.\nThe Plausible Conjunction theorem shows that each α satisfies the Plausible Conjunction Principle (Principle 3.3.2).\nTheorem 7.3 (Right Weakening). Suppose (R,>) is a plausible description, Ax = Ax(R), α∈Alg , H is an α-history, and f and g are both formulas. 1) If (α,H) |− f and Ax∪{f} |= g then (α,H) |− g. [Strong Right Weakening] 2) If (α,H) |− f and f |= g then (α,H) |− g. [Right Weakening] 3) If A→g ∈ Rs and (α,H) |−A then (α,H) |− g. [Modus Ponens for strict rules]\nTheorem 7.3(1) shows that α has the strong right weakening property, and hence has all the right weakening properties mentioned in Subsection 3.6.\nTheorem 7.4 (Consistency). Suppose (R,>) is a plausible theory, Ax = Ax (R), α ∈ {ϕ, π, ψ, β, β′}, and both f and g are any formulas. 1) If α |− f and α |− g then Ax∪{f, g} is satisfiable. 2) If (ψ,H) |− f then (ψ′,H) 6|− ¬f . 3) Suppose that whenever s∈Rsd[¬f ] and (π\n′,H+π′s) |−A(s) then Rsd[f ; s] = {}. If (π,H) |− f then (π′,H) 6|− ¬f .\nPart 1 of Theorem 7.4 says that PPL is strongly 2-consistent. Theorem 7.4(2) says that if there is sufficient evidence for ψ to prove f then the evidence for ¬f is too weak for ψ′ to register. Theorem 7.4(3) gives conditions under which a similar statement can be said about π and π′. In particular when either Rsd[¬f ] or > is empty.\nTheorem 7.5 (Truth Values). Suppose (R,>) is a plausible theory, α∈Alg , F is a finite set of formulas, and f is a formula. 1) V (α,¬¬f) = V (α, f). 2) V (α, f) = t iff V (α,¬f) = f. 3) V (α, f) = f iff V (α,¬f) = t. 4) V (α, f) = a iff V (α,¬f) = a. 5) V (α, f) = u iff V (α,¬f) = u.\n6) If V (α, ∧ F ) = t then for each f in F , V (α, f) = t. 7) If f ∈F and V (α, f) = t then V (α, ∨ F ) = t. 8) If α∈{ϕ, π, ψ, β, β′} then V (α, f)∈{t, f,u}. 9) If V (α, f) = a then α∈{ψ′, π′}.\n10) If V (α, f) = t then α |− f . (completeness) 11) If α∈{ϕ, π, ψ, β, β′} and α |− f then V (α, f) = t. (soundness)\nParts 1 to 5 of Theorem 7.5 show that negation is truth-functional with desirable properties. In Subsection 3.10 the desired relation between the truth values of a conjunction and its conjuncts is given by statement (4), and the desired relation between the truth values of a disjunction and its disjuncts is given by statement (5). Parts 6 and 7 of Theorem 7.5 show that PPL satisfies these relationships.\nThe primed algorithms β′, ψ′, and π′, assess the significance of evidence against a formula. Theorem 7.5(9) shows that the threshold of significance for ψ′ and π′ is so low that they can assess the evidence against both f and ¬f as significant. However Theorem 7.5(8) shows that the other algorithms have a 3-valued truth system. The expected completeness and soundness results are given by parts 10 and 11 of Theorem 7.5.\nThe final result shows the relationships between the various proof algorithms. Recall Definition 5.10 defines P(α) to be the set of all formulas provable from P using the proof algorithm α.\nTheorem 7.6 (The proof algorithm hierarchy). Suppose P = (R,>) is a plausible theory. 1) P(ϕ) ⊆ P(π) ⊆ P(ψ) ⊆ P(β) = P(β′) ⊆ P(ψ′) ⊆ P(π′). 2) If > is empty then P(ϕ) ⊆ P(π) = P(ψ) ⊆ P(β) = P(β′) ⊆ P(ψ′) = P(π′).\nSo β′ proves exactly the same formulas as β. Also if > is empty then π and ψ prove exactly the same formulas, as do π′ and ψ′. The set of formulas proved by π′ is very similar to the union of all extensions of an extension based logic, like Default Logic.\nThe hierarchy shown in Theorem 7.6 is consistent with the intuition that ambiguity propagating proof algorithms are more cautious than ambiguity blocking algorithms. A similar hierarchy for a Defeasible Logic, also consistent with this intuition, is given in Section 5 of (Billington, Antoniou, Governatori, & Maher, 2010).\nWhen a logic has several proof algorithms it is important to determine how they relate, because this gives a greater theoretical understanding of the logic. Theorem 7.6 shows that the proof algorithms of PPL are totally ordered according to reliability or level of confidence. Suppose that the plausible-reasoning situation gives no information concerning whether ambiguity should be blocked or propagated. If the proof algorithm hierarchy is not totally ordered then we could have two incomparable algorithms only one of which proved the formula of interest. In such circumstances it is not clear what should be concluded. By Theorem 7.6 no such dilemma can occur in PPL.\nWe shall now check that PPL satisfies all the principles in Section 3.\nPPL has strict and defeasible rules, and so can distinguish between factual and plausible statements. Moreover PPL does not use numbers, like probabilities, that could lead to a proved formula being more precise than the information used to derive it. So the Representation Principle (Principle 3.1) is satisfied.\nThe correspondence between the general ‘plausible-structure’ notation of Sections 1 and 3 and the particular notation of PPL is as follows. The plausible-structure S corresponds to the plausible description P = (R,>). If we let Ax = Ax(R), then Fact(S) corresponds to Ax , Thm(Fact(S)) corresponds to {f : Ax |= f}, and Thm(L, α,S) corresponds to P(α). For the rest of this section suppose α is in {π, ψ, β}.\nAs explained in the paragraph above Refinement 5.1, the Evidence Principle (Principle 3.2.1) is satisfied. In Subsection 6.1 we showed that α satisfies the Non-Monotonicity Principle (Principle 3.2.2). Hence α satisfies Principle 3.2.\nIn Subsection 6.3 we showed that all three elements of U = {¬s1,¬s2, ∨ {s1, s2}} were α-provable; but that the conjunction ∧ {¬s1,¬s2} was not α-provable. Thus α satisfies the Non-Conjunction Principle (Principle 3.3.1). Theorem 7.2 shows that α satisfies the Plausible Conjunction Principle (Principle 3.3.2). By Theorem 7.4(1), both s1 and s2 are not α-provable. Thus α satisfies the Non-Disjunction Principle (Principle 3.4). Because U is not satisfiable α satisfies the Non-3-Consistency Principle (Principle 3.7.3). Theorem 7.4(1) shows that α satisfies the Strong 2-Consistency Principle (Principle 3.7.2) and so satisfies the 1-Consistency Principle (Principle 3.7.1).\nBy I2 of Definition 5.9, ϕ and α are supraclassical. So by the remark after Principle 3.5, they satisfy the Plausible Supraclassicality Principle (Principle 3.5). Theorem 7.3(1) shows that α has the Strong Right Weakening property. So by the remark after Principle 3.6, α satisfies the Plausible Right Weakening Principle (Principle 3.6).\nBy I2 of Definition 5.9, ϕ is a factual proof algorithm. Subsection 6.2 shows that π and ψ are ambiguity propagating proof algorithms, and β is an ambiguity blocking proof algorithm. Also PPL makes the proof algorithm used explicit. Hence PPL satisfies the Many Proof Algorithms Principle (Principle 3.8). Theorems 7.1 and 5.17 show that ϕ and α satisfy the Decisiveness Principle (Principle 3.9). The truth-value system given in Subsection 5.4 and Theorem 7.5 shows that α satisfies the Included Middle Principle (Principle 3.10).\nThus PPL satisfies all the principles in Section 3."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We have tried to characterise those propositional logics that do plausible reasoning by suggesting some principles that such logics should satisfy. Four important examples of plausible reasoning are presented, and several principles are derived from these examples.\nPropositional Plausible Logic (PPL) has been defined. It satisfies all the principles, and deals with negation, conjunction, and disjunction. PPL has been applied to the first three examples, and several theorems about PPL are proved in the appendices. PPL has been implemented by George Wilson under the direction of Dr. Andrew Rock, who has implemented other Defeasible Logics. As far as we know, PPL is the only non-numeric non-monotonic logic that satisfies all the principles in Section 3 and also correctly reasons with all the examples in Section 3.\nFuture research could make PPL significantly more useful and powerful by incorporating variables in a similar way to the programming language Prolog.\nAcknowledgments\nThe author thanks Michael J. Maher for comments on an earlier version of this paper. The author also thanks René Hexel for helpful discussions about the contents of Section 3 Principles of Plausible Reasoning."
    }, {
      "heading" : "The Appendices",
      "text" : ""
    }, {
      "heading" : "Appendix A. Proof of Theorems 7.1 and 5.17",
      "text" : "Lemma A.1. Let L and M be any two sets of literals. 1) ∨ L |= ∨ M iff either L⊆M or ∨ M is a tautology. 2) ∧ M |= ∧ L iff either L⊆M or ∧ M is a contradiction."
    }, {
      "heading" : "Proof",
      "text" : "Let L and M be any two sets of literals. (1) If L⊆M or ∨ M is a tautology then ∨ L |= ∨ M . Conversely suppose ∨ L |= ∨ M . If L−M = {} then L ⊆M . So suppose there is a literal l such that l ∈ L−M . If ∨ M is not a tautology then there is a valuation v such that v( ∨ M) = F and v(l) = T. But that contradicts ∨ L |= ∨ M , so ∨ M is must be a tautology.\n(2) If L⊆M or ∧ M is a contradiction then ∧ M |= ∧ L. Conversely suppose ∧ M |= ∧ L. If L−M = {} then L ⊆M . So suppose there is a literal l such that l ∈ L−M . If ∧ M is not a contradiction then there is a valuation v such that v( ∧ M) = T and v(l) = F. But that contradicts ∧ M |= ∧ L, so ∧ M is must be a contradiction."
    }, {
      "heading" : "EndProofLemA.1",
      "text" : "Lemma A.2. Let C be a set of clauses. 1) l∈Err(C) iff ∼l∈Err(C). 2) Sat(C) ⊆ C. 3) ∨ {} /∈ Sat(C). 4) ∨ {} /∈ Res(Sat(C)). 5) C is satisfiable iff ∨ {} /∈Res(C). 6) Sat(C) is satisfiable, Res(Sat(C)) is satisfiable, and CorRes(Sat(C)) is satisfiable."
    }, {
      "heading" : "Proof",
      "text" : "Let C be a set of clauses. (1, 2, 3) These parts follow immediately from Definition 2.8. (4) Assume ∨ {} ∈ Res(Sat(C)). Since ∨ {} /∈ Sat(C), there is a literal l such that\n∨ {l}∈Res(Sat(C)) and ∨ {∼l}∈Res(Sat(C)). Since Sat(C) ⊆ C, Res(Sat(C)) ⊆ Res(C). So ∨ {l} ∈ Res(C) and ∨ {∼l} ∈ Res(C). Hence l ∈ Err(C) and ∼l ∈ Err(C). So by Definition 2.8, for all c in Sat(C), l /∈Lit(c). But ∨ {l}∈Res(Sat(C)), so there exists c in Sat(C) such that l∈Lit(c). This contradiction shows that ∨ {} /∈ Res(Sat(C)).\n(5) This is well known from classical propositional logic.\n(6) By parts (4) and (5) of this lemma, Sat(C) is satisfiable. Hence Res(Sat(C)) is satisfiable and so CorRes(Sat(C)) is satisfiable."
    }, {
      "heading" : "EndProofLemA.2",
      "text" : "We say that a set L of literals is contingent iff L is not empty and if a is any atom then {a,¬a} 6⊆L.\nLemma A.3. Suppose (R,>) is a plausible description, Ax = Ax (R), α ∈Alg , H is an α-history, and f is a formula. 1) Ax is satisfiable. 2) Each axiom in Ax is contingent. 3) Each axiom in Ax is either a literal\nor ∨ L where L is a finite set of literals such that |L|≥2.\n4) If R[f ] 6= {} then Ax∪{f} is satisfiable and Ax 6|= ¬f . 5) If (α,H) |− f then Ax∪{f} is satisfiable and Ax 6|= ¬f ."
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible description, Ax = Ax(R), α∈Alg, H is an α-history, and f is a formula.\n(1) By Definition 5.4(PD1), Ax = CorRes(Sat(Ax )). By Lemma A.2(6), CorRes(Sat(Ax )) is satisfiable. Hence Ax is satisfiable.\n(2) By the definitions, Ax = CorRes(Sat(Ax )) = SmpMinCtge(Res(Sat(Ax ))); so each axiom is either contingent or empty. But Ax is satisfiable, so each axiom is contingent.\n(3) This follows from part (2) and Ax = Smp(C) where C is a set of clauses. (4) Suppose R[f ] 6= {}. By Definition 5.5(2), there exists r in R such that Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= f . Hence Ax∪{f} is satisfiable, and so Ax 6|= ¬f . (5) Suppose (α,H) |− f . By Definition 5.9(I2,I3), either Ax |= f or Rsd[f ] 6= {}. If Ax |= f then by part (1), Ax∪{f} is satisfiable. If Rsd[f ] 6= {} then by part (4), Ax∪{f} is satisfiable. But Ax∪{f} is satisfiable implies Ax 6|= ¬f ."
    }, {
      "heading" : "EndProofLemA.3",
      "text" : "Lemma A.4. Suppose (R,>) is a plausible description, and Ax = Ax (R). 1) If r∈Rs then either A(r) = {}; or A(r) = {l}, where l is a literal; or A(r) = { ∧ L}, where |L|≥2 and L is contingent. 2) If r∈Rs then Ax∪A(r) |= c(r). Proof\nSuppose (R,>) is a plausible description, and Ax = Ax(R). (1) By Lemma A.3(2), if ∨ L∈Ax then ∨ L is contingent and so L is contingent. Hence, if {}⊂K⊂L then L−K is contingent. So ∼(L−K) is contingent. Therefore the result holds for all r in Rul(Ax ). So by Definition 5.4(PD2), the result holds for all r in Rs.\n(2) Take any r in Rul(Ax ), and suppose v is a valuation such that v(Ax∪A(r)) is the true truth value; that is, v |= Ax ∪A(r). Then either r is {} → c where c ∈Ax , or r is {smp( ∧ ∼(L−K))}→smp( ∨ K), where {}⊂K⊂L and ∨ L∈Ax . Now v |= c, so in the first case v |= c(r). In the second case v |= ∨ L, and v |= ∧ ∼(L−K). Then for all l ∈ L−K, v |= ∼l and so v 6|= l. But L = K ∪ (L−K). So v |= ∨ (K ∪ (L−K)) and hence v |= ∨ K. Thus v |= c(r) in the second case too. So the lemma holds for all r in Rul(Ax ). Take any r0 in Rs−Rul(Ax ), and suppose v |= Ax ∪A(r0). Then r0 is A(r0) → ∧ c(Rul(Ax , A(r0))). For each r in Rul(Ax , A(r0)), r is A(r0) → c(r). By the previous paragraph, v |= c(r). But this is true for every r in Rul(Ax , A(r0)) and so v |=∧ c(Rul(Ax , A(r0))). EndProofLemA.4\nLemma A.5. Suppose (R0, >) is a plausible description, Ax is its set of axioms, R⊆R0, f and g are formulas, α∈Alg , and {r, s}⊆R0.\n1) R[f ]⊆R. 2) If R′⊆R then R′[f ]⊆R[f ]. 3) If f ≡ g then R[f ] = R[g]. 4) If Ax |= f then R[ ∧ {f, g}] = R[g] and Foe(α, ∧ {f, g}, r) ⊆ Foe(α, g, r). 5) If Ax ∪{f} |= g then (a) Ax ∪{¬g} |= ¬f , (b) R[f ] ⊆ R[g], (c) R[f ; s] ⊆ R[g; s], (d) R[¬g] ⊆ R[¬f ], (e) Foe(α, g, r) ⊆ Foe(α, f, r). 6) If Ax∪{f, g} is unsatisfiable then R[f ] ⊆ R[¬g] and R[g] ⊆ R[¬f ]."
    }, {
      "heading" : "Proof",
      "text" : "(1) By Definition 5.5(2), R[f ]⊆R.\n(2) Suppose R′⊆R. Then R′[f ] = {r∈R′ : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= f} ⊆ {r∈R : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= f} = R[f ].\n(3) Suppose f ≡ g. By Definition 5.5(2), R[f ] = {r∈R : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= f} = {r∈R : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= g} = R[g].\n(4) Suppose Ax |= f . By Definition 5.5(2), R[g] = {r∈R : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= g} = {r∈R : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= f and Ax∪{c(r)} |= g} = {r∈R : Ax∪{c(r)} is satisfiable and Ax∪{c(r)} |= ∧ {f, g}} = R[ ∧ {f, g}].\nLet Claim 1 be: Foe(α, ∧ {f, g}, r) ⊆ Foe(α, g, r).\nIf α∈{ϕ, π′} or r = r then Foe(α, ∧ {f, g}, r) = {}. Hence Claim 1 holds.\nSuppose α = ψ′. Then Foe(ψ′, ∧ {f, g}, r) = {s∈R[¬ ∧ {f, g}] : s > r} and Foe(ψ′, g, r) =\n{s∈R[¬g] : s > r}. Take any s in Foe(ψ′, ∧ {f, g}, r). Then s∈R, Ax∪{c(s)} is satisfiable, Ax ∪{c(s)} |= ¬ ∧ {f, g}, and s > r. But Ax |= f , so Ax ∪{c(s)} |= ¬g and therefore s∈Foe(ψ′, g, r). Hence Claim 1 holds.\nSo suppose α ∈ {π, ψ, β, β′} and r 6= r. Then Foe(α, ∧ {f, g}, r) = {s ∈R[¬ ∧ {f, g}] : s 6< r} and Foe(α, g, r) = {s ∈ R[¬g] : s 6< r}. Take any s in Foe(α, ∧ {f, g}, r). Then s ∈ R, Ax ∪{c(s)} is satisfiable, Ax ∪{c(s)} |= ¬ ∧ {f, g}, and s 6< r. But Ax |= f , so Ax∪{c(s)} |= ¬g and therefore s∈Foe(α, g, r). Hence Claim 1 holds.\nThus Claim 1 is proved.\n(5) Suppose Ax ∪{f} |= g. By Definition 5.5(2,3), R[f ] = {r ∈ R : Ax ∪{c(r)} is satisfiable and Ax∪{c(r)} |= f} and R[f ; s] = {t∈R[f ] : t > s}. (a) Every valuation satisfies exactly one of f or ¬f . Hence Ax∪{¬g} |= ¬f . (b) Take any r in R[f ]. Then Ax ∪{c(r)} is satisfiable and Ax ∪{c(r)} |= f . Hence Ax∪{c(r)} |= g and so r∈R[g]. Thus R[f ] ⊆ R[g]. (c) Take any t in R[f ; s]. Then t∈R[f ] and t > s. By part (b), t∈R[g] and so t∈R[g; s]. Thus R[f ; s] ⊆ R[g; s]. (d) This follows from parts (a) and (b). (e) This follows from parts (a) and (c).\n(6) Suppose Ax ∪{f, g} is unsatisfiable. Take any r in R[f ]. Then Ax ∪{c(r)} is satisfiable and Ax ∪{c(r)} |= f . Hence Ax ∪{c(r)} |= ¬g. Therefore r ∈ R[¬g] and so R[f ] ⊆ R[¬g]. By swapping f and g we get R[g] ⊆ R[¬f ]."
    }, {
      "heading" : "EndProofLemA.5",
      "text" : "We need to extend the notation introduced in Definition 5.16.\nDefinition A.6. Suppose P = (R,>) is a plausible theory, α∈Alg, H is an α-history, F is a finite set of formulas, f is a formula, r∈Rsd[f ], and s∈R[¬f ]. 1) Let T [α,H,F ] be the evaluation tree of P whose root has the subject (α,H,F ); and let T (α,H,F ) be the proof value of the root of T [α,H,F ]. 2) Let T [α,H, f ] be the evaluation tree of P whose root has the subject (α,H, f); and let T (α,H, f) be the proof value of the root of T [α,H, f ]. 3) Let T [α,H, f, r] be the evaluation tree of P whose root has the subject (α,H, f, r); and let T (α,H, f, r) be the proof value of the root of T [α,H, f, r]. 4) Let T [α,H, f, r, s] be the evaluation tree of P whose root has the subject (α,H, f, r, s); and let T (α,H, f, r, s) be the proof value of the root of T [α,H, f, r, s]. 5) Let T [−(α,H,F )] be the evaluation tree of P whose root has the subject −(α,H,F ); and let T (−(α,H,F )) be the proof value of the root of T [−(α,H,F )].\nTheorem A.7 (Theorem 7.1 Decisiveness). Suppose P is a plausible theory, α∈Alg , H is an α-history, and x is either a formula or a finite set of formulas. 1) T [α,H, x] has finitely many nodes. 2) Either T (α,H, x) = +1 or T (α,H, x) = −1 but not both."
    }, {
      "heading" : "Proof",
      "text" : "(1) follows from Definition 5.14. (2) follows from part (1) of this lemma and Definition 5.13."
    }, {
      "heading" : "EndProofThmA.7",
      "text" : "Theorem A.8 (Theorem 5.17 Notational Equivalence). Suppose (R,>) is a plausible theory, α∈Alg , H is an α-history, F is a finite set of formulas, and f is a formula. 1) P (α,H,F ) = +1 iff (α,H) |−F iff T (α,H,F ) = +1. 2) P (α,H, f) = +1 iff (α,H) |− f iff T (α,H, f) = +1. 3) Suppose Ax 6|= f and α 6= ϕ and αr /∈H and r∈Rsd[f ] and f is satisfiable.\nThen For (α,H, f, r) = +1 iff T (α,H, f, r) = +1 iff (α,H+αr) |−A(r) and ∀s∈Foe(α, f, r), Dftd(α,H, f, r, s) = +1.\n4) Suppose Ax 6|= f and α ∈ Alg−{ϕ, π′} and αr /∈H and r∈Rsd[f ] and f is satisfiable and s∈Foe(α, f, r). Then Dftd(α,H, f, r, s) = +1 iff T (α,H, f, r, s) = +1 iff either ∃t∈Rsd[f ; s] such that αt /∈H and (α,H+αt) |−A(t); or α′s /∈H and (α′,H+α′s) 6|−A(s)."
    }, {
      "heading" : "Proof",
      "text" : "Let P = (R,>) be a plausible theory. The proof is by induction on the number of nodes in an evaluation tree of P. Let p be the only node of an evaluation tree of P.\nIf p satisfies T1 then the subject of p is (α,H, {}) and the proof value of p is +1. So T (α,H, {}) = +1. By P1, P (α,H, {}) = min{} = +1. By I1, (α,H) |− {}.\nIf p satisfies T2 then the subject of p is (α,H, f) and the proof value of p is +1. So T (α,H, f) = +1. By P2, P (α,H, f) = +1. By I2, (α,H) |− f .\nIf p satisfies T3 then the subject of p is (α,H, f) and the proof value of p is −1. So T (α,H, f) = −1. Let S(p) = {(α,H, f, r) : αr /∈H and r∈Rsd[f ]}. By T3, S(p) is empty. By P3, P (α,H, f) = max{For (α,H, f, r) : (α,H, f, r)∈S(p)} = max{} = −1. Since S(p) is empty, for all r in Rsd[f ], αr∈H. Hence I3.1 fails and so (α,H) 6|− f .\nSince p has no children, p does not satisfy T4 or T6. If p satisfies T5 then the subject of p is (α,H, f, r, s) and the proof value of p is −1. So T (α,H, f, r, s) = −1. Let S(p) = {(α,H+αt,A(t)) : αt /∈ H and t ∈ Rsd[f ; s]} ∪ {−(α′,H+α′s,A(s)) : α′s /∈H}. Also let S′ = {P (α,H+αt,A(t)) : αt /∈H and t∈Rsd[f ; s]} ∪ {−P (α′,H+α′s,A(s)) : α′s /∈H}. By T5, S(p) is empty, and so S′ is also empty. By P5, Dftd(α,H, f, r, s) = maxS′ = max{} = −1. Since S(p) = {}, we have for each t in Rsd[f ; s], αt∈H; and α\n′s∈H. Hence the last characterisation of Dftd(α,H, f, r, s) = +1 in part (4) is false.\nThus the result holds for all evaluation trees of P that have only the root node. Take any positive integer n. We shall denote the following inductive hypothesis by IndHyp. Suppose the result holds for all evaluation trees of P that have less than n+1 nodes. Let T be an evaluation tree of P that has n+1 nodes and let p be the root of T . Then p has at least one child.\nIf p satisfies T1 then the subject of p is (α,H,F ). By T1, IndHyp, P1, and I1, T (α,H,F ) = +1 iff for all f in F , T (α,H, f) = +1\niff for all f in F , P (α,H, f) = +1 iff P (α,H,F ) = +1 iff for all f in F , (α,H) |− f iff (α,H) |−F .\nSince p has a child, p does not satisfy T2. If p satisfies T3 then the subject of p is (α,H, f). Let S(p) = {(α,H, f, r) : αr /∈H and r∈Rsd[f ]}. By T3, IndHyp, P3, and I3, T (α,H, f) = +1 iff there exists (α,H, f, r) in S(p) such that T (α,H, f, r) = +1 iff there exists (α,H, f, r) in S(p) such that For (α,H, f, r) = +1 iff P (α,H, f) = +1. iff there exists (α,H, f, r) in S(p) such that For (α,H, f, r) = +1 iff there exists (α,H, f, r) in S(p) such that (α,H+αr) |−A(r) and ∀s ∈ Foe(α, f, r),\nDftd(α,H, f, r, s) = +1 iff there exists (α,H, f, r) in S(p) such that (α,H+αr) |−A(r) and ∀s∈Foe(α, f, r),\neither ∃t∈Rsd[f ; s] such that αt /∈H and (α,H+αt) |−A(t); or α′s /∈H and (α′,H+α′s) 6|−A(s)\niff ∃r∈Rsd[f ] such that I3.1 and I3.2 iff (α,H) |− f .\nIf p satisfies T4 then the subject of p is (α,H, f, r). By T4, IndHyp, and P4, T (α,H, f, r) = +1 iff T (α,H+αr,A(r)) = +1 and ∀s∈Foe(α, f, r), T (α,H, f, r, s) = +1 iff (α,H+αr) |−A(r) and ∀s∈Foe(α, f, r), Dftd(α,H, f, r, s) = +1 iff P (α,H+αr,A(r)) = +1 and ∀s∈Foe(α, f, r), Dftd(α,H, f, r, s) = +1 iff For (α,H, f, r) = +1.\nIf p satisfies T5 then the subject of p is (α,H, f, r, s). By T5, IndHyp, T6, T7, and P5, T (α,H, f, r, s) = +1 iff either ∃t∈Rsd[f ; s] such that αt /∈H and T (α,H+αt,A(t)) = +1;\nor α′s /∈H and T (−(α′,H+α′s,A(s))) = +1 iff either ∃t∈Rsd[f ; s] such that αt /∈H and P (α,H+αt,A(t)) = +1;\nor α′s /∈H and T (α′,H+α′s,A(s)) = −1 iff either ∃t∈Rsd[f ; s] such that αt /∈H and P (α,H+αt,A(t)) = +1;\nor α′s /∈H and P (α′,H+α′s,A(s)) = −1\niff Dftd(α,H, f, r, s) = +1.\nIf p satisfies T6 then the subject of p is −(α′,H, F ). By T6, T7, and IndHyp, T (−(α′,H, F )) = +1 iff T (α′,H, F ) = −1 iff P (α′,H, F ) = −1 iff (α′,H) 6|−F .\nThus the theorem is proved by induction."
    }, {
      "heading" : "EndProofThmA.8",
      "text" : ""
    }, {
      "heading" : "Appendix B. Proof of Theorems 7.2, 7.3, 7.4, and 7.5",
      "text" : "Theorem B.1 (Theorem 7.2 Plausible Conjunction). Suppose (R,>) is a plausible description, Ax = Ax(R), α∈Alg, H is an α-history, and f and g are both formulas. If Ax |= f and (α,H) |− g then (α,H) |− ∧ {f, g}."
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible description, Ax = Ax(R), α∈Alg , H is an α-history, and f and g are both formulas. Further suppose that Ax |= f and (α,H) |− g. We shall use Definition 5.9. Since (α,H) |− g, by Definition 5.9(I3), there exists r in Rsd[g] such that I3.1 and I3.2 both hold. By Lemma A.5(4), there exists r in Rsd[ ∧ {f, g}] such that I3.1 and I3.2 both hold. Thus (α,H) |− ∧ {f, g}."
    }, {
      "heading" : "EndProofTheoremB.1",
      "text" : "Theorem B.2 (Theorem 7.3 Right Weakening). Suppose (R,>) is a plausible description, Ax = Ax (R), α∈Alg, H is an α-history, and f and g are both formulas. 1) If (α,H) |− f and Ax∪{f} |= g then (α,H) |− g. [Strong Right Weakening] 2) If (α,H) |− f and f |= g then (α,H) |− g. [Right Weakening] 3) If A→g ∈ Rs and (α,H) |−A then (α,H) |− g. [Modus Ponens for strict rules] Proof\nSuppose (R,>) is a plausible description, Ax = Ax(R), α∈Alg, H is an α-history, and f and g are both formulas. Further suppose that (α,H) |− f . We shall use Definition 5.9.\n(1) Suppose Ax∪{f} |= g. If Ax |= f then Ax |= g and so by I2, (α,H) |− g. So suppose Ax 6|= f . Then α 6= ϕ.\nSince (α,H) |− f , by I3.1 for f , ∃r0∈R s d[f ] such that αr0 /∈H and (α,H+αr0) |−A(r0).\nBy Lemma A.5(5)(b), Rsd[f ] ⊆ R s d[g]. Hence r0∈R s d[g]. So I3.1 holds for g.\nBy Lemma A.5(5)(d,e), R[¬g] ⊆ R[¬f ] and Foe(α, g, r0) ⊆ Foe(α, f, r0). By Lemma A.5(5)(b,c), Rsd[f ] ⊆ R s d[g] and R s d[f ; s] ⊆ R s d[g; s].\nNow take any s0 in Foe(α, g, r0). Then s0 ∈ Foe(α, f, r0). If I3.2.1 holds for f then ∃t0 ∈R s d[f ; s0] such that αt0 /∈H and (α,H+αt0) |−A(t0). Hence t0 ∈R s d[g; s0] and so I3.2.1 holds for g. If I3.2.2 holds for f then α′s0 /∈H and (α ′,H+α′s0) 6|−A(s0). Hence I3.2.2 holds for g.\nThus I3.2 holds for g and so (α,H) |− g.\n(2) Suppose f |= g. Since f |= g, we have Ax∪{f} |= g. So by part (1), (α,H) |− g.\n(3) Suppose A→g ∈ Rs and (α,H) |−A. By Lemma A.4(1), either A = {}, or A = {a} where a is formula.\nCase 1: A = {}. By Definition 5.4, g = ∧ Ax and so Ax |= g. By Definition 5.9(I2), (α,H) |− g.\nCase 2: A = {a} where a is formula. By Definition 5.9(I1), (α,H) |− a. By Lemma A.4(2), Ax∪{a} |= g. So by part (1), (α,H) |− g."
    }, {
      "heading" : "EndProofThmB.2",
      "text" : "Definition B.3. Suppose H is an α-history. If α = ϕ then define H(ϕ :=π′) to be the sequence formed from H by just replacing each ϕ by π′. If α∈{π, π′, ψ, ψ′, β, β′} then define H(α :=π′) to be the sequence formed from H by just replacing each α by π′, and each α′ by π.\nIt is clear that H(α :=π′) is a π′-history.\nLemma B.4. Suppose P = (R,>) is a plausible theory, α∈Alg, H is an α-history, and x is either a formula or a finite set of formulas. If (α,H) |−x then (π′,H(α :=π′)) |−x. Hence P(α)⊆P(π′)."
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible theory. Let Y (n) denote the following conditional statement. “If α∈Alg, H is an α-history, f is a formula, F is a finite set of formulas, x∈{F, f}, T (α,H, x) = +1, and |T [α,H, x]| ≤ n then T (π′,H(α :=π′), x) = +1.”\nBy Theorem A.8(1,2), and Theorem A.7(1), it suffices to prove Y (n) by induction on n. Suppose n = 1. Let the antecedent of Y (1) hold. Let the only node of T [α,H, x] be p0.\nLet the root of T [π′,H(α :=π′), x] be q0. If p0 satisfies T1 then x = {} and so q0 satisfies T1. So by T1, T [π\n′,H(α :=π′), {}] has only one node and T (π′,H(α :=π′), {}) = +1. If p0 satisfies T2 then x = f and Ax |= f . So q0 satisfies T2 and hence T (π\n′,H(α :=π′), f) = +1. Since p0 has no children and the proof value of p0 is +1, p0 does not satisfy T3. Since the subject of p0 is (α,H, x), p0 does not satisfy T4, or T5, or T6. Thus the base case holds.\nTake any positive integer n. Suppose Y (n) is true. We shall prove Y (n+1). Suppose the antecedent of Y (n+1) holds and that |T [α,H, x]| = n+1. Then\nT (α,H, x) = +1. Let p0 be the root of T [α,H, x] and q0 be the root of T [π ′,H(α :=π′), x].\nIf p0 satisfies T1 then x = F . We see that q0 also satisfies T1. So t(p0) = ((α,H,F ),min,+1) and t(q0) = ((π\n′,H(α :=π′), F ),min, w0), where w0 = T (π\n′,H(α :=π′), F ) ∈ {+1,−1}. Let {pf : f ∈F} be the set of children of p0 and {qf : f ∈F} be the set of children of q0. Let f be any formula in F . Then the subject of pf is (α,H, f), and the subject of qf is (π\n′,H(α :=π′), f). Also |T [α,H, f ]| ≤ n and the proof value of pf is +1 because p0 is a min node with proof value +1. So by Y (n) the proof value of qf is +1. But this is true for each f , so w0 = +1, as required.\nSince p0 has a child, p0 does not satisfy T2. If p0 satisfies T3 then x = f . We see that q0 also satisfies T3. So\nt(p0) = ((α,H, f),max,+1) and t(q0) = ((π ′,H(α :=π′), f),max, w0), where w0 = T (π ′,H(α :=π′), f) ∈ {+1,−1}.\nWe shall adopt the following naming conventions. Each non-root node of T [α,H, f ] is denoted by pl(#, y) where l is the level of the node, # is the number in [1..6] such that the node satisfies T#, and y is a rule, or a formula, or a set, which distinguishes siblings. The proof value of pl(#, y) will be denoted by vl(#, y). For non-root nodes in T [π′,H(α :=π′), f ] we shall use ql(#, y), and its proof value will be denoted by wl(#, y).\nLet the set of children of p0 be {p1(4, r) : αr /∈H and r∈R s d[f ]}, where the tags of\nthese children are: t(p1(4, r)) = ((α,H, f, r),min, v1(4, r)). So +1 = max{v1(4, r) : αr /∈H and r∈Rsd[f ]}.\nLet the set of children of q0 be {q1(4, r) : π ′r /∈H(α :=π′) and r∈Rsd[f ]}, where the\ntags of these children are: t(q1(4, r)) = ((π ′,H(α :=π′), f, r),min, w1(4, r)). So w0 = max{w1(4, r) : π ′r /∈H(α :=π′) and r∈Rsd[f ]}.\nFrom above there exists r0 in R s d[f ] such that αr0 /∈H and v1(4, r0) = +1. By\nDefinition B.3, if π′r0∈H(α :=π ′) then αr0∈H. So if αr0 /∈H then π ′r0 /∈H(α :=π ′). Hence q1(4, r0) exists. We shall show that w1(4, r0) = +1 and hence that w0 = +1, as required.\nBecause p1(4, r0) is a min node whose proof value is +1, the proof value of every child of p1(4, r0) must be +1. p2(1, r0) is a child of p1(4, r0) such that t(p2(1, r0)) = ((α,H+αr0, A(r0)),min,+1).\nThe only child of q1(4, r0) is q2(1, r0) where t(q2(1, r0)) = ((π ′,H(α :=π′)+π′r0, A(r0)),min, w2(1, r0)). So w1(4, r0) = w2(1, r0).\nSince |T [α,H+αr0, A(r0)]| ≤ n and T (α,H+αr0, A(r0)) = +1, by Y (n) we have T (π′,H(α :=π′)+π′r0, A(r0)) = w2(1, r0) = +1. Hence w1(4, r0) = +1 and so w0 = +1, as required.\nSince the subject of p0 is (α,H, x), p0 does not satisfy T4, or T5, or T6. Thus Y (n), and hence the lemma, is proved by induction."
    }, {
      "heading" : "EndProofLemB.4",
      "text" : "Definition B.5. Suppose Pd = (R,>) is a plausible description, α∈Alg, H is an α-history, F is a finite set of formulas, f is a formula, r and s are any rules, T is an evaluation tree of Pd, and p is any node of T . If Subj (p) ∈ {(α,H,F ), (α,H, f), (α,H, f, r), (α,H, f, r, s),−(α,H, F )} then the history of p, Hist(p), is defined by Hist(p) = H, and the algorithm of p, alg(p), is defined by alg(p) = α.\nDefinition B.6. Suppose {α, λ} ⊆ Alg, H is a λ-history, and T is an evaluation tree of some plausible theory. If λ /∈{α,α′} then define λ(α :α′) = λ; else define α(α :α′) = α′ and α′(α :α′) = α. If H = (λ1r1, ..., λnrn) then define H(α :α ′) = (λ1(α :α ′)r1, ..., λn(α :α\n′)rn). Define T (α : α′) to be the tree formed from T by only changing the subject of each node as follows. For each node p of T replace alg(p) by alg(p)(α : α′), and replace Hist(p) by Hist(p)(α :α′).\nDefinition B.7. Suppose α∈Alg. α is isomorphic to α′, α ≃ α′, iff for each plausible theory P, if T is an evaluation tree of P then T (α :α′) is an evaluation tree of P.\nIt should be clear that if α ≃ α′ then for each plausible theory P, P(α) = P(α′).\nLemma B.8. β ≃ β′. Hence for each plausible theory P, P(β) = P(β′)."
    }, {
      "heading" : "Proof",
      "text" : "Suppose α∈{β, β′} and P = (R,>) is a plausible theory. Let Y (n) denote the following conditional statement. “If T is an evaluation tree of P and |T | ≤ n then T (α : α′) is an evaluation tree of P.” By Definition 5.14, it suffices to prove Y (n) by induction on n. By Definitions 5.13 and A.6, if T is an evaluation tree of P then either T = T [α,H, ...] or\nT = T [−(α′,H, F )]. So if T (α : α′) is an evaluation tree of P then either T (α : α′) = T [α′,H(α :α′), ...] or T (α :α′) = T [−(α,H(α :α′), F )].\nSuppose n = 1 and the antecedent of Y (1) holds. Let the root of T be p0. Since α∈{β, β′}, without loss of generality we can suppose alg(p0) = α. Let the root of T (α :α\n′) be q0. Since p0 has no children, q0 has no children. If p0 satisfies T1 then let Subj (p0) = (α,H, {}). Hence Subj (q0) = (α\n′,H(α :α′), {}) and so q0 satisfies T1. Thus T (α :α\n′) is an evaluation tree of P. If p0 satisfies T2 then let Subj (p0) = (α,H, f). Hence Subj (q0) = (α\n′,H(α :α′), f) and so q0 satisfies T2. Thus T (α :α\n′) is an evaluation tree of P. If p0 satisfies T3 then let Subj (p0) = (α,H, f). Hence Subj (q0) = (α\n′,H(α :α′), f). Since p0 has no children, S(p0) = {}. So if r∈R s d[f ] then αr∈H. Now αr∈H iff α\n′r∈H(α :α′). Hence S(q0) = {} and so q0 satisfies T3. Thus T (α :α\n′) is an evaluation tree of P. Since p0 and q0 have no children, p0 and q0 satisfy neither T4 nor T6. If p0 satisfies T5 then let Subj (p0) = (α,H, f, r, s). Hence Subj (q0) = (α\n′,H(α :α′), f, r, s). Since p0 has no children, S(p0) = {} and so S(p0, α) = {}. Hence if t ∈ R s d[f ; s] then αt∈H. Now αt∈H iff α′t∈H(α :α′). Also α′s∈H. Hence αs∈H(α :α′). So S(q0, α\n′) = {} and so S(q0) = {}. Thus q0 satisfies T5 and so T (α :α′) is an evaluation tree of P.\nAll cases have been considered and so Y (1) holds.\nIf T is any tree and p is any node of T for which Subj (p) is defined, then define the set S(p, T ) of subjects of the children of p in T by S(p, T ) = {Subj (c) : c is a child of p in T}.\nTake any integer n such that n ≥ 1. Suppose that Y (n) is true. We shall prove Y (n+1). Suppose the antecedent of Y (n+1) holds and that |T | = n+1. Let p0 be the root of T . If alg(p0) /∈{α,α\n′} then T (α :α′) is T and so T (α :α′) is an evaluation tree of P. So suppose alg(p0)∈{α,α ′}. Let q0 be the root of T (α :α ′).\nIf p0 satisfies T1 then let Subj (p0) = (α,H,F ). Hence Subj (q0) = (α ′,H(α :α′), F ). So q0 satisfies T1. Recall that S(p0, T [α,H,F ]) = {(α,H, f) : f ∈F}. So S(q0, T [α,H,F ](α : α′)) = {(α′,H(α :α′), f) : f ∈F} = S(q0, T [α\n′,H(α :α′), F ]), by T1. But for each (α,H, f) in S(p0, T [α,H,F ]), T [α,H, f ] is an evaluation tree of P and |T [α,H, f ]| ≤ n. So by Y (n), T [α,H, f ](α :α′) is an evaluation tree of P. Hence T [α,H, f ](α :α′) = T [α′,H(α : α′), f ]. Thus T (α :α′) = T [α,H,F ](α :α′) = T [α′,H(α :α′), F ] which is an evaluation tree of P.\nSince p0 has a child, p0 does not satisfy T2. If p0 satisfies T3 then let Subj (p0) = (α,H, f). Hence Subj (q0) = (α ′,H(α :α′), f). So q0 satisfies T3. Recall that S(p0, T [α,H, f ]) = {(α,H, f, r) : αr /∈H and r∈R s d[f ]}. Since αr ∈H iff α′r ∈H(α :α′) we have αr /∈H iff α′r /∈H(α :α′). So S(q0, T [α,H, f ](α :α ′)) = {(α′,H(α :α′), f, r) : αr /∈H and r∈Rsd[f ]} = {(α ′,H(α :α′), f, r) : α′r /∈H(α :α′) and r∈Rsd[f ]} = S(q0, T [α ′,H(α :α′), f ]), by T3. But for each (α,H, f, r) in S(p0, T [α,H, f ]), T [α,H, f, r] is an evaluation tree of P and |T [α,H, f, r]| ≤ n. So by Y (n), T [α,H, f, r](α : α′) is an evaluation tree of P. Hence T [α,H, f, r](α : α′) = T [α′,H(α : α′), f, r]. Thus T (α :α′) = T [α,H, f ](α :α′) = T [α′,H(α :α′), f ] which is an evaluation tree of P.\nIf p0 satisfies T4 then let Subj (p0) = (α,H, f, r). Hence Subj (q0) = (α ′,H(α :α′), f, r). Since αr ∈ H iff α′r ∈ H(α : α′) we have αr /∈ H iff α′r /∈ H(α : α′). So q0 satisfies T4. Recall that S(p0, T [α,H, f, r]) = {(α,H+αr,A(r))}∪{(α,H, f, r, s) : s∈Foe(α, f, r)}. Since Foe(α, f, r) = Foe(α′, f, r), S(q0, T [α,H, f, r](α :α\n′)) = {(α′,H(α :α′)+α′r,A(r))} ∪ {(α′,H(α :α′), f, r, s) : s∈Foe(α, f, r)} = S(q0, T [α ′,H(α :α′), f, r]), by T4. But T [α,H+\nαr,A(r)] is an evaluation tree of P and |T [α,H+αr,A(r)]| ≤ n. So by Y (n), T [α,H+ αr,A(r)](α :α′) is an evaluation tree of P. Hence T [α,H+αr,A(r)](α :α′) = T [α′,H(α :α′)+ α′r,A(r)]. Also for each (α,H, f, r, s) in S(p0, T [α,H, f, r]), T [α,H, f, r, s] is an evaluation tree of P and |T [α,H, f, r, s]| ≤ n. So by Y (n), T [α,H, f, r, s](α :α′) is an evaluation tree of P. Hence T [α,H, f, r, s](α :α′) = T [α′,H(α :α′), f, r, s]. Thus T (α :α′) = T [α,H, f, r](α : α′) = T [α′,H(α :α′), f, r] which is an evaluation tree of P.\nIf p0 satisfies T5 then let Subj (p0) = (α,H, f, r, s). Hence Subj (q0) = (α ′,H(α : α′), f, r, s). Since αr ∈H iff α′r ∈H(α : α′) we have αr /∈H iff α′r /∈H(α : α′). So q0 satisfies T5. Recall that S(p0, T [α,H, f, r, s]) = {(α,H+αt,A(t)) : αt /∈H and t∈R s d[f ; s]} ∪ {−(α′,H+α′s,A(s)) : α′s /∈H}. Also since α′r∈H iff αr ∈H(α :α′) we have α′r /∈H iff αr /∈H(α :α′). So S(q0, T [α,H, f, r, s](α :α\n′)) = {(α′,H(α :α′)+α′t, A(t)) : αt /∈H and t∈Rsd[f ; s]}∪{−(α,H(α :α\n′)+αs,A(s)) : α′s /∈H} = {(α′,H(α :α′)+α′t, A(t)) : α′t /∈H(α :α′) and t∈Rsd[f ; s]} ∪ {−(α,H(α :α\n′)+αs,A(s)) : αs /∈H(α :α′)} = S(q0, T [α ′,H(α :α′), f, r, s]), by T5.\nBut for each (α,H+αt,A(t)) in S(p0, T [α,H, f, r, s]), T [α,H+αt,A(t)] is an evaluation tree of P and |T [α,H+αt,A(t)]| ≤ n. So by Y (n), T [α,H+αt,A(t)](α :α′) is an evaluation tree of P. Hence T [α,H+αt,A(t)](α : α′) = T [α′,H(α :α′)+αt,A(t)]. Also if −(α′,H+ α′s,A(s)) ∈ S(p0, T [α,H, f, r, s]), then T [−(α\n′,H+α′s,A(s))] is an evaluation tree of P and |T [−(α′,H+α′s,A(s))]| ≤ n. So by Y (n), T [−(α′,H+α′s,A(s))](α : α′) is an evaluation tree of P. Hence T [−(α′,H+α′s,A(s))](α :α′) = T [−(α,H(α :α′)+αs,A(s))].\nThus T (α :α′) = T [α,H, f, r, s](α :α′) = T [α′,H(α :α′), f, r, s] which is an evaluation tree of P.\nIf p0 satisfies T6 then let Subj (p0) = −(α ′,H, F ). Hence Subj (q0) = −(α,H(α :α ′), F ). So q0 satisfies T6. Recall that S(p0, T [−(α\n′,H, F )]) = {(α′,H, F )}. So S(q0, T [−(α ′,H, F )](α :α′)) = {(α,H(α :α′), F )} = S(q0, T [−(α,H(α :α ′), F )]), by T6. But T [α′,H, F ] is an evaluation tree of P and |T [α′,H, F ]| ≤ n. So by Y (n), T [α′,H, F ](α :α′) is an evaluation tree of P. Hence T [α′,H, F ](α :α′) = T [α,H(α :α′), F ]. Thus T (α :α′) = T [−(α′,H, F )](α :α′) = T [−(α,H(α :α′), F )] which is an evaluation tree of P.\nTherefore Y (n+1), and hence the lemma, is proved by induction."
    }, {
      "heading" : "EndProofLemB.8",
      "text" : "Lemma B.9. Suppose P = (R,>) is a plausible theory, H is a ψ-history, and x is either a formula or a finite set of formulas. If (ψ,H) |−x then (ψ′,H(ψ :ψ′)) |−x. Hence P(ψ)⊆P(ψ′)."
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible theory. Let Y (n) denote the following conditional statement. “If H is a ψ-history, F is a finite set of formulas, f is a formula, x ∈ {F, f}, T (ψ,H, x) = +1, and |T [ψ,H, x]| ≤ n then T (ψ′,H(ψ :ψ′), x) = +1.”\nBy Theorem A.8(1,2), and Theorem 7.1(1), it suffices to prove Y (n) by induction on n.\nSuppose n = 1. Let the antecedent of Y (1) hold. Let the only node of T [ψ,H, x] be p0. Let the root of T [ψ′,H(ψ :ψ′), x] be q0.\nIf p0 satisfies T1 then x = {} and so q0 satisfies T1. So by T1, T [ψ ′,H(ψ :ψ′), {}] has only one node and T (ψ′,H(ψ :ψ′), {}) = +1. If p0 satisfies T2 then x = f and Ax |= f . So q0 satisfies T2 and hence T (ψ ′,H(ψ : ψ′), f) = +1. Since p0 has no children and the\nproof value of p0 is +1, p0 does not satisfy T3. Since the subject of p0 is (ψ,H, x), p0 does not satisfy T4, or T5, or T6. Thus the base case holds.\nTake any positive integer n. Suppose Y (n) is true. We shall prove Y (n+1).\nSuppose the antecedent of Y (n+1) holds and that |T [ψ,H, x]| = n+1. Then T (ψ,H, x) = +1. Let p0 be the root of T [ψ,H, x] and q0 be the root of T [ψ ′,H(ψ :ψ′), x].\nIf p0 satisfies T1 then x = F . We see that q0 also satisfies T1. So t(p0) = ((ψ,H,F ),min,+1) and t(q0) = ((ψ\n′,H(ψ :ψ′), F ),min, w0), where w0 = T (ψ\n′,H(ψ : ψ′), F ) ∈ {+1,−1}. Let {pf : f ∈ F} be the set of children of p0 and {qf : f ∈F} be the set of children of q0. Let f be any formula in F . Then the subject of pf is (ψ,H, f), and the subject of qf is (ψ\n′,H(ψ :ψ′), f). Also |T [ψ,H, f ]| ≤ n and the proof value of pf is +1 because p0 is a min node with proof value +1. So by Y (n) the proof value of qf is +1. But this is true for each f , so w0 = +1, as required.\nSince p0 has a child, p0 does not satisfy T2.\nIf p0 satisfies T3 then x = f . We see that q0 also satisfies T3. So t(p0) = ((ψ,H, f),max,+1) and t(q0) = ((ψ\n′,H(ψ :ψ′), f),max, w0), where w0 = T (ψ ′,H(ψ :ψ′), f) ∈ {+1,−1}.\nWe shall adopt the following naming conventions. Each non-root node of T is denoted by pl(#, y) where l is the level of the node, # is the number in [1..6] such that the node satisfies T#, and y is a rule, or a formula, or a set, which distinguishes siblings. The proof value of pl(#, y) will be denoted by vl(#, y). For non-root nodes in T [ψ\n′,H(ψ :ψ′), f ] we shall use ql(#, y), and its proof value will be denoted by wl(#, y).\nLet the set of children of p0 be {p1(4, r) : ψr /∈H and r ∈R s d[f ]}, where the tags of\nthese children are: t(p1(4, r)) = ((ψ,H, f, r),min, v1(4, r)). So +1 = max{v1(4, r) : ψr /∈H and r∈R s d[f ]}.\nLet the set of children of q0 be {q1(4, r) : ψ ′r /∈ H(ψ : ψ′) and r ∈ Rsd[f ]}, where the tags of these children are: t(q1(4, r)) = ((ψ ′,H(ψ : ψ′), f, r),min, w1(4, r)). So w0 = max{w1(4, r) : ψ ′r /∈H(ψ :ψ′) and r∈Rsd[f ]}.\nFrom above there exists r0 in R s d[f ] such that ψr0 /∈H and v1(4, r0) = +1. By Definition\nB.6, if ψ′r0∈H(ψ :ψ ′) then ψr0∈H. So if ψr0 /∈H then ψ ′r0 /∈H(ψ :ψ ′). Hence q1(4, r0) exists. We shall show that w1(4, r0) = +1 and hence that w0 = +1, as required.\nLet the set of children of p1(4, r0) be {p2(1, r0)} ∪ {p2(5, s) : s∈Foe(ψ, f, r0)}, where the tags of these children are: t(p2(1, r0)) = ((ψ,H+ψr0, A(r0)),min, v2(1, r0)); and t(p2(5, s)) = ((ψ,H, f, r0, s),max, v2(5, s)). So +1 = v1(4, r0) = min[{v2(1, r0)} ∪ {v2(5, s) : s∈Foe(ψ, f, r0)}]. Hence v2(1, r0) = +1; and for each s in Foe(ψ, f, r0), v2(5, s) = +1.\nLet the set of children of q1(4, r0) be {q2(1, r0)} ∪ {q2(5, s) : s∈R[¬f ; r0]}, where the tags of these children are: t(q2(1, r0)) = ((ψ\n′,H(ψ : ψ′)+ψ′r0, A(r0)),min, w2(1, r0)); and t(q2(5, s)) = ((ψ\n′,H(ψ :ψ′), f, r0, s),max, w2(5, s)). So w1(4, r0) = min[{w2(1, r0)} ∪ {w2(5, s) : s∈R[¬f ; r0]}].\nSince v2(1, r0) = +1, T (ψ,H+ψr0, A(r0)) = +1 and |T [ψ,H+ψr0, A(r0)]| ≤ n. So by Y (n), w2(1, r0) = T (ψ\n′,H(ψ :ψ′)+ψ′r0, A(r0)) = +1. Therefore w1(4, r0) = min{w2(5, s) : s ∈ R[¬f ; r0]}. If R[¬f ; r0] = {} then w1(4, r0) = min{} = +1 as desired. So suppose R[¬f ; r0] 6= {}.\nSince R[¬f ; r0]⊆Foe(ψ, f, r0), if q2(5, s) exists then p2(5, s) exists.\nFor each node, p2(5, s), let the set of children of p2(5, s) be {p3(1, t) : ψt /∈ H and t ∈ Rsd[f ; s]} ∪ {p3(6, s) : ψ\n′s /∈ H}, where the tags of these children are: t(p3(1, t)) = ((ψ,H+ψt,A(t)),min, v3(1, t)); and t(p3(6, s)) = (−(ψ\n′,H+ψ′s,A(s)),−, v3(6, s)). So +1 = v2(5, s) = max[{v3(1, t) : ψt /∈H and t∈R s d[f ; s]} ∪ {v3(6, s) : ψ\n′s /∈H}]. For each node, q2(5, s), let the set of children of q2(5, s) be {q3(1, t) : ψ\n′t /∈ H(ψ : ψ′) and t ∈ Rsd[f ; s]} ∪ {q3(6, s) : ψs /∈ H(ψ : ψ\n′)}, where the tags of these children are: t(q3(1, t)) = ((ψ\n′,H(ψ : ψ′)+ψ′t, A(t)),min, w3(1, t)); and t(q3(6, s)) = (−(ψ,H(ψ : ψ′)+ψs,A(s)),−, w3(6, s)). So w2(5, s) = max[{w3(1, t) : ψ\n′t /∈H(ψ :ψ′) and t∈Rsd[f ; s]} ∪ {w3(6, s) : ψs /∈H(ψ :ψ\n′)}]. By Definition B.6, ψt∈H iff ψ′t∈H(ψ :ψ′). So ψt /∈H iff ψ′t /∈H(ψ :ψ′). Therefore p3(1, t) exists iff q3(1, t) exists. If there exists t0 ∈ R s d[f ; s] such that v3(1, t0) = +1 then T (ψ,H+ψt0, A(t0)) = +1 and |T [ψ,H+ψt0, A(t0)]| ≤ n so by Y (n), w3(1, t0) = T (ψ′,H(ψ :ψ′)+ψ′t0, A(t0)) = +1. Hence w2(5, s) = +1.\nSo suppose no such t0 ∈ R s d[f ; s] exists. Then p3(6, s) exists such that ψ ′s /∈H and v3(6, s) = +1. By Definition B.6, ψ\n′s∈H iff ψs∈H(ψ :ψ′). So ψ′s /∈H iff ψs /∈H(ψ :ψ′). Hence q3(6, s) exists.\nLet the child of p3(6, s) be p4(1, s) where t(p4(1, s)) = ((ψ ′,H+ψ′s,A(s)),min, v4(1, s))\nand v3(6, s) = −v4(1, s). So v4(1, s) = −1. Let the child of q3(6, s) be q4(1, s) where t(q4(1, s)) = ((ψ,H(ψ :ψ ′)+ψs,A(s)),min, w4(1, s)) and w3(6, s) = −w4(1, s).\nAssume w4(1, s) = +1. Then T (ψ,H(ψ :ψ ′)+ψs,A(s)) = +1 and\n|T [ψ,H(ψ : ψ′)+ψs,A(s)]| ≤ n so by Y (n), T (ψ′,H(ψ : ψ′)(ψ : ψ′)+ψ′s,A(s)) = +1. But H(ψ : ψ′)(ψ : ψ′) = H. So T (ψ′,H+ψ′s,A(s)) = +1. From above −1 = v4(1, s) = T (ψ′,H+ψ′s,A(s)) = +1. This contradiction shows that w4(1, s) = −1. Hence w3(6, s) = +1 and so w2(5, s) = +1.\nSo in both cases for all s in R[¬f ; r0], w2(5, s) = +1 and so w1(4, r0) = +1. Hence w0 = +1, as required.\nSince the subject of p0 is (ψ,H, x), p0 does not satisfy T4, or T5, or T6. Thus Y (n), and hence the lemma, is proved by induction."
    }, {
      "heading" : "EndProofLemB.9",
      "text" : "Lemma B.10. Suppose (R,>) is a plausible theory, Ax = Ax (R), and f is a formula. If f is satisfiable and Ax 6|= f then Rsd[f ] is finite. Proof\nSuppose (R,>) is a plausible theory, Ax = Ax (R), and f is a formula. Also suppose f is satisfiable and Ax 6|= f . Take any α in Alg−{ϕ}. By Definition 5.14, T [α, (), f ] is finite, and so its root has only finitely many children. The root of T [α, (), f ] satisfies T3 of Definition 5.13. Therefore Rsd[f ] is finite. EndProofLemB.10\nTheorem B.11 (Theorem 7.4 Consistency). Suppose (R,>) is a plausible theory, Ax = Ax (R), α∈{ϕ, π, ψ, β, β′}, and both f and g are any formulas. 1) If α |− f and α |− g then Ax∪{f, g} is satisfiable. 2) If (ψ,H) |− f then (ψ′,H) 6|− ¬f . 3) Suppose that whenever s∈Rsd[¬f ] and (π\n′,H+π′s) |−A(s) then Rsd[f ; s] = {}. If (π,H) |− f then (π′,H) 6|− ¬f ."
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible theory, Ax = Ax (R), α∈{ϕ, π, ψ, β, β′}, and both f and g are any formulas.\n(1) Suppose α |− f and α |− g. So by Theorem A.8(2), T (α, (), f) = +1 and T (α, (), g) = +1.\nLet p0 be the root of T [α, (), f ] and q0 be the root of T [α, (), g]. Since the subject of p0 is (α, (), f), p0 does not satisfy T1, or T4, or T5, or T6. Since the subject of q0 is (α, (), g), q0 does not satisfy T1, or T4, or T5, or T6. Therefore p0 satisfies T2 or T3, and q0 satisfies T2 or T3. So there are four cases to consider.\nCase 1: p0 satisfies T2 and q0 satisfies T2. Then Ax |= f and Ax |= g. By Lemma A.3(1), Ax is satisfiable. Therefore Ax∪{f, g} is satisfiable.\nCase 2: p0 satisfies T2 and q0 satisfies T3. Then Ax |= f , r∈R[f ], Ax 6|= g, and α 6= ϕ. So α∈{π, ψ, β, β′}. Assume Ax∪{f, g} is unsatisfiable. By Lemma A.5(6), R[f ] ⊆ R[¬g]. So r∈R[¬g]. By T3, q0 has a child, q1, in T [α, (), g] such that t(q1) = ((α, (), g, rg),min,+1) and rg∈R s d[g]. By T4, q1 has a child, q2, in T [α, (), g] such that t(q2) = ((α, (), g, rg , r),max,+1). By T5, q2 has a child, q3, in T [α, (), g] such that pv(q3) = +1. By T5, Subj (q3) ∈ S(q2). By Definition 5.5(3), Rsd[g; r] = {} and so S(q2) = S(q2, α). However, A(r) = {}. So t(q3) = (−(α\n′, (α′r), {}),−,+1). By T6, q3 has a child, q4 in T [α, (), g] such that Subj (q4) = (α\n′, (α′r), {}). So by T1, pv(q4) = +1. But by T7, +1 = pv(q3) = −pv(q4) = −1. This contradiction shows that Ax∪{f, g} is satisfiable.\nCase 3: p0 satisfies T3 and q0 satisfies T2. This case is the same as Case 2 but with p and q interchanged and with f and g interchanged. So by doing the indicated interchanges the proof for Case 2 becomes a proof for Case 3.\nCase 4: p0 satisfies T3 and q0 satisfies T3. Then Ax 6|= f , Ax 6|= g, and α 6= ϕ. So α∈{π, ψ, β, β′}. By T3, p0 has a child, p1, in T [α, (), f ] such that t(p1) = ((α, (), f, rf ),min,+1) and rf ∈R s d[f ]. By T3, q0 has a child, q1, in T [α, (), g] such that t(q1) = ((α, (), g, rg),min,+1) and rg∈R s d[g].\nAssume Ax∪{f, g} is unsatisfiable. By Lemma A.5(6), Rsd[f ] ⊆ R[f ] ⊆ R[¬g] and Rsd[g] ⊆ R[g] ⊆ R[¬f ]. So rf ∈R[¬g] and rg∈R[¬f ].\nBy T4, either rf>rg; or p1 has a child, p2(rg), in T [α, (), f ] such that Subj (p2(rg)) = (α, (), f, rf , rg) and pv(p2(rg)) = +1. By T5, p2(rg) has a child, p3(rg) in T [α, (), f ] such that pv(p3(rg)) = +1 and Subj (p3(rg)) ∈ S(p2(rg)).\nSimilarly by T4, either rg>rf ; or q1 has a child, q2(rf ), in T [α, (), g] such that Subj (q2(rf )) = (α, (), g, rg , rf ) and pv(q2(rf )) = +1. By T5, q2(rf ) has a child, q3(rf ) in T [α, (), g] such that pv(q3(rf )) = +1 and Subj (q3(rf )) ∈ S(q2(rf )).\nCase 4.1: Subj (p3(rg)) = −(α ′, (α′rg), A(rg)).\nSince pv(p3(rg)) = +1, T (α ′, (α′rg), A(rg)) = −1. So by Theorem A.8(1), (α′, (α′rg)) 6|−A(rg). But Subj (q2(rg)) = (α, (αrg), A(rg)) and pv(q2(rg)) = +1. So T (α, (αrg), A(rg)) = +1. By Theorem A.8(1), (α, (αrg)) |−A(rg). By Lemmas B.4, B.9, and B.8, (α′, (α′rg)) |−A(rg). This contradiction shows that Case 4.1 cannot occur. Thus Subj (p3(rg)) = (α, (αt), A(t)) where t∈R s d[f ; rg].\nCase 4.2: Subj (q3(rf )) = −(α ′, (α′rf ), A(rf )).\nThis case is the same as Case 4.1 but with p and q interchanged and with f and g interchanged. So by doing the indicated interchanges the proof that Case 4.1 cannot occur becomes a proof that Case 4.2 cannot occur. Thus Subj (q3(rf )) = (α, (αt), A(t)) where t∈Rsd[g; rf ].\nIn summary Cases 4.1 and 4.2 have shown that we have either rf >rg or there is a t in R s d[f ; rg]; and also either rg>rf or there is a t in R s d[g; rf ].\nSo there exists tf (1) in R s d[f ; rg] ⊆ R s d[f ] ⊆ R[f ] ⊆ R[¬g]. Hence tf (1) > rg and\ntf (1) ∈ R[¬g]. So q2(rf ) can be replaced by q2(tf (1)), and q3(rf ) can be replaced by q3(tf (1)).\nAlso there exists tg(1) in R s d[g; rf ] ⊆ R s d[g] ⊆ R[g] ⊆ R[¬f ]. Hence tg(1) > rf and\ntg(1) ∈ R[¬f ]. So p2(rg) can be replaced by p2(tg(1)), and p3(rg) can be replaced by p3(tg(1)).\nSimilarly, the arguments in Cases 4.1 and 4.2 for these new nodes yield rules tf (2) and tg(2) with the following properties: tf (2) ∈ R s d[f ; tg(1)] ⊆ R s d[f ] ⊆ R[f ] ⊆ R[¬g]; and tg(2) ∈ R s d[g; tf (1)] ⊆ R s d[g] ⊆ R[g] ⊆ R[¬f ]. So tf (2) > tg(1) and tf (2)∈R[¬g] and tg(2) > tf (1) and tg(2)∈R[¬f ]. Hence tf (2) > tg(1) > rf and tg(2) > tf (1) > rg.\nThis process can be continued indefinitely to yield the following sequences of rules. rf < tg(1) < tf (2) < tg(3) < tf (4) < ... and rg < tf (1) < tg(2) < tf (3) < tg(4) < ... Now each tf (i)∈R s d[f ] and each tg(i)∈R s d[g]. Since α |− f and α |− g, by Lemma A.3(5), both f and g are satisfiable. But Ax 6|= f and Ax 6|= g, so by Lemma B.10, both Rsd[f ] and R s d[g] are finite. So for some i and some j>i, tf (2i) = tf (2j). Hence > is cyclic, which contradicts the definition of > as being acyclic. This contradiction shows that Ax∪{f, g} is satisfiable.\n(2) If H is a ψ-history then H is also a ψ′-history. Suppose (ψ,H) |− f . We shall use Definition 5.9. Assume (ψ′,H) |−¬f .\nBy I3.1 for ¬f , (*1) ∃s1∈R s d[¬f ] such that (ψ ′,H+ψ′s1) |−A(s1). By I3.2 for f either (ψ′,H+ψ′s1) 6|−A(s1), which contradicts (*1), or (*2) ∃r2∈R s d[f ; s1] such that (ψ,H+ψr2) |−A(r2). By I3.2 for ¬f either (ψ,H+ψr2) 6|−A(r2), which contradicts (*2), or (*3) ∃s3∈R s d[¬f ; r2] such that (ψ\n′,H+ψ′s3) |−A(s3). By I3.2 for f either (ψ′,H+ψ′s3) 6|−A(s3), which contradicts (*3), or (*4) ∃r4∈R s d[f ; s3] such that (ψ,H+ψr4) |−A(r4). So we have r4 > s3 > r2 > s1.\nWe can continue the reasoning in the above paragraph to create two arbitrarily long sequences s1, s3, ..., s2i−1, ... and r2, r4, ..., r2i, ... such that each s2i−1∈R s d[¬f ] and each r2i∈R s d[f ]. Moreover for each odd i, si+2 > ri+1 > si. Since (ψ,H) |− f , by Lemma A.3(5), f is satisfiable and Ax 6|= ¬f . Since (ψ′,H) |−¬f , by Lemma A.3(5), ¬f is satisfiable and Ax 6|= f . So by Lemma B.10, both Rsd[f ] and R s d[¬f ] are finite. So there is an even j and an even k such that j < k and rj = rk. Hence > is cyclic, contradicting its acyclicity. Thus (2) is proved.\n(3) Suppose that (*) whenever s∈Rsd[¬f ] and (π ′,H+π′s) |−A(s) then Rsd[f ; s] = {}.\nIf H is a π-history then H is also a π′-history. Suppose (π,H) |− f . We shall use Definition 5.9. Assume (π′,H) |−¬f .\nBy I3.1 for ¬f , (**) ∃s1∈R s d[¬f ] such that (π ′,H+π′s1) |−A(s1). By I3.2 for f either (π′,H+π′s1) 6|−A(s1), which contradicts (**), or ∃r2∈R s d[f ; s1] such that (π,H+πr2) |−A(r2), which contradicts (*). Thus (3) is proved. EndProofThmB.11\nTheorem B.12 (Theorem 7.5 Truth values). Suppose (R,>) is a plausible theory, α∈Alg , F is a finite set of formulas, and f is a formula.\n1) V (α,¬¬f) = V (α, f). 2) V (α, f) = t iff V (α,¬f) = f. 3) V (α, f) = f iff V (α,¬f) = t. 4) V (α, f) = a iff V (α,¬f) = a. 5) V (α, f) = u iff V (α,¬f) = u. 6) If V (α, ∧ F ) = t then for each f in F , V (α, f) = t. 7) If f ∈F and V (α, f) = t then V (α, ∨ F ) = t. 8) If α∈{ϕ, π, ψ, β, β′} then V (α, f)∈{t, f,u}. 9) If V (α, f) = a then α∈{ψ′, π′}.\n10) If V (α, f) = t then α |− f . (completeness) 11) If α∈{ϕ, π, ψ, β, β′} and α |− f then V (α, f) = t. (soundness)"
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible theory, α∈Alg, F is a finite set of formulas, and f is a formula. By Theorem 7.3(2), α |− f iff α |−¬¬f ; and so α 6|− f iff α 6|− ¬¬f .\n(1) This follows from Definition 5.18 and the equivalences noted above.\n(2) V (α, f) = t iff α |− f and α 6|− ¬f . V (α,¬f) = f iff α 6|− ¬f and α |−¬¬f . So (2) holds.\n(3) V (α, f) = f iff α 6|− f and α |−¬f . V (α,¬f) = t iff α |−¬f and α 6|− ¬¬f . So (3) holds.\n(4) V (α, f) = a iff α |− f and α |−¬f . V (α,¬f) = a iff α |−¬f and α |−¬¬f . So (4) holds.\n(5) V (α, f) = u iff α 6|− f and α 6|− ¬f . V (α,¬f) = u iff α 6|− ¬f and α 6|− ¬¬f . So (5) holds.\n(6) Suppose V (α, ∧ F ) = t. Then α |− ∧ F and α 6|− ¬ ∧ F . By Theorem B.2(2), for\neach f in F , α |− f . Take any f in F and assume α |−¬f . By Theorem B.2(2), α |− ∨ ¬F , where ¬F = {¬f : f ∈F}. So by Theorem B.2(2), α |−¬ ∧ F . This contradiction shows that for each f in F , α 6|− ¬f . Thus for each f in F , V (α, f) = t.\n(7) Suppose f ∈F and V (α, f) = t. Then α |− f and α 6|− ¬f . By Theorem B.2(2), α |− ∨ F . Assume α |−¬ ∨ F . By Theorem B.2(2), α |− ∧ ¬F and so α |−¬f . This contradiction shows that α 6|− ¬ ∨ F . Thus V (α, ∨ F ) = t.\n(8) Suppose α∈{ϕ, π, ψ, β, β′}. Recall V (α, f) = a iff α |− f and α |−¬f . So by Theorem 7.4(1), V (α, f) 6= a.\n(9) This is just the contrapositive of part (8).\n(10) Recall V (α, f) = t iff α |− f and α 6|− ¬f .\n(11) Suppose α∈{ϕ, π, ψ, β, β′} and α |− f . By Definition 5.18 and α |− f we have V (α, f)∈{a, t}. So by part (8), V (α, f) = t.\nEndProofThmB.12"
    }, {
      "heading" : "Appendix C. Proof of Theorem 7.6",
      "text" : "Lemma C.1. Suppose P = (R,>) is a plausible theory, α∈Alg , I is a ϕ-history, H is an α-history, and x is either a formula or a finite set of formulas. If (ϕ, I) |−x then (α,H) |−x. Hence P(ϕ)⊆P(α)."
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible theory, Ax = Ax(R), α∈Alg , H is an α-history, and x is either a formula or a finite set of formulas. Let (ϕ, I) |−x. We shall use Definition 5.9.\nCase 1: x is a formula. Let x = f . Then (ϕ, I) |− f . By Definition 5.9(I2), Ax |= f and (α,H) |− f .\nCase 2: x is a finite set of formulas. Let x = F . Then (ϕ, I) |−F . By I1, for all f in F , (ϕ, I) |− f . By Case 1, (α,H) |− f . So by I1, (α,H) |−F ."
    }, {
      "heading" : "EndProofLemC.1",
      "text" : "Definition C.2. If H is a π-history then define H(π :=ψ) to be the sequence formed from H by just replacing each π by ψ, and each π′ by ψ′.\nLemma C.3. Suppose P = (R,>) is a plausible theory, H is a π-history, and x is either a formula or a finite set of formulas. If (π,H) |−x then (ψ,H(π :=ψ)) |−x. Hence P(π)⊆P(ψ)."
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible theory and Ax is its set of axioms. Let Y (n) denote the following conditional statement. “If H is a π-history, x is either a formula or a finite set of formulas, T (π,H, x) = +1, and |T [π,H, x]| ≤ n then T (ψ,H(π :=ψ), x) = +1.”\nBy Theorem A.8(1,2), and Theorem A.7(1), it suffices to prove Y (n) by induction on n.\nSuppose n = 1. Let the antecedent of Y (1) hold. Let p0 be the root of T [π,H, x] and q0 be the root of T [ψ,H(π :=ψ), x]. Then p0 has no children. If p0 satisfies T1 then x = {} and so q0 satisfies T1. So by T1, T [ψ,H(π :=ψ), {}] has only one node and T (ψ,H(π :=ψ), {}) = +1. If p0 satisfies T2 then x = f and Ax |= f . So q0 satisfies T2 and hence T (ψ,H(π :=ψ), f) = +1. Since p0 has no children and the proof value of p0 is +1, p0 does not satisfy T3. Since the subject of p0 is (π,H, x), p0 does not satisfy T4, or T5, or T6. Thus the base case holds.\nTake any positive integer n. Suppose that Y (n) is true. We shall prove Y (n+1).\nSuppose the antecedent of Y (n+1) holds and that |T [π,H, x]| = n+1. Let p0 be the root of T [π,H, x] and q0 be the root of T [ψ,H(π :=ψ), x].\nIf p0 satisfies T1 then let x be F . We see that q0 also satisfies T1. So t(p0) = ((π,H,F ),min,+1) and t(q0) = ((ψ,H(π :=ψ), F ),min, w0), where w0 = T (ψ,H(π :=ψ), F ) ∈ {+1,−1}. Let {pf : f ∈F} be the set of children of p0 and {qf : f ∈F} be the set of children of q0. Let f be any formula in F . Then the subject of pf is (π,H, f), and the subject of qf is (ψ,H(π :=ψ), f). Also |T [π,H, f ]| ≤ n and the proof value of pf is +1 because p0 is a min node with proof value +1. So by Y (n) the proof value of qf is +1. But this is true for each f , so w0 = +1, as required.\nSince p0 has a child, p0 does not satisfy T2.\nIf p0 satisfies T3 then let x be f . We see that q0 also satisfies T3. So t(p0) = ((π,H, f),max,+1) and t(q0) = ((ψ,H(π :=ψ), f),max, w0), where w0 = T (ψ,H(π :=ψ), f) ∈ {+1,−1}.\nWe shall adopt the following naming conventions. Each non-root node of T [π,H, f ] is denoted by pl(#, y) where l is the level of the node, # is the number in [1..6] such that the node satisfies T#, and y is a rule, or a formula, or a set, which distinguishes siblings. The proof value of pl(#, y) will be denoted by vl(#, y). For non-root nodes in T [ψ,H(π :=ψ), f ] we shall use ql(#, y), and its proof value will be denoted by wl(#, y).\nLet the set of children of p0 be {p1(4, r) : πr /∈H and r∈R s d[f ]}, where the tags of\nthese children are: t(p1(4, r)) = ((π,H, f, r),min, v1(4, r)). So +1 = max{v1(4, r) : πr /∈H and r∈R s d[f ]}.\nLet the set of children of q0 be {q1(4, r) : ψr /∈H(π :=ψ) and r∈R s d[f ]}, where the\ntags of these children are: t(q1(4, r)) = ((ψ,H(π :=ψ), f, r),min, w1(4, r)). So w0 = max{w1(4, r) : ψr /∈H(π :=ψ) and r∈R s d[f ]}.\nFrom above there exists r0 in R s d[f ] such that πr0 /∈H and v1(4, r0) = +1. So\nt(p1(4, r0)) = ((π,H, f, r0),min,+1).\nLet the set of children of p1(4, r0) be {p2(1, r0)} ∪ {p2(5, s) : s∈Foe(π, f, r0)}, where the tags of these children are: t(p2(1, r0)) = ((π,H+πr0, A(r0)),min, v2(1, r0)); and t(p2(5, s)) = ((π,H, f, r0, s),max, v2(5, s)). So +1 = v1(4, r0) = min[{v2(1, r0)} ∪ {v2(5, s) : s∈Foe(π, f, r0)}]. Hence v2(1, r0) = +1; and for each s in Foe(π, f, r0), v2(5, s) = +1.\nLet the set of children of q1(4, r0) be {q2(1, r0)} ∪ {q2(5, s) : s∈Foe(ψ, f, r0)}, where the tags of these children are: t(q2(1, r0)) = ((ψ,H(π :=ψ)+ψr0, A(r0)),min, w2(1, r0)); and t(q2(5, s)) = ((ψ,H(π :=ψ), f, r0, s),max, w2(5, s)). So w1(4, r0) = min[{w2(1, r0)} ∪ {w2(5, s) : s∈Foe(ψ, f, r0)}].\nSince |T [π,H+πr0, A(r0)]| < n and T (π,H+πr0, A(r0)) = v2(1, r0) = +1, by Y (n) we have T (ψ,H(π :=ψ)+ψr0, A(r0)) = w2(1, r0) = +1. Hence (*) w1(4, r0) = min{w2(5, s) : s∈Foe(ψ, f, r0)}.\nFor each s in Foe(π, f, r0) let the set of children of p2(5, s) be {p3(1, t) : πt /∈H and t∈R s d[f ; s]} ∪ {p3(6, s) : π\n′s /∈H}, where the tags of these children are: t(p3(1, t)) = ((π,H+πt,A(t)),min, v3(1, t)); and t(p3(6, s)) = (−(π\n′,H+π′s,A(s)),−, v3(6, s)). So +1 = v2(5, s) = max[{v3(1, t) : πt /∈H and t∈R s d[f ; s]} ∪ {v3(6, s) : π ′s /∈H}.\nFor each s in Foe(ψ, f, r0) let the set of children of q2(5, s) be {q3(1, t) : ψt /∈H(π :=ψ) and t∈Rsd[f ; s]} ∪ {q3(6, s) : ψ\n′s /∈H(π :=ψ)}, where the tags of these children are: t(q3(1, t)) = ((ψ,H(π :=ψ)+ψt,A(t)),min, w3(1, t)); and t(q3(6, s)) = (−(ψ\n′,H(π :=ψ)+ψ′s,A(s)),−, w3(6, s)). So w2(5, s) = max[{w3(1, t) : ψt /∈H(π :=ψ) and t∈R s d[f ; s]} ∪ {w3(6, s) : ψ ′s /∈H(π :=ψ)}].\nTake any s in Foe(ψ, f, r0). We shall show that w2(5, s) = +1. Suppose there exists t0 such that πt0 /∈H and t0∈R s d[f ; s] and\n+1 = v3(1, t0) = T (π,H+πt0, A(t0)). Then p3(1, t0) exists, and ψt0 /∈H(π :=ψ) and so q3(1, t0) exists. Since |T [π,H+πt0, A(t0)]| < n, then by Y (n) we have T (ψ,H(π :=ψ)+ψt0, A(t0)) = w3(1, t0) = +1. Hence w2(5, s) = +1.\nSo suppose that such a t0 does not exist. Then v3(6, s) = +1 and so p3(6, s) exists and π′s /∈H. Hence ψ′s /∈H(π :=ψ), and so q3(6, s) exists. Let the child of p3(6, s) be p4(1, s) where t(p4(1, s)) = ((π\n′,H+π′s,A(s)),min, v4(1, s)). Then +1 = v3(6, s) = −v4(1, s). So v4(1, s) = −1 and hence T (π\n′,H+π′s,A(s)) = −1. By Theorem A.8(1), (π′,H+π′s) 6|−A(s). By Definition B.3 with α = ψ′ and Definition C.2, H(π :=ψ)(ψ′ :=π′) = H and (ψ′s)(ψ′ :=π′) = (π′s).\nLet the child of q3(6, s) be q4(1, s) where t(q4(1, s)) = ((ψ\n′,H(π :=ψ)+ψ′s,A(s)),min, w4(1, s)). Then w3(6, s) = −w4(1, s). Assume w4(1, s) = +1. Then T (ψ\n′,H(π :=ψ)+ψ′s,A(s)) = +1 and so by Theorem A.8(1), (ψ′,H(π :=ψ)+ψ′s) |−A(s). By Lemma B.4 with α = ψ′, (π′,H(π :=ψ)(ψ′ :=π′)++(ψ′s)(ψ′ :=π′)) |−A(s). But from the previous paragraph, this simplifies to (π′,H+π′s) |−A(s). This contradiction shows that w4(1, s) = −1. Hence w3(6, s) = +1. Therefore w2(5, s) = +1.\nThus for all s in Foe(ψ, f, r0), w2(5, s) = +1. So by (*), w1(4, r0) = +1 and hence w0 = +1, as required.\nSince the subject of p0 is (π,H, x), p0 does not satisfy T4, or T5, or T6. Thus Y (n), and hence the lemma, is proved by induction."
    }, {
      "heading" : "EndProofLemC.3",
      "text" : "Definition C.4. If H is a ψ-history then define H(ψ :=β) to be the sequence formed from H by just replacing each ψ by β, and each ψ′ by β′.\nLemma C.5. Suppose P = (R,>) is a plausible theory, H is a ψ-history, and x is either a formula or a finite set of formulas.\n1) If (ψ,H) |−x then (β,H(ψ :=β)) |−x. 2) If (ψ′,H) 6|−x then (β′,H(ψ :=β)) 6|−x. Hence P(ψ)⊆P(β) and P(β′)⊆P(ψ′)."
    }, {
      "heading" : "Proof",
      "text" : "Suppose (R,>) is a plausible theory, Ax is its set of axioms, H is a ψ-history, and x is either a formula or a finite set of formulas. Note that H is a ψ-history iff H is a ψ′-history. Let Y (k) and Z(k) denote the following conditional statements.\nY (k): If H is a ψ-history, x is either a formula or a finite set of formulas, T (ψ,H, x) = +1, and |T [ψ,H, x]| ≤ k then T (β,H(ψ :=β), x) = +1. Z(k): If H is a ψ′-history, x is either a formula or a finite set of formulas, T (ψ′,H, x) = −1, and |T [ψ′,H, x]| ≤ k then T (β′,H(ψ :=β), x) = −1.\nBy Theorem A.8(1,2) and Theorem A.7(2) it suffices to prove Y (k) and Z(k) by induction on k.\nSuppose k = 1.\nLet the antecedent of Y (1) hold. Let p0 be the root of T [ψ,H, x] and q0 be the root of T [β,H(ψ :=β), x]. Then p0 has no children. If p0 satisfies T1 then x = {} and so q0 satisfies T1. So by T1, T [β,H(ψ :=β), {}] has only one node and T (β,H(ψ :=β), {}) = +1. If p0 satisfies T2 then x = f and Ax |= f . So q0 satisfies T2 and hence T (β,H(ψ :=β), f) = +1. Since p0 has no children and the proof value of p0 is +1, p0 does not satisfy T3. Since the subject of p0 is (ψ,H, x), p0 does not satisfy T4 or T5 or T6. Thus Y (1) holds.\nLet the antecedent of Z(1) hold. Let m0 be the root of T [ψ ′,H, x] and n0 be the root\nof T [β′,H(ψ :=β), x]. Then m0 has no children. Since pv(m0) = T (ψ ′,H, x) = −1, m0 does not satisfy T1 or T2. If m0 satisfies T3 then x = f and for each r∈R s d[f ], ψ\n′r∈H. So n0 satisfies T3 and for each r∈R s d[f ], β\n′r∈H(ψ :=β). Hence by T3, n0 has no children and so −1 = pv(n0) = T (β ′,H(ψ :=β), f). Since the subject of m0 is (ψ ′,H, x), m0 does not satisfy T4 or T5 or T6. Thus Z(1) holds.\nTake any positive integer k. Suppose that both Y (k) and Z(k) are true. We shall prove both Y (k+1) and Z(k+1).\nSuppose the antecedent of Y (k+1) holds and |T [ψ,H, x]| = k+1. We must show T (β,H(ψ :=β), x) = +1. Let p0 be the root of T [ψ,H, x] and q0 be the root of T [β,H(ψ :=β), x].\nIf p0 satisfies T1 then let x be F . We see that q0 also satisfies T1. So t(p0) = ((ψ,H,F ),min,+1) and t(q0) = ((β,H(ψ :=β), F ),min, w0), where w0 = T (β,H(ψ :=β), F ) ∈ {+1,−1}. Let {pf : f ∈F} be the set of children of p0 and {qf : f ∈F} be the set of children of q0. Let f be any formula in F . Then the subject of pf is (ψ,H, f), and the subject of qf is (β,H(ψ :=β), f). Also |T [ψ,H, f ]| ≤ k and the proof value of pf is +1 because p0 is a min node with proof value +1. So by Y (k) the proof value of qf is +1. But this is true for each f , so by T1, w0 = +1, as required.\nSince p0 has a child, p0 does not satisfy T2.\nIf p0 satisfies T3 then let x be f . We see that q0 also satisfies T3. So t(p0) = ((ψ,H, f),max,+1) and t(q0) = ((β,H(ψ :=β), f),max, w0), where w0 = T (β,H(ψ :=β), f) ∈ {+1,−1}.\nWe shall adopt the following naming conventions. Each non-root node of T [ψ,H, f ] is denoted by pl(#, y) where l is the level of the node, # is the number in [1..6] such that the node satisfies T#, and y is a rule, or a formula, or a set, which distinguishes siblings. The proof value of pl(#, y) will be denoted by vl(#, y). For non-root nodes in T [β,H(ψ :=β), f ] we shall use ql(#, y), and its proof value will be denoted by wl(#, y).\nLet the set of children of p0 be {p1(4, r) : ψr /∈H and r∈R s d[f ]}, where the tags of\nthese children are: t(p1(4, r)) = ((ψ,H, f, r),min, v1(4, r)). So +1 = max{v1(4, r) : ψr /∈H and r∈R s d[f ]}. Hence there exists r0 in R s d[f ] such that ψr0 /∈H and v1(4, r0) = +1. So t(p1(4, r0)) = ((ψ,H, f, r0),min,+1).\nLet the set of children of q0 be {q1(4, r) : βr /∈H(ψ :=β) and r∈R s d[f ]}, where the\ntags of these children are: t(q1(4, r)) = ((β,H(ψ :=β), f, r),min, w1(4, r)). So w0 = max{w1(4, r) : βr /∈H(ψ :=β) and r∈R s d[f ]}.\nLet the set of children of p1(4, r0) be {p2(1, r0)} ∪ {p2(5, s) : s∈Foe(ψ, f, r0)}, where the tags of these children are: t(p2(1, r0)) = ((ψ,H+ψr0, A(r0)),min, v2(1, r0)); and t(p2(5, s)) = ((ψ,H, f, r0, s),max, v2(5, s)). So +1 = v1(4, r0) = min[{v2(1, r0)} ∪ {v2(5, s) : s∈Foe(ψ, f, r0)}]. Hence v2(1, r0) = +1; and for each s in Foe(ψ, f, r0), v2(5, s) = +1.\nLet the set of children of q1(4, r0) be {q2(1, r0)} ∪ {q2(5, s) : s∈Foe(β, f, r0)}, where the tags of these children are: t(q2(1, r0)) = ((β,H(ψ :=β)+βr0, A(r0)),min, w2(1, r0)); and t(q2(5, s)) = ((β,H(ψ :=β), f, r0, s),max, w2(5, s)). So w1(4, r0) = min[{w2(1, r0)} ∪ {w2(5, s) : s∈Foe(β, f, r0)}].\nSince |T [ψ,H+ψr0, A(r0)]| ≤ k and T (ψ,H+ψr0, A(r0)) = v2(1, r0) = +1, by Y (k) we have T (β,H(ψ :=β)+βr0, A(r0)) = w2(1, r0) = +1. Hence (*) w1(4, r0) = min{w2(5, s) : s∈Foe(β, f, r0)}.\nFor each s in Foe(ψ, f, r0) let the set of children of p2(5, s) be {p3(1, t) : ψt /∈H and t∈R s d[f ; s]} ∪ {p3(6, s) : ψ\n′s /∈H}, where the tags of these children are: t(p3(1, t)) = ((ψ,H+ψt,A(t)),min, v3(1, t)); and t(p3(6, s)) = (−(ψ\n′,H+ψ′s,A(s)),−, v3(6, s)). So +1 = v2(5, s) = max[{v3(1, t) : ψt /∈H and t∈R s d[f ; s]} ∪ {v3(6, s) : ψ ′s /∈H}].\nFor each s in Foe(β, f, r0) let the set of children of q2(5, s) be {q3(1, t) : βt /∈H(ψ :=β) and t∈Rsd[f ; s]} ∪ {q3(6, s) : β\n′s /∈H(ψ :=β)}, where the tags of these children are: t(q3(1, t)) = ((β,H(ψ :=β)+βt,A(t)),min, w3(1, t)); and t(q3(6, s)) = (−(β\n′,H(ψ :=β)+β′s,A(s)),−, w3(6, s)). So w2(5, s) = max[{w3(1, t) : βt /∈H(ψ :=β) and t∈R s d[f ; s]} ∪ {w3(6, s) : β ′s /∈H(ψ :=β)}].\nTake any s in Foe(β, f, r0). We shall show that w2(5, s) = +1. Observe that Foe(ψ, f, r0) = Foe(β, f, r0).\nSuppose there exists t0 such that ψt0 /∈H and t0∈R s d[f ; s] and\n+1 = v3(1, t0) = T (ψ,H+ψt0, A(t0)). Then p3(1, t0) exists. Since ψt0∈H iff βt0∈H(ψ :=β), we have βt0 /∈H(ψ :=β) and so q3(1, t0) exists. Since |T [ψ,H+ψt0, A(t0)]| ≤ k, then by Y (k) we have T (β,H(ψ :=β)+βt0, A(t0)) = w3(1, t0) = +1. Hence w2(5, s) = +1.\nSo suppose that such a t0 does not exist. Then v3(6, s) = +1 and so p3(6, s) exists and ψ′s /∈H. Since ψ′s∈H iff β′s∈H(ψ :=β), we have β′s /∈H(ψ :=β), and so q3(6, s) exists. Let the child of p3(6, s) be p4(1, s) where t(p4(1, s)) = ((ψ\n′,H+ψ′s,A(s)),min, v4(1, s)). Then +1 = v3(6, s) = −v4(1, s). So v4(1, s) = −1 and hence T (ψ\n′,H+ψ′s,A(s)) = −1. Since |T [ψ′,H+ψ′s,A(s)]| ≤ k, then by Z(k), T (β′,H(ψ :=β)+β′s,A(s)) = −1.\nLet the child of q3(6, s) be q4(1, s) where t(q4(1, s)) = ((β\n′,H(ψ :=β)+β′s,A(s)),min, w4(1, s)). Then w3(6, s) = −w4(1, s) = −T (β′,H(ψ :=β)+β′s,A(s)) = +1. Therefore w2(5, s) = +1.\nThus for all s in Foe(β, f, r0), w2(5, s) = +1. So by (*), w1(4, r0) = +1 and hence w0 = +1, as required.\nSince the subject of p0 is (ψ,H, x), p0 does not satisfy T4, or T5, or T6. Thus Y (k+1) is proved.\nTo prove Z(k+1) we suppose the antecedent of Z(k+1) holds and |T [ψ′,H, x]| = k+1. We must show T (β′,H(ψ :=β), x) = −1. Let m0 be the root of T [ψ\n′,H, x] and n0 be the root of T [β′,H(ψ :=β), x]. Then m0 has a child and pv(m0) = T (ψ ′,H, x) = −1.\nIf m0 satisfies T1 then let x be F . We see that n0 also satisfies T1. So t(m0) = ((ψ ′,H, F ),min,−1) and t(n0) = ((β ′,H(ψ :=β), F ),min, pv(n0)), where pv(n0) = T (β ′,H(ψ :=β), F ) ∈ {+1,−1}. Let {mf : f ∈F} be the set of children of m0 and {nf : f ∈F} be the set of children of n0. Let f be any formula in F . Then the subject of mf is (ψ ′,H, f), and the subject of nf is (β ′,H(ψ :=β), f). Also |T [ψ′,H, f ]| ≤ k. There exists f0∈F such at pv (mf0) = −1 because m0 is a min node with proof value −1. So by Z(k), pv(nf0) = T (β\n′,H(ψ :=β), f0) = −1. But n0 is a min node, so pv(n0) = −1, as required.\nSince pv(m0) = T (ψ ′,H, x) = −1, m0 does not satisfy T2.\nIf m0 satisfies T3 then let x be f . We see that n0 also satisfies T3. So t(m0) = ((ψ ′,H, f),max,−1) and t(n0) = ((β ′,H(ψ :=β), f),max, pv (n0)), where pv(n0) = T (β ′,H(ψ :=β), f) ∈ {+1,−1}.\nWe shall adopt the following naming conventions. Each non-root node of T [ψ′,H, f ] is denoted by ml(#, y) where l is the level of the node, # is the number in [1..6] such that the node satisfies T#, and y is a rule, or a formula, or a set, which distinguishes siblings. For non-root nodes in T [β′,H(ψ :=β), f ] we shall use nl(#, y).\nLet the set of children of m0 be {m1(4, r) : ψ ′r /∈H and r∈Rsd[f ]}, where the tags of\nthese children are: t(m1(4, r)) = ((ψ ′,H, f, r),min, pv (m1(4, r))). Recall that m0 has at least one child. So −1 = max{pv (m1(4, r)) : ψ ′r /∈H and r∈Rsd[f ]}. Hence if ψ\n′r /∈H and r∈Rsd[f ] then pv(m1(4, r)) = −1. Therefore t(m1(4, r)) = ((ψ ′,H, f, r),min,−1).\nLet the set of children of n0 be {n1(4, r) : β ′r /∈H(ψ :=β) and r∈Rsd[f ]}, where the\ntags of these children are: t(n1(4, r)) = ((β ′,H(ψ :=β), f, r),min, pv (n1(4, r))). So pv(n0) = max{pv (n1(4, r)) : β ′r /∈H(ψ :=β) and r∈Rsd[f ]}. If n0 does not have a child then pv(n0) = max{} = −1, as required. So suppose that n0 has at least one child.\nIf ψ′r /∈H and r∈Rsd[f ] then let the set of children of m1(4, r) be {m2(1, r)} ∪ {m2(5, s) : s∈R[¬f ; r]}, where the tags of these children are: t(m2(1, r)) = ((ψ\n′,H+ψ′r,A(r)),min, pv(m2(1, r))); and t(m2(5, s)) = ((ψ\n′,H, f, r, s),max, pv (m2(5, s))). So −1 = pv(m1(4, r)) = min[{pv(m2(1, r))} ∪ {pv (m2(5, s)) : s∈R[¬f ; r]}]. Therefore either pv(m2(1, r)) = −1 or there exists s0 in R[¬f ; r] such that pv(m2(5, s0)) = −1.\nIf β′r /∈H(ψ :=β) and r∈Rsd[f ] then let the set of children of n1(4, r) be {n2(1, r)} ∪ {n2(5, s) : s∈Foe(β\n′, f, r)}, where the tags of these children are: t(n2(1, r)) = ((β\n′,H(ψ :=β)+β′r,A(r)),min, pv(n2(1, r))); and t(n2(5, s)) = ((β\n′,H(ψ :=β), f, r, s),max, pv(n2(5, s))). So pv(n1(4, r)) = min[{pv (n2(1, r))} ∪ {pv (n2(5, s)) : s∈Foe(β ′, f, r)}].\nWe show that for each r in Rsd[f ] such that β ′r /∈H(ψ :=β), pv(n1(4, r)) = −1. We have |T [ψ′,H+ψ′r,A(r)]| ≤ k. If −1 = pv(m2(1, r)) = T (ψ ′,H+ψ′r,A(r)) then\nby Z(k), pv(n2(1, r)) = T (β ′,H(ψ :=β)+β′r,A(r)) = −1. Hence pv(n1(4, r)) = −1.\nSo suppose there exists s0 in R[¬f ; r] such that T (ψ ′,H, f, r, s0) = pv(m2(5, s0)) = −1.\nThen s0∈Foe(β ′, f, r). We shall show that T (β′,H(ψ :=β), f, r, s0) = pv(n2(5, s0)) = −1, and hence that pv(n1(4, r)) = −1.\nLet the set of children of m2(5, s0) be {m3(1, t) : ψ\n′t /∈H and t∈Rsd[f ; s0]} ∪ {m3(6, s0) : ψs0 /∈H}, where the tags of these children are: t(m3(1, t)) = ((ψ\n′,H+ψ′t, A(t)),min, pv (m3(1, t))), and t(m3(6, s0)) = (−(ψ,H+ψs0, A(s0)),−, pv (m3(6, s0))). So −1 = pv(m2(5, s0)) = max[{pv (m3(1, t)) : ψ\n′t /∈H and t∈Rsd[f ; s0]} ∪ {pv (m3(6, s0)) : ψs0 /∈H}]. Hence for all t in R s d[f ; s0] such that ψ\n′t /∈H we have −1 = pv(m3(1, t)) = T (ψ\n′,H+ψ′t, A(t)). Also if ψs0 /∈H then −1 = pv(m3(6, s0)) = T (−(ψ,H+ψs0, A(s0))).\nLet the set of children of n2(5, s0) be {n3(1, t) : β\n′t /∈H(ψ :=β) and t∈Rsd[f ; s0]} ∪ {n3(6, s0) : βs0 /∈H(ψ :=β)}, where the tags of these children are: t(n3(1, t)) = ((β\n′,H(ψ :=β)+β′t, A(t)),min, pv(n3(1, t))), and t(n3(6, s0)) = (−(β,H(ψ :=β)+βs0, A(s0)),−, pv (n3(6, s0))). So pv(n2(5, s0)) = max[{pv (n3(1, t)) : β ′t /∈H(ψ :=β) and t∈Rsd[f ; s0]} ∪\n{pv (n3(6, s0)) : βs0 /∈H(ψ :=β)}]. If n2(5, s0) has no children then pv(n2(5, s0)) = max{} = −1, as required. So suppose that n2(5, s0) has at least one child.\nCase 1: n3(1, t) is a child of n2(5, s0). Then β′t /∈H(ψ :=β) and t∈Rsd[f ; s0]. Hence ψ\n′t /∈H. So from above, −1 = pv(m3(1, t)) = T (ψ\n′,H+ψ′t, A(t)). Also |T [ψ′,H+ψ′t, A(t)]| ≤ k. So by Z(k), −1 = T (β′,H(ψ :=β)+β′t, A(t)) = pv(t(n3(1, t))).\nCase 2: n3(6, s0) is a child of n2(5, s0). Then βs0 /∈H(ψ :=β). Hence ψs0 /∈H. So from above, −1 = pv(m3(6, s0)) = T (−(ψ,H+ψs0, A(s0))). Therefore T (ψ,H+ψs0, A(s0)) = +1. Also |T [ψ,H+ψs0, A(s0)]| ≤ k. So by Y (k), T (β,H(ψ :=β)+βs0, A(s0)) = +1. But pv(n3(6, s0)) = −T (β,H(ψ :=β)+βs0, A(s0)) = −+ 1 = −1.\nThese two cases show that pv(n2(5, s0)) = −1, as required. Hence pv(n1(4, r)) = −1. Therefore pv(n0) = −1. Thus Z(k+1) is proved.\nTherefore Y (k) and Z(k) are proved by induction, and so the lemma is proved."
    }, {
      "heading" : "EndProofLemC.5",
      "text" : "Definition C.6. Suppose {α, γ, λ}⊆Alg , H is a α-history, and T is an evaluation tree of some plausible theory. If α /∈{γ, γ′, λ, λ′} then define α(γ :λ) = α; else define γ(γ :λ) = λ, γ′(γ :λ) = λ′, λ(γ :λ) = γ, and λ′(γ :λ) = γ′.\nIf H = (α1r1, ..., αnrn) then define H(γ :λ) = (α1(γ :λ)r1, ..., αn(γ :λ)rn). Define T (γ :λ) to be the tree formed from T by only changing the subject of each node as follows. For each node p of T replace alg(p) by alg(p)(γ : λ), and replace Hist(p) by Hist(p)(γ :λ).\nLemma C.7. Suppose P = (R,>) is a plausible theory such that > is empty. If T is an evaluation tree of P then T (π :ψ) is an evaluation tree of P. Hence P(ψ) = P(π) and P(π′) = P(ψ′)."
    }, {
      "heading" : "Proof",
      "text" : "Let P = (R,>) be a plausible theory such that > is empty. Then Foe(ψ′, f, r) = {} = Foe(π′, f, r) and Foe(π, f, r) = R[¬f ] = Foe(ψ, f, r). Let Y (n) denote the following conditional statement. “If T is an evaluation tree of P and |T | ≤ n then T (π :ψ) is an evaluation tree of P.” By Definition 5.14, it suffices to prove Y (n) by induction on n.\nSuppose n = 1 and the antecedent of Y (1) holds. Let the root of T be p0 and alg(p0) = α. If α /∈{π, ψ, ψ\n′, π′} then T (π :ψ) = T and so Y (1) holds. So suppose α∈{π, ψ, ψ′, π′}. Let the root of T (π :ψ) be q0, and let alg(q0) = λ. So α(π :ψ) = λ. Since p0 has no children, q0 has no children. If p0 satisfies T1 then let Subj (p0) = (α,H, {}). Hence Subj (q0) = (λ,H(π :ψ), {}) and so q0 satisfies T1. Thus T (π :ψ) is an evaluation tree of P. If p0 satisfies T2 then let Subj (p0) = (α,H, f). Hence Subj (q0) = (λ,H(π :ψ), f) and so q0 satisfies T2. Thus T (π :ψ) is an evaluation tree of P. If p0 satisfies T3 then let Subj (p0) = (α,H, f). Hence Subj (q0) = (λ,H(π :ψ), f). Since p0 has no children, S(p0) = {}. So if r∈R s d[f ] then αr∈H. Now αr∈H iff λr∈H(π :ψ). Hence S(q0) = {} and so q0 satisfies T3. Thus T (π :ψ) is an evaluation tree of P. Since p0 and q0 have no children, p0 and q0 satisfy neither T4 nor T6. If p0 satisfies T5 then let Subj (p0) = (α,H, f, r, s). Hence Subj (q0) = (λ,H(π :ψ), f, r, s).\nSince p0 has no children, S(p0) = {}. Hence if t∈R s d[f ; s] then αt∈H. Now αt∈H iff λt∈H(π :ψ). Also α′s∈H. Hence λ′s∈H(π :ψ). So S(q0) = {}. Thus q0 satisfies T5 and so T (π :ψ) is an evaluation tree of P.\nAll cases have been considered and so Y (1) holds.\nIf T is any tree and p is any node of T for which Subj (p) is defined, then define the set S(p, T ) of subjects of the children of p in T by S(p, T ) = {Subj (c) : c is a child of p in T}.\nTake any integer n such that n ≥ 1. Suppose that Y (n) is true. We shall prove Y (n+1). Suppose the antecedent of Y (n+1) holds and that |T | = n+1. Let the root of T be p0 and alg(p0) = α. If α /∈{π, ψ, ψ\n′, π′} then T (π :ψ) = T and so Y (n+1) holds. So suppose α∈{π, ψ, ψ′, π′}. Let the root of T (π :ψ) be q0, and let alg(q0) = λ. So α(π :ψ) = λ.\nIf p0 satisfies T1 then let Subj (p0) = (α,H,F ). Hence Subj (q0) = (λ,H(π :ψ), F ). So q0 satisfies T1. Recall that S(p0, T [α,H,F ]) = {(α,H, f) : f ∈F}. So S(q0, T [α,H,F ](π :ψ)) = {(λ,H(π :ψ), f) : f ∈F} = S(q0, T [λ,H(π :ψ), F ]), by T1. But for each (α,H, f) in S(p0, T [α,H,F ]), T [α,H, f ] is an evaluation tree of P and |T [α,H, f ]| ≤ n. So by Y (n), T [α,H, f ](π :ψ) is an evaluation tree of P. Hence T [α,H, f ](π :ψ) = T [λ,H(π :ψ), f ]. Thus T (π :ψ) = T [α,H,F ](π :ψ) = T [λ,H(π :ψ), F ] which is an evaluation tree of P.\nSince p0 has a child, p0 does not satisfy T2.\nIf p0 satisfies T3 then let Subj (p0) = (α,H, f). Hence Subj (q0) = (λ,H(π :ψ), f). So q0 satisfies T3. Recall that S(p0, T [α,H, f ]) = {(α,H, f, r) : αr /∈H and r∈R s d[f ]}. Since αr∈H iff λr∈H(π :ψ) we have αr /∈H iff λr /∈H(π :ψ). So S(q0, T [α,H, f ](π :ψ)) = {(λ,H(π :ψ), f, r) : αr /∈H and r∈Rsd[f ]} = {(λ,H(π :ψ), f, r) : λr /∈H(π :ψ) and r∈Rsd[f ]} = S(q0, T [λ,H(π :ψ), f ]), by T3. But for each (α,H, f, r) in S(p0, T [α,H, f ]), T [α,H, f, r] is an evaluation tree of P and |T [α,H, f, r]| ≤ n. So by Y (n), T [α,H, f, r](π :ψ) is an evaluation tree of P. Hence T [α,H, f, r](π :ψ) = T [λ,H(π :ψ), f, r]. Thus T (π :ψ) = T [α,H, f ](π :ψ) = T [λ,H(π :ψ), f ] which is an evaluation tree of P.\nIf p0 satisfies T4 then let Subj (p0) = (α,H, f, r). Hence Subj (q0) = (λ,H(π :ψ), f, r). Since αr∈H iff λr∈H(π :ψ) we have αr /∈H iff λr /∈H(π :ψ). So q0 satisfies T4. Recall that S(p0, T [α,H, f, r]) = {(α,H+αr,A(r))} ∪ {(α,H, f, r, s) : s∈Foe(α, f, r)}. Since Foe(α, f, r) = Foe(λ, f, r), S(q0, T [α,H, f, r](π :ψ)) = {(λ,H(π :ψ)+λr,A(r))} ∪ {(λ,H(π :ψ), f, r, s) : s∈Foe(α, f, r)} = S(q0, T [λ,H(π :ψ), f, r]), by T4. But T [α,H+αr,A(r)] is an evaluation tree of P and |T [α,H+αr,A(r)]| ≤ n. So by Y (n), T [α,H+αr,A(r)](π :ψ) is an evaluation tree of P. Hence T [α,H+αr,A(r)](π :ψ) = T [λ,H(π :ψ)+λr,A(r)]. Also for each (α,H, f, r, s) in S(p0, T [α,H, f, r]), T [α,H, f, r, s] is an evaluation tree of P and |T [α,H, f, r, s]| ≤ n. So by Y (n), T [α,H, f, r, s](π :ψ) is an evaluation tree of P. Hence T [α,H, f, r, s](π :ψ) = T [λ,H(π :ψ), f, r, s]. Thus T (π :ψ) = T [α,H, f, r](π :ψ) = T [λ,H(π :ψ), f, r] which is an evaluation tree of P.\nIf p0 satisfies T5 then let Subj (p0) = (α,H, f, r, s). Then α 6= π ′. Since\ns∈Foe(α, f, r), Foe(α, f, r) 6= {} and so α 6= ψ′. Therefore α∈{π, ψ} and so λ∈{π, ψ}. Now Subj (q0) = (λ,H(π :ψ), f, r, s). Since αr∈H iff λr∈H(π :ψ) we have αr /∈H iff λr /∈H(π :ψ). So q0 satisfies T5. Recall that S(p0, T [α,H, f, r, s]) = {(α,H+αt,A(t)) : αt /∈H and t∈Rsd[f ; s]}∪ {−(α\n′,H+α′s,A(s)) : α′s /∈H}. Also since α′s∈H iff λ′s∈H(π :ψ) we have α′s /∈H iff λ′s /∈H(π :ψ). So S(q0, T [α,H, f, r, s](π :ψ))\n= {(λ,H(π :ψ)+λt,A(t)) : αt /∈H and t∈Rsd[f ; s]}∪{−(λ ′,H(π :ψ)+λ′s,A(s)) : α′s /∈H} = {(λ,H(π :ψ)+λt,A(t)) : λt /∈H(π :ψ) and t∈Rsd[f ; s]}∪ {−(λ′,H(π :ψ)+λ′s,A(s)) : λ′s /∈H(π :ψ)} = S(q0, T [λ,H(π :ψ), f, r, s]), by T5.\nBut for each (α,H+αt,A(t)) in S(p0, T [α,H, f, r, s]), T [α,H+αt,A(t)] is an evaluation tree of P and |T [α,H+αt,A(t)]| ≤ n. So by Y (n), T [α,H+αt,A(t)](π :ψ) is an evaluation tree of P. Hence T [α,H+αt,A(t)](π :ψ) = T [λ,H(π :ψ)+λt,A(t)]. Also if −(α′,H+α′s,A(s)) ∈ S(p0, T [α,H, f, r, s]), then T [−(α\n′,H+α′s,A(s))] is an evaluation tree of P and |T [−(α′,H+α′s,A(s))]| ≤ n. So by Y (n), T [−(α′,H+α′s,A(s))](π :ψ) is an evaluation tree of P. Hence T [−(α′,H+α′s,A(s))](π :ψ) = T [−(λ′,H(π :ψ)+λ′s,A(s))].\nThus T (π :ψ) = T [α,H, f, r, s](π :ψ) = T [λ,H(π :ψ), f, r, s] which is an evaluation tree of P.\nIf p0 satisfies T6 then let Subj (p0) = −(α ′,H, F ). Then α∈{π, ψ} and so λ∈{ψ, π}.\nHence Subj (q0) = −(λ ′,H(π :ψ), F ). So q0 satisfies T6. Recall that S(p0, T [−(α ′,H, F )]) = {(α′,H, F )}. So S(q0, T [−(α\n′,H, F )](π :ψ)) = {(λ′,H(π :ψ), F )} = S(q0, T [−(λ\n′,H(π :ψ), F )]), by T6. But T [α′,H, F ] is an evaluation tree of P and |T [α′,H, F ]| ≤ n. So by Y (n), T [α′,H, F ](π :ψ) is an evaluation tree of P. Hence T [α′,H, F ](π :ψ) = T [λ′,H(π :ψ), F ]. Thus T (π :ψ) = T [−(α′,H, F )](π :ψ) = T [−(λ′,H(π :ψ), F )] which is an evaluation tree of P.\nTherefore Y (n+1), and hence the lemma, is proved by induction."
    }, {
      "heading" : "EndProofLemC.7",
      "text" : "Theorem C.8 (Theorem 7.6 The proof algorithm hierarchy). Suppose P = (R,>) is a plausible theory. 1) P(ϕ) ⊆ P(π) ⊆ P(ψ) ⊆ P(β) = P(β′) ⊆ P(ψ′) ⊆ P(π′). 2) If > is empty then P(ϕ) ⊆ P(π) = P(ψ) ⊆ P(β) = P(β′) ⊆ P(ψ′) = P(π′)."
    }, {
      "heading" : "Proof",
      "text" : "Suppose P = (R,>) is a plausible theory. By Lemma C.1, P(ϕ) ⊆ P(π). By Lemma C.3, P(π) ⊆ P(ψ). By Lemma C.5(1), P(ψ) ⊆ P(β). By Lemma B.8, P(β) = P(β′). By Lemma C.5(2), P(β′) ⊆ P(ψ′). By Lemma B.4, P(ψ′) ⊆ P(π′). So part (1) holds.\nPart (2) holds by part (1) and Lemma C.7."
    }, {
      "heading" : "EndProofThmC.8",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "The Logic of Conditionals: An Application of Probability to Deductive Logic. D",
      "author" : [ "E.W. Adams" ],
      "venue" : null,
      "citeRegEx" : "Adams,? \\Q1975\\E",
      "shortCiteRegEx" : "Adams",
      "year" : 1975
    }, {
      "title" : "On the logic of theory change: Partial meet contraction and revision functions",
      "author" : [ "C.E. Alchourròn", "P. Gärdenfors", "D. Makinson" ],
      "venue" : "Journal of Symbolic Logic,",
      "citeRegEx" : "Alchourròn et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Alchourròn et al\\.",
      "year" : 1985
    }, {
      "title" : "Nonmonotonic Reasoning",
      "author" : [ "G. Antoniou" ],
      "venue" : null,
      "citeRegEx" : "Antoniou,? \\Q1997\\E",
      "shortCiteRegEx" : "Antoniou",
      "year" : 1997
    }, {
      "title" : "The logic of conditionals",
      "author" : [ "H. Arlo-Costa", "P. Egré" ],
      "venue" : "The Stanford Encyclopedia of Philosophy (Winter 2016 edition)",
      "citeRegEx" : "Arlo.Costa and Egré,? \\Q2016\\E",
      "shortCiteRegEx" : "Arlo.Costa and Egré",
      "year" : 2016
    }, {
      "title" : "Knowledge Representation, Reasoning and Declarative Problem Solving",
      "author" : [ "C. Baral" ],
      "venue" : null,
      "citeRegEx" : "Baral,? \\Q2003\\E",
      "shortCiteRegEx" : "Baral",
      "year" : 2003
    }, {
      "title" : "Representing default rules in possibilistic logic",
      "author" : [ "S. Benferhat", "D. Dubois", "H. Prade" ],
      "venue" : "Proceedings of the Third International Conference on the Principles of Knowledge Representation and Reasoning",
      "citeRegEx" : "Benferhat et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Benferhat et al\\.",
      "year" : 1992
    }, {
      "title" : "A defeasible logic for clauses",
      "author" : [ "D. Billington" ],
      "venue" : "AI 2011: Advances in Artificial Intelligence 24th Australasian Joint Conference Perth, Australia, December 5-8,",
      "citeRegEx" : "Billington,? \\Q2011\\E",
      "shortCiteRegEx" : "Billington",
      "year" : 2011
    }, {
      "title" : "An inclusion theorem for defeasible logics",
      "author" : [ "D. Billington", "G. Antoniou", "G. Governatori", "M.J. Maher" ],
      "venue" : "ACM Transactions on Computational Logic,",
      "citeRegEx" : "Billington et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Billington et al\\.",
      "year" : 2010
    }, {
      "title" : "Propositional plausible logic: Introduction and implementation",
      "author" : [ "D. Billington", "A. Rock" ],
      "venue" : "Studia Logica,",
      "citeRegEx" : "Billington and Rock,? \\Q2001\\E",
      "shortCiteRegEx" : "Billington and Rock",
      "year" : 2001
    }, {
      "title" : "Defeasible logic is stable",
      "author" : [ "D. Billington" ],
      "venue" : "Journal of Logic and Computation,",
      "citeRegEx" : "Billington,? \\Q1993\\E",
      "shortCiteRegEx" : "Billington",
      "year" : 1993
    }, {
      "title" : "Propositional clausal defeasible logic",
      "author" : [ "D. Billington" ],
      "venue" : "Logics in Artificial Intelligence,",
      "citeRegEx" : "Billington,? \\Q2008\\E",
      "shortCiteRegEx" : "Billington",
      "year" : 2008
    }, {
      "title" : "A propositional plausible logic",
      "author" : [ "D. Billington" ],
      "venue" : "AI 2015: Advances in Artificial Intelligence,",
      "citeRegEx" : "Billington,? \\Q2015\\E",
      "shortCiteRegEx" : "Billington",
      "year" : 2015
    }, {
      "title" : "On the entailment problem for a logic of typicality",
      "author" : [ "R. Booth", "G. Casini", "T. Meyer", "I. Varzinczak" ],
      "venue" : "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI",
      "citeRegEx" : "Booth et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Booth et al\\.",
      "year" : 2015
    }, {
      "title" : "A propositional typicality logic for extending rational consequence",
      "author" : [ "R. Booth", "T. Meyer", "I. Varzinczak" ],
      "venue" : "Logic and Cognitive Systems,",
      "citeRegEx" : "Booth et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Booth et al\\.",
      "year" : 2013
    }, {
      "title" : "Conditional logics of normality: A modal approach",
      "author" : [ "C. Boutilier" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Boutilier,? \\Q1994\\E",
      "shortCiteRegEx" : "Boutilier",
      "year" : 1994
    }, {
      "title" : "Unifying default reasoning and belief revision in a modal framework",
      "author" : [ "C. Boutilier" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Boutilier,? \\Q1994\\E",
      "shortCiteRegEx" : "Boutilier",
      "year" : 1994
    }, {
      "title" : "Preferred subtheories: An extended logical framework for default reasoning",
      "author" : [ "G. Brewka" ],
      "venue" : "Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI89),",
      "citeRegEx" : "Brewka,? \\Q1989\\E",
      "shortCiteRegEx" : "Brewka",
      "year" : 1989
    }, {
      "title" : "Quick completeness proofs for some logics of conditionals",
      "author" : [ "J.P. Burgess" ],
      "venue" : "Notre Dame Journal of Formal Logic,",
      "citeRegEx" : "Burgess,? \\Q1981\\E",
      "shortCiteRegEx" : "Burgess",
      "year" : 1981
    }, {
      "title" : "On the evaluation of argumentation formalisms",
      "author" : [ "M. Caminada", "L. Amgoud" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Caminada and Amgoud,? \\Q2007\\E",
      "shortCiteRegEx" : "Caminada and Amgoud",
      "year" : 2007
    }, {
      "title" : "On the connection between non-monotonic inference systems and conditional logics",
      "author" : [ "G. Crocco", "P. Lamarre" ],
      "venue" : "Proceedings of the Third International Conference on the Principles of Knowledge Representation and Reasoning",
      "citeRegEx" : "Crocco and Lamarre,? \\Q1992\\E",
      "shortCiteRegEx" : "Crocco and Lamarre",
      "year" : 1992
    }, {
      "title" : "On a rule-based interpretation of default conditionals",
      "author" : [ "J.P. Delgrande" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence,",
      "citeRegEx" : "Delgrande,? \\Q2007\\E",
      "shortCiteRegEx" : "Delgrande",
      "year" : 2007
    }, {
      "title" : "Conditional objects and non-monotonic reasoning",
      "author" : [ "D. Dubois", "H. Prade" ],
      "venue" : "Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning",
      "citeRegEx" : "Dubois and Prade,? \\Q1991\\E",
      "shortCiteRegEx" : "Dubois and Prade",
      "year" : 1991
    }, {
      "title" : "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming, and n-person games",
      "author" : [ "P.M. Dung" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Dung,? \\Q1995\\E",
      "shortCiteRegEx" : "Dung",
      "year" : 1995
    }, {
      "title" : "Defeasible logic programming: an argumentative approach",
      "author" : [ "A.J. Garcia", "G.R. Simari" ],
      "venue" : "Theory and Practice of Logic Programming,",
      "citeRegEx" : "Garcia and Simari,? \\Q2004\\E",
      "shortCiteRegEx" : "Garcia and Simari",
      "year" : 2004
    }, {
      "title" : "Knowledge in Flux. Modeling the Dynamics of Epistemic States",
      "author" : [ "P. Gärdenfors" ],
      "venue" : null,
      "citeRegEx" : "Gärdenfors,? \\Q1988\\E",
      "shortCiteRegEx" : "Gärdenfors",
      "year" : 1988
    }, {
      "title" : "Ordered logic: defeasible reasoning for multiple agents",
      "author" : [ "P. Geerts", "D. Vermeir", "D. Nute" ],
      "venue" : "Decision Support Systems,",
      "citeRegEx" : "Geerts et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Geerts et al\\.",
      "year" : 1994
    }, {
      "title" : "Conditional entailment: bridging two approaches to default reasoning",
      "author" : [ "H. Geffner", "J. Pearl" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Geffner and Pearl,? \\Q1992\\E",
      "shortCiteRegEx" : "Geffner and Pearl",
      "year" : 1992
    }, {
      "title" : "System-z+: A formalism for reasoning with variablestrength defaults",
      "author" : [ "M. Goldszmidt", "J. Pearl" ],
      "venue" : "In Proceedings",
      "citeRegEx" : "Goldszmidt and Pearl,? \\Q1991\\E",
      "shortCiteRegEx" : "Goldszmidt and Pearl",
      "year" : 1991
    }, {
      "title" : "The quantitative/qualitative watershed for rules of uncertain inference",
      "author" : [ "J. Hawthorne", "D. Makinson" ],
      "venue" : "Studia Logica,",
      "citeRegEx" : "Hawthorne and Makinson,? \\Q2007\\E",
      "shortCiteRegEx" : "Hawthorne and Makinson",
      "year" : 2007
    }, {
      "title" : "A skeptical theory of inheritance in nonmonotonic semantic networks",
      "author" : [ "J.F. Horty", "R.H. Thomason", "D.S. Touretzky" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Horty et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Horty et al\\.",
      "year" : 1990
    }, {
      "title" : "Nonmonotonic reasoning, preferential models and cumulative logics",
      "author" : [ "S. Kraus", "D. Lehmann", "M. Magidor" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Kraus et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Kraus et al\\.",
      "year" : 1990
    }, {
      "title" : "S4 as the conditional logic of nonmonotonicity",
      "author" : [ "P. Lamarre" ],
      "venue" : "Proceedings of the Second International Conference on the",
      "citeRegEx" : "Lamarre,? \\Q1991\\E",
      "shortCiteRegEx" : "Lamarre",
      "year" : 1991
    }, {
      "title" : "What does a conditional knowledge base entail",
      "author" : [ "D. Lehmann", "M. Magidor" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Lehmann and Magidor,? \\Q1992\\E",
      "shortCiteRegEx" : "Lehmann and Magidor",
      "year" : 1992
    }, {
      "title" : "Ambiguity propagating defeasible logic and the wellfounded semantics",
      "author" : [ "F. Maier", "D. Nute" ],
      "venue" : "European Conference on Logics in Artificial Intelligence (JELIA2006),",
      "citeRegEx" : "Maier and Nute,? \\Q2006\\E",
      "shortCiteRegEx" : "Maier and Nute",
      "year" : 2006
    }, {
      "title" : "General theory of cumulative inference",
      "author" : [ "D. Makinson" ],
      "venue" : "In Proceedings of the Second International Workshop on Non-Monotonic Reasoning,",
      "citeRegEx" : "Makinson,? \\Q1988\\E",
      "shortCiteRegEx" : "Makinson",
      "year" : 1988
    }, {
      "title" : "Lossy inference rules and their bounds: a brief review",
      "author" : [ "D. Makinson", "J. Hawthorne" ],
      "venue" : "The Road to Universal Logic,",
      "citeRegEx" : "Makinson and Hawthorne,? \\Q2014\\E",
      "shortCiteRegEx" : "Makinson and Hawthorne",
      "year" : 2014
    }, {
      "title" : "A general account of argumentation with preferences",
      "author" : [ "S. Modgil", "H. Prakken" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Modgil and Prakken,? \\Q2013\\E",
      "shortCiteRegEx" : "Modgil and Prakken",
      "year" : 2013
    }, {
      "title" : "Logic for Applications (2nd edition). No. ISBN 0-388794893-7 in Graduate Texts in Computer",
      "author" : [ "A. Nerode", "R.A. Shore" ],
      "venue" : null,
      "citeRegEx" : "Nerode and Shore,? \\Q1997\\E",
      "shortCiteRegEx" : "Nerode and Shore",
      "year" : 1997
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "System z: A natural ordering of defaults with tractable applications to nonmonotonic reasoning",
      "author" : [ "J. Pearl" ],
      "venue" : "Theoretical Aspects of Reasoning about Knowledge (TARK-III),",
      "citeRegEx" : "Pearl,? \\Q1990\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1990
    }, {
      "title" : "A logical framework for default reasoning",
      "author" : [ "D. Poole" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Poole,? \\Q1988\\E",
      "shortCiteRegEx" : "Poole",
      "year" : 1988
    }, {
      "title" : "Argument-based extended logic programming with defeasible priorities",
      "author" : [ "H. Prakken", "G. Sartor" ],
      "venue" : "Journal of Applied Non-Classical Logics,",
      "citeRegEx" : "Prakken and Sartor,? \\Q1997\\E",
      "shortCiteRegEx" : "Prakken and Sartor",
      "year" : 1997
    }, {
      "title" : "A logic for default reasoning",
      "author" : [ "R. Reiter" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Reiter,? \\Q1980\\E",
      "shortCiteRegEx" : "Reiter",
      "year" : 1980
    }, {
      "title" : "A mathematical treatment of defeasible reasoning and its implementation",
      "author" : [ "G.R. Simari", "R.P. Loui" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Simari and Loui,? \\Q1992\\E",
      "shortCiteRegEx" : "Simari and Loui",
      "year" : 1992
    }, {
      "title" : "A clash of intuitions: The current state of nonmonotonic multiple inheritance systems",
      "author" : [ "D. Touretzky", "J. Horty", "R. Thomason" ],
      "venue" : "Proceedings of the 10th International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Touretzky et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Touretzky et al\\.",
      "year" : 1987
    }, {
      "title" : "Applying recent argumentation methods to some ancient examples of plausible",
      "author" : [ "D. Walton", "C. Tindale", "T. Gordon" ],
      "venue" : "reasoning. Argumentation,",
      "citeRegEx" : "Walton et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Walton et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 45,
      "context" : "However on page 114 of (Walton et al., 2014) there is a list of 11 characteristics of plausible reasoning, rather than characteristics of formal logics that do plausible reasoning.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "A pruned version of PPL is presented in (Billington, 2015).",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 42,
      "context" : "The elements of Plaus(S) can have a variety of forms; for example: defaults are used in Reiter’s Default Logic (Reiter, 1980), defeasible rules are used in ASPIC (Caminada & Amgoud, 2007) and ASPIC (Modgil & Prakken, 2013), and defeasible and warning rules are used in Defeasible Logic (Billington, 2008).",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "The elements of Plaus(S) can have a variety of forms; for example: defaults are used in Reiter’s Default Logic (Reiter, 1980), defeasible rules are used in ASPIC (Caminada & Amgoud, 2007) and ASPIC (Modgil & Prakken, 2013), and defeasible and warning rules are used in Defeasible Logic (Billington, 2008).",
      "startOffset" : 286,
      "endOffset" : 304
    }, {
      "referenceID" : 24,
      "context" : "The AGM postulates for belief change (Alchourròn, Gärdenfors, & Makinson, 1985; Gärdenfors, 1988), various properties of nonmonotonic consequence relations (Makinson, 1988; Kraus, Lehmann, & Magidor, 1990), and the postulates that a rule-based argumentation system should satisfy (Caminada & Amgoud, 2007).",
      "startOffset" : 37,
      "endOffset" : 97
    }, {
      "referenceID" : 34,
      "context" : "The AGM postulates for belief change (Alchourròn, Gärdenfors, & Makinson, 1985; Gärdenfors, 1988), various properties of nonmonotonic consequence relations (Makinson, 1988; Kraus, Lehmann, & Magidor, 1990), and the postulates that a rule-based argumentation system should satisfy (Caminada & Amgoud, 2007).",
      "startOffset" : 156,
      "endOffset" : 205
    }, {
      "referenceID" : 30,
      "context" : "The Non-Conjunction Principle is supported by the fact that the ‘And’ rule of (Kraus et al., 1990), (If a |∼ x and a |∼ y then a |∼ ∧ {x, y}.",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 45,
      "context" : "Of the 11 characteristics of plausible reasoning given on page 114 of (Walton et al., 2014), characteristic 8 is ‘stability’; which seems to mean (bottom of page 97 of (Walton et al.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 45,
      "context" : ", 2014), characteristic 8 is ‘stability’; which seems to mean (bottom of page 97 of (Walton et al., 2014)) that plausible statements are consistent.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 44,
      "context" : "The essence of Figure 3 in (Touretzky et al., 1987) is our third signpost example.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "There are three well-known non-monotonic logics, namely Default Logic, Circumscription, and Autoepistemic Logic; see (Antoniou, 1997) for an introduction.",
      "startOffset" : 117,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "Answer Set Programming (ASP) (Baral, 2003) is a well-known Knowledge Representation system.",
      "startOffset" : 29,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "Logics in this category include inheritance networks (Horty, Thomason, & Touretzky, 1990), the DeLP system of (Garcia & Simari, 2004), the ASPIC system mentioned in (Caminada & Amgoud, 2007), the logic in (Prakken & Sartor, 1997), Ordered logic (Geerts, Vermeir, & Nute, 1994), and most Defeasible Logics (Billington, 2008).",
      "startOffset" : 305,
      "endOffset" : 323
    }, {
      "referenceID" : 10,
      "context" : "The only Defeasible Logics that deal with conjunction and disjunction, besides PPL, are the logic in (Billington & Rock, 2001), let’s call it DL1, and the logic in (Billington, 2008), let’s call it DL8.",
      "startOffset" : 164,
      "endOffset" : 182
    }, {
      "referenceID" : 9,
      "context" : "9) fails for the plausible proof algorithms that define the Defeasible Logics in: (Billington, 1993), (Billington & Rock, 2001), (Maier & Nute, 2006), (Billington, 2008), and (Billington, 2011).",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "9) fails for the plausible proof algorithms that define the Defeasible Logics in: (Billington, 1993), (Billington & Rock, 2001), (Maier & Nute, 2006), (Billington, 2008), and (Billington, 2011).",
      "startOffset" : 151,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "9) fails for the plausible proof algorithms that define the Defeasible Logics in: (Billington, 1993), (Billington & Rock, 2001), (Maier & Nute, 2006), (Billington, 2008), and (Billington, 2011).",
      "startOffset" : 175,
      "endOffset" : 193
    }, {
      "referenceID" : 22,
      "context" : "Argumentation systems, (Dung, 1995), are well-known non-monotonic reasoning systems that can use rules, for example ASPIC (Caminada & Amgoud, 2007) and ASPIC (Modgil & Prakken, 2013).",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 20,
      "context" : "In (Delgrande, 2007) it is observed that the following reasoning systems satisfy both the And-rule and the Or-rule and hence do not do our plausible reasoning: systems based on intuitions from probability theory such as (Adams, 1975) and (Pearl, 1988), and from qualitative possibilistic logic (Dubois, Lang, & Prade, 1994), those based on C4 (Lamarre, 1991), CT4 (Boutilier, 1994a), and S (Burgess, 1981).",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "In (Delgrande, 2007) it is observed that the following reasoning systems satisfy both the And-rule and the Or-rule and hence do not do our plausible reasoning: systems based on intuitions from probability theory such as (Adams, 1975) and (Pearl, 1988), and from qualitative possibilistic logic (Dubois, Lang, & Prade, 1994), those based on C4 (Lamarre, 1991), CT4 (Boutilier, 1994a), and S (Burgess, 1981).",
      "startOffset" : 220,
      "endOffset" : 233
    }, {
      "referenceID" : 38,
      "context" : "In (Delgrande, 2007) it is observed that the following reasoning systems satisfy both the And-rule and the Or-rule and hence do not do our plausible reasoning: systems based on intuitions from probability theory such as (Adams, 1975) and (Pearl, 1988), and from qualitative possibilistic logic (Dubois, Lang, & Prade, 1994), those based on C4 (Lamarre, 1991), CT4 (Boutilier, 1994a), and S (Burgess, 1981).",
      "startOffset" : 238,
      "endOffset" : 251
    }, {
      "referenceID" : 31,
      "context" : "In (Delgrande, 2007) it is observed that the following reasoning systems satisfy both the And-rule and the Or-rule and hence do not do our plausible reasoning: systems based on intuitions from probability theory such as (Adams, 1975) and (Pearl, 1988), and from qualitative possibilistic logic (Dubois, Lang, & Prade, 1994), those based on C4 (Lamarre, 1991), CT4 (Boutilier, 1994a), and S (Burgess, 1981).",
      "startOffset" : 343,
      "endOffset" : 358
    }, {
      "referenceID" : 17,
      "context" : "In (Delgrande, 2007) it is observed that the following reasoning systems satisfy both the And-rule and the Or-rule and hence do not do our plausible reasoning: systems based on intuitions from probability theory such as (Adams, 1975) and (Pearl, 1988), and from qualitative possibilistic logic (Dubois, Lang, & Prade, 1994), those based on C4 (Lamarre, 1991), CT4 (Boutilier, 1994a), and S (Burgess, 1981).",
      "startOffset" : 390,
      "endOffset" : 405
    }, {
      "referenceID" : 34,
      "context" : "The consequence function of (Makinson, 1988) and the cumulative conditional knowledge bases of (Kraus et al.",
      "startOffset" : 28,
      "endOffset" : 44
    }, {
      "referenceID" : 30,
      "context" : "The consequence function of (Makinson, 1988) and the cumulative conditional knowledge bases of (Kraus et al., 1990) satisfy both the And-rule and the Or-rule.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 30,
      "context" : "Preferential conditional knowledge bases (Kraus et al., 1990) are cumulative.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "As noted in (Delgrande, 2007) the following systems are ‘essentially the same as’ rational closure and hence do not do our plausible reasoning: System Z (Pearl, 1990), systems based on conditional logic (Crocco & Lamarre, 1992), on modal logic (Boutilier, 1994b), on possibilistic logic (Benferhat, Dubois, & Prade, 1992), and on conditional objects (Dubois & Prade, 1991).",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 39,
      "context" : "As noted in (Delgrande, 2007) the following systems are ‘essentially the same as’ rational closure and hence do not do our plausible reasoning: System Z (Pearl, 1990), systems based on conditional logic (Crocco & Lamarre, 1992), on modal logic (Boutilier, 1994b), on possibilistic logic (Benferhat, Dubois, & Prade, 1992), and on conditional objects (Dubois & Prade, 1991).",
      "startOffset" : 153,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "The conditional logic C of (Delgrande, 2007) does not satisfy the Plausible Right Weakening Principle (Principle 3.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "Also C and the extensions of C considered in (Delgrande, 2007) have only one proof algorithm and so fail the Many Proof Algorithms Principle (Principle 3.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 39,
      "context" : "Apart from the problems mentioned in Section 5 of (Goldszmidt & Pearl, 1991), System Z (Pearl, 1990) and System Z (Goldszmidt & Pearl, 1991) are ambiguity propagating but not ambiguity blocking.",
      "startOffset" : 87,
      "endOffset" : 100
    }, {
      "referenceID" : 39,
      "context" : "1), they cannot prove anything about the example because the set of rules is not ‘consistent’ as defined in (Pearl, 1990; Goldszmidt & Pearl, 1991).",
      "startOffset" : 108,
      "endOffset" : 147
    }, {
      "referenceID" : 40,
      "context" : "The logic implemented by theorist (Poole, 1988) and the Preferred Subtheories logic in (Brewka, 1989) both generate consistent extensions and so fail the Non-3-Consistency Principle (Principle 3.",
      "startOffset" : 34,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "The logic implemented by theorist (Poole, 1988) and the Preferred Subtheories logic in (Brewka, 1989) both generate consistent extensions and so fail the Non-3-Consistency Principle (Principle 3.",
      "startOffset" : 87,
      "endOffset" : 101
    } ],
    "year" : 2017,
    "abstractText" : "Plausible reasoning concerns situations whose inherent lack of precision is not quantified; that is, there are no degrees or levels of precision, and hence no use of numbers like probabilities. A hopefully comprehensive set of principles that clarifies what it means for a formal logic to do plausible reasoning is presented. A new propositional logic, called Propositional Plausible Logic (PPL), is defined and applied to some important examples. PPL is the only non-numeric non-monotonic logic we know of that satisfies all the principles and correctly reasons with all the examples. Some important results about PPL are proved.",
    "creator" : "dvips(k) 5.996 Copyright 2016 Radical Eye Software"
  }
}