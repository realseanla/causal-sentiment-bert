{
  "name" : "1503.06902.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Note on Information-Directed Sampling and Thompson Sampling",
    "authors" : [ "Li Zhou" ],
    "emails" : [ "lizhou@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n06 90\n2v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\nThis note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers."
    }, {
      "heading" : "1 Introduction",
      "text" : "Amulti-armed bandit problem [1] is one of the sequential decision making problem. At each time the learner selects an action based on its current knowledge and arm-selection policy, and then receives reward of the action selected. Since the rewards of actions that are not selected are unknown, the learner needs to balance between exploit its current knowledge to select a best arm and explore potential best arms. In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4]. Each of these three algorithms maintains a posterior distribution indicating the probability of each arm/policy being optimal. However they have different rules to update this posterior distribution based on observed rewards."
    }, {
      "heading" : "2 Information-Directed Sampling",
      "text" : ""
    }, {
      "heading" : "2.1 Problem Formulation",
      "text" : "Information-Directed Sampling (IDS) [2] consider a Bayesian formulation of Multi-armed bandit problem. In this setting there is a set of actions (arms) A, and at time t ∈ [1, T ] the decision-maker chooses an action at. Action at then draws a reward ra,t from a reward distribution pa\n1. We assume that all rewards are i.i.d distributed and the reward distribution is stationary with respect to time t ∈ [1, T ].\nTo formulate Multi-armed bandit in a Bayesian way, We denote a∗ = argmaxa∈A Era∼pa [ra], which means a∗ is the arm with highest expected reward with respect to distribution pa, where a ∈ A. We also denote ra∗ the reward drawn from pa∗ . The decision-maker do not know the real\n1In the original paper they assume that the arms will first draw an outcome from an outcome distribution, then here is a fixed and known function that maps outcomes to rewards. However here for the sake of simplicity, we assume the outcome is equal to the reward.\nreward distribution pa, so it has its own estimate about these distributions at time step t, which we denote as p̂a,t. Because of this uncertainly, for each action a at time t, the decision-maker has a believe on whether this action has the highest expected reward. We denote this believe by αt(a) = P (a\n∗ = a|Ft−1), where Ft−1 is the history of past observations including the actions selected and the corresponding rewards. The decision-maker will update this posterior distribution at each time step based on Ft−1.\nInstead of sampling actions directly based on posterior distribution αt, IDS sample actions based on a distribution π. π is also a distribution over all actions and is constructed based on the posterior distribution αt. We are interested in the following expected regret\nE[Regret(T )] = E ra∗∼pa∗\nT ∑\nt=1 ra∗ − Ea∼π ra,t∼pa\nT ∑\nt=1\nra,t (1)"
    }, {
      "heading" : "2.2 Algorithm",
      "text" : "In multi-armed bandit problem, we want to balance between exploitation and exploration. IDS handle this trade-off by defining immediate regret △t(a) and information gain gt(a) of action a at time t."
    }, {
      "heading" : "2.2.1 Immediate Regret",
      "text" : "The immediate regret △t(a) is defined as\n△t(a) = E a∗∼αt\nra∗,t∼p̂a∗,t\n[ra∗,t|Ft−1]− E ra,t∼p̂a,t [ra,t|Ft−1] (2)\nThe idea behind this is that: the regret is defined by formula (1), however the decision-maker does not know the true pa∗ and pa for a ∈ A, so it uses p̂a∗ and p̂a instead to estimate the regret at time step t. Note that\nP (ra∗,t = r) = P (ra,t = r|a∗ = a) (3)\nSo\nE[ra∗,t|Ft−1] = E[ra,t|rb,t ≤ ra,t ∀b,Ft−1] (4)\nWe will show how to calculate each of these terms in section 2.3."
    }, {
      "heading" : "2.2.2 Information Gain",
      "text" : "Instead of doing pure exploitation using immediate regret, one would want to do some exploration to seek potential best arms. To do this, IDS defined a term: information gain, denoted as gt(a). The idea is that: we already have a posterior distribution over a∗, we hope that after we pull one of the arms, the entropy of this distribution decreases, so that we gain a certain amount of information about which arm has the highest expected reward. Let a∗t ∼ αt and a∗t+1 ∼ αt+1, and let H(a∗t ) denote the entropy of a∗t , then gt(a) is defined as\ngt(a) = E[H(a ∗ t )−H(a∗t+1)|Ft−1, at = a] (5)\nThe expectation is with respect to the random reward of arm a. To calculate this, one can sample reward from p̂a and then calculate the expectation above. However in the original paper they used the following way.\nFrom the property of mutual information we have:\nH(X) −H(X|Y ) = I(X,Y ) (6)\nand since E[H(a∗t+1)|Ft−1, at = a] = H(a∗t |ra,t), So\ngt(a) = I(a ∗ t , ra,t) (7)\nAlso from the property of mutual information we have:\nI(X,Y ) = EDKL(P (Y |X)||P (Y )) (8)\nSince we do not have the true distribution of ra,t, we use the posterior distribution p̂a,t, and we have:\ngt(a) = E a′∼αt\nDKL(p̂a,t(·|a′)||p̂a,t) (9)\nIn the equation above, p̂a,t is just the reward posterior distribution of arm a at time t, and p̂a,t(·|a′) is the reward posterior distribution conditioned on that a′ is the arm that has the highest mean reward. With this condition, the reward posterior distribution has to shift to satisfy this constrain. For example in Figure 1, we show 3 arms with mean reward as Gaussian distribution, suppose we want to calculate the reward posterior distribution of arm 2 and 3 conditioned on that arm 1 has the highest mean reward. We examine one point where the mean reward of arm 1 is 0.8. Then the mean reward of arm 2 and arm 3 cannot be greater than 0.8, so the probability mass of these two arms that is greater than 0.8 has to be cut off, and the remaining has to be normalized."
    }, {
      "heading" : "2.2.3 Optimization",
      "text" : "The goal of IDS at a single time step is to balance immediate regret △t(a) and information gain gt(a). There are many ways to do this, and in the paper the author choose the following way:\nπIDSt = argminπ∈D(A)\n{\nΨt(π) := △t(π)2 gt(π)\n}\n(10)\nNote that π is a distribution over all arms, and assuming g has at least 1 non-zero elements, then to find Ψt(π) it is equal to solve the following optimization problem:\nminimize Ψ(π) := (πT△)2 πT g\n(11)\nsubject to πT e = 1 (12)\nπ ≥ 0 (13)\nThe author stated that π can be very sparse, with only two non-zero elements, and then they try all possible combinations of two arms that gives the lowest Ψt(π). Given π, IDS sample an arm and pull that arm. I omit the detail here since it’s well described in the IDS paper."
    }, {
      "heading" : "2.3 Bernoulli Bandit Experiment",
      "text" : "In a K-armed Bernoulli bandit problem, there are K arms, and the reward of the i-th arm follows a Bernoulli distribution with mean Xi. In a Bayesian style learning algorithm, it is standard to model the mean reward of each arm using the Beta distribution:\nXi ∼ Beta(β1i , β2i ) (14) ri ∼ Bernoulli(Xi) (15)\nTo calculate △t(a) and gt(a), we first calculate αt(a). Let fi = Beta.pdf(x|β1i , β2i ) and Fi = Beta.cdf(x|β1i , β2i ) for all arm i, that is, fi and Fi are the PDF and CDF of the posterior distribution\nof Xi, then to calculate αt:\nαt(a) = P\n\n\n⋂\nj 6=i\n{Xj ≤ Xi}\n\n (16)\n=\n∫ 1\n0 fi(x)P\n\n\n⋂\nj 6=i\n{Xj ≤ Xi}|Xi = x\n\n dx (17)\n=\n∫ 1\n0 fi(x)\n\n\n∏\nj 6=i\nFj(x)\n\n dx (18)\n=\n∫ 1\n0\n[\nfi(x) Fi(x)\n]\nF̄ (x)dx (19)\nwhere F̄ (x) = ∏K\ni=1 Fi(x). To calculate this integral, we need to sample points from fi, Fi and F̄i, and then do summation, so it is quite time consuming.\nNext we need to calculate p̂a,t(·|a∗ = a), which is the same as calculating Mij := E[Xj |Xk ≤ Xi ∀k]\nMij = E[Xj |Xk ≤ Xi ∀k] (20)\n=\n∫ 1\n0 xP (Xj = x|Xk ≤ Xi ∀k) (21)\n=\n∫ 1 0 x P (Xj = x,Xk ≤ Xi ∀k) P (Xk ≤ Xi ∀k) dx (22)\nSuppose i 6= j, then\n(22) = 1\nαt(i)\n∫ 1\n0 xP (Xk ≤ Xi ∀k 6= j,Xj = x,Xi ≥ x)dx (23)\n= 1\nαt(i)\n∫ 1\n0 xP (Xj = x)P (Xk ≤ Xi ∀k 6= i or j,Xi ≥ x)dx (24)\n= 1\nαt(i)\n∫ 1\n0 xP (Xj = x)\n∫ 1\nx P (Xk ≤ y ∀k 6= i or j)P (Xi = y)dydx (25)\n= 1\nαt(i)\n∫ 1\n0 xP (Xj = x)\n∫ 1\nx\n(\nfi(y)F̄ (y)\nFi(y)Fj(y)\n)\ndydx (26)\n= 1\nαt(i)\n∫ 1\n0\n(\nfi(y)F̄ (y)\nFi(y)Fj(y)\n)∫ y\n0 xfj(x)dxdy (27)\n= 1\nαt(i)\n∫ 1\n0\n(\nfi(y)F̄ (y)\nFi(y)Fj(y)\n)\nQj(y)dy (28)\nWhere Qj(y) = ∫ y 0 xfj(x)dx. To calculate Qj(y) we also need to do sampling and then summation.\nSuppose i = j, then\n(22) = 1\nαt(t)\n∫ 1\n0 xP (Xi = x,Xk ≤ x ∀k 6= i) (29)\n= 1\nαt(i)\n∫ 1\n0 xfi(x)\n∏\nj 6=i\nFj(x)dx (30)\n= 1\nαt(i)\n∫ 1\n0\nxfi(x)\nFi(x) F̄ (x)dx (31)\nNow that we have αt(a) and Mij = E[Xj |Xk ≤ Xi ∀k], we can calculate △t(a) and gt(a).\nρ∗ = K ∑\ni=1\nαt(i)Mii (32)\n△t(i) = ρ∗ − β1i\nβ1i + β 2 i\n(33)\ngi =\nK ∑\ni=1\nαjKL\n(\nMji|| β1i\nβ1i + β 2 i\n)\n(34)\nWhere KL(p1||p2) is defined as KL(p1||p2) = p1 log(p1p2 ) + (1 − p1) log( 1−p1 1−p2\n) since p̂a,t follows Bernoulli distribution.\nAt each time step, we can calculate △t(a) and gt(a) by the above procedure and then solve the optimization problem to get π, and sample an arm based on π."
    }, {
      "heading" : "2.4 Regret Bound",
      "text" : "Here we prove a general regret bound, for specific regret bound, we can refer to the IDS paper. For a fixed deterministic λ ∈ R and a policy π such at Ψt(πt) ≤ λ, we have\nE[Regret(T, π)] ≤ √\nλH(α1)T (35)\nProve:\nE\nT ∑\nt=1\ngt(πt) = E T ∑\nt=1\nE[H(αt)−H(αt+1)|Ft−1] (36)\n= E\nT ∑\nt=1\n(H(αt)−H(αt+1)) (37)\n= H(α1)− EH(αT+1) (38) ≤ H(α1) (39)\nBy definition, Ψt(π) ≤ λ, so △t(π) ≤ √ λgt(π), so\nE(Regret(T, π)) = E\nT ∑\nt=1\n△t(π) (40)\n≤ √ λE T ∑\nt=1\n√\ngt(π) (41)\n≤ √ λT\n√ √ √ √\nE\nT ∑\nt=1\ngt(π) Caushy-Schwardsz inequality (42)\n≤ √\nλH(α1)T (43)\nIn the paper, the author proved that Ψ∗t ≤ |A/2|, so E(Regret(T, πIDS)) ≤ √ 1 2 |A|H(α1)T"
    }, {
      "heading" : "2.5 Potential Problems",
      "text" : "IDS showed a strong empirical results, however there are several potential problems. I think the main problem is that the algorithm is very time consuming as I run it, the reason is that it has 3 integral to calculate so we have to evaluate each integrand at a discrete grid of points. Another problem is that the paper didn’t mention why they choose such format of Ψ as the trade-off between △t and gt, since there are many ways to make this trade-off. Also it would be nice to see some generalization to contextual bandit."
    }, {
      "heading" : "3 Thompson Sampling",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "Thompson sampling (TS) [3, 5] is also a Bayesian style bandit algorithm, it can apply to both contextual bandit and standard Multi-armed bandit problems. Here we talk about the non-contextual version. Again, we assume there is an action set A, and at time step t Thompson sampling select action a and get reward ra,t. We also assume the reward of each arm ra follows some parametric distribution pa = P (r|a, θa) with mean µa, where θa is the parameter. Define past observations D consists of arms pulled and rewards observed. At the beginning, Thompson sampling assumes a prior distribution on parameters θa, and then after each time step, it will update the posterior distribution P (θa|D) based on past observations. Similar to IDS, the goal is to minimize the regret:\nE[Regret(T )] = E ra∗∼pa∗\nT ∑\nt=1\nra∗ − E ra,t∼pa\nT ∑\nt=1\nra,t (44)\nwhere a∗ is the arm with the highest expected reward, and a is the arm selected by Thompson sampling."
    }, {
      "heading" : "3.2 Algorithm",
      "text" : "Similar to IDS, Thompson sampling randomly select an action a according to its probability of being optimal. So action a is chosen with probability\n∫\nI\n[\nE(r|a, θ) = max a′\nE(r|a′, θ) ] P (θ|D)dθ (45)\nWhich is essential the same as the αt in IDS. However calculating αt is time consuming, and since in Thompson sampling, we do not need to use αt explicitly, and we only need samples from αt, so it suffices to draw a random parameter θ from posterior distribution. Algorithm 1 describes the procedure of Thompson sampling with Bernoulli bandit problem.\nAlgorithm 1 Thompson sampling with Bernoulli multi-armed bandit Require: α, β: prior parameter of a Beta distribution For each arm i = 1, ...,K set Si = 0, Fi = 0 for t = 1, ..., T do\nfor arm i = 1, ...,K do Draw θi from Beta(α+ Si, β + Fi) end for Play arm a = argmaxi θi, and observe reward rt if rt = 1 then Sa = Sa + 1 else Fa = Fa + 1 end if\nend for"
    }, {
      "heading" : "3.3 Regret",
      "text" : "Although Thompson sampling is a very old algorithm, proposed by [6], but the theoretical analysis is done very recently. We follow [5] and hope to give a intuitive explanation of the regret. Let µ∗ = maxi µi and △i = µ∗ − µi, where i ∈ A, and let ki(t) denote the number of times arm i has been played up to step t− 1. Then the expected total regret in time T + 1 can be written as\nE[Regret(T )] = ∑\ni\n△iE(ki(T + 1)) (46)\nHence to bound the expected regret, we need to bound E(ki(T + 1)) for all i ∈ A. To bound ki(T + 1) we need the following settings [5]: Define F B n,p(·) the cdf and fBn,p(·) the pdf of the binomial distribution with parameters n, p. Define F betaα,β (·) the cdf of beta distribution with parameters α, β. Let i(t) denote the arm played at time t, ki(t) denotes the number of plays of arm i until time t− 1, Si(t) denote the number of successes among the plays of arm i until t− 1 for the Bernoulli bandit case, µ̂(i) denote the empirical mean and θi(t) denote the sample mean reward of arm i at time t. We assume the first arm is the unique optimal arm, i.e,̇ µ∗ = µ1. For each arm i, we will choose two thresholds xi and yi such that µi < xi < yi < µ1. With different choices of xi and yi, we can get problem dependent and problem independent bound respectively. We also define Eµi (t) as the event that µ̂i(t) ≤ xi and Eθi (t) as the event that θi(t) ≤ yi. Finally,\ndefine Ft−1 = {i(w), ri(w)(w), w = 1, ..., t − 1} and pi,t = P (θ1(t) > yi|Ft−1). pi,t indicates what is the probability of the sample reward of arm 1 is greater than yi at time t.\nWe can decompose E(ki(T + 1)) into\nE[ki(T + 1)] =\nT ∑\nt=1\nP (i(t) = i) (47)\n=\nT ∑\nt=1\nP (i(t) = i, Eµi (t), E θ i (t)) (48)\n+ T ∑\nt=1\nP (i(t) = i, Eµi (t), E θ i (t)) (49)\n+\nT ∑\nt=1\nP (i(t) = i, Eµi (t)) (50)\nSo we need to bound (48), (49) and (50) respectively. To bound (48), [5] proved that\nP (i(t) = i, Eµi (t), E θ i (t)|Ft−1) ≤ (1− pi,t) pi,t P (i(t) = 1, Eµi (t), E θ i (t)|Ft−1) (51)\nand so\nT ∑\nt=1\nP (i(t) = i, Eµi (t), E θ i (t)) =\nT ∑\nt=1\nEP (i(t) = i, Eµi (t), E θ i (t)|Ft−1) (52)\n≤ T ∑\nt=1\nE\n[\n(1− pi,t) pi,t P (i(t) = 1, Eµi (t), E θ i (t)|Ft−1)\n]\n(53)\n≤ T−1 ∑\nk=0\nE\n[\n1\npi,τk+1 − 1\n]\n(54)\nwhere τk denotes the time step at which arm 1 is played for the k th time. (54) only involves pi,τk+1 because the posterior distribution of the parameters of arm 1 only changes when arm 1 gets pulled. Now we need to bound (54). Let k1(t) = j and S1(t) = s, from the fact that F betaα,β (y) = 1− FBα+β−1,y(α− 1) we have pi,t = P (θ1(t) > yi) = FBj+1(s), and since\nS1(t) ∼ Binomial(k1(t), µ1) (55) θ1(t) ∼ Beta(S1(t), k1(t)− S1(t)) (56)\nso each possible value S1(t) = s corresponding to a value of pi,τj+1 = F B j+1,y(s) with probability fBj,µ1(s), so\nE\n[\n1\npi,τk+1 − 1\n]\n=\nj ∑\ns=0\nfBj,µ1(s)\nFBj+1,y(s) (57)\nSo we have reduced the problem of bounding (54) to the problem of bounding a summation of a series of random variables involving binomial distribution. [5] provide details about how to bound (57), which is quite complicated.\nNow we bound (50). Let τk denote the time at which k th trial of arm i happens, and τ0 = 0.\nWe have\nT ∑\nt=1\nP (i(t) = i, Eµi (t)) ≤ E\n\n\nT−1 ∑\nk=0\nτk+1 ∑\nt=τk+1\nI(i(t) = i)I(Eµi (t))\n\n (58)\nSince Eµi (t) doesn’t change unless arm i is pulled, and ∑τk+1 t=τk+1 I(i(t) = i) = 1, so (58) is equal to\n(58) = E\n\n\nT−1 ∑\nk=0\nI(Eµi (τk + 1))\nτk+1 ∑\nt=τk+1\nI(i(t) = i)\n\n (59)\n= E\n[\nT−1 ∑\nk=0\nI(Eµi (τk + 1))\n]\n(60)\n≤ 1 + E [ T−1 ∑\nk=1\nI(Eµi (τk + 1))\n]\n(61)\n≤ 1 + T−1 ∑\nk=1\nexp(−kd(xi, µi)) (62)\n≤ 1 + 1 d(xi, µi)\n(63)\nWhere the second last inequality is from Chernoff bound and d(x, y) = x ln xy + (1− x) ln (1−x) (1−y) .\nSimilarly, [5] bound\nT ∑\nt=1\nP (i(t) = i, Eθi (t), E µ i (t)) ≤ Li(T ) + 1 (64)\nwhere Li(T ) = lnT d(xi,yi) Together with this three bounds and a choice of xi and yi for all i ∈ A, we\ncan get a problem independent bound O( √ NT lnN)."
    }, {
      "heading" : "4 Generalized Thompson Sampling",
      "text" : ""
    }, {
      "heading" : "4.1 Problem Formulation",
      "text" : "Generalized Thompson Sampling[4] is a contextual bandit problem, it is similar to expert-learning framework, and include Thompson Sampling as a special case. Let X and A be the set of context and arms, and let K = |A|. At time step t ∈ 1, ..., T , the decision-maker observes the context xt ∈ X and selects an arm at ∈ A. Then it receives reward rt ∈ {0, 1}, with expectation µ(xt, at). In [4] the reward is binary, but it is easy to generalize to continuous space. Different from classic Thompson Sampling algorithm, Generalized Thompson Sampling allows the decision-maker to have access to a set of experts E = {E1, E2, ..., En}, each E makes predicts about the average reward µ(xt, at). Let fi be the associated prediction function of expert Ei, the arm-selection policy is\nEi(x) = maxa∈A fi(x, a). Each expert could be a generalized linear model or other prediction model. The regret is defined as\nE [Regret(T )] = max 1≤i≤N\nN ∑\nt=1\nµ(xt, Ei(xi))− E [ T ∑\nt=1\nµ(xt, at)\n]\n(65)\nThat is, we are competing with the best expert."
    }, {
      "heading" : "4.2 Algorithm",
      "text" : "Generalized Thompson Sampling is described in Algorithm 2. We can see that it updates the weight wi,t+1 by wi,t+1 ∝ wi,t exp(−ηℓ(fi(xt, at), rt)), where ℓ is the loss function. The term ‘Generalized’ in ‘Generalized Thompson Sampling’ means that we can use different types of loss functions when updating wi. [4] described two loss functions: logarithmic loss and square loss. Logarithmic loss is defined as ℓ(r̂, r) = 1(r = 1) ln 1/r̂ + 1(r = 0) ln(1/(1 − r̂)), and square loss is defined as ℓ(r̂, r) = (r̂ − r)2. In next section, we will show that if the loss function is logarithmic loss, then Generalized Thompson Sampling takes the form of Thompson Sampling.\nAlgorithm 2 Generalized Thompson Sampling Require: η > 0, γ > 0, E1, ..., EN , prior p For each expert i = 1, ..., N set w1 = p, W1 = ||w1||1 for t = 1, ..., T do\nReceive context xt ∈ X for arm a = 1, ...,K do\nP (a) = (1− γ)∑Ni=1 wi,t1(Ei(xt)=a) Wt + γK\nend for Select arm at based on P (a), observe reward rt, update weights: ∀i : wi,t+1 = wi,t exp(−ηℓ(fi(xt, at), rt));Wt+1 = ∑\niwi,t+1 end for"
    }, {
      "heading" : "4.3 Connection with Expert-Learning and Thompson Sampling",
      "text" : "Generalized Thompson sampling has the format of expert exponential weighting, however it also fits Thompson sampling framework, there are two ways to see this, and in both ways we need to assume the loss is log loss, that is if an expert f predicts that the probability of r = 1 is p1 and the probability of r = 0 is 1− p1, then the log loss of expert f is ln 1p1 when reward is 1, and is ln 1 1−p1 when reward is 0. The first way to see this: we can think of Generalized Thompson Sampling as maintaining a posterior distribution of the weight of each expert, denoted as wt. This posterior distribution may be interpreted as the posterior probability that fi is the reward-maximizing expert. The update rule, for one step, is\nwi,t+1 ∝ wi,t exp(−ℓ(fi(xt, at), rt)) (66)\n∝ wi,t exp(− ln( 1\np(rt|xt, at) )) (67)\n∝ wi,tpi(rt|xt, at) (68)\nLet f∗ = fi be the event that fi is the reward-maximizing expert. From Bayesian rule we have, for one step\nP (f∗t+1 = fi|xt, at, rt) ∝ P (r = rt|f∗ = fi, xt, at)P (f∗ = fi) (69) ∝ wi,tpi(rt|xt, at) (70)\nWe can see that the update rule and Bayesian rule take the same format. Finally, the posterior distribution on f∗t is\nP (f∗t = fi) = wi,t ∑\ni wi,t (71)\nWe can also see it from a second way. Let yt, xt and rt be the selected arm, context and reward in time t, yt, xt, rt be the selected arms, contexts and rewards in time 1, ..., t respectively, then from Bayesian rule we have\np(yt|yt−1, rt−1, xt) = p(yt−1, yt, r t−1, xt)\np(yt−1, rt−1, xt) (72) = p(yt|rt−1, xt) p(yt−1|rt−1, xt) (73)\nAssume we have a uniform mixture of the distribution defined by the experts (Note that we are assuming uniform mixture over yt and yt−1, not yt), then we have\np(yt|rt−1, xt) p(yt−1|rt−1, xt) =\n∑\nf f(y t|rt−1, xt)\n∑ f f(y t−1|rt−1, xt) (74)\nFrom update rule we have:\np(yt|yt−1, rt−1, xt) = ∑ f wf,t−1f(yt|xt) Wt−1\n(75)\n=\n∑\nf wf,t−1f(yt|xt) ∑\nf wf,t−1 (76)\n=\n∑\nf w0 pf (r1|y1, x1) pf (r2|y2, x2, r1)... pf (rt−1|yt−1, xt−1, rt−2) f(yt|xt) ∑\nf w0 pf (r1|y1, x1) pf (r2|y2, x2, r1)... pf (rt−1|yt−1, xt−1, rt−2) (77)\n=\n∑\nf f(y t, xt−1, rt−1|xt)\n∑\nf f(y t−1, xt−1, rt−1)\n(78)\n=\n∑\nf f(y t|xt, rt−1)\n∑ f f(y t−1|xt−1, rt−1) (79)\nSo we can see that the update rule and Bayesian rule have the same format. However notice that in this view we are conditioned on xt while in the first view the posterior distribution of wt is conditioned on the xt−1."
    }, {
      "heading" : "4.4 Regret",
      "text" : "The basic idea of the derivation is that we assume a connection between the loss function and the regret: Define immediate regret △i(x) = µ(x, E∗(x)) − µ(x, Ei(x)), shifted loss of expert i l̂i(r|x, a) = ℓ(fi(x, a), r) − ℓ(f∗(x, a), r), and average shifted loss l̄ = Ert,at [ ∑ iwi,tl̂i(rt|xt, at) ] , we assume there is a constant k1, such that △i(xt) ≤ k1 √ l̄t. Also we make use of the self-boundedness\nproperty of the loss function: Er\n[ l̂i(r|x, a)2 ] ≤ k2Er [ l̂i(r|x, a) ] , which means the second moment\nis bounded by the first moment of the shifted loss. Then we can bound the expected regret by\n√ 4k2(e− 2)k1(1− γ) √\nT · ln 1 p1 + γT (80)\nDifferent loss has different choice of k1 and k2, and [4] proved that with square loss the expected regret bound is O( √\nln 1p1K 1/3T 2/3) and with logarithmic loss the expected regret bound is\nO( √\nln 1p1K 2/3T 2/3)."
    } ],
    "references" : [ {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi" ],
      "venue" : "arXiv preprint arXiv:1204.5721,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Learning to optimize via information-directed sampling",
      "author" : [ "Dan Russo", "Benjamin Van Roy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "An empirical evaluation of thompson sampling",
      "author" : [ "Olivier Chapelle", "Lihong Li" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Generalized thompson sampling for contextual bandits",
      "author" : [ "Lihong Li" ],
      "venue" : "arXiv preprint arXiv:1310.7163,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Further optimal regret bounds for thompson sampling",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "William R Thompson" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1933
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction Amulti-armed bandit problem [1] is one of the sequential decision making problem.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "1 Problem Formulation Information-Directed Sampling (IDS) [2] consider a Bayesian formulation of Multi-armed bandit problem.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "1 Problem Formulation Thompson sampling (TS) [3, 5] is also a Bayesian style bandit algorithm, it can apply to both contextual bandit and standard Multi-armed bandit problems.",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : "1 Problem Formulation Thompson sampling (TS) [3, 5] is also a Bayesian style bandit algorithm, it can apply to both contextual bandit and standard Multi-armed bandit problems.",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "3 Regret Although Thompson sampling is a very old algorithm, proposed by [6], but the theoretical analysis is done very recently.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "We follow [5] and hope to give a intuitive explanation of the regret.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "To bound ki(T + 1) we need the following settings [5]: Define F B n,p(·) the cdf and fB n,p(·) the pdf of the binomial distribution with parameters n, p.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "To bound (48), [5] proved that P (i(t) = i, E i (t), E θ i (t)|Ft−1) ≤ (1− pi,t) pi,t P (i(t) = 1, E i (t), E θ i (t)|Ft−1) (51) and so T ∑",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "[5] provide details about how to bound (57), which is quite complicated.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Similarly, [5] bound T ∑",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "1 Problem Formulation Generalized Thompson Sampling[4] is a contextual bandit problem, it is similar to expert-learning framework, and include Thompson Sampling as a special case.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "In [4] the reward is binary, but it is easy to generalize to continuous space.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "[4] described two loss functions: logarithmic loss and square loss.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "T · ln 1 p1 + γT (80) Different loss has different choice of k1 and k2, and [4] proved that with square loss the expected regret bound is O( √",
      "startOffset" : 76,
      "endOffset" : 79
    } ],
    "year" : 2015,
    "abstractText" : "This note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers.",
    "creator" : "LaTeX with hyperref package"
  }
}