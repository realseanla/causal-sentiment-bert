{
  "name" : "1409.3653.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Minimax Optimal Offline Policy Evaluation",
    "authors" : [ "Lihong Li", "Remi Munos" ],
    "emails" : [ "lihongli@microsoft.com", "remi.munos@inria.fr", "szepesva@cs.ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a minimax risk lower bound, and analyze the risk of two standard estimators. It is shown, and verified in simulation, that one is minimax optimal up to a constant, while another can be arbitrarily worse, despite its empirical success and popularity. The results are applied to related problems in contextual bandits and fixed-horizon Markov decision processes, and are also related to semi-supervised learning."
    }, {
      "heading" : "1 Introduction",
      "text" : "In reinforcement learning, one of the most fundamental problems is policy evaluation — estimate the average reward obtained by running a given policy to select actions in an unknown system. A straightforward solution is to simply run the policy and measure the rewards it collects. In many applications, however, running a new policy in the actual system can be expensive or even impossible. For example, flying a helicopter with a new policy can be risky as it may lead to crashes; deploying a new ad display policy on a website may be catastrophic to user experience; testing a new treatment on patients may simply be impossible for legal and ethical reasons; etc.\nThese difficulties make it critical to do off-policy policy evaluation (Precup et al., 2000, Sutton et al., 2010), which is sometimes referred to as offline evaluation in the bandit literature (Li et al., 2011) or counterfactual reasoning (Bottou et al., 2013). Here, we still aim to estimate the average reward of a target policy, but instead of being able to run the policy online, we only have access to a sample of observations made about the unknown system, which may be collected in the past using a different policy. Off-policy evaluation has been found useful in a number of important applications (Langford et al., 2008, Li et al., 2011, Bottou et al., 2013) and can also be looked as a key building block for policy optimization which, as in supervised learning, can often be reduced to evaluation, as long as the complexity of the policy class is well-controlled (Ng and Jordan, 2000). For example, it has played an important role in many optimization algorithms for Markov decision processes (e.g., Heidrich-Meisner and Igel 2009) and bandit problems (Auer et al., 2002, Langford and Zhang, 2008, Strehl et al., 2011). In the context of supervised learning, in the covariate shift literature, the problem of estimating losses under changing distributions is crucial for model selection (Sugiyama and Müller, 2005, Yu and Szepesvári, 2012) and also appears in active learning (Dasgupta, 2011). In the statistical literature, on the other hand, the problem appears in the context of randomized experiments. Here, the focus is on the two-action (binary) case where the goal is to estimate the difference between the expected rewards of the two actions (Hirano et al., 2003), which is slightly (but not essentially) different than our setting.\nar X\niv :1\n40 9.\n36 53\nv1 [\ncs .A\nI] 1\n2 Se\nThe topic of the present paper is off-policy evaluation in finite settings, under a mean squared error criterion (MSE). As opposed to the statistics literature (Hirano et al., 2003), we are interested in results for finite sample sizes. In particular, we are interested in limits of performance (minimax MSE) given fixed policies, but unknown stochastic rewards with bounded mean reward, as well as the performance of estimation procedures compared to the minimax MSE. We argue that the finite setting is not a key limitation when focusing on the scaling behavior of the MSE of algorithms. Moreover, we are not aware of prior work that would have studied the above problem (i.e., relating the MSE of algorithms to the best possible MSE). Our main results are as follows: We start with a lower bound on the minimax MSE, to set a target for the estimation procedures. Next, we derive the exact MSE of the likelihood ratio (or importance-weighted) estimator (LR), which is shown to have an extra (uncontrollable) factor as compared to the minimax MSE lower bound. Next, we consider the estimator which estimates the mean rewards by sample means, which we call the regression estimator (REG). The motivation of studying this estimator is both its simplicity and also because it is known that a related estimator is asymptotically efficient (Hirano et al., 2003). The main question is whether the asymptotic efficiency transfers into finite-time efficiency. Our answer to this is mixed: We show that the MSE of REG is within a constant factor of the minimax MSE lower bound, however, the “constant” depends on the number of actions (K), or a lower bound on the variance. We also show that the dependence of the MSE of REG on the number actions is unavoidable. In any case, for “small” action sets or high noise setting, the REG estimator can be thought of as a minimax near-optimal estimator. We also show that for small sample sizes (up to√ K) all estimators must suffer a constant MSE. Numerical experiments illustrate the tightness of the analysis. Implications for more complicated settings, such as policy evaluation in contextual bandits and Markov Decision Processes (MDPs). The question of designing a nearly minimax estimator independently of any problem parameters remains open. All the proofs ot given in the main text can be found in the supplementary material."
    }, {
      "heading" : "2 Multi-armed Bandit",
      "text" : "Let A = {1, 2, . . . ,K} be a finite set of K actions. Data Dn = {(Ai, Ri)}1≤i≤n is generated by the following process: 1 (Ai, Ri) are independent copies of (A,R), where P (A = a) = πD(a) and R ∼ Φ(·|A) for some unknown family of distributions {Φ(·|a)}a∈A and known policy πD. We are also given a known target policy π and want to estimate its value, vπΦ := EA∼π,R∼Φ(·|A)[R] based on the knowledge of Dn, πD and π, where the quality of an estimate v̂ constructed based on Dn (and π, πD) is measured by its mean-squared error, MSE (v̂) := E [ (v̂ − vπΦ)2 ] .\nDefine rΦ(a) := E[R|A = a] and σ2Φ(a) := V(R|A = a), where V(·) stands for the variance. Further, let π∗D := mina πD(a). For convenience, we will identify any function f : A → R with the K-dimensional vector whose kth component is f(k). Thus, rΦ, σ2Φ, etc. will also be looked at as vectors. Note that we do not assume that the rewards are bounded from either direction.\nA few quantities are introduced to facilitate discussions that follow: V1 := E [ V ( π(A)\nπD(A) R|A )] = ∑ a π2(a) πD(a) σ2Φ(a) ,\nV2 := V ( E [ π(A)\nπD(A) R|A\n]) = V ( π(A)\nπD(A) rΦ(A) ) = ∑ a π2(a) πD(a) rΦ(a) 2 − (vπΦ)2 .\nNote that V1 and V2 are functions of Φ, πD and π, but this dependence is suppressed. Also, V1 and V2 are independent in that there are no constants c, C > 0 such that cV1 ≤ V2 ≤ CV1 for any π, πD,Φ. Finally, let pa,n := (1− πD(a))n be the probability of having no sample of a in Dn."
    }, {
      "heading" : "2.1 A Minimax Lower Bound",
      "text" : "We start with establishing a minimax lower bound that characterizes the inherent hardness of the offpolicy evaluation problem. An estimator A can be considered as a function that maps (π, πD, Dn) to an estimate of vπΦ, denoted v̂A(π, πD, D n). Fix σ2 := (σ2(a))a∈A. We consider the minimax\n1The data Dn is actually a list, not a set. We keep the notation {(Ai, Ri)}1≤i≤n for historical reasons.\noptimal risk subject to σ2Φ(a) ≤ σ2(a) and 0 ≤ rΦ(a) ≤ Rmax for all a ∈ A:\nR∗n(π, πD, Rmax, σ 2) := inf\nA sup Φ:σ2Φ≤σ2,0≤rΦ≤Rmax E [ (v̂A(π, πD, D n)− vπΦ)2 ] ,\nwhere for vectors x, y ∈ RK , x ≤ y holds if and only if xi ≤ yi for 1 ≤ i ≤ K. For B ⊂ A, we let pB,n denote the probability that none of the actions in the data Dn falls into B: pB,n = P (A1, . . . , An 6∈ B). Note that this definition generalizes pa,n. We also let π(B) = ∑ a∈B π(a).\nTheorem 1. For any n > 0, πD, π, Rmax and σ2, one has\nR∗n(π, πD, Rmax, σ 2) ≥ 1\n4 max\n( R2max max\nB⊂A π2(B)pB,n, V1 n\n) .\nFurthermore,\nlim inf n→∞\nR∗n(π, πD, Rmax, σ 2)\nV1/n ≥ 1. (1)\nProof. To prove the first part of the lower bound, fix a subset B ⊂ A of actions and choose an environment Φ ∈ E , where E is the set of environments Φ such that σ2Φ ≤ σ2 and 0 ≤ rΦ ≤ Rmax. Introduce the notation EΦ to denote expectation when the data is generated by environment Φ.\nLet Dn be the data generated based on πD and Φ and let v̂A(Dn) denote the estimate produced by some algorithm A. Define S = {A1, . . . , An} to be the set of actions in the dataset that is seen by the algorithm. Clearly, for any Φ,Φ′ such that they agree on the complement of B (but may differ on actions in B),\nEΦ[v̂A(Dn)|S ∩B = ∅] = EΦ′ [v̂A(Dn)|S ∩B = ∅] . (2)\nNow, MSEΦ (v̂A) := EΦ[(v̂A(Dn)− vπΦ)2] ≥ EΦ[(v̂A(Dn)− vπΦ)2|S ∩B = ∅]P (S ∩B = ∅) and by adapting the argument that the MSE is lower bounded by the bias squared, EΦ[(v̂A(Dn) − vπΦ)\n2|S ∩ B = ∅] ≥ (EΦ[v̂A(Dn)|S ∩ B = ∅] − vπΦ)2. Hence, MSEΦ (v̂A) ≥ P (S ∩B = ∅) supΦ∈E(EΦ[v̂A(Dn)|S ∩ B = ∅] − vπΦ)2. We get an even smaller quantity if we further restrict the environments Φ to environments E0 that also satisfy rΦ = σ2Φ = 0 on A \\ B. Now, by (2), for all these environments, EΦ[v̂A(Dn)|S∩B = ∅] takes on a common value, denote it by vA. Hence, MSEΦ (v̂A) ≥ P (S ∩B = ∅) supΦ∈E0(vA − v π Φ) 2. Since vπΦ = ∑ a∈B π(a)rΦ(a), supΦ∈E0(vA − v π Φ) 2 ≥ R 2 max 4 π 2(B), where we use the shorthand π(B) = ∑ a∈B π(a). Plugging this into the previous inequality we get supΦ∈E MSEΦ (v̂A) ≥ P (S ∩B = ∅) R2max 4 π 2(B). Since A was arbitrary, we get R∗n(π, πD, Rmax, σ 2) ≥ P (S ∩B = ∅) R 2 max 4 π 2(B).\nFor the second part, consider a class of normal distributions with fixed reward variances σ2 but different reward expectations: Fp = {Φ0, . . . ,Φp−1}, where rΦi = 2i √ ε∆ ∈ RK , for some to-be-\nspecified vector ∆ ∈ RK+ that satisfies ∑ a π(a)∆(a) = 1. The data-generating distribution Φ is in Fp, but is unknown otherwise. It is easy to see that the policy value between any two distributions in Fp differ by at least 2 √ ε. Indeed, for any Φi,Φj ∈ Fp, |vπΦi − v π Φj | = 2 √ ε|i − j| ∑ a π(a)∆(a) = 2 √ ε|i − j| ≥ 2 √ ε. It follows that, in order to achieve a squared error less than ε, one needs to identify the underlying data-generating Φ from Fp, based on the observed sample Dn. The problem now reduces to finding a minimax lower bound for hypothesis testing in the given finite set Fp. We resort to the information-theoretic machinery based on Fano’s inequality (see, e.g., Raginsky and Rakhlin (2011)). Define an oracle which, when queried, outputs Y = (A,R) with A ∼ πD(·) and R ∼ Φ(·|A). Let the distribution of Y when Φ is used be denoted by PY |Φ. Let Fp collect p distributions such that Φ(·|a) is normal. Consider Φ,Φ′ ∈ Fp. Then,\nD(PY |Φ‖PY |Φ′) = ∑ a πD(a)D(Φ(·|a)‖Φ′(·|a)) = 2ε(i− j)2 ∑ a πD(a)∆(a) 2 σ(a)2 .\nThe divergence measures how much information is carried in one sample from the oracle to tell Φ from Φ′. To obtain the tightest lower bound, we should minimize the divergence. Subject to the\nconstraint ∑ a π(a)∆(a) = 1, the divergence is minimized by setting ∆(a) ∝ π(a) πD(a)\nσ2(a), and is 2ε(i− j)2/V1. Now setting p = 6, and applying Lemma 1, Theorem 1 and the “Information Radius bound” from Raginsky and Rakhlin (2011), we have n ≥ V14ε . Reorganizing terms and combining with the first term complete the proof of the first statement.\nFor the second part, note that it suffices to consider asymptotically unbiased estimators (cf. the generalized Cramer-Rao lower bound, Theorem 7.3 of Ibragimov and Has’minskii 1981). For any such estimator, the Cramer-Rao lower bound gives the result with the parametric family chosen to be p(a, y; θ) = πD(a)ϕ(y; r(a), σ2(a)), where θ = (r(a))a∈A is the unknown parameter to be estimated, and ϕ(·;µ, σ2) is the density of the normal distribution with mean µ and variance σ2 and the quantity to be estimated is ψ(θ) = ∑ a π(a)r(a). For details, see Appendix A.1.\nThe next corollary says that the minimax risk is constant when the number of samples is O( √ K): Corollary 1. For K ≥ 2, n ≤ √ K, supπ R ∗ n(π, πD, Rmax, σ 2) = Ω(R2max).\nProof. Choose B ⊂ A to minimize πD(B) subject to the constraint |B| = b √ Kc. Note that P (A1, . . . , An 6∈ B) = (1 − πD(B))n ≥ (1 − |B|/K)n ≥ (1 − 1/ √ K) √ K ≥ (1 − 1/ √ 2) √\n2. Choosing π such that π(B) = 1 gives the result.\nWe conjecture that the result can be strengthened by increasing the upper limit on n."
    }, {
      "heading" : "2.2 Likelihood Ratio Estimator",
      "text" : "One of the most popular estimators is known as the propensity score estimator in the statistical literature (Rosenbaum and Rubin, 1983, 1985), or the importance weighting estimator (Bottou et al., 2013). We call it the likelihood ratio estimator, as it estimates the unknown value using likelihood ratios, or importance weights:\nv̂LR(π, πD, D n) :=\n1\nn n∑ i=1 π(Ai) πD(Ai) Ri.\nIts distinguishing feature is that it is unbiased: E[v̂LR(π, πD, Dn)] = vπΦ, implying that the MSE is purely contributed by the variance of the estimator. The main result in this subsection shows that this estimator does not achieve the minimax lower bound up to any constant (by making V2 V1). The proof (given in the appendix) is based on a direct calculation using the law of total variance. Proposition 1. It holds that MSE (v̂LR(π, πD, Dn)) = (V1 + V2)/n .\nWe see that as compared to the lower bound on the minimax MSE, an extra V2/n factor appears. In the next section, we will see that this factor is superfluous, showing that the MSE of LR can be “unreasonably large”."
    }, {
      "heading" : "2.3 Regression Estimator",
      "text" : "For convenience, define n(a) := ∑n i=1 I(Ai = a) to be the number of samples for action a in Dn,\nand R(a) := ∑n i=1 I(Ai = a)Ri the total rewards of a. The regression estimator (REG) is given by\nv̂Reg(π,D n) := ∑ a π(a)r̂(a), where r̂(a) := { 0, if n(a) = 0; R(a) n(a) , otherwise .\nFor brevity, we will also write r̂(a) = I{n(a) > 0}R(a)n(a) , where we take 0 0 to be zero. The name of the estimator comes from the fact that it estimates the reward function, and the problem of estimating the reward function can be thought of as a regression problem.\nInterestingly, as can be verified by direct calculation, the REG estimator can also be written as\nv̂Reg(π,D n) =\n1\nn n∑ i=1 π(Ai) π̂D(Ai) Ri , (3)\nwhere π̂D(a) = n(a) n is the empirical estimate of πD(a). Hence, the main difference between LR and REG is that the former uses πD to reweight the data, while the latter uses the empirical estimates π̂D. It may appear that LR is superior since it uses the “right” quantity. Surprisingly, REG turns out to be much more robust than LR, as will be shown shortly; further discussion is made in Section D.\nFor the next statement, the counterpart of Proposition 1, the following quantities will be useful:\nV0,n := (∑ a π(a)rΦ(a)pa,n )2 + ∑ a π2(a)r2Φ(a) pa,n(1− pa,n) and\nV3,n := ∑ a E [ I{n(a) > 0} π̂D(a) − 1 πD(a) ] π(a)2σ2(a) .\nProposition 2. Fix π, πD. Assume that rΦ is nonnegative valued. Then it holds that MSE (v̂Reg(π,D\nn)) ≤ V0,n + (V1 + V3,n)/n. Further, for any Φ such that the rewards have normal distributions, defining bn = ∑ a π(a)rΦ(a)pa,n to be the bias of v̂Reg, MSE (v̂Reg) ≥ V1 n + 4b 2 n ( 1 + V1n ) + 2n ∑ a π2(a) πD(a) σ2Φ(a)pa,n.\nProof sketch. For the upper bound use that the MSE equals the sum of squared bias and the variance. It can be verified that REG is slightly biased: E[v̂Reg] − vπΦ = ∑ a π(a)rΦ(a)pa,n. For the variance term, we use the law of total variance to yield: V(v̂Reg) = E[V(v̂Reg|n(1), . . . , n(K))] + V(E[v̂Reg|n(1), . . . , n(K)]), where the first term is ∑ a π\n2(a)σ2(a)E[I{n(a) > 0}/n(a)], and the second term is upper bounded (Lemma 2) by ∑ a π\n2(a)r2Φ(a) pa,n(1 − pa,n). The proof is then completed by adding squared bias to variance, and using definitions of V0,n, V1, and V3. The lower bound follows from the (generalized) Cramer-Rao inequality.\nThe main result of this section is the following theorem that characterizes the MSE of REG in terms of the minimax optimal MSE. Theorem 2 (Minimax Optimality of the Regression Estimator). The following hold:\n(i) For any π, πD, σ2 = (σ2(a))a∈A, Φ such that mina rΦ(a) ≥ 0, maxa rΦ(a) ≤ Rmax, and σ2Φ ≤ σ2, it holds for any n > 0 that\nMSE (v̂Reg(π,Dn)) ≤ K {\nmin(4K,max a\nr2Φ(a) σ2Φ(a) ) + 5\n} R∗n(π, πD, Rmax, σ 2) , (4)\nwhere Dn = {(Ai, Ri)}i=1,...,n is an i.i.d. sample from (πD,Φ).\n(ii) A suboptimality factor of Ω(K) in the above result is unavoidable: For K > 2, there exists (π, πD) such that for any n ≥ 1,\nMSE (v̂Reg(π,Dn)) R∗n(π, πD, Rmax, 0) ≥ ne−2n/(K−1) .\nThus for n = (K − 1)/2, this ratio is at least K−12e .\n(iii) The estimator v̂Reg is asymptotically minimax optimal:\nlim sup n→∞\nMSE (v̂Reg(π,Dn))\nR∗n(π, πD, Rmax, σ 2) ≤ 1 .\nWe need the following lemma, which may be of interest on its own: Lemma 1. Let X1, . . . , Xn be n independent Bernoulli random variables with parameter p > 0. Letting Sn = ∑n i=1Xi, p̂ = Sn/n, Z = I{Sn>0} p̂ − 1 p , we have for any n and p that E [Z] ≤ 4/p.\nFurther, when np ≥ 34, we have E [Z] ≤ 2p √ 2 np (√ 3 2 ln ( np 2 ) + 1 ) . Proof of Theorem 2. First, we bound V3,n in terms of V1. From Lemma 1, E [ I{n(a)>0} π̂D(a) − 1πD(a) ] ≤\n4 πD(a) , while if nπ∗D ≥ 34, E [ I{n(a)>0} π̂D(a) − 1πD(a) ] ≤ 2πD(a) √ 2 nπD(a) (√ 3 2 ln ( nπD(a) 2 ) + 1 ) .\nPlugging these into the definition of V3,n, we have V3,n ≤ 4V1 for all n. Furthermore, when nπ∗D ≥ 34, thanks to monotonicity of the function t 7→ √ 2 t (√ 3 2 ln t+ 1 ) for t > 1, we have\nV3,n ≤ 2V1 √ 2\nnπD∗\n(√ 3\n2 ln\n( nπ∗D\n2\n) + 1 ) . (5)\nNow, to bound V0,n = ( ∑ a π(a)rΦ(a)pa,n) 2 + ∑ a π\n2(a)r2Φ(a) pa,n(1− pa,n), remember that one lower bound for R∗n is R 2 max maxa π 2(a)pa,n/4, where Rmax is the range for rΦ. Hence,\nV0,n = K 2\n( 1\nK ∑ a π(a)rΦ(a)pa,n )2 + ∑ a π2(a)r2Φ(a) pa,n(1− pa,n)\n≤ K ∑ a π2(a)r2Φ(a)p 2 a,n + ∑ a π2(a)r2Φ(a)pa,n(1− pa,n)\n≤ K ∑ a π2(a)r2Φ(a)pa,n ≤ K2 max a π2(a)r2Φ(a)pa,n . (6)\nHence, using R∗n ≥ V1/n,\nMSE (v̂Reg) ≤ V0,n + V1+V3n ≤ 4K 2 max a π2(a)r2Φ(a)pa,n + 5 V1 n ≤ (4K 2 + 5)R∗n . (7)\nOn the other hand, assuming that mina σ2(a) > 0, we also have V0,n ≤ K ∑ a π2(a)r2Φ(a)pa,n ≤ K max b∈A ( r2Φ(b) σ2(b) ) ∑ a pa,nπ 2(a)σ2(a) ≤ K max b∈A ( r2Φ(b) σ2(b) ) V1 n ,\nwhere in the last inequality we used that pa,n ≤ e−nπD(a) and e−x ≤ 1/x, which is true for any x > 0, and finally also the definition of V1. Similarly to the previous case, we get\nMSE (v̂Reg) ≤ { K max\nb∈A\n( r2Φ(b) σ2(b) ) + 5 } V1 n ≤ { K max b∈A ( r2Φ(b) σ2(b) + 5 )} R∗n .\nCombining this with (7) gives (4).\nFor the second part of the result, choose π(a) = πD(a) = 1/K, rΦ(a) = 1. For K ≥ 2, pa,n = (1 − 1/K)n = e−n log(1/(1−1/K)) = e−n log(1+1/(K−1)) ≥ e−n/(K−1). Hence, we have MSE (v̂Reg) ≥ (E [v̂Reg − vπΦ])2 = ( ∑ a π(a)rΦ(a)pa,n)\n2 ≥ e−2n/(K−1). Now, consider the LR estimator. Choosing σ2 = 0, we have V1 = 0 and so by Proposition 1,\nsup Φ:0≤rΦ≤1,σ2Φ=0 MSE (v̂LR) = sup Φ:0≤rΦ≤1,σ2Φ=0\nV2/n ≤ 1\nn .\nHence, MSE(v̂Reg)R∗n(π,πD,1,0) ≥ e−2n/(K−1)\nsup Φ:0≤rΦ≤1,σ2Φ=0\nMSE(v̂LR) ≥ ne−2n/(K−1).\nFinally, the for the last part, fix any π, πD, σ2, Φ such that σ2Φ ≤ σ2. Then, for n large enough,\nMSE (v̂Reg) ≤ V0,n + V1+V3n ≤ Ce −n/C + V1n\n( 1 + C √ lnn n ) , where C > 0 is a problem de-\npendent constant, and the second inequality used (5) and (6). Combining this with (1) of Theorem 1 gives the desired result."
    }, {
      "heading" : "2.4 Simulation Results",
      "text" : "This subsection corroborates our analysis with simulation results that empirically demonstrate the impact of key quantities on the MSE of the two estimators. Two sets of experiments are done, corresponding to the left and right panels in Figure 1. In all experiments, we repeat the data-generation process (with πD) 10,000 times, and compute the MSE of each estimator. All reward distributions are normal distributions with σ2 = 0.01 and different means. We then plot normalized MSE (MSE multiplied by sample size n), or nMSE, against n.\nThe first experiment is to compare the finite-time as well as asymptotic accuracy of v̂LR and v̂Reg. We choose K = 10, rΦ(a) = a/K, π(a) ∝ a. Three choices of πD are used: (a) πD(a) ∝ a, (b) πD(a) = 1/K, and (c) πD(a) ∝ (K − a). These choices lead to increasing values of V2 (with V1 approximately fixed). Clearly, the nMSE of v̂LR remains constant, equal to V1 + V2, as predicted in Proposition 1. In contrast, the nMSE of v̂Reg is large when n is small, because of the high bias, and then quickly converges to the asymptotic minimax rate V1 (Theorem 2, part iii). As V2 can be arbitrarily larger than V1, it follows that v̂Reg is preferred over v̂LR, as least for sufficiently large n that is needed to drive the bias down. It should be noted that in practice, after Dn is generated, it is easy to quantify the bias of v̂Reg simply by identifying the set of actions a with n(a) = 0.\nThe second experiment is to show how K affects the nMSE of v̂Reg. Here, we choose πD = 1/K, rΦ(a) = a/K, π(a) ∝ a, and vary K ∈ {50, 100, 200, 500, 1000}. As Figure 1 (right) shows, a larger K gives v̂Reg a harder time, which is consistent with Theorem 2 (part i). Not only does the maximum nMSE grow approximately linearly with K, the number of samples needed for nMSE to start decreasing also scales roughly as (K − 1)/2, as indicated by part ii of Theorem 2."
    }, {
      "heading" : "3 Extensions",
      "text" : "In this section, we consider extensions of our previous results to contextual bandits and Markovian Decision Processes, while implications to semi-supervised learning (Zhu and Goldberg, 2009) are discussed in the supplementary material."
    }, {
      "heading" : "3.1 Contextual Bandits",
      "text" : "The problem setup is as follows: In addition to the finite action set A = {1, 2, . . . ,K}, we are also given a context set X = {1, 2, . . . ,M}. A policy now is a map π : X → [0, 1]A such that for any x ∈ X , π(x) is a probability distribution over the action space A. For notational convenience, we will use π(a|x) instead of π(x)(a). The set of policies over X and A will be denoted by Π(X ,A). The process generating the data Dn = {(Xi, Ai, Ri)}1≤i≤n is described by the following: (Xi, Ai, Ri) are independent copies of (X,A,R), where X ∼ µ(·), A ∼ πD(·|X) and R ∼ Φ(·|A,X) for some unknown family of distributions {Φ(·|a, x)}a∈A,x∈X and known policy πD ∈ Π(X ,A) and context distribution µ. For simplicity, we fix Rmax = 1. We are also given a known target policy π ∈ Π(X ,A) and want to estimate its value, vπ,µΦ := EX∼µ,A∼π(·|X),R∼Φ(·|A,X)[R] based on the knowledge of Dn, πD, µ and π, where the quality of an estimate v̂ constructed based on Dn (and π, πD, µ) is measured by its mean squared error, MSE (v̂) := E [ (v̂ − vπ,µΦ )2 ] , just like in the case of contextless bandits. Let σ2Φ(x, a) = V(R) for R ∼ Φ(·|x, a), x ∈ X , a ∈ A. An estimator A can be considered as a function that maps (µ, π, πD, D n) to an estimate of vπ,µΦ , denoted v̂A(µ, π, πD, D n). Fix σ2 := (σ2(x, a))x∈X ,a∈A. The minimax optimal risk subject to σ2Φ(x, a) ≤ σ2(x, a) for all x ∈ X , a ∈ A is defined by R∗n(µ, π, πD, σ 2) := infA supΦ:σ2Φ≤σ2 E [ (v̂A(µ, π, πD, D n)− vπ,µΦ )2 ] .\nThe main observation is that the estimation problem for the contextual case can actually be reduced to the contextless bandit case by treating the context-action pairs as “actions” belonging to the product space X × A. For any policy π, by slightly abusing notation, let (µ ⊗ π)(x, a) = µ(x)π(a|x) be the joint distribution of (X,A) when X ∼ µ(·), A ∼ π(·|X). This way, we can map any contextual policy evaluation problem defined by µ,πD, π, Φ and a sample size n into a contextless policy evaluation problem defined by µ⊗ πD, µ⊗ π, Φ with action set X ×A. Therefore, with V1 and V2 defined similarly, one can conclude the following results: Theorem 3. Pick any n > 0, µ, πD, π and σ2. Then, one has R∗n(µ, π, πD, σ2) = Ω ( maxB⊂X×A{ ∑ (x,a)∈B µ(x)π(a|x)}2{1− ∑ (x,a)∈B µ(x)πd(a|x)}n + V1/n ) , MSE (v̂LR) = (V1 + V2)/n, and MSE (v̂Reg) ≤ CR∗n(µ, π, πD, σ2), for C = MK{min(4MK,maxx,a r2Φ(a)/σ2Φ(a)) + 5}R∗n(µ, π, πD, σ2). Furthermore, the MSE of the regression estimator approaches the minimax risk as sample size grows to infinity."
    }, {
      "heading" : "3.2 Markov Decision Processes",
      "text" : "Similarly, results in Section 2 can be naturally extended to fixed-horizon, finite Markov decision processes (MDPs). Here, an MDP is described by a tuple M = 〈X ,A, P,Φ, ν,H〉, where X = {1, . . . , N} is the set of states, A = {1, . . . ,K} the set of actions, P the transition kernel, Φ : X × A 7→ R the reward function, ν the start-state distribution, and H the horizon. A policy π : X 7→ [0, 1]K maps states to distributions over actions, and we use π(a|x) to denote the probability of choosing action a in state x. Given a policy π ∈ Π(X ,A), a trajectory of length H , denoted T = (X,A,R) (for X ∈ XH , A ∈ AH , and R ∈ RH ), is generated as follows: X(1) ∈ ν(·); for h ∈ {1, . . . ,H}, A(h) ∼ π(·|X(h)), R(h) ∼ Φ(·|X(h), A(h)), and X(h + 1) ∼ P (·|X(h), A(h)). The policy value is defined by vπΦ := ET [ ∑H h=1R(h)]. For simplicity, we again assume Rmax = 1. The off-policy evaluation problem is to estimate vπΦ from data D n = {Tt}1≤t≤n, where each trajectory Tt is independently generated by an exploration policy πD ∈ Π(X ,A). Here, we assume the reward distribution Φ is unknown; other quantities including ν, P , H , π, and πD are all known. Again, we measure the quality of an estimate v̂ by its mean squared error: MSE (v̂) := [ (v̂ − vπΦ)2 ] . By considering a length-H trajectory of state-actions as an “action,”, one can apply the results as in the previous subsection to conclude the following: Theorem 4. Pick any n > 0, ν, πD, π, P , H , and σ2. Then, one has R∗n(ν, π, πD, P,H, σ 2) = Ω ( maxB⊂T { ∑ (x,a)∈T µ(x, a)}2{1− ∑ (x,a)∈T µD(τ)}n + V1/n ) , MSE (v̂LR) = (V1 + V2)/n, and MSE (v̂Reg) ≤ CR∗n(ν, π, πD, P,H, σ2) for C = NH+1KH{min(4NH+1KH ,max(x,a)∈T r2Φ(x,a)\nσ2Φ(x,a) ) + 5}. Moreover, there are cases where such\nan exponential dependence is unavoidable. Finally, the MSE of the regression estimator approaches the minimax risk as sample size n grows to infinity."
    }, {
      "heading" : "4 Conclusions",
      "text" : "We have studied the fundamental problem of finite off-policy evaluation. Despite its importance, it appears that ours are the first results for the finite-sample setting. While the simplest estimator which uses importance weights (called LR) was found to be sensitive to the magnitude of importance weights, the regression estimator (REG), which estimates the mean rewards for each actions, was found to be less exposed to this value. While the sensitivity of LR is a “folk theorem”, we have not seen this result formally proven in the literature. We also found that the REG estimator has different qualities: It is minimax optimal up to a constant, which is the minimum of the squared number of actions, K2, and the maximal inverse reward variance. We showed that the dependence on the number of actions cannot in general be removed. There is still a gap of factor of K between our lower and upper bounds. We conjecture that the lower bound shows the correct order (which seems to be confirmed by the experiments). While it is not hard to design estimators that combine LR and REG, we did not find these attractive as they cannot be shown to be near-optimal in the above sense. Hence, it remains open to design an estimator which is minimax optimal up to a universal constant factor. One starting point is to investigate the many alternate estimators proposed in the literature (e.g., LR with clipped weights, or dividing by the sum of weights instead of dividing by n). While in the paper we focused on the simplest contextless, finite setting, we showed that our\nresults have implications to other, more contextual settings. However, we have only scratched the surface here: Much more work is needed, however, to provide a fuller analysis of sample based off-policy evaluation in these settings."
    }, {
      "heading" : "A Technical Details",
      "text" : "The appendix collects miscellaneous results that are needed in the main body of the text.\nA.1 Proof of the Second Part of Theorem 1\nWe provide here a full proof of the second part of Theorem 1. First, we need some background. Let X = (X ,A) be a measurable space, Θ ⊂ RK open, p ≡ p(·; θ)θ∈Θ be a family of densities with respect to ν, a σ-finite measure on X such that p(·; θ) is defined on the closure Θ̄ of Θ and p is measurable on the product σ-algebra ofX ×Θ where Θ is equipped with the σ-algebra of Borel sets. Denote by F (θ) = ∫ (∂ log p∂θ (x; θ))( ∂ log p ∂θ (x; θ))\n>p(x; θ)ν(dx) be the Fisher information matrix of p at θ. The family p is called regular if the following hold:\n(a) p(x; θ) is a continuous function on Θ for ν-almost all x;\n(b) p possesses finite Fisher’s information at each point θ ∈ Θ; (c) the function ψ(·; θ) is continuous in the space L2(ν). Theorem 5 (Cramer-Rao Lower Bound). Let p = (p(x; θ))x∈X ,θ∈Θ be a regular family of densities with information matrix F (θ) 0, θ ∈ Θ. Pick θ ∈ Θ and assume that ψ : Θ→ R, t : X → R are measurable such that u 7→ ∫ (t(x)− ψ(u))2p(x;u)ν(dx) is bounded in a neighborhood of θ and ψ\nis differentiable. Then, the bias d(u) = ∫ t(x)p(x;u)ν(dx)−ψ(u) is continuously differentiable in a neighborhood of the point θ ∈ Θ and\nE [ (t(X)− ψ(θ))2 ] ≥ (ψ′(θ) + d′(θ))> F−1(θ) (ψ′(θ) + d′(θ)) + ‖d′(θ)‖22 , (8)\nwhere X ∼ p(·; θ)ν(·).\nThe proof follows closely that of Theorem 7.3 of Ibragimov and Has’minskii (1981), which states this result for ψ(θ) = θ (and thus k = K) only, and is hence omitted.\nWith this, we can present the details of the proof of the second part of Theorem 1. Choose X = A × R, p(a, y; θ) = πD(a)ϕ(y; r(a), σ2(a)), where θ = (r(a))a∈A is the unknown parameter to be estimated, and ϕ(·;µ, σ2) is the density of the normal distribution with mean µ and variance σ2, Θ = R. It is easy to see that p = (p(·; θ)θ∈Θ) is a regular family. Let the quantity to be estimated be ψ(θ) = ∑ a π(a)r(a). By Theorem 5, for any estimator A, if v̂n is the estimate constructed by A based on the data Dn generated from p(·; θ) in an i.i.d. fashion, the bias dn(θ) = Eθ[v̂n] is differentiable on Θ and\nMSE (v̂) ≥ 1 n (ψ′(θ) + d′n(θ)) > F−1(θ) (ψ′(θ) + d′n(θ)) + ‖d′n(θ)‖ 2 2 , (9)\nwhere F (θ) is the Fisher information matrix underlying p(·; θ). If MSE (v̂n) 6→ 0 then lim supn→∞ MSE(v̂n) V1/n\n= +∞. Hence, it suffices to consider A such that MSE (v̂n) → 0. Then, by (9), 0 ≤ ‖d′n(θ)‖ 2 2 ≤ MSE (v̂n), hence we also have ‖d′n(θ)‖ 2 2 → 0.\nNow, a direct calculation shows that F (θ) = diag(. . . , πD(a)/σ2(a), . . .) and ψ′(θ) = π. Hence, ψ′(θ)>F−1(θ)ψ′(θ) = V1 and using again (9),\nlim sup n→∞\nMSE (v̂n)\nV1/n ≥ 1− 2 lim sup n→∞\n(d′n(θ)) >F−1(θ)ψ′(θ)\nV1 = 1 ,\nfinishing the proof.\nA.2 Proof for Proposition 1\nIn the proof, we use the shorthand v̂LR for v̂LR(π, πD, Dn). As already noted, the estimator is unbiased, so its MSE equals its variance. Since samples in Dn are independent, we have\nV(v̂LR) = 1 n V ( π(A) πD(A) R ) .\nThe law of total variance implies\nV(v̂LR) = 1 n E [ V ( π(A) πD(A) R|A )] + 1 n V [ E ( π(A) πD(A) R|A )] .\nThe first term equals\n1 n E [( π(A) πD(A) )2 σ2(A)|A )] = 1 n ∑ a πD(a) π2(a) π2D(a) σ2(a) = V1 n .\nThe second term is\n1 n V [ π(A) πD(A) rΦ(A) ] = 1 n [∑ a π2(a) πD(a) r2Φ(a)− (vπΦ) 2 ] = V2 n .\nCombining the two above completes the proof.\nA.3 Proof for Proposition 2\nWe note that the MSE is equal to the sum of the variance and the squared bias. Let us abbreviate v̂Reg(π,D n) by v̂Reg. First, notice that this estimate is (slightly) biased:\nE[v̂Reg] = ∑ a π(a)E[r̂(a)]\n= ∑ a π(a)E[E[r̂(a)|n(a)]]\n= ∑ a π(a)E[rΦ(a)I{n(a) > 0}+ 0× I{n(a) = 0}]\n= ∑ a π(a)rΦ(a)(1− pa,n).\nThus, the squared bias can be bounded as follows:\n(E[v̂Reg]− vπΦ) 2 = (∑ a π(a)rΦ(a)pa,n )2 .\nFor the variance term, we again use the law of total variance to yield:\nV(v̂Reg) = E[V(v̂Reg|n(1), . . . , n(K))] + V(E[v̂Reg|n(1), . . . , n(K)]).\nNow, conditioned on n(1), . . . , n(K), the estimates {r̂(a)}a∈A are independent, so, by distinguishing the case n(a) > 0 (for which the variance of r̂(a) is σ2(a)/n(a)) from the other case n(a) = 0 (for which this variance is 0), we have\nV(v̂Reg|n(1), . . . , n(K)) = ∑ a π2(a) (σ2(a) n(a) I{n(a) > 0}+ 0× I{n(a) = 0} ) .\nThus,\nE[V(v̂Reg|n(1), . . . , n(K))] = ∑ a π2(a)σ2(a)E [ 1 n(a) I{n(a) > 0} ] .\nFor the second variance term, we also distinguish the case n(a) > 0, for which E[r̂(a)|n(a)] = rΦ(a), from the case n(a) = 0, for which E[r̂(a)|n(a) = 0], thus\nE[v̂Reg|n(1), . . . , n(K)] = ∑ a π(a)(rΦ(a)I{n(a) > 0}+ 0× I{n(a) = 0}),\nHence, V(E[v̂Reg|n(1), . . . , n(K)]) = V( ∑ a π(a)rΦ(a)I{n(a) > 0}), which by Lemma 2 implies\nV( ∑ a π(a)rΦ(a)I{n(a) > 0}) ≤ ∑ a π2(a)r2Φ(a) pa,n(1− pa,n) .\nThe proof of the upper bound is then completed by adding squared bias to variance, and using definitions of V0,n, V1, and V3.\nFor the lower bound, use Theorem 5. As mentioned in Appendix A.1, the Fisher information matrix is F (θ) = diag(. . . , πD(a)/σ2(a), . . .) and if the target is ψ(θ) = ∑ a π(a)r(a), ψ\n′(θ) = π. Calculating the derivative of the bias and plugging into (8), we get the result.\nA.4 Proof for Lemma 1\nFor convenience, the lemma is restated here.\nLemma 1. Let X1, . . . , Xn be n independent Bernoulli random variables with parameter p > 0. Letting Sn = ∑n i=1Xi, p̂ = Sn/n, Z = I{Sn>0} p̂ − 1 p , we have for any n and p that\nE [Z] ≤ 4 p . (10)\nFurther, when np ≥ 34,\nE [Z] ≤ 2 p\n√ 2\nnp\n(√ 3\n2 ln (np 2 ) + 1 ) . (11)\nProof. According to the multiplicative Chernoff bound for the low tail (cf. Lemma 3 in the Appendix), for any 0 < δ ≤ 1, with probability at least 1− δ, we have\np̂ ≥ p− √ 2p\nn ln\n1 δ .\nDenote by Eδ the event when this inequality holds. Assuming 2\nnp ln\n1 δ ≤ 1/4 , (12)\nthanks to 1/(1− x) ≤ 1 + 2x which holds for any x ∈ [0, 1/2], on Eδ we have\nZ ≤ 1 p̂ − 1 p ≤ 1 p  1 1− √ 2 np ln 1 δ − 1  ≤ 2 p √ 2 np ln 1 δ .\nThen, since Z ≤ n, we have for every δ satisfying (12) that\nE[Z] ≤ 2 p\n√ 2\nnp ln\n1 δ + δn = 2 p\n(√ 2\nnp ln\n1 δ + np 2 δ\n) = 2 p f (np 2 , δ ) , (13)\nwhere f(u, δ) = √\n1 u ln 1 δ + uδ. Hence, it remains to choose δ to approximately minimize f(u, δ)\nsubject to the constraint δ ≥ e−u/4 (due to (12)). First, note that if we choose δ = e−u/4, then f(u, e−u/4) ≤ 12 + ue −u/4 < 2, showing that EZ ≤ 4/p, proving the first part of the result.\nTo get the second part, we choose δ = u−3/2, which satisfies (12) since u−3/2 ≥ e−u/4 for u ≥ 17. Then, f(u, u−3/2) = u−1/2 (√ 3 2 ln(u) + 1 ) . Plugging this into (13) finishes the proof.\nA.5 Technical Lemmas\nLemma 2. Using notation from Section 2.3, and wa = π(a)rΦ(a) one has\nV ∗ := V (∑ a π(a)rΦ(a)I{n(a) > 0} ) ≤ ∑ a∈A w2a pa,n(1− pa,n)\nprovided that r(a) ≥ 0 for all action a ∈ A.\nProof. Let Xa = I{n(a) > 0}. First, note that E [Xa] = pa,n and so\nV (∑ a∈A waI{n(a) > 0} ) = E [{∑ a∈A wa(Xa − pa,n) }2]\n= ∑ a,b∈A wawb E [(Xa − pa,n)(Xb − pb,n)]\n≤ ∑ a∈A w2a E [ (Xa − pa,n)2 ] (negative association)\n= ∑ a∈A w2a pa,n(1− pa,n) .\nLemma 3 (Multiplicative Chernoff Bound for the Lower Tail, Theorem 4.5 of Mitzenmacher and Upfal (2005)). LetX1, . . . , Xn be independent Bernoulli random variables with parameter p, Sn =∑n i=1Xi. Then, for any 0 ≤ β < 1,\nP ( Sn n ≤ (1− β)p ) ≤ exp ( −β 2np 2 ) ."
    }, {
      "heading" : "B Extension to Contextual Bandits",
      "text" : "In this section we consider an extension of our previous results to finite contextual bandits. As we shall soon see, the extension is seamless. The problem setup is as follows: In addition to the finite action set A = {1, 2, . . . ,K}, we are also given a context set X = {1, 2, . . . ,M}. A policy now is a map π : X → [0, 1]A such that for any x ∈ X , π(x) is a probability distribution over the action space A. For notational convenience, we will use π(a|x) instead of π(x)(a). The set of policies over X and A will be denoted by Π(X ,A). The process generating the data Dn = {(Xi, Ai, Ri)}1≤i≤n is described by the following: (Xi, Ai, Ri) are independent copies of (X,A,R), where X ∼ µ(·), A ∼ πD(·|X) and R ∼ Φ(·|A,X) for some unknown family of distributions {Φ(·|a, x)}a∈A,x∈X and known policy πD ∈ Π(X ,A) and context distribution µ. For simplicity, we fix Rmax = 1. We are also given a known target policy π ∈ Π(X ,A) and want to estimate its value, vπ,µΦ := EX∼µ,A∼π(·|X),R∼Φ(·|A,X)[R] based on the knowledge of Dn, πD, µ and π, where the quality of an estimate v̂ constructed based on Dn (and π, πD, µ) is measured by its mean squared error, MSE (v̂) := E [ (v̂ − vπ,µΦ )2 ] , just like in the case of contextless bandits.\nLet σ2Φ(x, a) = V(R) for R ∼ Φ(·|x, a), x ∈ X , a ∈ A. An estimator A can be considered as a function that maps (µ, π, πD, Dn) to an estimate of v π,µ Φ , denoted v̂A(µ, π, πD, D\nn). Fix σ2 := (σ2(x, a))x∈X ,a∈A. The minimax optimal risk subject to σ2Φ(x, a) ≤ σ2(x, a) for all x ∈ X , a ∈ A is defined by\nR∗n(µ, π, πD, σ 2) := inf\nA sup Φ:σ2Φ≤σ2 E [ (v̂A(µ, π, πD, D n)− vπ,µΦ ) 2 ] .\nThe main observation is that the estimation problem for the contextual case can actually be reduced to the contextless bandit case by treating the context-action pairs as “actions” belonging to the product space X ×A. For any policy π, by slightly abusing notation, let (µ⊗π)(x, a) = µ(x)π(a|x) be the joint distribution of (X,A) when X ∼ µ(·), A ∼ π(·|X). This way, we can map any contextual policy evaluation problem defined by µ,πD, π, Φ and a sample size n into a contextless policy evaluation problem defined by µ⊗ πD, µ⊗ π, Φ with action set X ×A. Let X ∼ µ(·), A ∼ πD(·|X), R ∼ Φ(·|X,A) and define\nV1 := E [ V ( π(A|X) πD(A|X) R|X,A )] = ∑ x,a µ(x) π2(a|x) πD(a|x) σ2Φ(x, a) ,\nV2 := V ( E [ π(A|X) πD(A|X) R|X,A ]) = V ( π(A|X) πD(A|X) rΦ(X,A) ) .\nNote that V1 and V2 are a function of µ, πD and π. In this case the LR and REG estimators take the following form\nv̂LR = 1\nn n∑ i=1 π(Ai|Xi) πD(Ai|Xi) Ri and v̂Reg = ∑ x,a µ(x)π(a|x)r̂(x, a) ,\nwhere now r̂(x, a) = ∑ i I{Xi = x,Ai = a}Ri/ ∑ i I{Xi = x,Ai = a}. Note that the regression estimator can also be computed in O(n) time independently of the size of X and A, based on rewriting it as a likelihood ratio estimator when πD is replaced by its empirical estimates (cf. (3)).\nThe mapping from contextual to contextless bandits gives rise to the following result, combined with Theorem 1, Proposition 1 and Theorem 2: Theorem 6. Pick any n > 0, µ, πD, π and σ2. Then, one has R∗n(µ, π, πD, σ2) = Ω ( maxB⊂X×A{ ∑ (x,a)∈B µ(x)π(a|x)}2{1− ∑ (x,a)∈B µ(x)πd(a|x)}n + V1/n ) , MSE (v̂LR) = (V1 + V2)/n, and MSE (v̂Reg) ≤ CR∗n(µ, π, πD, σ2), for C = MK{min(4MK,maxx,a r2Φ(a)/σ2Φ(a)) + 5}R∗n(µ, π, πD, σ2). Furthermore, the MSE of the regression estimator approaches the minimax risk as sample size grows to infinity."
    }, {
      "heading" : "C Extension to Markov Decision Processes",
      "text" : "In this section, we consider an extension to fixed-horizon, finite Markov decision processes (MDPs), which will be reduced to the bandit problem studied in Section 2. Here, an MDP is described by a tuple M = 〈X ,A, P,Φ, ν,H〉, where X = {1, . . . , N} is the set of states, A = {1, . . . ,K} the set of actions, P the transition kernel, Φ : X × A 7→ R the reward function, ν the start-state distribution, and H the horizon. A policy π : X 7→ [0, 1]K maps states to distributions over actions, and we use π(a|x) to denote the probability of choosing action a in state x. The set of policies over X and A is denoted by Π(X ,A). Given a policy π ∈ Π(X ,A), a trajectory of length H , denoted T = (X,A,R) (for X ∈ XH , A ∈ AH , and R ∈ RH ), is generated as follows: X(1) ∈ ν(·); for h ∈ {1, . . . ,H}, A(h) ∼ π(·|X(h)), R(h) ∼ Φ(·|X(h), A(h)), and X(h + 1) ∼ P (·|X(h), A(h)). The policy value is defined by vπΦ := ET [ ∑H h=1R(h)]. For simplicity, we again assume Rmax = 1.\nThe off-policy evaluation problem is to estimate vπΦ from data D n = {Tt}1≤t≤n, where each trajectory Tt is independently generated by an exploration policy πD ∈ Π(X ,A). Here, we assume the reward distribution Φ is unknown; other quantities including ν, P , H , π, and πD are all known. Again, we measure the quality of an estimate v̂ by its mean squared error: MSE (v̂) := [ (v̂ − vπΦ)2 ] .\nThe key observation is that, similarly to the contextual case, the off-policy evaluation problem in fixed-horizon, finite MDPs can be reduced to the multi-armed bandit case. Specifically, every possible length-H trajectory is an “augmented action” belong to the product space T = XH+1 ×AH . The total number of augmented actions is at most NH+1KH . The distribution over this augmented action space, induced by ν, P and policy π, is given by: µ(x(1), . . . , x(H+ 1), a(1), . . . , a(H)) := ν(x(1)) ∏H h=1 π(a(h)|x(h))P (x(h + 1)|x(h), a(h)) . This way, the off-policy evaluation problem is reduced to the bandit case with corresponding induced distributions over augmented actions.\nFor any (x, a) ∈ T , let rΦ(x, a) := E[R] and σ2Φ(x, a) := V(R), where R(h) ∼ Φ(·|x(h), a(h)). Define the minimax optimal risk subject to constraints σ2Φ(x, a) ≤ σ2(x, a) for all (x, a) ∈ T by:\nR∗n(ν, π, πD, P,H, σ 2) := inf\nA sup Φ:σ2Φ≤σ2 E [ (v̂A(ν, π, πD, P,H,D n)− vπΦ) 2 ] .\nSimilar to previous sections, one may adjust the definitions of quantities like V1 and V2, and conclude the following result using with Theorem 1, Proposition 1 and Theorem 2:\nTheorem 7. Pick any n > 0, ν, πD, π, P , H , and σ2. Then, one has R∗n(ν, π, πD, P,H, σ 2) = Ω ( maxB⊂T { ∑ (x,a)∈T µ(x, a)}2{1− ∑ (x,a)∈T µD(τ)}n + V1/n ) , MSE (v̂LR) = (V1 + V2)/n, and MSE (v̂Reg) ≤ CR∗n(ν, π, πD, P,H, σ2) for C = NH+1KH{min(4NH+1KH ,max(x,a)∈T r2Φ(x,a)\nσ2Φ(x,a) ) + 5}. Finally, the MSE of the regression esti-\nmator approaches the minimax risk as sample size n grows to infinity.\nFinally, it can be shown that in general an exponential dependence onH is unavoidable. An example is the “combination lock” MDP with N states X = {1, . . . , N} and K = 2 actions A = {L,R}; the start state is x∗ = 1. In any state x, action L takes the learner back to the initial state x∗, while action R takes the learner to state x + 1. Assume reward is always zero except in state N where it can be {0, Rmax}. It is easy to verify that, if there exists constant p∗ such that p∗ ≤ πD(L|x) for all x, then it takes exponentially many steps to reach state N from x∗ under policy πD. Consequently, it requires at least exponentially many trajectories to evaluate a policy π that always takes action R."
    }, {
      "heading" : "D Connection to Semi-supervised Learning",
      "text" : "In semi-supervised learning one is given a large unlabeled dataset together with a smaller, labeled dataset. The hope is that the large unlabeled dataset will help to decrease the error of an estimator whose job is to predict some value that depends on the unknown distribution generating the data. Clearly, the off-policy policy evaluation problem can be connected to semi-supervised learning: Given the data {(Ai, Ri)}i=1,...,n generated from πD and Φ, the goal being to predict vπΦ. A large “unlabelled” dataset {Aj}j=1,...,m with m n helps one to identify πD. Indeed, an intriguing idea is to use πD in some clever way to help improving the prediction of vπΦ. The most obvious way is to use it in the likelihood ratio estimator. However, as we have shown, the MSE of the likelihood ratio estimator can be much larger than that of the regression estimator, which does not use πD even if it is available. Further, the MSE of the regression estimator is unimprovable, apart from a constant factor for finite sample sizes, while it also rapidly approaches the optimal minimax MSE as the sample size grow. Hence, it seems unlikely that knowing πD can help in this problem.\nNote that the regression estimator can also be thought as the solution to a least-squares regression problem and our results thus have implications for using unlabelled data together with least-squares estimators. Indeed, if Xi ∈ {0, 1}K is chosen to be the Ais unit vector of the standard Euclidean basis, we can write r̂ = (X>X)†X>R, where † denotes pseudo-inverse, X ∈ Rn×K and R ∈ Rn are defined by R = (R1, . . . , Rn)>, X> = (X1, . . . , Xn). Notice that here Gn = 1nX\n>X = 1 n ∑n i=1XiX > i = diag(π̂D(1), . . . , π̂D(K)). Thus, 1 nX >X can be seen as an estimate of G =\nE [ XiX > i ] = diag(πD(1), . . . , πD(K)). Having access to a large unlabelled set U1, . . . , Um (i.e., m n) coming from the same distribution as the Xis, it is tempting to replace 1nX >X with a\n“better estimate”, Hm = 1m ∑m i=1 UiU > i . Taking m to the limit, we see that Hm converges to G. Now, replacing Gn with Hm ≈ G in the least squares estimate, and then taking the weighted sum of the resulting values with weights π(a), we get the likelihood ratio estimator. Again, since this was shown to be inferior to the regression estimator, replacingGn withHm sound like an idea of dubious status. In fact, preliminary experiments with simple simulated scenarios confirmed that Gn indeed should not be replaced with Hm, even when m is very large in least-squares regression estimation."
    } ],
    "references" : [ {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Counterfactual reasoning and learning systems: The example of computational advertising",
      "author" : [ "Léon Bottou", "Jonas Peters", "Joaquin Quiñonero-Candela", "Denis Xavier Charles", "D. Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Simard", "Ed Snelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bottou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 2013
    }, {
      "title" : "Two faces of active learning",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Dasgupta.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 2011
    }, {
      "title" : "Hoeffding and Bernstein races for selecting policies in evolutionary direct policy search",
      "author" : [ "V. Heidrich-Meisner", "C. Igel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Heidrich.Meisner and Igel.,? \\Q2009\\E",
      "shortCiteRegEx" : "Heidrich.Meisner and Igel.",
      "year" : 2009
    }, {
      "title" : "Efficient estimation of average treatment effects using the estimated propensity",
      "author" : [ "Keisuke Hirano", "Guido W. Imbens", "Geert Ridder" ],
      "venue" : "score. Econometrica,",
      "citeRegEx" : "Hirano et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Hirano et al\\.",
      "year" : 2003
    }, {
      "title" : "The epoch-greedy algorithm for contextual multi-armed bandits",
      "author" : [ "John Langford", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Langford and Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Langford and Zhang.",
      "year" : 2008
    }, {
      "title" : "Exploration scavenging",
      "author" : [ "John Langford", "Alexander L. Strehl", "Jennifer Wortman" ],
      "venue" : "In Proceedings of the Twenty-Fifth International Conference on Machine Learning,",
      "citeRegEx" : "Langford et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2008
    }, {
      "title" : "Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms",
      "author" : [ "Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang" ],
      "venue" : "In Proceedings of the Fourth International Conference on Web Search and Web Data Mining (WSDM-11),",
      "citeRegEx" : "Li et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "Probability and Computing: Randomized Algorithms and Probabilistic Analysis",
      "author" : [ "Michael Mitzenmacher", "Eli Upfal" ],
      "venue" : null,
      "citeRegEx" : "Mitzenmacher and Upfal.,? \\Q2005\\E",
      "shortCiteRegEx" : "Mitzenmacher and Upfal.",
      "year" : 2005
    }, {
      "title" : "PEGASUS: A policy search method for large MDPs and POMDPs",
      "author" : [ "A.Y. Ng", "M. Jordan" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Ng and Jordan.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ng and Jordan.",
      "year" : 2000
    }, {
      "title" : "Eligibility traces for off-policy policy evaluation",
      "author" : [ "Doina Precup", "Richard S. Sutton", "Satinder P. Singh" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Machine Learning",
      "citeRegEx" : "Precup et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2000
    }, {
      "title" : "Information-based complexity, feedback and dynamics in convex programming",
      "author" : [ "Maxim Raginsky", "Alexander Rakhlin" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Raginsky and Rakhlin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Raginsky and Rakhlin.",
      "year" : 2011
    }, {
      "title" : "The central role of the propensity score in observational studies for causal effects",
      "author" : [ "P. Rosenbaum", "D. Rubin" ],
      "venue" : null,
      "citeRegEx" : "Rosenbaum and Rubin.,? \\Q1983\\E",
      "shortCiteRegEx" : "Rosenbaum and Rubin.",
      "year" : 1983
    }, {
      "title" : "Reducing bias in observational studies using subclassification on the propensity score",
      "author" : [ "P. Rosenbaum", "D. Rubin" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Rosenbaum and Rubin.,? \\Q1985\\E",
      "shortCiteRegEx" : "Rosenbaum and Rubin.",
      "year" : 1985
    }, {
      "title" : "Input-dependent estimation of generalization error under covariate shift",
      "author" : [ "M. Sugiyama", "K. Müller" ],
      "venue" : "Statistics & Decisions,",
      "citeRegEx" : "Sugiyama and Müller.,? \\Q2005\\E",
      "shortCiteRegEx" : "Sugiyama and Müller.",
      "year" : 2005
    }, {
      "title" : "Analysis of kernel mean matching under covariate shift",
      "author" : [ "Yaoliang Yu", "Csaba Szepesvári" ],
      "venue" : "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,",
      "citeRegEx" : "Yu and Szepesvári.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yu and Szepesvári.",
      "year" : 2012
    }, {
      "title" : "Introduction to semi-supervised learning",
      "author" : [ "Xiaojin Zhu", "Andrew B. Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Zhu and Goldberg.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu and Goldberg.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : ", 2010), which is sometimes referred to as offline evaluation in the bandit literature (Li et al., 2011) or counterfactual reasoning (Bottou et al.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : ", 2011) or counterfactual reasoning (Bottou et al., 2013).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : ", 2013) and can also be looked as a key building block for policy optimization which, as in supervised learning, can often be reduced to evaluation, as long as the complexity of the policy class is well-controlled (Ng and Jordan, 2000).",
      "startOffset" : 214,
      "endOffset" : 235
    }, {
      "referenceID" : 2,
      "context" : "In the context of supervised learning, in the covariate shift literature, the problem of estimating losses under changing distributions is crucial for model selection (Sugiyama and Müller, 2005, Yu and Szepesvári, 2012) and also appears in active learning (Dasgupta, 2011).",
      "startOffset" : 256,
      "endOffset" : 272
    }, {
      "referenceID" : 4,
      "context" : "Here, the focus is on the two-action (binary) case where the goal is to estimate the difference between the expected rewards of the two actions (Hirano et al., 2003), which is slightly (but not essentially) different than our setting.",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "As opposed to the statistics literature (Hirano et al., 2003), we are interested in results for finite sample sizes.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "The motivation of studying this estimator is both its simplicity and also because it is known that a related estimator is asymptotically efficient (Hirano et al., 2003).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : ", Raginsky and Rakhlin (2011)).",
      "startOffset" : 2,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "Now setting p = 6, and applying Lemma 1, Theorem 1 and the “Information Radius bound” from Raginsky and Rakhlin (2011), we have n ≥ V1 4ε .",
      "startOffset" : 91,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "One of the most popular estimators is known as the propensity score estimator in the statistical literature (Rosenbaum and Rubin, 1983, 1985), or the importance weighting estimator (Bottou et al., 2013).",
      "startOffset" : 181,
      "endOffset" : 202
    }, {
      "referenceID" : 16,
      "context" : "In this section, we consider extensions of our previous results to contextual bandits and Markovian Decision Processes, while implications to semi-supervised learning (Zhu and Goldberg, 2009) are discussed in the supplementary material.",
      "startOffset" : 167,
      "endOffset" : 191
    } ],
    "year" : 2014,
    "abstractText" : "This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a minimax risk lower bound, and analyze the risk of two standard estimators. It is shown, and verified in simulation, that one is minimax optimal up to a constant, while another can be arbitrarily worse, despite its empirical success and popularity. The results are applied to related problems in contextual bandits and fixed-horizon Markov decision processes, and are also related to semi-supervised learning.",
    "creator" : "Creator"
  }
}