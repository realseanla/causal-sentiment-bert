{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", "abstract": "Sound and vision are the primary modalities that influence how we perceive the world around us. Thus, it is crucial to incorporate information from these modalities into language to help machines interact better with humans. While existing works have explored incorporating visual cues into language embeddings, the task of learning word representations that respect auditory grounding remains under-explored. In this work, we propose a new embedding scheme, sound-word2vec that learns language embeddings by grounding them in sound -- for example, two seemingly unrelated concepts, leaves and paper are closer in our embedding space as they produce similar rustling sounds. We demonstrate that the proposed embeddings perform better than language-only word representations, on two purely textual tasks that require reasoning about aural cues -- sound retrieval and foley-sound discovery. Finally, we analyze nearest neighbors to highlight the unique dependencies captured by sound-w2v as compared to language-only embeddings.", "histories": [["v1", "Mon, 6 Mar 2017 04:30:12 GMT  (43kb)", "https://arxiv.org/abs/1703.01720v1", "5 pages"], ["v2", "Fri, 28 Apr 2017 06:35:16 GMT  (59kb)", "http://arxiv.org/abs/1703.01720v2", "5 pages; 3 tables"], ["v3", "Thu, 10 Aug 2017 04:26:57 GMT  (227kb,D)", "http://arxiv.org/abs/1703.01720v3", "Accepted at EMNLP 2017. Contains 6 pages; 3 tables; 1 figure"], ["v4", "Tue, 29 Aug 2017 15:54:31 GMT  (227kb,D)", "http://arxiv.org/abs/1703.01720v4", "Accepted at EMNLP 2017. Contains 6 pages; 3 tables; 1 figure"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.SD", "authors": ["ashwin k vijayakumar", "ramakrishna vedantam", "devi parikh"], "accepted": true, "id": "1703.01720"}
