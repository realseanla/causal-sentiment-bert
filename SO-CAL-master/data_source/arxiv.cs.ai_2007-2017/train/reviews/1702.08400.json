{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Asymmetric Tri-training for Unsupervised Domain Adaptation", "abstract": "Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many methods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain.In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.", "histories": [["v1", "Mon, 27 Feb 2017 17:48:17 GMT  (1078kb)", "https://arxiv.org/abs/1702.08400v1", null], ["v2", "Thu, 16 Mar 2017 15:11:14 GMT  (1078kb)", "http://arxiv.org/abs/1702.08400v2", null], ["v3", "Sat, 13 May 2017 05:44:03 GMT  (1078kb)", "http://arxiv.org/abs/1702.08400v3", "TBA on ICML2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["kuniaki saito", "yoshitaka ushiku", "tatsuya harada"], "accepted": true, "id": "1702.08400"}
