{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2017", "title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation", "abstract": "The popularity of image sharing on social media reflects the important role visual context plays in everyday conversation. In this paper, we present a novel task, Image-Grounded Conversations (IGC), in which natural-sounding conversations are generated about shared photographic images. We investigate this task using training data derived from image-grounded conversations on social media and introduce a new dataset of crowd-sourced conversations for benchmarking progress. Experiments using deep neural network models trained on social media data show that the combination of visual and textual context can enhance the quality of generated conversational turns. In human evaluation, a gap between human performance and that of both neural and retrieval architectures suggests that IGC presents an interesting challenge for vision and language research.", "histories": [["v1", "Sat, 28 Jan 2017 05:06:11 GMT  (244kb,D)", "http://arxiv.org/abs/1701.08251v1", null], ["v2", "Thu, 20 Apr 2017 00:36:35 GMT  (1132kb,D)", "http://arxiv.org/abs/1701.08251v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["nasrin mostafazadeh", "chris brockett", "bill dolan", "michel galley", "jianfeng gao", "georgios p spithourakis", "lucy vanderwende"], "accepted": false, "id": "1701.08251"}
