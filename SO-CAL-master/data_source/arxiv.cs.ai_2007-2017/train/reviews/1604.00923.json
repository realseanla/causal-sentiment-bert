{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning", "abstract": "In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods---it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang and Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.", "histories": [["v1", "Mon, 4 Apr 2016 15:56:52 GMT  (517kb,D)", "http://arxiv.org/abs/1604.00923v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["philip s thomas", "emma brunskill"], "accepted": true, "id": "1604.00923"}
