{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Fusion of EEG and Musical Features in Continuous Music-emotion Recognition", "abstract": "Emotion estimation in music listening is confronting challenges to capture the emotion variation of listeners. Recent years have witnessed attempts to exploit multimodality fusing information from musical contents and physiological signals captured from listeners to improve the performance of emotion recognition. In this paper, we present a study of fusion of signals of electroencephalogram (EEG), a tool to capture brainwaves at a high-temporal resolution, and musical features at decision level in recognizing the time-varying binary classes of arousal and valence. Our empirical results showed that the fusion could outperform the performance of emotion recognition using only EEG modality that was suffered from inter-subject variability, and this suggested the promise of multimodal fusion in improving the accuracy of music-emotion recognition.", "histories": [["v1", "Wed, 30 Nov 2016 12:24:57 GMT  (245kb,D)", "http://arxiv.org/abs/1611.10120v1", "The short version of this paper is accepted to appear as an abstract in the proceedings of AAAI-17 (student abstract and poster program)"]], "COMMENTS": "The short version of this paper is accepted to appear as an abstract in the proceedings of AAAI-17 (student abstract and poster program)", "reviews": [], "SUBJECTS": "cs.AI cs.HC", "authors": ["nattapong thammasan", "ken-ichi fukui", "masayuki numao"], "accepted": false, "id": "1611.10120"}
