{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games", "abstract": "Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without any prior knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a competitive strategy that approached the performance of human experts and state-of-the-art methods.", "histories": [["v1", "Thu, 3 Mar 2016 15:01:54 GMT  (305kb,D)", "http://arxiv.org/abs/1603.01121v1", null], ["v2", "Tue, 28 Jun 2016 15:28:30 GMT  (274kb,D)", "http://arxiv.org/abs/1603.01121v2", "updated version, incorporating conference feedback"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.GT", "authors": ["johannes heinrich", "david silver"], "accepted": false, "id": "1603.01121"}
