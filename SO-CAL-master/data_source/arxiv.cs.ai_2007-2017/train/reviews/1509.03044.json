{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2015", "title": "Recurrent Reinforcement Learning: A Hybrid Approach", "abstract": "Successful applications of reinforcement learning in real-world problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent's entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we study reinforcement learning with deep neural networks, including RNN and LSTM, which are equipped with the desired property of being able to capture long-term dependency on history, and thus providing an effective way of learning the representation of hidden states. We further develop a hybrid approach that combines the strength of both supervised learning (for representing hidden states) and reinforcement learning (for optimizing control) with joint training. Extensive experiments based on a KDD Cup 1998 direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best across the board.", "histories": [["v1", "Thu, 10 Sep 2015 07:45:30 GMT  (129kb,D)", "http://arxiv.org/abs/1509.03044v1", "8 pages, 3 figures"], ["v2", "Thu, 19 Nov 2015 19:32:08 GMT  (2648kb,D)", "http://arxiv.org/abs/1509.03044v2", "11 pages, 6 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY", "authors": ["xiujun li", "lihong li", "jianfeng gao", "xiaodong he", "jianshu chen", "li deng", "ji he"], "accepted": false, "id": "1509.03044"}
