{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Safe and Efficient Off-Policy Reinforcement Learning", "abstract": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\\lambda$), with three desired properties: (1) low variance; (2) safety, as it safely uses samples collected from any behaviour policy, whatever its degree of \"off-policyness\"; and (3) efficiency, as it makes the best use of samples collected from near on-policy behaviour policies. We analyse the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. To our knowledge, this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\\lambda$), which was still an open problem. We illustrate the benefits of Retrace($\\lambda$) on a standard suite of Atari 2600 games.", "histories": [["v1", "Wed, 8 Jun 2016 17:34:13 GMT  (150kb,D)", "http://arxiv.org/abs/1606.02647v1", null], ["v2", "Mon, 7 Nov 2016 21:26:31 GMT  (184kb,D)", "http://arxiv.org/abs/1606.02647v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["r\u00e9mi munos", "tom stepleton", "anna harutyunyan", "marc g bellemare"], "accepted": true, "id": "1606.02647"}
