{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "Towards Neural Machine Translation with Latent Tree Attention", "abstract": "Building models that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in natural language processing. We introduce such a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline.", "histories": [["v1", "Wed, 6 Sep 2017 17:44:53 GMT  (662kb,D)", "http://arxiv.org/abs/1709.01915v1", "Presented at SPNLP 2017"]], "COMMENTS": "Presented at SPNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["james bradbury", "richard socher"], "accepted": false, "id": "1709.01915"}
