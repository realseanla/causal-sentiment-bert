{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Evaluating Complex Task through Crowdsourcing: Multiple Views Approach", "abstract": "With the popularity of massive open online courses, grading through crowdsourcing has become a prevalent approach towards large scale classes. However, for getting grades for complex tasks, which require specific skills and efforts for grading, crowdsourcing encounters a restriction of insufficient knowledge of the workers from the crowd. Due to knowledge limitation of the crowd graders, grading based on partial perspectives becomes a big challenge for evaluating complex tasks through crowdsourcing. Especially for those tasks which not only need specific knowledge for grading, but also should be graded as a whole instead of being decomposed into smaller and simpler subtasks. We propose a framework for grading complex tasks via multiple views, which are different grading perspectives defined by experts for the task, to provide uniformity. Aggregation algorithm based on graders variances are used to combine the grades for each view. We also detect bias patterns of the graders, and debias them regarding each view of the task. Bias pattern determines how the behavior is biased among graders, which is detected by a statistical technique. The proposed approach is analyzed on a synthetic data set. We show that our model gives more accurate results compared to the grading approaches without different views and debiasing algorithm.", "histories": [["v1", "Thu, 30 Mar 2017 17:25:47 GMT  (722kb)", "http://arxiv.org/abs/1703.10579v1", "8 pages, 13 figures, the paper is accepted by ICCSE 2016"]], "COMMENTS": "8 pages, 13 figures, the paper is accepted by ICCSE 2016", "reviews": [], "SUBJECTS": "cs.AI cs.HC", "authors": ["lingyu lyu", "mehmed kantardzic"], "accepted": false, "id": "1703.10579"}
