{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2016", "title": "Latent Dependency Forest Models", "abstract": "Probabilistic modeling is one of the foundations of modern machine learning and artificial intelligence. In this paper, we propose a novel type of probabilistic models named latent dependency forest models (LDFMs). A LDFM models the dependencies between random variables with a forest structure that can change dynamically based on the variable values. It is therefore capable of modeling context-specific independence. We parameterize a LDFM using a first-order non-projective dependency grammar. Learning LDFMs from data can be formulated purely as a parameter learning problem, and hence the difficult problem of model structure learning is circumvented. Our experimental results show that LDFMs are competitive with existing probabilistic models.", "histories": [["v1", "Thu, 8 Sep 2016 00:57:19 GMT  (276kb,D)", "https://arxiv.org/abs/1609.02236v1", "7 pages, 2 figures, conference"], ["v2", "Sun, 20 Nov 2016 15:51:35 GMT  (436kb,D)", "http://arxiv.org/abs/1609.02236v2", "10 pages, 3 figures, conference"]], "COMMENTS": "7 pages, 2 figures, conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shanbo chu", "yong jiang", "kewei tu"], "accepted": true, "id": "1609.02236"}
