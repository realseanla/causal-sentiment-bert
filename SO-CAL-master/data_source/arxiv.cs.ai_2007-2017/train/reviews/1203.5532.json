{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2012", "title": "On the Use of Non-Stationary Policies for Infinite-Horizon Discounted Markov Decision Processes", "abstract": "We consider infinite-horizon discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. We consider the algorithm Value Iteration and the sequence of policies $\\pi_1,...,\\pi_k$ it gen erates until some iteration $k$. We provide performance bounds for non-stationary policies involving the last $m$ generated policies that reduce the state-of-the-art bound for the last stationary policy $\\pi_k$ by a factor $\\frac{1-\\gamma}{1-\\gamma^m}$. In other words, and contrary to a common intuition, we show that it may be much easier to find a non-stationary approximately-optimal policy than a stationary one.", "histories": [["v1", "Sun, 25 Mar 2012 19:44:41 GMT  (4kb)", "https://arxiv.org/abs/1203.5532v1", "(2012)"], ["v2", "Fri, 30 Mar 2012 18:18:05 GMT  (19kb)", "http://arxiv.org/abs/1203.5532v2", null]], "COMMENTS": "(2012)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bruno scherrer"], "accepted": false, "id": "1203.5532"}
