{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2016", "title": "Deep Exploration via Bootstrapped DQN", "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.", "histories": [["v1", "Mon, 15 Feb 2016 10:54:20 GMT  (5872kb,D)", "http://arxiv.org/abs/1602.04621v1", null], ["v2", "Fri, 1 Jul 2016 16:23:55 GMT  (7516kb,D)", "http://arxiv.org/abs/1602.04621v2", null], ["v3", "Mon, 4 Jul 2016 17:11:52 GMT  (7516kb,D)", "http://arxiv.org/abs/1602.04621v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY stat.ML", "authors": ["ian osband", "charles blundell", "alexander pritzel", "benjamin van roy"], "accepted": true, "id": "1602.04621"}
