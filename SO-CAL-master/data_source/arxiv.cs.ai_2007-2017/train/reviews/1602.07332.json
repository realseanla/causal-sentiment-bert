{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2016", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations", "abstract": "Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".", "histories": [["v1", "Tue, 23 Feb 2016 22:00:40 GMT  (7812kb,D)", "http://arxiv.org/abs/1602.07332v1", "44 pages, 37 figures"]], "COMMENTS": "44 pages, 37 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ranjay krishna", "yuke zhu", "oliver groth", "justin johnson", "kenji hata", "joshua kravitz", "stephanie chen", "yannis kalantidis", "li-jia li", "david a shamma", "michael s bernstein", "fei-fei li"], "accepted": false, "id": "1602.07332"}
