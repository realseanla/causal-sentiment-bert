{"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "Authorship Attribution Using a Neural Network Language Model", "abstract": "In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models (NNLMs), generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly 2:5% reduction in perplexity and increases author classification accuracy by 3:43% on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at", "histories": [["v1", "Wed, 17 Feb 2016 04:06:28 GMT  (103kb,D)", "http://arxiv.org/abs/1602.05292v1", "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI'16)"]], "COMMENTS": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI'16)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["zhenhao ge", "yufang sun", "mark j t smith"], "accepted": true, "id": "1602.05292"}
