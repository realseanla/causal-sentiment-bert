{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning", "abstract": "Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.", "histories": [["v1", "Mon, 20 Feb 2017 16:32:07 GMT  (2697kb,D)", "http://arxiv.org/abs/1702.06054v1", "24 pages"]], "COMMENTS": "24 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["sahil sharma", "aravind s lakshminarayanan", "balaraman ravindran"], "accepted": true, "id": "1702.06054"}
