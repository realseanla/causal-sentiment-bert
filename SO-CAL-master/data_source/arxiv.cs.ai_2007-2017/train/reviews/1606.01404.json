{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Generating Natural Language Inference Chains", "abstract": "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.", "histories": [["v1", "Sat, 4 Jun 2016 18:34:51 GMT  (310kb,D)", "http://arxiv.org/abs/1606.01404v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["vladyslav kolesnyk", "tim rockt\\\"aschel", "sebastian riedel"], "accepted": false, "id": "1606.01404"}
