{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Generating Natural Questions About an Image", "abstract": "There has been an explosion of work in the vision &amp; language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks focus on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image often address abstract events that the objects evoke. In this paper, we introduce the novel task of 'Visual Question Generation (VQG)', where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, providing different and more abstract training data than the state-of-the-art captioning systems have used thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions given various images, there is still a wide gap with human performance. Our proposed task offers a new challenge to the community which we hope can spur further interest in exploring deeper connections between vision &amp; language.", "histories": [["v1", "Sat, 19 Mar 2016 07:27:15 GMT  (5296kb,D)", "https://arxiv.org/abs/1603.06059v1", null], ["v2", "Tue, 22 Mar 2016 06:54:58 GMT  (5298kb,D)", "http://arxiv.org/abs/1603.06059v2", null], ["v3", "Thu, 9 Jun 2016 01:20:49 GMT  (4546kb,D)", "http://arxiv.org/abs/1603.06059v3", "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["nasrin mostafazadeh", "ishan misra", "jacob devlin", "margaret mitchell", "xiaodong he", "lucy vanderwende"], "accepted": true, "id": "1603.06059"}
