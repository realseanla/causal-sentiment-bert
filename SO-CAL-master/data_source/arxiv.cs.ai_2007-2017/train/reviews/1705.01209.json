{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Lifelong Metric Learning", "abstract": "The state-of-the-art online learning approaches is only capable of learning the metric for predefined tasks. In this paper, we consider lifelong learning problem to mimic \"human learning\", i.e., endow a new capability to the learned metric for a new task from new online samples and incorporating previous experiences and knowledge. Therefore, we propose a new framework: lifelong metric learning (LML), which only utilizes the data of the new task to train the metric model while preserving the original capabilities. More specifically, the proposed LML maintains a common subspace for all learned metrics, named lifelong dictionary, transfers knowledge from the common subspace to each new metric task with task-specific idiosyncrasy, and redefines the common subspace over time to maximize performance across all metric tasks. We apply online Passive Aggressive optimization to solve the proposed LML framework. Finally, we evaluate our approach by analyzing several multi-task metric learning datasets. Extensive experimental results demonstrate effectiveness and efficiency of the proposed framework.", "histories": [["v1", "Wed, 3 May 2017 00:31:55 GMT  (926kb,D)", "https://arxiv.org/abs/1705.01209v1", "7 pages, 3 figures"], ["v2", "Mon, 12 Jun 2017 15:09:20 GMT  (600kb,D)", "http://arxiv.org/abs/1705.01209v2", "10 pages, 6 figures"]], "COMMENTS": "7 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["gan sun", "yang cong", "ji liu", "xiaowei xu"], "accepted": false, "id": "1705.01209"}
