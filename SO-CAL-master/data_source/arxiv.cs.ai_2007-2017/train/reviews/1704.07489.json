{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Multi-Task Video Captioning with Video and Entailment Generation", "abstract": "Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.", "histories": [["v1", "Mon, 24 Apr 2017 23:07:32 GMT  (4376kb)", "https://arxiv.org/abs/1704.07489v1", "Accepted at ACL 2017 (13 pages w/ supplementary)"], ["v2", "Tue, 8 Aug 2017 17:08:58 GMT  (5518kb)", "http://arxiv.org/abs/1704.07489v2", "ACL 2017 (14 pages w/ supplementary)"]], "COMMENTS": "Accepted at ACL 2017 (13 pages w/ supplementary)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["ramakanth pasunuru", "mohit bansal"], "accepted": true, "id": "1704.07489"}
