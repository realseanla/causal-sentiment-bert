{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jan-2014", "title": "Optimal Demand Response Using Device Based Reinforcement Learning", "abstract": "Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS's rescheduling problem as a reinforcement learning (RL) problem (referred to as the device based RL problem), and show that this RL problem decomposes over devices under reasonable assumptions. Compared with existing formulations, our new formulation (1) does not require explicitly modeling the user's dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also propose several new performance metrics for RL algorithms applied to the device based RL problem, and demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.", "histories": [["v1", "Wed, 8 Jan 2014 00:49:01 GMT  (242kb,D)", "http://arxiv.org/abs/1401.1549v1", null], ["v2", "Sat, 28 Jun 2014 04:24:47 GMT  (185kb,D)", "http://arxiv.org/abs/1401.1549v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY", "authors": ["zheng wen", "daniel o'neill", "hamid reza maei"], "accepted": false, "id": "1401.1549"}
