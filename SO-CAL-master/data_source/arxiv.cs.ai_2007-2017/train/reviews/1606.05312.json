{"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Successor Features for Transfer in Reinforcement Learning", "abstract": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. Our focus is on transfer where the reward functions vary across tasks while the environment's dynamics remain the same. The method we propose rests on two key ideas: \"successor features,\" a value function representation that decouples the dynamics of the environment from the rewards, and \"generalized policy improvement,\" a generalization of dynamic programming's policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice.", "histories": [["v1", "Thu, 16 Jun 2016 18:45:32 GMT  (84kb,D)", "http://arxiv.org/abs/1606.05312v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andr\\'e barreto", "r\\'emi munos", "tom schaul", "david silver"], "accepted": true, "id": "1606.05312"}
