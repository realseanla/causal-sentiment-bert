{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2015", "title": "A Neural Attention Model for Abstractive Sentence Summarization", "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.", "histories": [["v1", "Wed, 2 Sep 2015 13:20:40 GMT  (353kb,D)", "http://arxiv.org/abs/1509.00685v1", "Proceedings of EMNLP 2015"], ["v2", "Thu, 3 Sep 2015 19:55:45 GMT  (352kb,D)", "http://arxiv.org/abs/1509.00685v2", "Proceedings of EMNLP 2015"]], "COMMENTS": "Proceedings of EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["alexander m rush", "sumit chopra", "jason weston"], "accepted": true, "id": "1509.00685"}
