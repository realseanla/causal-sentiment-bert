{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2012", "title": "Policy Iteration for Relational MDPs", "abstract": "Relational Markov Decision Processes are a useful abstraction for complex reinforcement learning problems and stochastic planning problems. Recent work developed representation schemes and algorithms for planning in such problems using the value iteration algorithm. However, exact versions of more complex algorithms, including policy iteration, have not been developed or analyzed. The paper investigates this potential and makes several contributions. First we observe two anomalies for relational representations showing that the value of some policies is not well defined or cannot be calculated for restricted representation schemes used in the literature. On the other hand, we develop a variant of policy iteration that can get around these anomalies. The algorithm includes an aspect of policy improvement in the process of policy evaluation and thus differs from the original algorithm. We show that despite this difference the algorithm converges to the optimal policy.", "histories": [["v1", "Wed, 20 Jun 2012 15:16:29 GMT  (245kb)", "http://arxiv.org/abs/1206.5287v1", "Appears in Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI2007)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI2007)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["chenggang wang", "roni khardon"], "accepted": false, "id": "1206.5287"}
