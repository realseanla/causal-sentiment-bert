{"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Visual Storytelling", "abstract": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.", "histories": [["v1", "Wed, 13 Apr 2016 20:27:43 GMT  (3042kb,D)", "http://arxiv.org/abs/1604.03968v1", "to appear in NAACL 2016"]], "COMMENTS": "to appear in NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["ting-hao huang", "francis ferraro", "nasrin mostafazadeh", "ishan misra", "aishwarya agrawal", "jacob devlin", "ross b girshick", "xiaodong he", "pushmeet kohli", "dhruv batra", "c lawrence zitnick", "devi parikh", "lucy vanderwende", "michel galley", "margaret mitchell"], "accepted": true, "id": "1604.03968"}
