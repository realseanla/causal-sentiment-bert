{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks", "abstract": "This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers allow complex dependencies between the hidden states to be captured that traditional convolutional and fully-connected layers are not able to capture. In this paper, we develop the foundations for such an architecture: we derive the equations to perform exact differentiation through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one particularly standout example, we show that the method is capable of learning to play Sudoku given just input and output games, with no a priori information about the rules of the game; this task is virtually impossible for other neural network architectures that we have experimented with, and highlights the representation capabilities of our approach.", "histories": [["v1", "Wed, 1 Mar 2017 18:58:48 GMT  (445kb,D)", "http://arxiv.org/abs/1703.00443v1", "Submitted to ICML 2017"], ["v2", "Wed, 14 Jun 2017 17:59:07 GMT  (966kb,D)", "http://arxiv.org/abs/1703.00443v2", "ICML 2017"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI math.OC stat.ML", "authors": ["brandon amos", "j zico kolter"], "accepted": true, "id": "1703.00443"}
