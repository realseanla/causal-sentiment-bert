{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2013", "title": "Stochastic gradient descent algorithms for strongly convex functions at O(1/T) convergence rates", "abstract": "With a weighting scheme proportional to t, a traditional stochastic gradient descent (SGD) algorithm achieves a high probability convergence rate of O({\\kappa}/T) for strongly convex functions, instead of O({\\kappa} ln(T)/T). We also prove that an accelerated SGD algorithm also achieves a rate of O({\\kappa}/T).", "histories": [["v1", "Thu, 9 May 2013 21:31:47 GMT  (17kb,D)", "http://arxiv.org/abs/1305.2218v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["shenghuo zhu"], "accepted": false, "id": "1305.2218"}
