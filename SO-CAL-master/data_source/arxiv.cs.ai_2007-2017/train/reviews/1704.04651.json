{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "The Reactor: A Sample-Efficient Actor-Critic Architecture", "abstract": "In this work we present a new reinforcement learning agent, called Reactor (for Retrace-actor), based on an off-policy multi-step return actor-critic architecture. The agent uses a deep recurrent neural network for function approximation. The network outputs a target policy {\\pi} (the actor), an action-value Q-function (the critic) evaluating the current policy {\\pi}, and an estimated behavioral policy {\\hat \\mu} which we use for off-policy correction. The agent maintains a memory buffer filled with past experiences. The critic is trained by the multi-step off-policy Retrace algorithm and the actor is trained by a novel {\\beta}-leave-one-out policy gradient estimate (which uses both the off-policy corrected return and the estimated Q-function). The Reactor is sample-efficient thanks to the use of memory replay, and numerical efficient since it uses multi-step returns. Also both acting and learning can be parallelized. We evaluated our algorithm on 57 Atari 2600 games and demonstrate that it achieves state-of-the-art performance.", "histories": [["v1", "Sat, 15 Apr 2017 15:38:23 GMT  (727kb,D)", "http://arxiv.org/abs/1704.04651v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["audrunas gruslys", "mohammad gheshlaghi azar", "marc g bellemare", "remi munos"], "accepted": false, "id": "1704.04651"}
