{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2017", "title": "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks", "abstract": "Deep learning classifiers are known to be inherently vulnerable to manipulation by intentionally perturbed inputs, named adversarial examples. In this work, we establish that reinforcement learning techniques based on Deep Q-Networks (DQNs) are also vulnerable to adversarial input perturbations, and verify the transferability of adversarial examples across different DQN models. Furthermore, we present a novel class of attacks based on this vulnerability that enable policy manipulation and induction in the learning process of DQNs. We propose an attack mechanism that exploits the transferability of adversarial examples to implement policy induction attacks on DQNs, and demonstrate its efficacy and impact through experimental study of a game-learning scenario.", "histories": [["v1", "Mon, 16 Jan 2017 02:39:01 GMT  (2602kb)", "http://arxiv.org/abs/1701.04143v1", "14 pages, 5 figures, pre-print of submission to MLDM '17"]], "COMMENTS": "14 pages, 5 figures, pre-print of submission to MLDM '17", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["vahid behzadan", "arslan munir"], "accepted": false, "id": "1701.04143"}
