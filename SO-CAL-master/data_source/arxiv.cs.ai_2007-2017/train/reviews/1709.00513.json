{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Learning Loss for Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information provided by a large and accurate teacher network. We use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student.", "histories": [["v1", "Sat, 2 Sep 2017 01:03:08 GMT  (356kb,D)", "http://arxiv.org/abs/1709.00513v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["zheng xu", "yen-chang hsu", "jiawei huang"], "accepted": false, "id": "1709.00513"}
