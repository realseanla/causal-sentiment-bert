{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2015", "title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning", "abstract": "Multi-relational learning has received lots of attention from researchers in various research communities. Most existing methods either suffer from superlinear per-iteration cost, or are sensitive to the given ranks. To address both issues, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics, which can be generalized as a graph Laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration (SHOOI) version. We first induce the equivalence relation of the Schatten p-norm (0&lt;p&lt;\\infty) of a low multi-linear rank tensor and its core tensor. Then we achieve a much smaller matrix trace norm minimization problem. Finally, we develop two efficient augmented Lagrange multiplier algorithms to solve our problems with convergence guarantees. Extensive experiments using both real and synthetic datasets, even though with only a few observations, verified both the efficiency and effectiveness of our methods.", "histories": [["v1", "Sat, 26 Dec 2015 15:26:05 GMT  (768kb)", "https://arxiv.org/abs/1512.08120v1", "18 pages, 10 figures"], ["v2", "Sat, 16 Jan 2016 15:32:15 GMT  (700kb)", "http://arxiv.org/abs/1512.08120v2", "18 pages, 10 figures"]], "COMMENTS": "18 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["fanhua shang", "james cheng", "hong cheng"], "accepted": false, "id": "1512.08120"}
