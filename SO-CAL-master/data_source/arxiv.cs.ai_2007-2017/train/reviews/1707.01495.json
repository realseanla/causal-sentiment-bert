{"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2017", "title": "Hindsight Experience Replay", "abstract": "Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.", "histories": [["v1", "Wed, 5 Jul 2017 17:55:53 GMT  (1023kb,D)", "http://arxiv.org/abs/1707.01495v1", null], ["v2", "Mon, 10 Jul 2017 18:35:33 GMT  (1023kb,D)", "http://arxiv.org/abs/1707.01495v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE cs.RO", "authors": ["marcin", "rychowicz", "filip wolski", "alex ray", "jonas schneider", "rachel fong", "peter welinder", "bob mcgrew", "josh tobin", "pieter abbeel", "wojciech zaremba"], "accepted": true, "id": "1707.01495"}
