{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2015", "title": "Symmetry-invariant optimization in deep networks", "abstract": "Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem.", "histories": [["v1", "Thu, 5 Nov 2015 14:17:40 GMT  (1798kb,D)", "https://arxiv.org/abs/1511.01754v1", "Submitted to ICLR 2016. arXiv admin note: text overlap witharXiv:1511.01029"], ["v2", "Sat, 7 Nov 2015 19:01:03 GMT  (1798kb,D)", "http://arxiv.org/abs/1511.01754v2", "Submitted to ICLR 2016. arXiv admin note: text overlap witharXiv:1511.01029"]], "COMMENTS": "Submitted to ICLR 2016. arXiv admin note: text overlap witharXiv:1511.01029", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["vijay badrinarayanan", "bamdev mishra", "roberto cipolla"], "accepted": false, "id": "1511.01754"}
