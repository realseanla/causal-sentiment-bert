{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Extrapolation and learning equations", "abstract": "In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified.", "histories": [["v1", "Mon, 10 Oct 2016 16:47:36 GMT  (1884kb,D)", "http://arxiv.org/abs/1610.02995v1", "13 pages, 8 figures, 4 tables"]], "COMMENTS": "13 pages, 8 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["georg martius", "christoph h lampert"], "accepted": false, "id": "1610.02995"}
