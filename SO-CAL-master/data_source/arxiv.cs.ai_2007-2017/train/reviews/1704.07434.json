{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Paying Attention to Descriptions Generated by Image Captioning Models", "abstract": "To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliency-boosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization ability is, however, observed for the saliency-boosted model on unseen data.", "histories": [["v1", "Mon, 24 Apr 2017 19:51:16 GMT  (8097kb,D)", "http://arxiv.org/abs/1704.07434v1", null], ["v2", "Wed, 28 Jun 2017 10:13:45 GMT  (1069kb,D)", "http://arxiv.org/abs/1704.07434v2", null], ["v3", "Fri, 4 Aug 2017 11:24:45 GMT  (1140kb,D)", "http://arxiv.org/abs/1704.07434v3", "To appear in ICCV 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hamed r tavakoli", "rakshith shetty", "ali borji", "jorma laaksonen"], "accepted": false, "id": "1704.07434"}
