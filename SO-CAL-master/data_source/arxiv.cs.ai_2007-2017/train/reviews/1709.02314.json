{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Representation Learning for Visual-Relational Knowledge Graphs", "abstract": "Much progress has been made towards the goal of developing ML systems that are able to recognize and interpret visual scenes. With this paper, we propose query answering in visual-relational knowledge graphs (KGs) as a novel and important reasoning problem. A visual-relational KG is a KG whose entities are associated with image data. We introduce \\textsc{ImageGraph}, a publicly available KG with 1330 relation types, 14,870 entities, and 829,931 images. Visual-relational KGs naturally lead to several novel query types treating images as first-class citizens. We approach the query answering problems by combining ideas from the areas of KG embedding learning and deep learning for computer vision. The resulting ML models can answer queries such as \\textit{\"How are these two unseen images related to each other?\"} We also explore a novel zero-shot learning scenario where an image of an entirely new entity is linked to entities of an existing visual-relational KG. An extensive set of experiments shows that the proposed deep neural networks are able to answer the visual-relational queries efficiently and accurately.", "histories": [["v1", "Thu, 7 Sep 2017 15:31:54 GMT  (2033kb,D)", "http://arxiv.org/abs/1709.02314v1", null], ["v2", "Mon, 11 Sep 2017 16:41:07 GMT  (2167kb,D)", "http://arxiv.org/abs/1709.02314v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["daniel o\\~noro-rubio", "mathias niepert", "alberto garc\\'ia-dur\\'an", "roberto gonz\\'alez", "roberto j l\\'opez-sastre"], "accepted": false, "id": "1709.02314"}
