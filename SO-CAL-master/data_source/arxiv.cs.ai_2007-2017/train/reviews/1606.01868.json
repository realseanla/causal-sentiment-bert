{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Unifying Count-Based Exploration and Intrinsic Motivation", "abstract": "We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use sequential density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary sequential density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.", "histories": [["v1", "Mon, 6 Jun 2016 19:21:32 GMT  (2153kb,D)", "http://arxiv.org/abs/1606.01868v1", null], ["v2", "Mon, 7 Nov 2016 21:16:21 GMT  (2091kb,D)", "http://arxiv.org/abs/1606.01868v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marc g bellemare", "sriram srinivasan", "georg ostrovski", "tom schaul", "david saxton", "r\u00e9mi munos"], "accepted": true, "id": "1606.01868"}
