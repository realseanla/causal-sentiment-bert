{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Open Problem: Approximate Planning of POMDPs in the class of Memoryless Policies", "abstract": "Planning plays an important role in the broad class of decision theory. Planning has drawn much attention in recent work in the robotics and sequential decision making areas. Recently, Reinforcement Learning (RL), as an agent-environment interaction problem, has brought further attention to planning methods. Generally in RL, one can assume a generative model, e.g. graphical models, for the environment, and then the task for the RL agent is to learn the model parameters and find the optimal strategy based on these learnt parameters. Based on environment behavior, the agent can assume various types of generative models, e.g. Multi Armed Bandit for a static environment, or Markov Decision Process (MDP) for a dynamic environment. The advantage of these popular models is their simplicity, which results in tractable methods of learning the parameters and finding the optimal policy. The drawback of these models is again their simplicity: these models usually underfit and underestimate the actual environment behavior. For example, in robotics, the agent usually has noisy observations of the environment inner state and MDP is not a suitable model.", "histories": [["v1", "Wed, 17 Aug 2016 15:20:35 GMT  (586kb)", "http://arxiv.org/abs/1608.04996v1", "arXiv admin note: substantial text overlap witharXiv:1602.07764"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1602.07764", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kamyar azizzadenesheli", "alessandro lazaric", "animashree anandkumar"], "accepted": false, "id": "1608.04996"}
