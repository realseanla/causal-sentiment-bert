{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2012", "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search", "abstract": "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty. In this setting, a Bayes-optimal policy captures the ideal trade-off between exploration and exploitation. Unfortunately, finding Bayes-optimal policies is notoriously taxing due to the enormous search space in the augmented belief-state MDP. In this paper we exploit recent advances in sample-based planning, based on Monte-Carlo tree search, to introduce a tractable method for approximate Bayes-optimal planning. Unlike prior work in this area, we avoid expensive applications of Bayes rule within the search tree, by lazily sampling models from the current beliefs. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems.", "histories": [["v1", "Mon, 14 May 2012 17:20:29 GMT  (965kb,D)", "http://arxiv.org/abs/1205.3109v1", null], ["v2", "Sat, 13 Oct 2012 15:19:09 GMT  (846kb,D)", "http://arxiv.org/abs/1205.3109v2", "14 pages, 7 figures, includes supplementary material. Advances in Neural Information Processing Systems (NIPS) 2012"], ["v3", "Thu, 3 Jan 2013 14:44:59 GMT  (846kb,D)", "http://arxiv.org/abs/1205.3109v3", "14 pages, 7 figures, includes supplementary material. Advances in Neural Information Processing Systems (NIPS) 2012"], ["v4", "Wed, 18 Dec 2013 11:45:49 GMT  (846kb,D)", "http://arxiv.org/abs/1205.3109v4", "14 pages, 7 figures, includes supplementary material. Advances in Neural Information Processing Systems (NIPS) 2012"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["arthur guez", "david silver", "peter dayan"], "accepted": true, "id": "1205.3109"}
