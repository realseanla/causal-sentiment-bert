{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders", "abstract": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.", "histories": [["v1", "Wed, 5 Apr 2017 06:34:22 GMT  (4465kb,D)", "http://arxiv.org/abs/1704.01279v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SD", "authors": ["jesse engel", "cinjon resnick", "adam roberts", "sander dieleman", "mohammad norouzi", "douglas eck", "karen simonyan"], "accepted": true, "id": "1704.01279"}
