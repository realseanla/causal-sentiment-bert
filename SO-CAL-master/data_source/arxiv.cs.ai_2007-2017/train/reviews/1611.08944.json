{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Nonparametric General Reinforcement Learning", "abstract": "Reinforcement learning (RL) problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider RL in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm.", "histories": [["v1", "Mon, 28 Nov 2016 00:36:40 GMT  (195kb,D)", "http://arxiv.org/abs/1611.08944v1", "PhD thesis"]], "COMMENTS": "PhD thesis", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jan leike"], "accepted": false, "id": "1611.08944"}
