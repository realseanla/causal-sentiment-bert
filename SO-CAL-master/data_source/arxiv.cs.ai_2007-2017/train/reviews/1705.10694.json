{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Deep Learning is Robust to Massive Label Noise", "abstract": "Deep neural networks trained on large supervised datasets have led to impressive results in recent years. However, since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained. In this paper, we investigate the behavior of deep neural networks on training sets with massively noisy labels. We show that successful learning is possible even with an essentially arbitrary amount of noise. For example, on MNIST we find that accuracy of above 90 percent is still attainable even when the dataset has been diluted with 100 noisy examples for each clean example. Such behavior holds across multiple patterns of label noise, even when noisy labels are biased towards confusing classes. Further, we show how the required dataset size for successful training increases with higher label noise. Finally, we present simple actionable techniques for improving learning in the regime of high label noise.", "histories": [["v1", "Tue, 30 May 2017 15:10:51 GMT  (102kb,D)", "https://arxiv.org/abs/1705.10694v1", null], ["v2", "Wed, 31 May 2017 02:02:56 GMT  (102kb,D)", "http://arxiv.org/abs/1705.10694v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["david rolnick", "reas veit", "serge belongie", "nir shavit"], "accepted": false, "id": "1705.10694"}
