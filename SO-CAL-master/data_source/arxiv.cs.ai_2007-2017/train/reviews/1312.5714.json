{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Avoiding Confusion between Predictors and Inhibitors in Value Function Approximation", "abstract": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "histories": [["v1", "Thu, 19 Dec 2013 19:52:52 GMT  (51kb,D)", "http://arxiv.org/abs/1312.5714v1", null], ["v2", "Wed, 18 Feb 2015 15:35:56 GMT  (265kb,D)", "http://arxiv.org/abs/1312.5714v2", "14 pages, 3 figures, 23 references, Workshop paper in ICLR 2014 (updated based on reviewer comments)"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["patrick c connor", "thomas p trappenberg"], "accepted": false, "id": "1312.5714"}
