{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Enhanced Experience Replay Generation for Efficient Reinforcement Learning", "abstract": "Applying deep reinforcement learning (RL) on real systems suffers from slow data sampling. We propose an enhanced generative adversarial network (EGAN) to initialize an RL agent in order to achieve faster learning. The EGAN utilizes the relation between states and actions to enhance the quality of data samples generated by a GAN. Pre-training the agent with the EGAN shows a steeper learning curve with a 20% improvement of training time in the beginning of learning, compared to no pre-training, and an improvement compared to training with GAN by about 5% with smaller variations. For real time systems with sparse and slow data sampling the EGAN could be used to speed up the early phases of the training process.", "histories": [["v1", "Tue, 23 May 2017 13:36:00 GMT  (726kb,D)", "http://arxiv.org/abs/1705.08245v1", null], ["v2", "Mon, 29 May 2017 14:24:08 GMT  (726kb,D)", "http://arxiv.org/abs/1705.08245v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vincent huang", "tobias ley", "martha vlachou-konchylaki", "wenfeng hu"], "accepted": false, "id": "1705.08245"}
