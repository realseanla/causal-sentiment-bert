{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Batch Reinforcement Learning on the Industrial Benchmark: First Experiences", "abstract": "The Particle Swarm Optimization Policy (PSO-P) has been recently introduced and proven to produce remarkable results on interacting with academic reinforcement learning benchmarks in an off-policy, batch-based setting. To further investigate the properties and feasibility on real-world applications, this paper investigates PSO-P on the so-called Industrial Benchmark (IB), a novel reinforcement learning (RL) benchmark that aims at being realistic by including a variety of aspects found in industrial applications, like continuous state and action spaces, a high dimensional, partially observable state space, delayed effects, and complex stochasticity. The experimental results of PSO-P on IB are compared to results of closed-form control policies derived from the model-based Recurrent Control Neural Network (RCNN) and the model-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not only of interest for academic benchmarks, but also for real-world industrial applications, since it also yielded the best performing policy in our IB setting. Compared to other well established RL techniques, PSO-P produced outstanding results in performance and robustness, requiring only a relatively low amount of effort in finding adequate parameters or making complex design decisions.", "histories": [["v1", "Sat, 20 May 2017 05:31:52 GMT  (341kb)", "https://arxiv.org/abs/1705.07262v1", null], ["v2", "Thu, 27 Jul 2017 15:34:21 GMT  (341kb)", "http://arxiv.org/abs/1705.07262v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE cs.SY", "authors": ["daniel hein", "steffen udluft", "michel tokic", "alexander hentschel", "thomas a runkler", "volkmar sterzing"], "accepted": false, "id": "1705.07262"}
