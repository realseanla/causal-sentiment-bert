{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2014", "title": "On the Computational Efficiency of Training Neural Networks", "abstract": "It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.", "histories": [["v1", "Sun, 5 Oct 2014 10:54:07 GMT  (45kb,D)", "http://arxiv.org/abs/1410.1141v1", null], ["v2", "Tue, 28 Oct 2014 19:14:37 GMT  (45kb,D)", "http://arxiv.org/abs/1410.1141v2", "Section 2 is revised due to a mistake"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["roi livni", "shai shalev-shwartz", "ohad shamir"], "accepted": true, "id": "1410.1141"}
