{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards", "abstract": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of {\\em under-appreciated reward} regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its \\mbox{resulting} reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback.", "histories": [["v1", "Mon, 28 Nov 2016 20:15:55 GMT  (391kb,D)", "http://arxiv.org/abs/1611.09321v1", "Under review at ICLR 2017"], ["v2", "Wed, 25 Jan 2017 22:35:03 GMT  (992kb,D)", "http://arxiv.org/abs/1611.09321v2", "Under review at ICLR 2017"], ["v3", "Wed, 15 Mar 2017 22:55:17 GMT  (995kb,D)", "http://arxiv.org/abs/1611.09321v3", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Under review at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["ofir nachum", "mohammad norouzi", "dale schuurmans"], "accepted": true, "id": "1611.09321"}
