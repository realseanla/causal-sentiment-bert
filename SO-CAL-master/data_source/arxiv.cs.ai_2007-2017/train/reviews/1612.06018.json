{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2016", "title": "Self-Correcting Models for Model-Based Reinforcement Learning", "abstract": "When an agent cannot represent a perfectly accurate model of its environment's dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictions of the model; when flawed predictions are composed, even minor errors can compound and render the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the model to \"correct\" itself when it produces errors, substantially improving MBRL with flawed models. This paper theoretically analyzes this approach, illuminates settings in which it is likely to be effective or ineffective, and presents a novel error bound, showing that a model's ability to self-correct is more tightly related to MBRL performance than one-step prediction error. These results inspire an MBRL algorithm for deterministic MDPs with performance guarantees that are robust to model class limitations.", "histories": [["v1", "Mon, 19 Dec 2016 01:09:23 GMT  (235kb,D)", "https://arxiv.org/abs/1612.06018v1", "To appear in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material)"], ["v2", "Wed, 26 Jul 2017 18:53:51 GMT  (236kb,D)", "http://arxiv.org/abs/1612.06018v2", "Original paper appeared in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material), corrects a minor error in Lemma 1, and fixes some type-os"]], "COMMENTS": "To appear in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["erik talvitie"], "accepted": true, "id": "1612.06018"}
