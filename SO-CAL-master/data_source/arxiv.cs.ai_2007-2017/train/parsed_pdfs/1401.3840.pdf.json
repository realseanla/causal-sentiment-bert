{
  "name" : "1401.3840.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Grounding FO and FO(ID) with Bounds",
    "authors" : [ "Johan Wittocx", "Maarten Mariën", "Marc Denecker" ],
    "emails" : [ "johan.wittocx@cs.kuleuven.be", "maarten.marien@cs.kuleuven.be", "marc.denecker@cs.kuleuven.be" ],
    "sections" : [ {
      "heading" : null,
      "text" : "propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning.\nBesides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounder’s output can often be reduced significantly. A common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. In this paper we present a method to compute and add such redundant information automatically. Our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems.\nWe first present our method for classical first-order logic (FO) theories. Then we extend it to FO(ID), the extension of FO with inductive definitions, which allows for more concise and comprehensive input theories. We discuss implementation issues and experimentally validate the practical applicability of our method."
    }, {
      "heading" : "1. Introduction",
      "text" : "Grounding, or propositionalization, is the task of reducing a first-order theory and finite domain to an equivalent propositional theory, called a grounding. Grounding is used as a preprocessing phase in many logic-based reasoning systems. It serves to provide the user with a rich input language, while enabling the system to rely on efficient propositional solvers to perform the actual reasoning.\nExamples of systems that rely on grounding can be found in the area of finite first-order model generation (Claessen & Sörensson, 2003; McCune, 2003; East, Iakhiaev, Mikitiuk, & Truszczyński, 2006; Mitchell, Ternovska, Hach, & Mohebali, 2006; Torlak & Jackson, 2007; Wittocx, Mariën, & Denecker, 2008d). Such systems are in turn used as part of theorem provers (Claessen & Sörensson, 2003) and for lightweight software verification (Jackson, 2006). Currently, almost all Answer Set Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele, 2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrjänen, 2000; Syrjänen, 2009). Also in planning systems (Kautz & Selman, 1996) and relational data mining (Krogel, Rawles, Zelezný, Flach, Lavrac, & Wrobel, 2003) grounding is frequently used. This large number of applications indicates the importance of grounding in logic-based reasoning systems and the need to develop efficient grounders.\nA basic (naive) grounding method is by instantiating the variables in the input theory by all possible combinations of domain elements. Grounding in this way is polynomial in the size of the domain but exponential in the maximum width of a formula in the input theory, and may easily produce groundings of unwieldy size. Several techniques have been developed to efficiently produce smaller groundings. There are two main categories of such techniques. In the first, the input theory is rewritten such that the maximum width of the formulas decreases. Methods like clause splitting (Schulz, 2002) and partitioning (Ramachandran & Amir, 2005) belong to this category.\nc©2010 AI Access Foundation. All rights reserved.\nThe second type of techniques is applicable when besides the finite domain, additional data is available. This is often the case in practical model generation problems, such as the ones that are typical in ASP. In a graph problem the data could be an encoding of the input graph; in the context of planning, it could be a description of the initial and goal state, etc. Sometimes the data is explicitly available, e.g., in the form of a database, sometimes it is implicit, e.g., as a set of ground facts in the input theory. The second type of techniques aims at efficiently computing small groundings by taking the data into account.\nObserve that both types of techniques can be combined in a grounder. In this paper we mainly focus on a technique of the second category. To explain the intuition underlying our method, consider the following model generation problem.\nExample 1. Let T1 the first-order logic theory over the vocabulary {Edge, Sub}, consisting of the two sentences\n∀u∀v (Sub(u, v) ⊃ Edge(u, v)) (1) ∀x∀y∀z (Sub(x, y) ∧ Sub(x, z) ⊃ y = z), (2)\nT1 expresses that Sub is a subgraph of Edge with at most one outgoing edge in each vertex. Computing such a subgraph of a given graph G = 〈V,E〉 can be cast as a model generation problem with input theory T1 and data G. The data can be represented as a structure Iσ for the subvocabulary σ1 = {Edge} with domain V and EdgeIσ = E. A solution can be obtained by generating a model of T1 that expands Iσ with an interpretation of Sub.\nApplying the naive grounding algorithm produces |V |2 instantiations of (1) and |V |3 instantiations of (2). By taking the data into account, atoms over ‘Edge’ and ‘=’ can be substituted by their truth value in Iσ. Simplifying the resulting grounding then eliminates |E| instantiations of (1) and |V | instantiations of (2). Smart grounding algorithms interleave this substitution and simplification with the grounding process in order to avoid creating unnecessary parts of the grounding.\nObserve that substituting atoms over σ1 and then simplifying still produces a grounding of size O(|V |3). Indeed, the simplified grounding of (2) is the set of binary clauses ¬Sub(i, j) ∨ ¬Sub(i, k) such that i, j, k ∈ V and i 6= j. This set has size |V |3 − |V |.\nSome grounders apply reasoning on the ground theory to reduce it even further. In the example, the simplified grounding of (1) consists of the clauses ¬Sub(i, j) such that (i, j) 6∈ E. Since these are unit clauses, each of them is certainly true in every model of the ground theory. It follows that each binary clauses ¬Sub(i, j) ∨ ¬Sub(i, k) such that either ¬Sub(i, j) or ¬Sub(i, k) belongs to the simplified grounding of (1) is certainly true in every model of the ground theory and thus can be omitted from the simplified grounding of (2). The result is a grounding of size |E ./1=1 E|, where ./1=1 denotes the natural join matching the first columns. For a sparse graph, |E ./1=1 E| is much smaller than |V |3. However, since reasoning on the ground theory does not avoid creating all instantiations of a formula, it does not significantly speed up the grounding process.\nOne way to avoid a large grounding without relying on reasoning on the ground theory is by adding redundant information to formulas. This method is frequently used in ASP. For example,\n∀x∀y∀z(Edge(x, y) ∧ Sub(x, y) ∧ Edge(x, z) ∧ Sub(x, z) ⊃ y = z) (3)\nis equivalent to (2) given (1), but its grounding (without reasoning on the ground theory) is equal to the one obtained by the kind of reasoning on the ground theory illustrated above. This illustrates how adding redundant information may sometimes dramatically reduce the size of the grounding. Since current grounders are optimized to ground formulas like (3) without trying all instances, grounding may also speed up a lot.\nHowever, manually adding redundancy to formulas has its disadvantages: it leads to more complex and hence, less readable theories. Worse, it might introduce errors. It requires a good understanding of the used grounder, since it depends on the grounder what information is beneficial to add and where. Also, a human developer could easily miss useful information.\nThe above motivates a study of automated methods for deriving such redundant information and of principled ways of adding it to formulas. We develop an algorithm that, given a model generation problem with input theory T and input data Iσ, derives such redundant information, in the form of a pair of a symbolic upper and lower bound for each subformula of T . Each of these bounds is a formula over the vocabulary of Iσ. For instance, for Example 1, our algorithm will compute Edge(x, y) as upper bound for Sub(x, y), meaning that if Edge(x, y) is not true, then Sub(x, y) is not true either. We also show how to insert these bounds in the formulas of T . For example, inserting the upperbound Edge(x, y) for Sub(x, y) and the upperbound Edge(x, z) for Sub(x, z) transforms (2) into (3).\nThe rest of this paper is organized as follows. In the next section we recall some notions from first-order logic (FO) and we introduce the notations used throughout the paper. In Section 3 we formally define grounding and model generation with additional data. In Section 4 we introduce upper- and lowerbounds for formulas. We present an any-time algorithm to compute them in the context of FO input theories. We show how the bounds can be used to rewrite the input theory to an equivalent theory that has a smaller grounding.\nAlthough many search problems can be cast concisely and naturally as FO model generation problems, some problems require richer logics than FO. One such logic is FO(ID), an extension of FO with inductive definitions. Such definitions can be used to represent, e.g., the concept of reachability in a graph. In Section 5 we extend our rewriting method to FO(ID).\nIn Section 6 we discuss how to implement our algorithm to compute bounds. As a case study, we show for one particular grounding algorithm how it can be adapted to exploit bounds directly. We also present experimental results that indicate the impact of our method on grounding size and time. We end with related work and conclusions.\nThe current paper extends our previous work (Wittocx, Mariën, & Denecker, 2008c). Besides proofs for all main propositions and a more thorough experimental validation, also the following parts were added:\n• The theoretical result stating that our rewriting method certainly yields smaller groundings (Proposition 23);\n• The extension of the rewriting method to FO(ID) (Section 5);\n• The section about implementation issues (Section 6)."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "In this section, we introduce the conventions and notations used in this paper. We assume the reader is familiar with FO."
    }, {
      "heading" : "2.1 First-Order Logic",
      "text" : "A vocabulary Σ is a tuple 〈ΣP ,ΣF ,ΣV 〉 where ΣP , ΣF and ΣV are respectively sets of predicate symbols, function symbols and variables. We identify constants with zero-arity function symbols. Abusing notation, we will often leave out ΣV and simply write 〈ΣP ,ΣF 〉 to represent Σ. A vocabulary σ is a subvocabulary of Σ, denoted σ ⊆ Σ, if σP ⊆ ΣP , σF ⊆ ΣF and σV ⊆ ΣV .\nThroughout this paper variables are denoted by lowercase letters, predicate and function symbols by uppercase letters. Each predicate and function symbol has an associated arity n ∈ N. We often denote a predicate symbol P by P/n and a function symbol F by F/n to indicate their arities.\nTuples and sets of variables are denoted by x, y, z. A term over Σ is inductively defined by\n• A variable x ∈ Σ is a term;\n• If F/n is a function symbol of Σ and t1, . . . , tn are terms over Σ, then F (t1, . . . , tn) is a term.\nTuples of terms are denoted by t, t1, t2, . . . . A first-order logic formula over Σ is inductively defined by\n• If P/n is a predicate symbol and t1, . . . , tn are terms, then P (t1, . . . , tn) is a formula.\n• If t1 and t2 are two terms, then t1 = t2 is a formula.\n• If ϕ and ψ are formulas and x is a variable, then ¬ϕ, ϕ∧ψ, ϕ∨ψ, ∃x ϕ and ∀x ϕ are formulas.\nWe use ϕ ⊃ ψ, ϕ ≡ ψ and t1 6= t2 as a shorthands for respectively ¬ϕ ∨ ψ, (ϕ ⊃ ψ) ∧ (ψ ⊃ ϕ) and ¬(t1 = t2). An atom is a formula of the form P (t) or t1 = t2. A literal is an atom or the negation of an atom.\nAn occurrence of a formula ϕ as subformula in a formula ψ is positive, respectively negative, if it occurs in the scope of an even, respectively odd, number of negations.\nFor a formula ϕ, we often write ϕ[x] to indicate that x are its free variables. That is, if y ∈ x, then y occurs in ϕ, but not in the scope of a quantifier ∀y or ∃y in ϕ. For a variable x and a term t, the formula ϕ[x/t] denotes the result of replacing all free occurrences of x in ϕ by t. This notation is extended to tuples of variables and terms of the same length. A sentence is a formula without free variables. A theory is a finite set of sentences.\nA Σ-interpretation I consists of a domain D and\n• a domain element xI ∈ D for each variable x ∈ ΣV ;\n• a relation P I ⊆ Dn for each predicate symbol P/n ∈ ΣP ;\n• a function F I : Dn → D for each function symbol F/n ∈ ΣF .\nA Σ-structure is an interpretation of only the relation and function symbols of Σ. The restriction of a Σ-interpretation I to a vocabulary σ ⊆ Σ is denoted by I|σ. Vice versa, I is called an expansion of I|σ to Σ. For a variable x and domain element d, I[x/d] is the interpretation that assigns d to x and corresponds to I on all other symbols. This notation is extended to tuples of variables and domain elements of the same length. An interpretation I is called finite if its domain is finite.\nThe value tI of a term t in an interpretation I, and the satisfaction relation |= are defined as usual (e.g., Enderton, 2001). I is called a model of a formula ϕ if I |= ϕ. We denote by T1 |= T2 that every model of theory T1 is also a model of theory T2.\nA query is an expression of the form {x | ϕ}, where the free variables of ϕ are among x. A tuple d of domain elements is an answer to {x | ϕ} in a structure I if I[x/d] |= ϕ. The set of all answers to {x | ϕ} in I is denoted by {x | ϕ}I ."
    }, {
      "heading" : "2.2 Rewriting and Term Normal Form",
      "text" : "In this paper we will use the following well-known equivalences to rewrite formulas to logically equivalent formulas.\n1. Moving quantifiers\n∀x∀y ϕ ≡ ∀y∀x ϕ (4) ∃x∃y ϕ ≡ ∃y∃x ϕ (5) ∀x (ϕ ∧ ψ) ≡ (∀x ϕ) ∧ (∀x ψ) (6) ∃x (ϕ ∨ ψ) ≡ (∃x ϕ) ∨ (∃x ψ) (7) ∀x (ϕ ∨ ψ) ≡ ϕ ∨ (∀x ψ) if x does not occur free in ϕ (8) ∃x (ϕ ∧ ψ) ≡ ϕ ∧ (∃x ψ) if x does not occur free in ϕ (9)\n2. Moving negations\n¬(ϕ ∧ ψ) ≡ (¬ϕ) ∨ (¬ψ) (10) ¬(ϕ ∨ ψ) ≡ (¬ϕ) ∧ (¬ψ) (11) ¬(∀x ϕ) ≡ ∃x (¬ϕ) (12) ¬(∃x ϕ) ≡ ∀x (¬ϕ) (13)\n3. Flattening terms\nP (t1, . . . , ti, . . . , tn) ≡ ∃x (x = ti ∧ P (t1, . . . , ti−1, x, ti+1, . . . , tn)) (14)\nwhere x does not occur in P (t1, . . . , tn).\nTo facilitate the presentation, we will sometimes require that formulas are in term normal form (TNF). We say that a formula ϕ is in TNF, if every atomic subformula of ϕ is of the form P (x), F (x) = y or x = y, and all negations occur directly in front of atoms. Using (10)–(14), every formula can be transformed in an equivalent formula in TNF. We say that a theory is in TNF if all its sentences are."
    }, {
      "heading" : "2.3 SAT",
      "text" : "A vocabulary Σ is propositional if ΣF = ∅ and every predicate symbol in ΣP has arity zero. A propositional theory (PC theory) is a theory over a propositional vocabulary. A propositional clause is a disjunction of propositional literals. A PC theory is in conjunctive normal form (CNF) if all its sentences are clauses. The Boolean satisfiability problem (SAT) is the NP-complete problem of deciding for a PC theory whether it is satisfiable. The NP search problem corresponding to a SAT problem is the problem of computing a witness of the decision problem in the form of a model of the theory. SAT solvers typically operate by constructing such a model.\nContemporary SAT solvers exhibit impressive performance. As such, many NP problems can be solved efficiently by translating them to SAT. For instance, this is done in the areas of model generation (Claessen & Sörensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and relational data mining (Krogel et al., 2003). Most modern SAT solvers expect a CNF theory as input, instead of a general PC theory. When the input is a satisfiable theory, they return a model as a witness to their answer."
    }, {
      "heading" : "3. Model Generation and Grounding",
      "text" : "Model generation is the problem of computing a model of a logic theory T , usually in the context of a given finite domain, typically the Herbrand Universe. A model generator allows to decide the satisfiability of the theory in the context of this fixed domain. This is useful, e.g., in the context of lightweight verification (Jackson, 2006). Beyond determining satisfiability, there is a broad class of problems of which the answers are naturally given by the models of a declarative domain theory. For example, the model of a theory specifying a scheduling domain typically contains a (correct) schedule. Thus, a model generator applied to this theory will solve the scheduling problem for this domain.1 This idea of model generation as a declarative problem solving paradigm has been pioneered in the area of ASP (Marek & Truszczyński, 1999; Niemelä, 1999). In this area, answers to a problem are given by the models of an ASP theory.\nAs mentioned in the introduction, many practical model generation problems contain additional data besides the input theory and finite domain. This data can be implicit in the input theory. For\n1. For a set of problems of this kind, see, e.g., the benchmarks of the ASP-competition (http://dtai.cs.kuleuven. be/events/ASP-competition).\nexample, ASP problems can be split into two parts: a non-ground theory and a list of ground facts. The latter part essentially represents given data. In other contexts (Mitchell & Ternovska, 2005; Torlak & Jackson, 2007; Wittocx et al., 2008d), the data is given as a (partial) structure interpreting part of the vocabulary of the input theory. In this paper we assume without loss of generality that the data is represented by a structure. In practice, it is often the case that some preprocessing, e.g., materializing a view on a database, needs to be done before the data is in this format (see also Section 5.3.2)."
    }, {
      "heading" : "3.1 The Model Expansion Search Problem",
      "text" : "Model generation with an input theory and input structure is called model expansion. Model expansion for a logic L, denoted MX(L), is defined as follows.\nDefinition 1. Let T be an L-theory over a vocabulary Σ, σ a subvocabulary of Σ and Iσ a finite σ-structure. The model expansion search problem with input 〈T, Iσ〉 is the problem of computing a Σ-structure M such that M |= T and M |σ = Iσ.\nThe vocabulary σ is called the input vocabulary of the problem, the vocabulary Σ\\σ the expansion vocabulary. Iσ is called the input structure. We denote by M |=Iσ T that M is a solution to the model expansion search problem with input 〈T, Iσ〉. Similarly, for a formula ϕ over Σ we denote by M |=Iσ ϕ that M expands Iσ to Σ and satisfies ϕ.\nObserve that if σ = Σ, model expansion reduces to model checking, while if σ = 〈∅, ∅〉, it reduces to model generation for T with a given finite size. Also, if T is a theory over a vocabulary Σ containing no function symbols of arity greater than zero, Herbrand model generation for T can be simulated by model expansion. Indeed, let σ = 〈∅,ΣF 〉, and Iσ the structure with the Herbrand universe of T such that CIσ = C for every constant C ∈ ΣF .\nWe illustrate model expansion by two examples. In the examples in this paper, we often use many-sorted FO, since this leads to more concise and readable sentences. In many-sorted FO, the domain of an interpretation is partitioned in sorts (or types), each variable has an associated sort, each n-ary predicate symbol has an n-tuple of associated sorts and each n-ary function symbol an associated (n + 1)-tuple of sorts. If I is an interpretation and variable x has associated sort s, then xI ∈ sI , where sI denotes the set of domain elements of sort s. Similarly, if P/n has associated sorts (s1, . . . , sn), then P I ⊆ sI1×· · ·×sIn, if F/n has associated sorts (s1, . . . , sn+1), then F I : sI1 × · · · × sIn → sIn+1. We often denote P by P (s1, . . . , sn) and F by F (s1, . . . , sn) : sn+1 to indicate their associated sorts.\nExample 2 (Graph Colouring). The graph colouring problem is the problem of colouring a given graph with a given set of colours such that adjacent vertices have different colours. To express this problem in MX(FO), let V tx and Col be sorts and let σ = 〈{Edge(V tx, V tx)}, ∅〉. The sort Col denotes the given set of colours, the given graph is represented by V tx and Edge. Let Σ be the vocabulary 〈σP , {Colour(V tx) : Col}〉 and T the theory that consists of the sentence\n∀v1∀v2 (Edge(v1, v2) ⊃ Colour(v1) 6= Colour(v2)).\nThen model expansion with input theory T and input vocabulary σ expresses the graph colouring problem. Indeed, for any M |=Iσ T , ColourM is a proper colouring of the graph represented by Iσ.\nExample 3 (SAT). To represent the SAT problem in MX(FO), let σ be a vocabulary containing the two sorts Atom and Clause, representing the atoms and the clause of the input CNF theory, and the two predicates PosIn(Atom,Clause) and NegIn(Atom,Clause), to represent the positive, respectively negative, occurrences of atoms in clauses. The theory given by\n∀c ∃a ((PosIn(a, c) ∧ True(a)) ∨ (NegIn(a, c) ∧ ¬True(a)))\nover Σ = 〈σP ∪ {True(Atom)}, ∅〉 expresses the SAT problem: for any M |=Iσ T , the propositional structure represented by TrueM is a model of the CNF theory represented by Iσ. Indeed, the theory forces that every clause contains at least one true literal.\nAs shown by Mitchell and Ternovska (2005), it follows from Fagin’s (1974) theorem that model expansion for FO captures NP, in the following sense:\n• For any fixed T and σ the problem of deciding whether there exists a model of T expanding an input structure Iσ is in NP.\n• Vice versa, for any NP decision problem X on the class of finite σ-structures there is a vocabulary Σ ⊇ σ and a first-order Σ-theory T such that model expansion with input theory T expresses X, i.e., Iσ belongs to X iff there exists a Σ-structure M such that M |=Iσ T .\nThis result proves that any NP problem X can be expressed by an MX(FO) problem, and hence shows the broad applicability of MX(FO) solvers to solve NP problems.\nAs illustrated by the examples above, it is the intention that the theory T is an intuitive representation of a problem X. Not all NP problems can be represented in a natural manner in MX(FO). For instance, the problem of deciding whether a graph is connected can be expressed in MX(FO), but this requires a non-trivial encoding of a fixpoint operator in FO. Model expansion for richer logics than FO is better suited for such problems. In Section 5 we consider MX for FO(ID), an extension of FO with inductive definitions."
    }, {
      "heading" : "3.2 Reducing MX(FO) to SAT",
      "text" : "For the rest of this paper, let T be a theory over a vocabulary Σ, σ a subvocabulary of Σ and Iσ a finite σ-structure with domain D.\nSince for every FO theory T , deciding whether T has a model expanding Iσ is in NP, this problem can be reduced to a SAT problem Tprop in polynomial time. However, if we want to find models of T expanding Iσ by using a SAT solver, we need a method to translate models of Tprop into models of T . Moreover, if we are interested in finding all models of T expanding Iσ, a oneto-one correspondence between these models and the models of Tprop is needed. In this paper we focus on reductions that preserve all models, which is the setting in the ASP paradigm (Marek & Truszczyński, 1999; Niemelä, 1999).\nLet τ be the vocabulary of Tprop. To have a one-to-one correspondence between the models of T expanding Iσ and the models of Tprop, it should be possible to represent Σ-structures expanding Iσ by τ -structures. The most natural way to accomplish this is by choosing τ such that it contains a symbol Pd for every P/n ∈ ΣP and d ∈ Dn, and a symbol Fd,d′ for every F/n ∈ ΣF and (d, d′) ∈ Dn+1. A τ -structure making Pd, respectively Fd,d′ true then corresponds to a Σ structure M such that d ∈ PM , respectively FM (d) = d′. In this manner, every Σ-structure expanding Iσ has a corresponding τ -structure. Vice versa, every τ -structure A satisfying the requirement that for every function symbol F/n and d ∈ Dn, there is exactly one d′ ∈ D such that Fd,d′ is true in A, corresponds to a Σ-structure with the same domains as Iσ. That is, there is a one-to-one correspondence between the τ -structures satisfying for every function symbol F/n and d ∈ Dn the formula ( ∨\nd′∈D\nFd,d′\n) ∧  ∧ d′1∈D  ∧ d′2∈D\\d′1 ¬Fd,d′1 ∨ ¬Fd,d′2  (15) and the Σ-structures with domain D.\nDenote by Σdom(Iσ) the vocabulary Σ extended with a new constant symbol d for every d ∈ D. We call these new constants domain constants. Abusing notation, we will denote both domain elements and their corresponding domain constants by d. For a formula ϕ[x] and a tuple d of\ndomain constants, we call ϕ[x/d] an instance of ϕ. For a Σ-interpretation M expanding Iσ and a formula ϕ containing domain constants, we denote by M |= ϕ that the expansion of M to Σdom(Iσ) defined by interpreting every domain constant by its corresponding domain element, satisfies ϕ.\nDefinition 2. Two formulas ϕ1 and ϕ2 over Σdom(Iσ) are Iσ-equivalent if M |=Iσ ϕ1 iff M |=Iσ ϕ2, for every Σ-interpretation M .\nThe following are some straightforward results about Iσ-equivalence.\nLemma 3. 1. Two logically equivalent formulas are Iσ-equivalent. 2. ∧ d∈D ϕ[x/d] is Iσ-equivalent to ∀x ϕ[x].\n3. ∨ d∈D ϕ[x/d] is Iσ-equivalent to ∃x ϕ[x].\n4. If ϕ′ and ψ′ are Iσ-equivalent to respectively ϕ and ψ, then ¬ϕ′, ϕ′ ∧ ψ′, ϕ′ ∨ ψ′, ∃x ϕ′ and ∀x ϕ′ are Iσ-equivalent to respectively ¬ϕ, ϕ ∧ ψ, ϕ ∨ ψ, ∃x ϕ and ∀x ϕ.\n5. If ψ is a subformula of ϕ and is Iσ-equivalent to ψ′, then the result of replacing ψ by ψ′ in ϕ is Iσ-equivalent to ϕ.\nA formula is in ground normal form (GNF) if it contains no quantifiers and all its atomic subformulas are of the form P (d1, . . . , dn), F (d1, . . . , dn) = d or d1 = d2, where d1, . . . , dn, d are domain constants. A theory is in GNF if all its sentences are in GNF. A GNF theory is essentially propositional: by replacing in a GNF theory T every atom P (d) by Pd, F (d) = d\n′ by Fd,d′ , di = dj by > or ⊥ if, respectively, i = j or i 6= j, and adding the formula (15) for every function symbol F/n and d ∈ Dn, we obtain a propositional theory Tprop such that the models of T and Tprop correspond. Also note the similarity between GNF and TNF theories.\nDefinition 4. A grounding for T with respect to Iσ is a GNF theory Tg over Σdom(Iσ) such that T and Tg are Iσ-equivalent. Tg is called reduced if it does not contain symbols of σ."
    }, {
      "heading" : "3.2.1 Grounding Algorithms",
      "text" : "For the rest of this section, we assume that T is a theory in TNF. As explained in Section 2.2, we can make this assumption without loss of generality. Below we introduce, as a reference, the grounding for T with respect to Iσ obtained by the naive grounding algorithm mentioned in the introduction. We call this grounding the full grounding and define it formally by induction.\nDefinition 5. The full grounding Grfull(ϕ, Iσ) of a TNF sentence ϕ with respect to Iσ is defined by\nGrfull(ϕ) =  ϕ if ϕ is a literal Grfull(ψ1) ∧Grfull(ψ2) if ϕ is equal to ψ1 ∧ ψ2 Grfull(ψ1) ∨Grfull(ψ2) if ϕ is equal to ψ1 ∨ ψ2∧ d∈D Grfull(ψ[x/d]) if ϕ is equal to ∀x ψ[x]∨ d∈D Grfull(ψ[x/d]) if ϕ is equal to ∃x ψ[x]\n(16)\nThe full grounding for T with respect to Iσ is the theory consisting of the full groundings of all sentences in T with respect to Iσ.\nWe denote the full grounding by Grfull(T, Iσ), or by Grfull(T ) if Iσ is clear from the context. It follows directly from Lemma 3 that Grfull(T, Iσ) is indeed a grounding for T with respect to Iσ. The size of the full grounding is exponential in the maximal nesting depth of quantifiers in sentences of T , and polynomial in the domain size of Iσ.\nAn inductive definition like (16) can be evaluated in a top-down or bottom-up way. Both approaches are applied in current grounders. On the one hand, there are grounders that go top-down through the syntax trees of the sentences in T . When a subformula ϕ of the form ∀x ψ[x], respectively ∃x ψ[x] is reached, the grounding of ψ[x/d] is constructed for every domain constant d, and then ϕ is replaced by the conjunction, respectively disjunction, of all these groundings. The grounder of the dlv system (Perri et al., 2007) and the grounders gringo (Gebser et al., 2007) and GidL (Wittocx, Mariën, & Denecker, 2008b) take this approach.\nOther grounders go bottom-up through the syntax trees. For each subformula ϕ[x] a table is computed consisting of tuples d and corresponding groundings of ϕ[x/d]. These tables are computed first for atomic formulas and subsequently for compound formulas. For example, let ϕ[x, y, z] be the formula ψ[x, y] ∧ χ[y, z] and assume the tables for ψ and χ have been computed. Then the table for ϕ is computed by taking the natural join of the tables for ψ and χ on the value for y, and constructing the grounding for ϕ[x/dx, y/dy, z/dz] as the (possibly simplified) conjunction of the groundings for ψ[x/dx, y/dy] and χ[y/dy, z/dz]. Examples of grounders with a bottom-up approach are lparse (Syrjänen, 2000; Syrjänen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al., 2006).\nTo obtain a reduced grounding for T with respect to Iσ one could first construct the full grounding and then replace every subformula ϕ over σdom(Iσ) in it by > if Iσ |= ϕ and by ⊥ otherwise. The result can further be simplified by recursively replacing ⊥∧ ψ by ⊥, >∧ ψ by ψ, etc. The resulting grounding is the one computed by most current grounding algorithms and is often a lot smaller than the full grounding. We denote it by Grred(T, Iσ), or by Grred(T ) if Iσ is clear from the context.\nSmart grounding algorithms do not use the approach outlined above, but try to avoid creating the full grounding by substituting ground formulas over the input vocabulary σ as soon as possible. For example, a grounder with a top-down approach constructs the grounding of ∀x ψ[x], by grounding all instances ψ[x/d] one by one and then making the conjunction. During this process, all instances ψ[x/d] that are detected to be certainly true are omitted. As soon as an instance ψ[x/d] is detected to be certainly false, ⊥ is returned as grounding for ∀x ψ[x].\nA grounder using the bottom-up approach can reduce the size of the tables it computes by not storing tuples that have some default value, e.g., >, as corresponding grounding. In particular, if ϕ[x] is a formula over σ, it only stores the tuples d such that Iσ 6|= ϕ[x/d]. By reducing the size of the tables in this way, the reduced grounding can be obtained much more efficiently."
    }, {
      "heading" : "4. Grounding with Bounds",
      "text" : "In this section we present our method for reducing grounding size. As mentioned in the introduction, it is based on computing bounds for subformulas of the input theory T . Each bound for a subformula ϕ[x] is a formula over the input vocabulary σ. It describes a set of tuples d for which ϕ[x/d] is certainly true (false) in every model of T expanding any Iσ. The larger the set described by a bound, the more precise the bound is. Observe that the fact that bounds are formulas over σ means that they can be evaluated using the given structure Iσ.\nIn Section 4.1, we formally define bounds. Then we indicate how bounds can be inserted in T to obtain a new theory T ′. The reduced grounding of T ′ is often a lot smaller than the reduced grounding of T . The more precise the inserted bounds are, the smaller the grounding of T ′ becomes. However, we will see that T ′ is in general weaker than T and that additional axioms have to be added to T ′ to obtain equivalence with T . These additional axioms need to be grounded as well so that, if we are not careful, the total size of the grounded theory does not decrease at all. In Section 4.3, we search for sufficient conditions on the bounds to guarantee a smaller grounding.\nIn Section 4.4, we show how to derive bounds. Our method works in two stages. First, bounds for all subformulas of T are computed using an any-time algorithm. The longer the algorithm runs, the more precise bounds are derived. Often, the bounds derived at this stage do not lead to smaller groundings, for the reason explained in the previous paragraph. In the second stage, bounds that\nsatisfy the conditions to guarantee smaller groundings are derived from the ones computed in the first stage."
    }, {
      "heading" : "4.1 Bounds",
      "text" : "We distinguish between two kinds of bounds.\nDefinition 6. A certainly true bound (ct-bound) over σ with respect to T for a formula ϕ[x] is a formula ϕct[y] over σ such that y ⊆ x and T |= ∀x (ϕct[y] ⊃ ϕ[x]). Vice versa, a certainly false bound (cf-bound) over σ with respect to T for ϕ[x] is a formula ϕcf [z] over σ such that z ⊆ x and T |= ∀x (ϕcf [z] ⊃ ¬ϕ[x]).\nWe do not mention σ and T if they are clear from the context. Intuitively, a ct-bound ϕct for ϕ[x] provides for every structure Iσ a lower bound for the set of tuples for which ϕ is true in every model of T expanding Iσ. Indeed, for every M |=Iσ T we have that {x | ϕct}Iσ ⊆ {x | ϕ}M . Vice versa, a cf-bound ϕcf provides a lower bound on the set of tuples for which ϕ is false: {x | ϕcf}Iσ ⊆ {x | ¬ϕ}M for every M |=Iσ T . Observe that the negation of a ct-bound, respectively cf-bound, gives an upper bound on the set of tuples for which ϕ is false, respectively true, in at least one model of T expanding Iσ.\nExample 4 (Example 1 ctd.). Let ϕ1 be the subformula Sub(x, y) ∧ Sub(x, z) of T1. Then ¬Edge(x, y) ∨ ¬Edge(x, z) is a cf-bound over σ1 with respect to T1 for ϕ1. Indeed, one can derive from (1) that T1 entails\n∀x∀y∀z ((¬Edge(x, y) ∨ ¬Edge(x, z)) ⊃ ¬ϕ1) .\nObserve that > is a ct-bound for every sentence of T . Indeed, for every sentence ϕ of T , T |= ϕ and therefore T |= > ⊃ ϕ. Also, ⊥ is a ct-bound as well as a cf-bound for every formula. We call ⊥ the trivial bound. Intuitively, the trivial bound contains no information at all: {x | ⊥}Iσ = ∅ for every Iσ and x. According to the following definition, it is the least precise bound.\nDefinition 7. Let ψ[y] and χ[z] be two (ct- or cf-) bounds for ϕ[x]. We say that ψ[y] is more precise than χ[z] if ∀x (χ[z] ⊃ ψ[y]) is valid.\nIf ψ is a more precise bound for ϕ[x] than χ, ψ provides a larger lower bound because {x | χ}Iσ ⊆ {x | ψ}Iσ for every Iσ.\nDefinition 8. A c-map C for T over σ is a mapping from all subformulas ϕ of T to tuples (Cct(ϕ), Ccf(ϕ)), where Cct(ϕ) and Ccf(ϕ) are respectively a ct- and cf-bound for ϕ over σ with respect to T .\nThe notion of precision pointwise extends to c-maps. That is, if C1 and C2 are two c-maps for T , then C1 is more precise than C2 iff for every subformula ϕ of T , Cct1 (ϕ) is more precise than Cct2 (ϕ) and Ccf1 (ϕ) is more precise than Ccf2 (ϕ).\nLet M be a model of T and C a c-map for T over σ. From the definition of ct- and cf-bounds it follows immediately that for every subformula ϕ[x] of T , both M |= ∀x (Cct(ϕ) ⊃ ϕ) and M |= ∀x (Ccf(ϕ) ⊃ ¬ϕ) hold. We say that a structure satisfies C if it has precisely this property.\nDefinition 9. Let C be a c-map for T over σ. Then the theory C is defined by\nC ={∀x (Cct(ϕ) ⊃ ϕ) | ϕ[x] is a subformula of T} ∪ {∀x (Ccf(ϕ) ⊃ ¬ϕ) | ϕ[x] is a subformula of T}.\nA structure I satisfies C if I |= C.\nClearly, if C is a c-map for T over σ and M |=Iσ T , then M |= C. We call two formulas ϕ[x] and ψ[x] C-equivalent if {x | ϕ}I = {x | ψ}I for each structure I that satisfies C. Equivalently, ϕ and ψ are C-equivalent if C |= ∀x (ϕ ≡ ψ).\nA c-map is inconsistent if some formula ϕ is both certainly true and false for some tuple, according to that c-map:\nDefinition 10. A c-map C for T over σ is inconsistent if ∃x (Cct(ϕ) ∧ Ccf(ϕ)) is valid for some subformula ϕ[x] of T . A c-map C is Iσ-inconsistent if Iσ |= ∃x (Cct(ϕ)∧Ccf(ϕ)) for some subformula of T .\nProposition 11. If there exists an Iσ-inconsistent c-map for T over σ, then M 6|=Iσ T for every M . If there exists an inconsistent c-map for T over σ, then M 6|=Iσ T for every M and Iσ.\nProof. Let C be an Iσ-inconsistent c-map for T over σ and ϕ[x] a subformula of T such that Iσ |= ∃x (Cct(ϕ) ∧ Ccf(ϕ)). Then there exists a tuple of domain elements d such that Iσ[x/d] |= Cct(ϕ) and Iσ[x/d] |= Ccf(ϕ). Assume towards a contradiction that M |=Iσ T . Then M |= C, and hence M [x/d] |= Cct(ϕ) ⊃ ϕ and M [x/d] |= Ccf(ϕ) ⊃ ¬ϕ. Since M |σ = Iσ, it follows that M [x/d] |= ϕ and M [x/d] |= ¬ϕ. This is a contradiction.\nTo prove the second statement, let C be an inconsistent c-map for T over σ. Then C is a also an Iσ-inconsistent c-map for every σ-structure Iσ. As such, for any Iσ there is no model of T expanding Iσ."
    }, {
      "heading" : "4.2 C-Transformation",
      "text" : "For the rest of this section, fix a c-map C for T over σ. We now show how to insert the bounds of C into the sentences of T . This insertion is based on the following lemma.\nLemma 12. Let ϕ[x] be a subformula of T . Then ϕ is C-equivalent to ϕ∨Cct(ϕ) and to ϕ∧¬Ccf(ϕ).\nProof. We have to prove that C |= ∀x (ϕ ≡ (ϕ ∨ Cct(ϕ))) and C |= ∀x (ϕ ≡ (ϕ ∧ ¬Ccf(ϕ))). The former immediately follows from the fact that C |= ∀x (Cct(ϕ) ⊃ ϕ), the latter from the fact that C |= ∀x (Ccf(ϕ) ⊃ ¬ϕ) .\nAs a corollary of lemma 12 we have the following lemma.\nLemma 13. Let ψ be a sentence of T and ϕ a subformula of ψ. If ψ′ is the result of replacing the subformula ϕ in ψ by ϕ ∨ Cct(ϕ), by ϕ ∧ ¬Ccf(ϕ) or by (ϕ ∧ ¬Ccf(ϕ)) ∨ Cct(ϕ), then M |= ψ iff M |= ψ′ for every M that satisfies C.\nObserve that if Cct(ϕ) = Ccf(ϕ) = ⊥, then both ϕ∨Cct(ϕ) and ϕ∧¬Ccf(ϕ) are logically equivalent to ϕ. Hence, in this case the sentence ψ′ in Lemma 13 is essentially the sentence ψ. Intuitively, adding trivial bounds to a sentence ψ does not change the sentence at all.\nThe bounds assigned by C can be “inserted” in T by applying the transformation of Lemma 13 to all subformulas of T . The result is called a c-transformation of T , and is formally defined as follows.\nDefinition 14 (c-transformation). A c-transformation of a subformula ϕ of T with respect to C, denoted C〈ϕ〉, is the formula (ϕ′ ∧ ¬Ccf(ϕ)) ∨ Cct(ϕ) where ϕ′ is defined by\nϕ′ :=  ϕ if ϕ is an atom ¬C〈ψ〉 if ϕ is equal to ¬ψ C〈ψ〉 ∧ C〈χ〉 if ϕ is equal to ψ ∧ χ C〈ψ〉 ∨ C〈χ〉 if ϕ is equal to ψ ∨ χ ∃x C〈ψ〉 if ϕ is equal to ∃x ψ ∀x C〈ψ〉 if ϕ is equal to ∀x ψ\nA c-transformation C〈T 〉 of T with respect to C consists of a c-transformation with respect to C of every sentence of T .\nFrom Lemma 13, we derive the following.\nLemma 15. T and C〈T 〉 are C-equivalent.\nIn general T and C〈T 〉 are not logically equivalent. C〈T 〉 may have models that do not satisfy C, and therefore cannot be models of T . For example, let C be the c-map that assigns (>,⊥) to every sentence and (⊥,⊥) to every other subformula of T . Then all sentences in C〈T 〉 are of the form ϕ∨> and hence C〈T 〉 simplifies to >, which is in general not equivalent to T . To obtain from C〈T 〉 a theory that is equivalent to T , we must add C.\nTheorem 16. If C is a c-map for T over σ and C the theory defined in Definition 9, then C〈T 〉 ∪ C is equivalent to T .\nProof. Let M be a model of T . Then M |= C, and because of Lemma 13, M |= C〈T 〉 ∪ C. On the other hand, if M |= C〈T 〉 ∪ C, then by Lemma 13, M |= T .\nCorollary 17. If C is a c-map for T over σ, then T and C〈T 〉 ∪ C are Iσ-equivalent for any σstructure Iσ."
    }, {
      "heading" : "4.3 Atom-Based and Atom-Equal C-Maps",
      "text" : "Corollary 17 implies that we can compute a grounding for T with respect to Iσ by first computing a c-map C for T over σ and then grounding C〈T 〉 ∪ C. This approach is beneficial if the reduced grounding of C〈T 〉 ∪ C is smaller than the reduced grounding of T , and can be constructed at least as fast. In general these conditions are not satisfied. The more precise c-map C is, the smaller the reduced grounding of C〈T 〉 becomes, but the larger the reduced grounding of C is:\nProposition 18. If C1 is more precise than C2, then Grred(C1〈T 〉) is smaller than Grred(C2〈T 〉). Moreover, every subformula that occurs in Grred(C1〈T 〉) also occurs in Grred(C2〈T 〉).\nProof. (Sketch) Let ϕ[x] be a subformula of T and d a tuple of domain elements. It suffices to show that if C2〈ϕ〉[x/d] is replaced by >, respectively ⊥, when grounding, then this is also the case for C1〈ϕ〉[x/d]. This can be proven by induction. For the base case, assume ϕ is an atom. Then C2〈ϕ〉[x/d] is the formula ((ϕ ∧ ¬Ccf2 (ϕ)) ∨ Cct2 (ϕ))[x/d]. If this formula is replaced by > or ⊥ when grounding, there are three possibilities: ϕ is a formula over σ, Iσ[x/d] |= Cct2 (ϕ) or Iσ[x/d] |= Ccf2 (ϕ). Since C1 is more precise than C2, Iσ[x/d] |= Cct2 (ϕ) implies Iσ[x/d] |= Cct1 (ϕ) and Iσ[x/d] |= Ccf2 (ϕ) implies Iσ[x/d] |= Ccf1 (ϕ). We conclude that if C2〈ϕ〉[x/d] is replaced by > or ⊥ when grounding, then this is also the case for C1〈ϕ〉[x/d]. The inductive case is similar.\nProposition 19. If C1 is more precise than C2, then Grred(C1) is larger than Grred(C2).\nProof. (Sketch) Every sentence in C1 is of the form ∀x (Cct1 (ϕ) ⊃ ϕ) or ∀x (Ccf1 (ϕ) ⊃ ¬ϕ). The number of instances of Cct1 (ϕ) ⊃ ϕ in the reduced grounding of C1 is equal to the number of d such that Iσ[x/d] |= Cct1 (ϕ). Similarly for Ccf1 (ϕ) ⊃ ¬ϕ. Since C2 is less precise than C1, the number of instances in Grred(C2) of the corresponding sentences ∀x (Cct2 (ϕ) ⊃ ϕ) and ∀x (Ccf2 (ϕ) ⊃ ¬ϕ) is smaller.\nA c-map that is useful to reduce grounding size should therefore not be too precise, in order to avoid a large theory Grred(C), but still be precise enough to decrease the size of Grred(C〈T 〉). In this section, we present sufficient conditions to ensure these properties. We first define a class of c-maps that “avoid” a blow-up of Grred(C) by ensuring C can be replaced by an equivalent, smaller and easy-to-find theory CA. As such, Grred(C) can be replaced by the smaller theory Grred(CA). In the class we present, CA is a subset of C, namely the set of sentences in C that stem from the atomic subformulas of T :\nDefinition 20. Define the theory CA by\nCA ={∀x (Cct(ϕ) ⊃ ϕ) | ϕ[x] is an atomic subformula of T} ∪ {∀x (Ccf(ϕ) ⊃ ¬ϕ) | ϕ[x] is an atomic subformula of T}.\nWe call C atom-based if CA |= C.\nExample 5 (Example 1 ctd.). Let C2 be the c-map that assigns (⊥,¬(Edge(x, y) ∧ Edge(x, z))) to Sub(x, y) ∧ Sub(x, z) and (⊥,⊥) to every other subformula. C2 is not atom-based, since (C2)A is equivalent to >, while C2 contains the sentence\n∀x∀y∀z (¬(Edge(x, y) ∧ Edge(x, z)) ⊃ ¬(Sub(x, y) ∧ Sub(x, z))). (17)\nLet C3 be the c-map that assigns (⊥,¬Edge(x, y)) to Sub(x, y), (⊥,¬Edge(x, z)) to Sub(x, z) and corresponds to C2 on all other subformulas of T1. C3 is atom-based. Indeed, (C3)A consists of the (equivalent) sentences\n∀x∀y (¬Edge(x, y) ⊃ ¬Sub(x, y)) (18) ∀x∀z (¬Edge(x, z) ⊃ ¬Sub(x, z)) (19)\nand C3 consists of the sentences (17), (18) and (19). Both (18) and (19) imply (17), and therefore, (C3)A |= C3.\nClearly, a c-map assigning (⊥,⊥) to every non-atomic subformula of T is an example of an atombased c-map. As such, any c-map can be transformed into an atom-based one by replacing every bound assigned to a non-atomic subformula by ⊥. In the next section, we show how to compute more interesting atom-based c-maps.\nObserve that Grred(CA) contains only unit clauses. Combining the definition of atom-based c-map and Theorem 16 immediately gives the following result.\nProposition 21. Let C be an atom-based c-map for T over σ. Then T and C〈T 〉∪CA are equivalent, and hence Iσ-equivalent for every σ-structure Iσ.\nTo obtain small groundings using bounds, it is important that the information in the bounds is exploited wherever possible. In particular, if a ct- or cf-bound ψ is assigned to an atom P (x), then a similar bound should be assigned to every other atom of the form P (y). We call a c-map atom-equal if it has exactly this property for all atomic subformulas of T . That is, C is atom-equal if it assigns essentially the same bounds to atomic subformulas over the same predicate or function symbol:\nDefinition 22. A c-map C for a TNF theory T over σ is atom-equal if for every predicate symbol P/n there exist formulas ϕctP [x1, . . . , xn] and ϕ cf P [x1, . . . , xn] such that for every atom P (y1, . . . , yn) that occurs in T , Cct(P (y1, . . . , yn)) is equal to ϕctP [x1/y1, . . . , xn/yn] and Ccf(P (y1, . . . , yn)) is equal to ϕcfP [x1/y1, . . . , xn/yn], and similarly for function symbols.\nNote that if no predicate or function symbol occurs more than once in a theory T , then every c-map for T is atom-equal.\nExample 6 (Example 1 ctd.). Let T2 be the theory obtained by adding the sentence ∃w Sub(w,w) to T1. The only predicate that occurs more than once in T2 is the predicate Sub. Let C4 be a c-map for T2 that assigns the following bounds to the atomic subformulas of T2 over Sub: (⊥,¬Edge(u, v)) to Sub(u, v), (⊥,¬Edge(x, y)) to Sub(x, y), (⊥,¬Edge(x, z)) to Sub(x, z) and (⊥,¬Edge(w,w)) to Sub(w,w). Then C4 is atom-equal. Indeed, if we take ϕctSub = ⊥ and ϕcfSub = ¬Edge(x1, x2), then the conditions of Definition 22 are satisfied for predicate Sub.\nFor an atom-equal c-map C, CA in general contains many equivalent sentences. For example, for the c-map C4 as in Example 6, (C4)A contains amongst others, the equivalent sentences (18) and (19). It also contains ∀w ¬Edge(w,w) ⊃ ¬Sub(w,w), which is implied by (18). As a result, if C is an atom-equal c-map, grounding CA in a naive way yields a grounding that contains several formulas more than once. In the following proposition, we assume this redundancy is removed. In other words, we assume a grounding algorithm for CA that never adds the same GNF formula more than once to the grounding. This can be accomplished by grounding instead of CA the sentences ∀x (ϕctbP ⊃ P (x)) and ∀x (ϕcfbP ⊃ ¬P (x)) for every predicate symbol P , where ϕctbP and ϕcfbP are as in Definition 22, and similarly for function symbols.\nProposition 23. Let C be an atom-based, atom-equal c-map for a TNF theory T . If T has a model expanding Iσ, then Grred(C〈T 〉 ∪ CA) is at most as large as Grred(T ).\nIn the proof, we denote the size of a theory Tg by |Tg|.\nProof. The outline of this proof is as follows. First, we show that every subformula that occurs in Grred(C〈T 〉), occurs in Grred(T ). Then, we prove that no atom occurring in Grred(CA) occurs in Grred(C〈T 〉). Next, we show that every atom occurring in Grred(CA) occurs at least once in Grred(T ). Since we assumed Grred(CA) does not contain any formula more than once, it follows that |Grred(C〈T 〉)| ≤ |Grred(T )| − |Grred(CA)|, which concludes the proof.\nWe can directly apply Proposition 18 to show that every subformula of Grred(C〈T 〉) occurs in Grred(T ): if C′ is the trivial c-map, then Grred(T ) is equal to Grred(C′〈T 〉), and clearly C is more precise than C′.\nWe now show that none of the atoms occurring in Grred(CA) occur in Grred(C〈T 〉). Let P (d) be an atom occurring in Grred(C〈T 〉). Then there is an atomic subformula P (x) of T such that d 6∈ {x | Cct(P (x))}Iσ and d 6∈ {x | Ccf(P (x))}Iσ . Because C is atom-equal, it follows that for any subformula P (y) occurring in T , neither d ∈ {y | Cct(P (y))}Iσ nor d ∈ {y | Ccf(P (y))}Iσ . Therefore P (d) does not occur in Grred(CA).\nIt remains to show that every atom that occurs in Grred(CA) also occurs in Grred(T ). Let M be a model of Grred(T ). Such a model exists because we assumed that T has a model expanding Iσ. Let P (d) be an atom that does not occur in Grred(T ). If P is a predicate of the input vocabulary, then P (d) does not occur in Grred(CA) either. If on the other hand, P is in the expansion vocabulary, then the structure M ′ obtained from M by swapping the truth value of P (d) is also a model of Grred(T ). Since Grred(C〈T 〉 ∪ CA) is Iσ-equivalent to Grred(T ) and P 6∈ σ, it follows that M |= Grred(CA) and M ′ |= Grred(CA). Because Grred(CA) only contains unit clauses, we conclude that P (d) does not occur in Grred(CA).\nWe now have the following algorithm to create a small grounding for T with respect to Iσ: first compute an atom-based, atom-equal c-map C for T over σ (We will present an algorithm for this in Section 4.4). If C is Iσ-inconsistent, output ⊥ and stop. Else, output Grred(C〈T 〉 ∪ CA).\nIt follows from Propositions 11 and 21 that the result of this algorithm is indeed a grounding for T with respect to Iσ. Observe that the first step of this algorithm is independent of Iσ. If one has to solve several model expansion problems with a fixed input theory T and input vocabulary σ, but varying Iσ, it suffices to compute C only once.\nTo perform the last step of the algorithm, one could apply any off-the-shelf grounder on input C〈T 〉 ∪ CA."
    }, {
      "heading" : "4.4 Computing Bounds",
      "text" : "We now present an algorithm to compute a (non-trivial) c-map C. It is based on our work on approximate reasoning for FO (Wittocx, Mariën, & Denecker, 2008a). In general the resulting cmap is neither atom-based nor atom-equal, but an atom-based, atom-equal c-map can be derived from it."
    }, {
      "heading" : "4.4.1 Refining C-Maps",
      "text" : "Constructing a non-trivial c-map can be done by starting from the least precise c-map, i.e., the one that assigns (⊥,⊥) to every subformula of T , and then gradually refining it. Each refinement step consists of three operations:\n1. Choose a subformula ϕ of T .\n2. Compute from the current c-map C a new ct-bound ϕrct or cf-bound ϕrcf for ϕ. Below, we elaborate on this step: we present six different ways to obtain new ct- or cf-bounds, called refinement bounds, from T and C. If the sentences of T are represented by their “syntax trees”, each node corresponds to a subformula of T . Bottom-up refinement bounds are bounds for a node computed by considering the bounds assigned by C to its children. Vice versa, top-down refinement bounds are computed by looking at the parents and siblings of a node. Axiom refinement bounds are bounds for the roots, i.e., for the sentences of T , while input, copy and functional refinement bounds are in practice mainly bounds for atomic subformulas of T .\n3. Substitute Cct(ϕ) by Cct(ϕ) ∨ ϕrct, respectively Ccf(ϕ) by Ccf(ϕ) ∨ ϕrcf .\nAccording to the following lemma, a refinement step yields a new bound for ϕ that is more precise than the one assigned by C.\nLemma 24. If ψ and χ are two ct-bounds for ϕ with respect to T , then ψ ∨χ is also a ct-bound for ϕ. Moreover, ψ ∨ χ is more precise than ψ and more precise than χ. The same holds for cf-bounds.\nProof. Let ψ and χ be two ct-bounds for ϕ[x]. By definition, T |= ∀x (ψ ⊃ ϕ) and T |= ∀x (χ ⊃ ϕ). Therefore T |= ∀x ((ψ ∨ χ) ⊃ ϕ), which proves that ψ ∨ χ is a ct-bound for ϕ. Since |= ψ ⊃ (ψ ∨ χ) and |= χ ⊃ (ψ ∨ χ), ψ ∨ χ is a more precise bound than ψ and χ. The proof for cf-bounds is similar.\nWe conclude that repeatedly applying refinement steps leads to a more and more precise c-map. The resulting algorithm is an any-time algorithm. In Section 6 we will discuss a stop criterion for the algorithm. We will also give examples where it can reach a fixpoint, and examples where it cannot.\nWe now present the different ways to obtain refinement bounds.\nInput Refinement Let ϕ[x] be a formula over the input vocabulary σ. Since T |= ∀x (ϕ[x] ⊃ ϕ[x]) and T |= ∀x (¬ϕ[x] ⊃ ¬ϕ[x]), it is clear that ϕ[x] is a ct-bound and ¬ϕ[x] a cf-bound for ϕ[x]. We call these input refinement ct- and cf-bounds.\nAxiom Refinement If ϕ is a sentence of T , then > is an axiom refinement ct-bound for ϕ. This refinement bound states that a sentence of T is true in every model of T .\nBottom-Up Refinement For a compound subformula ϕ, depending on its structure, Table 1 gives the bottom-up refinement ct-bound ϕrct and cf-bound ϕ r cf for ϕ with respect to C. It is rather straightforward to obtain these formulas. For instance, the formula in the bottom-right of the table indicates that if ϕ is the formula ψ ∨ χ, then ϕ is certainly false for those tuples for which both ψ and χ are certainly false. Or, more formally, if both T |= Ccf(ψ) ⊃ ¬ψ and T |= Ccf(χ) ⊃ ¬χ, then T |= Ccf(ψ) ∧ Ccf(χ) ⊃ ¬(ψ ∨ χ).\nTop-Down Refinement In the case of top-down refinements, the bounds of a formula ψ are used to construct refinement bounds for one of its direct subformulas ϕ (i.e., ϕ is one of ψ’s children in the syntax tree). The top-down refinement ct-bounds ϕrct and cf-bounds ϕ r cf for ϕ are given in Table 2. In this table, the tuple y denotes the free variables of ψ that do not occur in ϕ and x′ denotes a new variable. We illustrate some of these refinement bounds. For further explanation why\nthese bounds are in a certain sense the most precise ones that can be obtained, we refer to our work on approximate reasoning (Wittocx et al., 2008a).\nLet ψ be the formula ∀x P (x, y). Recall that intuitively, the ct-bound Cct(ψ) indicates for which domain elements d, ∀x P (x, d) is certainly true. For such a d and an arbitrary d′ ∈ D, P (d′, d) must be true. Hence, Cct(ψ) is a ct-bound for ϕ. Indeed, since x does not occur free in Cct(ψ), T |= ∀x∀y (Cct(ψ) ⊃ P (x, y)) follows from T |= ∀y (Cct(ψ) ⊃ ∀x P (x, y)).\nNow let ψ be the formula P (x) ∧ Q(x, y). If we know that P (d1) ∧ Q(d1, d2) is certainly false, but Q(d1, d2) is certainly true, then P (d1) must be certainly false. Hence, ∃y Ccf(ψ) ∧ Cct(χ) is a cf-bound for P (x).\nLet ψ be the formula ∃x P (x, y) and assume that ∃x P (x, dy) is certainly true, but for all d′x, except dx, P (d′x, dy) is certainly false. Then we can conclude that P (dx, dy) must be true. This is precisely what is expressed by the formula Cct(ψ) ∧ ∀x′ (x 6= x′ ⊃ Ccf(ϕ)[x/x′]).\nFunctional Refinement If ϕ[x, y] is the formula F (x) = y, functional refinement bounds for ϕ take into account that F is a function. The functional refinement ct-bound ϕrct and cf-bound ϕ r cf are given by: ϕrct := ∀y′ (y′ 6= y ⊃ Ccf(ϕ)[y/y′])\nϕrcf := ∃y′ (Cct(ϕ)[y/y′] ∧ y 6= y′)\nwhere y′ is a new variable. Informally, the first of these formulas indicates that F (x) is certainly equal to y if for every y′ 6= y, F (x) is certainly not equal to y′. The second one says that F (x) is certainly not equal to y if F (x) is certainly equal to y′ for some y′ 6= y.\nCopy Refinement Let ϕ[x1, . . . , xn] and ψ[y1, . . . , ym] be two formulas such that ϕ[x1/z, . . . , xn/z] and ψ[y1/z, . . . , ym/z] are the same, modulo a renaming of their non-free variables. That is, ϕ and ψ have exactly the same syntax tree, but their variables may differ. Denote by E(ϕ,ψ) the set of all equalities xi = yj such that for some occurrence of xi in ϕ, yj occurs in the corresponding position in ψ. Then the formula ∃y1 . . . ∃ym (Cct(ψ) ∧ ∧ E(ϕ,ψ)) is a copy refinement ct-bound for ϕ and\nthe formula ∃y1 . . . ∃ym (Ccf(ψ) ∧ ∧ E(ϕ,ψ)) is a copy refinement cf-bound for ϕ. We also say that these are the copy-refinement bounds from ψ to ϕ.\nExample 7. Let ϕ be the formula P (x1, x1)∧∀s Q(x2, s) and ψ the formula P (y1, y2)∧∀t Q(y2, t). Because ϕ[x1/z, x2/z] is equal to ψ[y1/z, y2/z] modulo the renaming of s by t, these formulas satisfy the requirement for copy refinement. The set E(ϕ,ψ) is given by {x1 = y1, x1 = y2, x2 = y2} and hence,\n∃y1∃y2 (Cct(ψ) ∧ x1 = y1 ∧ x1 = y2 ∧ x2 = y2)\nis a copy refinement ct-bound for ϕ. Observe that if Cct(ψ) does not contain bounded occurrences of x1 or x2, this formula is equivalent to the simpler formula Cct(ψ)[y1/x1, y2/x1] ∧ x1 = x2.\nOne-Step Refinements We call ϕrct (ϕ r cf) a refinement ct-bound (cf-bound) for ϕ with respect to C if it is an input, axiom, bottom-up, top-down, functional or copy refinement ct-bound (cf-bound) for ϕ with respect to C. Lemma 25 states that a refinement ct-bound (cf-bound) is indeed a ct-bound (cf-bound).\nLemma 25. If ϕrct is a refinement ct-bound for ϕ with respect to C, then it is a ct-bound for ϕ. Similarly for cf-bounds.\nProof. The proof consists of a simple analysis of all cases. We proved some of the cases when we introduced input, bottom-up and top-down refinement. The proof of the other cases is similar.\nDefinition 26. Let C be a c-map for T over σ, ϕ a subformula of T , ϕrct a refinement ct-bound and ϕrcf a refinement cf-bound for ϕ with respect to C. An assignment Cr that corresponds to C, except that it assigns Cr(ϕ) = (Cct(ϕ) ∨ ϕrct, Ccf(ϕ)) or Cr(ϕ) = (Cct(ϕ), Ccf(ϕ) ∨ ϕrcf) is called a one-step refinement of C.\nFrom Lemma 24 and 25 we obtain the following result.\nProposition 27. Every one-step refinement of a c-map for T over σ is a c-map for T over σ.\nAs already mentioned at the beginning of this section, one can compute a c-map for T over σ by first assigning (⊥,⊥) to every subformula of T and then repeatedly applying one-step refinements. We call this nondeterministic any-time algorithm the refinement algorithm.\nExample 8 (Example 1 ctd.). Figure 1 shows a possible run of the refinement algorithm for input T and σ. Here, the sentences of T1 are represented by their syntax trees. The numbers indicate at which step the bounds are refined. The trivial bounds are not shown.\nIn step (1), ct-bound ⊥ for the first sentence is replaced by ⊥ ∨ > using axiom refinement. Of course, this new bound can be simplified to >. For all following steps, the figure shows simplified bounds. In step (2) and (3) the bounds of subformula Edge(u, v) are refined by input refinement. Then, top-down refinement is used to set the ct-bound of ¬Sub(u, v) ∨ Edge(u, v) to >. Next, by top-down refinement, ¬Edge(u, v) becomes the ct-bound for ¬Sub(u, v) and then the cf-bound for Sub(u, v).\nIn a similar way, the cf-bound y 6= z is derived for subformula Sub(x, y) ∧ Sub(x, z) (step (7) – (12)). Then, by copy refinement, the cf-bounds for Sub(x, y) becomes ∃u∃v (¬Edge(u, v) ∧ u = x ∧ v = y), wich simplifies to ¬Edge(x, y). Likewise, after simplification, ¬Edge(x, z) is the copy refinement cf-bound for Sub(x, z). Finally, two steps of bottom-up refinement are used to set the ct-bound of ¬(Sub(x, y) ∧ Sub(x, z)) to y 6= z ∨ ¬Edge(x, y) ∨ ¬Edge(x, z).\nAt this step, a fixpoint is reached: every one-step refinement that can be performed yields a bound that is logically equivalent to the one it tries to refine.\nExample 9. Consider a simplified planning problem, where actions should be scheduled such that if an action ap is a precondition of an action a0, then ap is performed at an earlier time point than a0. This problem is described by the theory T3, consisting of the sentence\n∀a0∀ap∀t0 Prec(ap, a0) ∧Do(a0, t0) ⊃ (∃tp tp < t0 ∧Do(ap, tp)).\nFrom this sentence, it follows that if a chain of i actions must be executed before a0 can be executed, then a0 cannot be executed before the ith timepoint. Therefore, for any i > 0, the following formula is a cf-bound for Do(a0, t0) over σ2 = {Prec,<}:\n∃a1 · · · ∃ai (Prec(a1, a0) ∧ . . . ∧ Prec(ai, ai−1)) ∧ ¬∃t1 · · · ∃ti (t1 < t0 ∧ . . . ∧ ti < ti−1).\nDenote this formula by χi. For any n > 0 and a sufficient number of steps, the refinement algorithm can derive that ψn := χ1 ∨ . . . ∨ χn is a cf-bound for Do(a0, t0). Clearly, for n1 6= n2, ψn1 is not logically equivalent to ψn2 . This indicates that the refinement algorithm will not reach a fixpoint for input T3 and σ2.\nAs shown by the examples, there are several issues concerning the practical implementation of the refinement algorithm.\n1. Due to the non-deterministic nature of the algorithm, a heuristic is needed to choose which bounds to refine and which kind of refinement to apply. A reasonable choice is to first apply all possible axiom and input refinements. Then, top-down refinement for formula ϕ is applied only if a bound for its parent or one of its siblings in the syntax tree has recently been refined. Similarly, bottom-up refinement is applied if a bound for one of ϕ’s children has been refined. Such a strategy was used in Example 1.\n2. The bounds should be simplified at regular time points, i.e., they should be replaced by equivalent but smaller formulas. If bounds are not simplified, they can only grow in size, rapidly leading to formulas of unwieldy size. A simplification algorithm is discussed in Section 6.\n3. To be able to detect that a fixpoint has been reached, one needs to find out that two bounds are equivalent. In general this is undecidable. To detect a fixpoint in at least some cases, one could use an FO theorem prover (and restrict its running time).\nIn case a fixpoint cannot be reached or detected, another stop criterion is needed. For example, one could restrict the number of one-step refinements, or the total time the refinement algorithm can use. Another stop criterion, and a simple fixpoint check are discussed in Section 6."
    }, {
      "heading" : "4.4.2 Extracting an Atom-Based and Atom-Equal C-Map",
      "text" : "The c-maps obtained by the refinement algorithm are in general neither atom-based nor atom-equal. To derive from an arbitrary c-map C an atom-equal c-map that is at least as precise as C, we first collect for each predicate P all bounds that are assigned to occurrences of P in the theory. Then the disjunction of these bounds is assigned as new bound to each occurrence of P . Because all bounds assigned to atoms over P are then essentially the same, we have an atom-equal c-map. We now present this method more formally:\nDefinition 28. Let C be a c-map for a TNF theory T and P/n a predicate. Let P (x11, . . . , x1n), . . . , P (xm1, . . . , xmn) be all occurrences of P in T and let y1, . . . , yn be n new variables. Denote by ϕict, respectively ϕ i cf , the formulas\n∃x′i1 · · · ∃x′in (Cct(P (xi1, . . . , xin))[xi1/x′i1, . . . , xin/x′in] ∧ y1 = x′i1 ∧ . . . ∧ yn = x′in)\nand\n∃x′i1 · · · ∃x′in (Ccf(P (xi1, . . . , xin))[xi1/x′i1, . . . , xin/x′in] ∧ y1 = x′i1 ∧ . . . ∧ yn = x′in),\nwhere the variables x′ij are new variables. The ct-copy closure of P (xk1, . . . , xkn) with respect to C is the disjunction ∨ 1≤i≤m ϕ i ct[y1/xk1, . . . , yn/xkn]. The cf-copy closure of P (xk1, . . . , xkn) is the\nformula ∨\n1≤i≤n ϕ i cf [y1/xk1, . . . , yn/xkn]. The copy-closure for atoms of the form F (x) = y is defined\nsimilarly.\nWe denote the ct-copy closure of an atom ϕ by copyCct(ϕ), and its cf-copy closure by copy C cf(ϕ).\nDefinition 29. The copy-closure of C is the c-map that assigns (copyCct(ϕ), copyCcf(ϕ)) to every atomic subformula ϕ of T , and corresponds to C on all other subformulas. Example 10. Let T4 be the theory consisting of the sentences ∀x (P (x) ⊃ R(x)) and ∀y (Q(y) ⊃ R(y)) and let C5 be a c-map over σ3 = {P,R} that assigns (P (x),⊥) to R(x) and (Q(y),⊥) to R(y). The copy-closure of C5 assigns\n((∃x′ (P (x′) ∧ x′ = x)) ∨ (∃x′ (Q(x′) ∧ x′ = x)), (∃x′ (⊥ ∧ x′ = x)) ∨ (∃x′ (⊥ ∧ x′ = x)))\nto R(x). These bounds simplify to (P (x) ∨ Q(x),⊥). Likewise, the copy-closure of C5 assigns to R(y) bounds that simplify to (P (y) ∨Q(y),⊥). Proposition 30. The copy-closure of a c-map is an atom-equal c-map.\nProof. This follows immediately from the definition of atom-equal c-map since for every predicate symbol P (or function symbol F ), the same bounds, namely the formulas ∨ 1≤i≤n ϕ i ct and ∨ 1≤i≤n ϕ i cf mentioned in definition 28, are assigned to every atom over P (respectively F ).\nRecall that a c-map C is atom-based if C is implied by CA, i.e., by all sentences in C that stem from bounds for atomic subformulas of T . A method to derive an atom-based c-map from an arbitrary c-map is based on the following observation. Let C be a c-map for T over σ and let ϕ[x] be the subformula χ ∧ ψ of T . If Cct(ϕ) is the formula Cct(χ) ∧ Cct(ψ), i.e., it is the bottom-up refinement ct-bound for ϕ with respect to C, then T |= ∀x (Cct(ϕ) ⊃ ϕ) is implied by T |= ∀x (Cct(χ) ⊃ χ) and T |= ∀x (Cct(ψ) ⊃ ψ). It is easy to check that the same property holds for all other bottom-up refinement bounds:\nLemma 31. Let C be a c-map for T over σ and ϕ[x] a subformula of T , and let ϕrct and ϕrcf be the bottom-up refinement bounds for ϕ with respect to C. If S is the set of direct subformulas of ϕ, i.e., its children in the syntax tree, and T ′ is the theory given by\nT ′ := {∀y Cct(ψ) ⊃ ψ | ψ[y] ∈ S} ∪ {∀y Ccf(ψ) ⊃ ¬ψ | ψ[y] ∈ S},\nthen T ′ |= ∀x ϕrct ⊃ ϕ and T ′ |= ∀x ϕrcf ⊃ ¬ϕ. Definition 32. A c-map C for T is called a bottom-up c-map if for every non-atomic subformula ϕ of T , Cct(ϕ) is the bottom-up ct-refinement bound for ϕ with respect to C, and Ccf(ϕ) is the bottom-up cf-refinement bound for ϕ with respect to C.\nThe next proposition follows directly from Lemma 31.\nProposition 33. A bottom-up c-map C is atom-based. Observe that a bottom-up c-map C for T is completely determined by the bounds it assigns to the atomic subformulas of T . Hence, given a c-map, one can derive a bottom-up c-map from it by retaining the bounds for the atomic subformulas and then computing the corresponding bottom-up c-map. We conclude that we can derive an atom-based, atom-equal c-map from an arbitrary c-map by deriving an atom-based c-map from its copy-closure.\nExample 11 (Example 1 ctd.). Let C6 be the fixpoint shown in Figure 1. This c-map is atom-equal (and equivalent to its copy-closure). The bottom-up c-map derived from C6 is shown in Figure 2. Observe that this c-map is less precise than C6. For instance, the cf-bound assigned by C6 to the conjunction Sub(x, y) ∧ Sub(x, z) is a disjunction of two bounds, namely bound y 6= z, obtained by top-down refinement, and bound ¬Edge(x, y)∨¬Edge(x, z), obtained by bottom-up refinement. In the c-map of Figure 2, only the latter bound is present.\nFor the c-map in Figure 2, the c-transformation of Sub(x, y) ∧ Sub(x, z) is given by\n((Sub(x, y) ∧ Edge(x, y)) ∧ (Sub(x, z) ∧ Edge(x, z))) ∧ (Edge(x, y) ∧ Edge(x, z)).\nThis formula contains repeated constraints Edge(x, y) and Edge(x, z) on the variables x, y and z. In general bottom-up c-maps produce many such repetitions. These could easily be eliminated to speed up the grounding process, but it depends on the used grounding algorithm which ones are best deleted."
    }, {
      "heading" : "5. Inductive Definitions",
      "text" : "Although all NP problems can be cast as MX(FO) problems, modelling such problems using pure FO can be extremely complex. In practice, modelling is often enhanced considerably by using extensions of FO with constructs such as inductive definitions, subsorts, aggregates, partial functions and arithmetic. For this enriched language we have implemented the model generator idp (Wittocx et al., 2008b; Wittocx & Mariën, 2008).2\nIn this paper we focus on grounding of the extension of FO with inductive definitions. It is well-known that in arbitrary domains, inductively definable concepts such as “reachability” are not FO-expressible. In finite domains however, they can be encoded (e.g., by encoding the fixpoint construction), but the process is tedious and leads to large theories. In this section we will extend the refinement algorithm to FO(ID) (Denecker, 2000; Denecker & Ternovska, 2008). This language extends FO with a construct for representing some of the most common types of inductive definitions: monotone induction and non-monotone induction such as induction over a well-founded order and iterated inductive definitions. Such definitions have many applications in real-life computational problems, e.g., in planning problems or problems involving reachability or dynamic systems (Denecker & Ternovska, 2008, 2007). At the same time, FO(ID) is also an integration of FO and logic programming.\n2. idp can be downloaded from http://dtai.cs.kuleuven.be/krr/software.html"
    }, {
      "heading" : "5.1 Three-Valued Structures",
      "text" : "While FO(ID) has a standard two-valued semantics, three-valued structures are used in the formal semantics of definitions. Indeed, an inductive definition defines a set by describing how to construct it. In the semantics, the intermediate stages of the construction are recorded by three-valued sets, representing for any object whether it belongs to the set or not, or whether this has not yet been derived. We therefore recall the basic concepts of three-valued logic.\nWe denote the truth values true, false and unknown by respectively t, f and u. A three-valued Σ-interpretation Ĩ consists of a domain D and\n• a domain element xĨ ∈ D for each variable x;\n• a function P Ĩ : Dn → {t, f,u} for each predicate symbol P/n;\n• a function F Ĩ : Dn → D for each function symbol F/n.\nIf P Ĩ(d) 6= u for every tuple d of domain elements and predicate symbol P , then Ĩ is two-valued: it corresponds to the interpretation I that assigns d ∈ P I iff P Ĩ(d) = t for every predicate P and corresponds to Ĩ on all other symbols.\nThe truth order ≤ on the set of truth values is induced by f < u < t, the precision order ≤p is induced by u <p f and u <p t. These orders are extended to three-valued Σ-structures: if Ĩ and J̃ correspond on ΣF , then we define\n• Ĩ ≤ J̃ iff P Ĩ(d) ≤ P J̃(d) for every d and P ;\n• Ĩ ≤p J̃ iff P Ĩ(d) ≤p P J̃(d) for every d, P .\nObserve that two-valued structures are maximally precise three-valued structures. On the other hand, the least precise three-valued structure assigns P Ĩ(d) = u for every d and P .\nWe define the truth value Ĩ(ϕ) of a formula ϕ in a three-valued interpretation Ĩ with domain D by the standard Kleene semantics:\n• Ĩ(P (t1, . . . , tn)) := P Ĩ(tĨ1, . . . , tĨn);\n• Ĩ(ϕ1 ∨ ϕ2) := lub≤{Ĩ(ϕ1), Ĩ(ϕ2)};\n• Ĩ(ϕ1 ∧ ϕ2) := glb≤{Ĩϕ1, Ĩ(ϕ2)};\n• Ĩ(∃x ϕ) := lub≤{Ĩ[x/d](ϕ) | d ∈ D};\n• Ĩ(∀x ϕ) := glb≤{Ĩ[x/d](ϕ) | d ∈ D}.\nAn atom of the form P (d), where d is a tuple of domain constants, is called a domain atom. For a truth value v and a domain atom P (d), we denote by Ĩ[P (d)/v] the interpretation that assigns v to P (d) and corresponds to Ĩ on all other symbols. This notation is extended to sets of domain atoms."
    }, {
      "heading" : "5.2 Inductive Definitions",
      "text" : "An FO(ID) theory is a set of FO sentences and definitions. A definition ∆ is a finite set of rules of the form3\n∀x (P (x)← ϕ),\n3. Usually, nested terms are allowed as arguments of P , but to facilitate the presentation, we only allow variables as arguments in this paper.\nwhere P is a predicate and ϕ an FO formula. The free variables of ϕ should be among x. P (x) is called the head of the rule, ϕ the body. Predicates that occur in the head of a rule of ∆ are called defined predicates of ∆. The set of all defined predicates of ∆ is denoted Def(∆). All other symbols are called open with respect to ∆. The set of open symbols of ∆ is denoted by Open(∆).\nObserve that an FO(ID) theory has the appearance of an FO theory augmented with a collection of logic programs. As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)’s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc.\nThe semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf, 1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes the semantics of all of the above mentioned types of inductive definitions in mathematics. We borrow the presentation of this semantics from Denecker and Vennekens (2007).\nDefinition 34. Let ∆ be a definition and Ĩ a three-valued structure. A well-founded induction for ∆ above Ĩ is a sequence 〈J̃ξ〉0≤ξ≤α of three-valued structures such that\n1. J̃0 assigns P J̃0(d) = u, if P is a defined predicate and corresponds to Ĩ on the open symbols;\n2. For each limit ordinal λ ≤ α, J̃λ = lub≤p{J̃ξ | ξ < λ};\n3. For every ordinal ξ, J̃ξ+1 relates to J̃ξ in one of the following ways:\n(a) J̃ξ+1 = J̃ξ[P (d)/t] for some domain atom P (d) such that P J̃ξ(d) = u and for some rule ∀x (P (x)← ϕ) in ∆, J̃ξ[x/d](ϕ) = t.\n(b) J̃ξ+1 = J̃ξ[U/f], where U is a set of domain atoms, such that for each P (d) ∈ U , P J̃ξ(d) = u and for all rules ∀x (P (x)← ϕ) in ∆, J̃ξ+1[x/d](ϕ) = f.\nIntuitively, (a) says that a domain atom P (d) can be made true if there is a rule with P (x) as head and body ϕ such that ϕ[x/d] is already true. On the other hand (b) explains that P (d) can be made false if there is no possibility of making a corresponding body true, except by circular reasoning. The set U , commonly called an unfounded set, is a witness to this: making all atoms in U false also makes all corresponding bodies false.\nA well-founded induction is called terminal if it cannot be extended anymore. The limit of a terminal well-founded induction is its last element. Denecker and Vennekens (2007) show that each terminal well-founded induction for ∆ above Ĩ has the same limit, which corresponds to the wellfounded model of ∆ extending Ĩ|Open(∆), and is denoted by wfm∆(Ĩ). The well-founded model is three-valued in general.\nA two-valued structure I satisfies a definition ∆ if I = wfm∆(I). An FO(ID) theory T is a finite set of FO sentences and definitions. I satisfies T if it satisfies all definitions and sentences in T . If ∆ is a definition over Σ and J a Σ|Open(∆)-structure, there exists at most one expansion I of J to Σ such that I |= ∆. A definition is called total if for any Σ|Open(∆)-structure J there is precisely one expansion I of J to Σ that satisfies ∆. Intuitively, total definitions correspond to well-formed definitions: for every defined predicate P , they define for each tuple of domain elements whether d belongs to the relation denoted by P or not. If a definition is not total, this typically indicates an error. Hence in practice, all definitions that occur in MX(FO(ID)) specifications are total. For example, this is the case for all MX(FO(ID)) specifications used in the second ASPcompetition (Denecker, Vennekens, Bond, Gebser, & Truszczyński, 2009). In general, checking whether a definition is total is undecidable. However, there are several broad and easily recognizable classes of total definitions. For example, all monotone and stratified definitions are total.\nWe give some examples of definitions and MX(FO(ID)) problems.\nExample 12. Definition ∆1 defines relation TC to be the transitive closure of relation R. ∆1 = {\n∀x∀y (TC(x, y)← R(x, y)). ∀x∀y (TC(x, y)← ∃z (TC(x, z) ∧ TC(z, y))). } Example 13. To cast the problem of finding a Hamiltonian path in a given graph as an MX(FO(ID)) problem, let\nσ = 〈{Edge/2}, ∅〉 Σ = {σP ∪ {Ham/2, Reached/1}, {Start/0}〉.\nPredicate Ham represents the edges that form the path and Reached the vertices that are in the path. The constant Start represents the first vertex of the path. Let T be the theory\n∀v1∀v2 (Ham(v1, v2) ⊃ Edge(v1, v2)). ∀v1∀v2∀v3 (Ham(v1, v2) ∧Ham(v1, v3) ⊃ v2 = v3). ∀v1∀v2∀v3 (Ham(v1, v3) ∧Ham(v2, v3) ⊃ v1 = v2). ∀v ¬Ham(v, Start). ∀v Reached(v).{\n∀v (Reached(v)← v = Start). ∀v (Reached(v)← ∃w (Reached(w) ∧Ham(w, v))).\n} .\nThen model expansion for input structure T and input vocabulary σ expresses the Hamiltonian path problem: in every model M |=Iσ T , the collection of edges (v1, v2) ∈ HamM forms a Hamiltonian path in the graph represented by EdgeIσ .\nA well-known concept that we will use later on in this section is the completion of a definition. The completion of a definition ∆ is an FO theory that is weaker than ∆, and is defined as follows.\nDefinition 35. The completion of a definition ∆ is the FO theory that contains for every P ∈ Def(∆) the sentence ∀x (P (x) ≡ ((x = y1 ∧ ϕ1) ∨ . . . ∨ (x = yn ∧ ϕn))), where ∀y1 (P (y1)← ϕ1), . . . , ∀yn (P (yn)← ϕn) are the rules in ∆ with P in the head.\nWe denote the completion of ∆ by Comp(∆). Clearly, every body of a rule in ∆ occurs in Comp(∆). If T is a theory then we denote by Comp(T ) the result of replacing in T all definitions by their completion. The following result states that the completion of T is weaker than T .\nTheorem 36 (Denecker & Ternovska, 2008). ∆ |= Comp(∆) and T |= Comp(T ) for every definition ∆ and FO(ID) theory T .\nThe SAT(ID) problem is the problem of deciding whether a given propositional FO(ID) theory is satisfiable. Currently there exist three SAT(ID) solvers. IDsat (Pelov & Ternovska, 2005) works by translating a SAT(ID) problem into an equivalent SAT problem and then calls a SAT solver. MidL (Mariën, Wittocx, & Denecker, 2007) and MiniSAT(ID) (Mariën, Wittocx, Denecker, & Bruynooghe, 2008) take a native approach. Mariën (2009) provides details on the specific form of propositional FO(ID) theories accepted by these solvers, and a method to transform arbitrary propositional FO(ID) theories into this form."
    }, {
      "heading" : "5.3 Grounding Inductive Definitions",
      "text" : "Like MX(FO) problems, MX(FO(ID)) problems can be reduced to SAT(ID) problems by grounding. In this section we extend grounding and the refinement algorithm of Section 4 to FO(ID). Without loss of generality (Mariën, Gilis, & Denecker, 2004), we assume that none of the predicates of the input vocabulary σ is defined by a definition in T , and no predicate is defined by more than one definition. Moreover, we assume that every rule body is in TNF."
    }, {
      "heading" : "5.3.1 Full and Reduced Grounding",
      "text" : "Let T be an FO(ID) theory. As for FO, a grounding Tg for T with respect to Iσ is a propositional FO(ID) theory that is Iσ-equivalent to T . We extend the notion of full and reduced grounding to definitions.\nDefinition 37. The full grounding of a rule ∀x P (x) ← ϕ with respect to Iσ is the set {P (d) ← Grfull(ϕ[x/d]) | d ∈ Dn}, where n is the number of variables in x. Similarly, the reduced grounding of ∀x (P (x) ← ϕ) is the set {P (d) ← Grred(ϕ[x/d]) | d ∈ Dn}. The full (reduced) grounding of a definition ∆ is the union of the full (reduced) groundings of all rules in ∆.\nThe full (reduced) grounding of an FO(ID) theory T is the set of the full (reduced) groundings of all sentences and definitions in T ."
    }, {
      "heading" : "5.3.2 Definitions Depending Only on σ",
      "text" : "We say that a definition ∆ depends on expansion symbols if Open(∆) 6⊆ σ. If ∆ does not depend on expansion symbols, then the interpretation of every predicate in Def(∆) is the same in every model M of T expanding Iσ. Indeed, for such a definition and any M |=Iσ T , M |Open(∆) is completely determined by Iσ. Therefore also wfm∆(M) only depends on Iσ.\nThe deductive database literature describes several algorithms to compute wfm∆(M) for a definition that does not depend on expansion symbols. Most of them are only defined for definitions where every rule body is a conjunction of atoms. But some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to handle full FO bodies.\nAssume ∆ is a definition that does not depend on expansion symbols. Let τ be the vocabulary 〈σP ∪Def(∆), σF 〉 and Iτ the τ -structure such that Iτ |σ = Iσ and Iτ |= ∆. Then clearly, M |=Iσ T iff M |=Iτ T for any structure M . However, a grounding for T \\∆ with respect to τ can be obtained more efficiently, since Grred(T \\∆, Iτ ) is necessarily smaller than Grred(T, Iσ). Indeed, T \\ ∆ is a subtheory of T , and Grred(T \\∆, Iτ ) does not contain symbols of Def(∆), while Grred(T, Iσ) does.\nObserve also that the set of c-maps for T over τ is a superset of the set of c-maps for T over σ, since the bounds assigned by the former c-maps are formulas over τ , instead of only over σ. As such, c-maps computed by the refinement algorithm for T over τ might yield more efficient grounding compared to c-maps computed for T over σ."
    }, {
      "heading" : "5.3.3 Bounds for Definitions",
      "text" : "We now extend the refinement algorithm to FO(ID).\nDefinition 38. A formula ϕ is a subformula of an FO(ID) theory T if it is a subformula of a sentence in T or a subformula of a rule body in a definition of T . A c-map for T over σ is an assignment of a ct- and cf-bound over σ to every subformula of T .\nNote that a c-map does not assign bounds to heads of rules in a definition. Our strategy to compute a c-map for an FO(ID) theory T is simple: construct the completion of T and apply the refinement algorithm on Comp(T ) to obtain a c-map C for Comp(T ). The restriction of C to the subformulas of T is a c-map for T . Indeed, every subformula ϕ of T occurs in Comp(T ) and since T |= Comp(T ), Comp(T ) |= ∀x (Cct(ϕ) ⊃ ϕ) and Comp(T ) |= ∀x (Ccf(ϕ) ⊃ ¬ϕ), also T |= ∀x (Cct(ϕ) ⊃ ϕ) and T |= ∀x (Ccf(ϕ) ⊃ ¬ϕ).\nIn order to use a c-map for grounding, we lift the definition of c-transformation to FO(ID) theories.\nDefinition 39. Let C be a c-map for a theory T and ∆ a definition in T . The c-transformation of a rule ∀x (P (t) ← ϕ) of ∆ is given by ∀x (P (t) ← C〈ϕ〉). The c-transformation C〈∆〉 of a\ndefinition ∆ is the set of c-transformations of rules in ∆. The c-transformation of T is the set of the c-transformations of the formulas and definitions in T .\nWe also lift the notion of C-equivalence to definitions.\nDefinition 40. Two definitions ∆1 and ∆2 are C-equivalent if for every structure I that satisfies C, I |= ∆1 iff I |= ∆2.\nHowever, Lemma 15 does not hold for FO(ID) theories: for a definition ∆, C〈∆〉 is not necessarily C-equivalent to ∆.\nExample 14. Let σ be the empty vocabulary and T the theory\nP\n{P ← P}.\nThis theory is unsatisfiable because the definition {P ← P} has only one model, in which P is false. This contradicts the sentence in T . Clearly, > is a ct-bound for P . If C is a c-map for T over σ assigning (>,⊥) to P , then C〈{P ← P}〉 = {P ← (P ∧ ¬⊥) ∨ >}, which is equivalent to {P ← >}. This definition has only a model that assigns true to P . Since this model also satisfies C, we conclude that {P ← P} and C〈{P ← P}〉 are not C-equivalent.\nDefinition 41. Let ∆ a definition of T . We call c-map C for T ∆-tolerant if C〈∆〉 and ∆ are C-equivalent. We call C T -tolerant if it is ∆-tolerant for every definition ∆ of T .\nIn the following, we say that a formula occurs positively (negatively) in a definition ∆ if it occurs positively (negatively) in a body of a rule in ∆.\nProposition 42. Let ∆ be a definition of a theory T . Then a c-map C for T over σ is ∆-tolerant if for every subformula ϕ of ∆ that contains a predicate P ∈ Def(∆), the following hold:\n1. If ∆ is not total, then Cct(ϕ) = Ccf(ϕ) = ⊥.\n2. If ϕ occurs positively in ∆ and P occurs positively in ϕ, then Cct(ϕ) = ⊥.\n3. If ϕ occurs negatively in ∆ and P occurs negatively in ϕ, then Ccf(ϕ) = ⊥.\nNote that the c-map of Example 14 violates the second condition. We will prove Proposition 42 by inductively constructing for any structure I that satisfies C, a sequence of three-valued structures that is a well-founded induction above I for both ∆ and C〈∆〉. If I |= ∆, we show that a terminal sequence with this property can be constructed, proving that I also satisfies C〈∆〉. If I 6|= ∆, a sequence with this property can be constructed such that its last element is not less precise than I. This shows that I does not satisfy C〈∆〉 either. To construct a well-founded induction for both ∆ and C〈∆〉, we prove that each step that extends a well-founded induction for ∆ is also a valid step to extend it for C〈∆〉. Step (3a) in Definition 34 is covered by Lemma 43, step (3b) by Lemma 44.\nLemma 43. Let I be a structure that satisfies a c-map C for T over σ and let J̃ ≤p I be a threevalued interpretation such that J̃ |σ is two-valued. Then J̃(ϕ) ≤p J̃(C〈ϕ〉) for every subformula ϕ of T .\nProof. We prove this lemma by induction. First assume ϕ[x] is an atom. Then C〈ϕ〉 is the formula (ϕ∧¬Ccf(ϕ))∨Cct(ϕ). If J̃(ϕ) = u, then clearly J̃(C〈ϕ〉) ≥p J̃(ϕ). If J̃(ϕ) = f, then J̃(Cct(ϕ)) must be false, since I |= C. Therefore J̃(C〈ϕ〉) = f. If on the other hand, J̃(ϕ) = t, then J̃(Ccf(ϕ)) = f and hence, J̃(C〈ϕ〉) = t.\nThe inductive cases are all very similar to the base case. We prove one of them. Assume ϕ is the formula ψ ∨ χ. Then C〈ϕ〉 is the formula ((C〈ψ〉 ∨ C〈χ〉) ∧ ¬Ccf(ϕ)) ∨ Cct(ϕ). If J̃(ϕ) = f, then J̃(ψ) = J̃(χ) = f, and by induction J̃(C〈ψ〉) = J̃(C〈χ〉) = f. Since also J̃(Cct(ϕ)) = f, we conclude that J̃(C〈ϕ〉) = f. If on the other hand J̃(ϕ) = t, then J̃(Ccf(ϕ)) = f. Also J̃(ψ) = t or J̃(χ) = t, and therefore J̃(C〈ψ〉) = t or J̃(C〈χ〉) = t. Hence J̃(C〈ϕ〉) = t.\nLemma 44. Let ∆ be a definition of T and C a c-map for T over σ that satisfies the three conditions of Proposition 42. Let I be a structure that satisfies C and J̃ ≤p I a three-valued interpretation such that J̃ |σ is two-valued. If U is a set of domain atoms defined in ∆ and unknown in J̃ , then for every subformula ϕ of ∆ such that J̃ [U/f](ϕ) 6= u, the following hold: • J̃ [U/f](ϕ) ≤ J̃ [U/f](C〈ϕ〉) if ϕ occurs negatively in ∆;\n• J̃ [U/f](ϕ) ≥ J̃ [U/f](C〈ϕ〉) if ϕ occurs positively in ∆; Proof. Denote H̃ := J̃ [U/f]. If J̃(ϕ) 6= u, the result follows immediately from Lemma 43.\nWe prove the case where J̃(ϕ) = u by induction. Assume that ϕ is an atom P (x). Since J̃(ϕ) = u and H̃(ϕ) 6= u, we know that P (xJ̃) ∈ U and H̃(ϕ) = f. Therefore H̃(C〈ϕ〉) = H̃((ϕ ∧ ¬Ccf(ϕ)) ∨ Cct(ϕ)) = H̃(Cct(ϕ)). If ϕ occurs negatively in ∆, then we have to prove that H̃(ϕ) ≤ H̃(C〈ϕ〉). Since H̃(ϕ) = f, this inequality holds regardless the value of Cct(ϕ) and Ccf(ϕ) in H̃. If on the other hand, ϕ occurs positively, we have to prove that H̃(ϕ) ≥ H̃(C〈ϕ〉). Since H̃(ϕ) = f and H̃(C〈ϕ〉) = H̃(Cct(ϕ)), this inequality can only hold if H̃(Cct(ϕ)) = f. Because the conditions on C ensure that Cct(ϕ) = ⊥, we can conclude that indeed H̃(Cct(ϕ)) = f.\nWe omit the inductive cases, since they are very similar to the base case.\nProof of Proposition 42. Let I be a structure that satisfies C. We have to prove that I |= ∆ iff I |= C〈∆〉. If ∆ is not total, the proof is trivial, since then ∆ and C〈∆〉 are equivalent.\nNow assume that ∆ is total and let 〈J̃ξ〉0≤ξ≤α be a well-founded induction for both ∆ and C〈∆〉 above I. We will prove that if J̃α is not two-valued, and J̃α <p I, there exists a J̃α+1 such that 〈J̃ξ〉0≤ξ≤α+1 is again a well-founded induction for ∆ and C〈∆〉. Also observe that if λ is a limit ordinal and 〈J̃ξ〉0≤ξ<λ is a well-founded induction for both ∆ and C〈∆〉, then the same holds for 〈J̃ξ〉0≤ξ≤λ.\nThis is sufficient to conclude the proof. Indeed, if I |= ∆, we can keep on extending the sequence until we end up in I, and derive that I |= C〈∆〉. If I 6|= ∆, then we will eventually extend the well-founded induction with a structure J̃α+1 6≤p I. But then, the well-founded model of C〈∆〉 will also be more precise than J̃α+1, which shows that I 6|= C〈∆〉.\nAssume that J̃α is not two-valued and J̃α <p I. Because ∆ is total, there exists a J̃α+1 such that 〈J̃ξ〉0≤ξ≤α+1 is a well-founded induction for ∆. We have to prove that it is also a well-founded induction for C〈∆〉. There are two possibilities: • J̃α+1 = J̃α[P (d)/t] for some domain atom P (d) and there is a rule ∀x (P (x) ← ϕ) in ∆\nsuch that J̃α[x/d](ϕ) = t. By Lemma 43, also J̃α[x/d](C〈ϕ〉) = t. Hence, 〈J̃ξ〉0≤ξ≤α+1 is a well-founded induction for C〈∆〉.\n• J̃α+1 = J̃α[U/f] and for every P (d) ∈ U and rule ∀x (P (x) ← ϕ) in ∆, J̃α+1[x/d](ϕ) = f. By Lemma 44, we conclude that also J̃α+1[x/d](C〈ϕ〉) = f. Therefore, 〈J̃ξ〉0≤ξ≤α+1 is a wellfounded induction for C〈∆〉.\nFrom Proposition 42 we derive the following procedure to compute a T -tolerant c-map for a theory T . First compute a c-map C for T that is not necessarily T -tolerant. Then, for every definition ∆ of T and every subformula ϕ of ∆, replace Cct(ϕ) and Ccf(ϕ) by ⊥, if this is required to satisfy the conditions of Proposition 42.\nWe conclude that the following algorithm produces a correct grounding for FO(ID) theory T :\n1. Compute a c-map C for T over σ.\n2. If C is inconsistent with respect to Iσ, output ⊥ and stop.\n3. Else, derive an atom-based, T-tolerant c-map C′ from C.\n4. Output Grred(C′〈T 〉 ∪ C′A), using any off-the-shelf grounder for FO(ID)."
    }, {
      "heading" : "6. Implementation and Experiments",
      "text" : "So far we have focussed mostly on grounding size. Proposition 23 guaranteed that grounding with bounds produces smaller groundings. In this section we are concerned with the efficiency and practical implementation of grounding with bounds. A first issue was mentioned at the end of Section 4.4.2: an atom-based c-map C computed by the refinement algorithm contains many repeated constraints on variables. To ground C〈T 〉 efficiently, such repetitions should be avoided as much as possible. Secondly, an efficient grounder consults bounds as soon as possible. In particular, it should use bounds to avoid unnecessary instantiations of variables, rather than to remove these instantiations afterwards. As a case study, we will show in detail how to adapt a basic “top-down style” grounding algorithm to efficiently exploit bounds. We sketch how the same principles can be applied for a “bottom-up style” grounder.\nIn the second part of this section we discuss some aspects of implementing the refinement algorithm. As we mentioned in Section 4.4.1, there are several issues concerning the practical implementation of this algorithm. In particular, a method to simplify bounds is needed, as well as a good stop criterion. We show how these issues can be addressed by representing bounds as first-order binary decision diagrams.\nFinally, we report on our implementation, called GidL, of the refinement and grounding algorithm. We present experimental results that show the impact of using bounds on grounding size and time."
    }, {
      "heading" : "6.1 Case Study: Top-Down Grounding with Bounds",
      "text" : "For the rest of this section, assume T is in TNF and fix an Iσ-consistent, atom-based c-map C for T over σ. We call a formula of the form ϕ∨ψ or ∃x ϕ a disjunctive formula. Vice versa, a conjunctive formula is a formula of the form ϕ ∧ ψ or ∀x ϕ.\nWe now present a simple “top-down style” grounding algorithm that exploits bounds without constructing C〈T 〉 ∪ CA explicitly. The algorithm is shown in Algorithm 1. Basically, it consults the bounds assigned by C whenever it substitutes the free variables of a formula ϕ[x] by domain constants d. If according to the bounds, ϕ[x/d] is certainly true, i.e., Iσ[x/d] |= Cct(ϕ), then the grounding of ϕ[x/d] is not computed. Instead, the algorithm then proceeds as if ϕ[x/d] is equal to >. Similarly if ϕ[x/d] is certainly false. In this way, the algorithm avoids creating unnecessary instantiations. One can check that if C is the trivial c-map, Algorithm 1 reduces to a straightforward top-down style grounding algorithm that produces Grfull(T ).\nLine 1 of Algorithm 1 checks whether one of the sentences of T is certainly false. If this is the case, then clearly T is unsatisfiable (cf. Definition 10), and this can be reported immediately. Before a sentence is grounded, line 4 checks whether this sentence is certainly true according to C. Only sentences that are not certainly true are grounded. Observe that both checks are simple syntactic checks and can be executed in constant time.\nFunction groundConj gets as input a formula ϕ[x] and returns a grounding for ∀x ϕ[x]. In particular, if ϕ is a sentence, then the result of applying groundConj to ϕ is a grounding for ϕ.\nIn groundConj, universal quantifiers are implicitly pushed inside conjunctions. That is, if ϕ[x] is a conjunction ψ1 ∧ . . . ∧ ψn, then for every i ∈ [1, n], the grounding of ∀x ψi is computed by applying groundConj to ψi. The conjunction of these groundings is returned as grounding for ∀x ϕ. According to equivalence (6) of Section 2.2, this transformation yields an equivalent formula.\nFunction groundConj only consults the c-map when variables are substituted by domain constants or when the input formula is an atom. As such, groundConj ignores (“eliminates”) the bounds assigned to conjunctive formulas. As we mentioned at the end of Section 4.4.2, this is important to avoid repeated constraints on a variable.\nIn groundConj(ϕ[x]), only those substitutions ϕ[x/d] for which Iσ[x/d] 6|= Cct(ϕ) are grounded (see, e.g., line 12). Indeed, the other substitutions yield a formula that is certainly true in all models of T expanding Iσ, and can therefore be omitted from the ground conjunction C that is computed.\nAlgorithm 1: Ground with Bounds Input: T , σ, Iσ and C Output: A grounding Tg for T with respect to Iσ if Ccf(ϕ) = > for some sentence ϕ of T then return ⊥;1 Tg := ∅;2 // Ground all sentences of T for every sentence ϕ of T do3 if Cct(ϕ) 6= > then Add groundConj(ϕ) to Tg;4\n// Ground all definitions of T for every definition ∆ of T do5 Add groundDef(∆) to Tg;6\n// Add the grounding of CA for every atomic subformula ϕ[x] of T do7 for every d such that Iσ[x/d] |= Cct(ϕ) do8 Add ϕ[x/d] to Tg;9\nfor every d such that Iσ[x/d] |= Ccf(ϕ) do10 Add ¬ϕ[x/d] to Tg;11\nreturn Tg;12\nFunction groundConj(ϕ[x])\nC := ∅;1 switch ϕ[x] do2 case ϕ is a literal3 for all d such that Iσ 6|= Cct(ϕ)[x/d] do4 if Iσ |= Ccf(ϕ)[x/d] then return ⊥;5 else Add ϕ[x/d] to C;6\ncase ϕ = ∀y ψ[x, y]7 return groundConj(ψ[x, y]);8\ncase ϕ = ∧ i ψi9\nC := ⋃ i groundConj(ψi);10\ncase ϕ is a disjunctive formula11 for all d such that Iσ 6|= Cct(ϕ)[x/d] do12 if Iσ |= Ccf(ϕ)[x/d] then return ⊥;13 else Add groundDisj(ϕ[x/d]) to C;14\nreturn ∧ C;15\nFunction groundDisj(ϕ[x])\nD := ∅;1 switch ϕ[x] do2 case ϕ is a literal3 for all d such that Iσ 6|= Ccf(ϕ)[x/d] do4 if Iσ |= Cct(ϕ)[x/d] then return >;5 else Add ϕ[x/d] to D;6\ncase ϕ = ∃y ψ[x, y]7 return groundDisj(ψ[x, y]);8\ncase ϕ = ∨ i ψi9\nD := ⋃ i groundDisj(ψi);10\ncase ϕ is a conjunctive formula11 for all d such that Iσ 6|= Ccf(ϕ)[x/d] do12 if Iσ |= Cct(ϕ)[x/d] then return >;13 else Add groundConj(ϕ[x/d]) to D;14\nreturn ∨ D;15\nFunction groundDef(∆)\n∆g := ∅;1 for every rule ∀x (P (x)← ϕ[y]) in ∆ do2 z := x \\ y;3 for every d such that Iσ 6|= Ccf(ϕ[y/d]) do4 if Iσ |= Cct(ϕ[y/d]) then ϕg := >;5 else ϕg := groundConj(ϕ[y/d]);6 n := the number of variables in z;7 Add P (x)[y/d, z/d ′ ]← ϕg to ∆g for every d ′ ∈ Dn;8\nreturn ∆g;9\nBefore ϕ[x/d] is grounded, it is checked whether this substitution yields a formula that is certainly false (see, e.g., line 13). If this is the case, the whole conjunction C will certainly be false, and therefore ⊥ is returned immediately. Observe that implicitly the formula Cct(ϕ) ∨ (¬Ccf(ϕ) ∧ ϕ) is grounded. Hence the correctness of groundConj follows from Lemma 13.\nFunction groundDisj is dual to groundConj. On input ϕ[x], it returns a grounding for ∃x ϕ[x]. It implicitly pushes existential quantifiers through disjunctions and eliminates the bounds assigned to disjunctive formulas.\nFunction groundDef returns a grounding for its input definition ∆. It grounds the rules of ∆ one-by-one. For each rule ∀x (P (x) ← ϕ[y]), only those substitutions ϕ[y/d] that are possibly true are tried (line 4). If ϕ[y/d] is certainly true, it is replaced by > (line 5).\nIn lines 7-11 of Algorithm 1, the theory CA is grounded. Recall that this is necessary to obtain a grounding that is Iσ-equivalent to T (see Proposition 21). Observe that if C is the trivial c-map, no output is produced when lines 7-11 are executed.\nThe computationally expensive steps in Algorithm 1 are the steps where the truth values in Iσ of (some of the) bounds assigned by C are computed. For large bounds, these steps can become infeasible. Indeed, the expression complexity of FO is PSPACE-complete (Stockmeyer, 1974). As such, grounding with too complex bounds may take more time and space than constructing the full grounding and simplifying it afterwards. The stop criterion of Section 6.2.3 for the refinement algorithm is designed to avoid too complex bounds. Our experiments in Section 6.3 show that carefully restricting the complexity of the bounds leads to faster grounding.\nWe stress that Algorithm 1 is just one example of a grounding algorithm that exploits bounds.4 The principle of consulting bounds as soon as possible can be applied to adapt other grounding algorithms as well. For example, recall that a bottom-up style grounder starts by storing all instances of atomic subformulas of T in a table. To exploit bounds efficiently, a bottom-up grounder should consult the bounds while constructing these tables and leave out, e.g., all instances that are certainly false. As such, it avoids unnecessary large tables, which in turn improves the speed of the subsequent grounding steps."
    }, {
      "heading" : "6.2 Implementing the Refinement Algorithm and Querying Bounds",
      "text" : "In this section we discuss some aspects of implementing the refinement algorithm. As mentioned above, applying a simplification method for first-order formulas to simplify the bounds at regular time points is essential for a good implementation. One can use Goubault’s (1995) method for this purpose. To this end, the bounds need to be represented by first-order binary decision diagrams. We show in this section that such a representation can be applied without too much overhead when applying one-step refinements. Moreover, using binary decision diagrams leads to extra benefits: we obtain a cheap equivalence check for bounds and an elegant algorithm to query bounds, which is needed to implement Algorithm 1. At the end of this section we discuss a stop criterion for the refinement algorithm and we discuss an implementation."
    }, {
      "heading" : "6.2.1 First-Order Binary Decision Trees and Diagrams",
      "text" : "We borrow the definition of first-order BDDs from Goubault (1995). Let ϕ, ψ1 and ψ2 be three formulas. The ternary if-then-else operator is denoted by “_”, and defined by ϕ _ ψ1;ψ2 := (ϕ ∧ ψ1) ∨ (¬ϕ ∧ ψ2). The formula ϕ _ ψ1;ψ2 is also represented by the graph shown in Figure 3.\nDefinition 45 (Goubault, 1995). FO binary decision trees (BDTs) and kernels are defined by simultaneous induction:\n• An atom is a kernel;\n4. The question whether top-down grounders can be made more efficient than bottom-up grounders is outside the scope of this paper, and still undecided.\nϕ\n||y y\ny y\n\"\"E EE\nEE EE\nE\nψ2 ψ1\nObserve that the graph representation of a BDT is a tree whose nodes are atoms or existentially quantified BDTs.\nGoubault (1995) showed that for every FO formula ϕ there exists a BDT ϕ′ such that ϕ and ϕ′ are equivalent. In an actual implementation, sharing, reducing and ordering are applied to obtain a simplified and compact representation of BDTs. Such representations are called reduced ordered binary decision diagrams (BDDs). Sharing means that isomorphic subtrees are stored at the same address in memory. Reducing involves exhaustively replacing subtrees of the form ϕ _ ψ;ψ by ψ. A BDT ϕ is ordered if the kernels appear in some fixed order on every path in the graph representation of ϕ.\nAs mentioned above, there are several important benefits of using BDDs to represent bounds for a formula:\n• An implementation of the refinement algorithm using BDDs allows us to use the simplification algorithm for BDDs of Goubault (1995).\n• As explained in Section 4.4, to detect that the refinement algorithm has reached a fixpoint, one needs to check the equivalence of bounds. Often, the BDDs representing two equivalent formulas will be equal.5 Hence, a cheap (but necessarily incomplete) equivalence check for two bounds consists of checking the syntactic equality of the two BDDs representing them. Since equal BDDs are stored at the same address, this check is done in constant time.\n• As we will show in Section 6.2.2, querying a bound ϕ[x], i.e., finding all tuples d such that Iσ[x/d] |= ϕ, can easily be implemented directly on a BDD representation of ϕ. Querying a bound is one of the main operations performed by a grounding algorithm that exploits bounds directly (such as Algorithm 1).\nOn the other hand, using BDDs does not result in too much overhead when computing a c-map. If ϕ, ψ and χ[x, y] are represented by BDDs, then a BDD representing ¬ϕ, ∃x ϕ, ∀x ϕ, ϕ ∧ ψ, ϕ ∨ ψ and χ[x/x′, y] can be computed efficiently (Bryant, 1986; Goubault, 1995). This implies that every one-step refinement on a c-map C can be implemented efficiently, even if the bounds assigned by C are BDDs."
    }, {
      "heading" : "6.2.2 Querying a Bound",
      "text" : "In Algorithm 1, the main operation performed on a bound ϕ[x] is querying: finding tuples d of domain constants such that Iσ |= ϕ[x/d]. Finding a tuple d such that Iσ 6|= ϕ[x/d] corresponds to querying ¬ϕ. We now show that querying a bound ϕ[x] can be done directly on the BDD representation by a simple backtracking algorithm.\n5. For propositional BDDs, this is always the case.\nThe idea is to traverse the BDD, starting from the root, and trying to end up in the leaf >. At each inner node ψ[y] _ ψ1;ψ2, the free variables in that node are replaced by domain constants dy. If Iσ |= ψ[y/dy], the algorithm continues via ψ1, otherwise via ψ2. If it ends up in ⊥, it backtracks. If on the other hand, it ends up in >, the performed substitutions constitute an answer for ϕ.\nFunction query implements the sketched query algorithm. It gets a bound ϕ[x] as input and returns a substitution [x/d] such that Iσ |= ϕ[x/d]. If no such substitution exists, it returns FAIL. This algorithm can easily be adapted to return all answers to ϕ[x] instead of just one.\nFunction query(ϕ[x])\nif ϕ = > then return the empty substitution;1 else if ϕ = ψ[y] _ ψ1;⊥ then2 for every tuple d such that Iσ |= ψ[y/d] do3 θ := query(ψ1[y/d]);4 if θ 6= FAIL then return θ ∪ [y/d]5\nelse if ϕ = ψ[y] _ ⊥;ψ2 then6 for every tuple d such that Iσ 6|= ψ[y/d] do7 θ := query(ψ2[y/d]);8 if θ 6= FAIL then return θ ∪ [y/d]9\nelse if ϕ is of the form ψ[y] _ ψ1;ψ2 then10 for every tuple d ∈ D|y| do11 if Iσ |= ψ[y/d] then θ := query(ψ1[y/d]);12 else θ := query(ψ2[y/d]);13 if θ 6= FAIL then return θ ∪ [y/d]14\nreturn FAIL;15\nIn lines 3 and 7, the algorithm needs to find tuples d such that respectively Iσ |= ψ[y/d] and Iσ 6|= ψ[y/d]. If ψ[y] is an atom P (y), this can be implemented by consulting the table P Iσ . If ψ is a kernel ∃x χ[x, y], function query can be applied recursively to find the tuples. Indeed, any answer (d′, d) to χ[x, y] provides a tuple d such that Iσ |= ψ[y/d]. Vice versa, Iσ 6|= ψ[y/d] if χ[x, y/d] has no answer.\nWe illustrate the query algorithm on an example.\nExample 15. Let ϕ[x, y] be the BDD shown in figure 4, and let {a, b} be the domain of Iσ, P Iσ = {b}, RIσ = {} and QIσ = {(b, b)}. To find an answer for ϕ[x, y], the query algorithm starts at the root P (x). Since none of its children are equal to ⊥, every domain constant is tried. Assume domain constant a is tried first. Because a 6∈ P Iσ , the algorithm continues with node R(a) _ >;⊥. Because the “else” child of this node is ⊥ and a 6∈ RIσ , the algorithm returns to the root and tries\ndomain element b. Since b ∈ P Iσ , it goes to node Q(b, y) _ >;⊥. Since the “else” child of this node is ⊥, the algorithm tries those substitutions d for y such that (b, y/d) ∈ QIσ . Thus, y is substituted by b. Finally, answer [x/b, y/b] is returned."
    }, {
      "heading" : "6.2.3 A Stop Criterion for the Refinement Algorithm",
      "text" : "As shown in Section 4.4, the c-map refinement algorithm does not reach a fixpoint on certain inputs. Also, even in the case a fixpoint can be found, computing it may take a long time, and the bounds assigned by the fixpoint can be so complex that querying becomes very inefficient. Using such bounds may severely slow down grounding. This indicates the need for a good stop criterion.\nSimple Stop Criteria A very simple stop criterion limits the number of one-step refinements that may be performed to a given maximum number m. This m may depend on the theory T . For instance, m can be set to C × (number of subformulas in T ), where C is some fixed constant.\nA slightly less naive technique, which can be combined with the previous, limits the “complexity” of the bounds by putting a fixed upper bound N on the number of nodes the BDD representation of a bound may have. If a one-step refinement would lead to a new bound with more nodes than N , this refinement is not performed. As this limits the number of applicable one-step refinements, the probability of reaching a fixpoint increases.\nStop Criteria via Estimators The experiments we present in Section 6.3 indicate that there exist appropriate values for C and N that produce positive results on most of the examples. Still, on some problems, grounding slows down severely, while the size of the produced grounding does not decrease. One of these problems is the following clique problem (entry 6 in Table 4).\nExample 16. Recall that a clique is a maximally connected graph. Let\nσ = 〈{Edge/2}, ∅〉, Σ = 〈σP ∪ {Clique/1}, ∅〉\nand T the theory\n∀x∀y (Clique(x) ∧ Clique(y) ⊃ (x = y ∨ Edge(x, y))). ∀x ((∀y (Clique(y) ∧ x 6= y ⊃ Edge(x, y))) ⊃ Clique(x)).\nIf EdgeIσ is symmetric, i.e., Iσ represents an undirected graph, a model of T expanding Iσ is a clique in Iσ that is not contained in a strictly larger clique in Iσ. Within a small number of iterations, the refinement algorithm finds for Clique(x) the ct-bound ∀x′ x 6= x′ ⊃ Edge(x, x′). This formula expresses that Clique(x) is certainly true in every solution if x is directly connected to every other vertex in the input graph. Clearly, for most graphs, no vertex satisfies this condition. So, for most graphs, ⊥ would be an equally precise ct-bound, but would allow much faster querying.\nThe situation is worse for the cf-bound for Clique(x). Since for an undirected graph, every single vertex is a clique, and thus occurs in at least one of the solutions, the cf-bound is necessarily unsatisfiable with respect to T . Yet, our implementation of the refinement algorithm came up with ∃x′ (¬Edge(x, x′) ∧ x 6= x′ ∧ (∀x′′ (x′ 6= x′′ ⊃ Edge(x′, x′′)))) as cf-bound. The query algorithm outlined above takes cubic time in the number of vertices to find out that no x satisfies this formula.\nTo avoid the problems illustrated by the example above, one could estimate the reward of a bound versus the cost of evaluating it. Recall that more precise bounds yield smaller grounding sizes. Therefore, the reward of a bound ψ is dictated by its precision. Given Iσ, it is possible to find a good estimate for the number of answers to ψ in Iσ (Demolombe, 1980), which is in turn a measure for the precision of ψ. For a fixed query algorithm, one can also estimate the cost cost(ψ) of computing an answer in Iσ to a query ψ. In the following, we assume that the reward of a bound is a positive real number, and its cost a strictly positive real number.\nGiven the reward and the cost of bounds, the complexity of a bound ψ can be limited by restricting the ratio\nr(ψ) := cost(ψ)\nreward(ψ) + 1 .\nIf a one-step refinement would replace a bound ψ1 by ψ2, but r(ψ1) < r(ψ2), then this refinement is not performed. Clearly, for all bounds ψ assigned by a c-map C computed according to this restriction, r(ψ) ≤ r(⊥) holds. Observe that to apply this restriction, an input structure Iσ is needed. However, the obtained bounds are independent of Iσ.\nIt is beyond the scope of this paper to describe in detail estimators for the reward and cost of bounds. The fairly naive estimator used for the experiments in the next section assigns ratios of the order O(|DIσ |), respectively O(|DIσ |3), to the ct-bound, respectively cf-bound, mentioned in Example 16. As such, if |DIσ | is large enough, these bounds will be avoided."
    }, {
      "heading" : "6.2.4 Implementation of the Refinement Algorithm",
      "text" : "Our implementation of the refinement algorithm, including the heuristic for choosing refinement bounds (Section 4.4.1) and stop criterion, is presented by Algorithm 6. The algorithm maintains a queue Q of one-step refinements that will be applied. Each of these is represented by a tuple 〈r, ϕ〉, where r is the type of the refinement, e.g., axiom refinement, and ϕ the formula on which r will be applied.\nAlgorithm 6: Refinement Algorithm Q := ∅; C := the trivial c-map for T ;1 for all sentences ϕ of T do Q.push(〈axiom, ϕ〉);2 for all subformulas ϕ of T over σ do3 Q.push(〈ct-input, ϕ〉); Q.push(〈cf-input, ϕ〉);4 while Q 6= ∅ and the maximum number of refinements is not reached do5 〈r, ϕ〉 := Q.pop();6 if r is a ct-refinement then7\nψ := the r-refinement bound for ϕ with respect to C;8 ψ := simplify(Cct(ϕ) ∨ ψ);9 if ψ 6= Cct(ϕ) and ψ is not too complex then10 Cct(ϕ) := ψ;11 for all 〈r, χ〉 such that the r-refinement bound for χ contains Cct(ϕ) do12 Q.push(〈r, χ〉);13\nelse14 . . . // Similar code for cf-refinements15\nreturn C;16\nAs explained in Section 4.4.1, our implementation starts by scheduling all possible axiom- and input-refinements. If in a later stage a bound is changed (line 11), then all refinement bounds that contain this bound are scheduled to be applied (line 13). For example, assume that T contains the formula ϕ ∧ ψ and that the ct-bound of ϕ is refined. Then bottom-up ct-refinement for ϕ ∧ ψ is scheduled since the bottom-up ct-refinement bound for that formula is given by Cct(ϕ) ∧ Cct(ψ), which contains Cct(ϕ). For the same reason also top-down cf-refinement for ψ is scheduled.\nThe algorithm applies all scheduled refinements, unless the maximum number of refinement steps is reached (line 5). The other part of the discussed stop criterion is applied in line 10. If the newly\ncomputed bound ψ is too complex, i.e., its BDD representation contains too many nodes or the ratio r(ψ) is above a certain threshold, ψ is not used.\nIf BDDs are used to represent the bounds assigned by C, line 8 can be implemented in linear time in the size of C. If we use Goubault’s simplification algorithm for BDDs for implementing line 9, the worst case complexity of this step is non-elementary in the size of Cct(ϕ) ∨ ψ (Goubault, 1995). The estimators we used to implement line 10 take linear time in the size of ψ. It may seem that the complexity of the simplification method limits the practical applicability of Algorithm 6. However, since large BDDs usually do not pass the test in line 10, the simplification method is rarely applied on large BDDs. In the experiments of the next section, the running time of the refinement algorithm is negligible compared to the running time of the grounding algorithm."
    }, {
      "heading" : "6.3 Experiments",
      "text" : "We implemented Algorithm 1 and Algorithm 6, using BDDs to represent bounds. The resulting grounder is called GidL. In this section, we present experiments, obtained with GidL, that show the impact of using bounds on grounding size and time.\nAs input for GidL, we used 37 benchmark problems, mainly taken from Asparagus.6 The details about the experiments are available at http://dtai.cs.kuleuven.be/krr/software.html. We used four different versions of GidL:\nGidLnb: Assigns 〈ϕ,¬ϕ〉 as bound to every atomic subformula ϕ over the input vocabulary, and 〈⊥,⊥〉 to every other subformula. As such, it creates the reduced grounding of the input theory.\nGidLbu: Assigns 〈ϕ,¬ϕ〉 as bound to every atomic subformula ϕ over the input vocabulary and then applies bottom-up refinements to obtain a bottom-up c-map.\nGidLmn: Limits the refinement algorithm to 4×(number of subformulas in T ) one-step refinements and allows a maximum of 4 internal nodes in each BDD used to represent the bounds. According to previous experiments (Wittocx et al., 2008b), this is the best setting when limiting the number of nodes.\nGidLr: Limits the refinement algorithm to 4× (number of subformulas in T ) one-step refinements. It limits the complexity of the derived bounds by estimating the number of answers and the cost, as described in the previous section.\nIn Table 3, the influence of bounds on the grounding size is shown. The second and third column show the ratio of the grounding size obtained with GidLmn and GidLr compared to Grred(T ). For GidLnb and GidLbu, this ratio is always equal to 1. When interpreting Table 3, it is important to note that small reductions in grounding size are not important. The reason being that all reductions that can be obtained by the refinement algorithm are also obtained by applying unit propagation on the grounding (see Section 7 for a discussion). Since there exist very efficient implementations of unit propagation, it is not beneficial to let the refinement algorithm find small reductions at a relatively high cost. We see that both GidLmn and GidLr reduce the grounding size with more than 50% in around 30% of the benchmarks. In 7, respectively 6, of the benchmarks there is a spectacular reduction of more than 95%.\nMore important than reductions in size are reductions in grounding time. Table 4 shows the running times of the different versions of GidL, and (between brackets) the ratio of the running time to the running time of GidLnb. The running time of the refinement algorithm is included (it never took more than 0.02 seconds). A time-out (###) of 600 seconds was used.\nOn many benchmarks, the reduction in grounding time with respect to GidLnb is due to the reduction in grounding size. Yet there are also several benchmarks where time decreases a lot, while\n6. http://asp.haiti.cs.uni-potsdam.de/\nthere is almost no reduction in size. This is mostly due to the creation of a bottom-up c-map, as can be seen from the running times of GidLbu. Applying bottom-up refinements leads to the assignment of non-trivial bounds to non-atomic subformulas. This allows for earlier pruning by a top-down style grounder, and hence faster grounding.\nFrom Table 4, we can see that GidLmn performs quite well. On half of the benchmarks, it is more than 44% faster than GidLnb. It is also more than 20% faster than GidLbu on half of the benchmarks. There are some outliers however. On benchmarks 6 and 11, it is far slower than GidLbu, while not producing a significantly smaller grounding. This indicates the use of a complex bound with relatively small reward. Compared to GidLmn, GidLr is faster and more robust, indicating that using estimators for the reward and cost of bounds pays off in most cases. In only two of the benchmarks, our naive estimator makes a wrong guess. In benchmark 1, a bound with high cost and no reward is allowed, in benchmark 7, a bound with low cost and high reward is not allowed by GidLr. It is part of future work to implement improved estimators.\nWe conclude from our experiments that grounding with bounds is applicable in practice. It often leads to smaller grounding sizes on standard benchmark problems, and if the bounds are carefully restricted, it yields a significant speed up. Since the time to compute bounds is small compared to the overall grounding time, computing them is essentially for free.\nIn general, a smaller grounding does not necessarily lead to faster propositional model generation. For example, grounding size (and time) increases when symmetry breaking formulas are added, but these formulas may drastically improve the overall solving time (Torlak & Jackson, 2007). Another example are clause-learning SAT solvers: the clauses learnt by these solvers are redundant, but may improve the solving time by orders of magnitude. The question arises whether our method of grounding with bounds may lead to slower overall model generation time compared to grounding without bounds. This is not the case. The experiments above show that in general, grounding with bounds is faster than grounding without bounds. Since grounding with bounds also produces smaller groundings, the subsequent initialization phase of the SAT solver is executed faster. If T1 and T2 are two groundings obtained by grounding the same input theory and structure with, respectively without bounds, it can be shown7 that the typical simplification steps applied in this initialization phase transform T1 and T2 in exactly the same simplified theory T3. Thus, after initialization, the SAT solver is applied on exactly the same theory, whether or not the grounder used bounds. It follows that in general, the overall model generation time does not increase when bounds are applied while grounding."
    }, {
      "heading" : "7. Related Work",
      "text" : "In the previous sections we described a method to obtain fast and compact grounding. Several such methods have been described in the literature. Some of them are — like ours — preprocessing techniques that rewrite the input theory. Other techniques involve reasoning on the propositional level. In this section we provide an overview. We indicate which ones can be applied to improve GidL. We also give an overview of existing grounders."
    }, {
      "heading" : "7.1 Methods to Optimize Grounding",
      "text" : "Derivation of Bounds To our knowledge, the methods proposed in the literature to derive bounds are less general than the one we presented in this paper. This is illustrated by Table 5, where we show for several grounders the impact of manually adding redundant information. For all the grounders in this table except GidL, manually adding redundancy may have a serious impact. For some grounders, the need to add redundancy can sometimes be avoided by writing the input theory in a specific format. For example, the grounder gringo (Gebser et al., 2007) uses a syntactic check to derive bounds: it derives that predicate q of the input vocabulary is a bound for predicate p if p\n7. The exact formulation and the proof of the property are beyond the scope of this paper."
    }, {
      "heading" : "37 Wire routing 0.92 0.99",
      "text" : ""
    }, {
      "heading" : "36 Weight bounded dominating set 1.00 1.00",
      "text" : ""
    }, {
      "heading" : "35 Waterbucket 0.36 0.36",
      "text" : ""
    }, {
      "heading" : "34 Train scheduling 0.25 0.25",
      "text" : ""
    }, {
      "heading" : "33 Toughnut 0.00 0.00",
      "text" : ""
    }, {
      "heading" : "32 Tarski 1.00 1.00",
      "text" : ""
    }, {
      "heading" : "31 Sudoku 0.75 0.75",
      "text" : ""
    }, {
      "heading" : "30 Spanningtree 0.06 0.06",
      "text" : ""
    }, {
      "heading" : "29 Solitaire 1.00 0.73",
      "text" : ""
    }, {
      "heading" : "28 Sokoban 0.59 0.59",
      "text" : ""
    }, {
      "heading" : "27 Social golfer 1.00 1.00",
      "text" : ""
    }, {
      "heading" : "26 Slitherlink 0.04 0.04",
      "text" : ""
    }, {
      "heading" : "25 Disjunctive scheduling 0.83 0.83",
      "text" : ""
    }, {
      "heading" : "24 Pigeonhole 1.00 1.00",
      "text" : ""
    }, {
      "heading" : "23 N-queens 1.00 1.00",
      "text" : ""
    }, {
      "heading" : "22 Missionaries 0.03 0.03",
      "text" : ""
    }, {
      "heading" : "21 Mirror puzzle 1.00 1.00",
      "text" : ""
    }, {
      "heading" : "20 Maze generation 0.90 0.90",
      "text" : ""
    }, {
      "heading" : "19 Magic series 1.00 1.00",
      "text" : ""
    }, {
      "heading" : "18 Labyrinth 0.99 0.99",
      "text" : ""
    }, {
      "heading" : "17 Knighttour 0.00 0.00",
      "text" : ""
    }, {
      "heading" : "16 Tower of Hanoi 1.00 1.00",
      "text" : ""
    }, {
      "heading" : "15 Hamiltonian circuit 0.01 0.01",
      "text" : ""
    }, {
      "heading" : "14 Algebraic groups 0.99 1.00",
      "text" : ""
    }, {
      "heading" : "13 Graph partitioning 0.94 1.00",
      "text" : ""
    }, {
      "heading" : "12 Golomb ruler 0.54 1.00",
      "text" : ""
    }, {
      "heading" : "11 FO-hamcircuit 0.94 0.99",
      "text" : ""
    }, {
      "heading" : "10 Fastfood 1.00 1.00",
      "text" : "is defined by a choice rule of the form, e.g., {p(X)} :- q(X). However, if this rule is replaced by {p(X)} :- dom(X), where dom denotes the domain, and the constraint :- p(X),not q(X),dom(X) is added, q is still a bound for p, but this is not detected by gringo, as can be seen in Table 5.\nThe grounder of the dlv system (Perri et al., 2007) may derive bounds by reasoning on the propositional level. As we explain below, the order in which rules and constraints are grounded is of crucial importance for such a method to pay off. Since dlv grounds rules before constraints, using a constraint to state that q is a bound for p does not improve grounding time.\nPropagation on the Propositional Level One of the techniques to produce smaller groundings consists of applying a constraint propagation method on the ground theory Tg and replacing by >, respectively ⊥, every ground literal that is derived to be true, respectively false. The resulting theory is then simplified. This technique is applied by the grounder psgrnd (East et al., 2006), which uses unit propagation (Davis & Putnam, 1960) and complete one-atom lookahead (Li & Anbulagan, 1997) as propagation methods. The latter is performed once the grounding is finished, the former is triggered each time a unit clause is added to the grounding. If an inconsistency is detected by unit propagation, the grounding process is terminated immediately. Observe that this technique yields small groundings but does not improve grounding speed, except for the (rare) case where the propagation method detects an inconsistency during grounding. Indeed, it does not avoid computing all ground instances of the formulas in the input theory.\nIf a propositional constraint propagation method is applied while the grounding is being constructed, the derived information could be used to refine bounds. For instance, if unit-propagation derives that the domain atom P (d1, . . . , dn) is true, then x1 = d1 ∧ . . . ∧ xn = dn is a ct-bound for P (x1, . . . , xn). These bounds could be used to speed up the construction of the rest of the grounding. For this method to be effective, however, some careful fine-tuning of the order in which sentences are grounded is required. It may even be necessary to alternatingly compute partial groundings of different sentences. To the best of our knowledge, this process has not been worked out or implemented with unit-propagation or one-atom lookahead as underlying propagation method. On the other hand, most ASP grounders apply it for the following limited propagation method: if all rules defining a predicate P are grounded, it is concluded that a domain atom P (d) is certainly true if it occurs in a ground rule of the form P (d) ← >, and certainly false if it does not occur in the head of any ground rule. In this case, a good grounding order can be derived from the dependency graph of the input theory (e.g., Cadoli & Schaerf, 2005; Perri et al., 2007). In GidL, this strategy is implemented for grounding definitions.\nSharing A second technique is called sharing and consists of detecting subformulas in the ground theory Tg that occur more than once. If such a subformula ϕ is detected, all its occurrences in Tg are replaced by a new atom P , and the sentence P ≡ ϕ is added. If ϕ is a large formula and occurs\noften in Tg, this may result in a significant grounding size reduction. Also, sharing improves the propagation in SAT solvers.\nShlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level. In GidL, we implemented a simple sharing technique using dynamic programming. We adapted function groundConj so that instead of returning a conjunction ∧ C, it creates a new atom P , adds the\nsentence P ≡ ∧ C to the grounding, and returns P . If groundConj is applied multiple times on\nthe same input ϕ, the same predicate P is returned each time, but P ≡ ∧ C is added only once. Function groundDisj is adapted in a similar fashion.\nClause splitting Clause splitting is a well-known rewriting technique applied in MACE style model generation (McCune, 2003). It consists of splitting a first-order clause\n∀x∀y∀z (ϕ1[x, z1] ∨ ϕ2[y, z2]) (20)\nwhere x 6∈ z2, y 6∈ z1 and z = z1 ∪ z2 into two new clauses\n∀x∀z1 (ϕ1[x, z1] ∨ S(z1 ∩ z2)) (21) ∀y∀z2 (¬S(z1 ∩ z2) ∨ ϕ2[y, z2]). (22)\nHere, S is a new predicate symbol. The full grounding of (20) is of the size O(|D|3), while the full grounding of (21) and (22) has only size O(|D|2).\nIf sharing is implemented by adapting the functions groundConj and groundDisj as explained above, the effect of clause splitting can be obtained by moving quantifiers according to the equivalences (4), (5), (8) and (9) of Section 2.2. For instance, we can apply equivalences (4) and (8) to replace (20) by ∀x∀z (ϕ1∨(∀y ϕ2)). Grounding the latter while applying sharing has the same effect as clause splitting. Similarly, the grounding size of ∃x∃y∃z (ϕ1[x, z1] ∧ ϕ2[y, z2]) can be reduced by replacing this formula by ∃x∃z (ϕ1 ∧ (∃y ϕ2)).\nThe simple heuristic to guide clause splitting described by Claessen and Sörensson (2003) can directly be applied to choose which quantifiers to move inside. We conclude that clause splitting could easily be incorporated in GidL.\nDatabase Techniques Several techniques for optimizing querying in databases can be used to optimize grounding. Examples are join-ordering strategies, backjumping and indexing techniques.\nOne of the most basic techniques to improve grounding speed consists of reordering (long) conjunctions or disjunctions of literals to speed up grounding. Which order is best depends on the grounding algorithm. Different strategies are described by, e.g, Leone, Perri, and Scarcello (2001), Syrjänen (1998, 2009) and in the database literature (Garcia-Molina, Ullman, & Widom, 2000). There is no problem implementing a similar technique in GidL. Also, reordering the nodes in the BDD representation of the bounds could optimize querying. It is part of future work to investigate such reordering strategies for BDDs.\nOne of the important methods in the dlv grounder is the use of a backjumping technique (Perri et al., 2007) to efficiently find all instances of a conjunction ϕ1 ∧ . . . ∧ ϕn that are possibly true, given (an overestimation of) the possibly true instances of each of the conjuncts ϕi. In GidL, this backjumping technique is applied to implement line 12 of function groundDisj. Indeed, if ϕ is the formula ϕ1 ∧ . . . ∧ ϕn, then line 12 amounts to finding all possible instances of ϕ, while the cf-bounds for ϕ1, . . . , ϕn provide an overestimation of the possibly true instances of these conjuncts. Similarly, the backjumping technique is applied to improve line 12 of groundConj, where all possibly false instances of a disjunction are calculated.\nCatalano, Leone, and Perri (2008) present an adaptation of indexing strategies for grounding.\nPartition-Based Reasoning Ramachandran and Amir (2005) describe a sophisticated grounding technique that can reduce the grounding size of FO theories, depending on the availability of some\ngraphical structure in these theories. This technique is not directly applicable in our case, since it produces groundings that are not necessarily Iσ-equivalent to the input theory. The only guarantee is that the ground theory is satisfiable iff the input problem is satisfiable."
    }, {
      "heading" : "7.2 Grounders",
      "text" : "A non-native approach to ground an MX(FO(ID)) problem consists of first translating it to an equivalent normal logic program under the well-founded semantics. This translation is described by Mariën et al. (2004). Next, a (slightly adapted) grounder for ASP is used to ground the logic program. This is the approach taken by MXidL (Mariën, Wittocx, & Denecker, 2006).\nThe first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a “bottom-up approach” (see Section 3.2.1). To construct a grounding of a sentence ϕ, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for ϕ is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm.\nkodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg, it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One of the features of kodkod is that it allows a user to give part of a solution to an MX problem as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this information to produce smaller groundings. GidL also allows for a three-valued structure as input. When applying the refinement algorithm, the set of tuples d for which the user indicates that P should be true is then used as initial ct-bound for P instead of ⊥. Similarly for the cf-bound. This leads to more efficient and compact groundings.\nmace (McCune, 2003) and paradox (Claessen & Sörensson, 2003) are finite model generators for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards. Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also methods that build the grounding incrementally are applied in these systems to avoid recomputing every grounding from scratch.\nEast et al. (2006) developed the grounder psgrnd for MX(PSpb). PSpb is a fragment of FO(ID), extended with pseudo-boolean constraints. As explained above, psgrnd performs reasoning on the ground theory to reduce memory usage and grounding size. The experiments performed by East et al. (2006) show that carefully designed data structures are of key importance to build an efficient grounder.\nASP grounders take as input a normal logic program and transform it into an equivalent ground normal logic program. As such, these grounders do not deal with (deeply) nested formulas. Currently, there are three ASP grounders: lparse (Syrjänen, 2000; Syrjänen, 2009), gringo (Gebser et al., 2007) and the grounding component of dlv (Perri et al., 2007). All of them use techniques from database theory to perform grounding efficiently.\nFinally, we mention the grounder spec2SAT (Cadoli & Schaerf, 2005). Its input theories are in the np-spec language, a language with Datalog-like syntax and semantics based on model minimality. The grounding algorithm implemented in spec2SAT is basically a simplified version of the grounding algorithm of dlv.\nIt would be interesting to compare the efficiency of the above mentioned grounders experimentally. However, it is currently not possible to conduct such an experiment in a scientifically fair way. There are several reasons for this. First, all grounders have a different input language, making it impossible to run them on the same input. Also, there are several output languages for grounders. A richer output language leads to more compact and fast grounding. For instance, for some prob-\nlems, lparse’s output size is necessarily cubic in the input domain size, while GidL’s output format allows for quadratic size. Thirdly, even if the input and output languages of all grounders were the same, an expert could easily manipulate experiments by carefully choosing his modelling style. For example, if he does not manually add bounds to the input theories, GidL has an advantage. If bodies of rules are not ordered, dlv is more likely to produce good results. Etc. Finally, because of the large amount of data processed by grounders, carefully designed data structures and an optimized implementation of the core grounding algorithm is very important to achieve fast grounding (East et al., 2006). However, several of the above mentioned grounders are not yet optimized in that sense. As such, it is difficult to derive conclusions about grounding algorithms by experimentally comparing the efficiency of current implementations of these algorithms."
    }, {
      "heading" : "8. Conclusions",
      "text" : "We presented a method to compute for a given theory, upper and lower bounds for all subformulas of that theory. We showed how these bounds can be used for efficiently creating small groundings in the context of Model Expansion for FO and FO(ID). Our method frees a user from manually discovering bounds and adding them to a theory.\nWe presented a top-down style grounding algorithm that incorporates bounds. We discussed implementation issues and showed by experiments that our method works in practice: on many benchmark problems, it leads to significant reductions in grounding size and time.\nFuture work includes the extension of our algorithm to compute bounds for richer logics, such as, e.g., extensions of FO with aggregates and arithmetic. On the implementation side, we plan to use more sophisticated estimators to evaluate whether a computed bound is beneficial for grounding."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Research supported by Research Foundation-Flanders (FWO-Vlaanderen) and by GOA 2003/08 “Inductive Knowledge Bases”. Johan Wittocx is research assistant of the Research FoundationFlanders (FWO-Vlaanderen)."
    } ],
    "references" : [ {
      "title" : "Logic Programming and Nonmonotonic Reasoning, 9th International Conference, LPNMR 2007, Tempe",
      "author" : [ "C. Baral", "G. Brewka", "J.S. Schlipf" ],
      "venue" : "AZ, USA, May 15-17,",
      "citeRegEx" : "Baral et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Baral et al\\.",
      "year" : 2007
    }, {
      "title" : "Graph-based algorithms for boolean function manipulation",
      "author" : [ "R.E. Bryant" ],
      "venue" : "IEEE Transactions on Computers,",
      "citeRegEx" : "Bryant,? \\Q1986\\E",
      "shortCiteRegEx" : "Bryant",
      "year" : 1986
    }, {
      "title" : "Compiling problem specifications into SAT",
      "author" : [ "M. Cadoli", "A. Schaerf" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Cadoli and Schaerf,? \\Q2005\\E",
      "shortCiteRegEx" : "Cadoli and Schaerf",
      "year" : 2005
    }, {
      "title" : "On demand indexing for the DLV instantiator",
      "author" : [ "G. Catalano", "N. Leone", "S. Perri" ],
      "venue" : "Workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP)",
      "citeRegEx" : "Catalano et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Catalano et al\\.",
      "year" : 2008
    }, {
      "title" : "New techniques that improve MACE-style model finding",
      "author" : [ "K. Claessen", "N. Sörensson" ],
      "venue" : "In Workshop on Model Computation (MODEL)",
      "citeRegEx" : "Claessen and Sörensson,? \\Q2003\\E",
      "shortCiteRegEx" : "Claessen and Sörensson",
      "year" : 2003
    }, {
      "title" : "A computing procedure for quantification theory",
      "author" : [ "M. Davis", "H. Putnam" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Davis and Putnam,? \\Q1960\\E",
      "shortCiteRegEx" : "Davis and Putnam",
      "year" : 1960
    }, {
      "title" : "Estimation of the number of tuples satisfying a query expressed in predicate calculus language",
      "author" : [ "R. Demolombe" ],
      "venue" : "In International Conference on Very Large Data Bases (VLDB),",
      "citeRegEx" : "Demolombe,? \\Q1980\\E",
      "shortCiteRegEx" : "Demolombe",
      "year" : 1980
    }, {
      "title" : "Extending classical logic with inductive definitions",
      "author" : [ "M. Denecker" ],
      "venue" : "J. (Eds.), International Conference on Computational Logic (CL),",
      "citeRegEx" : "Denecker,? \\Q2000\\E",
      "shortCiteRegEx" : "Denecker",
      "year" : 2000
    }, {
      "title" : "Inductive situation calculus",
      "author" : [ "M. Denecker", "E. Ternovska" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Denecker and Ternovska,? \\Q2007\\E",
      "shortCiteRegEx" : "Denecker and Ternovska",
      "year" : 2007
    }, {
      "title" : "A logic of nonmonotone inductive definitions",
      "author" : [ "M. Denecker", "E. Ternovska" ],
      "venue" : "ACM Transactions on Computational Logic (TOCL),",
      "citeRegEx" : "Denecker and Ternovska,? \\Q2008\\E",
      "shortCiteRegEx" : "Denecker and Ternovska",
      "year" : 2008
    }, {
      "title" : "Well-founded semantics and the algebraic theory of nonmonotone inductive definitions",
      "author" : [ "M. Denecker", "J. Vennekens" ],
      "venue" : null,
      "citeRegEx" : "Denecker and Vennekens,? \\Q2007\\E",
      "shortCiteRegEx" : "Denecker and Vennekens",
      "year" : 2007
    }, {
      "title" : "The second answer set programming competition",
      "author" : [ "M. Denecker", "J. Vennekens", "S. Bond", "M. Gebser", "M. Truszczyński" ],
      "venue" : "International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR),",
      "citeRegEx" : "Denecker et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Denecker et al\\.",
      "year" : 2009
    }, {
      "title" : "Tools for modeling and solving search problems",
      "author" : [ "D. East", "M. Iakhiaev", "A. Mikitiuk", "M. Truszczyński" ],
      "venue" : "AI Communications,",
      "citeRegEx" : "East et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "East et al\\.",
      "year" : 2006
    }, {
      "title" : "A Mathematical Introduction To Logic (Second edition)",
      "author" : [ "H.B. Enderton" ],
      "venue" : null,
      "citeRegEx" : "Enderton,? \\Q2001\\E",
      "shortCiteRegEx" : "Enderton",
      "year" : 2001
    }, {
      "title" : "Generalized first-order spectra and polynomial-time recognizable sets",
      "author" : [ "R. Fagin" ],
      "venue" : "Complexity of Computation,",
      "citeRegEx" : "Fagin,? \\Q1974\\E",
      "shortCiteRegEx" : "Fagin",
      "year" : 1974
    }, {
      "title" : "Rete: A fast algorithm for the many patterns/many objects match problem",
      "author" : [ "C. Forgy" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Forgy,? \\Q1982\\E",
      "shortCiteRegEx" : "Forgy",
      "year" : 1982
    }, {
      "title" : "Database System Implementation. PrenticeHall",
      "author" : [ "H. Garcia-Molina", "J.D. Ullman", "J. Widom" ],
      "venue" : null,
      "citeRegEx" : "Garcia.Molina et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Garcia.Molina et al\\.",
      "year" : 2000
    }, {
      "title" : "GrinGo : A new grounder for answer set programming",
      "author" : [ "M. Gebser", "T. Schaub", "S. Thiele" ],
      "venue" : "In Baral et al. (Baral et al.,",
      "citeRegEx" : "Gebser et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gebser et al\\.",
      "year" : 2007
    }, {
      "title" : "A BDD-based simplification and skolemization procedure",
      "author" : [ "J. Goubault" ],
      "venue" : "Logic Journal of IGPL,",
      "citeRegEx" : "Goubault,? \\Q1995\\E",
      "shortCiteRegEx" : "Goubault",
      "year" : 1995
    }, {
      "title" : "Software Abstractions: Logic, Language, and Analysis",
      "author" : [ "D. Jackson" ],
      "venue" : null,
      "citeRegEx" : "Jackson,? \\Q2006\\E",
      "shortCiteRegEx" : "Jackson",
      "year" : 2006
    }, {
      "title" : "Pushing the envelope: Planning, propositional logic and stochastic search",
      "author" : [ "H.A. Kautz", "B. Selman" ],
      "venue" : "In National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence (AAAI/IAAI),",
      "citeRegEx" : "Kautz and Selman,? \\Q1996\\E",
      "shortCiteRegEx" : "Kautz and Selman",
      "year" : 1996
    }, {
      "title" : "Comparative evaluation of approaches to propositionalization",
      "author" : [ "Krogel", "M.-A", "S. Rawles", "F. Zelezný", "P.A. Flach", "N. Lavrac", "S. Wrobel" ],
      "venue" : "International Conference on Inductive Logic Programming (ILP),",
      "citeRegEx" : "Krogel et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Krogel et al\\.",
      "year" : 2003
    }, {
      "title" : "Improving ASP instantiators by join-ordering methods",
      "author" : [ "N. Leone", "S. Perri", "F. Scarcello" ],
      "venue" : "International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR),",
      "citeRegEx" : "Leone et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Leone et al\\.",
      "year" : 2001
    }, {
      "title" : "Heuristics based on unit propagation for satisfiability problems",
      "author" : [ "C.M. Li" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Li,? \\Q1997\\E",
      "shortCiteRegEx" : "Li",
      "year" : 1997
    }, {
      "title" : "Stable models and an alternative logic programming paradigm",
      "author" : [ "V.W. Marek", "M. Truszczyński" ],
      "venue" : null,
      "citeRegEx" : "Marek and Truszczyński,? \\Q1999\\E",
      "shortCiteRegEx" : "Marek and Truszczyński",
      "year" : 1999
    }, {
      "title" : "Model Generation for ID-Logic",
      "author" : [ "M. Mariën" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Mariën,? \\Q2009\\E",
      "shortCiteRegEx" : "Mariën",
      "year" : 2009
    }, {
      "title" : "On the relation between ID-Logic and Answer Set Programming",
      "author" : [ "M. Mariën", "D. Gilis", "M. Denecker" ],
      "venue" : "European Conference on Logics in Artificial Intelligence (JELIA),",
      "citeRegEx" : "Mariën et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mariën et al\\.",
      "year" : 2004
    }, {
      "title" : "The IDP framework for declarative problem solving",
      "author" : [ "M. Mariën", "J. Wittocx", "M. Denecker" ],
      "venue" : "In Search and Logic: Answer Set Programming and SAT,",
      "citeRegEx" : "Mariën et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Mariën et al\\.",
      "year" : 2006
    }, {
      "title" : "MidL: a SAT(ID) solver",
      "author" : [ "M. Mariën", "J. Wittocx", "M. Denecker" ],
      "venue" : "In 4th Workshop on Answer Set Programming: Advances in Theory and Implementation,",
      "citeRegEx" : "Mariën et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mariën et al\\.",
      "year" : 2007
    }, {
      "title" : "SAT(ID): Satisfiability of propositional logic extended with inductive definitions",
      "author" : [ "M. Mariën", "J. Wittocx", "M. Denecker", "M. Bruynooghe" ],
      "venue" : "International Conference on Theory and Applications of Satisfiability Testing (SAT),",
      "citeRegEx" : "Mariën et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mariën et al\\.",
      "year" : 2008
    }, {
      "title" : "Mace4 reference manual and guide. CoRR, cs.SC/0310055",
      "author" : [ "W. McCune" ],
      "venue" : null,
      "citeRegEx" : "McCune,? \\Q2003\\E",
      "shortCiteRegEx" : "McCune",
      "year" : 2003
    }, {
      "title" : "A framework for representing and solving NP search problems",
      "author" : [ "D.G. Mitchell", "E. Ternovska" ],
      "venue" : null,
      "citeRegEx" : "Mitchell and Ternovska,? \\Q2005\\E",
      "shortCiteRegEx" : "Mitchell and Ternovska",
      "year" : 2005
    }, {
      "title" : "Model expansion as a framework for modelling and solving search problems",
      "author" : [ "D.G. Mitchell", "E. Ternovska", "F. Hach", "R. Mohebali" ],
      "venue" : "Tech. rep. TR 2006-24,",
      "citeRegEx" : "Mitchell et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2006
    }, {
      "title" : "Logic programs with stable model semantics as a constraint programming paradigm",
      "author" : [ "I. Niemelä" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence,",
      "citeRegEx" : "Niemelä,? \\Q1999\\E",
      "shortCiteRegEx" : "Niemelä",
      "year" : 1999
    }, {
      "title" : "Grounding for model expansion in k-guarded formulas with inductive definitions",
      "author" : [ "M. Patterson", "Y. Liu", "E. Ternovska", "A. Gupta" ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Patterson et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Patterson et al\\.",
      "year" : 2007
    }, {
      "title" : "Reducing inductive definitions to propositional satisfiability",
      "author" : [ "N. Pelov", "E. Ternovska" ],
      "venue" : "International Conference on Logic Programming (ICLP),",
      "citeRegEx" : "Pelov and Ternovska,? \\Q2005\\E",
      "shortCiteRegEx" : "Pelov and Ternovska",
      "year" : 2005
    }, {
      "title" : "Enhancing DLV instantiator by backjumping techniques",
      "author" : [ "S. Perri", "F. Scarcello", "G. Catalano", "N. Leone" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence,",
      "citeRegEx" : "Perri et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Perri et al\\.",
      "year" : 2007
    }, {
      "title" : "Compact propositional encodings of first-order theories",
      "author" : [ "D. Ramachandran", "E. Amir" ],
      "venue" : null,
      "citeRegEx" : "Ramachandran and Amir,? \\Q2005\\E",
      "shortCiteRegEx" : "Ramachandran and Amir",
      "year" : 2005
    }, {
      "title" : "A comparison of different techniques for grounding near-propositional cnf formulae",
      "author" : [ "S. Schulz" ],
      "venue" : "International Florida Artificial Intelligence Research Society Conference (FLAIRS),",
      "citeRegEx" : "Schulz,? \\Q2002\\E",
      "shortCiteRegEx" : "Schulz",
      "year" : 2002
    }, {
      "title" : "Exploiting subformula sharing in automatic analysis of quantified formulas",
      "author" : [ "I. Shlyakhter", "M. Sridharan", "R. Seater", "D. Jackson" ],
      "venue" : "Poster presented at Theory and Applications of Satisfiability Testing (SAT),",
      "citeRegEx" : "Shlyakhter et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Shlyakhter et al\\.",
      "year" : 2003
    }, {
      "title" : "The complexity of decision problems in automata and logic",
      "author" : [ "L.J. Stockmeyer" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Stockmeyer,? \\Q1974\\E",
      "shortCiteRegEx" : "Stockmeyer",
      "year" : 1974
    }, {
      "title" : "Implementation of local grounding for logic programs with stable model semantics",
      "author" : [ "T. Syrjänen" ],
      "venue" : "Tech. rep",
      "citeRegEx" : "Syrjänen,? \\Q1998\\E",
      "shortCiteRegEx" : "Syrjänen",
      "year" : 1998
    }, {
      "title" : "Lparse 1.0 user’s manual. http://www.tcs.hut.fi/Software/smodels/ lparse.ps.gz",
      "author" : [ "T. Syrjänen" ],
      "venue" : null,
      "citeRegEx" : "Syrjänen,? \\Q2000\\E",
      "shortCiteRegEx" : "Syrjänen",
      "year" : 2000
    }, {
      "title" : "Logic Programs and Cardinality Constraints: Theory and Practice. Doctoral dissertation, TKK Dissertations in Information and Computer Science TKK-ICS-D12, Helsinki University of Technology, Faculty of Information and Natural Sciences, Department of Information and Computer Science, Espoo, Finland",
      "author" : [ "T. Syrjänen" ],
      "venue" : null,
      "citeRegEx" : "Syrjänen,? \\Q2009\\E",
      "shortCiteRegEx" : "Syrjänen",
      "year" : 2009
    }, {
      "title" : "Kodkod: A relational model finder",
      "author" : [ "E. Torlak", "D. Jackson" ],
      "venue" : "International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS),",
      "citeRegEx" : "Torlak and Jackson,? \\Q2007\\E",
      "shortCiteRegEx" : "Torlak and Jackson",
      "year" : 2007
    }, {
      "title" : "Principles of database and knowledge-base systems, Vol. I",
      "author" : [ "J.D. Ullman" ],
      "venue" : null,
      "citeRegEx" : "Ullman,? \\Q1988\\E",
      "shortCiteRegEx" : "Ullman",
      "year" : 1988
    }, {
      "title" : "The well-founded semantics for general logic programs",
      "author" : [ "A. Van Gelder", "K.A. Ross", "J.S. Schlipf" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Gelder et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Gelder et al\\.",
      "year" : 1991
    }, {
      "title" : "The idp system. http://www.cs.kuleuven.be/~dtai/krr/ software/idpmanual.pdf",
      "author" : [ "J. Wittocx", "M. Mariën" ],
      "venue" : null,
      "citeRegEx" : "Wittocx and Mariën,? \\Q2008\\E",
      "shortCiteRegEx" : "Wittocx and Mariën",
      "year" : 2008
    }, {
      "title" : "Approximate reasoning in first-order logic theories",
      "author" : [ "J. Wittocx", "M. Mariën", "M. Denecker" ],
      "venue" : "International Conference on Knowledge Representation and Reasoning (KR),",
      "citeRegEx" : "Wittocx et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wittocx et al\\.",
      "year" : 2008
    }, {
      "title" : "Grounding with bounds",
      "author" : [ "J. Wittocx", "M. Mariën", "M. Denecker" ],
      "venue" : "AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Wittocx et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wittocx et al\\.",
      "year" : 2008
    }, {
      "title" : "The idp system: a model expansion system for an extension of classical logic",
      "author" : [ "J. Wittocx", "M. Mariën", "M. Denecker" ],
      "venue" : "In Workshop on Logic and Search (LaSh),",
      "citeRegEx" : "Wittocx et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wittocx et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Examples of systems that rely on grounding can be found in the area of finite first-order model generation (Claessen & Sörensson, 2003; McCune, 2003; East, Iakhiaev, Mikitiuk, & Truszczyński, 2006; Mitchell, Ternovska, Hach, & Mohebali, 2006; Torlak & Jackson, 2007; Wittocx, Mariën, & Denecker, 2008d).",
      "startOffset" : 107,
      "endOffset" : 302
    }, {
      "referenceID" : 19,
      "context" : "Such systems are in turn used as part of theorem provers (Claessen & Sörensson, 2003) and for lightweight software verification (Jackson, 2006).",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 42,
      "context" : "Currently, almost all Answer Set Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele, 2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrjänen, 2000; Syrjänen, 2009).",
      "startOffset" : 102,
      "endOffset" : 209
    }, {
      "referenceID" : 43,
      "context" : "Currently, almost all Answer Set Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele, 2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrjänen, 2000; Syrjänen, 2009).",
      "startOffset" : 102,
      "endOffset" : 209
    }, {
      "referenceID" : 38,
      "context" : "Methods like clause splitting (Schulz, 2002) and partitioning (Ramachandran & Amir, 2005) belong to this category.",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 30,
      "context" : "For instance, this is done in the areas of model generation (Claessen & Sörensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and relational data mining (Krogel et al.",
      "startOffset" : 60,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "For instance, this is done in the areas of model generation (Claessen & Sörensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and relational data mining (Krogel et al., 2003).",
      "startOffset" : 163,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : ", in the context of lightweight verification (Jackson, 2006).",
      "startOffset" : 45,
      "endOffset" : 60
    }, {
      "referenceID" : 33,
      "context" : "This idea of model generation as a declarative problem solving paradigm has been pioneered in the area of ASP (Marek & Truszczyński, 1999; Niemelä, 1999).",
      "startOffset" : 110,
      "endOffset" : 153
    }, {
      "referenceID" : 30,
      "context" : "As shown by Mitchell and Ternovska (2005), it follows from Fagin’s (1974) theorem that model expansion for FO captures NP, in the following sense:",
      "startOffset" : 12,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "As shown by Mitchell and Ternovska (2005), it follows from Fagin’s (1974) theorem that model expansion for FO captures NP, in the following sense:",
      "startOffset" : 59,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : "In this paper we focus on reductions that preserve all models, which is the setting in the ASP paradigm (Marek & Truszczyński, 1999; Niemelä, 1999).",
      "startOffset" : 104,
      "endOffset" : 147
    }, {
      "referenceID" : 36,
      "context" : "The grounder of the dlv system (Perri et al., 2007) and the grounders gringo (Gebser et al.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : ", 2007) and the grounders gringo (Gebser et al., 2007) and GidL (Wittocx, Mariën, & Denecker, 2008b) take this approach.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 42,
      "context" : "Examples of grounders with a bottom-up approach are lparse (Syrjänen, 2000; Syrjänen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al.",
      "startOffset" : 59,
      "endOffset" : 91
    }, {
      "referenceID" : 43,
      "context" : "Examples of grounders with a bottom-up approach are lparse (Syrjänen, 2000; Syrjänen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al.",
      "startOffset" : 59,
      "endOffset" : 91
    }, {
      "referenceID" : 32,
      "context" : "Examples of grounders with a bottom-up approach are lparse (Syrjänen, 2000; Syrjänen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al., 2006).",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 7,
      "context" : "In this section we will extend the refinement algorithm to FO(ID) (Denecker, 2000; Denecker & Ternovska, 2008).",
      "startOffset" : 66,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)’s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc.",
      "startOffset" : 18,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)’s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc. The semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf, 1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes the semantics of all of the above mentioned types of inductive definitions in mathematics.",
      "startOffset" : 18,
      "endOffset" : 489
    }, {
      "referenceID" : 7,
      "context" : "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)’s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc. The semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf, 1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes the semantics of all of the above mentioned types of inductive definitions in mathematics. We borrow the presentation of this semantics from Denecker and Vennekens (2007).",
      "startOffset" : 18,
      "endOffset" : 709
    }, {
      "referenceID" : 7,
      "context" : "Denecker and Vennekens (2007) show that each terminal well-founded induction for ∆ above Ĩ has the same limit, which corresponds to the wellfounded model of ∆ extending Ĩ|Open(∆), and is denoted by wfm∆(Ĩ).",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "MidL (Mariën, Wittocx, & Denecker, 2007) and MiniSAT(ID) (Mariën, Wittocx, Denecker, & Bruynooghe, 2008) take a native approach. Mariën (2009) provides details on the specific form of propositional FO(ID) theories accepted by these solvers, and a method to transform arbitrary propositional FO(ID) theories into this form.",
      "startOffset" : 25,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "But some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to handle full FO bodies.",
      "startOffset" : 45,
      "endOffset" : 58
    }, {
      "referenceID" : 45,
      "context" : "But some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to handle full FO bodies.",
      "startOffset" : 99,
      "endOffset" : 113
    }, {
      "referenceID" : 40,
      "context" : "Indeed, the expression complexity of FO is PSPACE-complete (Stockmeyer, 1974).",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "One can use Goubault’s (1995) method for this purpose.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "We borrow the definition of first-order BDDs from Goubault (1995). Let φ, ψ1 and ψ2 be three formulas.",
      "startOffset" : 50,
      "endOffset" : 66
    }, {
      "referenceID" : 18,
      "context" : "Definition 45 (Goubault, 1995).",
      "startOffset" : 14,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "Goubault (1995) showed that for every FO formula φ there exists a BDT φ′ such that φ and φ′ are equivalent.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "• An implementation of the refinement algorithm using BDDs allows us to use the simplification algorithm for BDDs of Goubault (1995).",
      "startOffset" : 117,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "If φ, ψ and χ[x, y] are represented by BDDs, then a BDD representing ¬φ, ∃x φ, ∀x φ, φ ∧ ψ, φ ∨ ψ and χ[x/x′, y] can be computed efficiently (Bryant, 1986; Goubault, 1995).",
      "startOffset" : 141,
      "endOffset" : 171
    }, {
      "referenceID" : 18,
      "context" : "If φ, ψ and χ[x, y] are represented by BDDs, then a BDD representing ¬φ, ∃x φ, ∀x φ, φ ∧ ψ, φ ∨ ψ and χ[x/x′, y] can be computed efficiently (Bryant, 1986; Goubault, 1995).",
      "startOffset" : 141,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : "Given Iσ, it is possible to find a good estimate for the number of answers to ψ in Iσ (Demolombe, 1980), which is in turn a measure for the precision of ψ.",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "If we use Goubault’s simplification algorithm for BDDs for implementing line 9, the worst case complexity of this step is non-elementary in the size of C(φ) ∨ ψ (Goubault, 1995).",
      "startOffset" : 161,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "For example, the grounder gringo (Gebser et al., 2007) uses a syntactic check to derive bounds: it derives that predicate q of the input vocabulary is a bound for predicate p if p",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 36,
      "context" : "The grounder of the dlv system (Perri et al., 2007) may derive bounds by reasoning on the propositional level.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : "This technique is applied by the grounder psgrnd (East et al., 2006), which uses unit propagation (Davis & Putnam, 1960) and complete one-atom lookahead (Li & Anbulagan, 1997) as propagation methods.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 36,
      "context" : "In this case, a good grounding order can be derived from the dependency graph of the input theory (e.g., Cadoli & Schaerf, 2005; Perri et al., 2007).",
      "startOffset" : 98,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "Shlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level.",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "Shlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level.",
      "startOffset" : 35,
      "endOffset" : 155
    }, {
      "referenceID" : 30,
      "context" : "Clause splitting Clause splitting is a well-known rewriting technique applied in MACE style model generation (McCune, 2003).",
      "startOffset" : 109,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "The simple heuristic to guide clause splitting described by Claessen and Sörensson (2003) can directly be applied to choose which quantifiers to move inside.",
      "startOffset" : 60,
      "endOffset" : 90
    }, {
      "referenceID" : 36,
      "context" : "One of the important methods in the dlv grounder is the use of a backjumping technique (Perri et al., 2007) to efficiently find all instances of a conjunction φ1 ∧ .",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "One of the important methods in the dlv grounder is the use of a backjumping technique (Perri et al., 2007) to efficiently find all instances of a conjunction φ1 ∧ . . . ∧ φn that are possibly true, given (an overestimation of) the possibly true instances of each of the conjuncts φi. In GidL, this backjumping technique is applied to implement line 12 of function groundDisj. Indeed, if φ is the formula φ1 ∧ . . . ∧ φn, then line 12 amounts to finding all possible instances of φ, while the cf-bounds for φ1, . . . , φn provide an overestimation of the possibly true instances of these conjuncts. Similarly, the backjumping technique is applied to improve line 12 of groundConj, where all possibly false instances of a disjunction are calculated. Catalano, Leone, and Perri (2008) present an adaptation of indexing strategies for grounding.",
      "startOffset" : 88,
      "endOffset" : 783
    }, {
      "referenceID" : 37,
      "context" : "Partition-Based Reasoning Ramachandran and Amir (2005) describe a sophisticated grounding technique that can reduce the grounding size of FO theories, depending on the availability of some",
      "startOffset" : 26,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : "mace (McCune, 2003) and paradox (Claessen & Sörensson, 2003) are finite model generators for FO.",
      "startOffset" : 5,
      "endOffset" : 19
    }, {
      "referenceID" : 42,
      "context" : "Currently, there are three ASP grounders: lparse (Syrjänen, 2000; Syrjänen, 2009), gringo (Gebser et al.",
      "startOffset" : 49,
      "endOffset" : 81
    }, {
      "referenceID" : 43,
      "context" : "Currently, there are three ASP grounders: lparse (Syrjänen, 2000; Syrjänen, 2009), gringo (Gebser et al.",
      "startOffset" : 49,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "Currently, there are three ASP grounders: lparse (Syrjänen, 2000; Syrjänen, 2009), gringo (Gebser et al., 2007) and the grounding component of dlv (Perri et al.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 36,
      "context" : ", 2007) and the grounding component of dlv (Perri et al., 2007).",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "This translation is described by Mariën et al. (2004). Next, a (slightly adapted) grounder for ASP is used to ground the logic program.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "This is the approach taken by MXidL (Mariën, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a “bottom-up approach” (see Section 3.",
      "startOffset" : 56,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "This is the approach taken by MXidL (Mariën, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a “bottom-up approach” (see Section 3.2.1). To construct a grounding of a sentence φ, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for φ is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm.",
      "startOffset" : 56,
      "endOffset" : 566
    }, {
      "referenceID" : 7,
      "context" : "This is the approach taken by MXidL (Mariën, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a “bottom-up approach” (see Section 3.2.1). To construct a grounding of a sentence φ, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for φ is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm. kodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg, it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One of the features of kodkod is that it allows a user to give part of a solution to an MX problem as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this information to produce smaller groundings. GidL also allows for a three-valued structure as input. When applying the refinement algorithm, the set of tuples d for which the user indicates that P should be true is then used as initial ct-bound for P instead of ⊥. Similarly for the cf-bound. This leads to more efficient and compact groundings. mace (McCune, 2003) and paradox (Claessen & Sörensson, 2003) are finite model generators for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards. Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also methods that build the grounding incrementally are applied in these systems to avoid recomputing every grounding from scratch. East et al. (2006) developed the grounder psgrnd for MX(PS).",
      "startOffset" : 56,
      "endOffset" : 2077
    }, {
      "referenceID" : 7,
      "context" : "This is the approach taken by MXidL (Mariën, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a “bottom-up approach” (see Section 3.2.1). To construct a grounding of a sentence φ, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for φ is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm. kodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg, it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One of the features of kodkod is that it allows a user to give part of a solution to an MX problem as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this information to produce smaller groundings. GidL also allows for a three-valued structure as input. When applying the refinement algorithm, the set of tuples d for which the user indicates that P should be true is then used as initial ct-bound for P instead of ⊥. Similarly for the cf-bound. This leads to more efficient and compact groundings. mace (McCune, 2003) and paradox (Claessen & Sörensson, 2003) are finite model generators for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards. Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also methods that build the grounding incrementally are applied in these systems to avoid recomputing every grounding from scratch. East et al. (2006) developed the grounder psgrnd for MX(PS). PS is a fragment of FO(ID), extended with pseudo-boolean constraints. As explained above, psgrnd performs reasoning on the ground theory to reduce memory usage and grounding size. The experiments performed by East et al. (2006) show that carefully designed data structures are of key importance to build an efficient grounder.",
      "startOffset" : 56,
      "endOffset" : 2347
    }, {
      "referenceID" : 12,
      "context" : "Finally, because of the large amount of data processed by grounders, carefully designed data structures and an optimized implementation of the core grounding algorithm is very important to achieve fast grounding (East et al., 2006).",
      "startOffset" : 212,
      "endOffset" : 231
    } ],
    "year" : 2010,
    "abstractText" : "Grounding is the task of reducing a first-order theory and finite domain to an equivalent propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning. Besides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounder’s output can often be reduced significantly. A common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. In this paper we present a method to compute and add such redundant information automatically. Our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems. We first present our method for classical first-order logic (FO) theories. Then we extend it to FO(ID), the extension of FO with inductive definitions, which allows for more concise and comprehensive input theories. We discuss implementation issues and experimentally validate the practical applicability of our method.",
    "creator" : "TeX"
  }
}