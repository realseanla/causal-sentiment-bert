{
  "name" : "1406.4679.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Propagation Depth of Local Consistency",
    "authors" : [ "Christoph Berkholz" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A constraint network (X,D,C) consists of a set X of n variables over a domain D of size d and a set of constraints C that restrict possible assignments of the variables. The constraint satisfaction problem (CSP) is to find an assignment of the variables with values from D such that all constraints are satisfied. The constraint satisfaction problem can be solved in exponential time by exhaustive search over all possible assignments. Constraint propagation is a technique to speed up the exhaustive search by restricting the search space in advance. This is done by iteratively propagating new constraints that follow from previous ones. Most notably, in local consistency algorithms the overall goal is to propagate new constraints to achieve some kind of consistency on small parts of the constraint network. Additionally, if local inconsistencies were detected, it follows that the constraint network is also globally inconsistent and hence unsatisfiable.\nThe k-consistency test [8] is a well-known local consistency technique, which enforces that every satisfying (k − 1)-partial assignment can be extended to a satisfying k-partial assignment. At the beginning, all partial assignments that violate a constraint were marked as inconsistent. Then the following inference rule is applied iteratively:\nIf h is a consistent `-partial assignment (` < k) for which there exists a variable x ∈ X such that h ∪ {x 7→ a} is inconsistent for all a ∈ D, then mark h and all its extensions as inconsistent.\nar X\niv :1\n40 6.\n46 79\nv1 [\ncs .A\nI] 1\n8 Ju\nn 20\n14\nAfter at most nk−1dk−1 propagation steps this procedure stops. If the empty assignment becomes inconsistent, we say that (strong) k-consistency cannot be established. In this case we know that the constraint network is globally inconsistent. Otherwise, if k-consistency can be established, we can use the propagated constraints to restrict the search space for a subsequent exhaustive search. There are several different k-consistency algorithms in the literature, especially for k = 2 (arc consistency) and k = 3 (path consistency), which all follow this propagation scheme. The main difference between these algorithms are the underlying data structure and the order in which they apply the propagation rule. It seems plausible to apply the propagation rule in parallel in order to detect local inconsistencies in different parts of the constraint network at the same time. Indeed, this intuition has been used to design parallel arc and path consistency algorithms [15,16]. On the other hand, the k-consistency test is known to be PTIME-complete [10,11] and hence not efficiently parallelizable (unless NC=PTIME). The main bottleneck for parallel approaches are the sequential dependencies in the propagation rule: some assignments will be marked as inconsistent after some other assignments became inconsistent.\nFor 2-consistency the occurrence of long chains of sequential dependencies has been observed very early [6] and was recently studied in depth in [4]. There are simple constraint networks for which 2-consistency requires Ω(nd) nested propagation steps. Ladkin and Maddux [14] used algebraic techniques to show that 3-consistency requires Ω(n2) nested propagation steps on binary constraint networks with constant domain. We extend these previous results and obtain a complete picture of the propagation depth of k-consistency. Our main result (Theorem 1) states that for every constant k ≥ 2 and given integers n, d there is a constraint network with n variables and domain size d such that every k-consistency algorithm has to perform Ω(nk−1dk−1) nested propagation steps. This lower bound is optimal as it is matched by the trivial upper bound nk−1dk−1 on the overall number of propagation steps. It follows that every parallel propagation algorithm for k-consistency has a worst case time complexity of Ω(nk−1dk−1). Since the best-known running time of a sequential algorithm for k-consistency is O(nkdk) [5] it follows that no significant improvement over the sequential algorithm is possible."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "As first pointed out by Feder and Vardi [7] the CSP is equivalent to the structure homomorphism problem where two finite relational structures A and B are given as input. The universe V (A) of structure A corresponds to the set of variables X and the universe V (B) of structure B corresponds to the domain D. The constraints are encoded into relations such that every homomorphism from A to B corresponds to a solution of the CSP. For the rest of this paper we mainly stick to this definition as it is more convenient to us. In fact, our main result benefits to a large extend from the fruitful connection between these two viewpoints.\nII\nIn the introduction we have presented k-consistency as a propagation procedure on constraint networks. Below we restate the definition in terms of a formal inference system (which is inspired by the proof system in [1] and is a generalization of [4]). This view allows us to gain insight into the structure of the propagation process and to formally state our main theorem afterwards. At the end of this section we provide a third characterization of k-consistency in terms of the existential pebble game, which is the tool of our choice in the proof of the main theorem."
    }, {
      "heading" : "2.1 CSP-refutations",
      "text" : "Given two σ-structures A and B, every line of our derivation system is a partial mapping from V (A) to V (B). The axioms are all partial mappings p : V (A) → V (B) that are not partial homomorphisms. We have the following derivation rule to derive a new inconsistent assignment p. For all partial mappings p′i ⊆ p, x ∈ V (A) and V (B) = {a1, . . . , an}:\np′1 ∪ {x 7→ a1} · · · p′n ∪ {x 7→ an} p\n(1)\nA CSP-derivation of p is a sequence (p1, . . . , p` = p) such that every pi is either an axiom or derived from lines pj , j < i, via the derivation rule (1). A CSPrefutation is a CSP-derivation of ∅. Every derivation of p can naturally be seen as a directed acyclic graph (dag) where the nodes are labeled with lines from the derivation, one node of in-degree 0 is labeled with p and all nodes of out-degree 0 are labeled with axioms. If pi is derived from pj1 , . . . , pjn using (1), then there is an arc from pi to each pj1 , . . . , pjn .\nGiven a CSP-derivation P , we let Prop(P ) be the set of propagated mappings p ∈ P , i. e. all lines in the derivation that are not axioms. We define the width of a derivation P to be width(P ) = maxp∈Prop(P ) |p|.1 Furthermore, depth(P ) denotes the depth of P which is the number of edges on the longest path in the dag associated with P . This measure characterizes the maximum number of nested propagation steps in P . Since CSP-derivations model the propagation process mentioned in the introduction, there is a CSP-refutation of width k − 1 if and only if k-consistency cannot be established.\nFurthermore, every propagation algorithm produces some CSP-derivation P . The total number of propagation steps performed by this algorithm is |Prop(P )| and the maximum number of nested propagation steps is depth(P ). Let A and B be two relational structures such that k-consistency cannot be established. We define the propagation depth depthk(A,B) := minP depth(P ) where the minimum is taken over all CSP-refutations P of width at most k − 1. Hence, the depthk(A,B) ≤ |V (A)|k−1|V (B)|k−1 is the number of sequential propagation steps that have to be performed by any sequential or parallel propagation algorithm for k-consistency.\n1 Note that this implies |p| ≤ width(P ) + 1 for all axioms p used in the derivation P . However, the size of the axioms can always be bounded by the maximum arity of the relations in A and B.\nIII"
    }, {
      "heading" : "2.2 Results and Related Work",
      "text" : "Our main theorem is a tight lower bound on the propagation depth.\nTheorem 1. For every integer k ≥ 2 there exists a constant ε > 0 and two positive integers n0, m0 such that for every n ≥ n0 and m ≥ m0 there exist two binary structures An and Bm with |V (An)| = n and |V (Bm)| = m such that depthk(An,Bm) ≥ εnk−1mk−1.\nWe are aware of two particular cases that have been discovered earlier. First, for the case k = 2 (arc consistency) the theorem can be shown by rather simple examples that occurred very early in the AI-community. The structure of this exceptional case is discussed in deep in a joint work of Oleg Verbitsky and the author of this paper [4]. Second, for k = 3 Ladkin and Maddux [14] showed that there is a fixed finite binary structure B and an infinite sequence of binary structures Ai such that depth\n3(Ai,B) = Ω(|V (Ai)|2). They used this result to argue that every parallel propagation algorithm for path consistency needs at least a quadratic number of steps. This is tight only for fixed structures B, Theorem 1 extends their result to the case when B is also given as input.\nOther related results investigate the decision complexity of the k-consistency test. To address this more general question one analyzes the computational complexity of the following decision problem.\nk-Cons\nInput : Two binary relational structures A and B. Question: Can k-consistency be established for A and B?\nKasif [10] showed that 2-Cons is complete for PTIME under LOGSPACE reductions. Kolaitis and Panttaja [11] extended this result to every fixed k ≥ 2. Moreover, they established that the problem is complete for EXPTIME if k is part of the input. In [3] the author showed that k-Cons cannot be decided in O(n k−3 12 ) on deterministic multi-tape Turing machines, where n is the overall input size. Hence, any algorithm solving k-Cons (regardless of whether it performs constraint propagation or not) cannot be much faster than the standard propagation approach. It also follows from this result that, parameterized by the number of pebbles k, k-Cons is is complete for the parameterized complexity class XP. It is also worth noting that Gaspers and Szeider [9] investigated the parameterized complexity of other parameterized problems related to k-consistency."
    }, {
      "heading" : "2.3 The Existential Pebble Game",
      "text" : "In this paragraph we introduce a third view on the k-consistency heuristic in terms of a combinatorial pebble game. The existential k-pebble game [12] is played by two players Spoiler and Duplicator on two relational structures A and B. There are k pairs of pebbles (p1, q1), . . . , (pk, qk) and during the game Spoiler moves the pebbles p1, . . . , pk to elements of V (A) and Duplicator moves\nIV\nthe pebbles q1, . . . , qk to elements of V (B). At the beginning of the game, Spoiler places pebbles p1, . . . , pk on elements of V (A) and Duplicator answers by putting pebbles q1, . . . , qk on elements of V (B). In each further round Spoiler picks up a pebble pair (pi, qi) and places pi on some element in V (A). Duplicator answers by moving the corresponding pebble qi to one element in V (B). Spoiler wins the game if he can reach a position where the mapping defined by pi 7→ qi is not a partial homomorphism from A to B.\nThe connection between the existential k-pebble game and the k-consistency heuristic was made by Kolaitis and Vardi [13]. They showed that one can establish k-consistency by computing a winning strategy for Duplicator. Going a different way, the next lemma states that there is also a tight correspondence between Spoiler’s strategy and CSP-refutations. The proof is a straightforward induction over the depth and given in Appendix A.1.\nLemma 2. Let A and B be two relational structures. There is a CSP-refutation for A and B of width k − 1 and depth d if and only if Spoiler has a strategy to win the existential k-pebble game on A and B within d rounds.\nUsing this lemma it suffices to prove lower bounds on the number of rounds in the existential pebble game in order to prove Theorem 1. To argue about strategies in the existential pebble game we use the framework developed in [3]. We start with a formal definition of strategies for Duplicator.\nDefinition 3. A critical strategy for Duplicator in the existential k-pebble game on structures A and B is a nonempty family H of partial homomorphisms from A to B together with a set crit(H) ⊆ H of critical positions satisfying the following properties:\n1. All critical positions are (k − 1)-partial homomorphisms. 2. If h ∈ H and g ⊂ h, then g ∈ H. 3. For every g ∈ H\\ crit(H), |g| < k, and every x ∈ V (A) there is an a ∈ V (B)\nsuch that g ∪ {x 7→ a} ∈ H.\nIf crit(H) = ∅, then H is a winning strategy. The setH is the set of good positions for Duplicator (therefore they are all partial homomorphisms). Non-emptiness and the closure property (2.) ensure that H contains the start position ∅. Furthermore, the closure property guarantees that the current position remains a good position for Duplicator when Spoiler picks up pebbles. The extension property (3.) ensures that, from every non-critical position, Duplicator has an appropriate answer if Spoiler puts a free pebble on x. It follows that if there are no critical positions, then Duplicator can always answer accordingly and thus wins the game. Otherwise, if Spoiler reaches a critical position, then Duplicator may not have an appropriate answer and the game reaches a critical state. In the next lemma we describe how to use critical strategies to prove lower bounds on the number of rounds.\nLemma 4. If H1, . . . ,Hl is a sequence of critical strategies on the same pair of structures and for all i < l and all p ∈ crit(Hi) it holds that p ∈ Hj \\ crit(Hj) for some j ≤ i+ 1, then Duplicator wins the l-round existential k-pebble game.\nV\nProof. Starting with i = 1, Duplicator answers according to the extension property of Hi, if the current position p is non-critical in Hi. Otherwise, p is noncritical in Hj for some j ≤ i+ 1 and Duplicator answers according to the extension property of Hj . This allows Duplicator to survive for at least l rounds. ut\nThe two structures A and B we construct are vertex colored graphs. They are built out of smaller graphs, called gadgets. Every gadget Q consists of two graphs QS and QD for Spoiler’s and Duplicator’s side, respectively. Hence, QS and QD will be subgraphs of A and B in the end. The gadgets contain boundary vertices, which are the vertices shared with other gadgets. To combine two strategies on two connected gadgets we need to ensure that the strategies agree on the boundary of the gadgets. Formally, let a boundary function of a strategy H on a gadget Q be a mapping β from the boundary of QS to the boundary of QD such that β(z) = h(z) for all h ∈ H and all z in the domain of β and h. We say that two strategies G and H on gadgets Q and Q′ are connectable, if their boundary functions agree on the common boundary vertices of Q and Q′. If G and H are two connectable critical strategies on gadgets Q = (QS , QD) and Q ′ = (Q′S , Q ′ D) it is not hard to see that the composition\nG ] H = {g ∪ h | g ∈ G, h ∈ H}\nis a critical strategy on QS∪Q′S and QD∪Q′D with crit(G]H) = crit(G)∪crit(H). Intuitively, playing according to the strategy G ] H on Q and Q′ means that Duplicator uses strategy G on Q and strategy H on Q′."
    }, {
      "heading" : "3 The Construction",
      "text" : ""
    }, {
      "heading" : "3.1 Overview of the Construction",
      "text" : "In this section we prove Theorem 1 for k ≥ 3. We let k := k−1 ≥ 2 and construct two vertex colored graphs An and Bm with O(n) and O(m) vertices such that Spoiler needs Ω(nkmk) rounds to win the existential (k + 1)-pebble game. We color the vertices of both graphs such that the colors partition the vertex set into independent sets, i. e. every vertex gets one color and there is no edge between vertices of the same color. The basic building blocks in our construction are sets of vertices which allow to store nkmk partial homomorphisms with k pebbles.\nWe introduce vertices xij (i ∈ [k], j ∈ [n]) in An and vertices xij (i ∈ [k], j ∈ [m] ∪ {0}) in Bm. For every i ∈ [k] the vertices xij form a block and are\nVI\ncolored with the same color (say Pxi), which is different from any other color in the entire construction. The vertices xi0 in structure Bm play a special role in our construction and are visualized by instead of in the pictures. However, they are colored with the same color Pxi as the other vertices x i j . Because of the coloring, Duplicator has to answer with some xij′ whenever Spoiler pebbles a vertex xij . Since there are nm positions for one pebble pair on vertices in one block, we get nkmk positions if every block has exactly one pebble pair on vertices. The vertices are used by Duplicator whenever Spoiler does not play the intended way. That is, if Spoiler pebbles a vertex in block i that he is not supposed to pebble now, then Duplicator answers with xi0. The construction will have the property that this is always a good situation for Duplicator.\nTo describe pebble positions on such vertex blocks, we define mappings a : [k]→ [n] and b : [k]→ [m] and call the pebble position {(xia(i), x i b(i)) | i ∈ [k]} valid. If such valid position is on the board, then Duplicator answers with xib(i) if Spoiler pebbles xia(i) and with x i 0 if Spoiler pebbles x i j for some j 6= a(i). We also need to name positions where Duplicator answers with xi0 for every vertex in block i and let T be the set of blocks where this happens. For a : [k] → [n], b : [k]→ [m] and T ⊆ [k] we call q = (a, b, T ) a configuration. The configuration q is valid if T = ∅ and invalid otherwise. For every configuration q and a set of xij vertices as in Figure 1 we define the following homomorphism that describes Duplicator’s behavior:\nhxq(x i j) = { xib(i), if j = a(i) and i /∈ T, xi0, otherwise.\nBy hx0 we denote the homomorphism h x 0(x i j) := x i 0 for all i ∈ [k], j ∈ [n]. We say that a position of (at most k+1) pebble pairs on these vertices is invalid if it is a subset of hxq for some invalid configuration q. For valid configurations q = (a, b, ∅) we say “q on x” to name the valid pebble position {(xia(i), x i b(i)) | i ∈ [k]}. Note that valid pebble positions are not invalid.2\nIn the entire construction there is one unique copy of the xij-vertices, which are denoted by xij . Our goal is to force Spoiler to pebble every valid position on x before he wins the game. He is supposed to do so in a specific predefined order. To fix this order we define a bijection α between valid configurations (a, b, ∅) and the numbers 0, . . . , nkmk − 1:\nα(q) := mk k∑ i=1 (a(i)− 1)nk−i + k∑ i=1 (b(i)− 1)mk−i.\nThus, α(q) is the rank of the tuple (a(1), . . . , a(k), b(1), . . . , b(k)) in lexicographical order. If α(q) < nkmk − 1, we define the successor q+ = (a+, b+, ∅) to be the unique valid configuration satisfying α(q+) = α(q) + 1. In the sequel we introduce gadgets to make sure that:\n2 There are pebble positions on the xij vertices that are neither valid nor invalid. However, such positions will not occur in our strategies.\nVII\n– Spoiler can reach the position α−1(0) on x from ∅, – Spoiler can reach α−1(i+ 1) on x from α−1(i) on x and – Spoiler wins from α−1(nkmk − 1) on x.\nIf we have these properties, we know that Spoiler has a winning strategy in the (k+ 1)-pebble game. To show that Spoiler needs at least nkmk rounds we argue that this is essentially the only way for Spoiler to win the game.\nWe start with an overview of the gadgets and how they are glued together to form the structures An and Bm. The boundary of our gadgets consists of input vertices and output vertices. For every gadget the set of input (output) vertices is a copy of the vertex set in Figure 1 and we write xij (y i j) to name them. This enables us to glue together the gadgets at their input and output vertices. The overall construction for the graph Bm is shown in Figure 2. The schema for An is similar, it contains Spoiler’s side of the corresponding gadgets which are glued together the same way as in Bm (just replace m by n and drop the vertices). There are four types of gadgets: the initialization gadget, the winning gadget, several increment gadgets and the switch.\nThe initialization gadget ensures that Spoiler can reach α−1(0) on x, i. e. the pebble position {(x11, x11), . . . , (xk1 , xk1)}. This gadget has only output boundary vertices and is used by Spoiler at the beginning of the game. There are increment\nVIII\ngadgets INClefti and INC right i for all i ∈ [k]. The input vertices of every increment gadget are identified with the x vertices as depicted in Figure 2. The increment gadgets (all together) ensure that Spoiler can increment a configuration. More precisely, for every valid configuration q with α(q) < nkmk − 1, there is one increment gadget INC such that Spoiler can reach q+ on the output of INC from q on the input. Every increment gadget is followed by a copy of the switch. The input of 2k switches is identified with the output of the 2k increment gadgets and the output of these switches is identified with a unique block of y-vertices and the input of one additional single switch (see Figure 2). The output of this switch is in turn identified with the unique block of x-vertices. The switches are used to perform the transition in the game from α−1(i) on x to α−1(i + 1) on x. Spoiler can pebble a valid position through one switch: from q on the input of a switch Spoiler can reach q on the output of that switch. Hence, Spoiler can simply pebble the incremented position α−1(i + 1) from the output of an increment gadget through two switches to the x-block.\nFinally, the winning gadget ensures that from α−1(nkmk − 1) on x Spoiler wins the game. The winning gadget has only input vertices, which are identified with the x-vertices. From α−1(nkmk−1) on the input, Spoiler can win the game by playing on this gadget. On the other hand, the gadget ensures that Spoiler can only win from α−1(nkmk − 1) on x and Duplicator does not lose from any other configuration on x."
    }, {
      "heading" : "3.2 The Gadgets",
      "text" : "We now describe the winning gadget and the increment gadgets in detail and provide strategies for Spoiler and Duplicator on them. Afterwards we briefly discuss the switch and the initialization gadget. In the next section we combine the partial strategies on the gadgets to prove Theorem 1.\nThe winning gadget is shown in Figure 3. On Spoiler’s side there is just one additional vertex a, which is connected to xin for all i ∈ [k]. On Duplicator’s side there are k additional vertices ai, i ∈ [k]. Every ai is connected to all input vertices except xim. We use one new vertex color to color the vertex a and all vertices ai. From the position {(x1n, x1m), . . . , (xkn, xkm)} “α−1(nkmk − 1) on x” Spoiler wins the game by placing the (k + 1)st pebble on a. Duplicator has to answer with some ai (because of the coloring). Since there is an edge between x i n and a in WINS but none between x i m and ai in WIND, Spoiler wins immediately. It is also not hard to see that for any other position where at least one pebble pair (xjn, x j m) is missing Duplicator can survive by choosing aj .\nThe increment gadgets enable Spoiler to reach the successor q+ from q. Recall that we identify every valid configuration q = (a, b, ∅) with the tuple (a(1), . . . , a(k), b(1), . . . , b(k)) ∈ [n]k× [m]k and define α(q) to be the rank (from 0 to nkmk−1) of this tuple in lexicographical order. Let q be a valid configuration with α(q) < nkmk − 1 and successor q+ = (a+(1), . . . , a+(k), b+(1), . . . , b+(k)). We use two types of increment gadgets, left and right, depending on whether the left-hand side of the tuple changes after incrementation or not. There are k\nIX\nincrement gadgets of each type. Spoiler uses them depending on which position the last carryover occurs. If\nq = (a(1), . . . , a(k), b(1), . . . , b(`− 1), b(`) < m, m, . . . ,m) and hence q+ = (a(1), . . . , a(k), b(1), . . . , b(`− 1), b(`) + 1, 1, . . . , 1),\nthen Spoiler uses the increment gadget INCright` to reach q + on the output from q on the input. If\nq = (a(1), . . . , a(`− 1), a(`) < n, n, . . . , n, m, . . . ,m) and hence q+ = (a(1), . . . , a(`− 1), a(`) + 1, 1, . . . , 1, 1, . . . , 1),\nthen Spoiler uses INCleft` . Thus, for every valid configuration q with α(q) < nkmk−1 there is exactly one applicable increment gadget. The increment gadgets\nare shown in Figure 4. All input vertices xij have at most one output vertex y i j′\nX\nas neighbor. Furthermore, if the gadget is applicable to a valid configuration q = (a, b, ∅), then the unique neighbor of xia(i) is y i a+(i) and the unique neighbor of xib(i) is y i b+(i) . This enables Spoiler to reach q+ on the output from q on the input by the following procedure. First, Spoiler places the remaining pebble on y1a+(1). Since this vertex is adjacent to x 1 a(1), Duplicator has to answer with y1 b+(1) , the only vertex that is adjacent to x1b(1). Afterwards, Spoiler picks up the pebble pair from (x1a(1), x 1 b(1)). On the second block Spoiler proceeds the same way: he pebbles y2a+(2), forces the position (y 2 a+(2), y 2 b+(2) ) and picks up the pebbles from (x2a(2), x 2 b(2)). By iterating this procedure Spoiler reaches q\n+ on the output.\nIf Spoiler tries to move a configuration through one increment gadget that is not applicable, then Duplicator can answer with an invalid configuration on the output as follows. On the one hand, if the gadget is not applicable because some b(i) does not have the specified value, then xib(i) is adjacent to y i 0. On the other hand, if some a(i) has the wrong value, then xia(i) is not adjacent to an output vertex. In both cases Duplicator can safely pebble yi0 if Spoiler queries some y i j and hence maintain an invalid output position. The next lemma provides these strategies, a formal proof is given in the full version of the paper.\nLemma 5. Let q = (a, b, T ) be a configuration and INC an increment gadget.\n1. If INC is applicable to q, then Spoiler can reach q+ on the output from q on the input of INC. 2. If INC is applicable to q, then there is a winning strategy for Duplicator with boundary function hxq on the input and h y q+ on the output. 3. If INC is not applicable to q, then there is a winning strategy for Duplicator with boundary function hxq on the input and h y qinv on the output for an invalid\nconfiguration qinv.\nThe switch is an extension of the “multiple input one-way switch” defined in [3] (which in turn is a generalization of [11]). The difference is that the old switch can only be used for the case n = 1. It requires some work to adjust the old switch to make it work for the more general setting. The technical details of the switch are shifted to Appendix A.2. At this point we focus on a high level description of the strategies and formalize them in Lemma 6. The proof of this lemma can also be found in Appendix A.2.\nAs mentioned earlier, Spoiler can simply move a valid position from the input to the output of the switch (Lemma 6(i)). Duplicator has a winning strategy called output strategy, where any position is on the output and hx0 is on the input (Lemma 6(ii)). This ensures that Spoiler cannot move backwards to reach q on the input from q on the output. Hence, this strategy forces Spoiler to play through the switches in the intended direction (as indicated by arrows Figure 2). Furthermore, for every invalid qinv Duplicator has a winning strategy where hxqinv is on the input and h y 0 is on the output (Lemma 6(iii)), which ensures that Spoiler cannot move invalid positions through the switch. This strategy is used by Duplicator whenever Spoiler plays on an increment gadget that is\nXI\nnot applicable. By Lemma 5, Duplicator can force an invalid configuration on the output of that increment gadget and hence on the input of the subsequent switch.\nTo ensure that Spoiler picks up all pebbles when reaching q on the output from q on the input, Duplicator has a critical input strategy with q on the input and hy0 on the output (Lemma 6(iv)). The critical positions are either contained in an output strategy, where q is on the output, or (for technical reasons) in a restart strategy. If Duplicator plays according to this input strategy, the only way for Spoiler to bring q from the input to the output is to pebble an output critical position inside the switch (using all the pebbles) and force Duplicator to switch to the corresponding output strategy.\nLemma 6. For every configuration q = (a, b, T ), the following statements hold in the existential (k + 1)-pebble game on the switch:\n(i) If q is valid, then Spoiler can reach q on the output from q on the input. (ii) Duplicator has a winning strategy Houtq with boundary function hx0 ∪ h y q. (iii) If q is invalid, then Duplicator has a winning strategy Hrestartq with boundary function hxq ∪ h y 0. (iv) If q is valid, then Duplicator has a critical strategy Hinq with boundary function hxq ∪h y 0 and sets of restart critical positions Crestart-critq,t (for t ∈ [k]) and\noutput critical positions Cout-critq such that: (a) crit(Hinq ) = ⋃ t∈[k] Crestart-critq,t ∪ Cout-critq , (b) Crestart-critq,t ⊆ Hrestart(a,b,{t}) and (c) Cout-critq ⊆ Houtq .\nAt the beginning of the game we want that Spoiler can reach the start configuration α−1(0) on x, which is the pebble position {(x11, x11), . . . , (xk1 , xk1)}. To ensure this, we use the initialization gadget and identify its output vertices yij with the block of xij vertices. As for the switch, this gadget is an extension of the initialization gadget presented in [3]. We now describe the strategies on the initialization gadget and formalize them in Lemma 7. A detailed description of the gadget and a proof of Lemma 7 is given in Appendix A.3. The main property of the gadget is that Spoiler can reach the start position q at the boundary (i) and Duplicator has a corresponding counter strategy (ii) in this situation. Furthermore, if an arbitrary position occurs at the boundary during the game, Duplicator has a strategy to survive (iii). This is only a critical strategy, but Duplicator can switch to the initial strategy (hence “restart” the game) if Spoiler moves to one of the critical positions.\nLemma 7. Let q = α−1(0). The following holds in the existential (k+ 1)-pebble game on INIT:\n(i) Spoiler can reach q on the output. (ii) There is a winning strategy Iinit for Duplicator with boundary function hyq. (iii) For every (valid or invalid) configuration q′ there is a critical strategy Iinitq′\nwith boundary function hyq′ and crit(Iinitq′ ) ⊆ Iinit.\nXII"
    }, {
      "heading" : "3.3 Proof of Theorem 1",
      "text" : "The size of the vertex set in every gadget is linear in n on Spoiler’s side and linear in m on Duplicator’s side. Since the overall construction uses a constant number of gadgets it follows that |V (An)| = O(n) and |V (Bm)| = O(m). To prove the lower bound on the number of rounds Spoiler needs to win the existential (k+1)pebble game we provide a sequence of critical strategies in Lemma 8 satisfying the properties stated in Lemma 4. For a critical strategy S we let Ŝ := S\\crit(S).\nLemma 8. Spoiler has a winning strategy in the existential (k+ 1)-pebble game on An and Bm. Furthermore, there is a sequence of critical strategies for Duplicator Gstart,F1,G1,F2,G2, . . . ,Gnkmk−2,Fnkmk−1 such that\ncrit(Gstart) ⊆ F̂1,\ncrit(Gi) ⊆ F̂i+1 ∪ Ĝstart, 1 ≤ i ≤ nkmk − 2,\ncrit(Fi) ⊆ Ĝi ∪ Ĝstart, 1 ≤ i ≤ nkmk − 2.\nProof (Proof of Theorem 1). For k = 2 the theorem follows from [4]. For k ≥ 3 consider the structures An and Bm (for k = k − 1) defined above. By Lemma 8 Spoiler wins the existential k-pebble game on An and Bm. Furthermore, it follows via Lemma 4 that Spoiler needs at least Ω(nk−1mk−1) rounds to win the game. To get structures with exactly n and m vertices we take the largest n′,m′ such that |V (An′)| ≤ n, |V (Bm′)| ≤ m and fill up the structures with an appropriate number of isolated vertices. ut\nProof (Proof of Lemma 8). To show that Spoiler has a winning strategy it suffices to prove the following three statements:\n(1) Spoiler can reach the position α−1(0) on x from ∅, (2) Spoiler can reach α−1(i+ 1) on x from α−1(i) on x (for i < nkmk − 1) and (3) Spoiler wins from α−1(nkmk − 1) on x.\nAssertion (1) follows from Lemma 7 and (3) is ensured by the winning gadget. For (2), Spoiler starts with the position q = α−1(i) on x. Since i < nkmk − 1 there is exactly one increment gadget applicable to q. Spoiler uses Lemma 5 to reach q+ = α−1(i + 1) on the output of that gadget. By applying Lemma 6.(i) twice, Spoiler can pebble q+ through the two switches to the x vertices.\nTo define the sequence of global critical strategies we combine the partial critical strategies on the gadgets using the ]-operator. There are three types of strategies: Gstart, Fi and Gi. To define Gi we let q = α−1(i). Duplicator plays according to hxq on x and according to h y 0 on y. She plays according to this strategy in the case when Spoiler reaches “q on x”. The critical strategy Gi is the combination of the following (pairwise connectable) strategies on the gadgets:\n– The critical strategy I initq on the initialization gadget (Lemma 7). – The winning strategy with boundary hxq and h y q+ on the increment gadget\napplicable to q (Lemma 5).\nXIII\n– The critical input strategy Hinq+ on the switch following the applicable increment gadget (Lemma 6). – The winning strategy with boundary hxq and h y qinv on the other increment\ngadgets not applicable to q (Lemma 5). – The winning strategy Hrestartqinv on the switches following the inapplicable in-\ncrement gadgets (Lemma 6). Here, qinv is the invalid configuration on the output of the corresponding increment gadget.\n– The output winning strategy Houtq on the single switch (Lemma 6).\nIf in the above setting Spoiler increments q through the applicable increment gadget and moves q+ = α−1(i + 1) through the subsequent switch, then Duplicator switches to the strategy Fi+1. To define Fi we fix q = α−1(i). In this strategy, Duplicator plays according to hx0 on x and according to h y q on y. This critical strategy is the combination of the following strategies on the gadgets.\n– The critical strategy I init0 on the initialization gadget. – The winning strategy with boundary hx0 and h y 0 on the increment gadgets. – The output strategy Houtq on the switches following the increment gadgets. – The critical input strategy Hinq on the single switch.\nThe critical positions in the strategies Gi and Fi are inside the switches and the initialization gadget. Recall that by Lemma 6.(iv) the critical positions on the switch can be divided into restart critical positions and output critical positions. Furthermore, all output critical positions of Gi, which are inside the switch following the applicable increment gadget, are contained as non-critical positions in Fi+1. All output critical position in Fi, which are inside the single switch, are contained as non-critical positions in Gi. Now we define Gstart, which contains all other critical positions of Gi and Fi. The critical strategy Gstart is the union of several other global strategies. The first one is Ginit, which is defined as G0 except that it contains the winning strategy I init on the initialization gadget. Thus, by Lemma 7, it contains every critical position on the initialization gadget as non-critical position. Note that the output critical positions of Ginit are contained as non-critical positions in F1. Since Ginit handles the critical positions on the initialization gadget and we discussed the output critical positions on the switches, it remains to consider the restart critical positions of the strategies. For this we construct a strategy Grestarti to handle the restart critical positions of Gi (for i ≥ 1) and of Ginit (for i = 0). Furthermore, we define for every i ≥ 1 a strategy F restarti to handle the restart critical positions of Fi.\nFor 0 ≤ i ≤ nkmk − 2 and t ∈ [k] we let q = α−1(i) = (a, b, ∅) and qt be the invalid configuration (a, b, {t}). The global strategy Grestarti,t is the combination of the following strategies on the gadgets.\n– The critical strategy I initqt on the initialization gadget. – The winning strategy with boundary hxqt and h y qinv on the increment gadgets.\nNote that, since qt is invalid, no increment gadget is applicable to qt. – The winning strategy Hrestartqinv on the switches following the increment gad-\ngets. Again, qinv is the invalid configuration at the output of the preceding increment gadget.\nXIV\n– The output winning strategy Houtqt on the single switch. Finally, we let Grestarti := ⋃ i∈[k] Grestarti,t . Note that by Lemma 6.(iv) every restart critical position of Gi is contained in Grestarti and every restart critical position of Ginit is contained in Grestart0 . Now we define for 1 ≤ i ≤ nkmk − 2, t ∈ [k], q = α−1(i) = (a, b, ∅) and qt := (a, b, {t}) the strategy F restarti,t analogously. It consists of the following partial strategies.\n– The critical strategy I init0 on the initialization gadget. – The winning strategy with boundary hx0 and h y 0 on the increment gadgets. – The winning strategy Hrestart0 on the switches after the increment gadgets. – The winning strategy Hrestartqt on the single switch.\nIn the end we let F restarti be the union of all F restarti,t . Note that every restart critical position of Fi is contained as non-critical position in F restarti . Finally, let\nGstart := Ginit ∪ ⋃\n0≤i≤nkmk−2\nGrestarti ∪ ⋃\n1≤i≤nkmk−2\nF restarti .\nTo conclude the proof note that the critical positions of Grestarti and F restarti are inside the initialization gadget and hence contained in Ĝinit. Thus they are not critical positions of Gstart. Hence, crit(Gstart) = crit(Ginit) ⊆ F̂1. ut"
    }, {
      "heading" : "4 Conclusion",
      "text" : "We have proven an optimal lower bound of Ω(nk−1dk−1) on the number of nested propagation steps in the k-consistency procedure on constraint networks with n variables and domain size d. It follows that every parallel propagation algorithm has to perform at least Ω(nk−1dk−1) sequential steps. Using (n+ d)O(k) processors (one for every instance of the inference rule), k-consistency can be computed in O(nk−1dk−1) parallel time, which is optimal for propagation algorithms. In addition, the best sequential algorithm runs in O(nkdk). The overhead compared to the parallel approach is mainly caused by the time needed to search for the next inconsistent assignment that might be propagated – and this seems to be the only task that can be parallelized.\nAlthough we have proven an optimal lower bound in the general setting, it might be interesting to investigate the propagation depth of k-consistency on restricted classes of structures. Especially, if in such cases the propagation depth is bounded by O(log(n + d)), we know that k-consistency is in NC and hence parallelizable.\nXV"
    }, {
      "heading" : "A Appendix",
      "text" : "This appendix contains proofs skipped in the main text. We first give a simple proof of the correspondence between the existential pebble game and CSPrefutations (Subsection A.1) followed by the definition and strategies on the switch (Subsection A.2) and the initialization gadget (Subsection A.3).\nA.1 Proof of Lemma 2\nLemma 9. (Reminder of Lemma 2) Let A and B be two relational structures. There is a CSP-refutation for A and B of width k − 1 and depth d if and only if Spoiler has a strategy to win the existential k-pebble game on A and B within d rounds.\nProof. For one direction assume that there is a CSP-refutation P of depth d and width k − 1. We show by induction over the depth that every partial mapping p of depth i occurring in the refutation defines a position of pebbles from which Spoiler can win the existential k-pebble game within i rounds. It follows that Spoiler can win the game from ∅ (all pebbles off the board) within d rounds. All mappings of depth i = 0 are axioms and thus not partial homomorphisms. Hence, Spoiler wins immediately. For the induction step assume that p has depth i > 0. Therefore, |p| < k and p is derived from p′1 ∪{x 7→ a1}, . . . , p′n ∪{x 7→ an} (p′j ⊆ p) each of depth < i. Spoiler can now reach one of these positions within one round by placing the remaining pebble on x. Depending on Duplicator’s choice (some aj ∈ V (B)) Spoiler moves to p′j ∪ {x 7→ aj} by picking up the pebbles in p \\ p′j . By induction assumption, Spoiler can win from p′j ∪ {x 7→ aj} within < i rounds and hence he can win from p within i rounds.\nTo prove the other direction we show by induction over the number of rounds that if Spoiler has an i-round winning strategy from a position p, then some p′ ⊆ p has a CSP-derivation of depth i. Since we assume that Spoiler has a d-round winning strategy from ∅, the lemma follows. For i = 0 the 0-round winning positions are precisely the axioms in our derivation system. Assume that Spoiler has an i-round winning strategy from p. In the next round in his strategy Spoiler first has to pick up at least one pebble. Let p′ ⊆ p be the new position and note that |p′| < k. By the definition of the game Spoiler also has a i-round winning strategy from p′. Let x ∈ V (A) be the element on which the next pebble is set. Since Spoiler has a strategy to win against every possible choice of Duplicator, we know that p′ ∪{x 7→ a1}, . . . , p′ ∪{x 7→ an} are positions from which Spoiler can win the game within i−1 rounds. For all these positions there is a pj ⊆ p′ ∪{x 7→ aj} that has a derivation of depth at most i− 1 by induction assumption. If for some j it holds that pj ⊆ p′ ⊆ p we are done. Otherwise, all pj are of the form pj = p ′ j ∪{x 7→ aj} with p′j ⊆ p′ ⊆ p. Thus, p has a derivation of depth at most i by applying the derivation rule (1).\nXVII\nS p\noi le\nr’ s\nsi d\ne M S\nD u\np li\nca to\nr’ s\nsi d\ne M D\nx 1 1\nx 1 n\nx 2 1\nx 2 n\nx k 1\nx k n\ny 1 1\ny 1 n\ny 2 1\ny 2 n\ny k 1\ny k n\na 1 1\na k n\nb 1 1\nb k n\nx 1 0\nx 2 0\nx k 0\nx 1 1\nx 1 2\nx 1 m\nx 2 1\nx 2 2\nx 2 m\nx k m\ny 1 0\ny 2 0\ny k 0\ny 1 1\ny 1 2\ny 1 m\ny 2 1\ny 2 2\ny 2 m\ny k m\na 1 1 ,1\na 1 m\n,k a 2 1 ,1\na 2 m ,k a 1 0 a 2 0\nb 1 0 ,1\nb1 1 b 1 m\nb 2 0 ,1\nb2 0 ,k\nb2 1 b 2 m\nF ig . 5 .\nS u b g ra\np h\no f\nth e\nsw it\nch .\nO n\nS p\no il er\n’s si\nd e,\na ll\nin n er\n-b lo\nck ed\ng es\na re\np re\nse n t\na n d\nth e\nin te\nrb lo\nck ed\ng es\na re\nin d ic\na te\nd .\nF o r\nth e\nfi rs t b lo ck o n D u p li ca to r’ s si d e, a ll in n er -b lo ck ed g es a re d ra w n . N o te th a t th er e is n o ed g e b et w ee n a i s ,l a n d bi 0 ,l .\nXVIII\nA.2 The Switch\nFor the reader familiar with the literature it is worth noting that the switch presented here is an extension of the “multiple input one-way switch” defined in [2,3]. The difference is that the old switch can only be used for the case n = 1. However, many strategies and technical definitions can directly be extended to this more general setting. The switch in [2,3] was in turn a further development of the work from Kolaitis and Panttaja [11], who constructed a switch for the special case n = 1 and m = 2.\nIn order to define the switch we construct the two graphs: MS for Spoiler’s side and MD for Duplicator’s side. Let\nV (MS) ={xij , aij , bij , yij | i ∈ [k], j ∈ [n]}, E(MS) = { {xij , aij}, {aij , bij}, {bij , yij} | i ∈ [k], j ∈ [n] } ∪ { {aij , ai ′ j′}, {bij , bi ′ j′}, {aij , bi ′ j′} | i, i′ ∈ [k]; i 6= i′; j, j′ ∈ [n] }\nThat is, within one block i ∈ [k] of MS the vertices ai1, ai2, . . . are pairwise connected to bi1, b i 2, . . . and between two blocks i and i ′ every vertex aij and b i j from block i is connected to every vertex ai ′\nj′ and b i′ j′ from block i ′. For Duplicator’s\nside of the graph, we define for i ∈ [k]:\nXi = {xis | 0 ≤ s ≤ m}, Y i = {yis | 0 ≤ s ≤ m} Ai+ = {ais,l | s ∈ [m], l ∈ [k]}, Ai = Ai+ ∪ {ai0} Bi+ = {bis,l | s ∈ [m], l ∈ [k]}, Bi = Bi+ ∪ {bi0,l | l ∈ [k]}.\nThe set of vertices of MD is V (MD) = ⋃ i∈[k] ( Xi ∪Ai ∪Bi ∪ Y i ) .\nThe graphs consist of k blocks, where the i-th block contains all vertices with upper index i. Furthermore there are four types of variables (drawn in one row in Figure 2) the input vertices x, the output vertices y, the vertices a and b (with several indices). Every block of every type of vertices gets a unique color. That is, all xij (y i j , a i j , b i j) in MS get the same color as the vertices X\ni (Y i, Ai, Bi, resp.) in MD. This ensures that Duplicator always has to answer with vertices of the same type in the same block.\nNow we describe the edges in MD. We first define the inner-block edges E i,\nwhich are also shown in Figure 2, and then the inter-block edges Ei,j :\nEi = ( {xi0} ×Ai ) (E1)\n∪ { {xis, ais,l} | s ∈ [m]; l ∈ [k] } (E2)\n∪ ( {ai0} ×Bi ) (E3)\n∪ { {ais,l, bis,l} | s ∈ [m]; l ∈ [k] } (E4)\nXIX\n∪ { {ais,l, bi0,l′} | s ∈ [m]; l, l′ ∈ [k]; l 6= l′ } (E5)\n∪ { {bis,l, yis} | s ∈ [m]; l ∈ [k] } (E6)\n∪ { {bi0,l, yis} | s ∈ [m] ∪ {0}; l ∈ [k] } , (E7)\nEi,j = { {ais,l, a j s′,l′}, | s, s ′ ∈ [m]; l, l′ ∈ [k]; l 6= l′ }\n(E8) ∪ { {bis,l, b j s′,l′} | s ∈ [m], s ′ ∈ [m]∪{0}; l, l′ ∈ [k]; l 6= l′ } (E9)\n∪ { {bi0,l, b j 0,l′} | l, l ′ ∈ [k] }\n(E10) ∪ { {ais,l, b j s′,l′} | s ∈ [m]; s ′∈ [m]∪{0}; l, l′ ∈ [k]; l 6= l′ } (E11)\n∪ { {ai0, a j s,l} | s ∈ [m]; l ∈ [k] } (E12)\n∪ { {ai0, b j s,l} | s ∈ [m] ∪ {0}; l ∈ [k] } (E13)\nFinally, E(MD) = ⋃ i∈[k]E i ∪ ⋃ i,j∈[k];i 6=j E i,j . The next lemma states the main properties of the switch. For this, recall the definition of critical strategies (Definition 3 on page V). The first statement (i) states that Spoiler can pebble a valid position from the input to the output. Duplicator uses the critical input strategies (iv) to ensure that Spoiler has to pebble a critical position inside the switch while he pebbles the valid position through the switch. Duplicator’s output strategy (ii) ensures that Spoiler cannot move backwards (i. e., reach q on the input from q on the output). The restart strategy (iii) makes sure that Spoiler cannot pebble an invalid position through the switch.\nLemma 10. (Reminder of Lemma 6) For every configuration q = (a, b, T ), the following statements hold in the existential (k+1)-pebble game on the switch:\n(i) If q is valid, then Spoiler can reach q on the output from q on the input. (ii) Duplicator has a winning strategy Houtq with boundary function hx0 ∪ h y q. (iii) If q is invalid, then Duplicator has a winning strategy Hrestartq with boundary function hxq ∪ h y 0. (iv) If q is valid, then Duplicator has a critical strategy Hinq with boundary function hxq ∪h y 0 and sets of restart critical positions Crestart-critq,t (for t ∈ [k]) and\noutput critical positions Cout-critq such that: (a) crit(Hinq ) = ⋃ t∈[k] Crestart-critq,t ∪ Cout-critq , (b) Crestart-critq,t ⊆ Hrestart(a,b,{t}) and (c) Cout-critq ⊆ Houtq .\nProof. Let q = (a, b, T ) be an arbitrary configuration. We first construct the strategy for Spoiler to prove (i). Starting from position {(x1a(1), x 1 b(1)), . . . , (x k a(k), x k b(k))}, Spoiler places the (k + 1)st pebble on a1a(1). Duplicator has to answer with a1b(1),l1 for some l1 ∈ [k], mapping the edge {x 1 a(1), a 1 a(1)} to some edge in (E2). Next, Spoiler picks up the pebble from x1a(1) and puts it on a 2 a(2). Again, Duplicator has to answer with a2b(2),l2 for some l2 ∈ [k] \\ {l1}. The index l2 has to be different from l1 because there is an edge between a 1 a(1) and a 2 a(2), but none between a1b(1),l1 and a 2 b(2),l1 in (E8). Following that scheme, Spoiler\nXX\ncan reach the position {(a1a(1), a 1 b(1),l1 ), . . . , (aka(k), a k b(k),lk )} for pairwise distinct l1, l2, · · · , lk. Now, Spoiler pebbles b1a(1) with the free pebble and Duplicator has to answer with a vertex in B1 (due to the vertex-colors) that is adjacent to all a1b(1),l1 , . . . , a k b(k),lk . This is only the case for b1b(1),l1 (due to (E4) and (E11)), since every vertex of the form b10,li is not adjacent to the vertex a i b(i),li according to (E5) and (E11). Furthermore, b1b(1),l1 is the only vertex of the form b 1 s,l (for s > 0) that is adjacent to aib(i),li . In the next step Spoiler picks up the pebble from a1a(1) and puts it on b 2 a(2). Duplicator has to answer with a vertex that is adjacent to all vertices b1b(1),l1 , a 2 b(2),l2 , . . . , akb(k),lk . Because of the missing edges in (E5), (E11) and (E9) (!) the only vertex with this property is b2b(2),l2 . Again, Spoiler picks up the pebble from a 2 a(2) and puts it on b 3 a(3). By the same argument as before, Duplicator has to answer with b3b(3),l3 , which is the only vertex adjacent to all of b1b(1),l1 , b 2 b(2),l2 , a3b(3),l3 , . . . , a k b(k),lk . Thus, Spoiler can reach {(b1a(1), b 1 b(1),l1 ), . . . , (bka(k), b k b(k),lk\n)} and from there he reaches {(y1a(1), y 1 b(1)), . . . , (y k a(k), y k b(k))} by successively pebbling the edges {b i a(i), y i a(i)}.\nIn order to derive the winning strategies for Duplicator in (ii) and (iii) we consider several total homomorphisms from Spoiler’s to Duplicator’s side. Consider the edges (E1), (E3) and (E7) connecting vertices with vertices in one block of Duplicator’s side. They can be used by Duplicator to pebble a vertex when Spoiler moves upwards. This is the crucial ingredient for Duplicator’s output strategies (ii). The first homomorphism is used when Spoiler plays the above strategy to get a valid position through the switch and has already taken all his pebbles from the input vertices. If he tries to pebble input vertices again, then Duplicator can move to xi0 and plays according to the following homomorphism:\nhoutq,σ(x i j) = x i 0\nhoutq,σ(a i a(i)) = a i b(i),σ(i) h out q,σ(a i j) = a i 0, j 6= a(i) houtq,σ(b i a(i)) = b i b(i),σ(i) h out q,σ(b i j) = b i 0,σ(j), j 6= a(i) houtq,σ(y i a(i)) = y i b(i) h out q,σ(y i j) = y i 0, j 6= a(i)\nwhere σ ∈ Sk is some permutation on [k]. The next homomorphism is used by Duplicator when there is some valid or invalid configuration q at the output of the switch.\nhoutq (x i j) = x i 0 houtq (a i j) = a i 0 houtq (b i j) = b i 0,j houtq (y i j) = h y q(y i j)\nSince houtq and all h out q,σ are total,\nHoutq :=\n{ ℘(houtq ), q is invalid,\n℘(houtq ) ∪ ⋃ σ∈Sk ℘(h out q,σ), otherwise,\nXXI\nis a winning strategy for Duplicator satisfying (ii). If a homomorphism maps all the aia(i) vertices to A i +, then it has to map all bi vertices to Bi+. This is due to the missing edges in (E5), (E11) and has also been used in Spoiler’s strategy above. On the other hand, if for at least one i ∈ [k] all aij are mapped to ai0, then every bij can be mapped to bi0,l, where l is chosen such that ajb(j),l is not in the image of the homomorphism for every j. Duplicator benefits from this, because she can now map the yij vertices arbitrarily using the edges (E7). This behavior is used in the following restart strategies. Note that a homomorphism mapping some aij to a i 0 also maps xij to x i 0, hence restart strategies require invalid input positions. For invalid q = (a, b, T ), letHrestartq := {℘(h) | h ∈ Hrestartq }, whereHrestartq is the set of total homomorphisms h satisfying the constraints h(xij) = h x q(x i j) and h(y i j) = y i 0. This set clearly satisfies (iii). As an example fix some t ∈ T and let g ∈ Hrestartq be the following homomorphism:\ng(xij) = h x q(x i j), g(aij) = a i b(i),i, if j = a(i) and i /∈ T , g(a i j) = a i 0, otherwise, g(bij) = b i 0,t, g(yij) = y i 0.\nIt remains to consider the critical input strategies (iv). They formalize the following behavior of Duplicator at the time when Spoiler wants to pebble a configuration q through the switch as in (i). Fix a valid configuration q = (a, b, ∅). If Spoiler pebbles aia(i) or b i a(i), Duplicator answers within A i + or B\ni \\Bi+, respectively. This allows her to answer on the boundary according to the boundary function defined in (iv). However, she may run into trouble when Spoiler places k pebbles on aia(i) and b i a(i) vertices, because they extend to a (k + 1)-clique on Spoiler’s side, but not on Duplicator’s side (on the blocks Ai+ and B i\\Bi+). These positions form the critical positions where Duplicator switches to an output or restart strategy. If all k pebbles are on a1a(1), . . . , a k a(k), as in Spoiler’s strategy (i), then Duplicator switches to the output strategy (i. e., she plays according to a homomorphism houtq,σ). In all other cases she switches to a restart strategy. For all ` ∈ [k] and permutations σ on [k] we define partial homomorphism hinq,σ,` as follows:\nhinq,σ,`(x i j) = h x q(x i j)\nhinq,σ,`(a i a(i)) = a i b(i),σ(i), i 6= σ −1(`) hinq,σ,`(a i a(i)) = undefined, i = σ −1(`)\nhinq,σ,`(a i j) = a i 0, j 6= a(i) hinq,σ,`(b i j) = b i 0,` hinq,σ,`(y i j) = y i 0\nXXII\nWe need to check that hinq,σ,` defines a homomorphism from MS\\{a σ−1(`) a(i) } to MD. For most parts this is easy to verify. The important part is to check that we do not map edges to the missing pairs in the edge sets (E5), (E8) and (E11) where we require that the indices l and l′ have to be different. The constraints of (E8) are fulfilled because of the permutation σ. The constraints of (E5) and (E11) are satisfied because we have chosen ` such that no vertex maps to ais,` for all i ∈ [k] and s ∈ [m]. This also shows that the partial homomorphism cannot be extended to a total homomorphism (where hinq,σ,` is defined on a i a(i) for i = σ −1(`)). Now we define a partial homomorphism hinq,σ for every permutation σ ∈ Sk.\nhinq,σ(x i j) = h x q(x i j),\nhinq,σ(a i a(i)) = a i b(i),σ(i),\nhinq,σ(a i j) = a i 0, j 6= a(i), hinq,σ(b i j) = undefined, hinq,σ(y i j) = y i 0.\nAgain it is not hard to see that hinq,σ defines a partial homomorphism from MS to MD. We cannot extend this partial homomorphism to a total homomorphism, because if we map bia(i) to some b i 0,l we will map to a missing edge in (E5) or (E11). Otherwise, if we chose some bib(i),l, we will map the edge {b i a(i), y i a(i)} in MS to the non-edge {bib(i),l, y i 0} in MD. Duplicator’s input strategy is the family of all subsets of all mappings hinq,σ,` and h in q,σ. We are ready to define the critical positions. For all σ ∈ Sk let\nhout-critq,σ := {(aia(i), a i b(i),σ(i)) | i ∈ [k]}\nand for all σ ∈ Sk and t, u ∈ [k] and s ∈ [n]\nhrestart-critq,σ,t,u,s := {(aia(i), a i b(i),σ(i)) | i ∈ [k] \\ {t}} ∪ {(b u s , b u 0,σ(t))}.\nNow we can define the sets used in (iv):\nHinq = {℘(hinq,σ) | σ ∈ Sk} ∪ {℘(hinq,σ,`) | σ ∈ Sk, ` ∈ [k]}, Cout-critq = {hout-critq,σ | σ ∈ Sk},\nCrestart-critq,t = {hrestart-critq,σ,t,u,s | σ ∈ Sk, u ∈ [k], s ∈ [n]}, crit(Hinq ) = ⋃ t∈[k] Crestart-critq,t ∪ Cout-critq .\nFirst note that hout-critq,σ ⊂ hinq,σ and hrestart-critq,σ,t,u,s ⊂ hinq,σ,σ(t). It holds that crit(H in q ) ⊆ Hinq . It easily follows from the definitions, that hout-critq,σ ⊂ houtq,σ . Furthermore, every hrestart-critq,σ,t,u,s can be extended to a homomorphism g ∈ Hrestart(a,b,{t}) by defining\ng(xij) = h x (a,b,{t})(x i j),\nXXIII\ng(aia(i)) = h restart-crit q,σ,t,u,s (a i a(i)) = a i b(i),σ(i), if i 6= t, g(ata(t)) = a t 0,\ng(aij) = a i 0, if j 6= a(i), g(bij) = b i σ(t), g(yij) = y i 0.\nThis proves statement b) and c) from (iv). It remains to show that Hinq is a critical strategy with critical positions crit(Hinq ).\nClaim. For all g ∈ Hinq with |g| ≤ k, either g ∈ crit(Hinq ) or for all z ∈ V (MS) there exist an h ∈ Hinq , such that g ⊆ h and z ∈ Dom(h).\nProof. As g is a partial homomorphism from Hinq (which only contains subsets of hinq,σ,` and h in q,σ), we can fix some σ ∈ Sk and ` ∈ [k] such that g is a subset of the following mapping\nxia(i) 7→ x i b(i), x i j 7→ xi0, if j 6= a(i), aia(i) 7→ a i b(i),σ(i), a i j 7→ ai0, if j 6= a(i),\nbij 7→ bi0,`, yij 7→ yi0.\nLet BS := {bij | i ∈ [k], j ∈ [n]} ⊆ V (MS). Case 1: |Dom(g) ∩ {aia(i) | i ∈ [k]}| = k. In this case, g = h out-crit q,σ and hence, g ∈ crit(Hinq ). Case 2: |Dom(g) ∩ {aia(i) | i ∈ [k]}| = k − 1. If Dom(g) ∩ BS 6= ∅, then g = hrestart-critq,σ,σ−1(l),u,s for some u ∈ [k] and s ∈ [n]. Thus, we can assume that Dom(g) ∩ BS = ∅ and show for all z that g satisfies the extension property. If z = aij , then h in q,σ extends g. If z = x i j ,z = b i j or z = y i j , then h in q,σ,` extends g. Case 3: |Dom(g) ∩ {aia(i) | i ∈ [k]}| ≤ k − 2. Let j1 and j2 be two distinct indices such that aj1a(j1), a j2 a(j2)\n/∈ Dom(g). Furthermore, we can without loss of generality assume that σ(j1) = `. For z 6= aj1a(j1) the homomorphism h in q,σ,` extends g. If z = aj1a(j1), then h in q,σ′,` extends g, where σ\n′ := {(i, σ(i)) | i ∈ [k] \\ {j1, j2}} ∪ {(j1, σ(j2)), (j2, σ(j1))}.\nut\nXXIV\nA.3 The Initialization Gadget\nAt the beginning of the game we want that Spoiler can reach the start configuration α−1(0) on x, which is the pebble position {(x11, x11), . . . , (xk1 , xk1)}. To ensure this, we introduce an initialization gadget and identify its output vertices yij with the block of x i j vertices. The main property is that Spoiler can force the start configuration on the output of the gadget. Another additional property is that from any position on the output of that gadget Duplicator does not lose. This property causes the main difficulties and is needed because other positions than the start position occur on the x vertices during the course of the game. In other applications one might need to initialize the game with other configurations than α−1(0). For this, we define the initialization gadget more generally for every valid configuration q.\nThe initialization gadget INITq is built out of two switches M1 and M2, vertices z in Spoiler’s graph and z1, z2 in Duplicator’s graph. The three vertices z, z1, z2 share one unique vertex color. Additionally, there are output boundary vertices yij of the usual form. The vertices z, z1, z2 and the boundary vertices are connected to M1 and M2 as shown in Figure 6 for a specific valid configuration q = (a, b, ∅). Lemma 11 (i)–(iii) provides the strategies on INITq. The main property is that Spoiler can reach the start position q at the boundary (i) and Duplicator has a corresponding counter strategy (ii) in this situation. Furthermore, if an arbitrary position occurs at the boundary during the game, Duplicator has a strategy to survive (iii).\nXXV\nLemma 11. (A slightly more general version of Lemma 7) For every valid configuration q = (a, b, ∅) the following holds in the existential (k+1)-pebble game on INITq:\n(i) Spoiler can reach q on the output. (ii) There is a winning strategy Iinit for Duplicator with boundary function hyq. (iii) For every (valid or invalid) configuration q′ there is a critical strategy Iinitq′\nwith boundary function hyq′ and crit(Iinitq′ ) ⊆ Iinit.\nSpoiler’s strategy is quite simple. First he pebbles z. Duplicator has to answer with either z1 or z2. Then Spoiler can reach {(xia(i), x i b(i)) | i ∈ [k]} by pebbling through either M1 or M2. To construct the strategies for Duplicator, we can combine the strategies of the switches M1 and M2 such that she plays an input strategy on one switch and a restart or output strategy on the other switch. Assume that Spoiler reaches a critical position on the switch where Duplicator plays the input strategy, say M1. Duplicator can now flip the strategies such that she plays a restart or output strategy on M1, depending on which kind of critical position Spoiler has reached, and an input strategy on M2.\nProof (Proof of Lemma 11). We start with developing the strategy for Spoiler (i). First, Spoiler pebbles z. Duplicator has to response with either z1 or z2. Depending on Duplicator’s choice, Spoiler can reach either {(aia(i), a i b(i)) | i ∈ [k]} or {(bia(i), b i b(i)) | i ∈ [k]}. By Lemma 6.(i) Spoiler reaches {(c i a(i), c i b(i)) | i ∈ [k]} ({(dia(i), d i b(i)) | i ∈ [k]}) and from there he can reach the position {(y i a(i), y i b(i)) | i ∈ [k]}. For Duplicator’s strategies we start with a discussion of possible moves outside of the switches. At the top of the gadget Duplicator can map z to z1 and is then forced to answer with haq at the input of M\n1 and for some R ⊆ [k] with hb(a,b,R) at the input of M 2. On the other hand, Duplicator can map z to z2 and play according to ha(a,b,R) and h b q. At the bottom of the switch the following three combinations define partial homomorphisms for all configurations q′:\nhc0 ∪ hd0 ∪ h y q′\nhcq ∪ hd0 ∪ hyq hc0 ∪ hdq ∪ hyq\nNow we can combine these partial strategies with the strategies on the switches described in Lemma 6. In strategy I in-it,q′ Duplicator plays an input strategy on switch i, a restart strategy on the other switch and according to an arbitrary configuration q′ on the y-block. These strategies were combined to the critical strategy I initq′ described in (iii).\nI in-1t,q′ := ℘({(z, z1)}) ]Hinq 〈M1〉 ] Hrestart(a,b,{t})〈M 2〉 ] ℘(hyq′) I in-2t,q′ := ℘({(z, z2)}) ]Hrestart(a,b,{t})〈M 1〉 ] Hinq 〈M2〉 ] ℘(h y q′)\nI initq′ := ⋃ t∈[k] (I in-1t,q′ ∪ I in-2t,q′ )\nXXVI\nAll critical positions of I in-it,q′ are restart or output critical positions on the switch M i. By Lemma 6.(iv).(b) every restart critical position of I in-1t,q′ is contained in one of the strategies I in-2t,q′ as non-critical position. Hence, the only critical positions crit(I initq′ ) of the combined strategy are output critical positions on the switches. These output critical positions will be contained in the strategies I init-i where Duplicator plays an output strategy on switch i. Together with I initq they form the winning strategy I init from (ii).\nI init-1 := ℘({(z, z2)}) ]Houtq 〈M1〉 ] Hinq 〈M2〉 ] ℘(hyq) I init-2 := ℘({(z, z1)}) ]Hinq 〈M1〉 ] Houtq 〈M2〉 ] ℘(hyq) I init := I init-1 ∪ I init-2 ∪ I initq\nI init is a union of critical strategies with boundary function hyq. To prove that I init is indeed a winning strategy on the gadget, we show that every critical position of one strategy is contained as non-critical position in another strategy. Critical positions are inside the input strategy Hinq on one of the switches. By Lemma 6.(iv) they are either contained in an output or restart strategy on the corresponding switch. Hence, all restart critical positions on M1 and M2 are contained in I initq and all output critical positions on M1 (M2) are contained in I init-1 (I init-2). Recall the notation Ŝ := S \\ crit(S), by Lemma 6.(iv) we get:\ncrit(I in-2R,q′) = crit(I init-1) = crit(Hinq 〈M2〉) ⊆ Houtq 〈M2〉 ∪ ⋃ t∈[k] Hrestart(q,{t})〈M 2〉\n⊆ Î init-2 ∪ ⋃ t∈[k] Î in-1{t},q,\ncrit(I in-1R,q′) = crit(I init-2) = crit(Hinq 〈M1〉) ⊆ Houtq 〈M1〉 ∪ ⋃ t∈[k] Hrestart(q,{t})〈M 1〉\n⊆ Î init-1 ∪ ⋃ t∈[k] Î in-2{t},q.\nHence, crit(I initq′ ) ⊆ I init and I init is a winning strategy. ut\nXXVII"
    } ],
    "references" : [ {
      "title" : "Constraint propagation as a proof system",
      "author" : [ "A. Atserias", "P. Kolaitis", "M. Vardi" ],
      "venue" : "Wallace, M. (ed.) Principles and Practice of Constraint Programming CP 2004. Lecture Notes in Computer Science, vol. 3258, pp. 77–91. Springer Berlin Heidelberg",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Lower bounds for existential pebble games and k-consistency tests",
      "author" : [ "C. Berkholz" ],
      "venue" : "Logic in Computer Science (LICS), 2012 27th Annual IEEE Symposium on. pp. 25–34",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Lower bounds for existential pebble games and k-consistency tests",
      "author" : [ "C. Berkholz" ],
      "venue" : "Logical Methods in Computer Science",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "On the speed of constraint propagation and the time complexity of arc consistency testing",
      "author" : [ "C. Berkholz", "O. Verbitsky" ],
      "venue" : "Chatterjee, K., Sgall, J. (eds.) Mathematical Foundations of Computer Science 2013, Lecture Notes in Computer Science, vol. 8087, pp. 159–170. Springer Berlin Heidelberg",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An optimal k-consistency algorithm",
      "author" : [ "M.C. Cooper" ],
      "venue" : "Artificial Intelligence 41(1), 89 – 95",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A problem simplification approach that generates heuristics for constraint-satisfaction problems",
      "author" : [ "R. Dechter", "J. Pearl" ],
      "venue" : "Tech. rep., Cognitive Systems Laboratory, Computer Science Department, University of California, Los Angeles",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "The computational structure of monotone monadic snp and constraint satisfaction: A study through datalog and group theory",
      "author" : [ "T. Feder", "M.Y. Vardi" ],
      "venue" : "SIAM Journal on Computing 28(1), 57–104",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Synthesizing constraint expressions",
      "author" : [ "E.C. Freuder" ],
      "venue" : "Commun. ACM 21, 958–966",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "The parameterized complexity of local consistency",
      "author" : [ "S. Gaspers", "S. Szeider" ],
      "venue" : "Proc. CP’11. pp. 302–316",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the parallel complexity of discrete relaxation in constraint satisfaction networks",
      "author" : [ "S. Kasif" ],
      "venue" : "Artificial Intelligence 45(3), 275 – 286",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "On the complexity of existential pebble games",
      "author" : [ "P.G. Kolaitis", "J. Panttaja" ],
      "venue" : "Proc. CSL’03. pp. 314–329",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "On the expressive power of datalog: Tools and a case study",
      "author" : [ "P.G. Kolaitis", "M.Y. Vardi" ],
      "venue" : "J. Comput. Syst. Sci. 51(1), 110–134",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A game-theoretic approach to constraint satisfaction",
      "author" : [ "P.G. Kolaitis", "M.Y. Vardi" ],
      "venue" : "Proc AAAI/IAAI’00. pp. 175–181",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "On binary constraint problems",
      "author" : [ "P.B. Ladkin", "R.D. Maddux" ],
      "venue" : "J. ACM 41(3),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "Parallel consistent labeling algorithms",
      "author" : [ "A. Samal", "T. Henderson" ],
      "venue" : "International Journal of Parallel Programming 16, 341–364",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Parallel path consistency",
      "author" : [ "S. Susswein", "T. Henderson", "J. Zachary", "C. Hansen", "P. Hinker", "G. Marsden" ],
      "venue" : "International Journal of Parallel Programming 20(6),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1991
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The k-consistency test [8] is a well-known local consistency technique, which enforces that every satisfying (k − 1)-partial assignment can be extended to a satisfying k-partial assignment.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "Indeed, this intuition has been used to design parallel arc and path consistency algorithms [15,16].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "Indeed, this intuition has been used to design parallel arc and path consistency algorithms [15,16].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "On the other hand, the k-consistency test is known to be PTIME-complete [10,11] and hence not efficiently parallelizable (unless NC=PTIME).",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, the k-consistency test is known to be PTIME-complete [10,11] and hence not efficiently parallelizable (unless NC=PTIME).",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "For 2-consistency the occurrence of long chains of sequential dependencies has been observed very early [6] and was recently studied in depth in [4].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "For 2-consistency the occurrence of long chains of sequential dependencies has been observed very early [6] and was recently studied in depth in [4].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "Ladkin and Maddux [14] used algebraic techniques to show that 3-consistency requires Ω(n) nested propagation steps on binary constraint networks with constant domain.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "Since the best-known running time of a sequential algorithm for k-consistency is O(nd) [5] it follows that no significant improvement over the sequential algorithm is possible.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "As first pointed out by Feder and Vardi [7] the CSP is equivalent to the structure homomorphism problem where two finite relational structures A and B are given as input.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Below we restate the definition in terms of a formal inference system (which is inspired by the proof system in [1] and is a generalization of [4]).",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "Below we restate the definition in terms of a formal inference system (which is inspired by the proof system in [1] and is a generalization of [4]).",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "The structure of this exceptional case is discussed in deep in a joint work of Oleg Verbitsky and the author of this paper [4].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "Second, for k = 3 Ladkin and Maddux [14] showed that there is a fixed finite binary structure B and an infinite sequence of binary structures Ai such that depth (Ai,B) = Ω(|V (Ai)|).",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "Kasif [10] showed that 2-Cons is complete for PTIME under LOGSPACE reductions.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "Kolaitis and Panttaja [11] extended this result to every fixed k ≥ 2.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "In [3] the author showed that k-Cons cannot be decided in",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "It is also worth noting that Gaspers and Szeider [9] investigated the parameterized complexity of other parameterized problems related to k-consistency.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "The existential k-pebble game [12] is played by two players Spoiler and Duplicator on two relational structures A and B.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "The connection between the existential k-pebble game and the k-consistency heuristic was made by Kolaitis and Vardi [13].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "To argue about strategies in the existential pebble game we use the framework developed in [3].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "The switch is an extension of the “multiple input one-way switch” defined in [3] (which in turn is a generalization of [11]).",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "The switch is an extension of the “multiple input one-way switch” defined in [3] (which in turn is a generalization of [11]).",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "As for the switch, this gadget is an extension of the initialization gadget presented in [3].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "For k = 2 the theorem follows from [4].",
      "startOffset" : 35,
      "endOffset" : 38
    } ],
    "year" : 2014,
    "abstractText" : "We establish optimal bounds on the number of nested propagation steps in k-consistency tests. It is known that local consistency algorithms such as arc-, pathand k-consistency are not efficiently parallelizable. Their inherent sequential nature is caused by long chains of nested propagation steps, which cannot be executed in parallel. This motivates the question “What is the minimum number of nested propagation steps that have to be performed by k-consistency algorithms on (binary) constraint networks with n variables and domain size d?” It was known before that 2-consistency requires Ω(nd) and 3-consistency requires Ω(n) sequential propagation steps. We answer the question exhaustively for every k ≥ 2: there are binary constraint networks where any k-consistency procedure has to perform Ω(nk−1dk−1) nested propagation steps before local inconsistencies were detected. This bound is tight, because the overall number of propagation steps performed by k-consistency is at most nk−1dk−1.",
    "creator" : "LaTeX with hyperref package"
  }
}