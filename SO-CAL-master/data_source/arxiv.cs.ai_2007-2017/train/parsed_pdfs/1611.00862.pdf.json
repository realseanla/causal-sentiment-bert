{
  "name" : "1611.00862.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Quantile Reinforcement Learning",
    "authors" : [ "Hugo Gilbert", "Paul Weng", "Gilbert Weng" ],
    "emails" : [ "hugo.gilbert@lip6.fr", "paweng@cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Reinforcement learning, Quantile, Ordinal Decision Model, Two-Timescale Stochastic Approximation"
    }, {
      "heading" : "1. Introduction",
      "text" : "Markov decision process and reinforcement learning are powerful frameworks for building autonomous agents (physical or virtual), which are systems that make decisions without human supervision in order to perform a given task. Examples of such systems abound: expert backgammon player (Tesauro, 1995), dialogue systems (Zhang et al., 2001), acrobatic helicopter flight (Abbeel et al., 2010) or human-level video game player (Mnih et al., 2015). However, the standard framework assumes that rewards are numeric, scalar and additive and that policies are evaluated with the expectation criterion. In practice, it may happen that such numerical rewards are not available, for instance, when the agent interacts with a human who generally gives ordinal feedback (e.g., “excellent”, “good”, “bad” and so on). Besides, even when this numerical information is available, one may want to optimize a criterion different than the expectation, for instance in one-shot decision-making.\nSeveral works considered the case where preferences are qualitative. Markov decision processes with ordinal reward have been investigated (Weng, 2012, 2011) and different ordinal decision criteria have been proposed in that context. More generally, preferencebased reinforcement learning (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014) has been proposed to tackle situations where the only available preferential information concerns pairwise comparisons of histories.\nc© 2016 H. Gilbert & P. Weng.\nar X\niv :1\n61 1.\n00 86\n2v 1\n[ cs\n.L G\n] 3\nIn this paper, we propose to search for a policy that optimizes a quantile instead of the expectation. Intuitively, the τ -quantile of a distribution is the value q such that the probability of getting a value lower than q is τ (and therefore the probability of getting a value greater than q is 1 − τ). The median is an example of quantile where τ = 0.5. Interestingly, in order to use this criterion only an order over valuations is needed.\nThe quantile criterion is extensively used as a decision criterion in many domains. In finance, it is a risk measure and is known as Value-at-Risk (Jorion, 2006). For its cloud computing services, Amazon reports (DeCandia et al., 2007) that they optimize the 99.9%- quantile1. In fact, decisions in the web industry are often made based on quantiles (Wolski and Brevik, 2014; DeCandia et al., 2007). More generally, in the service industry, because of skewed distributions (Benoit and Van den Poel, 2009), one generally does not want that customers are satisfied on average, but rather that most customers (e.g., 99% of them) to be as satisfied as possible.\nThe use of the quantile criterion can be explained by the nice properties it enjoys:\n• preferences and uncertainty can be valued on scales that are not commensurable,\n• preferences over actions or trajectories can be expressed on a purely ordinal scale,\n• preferences over policies are more robust than with the standard criterion of maximizing the expectation of cumulated rewards.\nThe contributions of this paper are as follows. To the best of our knowledge, we are the first to propose an RL algorithm to learn a policy optimal for the quantile criterion. This algorithm is based on stochastic approximation with two timescales. We present an empirical evaluation of our proposition on a version of Who wants to be a millionaire.\nThe paper is organized as follows. Section 2 presents the related work. Section 3 recalls the necessary background for presenting our approach. Section 4 states the problem we solve and introduce our algorithm, Quantile Q-learning. Section 5 presents some experimental results. Finally, we conclude in Section 6."
    }, {
      "heading" : "2. Related Work",
      "text" : "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White (1987) notably considered different cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations. In this context, he showed the sufficiency of working in a state space augmented with the sum of rewards obtained so far. Filar et al. (1989) investigated decision criteria that are variance-penalized versions of the standard ones. They formulated the obtained optimization problem as a non-linear program. Yu et al. (1998) optimized the probability that the total reward becomes higher than a certain threshold.\nAdditionally, in the artificial intelligence community, Liu and Koenig (2005, 2006) also investigated the use of EU as a decision criterion in MDPs. To optimize it, they proposed a\n1. Or 0.01%-quantile, depending on whether the problem is expressed in terms of costs or rewards.\nfunctional variation of Value Iteration. In the continuation of this work, Gilbert et al. (2015) investigated the use of Skew-Symmetric Bilinear (SSB) utility (Fishburn, 1981) functions — a generalization of EU that enables intransitive behaviors and violation of the independence axiom — as decision criteria in finite-horizon MDPs. Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014).\nIn theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014).\nRecent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function.\nCloser to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP. The quantile criterion they use is different to ours as it takes into account the uncertainty present in the parameters of the MDP.\nIn MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming. While quantiles in those works are defined on distributions over ordinal rewards, quantiles in this paper are defined on distributions over histories.\nMore recently, in the machine learning community, quantile-based criteria have been proposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning. Yu and Nikolova (2013) proposed an algorithm in the pure exploration setting for different risk measures, including Value-at-Risk. Carpentier and Valko (2014) studied the problem of identifying arms with extreme payoffs, a particular case of quantiles. Finally, Szörényi et al. (2015) investigated MAB problems where a quantile is optimized instead of the mean.\nThe algorithm we propose is based on stochastic approximation with two timescales, a technique proposed by Borkar (1997, 2008). This method has recently been exploited in achievability problems (Blackwell, 1956) in the context of multiobjective MDPs (Kalathil et al., 2014) and for learning SSB-optimal policies (Gilbert et al., 2016)."
    }, {
      "heading" : "3. Background",
      "text" : "We provide in this section the background information necessary to present our algorithm to learn a policy optimal for the quantile criterion."
    }, {
      "heading" : "3.1. Markov Decision Process",
      "text" : "Markov Decision Processes (MDPs) offer a powerful formalism to model and solve sequential decision-making problems (Puterman, 1994). A finite horizon MDP is formally defined as a tuple MT = (S,A,P,R, s0) where:\n• T is a finite horizon,\n• S is a finite set of states,\n• A is a finite set of actions,\n• P : S × A × S → R is a transition function with P(s, a, s′) being the probability of reaching state s′ when action a is performed in state s,\n• R : S ×A→ R is a bounded reward function and\n• s0 ∈ S is an initial state.\nIn this model, starting from initial state s0, an agent chooses at every time step t an action at to perform in her current state st, which she can observe. This action results in a new state st+1 ∈ S according to probability distribution P(st, at, .), and a reward signal R(st, at), which penalizes or reinforces the choice of this action.\nWe will call t-history ht a succession of t state-action pairs starting from state s0 (e.g., ht = (s0, a1, s1, . . . , st−1, at−1, st)). The action choices of the agent is guided by a policy. More formally, a policy π at an horizon T is a sequence of T decision rules (δT , . . . , δ1). Decision rules prescribe which action the agent should perform at a given time step. They can be Markovian if they only depend on the current state. Besides, a decision rule is either deterministic if it always selects the same action in a given situation or randomized if it prescribes a probability distribution over possible actions. Consequently, a policy can be Markovian, deterministic or randomized according to the type of its decision rules. Lastly, a policy is stationary if it applies the same decision rule at every time step, i.e., π = (δ, δ, . . .).\nPolicies can be compared with respect to different decision criteria. The usual criterion is the expected (discounted) sum of rewards, for which an optimal deterministic Markovian policy is known to exist for any horizon T . This criterion is defined as follows. First, the value of a history ht = (s0, a1, s1, . . . , st−1, at, st) is described as the (possibly discounted) sum of rewards obtained along it, i.e.,\nr(ht) = t∑ i=1 γi−1R(si−1, ai)\nwhere γ ∈ [0, 1] is a discount factor. Then, the value of a policy π = (δT , . . . , δ1) in a state s is set to be the expected value of the histories that can be generated by π from s. This value, given by the value function vπT : S → R can be computed iteratively as follows:\nvπ0 (s) = 0 vπt (s) = R(s, δt(s)) + γ ∑ s′∈S P(s, δt(s), s ′)vπt−1(s ′) (1)\nThe value vπt (s) is the expectation of cumulated rewards obtained by the agent if she performs action δt(s) in state s at time-step t and continues to follow policy π thereafter. The higher the values of vπt (s) are, the better. Therefore, value functions induce a preference relation %π over policies in the following way:\nπ %π π ′ ⇔ ∀s ∈ S, vπT (s) ≥ vπ ′ T (s)\nA solution to an MDP is a policy, called optimal policy, that ranks the highest with respect to %π. Such a policy can be found by solving the following equations, which yields the value function of an optimal policy:\nv∗0(s) = 0 v∗t (s) = max a∈A R(s, a) + γ ∑ s′∈S P(s, a, s′)v∗t−1(s ′)"
    }, {
      "heading" : "3.2. Reinforcement Learning",
      "text" : "In the reinforcement learning (RL) setting, the assumption of the knowledge of the environment is relaxed: both dynamics through the transition function and preferences via the reward function are not known anymore. While interacting with its environment, an RL agent tries to learn a good policy by trial and error.\nTo make finite horizon MDPs learnable, we assume the decision process is repeated infinitely many times. That is, when horizon T is reached, we assume that the agent automatically returns to the initial state and the problem starts over.\nA simple algorithm to solve such an RL problem is the Q-learning algorithm (see Algorithm 1), which estimates the Q-function:\nQt(s, a) = R(s, a) + γ ∑ s′∈S P(s, a, s′)Vt−1(s ′)\nAnd obviously, we have: Vt(s) = max a∈A Qt(s, a).\nIn Algorithm 1, Line 1 generally depends on the Qt−1(s, ·) and possibly on iteration n. A simple strategy to perform this choice is called ε-greedy where the best action dictated by Qt−1(s, ·) is chosen with probability 1 − ε (with ε a small positive value) or a random action is chosen otherwise. A schedule can be defined so that parameter ε tends to zero as n tends to infinity. Besides, αn(s, a) ∈ (0, 1) on Line 2 is a learning rate. In the general case, it depends on iteration n, state s and action a, although in practice it is often chosen as a constant."
    }, {
      "heading" : "3.3. Limits of standard criteria",
      "text" : "The standard decision criteria used in MDPs, which are based on expectation, may not be reasonable in some situations. Firstly, unfortunately, in many cases, the reward function R is not known. In those cases, one can try to recover the reward function from a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013). However, even for an expert user, the elicitation of the reward function can reveal burdensome. In inverse reinforcement learning (Ng and Russell, 2000), the expert is assumed to know an\nData: MT = (S,A,G,P,R, s0) MDP Result: Q begin\nQ0(s, a)←− 0,∀(s, a) ∈ S ×A s←− s0 t←− 1 for n = 1 to N do\n1 a←− choose action r, s′ ←− perform action a in state s\n2 Qt(s, a)←− Qt(s, a) + αn(s, a) ( r + γmaxa′∈AQt−1(s ′, a′)−Qt(s, a) ) 3 if t = T then s←− s0 t←− 1\nelse s←− s′ t←− t+ 1 end\nend\nend Algorithm 1: Q-learning\noptimal policy, which is rarely true in practice. In interactive settings (Regan and Boutilier, 2009; Weng and Zanuttini, 2013), this elicitation process can be cognitively very complex as it requires to balance several criteria in a complex manner and as it can imply a large number of parameters. In this paper, we address this problem by only assuming that a strict weak ordering over histories is known.\nSecondly, for numerous applications, the expectation of cumulated reward, as used in Equation 1, may not be the most appropriate criterion (even when a numeric reward function is defined). For instance, in case of high variance or when a policy is known to be only applied a few times, the solution given by this criterion may not be satisfying for risk-averse agent. Moreover, in some domains (e.g., web industry or more generally service industry), decisions about performance are often based on the minimal quality of 99% of the possible outcomes. Therefore, in this article we aim at using a quantile (defined in Section 3.5) as a decision criterion to solve an MDP."
    }, {
      "heading" : "3.4. MDP with End States",
      "text" : "In this paper, we work with episodic MDPs with end states. Such an MDP is formally defined as a tupleMT = (S,A,G,P, s0) where S, A, P, s0 are defined as previously, G ⊆ S is a finite set of end states and T is a finite maximal horizon (i.e., an end state is attained after at most T time steps.). We call episode a history starting from s0 and ending in a final state of G.\nWe assume that a preference relation is defined over end states: We write g′ ≺ g if end state g is preferred to end state g′. Without loss of generality, we assume that G = {g1, . . . , gn} and end states are ordered with increasing preference, i.e., g1 ≺ g2 ≺ . . . ≺ gn. The weak relation of ≺ is denoted .\nNote that a finite horizon MDP can be reformulated as an MDP with end states by state augmentation. Although the resulting MDP may have a large-sized state space, the two models are formally equivalent. We focus on episodic MDPs with end states to simplify the presentation of our approach."
    }, {
      "heading" : "3.5. Quantile Criterion",
      "text" : "We define quantiles of distributions over end states of G, which are ordered by . Let τ ∈ [0, 1] be a fixed parameter. Intuitively, the τ -quantile of a distribution of end states, is the value q ∈ G such that the probability of getting an end state equal or lower than q is τ and that of getting an end state equal or greater than q is 1 − τ . The 0.5-quantile, also known as median, can be seen as the ordinal counterpart of the mean. The 0-quantile (resp. 1-quantile) is the minimum (resp. maximum) of a distribution. More generally, quantiles, which have been axiomatically characterized by Rostek (2010), define decision criteria that have the nice property of not requiring numeric valuations, but only an order.\nThe formal definition of quantiles can be stated as follows. Let pπ denote the probability distribution over end states induced by a policy π from initial state s0, the cumulative distribution induced by pπ is then defined as F π where F π(g) = ∑ g′ g p\nπ(g′) is the probability of getting an end state not preferred to g when applying policy π. Similarly, the decumulative distribution induced by pπ is defined as Gπ(g) = ∑ g g′ p\nπ(g′) is the probability of getting an end state not lower than g.\nThese two notions of cumulative and decumulative enable us to define two kinds of criteria. First, given a policy π, we define the lower τ -quantile for τ ∈ (0, 1] as:\nqπ τ = min{g ∈ G |F π(g) ≥ τ} (2)\nwhere the min operator is with respect to . Then, given a policy π, we define the upper τ -quantile for τ ∈ [0, 1) as:\nqπτ = max{g ∈ G |Gπ(g) ≥ 1− τ} (3)\nwhere the max operator is with respect to . If τ = 0 or τ = 1 only one of qπ\nτ or qπτ is defined and we define the τ -quantile q π τ as that\nvalue. When both are defined, by construction, we have qπ τ qπτ . If those two values are equal, qπτ is defined as equal to them. For instance, this is always the case in continuous settings for continuous distributions. However, in our discrete setting, it could happen that those values differ, as shown by Example 1.\nExample 1 Consider an MDP where G = {g1 ≺ g2 ≺ g3}. Let π be a policy that attains each end state with probabilities 0.5, 0.2 and 0.3 respectively. It is easy to check that qπ\n0.5 = g1\nwhereas qπ0.5 = g2.\nWhen the lower and the upper quantiles differ, one may define the quantile as a function of the lower and upper quantiles (Weng, 2012). For simplicity, in this paper, we focus on optimizing directly the lower and the upper quantiles.\nThe quantile criterion is difficult to optimize, even when a numerical reward function is given and the quality of an episode is defined as the cumulative of rewards received along the episode. This difficulty comes notably from two related sources:\n• The quantile criterion is non-linear: for instance, the τ -quantile qπ̃τ of the mixed policy π̃ that generates an episode using policy π with probability p and π′ with probability 1− p is not equal to pqπτ + (1− p)qπ ′ τ .\n• The quantile criterion is non-dynamically consistent: A sub-policy at time step t of an optimal policy for horizon T may not be optimal for horizon T − t.\nIn decision theory (McClennen, 1990), three approaches have been considered for such kinds of decision criteria:\n1. Consequentialist approach: at each time step t, follow an optimal policy for the problem with horizon T − t and initial state st even if the resulting policy is not optimal at horizon T ;\n2. Resolute choice approach: at time step t = 0, apply an optimal policy for the problem with horizon T and initial state s0 and do not deviate from it;\n3. Sophisticated resolute choice approach (Jaffray, 1998; Fargier et al., 2011): apply a policy π (chosen at the beginning) that trades off between how much π is optimal for all horizons T, T − 1, . . . , 1.\nWith non-dynamically consistent preferences, it is debatable to adopt a consequentialist approach, as the sequence of decisions may lead to dominated results. In this paper, we adopt a resolute choice point of view. We leave the third approach for future work."
    }, {
      "heading" : "4. Quantile-based Reinforcement Learning",
      "text" : "In this section, we first state the problem solved in this paper and some useful properties. Then, we present our algorithm called Quantile Q-learning (or QQ-learning for short), which is an extension of Q-learning and exploits a two-timescale stochastic approximation technique."
    }, {
      "heading" : "4.1. Problem Statement",
      "text" : "In this paper, we aim at learning a policy that is optimal for the quantile criterion from a fixed initial state. We assume that the underlying MDP is an episodic MDP with end states. Let τ ∈ (0, 1) be a fixed parameter. Formally, the problem of determining a policy optimal for the lower/upper τ -quantile can be stated as follows:\nπ∗ = arg max π qπ τ\nor π∗ = arg max π qπτ (4)\nWe focus on learning a policy that is deterministic and Markovian. The optimal lower/upper quantiles satisfy the following lemmas:\nLemma 1 The optimal lower τ -quantile q∗ τ satisfies:\nq∗ τ = min{g : F ∗(g) ≥ τ} (5) F ∗(g) = min\nπ F π(g) ∀g ∈ G (6)\nand the optimal upper τ -quantile q∗τ satisfies:\nq∗τ = max{g : G∗(g) ≥ 1− τ} (7) G∗(g) = max\nπ Gπ(g) ∀g ∈ G (8)\nThen, if the optimal lower quantile (q∗ τ ) or upper quantile (q∗τ ) were known, the problem would be relatively easy to solve. By Lemma 1, an optimal policy for the lower quantile could be obtained as follows:\nπ∗ = arg min π F π(q− τ ) (9)\nwhere q− τ = g1 if q ∗ τ = g1 and q − τ = gi if q ∗ τ = gi+1. The reason one needs to use q − τ instead of the optimal lower quantile is that otherwise it may happen that the cumulative distribution of a non-optimal policy π is smaller than or equal to that of an optimal policy at the lower quantile q∗\nτ (but greater below q∗ τ as π is not optimal). In that case, the lower\nquantile of π might not be q∗ τ . Such a thing cannot happen for the upper quantile. In that sense, determining an optimal policy for the upper quantile is easier. By Lemma 1, it is simply given by:\nπ∗ = arg max π Gπ(q∗τ ) (10)\nIn practice, if the lower/upper quantiles were known, those policies could be computed by solving a standard MDP with the following reward functions:\nRθ(s) =  0 ∀s 6∈ G 0 ∀s = gi ∈ G, θ ≥ i 1 ∀s = gi ∈ G, θ ≤ i− 1\nfor the lower quantile with θ = k if q− τ = gk and\nRθ(s) =  0 ∀s 6∈ G 1 ∀s = gi ∈ G, θ ≤ i 0 ∀s = gi ∈ G, θ ≥ i+ 1\nfor the upper quantile with θ = k if q∗τ = gk. Note that Rθ can be rewritten to depend on the optimal lower quantile:\nRθ(s) =  0 ∀s 6∈ G 0 ∀s = gi ∈ G, θ ≥ i+ 1 1 ∀s = gi ∈ G, θ ≤ i\nwith θ = k if q∗ τ\n= gk. Solving an MDP with Rθ amounts to minimizing the probability of ending in a final state strictly less preferred than q∗ τ , which solves Equation 9. Similarly, solving an MDP with Rθ amounts to maximizing the probability of ending in a final state at least as preferred as q∗τ , which solves Equation 10.\nNow, the issue here is that the lower and upper quantiles are not known. We show in the next subsection that this problem can be overcome with a two-timescale stochastic approximation technique.\nData: MT = (S,A,G,P, s0) begin\nInitialize θ to a random value for n = 1, 2, . . . do\n4 Solve MDP MT with reward function Rθ if V ∗θ (s0) < 1− τ then\nθ ←− θ − 1/n else\nθ ←− θ + 1/n end\nend\nend Algorithm 2: Simple strategy for finding the optimal upper quantile"
    }, {
      "heading" : "4.2. QQ-learning",
      "text" : "As the lower and upper quantiles are not known, we let parameter θ vary in R+ during the learning steps and we refine the definition of the previous reward functions to make sure they are both well-defined for all θ ∈ R+ and smooth in θ:\nRθ(s) =  0 ∀s 6∈ G −1 ∀s = gi ∈ G, θ ≥ i+ 1 0 ∀s = gi ∈ G, θ ≤ i i− θ else\nRθ(s) =  0 ∀s 6∈ G 1 ∀s = gi ∈ G, θ ≤ i 0 ∀s = gi ∈ G, θ ≥ i+ 1 i+ 1− θ else\nIn the remaining of the paper, we present how to solve for the upper quantile. A similar approach can be developed for the lower quantile. In order to find the optimal upper quantile, one could use the strategy described in Algorithm 2. Value V ∗θ (s0) approximates the probability of reaching an end state whose index is at least as high as θ. If that value is smaller than 1 − τ , it means θ is too high and should be decreased. Otherwise θ is too small and should be increased. Parameter θ will then converge to the index of the optimal upper quantile, which is the maximal value for θ such that V ∗θ (s0) ≥ 1 − τ . The optimal policy for V ∗θ is an optimal policy for the upper τ -quantile.\nIn a reinforcement learning setting, the solve MDP part (line 4 in Algorithm 2) could be replaced by an RL algorithm such as Q-learning. The problem is that such algorithm is only guaranteed to converge to the solution when n→∞. It would be therefore difficult to integrate Q-learning in Algorithm 2. Instead, a good policy can be learned while searching for the correct value of θ. To that aim, we use a two-timescale technique (Borkar, 1997, 2008) in which Q-learning and the update of parameter θ are run concurrently but at different speeds (i.e., at two different timescales). For this to work, parameter θ needs to be seen as\nData: MT = (S,A,G,P, s0) with P unknown Result: Q begin\nQ(s, a)←− 0,∀(s, a) ∈ S ×A s←− s0 t←− 1 for n = 1 to N do\na←− choose action r, s′ ←− perform action a in state s Qt(s, a)←− Qt(s, a) + αn(s, a) ( r + γmaxa′∈AQt−1(s ′, a′)−Qt(s, a) ) if V ∗θ (s0) < 1− τ then θ ←− θ − 1/n else θ ←− θ + 1/n end if s′ ∈ G then\ns←− s0 t←− 1\nelse s←− s′ t←− t+ 1 end\nend\nend Algorithm 3: QQ-learning for the upper-quantile\nquasi-static for the Q-learning algorithm. This is possible if the ratio of the learning rate of Q-learning and that of the update of θ satisfies:\nlim n→∞ βn αn = 0 (11)\nwhere βn = 1/n is the learning rate for parameter θ and αn is the learning rate in the Qlearning algorithm. Equation 11 implies that parameter θ is changing at a slower timescale than the Q function."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "To demonstrate the soundness of our approach, we evaluate our algorithm on the domain, Who wants to be a millionaire. We present the experimental results below."
    }, {
      "heading" : "5.1. Domain",
      "text" : "In this popular television game show, a contestant needs to answer a maximum of 15 multiple-choice questions (with four possible answers) of increasing difficulty, for increasingly large sums, roughly doubling the pot at each question. At each time step, the contestant may decide to walk away with the money currently won. If she answers incorrectly, then all winnings are lost except what has been earned at a “guarantee point” (questions\n5 and 10). The player is allowed 3 lifelines (50:50, which removes two choices, ask the audience and call a friend for suggestions); each can only be used once. We used the first model of the Spanish 2003 version of the game presented by Perea and Puerto (2007). The probability of answering correctly is a function of the question’s number and increased by the lifelines used (if any)."
    }, {
      "heading" : "5.2. Results",
      "text" : "We plot the results (see Figure 1) obtained for two different learning runs, one with 1 million learning steps and the other with 10 million learning steps. The θ-updates were interleaved with a Q-learning phase using an ε-greedy exploration strategy with ε = 0.01 and with learning rate α(n) = 1/(n + 1)11/20. One can check that Equation 11 is satisfied. We optimize the upper quantile with τ = 0.3. During the learning process we maintain a vector f of frequencies with which each final state has been attained. We define the quantity score as the cross product between f and the vector of rewards obtained when attaining each final state given the current value of θ. Put another way, score is the value of the non-stationary policy that has been played since the beginning of the learning process. Moreover, at each iteration we compute V ∗θ (s0), the optimal value in s0 given the current value of θ.\nOn Figures 1(a) and 1(b) we observe the evolution of V ∗θ (s0) as the number of iterations increases. We observe that V ∗θ (s0) converges towards 1−τ = 0.7 as the number of iterations increases with oscillations of decreasing amplitude that are due to the ever changing θ value. Figures 1(c) and 1(d) show the evolution of the score as the number of iterations increases. Score converges towards a value of 0.7 but inferior. This is due to the exploration of the Q-learning algorithm. Lastly, 1(e) and 1(f ) plot the evolution of θ as the number of iteration increases. The value of θ converges towards a value of 4 which the one for which V ∗θ (s0) = 1− τ = 0.7."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have presented an algorithm for learning a policy optimal for the quantile criterion in the reinforcement learning setting, when the MDP has a special structure, which corresponds to repeated episodic decision-making problems. It is based on stochastic approximation with two timescales (Borkar, 2008). Our proposition is experimentally validated on the domain, Who wants to be millionaire.\nAs future work, it would be interesting to investigate how to choose the learning rate αn in order to ensure a fast convergence. Moreover, our approach could be extended to other settings than episodic MDPs. Besides, it would also be interesting to explore whether gradient-based algorithms could be developed for the optimization of quantiles, based on the fact that a quantile is solution of an optimization problem where the objective function is piecewise linear (Koenker, 2005)."
    } ],
    "references" : [ {
      "title" : "Autonomous helicopter aerobatics through apprenticeship learning",
      "author" : [ "Pieter Abbeel", "Adam Coates", "Andrew Y. Ng" ],
      "venue" : "International Journal of Robotics Research,",
      "citeRegEx" : "Abbeel et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Abbeel et al\\.",
      "year" : 2010
    }, {
      "title" : "Sébag. April: Active preference-learning based reinforcement learning",
      "author" : [ "R. Akrour", "M. Schoenauer" ],
      "venue" : "In ECML PKDD, Lecture Notes in Computer Science,",
      "citeRegEx" : "Akrour et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Akrour et al\\.",
      "year" : 2012
    }, {
      "title" : "Markov decision processes with average value-at-risk criteria",
      "author" : [ "Nicole Bäuerle", "Jonathan Ott" ],
      "venue" : "Mathematical Methods of Operations Research,",
      "citeRegEx" : "Bäuerle and Ott.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bäuerle and Ott.",
      "year" : 2011
    }, {
      "title" : "Benefits of quantile regression for the analysis of customer lifetime value in a contractual setting: An application in financial services",
      "author" : [ "D.F. Benoit", "D. Van den Poel" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "Benoit and Poel.,? \\Q2009\\E",
      "shortCiteRegEx" : "Benoit and Poel.",
      "year" : 2009
    }, {
      "title" : "An analog of the minimax theorem for vector payoffs",
      "author" : [ "D. Blackwell" ],
      "venue" : "Pacific Journal of Mathematics,",
      "citeRegEx" : "Blackwell.,? \\Q1956\\E",
      "shortCiteRegEx" : "Blackwell.",
      "year" : 1956
    }, {
      "title" : "Risk-constrained Markov decision processes",
      "author" : [ "V. Borkar", "Rahul Jain" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Borkar and Jain.,? \\Q2014\\E",
      "shortCiteRegEx" : "Borkar and Jain.",
      "year" : 2014
    }, {
      "title" : "Stochastic approximation : a dynamical systems viewpoint. Cambridge university press New Delhi, Cambridge, 2008. ISBN 978-0-521-51592-4",
      "author" : [ "Vivek S. Borkar" ],
      "venue" : "URL http://opac.inria.fr/ record=b1132816",
      "citeRegEx" : "Borkar.,? \\Q2008\\E",
      "shortCiteRegEx" : "Borkar.",
      "year" : 2008
    }, {
      "title" : "Stochastic approximation with time scales",
      "author" : [ "V.S. Borkar" ],
      "venue" : "Systems & Control Letters,",
      "citeRegEx" : "Borkar.,? \\Q1997\\E",
      "shortCiteRegEx" : "Borkar.",
      "year" : 1997
    }, {
      "title" : "Markov Decision Processes in Artificial Intelligence, chapter Non-Standard Criteria",
      "author" : [ "M. Boussard", "M. Bouzid", "A.I. Mouaddib", "R. Sabbadin", "P. Weng" ],
      "venue" : null,
      "citeRegEx" : "Boussard et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Boussard et al\\.",
      "year" : 2010
    }, {
      "title" : "Meet your expectations with guarantees: beyond worst-case synthesis in quantitative games",
      "author" : [ "Véronique Bruyère", "Emmanuel Filiot", "Mickael Randour", "Jean-François Raskin" ],
      "venue" : "In STACS,",
      "citeRegEx" : "Bruyère et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bruyère et al\\.",
      "year" : 2014
    }, {
      "title" : "Preference-based reinforcement learning",
      "author" : [ "R. Busa-Fekete", "B. Szörenyi", "P. Weng", "W. Cheng", "E. Hüllermeier" ],
      "venue" : "In European Workshop on Reinforcement Learning, Dagstuhl Seminar,",
      "citeRegEx" : "Busa.Fekete et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Busa.Fekete et al\\.",
      "year" : 2013
    }, {
      "title" : "Preferencebased Reinforcement Learning: Evolutionary Direct Policy Search using a Preference-based Racing Algorithm",
      "author" : [ "Robert Busa-Fekete", "Balazs Szorenyi", "Paul Weng", "Weiwei Cheng", "Eyke Hüllermeier" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Busa.Fekete et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Busa.Fekete et al\\.",
      "year" : 2014
    }, {
      "title" : "Algorithms for cvar optimization in MDPs",
      "author" : [ "Yinlam Chow", "Mohammad Ghavamzadeh" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Chow and Ghavamzadeh.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chow and Ghavamzadeh.",
      "year" : 2014
    }, {
      "title" : "Dynamo: amazon’s highly available key-value store",
      "author" : [ "G. DeCandia", "D. Hastorun", "M. Jampani", "G. Kakulapati", "A. Lakshman", "A. Pilchin", "S. Sivasubramanian", "P. Vosshall", "W. Vogels" ],
      "venue" : "ACM SIGOPS Operating Systems Review,",
      "citeRegEx" : "DeCandia et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "DeCandia et al\\.",
      "year" : 2007
    }, {
      "title" : "Percentile optimization in uncertain Markov decision processes with application to efficient exploration",
      "author" : [ "E. Delage", "S. Mannor" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Delage and Mannor.,? \\Q2007\\E",
      "shortCiteRegEx" : "Delage and Mannor.",
      "year" : 2007
    }, {
      "title" : "Resolute choice in sequential decision problems with multiple priors",
      "author" : [ "Hélène Fargier", "Gildas Jeantet", "Olivier Spanjaard" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Fargier et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Fargier et al\\.",
      "year" : 2011
    }, {
      "title" : "Percentiles and Markovian decision processes",
      "author" : [ "Jerzy A. Filar" ],
      "venue" : "Operations Research Letters,",
      "citeRegEx" : "Filar.,? \\Q1983\\E",
      "shortCiteRegEx" : "Filar.",
      "year" : 1983
    }, {
      "title" : "Variance-penalized Markov decision processes",
      "author" : [ "Jerzy A. Filar", "L.C.M. Kallenberg", "Huey-Miin Lee" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Filar et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Filar et al\\.",
      "year" : 1989
    }, {
      "title" : "An axiomatic characterization of skew-symmetric bilinear functionals, with applications to utility theory",
      "author" : [ "P.C. Fishburn" ],
      "venue" : "Economics Letters,",
      "citeRegEx" : "Fishburn.,? \\Q1981\\E",
      "shortCiteRegEx" : "Fishburn.",
      "year" : 1981
    }, {
      "title" : "Preference-based reinforcement learning: A formal framework and a policy iteration algorithm",
      "author" : [ "J. Fürnkranz", "E. Hüllermeier", "W. Cheng", "S.H. Park" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Fürnkranz et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fürnkranz et al\\.",
      "year" : 2012
    }, {
      "title" : "Solving MDPs with skew symmetric bilinear utility functions",
      "author" : [ "Hugo Gilbert", "Olivier Spanjaard", "Paolo Viappiani", "Paul Weng" ],
      "venue" : null,
      "citeRegEx" : "Gilbert et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Gilbert et al\\.",
      "year" : 1989
    }, {
      "title" : "Model-free reinforcement learning with skew-symmetric bilinear utilities",
      "author" : [ "Hugo Gilbert", "Bruno Zanuttini", "Paolo Viappiani", "Paul Weng", "Esther Nicart" ],
      "venue" : "In International Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Gilbert et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gilbert et al\\.",
      "year" : 2016
    }, {
      "title" : "Pure stationary optimal strategies in Markov decision processes",
      "author" : [ "Hugo Gimbert" ],
      "venue" : "In STACS,",
      "citeRegEx" : "Gimbert.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gimbert.",
      "year" : 2007
    }, {
      "title" : "Implementing resolute choice under uncertainty",
      "author" : [ "Jean-Yves Jaffray" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Jaffray.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jaffray.",
      "year" : 1998
    }, {
      "title" : "Value-at-Risk: The New Benchmark for Managing",
      "author" : [ "Philippe Jorion" ],
      "venue" : "Financial Risk. McGraw-Hill,",
      "citeRegEx" : "Jorion.,? \\Q2006\\E",
      "shortCiteRegEx" : "Jorion.",
      "year" : 2006
    }, {
      "title" : "A learning scheme for blackwell’s approachability in mdps and stackelberg stochastic games",
      "author" : [ "D. Kalathil", "V.S. Borkar", "R. Jain" ],
      "venue" : "Arxiv,",
      "citeRegEx" : "Kalathil et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalathil et al\\.",
      "year" : 2014
    }, {
      "title" : "Quantile Regression",
      "author" : [ "R. Koenker" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Koenker.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koenker.",
      "year" : 2005
    }, {
      "title" : "Risk-sensitive planning with one-switch utility functions: Value iteration",
      "author" : [ "Y. Liu", "S. Koenig" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Liu and Koenig.,? \\Q2005\\E",
      "shortCiteRegEx" : "Liu and Koenig.",
      "year" : 2005
    }, {
      "title" : "Functional value iteration for decision-theoretic planning with general utility functions",
      "author" : [ "Y. Liu", "S. Koenig" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Liu and Koenig.,? \\Q2006\\E",
      "shortCiteRegEx" : "Liu and Koenig.",
      "year" : 2006
    }, {
      "title" : "Rationality and dynamic choice: Foundational explorations",
      "author" : [ "E. McClennen" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "McClennen.,? \\Q1990\\E",
      "shortCiteRegEx" : "McClennen.",
      "year" : 1990
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis" ],
      "venue" : "Nature, 518:529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Algorithms for inverse reinforcement learning",
      "author" : [ "A.Y. Ng", "S. Russell" ],
      "venue" : null,
      "citeRegEx" : "Ng and Russell.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ng and Russell.",
      "year" : 2000
    }, {
      "title" : "Dynamic programming analysis of the TV game who wants to be a millionaire",
      "author" : [ "F. Perea", "J. Puerto" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "Perea and Puerto.,? \\Q2007\\E",
      "shortCiteRegEx" : "Perea and Puerto.",
      "year" : 2007
    }, {
      "title" : "Markov decision processes: discrete stochastic dynamic programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman.,? \\Q1994\\E",
      "shortCiteRegEx" : "Puterman.",
      "year" : 1994
    }, {
      "title" : "Percentile queries in multi-dimensional Markov decision processes",
      "author" : [ "Mickael Randour", "Jean-François Raskin", "Ocan Sankur" ],
      "venue" : "CoRR, abs/1410.4801,",
      "citeRegEx" : "Randour et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Randour et al\\.",
      "year" : 2014
    }, {
      "title" : "Regret based reward elicitation for Markov decision processes. In UAI, pages 444–451",
      "author" : [ "K. Regan", "C. Boutilier" ],
      "venue" : null,
      "citeRegEx" : "Regan and Boutilier.,? \\Q2009\\E",
      "shortCiteRegEx" : "Regan and Boutilier.",
      "year" : 2009
    }, {
      "title" : "Quantile maximization in decision theory",
      "author" : [ "M.J. Rostek" ],
      "venue" : "Review of Economic Studies,",
      "citeRegEx" : "Rostek.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rostek.",
      "year" : 2010
    }, {
      "title" : "Qualitative multi-armed bandits: A quantile-based approach",
      "author" : [ "Balázs Szörényi", "Róbert Busa-Fekete", "Paul Weng", "Eyke Hüllermeier" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Szörényi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szörényi et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal difference learning and td-gammon",
      "author" : [ "Gerald Tesauro" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Tesauro.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tesauro.",
      "year" : 1995
    }, {
      "title" : "Markov decision processes with ordinal rewards: Reference point-based preferences",
      "author" : [ "P. Weng" ],
      "venue" : "In ICAPS,",
      "citeRegEx" : "Weng.,? \\Q2011\\E",
      "shortCiteRegEx" : "Weng.",
      "year" : 2011
    }, {
      "title" : "Ordinal decision models for Markov decision processes",
      "author" : [ "P. Weng" ],
      "venue" : "In ECAI,",
      "citeRegEx" : "Weng.,? \\Q2012\\E",
      "shortCiteRegEx" : "Weng.",
      "year" : 2012
    }, {
      "title" : "Interactive value iteration for Markov decision processes with unknown rewards",
      "author" : [ "P. Weng", "B. Zanuttini" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Weng and Zanuttini.,? \\Q2013\\E",
      "shortCiteRegEx" : "Weng and Zanuttini.",
      "year" : 2013
    }, {
      "title" : "Utility, probabilistic constraints, mean and variance of discounted rewards in Markov decision processes",
      "author" : [ "D.J. White" ],
      "venue" : "OR Spektrum,",
      "citeRegEx" : "White.,? \\Q1987\\E",
      "shortCiteRegEx" : "White.",
      "year" : 1987
    }, {
      "title" : "QPRED: Using quantile predictions to improve power usage for private clouds",
      "author" : [ "R. Wolski", "J. Brevik" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Wolski and Brevik.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wolski and Brevik.",
      "year" : 2014
    }, {
      "title" : "Sample complexity of risk-averse bandit-arm selection",
      "author" : [ "Jia Yuan Yu", "Evdokia Nikolova" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Yu and Nikolova.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yu and Nikolova.",
      "year" : 2013
    }, {
      "title" : "Optimization models for the first arrival target distribution function in discrete time",
      "author" : [ "Stella X. Yu", "Yuanlie Lin", "Pingfan Yan" ],
      "venue" : "Journal of mathematical analysis and applications,",
      "citeRegEx" : "Yu et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 1998
    }, {
      "title" : "Spoken dialogue management as planning and acting under uncertainty",
      "author" : [ "B. Zhang", "Q. Cai", "J. Mao", "E Chang", "B. Guo" ],
      "venue" : "In 7th European Conference on Speech Communication and Technology,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 38,
      "context" : "Examples of such systems abound: expert backgammon player (Tesauro, 1995), dialogue systems (Zhang et al.",
      "startOffset" : 58,
      "endOffset" : 73
    }, {
      "referenceID" : 46,
      "context" : "Examples of such systems abound: expert backgammon player (Tesauro, 1995), dialogue systems (Zhang et al., 2001), acrobatic helicopter flight (Abbeel et al.",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : ", 2001), acrobatic helicopter flight (Abbeel et al., 2010) or human-level video game player (Mnih et al.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 30,
      "context" : ", 2010) or human-level video game player (Mnih et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "More generally, preferencebased reinforcement learning (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014) has been proposed to tackle situations where the only available preferential information concerns pairwise comparisons of histories.",
      "startOffset" : 55,
      "endOffset" : 132
    }, {
      "referenceID" : 19,
      "context" : "More generally, preferencebased reinforcement learning (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014) has been proposed to tackle situations where the only available preferential information concerns pairwise comparisons of histories.",
      "startOffset" : 55,
      "endOffset" : 132
    }, {
      "referenceID" : 24,
      "context" : "In finance, it is a risk measure and is known as Value-at-Risk (Jorion, 2006).",
      "startOffset" : 63,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "For its cloud computing services, Amazon reports (DeCandia et al., 2007) that they optimize the 99.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 43,
      "context" : "In fact, decisions in the web industry are often made based on quantiles (Wolski and Brevik, 2014; DeCandia et al., 2007).",
      "startOffset" : 73,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "In fact, decisions in the web industry are often made based on quantiles (Wolski and Brevik, 2014; DeCandia et al., 2007).",
      "startOffset" : 73,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White (1987) notably considered different cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations.",
      "startOffset" : 34,
      "endOffset" : 278
    }, {
      "referenceID" : 8,
      "context" : "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White (1987) notably considered different cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations. In this context, he showed the sufficiency of working in a state space augmented with the sum of rewards obtained so far. Filar et al. (1989) investigated decision criteria that are variance-penalized versions of the standard ones.",
      "startOffset" : 34,
      "endOffset" : 600
    }, {
      "referenceID" : 8,
      "context" : "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White (1987) notably considered different cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations. In this context, he showed the sufficiency of working in a state space augmented with the sum of rewards obtained so far. Filar et al. (1989) investigated decision criteria that are variance-penalized versions of the standard ones. They formulated the obtained optimization problem as a non-linear program. Yu et al. (1998) optimized the probability that the total reward becomes higher than a certain threshold.",
      "startOffset" : 34,
      "endOffset" : 782
    }, {
      "referenceID" : 18,
      "context" : "(2015) investigated the use of Skew-Symmetric Bilinear (SSB) utility (Fishburn, 1981) functions — a generalization of EU that enables intransitive behaviors and violation of the independence axiom — as decision criteria in finite-horizon MDPs.",
      "startOffset" : 69,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014).",
      "startOffset" : 146,
      "endOffset" : 223
    }, {
      "referenceID" : 19,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014).",
      "startOffset" : 146,
      "endOffset" : 223
    }, {
      "referenceID" : 34,
      "context" : "This work has also been extended to the multiobjective setting (Randour et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "In MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming.",
      "startOffset" : 29,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "This method has recently been exploited in achievability problems (Blackwell, 1956) in the context of multiobjective MDPs (Kalathil et al.",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "This method has recently been exploited in achievability problems (Blackwell, 1956) in the context of multiobjective MDPs (Kalathil et al., 2014) and for learning SSB-optimal policies (Gilbert et al.",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 21,
      "context" : ", 2014) and for learning SSB-optimal policies (Gilbert et al., 2016).",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "In the continuation of this work, Gilbert et al. (2015) investigated the use of Skew-Symmetric Bilinear (SSB) utility (Fishburn, 1981) functions — a generalization of EU that enables intransitive behaviors and violation of the independence axiom — as decision criteria in finite-horizon MDPs.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity.",
      "startOffset" : 147,
      "endOffset" : 351
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value.",
      "startOffset" : 147,
      "endOffset" : 512
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR.",
      "startOffset" : 147,
      "endOffset" : 1024
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization.",
      "startOffset" : 147,
      "endOffset" : 1146
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function.",
      "startOffset" : 147,
      "endOffset" : 1240
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function. Closer to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP.",
      "startOffset" : 147,
      "endOffset" : 1530
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function. Closer to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP. The quantile criterion they use is different to ours as it takes into account the uncertainty present in the parameters of the MDP. In MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming. While quantiles in those works are defined on distributions over ordinal rewards, quantiles in this paper are defined on distributions over histories. More recently, in the machine learning community, quantile-based criteria have been proposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning. Yu and Nikolova (2013) proposed an algorithm in the pure exploration setting for different risk measures, including Value-at-Risk.",
      "startOffset" : 147,
      "endOffset" : 2335
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function. Closer to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP. The quantile criterion they use is different to ours as it takes into account the uncertainty present in the parameters of the MDP. In MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming. While quantiles in those works are defined on distributions over ordinal rewards, quantiles in this paper are defined on distributions over histories. More recently, in the machine learning community, quantile-based criteria have been proposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning. Yu and Nikolova (2013) proposed an algorithm in the pure exploration setting for different risk measures, including Value-at-Risk. Carpentier and Valko (2014) studied the problem of identifying arms with extreme payoffs, a particular case of quantiles.",
      "startOffset" : 147,
      "endOffset" : 2471
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; Fürnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruyère et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function. Closer to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP. The quantile criterion they use is different to ours as it takes into account the uncertainty present in the parameters of the MDP. In MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming. While quantiles in those works are defined on distributions over ordinal rewards, quantiles in this paper are defined on distributions over histories. More recently, in the machine learning community, quantile-based criteria have been proposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning. Yu and Nikolova (2013) proposed an algorithm in the pure exploration setting for different risk measures, including Value-at-Risk. Carpentier and Valko (2014) studied the problem of identifying arms with extreme payoffs, a particular case of quantiles. Finally, Szörényi et al. (2015) investigated MAB problems where a quantile is optimized instead of the mean.",
      "startOffset" : 147,
      "endOffset" : 2597
    }, {
      "referenceID" : 33,
      "context" : "Markov Decision Process Markov Decision Processes (MDPs) offer a powerful formalism to model and solve sequential decision-making problems (Puterman, 1994).",
      "startOffset" : 139,
      "endOffset" : 155
    }, {
      "referenceID" : 31,
      "context" : "In those cases, one can try to recover the reward function from a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013).",
      "startOffset" : 79,
      "endOffset" : 154
    }, {
      "referenceID" : 35,
      "context" : "In those cases, one can try to recover the reward function from a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013).",
      "startOffset" : 79,
      "endOffset" : 154
    }, {
      "referenceID" : 41,
      "context" : "In those cases, one can try to recover the reward function from a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013).",
      "startOffset" : 79,
      "endOffset" : 154
    }, {
      "referenceID" : 31,
      "context" : "In inverse reinforcement learning (Ng and Russell, 2000), the expert is assumed to know an",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 35,
      "context" : "In interactive settings (Regan and Boutilier, 2009; Weng and Zanuttini, 2013), this elicitation process can be cognitively very complex as it requires to balance several criteria in a complex manner and as it can imply a large number of parameters.",
      "startOffset" : 24,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : "In interactive settings (Regan and Boutilier, 2009; Weng and Zanuttini, 2013), this elicitation process can be cognitively very complex as it requires to balance several criteria in a complex manner and as it can imply a large number of parameters.",
      "startOffset" : 24,
      "endOffset" : 77
    }, {
      "referenceID" : 36,
      "context" : "More generally, quantiles, which have been axiomatically characterized by Rostek (2010), define decision criteria that have the nice property of not requiring numeric valuations, but only an order.",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 40,
      "context" : "When the lower and the upper quantiles differ, one may define the quantile as a function of the lower and upper quantiles (Weng, 2012).",
      "startOffset" : 122,
      "endOffset" : 134
    }, {
      "referenceID" : 29,
      "context" : "In decision theory (McClennen, 1990), three approaches have been considered for such kinds of decision criteria:",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : "Sophisticated resolute choice approach (Jaffray, 1998; Fargier et al., 2011): apply a policy π (chosen at the beginning) that trades off between how much π is optimal for all horizons T, T − 1, .",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "Sophisticated resolute choice approach (Jaffray, 1998; Fargier et al., 2011): apply a policy π (chosen at the beginning) that trades off between how much π is optimal for all horizons T, T − 1, .",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 32,
      "context" : "We used the first model of the Spanish 2003 version of the game presented by Perea and Puerto (2007). The probability of answering correctly is a function of the question’s number and increased by the lifelines used (if any).",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "It is based on stochastic approximation with two timescales (Borkar, 2008).",
      "startOffset" : 60,
      "endOffset" : 74
    }, {
      "referenceID" : 26,
      "context" : "Besides, it would also be interesting to explore whether gradient-based algorithms could be developed for the optimization of quantiles, based on the fact that a quantile is solution of an optimization problem where the objective function is piecewise linear (Koenker, 2005).",
      "startOffset" : 259,
      "endOffset" : 274
    } ],
    "year" : 2016,
    "abstractText" : "In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire.",
    "creator" : "LaTeX with hyperref package"
  }
}