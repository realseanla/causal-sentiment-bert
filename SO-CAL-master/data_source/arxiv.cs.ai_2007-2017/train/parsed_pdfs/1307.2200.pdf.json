{
  "name" : "1307.2200.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Hang Dinh", "Hieu Dinh" ],
    "emails" : [ "htdinh@iusb.edu", "hieu.dinh@mathworks.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n22 00\nv1 [\ncs .A\nI] 8\nJ ul\n2 01\n3"
    }, {
      "heading" : "Introduction",
      "text" : "Heuristic search has been playing a practical role in solving hard problems. One of the most popular heuristic algorithms is A∗ search (Hart, Nilson, and Raphael 1968), which is essentially best-first search with an additive evaluation f(x) = g(x) + h(x), where g(x) is the cost of the current path from the start node to node x, and h(x) is an estimation of the cheapest cost h∗(x) from x to a solution node. The function h is called a heuristic function, or heuristic for short. An important property of A∗ search is its admissibility: A∗ will always return an optimal solution if the heuristic h it uses is admissible, meaning h(x) never exceeds h∗(x).\nResearch on A∗ and other similar heuristic search algorithms, such as IDA∗ (Korf 1985), has focused on understanding the impact of properties of the heuristic function on the quality of the search. A well-studied subclass of admissible heuristics is the one with the consistency property. Heuristic h is called consistent if h(x) ≤ c∗(x, x′) + h(x′) for all pairs of nodes (x, x′), where c∗(x, x′) is the cheapest cost from x to x′. Consistency was introduced in the original\nA∗ paper (Hart, Nilson, and Raphael 1968) and later became a desirable property of admissible heuristics for two perceptions. First, since the perfect heuristic h∗ is consistent, it is expected that a good heuristic should also be consistent. The consistency is believed to enable A∗ to forgo reopening nodes (Pearl 1984, p. 82) and thus can reduce the number of node expansions. Second, inconsistent admissible heuristics seem rare. In fact, it is assumed by many researchers (Korf 2000) that “almost all admissible heuristics are consistent.”\nThe portrait of inconsistent heuristics was usually painted negatively until recently, when Zahavi et al. (2007) discovered that inconsistency is actually not that bad. They demonstrated by empirical results that in many cases, inconsistency can be used to achieve large performance improvements of IDA∗. They then promoted the use of inconsistent heuristics and showed how to turn a consistent heuristic into an inconsistent heuristic using the bidirectional pathmax (BPMX) method of Felner et al. (2005). Follow-up studies (Felner et al. 2011; Zhang et al. 2009) have also provided positive results of inconsistent heuristics with A∗ search and encouraged researchers to explore inconsistency as a means to further improve the performance of A∗.\nIn another line of research on heuristics, there have been extensive investigations on the impact of the accuracy of the heuristic on the performance of A∗ (and IDA∗). While there are a few negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001; Helmert and Röger 2008), most studies (Pohl 1977; Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al. 2012) in this line support the intuition that in many search spaces, improving the accuracy of the heuristic can improve the efficiency of A∗. Some of the negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001) on the benefit of heuristic accuracy were actually obtained under the assumption that the heuristic is consistent. Other negative results only apply to specific planning domains (Helmert and Röger 2008) or contrived search spaces with an overwhelming number of solutions (Dinh et al. 2012).\nIn light of the newly discovered benefit of inconsistent heuristics and the well-established positive results on the accuracy of heuristics, it is natural to ask so which property, consistency/inconsistency or accuracy, of heuristics really matter to the performance of A∗?. Is there any relationship between these properties of heuristics? The goal of paper is\nto address these questions.\nIn this work, we first analyze a correlation between inconsistency and accuracy of heuristics. Our analytical result reveals that the level of inconsistency of a heuristic can serve as an upper bound on the level of accuracy of the heuristic (see Theorem 1 for details.) We then investigate the relationship between the inconsistency and accuracy of heuristics as well as their impact on the performance of A∗, by running experiments on a practical domain for the Knapsack problem taken from (Dinh et al. 2012).\nOur study differs from the previous works (Felner et al. 2011; Zhang et al. 2009) on inconsistent heuristics with A∗ in both the search space used and the construction of heuristics. While Felner et al. and Zhang et al. use undirected graphs and focus on the reduction in node re-expansions as a benefit of inconsistency, our experiments are done on a directed acyclic graph on which A∗ will never reopen nodes, regardless of the heuristic used. For this search graph, we use a family of heuristics that arise in practice, which allow us to compare the inconsistency level and the accuracy level of many heuristics within this family. Recall that Felner et al. and Zhang et al. incorporated BPMX into A∗ and compared the performance of A∗ with other less well-known heuristic algorithms (B, B’, C). However, as pointed out by Zahavi et al. (2007), BPMX is only applicable for undirected graphs, thus is inapplicable for the search space we consider."
    }, {
      "heading" : "Preliminaries",
      "text" : "Firstly, we would like to review basic background on A∗ search and introduce our notation.\nA typical search problem for A∗ is defined by an edgeweighted search graph G with a start node and a set of goal nodes called solutions. For each graph G, we will use V (G) and E(G) to denote the set of vertices and the set of edges of G. We will denote a general search space for A∗ as (G, c, x0, S), where G is a directed graph, c : E(G) → R+ is a function assigning a positive cost to each edge, x0 ∈ V (G) is the start node, and S ⊂ V (G) is the set of solution nodes. When x0 and S are not important in the current context, we may only write (G, c). Given a search space (G, c), for each node x ∈ V (G), let h∗(x) denote the cost of a cheapest path from x to a solution node.\nA heuristic function on a search space (G, c) is a function h : V (G) → R+, where h(v) is an estimation of h∗(x), for each x ∈ V (G). Since h∗(s) = 0 for every solution node s, we will assume that a heuristic function must have value zero at every solution node. We will write A∗(h) to refer to the A∗ search using heuristic h. Recall that A∗(h) is a specialized best-first search algorithm with the evaluation function f(x) = g(x) + h(x), where g(x) is the cost of the current path from the start node to node x. Details of the A∗(h) search on search space (G, c, x0, S) are described in Algorithm 1. The efficiency of A∗ is usually measured by the number of node expansions, i.e., the executions of Step 2c in Algorithm 1.\nAlgorithm 1 A∗ search on search space (G, c, x0, S) using heuristic h (Pearl 1984, p. 64)\n1. Initialize OPEN := {x0} and g(x0) := 0.\n2. Repeat until OPEN is empty.\n(a) Remove from OPEN and place on CLOSED a node x for which the function f = g + h is minimum.\n(b) If x is a solution, i.e., x ∈ S, exit with success and return x.\n(c) Otherwise, expand x, generating all its successors. For each successor x′ of x,\ni. If x′ is not on OPEN or CLOSED, estimate h(x′) and calculate f(x′) = g(x′)+h(x′) where g(x′) = g(x)+ c(x, x′), and put x′ to OPEN with pointer back to x.\nii. If x′ is on OPEN or CLOSED, compare g(x′) and g(x) + c(x, x′). If g(x′) > g(x) + c(x, x′), direct the pointer of x′ back to x and reopen x′ if it is in CLOSED.\n3. Exit with failure.\nInformedness and dominance. Admissible heuristics also possess a natural dominance property (Pearl 1984, Thm. 7, p. 81): for any admissible heuristic functions h1 and h2 on T , if h1 is more informed than h2, i.e., h1(x) > h2(x) for all non-solution node x, then A∗(h1) dominates A∗(h2), i.e., every node expanded by A∗(h1) is also expanded by A∗(h2).\nConsistency and monotonicity. The consistency is in fact equivalent to the monotonicity (Pearl 1984, Thm. 8, p. 83). Precisely, heuristic h on a search space (G, c) is consistent if and only if\nh(x) ≤ c(x, x′) + h(x′)\nfor all edges (x, x′) ∈ E(G)."
    }, {
      "heading" : "Inconsistency and Accuracy",
      "text" : "We now analyze the relationship between inconsistency and accuracy of heuristics. We begin with introducing metrics characterizing the inconsistency of a heuristic.\nTo characterize the level of inconsistency of a heuristic h, Zahavi et al. (2007) defined the following two terms:\n• Inconsistency rate of an edge (IRE): For each edge e = (u, v), let IRE(h, e) = |h(u) − h(v)|. The IRE of h is the average IRE(h, e) over all edges e of the search space.\n• Inconsistency rate of a node (IRN): For each node v, let IRN(h, v) be the maximal value of |h(u) − h(v)| for any node u adjacent to v. The IRN of h is the average IRN(h, v) over all nodes v of the search space.\nNote that neither IRN nor IRE defined above takes into account the edge costs. If the search space has uniform edge cost, we can say that a consistent heuristic has IRN or IRE at most 1. But if the search space has nonuniform edge costs, we are unable to determine if a heuristic is consistent by just\nlooking up its IRN or IRE. Additionally, the metrics IRN and IRE of Zahavi et al. (2007) were defined for undirected graphs, which are not suitable for the case of search graphs considered in this paper. Therefore, we define other metrics for the inconsistency to overcome these shortcomings.\nDefinition. Let h be a heuristic on a search space (G, c). The weighted inconsistency rate of h at edge e = (x, x′) is\nWIRE(h, e) def =\nh(x)− h(x′)\nc(x, x′) .\nThe weighted inconsistency rate of h, denoted WIRE(h), is the average of WIRE(h, e) over all edges e = (x, x′) ∈ E(G) where x is a non-solution node.\nThe notion of WIRE can be seen as a weighted analog of IRE with two minor caveats. First, we use (h(x)−h(x′)) instead of the absolute value |h(x) − h(x′)|, since the graphs we consider are directed. Second, when computing WIRE(h), we do not count WIRE(h, e) for edges e from a solution node, because there will be no node expansion made from a solution node. More precisely, if e = (x, x′) and x is a solution, then WIRE(h, e) has no impact on the search quality.\nClearly, if h is consistent, then WIRE(h) ≤ 1. The converse, however, is not necessarily true. Thus, we define the following metric that can be used to determine if a heuristic is consistent or inconsistent.\nDefinition. Let h be a heuristic on a search space (G, c). We say that h is inconsistent at node x if h(x) > c(x, x′)+h(x′) for some direct successor x′ of x, i.e., (x, x′) ∈ E(G). The inconsistent node rate of h, denoted INR(h), is the ratio of the number of non-solution nodes at which h is inconsistent over the number of all non-solution nodes.\nIn other words, INR(h) is the probability that h is inconsistent at a random non-solution node. Intuitively, the larger INR(h), the more inconsistent the heuristic h is. Note that since the heuristic value of any solution node is zero, a heuristic is never inconsistent at a solution node. Hence, we have the following fact:\nFact 1. Let h be any heuristic. Then h is inconsistent if and only if INR(h) > 0.\nFor the accuracy metrics of heuristics, we will adopt the accuracy notion that measures the distance between the heuristic value and the actual value by a multiplicative factor, which has also been adopted in many previous works (Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al. 2012)\nDefinition. Let h be a heuristic function on a search space (G, c, x0, S). For any non-solution node x, we define the accuracy rate of h at node x to be\nARN(h, x) def =\nh(x)\nh∗(x) .\nThe accuracy rate of h, denoted ARN(h), is the average of ARN(h, x) for all non-solution nodes x ∈ V (G) \\ S.\nThe accuracy rate of h at the start node x0 will be denoted ARS(h). That is,\nARS(h) def =\nh(x0)\nh∗(x0) .\nThis notion of accuracy rate is particularly meaningful for admissible heuristics. Intuitively, if h is admissible, then the largerARN(h, x), the more accurate the heuristic is at node x. The accuracy rate is in fact related to the informedness of admissible heuristics: for any two admissible heuristics h1 and h2 on the same search space, h1 is more informed than h2 iff ARN(h1, x) > ARN(h2, x) for all non-solution node x.\nWe will now prove a basic relationship between weighted inconsistency rate and accuracy rate.\nTheorem 1. Let h be a heuristic on a search space (G, c) and x ∈ V (G). If WIRE(h, e) ≤ ω for all edges e along a cheapest path from x to a solution node, thenARN(h, x) ≤ ω.\nProof. Let (x1, . . . , xℓ) be a cheapest path from x to a solution node, where x1 = x and xℓ is a solution, and assume WIRE(h, e) ≤ ω for all edges along this path. Then\nh(xi)− h(xi+1) ≤ ω · c(xi, xi+1) ∀i = 1, . . . , ℓ− 1 .\nOn the other hand, h∗(x) = ∑ℓ−1\ni=1 c(xi, xi+1). It follows that\nh(x1)− h(xℓ) =\nℓ−1 ∑\ni=1\n(h(xi)− h(xi+1))\n≤\nℓ−1 ∑\ni=1\nω · c(xi, xi+1) = ωh ∗(x) .\nSince xℓ is a solution, h(xℓ) = 0 by assumption. Thus, we have h(x) = h(x)− h(xℓ) ≤ ωh∗(x).\nCorollary 1. For any heuristic h, if WIRE(h, e) ≤ ω for all edges e then ARN(h, x) ≤ ω for all nodes x.\nThis means that an upper bound on the weighted inconsistency rates of a heuristic h is also an upper bound on the accuracy rates of h. In particular, if the heuristic h is consistent, then the less WIRE(h), the less accurate h can be. This suggests that imposing consistency on the heuristic can prevent improving the heuristic accuracy."
    }, {
      "heading" : "Experiments with Knapsack Problem",
      "text" : "We will experimentally investigate the relationship between inconsistency and accuracy of heuristics on a practical domain namely the Knapsack problem. This problem is NPcomplete and has applications in many fields, from business to cryptography. Our heuristics will also be built in a practical way, based on an approximation algorithm for the Knapsack problem."
    }, {
      "heading" : "Search Model for Knapsack",
      "text" : "A Knapsack instance is denoted by a tuple 〈X, p, w,C〉, where X is a finite set of items, p : X → Z+ is a function assigning profit to each item, w : X → Z+ is a function assigning weight to each item, and C > 0 is the capacity of the knapsack. Recall that the knapsack problem is to find a subset X∗ ⊆ X of items whose total weight does not exceed capacity C and whose total profit is maximal. We will write p(X) and w(X) to denote the total profit and the total weight, respectively, of all items in X , i.e., w(X) = ∑\ni∈X w(i) and p(X) = ∑\ni∈X p(i). For each positive integer n, let [n] = {1, 2, . . . , n}, and we may simply write [n] to represent a set of n items.\nHere we will adopt the search model for the Knapsack problem that has been employed in (Dinh et al. 2012). In particular, consider the Knapsack instance 〈[n], p, w, C〉. The search graph for this instance is a directed graph, in which each node (or state) is a nonempty subset X ⊆ [n] and each edge (X,X ′) corresponds to the removal of an item i ∈ X so that X \\ {i} = X ′. The cost of such an edge (X,X ′) is the profit of the removed item i. See Figure 1 for an example of edges from a node X = {1, 2, 3, 4}. The start node is the set [n]. A node X is designated as a solution if w(X) ≤ C.\nAn important property of this search space is that every path from node X to node X ′ has the same total cost, which equals the total profit of items in X\\X ′. Thanks to this property, A∗ will avoid reopening nodes from CLOSED. Thus, consistent heuristics are not needed in this case."
    }, {
      "heading" : "Heuristic Construction",
      "text" : "Consider the search space for a Knapsack instance 〈[n], p, w, C〉. We construct efficient admissible heuristics on this search space in a similar way to the construction of Dinh et al. (2012), but without constraints to obtain an accuracy guarantee, which is a lower bound on the minimal accurate rate. The main ingredient of this construction is an FPTAS (Fully Polynomial Time Approximation Scheme) due to Ibarra and Kim (1975), which is described in Algorithm 2 below. This FPTAS is an algorithm, denoted A, that returns a solution with total profit at least (1 − ǫ)Opt(X) to each Knapsack instance 〈X, p, w, c〉 and runs in time O ( |X |3/ǫ )\n(Vazirani 2001, p. 70), for any given ǫ ∈ (0, 1), where Opt(X) is the total profit of an optimal solution to the Knapsack instance 〈X, p, w, c〉. For each subset X ⊆ [n], let Aǫ(X) denote the total profit of the solution returned by algorithm A with error parameter ǫ to the Knapsack instance\n〈X, p, w, c〉. Then for any ǫ ∈ (0, 1),\n(1− ǫ)Opt(X) ≤ Aǫ(X) ≤ Opt(X) .\nSince h∗(X) = p(X)−Opt(X), it follows that\np(X)− Aǫ(X)\n1− ǫ ≤ h∗(X) ≤ p(X)−Aǫ(X) . (1)\nNote that the lower bound p(X)−Aǫ(X)1−ǫ can fall below zero, especially for large ǫ. Hence, for each parameter ǫ ∈ (0, 1), we define the following heuristic hǫ whose admissibility is guaranteed: for any non-solution node X ,\nhǫ(X) def = max\n{\np(X)− Aǫ(X)\n1− ǫ , 0\n}\n.\nSince the running time to compute Aǫ(X) is O ( |X |3ǫ−1 )\n, the running time to compute hǫ(X) is also O ( |X |3ǫ−1 ) , which is polynomial in both n and ǫ−1.\nAlgorithm 2 FPTAS for Knapsack (Vazirani 2001, p. 70) Given: Knapsack instance 〈X, p, w,C〉, and error parameter ǫ ∈ (0, 1). Let X = {a1, . . . , aj}.\n1. Let P = maxi∈X p(i) and K = ǫP/|X |.\n2. For each item i ∈ X , define new profit p′(i) = ⌊p(i)/K⌋. Let P ′ = ⌊P/K⌋.\n3. Let Si,q denote a subset of {a1, . . . , ai} so that p′(Si,q) = q and w(Si,q) is minimal, and let wi,q := w(Si,q) (if no such a set exists, let wi,q = ∞). Use dynamic programming to compute wi,q and Si,q for all i ∈ {1, . . . , |X |} and q ∈ {0, 1, . . . , |X |P ′}.\n4. Find the most profitable set S′ among Si,q with wi,q ≤ C.\n5. Return S′.\nWhile there is no accuracy guarantee on hǫ, it is intuitive to expect the growth in the accuracy of hǫ by reducing the FPTAS error parameter ǫ. It then remains to find if the inconsistency of hǫ will also grow as ǫ decreases."
    }, {
      "heading" : "Experiments",
      "text" : "For our experiments, we generate hard Knapsack instances 〈[n], p, w, C〉 from the following Knapsack instance distributions, or “types,” which are identified by Pisinger (2005) as difficult instances for best-known exact algorithms: Strongly correlated: For each item i ∈ [n], choose its\nweightw(i) as a random integer in the range [1, R] and set its profit p(i) = w(i) + R/10. This correlation between weights and profits reflects real-life situations where the profit of an item is proportional to its weight plus some fixed charge.\nInverse strongly correlated: For each item i ∈ [n], choose its profit p(i) as a random integer in the range [1, R] and set its weight w(i) = p(i) +R/10.\nAlmost strongly correlated: For each item i ∈ [n], choose its weight w(i) as a random integer in the range [1, R] and choose its profit p(i) as a random integer in the range [w(i) +R/10−R/500, w(i) +R/10 +R/500].\nSubset sum: For each item i ∈ [n], choose its weight w(i) as a random integer in the range [1, R] and set its profit p(i) = w(i). Knapsack instances of this type are instances of the subset sum problem.\nUncorrelated with similar weight: For each item i ∈ [n], choose its weight w(i) as a random integer in the range [100000, 100100] and choose its profit p(i) as a random integer in [1, R].\nMultiple strongly correlated: For each item i ∈ [n], choose its weight w(i) as a random integer in the range [1, R]. If w(i) is divisible by 6, set the profit p(i) = w(i)+3R/10. Otherwise, set p(i) = w(i)+2R/10. This family of instances is denoted mstr(3R/10, 2R/10, 6) by Pisinger (2005) and is the most difficult family of “multiple strongly correlated instances” considered by Pisinger.\nProfit ceiling: For each item i ∈ [n], choose its weight w(i) as a random integer in the range [1, R] and set its profit p(i) = 3 ⌈w(i)/3⌉. This family of instances is denoted pceil(3), which resulted in sufficiently difficult instances for experiments of Pisinger (2005).\nHere we set the data range parameter R := 1000. The knapsack capacity is chosen as C = (t/101)w([n]), where t is a random integer in the range [30, 70].\nIn our experiments, we generate one Knapsack instance 〈[n], p, w, C〉 of each type above. For each Knapsack instance generated, we run a series of A∗(h\nǫ ) with different\nvalues of ǫ, as well as breath-first search. We chose the sample points for ǫ with two consecutive points differed by a factor of 2, so as to clearly see the change in the number of node expansions made by A∗(h\nǫ ).\nThe main challenge of these experiments is to compute ARN(hǫ). It is typically too expensive to compute ARN of a heuristic on a practical search space, because it requires computing h∗(x) exactly for all non-solution nodes x. For the Knapsack search space, we can also rely on the given FPTAS A to compute h∗(X) for each node X ⊆ [n]. Our computation is based on the following proposition:\nProposition 1. For any 0 < γ < 1/Opt(X),\nh∗(X) = ⌊p(X)−Aγ(X)⌋ . (2)\nProof. Since Aγ(X) ≥ (1 − γ)Opt(X), we have\np(X)−Aγ(X) ≤ p(X)− (1− γ)Opt(X)\n= h∗(X) + γOpt(X) < h∗(X) + 1 .\nOn the other hand, from Equation (1), we have h∗(X) ≤ p(X) − Aγ(X). The proof is completed by noting that h∗(X) is an integer .\nSince Opt(X) < min {p(X),Opt([n]) + 1} for all nonsolution node X ⊆ [n], we compute h∗(X) as in Equation (2) with\nγ = 1/min {p(X),Opt([n]) + 1} . (3)\nThe value of Opt([n]) is obtained after running A∗(hǫ), which returns the optimal solution cost h∗([n]) = p([n])− Opt([n]).\nWhile using the FPTAS could save us a considerable amount of time, computing Aγ(X) with γ specified in (3) is still time-comsuming – it actually has pseudo-polynomial time complexity. As such, we limit our experiments to Knapsack instances of relatively small size (n = 20), for which each ARN(hǫ) can be computed within 10 hours.\nDetailed results of our experiments are shown in Tables 1– 7, each table corresponds to a Knapsack instance type listed above. In each of these tables, the first column gives the values of the FPTAS error parameter ǫ. The row with “BFS” in the first column presents the breath-first search. The column “Node Exps” contains the number of node expansions made by each search. The last four columns show data for ARS(hǫ), ARN(hǫ), INR(hǫ), and WIRE(hǫ), respectively. Data of the Multiple Strongly Correlated type (in Table 6) are not available for all sample values of ǫ due to lack of time. Figure 2 shows the trend of ARS(hǫ), ARN(hǫ), INR(hǫ) and WIRE(hǫ), averaged over all Knapsack instances, but the one of the Multiple Strongly Correlated type, in our experiments. Recall that all these Knapsack instances have the same number of items, n = 20, thus have the same search graph.\nOur data show that when the accuracy metrics ARN(hǫ) and ARS(hǫ) grow, then so do the inconsistency metrics INR(hǫ) and WIRE(hǫ). Loosely speaking, the accuracy and the inconsistency level of heuristics hǫ are somewhat correlated. This could explain why the inconsistent admissible heuristics can improve the efficiency of A∗. We observe, in addition, that for small ǫ (< 0.02), the values of ARN and INR are close to each other in many instances, such as Strongly Correlated, Almost Strongly Correlated, Subset Sum, and Multiple Strongly Correlated. Regarding the performance of A∗, our data also show a significant reduction in the number of node expansions when the heuristic is more accurate, and thus more inconsistent."
    }, {
      "heading" : "Conclusions and Future Work",
      "text" : "This work provides evidence that the inconsistency and accuracy of heuristics are related. Theoretical evidence suggests that the heuristic accuracy could be upper-bounded by its level of inconsistency. Thus, requiring the heuristic to be\nconsistent could limit the room to improve its accuracy. Empirical evidence with a family of practical admissible heuristics on Knapsack domains shows that the more accurate the heuristic, the more inconsistent it is. The experiments in this work also provide positive results about accurate heuristics and inconsistent admissible heuristics, that is, both the accuracy and the inconsistency of the heuristic can be used to improve the performance of A∗.\nStill, further investigation on both the inconsistency and accuracy of heuristics should be carried out. In particular, we have the following goals in mind for our future work:\n1. Investigate the relationship between ARN(h) and INR(h) in general cases.\n2. Establish good bounds on the number of node expansions in terms of both accuracy and inconsistency metrics of the heuristic used."
    } ],
    "references" : [ {
      "title" : "The time complexity of A* with approximate heuristics on multiple-solution search spaces",
      "author" : [ "H. Dinh", "H. Dinh", "L. Michel", "A. Russell" ],
      "venue" : "Journal of Artificial Intelligence Research 45:685–729.",
      "citeRegEx" : "Dinh et al\\.,? 2012",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2012
    }, {
      "title" : "On the value of good advice: The complexity of A* with accurate heuristics",
      "author" : [ "H. Dinh", "A. Russell", "Y. Su" ],
      "venue" : "Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07), 1140–1145.",
      "citeRegEx" : "Dinh et al\\.,? 2007",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2007
    }, {
      "title" : "Dual lookups in pattern databases",
      "author" : [ "A. Felner", "U. Zahavi", "J. Schaeffer", "R.C. Holte" ],
      "venue" : "Proceedings of the 19th international joint conference on Artificial intelligence, IJCAI’05, 103–108. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Felner et al\\.,? 2005",
      "shortCiteRegEx" : "Felner et al\\.",
      "year" : 2005
    }, {
      "title" : "Inconsistent heuristics in theory and practice",
      "author" : [ "A. Felner", "U. Zahavi", "R. Holte", "J. Schaeffer", "N. Sturtevant", "Z. Zhang" ],
      "venue" : "Artificial Intelligence 175(9-10):1570–1603.",
      "citeRegEx" : "Felner et al\\.,? 2011",
      "shortCiteRegEx" : "Felner et al\\.",
      "year" : 2011
    }, {
      "title" : "Performance measurement and analysis of certain search algorithms",
      "author" : [ "J. Gaschnig" ],
      "venue" : "Ph.D. Dissertation, CarnegieMellon University, Pittsburgh, PA.",
      "citeRegEx" : "Gaschnig,? 1979",
      "shortCiteRegEx" : "Gaschnig",
      "year" : 1979
    }, {
      "title" : "A formal basis for the heuristic determination of minimum cost paths",
      "author" : [ "P. Hart", "N. Nilson", "B. Raphael" ],
      "venue" : "IEEE Transactions on Systems Science and Cybernetics SCC-4(2):100–107.",
      "citeRegEx" : "Hart et al\\.,? 1968",
      "shortCiteRegEx" : "Hart et al\\.",
      "year" : 1968
    }, {
      "title" : "How good is almost perfect",
      "author" : [ "M. Helmert", "G. Röger" ],
      "venue" : "In Proceedings of AAAI-08",
      "citeRegEx" : "Helmert and Röger,? \\Q2008\\E",
      "shortCiteRegEx" : "Helmert and Röger",
      "year" : 2008
    }, {
      "title" : "Fast approximation algorithms for the knapsack and sum of subset problems",
      "author" : [ "O.H. Ibarra", "C.E. Kim" ],
      "venue" : "Journal of the ACM 22(4):463–468.",
      "citeRegEx" : "Ibarra and Kim,? 1975",
      "shortCiteRegEx" : "Ibarra and Kim",
      "year" : 1975
    }, {
      "title" : "Complexity analysis of admissible heuristic search",
      "author" : [ "R. Korf", "M. Reid" ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence (AAAI-98), 305–310.",
      "citeRegEx" : "Korf and Reid,? 1998",
      "shortCiteRegEx" : "Korf and Reid",
      "year" : 1998
    }, {
      "title" : "Time complexity of iterative-deepening-A",
      "author" : [ "R. Korf", "M. Reid", "S. Edelkamp" ],
      "venue" : "Artificial Intelligence 129(12):199–218.",
      "citeRegEx" : "Korf et al\\.,? 2001",
      "shortCiteRegEx" : "Korf et al\\.",
      "year" : 2001
    }, {
      "title" : "Iterative-deepening-a: an optimal admissible tree search",
      "author" : [ "R.E. Korf" ],
      "venue" : "Proceedings of the 9th international joint conference on Artificial intelligence - Volume 2, IJCAI’85, 1034–1036. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Korf,? 1985",
      "shortCiteRegEx" : "Korf",
      "year" : 1985
    }, {
      "title" : "Recent progress in the design and analysis of admissible heuristic functions",
      "author" : [ "R.E. Korf" ],
      "venue" : "Proceedings of the 17th National Conference on Artificial Intelligence (AAAI00), 1165–1170. AAAI Press / The MIT Press.",
      "citeRegEx" : "Korf,? 2000",
      "shortCiteRegEx" : "Korf",
      "year" : 2000
    }, {
      "title" : "Probabilistic analysis of the complexity of A",
      "author" : [ "Nam Huyn", "J.P. Rina Dechter" ],
      "venue" : "Artificial Intelligence 15:241–254.",
      "citeRegEx" : "Huyn and Dechter,? 1980",
      "shortCiteRegEx" : "Huyn and Dechter",
      "year" : 1980
    }, {
      "title" : "Heuristics: Intelligent Search Strategies for Computer Problem Solving",
      "author" : [ "J. Pearl" ],
      "venue" : "MA: Addison-Wesley.",
      "citeRegEx" : "Pearl,? 1984",
      "shortCiteRegEx" : "Pearl",
      "year" : 1984
    }, {
      "title" : "Where are the hard knapsack problems? Computers and Operations Research 32:2271–2284",
      "author" : [ "D. Pisinger" ],
      "venue" : null,
      "citeRegEx" : "Pisinger,? \\Q2005\\E",
      "shortCiteRegEx" : "Pisinger",
      "year" : 2005
    }, {
      "title" : "Practical and theoretical considerations in heuristic search algorithms",
      "author" : [ "I. Pohl" ],
      "venue" : "Elcock, W., and Michie, D., eds., Machine Intelligence, volume 8. Chichester: Ellis Horwood. 55–72.",
      "citeRegEx" : "Pohl,? 1977",
      "shortCiteRegEx" : "Pohl",
      "year" : 1977
    }, {
      "title" : "Average-case analysis of best-first search in two representative directed acyclic graphs",
      "author" : [ "A.K. Sen", "A. Bagchi", "W. Zhang" ],
      "venue" : "Artif. Intell. 155(1-2):183–206.",
      "citeRegEx" : "Sen et al\\.,? 2004",
      "shortCiteRegEx" : "Sen et al\\.",
      "year" : 2004
    }, {
      "title" : "Approximation Algorithms",
      "author" : [ "V. Vazirani" ],
      "venue" : "SpringerVerlag.",
      "citeRegEx" : "Vazirani,? 2001",
      "shortCiteRegEx" : "Vazirani",
      "year" : 2001
    }, {
      "title" : "Inconsistent heuristics",
      "author" : [ "U. Zahavi", "A. Felner", "J. Schaeffer", "N. Sturtevant" ],
      "venue" : "Proceedings of AAAI-07, 1211–1216.",
      "citeRegEx" : "Zahavi et al\\.,? 2007",
      "shortCiteRegEx" : "Zahavi et al\\.",
      "year" : 2007
    }, {
      "title" : "A* search with inconsistent heuristics",
      "author" : [ "Z. Zhang", "N.R. Sturtevant", "R. Holte", "J. Schaeffer", "A. Felner" ],
      "venue" : "Proceedings of the 21st international joint conference on Artificial intelligence, IJCAI’09, 634–639. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Zhang et al\\.,? 2009",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Research on A and other similar heuristic search algorithms, such as IDA (Korf 1985), has focused on understanding the impact of properties of the heuristic function on the quality of the search.",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "In fact, it is assumed by many researchers (Korf 2000) that “almost all admissible heuristics are consistent.",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "Follow-up studies (Felner et al. 2011; Zhang et al. 2009) have also provided positive results of inconsistent heuristics with A search and encouraged researchers to explore inconsistency as a means to further improve the performance of A.",
      "startOffset" : 18,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Follow-up studies (Felner et al. 2011; Zhang et al. 2009) have also provided positive results of inconsistent heuristics with A search and encouraged researchers to explore inconsistency as a means to further improve the performance of A.",
      "startOffset" : 18,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "While there are a few negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001; Helmert and Röger 2008), most studies (Pohl 1977; Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al.",
      "startOffset" : 39,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "While there are a few negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001; Helmert and Röger 2008), most studies (Pohl 1977; Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al.",
      "startOffset" : 39,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : "While there are a few negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001; Helmert and Röger 2008), most studies (Pohl 1977; Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al. 2012) in this line support the intuition that in many search spaces, improving the accuracy of the heuristic can improve the efficiency of A.",
      "startOffset" : 129,
      "endOffset" : 245
    }, {
      "referenceID" : 4,
      "context" : "While there are a few negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001; Helmert and Röger 2008), most studies (Pohl 1977; Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al. 2012) in this line support the intuition that in many search spaces, improving the accuracy of the heuristic can improve the efficiency of A.",
      "startOffset" : 129,
      "endOffset" : 245
    }, {
      "referenceID" : 0,
      "context" : "While there are a few negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001; Helmert and Röger 2008), most studies (Pohl 1977; Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al. 2012) in this line support the intuition that in many search spaces, improving the accuracy of the heuristic can improve the efficiency of A.",
      "startOffset" : 129,
      "endOffset" : 245
    }, {
      "referenceID" : 8,
      "context" : "Some of the negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001) on the benefit of heuristic accuracy were actually obtained under the assumption that the heuristic is consistent.",
      "startOffset" : 29,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "Other negative results only apply to specific planning domains (Helmert and Röger 2008) or contrived search spaces with an overwhelming number of solutions (Dinh et al.",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "Other negative results only apply to specific planning domains (Helmert and Röger 2008) or contrived search spaces with an overwhelming number of solutions (Dinh et al. 2012).",
      "startOffset" : 156,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "Research on A and other similar heuristic search algorithms, such as IDA (Korf 1985), has focused on understanding the impact of properties of the heuristic function on the quality of the search. A well-studied subclass of admissible heuristics is the one with the consistency property. Heuristic h is called consistent if h(x) ≤ c(x, x) + h(x) for all pairs of nodes (x, x), where c(x, x) is the cheapest cost from x to x. Consistency was introduced in the original A paper (Hart, Nilson, and Raphael 1968) and later became a desirable property of admissible heuristics for two perceptions. First, since the perfect heuristic h is consistent, it is expected that a good heuristic should also be consistent. The consistency is believed to enable A to forgo reopening nodes (Pearl 1984, p. 82) and thus can reduce the number of node expansions. Second, inconsistent admissible heuristics seem rare. In fact, it is assumed by many researchers (Korf 2000) that “almost all admissible heuristics are consistent.” The portrait of inconsistent heuristics was usually painted negatively until recently, when Zahavi et al. (2007) discovered that inconsistency is actually not that bad.",
      "startOffset" : 74,
      "endOffset" : 1122
    }, {
      "referenceID" : 0,
      "context" : "They then promoted the use of inconsistent heuristics and showed how to turn a consistent heuristic into an inconsistent heuristic using the bidirectional pathmax (BPMX) method of Felner et al. (2005). Follow-up studies (Felner et al.",
      "startOffset" : 180,
      "endOffset" : 201
    }, {
      "referenceID" : 0,
      "context" : ") We then investigate the relationship between the inconsistency and accuracy of heuristics as well as their impact on the performance of A, by running experiments on a practical domain for the Knapsack problem taken from (Dinh et al. 2012).",
      "startOffset" : 222,
      "endOffset" : 240
    }, {
      "referenceID" : 3,
      "context" : "Our study differs from the previous works (Felner et al. 2011; Zhang et al. 2009) on inconsistent heuristics with A in both the search space used and the construction of heuristics.",
      "startOffset" : 42,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : "Our study differs from the previous works (Felner et al. 2011; Zhang et al. 2009) on inconsistent heuristics with A in both the search space used and the construction of heuristics.",
      "startOffset" : 42,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : ") We then investigate the relationship between the inconsistency and accuracy of heuristics as well as their impact on the performance of A, by running experiments on a practical domain for the Knapsack problem taken from (Dinh et al. 2012). Our study differs from the previous works (Felner et al. 2011; Zhang et al. 2009) on inconsistent heuristics with A in both the search space used and the construction of heuristics. While Felner et al. and Zhang et al. use undirected graphs and focus on the reduction in node re-expansions as a benefit of inconsistency, our experiments are done on a directed acyclic graph on which A will never reopen nodes, regardless of the heuristic used. For this search graph, we use a family of heuristics that arise in practice, which allow us to compare the inconsistency level and the accuracy level of many heuristics within this family. Recall that Felner et al. and Zhang et al. incorporated BPMX into A and compared the performance of A with other less well-known heuristic algorithms (B, B’, C). However, as pointed out by Zahavi et al. (2007), BPMX is only applicable for undirected graphs, thus is inapplicable for the search space we consider.",
      "startOffset" : 223,
      "endOffset" : 1085
    }, {
      "referenceID" : 18,
      "context" : "To characterize the level of inconsistency of a heuristic h, Zahavi et al. (2007) defined the following two terms:",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : "Additionally, the metrics IRN and IRE of Zahavi et al. (2007) were defined for undirected graphs, which are not suitable for the case of search graphs considered in this paper.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "For the accuracy metrics of heuristics, we will adopt the accuracy notion that measures the distance between the heuristic value and the actual value by a multiplicative factor, which has also been adopted in many previous works (Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al. 2012)",
      "startOffset" : 229,
      "endOffset" : 334
    }, {
      "referenceID" : 0,
      "context" : "For the accuracy metrics of heuristics, we will adopt the accuracy notion that measures the distance between the heuristic value and the actual value by a multiplicative factor, which has also been adopted in many previous works (Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al. 2012)",
      "startOffset" : 229,
      "endOffset" : 334
    }, {
      "referenceID" : 0,
      "context" : "Here we will adopt the search model for the Knapsack problem that has been employed in (Dinh et al. 2012).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "We construct efficient admissible heuristics on this search space in a similar way to the construction of Dinh et al. (2012), but without constraints to obtain an accuracy guarantee, which is a lower bound on the minimal accurate rate.",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "We construct efficient admissible heuristics on this search space in a similar way to the construction of Dinh et al. (2012), but without constraints to obtain an accuracy guarantee, which is a lower bound on the minimal accurate rate. The main ingredient of this construction is an FPTAS (Fully Polynomial Time Approximation Scheme) due to Ibarra and Kim (1975), which is described in Algorithm 2 below.",
      "startOffset" : 106,
      "endOffset" : 363
    }, {
      "referenceID" : 14,
      "context" : "Experiments For our experiments, we generate hard Knapsack instances 〈[n], p, w, C〉 from the following Knapsack instance distributions, or “types,” which are identified by Pisinger (2005) as difficult instances for best-known exact algorithms: Strongly correlated: For each item i ∈ [n], choose its weightw(i) as a random integer in the range [1, R] and set its profit p(i) = w(i) + R/10.",
      "startOffset" : 172,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "This family of instances is denoted mstr(3R/10, 2R/10, 6) by Pisinger (2005) and is the most difficult family of “multiple strongly correlated instances” considered by Pisinger.",
      "startOffset" : 61,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "This family of instances is denoted pceil(3), which resulted in sufficiently difficult instances for experiments of Pisinger (2005).",
      "startOffset" : 116,
      "endOffset" : 132
    } ],
    "year" : 2013,
    "abstractText" : "Many studies in heuristic search suggest that the accuracy of the heuristic used has a positive impact on improving the performance of the search. In another direction, historical research perceives that the performance of heuristic search algorithms, such as A* and IDA*, can be improved by requiring the heuristics to be consistent – a property satisfied by any perfect heuristic. However, a few recent studies show that inconsistent heuristics can also be used to achieve a large improvement in these heuristic search algorithms. These results leave us a natural question: which property of heuristics, accuracy or consistency/inconsistency, should we focus on when building heuristics? While there are studies on the heuristic accuracy with the assumption of consistency, no studies on both the inconsistency and the accuracy of heuristics are known to our knowledge. In this study, we investigate the relationship between the inconsistency and the accuracy of heuristics with A* search. Our analytical result reveals a correlation between these two properties. We then run experiments on the domain for the Knapsack problem with a family of practical heuristics. Our empirical results show that in many cases, the more accurate heuristics also have higher level of inconsistency and result in fewer node expansions by A*. Introduction Heuristic search has been playing a practical role in solving hard problems. One of the most popular heuristic algorithms is A search (Hart, Nilson, and Raphael 1968), which is essentially best-first search with an additive evaluation f(x) = g(x) + h(x), where g(x) is the cost of the current path from the start node to node x, and h(x) is an estimation of the cheapest cost h(x) from x to a solution node. The function h is called a heuristic function, or heuristic for short. An important property of A search is its admissibility: A will always return an optimal solution if the heuristic h it uses is admissible, meaning h(x) never exceeds h(x). Research on A and other similar heuristic search algorithms, such as IDA (Korf 1985), has focused on understanding the impact of properties of the heuristic function on the quality of the search. A well-studied subclass of admissible heuristics is the one with the consistency property. Heuristic h is called consistent if h(x) ≤ c(x, x) + h(x) for all pairs of nodes (x, x), where c(x, x) is the cheapest cost from x to x. Consistency was introduced in the original A paper (Hart, Nilson, and Raphael 1968) and later became a desirable property of admissible heuristics for two perceptions. First, since the perfect heuristic h is consistent, it is expected that a good heuristic should also be consistent. The consistency is believed to enable A to forgo reopening nodes (Pearl 1984, p. 82) and thus can reduce the number of node expansions. Second, inconsistent admissible heuristics seem rare. In fact, it is assumed by many researchers (Korf 2000) that “almost all admissible heuristics are consistent.” The portrait of inconsistent heuristics was usually painted negatively until recently, when Zahavi et al. (2007) discovered that inconsistency is actually not that bad. They demonstrated by empirical results that in many cases, inconsistency can be used to achieve large performance improvements of IDA. They then promoted the use of inconsistent heuristics and showed how to turn a consistent heuristic into an inconsistent heuristic using the bidirectional pathmax (BPMX) method of Felner et al. (2005). Follow-up studies (Felner et al. 2011; Zhang et al. 2009) have also provided positive results of inconsistent heuristics with A search and encouraged researchers to explore inconsistency as a means to further improve the performance of A. In another line of research on heuristics, there have been extensive investigations on the impact of the accuracy of the heuristic on the performance of A (and IDA). While there are a few negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001; Helmert and Röger 2008), most studies (Pohl 1977; Gaschnig 1979; Nam Huyn 1980; Sen, Bagchi, and Zhang 2004; Dinh, Russell, and Su 2007; Dinh et al. 2012) in this line support the intuition that in many search spaces, improving the accuracy of the heuristic can improve the efficiency of A. Some of the negative results (Korf and Reid 1998; Korf, Reid, and Edelkamp 2001) on the benefit of heuristic accuracy were actually obtained under the assumption that the heuristic is consistent. Other negative results only apply to specific planning domains (Helmert and Röger 2008) or contrived search spaces with an overwhelming number of solutions (Dinh et al. 2012). In light of the newly discovered benefit of inconsistent heuristics and the well-established positive results on the accuracy of heuristics, it is natural to ask so which property, consistency/inconsistency or accuracy, of heuristics really matter to the performance of A?. Is there any relationship between these properties of heuristics? The goal of paper is to address these questions. In this work, we first analyze a correlation between inconsistency and accuracy of heuristics. Our analytical result reveals that the level of inconsistency of a heuristic can serve as an upper bound on the level of accuracy of the heuristic (see Theorem 1 for details.) We then investigate the relationship between the inconsistency and accuracy of heuristics as well as their impact on the performance of A, by running experiments on a practical domain for the Knapsack problem taken from (Dinh et al. 2012). Our study differs from the previous works (Felner et al. 2011; Zhang et al. 2009) on inconsistent heuristics with A in both the search space used and the construction of heuristics. While Felner et al. and Zhang et al. use undirected graphs and focus on the reduction in node re-expansions as a benefit of inconsistency, our experiments are done on a directed acyclic graph on which A will never reopen nodes, regardless of the heuristic used. For this search graph, we use a family of heuristics that arise in practice, which allow us to compare the inconsistency level and the accuracy level of many heuristics within this family. Recall that Felner et al. and Zhang et al. incorporated BPMX into A and compared the performance of A with other less well-known heuristic algorithms (B, B’, C). However, as pointed out by Zahavi et al. (2007), BPMX is only applicable for undirected graphs, thus is inapplicable for the search space we consider.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}