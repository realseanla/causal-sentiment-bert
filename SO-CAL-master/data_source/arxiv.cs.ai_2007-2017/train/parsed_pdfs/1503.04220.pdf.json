{
  "name" : "1503.04220.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fuzzy Mixed Integer Optimization Model for Regression Approach",
    "authors" : [ "Arindam Chaudhari", "Dipak Chatterjee" ],
    "emails" : [ "arindam_chau@yahoo.co.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Fuzzy Mixed Integer Optimization Model for Regression Approach\nArindam Chaudhari\nAssistant Professor (Computer Science Engineering),\nNIIT University, Rajasthan, India\narindam_chau@yahoo.co.in\nDipak Chatterjee\nPrincipal and Professor (Mathematics),\nInstitute of Engineering and Management, Kolkata,\nWest Bengal, India\nAbstract–Mixed Integer Optimization has been a topic of active research in past decades. It has been used to solve Statistical problems of classification and regression involving massive data. However, there is an inherent degree of vagueness present in huge real life data. This impreciseness is handled by Fuzzy Sets. In this Paper, Fuzzy Mixed Integer Optimization Method (FMIOM) is used to find solution to Regression problem. The methodology exploits discrete character of problem. In this way large scale problems are solved within practical limits. The data points are separated into different polyhedral regions and each region has its own distinct regression coefficients. In this attempt, an attention is drawn to Statistics and Data Mining community that Integer Optimization can be significantly used to revisit different Statistical problems. Computational experimentations with generated and real data sets show that FMIOM is comparable to and often outperforms current leading methods. The results illustrate potential for significant impact of Fuzzy Integer Optimization methods on Computational Statistics and Data Mining.\nKeywords–Mixed Integer Optimization; Fuzzy Sets; Regression; Polyhedral Regions\nI. INTRODUCTION\nIn last few decades, the availability of massive amounts of data in electronic form and significant advances in computational power have given rise to the development of disciplines of Data Mining and Mathematical Programming that is at the centre of modern Scientific and Engineering computation. Two central problems in this direction are data classification and regression. The present focus entails a study of the regression problem. Some of the popular methods for these problems include Decision Trees for classification and regression like CART, C5.0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14]. Decision Trees and MARS are heuristic in nature and are closer to statistical inference methods. SVM belongs to the category of separating hyper planes and utilize formal continuous optimization technique like quadratic optimization. These methods are at forefront of Data Mining and have significant impact in practical applications. While Continuous Optimization methods have been widely used in Statistics and have significant impact in last 30 years [1], Integer Optimization has limited impact in Statistical Computing. While statisticians have recognized that problems like classification and regression\ncan be formulated as Integer Optimization problems [1], the belief was formed in early 1970s that these methods are not tractable in practical computational settings. Due to the success of above methods belief of Integer Optimization’s impracticality, the applicability of Integer Optimization methods to problems of classification and regression has not been investigated.\nBesides this, huge real life data is characterized by an inherent degree of uncertainty and vagueness features. In order to tackle this impreciseness in large data volume Fuzzy Sets serve as an effective tool. Fuzzy theory was originally developed by Zadeh [15] to deal with problems involving linguistic terms [16], [17], [18] and have been successfully applied to various applications in Engineering and Science. It generalizes classical two-valued logic to multi-valued logic for reasoning under uncertainty. Further it is a model-less approach and is clever disguise of Probability Theory. In this Paper, we develop a methodology for regression viz. Fuzzy Mixed Integer Optimization Model (FMIOM) that utilizes Integer Optimization methods using Fuzzy Sets to exploit discrete character of these problems. Due to the significant advances in Integer Optimization in recent past it is possible to solve large scale problems within practical limits. The methodology incorporates clustering to reduce dimensionality, non–linear transformations to improve predictive power, Mixed Integer Optimization to group points together and eliminate outlier data to represent groups by polyhedral regions. The data points are separated into different polyhedral regions and each region has its own distinct regression coefficients. In this attempt, we have drawn the attention of Statistics and Data Mining community that Integer Optimization can be significantly used to revisit different Statistical problems. This Paper is organized as follows. In section II, the geometry of regression approach is illustrated. This is followed by FMIOM for regression in the next section. Computational results and discussions are presented in section IV. Finally, in section V conclusions are given."
    }, {
      "heading" : "II. GEOMETRY OF REGRESSION APPROACH",
      "text" : "In classical regression setting, we have n data points niRyRxyx i d iii ,.......,1,,),,( =∈∈ . We intend\nto find a linear relationship between ix and iy i.e., ixy ii ∀≈ 'β , where the coefficients dR∈β are found\nby minimizing ∑ =\n− n\ni ii xy 1 2' )( β or∑ =\n− n\ni ii xy 1\n' || β . In this\nprocess k disjoint regions d k RP ⊂ and corresponding coefficients KkR d\nk ,.......,1, =∈β are found such that\nif kPx ∈0 , the prediction for 0y will be 0\n'\n0 ˆ xy rβ= .\nHere, n points are assigned into K groups where, K is user defined parameter. In addition, optimization model is further enhanced to detect and eliminate outlier points in data set (Figure 2). In contrast, traditional regression models deal with outliers after slopes have been determined by examining which points contribute most to total prediction error [12], [13]. This procedure can often be deceiving because the model is heavily influenced by\noutlier points. After the points are assigned to K groups,\nwe determine coefficients kβ that best fit the data for group\nKkk ,.......,1, = and define polyhedra kP to represent each group using linear optimization methods. After coefficients\nand polyhedra are defined, we predict 0y value of new\npoint 0x . In fact, no partition of\nd R are created, so there is a\npossibility that new point 0x might not belong to any kP . In\nsuch a case, we assign it to the region rP that contains majority among F (a user defined number) nearest\nneighbors in training set and make prediction 0\n'\n0 ˆ xy rβ= .\nSimilarly to the classification model, we preprocess the data by clustering them into small clusters to reduce the dimension and thus computation time of optimization model (Figure 3)."
    }, {
      "heading" : "III. FMIOM FOR REGRESSION",
      "text" : "In this section, we present FMIOM approach for regression. We initiate with Mixed Integer Optimization Method (MIOM) to assign points to groups which is not practical because of dimensionality problems. We first assign points to clusters, then assign clusters to group of points, which\nare represented by polyhedral regions kP . A method is\nillustrated to automatically find non-linear transformations of explanatory variables to improve predictive power of method. Finally, the regression algorithm is presented."
    }, {
      "heading" : "A. Assigning Points to Groups",
      "text" : "The training data consists of n observations niRyRxyx i d iii ,.......,1,,),,( =∈∈ . We\nassume },.......,1{},,.......,1{ KKnN == and M as a\nlarge positive constant. We define binary variables for\nKk ∈ and Ni ∈ :\n   = 0 1 ,ika otherwise\nkx assigned\ni  →\nThe MIOM is as follows:\nimizemin ∑ =\nn\ni\ni\n1\nδ\nsubject to\n),1()( , ' ikikii aMxy −−−≥ βδ NiKk ∈∈ ,\n),1()( , ' ikikii aMxy −−−−≥ βδ NiKk ∈∈ , (1)\n∑ =\n∈= K\nk ik Nia 1 , ,1 0},1,0{, ≥∈ iika δ\nFrom the first and second constraints iδ is absolute error\nassociated with point ix . If ,1, =ika ),( ' ikii xy βδ −≥\n),(\n'\nikii xy βδ −−≥ and minimization of iδ sets iδ equal\nto || ' iki xy β− . If ,0, =ika right hand side of first two\nconstraints becomes negative, making them irrelevant\nbecause iδ is non-negative. Finally, third constraint limits assignment of each point to just one group. It has been\nfound that even for relatively small )100( ≈nn , the\nabove optimization model is difficult to solve in reasonable time. For this reason, a clustering algorithm is executed\ninitially to cluster nearby ix points together. After L such\nclusters are found, for nL << we solve FMIOM analogous to above optimization model, but with significantly fewer binary decision variables."
    }, {
      "heading" : "B. Clustering Algorithm",
      "text" : "Nearest Prototype )1,( >− kNPk clustering algorithm\ndefined on dR [4] in ),( yx space is used to find\nL clusters. The clustering algorithm initiates with n clusters, then continues to merge clusters with\npoints close to each other until L clusters are obtained."
    }, {
      "heading" : "C. Assigning Points to Groups: Practical Approach",
      "text" : "Continuing with the clustering algorithm of previous section we can find K clusters, define them as our final\ngroups and find the best k β coefficient for each group by solving separate linear regression problems. Such an approach does not combine points to minimize total absolute error. For this reason, we use clustering algorithm until we have KLL >, clusters and then solve MIOM that\nassigns L clusters into K groups to minimize total absolute error. Another key concern in regression models is the presence of outliers. The MIOM presented next removes\npotential outliers by eliminating points in clusters that tend to weaken the fit of predictor coefficients.\nLet },.......,1{, LLlCl =∈ be cluster l and denote )(il as\nsxi ' cluster. Similarly, to optimization problem (1), we\ndefine the following binary variables for }0{∪∈ Kk\nand :Ll ∈\n   = 0 1 ,lka otherwise\nkl assigned →\n(2)\nWe define 0=k as the outlier group in sense that points in\ncluster l with 1,0 =la will be eliminated. The following\nfuzzy optimization model assigns clusters to groups and allows possibility of eliminating clusters of points as outliers:\nimizemin ∑ =\nn\ni\ni\n1\n~ δ\nsubject to\n),1() ~ ( ~\n)(,\n'\nilkikii aMxy −−−≥ βδ NiKk ∈∈ ,\n),1() ~ ( ~\n,\n'\nikikii aMxy −−−−≥ βδ NiKk ∈∈ , (3)\n∑ =\n∈= K\nk lk Lla 0\n, ,1 ,|| ~||\n1 ,0∑ =\n≤ L\nl\nll NaC ρ\n0 ~\n},1,0{, ≥∈ ilka δ\nHere, M is a large positive constant and ρ~ is maximum fraction of points that can be eliminated as outliers. From\nthe first and second set of constraints i\nδ ~ is absolute error\nassociated to point i x . Both ρ~ and i\nδ ~ are modeled as the\nfollowing Fuzzy membership function:\n   \n   \n\n> +−\n≤≤\n< −+\n=\njj\njjj\nj\njjj\njj\njjj\nj\njx\nfw gfw\ng\nfwe\new wde\nd\nx j\n,\n,1\n,\n)(~µ (4)\nHere x denotes the variables ρ~ and iδ ~ .\nIf ,1 )(, = ilk\na ), ~ ( ~ ' ikii xy βδ −≥ ) ~ ( ~ ' ikii xy βδ −−≥ and\nminimization of i\nδ ~ sets it equal to | ~ | '\niki xy β− . If\n,0)(, =ilka the first two constraints become irrelevant\nbecause i\nδ ~ is non-negative. The third set of constraint\nlimits assignment of each cluster point to just one group including the outlier group. The last constraint limits percentage of points eliminated to be less than or equal to\npre-specified number ρ~ . If 1, =lka then all points in\ncluster l are assigned to group k i.e. lalk CG lk }1|{ , = ∪= .\nOptimization Problem (3) has KL variables as opposed to\nKn variables in Problem (2). The number of clusters L controls trade-off between the quality of solution and\nefficiency of computation. As L increases, the quality of solution increases but the efficiency of computation decreases."
    }, {
      "heading" : "D. Assigning Groups to Polyhedral Regions",
      "text" : "We identify K groups of points solving Optimization Problem (3). In this section, we establish a geometric\nrepresentation of group k by a polyhedron kP . It is possible\nfor convex hulls of K groups to overlap and thus we might\nnot be able to define disjoint regions of kP that contain all\npoints of group k . For this reason our approach is based on separating pairs of groups with the objective of minimizing\nsum of violations. We first outline how to separate group k\nto group rkr <, . We consider following two Fuzzy Linear\nOptimization Problems:\nimizemin ∑∑ ∈∈ + rk Gl l Gi\ni εε ~~\nsubject to\nkirkirk Giqxp ∈+−≤− , ~1, '' , ε (5)\nrirklrk Glqxp ∈−≥− ,~1, ' , ε\n0~,0~,1 '\n, ≥≥≥ lirk ep εε\nimizemin ∑∑ ∈∈ + rk Gl l Gi\ni εε ~~\nsubject to\nkirkirk Giqxp ∈+−≤− , ~1, '' , ε (6)\nrirklrk Glqxp ∈−≥− ,~1, ' , ε\n0~,0~,1' , ≥≥−≤ lirk ep εε\nHere e is vector of ones, iε ~ and lε ~ are modeled as Fuzzy membership function given by equation (4). Both problems\n(5) and (6) find a hyper plane rkrk qxp ,\n'\n, = that softly\nseparates points in group k from points in group k . The third constraint prevents the trivial hyper\nplane 0, =rkp and 0, =rkq for optimal solution. Problems\n(5) and (6) set the sum of elements of rkp , to be strictly\npositive and negative respectively. On solving problems (5) and (6) for every pair of groups, we assume\n,1,.......,1,|{ , ' , −=≤= kiqxpxP ikikk\n},.......,1,, ' , Kkiqxp ikik +=≥ (7)\nAfter kP is defined, we re-compute kβ using all points\ncontained in kP because it is possible that they are different\nfrom original kG that problem (7) obtained. The Optimization Problem is solved that minimizes absolute\ndeviation of all points in kP to find new kβ ."
    }, {
      "heading" : "E. Non linear Data Transformations",
      "text" : "To improve the predictive power of FMIOM, we augment explanatory variables with non-linear transformations. In particular, we consider transformations xx log,2 and x 1 applied to the co-ordinates of given points. We augment\neach d dimensional vector ' ,1, ),.......,( diii xxx = with djx xx\nji jiji ,.......,1, 1,log, , , 2 , = and\napply FMIOM to resulting d4 dimensional vectors, but the increased dimension slows computation time. For this reason simple heuristic method is used to choose which transformation of which variable to include in data set."
    }, {
      "heading" : "F. Regression Algorithm",
      "text" : "The regression algorithm comprises of following steps: (a) Nonlinear Transformation: Augment original data set with non-linear transformations using method discussed in section III (E); (b) Preprocessing: Use Fuzzy clustering algorithm to find nL << clusters of data points; (c) Assign clusters to groups: Solve Optimization Problem (3) to determine which points belong to which group while eliminating potential outliers; (d) Assign groups to polyhedral regions: Solve linear Optimization Problems (5) and (6) for all pairs of groups and define polyhedra as in\nEquation (7). (e) Re-computation of β : Once polyhedra kP\nare identified, re-compute kβ using only the points that\nbelong in kP . Given a new point 0x (augmented by same transformations as applied in training set data),\nkPx ∈0 then we predict 0 ' 0 ~ ˆ xy rβ= . Otherwise, we\nassign 0x to region rP that contains majority among its\nF neighboring points in training set and make\nprediction 0\n'\n0 ~ ˆ xy rβ= ."
    }, {
      "heading" : "IV. COMPUTATIONAL RESULTS AND DISCUSSIONS",
      "text" : "In this section, we discuss the performance of FMIOM on three real data sets viz. Boston, Abalone and Auto data and Friedman’s [7] generated data sets. A comparison is also made on performances of Linear Least Square Regression (LLSR) and Artificial Neural Networks (ANN) with Radial Basis Function (RBF) and generalized regression using MATLAB’s ANN Toolbox. Each regression data was split into three parts with 50%, 30% and 20% of data used for training, validation and testing respectively. The assignment to each set was done randomly and the process was repeated 10 times. The validation set was used to fine tune value of parameter K . In all cases, the FMIOM depicted in Equation (17) was solved and parameters LM , and ρ~ were set to 10000, 10 and 0.01 respectively.\nIn ANN, validation set was used to select the appropriate model viz. RBF against generalized regression, adjust number of epochs, number of layers, spread constant and accuracy parameter.\nTables I and II illustrate mean absolute error and mean squared error respectively of LLSR, ANN and FMIOM averaged over 10 random partitions on Friedman’s data\nsets. Tables III and IV illustrate mean absolute error and mean squared error respectively of regression methods averaged over 10 random partitions on Boston, Abalone and Auto data sets. The numbers in parenthesis are corresponding standard deviations. Tables V and VI illustrate average running time in CPU seconds of the methods for Friedman’s generated data sets and real data sets respectively. We measure the performance of different regression methods by their predictive ability and stability of their solutions. The prediction accuracy are measured using both mean absolute errors and mean squared errors between predicted against the actual response variable in testing set, given that mean absolute error is used as goodness of fit criterion for FMIOM and mean squared error is used as goodness of fit criterion for LLSR and ANN model. FMIOM used 2=K for all data sets, ANN always used RBF as preferred model with just one layer of nodes. Table 6 shows that FMIOM has relatively reasonable average running time as other methods for small data sets, but its run time explodes for larger Abalone data set. There exists a dramatic increase in run time with larger\ndata sets mainly due to M parameter in models (1) and (3). Because tight estimate of Mbig − parameter cannot be\ndetermined apriori, the large value of M seriously hampers the efficiency of FMIOM.\nThe computational experiments illustrated some benefits and shortcomings of FMIOM compared to other existing methods in Data Mining and Machine Learning. The results obtained from FMIOM can be improved further by enforcing continuity in boundaries, finding stronger\napproximations of parameter M and making more computational runs. Its main weakness arises from discontinuity of regression line at boundaries of polyhedral regions. FMIOM predictive performance of general continuous function is significantly hampered as evident with Friedman data set. Continuity can be imposed by modifications to the FMIOM in one dimensional case, but extension to higher dimensions is not evident with current model. Thus, as it currently stands if underlying function is continuous, perhaps a continuous model would be more appropriate. Another apparent challenge for FMIOM is maintaining reasonable computation time. Compared to heuristic based methods in Data Mining such as ANN and Classification Trees, FMIOM has much longer running time for larger data sets. However, FMIOM did have faster\nrunning times compared to other techniques like SVM in certain problems. The implementation of FMIOM can be improved to speed up its running time. It can implement a tailored quadratic programming solver for solving SVM sub-problems as done in all implementations of SVM. The FMIOM for regression can be tailored by implicitly branching on integer variables [3]. Such an implementation will also eliminate the need for Mbig − constraints in\nOptimization Problems (1) and (3) that can significantly hamper computation time of Integer Programming problems. In addition, because provably optimal solution is not critical in this context, we can prematurely terminate branch and bound procedure at 5% and 10% relative optimality gap or by time limit. However, even with all these improvements, FMIOM does not have superior performance over methods like Classification Trees with respect to time. This method would be appropriate for those who value prediction accuracy over computation time, which might be in areas of Medical and Genetic research. Third shortcoming of FMIOM is its lack of interpretability such as ANOVA interpretation. This weakness is shared by ANN and SVM. Unfortunately, not much can be done to improve this problem for FMIOM. Thus, if decision rules or variable importance information are vital to Data Mining application, tools such as Classification Trees would be more suitable. FMIOM may find similar audience that might find its classification accuracy more valuable. Also, FMIOM is able to handle categorical variables like SVM which might be an additional benefit in certain applications."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "FMIOM presents a new approach to solve Regression problem. The methodology exploits discrete character of problem and incorporates clustering to reduce dimensionality, non–linear transformations to improve predictive power, Mixed Integer Optimization to group points together and eliminate outlier data to represent\ngroups by polyhedral regions. In this way large scale problems are solved within practical limits. The data points are separated into different polyhedral regions. Each region has its own distinct regression coefficients. Computational results on real data sets are encouraging because FMIOM has outperformed other techniques like LLSR and ANN. We hope that these encouraging results will motivate Statistics, Data Mining and Machine Learning community to reexamine Integer Optimization as viable tool in Statistical Computing."
    } ],
    "references" : [ {
      "title" : "Mathematical Programming in Statistics",
      "author" : [ "T.S. Arthanari", "Y. Dodge" ],
      "venue" : "Wiley Classics Library Edition, Wiley Interscience, New York",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Optimal Decision Trees",
      "author" : [ "K.P. Bennett", "J.A. Blue" ],
      "venue" : "Mathematical Report 214, Department of Mathematical Sciences, Rensselaer Polytechnic Institute, Troy, New York",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Computational Study of families of Mixed Integer Quadratic Programming Problems",
      "author" : [ "D. Bienstock" ],
      "venue" : "Mathematical Programming, vol. 74, pp. 121–140",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Pattern Recognition with Fuzzy Objective Function Algorithms",
      "author" : [ "J.C. Bezdek" ],
      "venue" : "Plenum Press, New York",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Classification and Regression Trees",
      "author" : [ "L. Breiman", "J. Friedman", "R. Olshen", "C. Stone" ],
      "venue" : "Wadsworth International, Belmont, California",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Multivariate Adaptive Regression Splines",
      "author" : [ "J. Friedman" ],
      "venue" : "Annals of Statistics, vol. 19, no. 1, pp. 1–67",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Greedy Function Approximation: A Gradient Boosting  Machine",
      "author" : [ "J. Friedman" ],
      "venue" : " hhtp://wwwstat.stanford.edu/~jhf/ftp/trebst.ps",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Technical Report 989",
      "author" : [ "H. Kim", "W.Y. Loh", "CRUISE User Manual" ],
      "venue" : "Department of Statistics, University of Wisconsin, Madison",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Linear and Nonlinear separation of Patterns by Linear Programming",
      "author" : [ "O.L. Mangasarian" ],
      "venue" : "Operations Research, vol. 13, no. 3, pp. 444–452",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Programs for Machine Learning",
      "author" : [ "R. Quinlan" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1993
    }, {
      "title" : "Everything old is new again: A fresh look at historical approaches in Machine Learning",
      "author" : [ "R. Rifkin" ],
      "venue" : "PhD Thesis, Electrical Engineering, Computer Science and Operations Research, Massachusetts Institute of Technology, Cambridge",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Robust Regression and Outlier Detection",
      "author" : [ "P.J. Rousseeuw", "A.M. Leroy" ],
      "venue" : "Wiley, New York",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Modern Regression Methods",
      "author" : [ "T.P. Ryan" ],
      "venue" : "Wiley Series in Probability and Statistics, John Wiley and Sons, New York",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "The Nature of Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "Springer Verlag, New York",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Fuzzy Logic and its Applications",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Academic Press, New York",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "The concept of a linguistic variable and its application to approximate reasoning–I",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Information Science, vol. 8, pp. 199–249",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "The concept of a linguistic variable and its application to approximate reasoning–II",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Information Science, vol. 8, pp. 301–357",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "The concept of a linguistic variable and its application to approximate reasoning–III",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Information Science, vol. 8, pp. 43–80",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1976
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14].",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14].",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14].",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 8,
      "context" : "0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14].",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "0, CRUISE [2], [5], [8], [9], Multivariate Adaptive Regression Splines (MARS) [6] and Support Vector Machines (SVM) [9], [11], [14].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "While Continuous Optimization methods have been widely used in Statistics and have significant impact in last 30 years [1], Integer Optimization has limited impact in Statistical Computing.",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "While statisticians have recognized that problems like classification and regression can be formulated as Integer Optimization problems [1], the belief was formed in early 1970s that these methods are not tractable in practical computational settings.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "Fuzzy theory was originally developed by Zadeh [15] to deal with problems involving linguistic terms [16], [17], [18] and have been successfully applied to various applications in Engineering and Science.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "Fuzzy theory was originally developed by Zadeh [15] to deal with problems involving linguistic terms [16], [17], [18] and have been successfully applied to various applications in Engineering and Science.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 16,
      "context" : "Fuzzy theory was originally developed by Zadeh [15] to deal with problems involving linguistic terms [16], [17], [18] and have been successfully applied to various applications in Engineering and Science.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "Fuzzy theory was originally developed by Zadeh [15] to deal with problems involving linguistic terms [16], [17], [18] and have been successfully applied to various applications in Engineering and Science.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "In contrast, traditional regression models deal with outliers after slopes have been determined by examining which points contribute most to total prediction error [12], [13].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : "In contrast, traditional regression models deal with outliers after slopes have been determined by examining which points contribute most to total prediction error [12], [13].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "Nearest Prototype ) 1 , ( > − k NP k clustering algorithm defined on d R [4] in ) , ( y x space is used to find L clusters.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "Boston, Abalone and Auto data and Friedman’s [7] generated data sets.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "The FMIOM for regression can be tailored by implicitly branching on integer variables [3].",
      "startOffset" : 86,
      "endOffset" : 89
    } ],
    "year" : 2010,
    "abstractText" : "Mixed Integer Optimization has been a topic of active research in past decades. It has been used to solve Statistical problems of classification and regression involving massive data. However, there is an inherent degree of vagueness present in huge real life data. This impreciseness is handled by Fuzzy Sets. In this Paper, Fuzzy Mixed Integer Optimization Method (FMIOM) is used to find solution to Regression problem. The methodology exploits discrete character of problem. In this way large scale problems are solved within practical limits. The data points are separated into different polyhedral regions and each region has its own distinct regression coefficients. In this attempt, an attention is drawn to Statistics and Data Mining community that Integer Optimization can be significantly used to revisit different Statistical problems. Computational experimentations with generated and real data sets show that FMIOM is comparable to and often outperforms current leading methods. The results illustrate potential for significant impact of Fuzzy Integer Optimization methods on Computational Statistics and Data Mining. Keywords–Mixed Integer Optimization; Fuzzy Sets; Regression; Polyhedral Regions",
    "creator" : "PScript5.dll Version 5.2"
  }
}