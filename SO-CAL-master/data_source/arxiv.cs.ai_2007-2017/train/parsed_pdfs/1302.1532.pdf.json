{
  "name" : "1302.1532.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Standard Approach for Optimizing Belief Network Inference using Query DAGs",
    "authors" : [ "Adnan Darwiche" ],
    "emails" : [ "darwiche@aub.", "@rise." ],
    "sections" : null,
    "references" : [ {
      "title" : "Query DAGs: A practical paradigm for implementing belief network inference",
      "author" : [ "Adnan Darwiche", "Gregory Provan" ],
      "venue" : "In Proceedings of the 12th Conference on Uncertainty in Artificial In­ telligence (UAI),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Query DAGs: A practical paradigm for implementing belief-network inference",
      "author" : [ "Adnan Darwiche", "Gregory Provan" ],
      "venue" : "Journal of Artificial In­ telligence Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Inference in belief networks: A procedural guide",
      "author" : [ "Cecil Huang", "Adnan Darwiche" ],
      "venue" : "International Journal of Approximate Reasoning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1996
    }, {
      "title" : "Bayesian updating in recursive graphical models by local computation",
      "author" : [ "F.V. Jensen", "S.L. Lauritzen", "K.G. Olesen" ],
      "venue" : "Computational Statistics Quar­ terly,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1990
    }, {
      "title" : "Approxima­ tions in Bayesian belief universes for knowledge based systems",
      "author" : [ "Frank Jensen", "Stig K. Andersen" ],
      "venue" : "In Proceedings of the Sixth Con­ ference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "is the same as the time complexity of the underlying belief network algorithm; and (c) a Q-DAG evaluator is a very simple piece of software [1, 2] .",
      "startOffset" : 140,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "is the same as the time complexity of the underlying belief network algorithm; and (c) a Q-DAG evaluator is a very simple piece of software [1, 2] .",
      "startOffset" : 140,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "In a nutshell, when us­ ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero­ compression, and network-pruning [3, 4, 5].",
      "startOffset" : 211,
      "endOffset" : 220
    }, {
      "referenceID" : 3,
      "context" : "In a nutshell, when us­ ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero­ compression, and network-pruning [3, 4, 5].",
      "startOffset" : 211,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : "In a nutshell, when us­ ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero­ compression, and network-pruning [3, 4, 5].",
      "startOffset" : 211,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : "Details of this generation pro­ cess can be found in [1, 2].",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "Details of this generation pro­ cess can be found in [1, 2].",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "Each root node in a Q-DAG is either a nu­ meric node, Num, which is labeled with a number p in [0, 1], or an evidence-specific node, Esn, which is la­ beled with a pair (V, v) where V is a variable and v is a value of the variable.",
      "startOffset" : 95,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "Zero compression is an optimization technique that is typically implemented in algorithms based on join trees [5].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "Zero compression, as presented in [5], addresses this wasteful propagation by visiting entries in cliques to identify and annihilate the zero entries.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].",
      "startOffset" : 120,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].",
      "startOffset" : 120,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].",
      "startOffset" : 120,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "Figure 15, which is borrowed from [3], depicts the over­ all control of the join tree algorithm.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "Details of these operations are beyond the scope of this paper, but see [3] for a relatively comprehen­ sive discussion.",
      "startOffset" : 72,
      "endOffset" : 75
    } ],
    "year" : 2011,
    "abstractText" : "This paper proposes a novel, algorithm­ independent approach to optimizing belief network inference. Rather than designing op­ timizations on an algorithm by algorithm ba­ sis, we argue that one should use an unop­ timized algorithm to generate a Q-DAG, a compiled graphical representation of the be­ lief network, and then optimize the Q-DAG and its evaluator instead. We present a set of Q-DAG optimizations that supplant opti­ mizations designed for traditional inference algorithms, including zero compression, net­ work pruning and caching. We show that our Q-DAG optimizations require time linear in the Q-DAG size, and significantly simplify the process of designing algorithms for opti­ mizing belief network inference.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}