{
  "name" : "1404.4258.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized Approximate Linear Programming Approximation Accuracy",
    "authors" : [ "Gavin Taylor", "Connor Geer", "David Piekut" ],
    "emails" : [ "TAYLOR@USNA.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In recent years, the Reinforcement Learning community has paid considerable attention to creating value function approximation approaches which perform automated feature selection while approximating a value function (Kolter & Ng, 2009; Johns et al., 2010; Mahadevan & Liu, 2012; Liu et al., 2012). This approach frees researchers from hand-selecting and -tuning feature sets, while greatly increasing approximation accuracy. One of these ap-\nProceedings of the 31st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32.\nproaches, L1-Regularized Approximate Linear Programming (RALP) (Petrik et al., 2010; Taylor & Parr, 2012) is unique in that it results in an approximation of the optimal value function and makes use of off-policy samples.\nHowever, important aspects of RALP have not been fully explored or explained. In particular, the objective function of the linear program offers an opportunity to create a better approximation of the value function in some regions of the state space at the expense of others. Optimal policies of many realistic reinforcement learning problems heavily traffic some parts of the state space while avoiding others, making an understanding of this flexibility useful.\nTherefore, in Section 4, we derive a new error bound for the RALP approximation, which is tighter than those presented by Petrik et al. (2010) and provides new insight into the result of changing the state-relevance weights in the objective function. In addition, this bound provides an insight into the types of MDPs particularly well suited to the RALP approach. Finally, this section provides evidence that rather than weighting all states equally, as was done by Petrik et al. (2010) and Taylor and Parr (2012), states should instead be weighted in proportion to the stationary distribution under the optimal policy.\nAdditionally, if the sampling distribution is not uniform across the state space, the approximation can be heavily affected. In realistic domains, sampling is rarely uniform. Therefore, for RALP to be appropriate for these domains, it is important to address this lack of analysis and understand how the approximation is likely to be altered due to the problem’s available sampling scheme.\nSection 5 then discusses the impact on the approximation if learning is performed using samples drawn other than uniformly from the state space. We demonstrate that sampling from a distribution acts as a de facto alteration of the objective function. We also discuss the effects of sampling distributions in light of the bounds from Section 4.\nThe intuition provided by Sections 4 and 5 are then demon-\nar X\niv :1\n40 4.\n42 58\nv2 [\ncs .A\nI] 2\n4 A\npr 2\n01 4\nstrated experimentally in Section 6. Using a simple, easily visualized domain, we demonstrate the effect of the various parameters on approximation quality."
    }, {
      "heading" : "2. Notation and Problem Statement",
      "text" : "In this section, we formally define Markov decision processes and linear value function approximation. A Markov decision process (MDP) is a tuple (S,A, P, R, γ), where S is the measurable, possibly infinite set of states, and A is the finite set of actions. P ∶ S ×S ×A ↦ [0, 1] is the transition function, where P(s′∣s, a) represents the probability of transitioning from state s to state s′, given action a. The function R ∶ S ↦ R is the reward function, and γ, a number between 0 and 1, is the discount factor, representing the comparative desire for reward at the current time step to the desire for reward at the next time step.\nWe are concerned with finding a value function V that maps each state s ∈ S to the expected total γ-discounted reward for the process. Value functions can be useful in creating or analyzing a policy π ∶ S ×A→ [0, 1] such that for all s ∈ S, ∑a∈A π(s, a) = 1. The transition and reward functions for a given policy are denoted by Pπ and Rπ . We denote the Bellman operator for a given policy as Tπ , and the max Bellman operator simply as T. That is, for some state s ∈ S:\nTπV(s) = R(s)+ γ∫S P(ds ′∣s, π(s))V(s′)\nTV(s) = max π∈Π TπV(s).\nWe additionally denote the Bellman operator for selecting a particular action a as\nTaV(s) = R(s)+∫S P(ds ′∣s, a)V(s′).\nThe optimal value function V∗ satisfies TV∗(s) = V∗(s) for all s ∈ S. For simplicity, in this paper we will assume no noise exists in the MDP; results can be easily extended to noisy domains using Taylor and Parr’s (2012) approach of local smoothing.\nSets of samples, therefore, are defined as Σ ⊆ {(s, a, r, s′∣s, s′ ∈ S, a ∈ A}, where s′ is the state the agent arrived at given that it started in state s and took action a, and r = R(s). An individual sample in the set Σ will be denoted σ, and an element of a sample σ will be denoted with superscripts; that is, the s component of a σ = (s, a, r, s′) sample will be denoted σs.\nWe focus on linear value function approximation for discounted infinite-horizon problems, in which the value function is represented as a linear combination of possibly nonlinear basis functions (vectors). For each state s, we define\na vector Φ(s) of features. The rows of the basis matrix Φ correspond to Φ(s), and the approximation space is generated by the columns of the matrix. That is, the basis matrix Φ, and the approximate value function V̂ are represented as:\nΦ = (− Φ(s1) −⋮ ) V̂ = Φw.\nThis form of linear representation allows for the calculation of an approximate value function in a lower-dimensional space, which provides significant computational benefits over using a complete basis; if the number of features is small and the environment is noisy, this framework can also guard against overfitting any noise in the samples.\nIf we define Φ to be overcomplete, with potentially far more features than sampled states, then to receive the above benefits we must perform feature selection. In this process, a few features are chosen from the set, the span of which will represent the available linear approximation space. We can use L1 regularization to calculate a sparse w, in which nearly all features receive a weight of 0, thereby performing automated feature selection."
    }, {
      "heading" : "3. Previous Work",
      "text" : "RALP was introduced by Petrik et al. (2010) to extend the capabilities of the linear programming approach to value function approximation (d’Epenoux, 1963; Schweitzer & Seidmann, 1985; de Farias & Van Roy, 2003). Given a set of samples Σ, the linear program is defined as follows:\nmin w ρTΦw s.t. Tσa Φ(σs)w ≤ Φ(σs)w ∀σ ∈ Σ\n∥w∥−1 ≤ ψ, (1)\nwhere ρ is a distribution, which we call the state-relevance weights, in keeping with the (unregularized) Approximate Linear Programming (ALP) terminology of de Farias and Van Roy (2003). ∥w∥−1 is the L1 norm of the vector consisting of all weights excepting the one corresponding to the constant feature.\nThis final constraint, which contributes L1 regularization, provides several benefits. First, regularization in general ensures the linear program is bounded, and produces a smoother value function. Second, L1 regularization in particular produces a sparse solution, producing automated feature selection from an overcomplete feature set. Finally, the sparsity results in few of the constraints being active, speeding the search for a solution by a linear program solver, particularly if constraint generation is used.\nOther techniques have used L1 regularization in similar ways. LARS-TD (Kolter & Ng, 2009) and LC-MPI (Johns et al., 2010) both approximate the fixed point of the L1regularized LSTD problem. Mahadevan and Liu (2012)\nintroduced the use of mirror descent, which has a computation complexity which allows it to be better suited than many other approaches for online reinforcement learning problems. These above approaches are most reliable when samples are collected on-policy. Liu et al. (2012) introduced RO-TD, which converges to an approximation of the value function of a given policy, even when trained on offpolicy samples.\nIn contrast to the above approaches, RALP provides an approximation to the value function of the optimal policy, even when samples are drawn from non-optimal or random policies. Approximations produced by RALP have bounded error, and have performed well experimentally in comparison to other approaches. Finally, in noisy domains, the well-known weakness of linear programming approaches to value function approximation can be mitigated or eliminated using local smoothing (Taylor & Parr, 2012).\nThis previous work on RALP has largely ignored the staterelevance weights ρ in the objective function, setting ρ = 1 without discussion. However, a change in the objective function would obviously affect the solution of the linear program. In certain practical situations this would be useful to understand. For example, consider the task of calculating a value function for an aircraft in flight. The space of possible flight attitudes, velocities, etc. is very large. However, the percentage of this space trafficked in non-catastrophic flight is small; it is likely worthwhile to improve the approximation quality in this relevant portion, at the expense of accuracy in the remainder of the space.\nSome previous results exist regarding the effects of changing the state-relevance weights in the closely-related ALP (de Farias & Van Roy, 2003). However, the assumptions and types of appropriate problems are very different between ALP and RALP, making these previous results insufficient. First, ALP assumes a sample is drawn from every state-action pair, an assumption which is not required for RALP. This means it was not necessary with ALP to consider the behavior of the approximation between samples or in a continuous space. Furthermore, it was not necessary to consider the effects of sampling distributions at all. This assumption was later weakened by a followup paper (de Farias & Van Roy, 2004), but not to a degree necessary for large or continuous problems, particularly when large numbers of samples are not available. The second difference is ALP is unregularized, simplifying the definition of the feasible space of the linear program. Despite these differences, these previous results will serve as a useful guide.\nAdditionally, previous work does not cover the effects of sampling schemes on RALP. If states are sampled heavily in one portion of the state space, the linear program will\nchoose a solution which tightens constraints in that portion of the space over others. In realistic settings, it can be difficult to sample uniformly, making it especially important to understand the effect of other sampling distributions on the resulting approximate value function.\nThe remainder of this document fills in these gaps in the previous work."
    }, {
      "heading" : "4. State-Relevance Weights",
      "text" : "The theoretical results presented on RALP in the literature thus far offer no insights into the behavior of the approximation as the state-relevance weights are altered. Therefore, it is necessary to derive new bounds for RALP which contain ρ to understand its effects.\nThe approach we take follows the example of a proof introduced by de Farias and Van Roy (2003) to bound the ALP approximation, but we extend it match the weaker assumptions of RALP, along with the requirement that the weights be L1 regularized.\nWe begin by defining the relevant notation. In the following definitions, we will use R+ to refer to the set of nonnegative real numbers.\nDefinition 1 We introduce an operator H, defined by\n(HL)(s) = max a∈A ∫S p(s\n′∣s, a)L(s′) ds′,\nfor all L ∶ S →R+.\nTherefore, (HL)(s) represents the expected value of L of the next state if actions are chosen to maximize L.\nDefinition 2 A non-negative function L ∶ S →R+ is a Lyapunov function if there exists a subset of states B and a βL < 1 such that for all s ∈ S ∖B, γ(HL)(s) ≤ βLL(s).\nFor an example of a Lyapunov function defined over a MDP, consider the simple case of a random walk along the non-negative number line, so that s ∈ Z+. Assume a single action, in which the probability\np = p(st+1 = max(m − 1, 0)∣st = m) > 0.5\nand p(st+1 = m + 1∣st = m) = 1− p.\nIf L(s) = s and B = {0}, then L is a valid Lyapunov function, because L(s) is expected to decrease for all s ≠ 0. Lyapunov functions are often used to prove stability of Markov processes. Definition 2 differs from the definition commonly used for stability analysis in a few ways. First, in stability analysis, it is required that S be countable, and\nthat B be finite. We have made neither of these assumptions, though our bounds will be tightest when B is small. The second difference is we have added a multiplicative term of γ. Because of these differences, a Lyapunov function as defined in Definition 2 may not strictly evidence stability.\nBesides stability analysis, Lyapunov functions have also previously appeared in Reinforcement Learning literature, though in different contexts from our application (Perkins & Barto, 2003; Rohanimanesh et al., 2004).\nIn the remainder of this section, we will occasionally refer to the weighted max-norm, where for a vector U and a function F, ∥U∥∞,F = maxi ∣U(si) ⋅ F(si)∣, and the weighted L1 norm, where ∥U∥1,F = ∑i ∣U(si) ⋅ F(si)∣. We will start with the following Lemma. To conserve space, and because the proof is similar to one presented by de Farias and Van Roy (2003), we reserve the proof for Appendix A.\nLemma 1 Assume samples have been drawn from every possible state-action pair. LetW = {w ∶ ∥w∥−1 ≤ ψ}, and let w∗ = minw∈W ∥V∗ −Φw∥∞. Additionally, for a given Lyapunov function ΦwL, let\nw̄ = w∗ + ∥V∗ −Φw∗∥∞, 1ΦwL ( 2 1− βΦwL − 1)wL."
    }, {
      "heading" : "If a Lyapunov function ΦwL is constructed such that w̄ ∈",
      "text" : "W , then,\n∥V∗ −Φw̃∥1,ρ ≤ 2ρTΦwL 1− βΦwL min w∈W ∥V∗ −Φw∥∞, 1ΦwL .\nWe note that proving the existence of a Lyapunov function as required in the above lemma is trivial. First we construct a weight vector wL with all zeros but for a positive weight corresponding to the bias feature; this results in ΦwL being a valid Lyapunov function. Second, we note that in this case ∥w̄∥−1 = ∥w∗∥−1, meeting the requirement that w̄ ∈ W . We must now remove the assumption that a sample exists for every state-action pair. To enable us to bound the behavior of the value function between samples, we make the following assumption, similar to the sufficient sampling assumption made by Petrik et al. (2010):\nAssumption 1 Assume sufficient sampling, that is, for all s ∈ S and a ∈ A, there exists a σ ∈ Σ such that σa = a and:\n∥φ(σs)− φ(s)∥∞ ≤δφ ∥R(σs)− R(s)∥∞ ≤δR\n∥p(s′∣σs, a)− p(s′∣s, a)∥∞ ≤δP ∀s′ ∈ S\nThis assumption is not unrealistic. For example, if the reward function, basis functions, and transition functions are Lipschitz continuous, then appropriate values of δφ, δR, and δP are easily calculated given the greatest distance between any point in the state space and a sampled point.\nWe describe the maximum difference between the RALP solution and the true solution by using the limits from Assumption 1 to demonstrate the following Lemma:\nLemma 2 Let M1 be an MDP with optimal value function V∗1 , and let Σ be an incomplete set of samples drawn from M1 such that not all state-action pairs are sampled, but Assumption 1 is fulfilled. Therefore, the RALP for M1 has the constraint Tσa Φ(σs)w ≤ Φ(σs)w for all σ ∈ Σ, and the bounded L1 constraint, but is missing all other possible RALP constraints.\nThere exists an MDP M2 with an optimal value function V∗2 , identical in every way to M1 but for the reward function, such that the RALP solution with no missing constraints is equal to the RALP solution constructed on Σ, and ∥V∗1 −V∗2 ∥∞ ≤ 2(δφψ+δR+δPψ) 1−γ .\nProof Sketch: We first show that if R1 and R2 are the respective reward functions of M1 and M2,\n∥R1 − R2∥∞ ≤ 2(δφψ + δR + δPψ).\nWe then show that if ∥R1 − R2∥∞ ≤ δ,\n∥V∗1 −V∗2 ∥∞ ≤ δ\n1− γ .\nWe leave the details of the proof for Appendix B.\nWe are now prepared to present the first result of this paper.\nTheorem 1 Let ΦwL be a Lyapunov function as required by Lemma 1. Define Σ, MDPs M1 and M2 and their respective optimal value functions V∗1 and V ∗ 2 as in Lemma 2. Define ep = δφψ+ δR + δPψ. Let w̃ be the RALP solution to M1.\n∥V∗1 −Φw̃∥1,ρ ≤ 2ρTΦwL 1− βΦwL min w∈W ∥V∗2 −Φw∥∞, 1ΦwL + 2ep 1− γ .\nProof: Because w̃ is an optimal solution given all samples from M2, Lemmas 1 and 2 allow us\n∥V∗2 −Φw̃∥1,ρ ≤ 2ρTΦwL 1− βΦwL min w∈W ∥V∗2 −Φw∥∞, 1ΦwL .\nAdditionally, Lemma 2 gave us ∥V∗1 −V∗2 ∥∞ ≤ 2ep 1−γ .\nBecause ρ is a probability distribution,\n∥V∗1 −V∗2 ∥1,ρ ≤ ∥V∗1 −V∗2 ∥∞ ≤ 2ep\n1− γ .\nDue to the triangle inequality, Theorem 1 follows."
    }, {
      "heading" : "4.1. Discussion",
      "text" : "This bound is not only tighter than those presented in previous literature, but also allows us to analyze the RALP approximation quality in new ways. First, we can observe which Lyapunov functions would result in a better approximation, and discuss the characteristics of MDPs which allow for those Lyapunov functions, and therefore lend themselves particularly well to value function approximation by RALP. Second, as ρ now appears in our bound, the bound provides a way of relating our choice of ρ to approximation quality, allowing for more intuitive and successful parameter assignments. We address these in turn.\nThe Lyapunov function ΦwL appears in the first term of our bound in three places, namely the dot product with ρ, the definition of βΦwL , and in the norm defining the “optimal” w to which we compare our approximation. We first note that the bound becomes smaller as βΦwL decreases. This suggests that the more stable the MDP, the better RALP can approximate the value function.\nThis interpretation of Theorem 1 leads to other intuitive explanations. Consider an MDP with only L(s) = 1 ∀s ∈ S as a Lyapunov function. Now assume two nearby samples, one where a “good” action is taken, in the direction of positive reward, and another where a “bad” action is taken, in the direction of negative reward. When the linear program is solved, due to the proximity of the two samples, the constraint corresponding to the “bad” sample is nearly certain to be loose, and may as well be removed. However, if the MDP is highly stable, then these two extremely different samples would be unlikely, and both constraints are candidates for being tight. Therefore, more samples from an MDP with a small βL are likely to be involved in defining the feasible space, potentially resulting in an improved approximation.\nThe appearance of the Lyapunov function in the norm of the bound indicates the bound is tighter when the feature space Φ allows for a close approximation in areas where the Lyapunov function is small. Bertsimas et al. (1998) demonstrated that a small Lyapunov function value correlates in expectation with a higher probability in the stationary distribution of a Markov chain. This is particularly interesting when considering the appearance of the dot product between the Lyapunov function and ρ. This dot product makes it apparent that the approximation improves when ρ is large only where ΦwL is small. This provides evidence that the stationary distribution of the MDP under the opti-\nmal policy may be an advantageous setting for ρ. This evidence meshes well with the intuition that greater accuracy is most useful in frequently-visited states."
    }, {
      "heading" : "5. Sampling Distribution",
      "text" : "Imagine an MDP with a small finite state space and a single action. Ideal sampling would provide a single sample from each state, giving us an objective function of ∑s∈S ρ(s)Φ(s)w. However, if sampling from a distribution across the state space, this ideal situation would be unlikely; some states would go unsampled, while others would be sampled multiple times. Because the objective function is defined on samples, this means states that were sampled multiple times would appear in the objective function multiple times, causing the linear program to tighten constraints at those states at the expense of accuracy in other states.\nOf course, a similar scenario occurs in infinite state spaces as well. Multiple states near to each other may be sampled, while other regions have very few samples; this encourages the linear program to choose features and an approximate value function which tightens constraints in heavilysampled regions at the expense of sparsely-sampled regions. In this section we discuss the effects of sampling from an arbitrary distribution over the state space µ and ways this can help the researcher understand how to design sampling methods.\nObservation 1 Let Σ1 and Σµ be sample sets of equal cardinality N drawn from the state space from the uniform distribution and an arbitrary distribution µ, respectively. Let Φ1 and Φµ be the feature matrices defined over the states of sets Σ1 and Σµ. For any weight vector w,\nE [µΦ1w] = E [1Φµw] .\nThis observation is easy to support; both expectations equal ∫S µ(s)Φ(s)w ds. This means that for any w, sampling from a non-uniform distribution µ provides an equivalent objective function in expectation to sampling from a uniform distribution but setting the state-relevance weights equal to µ. We note this is not equivalent to expecting the same approximate value function as the constraints remain different; this makes the effect of altering the sampling distribution greater than that of altering the objective function alone.\nAdditionally, the bound presented in Theorem 1 offers an interpretation of results from sampling from a distribution. As sampling becomes less uniform, ep likely increases, due to the existence of larger unsampled regions. For example, this would happen in the Lipschitz-continuous case for fulfilling Assumption 1. However, for a sampling distribution\nwhich is dense where the Lyapunov value is small, then the total Lyapunov value in the numerator of the first addend is small, as well. Therefore, it may be that the ideal sampling distribution is one which is dense where the Lyapunov value is low, but still provides sufficient coverage for ep to be small."
    }, {
      "heading" : "6. Experimental Results",
      "text" : "In this section, we demonstrate experimentally the conclusions drawn in the previous sections. Previous literature has already demonstrated RALP’s effectiveness in common benchmark domains; the purpose of this section therefore, is to clearly illustrate the conclusions of the previous sections. In a simple, easily visualized domain, we make a series of comparisons. First, we compare the approximation accuracy of sampling from a domain with a stable Lyapunov function to the accuracy resulting from sampling from a domain without such a function. Next, we compare the accuracy of the approximation resulting from sampling uniformly to the accuracy of the approximation resulting from sampling from two different nonuniform distributions. Finally, we compare the approximation accuracy of calculating an approximation with ρ = 1 to the approximation accuracy of calculating an approximation when ρ is nonuniform. This is demonstrated using two different, nonuniform distributions.\nThe results were obtained by drawing samples and calculating an approximation 500 times for each compared approach; the error ∣V∗ −Φw∣ was then calculated, and averaged across all 500 trials. Finally, we calculate and display the difference between the average errors from the two approaches. So, if V̂Ai is the approximation from the i-th run on approach A, then when comparing two approaches, A and B, a point on the graphs of Figure 2 equals\n500 ∑ i=1 ∣V∗(s)− V̂Ai (s)∣ 500 − 500 ∑ i=1 ∣V∗(s)− V̂Bi (s)∣ 500 ."
    }, {
      "heading" : "6.1. Domain",
      "text" : "All of the experiments were run on a domain defined by a 25 by 25 grid world. This world included four reward regions in the corners of the grid, each of which consisted of 9 states, as can be seen in Figure 1(a). Two reward regions (colored gold) had a reward of 1, while the others had a reward of -1 (colored red). The remaining states had a reward of 0. Actions were to move one square in any of the four directions, unless constrained by a wall, in which case that action would result in no movement. The discount factor γ was set to 0.95. For all trials, the feature set consisted of symmetric Gaussian features centered around each σs with variances of 2, 5, 10, 15, 25, 50, and 75, plus the bias feature, resulting in 9n+1 features for n samples. The optimal value function can be seen in Figure 1(b).\nThis value function is an easy one for RALP to approximate with proper settings of the regularization parameter and sufficient sampling. The number of samples and choices of the regularization parameter ψ were therefore chosen to illustrate the differences between the results of the compared methods, not to optimize performance."
    }, {
      "heading" : "6.2. Lyapunov Stable Domain",
      "text" : "First, we demonstrate the improvement in RALP’s approximation when the domain has a stable Lyapunov function. A stable Lyapunov function can be created by forcing the actor into a defined area in the state space. In order to keep the representational difficulty of the optimal value functions the same, we created a Lyapunov function by eliminating actions which move the actor further from the nearest positive reward. This preserves the optimal policy of the unaltered domain, keeping the optimal value functions identical, making approximation accuracy a fair comparison.\nIn the domain without a stable Lyapunov function, the actor was free to move in the state space based on a random choice among the four actions. However, in the domain with a stable Lyapunov function, the actor was only allowed to move in the two directions which would not move it further from the nearest goal. We note that not all remaining actions are optimal, so sampling still includes offpolicy samples.\nThis creates a Lyapunov function where L(s) equals the Manhattan distance from s to the nearest of state (1,1) or (25,25), and B = {(1, 1), (25, 25)}. In each trial, we uniformly sampled 20 samples. The regularization parameter ψ was set to 0.2 and ρ was 1.\nThe result of subtracting the average errors of the approximation from the domain with a Lyapunov function from the domain without a Lyapunov function can be seen in Figure 2(a). Therefore, positive values indicate higher error from\nthe domain without a Lyapunov function.\nThe results show that an approximation learned from samples drawn from a stable domain is more accurate than an approximation learned from a less stable domain everywhere except for where s is near equal distance from the two goal states. This reinforces the intuition from Subsection 4.1 that samples drawn from a stable domain are more effective than those that are not, particularly near the most heavily-visited regions."
    }, {
      "heading" : "6.3. Sampling from a Nonuniform Distribution",
      "text" : "Next we illustrate the change in approximation accuracy when sampling from a nonuniform distribution µ. Section 5 presents evidence that a distribution which is most dense where L(s) is smallest may be advantageous. To create such a distribution, an agent was started at a random state, and was allowed to take the optimal policy for 25 steps. This was done 10,000 times, and the number of visits to each state was tabulated and normalized. This defined our distribution, which was heaviest on the edges and reward corners, and otherwise slightly increasing with increasing proximity to the positive reward regions. We will refer to this distribution as ζ.\nFor these trials, 20 samples were drawn per run from the\ndomain with the stable Lyapunov function as discussed in Subsection 6.2. The regularization parameter ψ was set to 1.5, and ρ = 1. The average error from the 500 uniformly sampled runs was subtracted from the average error from the 500 runs with µ = ζ; the result can be seen in Figure 2(b). Because the error from the nonuniform sampling was subtracted from the error from uniform sampling, the positive difference indicates the results from sampling from ζ were superior. The results show the distribution met the goal from Section 5; sampling was varied enough to keep ep low, while dense enough in the areas where L(s) was small.\nWe then subtracted ζ from 1 and normalized, making a distribution we will refer to as 1 − ζ, which was largest where an agent was least likely to traverse in the stable domain. We subtracted the average error from sampling from µ = 1 − ζ from the average error from sampling uniformly, to produce Figure 2(d). A positive value would indicate larger error from the approximations on uniformly sampled states. However, through the entirety of the state space, and particularly in the most trafficked areas, sampling from 1 − ζ gave us an inferior result. This provides evidence for the conclusions of Section 5 that a sampling distribution which is densest in the areas where the Lyapunov function is smallest would produce the best\napproximations."
    }, {
      "heading" : "6.4. Changing the State-Relevance Weights",
      "text" : "Lastly, we illustrate the effect on the approximation of changing the state-relevance weights. 200 samples were drawn uniformly from the state space; whereas the effects from the previous two experiments are most pronounced when samples are sparse, the effects from altering ρ are most pronounced when a number of constraints can be tightened in a given region.\nWe would prefer to set ρ to the stationary distribution; however, because the domain is not recurrent, this is not an option. However, the distribution ζ created for sampling in Subsection 6.3 is large where the Lyapunov function is small, making it a reasonable replacement. In one set of trials, ρ(s) was set to the value of this distribution at that state; in the other, ρ = 1. ψ was set to 4. Average error from the approximation resulting from a nonuniform ρ was subtracted from average error from the approximation resulting from a uniform ρ. Therefore, a positive value indicates a better approximation from the nonuniform ρ. Sampling was done uniformly from the stable domain.\nFigure 2(c) shows that nearly the entire state space was more accurate with a nonuniform ρ, except for where ζ(s) ≈ 0. The difference is small because with a large number of samples, both approximations were quite accurate.\nIn addition, we compared the uniform ρ approximation to an approximation using ρ = 1 − ζ, which is large where the Lyapunov value is large, resulting in an increased dot product ρΦw in Theorem 1. Again, we subtracted the error of the approximation resulting from ρ = ζ − 1 from the error of the approximation resulting from ρ = 1, producing Figure 2(e). A positive value indicates the nonuniform ρ approximated that state better than did using a uniform ρ. However, there are few positive values as the use of ρ = 1 − ζ resulted in a dramatically inferior approximation, particularly in areas where ρ was small.\nFrom both figures, it is clear that a higher ρ value in a given portion of the state space resulted in an improved approximation, particularly if designed with Theorem 1 in mind."
    }, {
      "heading" : "7. Conclusion",
      "text" : "The experimental success of RALP in previous literature, along with its easily-fulfilled assumptions, suggests promise for its application to real-life, complicated, and complex domains. Despite this promise, and despite the evidence that the effects are dramatic, no theory had been produced to analyze changes in the approximation quality given changes to the objective function parameter ρ, or due to differences in sampling strategies. These considerations\nare essential to the use of RALP in the real world; it is rarely possible to sample uniformly, and the importance of accuracy across the state space is rarely consistent.\nIn this paper, we demonstrate the importance of understanding these ideas, and produce a bound on the approximation error of RALP which is tighter and more informative than previous bounds. This bound provides intuition into the quality of the RALP approximation as a function of state-relevance weights and sampling distributions. In addition, we demonstrated that the quality of a RALP approximation is particularly good when the domain is stable and has a Lyapunov function with a small βL.\nFuture work remains, particularly in the area of solving the linear program quickly in the presence of large amount of data. Though convex optimization solvers are considered “fast,” with large amounts of data, the memory and time requirements may be too large for realistic use. Fortunately, it may be that the structure of the problem, the small percentage of tight constraints, and small percentage of active features will avail itself to faster, but equivalent, approaches."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Thank you to the anonymous reviewers for their help in improving this paper. Additionally, we are grateful for support from the Naval Research Laboratory Information Management & Decision Architecture Branch (Code 5580), as well as financial support by the Office of Naval Research, grant numbers N001613WX20992 and N0001414WX20507."
    }, {
      "heading" : "A. Proof of Lemma 1",
      "text" : "As stated in Section 4, this proof is very similar to Theorem 3 by de Farias and van Roy (2003), but we include it for clarity nonetheless. The Lemma constructs a point in the feasible space of the linear program which provides an approximation with bounded error, and then shows the point chosen by RALP must be no further than that constructed point. The proof first requires a series of additional Lemmas.\nLemma 3 For any functions V and V̄,\n∣TV̄ − TV∣ ≤ γ max π Pπ ∣V̄ −V∣ .\nProof: For any V and V̄,\nTV̄ − TV =max π (R + γPπV̄)−max π (R + γPπV)\n=R + γPπV̄ V̄ − R − γPπV V ≤γ max\nπ Pπ(V̄ −V)\n≤γ max π Pπ ∣V̄ −V∣ ,\nwhere πV and πV̄ represent the greedy policies with respect to value functions V and V̄. By reversing the terms, we can show TV − TV̄ ≤ γ maxπ Pπ ∣V̄ −V∣, leading to our result. ∎\nLemma 4 For any vector L with positive components and any vector V,\nTV ≤ V + (γHL + L)∥V −V∗∥∞, 1L .\nProof: Note that\n∣V∗(s)−V(s)∣ ≤ ∥V −V∗∥∞, 1L V(s).\nBecause of Lemma 3,\n∣(TV)(s)− (TV∗)(s)∣ ≤γ max π ∑ s′∈S Pπ(s, s′) ∣V(s′)−V∗(s′)∣\n≤γ∥V −V∗∥∞, 1L maxa∈A ∑s′∈S Pa(s, s′)L(s′) =γ∥V −V∗∥∞, 1L (HL)(s).\nDefine e = ∥V −V∗∥∞, 1L .\nTV(s) ≤V∗(s)+ γe(HL)(s) ≤V(s)+ eL(s)+ γe(HL)(s).\n∎\nLemma 5 Let wL be a weight vector such that ΦwL is a Lyapunov function, w be an arbitrary weight vector, and\nw̄ = w + ∥V∗ −Φw∥∞, 1ΦwL ( 2 1− βΦwL − 1)wL.\nThen, TΦw̄ ≤ Φw̄.\nProof: Let e = ∥V∗ −Φw∥∞, 1ΦwL . For any state s ∈ S,\n∣(TΦw̄)(s)− (TΦw)(s)∣\n= ∣(T [(Φw + e( 2 1− βΦwL − 1)ΦwL]) (s)− (TΦw)(s)∣ ≤γ max π ∑ s′∈S Pπ(s, s′)\n⋅ ∣(Φw(s′)+ e( 2 1− βΦwL − 1)ΦwL(s ′))−Φw(s′)∣\n≤γ max π ∑ s′∈S Pπ(s, s′)e( 2 1− βΦwL − 1)(ΦwL)(s ′)\n=γe( 2 1− βΦwL − 1)(HΦwL)(s).\nThe first line is a replacement of w̄ with its definition, the second is due to Lemma 3, the third is due to the cancellation of the two Φw terms and the fact that because ΦwL is a Lyapunov function, ( 21−βΦwL\n− 1) > 0 (note this is true for states in sets B and S ∖ B as defined in Definition 1), and the final line is due to Definition 2.\nFrom this, we can conclude\nTΦw̄ ≤ TΦw + γe( 2 1− βΦwL − 1) HΦwL.\nWe can apply Lemma 4 to get\nTΦw̄ ≤ Φw + e(γHΦwL +ΦwL), and therefore,\nTΦw̄ ≤Φw + e(γHΦwL +ΦwL)+ γe( 2\n1− βΦwL − 1) HΦwL\n=Φw + e( 2 1− βΦwL − 1)ΦwL − e( 2 1− βΦwL − 1)ΦwL\n+ e(γHΦwL +ΦwL)+ γe( 2\n1− βΦwL − 1) HΦwL\n=Φw̄ − e( 2 1− βΦwL − 1)ΦwL + e(γHΦwL +ΦwL)\n+ γe( 2 1− βΦwL − 1) HΦwL\n=Φw̄ + e(γHΦwL +ΦwL)− e( 2\n1− βΦwL − 1)(ΦwL − γHΦwL)\n≤Φw̄ + e(γHΦwL +ΦwL)− e(ΦwL + γHΦwL) =Φw̄.\nThe penultimate line can be shown given that ΦwL − γHΦwL > 0 and\n2 1− βΦwL − 1 = 2 1−maxs∈S∖B((γ(HΦwL)(s))/((ΦwL)(s))) − 1\n= max s∈S∖B (ΦwL)(s)+ γ(HΦwL)(s) (ΦwL)(s)− γ(HΦwL)(s)\n∎ Lemma 5 demonstrates that all constraints in RALP will be satisfied by w̄, with the exception of the constraint enforcing the L1 regularization. However, we have required even this constraint to be satisfied by requiring w̄ ∈ W . Therefore, w̄ lies in the feasible region for RALP.\nLemma 6 If every state-action pair is represented with a constraint in the RALP, a vector w̃ solves the RALP if and only if it solves\nargmin w\n∥V∗ −Φw∥1,ρ\ns.t. TaΦ(s)w ≤ Φ(s)w ∀s ∈ S , a ∈ A ∥w−1∥1 ≤ ψ\nProof: For any policy π, the Bellman operator Tπ is a contraction in max norm. If the Bellman error is one-sided, T is also monotonic. Therefore, for any V such that V ≥ TV, V ≥ TV ≥ T2V ≥ V∗. Therefore, any w that is a feasible solution to a RALP satisfies Φw ≥ V∗. From this, we can conclude\n∥V∗ −Φw∥1,ρ =∑ x∈S ρ(x) ∣V∗(x)−Φ(x)w∣\n=ρTΦw − ρTV∗.\nBecause V∗ is constant, minimizing ρTΦw with RALP constraints is equivalent to minimizing ∥V∗ −Φw∥1,ρ with RALP constraints. ∎ Given Lemmas 5 and 6, we can finally prove Lemma 1.\n∥V∗ −Φw̃∥1,ρ ≤∥V∗ −Φw̄∥1,ρ =∑\ns∈S ρ(s) ∣V∗ − (Φw̄)(s)∣\n=∑ s∈S\nρ(s)(ΦwL)(s) ∣V∗ − (Φw̄)(s)∣\n(ΦwL)(s)\n≤(∑ s∈S ρ(s)(ΦwL)(s))max s′∈S ∣V∗ − (Φw̄)(s′)∣ (ΦwL)(s′) =ρTΦwL∥V∗ −Φw̄∥∞,1/ΦwL ≤ρTΦwL (∥V∗ −Φw∗∥∞,1/ΦwL + ∥Φw̄ −Φw\n∗∥∞,1/ΦwL) ≤ρTΦwL(∥V∗ −Φw∗∥∞,1/ΦwL\n+ ∥V∗ −Φw∗∥∞,1/ΦwL ( 2\n1− βΦwL − 1)∥ΦwL∥∞,1/ΦwL)\n=2ρ TΦwL\n1− βΦwL ∥V∗ −Φw∗∥∞,1/ΦwL .\nThe penultimate line is due to the definition of w̄, and the final line occurs because ∥ΦwL∥∞,1/ΦwL = 1. ∎"
    }, {
      "heading" : "B. Proof of Lemma 2",
      "text" : "When a constraint does not exist in RALP for some state, this does not mean the value at that state is completely unconstrained; because we bounded the rate of change of all components of the approximate and true value functions in Assumption 1, the existence of a constraint constructed on a nearby state means the existence of what we will call an implied constraint.\nThis lemma explicitly constructs these implied constraints, and quantifies the maximum distance from the true constraint which would have existed had that state been sampled. It does this by building two MDPs, identical in every way, but for the reward function. M1 has been incompletely sampled, with sample set Σ. Every state-action pair therefore has either an explicit or implied constraint in the corresponding RALP. M2, however, has been completely sampled. Every state-action pair in the set Σ is identical in M2, but all state-action pairs not in Σ have a sample producing a constraint identical to the implied constraints of M1. Because the constraints are the same, the RALP solution is the same. We demonstrate the difference in the reward functions R1 and R2 is bounded, and thus, the difference in the optimal value functions V∗1 and V ∗ 2 is bounded.\nLemma 7 Given an MDP M1 such that Assumption 1 is true and incomplete sample set Σ, an MDP M2 exists such that constructing the RALP with constraints for all stateaction pairs results in an identical RALP solution to that of the RALP constructed from Σ, and ∥R1 − R2∥∞ ≤ 2(δφψ + δR + δPψ).\nProof: Consider an arbitrary state-action pair s, a, which is not represented by a sample in Σ. This means we are missing the constraint\nR1(s)+ γ ∑ x∈S [p(x∣s, a)Φ(x)]w ≤ Φ(s)w. (2)\nLet us refer to the sample in Σ which fulfills the sampling assumption with s and a as σ. We can now construct a bound for how incorrect each component of this constraint can be if we use the constraint at σ and our sampling assumption to replace the missing constraint. For instance, the reward function R(s) is easily bounded.\nR1(σs)− δR ≤ R1(s) ≤ R1(σs)+ δR We now bound Φ(s)w. Because the sampling assumption allows each basis function to change only a finite amount, and because ∥w−1∥1 ≤ ψ, and 1(s) = 1(σs),\nΦ(σs)w − δΦψ ≤ Φ(s)w ≤ Φ(σs)w + δΦψ\nThe final component is γ∑x∈S p(x∣s, a)Φ(x)w, which expresses our expected value at the next state. It will be convenient to separate the bias feature 1 from the rest of Φ.\nWe will denote the remainder of the design matrix as Φ−1, and the weights that correspond to Φ−1 as w−1. Similarly, we will denote the weight corresponding to 1 as w1.\n∑ x∈S p(x∣s, a)Φ(x)w =∑ x∈S p(x∣s, a)w1 + ∑ x∈S p(x∣s, a)Φ−1(x)w−1\n=w1 + ∑ x∈S p(x∣s, a)Φ−1(x)w−1\nAgain, we have bounded the allowable change in our expression of probability.\nw1+∑ x∈S p(x∣s, a)Φ−1(x)w−1\n≤w1 + ∑ x∈S [p(x∣σs, σa)+ δP]Φ−1(x)w−1 =w1 + ∑ x∈S p(x∣σs, σa)Φ−1(x)w−1 + δP ∑ x∈S Φ−1(x)w−1\nBecause each basis function Φ is can be standardized such that ∥Φ∥1 = 1, and because ∥w−1∥1 ≤ ψ, the second summation can be at most ψ. So,\n∑ x∈S p(x∣σs, σa)Φ(x)w − δPψ ≤ ∑ x∈S [p(x∣s, a)Φ(x)]w\n≤ ∑ x∈S p(x∣σs, σa)Φ(x)w + δPψ.\nWe now combine these results, and construct our implied constraint to take the place of the missing constraint expressed by Equation 2. We see that the maximum possible change by the approximate value function is δΦψ + δR + δPψ. So, the total cumulative error in the constraint is at most 2(δΦψ + δR + δPψ). So, we effectively have the following constraint:\nR1(s)+ q − γ ∑ x∈S [p(x∣s, a)Φ(x)]w ≥ Φ(s)w,\nwhere ∣q∣ ≤ 2(δΦψ + δR + δPψ). Let M2 be an MDP which is identical in every way to M1, except R2(s) = R1(s) + q. The RALP solution for M1 will be equivalent to the RALP solution for M2, and ∥R1 − R2∥∞ ≤ 2(δΦψ + δR + δPψ). ∎\nLemma 8 Let M1 and M2 be MDPs that differ only in their reward vectors R1 and R2. Let V∗1 and V ∗ 2 be their optimal value functions. Then, for ∥R1 − R2∥∞ ≤ δ, ∥V∗1 −V∗2 ∥∞ ≤ δ 1−γ .\nProof: Let s be an arbitrary point in the sets S1 and S2, and define r1i(s) and r2i(s) to be the i-th reward received in exploring M1 and M2 from state s, respectively. Note that\nV∗1 (s) = ∞ ∑ i=0 γiE [r1i(s)]\nand\nV∗2 (s) ≤ ∞ ∑ i=0 (γiE [r1i(s)+ δ])\n= ∞ ∑ i=0 γiE [r1i(s)]+ ∞ ∑ i=0 γiδ\nTherefore,\n∣V∗1 (s)−V∗2 (s)∣ ≤ ∞ ∑ i=0 γiE [r1i(s)]− ( ∞ ∑ i=0 γiE [r1i(s)]+ ∞ ∑ i=0 γiδ)\n= ∞ ∑ i=0 γiδ\n= δ 1− γ\nBecause this is true for an arbitrary s, ∥V∗1 −V∗2 ∥∞ ≤ δ\n1−γ . ∎ Lemma 2 is trivially proven by combining Lemmas 7 and 8, and is illustrated by Figure 3."
    } ],
    "references" : [ {
      "title" : "Geometric bounds for stationary distributions of infinite markov chains via lyapunov functions",
      "author" : [ "Bertsimas", "Dimitris", "Gamarnik", "David", "Tsitsiklis", "John N" ],
      "venue" : "Technical report, Massachusetts Institute of Technology,",
      "citeRegEx" : "Bertsimas et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bertsimas et al\\.",
      "year" : 1998
    }, {
      "title" : "The Linear Programming Approach to Approximate Dynamic Programming",
      "author" : [ "de Farias", "Daniela Pucci", "Van Roy", "Benjamin" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Farias et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Farias et al\\.",
      "year" : 2003
    }, {
      "title" : "On Constraint Sampling for the Linear Programming Approach to Approximate Dynamic Programming",
      "author" : [ "de Farias", "Daniela Pucci", "Van Roy", "Benjamin" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Farias et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Farias et al\\.",
      "year" : 2004
    }, {
      "title" : "A probabilistic production and inventory problem",
      "author" : [ "F. d’Epenoux" ],
      "venue" : "Management Science,",
      "citeRegEx" : "d.Epenoux,? \\Q1963\\E",
      "shortCiteRegEx" : "d.Epenoux",
      "year" : 1963
    }, {
      "title" : "Regularized Off-Policy TD-Learning",
      "author" : [ "Liu", "Bo", "Mahadevan", "Sridhar", "Ji" ],
      "venue" : "In Proceedings of the Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Liu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Sparse Q-Learning With Mirror Descent",
      "author" : [ "Mahadevan", "Sridhar", "Liu", "Bo" ],
      "venue" : "In Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Mahadevan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mahadevan et al\\.",
      "year" : 2012
    }, {
      "title" : "Lyapunov Design for Safe Reinforcement Learning",
      "author" : [ "Perkins", "Theodore J", "Barto", "Andrew G" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Perkins et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Perkins et al\\.",
      "year" : 2003
    }, {
      "title" : "Feature selection using regularization in approximate linear programs for markov decision processes",
      "author" : [ "Petrik", "Marek", "Taylor", "Gavin", "Parr", "Ronald", "Zilberstein", "Shlomo" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "Petrik et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Petrik et al\\.",
      "year" : 2010
    }, {
      "title" : "Coarticulation in Markov Decision Processes",
      "author" : [ "Rohanimanesh", "Khashayar", "Platt", "Robert", "Mahadevan", "Sridhar", "Grupen", "Roderic" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Rohanimanesh et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rohanimanesh et al\\.",
      "year" : 2004
    }, {
      "title" : "Generalized Polynomial Approximations in Markovian Decision Processes",
      "author" : [ "Schweitzer", "Paul J", "Seidmann", "Abraham" ],
      "venue" : "Journal of mathematical analysis and applications,",
      "citeRegEx" : "Schweitzer et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Schweitzer et al\\.",
      "year" : 1985
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In recent years, the Reinforcement Learning community has paid considerable attention to creating value function approximation approaches which perform automated feature selection while approximating a value function (Kolter & Ng, 2009; Johns et al., 2010; Mahadevan & Liu, 2012; Liu et al., 2012).",
      "startOffset" : 217,
      "endOffset" : 297
    }, {
      "referenceID" : 7,
      "context" : "proaches, L1-Regularized Approximate Linear Programming (RALP) (Petrik et al., 2010; Taylor & Parr, 2012) is unique in that it results in an approximation of the optimal value function and makes use of off-policy samples.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "Therefore, in Section 4, we derive a new error bound for the RALP approximation, which is tighter than those presented by Petrik et al. (2010) and provides new insight into the result of changing the state-relevance weights in the objective function.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "Therefore, in Section 4, we derive a new error bound for the RALP approximation, which is tighter than those presented by Petrik et al. (2010) and provides new insight into the result of changing the state-relevance weights in the objective function. In addition, this bound provides an insight into the types of MDPs particularly well suited to the RALP approach. Finally, this section provides evidence that rather than weighting all states equally, as was done by Petrik et al. (2010) and Taylor and Parr (2012), states should instead be weighted in proportion to the stationary distribution under the optimal policy.",
      "startOffset" : 122,
      "endOffset" : 488
    }, {
      "referenceID" : 7,
      "context" : "Therefore, in Section 4, we derive a new error bound for the RALP approximation, which is tighter than those presented by Petrik et al. (2010) and provides new insight into the result of changing the state-relevance weights in the objective function. In addition, this bound provides an insight into the types of MDPs particularly well suited to the RALP approach. Finally, this section provides evidence that rather than weighting all states equally, as was done by Petrik et al. (2010) and Taylor and Parr (2012), states should instead be weighted in proportion to the stationary distribution under the optimal policy.",
      "startOffset" : 122,
      "endOffset" : 515
    }, {
      "referenceID" : 3,
      "context" : "(2010) to extend the capabilities of the linear programming approach to value function approximation (d’Epenoux, 1963; Schweitzer & Seidmann, 1985; de Farias & Van Roy, 2003).",
      "startOffset" : 101,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "RALP was introduced by Petrik et al. (2010) to extend the capabilities of the linear programming approach to value function approximation (d’Epenoux, 1963; Schweitzer & Seidmann, 1985; de Farias & Van Roy, 2003).",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Liu et al. (2012) introduced RO-TD, which converges to an approximation of the value function of a given policy, even when trained on offpolicy samples.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "Besides stability analysis, Lyapunov functions have also previously appeared in Reinforcement Learning literature, though in different contexts from our application (Perkins & Barto, 2003; Rohanimanesh et al., 2004).",
      "startOffset" : 165,
      "endOffset" : 215
    }, {
      "referenceID" : 7,
      "context" : "To enable us to bound the behavior of the value function between samples, we make the following assumption, similar to the sufficient sampling assumption made by Petrik et al. (2010):",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "Bertsimas et al. (1998) demonstrated that a small Lyapunov function value correlates in expectation with a higher probability in the stationary distribution of a Markov chain.",
      "startOffset" : 0,
      "endOffset" : 24
    } ],
    "year" : 2014,
    "abstractText" : "Recent interest in the use of L1 regularization in the use of value function approximation includes Petrik et al.’s introduction of L1-Regularized Approximate Linear Programming (RALP). RALP is unique among L1-regularized approaches in that it approximates the optimal value function using off-policy samples. Additionally, it produces policies which outperform those of previous methods, such as LSPI. RALP’s value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program, and by the distribution from which samples are drawn; however, there has been no discussion of these considerations in the previous literature. In this paper, we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality, using both theoretical and experimental illustrations. The results provide insight not only onto these effects, but also provide intuition into the types of MDPs which are especially well suited for approximation with RALP.",
    "creator" : "LaTeX with hyperref package"
  }
}