{
  "name" : "1701.04663.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Intrinsically Motivated Acquisition of Modular Slow Features for Humanoids in Continuous and Non-Stationary Environments",
    "authors" : [ "Varun Raj Kompella", "Laurenz Wiskott" ],
    "emails" : [ "laurenz.wiskott}@ini.rub.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning (RL) [8; 36] provides a basic framework for an actively exploring agent to acquire desired taskspecific behaviors by maximizing the accumulation of taskdependent external rewards through simple trial-and-error interactions with the environment. In high-dimensional real world environments, however, RL can be slow since external rewards are usually sparsely available and can sometimes be extremely difficult to obtain by pure random exploration. Fortunately, most real world transitions lie on a\nlow-dimensional manifold. Learning a compact representation (feature abstraction) of the environment sensed through high-dimensional sensory inputs, can therefore speed up exploration and the subsequent task learning [17; 18; 11; 15; 22].\nIn environments that are non-stationary and partially observable, a single abstraction is probably not sufficient to encode most variations, in which case it would be beneficial to learn a repertoire of spatially or temporally local abstractions that can potentially be translated to multiple skills. In the absence of external supervision, how can the agent be motivated to learn these abstractions? The agent would need to be intrinsically motivated. Over the recent years, intrinsic motivation (IM) has been considered a useful tool for adaptive autonomous agents or robots [33; 2]. There exists several computational approaches that model different IM signals for RL agents, for example, IM signals that are based on novelty [7], prediction error [30; 4], knowledge/prediction improvements [29] and those that are based on the competence to reach a certain goal [28]. Refer to [33; 2] for a survey on the pros and cons of these approaches.\nMost of the intrinsically motivated RL techniques have been applied to exploring agents in simple domains [1; 35; 26; 27], agents that use hand-designed or pre-trained state abstractions of high-dimensional environments [14; 25], or agents that are provided with a low-dimensional taskspace [3]. Very few have addressed the issue of learning task-independent low-dimensional abstractions from highdimensional inputs while simultaneously exploring the environment. The main problem in such scenarios is to learn abstractions from non-i.i.d and potentially non-stationary sensory inputs that are a function of the agent’s actions and other unknown time-varying factors in the environment. Mugan and Kuipers QLAP [24], Xu and Kuipers OSH [42] and Kompella et al.’s Curious Dr. MISFA [19; 9] are a few closely related examples in the direction of learning feature abstractions from action sequences that are specific to localized regions in the environment. QLAP learns simplified predictable knowledge by discretizing low-level sensorimotor experience through defining landmarks and observing contingencies between the landmarks. It assumes that there exists a low-level sensory model that can, e.g., track the positions of the objects in the scene. OSH builds a collection of multi-level object representations from camera images. It uses a “modelar X iv :1 70 1.\n04 66\n3v 1\n[ cs\n.A I]\n1 7\nJa n\n20 17\nlearning through tracking” [23] strategy to model the static background and the individual foreground objects assuming that the image background is static.\nCurious Dr. MISFA is by far the closest that comes to addressing the problem of learning task-independent multiple abstractions from raw images online in the absence of any external guidance. The agent actively explores within a set of high-dimensional video streams1 and learns to select the stream where it can find the next easiest (quickest) to learn a slow feature (SF; [41]) abstraction. It does this optimally while simultaneously updating the SF abstractions using Incremental Slow Feature Analysis (IncSFA; [10]). IncSFA is based on the slowness principle [21; 6], which states that the underlying causes of fast changing inputs vary at a much slower timescale. IncSFA uses the temporal correlations within the inputs to extract SFs online. SFs have been shown to be useful for RL as they capture the transition process generating the raw sensory inputs [40; 34; 11; 20; 5]. The result of the learning process of Curious Dr. MISFA is an optimal sequence of SF abstractions acquired in the order from easy to difficult-to-learn ones, principally similar to the learning process of Utgoff and Stracuzzi’s manylayered learning [38]. Curious Dr. MISFA has also been used to show a continual emergence of reusable unsupervised skills on a humanoid robot (topple, grasp, pick-place a cup) while acquiring SF abstractions from raw-pixel vision [12; 13], the first of its kind.\nCurious Dr. MISFA’s application is, however, limited to discrete domains constrained by a pre-defined discrete state space and has design limitations that make it unstable in certain situations. This paper presents a significant improvement that is applicable to continuous environments, is computationally less expensive, simpler to use with fewer hyper parameters, and stable in non-stationary environments where the statistics change abruptly over time. We demonstrate these improvements empirically and make our Python code of the algorithm available online as open source. Next, we discuss details of our proposed algorithm."
    }, {
      "heading" : "2 CD-MISFA 2.0",
      "text" : "We discuss here the details of our new method. To keep it short, we refer to the original Curious Dr. MISFA as CDMISFA 1.0 and our new method as CD-MISFA 2.0 (refer Section 3.1 for a detailed comparison between the two methods). Next, we provide an intuitive analogical example to explain the underlying problem that is being solved.\nIntuition. Consider a camera equipped agent viewing different channels on a television. Each channel generates a continuous stream of images (that may or may not be predictable). The agent at any instant can access information only from a single channel. It can explore the channels by selecting a particular channel for a period of time and then switch. The distribution of images received by the agent as a consequence of its exploration, in most cases, is nonstationary. This makes it infeasible to learn a single abstraction encoding all the channel streams. The problem can be\n1A video stream could be generated as a consequence of executing a particular agent’s behavior.\nsimplified by learning abstractions of individual channels that generate inputs from a stationary distribution. But how can the agent find out (a) the channel and (b) for how long to observe the channel, to know that there exists a stationary distribution? We discuss next the details of the CD-MISFA 2.0 algorithm that addresses a general version of this problem.\nEnvironment. The environment considered is similar to the one of CD-MISFA 1.0. It consists of n sources of observation streams X = {x1, ...,xn : xi(t) = (x1i (t), ..., xIi (t)) ∈ R\nI∈N}. These streams could be image sequences observed over different head rotation angles of a robot or while executing different time-varying behaviors. The agent explores the streams with two actions: {stay, switch}. When the agent takes the stay action, the current stream xi remains the same and it receives a hand-set number of τ observations from that stream. When it takes the action switch, the agent selects a stream xj 6=i uniformly randomly from one of the other n − 1 streams and it receives τ observations from the new stream.\nGoal. The goal of the agent is to learn a sequence of slow feature abstractions Φ = {φ1, ..., φm; m ≤ n} that each encode one or more of the observation streams in X . φi is generally a matrix of parameters. The order of the sequence is such that φ1 encodes the easiest and φm the most difficult learnable stream in X . CD-MISFA 2.0 achieves this goal by iterating over the following steps: (1) Find the easiest novel observation stream while simultaneously learning an abstraction encoding it. (2) Store the abstraction and use it to filter known or similar observation streams. (3) Continue with step (1) on the remaining streams.\nArchitecture. The architecture includes: (a) Adaptive abstraction. A single adaptive abstraction φ̂ is updated online using IncSFA for each observation x(t). Details on the learning rules of IncSFA can be found in Kompella’s previous work [10]. The instantaneous output of the adaptive abstraction for the observation x(t) is given by:\ny(t) = φ̂x(t). (1) (b) Gating system. A gating system is used to accomplish\ntwo tasks: (1) Decide when to stop updating φ̂ and store it φi ← φ̂. Once stored, φi is frozen and a new φ̂ is created. (2) Use the stored frozen abstractions to filter observations from known or similar input streams while updating the new φ̂.\nFor the first task, we estimate and use the time derivative of the slowness measure [41]. Slowness measure of a timevarying signal y is defined as:\nη(y) = 1\n2 π √ E(ẏ2) Var(y) , (2)\nwhere ẏ represents the temporal derivative of y, E and Var represent the expectation and variance. This measure quantifies how fast or slow a signal changes in time. We compute η values of all the output components of the adaptive abstraction online. When the abstraction has converged, the ηs will converge as well and their derivative will tend towards zero. The gating system uses the following condition to check when to stop updating the adaptive abstraction:\n|η̇(yi(t))| < δ, ∀yi(t) ∈ y(t). (3)\nFor the second task, we compute an instantaneous η\nηinst(y) = 1\n2 π √ Eτ (ẏ2) Varτ (y) , (4)\nfor each output component yi ∈ y, where Eτ and Varτ are the mean and variance of only the τ samples. When τ is large ηinst(y) = η(y). We also track a moving standard deviation (SD) for each ηinst(yi(t)). When φ̂ is saved, the estimated SDs are also saved. To find out if a new set of τ samples is novel, ηinst of all the frozen abstractions are computed for the new samples according to Eq. (4) and then checked if they lie outside two times their corresponding SDs.\n(c) Curiosity-Driven Reinforcement Learner (CDRL). A CDRL is used to find (a) the unknown order of the observation streams in terms of the difficulty of learning them with IncSFA, and (b) the optimal sequence of actions (stay or switch) required to learn Φ. Let s ∈ S = {s1, ..., sn} denote the indices of the observation streams and u ∈ U = {u1, ..., um} denote the indices of the abstractions to be learned. Let A = {0 = stay, 1 = switch}. The goal of the CDRL reduces to learning an observation stream selection policy π∗ : S × U → A that maps an optimal action for each stream xi to learn the abstraction φi. For example, consider an environment with 5 streams with x3 being the easiest to learn and x1 the next. To learn the first abstraction, π∗(., u1) is a vector [1, 1, 0, 1, 1], and the second abstraction π∗(., u2) is [0, 1, 1, 1, 1]. How can the CDRL learn such a π∗? Since the desired Φ is an ordered finite set of unique abstractions, it follows that the corresponding sub-policy π∗(., ui) (denoted in short as π∗ui ) required to learn the abstraction φi is unique. Therefore, π∗ is learned sequentially by learning unique sub-policies in the order {π∗u1 , ..., π ∗ um}.\nThe convergence of the agent’s sub-policies πui : S → A to their optimal (π∗ui ) is guided through internal rewards for each tuple (current state s, current action a, future state s′):\nrss ′ a = ( −〈ξ̇〉τt + βZσ(〈ξ〉τt ) ) , (5)\nwhere Zσ(x) = e−x 2/2σ2 is a Gaussian function, σ and β are scalar constants, ξ(t) denotes the Frobenius norm ‖φ̂(t + 1) − φ̂(t)‖ and 〈ξ〉τt = 1τ ∑t+τ−1 t ξ(t), 〈ξ̇〉τt = 〈ξ〉τt − 〈ξ〉τt−τ . The RL objective learns a policy that maximizes the accumulation of these rewards over time. There are two terms in the reward equation, maximizing the first term would result in a policy that shifts the agent to states where the weight-change decreases sharply (〈ξ̇〉τt < 0). This term is often referred in the literature as the curiosity reward [31; 32]. Intuitively, the curiosity reward term is responsible for finding the easiest observation stream. Maximizing the second objective results in a policy that improves the developing φ̂ to better encode the observations, making it an expert. We refer to the second term as the expert reward.\nA reward function R : S × A × S → R (tensor of size |S| × |A| × |S|) is estimated online using the instantaneous rewards as:\nR← 1 t R̃+\n( 1− 1\nt\n) R, (6)\nwhere R̃ is an instantaneous tensor (same shape asR) with its (s, a, s′) component equal to the instantaneous reward rss ′ a and all other components equal to zero. After every τ observations, a value function Q and the sub-policy πui are updated using the estimatedR via Least Squares Policy Iteration (LSPI; [16]).\nLearning Process. Figure 1(a) shows the control flow diagram of the algorithm. At t = 0, the agent begins by observing τ samples from the current stream. Since there are no previously learned abstractions, the set of τ samples is novel and is used to update φ̂. Condition (3) is checked and if not met, R and πu1 are updated according to Eq. (6) and LSPI algorithm respectively. The agent uses a decaying -greedy strategy [36] on πu1 to take a new action and the process repeats. After a few iterations, the sub-policy πu1 converges to π∗u1 . The converging πu1 also enables φ̂ to converge. When |η̇| < δ, φ̂ and πu1 are saved (φ1 = φ̂), is reset to its initial value and a new φ̂ is created. The gating system then uses the frozen φ1 to check if the new set of τ samples is novel. Only novel sets are forwarded to update the new φ̂. The algorithm iterates and learns (π∗u2 , φ2) corresponding to the next easily learnable observation stream. The algorithm terminates when all abstractions have been learned. The final result is (π∗,Φ).\nHyper Parameters. The hyper parameters that are used by the algorithm are as follows: (1) IncSFA learning rate ν, (2) threshold δ, (3) β, (4) decay multiplier, (5) τ and (6) reward parameter σ. ν is quite intuitive to set [10]. δ is generally set to values between 0.0004 − 0.001 depending on how well the expert modules need to encode the inputs. β is set to ν log 2/(2(n − 1)), where n is the number of streams (a derivation is beyond the scope of this paper). See Section 3.3 for a discussion on setting τ and σ. A Python code of CD-MISFA 2.0 is available for download at https://varunrajk.gitlab.io/\n(a) (c)\nReward Function (Rstay) s1Rst (t) s2Rst (t) s3Rst (t)\nϵ (t) ϵ = 0.8\nIterations\nCDMISFA-1.0\nϵ\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 0 100 200 300 400 500 600 700\n1.2\n0.8\n0.6\n0.4\n0.2\n0.0\n1.0\nReward Function (Rstay)\nIterations\nCDMISFA-2.0 0.0020\n0.0015\n0.0010\n0.0005\n0.0000 0 100 200 300 400 500 600 700\nϵ\n1.2\n0.8\n0.6\n0.4\n0.2\n0.0\n1.0\ns1Rst (t) s2Rst (t) s3Rst (t) ϵ (t)\nϵ = 0.8\n(b) (d)\nSub-Policy (πu1)\nIterations\nπu1(s1)\nπu1(s2)\nπu1(s3)\nSwitch\nStay\nSwitch\nStay\nSwitch\nStay 0 100 200 300 400 500 600 700\nSub-Policy (πu1)\nIterations\nπu1(s1)\nπu1(s2)\nπu1(s3)\nSwitch\nStay\nSwitch\nStay\nSwitch\nStay 0 100 200 300 400 500 600 700\nFigure 2: CD-MISFA 1.0 vs CD-MISFA 2.0. (a) CD-MISFA 1.0 Reward Function (stay action only). It gets updated locally and this results in instability. (b) Unstable CD-MISFA 1.0 sub-policy πu1 . (c) CD-MISFA 2.0 reward function and its stable (d) sub-policy πu1 ."
    }, {
      "heading" : "3 Experimental Results",
      "text" : "Here, we evaluate the performance of our algorithm. The desired result is a sequence of SF abstractions acquired in the order of increasing learning difficulty. We use curiosity function values [19] as a metric to quantify the learning difficulty of an observation stream w.r.t IncSFA."
    }, {
      "heading" : "3.1 CD-MISFA 1.0 vs CD-MISFA 2.0",
      "text" : "We compare our method with the previous CD-MISFA 1.0 algorithm:\n(a) CD-MISFA 1.0 uses a clustering algorithm called the Robust Online Clustering (ROC) [43] coupled to the IncSFA. ROC maintains estimates of IncSFA outputs that are correlated to some pre-defined discrete meta-class labels (e.g. proprioception; the joint angles of a humanoid robot). The ROC error is used to decide when to stop updating φ̂ and to check if τ samples are novel. The disadvantages of using ROC are: (a) It requires discrete meta-class labels, which can be hard to provide in general environments (e.g. see Section 3.4). (b) It limits the abstractions to be correlated to the labels. (c) It restricts the algorithm’s application to discrete environments. (d) It adds to the overall computational complexity. CD-MISFA 2.0 does not use ROC, instead it uses the lowcomplex, continuous-time slowness measure to check when to stop learning and how to filter the encoded inputs. This extends its application to continuous domains with relatively fewer hyper parameters to be set. The method does not require any meta-class labels and the abstractions learned are not constrained in any way.\n(b) CD-MISFA 1.0 uses a tabular reward function update rule [9]: R̃ss ′ a ← α rss ′ a + (1 − α)R̃ss ′\na ; R ← R̃/‖R̃‖, where α is a constant. This rule only makes local tabular updates of the (s, a, s′) tuple entries. We found cases where\nCD-MISFA 1.0 becomes unstable using this reward function. To demonstrate this, we select an environment consisting of 3 nonlinear oscillatory streams [9] each learnable by IncSFA:\nx1 : { x1(t) = sin(4 θt − π/4.)− cos(44 θt)2 x2(t) = cos(44 θt) , (7)\nx2 :\n{ x1(t) = sin(3 θt) + cos(27 θt) 2\nx2(t) = cos(27 θt) , and (8)\nx3 : { x1(t) = cos(12 θt) x2(t) = cos(2 θt) + cos(12 θt) 2 , (9)\nwhere θt = 2πt/500. It can be found based on the learning difficulty values [9] that the slowest feature of the stream x1 is the easiest to learn followed by x2 and then x3. The learning parameters are set as ν = 0.05, τ = 100, σ = 0.0009. We initialized to 1.2, so that the agent explores long enough. However, when used as a probability, any value of > 1 is considered as 1. decays with a multiplier equal to 0.998 and is set to 0 when it reaches the value of 0.8. Figure 2(a) shows the updating CD-MISFA 1.0 reward function for the stay action over algorithm iterations. Since x1 is the easiest to learn, the algorithm finds the stay action in s1 most rewarding. As decays< 1, the agent tends to spend more time in s1 updating the reward function locally. When is set to zero, the reward function corresponding to s1 decreases (because the curiosity rewards diminish), while the rest of the reward function entries remain the same. This results in an unstable policy as soon as s1 reward value goes below that of s2 and the module hasn’t converged yet. The instability reoccurs for the reward value at s2. This is not the case in CD-MISFA 2.0 (Figure 2(c),(d)). The reward function is estimated using rewards that modify the whole function (Eq. (6)). The policy therefore remains stable."
    }, {
      "heading" : "3.2 Oscillatory Streams Environment",
      "text" : "We now investigate further the complete learning behavior of CD-MISFA 2.0 in the environment considered above. We used the same set of hyper-parameters: ν = 0.05, δ = 0.0006, τ = 100, σ = 0.0009, is initialized to 1.2, with a 0.999 decay multiplier. However, when < 0.8, the decay multiplier is set to 0.95 to speed up the experiment. We executed the algorithm for 20 trials with different random initializations (seeds) and achieved optimal results for all the trials. An optimal result here is the abstraction set Φ∗ = {φ1, φ2, φ3}, where φ1 encodes x1 (easiest to learn), φ2 encodes x2 (next easier), and φ3 encodes x3. The optimal result also includes the policy to learn these abstractions for the given environment; π∗ = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.\nFigure 3 shows the results of the experiment. For each trial, the agent begins exploring the three streams initially by executing actions stay and switch at random. The derivative of η is high as the agent switches between the streams (Figure 3(a)). During this period, R becomes stable (Figure 3(b)). Since x1 is the easiest stream to encode, the stay action in state s1 is most rewarding. This is also reflected in the value function (averaged over 20 trials; Figure 3(c)) and the subpolicy learned (Figure 3(d)). As decays, the agent begins to exploit the learned sub-policy and the η̇ begins to drop. Once\nit drops below δ, the adaptive abstraction is saved φ1 = φ̂ and a new φ̂ is created. The process repeats, but now the gating system prevents re-learning x1 and therefore the agent finds staying in s2 most rewarding. It learns an abstraction corresponding to x2 and then x3. This experiment has demonstrated that the algorithm learns the optimal policy in a stationary environment."
    }, {
      "heading" : "3.3 Non-Stationary Dynamic Environments",
      "text" : "Here, we discuss results of experiments conducted in non-stationary environments, where the statistics changes abruptly in time. Consider an environment with 3 streams; the first stream is generating zeros, the second stream is x2 (Eq. (8)) and the third is x3 (Eq. (9)). Since x2 is easier to learn than x3, the optimal sub-policy is [1, 0, 1]. We let the algorithm’s policy stabilize and when of the decaying -greedy strategy falls below a constant c, we replace the zero-stream with x1 (Eq. (7)). The new optimal sub-policy after that signal swap is [0, 1, 1], since x1 is now the easiest to learn. For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy. We simulate different non-stationary environments by setting different values for c ∈ { 1., 0.96, 0.93, 0.9, 0.86, 0.83, 0.8, 0.76, 0.73, 0.7, 0.6, 0.5, 0.3, 0.1}. For these non-\nstationary environments, we address the following questions:\n1. Is the algorithm stable when decays to zero? That is, does it converge to a particular policy consistently over many trials of random initializations?\n2. To which policy does the algorithm converge?\n3. What hyper-parameters effect the result?\nFirst, we discuss the performance of the algorithm for hyper-parameters similar those in the previous experiments, except for σ = 0.0001. Figure 4(a) shows the learned subpolicy πu1 (after ≈ 0) averaged over 10 randomly initialized (seed) trials for each value of c ∈ { 1., 0.96, 0.93, 0.9, 0.86, 0.83, 0.8, 0.76, 0.73, 0.7, 0.6, 0.5, 0.3, 0.1}. It is clear that there is a value d ≤ 1, so that for c > d, the algorithm consistently converges to the new optimal policy (except for the values close to d). While, for c < d the algorithm converges to the old optimal policy. We denote d as the pointof-no-return . This result shows that the algorithm remains stable when the decays to zero, and converges to the old optimal policy if the environment statistics change at any < d. If the environment changes when > d, then the algorithm learns the new optimal policy consistently. Next, we discuss if different hyper parameters effect this behavior.\nFigure 4(b) shows the same experiment with 10 random initializations for a higher value of σ = 0.008. It is clear that for a higher value of σ, d is higher, therefore, pushing the decision boundary to stick to the old optimal. This is also\nevident from the Table 1. σ controls the effect of the expert rewards (Eq. (5)). Therefore, expert rewards bias the agent to become an expert by exploiting the learned old optimal instead of exploring to learn the new optimal. Lastly, we have also conducted the same experiment for different values of IncSFA learning rate ν and τ , keeping the rest of the parameters fixed to their values of the previous experiment. Table 2 shows how ν and τ have no significant effect on d, with the exception of τ = 10, where we suspect the value is too low to estimate the rewards properly. The above results show that the algorithm is stable in the above non-stationary environments and converges to either the old optimal or the new optimal solution depending on the value of . The result also demonstrates the effect of the expert rewards on the system."
    }, {
      "heading" : "3.4 Curiosity-Driven Vision-Enabled iCub",
      "text" : "An important open problem in vision-based developmental robotics is, how can an online vision-enabled humanoid robot akin to a human baby focus/shift its attention towards events that it finds interesting? Can its curiosity to explore also drive learning abstractions? We present here an experiment to demonstrate that this is possible using CD-MISFA 2.0. To this end, we use the iCub Simulation software [39]. An iCub is placed next to a table with three objects of different sizes (Figure 5(a)). The environment is dynamic and continuous; all the three object’s positions (unknown to the iCub) change at every time t. Object-1’s x-position changes uniformly randomly within the range (-0.4,-0.6) and its yposition is either 0.4 or 0.6 and toggles at a fixed unknown frequency. Both x and y-position of object-2 change uniformly randomly. Object-3 performs a random walk with its y-position changing slowly compared to its x-position. The three object’s movements depict three distinct dynamic events in the iCub’s environment.\nThe iCub has two onboard camera eyes and the images captured are converted to grayscale and downscaled to a size of 128x48 pixels. Figure 5(b) shows a sample input image. The iCub explores by rotating its head over a single joint. We use three joint positions such that it can view the objects over three overlapping perspectives: left (LP), center (CP) and right (RP), each generating a stream of high-dimensional observations {x1,x2,x3}. IncSFA finds the streams x1 and x3 learnable and x2 unlearnable since only object-1 and 3’s positions have a temporal structure. Furthermore, we calculated the learning difficulty values [19] and found that x1 is easier to encode by IncSFA than x3.\nIt is not straightforward to apply CD-MISFA 1.0 in this environment since the dynamics (changing object’s positions) have no correlation to the robot’s proprioception. Therefore, it is hard to provide any discrete meta-class labels to the ROC (see Section 3.1) to make any progress in learning abstractions. On the other hand, since CD-MISFA 2.0 does not require any pre-defined labels, we expect that it first learns an abstraction encoding the position of object-1 and then an abstraction encoding the position of object-3 (see Kompella et al.’s work [10] for details on why IncSFA learns the positions). The experiment would then terminate after this as there are no other IncSFA learnable events in the environment.\nWe used hyper-parameters similar to the previous experiments, except for ν = 0.01, τ = 40, σ = 0.01, δ = 0.0008. S = {s1, s2, s3} corresponds to {x1,x2,x3}. We conducted 10 trials of the experiment with different random seed values and the algorithm found the optimal policy in all the trials. Figures 5(c)-(f) show the cumulative results. For each trial, the iCub starts exploring by moving its head using the\nactions stay and switch. It receives high curiosity-rewards for the observations from x1 compared to the other streams. Therefore, as the decays, it finds the stay action in state s1 most valuable (Figure 5(c)) and the sub-policy converges to πu1 = [0, 1, 1] (Figure 5(d)). The converging πu1 enables φ̂ to converge and |η̇| begins to drop (Figure 5(e)). Once it drops below δ, the adaptive abstraction is saved (φ1 ← φ̂), is reset and a new φ̂ is created. The process repeats, but the gating system prevents re-learning x1 and the agent now learns πu2 = [1, 1, 0] and an abstraction φ2 corresponding to x3. The process continues, however, the system never converges to a third abstraction since the dynamics of x2 is uniformly random (therefore not shown in the figures). Figure 5(f) topleft shows the output of y(t) = φ1x1(t). φ1 encodes the two y-positions of object-1. This is also evident from Figure 5(f) top-right, where we plotted the last 200 output values (before the abstraction was frozen) with respect to the y-position of object-1. The red line shows a polynomial fit over these values. Similarly, Figures 5(f) bottom show that φ2 encodes the y-position of object-3. How can these abstractions be useful? They can be used by the iCub to interact with the objects in a predictable way. An eight times sped up video of this experiment can be found here: https:// varunrajk.gitlab.io/videos/iCubExp8x.mp4"
    }, {
      "heading" : "4 Conclusion",
      "text" : "This paper presents an online learning system that enables an agent to learn to look in regions where it can find the next easiest yet unknown regularity in its high-dimensional sensory inputs. We have shown through experiments that the method is stable in certain non-stationary environments. The iCub experiment demonstrates that the reliable performance of the algorithm extends to high-dimensional image inputs, making it valuable for vision-based developmental learning. Our future work involves implementing the algorithm in environments where the input observation streams are generated as a consequence of executing different time-varying behaviors (e.g. options [37]) and also in environments where it can learn to reuse the learned modular abstractions to solve an external task."
    } ],
    "references" : [ {
      "title" : "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization",
      "author" : [ "B. Bakker", "J. Schmidhuber" ],
      "venue" : "In F. Groen et al., editor, Proc. 8th Conference on Intelligent Autonomous Systems",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Intrinsically motivated learning systems: an overview",
      "author" : [ "G. Baldassarre", "M. Mirolli" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Active learning of inverse models with intrinsically motivated goal exploration in robots",
      "author" : [ "A. Baranes", "P. Oudeyer" ],
      "venue" : "Robotics and Autonomous Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Intrinsically motivated learning of hierarchical collections of skills",
      "author" : [ "A.G. Barto", "S. Singh", "N. Chentanez" ],
      "venue" : "Proceedings of International Conference on Developmental Learning (ICDL)",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Construction of approximation spaces for reinforcement learning",
      "author" : [ "W. Böhmer", "S. Grünewälder", "Y. Shen", "M. Musial", "K. Obermayer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Sparse coding in the primate cortex",
      "author" : [ "P. Földiák", "M.P. Young" ],
      "venue" : "The handbook of brain theory and neural networks,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1995
    }, {
      "title" : "Bayesian surprise attracts human attention",
      "author" : [ "L. Itti", "P.F. Baldi" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Reinforcement learning: a survey",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.W. Moore" ],
      "venue" : "Journal of AI research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1996
    }, {
      "title" : "Slowness Learning for Curiosity- Driven Agents",
      "author" : [ "V.R. Kompella" ],
      "venue" : "PhD thesis, Informatics Department, Università della Svizzera Italiana,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Incremental slow feature analysis: Adaptive low-complexity slow feature updating from high-dimensional input streams",
      "author" : [ "V.R. Kompella", "M. Luciw", "J. Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Autoincsfa and vision-based developmental learning for humanoid robots",
      "author" : [ "V.R. Kompella", "L. Pape", "J. Masci", "M. Frank", "J. Schmidhuber" ],
      "venue" : "In IEEE-RAS International Conference on Humanoid Robots,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Explore to see, learn to perceive, get the actions for free: Skillability",
      "author" : [ "V.R. Kompella", "M. Stollenga", "M. Luciw", "J. Schmidhuber" ],
      "venue" : "In International Joint Conference on Neural Networks (IJCNN),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Continual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robots",
      "author" : [ "V.R. Kompella", "M. Stollenga", "M. Luciw", "J. Schmidhuber" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Autonomous skill acquisition on a mobile manipulator",
      "author" : [ "G. Konidaris", "S. Kuindersma", "R. Grupen", "A.G. Barto" ],
      "venue" : "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Evolving deep unsupervised convolutional networks for visionbased reinforcement learning",
      "author" : [ "J. Koutnı́k", "J. Schmidhuber", "F. Gomez" ],
      "venue" : "In Proceedings of the 2014 conference on Genetic and evolutionary computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Reinforcement learning on slow features of high-dimensional input streams",
      "author" : [ "R. Legenstein", "N. Wilbert", "L. Wiskott" ],
      "venue" : "PLoS Computational Biology,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "An intrinsic value system for developing multiple invariant representations with incremental slowness learning",
      "author" : [ "M. Luciw", "V.R. Kompella", "S. Kazerounian", "J. Schmidhuber" ],
      "venue" : "Frontiers in Neurorobotics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Low complexity protovalue function learning from sensory observations with incremental slow feature analysis",
      "author" : [ "M. Luciw", "J. Schmidhuber" ],
      "venue" : "In Proc. 22nd International Conference on Artificial Neural Networks (ICANN),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Removing time variation with the antihebbian differential synapse",
      "author" : [ "G. Mitchison" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1991
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "The initial development of object knowledge by a learning robot",
      "author" : [ "J. Modayil", "B. Kuipers" ],
      "venue" : "Robotics and autonomous systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Autonomous learning of highlevel states and actions in continuous environments",
      "author" : [ "J. Mugan", "B. Kuipers" ],
      "venue" : "IEEE Transactions on Autonomous Mental Development,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Confidence-based progress-driven self-generated goals for skill acquisition in developmental robots",
      "author" : [ "H. Ngo", "M. Luciw", "A. Förster", "J. Schmidhuber" ],
      "venue" : "Frontiers in Psychology,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Learning tactile skills through curious exploration",
      "author" : [ "L. Pape", "C.M. Oddo", "M. Controzzi", "C. Cipriani", "A. Förster", "M.C. Carrozza", "J. Schmidhuber" ],
      "venue" : "Frontiers in neurorobotics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Which is the best intrinsic motivation signal for learning multiple skills? Intrinsic motivations and open-ended development",
      "author" : [ "V.G. Santucci", "G. Baldassarre", "M. Mirolli" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Evolution and learning in an intrinsically motivated reinforcement learning robot",
      "author" : [ "M. Schembri", "M. Mirolli", "G. Baldassarre" ],
      "venue" : "Proceedings of the 9th European Conference on Artificial Life (ECAL2007),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    }, {
      "title" : "Curious model-building control systems",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "In Proceedings of the International Joint Conference on Neural Networks, Singapore,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1991
    }, {
      "title" : "A possibility for implementing curiosity and boredom in model-building neural controllers",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Proc. of the  International Conference on Simulation of Adaptive Behavior: From Animals to Animats,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1991
    }, {
      "title" : "Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Connection Science,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2006
    }, {
      "title" : "Formal theory of creativity, fun, and intrinsic motivation (1990–2010)",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "IEEE Transactions on Autonomous Mental Development,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2010
    }, {
      "title" : "Maximizing fun by creating data with easily reducible subjective complexity",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "In Intrinsically Motivated Learning in Natural and Artificial Systems,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2013
    }, {
      "title" : "On the relation of slow feature analysis and laplacian eigenmaps",
      "author" : [ "H. Sprekeler" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2011
    }, {
      "title" : "Competence progress intrinsic motivation",
      "author" : [ "A. Stout", "A. G Barto" ],
      "venue" : "In Development and Learning (ICDL),",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1998
    }, {
      "title" : "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "R.S. Sutton", "D. Precup", "S. Singh" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1999
    }, {
      "title" : "Many-layered learning",
      "author" : [ "P.E. Utgoff", "D.J. Stracuzzi" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2002
    }, {
      "title" : "An open-source simulator for cognitive robotics research: The prototype of the icub humanoid robot",
      "author" : [ "A. Cangelosi", "F. Nori" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2008
    }, {
      "title" : "Estimating driving forces of nonstationary time series with slow feature analysis",
      "author" : [ "L. Wiskott" ],
      "venue" : "arXiv preprint cond-mat/0312317,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2003
    }, {
      "title" : "Slow feature analysis: Unsupervised learning of invariances",
      "author" : [ "L. Wiskott", "T. Sejnowski" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2002
    }, {
      "title" : "Steps Towards the Object Semantic Hierarchy",
      "author" : [ "C. Xu" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2011
    }, {
      "title" : "Improving the robustness of online agglomerative clustering method based on kernel-induce distance measures",
      "author" : [ "D. Zhang", "S. Chen", "K. Tan" ],
      "venue" : "Neural processing letters,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "There exists several computational approaches that model different IM signals for RL agents, for example, IM signals that are based on novelty [7], prediction error [30; 4], knowledge/prediction improvements [29] and those that are based on the competence to reach a certain goal [28].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 27,
      "context" : "There exists several computational approaches that model different IM signals for RL agents, for example, IM signals that are based on novelty [7], prediction error [30; 4], knowledge/prediction improvements [29] and those that are based on the competence to reach a certain goal [28].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 26,
      "context" : "There exists several computational approaches that model different IM signals for RL agents, for example, IM signals that are based on novelty [7], prediction error [30; 4], knowledge/prediction improvements [29] and those that are based on the competence to reach a certain goal [28].",
      "startOffset" : 280,
      "endOffset" : 284
    }, {
      "referenceID" : 2,
      "context" : "Most of the intrinsically motivated RL techniques have been applied to exploring agents in simple domains [1; 35; 26; 27], agents that use hand-designed or pre-trained state abstractions of high-dimensional environments [14; 25], or agents that are provided with a low-dimensional taskspace [3].",
      "startOffset" : 291,
      "endOffset" : 294
    }, {
      "referenceID" : 22,
      "context" : "Mugan and Kuipers QLAP [24], Xu and Kuipers OSH [42] and Kompella et al.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 40,
      "context" : "Mugan and Kuipers QLAP [24], Xu and Kuipers OSH [42] and Kompella et al.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : "learning through tracking” [23] strategy to model the static background and the individual foreground objects assuming that the image background is static.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 39,
      "context" : "The agent actively explores within a set of high-dimensional video streams1 and learns to select the stream where it can find the next easiest (quickest) to learn a slow feature (SF; [41]) abstraction.",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 9,
      "context" : "It does this optimally while simultaneously updating the SF abstractions using Incremental Slow Feature Analysis (IncSFA; [10]).",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 36,
      "context" : "MISFA is an optimal sequence of SF abstractions acquired in the order from easy to difficult-to-learn ones, principally similar to the learning process of Utgoff and Stracuzzi’s manylayered learning [38].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 9,
      "context" : "Details on the learning rules of IncSFA can be found in Kompella’s previous work [10].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 39,
      "context" : "For the first task, we estimate and use the time derivative of the slowness measure [41].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : ", u1) is a vector [1, 1, 0, 1, 1], and the second abstraction π∗(.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : ", u1) is a vector [1, 1, 0, 1, 1], and the second abstraction π∗(.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : ", u1) is a vector [1, 1, 0, 1, 1], and the second abstraction π∗(.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : ", u1) is a vector [1, 1, 0, 1, 1], and the second abstraction π∗(.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : ", u2) is [0, 1, 1, 1, 1].",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : ", u2) is [0, 1, 1, 1, 1].",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : ", u2) is [0, 1, 1, 1, 1].",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : ", u2) is [0, 1, 1, 1, 1].",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "After every τ observations, a value function Q and the sub-policy πui are updated using the estimatedR via Least Squares Policy Iteration (LSPI; [16]).",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 34,
      "context" : "The agent uses a decaying -greedy strategy [36] on πu1 to take a new action and the process repeats.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "ν is quite intuitive to set [10].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "We use curiosity function values [19] as a metric to quantify the learning difficulty of an observation stream w.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 41,
      "context" : "0 uses a clustering algorithm called the Robust Online Clustering (ROC) [43] coupled to the IncSFA.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "0 uses a tabular reward function update rule [9]: R̃ ′ a ← α r ′ a + (1 − α)R̃ ′ a ; R ← R̃/‖R̃‖, where α is a constant.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "To demonstrate this, we select an environment consisting of 3 nonlinear oscillatory streams [9] each learnable by IncSFA:",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "It can be found based on the learning difficulty values [9] that the slowest feature of the stream x1 is the easiest to learn followed by x2 and then x3.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "The optimal result also includes the policy to learn these abstractions for the given environment; π∗ = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.",
      "startOffset" : 105,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "The optimal result also includes the policy to learn these abstractions for the given environment; π∗ = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.",
      "startOffset" : 105,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "The optimal result also includes the policy to learn these abstractions for the given environment; π∗ = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.",
      "startOffset" : 116,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "The optimal result also includes the policy to learn these abstractions for the given environment; π∗ = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.",
      "startOffset" : 116,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "The optimal result also includes the policy to learn these abstractions for the given environment; π∗ = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.",
      "startOffset" : 127,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "The optimal result also includes the policy to learn these abstractions for the given environment; π∗ = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.",
      "startOffset" : 127,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "Since x2 is easier to learn than x3, the optimal sub-policy is [1, 0, 1].",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "Since x2 is easier to learn than x3, the optimal sub-policy is [1, 0, 1].",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "The new optimal sub-policy after that signal swap is [0, 1, 1], since x1 is now the easiest to learn.",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "The new optimal sub-policy after that signal swap is [0, 1, 1], since x1 is now the easiest to learn.",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy.",
      "startOffset" : 40,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy.",
      "startOffset" : 40,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy.",
      "startOffset" : 84,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy.",
      "startOffset" : 84,
      "endOffset" : 93
    }, {
      "referenceID" : 37,
      "context" : "To this end, we use the iCub Simulation software [39].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "Furthermore, we calculated the learning difficulty values [19] and found that x1 is easier to encode by IncSFA than x3.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "’s work [10] for details on why IncSFA learns the positions).",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "deviation (shaded region) of the stream selection policy: {[0, 1, 1], [1, 1, 0]}.",
      "startOffset" : 59,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "deviation (shaded region) of the stream selection policy: {[0, 1, 1], [1, 1, 0]}.",
      "startOffset" : 59,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "deviation (shaded region) of the stream selection policy: {[0, 1, 1], [1, 1, 0]}.",
      "startOffset" : 70,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "deviation (shaded region) of the stream selection policy: {[0, 1, 1], [1, 1, 0]}.",
      "startOffset" : 70,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "Therefore, as the decays, it finds the stay action in state s1 most valuable (Figure 5(c)) and the sub-policy converges to πu1 = [0, 1, 1] (Figure 5(d)).",
      "startOffset" : 129,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "Therefore, as the decays, it finds the stay action in state s1 most valuable (Figure 5(c)) and the sub-policy converges to πu1 = [0, 1, 1] (Figure 5(d)).",
      "startOffset" : 129,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "The process repeats, but the gating system prevents re-learning x1 and the agent now learns πu2 = [1, 1, 0] and an abstraction φ2 corresponding to x3.",
      "startOffset" : 98,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "The process repeats, but the gating system prevents re-learning x1 and the agent now learns πu2 = [1, 1, 0] and an abstraction φ2 corresponding to x3.",
      "startOffset" : 98,
      "endOffset" : 107
    }, {
      "referenceID" : 35,
      "context" : "options [37]) and also in environments where it can learn to reuse the learned modular abstractions to solve an external task.",
      "startOffset" : 8,
      "endOffset" : 12
    } ],
    "year" : 2017,
    "abstractText" : "A compact information-rich representation of the environment, also called a feature abstraction, can simplify a robot’s task of mapping its raw sensory inputs to useful action sequences. However, in environments that are non-stationary and only partially observable, a single abstraction is probably not sufficient to encode most variations. Therefore, learning multiple sets of spatially or temporally local, modular abstractions of the inputs would be beneficial. How can a robot learn these local abstractions without a teacher? More specifically, how can it decide from where and when to start learning a new abstraction? A recently proposed algorithm called Curious Dr. MISFA addresses this problem. The algorithm is based on two underlying learning principles called artificial curiosity and slowness. The former is used to make the robot self-motivated to explore by rewarding itself whenever it makes progress learning an abstraction; the later is used to update the abstraction by extracting slowly varying components from raw sensory inputs. Curious Dr. MISFA’s application is, however, limited to discrete domains constrained by a predefined state space and has design limitations that make it unstable in certain situations. This paper presents a significant improvement that is applicable to continuous environments, is computationally less expensive, simpler to use with fewer hyper parameters, and stable in certain non-stationary environments. We demonstrate the efficacy and stability of our method in a vision-based robot simulator.",
    "creator" : "LaTeX with hyperref package"
  }
}