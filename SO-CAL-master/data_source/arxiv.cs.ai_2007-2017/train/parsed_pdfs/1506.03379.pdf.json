{
  "name" : "1506.03379.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Online Discovery Problem and Its Application to Lifelong Reinforcement Learning",
    "authors" : [ "Emma Brunskill", "Lihong Li" ],
    "emails" : [ "ebrunskill@cs.cmu.edu", "lihongli@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Transfer learning, the ability to take prior knowledge and use it to perform well on a new task, is an essential capability of intelligence. Tasks themselves often involve multiple steps of decision making under uncertainty. Therefore, lifelong learning across multiple reinforcement-learning (RL) [24] tasks is of significant interest. Potential applications are enormous, from leveraging information across customers, to speeding robotic manipulation in new environments. In the last decades, there has been much previous work on this problem, which predominantly focuses on providing promising empirical results but with little formal performance guarantees (e.g., [21, 26, 25, 22] and the many references therein), or in the offline/batch setting [15], or for multi-armed bandits [1].\nIn this paper, we focus on a special case of lifelong reinforcement learning which captures a class of interesting and challenging applications. We assume that all tasks, modeled as finite Markov decision processes or MDPs, have the same state and action spaces, but may differ in their transition probabilities and reward functions. Furthermore, the tasks are elements of a finite collection of MDPs that are initially unknown to the agent. Such a setting is particularly motivated by applications to user personalization, such as domains like education, healthcare and online marketing, where one can consider each “task” as interacting with one particular individual, and the goal is to leverage prior experience to improve performance with later users. Indeed assuming all users can be treated as roughly falling into a finite set of groups has already been explored in multiple such domains [7, 17, 19], as it offers a form of partial personalization, allowing the system to more quickly learn good interactions with the user (than learning for each user separately) but still offering much more personalization than modeling all individuals as the same.\nA critical issue in transfer or lifelong learning is how and when to leverage information from previous tasks in solving the current one. If the new task represents a different MDP with a different optimal policy, then leveraging prior task information may actually result in substantially worse performance than learning with no prior information, a\nar X\niv :1\n50 6.\n03 37\n9v 1\n[ cs\n.L G\n] 1\n0 Ju\nn 20\nphenomenon known as negative transfer [25]. Intuitively, this is partly because leveraging prior experience (in the form of samples, value functions, policies or others) can prevent an agent from visiting parts of the state space which differ in the new task, and yet would be visited under the optimal policy for the new task. In other words, there is a unique need for multi-level exploration in lifelong reinforcement learning: in addition to exploration typically needed to obtain optimal policies in single-task RL (i.e., within-task learning), the agent also needs sufficient exploration to uncover relations among tasks (i.e., cross-task transfer).\nTo this end, the agent faces an online discovery problem: the new task may be the same1 as one of the prior tasks, or may be a novel one. The agent can choose to treat each new task as an novel task or as an instance of a prior task. Failing to correctly treat a novel task as new, or treating an existing task as the same as a prior task, will lead to sub-optimal performance. In Section 2, we formulate a novel online-discovery problem that captures such a challenge, and present an algorithm that achieves optimal performance with matching upper and lower regret bounds. These results are then used in Section 3 to create a new lifelong learning algorithm. Not only does the new algorithm relax multiple critical assumptions needed by prior work, it can also immediately start to share information across tasks and is guaranteed to have substantially lower overall sample complexity than single-task learning over a sequence of tasks.\nThe main contributions are as follows. First, we propose a novel lifelong reinforcement-learning algorithm, designed to have efficient, simultaneous exploration for within-task learning and cross-task transfer when tasks are drawn from a finite set of discrete state and action MDPs. Second, we analyze the algorithm’s sample complexity, a theoretical measure of learning speed in online reinforcement learning. Our results show how knowledge transfer provably decreases sample complexity, compared to single-task reinforcement learning, when solving a sequence of tasks. Third, we provide simulation results that compare our algorithms to single-task learning as well as to state-of-theart lifelong learning algorithms, that illustrate the benefits and relative advantages of the new algorithm. Finally, as a by-product, we formalize a novel online discovery problem and give optimal algorithms, as a means to facilitate development of our lifelong learning algorithm. This contribution may be of broader interest in other related metalearning problems with a need for similar exploration to uncover inter-task relation.\nRelated Work. There have been substantial interests in lifelong learning across sequential decision making tasks for multiple decades (see, e.g., [21, 22], and the many references therein). Lifelong RL is closely related to transfer RL, in which information (or data) from source MDPs is used to accelerate learning in the target MDP: [25] provides an excellent survey of work in this area. A distinctive element in lifelong RL is that every task is both a target and a source task. Consequently, the agent has to explore the current task once in a while to allow better knowledge to be transferred to solve tasks in the future; this is the motivation for the online discovery problem we study here.\nThe setting we consider, of sampling MDP models from a finite set, is closely related to multiple previously considered setups. [13] describe hidden parameter MDPs, which cover our setting in this work as well as others where there is a latent variable that captures some key aspects of each task encountered. [26] tackle a similar problem using a hierarchical Bayesian model for the distribution from which tasks are generated. To our best knowledge, the vast majority of prior work on lifelong learning and transfer learning has focused on algorithmic and empirical innovations, and there has been very little formal analysis before our presented work for online learning. An exception is a twophase algorithm [3] with provably small sample complexity, but makes a few critical assumptions.\nThe online discovery problem appears new, although it has connections to several other existing problems. One is bandit problems [4] that also require an effective exploration/exploitation trade-off. However, in bandits every action leads to an observed loss, while in online discovery, only one action has observable loss. The apple tasting (AT) problem [9] has a similar flavor, but with a different structure in the loss matrix; furthermore, the analysis is in the mistake bound model that is not suitable here. [5] tackles “optimal discovery” in a very different setting, focusing on quick identification of hidden elements given access to different sampling distributions (called “experts”). Finally, ODP is related to the missing mass problem (MMP) [18]. While MMP is a pure prediction problem, ODP involves decision making, hence requires balancing exploration and exploitation."
    }, {
      "heading" : "2 The Online Discovery Problem",
      "text" : "Motivated by the need for cross-task exploration in lifelong RL, in this section, we study a novel online discovery problem that will play a crucial role in developing new lifelong RL algorithms in Section 3. In addition to the appli-\n1Even if no identical MDP is experienced, MDPs with similar model parameters have similar value functions. Thus, fintely many policies suffice to represent -optimal policies for all MDPs with shared state/actions.\ncation here, this problem may be of independent interest in other meta-learning problems where there is a need for efficient exploration to uncover cross-task relation."
    }, {
      "heading" : "2.1 Formulation",
      "text" : "We now describe the online discovery problem (ODP), a sequential game where the agent decides in each round whether to explore the item in that round. LetM be an unknown set of C items to be discovered by a learner, and A = {0 (“exploitation”), 1 (“exploration”)} a set of two actions. The learner need not know C. Initially, the set of discovered itemsM1 is ∅. The learner is also given four constants, ρ0 < ρ1 ≤ ρ2 ≤ ρ3, specifying the loss matrix L in Table 1. The game proceeds as follows. For round t = 1, 2, . . . , T :\n• Environment selects an item Mt ∈M. • Without knowing identity of Mt, the learner chooses action At ∈ A, and suffers a loss Lt = L(At, I {Mt ∈Mt}), where L is the loss matrix in Table 1. The learner observes Lt when At = 1, and ⊥ (“no observation”) otherwise.\n• If At = 1, thenMt+1 ←Mt ∪ {Mt}; otherwise,Mt+1 ←Mt. At the beginning of round t, we define Ht := (A1, L1,M2, A2, L2, . . . , At−1, Lt−1,Mt−1), the history up to t. An algorithm is admissible, if it chooses actions At based on Ht and possibly an external source of randomness. As in the online-learning literature, we distinguish two settings. In the stochastic setting, the environment picks Mt in an i.i.d. (independent and identically distributed) manner from an unknown distribution µ overM. In the adversarial setting, the sequence (Mt)t can be generated by an adversarial in an arbitrary way that depends on Ht.\nIf the learner knew the identity of Mt, the optimal strategy is to choose At = 1 if Mt /∈ Mt and At = 0 otherwise. After T rounds, this ideal strategy has the optimal loss of L∗(T ) := ρ2C∗ + ρ0(T − C∗), where C∗ ≤ C is the number of distinct items in the sequence (Mt)t. The challenge, of course, is that the learner does not know Mt before selecting action At. She thus has to balance exploration (taking At = 1 to see if Mt is novel) and exploitation (taking At = 0 to yield small loss ρ0 if it is likely thatMt ∈Mt). Clearly, over- and under-exploration can lead to suboptimal strategies. Therefore, we are interested in finding algorithms A to have smallest cumulative loss as possible. Formally, an algorithm A for online discovery is a (possibly stochastic) policy that maps histories to actions: At = A(Ht). The total T -round loss suffered by A is L(A, T ) := ∑T t=1 Lt. The T -round expected regret of an algorithm A is defined by R̄(A, T ) = E[L(A, T )−L∗(T )], where the expectation is taken with respect to any randomness in the environment as well as in A.\nIt should be noted that we could set ρ0 = 0 without affecting the definition of regret. However, we allow it to be positive for convenience when mapping lifelong RL into online discovery later."
    }, {
      "heading" : "2.2 The Explore-First Algorithm",
      "text" : "In the stochastic case, it can be shown that if an algorithm takes a total ofE explorations, its expected regret is smallest if these exploration rounds occur at the very beginning. The resulting strategy is sometimes called EXPLORE-FIRST, or EXPFIRST for short, in the multi-armed bandit literature.\nWith knowledge of T , C and µm := minM∈M µ(M), one may set E to E∗ = E(C, µm) := µ−1m ln C δ , so that all items inM will be discovered in the first E∗ rounds, with probability 1 − δ. After that, it is safe to always exploit (At ≡ 0). The total expected loss can be upper bounded as: E[L(EXPFIRST, T )] ≤ ρ1E∗+ρ2C+ρ0(T−E∗)+δρ3T , where the first two terms correspond to the loss incurred in the exploration rounds, the third the loss incurred in exploitation rounds, and the last the loss incurred in the lower-probability event (that some item inM does not occur\nin the first E∗ rounds). This upper bound can be minimized by optimizing δ, provided that C and ρi’s are known. The result is summarized in the following proposition, proved in Appendix B.1: Proposition 1. With E∗ = µ−1m ln Cδ with δ = ρ1 ρ3µmT , EXPFIRST has the following regret bound:\nR̄(EXPFIRST, T ) ≤ ρ1 µm\n( lnT + ln\nCµmρ3 ρ1 + 1\n) ."
    }, {
      "heading" : "2.3 Forced Exploration",
      "text" : "While EXPFIRST is effective in stochastic ODPs, in many applications, the task generation distribution may be nonstationary (e.g., different types of users may use the Internet at different time-of-the-day) or even adversarial (e.g., an attacker may present certain MDPs in earlier rounds in lifelong RL to cause an algorithm to perform poorly in future MDPs). We now study a simple yet more general algorithm, FORCEDEXP (for Forced Exploration), and proves an upper bound for its regret. In the next subsection, we will present a matching lower bound, indicating optimality of this algorithm.\nBefore the game starts, the algorithm determines a fixed schedule for exploration. Specifically, it pre-decides a sequence of “exploration rates”: η1, η2, . . . , ηT ∈ [0, 1]. Then, in round t, it chooses the exploration action with probability ηt: P {At = 1} = ηt and P {At = 0} = 1− ηt. The main result about FORCEDEXP is the following theorem, proved in Section B.3 Theorem 2. If we run FORCEDEXP with non-increasing exploration rates η1 ≥ · · · ≥ ηT > 0, then\nE[L(FORCEDEXP, T )] ≤ ρ0T + Cρ3 ηT + ρ1 T∑ t=1 ηt.\nThe theorem directly implies the following corollary (proved in Section B.4): Corollary 3. If we set ηt = t−α (polynomial decaying rate) for some parameter α ∈ (0, 1), then\nR̄(FORCEDEXP, T ) ≤ Cρ3Tα + ρ1\n1− α T 1−α .\nIf we set ηt ≡ η for some η ∈ (0, 1) (fixed rate), then\nR̄(FORCEDEXP, T ) ≤ Cρ3 η + ηρ1T .\nFurthermore, the bounds above are both on the order of O( √ T ) by setting α = 1/2 and η = √ Cρ3/(ρ1T ), respectively.\nThe results show that FORCEDEXP eventually performs as well as the optimal policy that knows the identity of Mt in every round t, no matter howMt is generated. Moreover, the per-round regret decays on the order of 1/ √ T , which we will show to be optimal. Although in the worst case FORCEDEXP with fixed rate achieves a regret of the same order as the one with polynomial rates, it is expected that the latter is better in the stochastic setting, at least empirically.\nNote that knowledge of relevant quantities such as ρs and T is useful to optimize parameters in Corollary 3. In particular, the value of η as given in the corollary depends on T . When T is unknown, one can still apply the standard doubling trick to get the same O( √ T ) regret bound (Section B.4)."
    }, {
      "heading" : "2.4 Lower Bound",
      "text" : "The main result in this section, Theorem 4, shows theO( √ T ) regret bound for FORCEDEXP is essentially not improvable, in term of T -dependence, even in the stochastic case. The idea of the proof, given in Section B.5, is to construct a hard instance of stochastic ODP. On one hand, Ω( √ T ) regret is suffered unless all C items inM are discovered. On the other hand, most of the items have low probability µm of being sampled, requiring the learner to take the exploration action A = 1 many times to discover all C items. The lower bound follows from an appropriate value of µm.\nTheorem 4. There exists an online discovery problem, where every admissible algorithm suffers an expected regret of Ω( √ T ).\nAlthough the lower bound matches the upper bounds in terms of T , we have not attempted to match dependence on other quantities like C, which are often less important than T .\nThis lower bound may seem to contradict EXPFIRST’s logarithmic upper bound in Proposition 1. However, that upper bound is a problem-specific bound and requires knowledge of C and µm. Without knowing µm, the algorithm has to choose µm = Θ( 1√T ) in the exploration phrase; otherwise, there is a chance it may not be able to discover an item M with µ(M) = Ω( 1√ T ), suffering Ω( √ T ) expected regret. With this value of µm, the bound in Proposition 1 has an\nÕ( √ T ) dependence."
    }, {
      "heading" : "3 PAC-MDP Lifelong Reinforcement Learning",
      "text" : "Building on the ODP results established in Section 2, we now turn to lifelong reinforcement learning."
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "We consider reinforcement learning [24] in discrete-time, finite MDPs specified by a five-tuple: 〈S,A, P,R, γ〉, where S is the set of states, A the set of actions, P the transition probability function, R : S × A → [0, 1] the reward function, and γ ∈ (0, 1) the discount factor. Denote by S and A the numbers of states and actions, respectively. A policy π : S → A specifies what action to take in a given state. Its state and state–action value functions are denoted by V π(s) and Qπ(s, a), respectively. The optimal value functions for an optimal policy π∗ are V ∗ and Q∗ so that V ∗(s) = maxπ V\nπ(s) and Q∗(s, a) = maxπ Qπ(s, a), for all s and a. Finally, let Vmax be a known upper bound of V ∗(s), which is at most 1/(1− γ). In RL, P and R are initially unknown to the agent, who must learn to optimize its policy via interaction with the MDP. Various frameworks have been proposed to capture the learning speed or effectiveness of a single-task online reinforcement-learning algorithm, such as regret analysis (e.g., [10]). Here, we focus on another useful notion known as sample complexity of exploration [11], or sample complexity for short. However, some of our ideas, especially those related to cross-task transfer and the online discovery problem, may also be useful for regret analysis.\nFix parameters , δ > 0. Any RL algorithm A can be viewed as a nonstationary policy, whose value functions, V A and QA, are defined similarly to the stationary-policy case. When A is run on an unknown MDP, we call it a mistake at step t if the algorithm chooses a suboptimal action, namely, V ∗(st) − V At(st) > . If the number of mistakes is at most ζ( , δ) with probability at least 1 − δ, for any fixed > 0 and δ > 0, then the sample complexity of A is ζ. Furthermore, if ζ is polynomial in S, A, 1/(1− γ), 1/ , and ln(1/δ), then A is called PAC-MDP [23]. Note that the definition of sample complexity does not impose any condition on when the -suboptimal steps occur: in fact, some of these sub-optimal steps may occur indefinitely far into the future.\nThe earliest, and most representative, PAC-MDP algorithms for finite MDPs are model-based algorithms, E3 [12] and RMAX [2, 11]. In the heart of both algorithms is the distinction between known and unknown states. When an RMAX agent acts in an unknown environment, it maintains an estimated model for the unknown MDP, using empirically observed transitions and rewards. When it has taken a certain action in a state sufficiently often, as specified by a threshold of how many times the action has been taken in that state, it has high confidence in the accuracy of its estimated model in that state–action pair (thanks to concentration inequalities like the Azuma-Hoeffding inequality), and that state–action pair is considered known. Other (unknown) state–actions are assigned maximal reward to encourage exploration. With such an optimism-in-the-face-of-uncertainty principle, RMAX can be shown to either explore (reaching an unknown state–action in a short amount of time) or exploit (achieving near-optimal discounted cumulative reward). Since the number of visits to unknown state–action pairs is bounded by a polynomial function in relevant quantities, RMAX is PAC-MDP. The intuition behind E3 is similar."
    }, {
      "heading" : "3.2 Balancing Cross-task Exploration/Exploitation in Lifelong RL",
      "text" : "In lifelong reinforcement learning, the agent seeks to maximize its reward as it acts in a sequence of T MDPs. If prior tasks are related to future tasks, we expect leveraging knowledge of this prior experience may lead to enhanced\nperformance. Formally, following previous work [3, 26], and motivated by numerous applications [21, 26, 25, 22] we assume a finite set M of possible MDPs. The agent solves a sequence of T tasks, with Mt ∈ M denoting the MDP corresponding to the t-th task. Before solving the task, the agent does not know whether or not Mt has been encountered before. It then acts in Mt for H steps, where H is a given horizon, and is allowed to take advantage of any information extracted from solving previous tasks {M1, . . . ,Mt−1}. Our setting, however, is more general, as the sequence of tasks may be chosen in an adversarial (instead of stochastic [3, 26]) way. A consequence of is that there is no minimum task sampling probability as in previous work (such as the quantity pmin in [3]). Furthermore, we do not assume knowledge of the number of distinct MDPs, C = |M|, or an upper bound of C. All these distinctions make the setting more applicable to capture a broader range of problems.\nWhile provably efficient exploration-exploitation tradeoffs have been extensively studied in single-task RL [10, 23], there is an additional, similar trade-off at the task level in lifelong learning. This arises because the agent does not know in advance if the new task is identical2 to a previously solved MDP, or if it is a novel MDP. The only way to identify similarity between the new task and previous ones is to explore the task sufficiently. The aim of such exploration is not to maximize reward in the current task, but to infer task identity (which may not help maximize total reward in the current task). Therefore, a lifelong learning agent needs to mix and balance such task-level exploration with within-task exploration that is common in single-task RL.\nThis observation inspired our abstraction of the online discovery problem, which we now apply to lifelong RL. Here, the exploration action (At = 1 in Section 2) corresponds to doing complete exploration in the current task, while the exploitation action (At = 0) corresponds to applying transferred knowledge to accelerate learning. We employ FORCEDEXP for this case, which is outlined in Algorithm 2 of Section A. Overloading terminology, we will also use FORCEDEXP to refer to FORCEDEXP applied to continuous lifelong RL. At round t, if exploration is to happen, it performs PAC-EXPLORE [8] (Algorithm 1 of Section A) to get an accurate model for Mt in all states, which allows it to discover a new distinct MDP fromM. If the empirical MDP, M̂t, is considered new3, it is added to the set M̂ of discovered MDPs. If exploration is not to happen, the agent assumes Mt is in M̂, and follows the Finite-Model-RL algorithm [3], which is an extension of RMAX to work with finitely many MDP models. Due to space limitation, full algorithmic details are given in Section A.\nIn its current form, the algorithm chooses At for task Mt before seeing any data collected by acting in Mt. It is straightforward to change the algorithm so that it can switch from an exploitation mode (At = 0) to exploration after collecting data in Mt, if there is sufficient evidence that Mt is different from all MDPs already found in M̂. Although this change does not improve worst-case sample complexity, it can be beneficial in practice. On the other hand, switching from exploration to exploitation is in general not helpful, as shown in the following example. Let S = {s} contains a single state s, so that P (s|s) = 1, and MDPs inM differ only in their reward functions. Suppose at some step t, the agent has discovered a set of distinct MDPs M̂ from the past, and chooses to do exploration (At = 1). After taking some steps in Mt, if the agent decides to switch to exploitation before making every action known, there is a risk of under-exploration: Mt may be a new MDP not encountered before, but it has the same rewards on optimal actions for already-discovered MDPs in M̂, but the optimal action in Mt yields even higher reward. By discontinuing exploration, an agent may fail to find the real optimal action in Mt and suffers high sample complexity."
    }, {
      "heading" : "3.3 Sample Complexity Analysis",
      "text" : "This section gives a sample-complexity analysis for the lifelong algorithm in the previous subsection. For convenience, we use θM to denote the dynamics of an MDP M ∈ M: for each (s, a), θM (·|s, a) is an (S + 1)-dimensional vector, with the first S components giving the transition probabilities to corresponding next states, P (s′|s, a), and the last component the average immediate reward, R(s, a). The model difference in (s, a) between M and M ′, denoted ‖θM (·|s, a) − θM ′(·|s, a)‖, is the `2-distance between the two vectors. Finally, we let N be an upper bound on the number of next states in the transition models in all MDPs M ∈ M; N ≤ S and can be much smaller in many problems.\nThe following assumptions are needed:\n2Or has a near-identical MDP model that leads to -optimal policies of a prior task. 3Specifically, we check if the new MDP parameters differ by at least Γ (an input parameter) in at least one state–action pair to\nall existing MDPs. If so, we add the MDP to the set. More details about Γ are given shortly.\n1. There exists a known quantity Γ > 0 such that for every two distinct MDPs M,M ′ ∈ M, there exists some (s, a) so that ‖θM (·|s, a)− θM ′(·|s, a)‖ > Γ; 2. There is a known diameter D, such that for every MDP inM, any state s′ is reachable from any state s in at most D steps on average;\n3. There are H ≥ H0 = O ( SAN log SATδ max{Γ −2, D2} ) steps per task.\nThe first assumption requires two distinct MDPs differ by a sufficient amount in their dynamics in at least one state– action pair, and is made for convenience to encode prior knowledge about Γ. Note that if Γ is not known beforehand, one can set Γ = Γ0 = O ( (1−γ)√ NVmax ) : if two MDPs differ by no more than Γ0 in every state–action pair, an -optimal policy in one MDP will be an O( )-optimal policy in another. The second and third assumptions are the major ones needed in our analysis. The diameter, introduced by [10], and the long enough horizon H together make it possible for an agent to uncover whether the current task has been discovered before.\nWith these assumptions, the main result is as follows. Note that tighter bounds (in 1/(1−γ) etc.) for ρ0 and ρ1 should be possible by leveraging the refined but more involved single-task analysis of [14], which we defer to future work.\nTheorem 5. Let Algorithm 2 be run for a sequence of T tasks, each of which is from a setM of C MDPs. Then, with probability at least 1− δ, the expected number of steps in which the policy is not -optimal across all T tasks is\nÕ ( ρ0T + Cρ3T α + ρ1\n1− α T 1−α\n) ,\nwhere ρ0 = CD\nΓ2 , ρ1 = ρ2 = SANV 3max 3(1− γ)3 , ρ3 = H . (1)\nThe theorem suggests a substantial reduction in sample complexity with Algorithm 2: while single-task RL typically has a per-task sample complexity ζs that at least scales linearly with SA and with similar dependence on , 1/(1− γ) and Vmax as ρ1. It is worth noting a subtle difference between between Theorems 5 and a prior result [3]. The previous result [3] gives a high-probability bound on the actual sample complexity, while Theorem 5 gives a high-probability bound on the expected sample complexity. However, this technical difference may not matter much in reality when T 1. The proof proceeds by analyzing the sample complexity bounds for all four possible cases (as in the online discovery problem) when solving the tth MDP, and then combining them with Theorem 2 to yield the desired results. The major steps is to ensure that when exploration happens (At = 1), the identity ofMt will be uncovered successfully (with high probability). This is achieved by a couple of key technical lemmas below. A detailed proof is given in Section D.3.\nThe first lemma ensures all state–actions can be visited sufficiently often in finitely many steps, when the MDP has a small diameter. The proof is in Section D.1.\nLemma 6. For a given MDP, PAC-EXPLORE with known threshold me will visit all state–action pairs at least me times in no more than H0(me) = O(SADme) steps with probability at least 1 − δ, where me ≥ m0 = O ( ND2 log Nδ ) .\nThe second lemma establishes the fact that when PAC-EXPLORE is run on a sequence of T tasks, with high probability, it successfully infers whether Mt has been included in M̂, for every t. This result follows from the Lemma 6 and the assumption involving Γ. The proof is in Section D.2.\nLemma 7. If the known threshold is set to m = 72N log 4SATδ max{Γ −2, D2} and horizon H ≥ H0(m) (where H0(·) is given in Lemma 6), then the following holds with probability at least 1 − 2δ: for every task in the sequence, the algorithm detects it is a new task if and only if the corresponding MDP has not been seen before."
    }, {
      "heading" : "4 Experiments",
      "text" : "We use a simple grid-world environment with 4 tasks to illustrate the salient properties of FORCEDEXP. All tasks had the same 25-cell square grid layout and 4 actions (up, down, left, right). We provide full details in the supplementary materials, but briefly actions are stochastic, and in each of the 4 MDPs one corner offers high reward (sampled from a Bernoulli with parameter 0.75) and all other rewards are 0. In MDP 4 both the same corner as MDP 3 is rewarding, and the opposite corner is a Bernoulli with parameters 0.99.\nWe also compare to a Bayesian hierarchical multi-task learning algorithm, or HMTL, of [26]. HMTL learns a Bayesian multi-task posterior distribution across tasks, and leverages as a prior when interacting with each new task. HMTL performs no explicit exploration and has no formal guarantees, but performed well on a challenging real-time strategy game.\nWe evaluated the algorithms on three illustrative variants of the grid world task:\n• EvenProb: All four MDPs are sampled with equal probability of 0.25. • UnevenProb: 3 MDPs each have probability 0.31 and 1 MDP has a tiny probability 0.07. • Nonstationary: Across 100 tasks all 4 MDPs have identical frequencies, but MDPs 1–3 appear in phase-one\nexploration of EXPFIRST, then MDP 4 is shown for 25 tasks, and followed by the only MDPs 1–3.\nAs expected, all algorithms did well in the EvenProb setting, and we do not further discuss this case here. For the UnevenProb setting, the EXPFIRST algorithm suffers (see Figure 1a) since it must make the first ( exploration) phase very long to have a high probability of learning all tasks. In contrast, our new algorithm FORCEDEXP quickly obtains good performance. HMTL also performs well since no explicit exploration is required.\nIn Figure 1b, an adversary introduces MDP 4 after the initial exploration phase of EXPFIRST completes (which does not violate the minimum probability of each MDP across the entire horizon of tasks, and the upper bound on the number of MDPs given to EXPFIRST). This new task (MDP 4) can obtain similar rewards as MDP 1 using the same policy as for MDP 1, but can obtain higher rewards if the agent explicitly explores in the new domain. Our FORCEDEXP algorithm will randomly explore and identify this new optimal policy, which is why it eventually picks up the new MDP and obtains higher reward. EXPFIRST sometimes successfully infers the task belongs to a new MDP, but only if it happens to encounter the state that distinguished MDPs 1 and 4. HMTL does not explore actively, so it consistently fails to identity MDP 4 as a novel MDP. Consequently, whenever it faces MDP 4, it always learns to use the optimal policy for MDP 1, which is however sub-optimal in MDP 4.\nThese results suggest FORCEDEXP can have comparable or significantly better performance than prior approaches when there is a highly nonuniform or nonstationary task generation process. These benefits are direct consequences of the effective cross-task exploration built into the algorithm."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we consider a class of lifelong reinforcement learning problems that capture a broad range of interesting applications. Our work emphasizes the need for effective cross-task exploration that is unique in lifelong learning. This led to a novel online discovery problem, for which we give optimal algorithms with matching upper and lower regret bounds. With this technical tool, we developed a new lifelong RL algorithm, and analyzed its total sample complexity across a sequence of tasks. Our theory quantifies how much gain is obtained by lifelong learning, compared to singletask learning, even if the tasks are adversarially generated. The algorithm was empirically evaluated in a simulated problem, demonstrating its relative strengths compared to prior work.\nWhile we focus on algorithms with formal sample-complexity guarantees, recent work [20] has shown the benefit of a Bayesian approach similar to Thompson sampling for RL domains, and provided Bayesian regret guarantees. As these\nresults rely on an input prior over the MDP, they could easily incorporate a prior learned across multiple tasks. One interesting future direction would be an empirical and theoretical investigation along this line of work for lifelong RL."
    }, {
      "heading" : "A Algorithm Pseudocode",
      "text" : "In the following, define Rmax := 1.\nAlgorithm 1 PAC-EXPLORE Algorithm [8] 0: Input: me (known threshold), D (diameter) 1: Set L← 3D 2: while some (s, a) has not been visited at least me times do 3: Let s be the current state 4: if all a have been tried me times then 5: Start a new L-step episode 6: Construct an empirical known-state MDP M̂K with the reward of all known (s, a) pairs set to 0, all unknown\nset to Rmax, the transition model of all known (s, a) pairs set to the estimated parameters and the unknown to self loops\n7: Compute an optimistic L-step policy π̂ for M̂K 8: From the current state, follow π̂ for L steps, or until an unknown state is reached 9: else\n10: Execute a that has been tried the least 11: end if 12: end while\nAlgorithm 2 Lifelong Learning with FORCEDEXP for Cross-task Exploration 1: Input: α ∈ (0, 1], m ∈ N 2: Initialize M̂ ← ∅ 3: for t = 1, 2, . . . do 4: Generate a random number ξ ∼ Uniform(0, 1) 5: if ξ < t−α then 6: Run PAC-EXPLORE to fully explore all states in Mt, so that every action is taken in every state for at least\nm times. 7: After the above exploration completes, choose actions according to the optimal policy in the empirical model\nM̂t. 8: if for all existing models M̂ ∈ M̂, M̂t has a non-overlapping confidence intervals in some state–action pair then 9: M̂ ← M̂ ∪ {M̂t}\n10: end if 11: else 12: Run Finite-Model-RL [3] with M̂ 13: end if 14: end for"
    }, {
      "heading" : "B Proofs for Section 2",
      "text" : ""
    }, {
      "heading" : "B.1 Proof for Proposition 1",
      "text" : "As explained in the text, the expected total loss of EXPFIRST is at most\nE[L(EXPFIRST, T )] ≤ ρ1E∗ + ρ2C + ρ0(T − E∗) + δρ3T ,\nThe optimal strategy has the loss of L∗ = ρ2C + ρ0(T − C). Therefore, the regret of EXPFIRST may be bounded as\nR̄(EXPFIRST, T ) = E[L(EXPFIRST, T )]− L∗\n≤ ρ1E∗ + δρ3T + ρ0(C − E∗) < ρ1E ∗ + δρ3T\n= ρ1 µm ln C δ + δρ3T , (2)\nwhere we have made use of the fact that E∗ > C. The right-hand side of the last equation is a function of δ, in the form of f(δ) := a − b ln δ + cδ, for a = ρ1µm lnC, b = ρ1 µM\n, and c = ρ3T . Because of convexity of f , its minimum can be found by solving f ′(δ) = 0 for δ, giving\nδ∗ = b\nc = ρ1 ρ3µmT .\nSubstituting δ∗ for δ in Equation 2 gives the desired bound."
    }, {
      "heading" : "B.2 Lemma 8",
      "text" : "Lemma 8. Fix M ∈ M, and let 1 ≤ t1 < t2 < . . . < tm ≤ T be the rounds for which Mt = M . Then, the expected total loss incurred in these rounds is bounded as:\nL̄M (FORCEDEXP) < (mρ0 + ρ2 − ρ3)L̄1 + (ρ3 − ρ0)L̄2 + ρ1L̄3 ,\nwhere L̄1 := ∑ i ∏ j<i(1− ηtj )ηti , L̄2 := ∑ i ∏ j<i(1− ηtj )ηti · i, and L̄3 := ∑ i (∏ j<i(1− ηtj )ηti ∑ j>i ηtj ) .\nProof. Let L̄M (FORCEDEXP) be the expected total loss incurred in the rounds t where Mt = M : 1 ≤ t1 < t2 < · · · < tm ≤ T for some m ≥ 0. Let I ∈ {1, 2, . . . ,m,m+ 1} be the random variable, so that M is first discovered in round tI . That is,\nAtj = { 0, if j < I 1, if j = I .\nNote that I = m + 1 means M is never discovered; such a notation is for convenience in the analysis below. The corresponding loss is given by\n(I − 1)ρ3 + ρ2 + ∑ j>I ( ρ0I { Atj = 0 } + ρ1I { Atj = 1 }) ,\nwhose expectation, conditioned on I , is at most (I − 1)ρ3 + ρ2 + ∑ j>I ( ρ0 + ρ1ηtj ) .\nSince FORCEDEXP chooses to explore in step t with probability, we have that P {I = i} = ∏ j<i (1− ηtj )ηti .\nTherefore, L̄M (FORCEDEXP) can be bounded by\nL̄M (FORCEDEXP)\n≤ m+1∑ i=1 P {I = i} (I − 1)ρ3 + ρ2 +∑ j>I ( ρ0 + ρ1ηtj ) =(mρ0 + ρ2 − ρ3)L̄1 + (ρ3 − ρ0)L̄2 + ρ1L̄3, ,\nwhere\nL̄1 = ∑ i ∏ j<i (1− ηtj )ηti ,\nL̄2 = ∑ i ∏ j<i (1− ηtj )ηti · i , L̄3 = ∑ i ∏ j<i (1− ηtj )ηti ∑ j>i ηtj  ."
    }, {
      "heading" : "B.3 Proof for Theorem 2",
      "text" : "For each M ∈M, Lemma 8 gives an upper bound of loss incurred in rounds t for which Mt = M : L̄M (FORCEDEXP) ≤ (mρ0 + ρ2 − ρ3)L̄1 + (ρ3 − ρ0)L̄2 + ρ1L̄3 ,\nwhere\nL̄1 := ∑ i ∏ j<i (1− ηtj )ηti ,\nL̄2 := ∑ i ∏ j<i (1− ηtj )ηti · i , L̄3 := ∑ i ∏ j<i (1− ηtj )ηti ∑ j>i ηtj\n . We next bound the three terms of L̄M (FORCEDEXP), respectively.\nTo bound L̄1, we define a random variable I , taking values in {1, 2, . . . ,m,m+ 1}, whose probability mass function is given by\nP {I = i} =\n{∏ j<i ( 1− ηtj ) ηti , if i ≤ m∏\nj≤m ( 1− ηtj ) , if i = m+ 1.\n(3)\nTherefore, I is like a geometrically distributed random variable, except that the parameter for the ith draw is not the same and is ηti . Consequently,\nL̄1 = ∑ i P {I = i} ≤ 1 .\nTo bound L̄2, we use the same random variable I:\nL̄2 = m∑ i=1 P {I = i} · i\n≤ m∑ i=1 P {I ≥ i} (Corollary of Theorem 3.2.1 of [6])\n= m∑ i=1 ∏ j<i (1− ηtj ) (By definition of I in Equation 3)\n≤ m∑ i=1 ∏ j<i (1− ηtT ) (By assumption that η1 ≥ · · · ≥ ηT )\n= 1\nηT (1− (1− ηT )m) ≤\n1\nηT .\nTo bound L̄3, we have\nL̄3 ≤ m∑ i=1 ∏ j<i (1− ηtj )ηti m∑ j=1 ηtj = L̄1 m∑ j=1 ηtj ≤ m∑ j=1 ηtj .\nPutting all three bounds above, we have\nL̄M (FORCEDEXP) ≤ mρ0 + ρ3 − ρ0 ηT + ρ1 m∑ j=1 ηtj .\nNow sum up all L̄M (FORCEDEXP) over all M ∈M, and we have E[L(FORCEDEXP, T )] = ∑ M∈M L̄M (FORCEDEXP)\n≤ ρ0T + Cρ3 ηT + ρ1 T∑ i=1 ηti ."
    }, {
      "heading" : "B.4 Proof for Corollary 3",
      "text" : "For polynomial exploration rates ηt = t−α, we have\nT∑ t=1 ηt = 1 + T∑ t=2 t−α\n≤ 1 + ∫ T\n1\nt−αdt\n= 1 + 1\n1− α t1−α ∣∣T t=1\n= 1 + 1 1− α ( T 1−α − 1 ) ≤ T 1−α\n1− α .\nThe total regret follows immediately from Theorem 2. Furthermore, if one sets α = 1/2, the regret bound becomes O(Cρ3 + 2ρ1) √ T = O( √ T ).\nFor constant exploration rates ηt ≡ η, the regret follows directly from Theorem 2:\nCρ3 η + ρ1 η∑ t=1 = Cρ3 η + ηρ1T .\nThe optimal η value to minimize the right-hand side above is η = √\nCρ3 ρ1T , which yields the final regret bound of\n2 √ Cρ1ρ3T = O( √ T ).\nOptimizing η in constant exploration when T is unknown. The optimal η value above depends on T . If T is unknown, the following standard doubling trick may be used to yield the same O( √ T ) regret bound. The idea is to guess the value of T , and then double the guess when the number of steps exceeds the current guess. Concretely, let m be the integer such that\n2m − 1 = m−1∑ τ=0 2τ < T ≤ m∑ τ=0 2τ = 2m+1 − 1 .\nLet phase τ consist of steps t between 2τ−1 (exclusively) and 2τ (inclusively); that is, t ∈ (2τ−1, 2τ ]. In phase τ , the guess of T is Tτ = 2τ , and the learning rate can be set at ητ = 1/ √ Tτ = √ 2−τ in those steps. Theorem 2 implies the total regret in phase τ is at most Cρ3 ητ + ρ1ητ (2 τ − 2τ−1) = (Cρ3 + ρ1/2) √ 2τ . Therefore, the total regret across all T steps is at most m∑ τ=0 (Cρ3 + ρ1/2) √ 2τ ≤ (2 + √ 2)(Cρ3 + ρ1 2 )2m/2\n≤ (2 + √\n2)(Cρ3 + ρ1 2\n) √ T = O( √ T ) ,\nwhere we have used the condition that 2m − 1 < T in the second inequality."
    }, {
      "heading" : "B.5 Proof for Theorem 4",
      "text" : "We construct a stochastic online discovery problem withM = {1, 2, . . . , C} and distribution µ so that\nµ(M) = { µm, if M < C 1− Cµm, if M = C ,\nwhere µm = 1/ √ T 1. For every M ∈M, define TM ∈ {1, . . . , T,∞} as the first time M is discovered; that is TM = min{t |Mt = M,At = 1} . Furthermore, let 1 ≤ t1 < t2 < · · · < tE ≤ T be the rounds in which exploration happens (that is, At = 1); denote by E the set {t1, t2, . . . , tE}. Since the two random variables Mt and At are independent, conditioned on Ht, we have for any i ∈ {1, 2, . . . , E} and any M ∈M that\nP {Mti = M} = µ(M) . Conditioning on E being the rounds of exploration, we want to lower bound the number E of exploration rounds so that the probability of discovering all items in M is at most δ (which is necessary for the expected regret to be O( √ T )). First, note that the events {TM < ∞}M∈M are negatively correlated, since discovering some M1 in E can only decrease the probability of discovering M2 6= M1 in E . Therefore, we have P {∀M,TM <∞} ≤\n∏ M∈M P {TM <∞} ≤ (1− (1− µm)E)C−1 .\nMaking the last expression to be 1− δ, we have E = ln ( 1− (1− δ) 1 C−1 ) ln(1− µm)\n= Ω\n( ln ( 1− (1− δC ) ) −µm )\n= Ω\n( 1\nµm ln C δ\n) ,\nfor sufficiently small µm and δ.\nFor simplicity, assume ρ0 = 0 without loss of generality; otherwise, we can just define a related problem with ρ′i := ρi− ρ0, where the loss is just shifted by a constant and the regret remains unchanged. With this assumption, the optimal expected loss is Cρ2 (c.f., L∗(T )).\nThe expected loss of the is now at least (E −C)ρ1 +Cρ2 + δ(T −E)µmρ3, where the first two terms are for the loss incurred during the E exploration rounds; and the last term for the δ-probability event that some item is not discovered in the exploration rounds, which leads to ρ3 loss when it is encountered in any of the remaining T − E rounds. Finally, the regret can be readily computed as\n(E − C)ρ1 + δ(T − E)µmρ3 = Ω ( ρ3Tµm +\nρ2 µm ln C δ\n) ,\ncompleting the proof with the fact that µm = 1/ √ T ."
    }, {
      "heading" : "22 25242321",
      "text" : ""
    }, {
      "heading" : "C Experiment Details",
      "text" : "All four MDPs had the same 25-cell square grid layout and 4 actions (up, down, left, right), as shown in Figure 2. Actions succeed in their intended direction with probability 0.85 and with probability 0.05 go in the other directions (unless halted by a wall). For all actions corner states s5,s20, and s25 stay in the same state with probability 0.95 or transition back to the start state (for all actions). The start state is at the center of the square grid (s13). The dynamics of all MDPs are identical. All rewards are sampled from binomial distributions. All rewards have parameter 0.0 unless otherwise noted. In MDP 1, corner state s20 has a reward parameter of 0.75. In MDP 2, corner state s5 has a reward parameter of 0.75. In MDP 3, corner state s25 has a reward parameter of 0.75. In MDP 4, corner state s25 has a reward parameter of 0.75 and corner state s1 has a reward parameter of 0.99.\nWe also compare to a Bayesian hierarchical multi-task learning algorithm, or HMTL, of [26]. HMTL learns a Bayesian multi-task posterior distribution across tasks, and leverages as a prior when interacting with each new task. HMTL selects the MAP estimate of the model parameters, computes a policy for this models, executes for a fixed number of steps, updates the posterior for the current task, and repeats. No formal guarantees are provided, although empirically HMTL did well on a challenging real-time strategy game.\nFor HMTL, we set the interval between recomputing the MAP model parameters at 20 steps: this choice was made after informal experimentation suggested this value improved performance compared to longer intervals.\nIn all cases, EXPFIRST is given an upper bound on the number of MDPs (4) and the minimum probability of any of the MDPs across the 100 tasks. HMTL is also provided with an upper bound on the number of MDPs, though HMTL is also capable of learning this directly, and HMTL is used only with a two-level hierarchy (e.g. a class consists of a single MDP). For FORCEDEXP, we compare two variants. The approach labeled “FE” in the figures uses a polynomially decaying exploration rate, tα with α = 0.5, for all experiments. Performance does vary with the choice of α but α = 0.5 gave good results in our preliminary investigations. Interestingly, this is consistent with the theoretical result that α = 0.5 minimizes dependence on T for polynomially decaying exploration rates (c.f., Corollary 3). We also evaluated the FORCEDEXP algorithm using a constant exploration rate 2√\nT for some earlier\nexperiments: as expected performance was similar but slightly worse generally than using a decaying exploration rate, and so we focus the comparison on the decaying exploration rate variant."
    }, {
      "heading" : "D Proofs for Section 3",
      "text" : ""
    }, {
      "heading" : "D.1 Proof of Lemma 6",
      "text" : "Proof. The proof follows closely to that of [8]. Consider the beginning of an episode, and let K be the set of known state–action pairs which have been visited by the agent at least me times. For each (s, a) ∈ k, the `1 distance between the empirical estimate and the actual next-state distribution is at most [11] ([Lemma 8.5.5]): α = √ 8N me log 2Nδ . Let\nMK be the known-state MDP, which is identical to M̂K except that the transition probabilities are replaced by the true ones for known state–action pairs. Following the same line of reasoning as [8], one may lower-bound the probability that an unknown state is reached within the episode by pe ≥ 1/6 − 3αD. Therefore, pe is bounded by 1/12 as long as αD ≤ 1/36. The latter is guaranteed if me ≥ m0 = O ( ND2 log Nδ ) . The rest of the proof is the same as [8], invoking Lemma 56 of [16] to get an upper bound of H , as stated in the lemma as H0(m)."
    }, {
      "heading" : "D.2 Proof of Lemma 7",
      "text" : "Proof. For task Mt, let Et be the event that all state–action pairs become known after H steps; Lemma 6 with a union bound shows all events {Et}t∈{1,2,...,T} hold with probability at least 1 − δ. For every fixed t, under event Et, every state–action pair has at least m samples to estimate its transition probabilities and average reward after H steps. Applying Lemma 8.5.5 of [11] on the transition distribution, we can upper bound, with probability at least 1− δ2SAT , the `1 error of the transition probability estimates by:\nT =\n√ 8N\nm log\n4SAT δ ≤ Γ 3 .\nSimilarly, an application of Hoeffding’s inequality gives the following upper bound, with probability at least 1− δ2SAT , on the reward estimate:\nR =\n√ 2\nm log\n4SAT δ ≤ Γ 6 √ N .\nApplying a union bound over all states, actions, and tasks, the above concentration results hold with probability at least 1− δ for an agent running on T tasks. The rest of the proof is to show that task identification succeeds when the above concentration inequalities hold.\nTo do this, consider the following two mutually exclusive cases:\n1. If Mt is new, then, by assumption, for every M ′ ∈ M̂, there exists some (s, a) for which the two models differ by at least Γ in `2 distance; that is, ‖θMt(·|s, a)− θM ′(·|s, a)‖2 ≥ Γ. It follows from the equality,\n‖θMt(·|s, a)− θM ′(·|s, a)‖22 = ∑ 1≤s′≤S (θMt(s ′|s, a)− θM ′(s′|s, a)) 2 [error in transition probability estimates]\n+ (θMt(S + 1|s, a)− θM ′(S + 1|s, a)) 2 , [error in reward estimate]\nthat at least one of two terms on the right-hand side above is at least Γ2/2. If the first term is larger than Γ2/2, then the `1 distance between the two next-state transition distributions is at least Γ/ √ 2, which is larger than 2 T = 2Γ/3. It implies that the `1-balls of transition probability estimates for (s, a) betweenMt andM ′ do not overlap, and we will identifyMt as a new MDP. Similarly, if the second term is larger than Γ2/2, then using R we can still identify Mt as a new MDP. 2. If Mt is not new, we claim that the algorithm will correctly identify it as some previously solved MDP, say M ′′ ∈ M̂. In particular, confidence intervals of its estimated model in every state–action pair must overlap with M ′′, since both models’ confidence intervals contain the true model parameters. On the other hand, for any M ′ ∈ M̂ \\ {M ′′}, its model estimate’s confidence intervals do not have overlap with that of Mt’s in at least one state–action pair, as shown in case 1. Therefore, the algorithm can find the unique and correct M ′′ ∈ M̂ that is the same as Mt.\nFinally, the lemma is proved with a union bound over all tasks, states and actions, and with the probability that Et fails to hold for some t."
    }, {
      "heading" : "D.3 Proof of Theorem 5",
      "text" : "Proof. We consider each possible case when solving the tth task, Mt. As shown in previous lemma, with probability 1 − δ, the following event Et hold for all t ∈ [T ]: after PAC-EXPLORE is run on Mt, Algorithm 2 will discover the identity of Mt correctly. That is, if Mt is a new MDP, it will be added to M̂; otherwise, M̂ remains unchanged. In the following, we assume Et holds for every t, and consider the following cases:\n(a) Exploitation in discovered tasks: we choose to exploit (line 12 in Alg 2) and Mt has been already discovered. In this case, Finite-Model-RL is used to do model elimination (within M̂) and to transfer samples from previous tasks that correspond to the same MDP as the current task Mt. Therefore, with a similar analysis, we can get a per-task sample complexity of at most O(CDm) = Õ(CDΓ2 ) = ρ0.\n(b) Exploitation in undiscovered tasks: we choose to exploit and Mt has not been discovered. Running FiniteModel-RL in this case can end up with an arbitrarily poor policy which follows a non- -optimal policy in every step. Therefore, the sample complexity can be as large as H = ρ3.\n(c) Exploration: we choose to explore using PAC-EXPLORE (lines 6—9 in Alg 2). In this case, with high probability, it takes at most H0(m) steps to make every state known, so that the model parameters can be estimated to within accuracy O(Γ). After that, we can reliably decide whether Mt is a new MDP or not. With sample transfer, the additional steps where -sub-optimal policies are taken in the MDP corresponding to Mt (accumulated across all tasks in the T -sequence) is at most ζs, the single-task sample complexity. The total sample complexity for tasks corresponding to this MDP is therefore at most H0(m)T (Mt) + ζs = ρ2T (Mt) + ζs, where T (Mt) is the number of times this MDP occurs in the T -sequence.\nFinally, when Algorithm 2 is run on a sequence of T tasks, the total sample complexity—the number of steps in all tasks for which the agent does not follow an -optimal policy—is given by one of the three cases above. The expected sample complexity can therefore be upper bounded by evoking Theorem 2, completing the proof with an application of union bound that takes care of error probabilities."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Transferring knowledge across a sequence of related tasks is an important challenge in reinforce-<lb>ment learning. Despite much encouraging empirical evidence that shows benefits of transfer, there<lb>has been very little theoretical analysis. In this paper, we study a class of lifelong reinforcement-<lb>learning problems: the agent solves a sequence of tasks modeled as finite Markov decision processes<lb>(MDPs), each of which is from a finite set of MDPs with the same state/action spaces and different<lb>transition/reward functions. Inspired by the need for cross-task exploration in lifelong learning, we<lb>formulate a novel online discovery problem and give an optimal learning algorithm to solve it. Such<lb>results allow us to develop a new lifelong reinforcement-learning algorithm, whose overall sample<lb>complexity in a sequence of tasks is much smaller than that of single-task learning, with high prob-<lb>ability, even if the sequence of tasks is generated by an adversary. Benefits of the algorithm are<lb>demonstrated in a simulated problem.",
    "creator" : "LaTeX with hyperref package"
  }
}