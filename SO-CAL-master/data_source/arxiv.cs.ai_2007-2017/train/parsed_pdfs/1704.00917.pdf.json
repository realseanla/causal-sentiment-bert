{
  "name" : "1704.00917.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DERIVING PROBABILITY DENSITY FUNCTIONS FROM PROBABILISTIC FUNCTIONAL PROGRAMS",
    "authors" : [ "CLAUDIO RUSSO" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Probabilistic programming promises to arm data scientists with declarative languages for specifying their probabilistic models, while leaving the details of how to translate those models to efficient sampling or inference algorithms to a compiler. Many widely used machine learning techniques that might be employed by such a compiler use the probability density function (PDF) of the model as input. Such techniques include maximum likelihood or maximum a posteriori estimation, L2 estimation, importance sampling, and Markov chain Monte Carlo (MCMC) methods (Scott, 2001; Bishop, 2006).\nHowever, despite their utility, density functions have been largely absent from the literature on probabilistic functional programming (Ramsey and Pfeffer, 2002; Goodman et al., 2008; Kiselyov and Shan, 2009). This is because the relationship between programs and their density functions is not straightforward: for a given program, the PDF may not exist or may be non-trivial to calculate. Such programs are not merely infrequent pathological curiosities but in fact arise in many ordinary scenarios. In this paper, we define, prove correct, and implement an algorithm for automatically computing PDFs for a large class of programs written in a rich probabilistic programming language. An abridged version of this paper was published as (Bhat et al., 2013).\n1998 ACM Subject Classification: F.3.2 [LOGICS AND MEANINGS OF PROGRAMS]: Semantics of Programming Languages, G.3 [PROBABILITY AND STATISTICS], I.2.5 [ARTIFICIAL INTELLIGENCE]: Programming Languages and Software.\nKey words and phrases: probability density function, probabilistic programming, Markov chain Monte Carlo.\nLOGICAL METHODS IN COMPUTER SCIENCE DOI:10.2168/LMCS-???\nc© Sooraj Bhat, Johannes Borgström, Andrew D. Gordon, and Claudio Russo Creative Commons\n1\nar X\niv :1\n70 4.\n00 91\n7v 1\n[ cs\n.P L\n] 4\nA pr\nProbability density functions. In this work, probabilistic programs correspond directly to probability distributions, which are important because they are a powerful formalism for data analysis. However, many techniques we would like to use require the probability density function of a distribution instead of the distribution itself. Unfortunately, not every distribution has a density function.\nDistributions. One interpretation of a probabilistic program is that it is a simulation that can be run to generate a random sample from some set Ω of possible outcomes. The corresponding probability distribution P characterizes the program by assigning probabilities to different subsets of Ω (events). The probability P(A) for a subset A of Ω corresponds to the proportion of runs that generate an outcome in A, in the limit of an infinite number of repeated runs of the simulation.\nConsider for example a simple mixture of Gaussians, here written in Fun (Borgström et al., 2011), a probabilistic functional language embedded within F# (Syme et al., 2007).\nif flip 0.7 then random(Gaussian(0.0, 1.0)) else random(Gaussian(4.0, 1.0)) The program above specifies a distribution on the real line (Ω is R) and corresponds to a generative process that flips a biased coin and then generates a number from one of two Gaussian distributions, both with standard deviation 1.0 but with mean either 0.0 or 4.0 depending on the result of the coin toss. In this example, we will be more likely to produce a value near 0.0 than near 4.0 because of the bias. The probability P(A) for A = [0,1], for instance, is the proportion of runs that generate a number between 0 and 1.\nDensities. A distribution P is a function that takes subsets of Ω as input, but for many purposes it turns out to be more convenient if we can find a function f that takes elements of Ω directly, while still somehow capturing the same information as P.\nWhen Ω is the real line, we are interested in a function f that satisfies P(A) = ∫\nA f (x) dx for all intervals A, and we call f the probability density function (PDF) of the distribution P. In other words, f is a function where the area under its curve on an interval A gives the probability P(A) of generating an outcome falling in that interval. The PDF of our example (pictured below) is given by the function\nf (x) = 0.7 ·pdf Gaussian(0.0, 1.0, x)+0.3 ·pdf Gaussian(4.0, 1.0, x) where pdf Gaussian(mean, sdev, ·) is the PDF of a Gaussian distribution with mean mean and standard deviation sdev (the famous “bell curve” from statistics).\n−2 0 2 4 6\n0. 00\n0. 10\n0. 20\n0. 30\n−1 1 3 5\nThe PDF takes higher values where the generative process described above is more likely to generate an outcome. Now we see that the aforementioned probability P(A) for A = [0,1] is simply the area under this curve between 0 and 1. Note that, while there are some loose similarities, the expression for the PDF is different from the expression comprising the source program. In more complicated programs, the correspondence with the PDF is even less obvious.\nNon-existence. Sometimes a distribution does not have a PDF. For example, if we change the else-clause in our example to return 4.0 directly, instead of drawing from a Gaussian with mean 4.0, we get the following probabilistic program, which does not have a PDF:\nif flip 0.7 then random(Gaussian(0.0, 1.0)) else 4.0 In short, the problem is that there is a non-zero amount of probability mass located on a zero-width interval (the process now returns 4.0 with probability 0.3), but integrals on such intervals yield zero, so we would never find a function that could satisfy the properties of being a PDF.\nIt is not always obvious which program modifications ruin the property of having a PDF, especially for multivariate distributions (thus far we have only given univariate distributions as examples). This can be a problem if one is innocently exploring different variations of the model. Details and examples are given by Bhat et al. (2012), who provide the theory for addressing this problem, which we extend and implement in this work.\nThe task of data analysis. So far we have detailed what PDFs are, but not why we want them. We motivate the desire by explaining one popular use-case of PDFs that arises when applying Bayesian learning to data analysis.\nIn the previous examples, the programs specify fully-known probability distributions. While there is indeed randomness in the probabilistic behavior of the samples they generate, the nature of this uncertainty is entirely known—we know the means and variances of the Gaussians, as well as the bias between the two, thus we can characterize the random behavior.\nIn real-world analysis tasks, we rarely have this luxury. Instead, we are often in the position of trying to figure out what the parameters should be, given the data we see. Thus, applications typically deal with parameterized models (families of distributions indexed by a parameter), and they try to learn something about which distributions in that family best explain the observed data. For example, in the following, moG is a parameterized model, indexed by the parameter (mA,mB):\nlet moG (mA,mB) = if flip 0.7 then random(Gaussian(mA, 1.0)) else random(Gaussian(mB, 1.0))\nIt specifies an infinite number of distributions, one for each choice of mA and mB. Now, as a data analyst, we may be presented a dataset that is a sequence of numeric values, and we may also have some domain-specific reason to believe that it can be well modelled as a biased mixture of Gaussians as specified by moG1. We now face the task of figuring out which choices of mA and mB are likely. Intuitively, if we see a clump of datapoints around 0.0, we might be inclined to believe that the mean of one of the Gaussians is 0.0. Note that this is a one-dimensional, probabilistic version of the problem of clustering. The means are the cluster centroids.\nBayesian learning. Bayesian modelling formalizes this task by requiring the modeller to provide as input a prior distribution over the parameters and a generating distribution over the data. These are used to construct a posterior distribution over the parameters as the output.\n1Whether this is actually an appropriate modelling choice depends on whether it captures enough of the essence of the true, unknown data-generating process (i.e. Nature), and is an entirely separate discussion.\nThe prior distribution is a distribution over the possible values of the parameters and captures our belief about what values they are likely to take, before having seen any data (to a Bayesian, “belief” is synonymous with “probability distribution”). The following is one possible prior:\nlet prior () = let mA = random(Uniform(−10.0, 10.0)) in let mB = random(Uniform(−10.0, 10.0)) in (mA, mB)\nThis specifies that we are certain that the means lie between -10.0 and 10.0, but are otherwise uncertain, and our uncertainty about each mean is uniformly distributed between -10.0 and 10.0. This is of course a very particular assertion and is hopefully informed by domain knowledge. In practice, in the absense of domain knowledge, we can select a prior that reflects our open-mindedness about which values the means can take, such as a pair of Gaussians with a high standard deviation. The prior produces a distribution over pairs of means as output, rather than taking a pair of means as an input, as moG does.\nThe generating distribution is a model of how we believe Nature is generating a dataset given a specific choice of the parameter. In our example this is given by moG together with\nlet gen n (mA,mB) = [| for i in 1 .. n→moG (mA,mB) |] This specifies a (parameterized) model for a dataset that is generated as an array of n independent and identically distributed (i.i.d.) values generated by moG.\nThe posterior distribution is a distribution over the possible values of the parameters and captures our belief about what values they are likely to take, after seeing the data. The posterior is related to the prior and generating distributions by Bayes’ rule, which gives us a way to describe how the prior is updated with the observed data to yield the posterior. Intuitively, this update represents the fact that our understanding about the world (our belief about the parameters) evolves based on the data we see. Bishop provides an excellent account of Bayesian learning (Bishop, 2006).\nUse-case of PDFs: Bayesian inference with MCMC. Unfortunately, while prior distributions and generating distributions are often straightforward to work with (we have control over them as the modeller), the posterior distributions often end up intractable or unwieldy to work with (Bayes’ rule dictates their form).\nMarkov chain Monte Carlo (MCMC) methods are one class of techniques that let us actually do something productive with the posterior distribution—MCMC can be used to generate samples from the posterior distribution. The idea of MCMC is to construct a Markov chain in the parameter space of the model, whose equilibrium distribution is the posterior distribution over model parameters. Neal (1993) gives an excellent review of MCMC methods. Here we use Filzbach (Purves and Lyutsarev, 2012), an adaptive MCMC sampler based on the Metropolis-Hastings algorithm. All that is required for such algorithms is the ability to calculate the posterior density given a set of parameters, up to proportion. The posterior does not need to be from a mathematically convenient family of distributions. Samples from the posterior can then serve as its representation, or be used to calculate marginal distributions of parameters or other integrals under the posterior distribution.\nThe posterior density is a function of the PDFs of the various pieces of the model, so to perform inference using MCMC, we also need functions to compute the PDFs. Below, pdf moG gives the PDF of a single data point, while pdf gen gives the PDF of an array of independent data points drawn from the same distribution (iid).\nlet pdf prior (mA,mB) = pdf Uniform(−10.0, 10.0, mA) ∗ pdf Uniform(−10.0, 10.0, mB) let pdf moG (mA,mB) x = 0.7 ∗ pdf Gaussian(mA, 1.0, x) + 0.3 ∗ pdf Gaussian(mB, 1.0, x) let pdf gen (mA,mB) xs = product [| for x in xs→pdf moG (mA,mB) x |] (The product function multiplies together the elements of an array, returning 1.0 on the empty array.) Filzbach and other MCMC libraries require users to write these three functions2, in addition to the generative probabilistic functions prior and gen (which are used for model validation).\nThe goal of this paper is to instead compile these density functions from the generative code. This relieves domain experts from having to write the density code in the first place, as well as from the error-prone task of manually keeping their model code and their density code in synch. Instead, both the PDF and synthetic data are derived from the same declarative specification of the model.\nContributions of this paper. This work defines and applies automated techniques for computing densities to real inference problems from various scientific applications. The primary technical contribution is a density compiler that is correct, useful, and relatively simple and efficient. Specifically:\n• We provide the first implementation of a density compiler based on the specification by Bhat et al. (2012). We compile programs in the probabilistic language Fun (described in Section 2.1) to their corresponding density functions (Section 3). • We prove that the compilation algorithm is sound (Theorem 1). This is the first such proof\nfor any variant of this compiler. • We show that the compiler greatly reduces the development effort of domain experts by\nfreeing them from writing tricky density code and that the produced code is comparable in performance to density functions hand-coded by experts. Our evaluation is based on textbook examples and on models from ecology (Section 4)."
    }, {
      "heading" : "2. LANGUAGES",
      "text" : "In order to describe the density compiler, we first specify its input (source) and output (target) language. Both languages are variants of a simple first-order functional language where the results of subcomputations can be bound to variables using a let construct.\n2.1. Fun: Probabilistic Expressions (Review). Our source language is a version of the core calculus Fun (Borgström et al., 2011), without observation. To mark certain program points as impossible, we add a fail construct (Kiselyov and Shan, 2009). Fun is a first-order functional language without recursion that extends the language of Ramsey and Pfeffer (2002), and this version has a natural semantics in the sub-probability monad. Our implementation efficiently supports a richer language with records and fixed-size arrays and array comprehensions, which can be given a semantics in this core (records and arrays can be encoded as tuples, and comprehensions of fixed size as their unrolling).\n2The actual implementation works with log-densities, as discussed in Section 4.\n2.1.1. Syntax and Types of Fun: The language Fun has base types int, real and unit, product types (denoting pairs), and sum types (denoting tagged unions). A type is said to be discrete if it does not contain real. We let c range over constant data of base type, n over integers and r over real numbers. We write ty(c) = t to mean that constant c has type t."
    }, {
      "heading" : "Types of Fun:",
      "text" : "t,u ::= int | real | unit | (t1 ∗ t2) | (t1 + t2)\nWe take bool , unit+unit, and let ∗ associate to the right. We assume a collection of total deterministic functions on these types, including arithmetic and logical operators. Each operation op of arity n has a signature of the form val op: t1 ∗ · · · ∗ tn→ tn+1. We also assume standard families of primitive probability distributions, including the following. Distributions: Dist : (t1 ∗ · · · ∗ tn)→ t Bernoulli : (real)→ bool Poisson : (real)→ int Gaussian : (real∗ real)→ real Beta : (real∗ real)→ real Gamma : (real∗ real)→ real\nAbove, the names xi of the arguments to the distributions are present for documentation only. A Bernoulli(bias) distribution corresponds to a coin flip with probability bias to come up true. The Poisson(rate) distribution describes the number of occurrences of independent events that occur at the given average rate. The Gaussian(mean,stdev) distribution is also known as the normal distribution; its PDF has a symmetrical bell shape. The Beta(a,b) distribution is a suitable prior for the parameter of Bernoulli distributions, and intuitively means that a− 1 counts of true and b− 1 events of false have been observed. Similarly the Gamma(shape,scale) distribution is a suitable prior for the parameter of Poisson. The parameters of distributions only make sense within certain ranges (e.g., the bias of the Bernoulli distribution must be in the interval [0, 1]). Outside these ranges, attempting to draw a value from the distribution (e.g., Bernoulli(2.0)) results in a failure (fail below)."
    }, {
      "heading" : "Expressions of Fun:",
      "text" : "V ::= value x variable c scalar constant (V,V ) tuple constructor inlu V left sum constructor inrt V right sum constructor\nM,N ::= expression x | c variable and scalar constant (M,N) | fst M | snd M pairing and projections from a pair inlu M | inrt M sum constructors match M with inl x1→ N1 | inr x2→ N2 matching (scope of xi is Ni) let x = M in N let (scope of x is N) op(M) primitive operation (deterministic) random(Dist(M)) primitive distribution failt failure\nThe let and match statements bind their variables (x,x1,x2); we identify expressions up to alpharenaming of bound variables. Above, inl (resp. inr) generates a value corresponding to the left (resp. right) branch of a sum type. Values of sum type are deconstructed by the match construct, which behaves as either the left (N1) or the right (N2) branch depending on the result of M.\nTo ensure that a program has at most one type in a given typing environment, inl and inr are annotated with a type (see (FUN INL) below). The expression fail is annotated with the type it is used at. These types are included only for the convenience of our technical development, and can usually be inferred given a typable source program: we omit these types where they are not used. () is the unit constant.\nA source language term M is pure, written “M pure”, iff M does not contain any occurrence of random or fail.\nWe write Uniform for Beta(1.0,1.0). In the binders of let and match expressions, we let stand for a variable that does not appear free in the scope of the binder. We make use of standard sugar for let, such as writing M;N for let = M in N. We write if M then N1 else N2 for match M with inl → N1 | inr → N2; this is most commonly used when M is Boolean. We let the tuple (M1,M2, . . . ,Mn) stand for (M1,(M2, . . . ,Mn)). Similarly, we write let x1,x2, . . . ,xn = V in N for let x1 = fst V in let z = snd V in let x2, . . . ,xn = z in N when z ] N.\nWhen X is a term from some language (possibly with binders), we write x1, . . . ,xn ] X if none of the xi appear free in X .\nWe write Γ `M : t to mean that in the type environment Γ = x1 : t1, . . . ,xn : tn (xi distinct) the expression M has type t. Apart from the following, the typing rules are standard. In (FUN INL), (FUN INR) (not shown) and (FUN FAIL), type annotations are used in order to obtain a unique type. In (FUN RANDOM), a random variable drawn from a distribution of type (x1 : t1 ∗ · · · ∗ xn : tn)→ t has type t."
    }, {
      "heading" : "Selected Typing Rules: Γ `M : t",
      "text" : "(FUN INL) Γ `M : t\nΓ ` inlu M : t +u\n(FUN FAIL)\nΓ ` failt : t\n(FUN RANDOM) Dist : (t1 ∗ · · · ∗ tn)→ t Γ `M : (t1 ∗ · · · ∗ tn)\nΓ ` random(Dist(M)) : t\nSubstitutions, ranged over by σ ,ρ , are finite maps [x1 7→ M1, . . . ,xn 7→ Mn] from variables to pure expressions. We write Mσ for the result of substituting all free occurrences of variables x ∈ dom(σ) in M with σ(x), avoiding capture of bound variables. To compose two substitutions with disjoint domains, we write [x1 7→ M1, . . . ,xn 7→ Mn]σ for [x1 7→ M1σ , . . . ,xn 7→ Mnσ ]∪σ . A substitution is called closed if the expressions in its range do not contain any free variables. A value substitution is a substitution where each expression in its range is a value. Below, we define what it means for a closed value substitution to be a valuation for a type environment."
    }, {
      "heading" : "Typing Rules for Closed Value Substitutions: Γ ` σ",
      "text" : "(SUBST EMPTY)\nε ` []\n(SUBST VAR) Γ ` σ ε `V : t Γ,x : t ` σ [x 7→V ]\nThere is a default value at each type t, written 0t , that is returned from operations op where they otherwise would be undefined, e.g. r/0.0 = 0real = log(−1). Default Value: 0t 0unit := () 0int := 0 0real := 0.0 0t∗u := (0t ,0u) 0t+u := inl 0t\n2.1.2. Semantics of Fun. As usual, for precision concerning probabilities over uncountable sets, we turn to measure theory. The interpretation of a type t is the set Vt of closed values of type t (real numbers, integers etc.). Below we consider only Lebesgue-measurable sets of values, defined using the standard (Euclidian) metric, and ranged over by A,B. Indeed, the power of the axiom of choice is needed to construct a non-measurable set (Solovay, 1970).\nA measure µ over t is a function, from (measurable) subsets of Vt to the non-negative real numbers extended with ∞, that is σ -additive, that is, µ(∅)= 0.0 and µ(∪iAi)=Σiµ(Ai) if A1,A2, . . . are pair-wise disjoint. We write |µ| for µ(Vt); the measure µ is called a probability measure if |µ|= 1.0, and a sub-probability measure if |µ| ≤ 1.0.\nWe associate a default or stock measure to each type, inductively defined as the counting measure on Z and {()}, the Lebesgue measure on R, and the Lebesgue-completion of the product and disjoint sum, respectively, of the two measures for t ∗u and t+u. In particular, the counting measure on a discrete type assigns measure k to all sets of finite size k, and measure ∞ to all infinite sets.\nIf f is a non-negative (measurable) function t → real, we let ∫\nt f be the Lebesgue integral of f with respect to the stock measure on t if the integral is defined, and otherwise 0. This integral coincides with Σx∈Vt f (x) for discrete types t, and with the standard Riemann integral (if it is defined) on t = real. We write ∫ t f (x) dx for ∫ t λx. f (x), and ∫ t f (x) dµ(x) for Lebesgue integration with respect to the measure µ on t. Below, we often elide the index t; indeed, we may consider any function t→ real as a function from the measurable space ]uVu that is zero except on Vt .\nThe Iverson brackets [p] are 1.0 if predicate p is true, and 0.0 otherwise. We write ∫\nA f for∫ λx.[x ∈ A] · f (x) when A⊂Vt . The function g is a density of µ (with respect to the stock measure)\nif ∫ A 1dµ(x) = ∫\nA g for all A. If µ is a (sub-)probability measure, then we say that g as above is its"
    }, {
      "heading" : "PDF.",
      "text" : "To turn expressions into density functions, we first need a way of interpreting a closed Fun expression M as a sub-probability measure PM over its return type. Open fail-free Fun expressions have a straightforward semantics (Ramsey and Pfeffer, 2002) as computations in the probability monad (Giry, 1982). In order to treat the fail primitive, we use an existing extension (Gordon et al., 2013) of the semantics of Ramsey and Pfeffer (2002) to a richer monad: the subprobability monad (Panangaden, 1999)3. Compared to the operations of the probability monad, the sub-probability monad additionally admits a zero constant, yielding the zero measure. To accommodate the zero measure, the carrier set is extended from probability measures to sub-probability measures, i.e., admitting all µ with |µ| ≤ 1.\nBelow we recapitulate the semantics of Fun by Gordon et al. (2013). Here σ is a closed value substitution whose domain contains all the free variables of M, and detOp(M) ranges over op(M), fst M, snd M, inl M and inr M. We also let either f g (inl V ), f V and either f g (inr V ), g V .\n3Sub-probabilities are also used in our compilation of match (and if) statements, where the probability that we have entered a particular branch may be less than 1.\nMonadic Semantics of Fun with fail: P[[M]] σ (µ >>= f ) A , ∫\nf (x)(A)dµ(x) Sub-probability monad’s bind (returnV ) A , 1 if V ∈ A, else 0 Sub-probability monad’s return zero A , 0 Sub-probability monad’s zero\nBelow we assume that z ] N,N1,N2,σ and x,x1,x2 ] z,σ .\nP[[x]] σ , return σ(x) P[[c]] σ , return c P[[detOp(M)]] σ , P[[M]] σ >>= λx.return detOp(x) P[[(M,N)]] σ , P[[M]] σ >>= λ z.P[[N]] σ >>= λw.return (z,w) P[[let x = M in N]] σ , P[[M]] σ >>= λ z.P[[N]] (σ [x 7→ z]) P[[match M with inl x1→ N1 | inr x2→ N2]] σ , P[[M]] σ >>= either (λ z.P[[N1]] (σ [x1 7→ z])) (λ z.P[[N2]] (σ [x2 7→ z])) P[[random(Dist(M))]] σ , P[[M]] σ >>= λ z.µDist(z) P[[fail]] σ , zero\nWe let the semantics of a closed expression M be PM , P[[M]] ε , where ε denotes the empty substitution. Lemma 1. If Γ `M : t and Γ ` σ then P[[M]] σ is a sub-probability measure on type t.\nProof. By induction on M.\n2.2. Target Language for Density Computations. For the target language of the density compiler, denoted ∫ un, we use a pure version of Fun augmented with real-valued first-order functions and stock integration.\nExpressions of the Target Language: E,F\nE,F ::= target expression x | c variable and scalar constant (E,F) | fst E | snd E pairing and projections from a pair inlu E | inrt E sum constructors match E with inl x1→ F1 | inr x2→ F2 matching (scope of xi is Fi) let x = E in F let (scope of x is F) op(E) primitive operation∫\nt λ (x1, . . . ,xn). E stock integration\nAbove, the binders in let and match are as in Fun. Additionally, in ∫\nt λ (x1, . . . ,xn). E the variables x1, . . . ,xn bind into E.\nIf a Fun term M is pure then M is also an expression in the syntax of ∫ un, and we silently\ntreat it as such. In particular, a Fun substitution σ is also a valid ∫ un substitution, and substitution\napplication Eσ for ∫ un is defined in the same way as for Fun.\nThe typing rule involving integration is as follows. The other typing rules are as in Fun."
    }, {
      "heading" : "Typing Rule for Integration: Γ ` E : t",
      "text" : "(TARGET INT)\nΓ,x1 : t1, . . . ,xn : tn ` E : real Γ ` ∫ t1∗···∗tn λ (x1, . . . ,xn). E : real\nLemma 2 (Standard results for the type system of ∫ un).\n(1) Substitution lemma: if Γ,x : t,Γ′ ` E : u and Γ ` F : t, then Γ,Γ′ ` E[x 7→ F ] : u. (2) Strengthening: if Γ,x : t,Γ′ ` E : u and x ] E, then Γ,Γ′ ` E : u. (3) Weakening: if Γ,Γ′ ` E : u and x ] Γ,Γ′, then Γ,x : t,Γ′ ` E : u.\n2.2.1. Semantics of ∫ un. The target language ∫ un is equipped with a denotational semantics, written I [[F ]] σ where σ is a substitution of closed values for variables with dom(σ)⊇ fv(F). We define this semantics by re-interpreting the monadic semantics of Subsection 2.1.2 with respect to the identity monad: in this monad, return is the identity function, and bind ordinary function application. We rely on an auxiliary semantics I [[λ (z1, . . . ,zn). E]] σ that returns a function to be integrated.\nIdentity Monad and Denotational Semantics of ∫ un: I [[λ (z1, . . . ,zn). E]] σ and I [[E]] σ\nV >>= f , f (V ) Identity monad’s bind (returnV ),V Identity monad’s return I [[ ∫ t λ (z1, . . . ,zn). F ]] σ , { ∫ t I [[λ (z1, . . . ,zn). F ]] σ if the integral is well-defined 0 otherwise (the other cases of I [[E]] σ are the same as the monadic semantics in Subsection 2.1.2)\nI [[λ (z1, . . . ,zn). F ]] σ , f where f (V1, . . . ,Vn), I [[F ]] σ [z1 :=V1, . . . ,zn :=Vn] and z1, . . . ,zn ] σ\nWe write E ≡ F if there are Γ, t such that Γ ` E : t and Γ ` F : t and for all σ such that Γ ` σ we have I [[E]] σ = I [[F ]] σ . Lemma 3.\n(1) If Γ ` E : t and Γ ` σ then I [[E]] σ is a value of type t. (2) If Γ,x1 : t1, . . . ,xn : tn ` E : real and Γ ` σ and dom(σ)⊇ fv(E) then I [[λ (z1, . . . ,zn). E]] σ\nis a function of type t1 ∗ · · · ∗ tn→ real.\nProof. (1) and (2) are proved jointly, by induction on E."
    }, {
      "heading" : "3. THE DENSITY COMPILER",
      "text" : "We compute the PDF of a Fun program by compilation into ∫ un. Our compilation is based on that of Bhat et al. (2012), with modifications to treat fail statements, match (and general if) statements, pure (i.e., deterministic) let bindings, and integer arithmetic.\nThe compiler translates a well-typed Fun source term M into a function λ z. F computing the density (PDF) of M. Given an implementation of stock integration, the ∫ un expression F [z 7→ V ] may be executed to evaluate the density of M at any value V of the type of M. Like traditional compilers, our compiler is compositional and deterministic, producing a unique translation if any\nat all (Lemma 6). Unlike traditional compilers, our compiler is partial and will fail to produce a translation for some well-typed source terms. In particular, if M does not have a density function then the compiler will fail to produce an F . However, it may also fail if M has a PDF, but the compiler is just not complete enough to compute it. In particular, let-bound expressions must either be pure or have a PDF, even if their result is not used. The correctness statement for the compiler is given by Theorem 1.\nWe will use a version of the moG function from the introduction as a running example (Figure 1), with some expansion in order to make use of more of the translation rules.\nThe structure of this section is as follows. In Section 3.1 we provide an intuitive outline of the compilation. We make preliminary definitions, such as the syntax of probability contexts ϒ, in Section 3.2. We define the compiler itself in Section 3.3 in terms of a couple of judgments. These judgments are inductively defined relations, but they in fact are partial functions and hence have a direct executable interpretation. Finally, in Section 3.4 we state and prove correctness of the compiler.\n3.1. Outline. The simplest case in the density compilation is fail, which compiles to the function that always returns zero. The compilation works on the let-structure of the term: a sequence of random lets, as in let x1 = random(Dist1(V1)) in . . . in (x1, . . . ,xn) is compiled to the product of the PDFs of the distributions Dist1, . . . ,Distn, following the chain rule of probability.\nIf the sequence of lets instead has a discrete deterministic return expression M, then M has a probability for each possible value V . This probability is computed by integrating the joint PDF of x1, . . . ,xn over the set of values where M evaluates to V . A continuous deterministic return expression M is treated as a mathematical function fM, using the change of variables rule of integration. In the one-dimensional case, if fM(x) has inverse f−1, the PDF of fM at r is given by the PDF of x at f−1(r), multiplied with the derivative of f−1(r). Another simple case is projection M = xn, where we simply integrate the joint PDF over the set of all possible values for the other variables x1, . . . ,xn−1.\nIf a distribution Dist returns a sum type (e.g., Bernoulli) we can write Distl for the subdistribution yielding only the left part of the sum, and Distr for the right part. By additivity of probability, we can compile the match expression match random(Dist(V )) with inl y→ M | inr z→ N to the sum of the PDFs of let y = random(Distl(V )) in M and let z = random(Distr(V )) in N.\nIn a nested let, such as let xi = (let y1 = random(Dist(V )) in . . . in Mi) in . . ., the expression bound to xi denotes some subprobability distribution. We compute its PDF by recursively compiling the inner let sequence, holding x1, . . . ,xi−1 fixed. Pure lets, as in let x = M in N where M is pure, have the same PDF as N[x 7→M]. The compilation algorithm applies the substitution lazily to avoid introducing unnecessary copies of M.\n3.2. Probability contexts. The density compilation is based on the let-structure of the expression. Variables that are bound in outer lets are referred to as parameters, and are treated as constants. A probability context gathers the variables that are bound in the current sequence of lets, together with the pure expressions defining the deterministic variables."
    }, {
      "heading" : "Probability Context: ϒ",
      "text" : "ϒ ::= probability context ε empty context ϒ,x random variable ϒ,x = E deterministic variable\nExample 1. The probability context at line 7 of Figure 1 is ϒ7 := branch, temp, result= temp + mB, containing two random variables and one deterministic variable.\nFor a probability context to be well-formed, it has to be well-scoped and well-typed."
    }, {
      "heading" : "Well-formed probability context: Γ ` ϒ wf",
      "text" : "(ENV EMPTY)\nΓ ` ε wf\n(ENV VAR) Γ ` ϒ wf Γ ` x : T x ] ϒ\nΓ ` ϒ,x wf\n(ENV CONST) Γ ` ϒ wf Γ ` x : T\nx ] ϒ Γ ` E : T Γ ` ϒ,x = E wf\nExample 2. The probability context ϒ7 is well-formed: Γ7 ` ϒ7 wf, where the type context Γ7 := mA:real,mB:real,branch:bool, temp:real, result:real also contains the types of the parameters mA and mB.\nGiven a well-formed context ϒ, we can extract the random variables rands(ϒ), and an idempotent substitution σϒ (i.e., Eσϒ = (Eσϒ)σϒ always) that gives values to the deterministic variables.\nRandom variables rands(ϒ) and values of deterministic variables σϒ rands(ε) , ε σε , []\nrands(ϒ,x) , rands(ϒ),x σϒ,x , σϒ rands(ϒ,x = E) , rands(ϒ) σϒ,x=E , [x 7→ E]σϒ\nExample 3. We have rands(ϒ7) = branch, temp and σϒ7 = [result 7→ temp+mB].\nLemma 4. If Γ ` ϒ wf then dom(σϒ) ] range(σϒ)\nProof. By simultaneous induction on the derivations of Γ ` ϒ wf and σϒ.\n3.3. Compilation rules. A probability context ϒ is used together with a density expression (E below), which is an open term that expresses the joint density of the random variables in the context and the constraints that have been collected when choosing branches in match statements. Intuitively, the density expression E is the body of the PDF of the current branch. The main judgment of the compiler is ϒ;E ` dens(M)⇒ λ z. F , which associates a Fun term M with its density function z 7→ F . Parameters may occur free in F , and z binds into F . The auxiliary judgment ϒ;E `marg({x1, . . . ,xk})⇒ F yields a density expression F for the variables in its argument, marginalizing (i.e., integrating) out all other random variables in ϒ from E."
    }, {
      "heading" : "Inductively Defined Judgments of the Compiler:",
      "text" : "ϒ;E ` dens(M)⇒ λ z. F in ϒ;E the function λ z. F gives the PDF of M ϒ;E `marg(X)⇒ F in ϒ;E expression F gives the density of the variables in X\nWe begin the description of the compiler proper with the following judgment of marginal density, which computes an expression for the joint marginal PDF of the random variables in its argument. The variables in the argument are free in the computed expression. Below, we write x1, . . . ,xn \\Y for the tuple of variables arising from x1, . . . ,xn by deleting all instances of variables in Y , and dually for x1, . . . ,xn∩Y ."
    }, {
      "heading" : "Marginal Density: ϒ;E `marg(X)⇒ F",
      "text" : "(MARGINAL) X ⊆ rands(ϒ) rands(ϒ)\\X = y1, ...,yn\nϒ;E `marg(X)⇒ ∫ λ (y1, ...,yn). Eσϒ\nHere we first substitute in the deterministic let-bound variables, as given by σϒ, and then integrate out the remaining random variables y1, ...,yk. In the definition of the compiler, marg(X) is also used with X =∅, to compute the probability of being in the current branch of the program.\nExample 4. Here ϒ7;E `marg({temp})⇒Ftemp where Ftemp := ∫\nλ (branch).E[result 7→ temp+mB], which will be used when computing the PDF of the variable result on line 7 (cf. Examples 5, 10).\nThe main judgment of the compiler is the dens judgment ϒ;E ` dens(M)⇒ λ z. F , which gives the density z 7→ F of M in the current context ϒ, where E is the accumulated body of the density function so far. In this judgment, z is binding into F . We introduce fresh variables during the compilation: in the rules below we assume that z,w ] ϒ,E,M.\nDensity Compiler, base cases: ϒ;E ` dens(M)⇒ λ z. F (VAR DET) (x = E ′) ∈ ϒ ϒ;E ` dens(E ′)⇒ λ z. F\nϒ;E ` dens(x)⇒ λ z. F\n(VAR RND) x ∈ rands(ϒ) ϒ;E `marg({x})⇒ F\nϒ;E ` dens(x)⇒ λx. F (CONSTANT) ε `V : t t discrete ϒ;E `marg(∅)⇒ F\nϒ;E ` dens(V )⇒ λ z. F · [z =V ]\n(FAIL)\nϒ;E ` dens(fail)⇒ λ z. 0.0\nFor a deterministic variable, (VAR DET) recurses into its definition. The rule (VAR RND) computes the marginal density of a random variable using the marg judgment. The (CONSTANT) rule states that the probability density of a discrete constant V (built from sums and products of integers and units) is the probability of being in the current branch at V , and 0 elsewhere. Note the absence of a rule for real number constants, since they do not possess a density. The (FAIL) rule gives that the density of fail is zero.\nExample 5. By (VAR RND) we get ϒ7;E ` dens(temp)⇒ λ temp. Ftemp as computed in Example 4. To compute the PDF at line 7, (VAR DET) yields that ϒ7;E ` dens(result)⇒ λ z. F7 where ϒ7;E ` dens(temp+mB)⇒ λ z. F7 is computed in Example 10.\nDensity Compiler, random variables : ϒ;E ` dens(M)⇒ λ z. F (RANDOM CONST)\nM pure rands(ϒ) ] (Mσϒ) ϒ;E `marg(∅)⇒ F ϒ;E ` dens(random(Dist(M)))⇒ λ z. F · PDFDist(Mσϒ)(z)\n(RANDOM RND) ¬(M pure∧ rands(ϒ) ] (Mσϒ)) ϒ;E ` dens(M)⇒ λw. F ϒ;E ` dens(random(Dist(M)))⇒ λ z. ∫ λw.F · PDFDist(w)(z)\nIn (RANDOM CONST), a random variable drawn from a primitive distribution with a constant argument has the expected PDF (multiplied with the probability that we are in the current branch). Its precondition that M pure and rands(ϒ) ] (Mσϒ) intuitively means that M is constant under ϒ. (RANDOM RND) treats calls to random with a random argument by marginalizing over the argument to the distribution. We here require that each primitive distribution Dist has a PDF for each value w of its arguments, denoted PDFDist(w).\nExample 6. Using rule (RANDOM CONST), we can compute the density at line 4 as ϒ;E ` dens(random(Gaussian(mA,1.0)))⇒ λ z. Fthen · PDFGaussian(mA,1.0)(z) where Fthen intuitively yields the probability of being in the current branch, and is computed using (MARGINAL) as ∫ λ (temp,branch).E.\nDensity Compiler, rules for tuples: ϒ;E ` dens(M)⇒ λ z. F (TUPLE VAR) ϒ;E `marg({x1, ...,xk})⇒ F k ≥ 2 x1, ...,xk distinct\nϒ;E ` dens((x1, ...,xk))⇒ λ z. let x1, ...,xk = z in F (TUPLE PROJ L) ϒ;E ` dens(M)⇒ λw. F ϒ;E ` dens(fst M)⇒ λ z1. ∫ λ z2. let w = (z1,z2) in F\n(TUPLE PROJ R) ϒ;E ` dens(M)⇒ λw. F\nϒ;E ` dens(snd M)⇒ λ z2. ∫ λ z1. let w = (z1,z2) in F\nThe rule (TUPLE VAR) computes the joint marginal density of a tuple of variables4. The rules (TUPLE PROJ L) and (TUPLE PROJ R) integrate out the right or the left component of a pair, respectively.\nDensity Compiler, let: ϒ;E ` dens(let x = M in N)⇒ F\n(LET DET) M pure ϒ,x = M;E ` dens(N)⇒ λ z. F\nϒ;E ` dens(let x = M in N)⇒ λ z. F\n(LET RND) ¬(M pure) ε;1 ` dens(M)⇒ λx. F1\nϒ,x;E ·F1 ` dens(N)⇒ λ z. F2 ϒ;E ` dens(let x = M in N)⇒ λ z. F2\n4Joint marginal densities for tuples of expressions can be computed if those expressions are conditionally independent (Bhat et al., 2012). As an example, (x,y+3) has a PDF whenever (x,y) does. However, the rules in this paper do not support such expressions, to avoid additional complexity.\nThe rule (LET DET) simply adds a pure let-binding to the context. In (LET RND), we compute the density of the let-bound variable in an empty context, and multiply it into the current accumulated density when computing the density of the body.\nExample 7. The density expression for the program fragment in Figure 1 is computed using (LET RND) as F2 where branch;1 · Fbranch ` dens(N) ⇒ λ z. F2, the expression N is lines 2-7, and Fbranch = ( ∫ λ ().1) · PDFBernoulli(0.7)(z) is computed by ε;1 ` dens(random(Bernoulli(0.7)))⇒\nλ z. Fbranch using previously seen rules. Here Fbranch ≡ PDFBernoulli(0.7), since I [[ ∫\nλ ().1]] σ = 1.\nThe let expression on line 2 of Figure 1 is also handled by (LET RND), while the one on line 6 is handled by (LET DET) since temp+mB pure.\nFor deterministic matches we use four deterministic operations, which we assume do not occur in source programs. We let isL : t +u→ real be the indicator function for the left branch defined as isL(V ) := match V with inl x : 1.0 | inr x : 0.0, and dually for isR. To destruct a value of sum type we use fromL : t +u→ t defined as fromL(V ) := match V with inl x : x | inr x : 0t , and its dual fromR.\nDensity Compiler, rules for sums and match: ϒ;E ` dens(match M with . . .)⇒ λ z. F (SUM CON L)\nϒ;E ` dens(M)⇒ λ z. F ϒ;E ` dens(inl M)⇒ λw. match w with inl z→ F | inr → 0 (SUM CON R) ϒ;E ` dens(M)⇒ λ z. F ϒ;E ` dens(inr M)⇒ λw. match w with inl → 0 | inr z→ F (MATCH DET)\nM pure ϒ,x1 = fromL(M);E · isL(M) ` dens(N1)⇒ λ z. F1 ϒ,x2 = fromR(M);E · isR(M) ` dens(N2)⇒ λ z. F2\nϒ;E ` dens(match M with inl x1→ N1 | inr x2→ N2)⇒ λ z. F1 +F2 (MATCH RND)\n¬(M pure) ϒ,x1;E · let w = inl x1 in F ` dens(N1)⇒ λ z. F1 ε;1 ` dens(M)⇒ λw. F ϒ,x2;E · let w = inr x2 in F ` dens(N2)⇒ λ z. F2\nϒ;E ` dens(match M with inl x1→ N1 | inr x2→ N2)⇒ λ z. F1 +F2 (FROML) (x = fromL(M)) ∈ ϒ ϒ;E ` dens(M)⇒ λw. F ϒ;E ` dens(fromL(M))⇒ λ z. let w = inl z in F\n(FROMR) (x = fromR(M)) ∈ ϒ ϒ;E ` dens(M)⇒ λw. F ϒ;E ` dens(fromR(M))⇒ λ z. let w = inr z in F\n(SUM CON L) states that the density of inl M is the density of M in the left branch of a sum, and 0 in the right. Its dual is (FROML). (MATCH DET) is based on (LET DET), and we additionally multiply the constraint that we are in the correct branch (isL(M) or isR(M)) with the joint density expression. We employ the functions fromL and fromR and their associated rules (FROML)\nand (FROMR) to avoid additional calls to (MATCH DET) arising from (VAR DET) if the compilation of the density of Ni requires computing the density of the match-bound variable yi, as in match fst z with inl y1→ y1 | inr y2→ y2. Since we assume that fromL and fromR do not appear in source programs, these rules are only ever used in the case described above. The (MATCH RND) rule is based on (LET RND), and we again multiply in the constraint that we are in the left (or right) branch of the match.\nExample 8. The match selector on line 3 is a pure expression, so rule (MATCH DET) applies. For the left branch, we let E4 ≡ PDFBernoulli(0.7)(branch) · PDFGaussian(0,1.0)(temp) · isL(branch) and ϒ4 = branch, temp, = fromL(branch) and compute\nϒ4;E4 ` dens(random(Gaussian(mA, 1.0)))⇒ λ z. F4 where F4 is computed using (RANDOM CONST) and (MARGINAL) as(∫\nλ (branch, temp).E4 ) ·PDFGaussian(mA,1.0)(z)\nHere I [[ ∫\nλ (branch, temp).E4]] σ = 0.7, so the contribution of the left branch to the PDF of the match is the PDF of the branch scaled by the probability of entering the left branch. In general, this holds when the branch expression is independent from the body of the branch.\nFor the right branch, see Example 10. We then obtain the PDF of the match as the sum of the PDFs of the two branches.\nOur implementation of the compiler uses the following derived rules for if statements where the branching expression is of type bool, and does not treat other sum types nor matches."
    }, {
      "heading" : "Derived rules for if statements",
      "text" : "(IF DET) M pure ϒ;E · isL(M) ` dens(N1)⇒ λ z. F1 ϒ;E · isR(M) ` dens(N2)⇒ λ z. F2 ϒ;E ` dens(if M then N1 else N2)⇒ λ z. F1 +F2 (IF RND)\n¬(M pure) ϒ;E · let w = true in F ` dens(N1)⇒ λ z. F1 ε;1 ` dens(M)⇒ λw. F ϒ;E · let w = false in F ` dens(N2)⇒ λ z. F2\nϒ;E ` dens(if M then N1 else N2)⇒ λ z. F1 +F2\nExample 9. Since the match-bound variables (lines 4 and 5) do not appear in the bodies of the match branches, we can instead use rule (IF DET) to avoid adding them to the probability context when computing the PDF of the body (cf. Example 1).\nDensity compiler, discrete operations : ϒ;E ` dens( f (M))⇒ F (DISCRETE) M pure ϒ;E `marg({x1, . . . ,xn})⇒ F f 6∈ {fromL, fromR}\nf : t→ u u discrete rands(ϒ)∩ fv(Mσϒ) = x1, . . . ,xn ϒ;E ` dens( f (M))⇒ λw. ∫ λ (x1, . . . ,xn). F · [w = f (Mσϒ)]\nThe (DISCRETE) rule for discrete operations, such as logical and comparison operations and integer arithmetic, computes the expectation of an indicator function over the joint distribution of the random variables occurring in the expression.\nFor numeric operations on real numbers we mimic the change of variable rule of integration (often summarized as “dx = dxdy dy”), multiplying the density of the argument with the derivative of the inverse of the operation. For operations of more than one argument (e.g., (PLUS RND) below), we instead use the matrix volume of the Jacobian matrix of the inverse operation (Ben-Israel, 1999). We only require that the operation is invertible on a restricted domain, namely where the PDF of its argument is non-zero. This is exemplified by the following rules.\nDensity compiler, numeric operations on reals : ϒ;E ` dens( f (M))⇒ F (INVERSE)\nϒ;E ` dens(M)⇒ λw. F ϒ;E ` dens(1/M)⇒ λ z. (let w = 1/z in F) · (1/z2) (EXP) ϒ;E ` dens(M)⇒ λw. F ϒ;E ` dens(exp(M))⇒ λ z. if z > 0.0 then(let w = log(z) in F) · (1/z) else 0.0 (LOG) ϒ;E ` dens(M)⇒ λw. F ¬∃σ ,r < 0,c 6= 0.I [[let w = r in F ]] σ = c\nϒ;E ` dens(log(M))⇒ λ z. (let w = exp(z) in F) ·exp(z) (SCALE) c 6= 0 ϒ;E ` dens(M)⇒ λw. F ϒ;E ` dens(c ·M)⇒ λ z. (let w = z/c in F) · (1/abs(c)) (PLUS DET) N pure rands(ϒ) ] (Nσϒ) ϒ;E ` dens(M)⇒ λw. F\nϒ;E ` dens(M+N)⇒ λ z. let w = z−Nσϒ in F (PLUS RND) ¬(N pure∧ rands(ϒ) ] (Nσϒ)) ϒ;E ` dens((M,N))⇒ λw. F\nϒ;E ` dens(M+N)⇒ λ z. ∫ λw1. let w = (w1,z−w1) in F\nFor the (LOG) rule above, we require that negative arguments to log have zero density.\nExample 10. Letting E7 ≡ PDFBernoulli(0.7)(branch) · PDFGaussian(0,1.0)(temp) · isR(branch), the sum on line 6 is evaluated using rule (PLUS DET) as\nϒ7;E7 ` dens(temp+mB)⇒ λ z. F7 where\nF7 = let temp = z−mB in ∫ λbranch. E7 ≡ λ z.0.3 ·PDFGaussian(mB,1.0)(z)\nsince PDFGaussian(0.0,1.0)(z− r) = PDFGaussian(r,1.0)(z) for all r. Thus we obtain that the PDF of the program fragment in Figure 1 is given by\nλ z.F4 +F7 ≡ λ z.0.7 ·PDFGaussian(mA,1.0)(z)+0.3 ·PDFGaussian(mB,1.0)(z).\nFinally, as another example of compilation, the if statement in the program let p = random(Beta(1.0,1.0)) in // the uniform distribution on the unit interval let b = random(Bernoulli(p)) in if b then p+1.0 else p\nis handled by (IF DET), yielding a density function that (modulo trivial integrals) is equivalent to λ z. let p = z−1 in ∫ λb.[0≤ p≤ 1] · (if b then p else 1− p) · isL(b)\n+ ∫ λb.[0≤ z≤ 1] · (if b then z else 1− z) · isR(b)\n≡ λ z. [1≤ z≤ 2] · (z−1)+ [0≤ z≤ 1] · (1− z)\n3.4. Compiler Correctness. These derived judgments relate the types of the various terms occurring in the marg and dens judgments.\nLemma 5 (Derived Judgments). If Γ,Γϒ ` ϒ wf and dom(Γϒ) = rands(ϒ)∪dom(σϒ) and Γ,Γϒ ` E : real then\n(1) If ϒ;E `marg(X)⇒ F and X = {x1, . . . ,xn} and Γϒ ` (x1, . . . ,xn) : (t1 ∗ · · · ∗ tn) then Γ,x1 : t1, . . . ,xn : tn ` F : real. (2) If ϒ;E ` dens(M)⇒ λ z. F and Γ,Γϒ `M : t then Γ,z : t ` F : real."
    }, {
      "heading" : "Proof.",
      "text" : "(1) By inversion of (MARGINAL), F = ∫ λ (y1, ...,yk). Eσϒ where X ⊆ rands(ϒ) and (rands(ϒ)\\\nX) = y1, ...,yk. By Γ,Γϒ ` ϒ wf we have that Γ,Γϒ ` σϒ(x) : Γϒ(x) for all x∈ domσϒ. Without loss of generality, let Γϒ ≡ ΓX ,ΓY ,Γσϒ where domΓX = X , domΓY = {y1, ...,yk} and domΓσϒ = domσϒ. By repeated application of Lemma 2.1 we obtain Γ,ΓX ,ΓY `Eσϒ : real. By (TARGET INT) we then derive Γ,ΓX ` ∫ λ (y1, . . . ,yn). Eσϒ : real. By repeated inversion of Γϒ ` (x1, . . . ,xn) : (t1 ∗ · · · ∗ tn) we can conclude that ΓX = x1 : t1, ...,xn : tn, which gives us the result Γ,x1 : t1, . . . ,xn : tn ` F : real.\n(2) By induction on the derivation of ϒ;E ` dens(M)⇒ λ z. F , using (1). The density compiler is deterministic.\nLemma 6 (Determinism). If Γ ` ϒ wf and Γ ` E : real then\n(1) If ϒ;E `marg(X)⇒ F1 and ϒ;E `marg(X)⇒ F2 then F1 = F2. (2) If ϒ;E ` dens(M)⇒ λ z. F1 and ϒ;E ` dens(M)⇒ λ z. F2 then F1 = F2."
    }, {
      "heading" : "Proof.",
      "text" : "(1) The (MARGINAL) rule is deterministic. (2) By induction on M, using (1). In every case, at most one compilation rule applies.\nWe also need a technical lemma relating pure Fun expressions with their ∫ un counterparts.\nLemma 7. If M is pure and Γ `M : t and Γ ` σ then P[[M]] σ = return I [[M]] σ .\nProof. By induction on M.\nThe soundness theorem asserts that, for all closed expressions M, the density function given by the density compiler indeed characterizes (via stock integration) the distribution of M given by the monadic semantics.\nTheorem 1 (Soundness). If ε;1 ` dens(M)⇒ λ z. F and ε `M : t then (P[[M]] ε) A = ∫\nA I [[λ z. F ]] ε\nProof. We let y := y1, . . . ,yn := rands(ϒ), and otherwise use the meta-variables from the derivation rules.\nWe prove the theorem by joint induction on the derivations of dens(M) and M : t, using the following induction hypothesis (IH):\nif Γ,Γϒ ` ϒ wf and dom(Γϒ) = rands(ϒ)∪dom(σϒ) (3.1) and Γ,Γϒ `M : t and Γ,Γϒ ` E : real and ϒ;E ` dens(M)⇒ λ z. F (3.2)\nand Γ ` ρ and µ(B) = ∫\nB I [[λy. Eσϒ]] ρ and |µ| ≤ 1 (3.3)\nand (∀ρ ′.Γϒ ` ρ ′∧I [[E]] ρρ ′ 6= 0.0⇒ (3.4) ((σϒ(x) = fromL(M)⇒∃V.I [[Mσϒ]] ρρ ′ = inl V ) (3.5) ∧(σϒ(x) = fromR(M)⇒∃V.I [[Mσϒ]] ρρ ′ = inr V ))) (3.6)\nthen (µ >>= λy. P[[Mσϒ]] ρ) A = ∫\nA I [[λ z. F ]] ρ. (3.7)\nWe first note that since the density expression is only ever modified by multiplication with other real-valued expressions, the conjunct at 3.4-3.6 of IH can only be invalidated when a deterministic variable x = fromL(M) or x = fromR(M) is added to ϒ, which only can occur in rule (MATCH DET). In the left branch of the match, I [[E · isL(Mσϒ)]] ρρ ′ 6= 0.0 implies that I [[isL(Mσϒ)]] ρρ ′ 6= 0.0, so I [[Mσϒ]] ρρ ′ = inl V for some V . A symmetric argument applies to the right branch of the match.\nWe proceed with the induction. (VAR DET)\nLHS = (µ >>= λy.return zσϒρ) A (by Lemma 7) = (µ >>= λy.P[[E ′σϒ]] ρ) A\n(by IH) = RHS\n(VAR RND) Assume that x = yi, and let y′ = y\\ x. LHS = (µ >>= λy.return yi) A = ∫ λy.(I [[Eσϒ]] ρ) · [yi ∈ A] = ∫\nA λyi.\n∫ I [[λy′.Eσϒ]] ρ = RHS\n(CONSTANT) LHS=(µ >>= λy.returnV )A= ∫ λy.(I [[Eσϒ]] ρ)·[V ∈A] = ( ∫\nI [[λy.Eσϒ]] ρ)·∑ x∈A [x=V ] =RHS\n(FAIL) LHS = 0.0 = RHS.\n(RANDOM CONST)\nLHS = (µ >>= λy.(P[[Mσϒ]] ρ >>= λx.µDist(x)) A\n(by Lemma 7) = (µ >>= λy.(return (I [[Mσϒ]] ρ)>>= λx.µDist(x))) A\n(by monad laws) = (µ >>= λ .µDist(I [[Mσϒ]] ρ)) A = µDist(I [[Mσϒ]] ρ)(A) · ∫ I [[λy.Eσϒ]] ρ\n= ∫\nA I [[λ z.\n(∫ λy.Eσϒ ) · PDFDist(Mσϒ)(z)]] ρ = RHS\n(RANDOM RND) Let ν A = ∫\nA I [[λw. F ]] ρ . LHS = (µ >>= λy.(P[[Mσϒ]] ρ >>= λ z.µDist(z))) A\n(by monad laws) = ((µ >>= λy.P[[Mσϒ]] ρ)>>= λ z.µDist(z)) A\n(by induction) = (ν >>= λ z.µDist(z)) A = ∫\nλ z.µDist(z)(A) dν = ∫\nλ z.((I [[λw. F ]] ρ) z) ·µDist(z)(A) = ∫\nλw.(I [[F ]] ρ) ·µDist(w)(A) = ∫ λw.(I [[F ]] ρ) · ∫\nA PDFDist(w) = ∫\nA λ z.\n∫ λw.(I [[F ]] ρ) · PDFDist(w)(z)\n= RHS (TUPLE VAR) Let x = x1, . . . ,xk, and y′ = y\\ x. Let ν A = ∫ A I [[λx. F ]] ρ .\nLHS=(µ >>= λy.return (x))A= ∫ λy.(I [[Eσϒ]] ρ ·[(x)∈A])= ∫\nA λ (z).\n∫ I [[λy′.Eσϒ]] ρ =RHS\n(TUPLE PROJ L) Let ν A = ∫\nA I [[λw. F ]] ρ . LHS = (ν >>= λw. return fst w) A\n= ∫ λw.[fst w ∈ A] dν\n= ∫ λw.[fst w ∈ A] ·I [[F ]] ρ\n= ∫ λ (z1,z2).let w = (z1,z2) in [z1 ∈ A] ·I [[F ]] ρ\n= ∫\nA λ z1.\n∫ λ z2.I [[let w = (z1,z2) in F ]] ρ = RHS\n(TUPLE PROJ R) As (TUPLE PROJ L).\n(LET DET)\nLHS = (µ >>= λy.(P[[Mσϒ]] ρ >>= λv.P[[Nσϒ]] ρ,x 7→ v)) A (by Lemma 7) = (µ >>= λy.(return (I [[Mσϒ]] ρ)>>= λv.P[[Nσϒ]] ρ,x 7→ v)) A\n(by monad laws) = (µ >>= λy.P[[Nσϒ]] ρ,x 7→I [[Mσϒ]] ρ) A = (µ >>= λy.P[[N]] σϒ,x 7→Mρ) A = RHS\n(LET RND) Let ν B := ∫ B I [[λy,x.(E ·F1)σϒ]] ρ . By induction (P[[M]]ρ ′) C = ∫\nC I [[F1]] ρ ′ whenever Γ,Γϒ ` ρ ′.\nLHS = (µ >>= λy.(P[[Mσϒ]] ρ >>= λ z.P[[Nσϒ]] ρ,x 7→ z)) A (by monad laws) = ((µ >>= λy.(P[[Mσϒ]] ρ >>= λ z.return y,z))>>= λy,z.P[[N]] σϒρ,x 7→ z) A\nThen\n(µ >>= λy.(P[[Mσϒ]] ρ >>= λx.return y,x)) B = ∫ λy.(P[[Mσϒ]] ρ >>= λx.return y,x) B dµ\n= ∫ λy.(I [[Eσϒ]] ρ) · (P[[Mσϒ]] ρ >>= λx.return y,x) B\n= ∫ λy.(I [[Eσϒ]] ρ) · ∫ λx.[y,x ∈ B]dP[[Mσϒ]] ρ\n(induction) = ∫ λy.(I [[Eσϒ]] ρ) · ∫ λx.I [[F1σϒ]] ρ · [y,x ∈ B]\n= ∫\nB λy,x.I [[Eσϒ ·F1σϒ]] ρ\n= ν By induction, (ν >>= λy,z.P[[Nσϒ]] ρ,x 7→ z) A = RHS.\n(SUM CON L) Let ν A = ∫\nA I [[λ z. F ]] ρ . LHS = (ν >>= λ z. .return inl z) A = ∫ λ z.[inl z ∈ A] dν = ∫\nA either I [[F ]] ρ λ .0 = RHS\n(SUM CON R) As (SUM CON L). (MATCH DET) Let\nL = {(V1, . . . ,Vn) | ∃W.I [[Mσϒ]] ρ[y1, . . . ,yn 7→V1, . . . ,Vn] = inl W and ε `Vi : Γϒ(yi) for all i}.\nThen µ = µL+µR where µL(B) = µ(B∩L) = ∫\nB λ (rands(ϒ)).I [[Eσϒ · isL(Mσϒ)]] ρ and µR(B) = µ(B\\L) = ∫ B λ (rands(ϒ)).I [[Eσϒ · isR(Mσϒ)]] ρ . By additivity, we have\nLHS = (µL+µR >>= P[[M′σϒ]] ρ) A = (µL >>= P[[M′σϒ]] ρ) A+(µR >>= P[[M′σϒ]] ρ) A\nWe let EN := matchI [[Mσϒ]] ρ with inl z : P[[N1]] (σ ,x1 7→ z) | inr z : P[[N2]] (σ ,x2 7→ z). Then\n(µL >>= P[[M′σϒ]] ρ) A = (µL >>= λy.(P[[Mσϒ]] ρ >>= either (λ z.P[[N1]] (σ ,x1 7→ z)) (λ z.P[[N2]] (σ ,x2 7→ z)))) A\n(by Lemma 7) = (µL >>= λy.(return (I [[Mσϒ]] ρ)>>= either . . .)) A (by monad laws) = (µL >>= λy.EN) A\n= ∫\nL λy.EN(A) dµL+ ∫ L λy.EN(A) dµL\n= ∫\nL λy.EN(A) dµL (∀V ∈ L ∃W. I [[Mσϒ]] ρ[y :=V ] = inl W ) = ∫\nL λy.P[[N1σϒ]] (ρ,x1 7→ fromL I [[Mσϒ]] ρ)(A) dµL\n= (µL >>= λy.P[[N1σϒ]] (ρ,x1 7→ fromL I [[Mσϒ]] ρ)) A (by induction) = ∫\nA I [[λ z. F1]] ρ Symmetrically, (µR >>= P[[M′σϒ]] ρ) A = ∫ A I [[λ z. F2]] ρ , so LHS = ∫\nA I [[λ z. F1]] ρ +∫ A I [[λ z. F2]] ρ = ∫ A λ z. I [[F1]] ρ +I [[F2]] ρ = RHS.\n(MATCH RND) Write Ei := P[[Niσϒ]] (ρ,xi 7→ z). Here either λ z.E1 λ z.E2 =β λv.match v with inl z : N1 | inr z : N2. As in case (LET RND) we let ν B := ∫ B I [[λy,x1.Eσϒ ·\nlet w = inl x1 in Fσϒ]] ρ , and get LHS = (µ >>= λy.(P[[Mσϒ]] ρ >>= λv.return y,v)\n>>= λy,v.match v with inl z : N1 | inr z : N2) A = ν >>= λy,v.match v with inl z : N1 | inr z : N2) A (∗)\nWe proceed as in case (MATCH DET) but with L := {(V1, . . . ,Vn, inl W ) | ε `Vi : Γϒ(yi) for all i}, yielding (∗) = ∫ A λ z.I [[F1 +F2]] ρ = RHS.\n(FROML) Let ν B := ∫\nB I [[λw. F ]] ρ . LHS = (µ >>= λy.return fromL(Mσϒρ)) A\n(monad law) = ((µ >>= λy.return Mσϒρ)>>= return◦ fromL) A (induction) = (ν >>= return◦ fromL) A\n(definition) = ∫\nt+u λw. [fromL(w) ∈ A] ·I [[F ]] ρ\nBy part 3.4-3.6 of the IH, I [[F ]] ρρ ′ 6= 0.0 implies I [[Mσϒρρ ′]] = inl V , so we have∫ t+u [fromL(x) ∈ A] ·I [[F ]] ρ = ∫ A I [[F ]] ρ[z := inl x]dx = RHS\n(FROMR) As (FROML).\n(DISCRETE) Let ν B := ∫\nB I [[λx. F ]] ρ and y\\ x = z LHS = (µ >>= λy.return f (Mσϒρ)) A\n= ∫ λy.[ f (I [[Mσϒ]] ρ) ∈ A]dµ\n= ∫ λy.[ f (I [[Mσϒ]] ρ) ∈ A] ·I [[Eσϒ]] ρ\n= ∑ w∈A\n∫ I [[λ z.[w = f (Mσϒ)] · ∫ λx.Eσϒ]] ρ = RHS\n(PLUS RND) Let ν B = ∫\nB I [[λw. F ]] ρ . LHS = ((µ >>= λy.return (M,N)σϒρ)>>= λ (x1,x2). return x1 + x2) A\n(Lemma 7) = ((µ >>= λy.P[[(M,N)σϒ]] ρ)>>= λ (x1,x2). return x1 + x2) A (induction) = (ν >>= λ (x1,x2). return x1 + x2)) A\n= ∫ λx,y.[x+ y ∈ A]dν\n= ∫ λx,y.[x+ y ∈ A] · (let w = (x,y) in I [[F ]] ρ)\n(z := x+ y) = ∫\nA λ z.\n∫ λx.I [[let w = (x,z− x) in F ]] ρ\n= RHS\nNumeric operations on real: Assume that f is strictly monotonic and g := f−1 has a continuous derivative for f (x) ∈ A. Let ν B := ∫ B I [[λw. F ]] ρ .\nLHS = (µ >>= λy.return f (I [[Mσϒ]] ρ)) A (monad law) = ((µ >>= λy.return I [[Mσϒ]] ρ)>>= λw. return f (w)) A (induction) = (ν >>= λw. return f (w)) A\n= ∫ λw. [ f (w) ∈ A] ·I [[F ]] ρ\n(change of variables) = ∫\nA λ z.(I [[let w = g(z) in F ]] ρ) ·g′(w) = RHS\nFor the base case of the induction, we have that E = 1, µ is the probability measure on the unit type, and all of Γ, Γϒ, σϒ and ρ are empty. Clearly, (IH) holds for the base case.\nPart 3.4-3.6 of the induction hypothesis above is used when attempting to evaluate match-bound variables (e.g., x = fromL(M)) for valuations that give the other branch (e.g., I [[M]] σ = inr V ). For such valuations the density is always zero (since, e.g., isL(inr V ) = 0.0)."
    }, {
      "heading" : "4. EVALUATION",
      "text" : "We evaluate the compiler on several synthetic textbook examples and several real examples from scientific applications. We wish to validate that the density compiler handles these examples, and understand how much the compiler reduces the developer burden, and its performance impact.\n4.1. Implementation. Since Fun is a sublanguage of F#, we implement our models as F# programs, and use the quotation mechanism of F# to capture their syntax trees. Running the F# program corresponds to sampling data from the model. To compute the PDF, the compiler takes the syntax tree (of F# type Expr) of the model and produces another Expr corresponding to a deterministic F# program as output. We then use run-time code generation to compile the generated Expr to MSIL bytecode, which is just-in-time compiled to executable machine code when called, just as for statically compiled F# code. Our implementation supports immutable arrays and records, which are both translated using adaptations of the corresponding rules for tuples. For efficiency, the implementation must avoid introducing redundant computations, translating the use of substitution in the formal rules to more efficient let-bindings that share the values of expressions that would otherwise be re-computed. As is common practice, our implementation and Filzbach (Purves and Lyutsarev, 2012) both work with the logarithm of the density, which avoids products of densities in favor of sums of log-densities where possible, to avoid numerical underflow. It also performs some simple but effective peephole optimization to elide canceling applications of Log and Exp and additions of 0."
    }, {
      "heading" : "Code Examples.",
      "text" : "Example 4.1 (Mixture Of Gaussians). To illustrate the implementation, here is the actual F# code expressing a mixture of Gaussians (a variant of our introductory example):\ntype W = {bias: double; mean: double[]; sd: double[]}\n[<Fun>] let prior () = { bias = random(Uniform(0.0, 1.0))\nmean = [| for i in 0..1→random(Uniform(−1000.0, 1000.0)) |] sd = [| for i in 0..1→random(Uniform(100.0, 500.0)) |] }\nlet xs = [| for i in 1..100→ () |]\n[<Fun>] let model w = [| for x in xs→\nif random(Bernoulli(w.bias)) then random(Gaussian(w.mean.[0],w.sd.[0]) else random(Gaussian(w.mean.[1],w.sd.[1]) |]\nThe code uses both records (to structure the prior w of record type W) and arrays; both are encoded as tuples in the core language. The model function receives multiple inputs in an array xs and return an array multiple outputs from the model. The outputs of model are constructed using an array comprehension. The [<Fun>] attributes declare that definitions prior and model should be made available as quoted expression trees (as well as executable functions) so their code can be inspected by the density compiler.\nThe probability density function compiled for function model is (after manual reformatting to match the notation in this article):\nlet logPdf = fun w (ys:real[])→\nlogprodBy((fun i→ let x = xs.[i] Log(Exp(Log(pdf Gaussian(w.mean.[0], w.sd.[0], ys.[i])) +\nlet b=true in Log(pdf Bernoulli(w.bias,b))) + Exp(Log(pdf Gaussian(w.mean.[1], w.sd.[1], ys.[i])) +\nlet b=false in Log(pdf Gaussian(w.bias, b))))), xs.Length)\nThe helper function logprodBy( f ,n) computes the sum of the log densities f (0)+ · · ·+ f (n− 1). Notice the insertion of logarithms to avoid underflow.\nThe effect of disabling our simple peephole optimizer is to produce both less readable and less efficient code:\nlet logPdf = fun w (ys:real[])→\nlogprodBy( (fun i→ let x = xs.[i] Log(Exp(Log(pdf Gaussian(w.mean.[0],w.sd.[0], ys.[i]))+\nLog(Exp(Log(1.0)+ let b2=true Log(pdf Bernoulli(w.bias, b2))+ Log(Exp(Log(1.0))))))+\nExp(Log(pdf Gaussian(w.mean.[1],w.sd.[1], ys.[i1]))+ Log(Exp(Log(1.0)+\nlet b2=false Log(pdf Bernoulli(w.bias, b2))+ Log(Exp(Log(1.0)))))))),\nxs.Length)\nExample 4.2 (Linear Regression). For an example involving arithmetic we take linear regression. Given some noisy sample of points, the task is to estimate the parameters of a line fitting the points, yielding the line’s slope a, intercept b and an estimate of the noise.\nThe generative model is expressed as the following F# code:\ntype W = {a: double; b: double; noise: double}\n[<Fun>] let prior () = { a = random(Uniform(−1000.0, 1000.0))\nb = random(Uniform(−1000.0, 1000.0)) noise = random(Uniform(0.001, 100.0))}\nlet xs = [| −100.0 .. 100.0 |]\n[<Fun>] let model w = [| for x in xs→\nlet m = w.a ∗ x + w.b let d = w.noise random(Gaussian(m, d)) |]\nThe (log) probability density function compiled for function model is:\nlet logPdf = fun w ys→\nlogprodBy((fun i→ let x = xs.[i] Log(pdf Gaussian(w.a∗x+w.b, w.noise, ys.[i]))),\nxs.Length)\nExample 4.3 (Mixture Of Regressions). Combining aspects of the previous two examples we construct a mixture of two linear regressions, in which the slope and intercept of the line is selected by a latent boolean indicator variable.\ntype W = {bias:double; a: double[]; b: double[]; noise: double}\n[<Fun>] let prior () = { bias = rand.Uniform(0.0, 1.0)\na = [| for i in 0..1→ rand.Uniform(−1000.0, 1000.0)|] b = [| for i in 0..1→ rand.Uniform(−1000.0, 1000.0)|] noise = rand.Uniform(0.001, 100.0) }\nlet xs = [| −100.0 .. 100.0 |]\n[<Fun>] let model w =\n[| for x in xs→ if rand.Bernoulli(w.Bias) then let m = w.a.[0] ∗ x + w.b.[0]\nrand.Gaussian(m, w.noise) else let m = w.a.[1] ∗ x + w.b.[1]\nrand.Gaussian(m, w.noise)|]\nThe (log) probability density function compiled for function model is:\nlet logPdf = fun w ys→ logprodBy(\n(fun i→ let x = xs.[i] Log( Exp(Log(pdf Gaussian(w.a.[0]∗x+w.b.[0], w.noise, ys.[i])) +\nlet b=true in Log(pdf Bernoulli(w.bias, b))) + Exp(Log(pdf Gaussian(w.a.[1]∗x+w.b.[1], w.noise, ys.[i])) +\nlet b=false in Log(pdf Bernoulli(w.bias, b))))), xs.Length);\n4.2. Metrics. We consider scientific models with existing implementations for MCMC-based inference, written by domain experts. We are interested in how the modelling and inference experience would change, in terms of developer effort and performance impact, when adopting the Fun-based solution.\nWe assess the reduction in developer burden by measuring the code sizes (in lines-of-code (LOC)) of the original implementations of model and density code, and of the corresponding Fun model. For the synthetic examples, we have written both the model and the density code. The original implementations of the scientific models contain helper code such as I/O code for reading and writing data files in an application-specific format. Our LOC counts do not consider such helper code, but only count the code for generating synthetic data from the model, code for computing the logarithm of the posterior density of the model, and model-related code for setting up and interacting with Filzbach itself. We also compare the running times of the original implementations versus the Fun versions for MCMC-based inference using Filzbach, not including data manipulation before and after running inference.\nSynthetic examples. Our synthetic examples are three classic problems: the unsupervised learning task mixture of Gaussians (Example 4.1) the supervised learning task linear regression (Example 4.2), and a mixture of regressions (Example 4.3). Example 4.1 can be thought of as a probabilistic version of k-means clustering: inference is trying to determine the unknown mixing bias and the means and variances of the Gaussian components. In Example 4.2 inference is trying to determine\nthe coefficients of the line. In Example 4.3 inference is trying to determine the coefficients and mixing bias of two lines.\nSpecies distribution (McInerny and Purves, 2011). The species distribution problem is to give the probability that certain species will be present at a given site, based on climate factors. It is a problem of long-standing interest in ecology and has taken on new relevance in light of the issue of climate change. The particular model that we consider is designed to mitigate regression dilution arising from uncertainty in the predictor variables, for example, measurement error in temperature data. Inference tries to determine various features of the species and the environment, such as the optimal temperature preferred by a species, or the true temperature at a site (see code and density function in Appendix A).\nGlobal carbon cycle. (Smith et al., 2012). The dynamics of the Earth’s climate are intertwined with the terrestrial carbon cycle, and better carbon models (modelling how carbon in the air gets converted to biomass) enable better constrained projections about these systems. We consider a fully data-constrained terrestrial carbon model, which is composed of various submodels for smaller processes such as net primary productivity, the fine root mortality rate, or the fraction of trees that are evergreen versus deciduous. Inference tries to determine the different parameters of these submodels.\nDiscussion. Table 1 reports the metrics for each example. The LOC numbers show significant reduction in code size, with more significant savings as the size of the model grows. The larger models (where the Fun versions are ≈ 25% of the size of the original) are more indicative of the savings in developer and maintenance effort because Filzbach interaction code, which is roughly the same in all models, takes up a larger fraction of the smaller models. We find the running times encouraging: we have made little attempt to optimize the generated code, and preliminary testing indicates that much of the performance slow-down is due to constant factors.\nThe global carbon cycle model is composed of submodels, each with their own dataset. Unfortunately, it is unclear from the original source code how this composition translates to a run of inference, making it difficult to know what constitutes a fair comparison. Thus, we do not report a running time for the full model. However, we can measure the running time of individual submodels, such as net primary productivity, where the data and control flow are simpler."
    }, {
      "heading" : "5. RELATED WORK",
      "text" : "We have presented the first algorithm for deriving density functions from generative processes, that is both proved correct and implemented. An abridged version of this paper appears as Bhat et al. (2013). The correctness proof (for a version of the source language without pure let and general match) was recently mechanized in Isabelle by Eberl et al. (2015).\nThis paper builds on work by Bhat et al. (2012) who develop a theoretical framework for computing PDFs, but describe no implementation nor correctness proof. The density compiler of Section 3 has a simpler presentation, with two judgments compared to five, and has rules for pure lets and operations on integers. Our paper also uses a richer language (Fun), which adds fail, match and general if (and for performance reasons, pure let).\nGordon et al. (2013) describe a naive density calculation routine for Fun without random lets; this sublanguage does not cover many useful classes of models such as hierarchical and mixture models.\nThe BUGS system computes densities from declaratively specified models to perform Gibbs sampling (Gilks et al., 1994). However, the models are not compositional as in this work, and only the joint density over all variables is possible. The AutoBayes system also computes densities for deriving maximum likelihood and Bayesian estimators for a significant class of statistical models (Schumann et al., 2008). It is not formally specified and does not appear to be compositional. Neither system addresses the non-existence of PDFs, presumably restricting expressivity in order to avoid the issue.\nStan (Stan Development Team, 2014) is a probabilistic programming language that supports various forms of MCMC. The Stan language is a derivative of BUGS, and is compiled to efficient C++ sampling code, in part based on automatically derived density functions. For models with latent variables, including mixture models such as mixture of Gaussians, and for models that perform non-linear computations on random values, Stan’s users are required to manually manipulate the log probability density function, using a primitive operation increment log prob. Stan employs automatic differentiation (Griewank and Walther, 2008) of log posteriors in order to apply gradientbased Hamiltonian MCMC algorithms. This relieves the user from coding error-prone derivatives.\nInference for the Church language also uses MCMC, but works with distributions over runs of a program instead of over its return value (Wingate et al., 2011), circumventing the need for a PDF.\nThere are many other systems for probabilistic programming, some of which provide a way to compute density functions or an analogous object; however, they sacrifice some other feature to do so. Several languages only provide support for finite, discrete distributions, but provide access to the probability mass function (Ramsey and Pfeffer, 2002; Kiselyov and Shan, 2009). Like BUGS and Stan, systems like the Hierarchical Bayes Compiler (Daumé III, 2007) are not formally defined and require models to be specified in a monolithic way, whereas large models in Fun can be composed of smaller models. Probabilistic logic languages like Markov Logic (Domingos et al., 2008) do not have generative semantics but instead have semantics in undirected graphical models which are equipped with potential functions that are analogous to density functions. The language is constructed in a way that guarantees the existence of a potential function, but which eliminates the possibility to express models that require pure let, such as the global carbon cycle model."
    }, {
      "heading" : "6. CONCLUSIONS AND FUTURE WORK",
      "text" : "We have described a compiler for automatically computing probability density functions for programs from a rich Bayesian probabilistic programming language, proven the algorithm correct, and shown its applicability to real-world scientific models.\nThe inclusion of fail in the language appears useful for scientific models, giving a simple facility to exclude branches that are scientifically impossible from consideration. However, more investigation is needed to settle this claim.\nA drawback of the compiler is that terms of composite type are required either to have a PDF or to be pure, ruling out terms such as (0.0, random(Uniform)). One possibility for future work would be to refine the types of expressions with determinacy information, and make use of this additional information to admit more joint distributions (cf. (TUPLE VAR))."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Manuel Eberl and Tobias Nipkow for helpful comments, in particular on the semantics of the integration operator."
    }, {
      "heading" : "APPENDIX A. SPECIES DISTRIBUTION (MCINERNY AND PURVES, 2011)",
      "text" : "let Nspecies = 20 let Nsamples = 2000\ntype W = { Topt: double[]\nTbreadth: double[] MaxProb: double[] Terr: double Yerr: double Ttrue: double[]}\ntype Y = {Tobs: double[]; Y: double[][]}\nlet CalcSpProb w t sp = let z = (t − w.Topt.[sp]) / w.Tbreadth.[sp] w.MaxProb.[sp] ∗ exp (− z∗z)\n[<Fun>] let prior () = { Topt = [| for j in 0..Nspecies−1→random(Uniform(0.1, 50.0)) |]\nTbreadth = [| for j in 0..Nspecies−1→random(Uniform(0.1, 50.0)) |] MaxProb = [| for j in 0..Nspecies−1→random(Uniform(0.1, 1.0)) |] Terr = random(Uniform(0.1, 10.0)) Yerr = random(Uniform(0.01, 0.5))\nTtrue = [| for i in 0..Nsamples−1→random(Uniform(5.0, 30.0))|] }\nlet samplesR = [|0..Nsamples−1|] let speciesR = [|0..Nspecies−1|]\n[<Fun>] let model w =\nlet tobs = [| for i in samplesR→random(Gaussian(w.Ttrue.[i], w.Terr)) |] let y = [| for i in samplesR→\n[| for j in speciesR→ let p = CalcSpProb w w.Ttrue.[i] j random(Gaussian(p, w.Yerr)) |] |]\n{ Tobs = tobs Y = y }\n// the generated log probability density function for model let logPdf =\nfun w ys→ let y = ys.Y let tobs = ys.Tobs let i0 = samplesR (logprodBy((fun i1→\nlet i=samplesR.[i1] Log(pdf Gaussian(w.Ttrue.[i],w.Terr,tobs.[i1]))),\nsamplesR.Length)) + (let i2 = samplesR logprodBy((fun i3→\nlet i=samplesR.[i3] let i4=speciesR logprodBy((fun i5→\nlet j=speciesR.[i5] Log(pdf Gaussian(CalcSpProb w (w.Ttrue.[i]) j,w.Yerr,y.[i3].[i5]))),\nspeciesR.Length)), samplesR.Length))"
    } ],
    "references" : [ {
      "title" : "The change of variables formula using matrix volume",
      "author" : [ "A. Ben-Israel" ],
      "venue" : "SIAM Journal of Matrix Analysis,",
      "citeRegEx" : "Ben.Israel.,? \\Q1999\\E",
      "shortCiteRegEx" : "Ben.Israel.",
      "year" : 1999
    }, {
      "title" : "A type theory for probability density functions",
      "author" : [ "S. Bhat", "A. Agarwal", "R.W. Vuduc", "A.G. Gray" ],
      "venue" : "In J. Field and M. Hicks, editors,",
      "citeRegEx" : "Bhat et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bhat et al\\.",
      "year" : 2012
    }, {
      "title" : "Deriving probability density functions from probabilistic functional programs. In Tools and Algorithms for the Construction and Analysis of Systems, 19th International Conference",
      "author" : [ "S. Bhat", "J. Borgström", "A.D. Gordon", "C. Russo" ],
      "venue" : "TACAS 2013,",
      "citeRegEx" : "Bhat et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bhat et al\\.",
      "year" : 2013
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 2006
    }, {
      "title" : "Measure transformer semantics for Bayesian machine learning",
      "author" : [ "J. Borgström", "A.D. Gordon", "M. Greenberg", "J. Margetson", "J. Van Gael" ],
      "venue" : "In European Symposium on Programming (ESOP’11),",
      "citeRegEx" : "Borgström et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Borgström et al\\.",
      "year" : 2011
    }, {
      "title" : "Probabilistic inductive logic programming, pages 92–117",
      "author" : [ "P. Domingos", "S. Kok", "D. Lowd", "H. Poon", "M. Richardson", "P. Singla" ],
      "venue" : null,
      "citeRegEx" : "Domingos et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Domingos et al\\.",
      "year" : 2008
    }, {
      "title" : "A verified compiler for probability density functions",
      "author" : [ "M. Eberl", "J. Hölzl", "T. Nipkow" ],
      "venue" : "24th European Symposium on Programming: ESOP 2015,",
      "citeRegEx" : "Eberl et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Eberl et al\\.",
      "year" : 2015
    }, {
      "title" : "A language and program for complex Bayesian modelling",
      "author" : [ "W.R. Gilks", "A. Thomas", "D.J. Spiegelhalter" ],
      "venue" : "The Statistician,",
      "citeRegEx" : "Gilks et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Gilks et al\\.",
      "year" : 1994
    }, {
      "title" : "A categorical approach to probability theory",
      "author" : [ "M. Giry" ],
      "venue" : null,
      "citeRegEx" : "Giry.,? \\Q1982\\E",
      "shortCiteRegEx" : "Giry.",
      "year" : 1982
    }, {
      "title" : "Church: a language for generative models",
      "author" : [ "N. Goodman", "V.K. Mansinghka", "D.M. Roy", "K. Bonawitz", "J.B. Tenenbaum" ],
      "venue" : "In Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Goodman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Goodman et al\\.",
      "year" : 2008
    }, {
      "title" : "A model-learner pattern for Bayesian reasoning",
      "author" : [ "A.D. Gordon", "M. Aizatulin", "J. Borgström", "G. Claret", "T. Graepel", "A. Nori", "S. Rajamani", "C. Russo" ],
      "venue" : "In POPL,",
      "citeRegEx" : "Gordon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2013
    }, {
      "title" : "Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation",
      "author" : [ "A. Griewank", "A. Walther" ],
      "venue" : "SIAM, 2nd edition,",
      "citeRegEx" : "Griewank and Walther.,? \\Q2008\\E",
      "shortCiteRegEx" : "Griewank and Walther.",
      "year" : 2008
    }, {
      "title" : "Embedded probabilistic programming",
      "author" : [ "O. Kiselyov", "C. Shan" ],
      "venue" : "In Domain-Specific Languages,",
      "citeRegEx" : "Kiselyov and Shan.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kiselyov and Shan.",
      "year" : 2009
    }, {
      "title" : "Fine-scale environmental variation in species distribution modelling: regression dilution, latent variables and neighbourly advice",
      "author" : [ "G. McInerny", "D. Purves" ],
      "venue" : "Methods in Ecology and Evolution,",
      "citeRegEx" : "McInerny and Purves.,? \\Q2011\\E",
      "shortCiteRegEx" : "McInerny and Purves.",
      "year" : 2011
    }, {
      "title" : "Probabilistic inference using Markov chain Monte Carlo methods",
      "author" : [ "R.M. Neal" ],
      "venue" : "Technical Report CRG-TR-93-1, Dept. of Computer Science,",
      "citeRegEx" : "Neal.,? \\Q1993\\E",
      "shortCiteRegEx" : "Neal.",
      "year" : 1993
    }, {
      "title" : "The category of Markov kernels",
      "author" : [ "P. Panangaden" ],
      "venue" : "Electronic Notes in Theoretical Computer Science,",
      "citeRegEx" : "Panangaden.,? \\Q1999\\E",
      "shortCiteRegEx" : "Panangaden.",
      "year" : 1999
    }, {
      "title" : "Stochastic lambda calculus and monads of probability distributions",
      "author" : [ "N. Ramsey", "A. Pfeffer" ],
      "venue" : "In POPL,",
      "citeRegEx" : "Ramsey and Pfeffer.,? \\Q2002\\E",
      "shortCiteRegEx" : "Ramsey and Pfeffer.",
      "year" : 2002
    }, {
      "title" : "AutoBayes program synthesis system users manual",
      "author" : [ "J. Schumann", "T. Pressburger", "E. Denney", "W. Buntine", "B. Fischer" ],
      "venue" : "Technical Report NASA/TM–2008–215366, NASA Ames Research Center,",
      "citeRegEx" : "Schumann et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Schumann et al\\.",
      "year" : 2008
    }, {
      "title" : "Parametric Statistical Modeling by Minimum Integrated Square",
      "author" : [ "D. Scott" ],
      "venue" : "Error. Technometrics,",
      "citeRegEx" : "Scott.,? \\Q2001\\E",
      "shortCiteRegEx" : "Scott.",
      "year" : 2001
    }, {
      "title" : "The climate dependence of the terrestrial carbon cycle; including parameter and structural uncertainties",
      "author" : [ "M.J. Smith", "M.C. Vanderwel", "V. Lyutsarev", "S. Emmott", "D.W. Purves" ],
      "venue" : "Biogeosciences Discussions,",
      "citeRegEx" : "Smith et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2012
    }, {
      "title" : "A model of set-theory in which every set of reals is Lebesgue measurable",
      "author" : [ "R.M. Solovay" ],
      "venue" : "The Annals of Mathematics, Second Series,",
      "citeRegEx" : "Solovay.,? \\Q1970\\E",
      "shortCiteRegEx" : "Solovay.",
      "year" : 1970
    }, {
      "title" : "Lightweight implementations of probabilistic programming languages via transformational compilation",
      "author" : [ "D. Wingate", "A. Stuhlmueller", "N. Goodman" ],
      "venue" : "In Proceedings of the 14th Intl. Conf. on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Wingate et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wingate et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Such techniques include maximum likelihood or maximum a posteriori estimation, L2 estimation, importance sampling, and Markov chain Monte Carlo (MCMC) methods (Scott, 2001; Bishop, 2006).",
      "startOffset" : 159,
      "endOffset" : 186
    }, {
      "referenceID" : 3,
      "context" : "Such techniques include maximum likelihood or maximum a posteriori estimation, L2 estimation, importance sampling, and Markov chain Monte Carlo (MCMC) methods (Scott, 2001; Bishop, 2006).",
      "startOffset" : 159,
      "endOffset" : 186
    }, {
      "referenceID" : 16,
      "context" : "However, despite their utility, density functions have been largely absent from the literature on probabilistic functional programming (Ramsey and Pfeffer, 2002; Goodman et al., 2008; Kiselyov and Shan, 2009).",
      "startOffset" : 135,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "However, despite their utility, density functions have been largely absent from the literature on probabilistic functional programming (Ramsey and Pfeffer, 2002; Goodman et al., 2008; Kiselyov and Shan, 2009).",
      "startOffset" : 135,
      "endOffset" : 208
    }, {
      "referenceID" : 12,
      "context" : "However, despite their utility, density functions have been largely absent from the literature on probabilistic functional programming (Ramsey and Pfeffer, 2002; Goodman et al., 2008; Kiselyov and Shan, 2009).",
      "startOffset" : 135,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "An abridged version of this paper was published as (Bhat et al., 2013).",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "Consider for example a simple mixture of Gaussians, here written in Fun (Borgström et al., 2011), a probabilistic functional language embedded within F# (Syme et al.",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "Details and examples are given by Bhat et al. (2012), who provide the theory for addressing this problem, which we extend and implement in this work.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Bishop provides an excellent account of Bayesian learning (Bishop, 2006).",
      "startOffset" : 58,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "Neal (1993) gives an excellent review of MCMC methods.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "Specifically: • We provide the first implementation of a density compiler based on the specification by Bhat et al. (2012). We compile programs in the probabilistic language Fun (described in Section 2.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Our source language is a version of the core calculus Fun (Borgström et al., 2011), without observation.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "To mark certain program points as impossible, we add a fail construct (Kiselyov and Shan, 2009).",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "Our source language is a version of the core calculus Fun (Borgström et al., 2011), without observation. To mark certain program points as impossible, we add a fail construct (Kiselyov and Shan, 2009). Fun is a first-order functional language without recursion that extends the language of Ramsey and Pfeffer (2002), and this version has a natural semantics in the sub-probability monad.",
      "startOffset" : 59,
      "endOffset" : 316
    }, {
      "referenceID" : 20,
      "context" : "Indeed, the power of the axiom of choice is needed to construct a non-measurable set (Solovay, 1970).",
      "startOffset" : 85,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "Open fail-free Fun expressions have a straightforward semantics (Ramsey and Pfeffer, 2002) as computations in the probability monad (Giry, 1982).",
      "startOffset" : 64,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "Open fail-free Fun expressions have a straightforward semantics (Ramsey and Pfeffer, 2002) as computations in the probability monad (Giry, 1982).",
      "startOffset" : 132,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "In order to treat the fail primitive, we use an existing extension (Gordon et al., 2013) of the semantics of Ramsey and Pfeffer (2002) to a richer monad: the subprobability monad (Panangaden, 1999)3.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : ", 2013) of the semantics of Ramsey and Pfeffer (2002) to a richer monad: the subprobability monad (Panangaden, 1999)3.",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "Open fail-free Fun expressions have a straightforward semantics (Ramsey and Pfeffer, 2002) as computations in the probability monad (Giry, 1982). In order to treat the fail primitive, we use an existing extension (Gordon et al., 2013) of the semantics of Ramsey and Pfeffer (2002) to a richer monad: the subprobability monad (Panangaden, 1999)3.",
      "startOffset" : 133,
      "endOffset" : 281
    }, {
      "referenceID" : 8,
      "context" : "Open fail-free Fun expressions have a straightforward semantics (Ramsey and Pfeffer, 2002) as computations in the probability monad (Giry, 1982). In order to treat the fail primitive, we use an existing extension (Gordon et al., 2013) of the semantics of Ramsey and Pfeffer (2002) to a richer monad: the subprobability monad (Panangaden, 1999)3. Compared to the operations of the probability monad, the sub-probability monad additionally admits a zero constant, yielding the zero measure. To accommodate the zero measure, the carrier set is extended from probability measures to sub-probability measures, i.e., admitting all μ with |μ| ≤ 1. Below we recapitulate the semantics of Fun by Gordon et al. (2013). Here σ is a closed value substitution whose domain contains all the free variables of M, and detOp(M) ranges over op(M), fst M, snd M, inl M and inr M.",
      "startOffset" : 133,
      "endOffset" : 708
    }, {
      "referenceID" : 1,
      "context" : "Our compilation is based on that of Bhat et al. (2012), with modifications to treat fail statements, match (and general if) statements, pure (i.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "4Joint marginal densities for tuples of expressions can be computed if those expressions are conditionally independent (Bhat et al., 2012).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : ", (PLUS RND) below), we instead use the matrix volume of the Jacobian matrix of the inverse operation (Ben-Israel, 1999).",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "Species distribution (McInerny and Purves, 2011).",
      "startOffset" : 21,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "(Smith et al., 2012).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "An abridged version of this paper appears as Bhat et al. (2013). The correctness proof (for a version of the source language without pure let and general match) was recently mechanized in Isabelle by Eberl et al.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "An abridged version of this paper appears as Bhat et al. (2013). The correctness proof (for a version of the source language without pure let and general match) was recently mechanized in Isabelle by Eberl et al. (2015). This paper builds on work by Bhat et al.",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : "An abridged version of this paper appears as Bhat et al. (2013). The correctness proof (for a version of the source language without pure let and general match) was recently mechanized in Isabelle by Eberl et al. (2015). This paper builds on work by Bhat et al. (2012) who develop a theoretical framework for computing PDFs, but describe no implementation nor correctness proof.",
      "startOffset" : 45,
      "endOffset" : 269
    }, {
      "referenceID" : 1,
      "context" : "An abridged version of this paper appears as Bhat et al. (2013). The correctness proof (for a version of the source language without pure let and general match) was recently mechanized in Isabelle by Eberl et al. (2015). This paper builds on work by Bhat et al. (2012) who develop a theoretical framework for computing PDFs, but describe no implementation nor correctness proof. The density compiler of Section 3 has a simpler presentation, with two judgments compared to five, and has rules for pure lets and operations on integers. Our paper also uses a richer language (Fun), which adds fail, match and general if (and for performance reasons, pure let). Gordon et al. (2013) describe a naive density calculation routine for Fun without random lets; this sublanguage does not cover many useful classes of models such as hierarchical and mixture models.",
      "startOffset" : 45,
      "endOffset" : 679
    }, {
      "referenceID" : 7,
      "context" : "The BUGS system computes densities from declaratively specified models to perform Gibbs sampling (Gilks et al., 1994).",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "The AutoBayes system also computes densities for deriving maximum likelihood and Bayesian estimators for a significant class of statistical models (Schumann et al., 2008).",
      "startOffset" : 147,
      "endOffset" : 170
    }, {
      "referenceID" : 11,
      "context" : "Stan employs automatic differentiation (Griewank and Walther, 2008) of log posteriors in order to apply gradientbased Hamiltonian MCMC algorithms.",
      "startOffset" : 39,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "Inference for the Church language also uses MCMC, but works with distributions over runs of a program instead of over its return value (Wingate et al., 2011), circumventing the need for a PDF.",
      "startOffset" : 135,
      "endOffset" : 157
    }, {
      "referenceID" : 16,
      "context" : "Several languages only provide support for finite, discrete distributions, but provide access to the probability mass function (Ramsey and Pfeffer, 2002; Kiselyov and Shan, 2009).",
      "startOffset" : 127,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "Several languages only provide support for finite, discrete distributions, but provide access to the probability mass function (Ramsey and Pfeffer, 2002; Kiselyov and Shan, 2009).",
      "startOffset" : 127,
      "endOffset" : 178
    }, {
      "referenceID" : 5,
      "context" : "Probabilistic logic languages like Markov Logic (Domingos et al., 2008) do not have generative semantics but instead have semantics in undirected graphical models which are equipped with potential functions that are analogous to density functions.",
      "startOffset" : 48,
      "endOffset" : 71
    } ],
    "year" : 2017,
    "abstractText" : "The probability density function of a probability distribution is a fundamental concept in probability theory and a key ingredient in various widely used machine learning methods. However, the necessary framework for compiling probabilistic functional programs to density functions has only recently been developed. In this work, we present a density compiler for a probabilistic language with failure and both discrete and continuous distributions, and provide a proof of its soundness. The compiler greatly reduces the development effort of domain experts, which we demonstrate by solving inference problems from various scientific applications, such as modelling the global carbon cycle, using a standard Markov chain Monte Carlo framework.",
    "creator" : "LaTeX with hyperref package"
  }
}