{
  "name" : "1312.4287.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Guido Governatori", "Francesco Olivieri", "Simone Scannapieco", "Antonino Rotolo", "Matteo Cristani" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 2.\n42 87\nv1 [\ncs .L\nO ]\n1 6\nD ec\n2 01"
    }, {
      "heading" : "1 Introduction and Motivation",
      "text" : "Over the years many dialogue games for argumentation have been proposed to study questions such as which conclusions are justified, or how procedures for debate and conflict resolution should be structured to arrive at a fair and just outcome. We observed that the outcome of a debate does not solely depend on the premises of a case, but also on the strategies that parties in a dispute actually adopt. According to our knowledge, this aspect has not received the proper attention in the literature of the field.\nAlmost all the AI literature on the strategic aspects of argumentation (see Section 3 for a brief overview) assumes to work with argument games with complete information, i.e., dialogues where the structure of the game is common knowledge among the players. Consider, however, the following example due to (Satoh and Takahashi 2011) (which in turn modifies an example taken from (Okuno and Takahashi 2009)):\np0 : “You killed the victim.” c1 : “I did not commit murder! There is no evidence!” p1 : “There is evidence. We found your ID card near the scene.” c2 : “It’s not evidence! I had my ID card stolen!” p2 : “It is you who killed the victim. Only you were near the scene at the time of the murder.” c3 : “I didn’t go there. I was at facility A at that time.” p3 : “At facility A? Then, it’s impossible to have had\nyour ID card stolen since facility A does not allow a person to enter without an ID card.”\nThis dialogue exemplifies an argument game occurring in witness examinations in legal courts. The peculiarity of this game is the fact that the exchange of arguments reflects an\nCopyright c© 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nasymmetry of information between the players: each player does not know the other player’s knowledge, thus she cannot predict which arguments are attacked and which counterarguments are employed for attacking the arguments. Indeed, (Satoh and Takahashi 2011) points out, for instance, that p3 attacks c2, but only when c3 is given: hence, the attack p3 of the proponent is made possible only when the opponent discloses some private information with the move c3.\nDespite the encouraging results offered by (Satoh and Takahashi 2011), we argue that relaxing the complete-information assumption leads in general to non-tractable frameworks. In this paper, in particular, we explore the computational cost of argument games of incomplete information where the (internal) logical structure of arguments is considered.\nIn this case relaxing complete information, such as when players do not share the same beliefs and set of arguments, simply amounts to the fact they have different logical theories, i.e., different sets of rules from which arguments supporting logical conclusions can be built. Hence, if the proponent, having a theory T , has the objective to prove that some l is true, there is no obvious way for preferring an argument for l obtained from the minimal subset of T (which could at first sight minimise the chances of successful attacks from the opponent) over the maximal set of arguments obtained from the whole T (which could at a first sight maximise the chances to defeat any counterarguments of the opponent).\nThe layout of the paper is as follows. Section 2 offers a gentle introduction and motivation for our research problem. Section 3 reviews relevant related work, thus presenting further motivations behind our contribution. Section 4 presents the logic used for building arguments in dialogues (Argumentation Logic): it is a variant of Defeasible Logic (Antoniou et al. 2001) having linear complexity; another logic (Agent Logic has linear complexity as well) is subsequently recalled from (Governatori and Rotolo 2008). In this second logic, it is possible to formulate the NPcomplete “Restoring Sociality Problem”; the objective is to prove that this problem can be mapped into the problem of interest here, the so-called “Strategic Argumentation Problem”, which rather consists in successfully deciding for each player what move to play at each argument turn (thus showing that the Strategic Argumentation Problem is NPcomplete as well). Section 5 defines dialogue protocols for\ngames of incomplete information based on Argumentation Logic and formulates the Strategic Argumentation Problem. Section 6 shows how to transform a theory in Agent Logic into an equivalent one in Argumentation Logic, and presents the main theorem of computational complexity for argument games."
    }, {
      "heading" : "2 A Gentle Introduction to the Problem",
      "text" : "In the most typical forms of strategic argumentation, two players exchange arguments in a dialogue game: in the simplest case, a proponent (hereafter Pr) has the objective to prove a conclusion l (a literal of the language) and an opponent (hereafter Op) presents counterarguments to the moves of Pr. If we frame this intuition in proof-theoretic settings, such as in those developed in (Governatori et al. 2004; Prakken 2010; Toni 2013) where arguments are defined as inference trees formed by applying rules, exchanging arguments means exchanging logical theories (consisting of rules) proving conclusions. Assume, for instance, that the argument game is based on a finite set F of indisputable facts and a finite set R of rules: facts initially fire rules and this leads to building proofs for literals.\nIf R and F are common knowledge of Pr and Op, successful strategies in argument games are trivially identified: each player can compute if the entire theory (consisting of F and R) logically entails l. In this situation the game consists of a single move.\nSuppose now that F is known by both players, but R is partitioned into three subsets: a set RCom known by both players and two subsets RPr and ROp corresponding, respectively, to Pr’s and Op’s private knowledge (what Pr and Op privately know to be true). This scenario exemplifies an argument game of incomplete information. In this context, each player can use all rules belonging to her private knowledge (RPr or ROp) as well as all the public rules. These rules are not just the rules in RCom but also rules that, though initially belonging to the private information of other player, have been used in previous turns.\nLet us suppose to work with a skeptical non-monotonic framework, i.e., a logical machinery where, whenever two conflicting conclusions are obtainable from different rules, the system refrains to take a decision. Assuming a game where players has private and public knowledge, the problem of deciding what move (set of rules) to play at each turn amounts to establish whether there is any subset of her rules that can be successful. Is there any safe criterion to select successful strategies?\nConsider the following three examples.\nPr and Op are debating about the truthfulness of a statement, we say l; Pr is arguing that l is the case, whilst Op answers back the truthfulness of the opposite claim (henceforth ¬l). Each player has her own (private) arguments, not known by the opponent, but they both share the factual knowledge as well as some inference rules. Suppose Pr has the following private arguments:\nP1 : a ⇒ b ⇒ c ⇒ l P2 : ¬b ⇒ ¬e ⇒ f P3 : ¬b ⇒ ¬e ⇒ g P4 : d ⇒ c,\nwhile Op has\nO1 : a ⇒ e ⇒ ¬l O2 : d ⇒ ¬b O3 : f ⇒ ¬l,\nwhere F = {a,d} and RCom = {g ⇒¬l}. The notation used is to exemplify arguments as chains of rules. For instance, argument P1 implies that RPr contains three rules r1 : a⇒ b, r2 : b ⇒ c, and r3 : c ⇒ l.\nThe point of the example being that if Pr decides to announce all his private arguments, then she is not able to prove her thesis l. Indeed, she would not have counterarguments defeating O3 and RCom. If instead she argues with P1 and the subpart ¬b ⇒ ¬e of P2, keeping hidden from Op the way to prove the premises d of O2, then she proves l\nConsider now this new setting:\nF = {a,d, f}\nRCom = /0\nRPr = {a ⇒ b, d ⇒ c, c ⇒ b}\nROp = {c ⇒ e, e, f ⇒¬b}\nIf Pr’s intent is to prove b and she plays {a ⇒ b}, then Pr wins the game. However, if Pr plays {d ⇒ c, c ⇒ b} (or even RPr), this allows Op to succeed. Here, a minimal subset of RPr is successful. However, the situation (for similar reasons) can be reversed for Pr:\nF = {a,d, f}\nRCom = /0\nRPr = {a ⇒ b, d ⇒¬c}\nROp = {d,c ⇒¬b, f ⇒ c}\nIn this second case, the move {a ⇒ b} is not successful for Pr, while playing with the whole RPr ensures victory.\nIn the remainder of this paper, we will study this research question in the context of Defeasible Logic. We will show that the problem of deciding what set of rules to play (Strategic Argumentation Problem) at a given move is NP-complete even when the problem of deciding whether a given theory (defeasibly) entails a literal can be computed in polynomial time. We will map the NP-complete Restoring Sociality Problem proposed in (Governatori and Rotolo 2008) into the Strategic Argumentation Problem. To this end, we first propose a standard Defeasible Logic to formalise the argumentation framework (Subsection 4.1) and then we present the BIO agent defeasible logic (Subsection 4.2). Finally, in Section 6 we show how to transform an agent defeasible logic into an equivalent argumentation one and we present the main theorem of computational complexity."
    }, {
      "heading" : "3 Related Work",
      "text" : "Despite the game-like character of arguments and debates, game-theoretic investigations of argumentation are still rare in the AI argumentation literature and in the game theory one as well (an exception in this second perspective is (Glazer and Rubinstein 2001)).\nMost existing game-theoretic investigations of argumentation in AI, such as (Procaccia and Rosenschein 2005; Matt and Toni 2008; Riveret et al. 2008; Rahwan and Larson 2009; Grossi and van der Hoek 2013) proceed within Dung’s abstract argumentation paradigm, while (Roth et al. 2007), though working on argumentation semantics related with Dung’s approach, develop a framework where also the logical internal structure of arguments is made explicit.\n(Matt and Toni 2008) presents a notion of argument strength within the class of games of strategy. The measure of the strength of an argument emerges from confronting proponent and opponent via a repeated game of argumentation strategy such that the payoffs reflect the long term interaction between proponent and opponent strategies.\nOther types of game analyses have been used for argumentation. In particular, argumentation games have been reconstructed as two-player extensive-form games of perfect information (Procaccia and Rosenschein 2005; Riveret et al. 2008; Grossi and van der Hoek 2013). (For a discussion on using extensive-form games, see also (Rahwan and Larson 2009).) While (Grossi and van der Hoek 2013) works on zero-sum games, (Riveret et al. 2008) does not adopt this view because preferences over outcomes are specified in terms of expected utility combining the probability of success of arguments (with respect to a third party, an adjudicator such as a judge) with the costs and benefits associated to arguments, thus making possible that argument withdrawn be the most preferred option. Besides this difference, in both approaches uncertainty is introduced due to different probabilities of success depending on a third party, such as external audience or a judge, whose attitude towards the arguments exchanged by proponent and opponent is uncertain.\nAll these works assume that argument games have complete information, which, we noticed, is an oversimplification is many real-life contexts (such as in legal disputes). How to go beyond complete information? In game-theoretic terms, one of the simplest ways of analyzing argument games of incomplete information is to frame them as Bayesian extensive games with observable actions (Osborne and Rubinstein 1999, chap. 12): this is possible because every player observes the argumentative move of the other player and uncertainty only derives from an initial move of chance that distributes (payoff-relevant) private information among the players corresponding to logical theories: hence, chance selects types for the players by assigning to them possibly different theories from the set of all possible theories constructible from a given language. If this hypothesis is correct, notice that (i) Bayesian extensive games with observable actions allow to simply extend the argumentation models proposed, e.g., in (Riveret et al. 2008;\nGrossi and van der Hoek 2013), and (ii) the probability distributions over players’ types can lead to directly measuring the probability of justification for arguments and conclusions, even when arguments are internally analyzed (Riveret, Rotolo, and Sartor 2012). Despite this fact, however, complexity results for Bayesian games are far from encouraging (see (Gottlob, Greco, and Mancini 2007) for games of strategy). If we move to Bayesian extensive games with observable actions things not encouraging, too. Indeed, we guess that considerations similar to those presented by (Chalkiadakis and Boutilier 2007) can be applied to argument games: the calculation of the perfect Bayesian equilibrium solution can be tremendously complex due to both the size of the strategy space (as a function of the size of the game tree, and it can be computationally hard to compute it (Dimopoulos, Nebel, and Toni 2002)), and the dependence between variables representing strategies and players’ beliefs. A study of these game-theoretical issues cannot be developed here and is left to future work: this paper, instead, considers a more basic question: the computational problem of exploring solutions in the logical space of strategies when arguments have an internal structure.\nIn this sense, this contribution does not directly develop any game-theoretic analysis of argumentation games of incomplete information, but it offers results about the computation cost for logically characterizing the problems that any argumentation game with incomplete information potentially rises. Relevant recent papers that studied argumentation of incomplete information without any direct game-theoretic analysis are (Okuno and Takahashi 2009) and (Satoh and Takahashi 2011), which worked within the paradigm of abstract argumentation. The general idea in these works is to devise a system for dynamic argumentation games where agents’ knowledge bases can change and where such changes are precisely caused by exchanging arguments. (Okuno and Takahashi 2009) presents a first version of the framework and an algorithm, for which the authors prove a termination result. (Satoh and Takahashi 2011) generalizes this framework (by relaxing some constraints) and devises a computational method to decide which arguments are accepted by translating argumentation framework into logic programming; this further result, however, is possible only when players are eager to give all the arguments, i.e., when proponent and opponent eventually give all possible arguments in the game."
    }, {
      "heading" : "4 Logic",
      "text" : "In this section we shall introduce the two logics used in this paper. The first is the logic used in a dialogue game. This is the logic to represent the knowledge of the players, the structure of the arguments, and perform reasoning. We call this logic “Argumentation logic” and we use the Defeasible Logic of (Antoniou et al. 2001). (Governatori et al. 2004) provides the relationships between this logic (and some of its variants) and abstract argumentation, and (Thakur et al. 2007) shows how to use this logic for dialogue games. The second logic, called Agent Logic, is the logic in which the “restoring sociality problem” (a known NP-completed problem)\n(Governatori and Rotolo 2008) was formulated. It is included in this paper to show how to reduce the restoring sociality problem into the strategic argumentation problem, proving thus that the later is also an NP-complete problem. The Agent Logic is an extension of Defeasible Logic with modal operators for Beliefs, Intentions and Obligations (Governatori and Rotolo 2008).\nAdmittedly, this section takes a large part of this paper, but is required to let the reader comprehend the mechanisms behind our demonstration of NP-completeness."
    }, {
      "heading" : "4.1 Argumentation Logic",
      "text" : "A defeasible argumentation theory is a standard defeasible theory consisting of a set of facts or indisputable statements, a set of rules, and a superiority relation > among rules saying when a single rule may override the conclusion of another rule. We have that φ1, . . . ,φn → ψ is a strict rule such that whenever the premises φ1, . . . ,φn are indisputable so is the conclusion ψ . A defeasible rule φ1, . . . ,φn ⇒ ψ is a rule that can be defeated by contrary evidence. Finally, φ1, . . . ,φn ❀ ψ is a defeater that is used to prevent some conclusion but cannot be used to draw any conclusion.\nDefinition 1 (Language). Let PROP be a set of propositional atoms and Lblarg be a set of labels. Define:\nLiterals Lit = PROP∪{¬p|p ∈ PROP}\nIf q is a literal, ∼q denotes the complementary literal (if q is a positive literal p then ∼q is ¬p; and if q is ¬p, then ∼q is p);\nRules r : φ1, . . . ,φn →֒ ψ ,\nwhere r ∈ Lblarg is a unique label, A(r) = {φ1, . . . ,φn} ⊆ Litarg is the antecedent of r, C(r) = ψ ∈ Litarg is the consequent of r, and →֒∈ {→,⇒,❀} is the type of r.\nWe use R[q] to indicate all rules with consequent q. We denote the sets of strict, rules, strict and defeasible rules, and defeaters with Rs, Rd, Rsd, and Rdft, respectively.\nDefinition 2 (Defeasible Argumentation Theory). A defeasible argumentation theory is a structure\nDarg = (F,R,>)\nwhere\n• F ⊆ Lit is a finite set of facts; • R is the finite set of rules; • The superiority relation > is acyclic, irreflexive, and\nasymmetric.\nDefinition 3 (Proofs). Given an agent theory D, a proof P of length n in D is a finite sequence P(1), . . . ,P(n) of labelled formulas of the type +∆q, −∆q, +∂q and−∂q, where the proof conditions defined in the rest of this section hold. P(1..n) denotes the initial part of the derivation of length n.\nWe start with some terminology.\nDefinition 4. Given # ∈ {∆,∂} and a proof P in D, a literal q is #-provable in D if there is a line P(m) of P such that P(m) = +#q. A literal q is #-rejected in D if there is a line P(m) of P such that P(m) =−#q.\nThe definition of ∆ describes just forward chaining of strict rules:\n+∆: If P(n+ 1) = +∆q then (1) q ∈ F or (2) ∃r ∈ Rs[q] s.t. ∀a ∈ A(r). a is ∆-provable. −∆: If P(n+ 1) =−∆q then (1) q /∈ F and (2) ∀r ∈ Rs[q]. ∃a ∈ A(r) s.t. a is ∆-rejected.\nFor a literal q to be definitely provable either is a fact, or there is a strict rule with head q, whose antecedents have all been definitely proved previously. And to establish that q cannot be definitely proven we must establish that every strict rule with head q has at least one antecedent is definitely rejected.\nThe following definition is needed to introduce the defeasible provability.\nDefinition 5. A rule r ∈ Rsd is applicable in the proof condition for ±∂ iff ∀a ∈ A(r), +∂a ∈ P(1..n). A rule r is discarded in the condition for ±∂ iff ∃a ∈ A(r) such that −∂a ∈ P(1..n). +∂ : If P(n+ 1) = +∂q then (1)+∆q ∈ P(1..n) or (2) (2.1) −∆∼q ∈ P(1..n) and\n(2.2) ∃r ∈ Rsd[q] s.t. r is applicable, and (2.3) ∀s ∈ R[∼q]. either s is discarded, or\n(2.3.1) ∃t ∈ R[q] s.t. t is applicable and t > s. −∂ : If P(n+ 1) =−∂q then\n(1) −∆X q ∈ P(1..n) and either (2.1) +∆∼q ∈ P(1..n) or (2.2) ∀r ∈ Rsd[q]. either r is discarded, or (2.3) ∃s ∈ R[∼q] s.t. s is applicable, and\n(2.3.1) ∀t ∈ R[q]. either t is discarded, or t 6> s.\nTo show that q is defeasibly provable we have two choices: (1) We show that q is already definitely provable; or (2) we need to argue using the defeasible part of a theory D. For this second case, ∼q is not definitely provable (2.1), and there exists an applicable strict or defeasible rule for q (2.2). Every attack s is either discarded (2.3), or defeated by a stronger rule t (2.3.1). −∂X q is defined in an analogous manner and follows the principle of strong negation which is closely related to the function that simplifies a formula by moving all negations to an inner most position in the resulting formula, and replaces the positive tags with the respective negative tags, and the other way around (Antoniou et al. 2000)."
    }, {
      "heading" : "4.2 Agent Logic",
      "text" : "A defeasible agent theory is a standard defeasible theory enriched with 1) modes for rules, 2) modalities (belief, intention, obligation) for literals, and 3) relations for conversions and conflict resolution. We report below only the distinctive features. For a detailed exposition see (Governatori and Rotolo 2008).\nDefinition 6 (Language). Let PROP and Lit be a set of propositional atoms and literals as in Definition 1, MOD = {BEL, INT,OBL} be the set of modal operators, and Lblsoc be a set of labels. Define:\nModal literals\nModLit = {Xl|l ∈ Lit,X ∈ {OBL, INT}};\nRules r : φ1, . . . ,φn →֒X ψ ,\nwhere r ∈ Lblsoc is a unique label, A(r) = {φ1, . . . ,φn} ⊆ Lit∪ModLit is the antecedent of r, C(r) = ψ ∈ Lit is the consequent of r, →֒∈ {→,⇒,❀} is the type of r, and X ∈ MOD is the mode of r.\nRX (RX [q]) denotes all rules of mode X (with consequent q), and R[q] = ⋃\nX∈{BEL,OBL,INT} R X [q].\nObservation 1. Rules for intention and obligation are meant to introduce modalities: for example, if we have the intention rule r : a ⇒INT b and we derive a, then we obtain INTb. On the contrary, belief rules produce literals and not modal literals.\nRule conversion It is sometimes meaningful to use rules for a modality Y as they were for another modality X , i.e., to convert one mode of conclusions into a different one. Formally, we define the asymmetric binary convert relation Cv ⊆ MOD × MOD such that Cv(Y,X) means ‘a rule of mode Y can be used also to produce conclusions of mode X’. This corresponds to the following rewriting rule:\nXa1, . . . ,Xan A(r) = a1, . . . ,an ⇒Y b Xb Cv(Y,X)\nwhere A(r) 6= /0 and A(r)⊆ Lit.\nConflict-detection/resolution We define an asymmetric binary conflict relation Cf ⊆ MOD × MOD such that Cf(Y,X) means ‘modes Y and X are in conflict and mode Y prevails over X’.\nDefinition 7 (Defeasible Agent Theory). A defeasible agent theory is a structure\nDsoc = (Fsoc,R BEL,RINT,ROBL,>soc,V ,F )\nwhere\n• Fsoc ⊆ Lit∪ModLit is a finite set of facts; • RBEL, ROBL, RINT are three finite sets of rules for beliefs,\nobligations, and intentions; • The superiority (acyclic) relation >soc=>smsoc ∪> Cf soc such\nthat: i. >smsoc⊆ R X ×RX such that if r >soc s then r ∈ RX [p] and s ∈ RX [∼p]; and ii. >Cfsoc is such that ∀r ∈ R Y [p],∀s ∈\nRX [∼p] if Cf(Y,X) then r >Cfsoc s. • V = {Cv(BEL,OBL),Cv(BEL, INT)} is a set of convert\nrelations; • F = {Cf(BEL,OBL),Cf(BEL, INT),Cf(OBL, INT)} is a\nset of conflict relations.\nA proof is now a finite sequence of labelled formulas of the type +∆X q, −∆X q, +∂X q and −∂X q.\nThe following definition states the special status of belief rules, and that the introduction of a modal operator corresponds to being able to derive the associated literal using the rules for the modal operator.\nDefinition 8. Given # ∈ {∆,∂} and a proof P in D, q is #- provable in D if there is a line P(m) of P such that either\n1. q is a literal and P(m) = +#BELq, or 2. q is a modal literal X p and P(m) = +#X p, or 3. q is a modal literal ¬X p and P(m) =−#X p. Instead, q is #-rejected in D if\n4. q is a literal and P(m) =−#BELq or 5. q is a modal literal X p and P(m) =−#X p, or 6. q is a modal literal ¬X p and P(m) = +#X p.\nWe are now ready to report the definition of ∆X . +∆X : If P(n+ 1) = +∆X q then\n(1) q ∈ F if X = BEL or Xq ∈ F or (2) ∃r ∈ RXs [q] s.t. ∀a ∈ A(r). a is ∆-provable or (3) ∃r ∈ RYs [q] s.t. Cv(Y,X) ∈ C and\n∀a ∈ A(r). Xa is ∆-provable. −∆X : If P(n+ 1) =−∆X q then\n(1) q /∈ F if X = BEL and Xq /∈ F and (2) ∀r ∈ RXs [q]. ∃a ∈ A(r) s.t. a is ∆-rejected and (3) ∀r ∈ RYs [q]. if Cv(Y,X) ∈ C then\n∃a ∈ A(r) s.t. Xa is ∆-rejected. The sole difference with respect to +∆ is that now we may use rule of a different mode, namely Y , to derive conclusions of mode X through the conversion mechanism. In this framework, only belief rules may convert to other modes. That is the case, every antecedent of the belief rule r ∈ RY in clause (3) must be (definitely) proven with modality X .\nWe reformulate definition of being applicable/discarded, taking now into account also Cv and Cf relations. Definition 9. Given a proof P and X ,Y,Z ∈ MOD • A rule r is applicable in the proof condition for ±∂X iff\n1. r ∈ RX and ∀a ∈ A(r), a is ∂ -provable, or 2. r ∈RY , Cv(Y,X)∈C , and ∀a∈A(r), Xa is ∂ -provable.\n• A rule r is discarded in the condition for ±∂X iff 3. r ∈ RX and ∃a ∈ A(r) such that a is ∂ -rejected; or 4. r ∈ RY and, if Cv(Y,X), then ∃a ∈ A(r) such that Xa is\n∂ -rejected, or 5. r ∈ RZ and either ¬Cv(Z,X) or ¬Cf(Z,X).\nWe are now ready to provide proof conditions for ±∂X :\n+∂X : If P(n+ 1) = +∂X q then (1)+∆Xq ∈ P(1..n) or (2) (2.1) −∆X∼q ∈ P(1..n) and\n(2.2) ∃r ∈ Rsd [q] s.t. r is applicable, and (2.3) ∀s ∈ R[∼q] either s is discarded, or\n(2.3.1) ∃t ∈ R[q] s.t. t is applicable and t > s, and either t,s ∈ RZ , or Cv(Y,X) and t ∈ RY\n−∂X : If P(n+ 1) =−∂X q then (1) −∆Xq ∈ P(1..n) and either\n(2.1) +∆X∼q ∈ P(1..n) or (2.2) ∀r ∈ Rsd [q], either r is discarded, or (2.3) ∃s ∈ R[∼q], s.t. s is applicable, and\n(2.3.1) ∀t ∈ R[q] either t is discarded, or t 6> s, or t ∈ RZ,s ∈ RZ ′ , Z 6= Z′ and,\nif t ∈ RY then ¬Cv(Y,X).\nAgain, the only difference with respect to +∂ is that we have rules for different modes, and thus we have to ensure the appropriate relationships among the rules. Hence, clause (2.3.1) prescribes that either attack rule s and counterattack rule t have the same mode (i.e., s, t ∈ RZ), or that t can be used to produce a conclusion of the mode X (i.e., t ∈ RY and Cv(Y,X)). Notice that this last case is reported for the sake of completeness but it is useless in our framework since it plays a role only within theories with more than three modes.\nBeing the strong negation of the positive counterpart, −∂X q is defined in an analogous manner.\nWe define the extension of a defeasible theory as the set of all positive and negative conclusions. In (Maher 2001; Governatori and Rotolo 2008), authors proved that the extension calculus of a theory in both argumentation and agent logic is linear in the size of the theory.\nLet us introduce some preliminary notions, which are needed for formulating the “restoring sociality problem” (Governatori and Rotolo 2008) (and recalled below).\n• Given an agent defeasible theory D, a literal l is supported in D iff there exists a rule r ∈ R[l] such that r is applicable, otherwise l is not supported. For X ∈ MOD we use +ΣX l and −ΣX l to indicate that l is supported / not supported by rules for X .\n• Primitive intentions of an agent are those intentions given as facts in a theory.\n• Primary intentions and obligations are those derived using only rules for intentions and obligations (without any rule conversion).\n• A social agent is an agent for which obligation rules are stronger than any conflicting intention rules but weaker than any conflicting belief rules."
    }, {
      "heading" : "4.3 Restoring Sociality Problem",
      "text" : "INSTANCE: Let I be a finite set of primitive intentions, OBLp a primary obligation, and D a theory such that I ⊆ F , D ⊢ −∂OBLp, D ⊢ −ΣOBL∼p, D ⊢ +∂INT∼p, D ⊢ +ΣOBLp and D ⊢ −ΣBEL∼p. QUESTION: Is there a theory D′ equal to D apart from containing only a proper subset I′ of I instead of I, such that ∀q if D ⊢+∂OBLq then D′ ⊢ ∂OBLq and D′ ⊢+∂OBLp? Let us the consider the theory consisting of\nF = {INTp, INTs}\nR = {r1 : p,s ⇒BEL q r2 : ⇒OBL ∼q r3 : ⇒BEL s}\n>= {r1 > r2}\nr1 is a belief rule and so the rule is stronger than the obligation rule r2. In addition we have that the belief rule is not applicable (i.e., −ΣBELq) since there is no way to prove +∂BELp. There are no obligation rules for q, so −∂OBLq. However, rule r1 behaves as an intention rule since all its antecedent can be proved as intentions, i.e., +∂INTp and +∂INTs. Hence, since r1 is stronger than r2, the derivation of +∂OBL∼q is prevented against the sociality of the agent.\nThe related decision problem is whether it is possible to avoid the “deviant” behaviour by giving up some primitive intentions, retaining all the (primary) obligations, and maintaining a set of primitive intentions as close as possible to the original set of intentions.\nTheorem 10 ((Governatori and Rotolo 2008)). The Restoring Sociality Problem is NP-complete."
    }, {
      "heading" : "5 Dialogue Games",
      "text" : "The form of a dialogue game involves a sequence of interactions between two players, the Proponent Pr and the Opponent Op. The content of the dispute being that Pr attempts to assess the validity of a particular thesis (called critical literal within our framework), whereas Op attacks Pr’s claims in order to refute such thesis. We shift such position in our setting by stating that the Opponent has the burden of proof on the opposite thesis, and not just the duty to refute the Proponent’s thesis.\nThe challenge between the parties is formalised by means of argument exchange. In the majority of concrete instances of argumentation frameworks, arguments are defined as chains of reasoning based on facts and rules captured in some formal language (in our case, a defeasible derivation P). Each party adheres to a particular set of game rules as defined below.\nThe players partially shares knowledge of a defeasible theory. Each participant has a private knowledge regarding some rules of the theory. Other rules are known by both parties, but this set may be empty. These rules along with all the facts of the theory and the superiority relation represent the common knowledge of both participants.\nBy putting forward a private argument during a step of the game, the agent increases the common knowledge by the rules used within the argument just played.\nDefine the argument theory to be Darg = (F,R,>) such that i. R = RPr ∪ROp ∪ RCom, ii. RPr (ROp) is the private knowledge of the Proponent (Opponent), and iii. RCom is the (possibly empty) set of rules known by both participants. We use the superscript notation Diarg, R i Pr, R i Op, and R i Com to denote such sets at turn i. We assume that Darg is coherent and consistent, i.e., there is no literal p such that: i. Darg ⊢ ±∂ p, and ii. Darg ⊢+∂ p and Darg ⊢+∂∼p.\nWe now formalise the game rules, that is how the common theory Diarg is modified based on the move played at turn i.\nThe parties start the game by choosing the critical literal l to discuss about: the Proponent has the burden to prove +∂ l by using the current common knowledge along with a subset of RPr, whereas the Opponent’s final goal is to prove +∂∼l using ROp instead of RPr.\nThe players may not present arguments in parallel: they take turn in making their move.\nThe repertoire of moves at each turn just includes 1) putting forward an argument, and 2) passing.\nWhen putting forward an argument at turn i, the Proponent (Opponent) may bring a demonstration P whose terminal literal differs from l (∼l). When a player passes, she declares her defeat and the game ends. This happens when\nthere is no combination of the remaining private rules which proves her thesis.\nHence, the initial state of the game is T 0arg = (F,R 0 Com,>)\nwith R0Com = RCom, and R 0 Pr = RPr, R 0 Op = ROp.\nIf T 0arg ⊢ +∂ l, the Opponent starts the game. Otherwise, the Proponent does so.\nAt turn i, if Proponent plays Riarg, then\n• T i−1arg ⊢+∂∼l (T i−1arg ⊢ −∂ l if i = 1);\n• Riarg ⊆ R i−1 Pr ;\n• T iarg = (F,R i Com,>);\n• RiPr = R i−1 Pr \\R i arg, R i Op = R i−1 Op , and R i Com = R i−1 Com∪R i arg;\n• T iarg ⊢+∂ l.\nAt turn i, if Opponent plays Riarg, then\n• T i−1arg ⊢+∂ l;\n• Riarg ⊆ R i−1 Op ;\n• T iarg = (F,R i Com,>);\n• RiPr = R i−1 Pr , R i Op = R i−1 Op \\R i arg, and R i Com = R i−1 Com∪R i arg;\n• T iarg ⊢+∂∼l."
    }, {
      "heading" : "5.1 Strategic Argumentation Problem",
      "text" : "PROPONENT’S INSTANCE FOR TURN i: Let l be the critical literal, Ri−1Pr be the set of the private rules of the Proponent, and T i−1arg be such that either T i−1 arg ⊢ −∂ l if i = 1, or Di−1arg ⊢ +∂∼l otherwise. QUESTION: Is there a subset Riarg of R i−1 Pr such that D i arg ⊢ +∂ l? OPPONENT’S INSTANCE FOR TURN i: Let l be the critical literal, Ri−1Op be the set of the private rules of the Opponent, and Di−1arg be such that D i−1 arg ⊢+∂ l. QUESTION: Is there a subset Riarg of R i−1 Op such that D i arg ⊢ +∂∼l?"
    }, {
      "heading" : "6 Reduction",
      "text" : "We now show how to transform Agent Logic (Section 4.2) into Argumentation Logic (Section 4.1). Basically, we need to act by transforming both literals and rules: whereas the agent theory deals with three different modes of rules and modal literals, the argumentation theory has rules without modes and literals.\nThe two main ideas of transformations proposed in Definitions 11 and 12 are\n• Flatten all modal literals with respect to internal negations modalities. For instance, ∼p is flattened into the literal not p, while OBLq is obl q.\n• Remove modes from rules for BEL, OBL and INT. Thus, a rule with mode X and consequent p is transformed into a standard, non-modal rule with conclusion X p. An exception is when we deal with belief rules, given that they do not produce modal literals. Therefore, rule ⇒OBL p is translated in ⇒ obl p, while rule ⇒BEL q becomes ⇒ q.\nFunction pflat flattens the propositional part of a literal and syntactically represents negations; function flat flattens modalities.\nDefinition 11. Let Dsoc be a defeasible agent theory. Define two syntactic transformations pflat : Litsoc → PROParg and flat : ModLitsoc∪Litsoc → Litarg as\npflat(p) =\n{\np ∈ PROParg if p ∈ PROPsoc not q ∈ PROParg if p = ¬q, q ∈ PROPsoc\nflat(p) =\n\n     \n      pflat(q) if p = q, obl pflat(q) if p = OBLq ¬obl pflat(q) if p = ¬OBLq int pflat(q) if p = INTq ¬int pflat(q) if p = ¬INTq.\nGiven that in BIO a belief modal literal is not BELp but simply p, we have that flat(p) = pflat(p) whenever the considered mode is BEL, while flat(X p) = x pflat(p) if X = {OBL, INT}.\nWe need to redefine the concept of complement to map BIO modal literals into an argumentation logic with literals obtained through flat. Thus, if q ∈ PROParg is a literal p then ∼q is not p; and if q is not p, then ∼q is p. Moreover, if q ∈ Litarg is x pflat(p) then ∼q = x pflat(∼p); and q is ¬x pflat(p) then ∼q = x pflat(p).\nWe now propose a detailed description of facts and rules introduced by Definition 12.\nIn the “restoring sociality problem” we have to select a subset of factual intentions, while in the “strategic argumentation problem” we choose a subset of rules to play to defeat the opponent’s argument. Therefore, factual intentions are modelled as strict rules with empty antecedent (rp), while factual beliefs and obligations are facts of Darg.\nWe recall that, while proving±#X q, a rule in BIO may fire if either is of mode X , through Cv, or through Cf. Hence, a rule r in Dsoc has many counterparts in Darg.\nSpecifically, r f l is built from r by: removing the mode, and flattening each antecedent of r as well as the consequent p which in turn embeds the mode introduced by r.\nMoreover, if r ∈ RBEL[p] then it may be used through conversion to derive X p. To capture this feature we introduce a rule rCvx with conclusion x pflat(p) and where for each antecedent a ∈ A(r) the corresponding in A(rCvx) is x pflat(a) according either to clause (3) of +∆X or to condition 2. of Definition 9.\nIn Dsoc, it is easy to determine which rule may fire against one another, being that consequents of rules are non-modal literals. Even when the rules have different modes and the conflict mechanism is used, their conclusions are two complementary literals. Given the definition of complementary literals obtained through flat we have introduced after Definition 11, this is not the case for the literals in Darg. The situation is depicted in the following theory.\nr : a ⇒OBL p r f l : a ⇒ obl p s : b ⇒INT ¬p s f l : b ⇒ int not p t : c ⇒BEL p t f l : c ⇒ p.\nHere, r may fire against s through Cf(OBL, INT) while r f l cannot, given that obl p is not the complement of int not p. In the same fashion, if we derive +∂BELc then t may fire against s because of Cf(BEL, INT), while if we have either +∂OBLc or +∂INTc then the conflict between beliefs and intentions is activated by the use of r through either Cv(BEL,OBL) or Cv(BEL, INT), respectively. Nonetheless, in both cases there is no counterpart of t in Darg able to fire against int not p.\nTo obviate this issue, we introduce a defeater rC f OI where we flatten the antecedents of r and the conclusion is the intention of the conclusion of r, namely int pflat(C(r)). This means that when r fires, so does rC f OI attacking s f l . Notice that being rC f OI a defeater, such a rule cannot derive directly +∂ int pflat(p) but just prevents the opposite conclusion. The same idea is adopted for rules rC f belx and rCvyC f x: defeaters rC f belx are needed to model conflict between beliefs and intentions (as rule t in the previous example), whereas defeaters rCvyC f x take care of situations where r ∈ RZ may be used to convert Z into Y and Z prevails over X by Cf.\nThus in the previous example, we would have: rC f OI : a ❀ int p, tC f belint : c ❀ int p, tC f belint : c ❀ int p, tCvxC f int : x c ❀ int p, with x ∈ {obl, int}.\nAntecedents in BIO may be negation of modal literals; in that framework, a theory proves ¬X p if such theory rejects X p (as stated by condition 3. of Definition 8). In Darg we have to prove ¬x pflat(p) This is mapped in Darg through conditions 8–10 of Definition 12 and the last condition of >.\nDefinition 12. Let Dsoc = (Fsoc,RBEL,ROBL,RINT,>soc,V ,F ) be a defeasible agent theory. Define Darg = (F,R,>) an argumentation defeasible theory such that\nF = {flat(p)|p ∈ Fsoc, p ∈ Lit or p = OBLq} (1)\nR = {rp : → int pflat(p)|INTp ∈ Fsoc} (2)\n∪{r f l : ⋃\na∈A(r)\nflat(a) →֒ flat(p)|r ∈ RX [q],\nX = BEL and p = q, or p = Xq ∈ ModLit} (3)\n∪{rCvx : ⋃\na∈A(r)\nx pflat(a) →֒ x pflat(p)|r ∈ RBELsd [p],\nA(r) 6= /0,A(r)⊆ Lit,x ∈ {obl, int}} (4)\n∪{rCvyC f x : ⋃\ny pflat(a)∈A(rCvy)\ny pflat(a)❀ x pflat(p)|\nrCvy ∈ R[y pflat(p)],x,y ∈ {obl, int},x 6= y} (5)\n∪{rC f belx : ⋃\na∈A(r)\nflat(a)❀ x pflat(p)|r ∈ RBEL[p],\nx ∈ {obl, int}} (6)\n∪{rC f OI : ⋃\na∈A(r)\nflat(a)❀ int pflat(p)|r ∈ ROBL[p]} (7)\n∪{rdum−xp : x pflat(p)⇒ xp|r ∈ R Y .¬X p ∈ A(r)} (8)\n∪{rdum−negxp : ⇒∼xp|rdum−xp ∈ R} (9)\n∪{rneg−xp : ∼xp ⇒¬x pflat(p)|rdum−negxp ∈ R} (10)\n>= {(rα ,sβ )|(r,s) ∈>soc,α,β ∈ { f l,Cvx,CvxC f y, C f belx,C f OI}}\n∪{(r f l ,sneg−xp)|r f l ∈ R[x pflat(p)]}\n∪{(rdum−xp,sdum−negxp)|rdum−xp,sdum−negxp ∈ R}. (11)\nWe name Darg the argumentation counterpart of Dsoc.\nThe following result is meant to prove the correctness of the transformation given in Definition 12. This is the case when the transformation preserves the positive and negative provability for any given literal.\nTheorem 13. Let Dsoc = (Fsoc,RBEL,ROBL,RINT,>soc ,V ,F ) be a defeasible agent theory and Darg = (F,R,>) the argumentation counterpart of Dsoc. Given p ∈ Lit ∪ ModLit and # = {∆,∂}:\n1. Dsoc ⊢ ±#BELp iff Darg ⊢ ±#flat(p); 2. Dsoc ⊢ ±#X p iff Darg ⊢ ±#flat(X p), X ∈ {OBL, INT}.\nProof. The proof is by induction on the length of a derivation P. For the inductive base, we consider all possible derivations of length 1 for a given literal q. Given the proof tags’ specifications as in Definitions 11 and 12, the inductive base only takes into consideration derivations for ±∆, since to prove ±∂q requires at least 2 steps.\nP(1) = +∆Xq. This is possible either when clause (1), or (2) of +∆X in Dsoc holds.\nFor (1), we have either i. q ∈ Fsoc and X = BEL or OBLq ∈ Fsoc then flat(q) ∈ F or flat(OBLq) ∈ F by condition (1) of Definition 12; or ii. INTq ∈ Fsoc then there exists rq ∈ Rs[int pflat(q)], A(rq) = /0, by condition (2) of Definition 12. Cases i. and ii. as seen together state that either if\nX = BEL then Darg ⊢+∆flat(q), or Darg ⊢ +∆flat(Xq) otherwise, by clause (1), or (2) of +∆ in Darg.\nConcerning (2) of +∆X , there exists r ∈ RXs [q] such that A(r) = /0. Hence, if X = BEL then we have r f l ∈ Rs[flat(q)], otherwise we have r f l ∈ Rs[x pflat(q)] with x = {obl, int}, where both situations follow by condition (3) of Definition 12 and A(r f l) = /0. Thus, Darg ⊢ +∆flat(q) or Darg ⊢ +∆flat(Xq), respectively, by clause (2) of +∆ in Darg.\nP(1) = +∆flat(q). This is possible either when clause (1), or (2) of +∆ in Darg holds.\nFor (1), we have either pflat(q)∈F with q= p and p∈Lit, or obl pflat(p) ∈ F with q = OBLp; hence, by Definition 11 and condition (1) of Definition 12, we conclude that p∈ Fsoc or OBLp ∈ Fsoc, respectively. Thus, either Dsoc ⊢ +∆BELp or Dsoc ⊢+∆OBLp by clause (1) of +∆X in Dsoc.\nConcerning (2) of +∆X , we consider if either i. q = p and p ∈ Lit or q = OBLp, or ii. q = INTp.\nCase i., there exists r f l ∈ Rs[flat(p)], A(r f l) = /0. Therefore, there exists r ∈ RXs [p], with A(r) = /0 and X = {BEL,OBL}, by condition (3) of Definition 12. Thus, Dsoc ⊢ +∆X p by clause (2) of +∆X in Dsoc.\nCase ii., two possible situations arise: a) There exists r f l ∈ Rs[int pflat(p)], A(r f l) = /0, then there exists r ∈ RINTs [p], A(r) = /0, by condition (3) of Definition 12; or b) there exists rp ∈ Rs[int pflat(p)], then INTp ∈ Fsoc by condition (2) of Definition 12. For a) as well as for b), Dsoc ⊢ +∆INTp by clause (2) or (1), respectively, of +∆X in Dsoc.\nP(1) =−∆X q, P(1) =−∆flat(q). Both demonstrations are the same as and use the same ideas of cases P(1) = +∆X q or P(1) = +∆flat(q), respectively.\n(P(1) = −∆X q) Clause (1) and (2) of −∆X in Dsoc are satisfied. Thus, q 6∈ Fsoc (q ∈ Lit) or Xq 6∈ Fsoc and, consequently, neither flat(q) ∈ F nor obl pflat(q) ∈ F, and there is no rule rq that proves INTq. Moreover, for all r ∈ RXs [q] then A(r) 6= /0 and, accordingly, the same situation holds for all the corresponding rules of type r f l in Darg.\nThe same reasoning applies for the other direction.\nP(n+ 1) = +∆Xq. If q ∈ Fsoc and X = BEL, or Xq ∈ Fsoc and X = {OBL, INT}, then the case is the same as the corresponding inductive base.\nIf there exists r ∈ RXs [q] such that a is ∆-provable at P(n), for all a ∈ A(r), meaning that: a) There exists r f l : ⋃\na∈A(r) flat(a)→ flat(q) with X = BEL, or there exists r f l : ⋃\na∈A(r) flat(a) → x pflat(q) with x ∈ {obl, int} by condition (3) of Definition 12; and b) flat(a) is ∆-provable for all flat(a) ∈ A(r f l), by inductive hypothesis. Hence, Darg ⊢ +∆flat(q) or Darg ⊢ +∆flat(Xq), respectively, by clause (2) of +∆ in Darg.\nFinally, if clause (3) of +∆X in Dsoc is the case, then there exists r ∈ RYs [q] such that Cv(Y,X) ∈ V and Xa is ∆-provable at P(n), for all a ∈ A(r). Thus, there exists rCvx ∈ Rs[x pflat(q)] by condition (4) of Definition 12, and\nx pflat(a) is ∆-provable for all x pflat(a) ∈ A(rCvx), by inductive hypothesis. Again, Darg ⊢+∆flat(Xq) by clause (2) of +∆ in Darg.\nP(n+ 1) = +∆flat(q). This is possible either when clause (1), or (2) of +∆ in Darg holds.\nIf flat(q)∈ F, then the proof is the same as the corresponding inductive base.\nOtherwise, we consider if either i. q = p and p ∈ Lit, or ii. q = X p with X = {OBL, INT}.\nCase i., there exists r f l ∈ Rs[flat(p)], such that flat(a) is ∆- provable at P(n), for all flat(a)∈A(r f l). Therefore: a) There exists r ∈ RBELs [p] by condition (3) of Definition 12; and b) a is ∆-provable for all a ∈ A(r) by inductive hypothesis. Thus, Dsoc ⊢+∆BELp by clause (2) of +∆X in Dsoc.\nCase ii. is divided in two sub-cases. First sub-case, there exists r f l ∈ Rs[x pflat(p)] such that flat(a) is ∆-provable at P(n), for all flat(a) ∈ A(r f l). This case is analogous to the previous case. Second sub-case, there exists rCvx ∈ Rs[x pflat(p)], with x = {obl, int}, such that x pflat(a) is ∆- provable for all x pflat(a) ∈ A(rCvx). Therefore, the following two conditions are satisfied: a) There exists r ∈ RBELs [p] by condition (4) of Definition 12, and b) Xa is ∆-provable for all a ∈ A(r) by inductive hypothesis. Thus, Dsoc ⊢+∆X p by clause (3) of +∆X in Dsoc.\nP(n+ 1) =−∆Xq. Clauses (1)–(3) of −∆X in Dsoc hold. For (1), q 6∈ Fsoc (q ∈ Lit) or Xq 6∈ Fsoc. Consequently, neither flat(q) ∈ F nor obl pflat(q) ∈ F, and there is no rule rq to support int pflat(q).\nFor (2), for all r ∈ RXs [q] there exists a ∈ A(r) such that a is ∆-rejected at P(n). Accordingly, for all the corresponding rules of type r f l in Darg, there exists flat(a) ∈ A(r f l) which is ∆-rejected by inductive hypothesis. Hence, we conclude that Darg ⊢ −∆flat(q) if X = BEL.\nFinally, the same reasoning applies for all rules r ∈ RYs [q], with Cv(Y,X) ∈ V , where there exists a ∈ A(r) such that Xa is ∆-rejected at P(n). Thus, we conclude that Darg ⊢ −∆flat(Xq), with X = {OBL, INT}.\nP(n+1)=−∆flat(q). The proof follows the inductive base and the case P(n) =−∆X q.\nP(n+ 1) = +∂X q. Clauses (1) and (2.1) of +∂X have already been proved for the inductive step of ±∆X .\nIf clause (2.2) of +∂X is the case, then there exists r ∈ Rsd[q] such that r is applicable at P(n + 1) (i.e., a is ∂ - provable at P(n) in Dsoc, for all a ∈ A(r)) and either clause (2.3) or (2.3.1) is satisfied.\nWe have two cases. If r ∈ RX then there exists either r f l ∈Rsd[flat(q)] when X =BEL, or r f l ∈Rsd[x pflat(q)] otherwise by condition (3) of Definition 12. Thus, flat(a) is ∂ -provable at P(n) in Darg, for all flat(a) ∈ A(r f l) by inductive hypothesis. If r ∈ RY and X = {OBL, INT}, then there exists either rCvx ∈ Rsd[x flat(q)] by condition (4) of Definition 12. Hence, x pflat(a) is ∂ -provable at P(n) in Darg, for all flat(a) ∈ A(rCvx) by inductive hypothesis. We conclude\nthat clause (2.2) of +∂ holds in Darg by inductive hypothesis.\nFor, clause (2.3) if s ∈ R[∼q] is discarded, then we have the following cases.\na. s ∈ RX , then there exists a ∈ A(s) which is ∂ -rejected at P(n). Thus, flat(a) is ∂ -rejected at P(n) in Darg by inductive hypothesis, and therefore s f l is discarded in Darg.\nb. s ∈ RBEL and X ∈ {OBL, INT}, then there exists a ∈ A(s) such that Xa is ∂ -rejected at P(n). Hence, x pflat(a) is ∂ - rejected at P(n) in Darg by inductive hypothesis and we conclude that sCvx is discarded in Darg. c. X = BEL and s ∈ RZ with Z ∈ {OBL, INT}, or X = OBL and s ∈ RINT. We conclude that s f l is discarded because either X = BEL and s f l 6∈ R[∼flat(q)], or s f l 6∈ R[∼x pflat(q)] otherwise.\nFinally, we consider clause (2.3.1) of +∂X . Following the above reasoning, if t is applicable in Dsoc, then t f l or tCvx is applicable in Darg as well.\nIf t,s ∈ RZ and t >smsoc s, then tα > sα with α ∈ { f l,Cvz} by condition (11) of Definition 12. If X = BEL, there is no need for further analysis given that the transformation does not produce additional rules for pflat(q), for any literal q.\nOtherwise, we have either\ni. t ∈ RBEL[q] and s ∈ RX [∼q]: thus tCvx > s f l;\nii. t ∈ RBEL[q] and s ∈ RX [∼q]: thus tC f belx > s f l , with tC f belx : ⋃ flat(a)❀ x pflat(q);\niii. s, t ∈ RBEL: thus either a) tC f belx > sCvx, or b) tCvyC f x > sCvx, with tCvyC f x : ⋃ y pflat(a)❀ x pflat(q);\nby condition (11) and Cf(BEL,X) for i. and ii., t >smsoc s for the last case. It only remains to prove that tC f belx and tCvyC f x are applicable in Darg. If t is applicable in Dsoc at P(n+ 1), then any a ∈ A(t) is ∂ -provable in Dsoc at P(n) and so is flat(a) in Darg by inductive hypothesis. We conclude that tC f belx is applicable in Darg at P(n+ 1). Instead, if t is applicable in Dsoc at P(n+ 1) through Cv(BEL,Y ), then Ya is ∂ -provable in Dsoc at P(n) for every a ∈ A(t). By inductive hypothesis, any y pflat(a) ∈ A(tCvyC f x) is ∂ -provable as well. Hence, tCvyC f x is applicable in Darg as well.\nThis completes the analysis when sα with α ∈ { f l,Cvx}; we now analyse other possible attacks in Darg and first proceed for X = OBL, then for X = INT.\nSuppose there is a rule w ∈ RBEL[∼q]; w produces rules wC f belx and wCvyC f x. In the first case w would fire against Xq due to Cf(BEL,X). If w is discarded in Dsoc at P(n+1), then there exists a ∈ A(w) such that a is ∂ -rejected in Dsoc at P(n). By inductive hypothesis, we conclude that flat(a) ∈ A(wC f belx) is ∂ -rejected in Darg at P(n). Otherwise, w is defeated by an applicable t in Dsoc. Assume there is no t ∈ RBEL[q] stronger than w. Thus, Dsoc ⊢ −∂X q, against the hypothesis. Therefore, t >soc w and the corresponding of t in D is stronger than wC f belx by construction of > in Definition 12.\nAn analogous reasoning applies for wCvyC f x. Here, w would be applicable through Cv(BEL,Y ) and then fire against Xq by Cf(BEL,X). If w is discarded, then there\nexists a ∈ A(w) such that Ya is ∂ -rejected in Dsoc at P(n). By inductive hypothesis, y pflat(a) is ∂ -rejected in Darg and wCvyC f x is discarded at P(n+1). Otherwise, w is defeated in Dsoc by an applicable t ∈ RBEL[q] either directly, or through conversion. In both cases, the corresponding rule of t in Darg is stronger than wCvyC f x by construction of > in Definition 12. Notice that if w is applicable in Dsoc at P(n+ 1) then Ya is ∂ -provable at P(n) for any a ∈ A(w) and, consequently, so is y pflat(a) by inductive hypothesis, making wCvyC f x applicable in Darg.\nA final analysis is in order when X = INT and we consider wC f OI : ⋃\nflat(a) ❀ int pflat(∼q). The counterpart in Dsoc is w ∈ ROBL[∼q], which is either discarded, or defeated by a stronger rule t. Again, if w is discarded in Dsoc, then there exists a ∈ A(w) such that a is ∂ -rejected in Dsoc at P(n). By inductive hypothesis, flat(a) is ∂ -rejected in Darg and wC f OI is discarded at P(n+ 1). Otherwise w is defeated either by an applicable t ∈ ROBL[q], or t ∈ RBEL[q] (in this last case directly, or through Cf(BEL,OBL), or through Cf(BEL, INT)). This relation is preserved in > between wC f OI and the corresponding rule of t in Darg by condition (11) of Definition 12. If the t is in ROBL, then tC f OI ∈ R[int pflat(q)]. Stating that t is applicable in Dsoc at P(n+ 1) means that every antecedent a is ∂ -provable. By inductive hypothesis, so is the corresponding flat(a) in Darg, making tC f OI applicable at P(n+ 1).\nP(n+ 1) = +∂flat(q). Clauses (1) and (2.1) of +∂ have already been proved for the inductive base of ±∆.\nIf q = p and p ∈ Lit, then the only rules to consider as support/attack flat(p) are obtained through condition (3) of Definition 12. Therefore, by inductive hypothesis, for any applicable rule r f l the corresponding rule r in Dsoc is applicable as well, and the same reasoning holds for discarded rules. Moreover, the superiority relation is isomorphic for such rules. Hence, Dsoc ⊢+∂BELp.\nProofs that if a rule is applicable/discarded in Darg at P(n+1) then so is the corresponding rule in Dsoc at P(n+1), are analogous to the various cases studied for the inductive step of +∂X . Specifically, we use sα for rules captured by the quantifier in clause (2.3) of +∂ and tβ for those in the scope of the quantifier of clause (2.3.1).\nIt remains to argue that every applicable attack rule is defeated. By the construction of the superiority relation, this statement is straightforward for the rules which have a natural counterpart in Darg, i.e., t f l > s f l , tCvx > sCvx, tCvy > sCvx when Cf(Y,X) ∈ F , tCvy > sCvx.\nSuppose sα ∈ R[x pflat(∼p)], with α ∈ {CvyC f x,C f belx}. Such a rule is defeated by tCvx ∈ R[x pflat(p)], tCvyC f x ∈ R[x pflat(p)], or tC f belx ∈ R[x pflat(p)]. All these rule have the same counterpart rule, namely t ∈ RBEL[p], what changes is how t is made applicable to challenge s ∈ RBEL[∼p]. Again, due to construction of > in Definition 12 tβ > sα , β ∈ {Cvx,CvyC f x,C f belx}, are so because t >soc s.\nThe case when sC f OI ∈ R[x pflat(∼p)] differs from the previous one in that it can be defeated also by tC f OI . Once more we have that t >smsoc s by construction of >.\nAt last, we analyse the case when Darg ⊢+∂¬x pflat(p). The only rule that may fire to prove ¬x pflat(p) is rneg−xp, which is applicable whenever any rule rdum−xp is discarded at in Darg at P(n+ 1), due to conditions 8–10 and construction of > in Definition 12. That is the case if x pflat(p) is ∂ -rejected in Darg at P(n). By inductive hypothesis, Dsoc ⊢ −∂X p at P(n), thus, by Definition 8 clause 3, ¬X p is ∂ -provable in Dsoc at P(n+ 1).\nP(n+ 1) = −∂X q, −∂flat(q). The main reasoning follows straightforwardly from the case given that the proof conditions for −∂X and −∂ are the strong negation of +∂X and +∂ , respectively. Clauses (1) and (2.1) of −∂X (−∂ ) have already been proved in the inductive step of ±∆X (±∆), as well as clauses (2.2)-(2.3) in the inductive step of +∂X (+∂ ).\nBy construction of the superiority relation given in Definition 12, if t 6>soc s, then no superiority relation may exist between any transformations of t and s in Darg.\nFinally, concerning P(n+ 1) = −∂X , we must consider the case when ¬Xq is ∂ -provable in Dsoc at P(n+ 1). This is the case when clause 3. of Definition 8 is satisfied, i.e., when Dsoc ⊢ −∂X q at P(n). By inductive hypothesis, Darg ⊢−∂x pflat(q) at P(n), making rules rdum−xp discarded in Darg at P(n+1). Accordingly, Darg ⊢+∂∼xp at P(n+2) and we conclude that rules rneg−xp prove ¬x pflat(q) at P(n+ 3).\nIn order to show the final result that the Strategic Argumentation Problem is NP-Complete, we first prove that the proposed transformation is polynomial.\nTheorem 14. There is a linear transformation from any defeasible agent theory Tsoc to its argumentation counterpart Targ.\nProof. The transformation rules of Definition 12 are applied once to each rule and each tuple of the superiority relation. Transformation rule (1) maps one fact in Tsoc into one fact in Targ. Transformation rule (2) maps one primitive intention Tsoc into one strict rule in Targ. Rule (3) and (7) again copy one rule into one rule. Rules (4)–(6) generate two rules in Targ for every belief rule in Tsoc. Rules (8)–(10) generate a total of three rules in Targ for each negative modal literal in Tsoc. Rule (11) generates thirty-two tuples in Targ for each tuple in >soc and two tuples for each negative modal literal in in Tsoc.\nThe above reasoning shows that the transformation performs a number of steps that is, in the worst case, smaller than thirty-two times the size of the defeasible agent theory, and this proves the claim.\nTheorem 15. The Strategic Argumentation Problem is NPComplete.\nProof. First, the Strategic Argumentation Problem is polynomially solvable on non-deterministic machines since, given a defeasible argumentation theory Darg, we guess a set of rules Riarg and we can check the extension in polynomial time (Maher 2001).\nSecond, the Strategic Argumentation Problem is NPhard. In fact, we map the Restoring Sociality Problem\n(Governatori and Rotolo 2008) into the Strategic Argumentation Problem. Given a (deviant) defeasible agent theory Dsoc, Dsoc is mapped into its argumentation counterpart Darg (Definition 12). The transformation is polynomial (Theorem 14) and correct (Theorem 13)."
    }, {
      "heading" : "6.1 Discussion",
      "text" : "In this paper we concentrated in a game with a symmetry on what the two parties have to prove: Pr has to prove l (i.e., +∂ l) while Op has to prove ∼l (i.e., +∂∼l); however, it is possible to have games where the two parties have different burden on proof, namely, the proponent Pr has to prove l and the opponent Op has to disprove it. In Defeasible Logic this can be achieve either by proving that the opposite holds, namely +∂∼l or simply by showing that l is not provable, i.e., −∂ l. In this case we have two different types of strategic argumentation problems: one for the proponent (which is the same as the current one), and one for the opponent. For the opponent, the related decision problem is if there exists a subset of her private rules such that adding it to current public rule make that the resulting theory proves −∂ l. The proof conditions for +∂ and −∂ are the strong negation of each other (Antoniou et al. 2000); hence this version of the strategic argumentation problem is coNP-complete.\nThe NP-completeness result of the paper is proved for the ambiguity blocking, team defeat variant of Defeasible Logic. However, the proof of the result does not depend on the specific features of this particular variant of the logic, and the result extends to the other variants of the logic (see (Billington et al. 2010) for the definition of the various variants). The version of the argumentation logic presented in this paper does not correspond to the grounded semantics for Dung’s style abstract argumentation framework (though it is possible to give such a semantics for it, see (Governatori et al. 2004)). However, the ambiguity blocking variant corresponds to Dung’s grounded semantics (Governatori et al. 2004). Accordingly, strategic argumentation seems to be a computationally infeasible problem in general.\nFinally, in our game we chose that the superiority relation is known a priori by both players. If not so, the problem reduces to revising the corresponding Agent Logic by changing a combination of rules and superiority relation. The problem of revising a defeasible theory by only modifying the superiority relation has proven to be NP-complete in (Governatori et al. 2012)."
    }, {
      "heading" : "7 Summary",
      "text" : "Almost all research in AI on argumentation assumes that strategic dialogues are games of complete information, i.e., dialogues where the structure of the game is common knowledge among the players. Following (Okuno and Takahashi 2009; Satoh and Takahashi 2011) we argue that argument games work under incomplete information: each player does not know the other player’s knowledge, thus she cannot predict which arguments are attacked and which counterarguments are employed for attacking the arguments; hence, argument moves can disclose such private information, thus allowing the other player to attack.\nWhile it is outside the scope of this paper how to analyse strategic dialogues in game-theoretic terms, our research effort is preliminary to this analysis, since it studies the computation cost for logically characterising the problem that any argumentation game with incomplete information potentially rises. We have shown that the problem of deciding what set of rules to play (“Strategic Argumentation Problem”) at a given move is NP-complete even when the problem of deciding whether a given theory (defeasibly) entails a literal can be computed in polynomial time. To this end, we mapped the NP-complete “Restoring Sociality Problem” proposed in (Governatori and Rotolo 2008) into the strategic argumentation problem."
    } ],
    "references" : [ {
      "title" : "A family of defeasible reasoning logics and its implementation",
      "author" : [ "Antoniou" ],
      "venue" : "ECAI",
      "citeRegEx" : "Antoniou,? \\Q2000\\E",
      "shortCiteRegEx" : "Antoniou",
      "year" : 2000
    }, {
      "title" : "Representation results for defeasible logic",
      "author" : [ "Antoniou" ],
      "venue" : "ACM Transactions on Computational Logic 2(2):255–287",
      "citeRegEx" : "Antoniou,? \\Q2001\\E",
      "shortCiteRegEx" : "Antoniou",
      "year" : 2001
    }, {
      "title" : "An inclusion theorem for defeasible logics",
      "author" : [ "Billington" ],
      "venue" : "ACM Trans. Comput",
      "citeRegEx" : "Billington,? \\Q2010\\E",
      "shortCiteRegEx" : "Billington",
      "year" : 2010
    }, {
      "title" : "Coalitional bargaining with agent type uncertainty",
      "author" : [ "Chalkiadakis", "G. Boutilier 2007] Chalkiadakis", "C. Boutilier" ],
      "venue" : null,
      "citeRegEx" : "Chalkiadakis et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chalkiadakis et al\\.",
      "year" : 2007
    }, {
      "title" : "On the computational complexity of assumption-based argumentation for default reasoning",
      "author" : [ "Nebel Dimopoulos", "Y. Toni 2002] Dimopoulos", "B. Nebel", "F. Toni" ],
      "venue" : null,
      "citeRegEx" : "Dimopoulos et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Dimopoulos et al\\.",
      "year" : 2002
    }, {
      "title" : "Debates and decisions: On a rationale of argumentation rules. Games and Economic Behavior 36(2):158–173",
      "author" : [ "Glazer", "J. Rubinstein 2001] Glazer", "A. Rubinstein" ],
      "venue" : null,
      "citeRegEx" : "Glazer et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Glazer et al\\.",
      "year" : 2001
    }, {
      "title" : "Complexity of pure equilibria in bayesian games",
      "author" : [ "Greco Gottlob", "G. Mancini 2007] Gottlob", "G. Greco", "T. Mancini" ],
      "venue" : null,
      "citeRegEx" : "Gottlob et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gottlob et al\\.",
      "year" : 2007
    }, {
      "title" : "BIO logical agents: Norms, beliefs, intentions in defeasible logic",
      "author" : [ "Governatori", "G. Rotolo 2008] Governatori", "A. Rotolo" ],
      "venue" : "Journal of Autonomous Agents and Multi Agent Systems",
      "citeRegEx" : "Governatori et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Governatori et al\\.",
      "year" : 2008
    }, {
      "title" : "Argumentation semantics for defeasible logic",
      "author" : [ "Governatori" ],
      "venue" : "J. Log",
      "citeRegEx" : "Governatori,? \\Q2004\\E",
      "shortCiteRegEx" : "Governatori",
      "year" : 2004
    }, {
      "title" : "Revision of defeasible logic preferences",
      "author" : [ "Governatori" ],
      "venue" : "CoRR abs/1206.5833",
      "citeRegEx" : "Governatori,? \\Q2012\\E",
      "shortCiteRegEx" : "Governatori",
      "year" : 2012
    }, {
      "title" : "Audience-based uncertainty in abstract argument games",
      "author" : [ "Grossi", "D. van der Hoek 2013] Grossi", "W. van der Hoek" ],
      "venue" : "In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Grossi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Grossi et al\\.",
      "year" : 2013
    }, {
      "title" : "A gametheoretic measure of argument strength for abstract argumentation",
      "author" : [ "Matt", "P. Toni 2008] Matt", "F. Toni" ],
      "venue" : null,
      "citeRegEx" : "Matt et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Matt et al\\.",
      "year" : 2008
    }, {
      "title" : "Argumentation system with changes of an agent’s knowledge base",
      "author" : [ "Okuno", "K. Takahashi 2009] Okuno", "K. Takahashi" ],
      "venue" : null,
      "citeRegEx" : "Okuno et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Okuno et al\\.",
      "year" : 2009
    }, {
      "title" : "A Course in Game Theory",
      "author" : [ "Osborne", "M.J. Rubinstein 1999] Osborne", "A. Rubinstein" ],
      "venue" : null,
      "citeRegEx" : "Osborne et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Osborne et al\\.",
      "year" : 1999
    }, {
      "title" : "An abstract framework for argumentation with structured arguments",
      "author" : [ "H. Prakken" ],
      "venue" : "Argument & Computation",
      "citeRegEx" : "Prakken,? \\Q2010\\E",
      "shortCiteRegEx" : "Prakken",
      "year" : 2010
    }, {
      "title" : "Extensive-form argumentation games",
      "author" : [ "Procaccia", "A. Rosenschein 2005] Procaccia", "J. Rosenschein" ],
      "venue" : "EUMAS 2005,",
      "citeRegEx" : "Procaccia et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Procaccia et al\\.",
      "year" : 2005
    }, {
      "title" : "Argumentation and game theory",
      "author" : [ "Rahwan", "I. Larson 2009] Rahwan", "K. Larson" ],
      "venue" : "In Argumentation in Artificial Intelligence. Springer",
      "citeRegEx" : "Rahwan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rahwan et al\\.",
      "year" : 2009
    }, {
      "title" : "Heuristics in argumentation: A game theory investigation",
      "author" : [ "Riveret" ],
      "venue" : "COMMA, volume 172 of Frontiers in Artificial Intelligence and Applications,",
      "citeRegEx" : "Riveret,? \\Q2008\\E",
      "shortCiteRegEx" : "Riveret",
      "year" : 2008
    }, {
      "title" : "Probabilistic rule-based argumentation for norm-governed learning agents",
      "author" : [ "Rotolo Riveret", "R. Sartor 2012] Riveret", "A. Rotolo", "G. Sartor" ],
      "venue" : null,
      "citeRegEx" : "Riveret et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Riveret et al\\.",
      "year" : 2012
    }, {
      "title" : "Strategic argumentation: a game theoretical investigation",
      "author" : [ "Roth" ],
      "venue" : "In ICAIL ’07: Proceedings of the 11th International Conference on Artificial Intelligence and Law,",
      "citeRegEx" : "Roth,? \\Q2007\\E",
      "shortCiteRegEx" : "Roth",
      "year" : 2007
    }, {
      "title" : "A semantics of argumentation under incomplete information",
      "author" : [ "Satoh", "K. Takahashi 2011] Satoh", "K. Takahashi" ],
      "venue" : "In Proceedings of Jurisn",
      "citeRegEx" : "Satoh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Satoh et al\\.",
      "year" : 2011
    }, {
      "title" : "Dialogue games in defeasible logic",
      "author" : [ "Thakur" ],
      "venue" : "Australian Conference on Artificial Intelligence,",
      "citeRegEx" : "Thakur,? \\Q2007\\E",
      "shortCiteRegEx" : "Thakur",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "If we frame this intuition in proof-theoretic settings, such as in those developed in (Governatori et al. 2004; Prakken 2010; Toni 2013) where arguments are defined as inference trees formed by applying rules, exchanging arguments means exchanging logical theories (consisting of rules) proving conclusions.",
      "startOffset" : 86,
      "endOffset" : 136
    } ],
    "year" : 2013,
    "abstractText" : "In this paper we study the complexity of strategic argumentation for dialogue games. A dialogue game is a 2-player game where the parties play arguments. We show how to model dialogue games in a skeptical, non-monotonic formalism, and we show that the problem of deciding what move (set of rules) to play at each turn is an NP-complete problem.",
    "creator" : "LaTeX with hyperref package"
  }
}