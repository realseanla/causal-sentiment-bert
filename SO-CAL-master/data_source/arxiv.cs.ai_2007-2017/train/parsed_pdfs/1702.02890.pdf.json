{
  "name" : "1702.02890.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Answer Set Solving with Bounded Treewidth Revisited∗",
    "authors" : [ "Johannes K. Fichte", "Michael Morak", "Markus Hecher", "Stefan Woltran" ],
    "emails" : [ "lastname@dbai.tuwien.ac.at" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Parameterized algorithms [14, 5] have attracted considerable interest in recent years and allow to tackle hard problems by directly exploiting a small parameter of the input problem. One particular goal in this field is to find guarantees that the runtime is exponential exclusively in the parameter, and polynomial in the input size (so-called fixed-parameter tractable algorithms). A parameter that has been researched extensively is treewidth [16, 2]. Generally speaking, treewidth measures the closeness of a graph to a tree, based on the observation that problems on trees are often easier than on arbitrary graphs. A parameterized algorithm exploiting small treewidth takes a tree decomposition, which is an arrangement of a graph into a tree, and evaluates the problem in parts, via dynamic programming (DP) on the tree decomposition.\nASP [3, 13] is a logic-based declarative modelling language and problem solving framework where solutions, so called answer sets, of a given logic program directly represent the solutions of the modelled problem. Jakl et al. [11] give a DP algorithm for disjunctive rules only, whose runtime is linear in the input size of the program and double exponential in the treewidth of a particular graph representation of the program structure. However, modern ASP systems allow for an extended syntax that includes, among others, weight rules and choice rules. Pichler et al. [15] investigated the complexity of programs with weight rules. They also presented DP algorithms for programs with cardinality rules (i.e., restricted version of weight rules), but without disjunction.\nIn this paper, we propose DP algorithms for finding answer sets that are able to directly treat all kinds of ASP rules. While such rules can be transformed into disjunctive rules, we avoid the resulting polynomial overhead with our algorithms. In particular, we present two approaches based on two different types of graphs representing the program structure. Firstly, we consider the primal graph, which allows for an intuitive algorithm that also treats the extended ASP rules. While for a given disjunctive program the treewidth of the primal graph may be larger than treewidth of the graph representation used by\n∗This is the authors self-archived copy including detailed proofs. A preliminary version of the paper was presented on the workshop TAASP’16. Research was supported by the Austrian Science Fund (FWF), Grant Y698. †Also affiliated with the Institute of Computer Science and Computational Science at University of Potsdam, Germany.\nar X\niv :1\n70 2.\n02 89\n0v 1\n[ cs\n.L O\n] 9\nF eb\n2 01\nJakl et al. [11], our algorithm uses simpler data structures and lays the foundations to understand how we can handle also extended rules. Our second graph representation is the incidence graph, a generalization of the representation used by Jakl et al.. Algorithms for this graph representation are more sophisticated, since weight and choice rules can no longer be completely evaluated in the same computation step. Our algorithms yield upper bounds that are linear in the program size, double-exponential in the treewidth, and single-exponential in the maximum weights. We extend two algorithms to count optimal answer sets. For this particular task, experiments show that we are able to outperform existing systems from multiple domains, given input instances of low treewidth, both randomly generated and obtained from real-world graphs of traffic networks. Our system is publicly available on github1."
    }, {
      "heading" : "2 Formal Background",
      "text" : ""
    }, {
      "heading" : "2.1 Answer Set programming (ASP)",
      "text" : "ASP is a declarative modeling and problem solving framework; for a full introduction, see, e.g., [3, 13]. Stateof-the-art ASP grounders support the full ASP-Core-2 language [4] and output smodels input format [19], which we will use for our algorithms. Let `, m, n be non-negative integers such that ` ≤ m ≤ n, a1, . . ., an distinct propositional atoms, w, w1, . . ., wn non-negative integers, and l ∈ {a1,¬a1}. A choice rule is an expression of the form, {a1; . . . ; a`} ← a`+1, . . . , am,¬am+1, . . . ,¬an, a disjunctive rule is of the form a1 ∨ · · · ∨ a` ← a`+1, . . . , am,¬am+1, . . . ,¬an and a weight rule is of the form a` ← w 6 {a`+1 = w`+1, . . . , am = wm, ¬am+1 = wm+1, . . . ,¬an = wn}. Finally, an optimization rule is an expression of the form l[w]. A rule is either a disjunctive, a choice, a weight, or an optimization rule.\nFor a choice, disjunctive, or weight rule r, let Hr := {a1, . . . , a`}, B+r := {a`+1, . . . , am}, and B−r := {am+1, . . . , an}. For a weight rule r, let wght(r, a) map atom a to its corresponding weight wi in rule r if a = ai for `+ 1 ≤ i ≤ n and to 0 otherwise, let wght(r,A) := ∑ a∈A wght(r, a) for a set A of atoms, and let bnd(r) := w be its bound. For an optimization rule r, let cst(r) := w and if l = a1, let B + r := {a1} and B−r := ∅; or if l = ¬a1, let B−r := {a1} and B+r := ∅. For a rule r, let at(r) := Hr ∪B+r ∪B−r denote its atoms and Br := B + r ∪ {¬b | b ∈ B−r } its body. A program Π is a set of rules. Let at(Π) := {at(r) | r ∈ Π} and let CH(Π),DISJ(Π),OPT(Π) and WGT(Π) denote the set of all choice, disjunctive, optimization and weight rules in Π, respectively.\nA set M ⊆ at(Π) satisfies a rule r if (i) (Hr∪B−r )∩M 6= ∅ or B+r 6⊆M for r ∈ DISJ(Π), (ii) Hr∩M 6= ∅ or Σai∈M∩B+r wi + Σai∈B−r \\M wi < bnd(r) for r ∈WGT(Π), or (iii) r ∈ CH(Π) ∪OPT(Π). M is a model of Π, denoted by M Π, if M satisfies every rule r ∈ Π. Further, let Mod(C,Π) := {C | C ∈ C, C Π} for C ⊆ 2at(Π).\nThe reduct rM (i) of a choice rule r is the set {a ← B+r | a ∈ Hr ∩ M,B−r ∩ M = ∅} of rules, (ii) of a disjunctive rule r is the singleton {Hr ← B+r | B−r ∩ M = ∅}, and (iii) of a weight rule r is the singleton {Hr ← w′ 6 [a = wght(r, a) | a ∈ B+r ]} where w′ = bnd(r) − Σa∈B−r \\M wght(r, a). ΠM := {r′ | r′ ∈ rM , r ∈ Π} is called GL reduct of Π with respect to M . A set M ⊆ at(Π) is an answer set of program Π if (i) M Π and (ii) there is no M ′ (M such that M ′ ΠM , that is, M is subset minimal with respect to ΠM .\nWe call cst(Π,M,A) := Σr∈OPT(Π), A∩[(B+r ∩M)∪(B−r \\M)] 6=∅ cst(r) the cost of model M for Π with respect\nto the set A ⊆ at(Π). An answer set M of Π is optimal if its cost is minimal over all answer sets. 1See https://github.com/daajoe/dynasp.\nExample 1. Let Π := { r1︷ ︸︸ ︷ {a; b} ← c; r2︷ ︸︸ ︷ c← 1 6 {b = 1,¬a = 1}; r3︷ ︸︸ ︷\nd ∨ a←}. Then, the sets {a}, {c, d} and {b, c, d} are answer sets of Π.\nGiven a program Π, we consider the problems of computing an answer set (called AS) and outputting the number of optimal answer sets (called #AspO).\nNext, we show that under standard complexity-theoretic assumptions #Asp is strictly harder than #SAT.\nTheorem 1. #Asp for programs without optimization is #·coNP-complete.\nProof. Observe that programs containing choice and weight rules can be compiled to disjunctive ones (normalization) without these rule types (see [8]) using a polynomial number (in the original program size) of rules. Membership follows from the fact that, given such a nice program Π and an interpretation I, checking whether I is an answer of Π is coNP-complete, see e.g., [12]. Hardness is a direct consequence of #·coNP-hardness for the problem of counting subset minimal models of a CNF formula [6], since answer sets of negation-free programs and subset-minimal models of CNF formulas are essentially the same objects.\nRemark 1. The counting complexity of #Asp including optimization rules (i.e., where only optimal answer sets are counted) is slightly higher; exact results can be established employing hardness results from other sources [10]."
    }, {
      "heading" : "2.2 Tree Decompositions",
      "text" : "Let G = (V,E) be a graph, T = (N,F, n) a rooted tree, and χ : N → 2V a function that maps each node t ∈ N to a set of vertices. We call the sets χ(·) bags and N the set of nodes. Then, the pair T = (T, χ) is a tree decomposition (TD) of G if the following conditions hold: (i) all vertices occur in some bag, that is, for every vertex v ∈ V there is a node t ∈ N with v ∈ χ(t); (ii) all edges occur in some bag, that is, for every edge e ∈ E there is a node t ∈ N with e ⊆ χ(t); and (iii) the connectedness condition: for any three nodes t1, t2, t3 ∈ N , if t2 lies on the unique path from t1 to t3, then χ(t1) ∩ χ(t3) ⊆ χ(t2). We call max{|χ(t)| − 1 | t ∈ N} the width of the TD. The treewidth tw(G) of a graph G is the minimum width over all possible TDs of G.\nNote that each graph has a trivial TD (T, χ) consisting of the tree ({n}, ∅, n) and the mapping χ(n) = V . It is well known that the treewidth of a tree is 1, and a graph containing a clique of size k has at least treewidth k− 1. For some arbitrary but fixed integer k and a graph of treewidth at most k, we can compute a TD of width 6 k in time 2O(k\n3) · |V | [2]. Given a TD (T, χ) with T = (N, ·, ·), for a node t ∈ N we say that type(t) is leaf if t has no children; join if t has children t′ and t′′ with t′ 6= t′′ and χ(t) = χ(t′) = χ(t′′); int (“introduce”) if t has a single child t′, χ(t′) ⊆ χ(t) and |χ(t)| = |χ(t′)|+ 1; rem (“removal”) if t has a single child t′, χ(t) ⊆ χ(t′) and |χ(t′)| = |χ(t)| + 1. If every node t ∈ N has at most two children, type(t) ∈ {leaf, join, int, rem}, and bags of leaf nodes and the root are empty, then the TD is called nice. For every TD, we can compute a nice TD in linear time without increasing the width [2]. In our algorithms, we will traverse a TD bottom up, therefore, let post-order(T, t) be the sequence of nodes in post-order of the induced subtree T ′ = (N ′, ·, t) of T rooted at t.\nExample 2. Figure 1 (left) shows a graph G1 together with a TD of G1 that is of width 2. Note that G1 has treewidth 2, since it contains a clique on the vertices {a, b, c}. Further, the TD T in Figure 2 is a nice TD of G1.\nAlgorithm 1: Algorithm DPA(T ) for Dynamic Programming on TD T for ASP. In: Table algorithm A, nice TD T = (T, χ) with T = (N, ·, n) of G(Π) according to A. Out: Table: maps each TD node t ∈ T to some computed table τt.\n1 for iterate t in post-order(T,n) do 2 Child-Tabs := {Tables[t′] | t′ is a child of t in T} 3 Tables[t] := A(t, χ(t),Πt, at≤t,Child-Tabs)"
    }, {
      "heading" : "2.3 Graph Representations of Programs",
      "text" : "In order to use TDs for ASP solving, we need dedicated graph representations of ASP programs. The primal graph P (Π) of program Π has the atoms of Π as vertices and an edge a b if there exists a rule r ∈ Π and a, b ∈ at(r). The incidence graph I(Π) of Π is the bipartite graph that has the atoms and rules of Π as vertices and an edge a r if a ∈ at(r) for some rule r ∈ Π. These definitions adapt similar concepts from SAT [17].\nExample 3. Recall program Π of Example 1. We observe that graph G1 (G2) in the left (right) part of Figure 1 is the primal (incidence) graph of Π."
    }, {
      "heading" : "2.4 Sub-Programs",
      "text" : "Let T = (T, χ) be a nice TD of graph representation H ∈ {I(Π), P (Π)} of a program Π. Further, let T = (N, ·, n) and t ∈ N . The bag-rules are defined as Πt := {r | r ∈ Π, at(r) ⊆ χ(t)} if H is the primal graph and as Πt := Π ∩ χ(t) if H is the incidence graph. Further, the set at≤t := {a | a ∈ at(Π) ∩ χ(t′), t′ ∈ post-order(T, t)} is called atoms below t, the program below t is defined as Π≤t := {r | r ∈ Πt′ , t′ ∈ post-order(T, t)}, and the program strictly below t is Π<t := Π≤t \\ Πt. It holds that Π≤n = Π<n = Π and at≤n = at(Π).\nExample 4. Intuitively, TDs of Figure 1 enable us to evaluate Π by analyzing sub-programs ({r1, r2} and {r3}) and combining results agreeing on a. Indeed, for the given TD of Figure 1 (left), Π≤t1 = {r1, r2}, Π≤t2 = {r3} and Π = Π≤t3 = Π<t3 = Πt1 ∪Πt2 . For the TD of Figure 1 (right), we have Π≤t1 = {r1, r2} and at≤t1 = {b, c}, as well as Π≤t3 = {r3} and at≤t3 = {a, d}. Moreover, for TD T of Figure 2, Π≤t1 = Π≤t2 = Π≤t3 = Π<t4 = ∅, at≤t3 = {a, b} and Π≤t4 = {r1, r2}."
    }, {
      "heading" : "3 ASP via Dynamic Programming on TDs",
      "text" : "In the next two sections, we propose two dynamic programming (DP) algorithms, DPPRIM and DP INC, for ASP without optimization rules based on two different graph representations, namely the primal and the incidence graph. Both algorithms make use of the fact that answer sets of a given program Π are (i) models of Π and (ii) subset minimal with respect to ΠM . Intuitively, our algorithms compute, for each TD node t, (i) sets of atoms—(local) witnesses—representing parts of potential models of Π, and (ii) for each local witness M subsets of M—(local) counterwitnesses—representing subsets of potential models of ΠM which (locally) contradict that M can be extended to an answer set of Π. We give the the basis of our algorithms in Algorithm 1 (DPA), which sketches the general DP scheme for ASP solving on TDs. Roughly, the algorithm splits the search space based on a given nice TD and evaluates the input program Π in parts. The results are stored in so-called tables, that is, sets of all possible tuples of witnesses and counterwitnesses for a given TD node. To this end, we define the table algorithms PRIM and INC, which compute tables for a node t of the TD using the primal graph P (Π) and incidence graph I(Π), respectively. To be more concrete, given a table algorithm A ∈ {PRIM, INC}, algorithm DPA visits every node t ∈ T in post-order; then, based on Πt, computes a table τt for node t from the tables of the children of t, and\nAlgorithm 2: Table algorithm PRIM(t, χt,Πt, ·,Child-Tabs). In: Bag χt, bag-rules Πt and child tables Child-Tabs of node t. Out: Table τt.\n1 if type(t) = leaf then τt := {〈∅, ∅〉} /* Abbreviations see Footnote 2. */ 2 else if type(t) = int, a ∈ χt is introduced and τ ′ ∈ Child-Tabs then 3 τt := {〈M+a , Mod({M} ∪ [C t {a}] ∪ C,Π M+a t )〉 | 〈M, C〉 ∈ τ ′,M+a Πt} ⋃ 4 {〈M, Mod(C,ΠMt )〉 | 〈M, C〉 ∈ τ ′,M Πt} 5 else if type(t) = rem, a 6∈ χt is removed and τ ′ ∈ Child-Tabs then 6 τt := {〈M−a , {C−a | C ∈ C}〉 | 〈M, C〉 ∈ τ ′} 7 else if type(t) = join and τ ′, τ ′′ ∈ Child-Tabs with τ ′ 6= τ ′′ then 8 τt := {〈M, (C′ ∩ C′′) ∪ (C′ ∩ {M}) ∪ ({M} ∩ C′′)〉 | 〈M, C′〉 ∈ τ ′, 〈M, C′′〉 ∈ τ ′′}\nstores τt in Tables[t]."
    }, {
      "heading" : "3.1 Using Decompositions of Primal Graphs",
      "text" : "In this section, we present our algorithm PRIM in two parts: (i) finding models of Π and (ii) finding models which are subset minimal with respect to ΠM . For sake of clarity, we first present only the first tuple positions (red parts) of Algorithm 2 (PRIM) to solve (i). We call the resulting table algorithm MOD.\nExample 5. Consider program Π from Example 1 and in Figure 2 (left) TD T = (·, χ) of P (Π) and the tables τ1, . . ., τ12, which illustrate computation results obtained during post-order traversal of T by DPMOD. Table τ1 = {〈∅〉} as type(t1) = leaf. Since type(t2) = int, we construct table τ2 from τ1 by taking M1.i and M1.i ∪ {a} for each M1.i ∈ τ1 (corresponding to a guess on a). Then, t3 introduces b and t4 introduces c. Πt1 = Πt2 = Πt3 = ∅, but since χ(t4) ⊆ at(r1) ∪ at(r2) we have Πt4 = {r1, r2} for t4. In consequence, for each M4.i of table τ4, we have M4.i {r1, r2} since MOD enforces satisfiability of Πt in node t. We derive tables τ7 to τ9 similarly. Since type(t5) = rem, we remove atom b from all elements in τ4 to construct τ5. Note that we have already seen all rules where b occurs and hence b can no longer affect witnesses during the remaining traversal. We similarly construct τt6 = τ10 = {〈∅〉, 〈a〉}. Since type(t11) = join, we construct table τ11 by taking the intersection τ6 ∩ τ10. Intuitively, this combines witnesses agreeing on a. Node t12 is again of type rem. By definition (primal graph and TDs) for every r ∈ Π, atoms at(r) occur together in at least one common bag. Hence, Π = Π≤t12 and since τ12 = {〈∅〉}, we can construct a model of Π from the tables. For example, we obtain the model {a, d} = M11.2 ∪M4.2 ∪M9.3.\nObservation 1. Let Π be a program and T a TD of the primal graph of Π. Then, for every rule r ∈ Π there is at least one bag in T containing all atoms of r.\nProof. By Definition the primal graph contains a clique on all atoms a participating in a rule r. Since a TD must contain each edge of the original graph in some bag and has to be connected, it follows that there is at least one bag containing all (clique) atoms a of r.\nPRIM is given in Algorithm 2. Tuples in τt are of the form 〈M, C〉. Witness M ⊆ χ(t) represents a model of Πt witnessing the existence of M\n′ ⊇M with M ′ Π≤t. The family C ⊆ 2M contains sets of models C ⊆M of the GL reduct (Πt)\nM . C witnesses the existence of a set C ′ with counterwitness C ⊆ C ′ ( M ′ and C ′ (Π≤t)M ′ . There is an answer set of Π if table tn for root n contains 〈∅, ∅〉. Since in Example 5 we already explained the first tuple position and thus the witness part, we only briefly describe the parts for counterwitnesses. In the introduce case, we want to store only counterwitnesses for not being minimal with respect to the GL reduct of the bag-rules. Therefore, in Line 3 we construct for M+a counterwitnesses from either some witness M (M ( M+a ), or of any C ∈ C, or of any C ∈ C extended by a (every C ∈ C was already a counterwitness before). Line 4 ensures that only counterwitnesses that are models of the\n2 S t {e} := {S ∪ {e} | S ∈ S}, S+e := S ∪ {e}, and S−e := S \\ {e}\nGL reduct ΠMt are stored (via Mod(·, ·)). Line 6 restricts counterwitnesses to its bag content, and Line 8 enforces that child tuples agree on counterwitnesses.\nExample 6. Consider Example 5, its TD T = (·, χ), Figure 2 (right), and the tables τ1, . . ., τ12 obtained by DPPRIM. Since we have at(r1) ∪ at(r2) ⊆ χ(t4), we require C4.i.j {r1, r2}M4.i for each counterwitness C4.i.j ∈ C4.i in tuples of τ4. For M4.5 = {a, b, c} observe that the only counterwitness of {r1, r2}M4.5 = {a← c, b← c, c← 1 ≤ {b = 1}} is C4.5.1 = {a}. Note that witness M11.2 of table τ11 is the result of joining M4.2 with M9.1 and witness M11.3 (counterwitness C11.3.1) is the result of joining M4.3 with M9.3 (C4.3.1 with C9.3.1), and M4.5 with M9.3 (C4.5.1 with C9.3.2). C11.3.1 witnesses that neither M4.3∪M9.3 nor M4.5 ∪M9.3 forms an answer set of Π. Since τ12 contains 〈∅, ∅〉 there is no counterwitness for M11.2, we can construct an answer set of Π from the tables, e.g., {a} can be constructed from M4.2 ∪M9.1.\nTheorem 2. Given a program Π, the algorithm DPPRIM is correct and runs in time O(22 k+2 · ‖P (Π)‖) where k is the treewidth of the primal graph P (Π).\nProof. We refer to Appendix B.1."
    }, {
      "heading" : "3.2 Using Decompositions of Incidence Graphs",
      "text" : "Our next algorithm (DP INC) takes the incidence graph as graph representation of the input program. The treewidth of the incidence graph is smaller than the treewidth of the primal graph plus one, cf., [17, 7]. More importantly, the incidence graph does not enforce cliques on at(r) for some rule r. The incidence graph, compared to the primal graph, additionally contains rules as vertices and its relationship to the atoms in terms of edges. By definition, we have no guarantee that all atoms of a rule occur together in the same bag of TDs of the incidence graph. For that reason, we cannot locally check the satisfiability of a rule when traversing the TD without additional stored information (so-called rule-states that intuitively represent how much of a rule is already (dis-)satisfied). We only know that for each rule r there is a path p = tint, t1, . . . , tm, trem where tint introduces r and trem removes r and when considering trem in the table algorithm we have seen all atoms that occur in rule r. Thus, on removal of r in trem we ensure that r is satisfied while taking rule-states for choice and weight rules into account. Consequently, our tuples will contain a witness, its rule-state, and counterwitnesses and their rule-states.\nA tuple in τt for Algorithm 3 (INC) is a triple 〈M,σ, C〉. The set M ⊆ at(Π) ∩ χ(t) represents again a witness. A rule-state σ is a mapping σ : Πt → N0∪{∞}. A rule state for M represents whether rules of χ(t) are either (i) satisfied by a superset of M or (ii) undecided for M . Formally, the set SR(Πt, σ) of satisfied bag-rules Πt consists of each rule r ∈ Πt such that σ(r) =∞. Hence, M witnesses a model M ′ ⊇M where M ′ Π<t ∪ SR(Πt, σ). C concerns counterwitnesses.\n3σ ] ρ := {(x,Σ(x,c1)∈σc1 + Σ(x,c2)∈ρc2) | (x, ·) ∈ σ ∪ ρ}; σ + r := σ ∪ {(r, 0)}; σ−S := {(x, y) ∈ σ | x 6∈ S}.\nAlgorithm 3: Table algorithm INC(t, χt,Πt, at≤t,Child-Tabs).\nIn: Bag χt, bag-rules Πt, atoms-below at≤t, child tables Child-Tabs of t. Out: Tab. τt. 1 if type(t) = leaf then τt := {〈∅, ∅, ∅〉} /* Abbreviations see Footnote 3. */ 2 else if type(t) = int, a ∈ χt \\Πt is introduced and τ ′ ∈ Child-Tabs then 3 τt := {〈M+a , σ ] SatRules(Π̇(t,σ)t ,M+a ), {〈M,σ ] SatRules(Π̇ (t,σ,M+a ) t ,M)〉} ∪ 4 {〈C+a , ρ] SatRules(Π̇ (t,ρ,M+a ) t , C + a )〉 | 〈C, ρ〉 ∈ C} ∪ 5 {〈C, ρ] SatRules(Π̇(t,ρ,M + a ) t , C)〉 | 〈C, ρ〉 ∈ C}〉 | 〈M,σ, C〉 ∈ τ ′} ⋃ 6 {〈M,σ ] SatRules(Π̇(t,σ)t ,M), 7 {〈C, ρ] SatRules(Π̇(t,ρ,M)t , C)〉 | 〈C, ρ〉 ∈ C}〉 | 〈M,σ, C〉 ∈ τ ′} 8 else if type(t) = int, r ∈ χt ∩Πt is introduced and τ ′ ∈ Child-Tabs then 9 τt := {〈M,σ+r ] SatRules({ṙ}(t,σ + r }),M),\n10 {〈C, ρ+r ] SatRules({ṙ}(t,ρ + r ,M), C)〉 | 〈C, ρ〉 ∈ C}〉 | 〈M,σ, C〉 ∈ τ ′} 11 else if type(t) = rem, a 6∈ χt is removed atom and τ ′ ∈ Child-Tabs then 12 τt := {〈M−a , σ ] UpdtWgt(Πt,M, a), 13 {〈C−a , ρ] UpdtWgt&Ch(Πt,M,C, a)〉 | 〈C, ρ〉 ∈ C}〉 | 〈M,σ, C〉 ∈ τ ′} 14 else if type(t) = rem, r 6∈ χt is removed rule and τ ′ ∈ Child-Tabs then 15 τt := {〈M,σ−{r}, { 〈C, ρ−{r}〉 | 〈C, ρ〉 ∈ C, ρ(r) =∞ } 〉 | 〈M,σ, C〉 ∈ τ ′, σ(r) =∞} 16 else if type(t) = join and τ ′, τ ′′ ∈ Child-Tabs with τ ′ 6= τ ′′ then 17 τt := {〈M,σ′ ] σ′′, {〈C, ρ′ ] ρ′′〉 | 〈C, ρ′〉 ∈ C′, 〈C, ρ′′〉 ∈ C′′} ∪ 18 {〈M,ρ] σ′′〉 | 〈M,ρ〉 ∈ C′} ∪ 19 {〈M,σ′ ] ρ〉 | 〈M,ρ〉 ∈ C′′}〉 | 〈M,σ′, C′〉 ∈ τ ′, 〈M,σ′′, C′′〉 ∈ τ ′′}\nWe compute a new rule-state σ from a rule-state, “updated” bounds for weight rules (UpdtWgt), and satisfied rules (SatRules, defined below). We define UpdtWgt(Πt,M, a) := σ\n′ depending on an atom a with σ′(r) := wght(r, {a} ∩ [(B−r \\M) ∪ (B+r ∩M)]), if r ∈ WGT(Πt). We use binary operator ]3 to combine rule-states, which ensures that rules satisfied in at least one operand remain satisfied. Next, we explain the meaning of rule-states.\nExample 7. Consider program Π from Example 1 and TD T ′ = (·, χ) of I(Π) and the tables τ1, . . ., τ18 in Figure 3 (left). We are only interested in the first two tuple positions (red and green parts) and implicitly assume that “i” refers to Line i in the respective table. Consider M4.1 = {c} in table τ4. Since Hr2 = {c}, witness M4.1 = {c} satisfies rule r2. As a result, σ4.1(r2) = ∞ remembering satisfied rule r2 for M4.1. Since c /∈ M4.2 and B+r1 = {c}, M4.2 satisfies rule r1, resulting in σ4.2(r1) = ∞. Rule-state σ4.1(r1) represents that r1 is undecided for M4.2. For weight rule r2, rule-states remember the sum of body weights involving removed atoms. Consider M6.2 = M6.3 = ∅ of table τ6. We have σ6.2(r2) 6= σ6.3(r2), because M6.2 was obtained from some M5.i of table τ5 with b 6∈M5.i and b occurs in B+r2 with weight 1, resulting in σ6.3(r2) = 1; whereas M6.3 extends some M5.j with b /∈M5.j.\nIn order to decide in node t whether a witness satisfies rule r ∈ Πt, we check satisfiability of program Ṙ(r) constructed by Ṙ, which maps rules to state-programs. Formally, for M ⊆ χ(t) \\Πt, SatRules(Ṙ,M) := σ where σ(r) :=∞ if (r,R) ∈ Ṙ and M R.\nDefinition 1. Let Π be a program, T = (·, χ) be a TD of I(Π), t be a node of T , P ⊆ Πt, and σ : Πt → N0 ∪ {∞} be a rule-state. The state-program P(t,σ) is obtained from P ∪ {← Br | r ∈ CH(P), Hr ( at≤t}4 by\n1. removing rules r with σ(r) =∞ (“already satisfied rules”);\n2. removing from every rule all literals a,¬a with a 6∈ χ(t); and 4We require to add {← Br | r ∈ CH(P), Hr ( at≤t} in order to decide satisfiability for corner cases of choice rules involving\ncounterwitnesses of Line 3 in Algorithm 3.\n3. setting new bound max{0,bnd(r)− σ(r)−wght(r, at(r) \\ at≤t)} for weight rule r.\nWe define Ṗ(t,σ) : P → 2P(t,σ) by Ṗ(t,σ)(r) := {r}(t,σ) for r ∈ P.\nExample 8. Observe Π (t1,∅) t1 = {{b} ← c,← c, c← 0 ≤ {b = 1}} and Π (t2,∅) t2 = {{a} ←,← 1 ≤ {¬a = 1}} for Πt1 , Πt2 of Figure 1(right).\nThe following example provides an idea how we compute models of a given program using the incidence graph. The resulting algorithm IMOD is the same as INC, except that only the first two tuple positions (red and green parts) are considered.\nExample 9. Again, we consider Π of Example 1 and in Figure 3 (left) T ′ as well as tables τ1, . . ., τ18. Table τ1 = {〈∅, ∅〉} as type(t1) = leaf. Since type(t2) = int and t2 introduces atom c, we construct τ2 from τ1 by taking M2.1 := M1.1∪{c} and M2.2 := M1.1 as well as rule-state ∅. Because type(t3) = int and t3 introduces rule r1, we consider state program L3 := {r1}(t3,{(r1,0)}) = {← c} for SatRules(L̇3,M2.1) = {(r1, 0)} as well as SatRules(L̇3,M2.2) = {(r1,∞)} (according to Line 9 of Algorithm 3). Because type(t4) = int and t4 introduces rule r2, we consider M3.1 := M2.1 and M3.2 := M2.2 and state program L4 := {r2}(t4,{(r2,0)}) = {c ← 0 6 {}} = {c←} for SatRules(L̇4,M3.1) = {(r2,∞)} as well as SatRules(L̇4,M3.2) = {(r2, 0)} (see Line 9). Node t5 introduces b (table not shown) and node t6 removes b. Table τ6 was discussed in Example 7. When we remove b in t6 we have decided the “influence” of b on the satisfiability of r1 and r2 and thus all rules where b occurs. Tables τ7 and τ8 can be derived similarly. Then, t9 removes rule r2 and we ensure that every witness M9.1 can be extended to a model of r2, i.e., witness candidates for τ9 are M8.i with σ8.i(r2) = ∞. The remaining tables are derived similarly. For example, table τ17 for join node t17 is derived analogously to table τ17 for algorithm PRIM in Figure 2, but, in addition, also combines the rule-states as specified in Algorithm 3.\nSince we already explained how to obtain models, we only briefly describe how we handle the counterwitness part. Family C consists of tuples (C, ρ) where C ⊆ at(Π) ∩ χ(t) is a counterwitness in t to M . Similar to the rule-state σ the rule-state ρ for C under M represents whether rules of the GL reduct ΠMt are either (i) satisfied by a superset of C or (ii) undecided for C. Thus, C witnesses the existence of C ′ (M ′ satisfying C ′ (Π<t ∪ SR(Πt, ρ))M ′ since M witnesses a model M ′ ⊇M where M ′ Π<t ∪ SR(Πt, ρ). In consequence, there exists an answer set of Π if the root table contains 〈∅, ∅, ∅〉. In order to locally decide rule satisfiability for counterwitnesses, we require state-programs under witnesses.\nAlgorithm 4: Algorithm #OINC(t, χt,Πt, at≤t,Child-Tabs).\nIn: Bag χt, bag-rules Πt, atoms-below at≤t, child tables Child-Tabs of t. Out: Tab. τt. /* For 〈M,σ, C, c, n〉, we only state affected parts (cost c and count n); ‘‘. . . ’’ indicates\ncomputation as before. * . . . + denotes a multiset. */ 1 if type(t) = leaf then τt := {〈∅, . . . , 0, 1〉} 2 else if type(t) = int, a ∈ χt \\Πt is introduced and τ ′ ∈ Child-Tabs then 3 τt := {〈M, . . . , cst(Π, ∅, {a}) + c, n〉 | 〈M,σ, C, c, n〉 ∈ τ ′}\n⋃ 4 {〈M+a , . . . , cst(Π, {a}, {a}) + c, n〉 | 〈M,σ, C, c, n〉 ∈ τ ′} 5 else if type(t) = int or rem, removed or introduced r ∈ Πt, τ ′ ∈ Child-Tabs then 6 τt := {〈M, . . . , c, n〉 | 〈M,σ, C, c, n〉 ∈ τ ′, . . .} 7 else if type(t) = rem, a /∈ χt is removed atom and τ ′ ∈ Child-Tabs then 8 τt := cnt(kmin(*〈M−a , . . . , c, n〉 | 〈M,σ, C, c, n〉 ∈ τ ′+)) 9 else if type(t) = join and τ ′, τ ′′ ∈ Child-Tabs with τ ′ 6= τ ′′ then 10 τt := cnt(kmin(*〈M, . . . , c′ + c′′ − cst(Π,M, χt), n′ · n′′〉 11 | 〈M,σ′, C′, c′, n′〉 ∈ τ ′, 〈M,σ′′, C′′, c′′, n′′〉 ∈ τ ′′+))\nDefinition 2. Let Π be a program, T = (·, χ) be a TD of I(Π), t be a node of T , P ⊆ Πt, ρ : Πt → N0∪{∞} be a rule-state and M ⊆ at(Π). We define state-program P(t,ρ,M) by [S(t,ρ)]M where S := P ∪ {← Br | r ∈ CH(P), ρ(r) > 0}, and Ṗ(t,ρ,M) : P → 2P(t,ρ,M) by Ṗ(t,ρ,M)(r) := {r}(t,ρ,M) for r ∈ P.\nWe compute a new rule-state ρ for a counterwitness from an earlier rule-state, satisfied rules (SatRules), and both (a) “updated” bounds for weight rules or (b) “updated” value representing whether the head can still be satisfied (ρ(r) ≤ 0) for choice rules r (UpdtWgt&Ch). Formally, UpdtWgt&Ch(Πt,M,C, a) := σ′ depending on an atom a with (a) σ′(r) := wght(r, {a} ∩ [(B−r \\M) ∪ (B+r ∩ C)]), if r ∈ WGT(Πt); and (b) |{a} ∩Hr ∩ (M \\ C)|, if r ∈ CH(Πt).\nTheorem 3. The algorithm DP INC is correct.\nProof. (Idea) A tuple at a node t guarantees that there exists a model for the ASP sub-program induced by the subtree rooted at t. Since this can be done for each node type, we obtain soundness. Completeness follows from the fact that while traversing the tree decomposition every answer set is indeed considered. The full proof is rather tedious as each node type needs to be investigated separately. For more details, we refer the reader to Appendix B.2.\nTheorem 4. Given a program Π, algorithm DP INC runs in time O(22 k+2·`k+1 ·‖I(Π)‖), where k := tw(I(Π)), and ` := max{3,bnd(r) | r ∈WGT(Π)}.\nProof. We refer the reader to Appendix B.3.\nThe runtime bounds stated in Theorem 4 appear to be worse than in Theorem 2. However, tw(I(Π)) ≤ tw(P (Π)) + 1 and tw(P (Π)) ≥ max{|at(r)| | r ∈ Π} for a given program Π. Further, there are programs where tw(I(Π)) = 1, but tw(P (Π)) = k, e.g., a program consisting of a single rule r with |at(r)| = k. Consequently, worst-case runtime bounds of DPPRIM are at least double-exponential in the rule size and DPPRIM will perform worse than DP INC on input programs containing large rules. However, due to the rule-states, data structures of DP INC are much more complex than of DPPRIM. In consequence, we expect DPPRIM to perform better in practice if rules are small and incidence and primal treewidth are therefore almost equal. In summary, we have a trade-off between (i) a more general parameter decreasing the theoretical worst-case runtime and (ii) less complex data structures decreasing the practical overhead to solve AS."
    }, {
      "heading" : "3.3 Extensions for Optimization and Counting",
      "text" : "In order to find an answer set of a program with optimization statements or the number of optimal answer sets (#AspO), we extend our algorithms PRIM and INC. Therefore, we augment tuples stored in tables with an integers c and n describing the cost and the number of witnessed sets. Due to space restrictions, we only present adaptions for INC. We state which parts of INC we adapt to compute the number of optimal answer sets in Algorithm 4 (#OINC). To slightly simplify the presentation of optimization rules, we assume without loss of generality that whenever an atom a is introduced in bag χ(t) for some node t of the TD, the optimization rule r, where a occurs, belongs to the bag χ(t). First, we explain how to handle costs making use of function cst(Π,M,A) as defined in Section 2. In a leaf (Line 1) we set the (current) cost to 0. If we introduce an atom a (Line 2–4) the cost depends on whether a is set to true or false in M and we add the cost of the “child” tuple. Removal of rules (Line 5–6) is trivial, as we only store the same values. If we remove an atom (Line 7–8), we compute the minimum costs only for tuples 〈M−a , σ, C, c, n〉 where c is minimal among M−a , σ, C, that is, for a multiset S we let kmin(S) := *〈M−a , σ, C, c, n〉 | c = min{c′ : 〈M−a , σ, C, c′, ·〉 ∈ S}, 〈M−a , σ, C, c, n〉 ∈ S+. We require a multiset notation for counting (see below). If we join two nodes (Line 9–11), we compute the minimum value in the table of one child plus the minimum value of the table of the other child minus the value of the cost for the current bag, which is exactly the value we added twice. Next, we explain how to handle the number of witnessed sets that are minimal with respect to the cost. In a leaf (Line 1), we set the counter to 1. If we introduce/remove a rule or introduce an atom (Line 2–6), we can simply take the number n from the child. If we remove an atom (Line 7–8) we first obtain a multiset from computing kmin, which can contain several tuples for M−a , σ, C, c as we obtained M−a either from M \\ {a} if a ∈M or M if a /∈M giving rise multiple solutions, that is, cnt(S) := {〈M,σ, C, c, ∑ 〈M,σ,C,c,n′〉∈S n\n′〉 | 〈M,σ, C, c, n〉 ∈ S}. If we join nodes (Line 7–9), we multiply the number n′ from the tuple of one child with the number n′′ from the tuple of the other child, restrict results with respect to minimum costs, and sum up the resulting numbers.\nCorollary 1. Given a program Π, algorithm #OINC runs in time O(log(m) · 22k+2·`k+1‖I(Π)‖2), where k := tw(I(Π)), ` := max{3,bnd(r) : r ∈WGT(Π)}, and m := Σr∈OPT(Π) wght(r)."
    }, {
      "heading" : "4 Experimental Evaluation",
      "text" : "We implemented the algorithms DPPRIM and DP INC into a prototypical solver DynASP2(·) and performed experiments to evaluate its runtime behavior. Clearly, we cannot hope to solve programs with graph representations of high treewidth. However, programs involving real-world graphs such as graph problems on transit graphs admit TDs of small width. We used both random and structured instances for our benchmarks. We refer to Appendix C for instance, machine and solver configurations and descriptions. The random instances (Sat-TGrid, 2QBF-TGrid, ASP-TGrid, 2ASP-TGrid) were designed to have a high number of variables and solutions and treewidth at most three. The structured instances model various graph problems (2Col, 3Col, Ds, St cVc, sVc) on real world mass transit graphs. For a graph, program 2Col counts all 2-colorings, 3Col counts all 3-colorings, Ds counts all minimal dominating sets, St counts\nall Steiner trees, cVc counts all cardinality-minimal vertex covers, and sVc counts all subset-minimal vertex covers. In order to draw conclusions about the efficiency of DynASP2, we mainly inspected the cpu running time and number of timeouts using the average over three runs per instance (three fixed seeds allow certain variance [1] for heuristic TD computation). We limited available memory (RAM) to 4GB (to run SharpSAT on large instances), and cpu time to 300 seconds, and then compared DynASP2 with the dedicated #SAT solvers SharpSAT [20] and Cachet [18], the QBF solver DepQBF0, and the ASP solver Clasp [9]. Figure 4 illustrates runtime results as a cactus plot. Table 1 reports on the average running times, numbers of solved instances and timeouts on the structured instance sets.\nSummary. Our empirical benchmark results confirm that DynASP2 exhibits competitive runtime behavior if the input instance has small treewidth. Compared to state-of-the-art Asp and Qbf solvers, DynASP2 has an advantage in case of many solutions, whereas Clasp and DepQBF0 perform well if the number of solutions is relatively small. However, DynASP2 is still reasonably fast on structured instances with few solutions as it yields the result mostly within less than 10 seconds. We observed that INC seems to be the better algorithm in our setting, indicating that the smaller width obtained by decomposing the incidence graph generally outweighs the benefits of simpler solving algorithms for the primal graph. However, if INC and PRIM run with graphs of similar width, PRIM benefits from its simplicity. A comparison to existing #SAT solvers suggests that, on random instances, they have a lower overhead (which is not surprising, since our algorithms are built for ASP), but, after about 150 seconds, our algorithms were still able to solve more instances than all other #SAT competitors."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we presented novel DP algorithms for ASP, extending previous work [11] in order to cover the full ASP syntax. Our algorithms are based on two graph representations of programs and run in linear time with respect to the treewidth of these graphs and weights used in the program. Experiments indicate that our approach seems to be suitable for practical use, at least for certain classes of instances with low treewidth, and hence could fit into a portfolio-based solver."
    }, {
      "heading" : "A Additional Examples",
      "text" : "In the following example, we briefly describe how we compute counterwitnesses using Algorithm 3 (INC) for selected interesting cases. The example is similar to Example 6, which, however, describes handling counterwitnesses for Algorithm PRIM.\nExample 10. We consider Π of Example 1 and T ′ = (·, χ) of Figure 3 and explain how we compute tables τ1, . . ., τ18 in Figure 3 (right) using DP INC. Table τ1 = 〈∅, ∅, ∅〉 as type(t1) = leaf. Node t2 introduces atom c, resulting in table {〈{c}, ∅, {(∅, ∅)}〉, 〈∅, ∅, ∅〉}. Then, node t3 introduces rule r1 and node t4 introduces rule r2. As a result, table τ4 additionally contains computed rule-states (see SatRules) for witnesses and counterwitnesses of τ3. Node t5 introduces atom b, while t6 removes b. Next, we focus on table τ6, since rule-states for counterwitnesses require updates for choice rule r1 (see UpdtWgt&Ch). Witness M6.2 = {c} is obtained by extending some witness M5.i ⊇ {b} of τ5. For counterwitness C6.2.1 = {c} we require to remember σ6.2.1(r1) = 1 (see UpdtWgt&Ch), since t6 removes b and C6.2.1 stems from some C5.i.j1 with b 6∈ C5.i.j1 . The set C5.i.j1 cannot be a model of the GL reduct {r1} M5.i unless r1 is satisfied because of its body, since b ∈ M5.i and b 6∈ C5.i.j1 . For choice rule r1, σ6.2.1(r1) 6= ∞ and σ6.2.1(r1) 6= 0 indicates that we can satisfy r1 only by B\n+(r1) \\M 6= ∅ (see P ∪ {· · · , ρ(r) > 0} in Definition 2). The remaining counterwitness C6.2.2 = ∅ was obtained by some C5.i.j2 with b 6∈ C5.i.j2 , since σ6.2.2(r2) = 0). Further, C6.2.3 = ∅ stems from C5.i.j3 ⊇ {b}, since σ6.2.3(r2) = 1."
    }, {
      "heading" : "B Omitted Proofs",
      "text" : "B.1 Proof of Theorem 2 (Correctness result of PRIM)\nProposition 1. The algorithm DPPRIM is correct. Proof (Sketch). Let Π be the given program and T = (T, χ) the TD, where T = (N, ·, n). We obtain correctness by slightly modifying the proof of Theorem 2 as well as relevant definitions and propositions following Appendix B.2. More precisely, we drop the mappings σ and relevant conditions for mappings σ and replace them by satisfiability of the respective rules. By definition of a primal graph of a program, we know that for every rule r ∈ Π there is a node t ∈ N such that χ(t) ⊆ at(r). Hence, for a node t we can decide satisfiability of a rule directly, if bag χ(t) contains all atoms of a rule, when computing the tables. We directly obtain completeness and soundness, which yields the proposition.\nProposition 2. Given a program Π and a TD T = (T, χ) of the primal graph P (Π) of width k with T = (N, ·, ·). For every node t ∈ N , there are at most 2k+1 · 22k+1 tuples in table τt, which is constructed by algorithm DP INC. Proof. Let Π be a program, P (Π) its primal graph, and T = (T, χ) a TD of P (Π) with T = (N, ·, ·). For every node t ∈ T , we have by definition of a tree decomposition and its width a maximum bag size of k + 1, i.e., |χ(t)| − 1 ≤ k. Therefore, we can have 2k+1 many witnesses and for each witness a subset of the set of witnesses consisting of at most 22 k+1 many counterwitnesses. Consequently, there are at most 2k+1 · 22k+1 tuples per node. Hence, the proposition is true.\nNow, we are in situation to prove Theorem 2.\nProof of Theorem 2. Let Π be a program, I(Π) = (V, ·) its incidence graph, and k be the treewidth of P (Π). Proposition 1 establishes correctness. Then, we can compute in time 2O(k\n3) · |V | a TD of width at most k [1]. We take such a TD and compute in linear time a nice TD [7]. Let T = (T, χ) be such a nice TD with T = (N, ·, n). Since the number of nodes in N is linear in the graph size and since for every node t ∈ N the table τt is bounded by 2k+1 · 22 k+1 according to Proposition 2, we obtain a running time of O(22k+2 · ‖P (Π)‖). Consequently, the theorem sustains.\nB.2 Proof of Theorem 3 (Correctness result of INC)\nIn the following, we provide insights on the correctness of Algorithm 3 (INC). The correctness proof of these algorithms need to investigate each node type separately. We have to show that a tuple at a node t guarantees existence of a model for the program Π≤t, proving soundness. Conversely, one can show that each candidate answer set is indeed evaluated while traversing the TD, which provides completeness. We employ this idea using the notions of (i) partial solutions consisting of partial models and the notion of (ii) local partial solutions.\nDefinition 3. Let Π be a program, T = (T, χ) be a TD of the incidence graph I(Π) of Π, where T = (N, ·, ·), and t ∈ N be a node. Further, let M,C ⊆ at≤t be sets and σ : Π≤t → N0 ∪ {∞} a mapping. The tuple (C, σ) is a partial model for t under M if the following conditions hold:\n1. C (Π<t)M ,\n2. for r ∈ Π≤t we have σ(r) = 0 or σ(r) =∞,\n3. (a) for r ∈ DISJ(Π≤t) we have B−r ∩M 6= ∅ or B+r ∩ at≤t 6⊆ C or Hr ∩ C 6= ∅ if and only if σ(r) =∞, (b) for r ∈WGT(Π≤t) we have wght(r, (at(r) \\ at≤t) ∪ (B−r \\M) ∪ (B+r ∩C)) < bnd(r) or Hr ∩C 6= ∅\nif and only if σ(r) =∞, and (c) for r ∈ CH(Π≤t) we have B−r ∩M 6= ∅ or B+r ∩ at≤t 6⊆ C or both Hr ⊆ at≤t and Hr ∩ (M \\ C) = ∅\nif and only if σ(r) =∞.\nDefinition 4. Let Π be a program, T = (T, χ) where T = (N, ·, n) be a TD of I(Π), and t ∈ N be a node. A partial solution for t is a tuple (M,σ, C) where (M,σ) is a partial model under M and C is a set of partial models (C, ρ) under M with C (M .\nThe following lemma establishes correspondence between answer sets and partial solutions.\nLemma 1. Let Π be a program, T = (T, χ) be a TD of the incidence graph I(Π) of program Π, where T = (·, ·, n), and χ(n) = ∅. Then, there exists an answer set M for Π if and only if there exists a partial solution u = (M,σ, ∅) with σ−1(∞) = Π for root n.\nProof. Given an answer set M of Π we construct u = (M,σ, ∅) with σ(r) :=∞ for r ∈ Π such that u is a partial solution for n (according to Definition 4). For the other direction, Definitions 3 and 4 guarantee that M is an answer set if there exists some tuple u. In consequence, the lemma holds.\nNext, we require the notion of local partial solutions corresponding to the tuples obtained in Algorithm 3.\nDefinition 5. Let Π be a program, T = (T, χ) a TD of I(Π), where T = (N, ·, n), t ∈ N be a node, M,C ⊆ at(Π) sets, and σ : Π→ N0∪{∞} be a mapping. We define the local rule-state σt,M,C := (σ]σ′)−Π<t for C under M of node t where σ′ : Πt → N0 ∪ {∞} by\nσ′(r) := { wght(r, (at≤t \\ χ(t)) ∩ [(B−r \\M) ∪ (B+r ∩ C)]) r ∈WGT(Πt) |(at≤t \\ χ(t)) ∩Hr ∩ (M \\ C)| r ∈ CH(Πt)\nDefinition 6. Let Π be a program, T = (T, χ) a TD of the incidence graph I(Π), where T = (N, ·, n), and t ∈ N be a node. A tuple u = 〈M,σ, C〉 is a local partial solution for t if there exists a partial solution û = (M̂, σ̂, Ĉ) for t such that the following conditions hold:\n1. M = M̂ ∩ χ(t),\n2. σ = σ̂t,M̂,M̂ , and\n3. C = {〈Ĉ ∩ χ(t), ρ̂t,M̂,Ĉ〉 | (Ĉ, ρ̂) ∈ Ĉ}.\nWe denote by ût the local partial solution u for t given partial solution û.\nThe following proposition provides justification that it suffices to store local partial solutions instead of partial solutions for a node t ∈ N .\nLemma 2. Let Π be a program, T = (T, χ) a TD of I(Π), where T = (N, ·, n), and χ(n) = ∅. Then, there exists an answer set for Π if and only if there exists a local partial solution of the form 〈∅, ∅, ∅〉 for the root n ∈ N .\nProof. Since χ(n) = ∅, every partial solution for the root n is an extension of the local partial solution u for the root n ∈ N according to Definition 6. By Lemma 1, we obtain that the lemma is true.\nIn the following, we abbreviate atoms occurring in bag χ(t) by att, i.e., att := χ(t) \\Πt.\nProposition 3 (Soundness). Let Π be a program, T = (T, χ) a TD of incidence graph I(Π), where T = (N, ·, ·), and t ∈ N a node. Given a local partial solution u′ of child table τ ′ (or local partial solution u′ of table τ ′ and local partial solution u′′ of table τ ′′), each tuple u of table τt constructed using table algorithm INC is also a local partial solution.\nProof. Let u′ be a local partial solution for t′ ∈ N and u a tuple for node t ∈ N such that u was derived from u′ using table algorithm INC. Hence, node t′ is the only child of t and t is either removal or introduce node.\nAssume that t is a removal node and r ∈ Πt′ \\ Πt for some rule r. Observe that u = 〈M,σ, C〉 and u′ = 〈M,σ′, C′〉 are the same in witness M . According to Algorithm 3 and since u is derived from u′, we have σ′(r) = ∞. Similarly, for any 〈C ′, ρ′〉 ∈ C′, ρ′(r) = ∞. Since u′ is a local partial solution, there exists a partial solution û′ of t′, satisfying the conditions of Definition 6. Then, û′ is also a partial solution for node t, since it satisfies all conditions of Definitions 3 and 4. Finally, note that u = (û′)t since the projection of û′ to the bag χ(t) is u itself. In consequence, the tuple u is a local partial solution.\nFor a ∈ att′ \\ att as well as for introduce nodes, we can analogously check the proposition. Next, assume that t is a join node. Therefore, let u′ and u′′ be local partial solutions for t′, t′′ ∈ N , respectively, and u be a tuple for node t ∈ N such that u can be derived using both u′ and u′′ in accordance with the INC algorithm. Since u′ and u′′ are local partial solutions, there exists partial solution û′ = (M̂ ′, σ̂′, Ĉ′) for node t′ and partial solution û′′ = (M̂ ′′, σ̂′′, Ĉ′′) for node t′′. Using these two partial solutions, we can construct û = (M̂ ′ ∪ M̂ ′′, σ̂′ ] σ̂′′, Ĉ′ ./ Ĉ′′) where ./ (·, ·) is defined in accordance with Algorithm 3 as follows:\nĈ′ ./ Ĉ′′ :={(Ĉ ′ ∪ Ĉ ′′, ρ̂′ ] ρ̂′′) | (Ĉ ′, ρ̂′) ∈ Ĉ′, (Ĉ ′′, ρ̂′′) ∈ Ĉ′′, Ĉ ′ ∩ att = Ĉ ′′ ∩ att}∪ {(Ĉ ′ ∪ M̂ ′′, ρ̂′ ] σ̂′′) | (Ĉ ′, ρ̂′) ∈ Ĉ′, Ĉ ′ ∩ att = M̂ ′′ ∩ att}∪ {(M̂ ′ ∪ Ĉ ′′, σ̂′ ] ρ̂′′) | (Ĉ ′′, ρ̂′′) ∈ Ĉ′′, M̂ ′ ∩ att = Ĉ ′′ ∩ att}.\nThen, we check all conditions of Definitions 3 and 4 in order to verify that û is a partial solution for t. Moreover, the projection ût of û to the bag χ(t) is exactly u by construction and hence, u = ût is a local partial solution.\nSince we have provided arguments for each node type, we established soundness in terms of the statement of the proposition.\nProposition 4 (Completeness). Let Π be a program, T = (T, χ) where T = (N, ·, ·) be a TD of I(Π) and t ∈ N be a node. Given a local partial solution u of table τt, either t is a leaf node, or there exists a local partial solution u′ of child table τ ′ (or local partial solution u′ of table τ ′ and local partial solution u′′ of table τ ′′) such that u can be constructed by u′ (or u′ and u′′, respectively) and using table algorithm INC.\nProof. Let t ∈ N be a removal node and r ∈ Πt′ \\Πt with child node t′ ∈ N . We show that there exists a tuple u′ in table τt′ for node t\n′ such that u can be constructed using u′ by INC (Algorithm 3). Since u is a local partial solution, there exists a partial solution û = (M̂, σ̂, Ĉ) for node t, satisfying the conditions of Definition 6. Since r is the removed rule, we have σ̂(r) = ∞. By similar arguments, we have ρ̂(r) = ∞ for any tuple (Ĉ, ρ̂) ∈ Ĉ. Hence, û is also a partial solution for t′ and we define u′ := ût′ , which is the projection of û onto the bag of t′. Apparently, the tuple u′ is a local partial solution for node t′ according to Definition 6. Then, u can be derived using INC algorithm and u′. By similar arguments, we establish the proposition for a ∈ att′ \\ att and the remaining (three) node types. Hence, the propositions sustains.\nNow, we are in situation to prove Theorem 3.\nProof of Theorem 3. We first show soundness. Let T = (T, χ) be the given TD, where T = (N, ·, n). By Lemma 2 we know that there is an answer set for Π if and only if there exists a local partial solution for the root n. Note that the tuple is of the form 〈∅, ∅, ∅〉 by construction. Hence, we proceed by induction starting from the leaf nodes. In fact, the tuple 〈∅, ∅, ∅〉 is trivially a partial solution by Definitions 3 and 4 and also a local partial solution of 〈∅, ∅, ∅〉 by Definition 6. We already established the induction step in Proposition 3. Hence, when we reach the root n, when traversing the TD in post-order by Algorithm DP INC, we obtain only valid tuples inbetween and a tuple of the form 〈∅, ∅, ∅〉 in the table of the root n witnesses an answer set. Next, we establish completeness by induction starting from the root n. Let therefore, M be an arbitrary answer set of Π. By Lemma 2, we know that for the root n there exists a local partial solution of the form 〈∅, ∅, ∅〉 for partial solution 〈M,σ, ∅〉 with σ(r) = ∞ for r ∈ Π. We already established the induction step in Proposition 4. Hence, we obtain some (corresponding) tuples for every node t. Finally, stopping at the leaves n. In consequence, we have shown both soundness and completeness resulting in the fact that Theorem 3 is true.\nTheorem 3 states that we can decide the problem Cons by means of Algorithm DP INC, which uses Algorithm 3.\nB.3 Proof of Theorem 4 (Worst-case Runtime Bounds of INC)\nFirst, we give a proposition on worst-case space requirements in tables for the nodes of our algorithm.\nProposition 5. Given a program Π, a TD T = (T, χ) with T = (N, ·, ·) of the incidence graph I(Π), and a node t ∈ N . Then, there are at most 2k+1 · `k+1 · 22k+1·`k+1 tuples in τt using algorithm DP INC for width k of T and bound ` = max{3,bnd(r) : r ∈WGT(Π)}.\nProof (Sketch). Let Π be the given program, T = (T, χ) a TD of the incidence graph I(Π), where T = (N, ·, ·), and t ∈ N a node of the TD. Then, by definition of a decomposition of the primal graph for each node t ∈ N , we have |χ(t)|− 1 ≤ k. In consequence, we can have at most 2k+1 many witnesses, and for each witness a subset of the set of witnesses consisting of at most 22 k+1\nmany counterwitnesses. Moreover, we observe that Algorithm 3 can be easily modified such that a state σ : Πt → N0 ∪ {∞} for node t ∈ N assigns each weight rule r ∈WGT(Π) a non-negative integer σ(r) ≤ bnd(r) + 1, each choice rule r ∈ CH(Π) a non-negative integer σ(r) ≤ 2 and each disjunctive rule r ∈ DISJ(Π) a non-negative integer σ(r) ≤ 1. This is the case since we need to model σ(r) = 0 and σ(r) =∞ for each disjunctive rule r. Moreover, for choice rules r, it suffices to additionally model whether 1 ≤ σ(r) <∞, and for weight rules r, we require to remember any weight 1 ≤ σ(r) ≤ bnd(r). In total, we need to distinguish `k+1 different rule-states for each witness of a tuple in the table τt for node t. Since for each witness in the table τt for node t ∈ N we remember rule-states for at most k + 1 rules, we store up to `k+1 many combinations per witness. In total we end up with at most 22 k+1·`k+1 many counterwitnesses for each witness and rule-state in the worst case. Thus, there are at most 2k+1 · `k+1 · 22k+1·`k+1 tuples in table τt for node t. In consequence, we established the proposition.\nProof of Theorem 4. Let Π be a program, I(Π) = (V, ·) its incidence graph, and k be the treewidth of P (Π). Then, we can compute in time 2O(k\n3) · |V | a TD of width at most k [1]. We take such a TD and compute in linear time a nice TD [7]. Let T = (T, χ) be such a nice TD with T = (N, ·, ·). Since the number of nodes in N is linear in the graph size and since for every node t ∈ N the table τt is bounded by 2k+1 · `k+1 · 22k+1·`k+1 according to Proposition 5, we obtain a running time of O(22k+2·`k+1‖I(Π)‖). Consequently, the theorem sustains.\nB.4 Correctness of the Algorithm DP#OINC The following propositions states that we can use Algorithm DP#OINC to actually count optimal answer sets.\nProposition 6. The algorithm DP#OINC is correct.\nProof (Sketch). We follow the proof of Theorem 3. First, we additionally need to take care of the optimization rules obtained by extending Definitions 3–6, the lemmas and propositions accordingly. In order to handle the counting, we have to extend Definitions 3–6 by counters. Further, we additionally need to ensure and prove in the induction steps, which are established by Propositions 3 and 4, that any fixed partial solution is obtained from child to parent via a corresponding local partial solution by the algorithm."
    }, {
      "heading" : "C Experiments",
      "text" : "C.1 Solvers\nThe solvers tested include our own prototypical implementation, which we refer to as DynASP, and the existing solvers\n• Cachet 1.21 [18], which is a SAT model counter,\n• DepQBF05, which is the solver DepQBF [9] where we added a naive implementation using methods described by Lonsing [8],\n• Clasp 3.1.4 [9], which is an ASP solver, and\n• SharpSAT 12.08 [20], which is a SAT model counter.\nC.2 Environment\nWe ran the experiments on an Ubuntu 12.04 Linux cluster of 3 nodes with two AMD Opteron 6176 SE CPUs of 12 physical cores each at 2.3Ghz clock speed and 128GB RAM. Input instances were given to the solvers via shared memory. All solvers have been compiled with gcc version 4.9.3. Available memory was limited to 4GB RAM, which was necessary to run SharpSAT on larger instances, and CPU time to 300 seconds. We used default options for cachet and SharpSAT, “–qdc” for DepQBF0, “–stats=2 –opt-mode=optN -n 0 –opt-strategy=usc -q” and no solution printing/recording for clasp. We also benchmarked clasp with the flag “bb”. However, “usc” outperformed “bb” on all our benchmarks. All solvers have been executed in single core mode.\nC.3 Instances\nWe used both random and structured instances for benchmark sets, which we briefly describe below. The benchmark sets, including instances and encodings, as well as results are available online on github6.\nThe random instances (Sat-TGrid, 2QBF-TGrid, ASP-TGrid, 2ASP-TGrid) were designed to have a high number of variables and solutions and treewidth at most three. The instances are constructed as follows: Let k and ` be some positive integers and p a rational number such that 0 < p ≤ 1. An instance F of Sat-TGrid(k, l, p) consists of the set V = {(1, 1), . . . , (1, `), (2, `), . . . , (k, `)} of variables and with probability p for each variable (i, j) such that 1 < i ≤ k and 1 < j ≤ ` a clause s1(i, j), s2(i− 1, j), s3(i, j − 1), a clause s4(i, j), s5(i− 1, j), s6(i− 1, j − 1), and a clause s7(i, j), s8(i− 1, j − 1), s9(i, j − 1) where si ∈ {−,+} is selected with probability one half. In that way, such an instance has an underlying dependency graph that consists of various triangles forming for probability p = 1 a graph that has a grid as subgraph. Let q be a rational number such that 0 < q ≤ 1. An instance of the set 2Qbf-TGrid(k, l, p, q) is of the form ∃V1.∀V2.F where a variable belongs to V1 with probability q and to V2 otherwise. Instances of the sets ASP-TGrid or 2ASP-TGrid have been constructed in a similar way, however, as an Asp program instead of a formula. Note that the number of answer sets and the number of satisfiable assignments correspond. We fixed the parameters to p = 0.85, k = 3, and l ∈ {40, 80, . . . , 400} to obtain instances that have with high probability a small fixed width, a high number of variables and solutions. Further, we took fixed random seeds and generated 10 instances to ensure a certain randomness.\nThe structured instances model various graph problems (2Col, 3Col, Ds, St cVc, sVc) on real world mass transit graphs of 82 cities, metropolitan areas, or countries. The graphs were extracted from publicly available mass transit data feeds [2] using gtfs2graphs [5] and split by transportation type, e.g., train, metro,\n5See https://github.com/hmarkus/depqbf/tree/depqbf0 6See https://github.com/daajoe/lpnmr17 experiments.\ntram. We excluded bus networks as size and treewidth were too large. For an input graph, the 2Col encoding counts all minimal sets S of vertices s.t. there are two sets F and S where no two neighboring vertices v and w belong to F ; 3Col counts all 3-colorings; Ds counts all minimal dominating sets; St counts all Steiner trees; cVc counts all minimal vertex covers; and sVc counts all subset-minimal vertex covers. Since we cannot expect to solve instances of high treewidth efficiently, we restricted the instances to those where we were able to find decompositions of width below 20 within 60 seconds.\nC.4 Extended Discussion on the Results\nIn order to draw conclusions about the efficiency of our approach, we mainly inspected the total cpu running time and number of timeouts on the random and structured benchmark sets. Note that we did not record I/O times. The runtime for DynASP2(·) includes decomposition times using heuristics from [3, 4]. We randomly generated three fixed seeds for the decomposition computation to allow a certain variance [1]. When evaluating the results, we took the average over the three runs per instance. Figure 4 illustrates solver runtime on the various random instance sets and a selected structured instance set as a cactus plot. Table 1 reports on the average running times, number of solved instances, and number of timeouts of the solvers on the structured instance sets.\nC.4.1 Results.\nSAT-TGrid and Asp-TGrid: Cachet solved 125 instances. Clasp always timed out. A reason could be the high number of solutions as Clasp counts the models by enumerating them (without printing them). DynASP2(·) solved each instance within at most 270 seconds (on average 67 seconds). The best configuration with respect to runtime was PRIM. However, the running times of the different configurations were close. We observed as expected a sub-polynomial growth in the runtime with an increasing number of solutions. SharpSAT timed out on 3 instances and ran into a memory out on 7 instances, but solved most of the instances quite fast. Half of the instances were solved within 1 second and more than 80% of the instances within 10 seconds, and about 9% of the instances took more than 100 seconds. The number of solutions does not have an impact on the runtime of SharpSAT. SharpSAT was the fastest solver in total. However, DynASP2(·) solved all instances. The results are illustrated in the two left graphs of Figure 4.\n2QBF-TGrid and 2ASP-TGrid: Clasp solved more than half of the instances in less than 1 second, however, timed out on 59 instances. DepQBF0 shows a similar behavior as Clasp, which is not surprising as both solvers count the number of solutions by enumerating them and hence the number of solutions has a significant impact on the runtime of the solver. However, Clasp is faster throughout than DepQBF0. DynASP2(INC) solved half of the instances within less than 1 second, about 92% of the instances within less than 10 seconds, and provided solutions also if the instance had a large number of answer sets. DynASP2(PRIM) quickly produced timeouts due to large rules in program that produced a significantly larger width of the computed decompositions.\nStructured instances: Clasp solved most of the structured instances reasonably fast. However, the number of solutions has again, similar to the random setting, a significant impact on its performance. If the number of solutions was very high, then Clasp timed out. If the instance has a small number of solutions, then Clasp yields the number almost instantly. However, DynASP2(·) also provided a solution within a second. DynASP2(·) solved for each set but the set St more than 80% of the instances in less than 1 second and the remaining instances in less than 100 seconds. For St the situation was different. Half of the instances were solved in less than 10 seconds and a little less than the other half timed out. Similar to the random setting, DynASP2(·) ran still fast on instances with a large number of solutions."
    }, {
      "heading" : "Appendix References",
      "text" : "[1] Hans L. Bodlaender. A linear-time algorithm for finding tree-decompositions of small treewidth. SIAM J. Comput., 25(6):1305–1317, 1996.\n[2] J. et al. Czebotar. GTFS data exchange. www.gtfs-data-exchange.com, 2016.\n[3] Holger Dell and Frances Rosamond. The 1st parameterized algorithms and computational experiments challenge – Track A: Treewidth. Technical report, 2016.\n[4] Artan Dermaku, Tobias Ganzow, Georg Gottlob, Ben McMahan, Nysret Musliu, and Marko Samer. Heuristic methods for hypertree decomposition. In MICAI’08, pages 1–11. Springer, 2008.\n[5] J. K. Fichte. daajoe/gtfs2graphs – a GTFS transit feed to graph format converter. https://github. com/daajoe/gtfs2graphs, 2016.\n[6] M. Gebser, B. Kaufmann, and T. Schaub. Conflict-driven answer set solving: From theory to practice. AIJ, 187–188, 2012.\n[7] Ton Kloks. Treewidth. Computations and Approximations, volume 842 of LNCS. Springer, 1994.\n[8] F. Lonsing. Personal communication, 2016.\n[9] F. Lonsing and A. Biere. DepQBF: A dependency-aware QBF solver system description. J. Sat., Bool. Model. and Comp., 7, 2010."
    } ],
    "references" : [ {
      "title" : "A linear-time algorithm for finding tree-decompositions of small treewidth",
      "author" : [ "Hans L. Bodlaender" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Czebotar. GTFS data exchange",
      "author" : [ ],
      "venue" : "www.gtfs-data-exchange.com,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "The 1st parameterized algorithms and computational experiments challenge – Track A: Treewidth",
      "author" : [ "Holger Dell", "Frances Rosamond" ],
      "venue" : "Technical report,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Heuristic methods for hypertree decomposition",
      "author" : [ "Artan Dermaku", "Tobias Ganzow", "Georg Gottlob", "Ben McMahan", "Nysret Musliu", "Marko Samer" ],
      "venue" : "In MICAI’08,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Fichte. daajoe/gtfs2graphs – a GTFS transit feed to graph format converter",
      "author" : [ "K. J" ],
      "venue" : "https://github. com/daajoe/gtfs2graphs,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Conflict-driven answer set solving: From theory to practice",
      "author" : [ "M. Gebser", "B. Kaufmann", "T. Schaub" ],
      "venue" : "AIJ, 187–188,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Treewidth. Computations and Approximations, volume",
      "author" : [ "Ton Kloks" ],
      "venue" : "LNCS. Springer,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1994
    }, {
      "title" : "DepQBF: A dependency-aware QBF solver system description",
      "author" : [ "F. Lonsing", "A. Biere" ],
      "venue" : "J. Sat., Bool. Model. and Comp.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Parameterized algorithms [14, 5] have attracted considerable interest in recent years and allow to tackle hard problems by directly exploiting a small parameter of the input problem.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "A parameter that has been researched extensively is treewidth [16, 2].",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "ASP [3, 13] is a logic-based declarative modelling language and problem solving framework where solutions, so called answer sets, of a given logic program directly represent the solutions of the modelled problem.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 2,
      "context" : ", [3, 13].",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "Stateof-the-art ASP grounders support the full ASP-Core-2 language [4] and output smodels input format [19], which we will use for our algorithms.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "Hardness is a direct consequence of #·coNP-hardness for the problem of counting subset minimal models of a CNF formula [6], since answer sets of negation-free programs and subset-minimal models of CNF formulas are essentially the same objects.",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "For some arbitrary but fixed integer k and a graph of treewidth at most k, we can compute a TD of width 6 k in time 2O(k ) · |V | [2].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "For every TD, we can compute a nice TD in linear time without increasing the width [2].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : ", [17, 7].",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "In order to draw conclusions about the efficiency of DynASP2, we mainly inspected the cpu running time and number of timeouts using the average over three runs per instance (three fixed seeds allow certain variance [1] for heuristic TD computation).",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 7,
      "context" : "We limited available memory (RAM) to 4GB (to run SharpSAT on large instances), and cpu time to 300 seconds, and then compared DynASP2 with the dedicated #SAT solvers SharpSAT [20] and Cachet [18], the QBF solver DepQBF0, and the ASP solver Clasp [9].",
      "startOffset" : 246,
      "endOffset" : 249
    } ],
    "year" : 2017,
    "abstractText" : "Parameterized algorithms are a way to solve hard problems more efficiently, given that a specific parameter of the input is small. In this paper, we apply this idea to the field of answer set programming (ASP). To this end, we propose two kinds of graph representations of programs to exploit their treewidth as a parameter. Treewidth roughly measures to which extent the internal structure of a program resembles a tree. Our main contribution is the design of parameterized dynamic programming algorithms, which run in linear time if the treewidth and weights of the given program are bounded. Compared to previous work, our algorithms handle the full syntax of ASP. Finally, we report on an empirical evaluation that shows good runtime behaviour for benchmark instances of low treewidth, especially for counting answer sets.",
    "creator" : "LaTeX with hyperref package"
  }
}