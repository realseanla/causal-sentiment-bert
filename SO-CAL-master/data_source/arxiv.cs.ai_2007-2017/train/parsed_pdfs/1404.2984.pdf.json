{
  "name" : "1404.2984.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distribution-Aware Sampling and Weighted Model Counting for SAT",
    "authors" : [ "Supratik Chakraborty", "Daniel J. Fremont", "Kuldeep S. Meel", "Sanjit A. Seshia", "Moshe Y. Vardi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 4.\n29 84\nv1 [\ncs .A\nI] 1\n1 A\npr 2\n01 4"
    }, {
      "heading" : "1 Introduction",
      "text" : "Given a set of weighted elements, computing the cumulative weight of all elements that satisfy a set of constraints is a fundamental problem that arises in many contexts. Known variously as weighted model counting, discrete integration and partition function computation, this problem has applications in machine learning, probabilistic reasoning, statistics, planning and combinatorics, among other areas (Roth 1996; Sang et al. 2004; Domshlak and Hoffmann 2007; Xue, Choi, and Darwiche 2012). A closely related problem is that of sampling elements satisfying a set of constraints, where the probability of choosing an element is proportional to its weight. The latter problem, known as weighted sampling, also has important applications in probabilistic reasoning, machine learning, statistical physics, constrained random verification and other domains (Jerrum and Sinclair 1996; Bacchus, Dalmao, and Pitassi 2003; Naveh et al. 2006;\nCopyright c© 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nMadras and Piccioni 1999). Unfortunately, the exact versions of both problems are computationally hard. Weighted model counting can be used to count the number of satisfying assignments of a CNF formula; hence it is #P -hard (Valiant 1979). It is also known that an efficient algorithm for weighted sampling would yield a fully polynomial randomized approximation scheme (FPRAS) for #P-complete inference problems (Jerrum and Sinclair 1996; Madras and Piccioni 1999) – a possibility that lacks any evidence so far. Fortunately, approximate solutions to weighted model counting and weighted sampling are good enough for most applications. Consequently, there has been significant interest in designing practical approximate algorithms for these problems.\nSince constraints arising from a large class of real-world problems can be modeled as propositional CNF (henceforth CNF) formulas, we focus on CNF and assume that the weights of truth assignments are given by a weight function w(·) defined on the set of truth assignments. Roth showed that approximately counting the models of a CNF formula is NP-hard even when the structure of the formula is severely restricted (Roth 1996). By a result of Jerrum, Valiant and Vazirani (Jerrum, Valiant, and Vazirani 1986), we also know that approximate model counting and almost uniform sampling (a special case of approximate weighted sampling) are polynomially inter-reducible. Therefore, it is unlikely that there exist polynomial-time algorithms for either approximate weighted model counting or approximate weighted sampling (Karp, Luby, and Madras 1989). Recently, a new class of algorithms that use pairwise independent random parity constraints and a MAP (maximum a posteriori probability)-oracle have been proposed for solving both problems (Ermon et al. 2013a; Ermon et al. 2014; Ermon et al. 2013b). These algorithms provide strong theoretical guarantees (FPRAS relative to the MAP oracle), and have been shown to scale to medium-sized problems in practice. While this represents a significant step in our quest for practically efficient algorithms with strong guarantees for approximate weighted model counting and approximate weighted sampling, the use of MAP-queries presents issues that need to be addressed in practice. First, the use of MAP-queries along with parity constraints poses scalability hurdles (Ermon et al. 2014; Ermon et al. 2013c). Second, existing MAP-query solvers work best when the distribution\nof weights is represented by a graphical model with small tree-width – a restriction that is violated in several real-life problems. While this does not pose problems in practical applications where an approximation of the optimal MAP solution without guarantees of the approximation factor suffices, it presents significant challenges when we demand the optimal MAP solution. This motivates us to ask if we can design approximate algorithms for weighted model counting and weighted sampling that do not invoke MAP-oracles at all, and do not assume any specific representation of the weight distribution.\nOur primary contribution is an affirmative answer to the above question under mild assumptions on the distribution of weights. Specifically, we show that two recently-proposed algorithms for approximate (unweighted) model counting (Chakraborty, Meel, and Vardi 2013a) and near-uniform (unweighted) sampling (Chakraborty, Meel, and Vardi 2013b) can be adapted to work in the setting of weighted assignments, using only a SAT solver (NP-oracle) and a black-box weight function w(·). For the algorithm to work well in practice, we require that tilt of the weight function, which is the ratio of the maximum weight of a satisfying assignment to the minimum weight of a satisfying assignment, is small. We present arguments why this is a reasonable assumption in some important classes of problems. We also present an adaptation of our algorithm for problem instances where the tilt is large. The adapted algorithm requires a pseudoBoolean SAT solver instead of a (regular) SAT solver as an oracle."
    }, {
      "heading" : "2 Notation and Preliminaries",
      "text" : "Let F be a Boolean formula in conjunctive normal form (CNF), and let X be the set of variables appearing in F . The set X is called the support of F . Given a set of variables S ⊆ X and an assignment σ of truth values to the variables in X , we write σ|S for the projection of σ onto S. A satisfying assignment or witness of F is an assignment that makes F evaluate to true. We denote the set of all witnesses of F by RF . For notational convenience, whenever the formula F is clear from the context, we omit mentioning it. Let D ⊆ X be a subset of the support such that there are no two satisfying assignments that differ only in the truth values of variables in D. In other words, in every satisfying assignment, the truth values of variables in X \\ D uniquely determine the truth value of every variable in D. The set D is called a dependent support of F , and X \\ D is called an independent support. Note that there may be more than one independent support: (a ∨ ¬b) ∧ (¬a ∨ b) has three, namely {a}, {b} and {a, b}. Clearly, if I is an independent support of F , so is every superset of I.\nLet w(·) be a function that takes as input an assignment σ and yields a real number w(σ) ∈ (0, 1] called the weight of σ. Given a set Y of assignments, we use w(Y ) to denote Σσ∈Y w(σ). Our main algorithms (see Section 4) make no assumptions about the nature of the weight function, treating it as a black-box function. In particular, we do not assume that the weight of an assignment can be factored into the weights of projections of the assignment on\nspecific subsets of variables. The exception to this is Section 6, where we consider possible improvements when the weights are given by a known function, or “white-box”. Three important quantities derived from the weight function are wmax = max\nσ∈RF w(σ), wmin = min σ∈RF w(σ), and the tilt\nρ = wmax/wmin. Our algorithms require an upper bound on the tilt, denoted r, which is provided by the user. As tight a bound as possible is desirable to maximize the efficiency of the algorithms. We define MAP (maximum a posteriori probability) for our distribution of weights to be wmax w(RF ) .\nWe write Pr [X : P ] for the probability of outcome X when sampling from a probability space P . For brevity, we omit P when it is clear from the context. The expected value of the outcome X is denoted E [X ].\nA special class of hash functions, called k-wise independent hash functions, play a crucial role in our work (Bellare, Goldreich, and Petrank 1998). Let n,m and k be positive integers, and let H(n,m, k) denote a family of k-wise independent hash functions mapping {0, 1}n to {0, 1}m. We use h R ←− H(n,m, k) to denote the probability space obtained by choosing a hash function h uniformly at random from H(n,m, k). The property of k-wise independence guarantees that for all α1, . . . αk ∈ {0, 1}m\nand for all distinct y1, . . . yk ∈ {0, 1} n, Pr\n[\nk ∧\ni=1\nh(yi) = αi\n: h R ←− H(n,m, k)\n]\n= 2−mk. For every α ∈ {0, 1}m and\nh ∈ H(n,m, k), let h−1(α) denote the set {y ∈ {0, 1}n | h(y) = α}. Given RF ⊆ {0, 1}n and h ∈ H(n,m, k), we use RF,h,α to denote the set RF ∩ h\n−1(α). Our work uses an efficient family of hash functions, denoted as Hxor(n,m, 3). Let h : {0, 1}n → {0, 1}m be a hash function in the family, and let y be a vector in {0, 1}n. Let h(y)[i] denote the ith component of the vector obtained by applying h to y. The family of hash functions of interest is defined as {h(y) | h(y)[i] = ai,0 ⊕ ( n ⊕\nl=1\nai,l ·\ny[l]), ai,j ∈ {0, 1}, 1 ≤ i ≤ m, 0 ≤ j ≤ n}, where ⊕ denotes the xor operation. By choosing values of ai,j randomly and independently, we can effectively choose a random hash function from the family. It has been shown in (Gomes, Sabharwal, and Selman 2007) that this family of hash functions is 3-independent.\nGiven a CNF formulaF , an exact weighted model counter returns w(RF ). An approximate weighted model counter relaxes this requirement to some extent: given tolerance ε > 0 and confidence 1 − δ ∈ (0, 1], the value v returned by the\ncounter satisfies Pr[ w(RF )\n1 + ε ≤ v ≤ (1+ ε)w(RF )] ≥ 1− δ.\nA related type of algorithm is a weighted-uniform probabilistic generator, which outputs a witness w ∈ RF such that Pr [w = y] = w(y) /w(RF ) for every y ∈ RF . An almost weighted-uniform generator relaxes this requirement, ensuring that for all y ∈ RF , we have w(y)\n(1 + ε)w(RF )\n≤ Pr [w = y] ≤ (1 + ε)w(y)\nw(RF ) . Probabilistic generators are\nallowed to occasionally “fail” by not returning a witness (when RF is non-empty), with the failure probability upper bounded by δ."
    }, {
      "heading" : "3 Related Work",
      "text" : "Marrying strong theoretical guarantees with scalable performance is the holy grail of research in the closely related areas of weighted model counting and weighted sampling. The tension between the two objectives is evident from a survey of the literature. Earlier algorithms for weighted model counting can be broadly divided into three categories: those that give strong guarantees but scale poorly in practice, those that give weak guarantees but scale well in practice, and some recent algorithms that attempt to bridge this gap by making use of a MAP-oracle and random parity constraints. Techniques in the first category attempt to compute the weighted model count exactly by enumerating partial solutions (Sang, Bearne, and Kautz 2005) or by converting the CNF formula to alternative representations (Darwiche 2004; Choi and Darwiche 2013). Unfortunately, none of these approaches scale to large problem instances. Techniques in the second category employ variational methods, sampling-based methods or other heuristic methods. Variational methods (Wainwright and Jordan 2008; Gogate and Dechter 2011) work extremely well in practice, but do not provide guarantees except in very special cases. Sampling-based methods are usually based on importance sampling (e.g. (Gogate and Dechter 2011)), which provide weak one-sided bounds, or on Markov Chain Monte Carlo (MCMC) sampling (Jerrum and Sinclair 1996; Madras 2002). MCMC sampling is perhaps the most popular technique for both weighted sampling and weighted model counting. Several MCMC algorithms like simulated annealing and the Metropolis-Hastings algorithm have been studied extensively in the literature (Kirkpatrick, Gelatt, and Vecchi 1983; Madras 2002). While MCMC sampling is guaranteed to converge to a target distribution under mild requirements, convergence is often impractically slow (Jerrum and Sinclair 1996). Therefore, practical MCMC sampling-based tools use heuristics that destroy the theoretical guarantees. Several other heuristic techniques that provide weak onesided bounds have also been proposed in the literature (Gomes, Sabharwal, and Selman 2006).\nRecently, Ermon et al. proposed new hashingbased algorithms for approximate weighted model counting and approximate weighted sampling (Ermon et al. 2013a; Ermon et al. 2013b; Ermon et al. 2013c; Ermon et al. 2014). Their algorithms use random parity constraints as pair-wise independent hash functions to partition the set of satisfying assignments of a CNF formula into cells. A MAP oracle is then queried to obtain the maximum weight of an assignment in a randomly chosen cell. By repeating the MAP queries polynomially many times for randomly chosen cells of appropriate expected sizes, Ermon et al showed that they can provably compute approximate weighted model counts and also\nprovably achieve approximate weighted sampling. The performance of Ermon et al’s algorithms depend crucially on the ability to efficiently answer MAP queries. Complexitywise, MAP is significantly harder than CNF satisfiability, and is known to be NPPP -complete (Park 2002). The problem is further compounded by the fact that the MAP queries generated by Ermon et al’s algorithms have random parity constraints built into them. Existing MAP-solving techniques work efficiently when the weight distribution of assignments is specified by a graphical model, and the underlying graph has specific structural properties. With random parity constraints, these structural properties are likely to be violated very often. In (Ermon et al. 2013c), it has been argued that a MAP-oracle-based weighted model-counting algorithm proposed in (Ermon et al. 2013a) is unlikely to scale well to large problem instances. Since MAP solving is also crucial in the weighted sampling algorithm of (Ermon et al. 2013b), the same criticism applies to that algorithm as well. Several relaxations of the MAPoracle-based algorithm proposed in (Ermon et al. 2013a), were therefore discussed in (Ermon et al. 2013c). While these relaxations help reduce the burden of MAP solving, they also significantly weaken the theoretical guarantees.\nIn later work (Ermon et al. 2014), Ermon et al showed how the average size of parity constraints in their weighted model counting and weighted sampling algorithms can be reduced using a new class of hash functions. This work, however, still stays within the same paradigm as their earlier work – i.e, it uses MAP-oracles and XOR constraints. Although Ermon et al’s algorithms provide a 16-factor approximation in theory, in actual experiments, they use relaxations and timeouts of the MAP solver to get upper and lower bounds of the optimal MAP solution. Unfortunately, these bounds do not come with any guarantees on the factor of approximation. Running the MAP solver to obtain the optimal value is likely to take significantly longer, and is not attempted in Ermon et al’s work.\nThe algorithms developed in this paper are closely related to two algorithms proposed recently by Chakraborty, Meel and Vardi (Chakraborty, Meel, and Vardi 2013a; Chakraborty, Meel, and Vardi 2013b). The first of these (Chakraborty, Meel, and Vardi 2013a) computes the approximate (unweighted) modelcount of a CNF formula, while the second algorithm (Chakraborty, Meel, and Vardi 2013b) performs near-uniform (unweighted) sampling. Like Ermon et al’s algorithms, these algorithms make use of parity constraints as pair-wise independent hash functions, and can benefit from the new class of hash functions proposed in (Ermon et al. 2014). Unlike, however, Ermon et al’s algorithms, Chakraborty et al. use a SAT solver (NPoracle) specifically engineered to handle parity constraints efficiently."
    }, {
      "heading" : "4 Algorithm",
      "text" : "We now present algorithms for approximate weighted model counting and approximate weighted sampling, assuming a small bounded tilt and a black-box weight function.\nRecalling that the tilt concerns weights of only satisfying assignments, our assumption about it being bounded by a small number is reasonable in several practical situations. For example, when solving probabilistic inference with evidence by reduction to weighted model counting (Chavira and Darwiche 2008), every satisfying assignment of the CNF formula corresponds to an assignment of values to variables in the underlying probabilistic graphical model that is consistent with the evidence. Furthermore, the weight of a satisfying assignment is the joint probability of the corresponding assignment of variables in the probabilistic graphical model. A large tilt would therefore mean existence of two assignments that are consistent with the evidence, but one of which is overwhelmingly more likely than the other. In several real-world problems (see, e.g. Sec 8.3 of (Dıez and Druzdzel 2006)), this is considered unlikely given that numerical conditional probability values are often obtained from human experts providing qualitative and rough quantitative data.\nOur weighted model counting algorithm, called WeightMC, is best viewed as an adaptation of the ApproxMC algorithm proposed by Chakraborty, Meel and Vardi (Chakraborty, Meel, and Vardi 2013a) for approximate unweighted model counting. Similarly, our weighted sampling algorithm, called WeightGen, can be viewed as an adaptation of the the UniWit algorithm (Chakraborty, Meel, and Vardi 2013b), originally proposed for near-uniform unweighted sampling. The key idea in both ApproxMC and UniWit is to partition the set of satisfying assignments into “cells” containing roughly equal numbers of satisfying assignments, using a random hash function from the family Hxor(n,m, 3). A random cell is then chosen and inspected to see if the number of satisfying assignments in it is smaller than a pre-computed threshold. The threshold, in turn, depends on the desired approximation factor or tolerance ε. If the chosen cell is small enough, UniGen samples uniformly from the chosen small cell to obtain a near-uniformly generated satisfying assignment. ApproxMC multiplies the number of satisfying assignments in the cell by a suitable scaling factor to obtain an estimate of the model count. ApproxMC is then repeated a number of times (depending on the desired confidence: 1 − δ) and the statistical median of computed counts taken to give the final approximate model count. For weighted model counting and sampling, the primary modification that needs to be done to ApproxMC and UniGen is that instead of requiring “cells” to have roughly equal numbers of satisfying assignments, we now require them to have roughly equal weights of satisfying assignments. To ensure that all weights lie in [0, 1], we scale weights by a factor of 1\nwmax . Unlike earlier works (Ermon et al. 2013a;\nErmon et al. 2013c), however, we do not require a MAPoracle to get wmax; instead we estimate wmax online without incurring any additional performance cost.\nA randomly chosen hash function from Hxor(n,m, 3) consists of m XOR constraints, each of which has expected size n/2. Although ApproxMC and UniWit were shown to scale for few thousands of variables, the performance erodes\nrapidly after a few thousand variables. It has recently been showin in (Chakraborty, Meel, and Vardi 2014) that by using random parity constraints on the independent support of a formula (which can be orders of magnitude smaller than the complete support), we can significantly reduce the size of XOR constraints. We use this idea in our work. For all our benchmark problems, obtaining the independent support of CNF formulae has been easy, once we examine the domain from which the problem originated.\nBoth WeightMC and WeightGen assume access to a subroutine called BoundedWeightSAT that takes a CNF formula F , a “pivot”, an upper bound r of the tilt and an upper bound wmax of the maximum weight of a satisfying assignment in the independent support set S. It returns a set of satisfying assignments of F such that the total weight of the returned assignments scaled by 1/wmax exceeds pivot. It also updates the minimum weight of a satisfying assignment seen so far and returns the same. BoundedWeightSAT accesses a subroutineAddBlockClause that takes as inputs a formula F and a projected assignment σ|S , computes a blocking clause for σ|S , and returns the formula F ′ obtained by conjoining F with the blocking clause thus obtained. Both algorithms also accept as input a positive real-valued parameter r which is an upper bound on ρ. Finally, the algorithms assume access to an NP-oracle, which in particular can decide SAT."
    }, {
      "heading" : "4.1 WeightMC Algorithm",
      "text" : "The pseudocode for WeightMC is shown in Algorithm 1. The algorithm takes a CNF formula F , tolerance ε ∈ (0, 1), confidence parameter δ ∈ (0, 1), independent support S, and tilt upper bound r, and returns an approximate weighted model count. WeightMC invokes an auxiliary procedure WeightMCCore that computes an approximate weighted model count by randomly partitioning the space of satisfying assignments using hash functions from the family Hxor(|S|,m, 3), where S denotes an independent support of F . After invoking WeightMCCore sufficiently many times, WeightMC returns the median of the non-⊥ counts returned by WeightMCCore.\nTheorem 1. Given a propositional formula F , ε ∈ (0, 1), δ ∈ (0, 1), independent support S, and tilt bound r, suppose WeightMC(F, ε, δ, S, r) returns c. Then Pr [ (1 + ε) −1\n· w(RF )) ≤ c ≤ (1 + ε) · w(RF ))] ≥ 1− δ.\nTheorem 2. Given an oracle for SAT, WeightMC(F, ε, δ, S, r) runs in time polynomial in log2(1/δ), r, |F | and 1/ε relative to the oracle.\nWe defer all proofs to the supplementary material for lack of space."
    }, {
      "heading" : "4.2 WeightGen Algorithm",
      "text" : "The pseudocode for WeightGen is presented in Algorithm 4. WeightGen takes in a CNF formula F , tolerance ε > 1.71, tilt upper bound r, and independent support S and returns a random (approximately weighted-uniform) satisfying assignment. WeightGen first computes κ and pivot and uses them to compute hiThresh and loThresh, which quantify the size of a “small” cell. The easy case of the weighted\ncount being less than hiThresh is handled in lines 6–9. Otherwise, WeightMC is called to estimate the weighted model count, which is used to estimate the range of candidate values for m. The choice of parameters for WeightMC is motivated by technical reasons. The loop in 13–19 terminates when a small cell is found and a sample is picked weighteduniformly at random. Otherwise, the algorithm reports a failure.\nTheorem 3. Given a CNF formula F , tolerance ε > 1.71, tilt bound r, and independent support S, for every y ∈ RF we have w(y)\n(1 + ε)w(RF ) ≤\nPr [WeightGen(F, ε, r,X) = y] ≤ (1 + ε) w(y)\nw(RF ) .\nAlso, WeightGen succeeds (i.e. does not return ⊥) with probability at least 0.62.\nTheorem 4. Given an oracle for SAT, WeightGen(F, ε, r, S) runs in time polynomial in r, |F | and 1/ε relative to the oracle."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "In our implementations of WeightGen and WeightMC, BoundedWeightSAT is implemented using CryptoMiniSAT (Cry ), a SAT solver that handles xor clauses efficiently. CryptoMiniSAT uses blocking clauses to prevent already generated witnesses from being generated again. Since the independent support of F determines every satisfying assignment of F , blocking clauses can be restricted to only variables in the set S. We implemented this optimization in CryptoMiniSAT, leading to significant improvements in performance. We used “random device” implemented in C++11 as source of pseudo-random numbers to make random choices in WeightGen and WeightMC.\nAlgorithm 1 WeightMC(F, ε, δ, S, r) 1: counter ← 0;C ← emptyList; wmax ← 1; 2: pivot ← 2 × ⌈e3/2 ( 1 + 1\nε\n)\n2\n⌉;\n3: t ← ⌈35 log2(3/δ)⌉; 4: repeat 5: (c,wmax) ← WeightMCCore(F, S,pivot, r,wmax); 6: counter ← counter + 1; 7: if c 6= ⊥ then 8: AddToList(C, c · wmax); 9: until counter < t 10: finalCount ← FindMedian(C); 11: return finalCount;"
    }, {
      "heading" : "5 Experimental Results",
      "text" : "To evaluate the performance of WeightGen and WeightMC, we built prototype implementations and conducted an extensive set of experiments. The suite of benchmarks was made up of problems arising from various practical domains as well as problems of theoretical interest. Specifically, we used bit-level unweighted versions of constraints arising from grid networks, plan recognition,\nAlgorithm 2 WeightMCCore(F, S, pivot, r,wmax) 1: (Y,wmax) ← BoundedWeightSAT(F, pivot, r,wmax, S); 2: if w(Y ) /wmax ≤ pivot then 3: return w(Y ); 4: else 5: i ← 0; 6: repeat 7: i ← i + 1; 8: Choose h at random from Hxor(|S|, i, 3); 9: Choose α at random from {0, 1}i ; 10: (Y,wmax) ← BoundedWeightSAT(F ∧ (h(x1, . . . x|S|) =\nα), pivot, ρ,wmax, S);\n11: until (0 < w(Y ) /wmax ≤ pivot) or i = n 12: if w(Y ) /wmax > pivot or w(Y ) = 0 then return (⊥,wmax); 13: elsereturn ( w(Y ) · 2i−1\nwmax ,wmax);\nAlgorithm 3 BoundedWeightSAT(F, pivot, r,wmax, S) 1: wmin ← wmax/r; wtotal ← 0;Y = {}; 2: repeat 3: y ← SolveSAT(F ); 4: if y == UNSAT then 5: break; 6: Y = Y ∪ y; 7: F = AddBlockClause(F, y|S); 8: wtotal ← wtotal + w(y); 9: wmin ← min(wmin, w(y)); 10: until wtotal/(wmin · r) > pivot; 11: return (Y,wmin · r);\nDQMR networks, bounded model checking of circuits, bitblasted versions of SMT-LIB (SMT ) benchmarks, and ISCAS89 (Brglez, Bryan, and Kozminski 1989) circuits with parity conditions on randomly chosen subsets of outputs and next-state variables (Sang, Bearne, and Kautz 2005; John and Chakraborty 2011). While our algorithm is agnostic to the weight oracle, other tools that we used for comparison require the weight of an assignment to be the product of the weights of its literals. Consequently, to create weighted problems with tilt at most some bound r, we randomly selected m = max(15, n/100) of the variables and assigned them the weight w such that (w/(1−w))m = r, their negations the weight 1−w, and all other literals the weight 1. Unless mentioned otherwise, our experiments for WeightMC used r = 3, ǫ = 0.8, and δ = 0.2, while our experiments for WeightGen used r = 3 and ǫ = 5.\nTo facilitate performing multiple experiments in parallel, we used a high performance cluster, each experiment running on its own core. Each node of the cluster had two quad-core Intel Xeon processors with 4GB of main memory. We used 2500 seconds as the timeout of each invocation of BoundedWeightSAT and 20 hours as the overall timeout for WeightGen and WeightMC. If an invocation of BoundedWeightSAT timed out in line 10 (WeightMC) and line 17 (WeightGen), we repeated the execution of the corresponding loops without incrementing the variable i (in both algorithms). With this setup, WeightMC and WeightGen were able to successfully return weighted counts and generate weighted random instances for formulas with close to 64,000 variables.\nWe compared the performance of WeightMC with the SDD Package (sdd ), a state-of-the-art tool which\nAlgorithm 4 WeightGen(F, ε, r, S) /*Assume ε > 1.71 */\n1: wmax ← 1; Samples = {}; 2: (κ, pivot) ← ComputeKappaPivot(ε); 3: hiThresh ← 1 + (1 + κ)pivot; 4: loThresh ← 1\n1 + κ pivot;\n5: (Y,wmax) ← BoundedWeightSAT(F, hiThresh, r,wmax, S); 6: if (w(Y ) /wmax ≤ hiThresh) then 7: Choose y weighted-uniformly at random from Y ; 8: return y; 9: else 10: (C,wmax) ← WeightMC(F, 0.8, 0.2); 11: q ← ⌈logC − log wmax + log 1.8 − log pivot⌉; 12: i ← q − 4; 13: repeat 14: i ← i + 1; 15: Choose h at random from Hxor(|S|, i, 3); 16: Choose α at random from {0, 1}i; 17: (Y,wmax) ← BoundedWeightSAT(F ∧ (h(x1, . . . x|S|) =\nα), hiThresh, r,wmax, S);\n18: W ← w(Y ) /wmax 19: until (loThresh ≤ W ≤ hiThresh) or (i = q) 20: if (W > hiThresh) or (W < loThresh) then return ⊥ 21: else Choose y weighted-uniformly at random from Y ; return y;\nAlgorithm 5 ComputeKappaPivot(ε)\n1: Find κ ∈ [0, 1) such that ε = (1 + κ)(2.36 + 0.51\n(1 − κ)2 ) − 1 ;\n2: pivot ← ⌈e3/2 ( 1 + 1\nκ\n)\n2\n⌉; return (κ, pivot)\ncan perform exact weighted model counting by compiling CNF formulae into Sentential Decision Diagrams (Choi and Darwiche 2013). (We also tried to compare our tools against Cachet, WISH and PAWS but we have not been able to run these tools on our systems.) Our results are shown in Table 1, where column 1 lists the benchmarks and columns 2 and 3 give the number of variables and clauses for each benchmark. Column 4 lists the time taken by WeightMC, while column 5 lists the time taken by SDD. We also measured the time taken by WeightGen to generate samples, which we will discuss later in this section, and list it i column 6. “T” and “mem” indicate that an experiment exceeded our imposed 20-hour and 4GB-memory limits, respectively. While SDD was generally superior for small problems, WeightMC was significantly faster for all benchmarks with more than 1,000 variables.\nTo evaluate the quality of the approximate counts returned by WeightMC, we computed exact weighted model counts using the SDD tool for a subset of our benchmarks. Figure 1 shows the counts returned by WeightMC, and the exact counts from SDD scaled up and down by (1 + ε). The weighted model counts are represented on the y-axis, while the x-axis represents benchmarks arranged in increasing order of counts. We observe, for all our experiments, that the weighted counts returned by WeightMC lie within the tolerance of the exact counts. Over all of the benchmarks, the L1 norm of the relative error was 0.036, demonstrating that in practice WeightMC is substantially more accurate than the theoretical guarantees provided by Theorem 3.\nIn another experiment, we studied the effect of different values of the tilt bound r on the runtime of WeightMC. Run-\ntime as a function r is shown for several benchmarks in Figure 3, where times have been normalized so that at the lowest tilt (r = 1) each benchmark took one time unit. Each runtime is an average over five runs on the same benchmark. The theoretical linear dependence on the tilt shown in Theorem 2 can be seen to roughly occur in practice.\nSince a probabilistic generator is likely to be invoked many times with the same formula and weights, it is useful to perform the counting on line 10 of WeightGen only once, and reuse the result for every sample. Reflecting this, column 6 in Table 1 lists the time, averaged over a large number of runs, taken by WeightGen to generate one sample given that the weighted model count on line 10 has already been found. It is clear from Table 1 that WeightGen scales to formulas with thousands of variables."
    }, {
      "heading" : "6 White-Box Weight Functions",
      "text" : "As noted above, the runtime of WeightMC is proportional to the tilt of the weight function, which means that the algorithm becomes impractical when the tilt is large. If the assignment weights are given by a known polynomial-timecomputable function instead of an oracle, we can do better. We abuse notation slightly and denote this weight function by w(X), where X is the set of support variables of the Boolean formula F . The essential idea is to partition the set of satisfying assignments into regions within which the tilt is small. Defining RF (a, b) = {σ ∈ RF |a < w(σ) ≤ b}, we have w(RF ) = w(RF (wmin, wmax)). If we use a partition of the form RF (wmin, wmax) = RF (wmax/2, wmax) ∪ RF (wmax/4, wmax/2) ∪ · · · ∪ RF (wmax/2 N , wmax/2 N−1), where wmax/2N ≤ wmin, then in each partition region the tilt is at most 2. Note that we do not need to know the actual values of wmin and wmax: any bounds L and H such that 0 < L ≤ wmin and wmax ≤ H will do (although if the bounds are too loose, we may partition RF into more regions than necessary). If assignment weights are poly-time computable, we can add to F a constraint that eliminates all assignments not in a particular region. So we can run WeightMC on each region in turn, passing 2 as the upper bound on the tilt, and sum the results to get w(RF ). This idea is implemented in PartitionedWeightMC (Algorithm 6).\nThe correctness and runtime of PartitionedWeightMC are established by the following theorems, whose proof is deferred to Appendix.\nAlgorithm 6 PartitionedWeightMC(F, ε, δ, S, L,H)\n1: N ← ⌈log2 H/L⌉+ 1; δ ′ ← δ/N ; c ← 0 2: for all 1 ≤ m ≤ N do 3: G ← F ∧ (H/2m < w(X) ≤ H/2m−1) 4: d ← WeightMC(G, ε, δ′, S, 2) 5: if (d = ⊥) then return ⊥ 6: c ← c+ d 7: return c\nTheorem 5. If PartitionedWeightMC(F, ε, δ, S, L,H) returns c (and all arguments are in the required ranges), then\nPr [ c 6= ⊥ ∧ (1 + ε)−1w(RF ) ≤ c ≤ (1 + ε)w(RF )) ] ≥ 1−δ.\nTheorem 6. With access to an NP oracle, the runtime of PartitionedWeightMC(F, ε, δ, S, L,H) is polynomial in |F |, 1/ε, log(1/δ), and log r = log(H/L).\nThe reduction of the runtime’s dependence on the tilt bound r from linear to logarithmic can be a substantial saving. If the assignment weights are products of literal weights, as is the case in many applications, the best a priori bound on the tilt ρ given only the literal weights is exponential in n. Thus, unless the structure of the problem allows a better bound on ρ to be used, WeightMC will not be practical. In this situation PartitionedWeightMC can be used to maintain polynomial runtime.\nWhen implementing PartitionedWeightMC in practice the handling of the weight constraint H/2m < w(X) ≤ H/2m−1 is critical to efficiency. If assignment weights are sums of literal weights, or equivalently products of literal weights (we just take logarithms), then the weight constraint is a pseudo-Boolean constraint. In this case we may replace the SAT-solver used by WeightMC with a pseudo-Poolean satisfiability (PBS) solver. While a number of PBS-solvers exist (Manquinho and Roussel 2012), none have the specialized handling of XOR clauses that is critical in making WeightMC practical. The design of such solvers is a clear direction for future work. We also note that the choice of 2 as the tilt bound for each region is arbitrary, and the value may be adjusted depending on the application: larger values will decrease the number of regions, but increase the difficulty of counting within each region. Finally, note that the same partitioning idea can be used to reduce WeightGen’s dependence on r to be logarithmic."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we considered approximate approaches to the twin problems of distribution-aware sampling and weighted model counting for SAT. For approximation techniques that provide strong theoretical two-way bounds, a major limitation is the reliance on potentially-expensive maximum a posteriori (MAP) queries. We showed how to remove this reliance on MAP queries, while retaining strong theoretical guarantees. First, we provided model counting and sampling algorithms that work with a black-box model of giving weights to assignments, requiring access only to an NPoracle, which is efficient for small tilt values. Experimental results demonstrate the effectiveness of this approach in practice. Second, we provide an alternative approach that\npromises to be efficient for tilt value, requiring, however, a white-box model of weighting and access to a pseudoBoolean solver. As a next step, we plan to empirically evaluate this latter approach using pseudo-Boolean solvers designed to handle parity constraints efficiently."
    }, {
      "heading" : "A Analysis of WeightMC",
      "text" : "In this section we denote the quantity log2 W (RF ) − log2 pivot+1 by m. For simplicity of exposition, we assume henceforth that m is an integer. A more careful analysis removes this restriction with only a constant factor scaling of the probabilities.\nLemma 4. Let algorithm WeightMCCore, when invoked from WeightMC, return c with i being the final value of the loop counter in WeightMCCore. Then Pr [ (1 + ε)−1 · W (RF ) ≤ c ≤ (1 + ε) · W (RF ) ∣ ∣ ∣ c 6= ⊥ ∧ i ≤ m ] ≥ 1− e−3/2.\nProof. Referring to the pseudocode of WeightMCCore, the lemma is trivially satisfied if W (RF ) ≤ pivot . Therefore, the only non-trivial case to consider is when W (RF ) > pivot and WeightMCCore returns from line 13. In this case, the count returned is 2i · W (RF,h,α), where α, i and h denote (with abuse of notation) the values of the corresponding variables and hash functions in the final iteration of the repeat-until loop in lines 6–11 of the pseudocode. From the pseudocode of WeightMCCore, we know that pivot = ⌈e3/2(1 + 1/ε)2⌉. The lemma is now proved by showing that for every i in {0, . . .m}, h ∈ H(n, i, 3), and α ∈ {0, 1}i, we have Pr [ (1 + ε)−1 · W (RF ) ≤ 2 iW (RF,h,α) ≤ (1 + ε) · W (RF )] ≥ 1− e −3/2.\nFor every y ∈ {0, 1}n and α ∈ {0, 1}i, define an indicator variable γy,α as follows: γy,α = W (y) if h(y) = α, and γy,α = 0 otherwise. Let us fix α and y and choose h uniformly at random from H(n, i, 3). The random choice of h induces a probability distribution on γy,α such that Pr [γy,α = W (y)] = Pr [h(y) = α] = 2\n−i, and E [γy,α] = W (y)Pr [γy,α = W (y)] = 2\n−iW (y). In addition, the 3- wise independence of hash functions chosen fromH(n, i, 3) implies that for every distinct ya, yb, yc ∈ RF , the random variables γya,α, γyb,α and γyc,α are 3-wise independent.\nLet Γα = ∑\ny∈RF\nγy,α and µα = E [Γα]. Clearly,\nΓα = W (RF,h,α) and µα = ∑\ny∈RF\nE [γy,α] = 2 −iW (RF ).\nTherefore, using Lemma 3 with β = ε/(1 + ε), we have\nPr\n[\nW (RF )\n(\n1− ε\n1 + ε\n)\n≤ 2iW (RF,h,α) ≤ (1 + ε\n1 + ε )W (RF )\n]\n≥ 1 − e−3/2. Simplifying and noting that ε 1 + ε < ε for all ε > 0, we obtain\nPr [ (1 + ε)−1 · W (RF ) ≤ 2 iW (RF,h,α) ≤ (1 + ε) · W (RF ) ] ≥ 1− e−3/2.\nLemma 5. Given W (RF ) > pivot , the probability that an invocation of WeightMCCore from WeightMC returns non⊥ with i ≤ m, is at least 1− e−3/2.\nProof. Let pi (0 ≤ i ≤ n) denote the conditional probability that WeightMCCore terminates in iteration i of the repeat-until loop (lines 6–11 of the pseudocode) with 0 < W (RF,h,α) ≤ pivot , given W (RF ) > pivot . Since the choice of h and α in each iteration of the loop are independent of those in previous iterations, the conditional probability that WeightMCCore returns non-⊥ with i ≤ m, given W (RF ) > pivot , is p0 + (1 − p0)p1 + · · · + (1 − p0)(1 − p1) · · · (1 − pm−1)pm. Let us denote this sum by P . Thus, P = p0 + m ∑\ni=1\ni−1 ∏\nk=0\n(1 − pk)pi ≥\n(\np0 +\nm−1 ∑\ni=1\ni−1 ∏\nk=0\n(1− pk)pi\n)\npm +\nm−1 ∏\ns=0\n(1 − ps)pm = pm.\nThe lemma is now proved by showing that pm ≥ 1− e−3/2. It was shown in the proof of Lemma 4 that Pr [ (1 + ε)−1 · W (RF ) ≤ 2 iW (RF,h,α) ≤ (1 + ε) · W (RF ) ] ≥ 1 − e−3/2 for every i ∈ {0, . . . ,m}, h ∈ H(n, i, 3) and α ∈ {0, 1}i. Substituting m for i, re-arranging terms and noting that the definition of m implies 2−mW (RF ) = pivot/2, we get Pr [ (1 + ε)−1(pivot/2) ≤ W (RF,h,α) ≤ (1 + ε)(pivot/2)] ≥ 1 − e −3/2. Since 0 < ε ≤ 1 and pivot > 4, it follows that Pr [0 < W (RF,h,α) ≤ pivot ] ≥ 1 − e\n−3/2. Hence, pm ≥ 1− e −3/2.\nLemma 6. Let an invocation of WeightMCCore from WeightMC return c. Then Pr [ c 6= ⊥ ∧ (1 + ε)−1 · w(RF ) ≤ c · wmax ≤ (1 + ε) · w(RF )] ≥ (1− e −3/2)2 > 0.6.\nProof. It is easy to see that the required probability is at least as large as Pr [\nc 6= ⊥ ∧ i ≤ m ∧ (1 + ε)−1w(RF ) ≤ c · wmax ≤ (1 + ε) · w(RF )]. Dividing by wmax and applying Lemmas 4 and 5, this probability is ≥ (1− e−3/2)2.\nWe now turn to proving that the confidence can be raised to at least 1 − δ for δ ∈ (0, 1] by invoking WeightMCCore O(log2(1/δ)) times, and by using the median of the non-⊥ counts thus returned. For convenience of exposition, we use η(t,m, p) in the following discussion to denote the probability of at least m heads in t independent tosses of a biased coin with Pr [heads ] = p. Clearly, η(t,m, p) =\nt ∑\nk=m\n(\nt\nk\n)\npk(1− p)t−k.\nTheorem 1. Given a propositional formula F and parameters ε (0 < ε ≤ 1) and δ (0 < δ ≤\n1), suppose WeightMC(F, ε, δ,X, r) returns c. Then Pr [ (1 + ε) −1\n· w(RF )) ≤ c ≤ (1 + ε) · w(RF ))] ≥ 1− δ.\nProof. Throughout this proof, we assume that WeightMCCore is invoked t times from WeightMC, where t = ⌈35 log2(3/δ)⌉ (see pseudocode for ComputeIterCount in Section ??). Referring to the pseudocode of WeightMC, the final count returned is the median of the non-⊥ counts obtained from the t invocations of WeightMCCore. Let Err denote the event that the median is not in [ (1 + ε)−1 · W (RF ) , (1 + ε) · W (RF ) ]\n. Let “#non⊥ = q” denote the event that q (out of t) values returned by WeightMCCore are non-⊥. Then, Pr [Err] = t ∑\nq=0\nPr [Err | #non⊥ = q] · Pr [#non⊥ = q].\nIn order to obtain Pr [Err | #non⊥ = q], we define a 0- 1 random variable Zi, for 1 ≤ i ≤ t, as follows. If the ith invocation of WeightMCCore returns c, and if c is either ⊥ or a non-⊥ value that does not lie in the interval [(1+ε)−1 ·W (RF ) , (1+ε)·W (RF )], we set Zi to 1; otherwise, we set it to 0. From Lemma 6, Pr [Zi = 1] = p < 0.4. If Z denotes t ∑\ni=1\nZi, a necessary (but not sufficient) condi-\ntion for event Err to occur, given that q non-⊥s were returned by WeightMCCore, is Z ≥ (t − q + ⌈q/2⌉). To see why this is so, note that t− q invocations of WeightMCCore must return ⊥. In addition, at least ⌈q/2⌉ of the remaining q invocations must return values outside the desired interval. To simplify the exposition, let q be an even integer. A more careful analysis removes this restriction and results in an additional constant scaling factor for Pr [Err]. With our simplifying assumption, Pr [Err | #non⊥ = q] ≤ Pr[Z ≥ (t − q + q/2)] = η(t, t − q/2, p). Since η(t,m, p) is a decreasing function of m and since q/2 ≤ t − q/2 ≤ t, we have Pr [Err | #non⊥ = q] ≤ η(t, t/2, p). If p < 1/2, it is easy to verify that η(t, t/2, p) is an increasing function of p. In our case, p < 0.4; hence, Pr [Err | #non⊥ = q] ≤ η(t, t/2, 0.4).\nIt follows from the above that Pr [Err] = t ∑\nq=0\nPr [Err | #non⊥ = q] · Pr [#non⊥ = q] ≤ η(t, t/2, 0.4)· t ∑\nq=0\nPr [#non⊥ = q] = η(t, t/2, 0.4). Since\n(\nt\nt/2\n)\n≥\n(\nt\nk\n)\nfor all t/2 ≤ k ≤ t, and since\n(\nt\nt/2\n)\n≤\n2t, we have η(t, t/2, 0.4) = t ∑\nk=t/2\n(\nt\nk\n)\n(0.4)k(0.6)t−k ≤\n(\nt\nt/2\n) t ∑\nk=t/2\n(0.4)k(0.6)t−k ≤ 2t t ∑\nk=t/2\n(0.6)t(0.4/0.6)k ≤"
    }, {
      "heading" : "2t ·3 ·(0.6×0.4)t/2 ≤ 3 ·(0.98)t. Since t = ⌈35 log2(3/δ)⌉,",
      "text" : "it follows that Pr [Err] ≤ δ.\nTheorem 2. Given an oracle for SAT, WeightMC(F, ε, δ, S, r) runs in time polynomial in log2(1/δ), r, |F | and 1/ε relative to the oracle.\nProof. Referring to the pseudocode for WeightMC, lines 1– 3 take O(1) time. The repeat-until loop in lines 4–9 is repeated t = ⌈35 log2(3/δ)⌉ times. The time taken for each iteration is dominated by the time taken by WeightMCCore. Finally, computing the median in line 10 takes time linear in t. The proof is therefore completed by showing that WeightMCCore takes time polynomial in |F |, r and 1/ε relative to the SAT oracle.\nReferring to the pseudocode for WeightMCCore, we find that BoundedWeightSAT is called O(|F |) times. Observe that when the loop in BoundedWeightSAT terminates, wmin is such that each y ∈ RF whose weight was added to wtotal has weight at least wmin. Thus since the loop terminates when wtotal/wmin > r · pivot, it can have iterated at most (r · pivot) + 1 times. Therefore each call to BoundedWeightSAT makes at most (r · pivot) + 1 calls to the SAT oracle, and takes time polynomial in |F |, r, and pivot relative to the oracle. Since pivot is in O(1/ε2), the number of calls to the SAT oracle, and the total time taken by all calls to BoundedWeightSAT in each invocation of WeightMCCore is polynomial in |F |, r and 1/ε relative to the oracle. The random choices in lines 8 and 9 of WeightMCCore can be implemented in time polynomial in n (hence, in |F |) if we have access to a source of random bits. Constructing F ∧ h(z1, . . . zn) = α in line 10 can also be done in time polynomial in |F |."
    }, {
      "heading" : "B Analysis of WeightGen",
      "text" : "For convenience of analysis, we assume that log(W (RF )− 1)− log pivot is an integer, where pivot is the quantity computed by algorithm ComputeKappaPivot (see Section 4). A more careful analysis removes this assumption by scaling the probabilities by constant factors. Let us denote log(W (RF ) − 1) − log pivot by m. The expression used for computing pivot in algorithm ComputeKappaPivot ensures that pivot ≥ 17. Therefore, if an invocation of WeightGen does not return from line 8 of the pseudocode, then W (RF ) ≥ 18. Note also that the expression for computing κ in algorithm ComputeKappaPivot requires ε ≥ 1.71 in order to ensure that κ ∈ [0, 1) can always be found.\nIn the case where W (RF ) ≤ 1 + (1 + κ)pivot, BoundedWeightSAT returns all witnesses of F and WeightGen returns a perfect weighted-uniform sample on line 8. So we restrict our attention in the lemmas below to the other case, where as noted above we haveW (RF ) ≥ 18. The following lemma shows that q, computed in line 11 of the pseudocode, is a good estimator of m.\nLemma 7. Pr[q − 3 ≤ m ≤ q] ≥ 0.8\nProof. Recall that in line 10 of the pseudocode, an approximate weighted model counter is invoked to obtain an estimate, C, of w(RF ) with tolerance 0.8 and confidence 0.8. By the definition of approximate weighted model counting, we have Pr[ C\n1.8 ≤ w(RF ) ≤ (1.8)C] ≥ 0.8.\nDefining c = C/wmax, we have Pr[log c − log(1.8) ≤ logW (RF ) ≤ log c + log(1.8)] ≥ 0.8. It follows that Pr[log c − log(1.8) − log pivot − log( 1\n1− 1/W (RF ) ) ≤\nlog(W (RF )−1)−log pivot ≤ log c−log pivot+log(1.8)− log( 1\n1− 1/W (RF ) )] ≥ 0.8. Substituting q = ⌈logC − logwmax + log 1.8 − log pivot⌉ = ⌈log c + log 1.8 − log pivot⌉, and using the bounds wmax ≤ 1, log 1.8 ≤ 0.85, and log( 1\n1− 1/W (RF ) ) ≤ 0.12 (since W (RF ) ≥ 18\nat line 10 of the pseudocode, as noted above), we have Pr[q − 3 ≤ m ≤ q] ≥ 0.8.\nThe next lemma provides a lower bound on the probability of generation of a witness. Let wi,y,α denote the probability\nPr\n[\npivot 1 + κ ≤ W (RF,h,α) ≤ 1 + (1 + κ)pivot ∧ h(y) = α\n]\n,\nwith h R ←− Hxor(n, i, 3). The proof of the lemma also provides a lower bound on wm,y,α.\nLemma 8. For every witness y ∈ RF , Pr[y is output] ≥ 0.8(1− e−3/2)W (y)\n(1.06 + κ)(W (RF )− 1)\nProof. Let U denote the event that witness y ∈ RF is output by WeightGen on inputs F , ε, r, and X . Let pi,y denote the probability that we exit the loop at line 19 with a particular value of i and y ∈ RF,h,α, where α ∈ {0, 1}i is the value chosen on line 16. Then, Pr[U ] =\nq ∑\ni=q−3\nW (y) W (Y ) pi,y\ni−1 ∏\nj=q−3\n(1 − pj,y), where Y is the set re-\nturned by BoundedWeightSAT on line 17. Let fm = Pr[q− 3 ≤ m ≤ q]. From Lemma 7, we know that fm ≥ 0.8. From line 20, we also know that 1\n1 + κ pivot ≤ W (Y ) ≤ 1+(1+\nκ)pivot. Therefore, Pr[U ] ≥ W (y)\n1 + (1 + κ)pivot · pm,y · fm.\nThe proof is now completed by showing pm,y ≥ 1\n2m (1 −\ne−3/2), as then we have Pr[U ] ≥ 0.8(1− e−3/2)\n(1 + (1 + κ)pivot)2m ≥\n0.8(1− e−3/2)\n(1.06 + κ)(W (RF ) | − 1) . The last inequality uses the observation that 1/pivot ≤ 0.06. To calculate pm,y, we first note that since y ∈ RF , the requirement “y ∈ RF,h,α” reduces to “y ∈ h−1(α)”. For α ∈ {0, 1}n, we define\nwm,y,α = Pr\n[\npivot 1 + κ ≤ W (RF,h,α) ≤ 1 + (1 + κ)\npivot ∧ h(y) = α : h R ←− Hxor(n,m, 3)\n]\n. Then we\nhave pm,y = Σα∈{0,1}m ( wm,y,α · 2 −m )\n. So to prove the desired bound on pm,y it suffices to show that wm,y,α ≥ (1 − e\n−3/2)/2m for every α ∈ {0, 1}m and y ∈ {0, 1}n.\nTowards this end, let us first fix a random y. Now we define an indicator variable γz,α for every z ∈ RF \\ {y} such that γz,α = W (z) if h(z) = α, and γz,α = 0 otherwise. Let us fix α and choose h uniformly at random from Hxor(n,m, 3). The random choice of h induces a probability distribution on γz,α such that E[γz,α] = W (z)Pr[γz,α = W (z)] = W (z)Pr[h(z) = α] = W (z) /2m. Since we have fixed y, and since hash functions chosen from Hxor(n,m, 3) are 3-wise independent, it follows that for every distinct za, zb ∈ RF \\ {y}, the random variables γza,α, γzb,α are 2-wise independent. Let Γα = ∑\nz∈RF \\{y}\nγz,α and µα =\nE[Γα]. Clearly, Γα = W (RF,h,α) − W (y) and µα = ∑\nz∈W(RF )\\{y}\nE[γz,α] = (W (RF ) − W (y))/2 m. Since\npivot = (W (RF ) − 1)/2 m ≤ (W (RF ) − W (y))/2 m, we have Pr[ pivot\n1 + κ ≤ W (RF,h,α) ≤ 1 + (1 + κ)pivot]\n≥ Pr[ W (RF )−W (y)\n(1 + κ)2m ≤ W (RF,h,α) ≤ 1 + (1 +\nκ) W (RF )− 1\n2m ] ≥ Pr[\nW (RF )−W (y)\n2m(1 + κ) ≤ W (RF,h,α)−\nW (y) ≤ (1 + κ) (W (RF )−W (y))\n2m ]. Since pivot =\n⌈e3/2(1 + 1/κ)2⌉ and the variables γz,α are 2-wise independent and in the range [0, 1], we may apply Lemma 3 with β = κ/(1 + κ) to obtain Pr[ pivot\n1 + κ ≤ W (RF,h,α) ≤\n1+ (1+κ)pivot] ≥ 1− e−3/2. Since h is chosen at random from Hxor(n,m, 3), we also have Pr[h(y) = α] = 1/2\nm. It follows that wm,y,α ≥ (1− e −3/2)/2m.\nThe next lemma provides an upper bound of wi,y,α and pi,y.\nLemma 9. For i < m, both wi,y,α and pi,y are bounded above by 1\nW (RF )− 1\n1 (\n1− 1+κ2m−i )2 .\nProof. We will use the terminology introduced in the proof of Lemma 8. Clearly, µα = W (RF )−W (y)\n2i . Since each\nγz,α takes values in [0, 1], V [γz,α] ≤ E [γz,α]. Therefore, σ2z,α ≤ ∑\nz 6=y,z∈RF\nE [γz,α] ≤ ∑\nz∈RF\nE [γz,α] = E [Γα] ≤\n2−m(W (RF ) − W (y)). So Pr[ pivot\n1 + κ ≤ W (RF,h,α) ≤\n1 + (1 + κ)pivot] ≤ Pr[W (RF,h,α) − W (y) ≤ (1 + κ)pivot]. From Chebyshev’s inequality, we know that Pr [|Γα − µz,α| ≥ λσz,α] ≤ 1/λ 2 for every λ > 0. Pr[W (RF,h,α) − W (y) ≤ (1 + κ) (W (RF )−W (y))\n2i ]\n≤ Pr\n[\n|(W (RF,h,α)−W (y))− W (RF )− 1\n2i |\n≥ (1− 1 + κ 2m−i ) W (RF )−W (y) 2i\n]\n≤ 1\n(\n1− (1+κ)2m−i\n)2 ·\n2i\nW (RF )− 1 . Since h is chosen at random from Hxor(n,m, 3), we also have Pr[h(y) = α] = 1/2i. It follows that wi,y,α ≤ 1\nW (RF )− 1\n1 (\n1− 1+κ2m−i )2 .\nThe bound for pi,y is easily obtained by noting that pi,y = Σα∈{0,1}i ( wi,y,α · 2 −i ) .\nLemma 10. For every witness y ∈ RF , Pr[y is output] ≤ (1 + κ)W (y)\nW (RF )− 1 (2.23 +\n0.48\n(1− κ)2 )\nProof. We will use the terminology introduced in the proof of Lemma 8. Using pivot\n1 + κ ≤ W (Y ), we\nhave Pr[U ] = q ∑\ni=q−3\nW (y) W (Y ) pi,y\ni ∏\nj=q−3\n(1 − pj,y) ≤\n1 + κ pivot W (y)\nq ∑\ni=q−3\npi,y . Now we subdivide the calculation\nof Pr[U ] into three cases depending on the value of m. Case 1 : q − 3 ≤ m ≤ q. Now there are four values that m can take.\n1. m = q − 3. We know that pi,y ≤ Pr[h(y) = α] = 1\n2i ,\nso Pr[U |m = q − 3] ≤ 1 + κ pivot · W (y) 2q−3 15 8 . Substituting the values of pivot and m gives Pr[U |m = q − 3] ≤ 15(1 + κ)W (y)\n8(W (RF )− 1) .\n2. m = q − 2. For i ∈ [q − 2, q] pi,y ≤\nPr[h(y) = α] = 1\n2i Using Lemma 9, we get\npq−3,y ≤ 1\nW (RF )− 1\n1 (\n1− 1+κ2 )2 . Therefore, Pr[U\n|m = q − 2] ≤ 1 + κ\npivot W (y)\n1\nW (RF )− 1\n4\n(1− κ) 2 +\n1 + κ pivot W (y) 1 2q−2 7 4 . Noting that pivot = W (RF )− 1\n2m > 10, we obtain Pr[U |m = q − 2] ≤\n(1 + κ)W (y) W (RF )− 1 ( 7 4 + 0.4 (1 − κ)2 )\n3. m = q− 1. For i ∈ [q− 1, q], pi,y ≤ Pr[h(y) = α] = 1\n2i .\nUsing Lemma 9, we get pq−3,y + pq−2,y ≤ 1\nW (RF )− 1\n( 1 (\n1− 1+κ22 )2 +\n1 (\n1− 1+κ2 )2\n)\n=\n1\nW (RF )− 1\n(\n16\n(3 − κ)2 +\n4\n(1 − κ)2\n)\n. Therefore,\nPr[U |m = q − 1] ≤ 1 + κ\npivot W (y)\n( 1\nW (RF )− 1\n(\n16\n(3− κ)2 +\n4\n(1− κ)2\n)\n+ 1 2q−1 3 2 ) . Since\npivot = W (RF )− 1\n2m > 10 and κ ≤ 1,\nPr[U |m = q − 1] ≤ (1 + κ)W (y)\nW (RF )− 1 (1.9 +\n0.4\n(1 − κ)2 ).\n4. m = q. We have pq,y ≤ Pr[h(y) = α] = 1\n2q , and\nusing Lemma 9 we get pq−3,y + pq−2,y + pq−1,y ≤\n1\nW (RF )− 1\n(\n1 (\n1− 1+κ23 )2 +\n1 (\n1− 1+κ22 )2 +\n1 (\n1− 1+κ2 )2\n)\n=\n1\nW (RF )− 1\n(\n64\n(7− κ)2 +\n16\n(3− κ)2 +\n4\n(1− κ)2\n)\n.\nSo Pr[U |m = q] ≤ 1 + κ pivot W (y) ( 1\nW (RF )− 1\n(\n64\n(7 − κ)2 +\n16\n(3 − κ)2 +\n4\n(1 − κ)2\n)\n+ 1\nUsing pivot = W (RF )− 1\n2m > 10 and κ ≤ 1, we obtain\nPr[U |m = q] ≤ (1 + κ)W (y)\nW (RF )− 1 (1.58 +\n0.4\n(1− κ)2 ).\nSince Pr[U |q − 3 ≤ m ≤ q] ≤ max q−3≤i≤q (Pr[U |m = i]), we havePr[U |q−3 ≤ m ≤ q] ≤ 1 + κ\nW (RF )− 1 (1.9+\n0.4\n(1− κ)2 )\nfrom the m = q − 1 case above. Case 2 : m < q − 3. Since pi,y ≤ Pr[h(y) = α] = 1\n2i ,\nwe have Pr[U |m < q − 3] ≤ 1 + κ\npivot W (y) ·\n1 2q−3 15 8 .\nSubstituting the value of pivot and maximizing m− q + 3, we get Pr[U |m < q − 3] ≤ 15(1 + κ)W (y)\n16(W (RF )− 1) .\nCase 3 : m > q. Using Lemma 9, we know that\nPr[U |m > q] ≤ 1 + κ\npivot\nW (y)\nW (RF )− 1\nq ∑\ni=q−3\n1 (\n1− 1+κ2m−i )2 .\nThe R.H.S. is maximized whenm = q+1. HencePr[U |m >\nq] ≤ 1 + κ\npivot\nW (y)\nW (RF )− 1 ×\nq ∑\ni=q−3\n1 (\n1− 1+κ2q+1−i )2 . Not-\ning that pivot = W (RF )− 1\n2m > 10 and expand-\ning the above summation we have Pr[U |m > q] ≤ (1 + κ)W (y)\nW (RF )− 1\n1\n10\n(\n256\n(15− κ)2 +\n64\n(7− κ)2 +\n16\n(3− κ)2 +\n2\n(1− κ)2\n)\n.\nUsing κ ≤ 1 for the first three summation terms, we obtain Pr[U |m > q] ≤ (1 + κ)W (y)\nW (RF )− 1 (0.71 +\n0.4\n(1− κ)2 )\nSumming up all the above cases, Pr[U ] = Pr[U |m < q−3]×Pr[m < q−3]+Pr[U |q−3 ≤ m ≤ q]×Pr[q−3 ≤ m ≤ q] + Pr[U |m > q] × Pr[m > q]. From Lemma 7 we have Pr[m < q − 1] ≤ 0.2 and Pr[m > q] ≤ 0.2, so\nPr[U ] ≤ (1 + κ)W (y)\nW (RF )− 1 (2.23 +\n0.48\n(1− κ)2 )\nCombining Lemmas 8 and 10, the following lemma is obtained.\nLemma 11. For every witness y ∈ RF , if ε > 1.71, then w(y) (1 + ε)w(RF ) ≤ Pr [WeightGen(F, ε, r,X) = y] ≤ (1 + ε) w(y)\nw(RF ) .\nProof. In the case where W (RF ) ≤ 1 + (1 + κ)pivot, the result holds because WeightGen returns a perfect weighted-uniform sample. Otherwise, using Lemmas 8 and 10 and substituting (1 + ε) = (1 + κ)(2.36 + 0.51\n(1 − κ)2 ) =\n18 17 (1 + κ)(2.23 + 0.48 (1− κ)2 ),\nvia the inequality 1.06 + κ\n0.8(1− e−3/2) ≤\n18 17 (1 + κ)(2.23 +\n0.48\n(1− κ)2 ) we have the bounds\nW (y)\n(1 + ε)(W (RF )− 1) ≤\nPr [WeightGen(F, ε, r,X) = y] ≤ 18\n17 (1 + ε)\nW (y)\nW (RF )− 1 .\nUsing W (RF ) ≥ 18, we obtain the desired result.\nLemma 12. Algorithm WeightGen succeeds (i.e. does not return ⊥) with probability at least 0.62.\nProof. If W (RF ) ≤ 1 + (1 + κ)pivot, the theorem holds trivially. Suppose W (RF ) > 1 + (1 + κ)pivot and let Psucc denote the probability that a run of the algorithm succeeds. Let pi with q − 3 ≤ i ≤ q denote the conditional probability that WeightGen (F , ε, r, X) terminates in iteration i of the repeat-until loop (lines 13–19) with pivot\n1 + κ ≤\nW (RF,h,α) ≤ 1 + (1 + κ)pivot, given that W (RF ) > 1 + (1 + κ)pivot. Then Psucc = q ∑\ni=q−3\npi\ni ∏\nj=q−3\n(1 − pj).\nLetting fm = Pr[q − 3 ≤ m ≤ q], by Lemma 7 we have Psucc ≥ pmfm ≥ 0.8pm. The theorem is now proved by using Lemma 3 to show that pm ≥ 1− e−3/2 ≥ 0.776. For every y ∈ {0, 1}n and α ∈ {0, 1}m, define an indicator variable νy,α as follows: νy,α = W (y) if h(y) = α, and νy,α = 0 otherwise. Let us fix α and y and choose h uniformly at random from Hxor(n,m, 3). The random choice of h induces a probability distribution on νy,α, such that Pr[νy,α = W (y)] = Pr[h(y) = α] = 2\n−m and E[νy,α] = W (y)Pr[νy,α = 1] = 2\n−mW (y). In addition 3-wise independence of hash functions chosen from Hxor(n,m, 3) implies that for every distinct ya, yb, yc ∈ RF , the random variables νya,α, νyb,α and νyc,α are 3-wise independent.\nLet Γα = ∑\ny∈RF\nνy,α and µα = E [Γα]. Clearly,\nΓα = W (RF,h,α) and µα = ∑\ny∈RF\nE [νy,α] =\n2−mW (RF ). Since pivot = ⌈e3/2(1 + 1/ǫ)2⌉, we have 2−mW (RF ) ≥ e3/2(1 + 1/ε)2, and so\nusing Lemma 3 with β = κ/(1 + κ) we ob-\ntain Pr\n[\nW (RF )\n2m .\n(\n1− κ\n1 + κ\n)\n≤ W (RF,h,α)\n≤ (1 + κ 1 + κ ) W (RF ) 2m\n]\n> 1 − e−3/2. Simplify-\ning and noting that κ\n1 + κ < κ for all κ > 0,\nwe have Pr\n[\n(1 + κ)−1 · W (RF )\n2m ≤ W (RF,h,α)\n≤ (1 + κ) · W (RF )\n2m\n]\n> 1 − e−3/2. Also, pivot\n1 + κ =\n1\n1 + κ\nW (RF )− 1\n2m ≤\nW (RF )\n(1 + κ)2m and 1 + (1 + κ)pivot =\n1 + (1 + κ)(W (RF )− 1)\n2m ≥\n(1 + κ)W (RF )\n2m .\nTherefore, pm = Pr[ pivot\n1 + κ ≤ W (RF,h,α) ≤\n1 + (1 + κ)pivot] ≥ Pr\n[\n(1 + κ)−1 · W (RF )\n2m\n≤ W (RF,h,α) ≤ (1 + κ) · W (RF )\n2m\n]\n≥ 1− e−3/2.\nBy combining Lemmas 11 and 12, we get the following:\nTheorem 3. Given a CNF formula F , tolerance ε > 1.71, tilt bound r, and independent support S, for every y ∈ RF we have w(y)\n(1 + ε)w(RF ) ≤\nPr [WeightGen(F, ε, r,X) = y] ≤ (1 + ε) w(y)\nw(RF ) .\nAlso, WeightGen succeeds (i.e. does not return ⊥) with probability at least 0.62.\nTheorem 4. Given an oracle for SAT, WeightGen(F, ε, r, S) runs in time polynomial in r, |F | and 1/ε relative to the oracle.\nProof. Referring to the pseudocode for WeightGen, the runtime of the algorithm is bounded by the runtime of the constant number (at most 5) of calls to BoundedWeightSAT and one call to WeightMC (with parameters δ = 0.2, ε = 0.8). As shown in Theorem 1, the call to WeightMC can be done in time polynomial in |F | and r relative to the oracle. Every invocation of BoundedWeightSAT can be implemented by at most (r · pivot) + 1 calls to a SAT oracle (as in the proof of Theorem 2), and the total time taken by all calls to BoundedWeightSAT is polynomial in |F |, r and pivot relative to the oracle. Since pivot = O(1/ε2), the runtime of WeightGen is polynomial in r, |F | and 1/ε relative to the oracle."
    }, {
      "heading" : "C Analysis of Partitioned WeightMC",
      "text" : "Theorem 5. If PartitionedWeightMC(F, ε, δ, S, L,H) returns c (and all arguments are in the required ranges), then\nPr [ c 6= ⊥ ∧ (1 + ε)−1w(RF ) ≤ c ≤ (1 + ε)w(RF )) ] ≥ 1−δ.\nProof. For future reference note that since N ≥ 1 and δ < 1, we have (1 − δ′)N = (1 − δ/N)N ≥ 1 − δ.\nDefine Gm = F ∧ (H/2m < w(X) ≤ H/2m−1), the formula passed to WeightMC in iteration m. Clearly, we have w(RF ) = N ∑\nm=1\nw(RGm). Since w(·) is poly-time com-\nputable, the NP oracle used in WeightMC can decide the satisfiability of Gm, and so WeightMC will return a value dm. Now since H/2m and H/2m−1 are lower and upper bounds respectively on the weights of any solution to Gm, by Theorem 1 we have\nPr [ dm 6= ⊥ ∧ (1 + ε) −1w(RGm) ≤ dm ≤ (1 + ε)w(RGm) ] ≥ 1−δ′\nfor every m, and so\nPr [ c 6= ⊥ ∧ (1 + ε)−1w(RF ) ≤ c ≤ (1 + ε)w(RF ) ]\n= Pr\n[\nc 6= ⊥ ∧ (1 + ε)−1 ∑\nm\nw(RGm) ≤ c ≤ (1 + ε) ∑\nm\nw(RGm)\n]\n≥ (1− δ′) N ≥ 1− δ\nas desired.\nTheorem 6. With access to an NP oracle, the runtime of PartitionedWeightMC(F, ε, δ, S, L,H) is polynomial in |F |, 1/ε, log(1/δ), and log r = log(H/L).\nProof. Put r = H/L. By Theorem 2, each call to WeightMC runs in time polynomial in |G|, 1/ε and log(1/δ′) (the tilt bound is constant). Clearly |G| is polynomial in |F |. Since δ′ = δ/N we have log(1/δ′) = log(N/δ) = O(log((logk r)/δ)) = O(log log r + log(1/δ)). Therefore each call to WeightMC runs in time polynomial in |F |, 1/ε, log(1/δ), and log log r. Since there are N = O(log r) calls, the result follows."
    } ],
    "references" : [ {
      "title" : "Algorithms and complexity results for #SAT and Bayesian inference",
      "author" : [ "Dalmao Bacchus", "F. Pitassi 2003] Bacchus", "S. Dalmao", "T. Pitassi" ],
      "venue" : "In Proc. of FOCS,",
      "citeRegEx" : "Bacchus et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bacchus et al\\.",
      "year" : 2003
    }, {
      "title" : "Uniform generation of NPwitnesses using an NP-oracle",
      "author" : [ "Goldreich Bellare", "M. Petrank 1998] Bellare", "O. Goldreich", "E. Petrank" ],
      "venue" : "Information and Computation",
      "citeRegEx" : "Bellare et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bellare et al\\.",
      "year" : 1998
    }, {
      "title" : "Combinational profiles of sequential benchmark circuits",
      "author" : [ "Bryan Brglez", "F. Kozminski 1989] Brglez", "D. Bryan", "K. Kozminski" ],
      "venue" : "In ISCAS",
      "citeRegEx" : "Brglez et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Brglez et al\\.",
      "year" : 1989
    }, {
      "title" : "A scalable approximate model counter",
      "author" : [ "Meel Chakraborty", "S. Vardi 2013a] Chakraborty", "K.S. Meel", "M.Y. Vardi" ],
      "venue" : "In Proc. of CP,",
      "citeRegEx" : "Chakraborty et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chakraborty et al\\.",
      "year" : 2013
    }, {
      "title" : "A scalable and nearly uniform generator of SAT witnesses",
      "author" : [ "Meel Chakraborty", "S. Vardi 2013b] Chakraborty", "K. Meel", "M. Vardi" ],
      "venue" : "In Proc. of CAV",
      "citeRegEx" : "Chakraborty et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chakraborty et al\\.",
      "year" : 2013
    }, {
      "title" : "Balancing scalability and uniformity in sat witness generator",
      "author" : [ "Meel Chakraborty", "S. Vardi 2014] Chakraborty", "K. Meel", "M. Vardi" ],
      "venue" : "In To Appear in DAC",
      "citeRegEx" : "Chakraborty et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chakraborty et al\\.",
      "year" : 2014
    }, {
      "title" : "and Darwiche",
      "author" : [ "M. Chavira" ],
      "venue" : "A.",
      "citeRegEx" : "Chavira and Darwiche 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "and Darwiche",
      "author" : [ "A. Choi" ],
      "venue" : "A.",
      "citeRegEx" : "Choi and Darwiche 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "M",
      "author" : [ "F.J. Dıez", "Druzdzel" ],
      "venue" : "J.",
      "citeRegEx" : "Dıez and Druzdzel 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "and Hoffmann",
      "author" : [ "C. Domshlak" ],
      "venue" : "J.",
      "citeRegEx" : "Domshlak and Hoffmann 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Taming the curse of dimensionality: Discrete integration by hashing and optimization",
      "author" : [ "Ermon" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "Ermon,? \\Q2013\\E",
      "shortCiteRegEx" : "Ermon",
      "year" : 2013
    }, {
      "title" : "B",
      "author" : [ "S. Ermon", "C.P. Gomes", "A. Sabharwal", "Selman" ],
      "venue" : "2013b. Embed and project: Discrete sampling with universal hashing. In Proc of NIPS, 2085–",
      "citeRegEx" : "Ermon et al. 2013b",
      "shortCiteRegEx" : null,
      "year" : 2093
    }, {
      "title" : "Optimization with parity constraints: From binary codes to discrete integration",
      "author" : [ "Ermon" ],
      "venue" : null,
      "citeRegEx" : "Ermon,? \\Q2013\\E",
      "shortCiteRegEx" : "Ermon",
      "year" : 2013
    }, {
      "title" : "Low-density parity constraints for hashing-based discrete integration",
      "author" : [ "Ermon" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "Ermon,? \\Q2014\\E",
      "shortCiteRegEx" : "Ermon",
      "year" : 2014
    }, {
      "title" : "and Dechter",
      "author" : [ "V. Gogate" ],
      "venue" : "R.",
      "citeRegEx" : "Gogate and Dechter 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Model counting: A new strategy for obtaining good bounds",
      "author" : [ "Sabharwal Gomes", "C. Selman 2006] Gomes", "A. Sabharwal", "B. Selman" ],
      "venue" : "In Proc. of AAAI,",
      "citeRegEx" : "Gomes et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Gomes et al\\.",
      "year" : 2006
    }, {
      "title" : "Near uniform sampling of combinatorial spaces using XOR constraints",
      "author" : [ "Sabharwal Gomes", "C. Selman 2007] Gomes", "A. Sabharwal", "B. Selman" ],
      "venue" : "In Proc. of NIPS,",
      "citeRegEx" : "Gomes et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gomes et al\\.",
      "year" : 2007
    }, {
      "title" : "and Sinclair",
      "author" : [ "M. Jerrum" ],
      "venue" : "A.",
      "citeRegEx" : "Jerrum and Sinclair 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Random generation of combinatorial structures from a uniform distribution",
      "author" : [ "Valiant Jerrum", "M. Vazirani 1986] Jerrum", "L. Valiant", "V. Vazirani" ],
      "venue" : null,
      "citeRegEx" : "Jerrum et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Jerrum et al\\.",
      "year" : 1986
    }, {
      "title" : "and Chakraborty",
      "author" : [ "A. John" ],
      "venue" : "S.",
      "citeRegEx" : "John and Chakraborty 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Monte-Carlo approximation algorithms for enumeration problems",
      "author" : [ "Luby Karp", "R. Madras 1989] Karp", "M. Luby", "N. Madras" ],
      "venue" : "Journal of Algorithms",
      "citeRegEx" : "Karp et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Karp et al\\.",
      "year" : 1989
    }, {
      "title" : "M",
      "author" : [ "S. Kirkpatrick", "C.D. Gelatt", "Vecchi" ],
      "venue" : "P.",
      "citeRegEx" : "Kirkpatrick. Gelatt. and Vecchi 1983",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "and Piccioni",
      "author" : [ "N. Madras" ],
      "venue" : "M.",
      "citeRegEx" : "Madras and Piccioni 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "and Roussel",
      "author" : [ "V. Manquinho" ],
      "venue" : "O.",
      "citeRegEx" : "Manquinho and Roussel 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Constraintbased random stimuli generation for hardware verification",
      "author" : [ "Naveh" ],
      "venue" : "In Proc of IAAI,",
      "citeRegEx" : "Naveh,? \\Q2006\\E",
      "shortCiteRegEx" : "Naveh",
      "year" : 2006
    }, {
      "title" : "J",
      "author" : [ "Park" ],
      "venue" : "D.",
      "citeRegEx" : "Park 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Performing bayesian inference by weighted model counting",
      "author" : [ "Bearne Sang", "T. Kautz 2005] Sang", "P. Bearne", "H. Kautz" ],
      "venue" : "In Prof. of AAAI,",
      "citeRegEx" : "Sang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Sang et al\\.",
      "year" : 2005
    }, {
      "title" : "Combining component caching and clause learning for effective model counting",
      "author" : [ "Sang" ],
      "venue" : "In Proc. of SAT",
      "citeRegEx" : "Sang,? \\Q2004\\E",
      "shortCiteRegEx" : "Sang",
      "year" : 2004
    }, {
      "title" : "M",
      "author" : [ "M.J. Wainwright", "Jordan" ],
      "venue" : "I.",
      "citeRegEx" : "Wainwright and Jordan 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Basing decisions on sentences in decision diagrams",
      "author" : [ "Choi Xue", "Y. Darwiche 2012] Xue", "A. Choi", "A. Darwiche" ],
      "venue" : null,
      "citeRegEx" : "Xue et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "[Chavira and Darwiche 2008] Chavira, M.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : "[Choi and Darwiche 2013] Choi, A.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 8,
      "context" : "[Dıez and Druzdzel 2006] Dıez, F.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "[Domshlak and Hoffmann 2007] Domshlak, C.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "[Ermon et al. 2013b] Ermon, S.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "[Gogate and Dechter 2011] Gogate, V.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "[Jerrum and Sinclair 1996] Jerrum, M.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "[John and Chakraborty 2011] John, A.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 21,
      "context" : "[Kirkpatrick, Gelatt, and Vecchi 1983] Kirkpatrick, S.",
      "startOffset" : 0,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "[Madras and Piccioni 1999] Madras, N.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 23,
      "context" : "[Manquinho and Roussel 2012] Manquinho, V.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 25,
      "context" : "[Park 2002] Park, J.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 28,
      "context" : "[Wainwright and Jordan 2008] Wainwright, M.",
      "startOffset" : 0,
      "endOffset" : 28
    } ],
    "year" : 2014,
    "abstractText" : "Given a CNF formula and a weight for each assignment of values to variables, two natural problems are weighted model counting and distribution-aware sampling of satisfying assignments. Both problems have a wide variety of important applications. Due to the inherent complexity of the exact versions of the problems, interest has focused on solving them approximately. Prior work in this area scaled only to small problems in practice, or failed to provide strong theoretical guarantees, or employed a computationally-expensive maximum a posteriori probability (MAP) oracle that assumes prior knowledge of a factored representation of the weight distribution. We present a novel approach that works with a black-box oracle for weights of assignments and requires only an NPoracle (in practice, a SAT-solver) to solve both the counting and sampling problems. Our approach works under mild assumptions on the distribution of weights of satisfying assignments, provides strong theoretical guarantees, and scales to problems involving several thousand variables. We also show that the assumptions can be significantly relaxed while improving computational efficiency if a factored representation of the weights is known.",
    "creator" : "LaTeX with hyperref package"
  }
}