{
  "name" : "1603.01182.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Network Unfolding Map by Edge Dynamics Modeling",
    "authors" : [ "Filipe Alves Neto Verri", "Paulo Roberto Urio", "Liang Zhao" ],
    "emails" : [ "filipeneto@usp.br.", "urio@usp.br.", "zhao@usp.br." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Complex networks, nonlinear dynamical systems, semi-supervised learning, particle competition.\nI. INTRODUCTION\nOVER the last decades, the world has been experiencing adata deluge. An impressive volume of data is produced in a short period, far more than people would be able to absorb. With those data, machine learning helps us enhance our understanding of the world. Broadly, machine learning tasks can be classified by the availability of the desired output. Supervised-learning techniques work with the desired output for each input data instance—the input data are completely labeled. Conversely, unsupervised learning techniques try to discover the intrinsic structure of a completely unlabeled data set [1], [2].\nSemi-supervised learning (SSL) is another category of machine learning. It is in between the unsupervised and super-\nThis research was supported by the São Paulo State Research Foundation (FAPESP), the Coordination for the Improvement of Higher Education Personnel (CAPES), and the Brazilian National Research Council (CNPq).\nF.A.N. Verri and P.R. Urio are with the Institute of Mathematical and Computer Sciences, University of São Paulo, São Carlos, SP, Brazil. Email: {filipeneto,urio}@usp.br.\nL. Zhao is with Ribeirão Preto School of Philosophy, Science and Literature, University of São Paulo, Ribeirão Preto, SP, Brazil. Email: zhao@usp.br.\nvised learning paradigms where both unlabeled and labeled data are taken into account in class or cluster formation and prediction process [3], [4]. In real-world applications, we usually have partial knowledge on a given dataset. For example, we certainly do not know every movie actor except a few famous ones; in a large-scale social network, we just know some friends; in biological domain, we are far away from completely obtaining a figure of the functions of all genes, but we know the functions of some of them. Sometimes, although we have a complete or almost complete knowledge of a dataset, labeling it by hand is lengthy and expensive, so, it is necessary to restrict the labeling scope. For these reasons, partially labeled datasets are often encountered. In this sense, supervised and unsupervised learning can be considered as extreme and special cases of semi-supervised learning. Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]–[12]. Among the approaches listed above, graph-based SSL has been triggered much attention. In this case, each data instance is represented by a vertex and is linked to other vertices according to a predefined affinity rule. The labels are propagated to the whole graph using a particular optimization heuristic [13].\nInterpreting the graph representation of the data as a complex network grants us further access to information. Complex networks are large-scale graphs with nontrivial topology [14]. Such networks introduce a powerful tool to describe the interplay of topology, structure, and dynamics of complex systems [14], [15]. Networks also turn out to be an important mechanism for data representation and analysis. For this reason, we consider the network-based approach for SSL in this work. However, the above-mentioned network-based approach focuses on the optimization of the label propagation result and pays little attention to the detailed dynamics of the learning process. On the other hand, it is well-known that collective neural dynamics generates rich information, and such a “redundancy” processing handles the adaptability and robustness of the learning process. Moreover, traditional graph-based techniques have high computational complexity, usually at cubic order [16]. A common strategy to overcome this disadvantage is using a set of sparse prototypes derived from the data [12]. However, such a sampling process usually loses information of the original data.\nTaking into account the facts above, we study a new type of dynamical competitive learning mechanism in a complex network, called particle competition. Consider a network, where\nar X\niv :1\n60 3.\n01 18\n2v 1\n[ cs\n.A I]\n3 M\nar 2\n01 6\nseveral particles walk and compete to occupy as many vertices as possible while attempting to reject intruder particles. Each particle performs a combined random and preferential walk by choosing a neighbor vertex to visit. Finally, it is expected that each particle occupies a subset of vertices, called a community of the network. In this way, community detection is a direct result of the particle competition. The particle competition model was originally proposed in [17] and extended for the data clustering task in [18]. Later, it has been applied to semisupervised learning [19], [20] where the particle competition is formally represented by a nonlinear stochastic dynamical system. In all the models mentioned above, the authors concern vertex dynamics—how each vertex changes its state (the level of dominance of each particle). Intuitively, vertex dynamics is a rough modeling of a network because each vertex can have several edges. A problem with the data analysis in this approach is the overlapping nature of the vertices, where a data item (a vertex in the networked form) can belong to more than one class. Therefore, it is interesting to know how each edge changes its state in the competition process to acquire detailed knowledge of the dynamical system behavior.\nIn this paper, we propose a transductive semi-supervised learning model that employs a dynamical system in complex networks. In this dynamical system, namely Edge Domination System, particles compete for the dominance of edges in a network. Subnetworks are generated with the edges grouped by class dominance. Here, we call each subnetwork an unfolding. The learning model employs the unfoldings to classify unlabeled data. The proposed model offers satisfactory performance on semi-supervised learning problems, in both artificial and real data. Moreover, it has shown to be suitable for detecting overlapping regions of data points by simply counting the edges dominated by each class of particles. Moreover, it has low computational complexity order.\nIn comparison to the original particle competition models and other graph-based semi-supervised learning techniques, the proposed one presents the following salient features: • Particles compete to dominate edges instead of nodes.\nThe intuition is that the edge domination modeling gives more details than vertex domination one. This is because each vertex may have from one to several edges. Edges determine the network’s topological structure. Consequently, our model has the benefit of granting essential information concerning overlapping vertices (data items) by simply counting the edges dominated by each class of particles. Computer simulations show the proposed technique achieves a good classification accuracy and it is suitable for situations with a small number of labeled samples. • We allow the direct conflict between particles of different classes, causing them to be absorbed and removed from the system. This mechanism contrasts with works that incorporate a preferential walking mechanism where particles tend to avoid rival particles. As a consequence, the number of active particles in the system varies over time. It is worth noting that the elimination of preferential walking mechanism largely simplifies the dynamical rules of particle competition model. Now, the new model is\ncharacterized by competition of only random walking particles, which, in turn, permits us to find out an equivalent deterministic version. The original particle competition model is intrinsically stochastic. Then, each run may generate a different result. Consequently, it has high computation cost. In this work, we find out a deterministic system with running time independent of the number of particles, and we demonstrate that it is mathematically equivalent to the stochastic model. Moreover, the deterministic model has linear time order and ensures a stable learning model. In other words, the model generates the same output for each run with the same input. Furthermore, the system is simpler and easier to be understood and implemented. Such a feature makes the proposed model more efficient than the original particle competition model. • In classical graph-based semi-supervised learning techniques, usually, an objective function is defined for optimization. Such function considers not only the label information as they can also be built upon the semisupervised assumptions of smoothness, cluster, or manifold. In particle competition models, an objective function is not defined. Instead, dynamical rules which govern the time evolution of particles and vertices (or edges) are defined. Those dynamical rules mimic the phenomenon observed in some natural and social systems, such as resource competition among animals, territory exploration by humans (animal), election campaigns, etc. In other words, a particle competition technique is typically nature inspired. In those kinds of techniques, we have focused on behavior modeling instead of objective modeling. Certain objectives can be achieved if the corresponding behavioral rules are properly defined. That way, we may classify classical graph-based semi-supervised learning techniques as objective-based design and the particle competition technique as behavior-based design.\nThe remainder of this paper is organized as follows. The proposed particle competition system is studied in Section II. Our transductive semi-supervised learning model is represented in Section III. In Section IV, results of computer simulations are shown to assess the proposed model performance on both artificial and real-world datasets. Finally, Section V concludes this paper.\nII. EDGE DOMINATION SYSTEM\nIn this section, we give an introduction to the Edge Domination System—a particle competition system for edge domination—explaining its basic design. Whenever pertinent, we go into detail for further clarification.\nA. Overview\nWe consider a complex network expressed by a simple, unweighted, undirected graph G = (V, E), where V is the set of vertices and E ⊆ V × V is the set of edges. If two vertices are considered similar, an edge connects them. This network contains |V| = l + u vertices that can be either labeled or unlabeled data points. The set L = {v1, . . . , vl}\ncontains the labeled vertices, where a vertex vi ∈ L is labeled with yi ∈ {1, . . . , C}. We also use the terms label and class synonymously—if a vertex is labeled with c, we say this vertex belongs to class c. The set U = {vl+1, . . . , vl+u} contains the unlabeled vertices. We suppose that l u. Thus, we have that L ∩ U = ∅ and V = L ∪ U . The network is represented by the adjacency matrix A = (aij) where aij = aji = 1 if vi is connected to vj . We denote (i, j) it be the edge between vertices vi and vj . For practical reasons, consider a connected network and at least one labeled vertex of each class.\nIn this model, particles are the objects that flow within the network while carrying a label. Labeled vertices are sources for particles of the same class and sinks for particles of other classes. After being released a particle randomly walks the network. There is equal probability among adjacent vertices to be chosen as the next vertex. Consider a particle that is in vi. This particle decides to move to vj with probability\naij deg vi\nwith deg vi denoting the degree of vi. In each step, at the moment of a particle decides to move to a next vertex, it can be absorbed (removed from the system). If a particle is not absorbed we say that it has survived and it remains active; and if it survives, then it continues walking. Otherwise, the absorbed particle ceases to affect the system. The absorption depends on the level of subordination and domination of a class against all other classes.\nTo determine the level of domination and subordination of each class in an edge, we take into account the active particles in the system. The current directed domination ñcij(t) is the number of active particles belonging to class c that decided to move from vi to vj at time t and survived. Similarly, the current relative subordination σ̃cij is the fraction of active particles that do not belong to class c and have successfully passed through edge (i, j), regardless of direction, at time t. Mathematically, we define the latter as\nσ̃cij :=  1− ñcij + ñ c ji∑C q=1 ñ q ij + ñ q ji if ∑C q=1 ñ q ij + ñ q ji > 0, 1\nC otherwise.\nThe survival of a particle depends on the current relative subordination of the edge and the destination vertex. If a particle decides to move into a sink, it will be absorbed. If the destination vertex is not a sink, its survival probability is\n1− λσ̃cij(t)\nwhere λ ∈ [0, 1] is the competition parameter. A source generates particles according to its degree and the current number of active particles in the system. Let ñc(t) be the number of active particles belonging to class c in the system at time t. A source generates new particles if ñc(t) < ñc(0).\nLet Gc = {vi|vi ∈ L and yi = c} be the set of sources for particles that belong to class c. The number of new particles\nbelonging to class c in vi at time t follows the distribution{ B(ñc(0)− ñc(t), ρci ) if ñc(0)− ñc(t) > 0, B(1, 0) otherwise,\nwhere\nρci :=  deg vi∑ vj∈Gc deg vj if vi ∈ Gc,\n0 otherwise,\nand B(n, p) is a binomial distribution. In other words, if the number of active particles is fewer than ñc(0), each source performs ñc(0)− ñc(t) trials with probability ρci of generating a new particle.\nTherefore, the expected number of new particles belonging to class c in vi at time t is{\nρci ñ c(0)− ñc(t) if ñc(0)− ñc(t) > 0,\n0 otherwise.\nWe are interested in the total number of visits of particles of each class to each edge. Thus, we introduce the cumulative domination δ̃cij(t) that is the total number of particles belonging to class c that passed through edge (i, j) up to time t. Mathematically, this is defined as\nδ̃cij(t) := t∑ τ=1 ñcij(τ) .\nUsing cumulative domination, we can group the edges by class domination. For each class c, the subset of edges Ec(t) ⊆ E is\nEc(t) := { ij ∣∣∣∣arg max q ( δ̃qij(t) + δ̃ q ji(t) ) = c } .\nWe define the subnetwork\nGc(t) := (V, Ec(t)) (1)\nas the unfolding of network G according to class c at time t. We interpret the unfolding as a subspace with the most relevant relationships for a given class. We use the available information in these subnetworks for the study of overlapping regions and for semi-supervised learning.\nB. An Illustrative Example\nAn iteration of the system’s evolution is illustrated in Figure 1. The considered system contains 22 active particles at time t and 20 at time t+ 1. In an iteration every particle moves to a neighbor vertex, without preference. The movement of a particle is indicated by an arrow. An interrupted line indicates the edge to absorb the coming particle. A total of 8 particles are absorbed during this iteration, and the red and green sources have generated 4 new particles to compensate the absorbed ones.\nAt time t for example, one of the red particles passing through edge (1, 3) is absorbed, which is due to a current edge dominance of 0.5 in that edge (one red particle and one green particle). Conversely, all green particles that moved through edge (5, 7) remained active at time t+ 1. Since there is no rival particle (red particle) passing through this edge, the new\nvalue of the current edge dominance is 1 and 0 for green and red classes, respectively.\nIn edge (2, 4) two green particles and one red particle chose to pass through. One green particle is absorbed and has not affected the new current level of dominance. Since one particle of each class successfully passed through edge (2, 4), the new current level of dominance on this edge is 0.5. The same occurs for edge (4, 7) where no particles have passed through and, thus, the current level of dominance is set equally among classes.\nIn edges (2, 5) and (3, 6), particles have tried to move into a source of rival particles (sinks). These particles are absorbed independently from the current level of dominance.\nAfter this step, our edge-centric system could measure the overlapping nature of the v4 by counting the edges dominated by each class. A vertex-centric approach would have lost such information.\nC. Mathematical Modeling\nFormally, we define Edge Domination System as a dynamical system X̃(t). Let ñci (t) be the number of active particles\nbelonging to class c in vi at time t. The state of this dynamical system is\nX̃(t) := [ ñc(t) ∆̃c(t) ]T , (2)\nwhere ñc(t) := [ ñci (t) ] i ,\n∆̃c(t) := ( δ̃cij(t) ) i,j .\nLet g̃ci (t) and ã c i (t) be, respectively, the number of particles generated and absorbed by vi at time t. The evolution function φ̃ of the dynamical system is\nφ̃ :  ñci (t+ 1) = ñ c i (t) + ∑ j ( ñcji(t+ 1)− ñcij(t+ 1) ) + g̃ci (t+ 1)− ãci (t+ 1) ,\nδ̃cij(t+ 1) = δ̃ c ij(t) + ñ c ij(t+ 1) .\nIntuitively, the number ñci of active particles that are in a vertex is the total number of particles arriving minus the number of particles leaving or being absorbed; additionally for labeled vertices, the number of generated particles. Moreover, to calculate the total number δ̃cij of visits of particles to an edge, we simply add up the number ñcij at each time. Values ñcij , g̃ c i , and ã c i are obtained stochastically according to the dynamics of walking, absorption, and generation. The initial state of the system is given by an arbitrary number ñci (0) of initial active particles and{ ñcij(0) = 0 ,\nδ̃cij(0) = 0 .\nIn order to achieve the desirable network unfolding, it is necessary to average the results of several simulations of the system with a very large number of initial particles ñci (0). In this way, the computational cost of such a simulation is very high. Conversely, we provide an alternative system X(t) that achieves similar results in a deterministic manner. The alternative system considers that there exists an asymptotically infinite number of initial active particles. More details will follow.\nD. Alternative Mathematical Modeling\nConsider the dynamical system\nX(t) :=  nc(t) = [ nci (t) ] i N c(t) = ( ncij(t) ) i,j\n∆c(t) = ( δcij(t) ) i,j  that is a nonlinear Markovian dynamical system with the deterministic evolution function\nφ :  nc(t+ 1) = nc(t)× P c(X(t)) + gc(nc(t)) N c(t+ 1) = diagnc(t)× P c(X(t)) ∆c(t+ 1) = ∆c(t) +N c(t+ 1) , (3)\nwhere P c(X) := ( pcij(X) ) i,j ,\npcij(X) := 0 if vj ∈ L and yj 6= c ,aij deg vi ( 1− λσcij(X) ) otherwise, (4)\nσcij(X) :=  1− ncij + n c ji∑C q=1 n q ij + n q ji if ∑C q=1 n q ij + n q ji > 0, 1\nC otherwise,\n(5)\nand gc(n) := [ gci (n) ] i ,\ngci (n) := ρ c i max{0, 1 · nc(0)− 1 · n} . (6)\nThe initial state of the system X is given by an arbitrary discrete distribution nc(0) of initial active particles and{\nncij(0) = 0 , δcij(0) = 0 .\nIf the initial number of particles in each vertex in system X̃ follows the distribution of initial particles in system X , we will provide evidence that the unfolding result tends to be the same for both systems as ñci (0) → ∞ for all c ∈ {1, . . . , C} and i ∈ {1, . . . , |V|}.\nE. Mathematical Analysis\nIn the previous subsections, we modeled two possibly equivalent systems, X and X̃ . In this section, we present mathematical results that explain the equivalence of both systems under certain assumptions.\nTheorem 1. Asymptotic equality between systems X and X̃ . Assuming that\nE [ σ̃cij(t) ] → 1−\nE [ ñcij(t) ] + E [ ñcji(t) ]∑C q=1 E [ ñqij(t) ] + E [ ñqji(t)\n] and E[g̃ci (t+ 1)]→ ρci max{0, ñc(0)− E[ñc(t)]} as\nñci (0)→∞,\nfor all i, j ∈ V , t > 0, and c ∈ {1, . . . , C}, we have\nnci (t) = κE[ñ c i (t)] , n c ij(t) = κE [ ñcij(t) ] , and\nδcij(t) = κE [ δ̃cij(t) ] ,\nfor k > 0 constant.\nIn order to prove Theorem 1, we study the following mechanisms of the particle competition system:\n1) Particle motion and absorption study: In the proposed system, each particle moves independently from the others. Particle movement through an edge affects the absorption of rival particles only on the next iteration. Such conditions are favorable to naturally regard the system’s evolution in terms of the distribution of particles over the nerwork. Next, we present a formal model for particle movement.\nLet Iij(p, t+ 1) be a discrete random variable that is 1 if particle p was in vi at time t and moved into vj at time t+ 1; and it is 0 otherwise. Since each particle in a vertex moves independently, we can write this probability in terms of a\nparticle’s class; that is, Icij(t + 1) = Iij(p, t + 1) for any particle p that belongs to class c and is in vi at time t.\nThe probability Pr [ Icij(t+ 1) = 1 ] is affected by the movement decision of a particle and whether it was absorbed after the decision. By formulation, in dynamical system X̃ the conditional probability, given that σ̃cij(t) = ξ, is\nPr [ Icij(t+ 1) = 1 ∣∣σ̃cij(t) = ξ] =\n0 if vj ∈ L and yj 6= c ,aij deg vi (1− λ · ξ) otherwise.\nThat is, when a particle tries to move into a sink, the survival probability is zero. Otherwise, a particle only reaches vj if it chooses to move into the vertex and it is not absorbed.\nLet fσ̃cij(t) be the probability density function of the random variable σ̃cij . Hence, the probability Pr [ Icij(t+ 1) = 1 ] is∫ ∞\n−∞ Pr [ Icij(t+ 1) = 1 ∣∣σ̃cij(t) = ξ] fσ̃cij(t)(ξ) dξ =\n∫ ∞ −∞ aij deg vi (1− λ · ξ) fσ̃cij(t)(ξ) dξ\n= aij\ndeg vi (∫ ∞ −∞ fσ̃cij(t)(ξ) dξ − λ ∫ ∞ −∞ ξfσ̃cij(t)(ξ) dξ ) =\naij deg vi\n( 1− λE [ σ̃cij(t) ]) ,\nif vj ∈ U or vj ∈ L ∧ yj 6= c . Otherwise, it is zero. Furthermore, σ̃cij is convex with fixed values of ñ q ij , ñ q ji for all q 6= c. Thus, with the Jensen’s inequality [21], we have\nE [ σ̃cij(t) ] ≥ 1−\nE [ ñcij(t) ] + E [ ñcji(t) ]∑C q=1 E [ ñqij(t) ] + E [ ñqji(t)\n] . (7) 2) Particle generation study: In dynamical system X̃ the expected number of particles belonging to class c generated at vi at time t is\nE[g̃ci (t+ 1)] = ∞∑ η=0 E[g̃ci (t+ 1)|ñc(t) = η] Pr[ñc(t) = η] .\nThe conditional expectation E[g̃ci (t+ 1)|ñc(t) = η] is, by formulation,\nρci ·max{0, ñc(0)− η} ,\nand thus, E[g̃ci (t+ 1)] is\nρci ∞∑ η=0 max{0, ñc(0)− η}Pr[ñc(t) = η]\n= ρci E[max{0, ñc(0)− ñc(t)}] .\nSince max{0, x} is convex for all x ∈ R and according to Jensen’s inequality, we have\nE[g̃ci (t+ 1)] ≥ ρci max{0, ñc(0)− E[ñc(t)]} . (8)\n3) Expected edge domination study: At the beginning of system X̃ we have\nδ̃cij(0) = 0\nand, for t ≥ 0, E [ δ̃cij(t+ 1) ] = E [ δ̃cij(t) + ñ c ij(t+ 1) ] = E [ δ̃cij(t) ] + E [ ñcij(t+ 1) ] . (9)\nGiven that ñci (t) = η is known and since each particle in a vertex moves independently, the number of particles that successfully reaches vj at time t+ 1 is\nñcij(t+ 1) = η∑ k=1 Iij(pk, t+ 1) ,\nwhere pk is a particle that belongs to class c and is in vi. Then, the expected value E [ ñcij(t+ 1) ] is\n∞∑ η=0 E [ ñcij(t+ 1) ∣∣ñci (t) = η]Pr[ñci (t) = η] =\n∞∑ η=0 Pr[ñci (t) = η] · η∑ k=1 E[Iij(pk, t+ 1)]\n= ∞∑ η=0 Pr[ñci (t) = η] · η∑ k=1 Pr[Iij(pk, t+ 1) = 1]\n= ∞∑ η=0 ηPr[ñci (t) = η] · Pr [ Icij(t) = 1 ] .\nFinally, E [ ñcij(t+ 1) ] = E[ñci (t)] Pr [ Icij(t) = 1 ] (10)\nfor all t ≥ 0, c = {1, . . . , C}, and i, j ∈ {1, . . . , |V|}. 4) Expected number of particles study: We know the number of particles at the beginning of system X̃(t), so\nE[ñci (0)] = ñ c i (0)\nand, for all t ≥ 0, the expected value E[ñci (t+ 1)] is E[ñci (t)] + ∑ j ( E [ ñcji(t+ 1) ] − E [ ñcij(t+ 1) ]) + E[g̃ci (t+ 1)]− E[ãci (t+ 1)] .\nHowever, the expected number of particles that were absorbed in vi is the expected number of particles in vi minus the expected number of particles that survived when moving away. Thus, E[ñci (t+ 1)] can be written as\nE[ñci (t)] + ∑ j ( E [ ñcji(t+ 1) ] − E [ ñcij(t+ 1) ]) + E[g̃ci (t+ 1)]− E[ñci (t)]−∑ j E [ ñcij(t+ 1)\n] . And, finally\nE[ñci (t+ 1)] = ∑ j E [ ñcji(t+ 1) ] + E[g̃ci (t+ 1)] , (11)\nfor all t ≥ 0, c = {1, . . . , C}, and i ∈ {1, . . . , |V|}.\n5) Scale invariance study: The unfolding Gc(t) from system X is invariant under real positive multiplication of the input nc(0). In order to prove this property, consider the following lemma.\nLemma 1. System X has positive multiplicative scaling behavior of order 1. That is\nX(t) = Xt | X(0) = X0 ⇐⇒ X(t) = κXt | X(0) = κX0 (12)\nfor all t > 0 and κ > 0.\nProof of Lemma 1: Consider an arbitrary initial state\nX0 =  nci (0) = ηi , ncij(0) = 0 ,\nδcij(0) = 0 .\nfor all i, j, c. We have that\nσcij(κX0) = σ c ij(X0) =\n1 C ,\npcij(κX0) = p c ij(X0)\n=  0 if vj ∈ L and yj 6= c , aij\ndeg vi\n( 1− λ\nC\n) otherwise,\nncij(1;κX0) = n c ij(0;κX0) · pcij(κX0) = κ · ηipcij(X0)\n= κ · ncij(1;X0) ,\ngci (κn, 1) = g c i (n, 1) = 0,\nnci (1;κX0) = ∑ j ncji(1;κX0) + g c i (n c i (0;κX0) , 1)\n= κ ∑ j ncji(1;X0) + 0 = κ · nci (1;X0),\nand\nδcij(1;κX0) = δ c ij(0;κX0) + n c ij(1;κX0) =\n0 + κ · ncij(1;X0) = κ · δcij(1;X0)\nThus, Relation (12) holds true for t = 1. Assuming that Relation (12) holds true for some value of t, we show that the relation holds true for t+ 1.\nσcij(κXt+1) = 1− κncij + κn c ji∑C\nq=1 ( κnqij + κn q ji ) = 1− ncij + n c ji∑C\nq=1 ( nqij + n q ji ) = σcij(Xt+1), if ncij+ncji > 0,∀c or\nσcij(κXt+1) = σ c ij(Xt+1) =\n1 C , otherwise,\npcij(κXt+1) = p c ij(Xt+1)\n= 0 if vj ∈ L and yj 6= c ,aij deg vi ( 1− λσcij(Xt+1) ) otherwise,\nncij(t+ 1;κX0) = n c ij(t;κX0)p c ij(κXt)\n= κ · ncij(t;X0)pcij(Xt) = κ · ncij(t+ 1;X0)\ngci (κn, t+ 1) = κρ c i max{0, 1 · nc(0)− 1 · n}\n= κ · gci (n, t+ 1),\nnci (t+ 1;κX0) = ∑ j ncji(t+ 1;κX0) + g c i (n c i (t;κX0) , t+ 1)\n= κ ∑ j ncji(t+ 1;X0) + κ · gci (nci (t;X0) , t+ 1)\n= κ · nci (t+ 1;X0),\nand\nδcij(t+ 1;κX0) = δ c ij(t;κX0) + n c ij(t+ 1;κX0)\n= κδcij(t;X0) + κ · ncij(t+ 1;X0) = κ · δcij(t+ 1;X0)\nSo Relation (12) indeed holds true for t+ 1. Since both the basis and the inductive step have been performed, by mathematical induction, the lemma is proved for all t ≥ 0 natural.\nFinally, using these studies, we may prove the theorem. Proof of Theorem 1: By Equations (9)–(11), we have E[ñci (t+ 1)] = ∑ j E [ ñcji(t+ 1) ] + E[g̃ci (t+ 1)] , E [ ñcij(t+ 1) ] = E[ñci (t)] Pr [ Icij(t) = 1 ] , E [ δ̃cij(t+ 1) ] = E [ δ̃cij(t) ] + E [ ñcij(t+ 1) ] ,\nwhich is system X assuming that Inequations (7) and (8) tend to equality when there is a large number of particles and κñci (0) = n c i (0), for any k > 0 constant (scale invariance property).\nRemark 1. Even if convergence of Inequations (7) and (8) were not true, another property that possibly makes the two systems equivalent is the compensation over time. At the beginning, both systems are equal; however, in the next iteration both absorption probability (7) and generated particles (8) are underestimated. Consequently, particles that have survived may compensate the ones that were not generated. Furthermore, the lower the number of absorbed particles in an iteration, the higher the absorption probability in the next iteration. Likewise, the lower the number of generated particles in an iteration, the higher is the expected number of new particles in the next iteration.\nIII. SEMI-SUPERVISED LEARNING BY EDGE DOMINATION\nUnfoldings generated by Edge Domination System are incorpored in a semi-supervised learning model. Consider two sets Xlabeled = {x1, . . . , xl} and Xunlabeled = {xl+1, . . . , xl+u} such that xi ∈ RD for all i. Each data point xi ∈ Xlabeled is associated with a label yi ∈ {1, . . . , C}. In the semisupervised learning setting, our goal is to correctly assign existing labels to the unlabeled data Xunlabeled.\nIn short, the proposed learning model has three steps: a) a network is constructed based on a dataset, where vertices repesent data points and edges represent similarity relationship; b) Edge Domination System is applied to obtain the unfoldings, that is, a distinct set of edges for each class of the dataset; and c) infer labels for every data point in Xunlabeled.\nNext, each step of the proposed learning model is presented in detail. Further to the model’s algorithm description, its computational complexity analysis is also presented.\nSince the proposed dynamical system takes place on a complex network, the problem dataset needs to be represented in a graph structure. Therefore, the first step of our learning model is to obtain a graph representation that stands as a complex network for the system. Each data point must be associated with a single vertex of the network. Moreover, the network must be sparse, undirected, and unweighted. Labeled vertices correspond to the set of points in Xlabeled, and unlabeled vertices to the set of points in Xunlabeled. Two vertices are connected by an edge if they have a relationship of similarity, which is determined by some metric or by the particular problem. Any graph construction method that satisfies such conditions may be used in this step. The k-NN graph construction method is one of them.\nThe second step is to run system X defined by Equation (3) using the constructed complex network as its input. Two conditions are satisfied on the system initialization. First, no class should be privileged. Second, during the first iterations, all particles should be able to flow within the network with a small probability of absorption. Thus, the initial conditions of the system, for all i, j, and c ∈ {1, . . . , C}, are\nnci (0) = deg vi 2 |E| ,\nncij(0) = 0 , δcij(0) = 0 .\n(13)\nSince there are always particles in the system, the iteration of system X should be stopped if the time limit has been reached. The time limit parameter τ controls the maximum number of iterations of the system.\nAt the last step, the networks Gc(τ) are used in the vertex classification. We assign a label yj ∈ {1, . . . , C} for each unlabeled vertex vj ∈ U , with the information provided by the networks Gc. Label yj is assigned based on the density of edges in its neighborhood. Formally, the label for the vj is\nyj = arg max c∈{1,...,C}\n|E(Nc,j)| , (14)\nwhere Nc,j is the neighborhood of vj in the unfolding Gc(τ). We denote the number of edges in this neighborhood as |E(Nc,j)|.\nA. Algorithm\nAlgorithm 1 summarizes the steps of our learning model. The algorithm accepts the labeled dataset Xlabeled, the unlabeled dataset Xunlabeled, and 2 user-defined parameters—the competition (λ) parameter of the system X and the time limit parameter (τ ). Moreover, it is necessary to choose a network formation technique.\nAlgorithm 1 Semi-supervised Learning by Edge Domination. 1: function CLASSIFIER(Xlabeled, Xunlabeled, λ, τ ) 2: G← BUILDNETWORK(Xlabeled, Xunlabeled) 3: subnetworks ← UNFOLD(G, λ, τ ) 4: return CLASSIFY(Xunlabeled, subnetworks) 5: end function\nAlgorithm 2 Edge Domination System. 1: function UNFOLD(G, λ, τ ) 2: for c ∈ {1, . . . , C} do 3: nc ← n0(G, c) . Equation (13) 4: N c ← N0(G, c) 5: ∆c ← ∆0(G, c) 6: end for 7: for t ∈ {1, . . . , τ} do 8: for c ∈ {1, . . . , C} do 9: P c ← P (G, N1, . . . , NC , λ) . Equation (4) 10: gc ← g(G, nc, t) . Equation (6) 11: N c ← diagnc × P c 12: nc ← nc × P c + gc 13: ∆c ← ∆c +N c 14: end for 15: end for 16: return SUBNETWORKS(G, ∆c) . Equation (1) 17: end function\nThe first step of the learning model is mapping the original data to a network using a chosen network formation technique. Afterwards, we unfold the network as described in Algorithm 2. This algorithm iterates the Edge Domination System producing one subnetwork for each class. Steps 2–6 initialize the system state as indicated in Equation (13). Steps 7–15 iterate the system until τ using the evolution function φ (3). Step 16 calculates and returns the unfoldings for each class. Back to Algorithm 1, by using the produced unfoldings, the unlabeled data are classified as described in Equation (14).\nB. Computational Complexity and Running Time\nHere, we provide the computational complexity analysis step by step.\nThe construction of the complex network from the input dataset depends on the chosen graph construction method. Since |V| = |Xlabeled| + |Xunlabeled| is the number of data samples. The k-NN method, for example, has complexity order of O(D |V| log |V|) using multidimensional binary search tree [22].\nThe second step is running system X defined by Equation (3). Using sparse matrices, the system initialization, steps 2–6 of Algorithm 2, has complexity order of O(C |V|+ C |E|). The system iteration calculates τC times the evolution function φ (3) represented in steps 8–14. The time complexity of each part of the system evolution is presented below. • Step 9, computation of the matrix P c. This matrix has |E|\nnon-zero entries. It is necessary to calculate σcij for each non-zero entry. Hence, this step has complexity order of\nO(C |E|). However, the denominator of Equation (5) is the same for all values of c. • Step 10, computation of the vector gc. This vector has |L| non-zero entries. It is also necessary to calculate the total number of particles in the system. So, this calculation has time complexity order of O(|L|+ |V|). • Step 11, computation of the matrix N c. The multiplication between a diagonal matrix and a sparse matrix with |E| non-zero entries has time complexity order of O(|E|). • Step 12, computation of the vector nc. Suppose that 〈k〉 is the average node degree of the input network; it follows that this can be performed in O(|V| 〈k〉) = O(|E|). • Step 13, computation of the matrix ∆c. This sparse matrix summation has complexity order of O(|E|).\nAfter system evolution, the unfolding process performs O(C |E|) operations. Thus, the total time complexity order of the system simulation is O(τC |E|+ τC |V|). However, the value of τ is fixed and the value of C is usually very small.\nThe vertex labeling step is the last step of the learning model. The time complexity of this step depends on the calculation of the number of edges in the neighborhood of each unlabeled vertex in each unfolding. It can be efficiently calculated by using one step of a breadth-first search in Gc. Hence, the order of the average-time complexity is O ( C |U| 〈k〉2 ) ≈ O(C |E|).\nIn summary, considering the discussion above, our learning model runs in O(D |V| log |V|+ C |E|+ C |V|) including the transformation from vector-based dataset to a network. Furthermore, every graph-based algorithm must either implicitly or explicitly construct the graph. Table I compares the time complexity of common graph-based techniques disregarding the graph construction step. Only the proposed Edge Domination and Minimum Tree Cut [27] have linear time, though the latter must either receive or construct a spanning tree. Consequently, the Minimum Tree Cut has a performance similar to the scalable version of traditional algorithms, such as those using subsampling practices.\nFigure 2 depicts the running time of single iterations of the system varying the number of vertices and edges, respectively. With 10 independent runs, we measure the time for 30 iterations, totalizing 300 samples for each network size. We set λ = 1, two classes, and 5% of labeled vertices. Experiments were run on an Intel R© CoreTM i7 CPU 860 @ 2.80GHz with 16 GB RAM memory DDR3 @ 1333 MHz. This experiment demonstrates that the system iteration runs in linear time\nfor the number of vertices and edges, which conforms our theoretical analysis.\nIV. COMPUTER SIMULATIONS\nTo study the systems X and X̃ , we present experimental analyses that concern their equivalence. Additionally, we study the meaning of the parameters of our learning model. After that, we evaluate the model performance using both artificial and synthetic datasets. Then, we show the unfolding process and the learning model on synthetic data. Finally, we present the simulation results for a well-known benchmark dataset and for a real application on human activity recognition.\nA. Experimental Analysis\nIn this section, we present a experiment that assesses the equivalence between the unfolding results of both systems with an increasing initial number of particles in system X̃ .\nThe networks used for the analysis were generated by the following model: A complex network is constructed given a labeled vector y, a number m > 0 of edges by vertex, and a weight p ∈ [0, 1] that controls the preferential attachment between vertices of different classes. The resulting network contains |y| vertices. For each vi, m edges are randomly connected, with replacement. If yi = yj , the preferential attachment weight is 1 − p; otherwise, the weight is p. The parameter p is proportional to the overlap between classes.\nIf there exists a positive constant κ such that\nδ̃cij(t) = κδ c ij(t),\nboth systems generate the same unfoldings. In order to assess this proportionality, both systems were simulated in 10 different networks G(y, 3, 0.05), with |y| = 200 vertices arranged in two classes. The system’s parameter was discretized in λ = {0, 0.5, 1}. Varying the total number of initial particles, we set ñci (0) = n c i (0) ∼ deg vi for all c ∈ {0, . . . , C} and i ∈ {1, . . . , |V|}.\nWe consider the correlation between the cumulative domination matrices of systems X and X̃ . If the two matrices are proportional, then they must be correlated. Values of correlation close to 1 indicate the cumulative domination matrices are proportional. In Figure 3, the correlation is depicted. As the number of initial particles increases, the correlation approaches 1. This result suggests that both systems generate the same unfolding when the number of initial particles grows to infinity.\nB. Parameter Analysis\nThe Edge Domination model has two parameters apart from the network construction. In this section, we discuss their meaning. To do so, the learning model was applied in synthetic datasets whose data items were sampled from a three dimensional knot torus v(θ) with parametric curve\nx(θ) = r(θ) cos 3θ,\ny(θ) = r(θ) sin 3θ,\nz(θ) = − sin 4θ,\nwhere θ ∈ [0, 2π] and r(θ) = 2 + cos 4θ. We sampled 500 data items uniformly along the possible values of θ. We randomly split the data items from 2 to 10 classes so that the samples with adjacent θ belongs to the same class. We also added to each sample a random noise in each dimension with distribution N (0, σ) with σ = 0.25 and 0.35. Figure 4 depicts an example of the dataset with 4 classes with and without noise. Since the dataset has a complex form, a small change of parameter value may generate different results. Therefore, it is suitable to study the sensitivity of parameters.\nWe run the Edge Domination model with parameters λ ∈ {0.25, 0.5, 0.75, 1} and τ = 500. Finally, 30 unbiased sets of\n40 labeled points were employed. The k-NN is used for the network construction with k = {4, 5, . . . , 10}.\nBelow, we discuss each parameter of the model. 1) Discussion about the network construction parameter:\nIn our model, the input network must be simple (between any pair of vertices there must exist at most one edge), unweighted, undirected, and connected. Besides these requirements, two vertices must be connected if their data items are considered similar enough to the particular problem. In our experiments, we use k-NN graph with Euclidean distance since it was proved to approximate the low-dimensional manifold of a set of points [28]. Usually, the smaller the value of k, the better are the results.\n2) Discussion about the system parameter: The Edge Domination system has only one parameter: the competition parameter λ. This parameter defines the intensity of competition between particles. When λ = 0, particles randomly walk the network, without competition. As λ approaches to 1, particles are more likely to compete and, consequently, to be absorbed. Figure 5 depicts the average error of our method with different values of λ. Based on the figure, our model is not sensitive to λ. In general, we suggest setting λ = 1 because of better and more consistent classification than with other values.\n3) Discussion about the system iteration stopping parameter: The time limit parameter τ controls when the simulation should stop. The parameter τ must be at least as large as the diameter of the input network. That way, it is guaranteed every edge to be visited by a particle. Since the network diameter is usually a small value, the simulation stops in few iterations.\nC. Simulations on Artificial Datasets\nFor better understanding the details of the Edge Domination system, in this subsection we illustrate it using two synthetic datasets. Each dataset has a different class distribution— banana shape and Highleyman. (The datasets were generated using the PRTools framework [29].) The banana shape dataset is uniformly distributed along specific shape, and then superimposed on a normal distribution with the standard deviation of 1 along the axes. In Highleyman distribution, two classes are defined by bivariate normal distributions with different parameters. Because the datasets are not in a network representation, we use the k-Nearest Neighbor (k-NN) graph\nconstruction method to transform them into respective network form. In the constructed network, a vertex represents a datum, and it connects to its k nearest neighbors—determined by the Euclidean distance. We set the parameter λ = 1 for the simulation.\nFirstly, the technique is tested on the Highleyman dataset. Each class has 300 samples, of which 6 are labeled. (We set k = 10 for the k-NN algorithm.) We can observe that the labeled data points of the green class form a barrier to samples of the red class. The unfoldings Gred(100) and Ggreen(100) are presented in Figure 6a. In this figure, blue squares represent vertices that are connected by edges of both unfoldings. Besides of the labeled data of green class forming a barrier, the constructed subnetworks are still connected—there is a single component connecting all the vertices of the subnetwork. It is better visualized in Figure 6b. In this figure, the same unfoldings are presented, but the positions of the vertices are not imposed by the original data. Furthermore, colors of vertices in the figure indicates the result of classification. The overlapping data can be identified by the vertices that belongs to two or more unfoldings. This result reveals that the competition system in edges provide more information than the competition in vertices, since it is able to identify the overlapping vertices as part of the system, that is, without special treatments or modifications.\nThe last synthetic dataset has 600 samples equally split into two classes. In Figure 7a, the initial state of the system is illustrated, where the dataset is represented by a network. (The network representation is obtained by setting k = 4 for the k-NN–graph construction.) At this stage, the edges are\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●●\n●\n● ●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●●\n●\n●\n●\n●\n●\n●●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n● ●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n●\n● ●\n(a)\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●●\n●\n● ●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●●\n●\n●\n●\n●\n●\n●●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n● ●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n●\n● ●\n(b)\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●●\n●\n● ●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●●\n●\n●\n●\n●\n●\n●●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n● ●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n●\n● ●\n(c)\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●●\n●\n● ●\n●\n●\n●\n● ●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n● ●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n● ● ● ●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n●\n● ●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●●\n●\n●\n●\n●\n●\n●●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n● ●\n●\n●\n(d)\nFig. 7. System evolution on a banana-shaped distribution dataset. Red and green colors represent the two classes. Unlabeled points are black one; labeled vertices are represented by larger and colored points. Edges are colored according to the dominating class at current iteration, where a light gray point stands for a vertex, which is not dominated yet. (a) The network representation of the dataset, at the beginning of the system. (b) and (c) System iteration at time 4 and 20, respectively. (d) The result of the dataset classification.\nnot dominated by any of the classes. Starting from this state, labeled vertices (sources) generate particles that carry the label of the sources. Though the particles are not shown, Figures 7b and 7c are snapshots of the system evolution—at time 4 and 20—where each edge is colored by its dominating class at that iteration. In these illustrations, a solid red line stands for an edge (i, j) that δredij +δ red ji > δ green ij +δ green ji , while a dashed green line stands for the opposite. When δredij + δ red ji = δ green ij + δ green ji , an edge is drawn in a solid light gray line. As expected, edges close to sources are dominated firstly, and farther edges are progressively dominated. At time 20, Figure 7c, every edge has been dominated and the edge domination does not change anymore. Figure 7d shows dataset classification following the system result. In this example, with 1% of points in the labeled set, the technique is able to correctly identify the pattern formed by each class. Both results are satisfactory, reinforcing the ability of the technique of learning arbitrary class distributions.\nD. Simulations on Benchmark Datasets\nWe compare our model with 14 semi-supervised techniques tested on Chapelle’s benchmark [3]. The benchmark is formed by seven datasets that have 1500 data points, except for BCI that has 400 points. The datasets are described in [3].\nFor each dataset, 24 distinct, unbiased sets (splits) of labeled points are provided within the benchmark. Half of the splits are formed by 10 labeled points, and the other half by 100 labeled points. The author of the benchmark ensured that each split contains at least one data point of each class. The result is the average test error—the proportion of data\npoints incorrectly labeled—over the splits. We compare our results with the following techniques: 1-Nearest Neighbors (1- NN), Support Vector Machines (SVM), Maximum variance unfolding (MVU + 1-NN), Laplacian eigenmaps (LEM + 1- NN), Quadratic criterion and class mass regularization (QC + CMR), Discrete regularization (Discrete reg.), Transductive support vector machines (TSVM), Cluster kernels (ClusterKernel), Low-density separation (LDS), Laplacian regularized least squares (Laplacian RLS), Local and global consistency (LGC), Label propagation (LP), Linear neighborhood propagation (LNP), and Network-Based Stochastic Semisupervised Learning (Vertex Domination), The simulation results were collected from [3], except for LGC, LP, LNP, and Vertex Domination that are found in [18].\nFor the simulation of the Edge Domination system, we discretize the interval of the parameter in λ = {0, 0.125, . . . , 1}. Also, we vary the k-NN parameter k ∈ {1, 2, . . . , 10}. We tested every combination of k and λ. Moreover, we fixed τ = 1000. In Table II we present the results with the standard deviation over the splits, along with the best combination of parameters that generated the best accuracy result.\nThe test error comparison for 10 labeled points are shown in Table III; comparison for 100 labeled points are in Table IV. Apart from each dataset, the last column is the average performance rank of a technique over the datasets. A ranking arranges the methods under comparison by test error rate in ascending order. For a single dataset, we assign rank 1 for the method with the lowest average test error on that dataset, then rank 2 for the method with the second lowest test error, and so on. The average ranking is the average value of the rankings of the method on all the datasets. The smaller the ranking score, the better the method has performed.\nFrom the average rank column, the Edge Domination technique is not the best ranked, but it is in the best group of techniques in both 10 labeled and 100 labeled cases.\nWe statistically compare the results presented in Tables III and IV. For all tests we set a significance level of 5%. First, we use a test based on the average rank of each method to evaluate the null hypothesis that all the techniques are equivalent. With the Friedman test [30], there is statistically significant difference between the rank of the techniques\nSince the Friedman test result reports statistical significance, we use the Wilcoxon signed-rank test [30]. In this pairwise difference test, we test for the null hypothesis that the first technique has greater or equal error results than the second. If rejected at a 5% significance level, then we say the first\ntechnique is superior to the second. By analyzing results for 10 and 100 labeled points together, we conclude that our technique is superior to 1-NN, LEM + 1-NN, and MVU + 1- NN. Examining separately, for 10 labeled points, our method is also superior to discrete regularization, cluster kernel, and SVM. For 100 labeled points, it is also superior to LNP and LGC; whereas Laplacian RLS is superior to ours.\nE. Simulations on Human Activity Dataset\nThe Human Activity Recognition Using Smartphones [31] dataset comprises of 10299 data samples. Each sample matches 561 features extracted from motion sensors attached to a person during a time window. Each person performed six activities which are target labels in the dataset—walking (WK), walking upstairs (WU), walking downstairs (WD), sitting (ST), standing (SD), and laying down (LD).\nWe use k-NN with k = 7 for the dataset network representation once it is the smallest value that generates a connected network. The parameters are fixed in λ = 1 and τ = 1000. We compare our results with the ones published in [31], splitting the problem into six binary classification tasks.\nTable V summarises the results. For our technique, we provide the precision, recall and F Score using 5%, 10%, and 20% of labeled samples. We average the results of 10 inde-\npendent labeled set for each configuration. We also provide the original results from [31] using SVM with approximately 70% of labeled samples. Our technique performs as well as SVM using far fewer labeled samples and using the suggested parameter set. Such a feature is quite attractive because it may represents a big saving in money or efforts when involving manually data labeling in semi-supervised learning.\nV. CONCLUSION\nWe presented a transductive semi-supervised learning technique based on a dynamical system on complex networks. First, the input data is mapped into a network. Then, the Edge Domination System particle competition runs on this network. At this stage, particles compete for edges in the network. When a particle passes through an edge, it increases its class dominance over the edge while decreasing other classes’ dominance. Three dynamics—of particle walking, absorption and production—provide a biologically inspired scenario of competition and cooperation. Finally, labels are assigned according to the closest labeled vertex in the subnetworks generated by the system. As a result, the system unfolds the original network by grouping edges dominated by the same class. Furthermore, rigorous studies have been done on the novel Edge Domination System.\nThe deterministic system implementation brings advantages over its stochastic counterpart. The time complexity of the deterministic one does not depend on the number of particles, so we are benefited from better results when considering an asymptotically infinite number of initial particles. Besides, the Edge Domination system presents a stable transductive semisupervised learning technique with a subquadratic order of complexity. Computer simulations show the proposed technique achieves good classification accuracy and it is suitable to be applied in the situation where a small number of labeled samples are available. Another interesting feature of the proposed model is that it directly provides the overlapping information of each vertex or a subset of vertices.\nAs future works, we would like to investigate the mathematical property of the Edge Domination system on directed or weighted networks. Besides of this, it is interesting to further improve the runtime via network sampling methods or estimation methods. In this way, the model will be suitable to be applied to process large enough datasets or streaming data. Another interesting research is to treat the labels on edges instead of the nodes in semi-supervised learning environment.\nREFERENCES\n[1] C. M. Bishop, Pattern Recognition and Machine Learning. Secaucus, NJ: Springer, 2006. [2] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach, 3rd ed. Upper Saddle River, NJ: Prentice Hall Press, 2009. [3] O. Chapelle, B. Schölkopf, and A. Zien, Eds., Semi-Supervised Learning. Cambridge, MA: MIT Press, 2006. [4] X. Zhu and A. B. Goldberg, Introduction to Semi-Supervised Learning. Morgan and Claypool Publishers, 2009, vol. 3. [5] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell, “Text classification from labeled and unlabeled documents using EM,” Machine Learning, vol. 39, no. 2-3, pp. 103–134, 2000. [6] M. Loog and A. C. Jensen, “Semi-Supervised Nearest Mean Classification Through a Constrained Log-Likelihood,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 5, pp. 995–1006, 2015. [7] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl, “Constrained kmeans clustering with background knowledge,” in Proc. of the 18th International Conference on Machine Learning, 2001, pp. 577–584. [8] Z.-H. Zhou and M. Li, “Tri-training: exploiting unlabeled data using three classifiers,” IEEE Trans. Knowl. Data Eng., vol. 17, no. 11, pp. 1529–1541, 2005.\n[9] V. N. Vapnik, Statitical learning theory. New York, NY: Wiley, 1998. [10] T. C. Silva and L. Zhao, “Semi-supervised learning guided by the\nmodularity measure in complex networks,” Neurocomputing, vol. 78, no. 1, pp. 30–37, 2012. [11] L. Cheng and S. J. Pan, “Semi-supervised Domain Adaptation on Manifolds,” IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 12, pp. 2240–2249, 2014. [12] K. Zhang, L. Lan, J. T. Kwok, S. Vucetic, and B. Parvin, “Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 3, pp. 444–457, 2015.\n[13] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization: A geometric framework for learning from labeled and unlabeled examples,” J. Mach. Learn. Res., vol. 7, pp. 2399–2434, 2006. [14] A. Barrat, M. Barthélemy, and A. Vespignani, Dynamical processes on complex networks. New York, NY: Cambridge University Press, 2008. [15] M. E. J. Newman, Networks: An Introduction, 1st ed. New York, NY: Oxford University Press, 2010. [16] X. Zhu, A. B. Goldberg, and T. Khot, “Some new directions in graph-based semi-supervised learning,” Proc. of the EEE International Conference on Multimedia and Expo, pp. 1504–1507, 2009. [17] M. G. Quiles, L. Zhao, R. L. Alonso, and R. A. F. Romero, “Particle competition for complex network community detection,” Chaos, vol. 18, no. 3, 2008. [18] T. C. Silva and L. Zhao, “Network-based stochastic semisupervised learning,” IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 451–66, 2012. [19] ——, “Network-based stochastic semisupervised learning,” IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 451–466, 2012. [20] ——, “Detecting and preventing error propagation via competitive learning,” Neural Networks, vol. 41, pp. 70–84, 2013. [21] J. Jensen, “Sur les fonctions convexes et les inégalités entre les valeurs moyennes,” Acta Mathematica, vol. 30, no. 1, pp. 175–193, 1906. [22] J. L. Bentley, “Multidimensional binary search trees used for associative searching,” Commun. ACM, vol. 18, no. 9, pp. 509–517, 1975. [23] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf, “Learning with local and global consistency,” in Advances in Neural Information Processing Systems 16, S. Thrun, L. Saul, and B. Schölkopf, Eds. MIT Press, 2004, pp. 321–328. [24] R. Collobert, F. Sinz, J. Weston, and L. Bottou, “Large Scale Transductive SVMs,” Journal of Machine Learning Research, vol. 7, no. 7, pp. 1687–1712, 2006. [25] B. Wang, Z. Tu, and J. K. Tsotsos, “Dynamic label propagation for semi-supervised multi-class multi-label classification,” Proc. of the IEEE International Conference on Computer Vision, pp. 425–432, 2013. [26] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data with label propagation,” School Comput Sci Carnegie Mellon Univ Pittsburgh PA Tech Rep, vol. 54, no. CMU-CALD-02-107, pp. 1–19, 2002. [27] Y. M. Zhang, K. Huang, G. G. Geng, and C. L. Liu, “MTC: A Fast and Robust Graph-Based Transductive Learning Method,” IEEE Trans Neural Netw Learn Syst, vol. 26, no. 9, pp. 1979–1991, 2014. [28] J. B. Tenenbaum, V. de Silva, and J. C. Langford, “A global geometric framework for nonlinear dimensionality reduction,” Science, vol. 290, no. 5500, p. 2319, 2000. [29] R. Duin, P. Juszczak, P. Paclı́k, E. Pekalska, D. de Ridder, D. Tax, and S. Verzakov, “PR-Tools4.1, a matlab toolbox for pattern recognition,” 2007. [30] M. Hollander and D. A. Wolfe, Nonparametric Statistical Methods, 2nd ed. Wiley-Interscience, 1999. [31] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A Public Domain Dataset for Human Activity Recognition Using Smartphones,” in 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013, 2013.\nAPPENDIX\nSIMULATION ON MAGIC GAMMA TELESCOPE\nDue to the low time-complexity of our technique, we can test it in a larger dataset. The MAGIC Gamma Telescope [Bock2004] consist of 19020 with 11 features each. Classification accuracy is not meaningful for this data, since classifying a background event as signal is worse than classifying a signal event as background. To deal with this issues, alternative performance indices based on ROC curve are used. For detailed explanation of such indices refer to [Bock2004]. To calculate the ROC curve, we modify our vertex labeling step to result in a fuzzy classification.\nSimilarly to the previous experiment, we fixed k = 9, λ = 1, and τ = 1000. Table VI summarises the obtained results from 10 independent sets with 10% of labeled samples. Some of the results from [Bock2004] are provided as well. The Edge Domination model outperforms all of the methods under comparison using far fewer labeled samples.\n[Bock2004] R. Bock, A. Chilingarian, M. Gaug, F. Hakl, T. Hengstebeck, M. Jiřina, J. Klaschka, E. Kotrč, P. Savický, S. Towers, A. Vaiciulis, and W. Wittek, “Methods for multidimensional event classification: a case study using images from a Cherenkov gamma-ray telescope,” Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, vol. 516, no. 2-3, pp. 511—528, 2004."
    } ],
    "references" : [ {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Secaucus, NJ: Springer,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Artificial Intelligence: A Modern Approach, 3rd ed",
      "author" : [ "S. Russell", "P. Norvig" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Semi-Supervised Learning",
      "author" : [ "O. Chapelle", "B. Schölkopf", "A. Zien", "Eds" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Introduction to Semi-Supervised Learning",
      "author" : [ "X. Zhu", "A.B. Goldberg" ],
      "venue" : "Morgan and Claypool Publishers,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Text classification from labeled and unlabeled documents using EM",
      "author" : [ "K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell" ],
      "venue" : "Machine Learning, vol. 39, no. 2-3, pp. 103–134, 2000.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Semi-Supervised Nearest Mean Classification Through a Constrained Log-Likelihood",
      "author" : [ "M. Loog", "A.C. Jensen" ],
      "venue" : "IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 5, pp. 995–1006, 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Constrained kmeans clustering with background knowledge",
      "author" : [ "K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schroedl" ],
      "venue" : "Proc. of the 18th International Conference on Machine Learning, 2001, pp. 577–584.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Tri-training: exploiting unlabeled data using three classifiers",
      "author" : [ "Z.-H. Zhou", "M. Li" ],
      "venue" : "IEEE Trans. Knowl. Data Eng., vol. 17, no. 11, pp. 1529–1541, 2005.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Statitical learning theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "New York, NY: Wiley,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Semi-supervised learning guided by the modularity measure in complex networks",
      "author" : [ "T.C. Silva", "L. Zhao" ],
      "venue" : "Neurocomputing, vol. 78, no. 1, pp. 30–37, 2012.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Semi-supervised Domain Adaptation on Manifolds",
      "author" : [ "L. Cheng", "S.J. Pan" ],
      "venue" : "IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 12, pp. 2240–2249, 2014.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines",
      "author" : [ "K. Zhang", "L. Lan", "J.T. Kwok", "S. Vucetic", "B. Parvin" ],
      "venue" : "IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 3, pp. 444–457, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "M. Belkin", "P. Niyogi", "V. Sindhwani" ],
      "venue" : "J. Mach. Learn. Res., vol. 7, pp. 2399–2434, 2006.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Dynamical processes on complex networks",
      "author" : [ "A. Barrat", "M. Barthélemy", "A. Vespignani" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Some new directions in graph-based semi-supervised learning",
      "author" : [ "X. Zhu", "A.B. Goldberg", "T. Khot" ],
      "venue" : "Proc. of the EEE International Conference on Multimedia and Expo, pp. 1504–1507, 2009.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Particle competition for complex network community detection",
      "author" : [ "M.G. Quiles", "L. Zhao", "R.L. Alonso", "R.A.F. Romero" ],
      "venue" : "Chaos, vol. 18, no. 3, 2008.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Network-based stochastic semisupervised learning",
      "author" : [ "T.C. Silva", "L. Zhao" ],
      "venue" : "IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 451–66, 2012.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Network-based stochastic semisupervised learning",
      "author" : [ "——" ],
      "venue" : "IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 451–466, 2012.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Detecting and preventing error propagation via competitive learning",
      "author" : [ "——" ],
      "venue" : "Neural Networks, vol. 41, pp. 70–84, 2013.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sur les fonctions convexes et les inégalités entre les valeurs moyennes",
      "author" : [ "J. Jensen" ],
      "venue" : "Acta Mathematica, vol. 30, no. 1, pp. 175–193, 1906.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1906
    }, {
      "title" : "Multidimensional binary search trees used for associative searching",
      "author" : [ "J.L. Bentley" ],
      "venue" : "Commun. ACM, vol. 18, no. 9, pp. 509–517, 1975.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Schölkopf" ],
      "venue" : "Advances in Neural Information Processing Systems 16, S. Thrun, L. Saul, and B. Schölkopf, Eds. MIT Press, 2004, pp. 321–328.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Large Scale Transductive SVMs",
      "author" : [ "R. Collobert", "F. Sinz", "J. Weston", "L. Bottou" ],
      "venue" : "Journal of Machine Learning Research, vol. 7, no. 7, pp. 1687–1712, 2006.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Dynamic label propagation for semi-supervised multi-class multi-label classification",
      "author" : [ "B. Wang", "Z. Tu", "J.K. Tsotsos" ],
      "venue" : "Proc. of the IEEE International Conference on Computer Vision, pp. 425–432, 2013.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning from labeled and unlabeled data with label propagation",
      "author" : [ "X. Zhu", "Z. Ghahramani" ],
      "venue" : "School Comput Sci Carnegie Mellon Univ Pittsburgh PA Tech Rep, vol. 54, no. CMU-CALD-02-107, pp. 1–19, 2002.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "MTC: A Fast and Robust Graph-Based Transductive Learning Method",
      "author" : [ "Y.M. Zhang", "K. Huang", "G.G. Geng", "C.L. Liu" ],
      "venue" : "IEEE Trans Neural Netw Learn Syst, vol. 26, no. 9, pp. 1979–1991, 2014.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "A global geometric framework for nonlinear dimensionality reduction",
      "author" : [ "J.B. Tenenbaum", "V. de Silva", "J.C. Langford" ],
      "venue" : "Science, vol. 290, no. 5500, p. 2319, 2000.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "PR-Tools4.1, a matlab toolbox for pattern recognition",
      "author" : [ "R. Duin", "P. Juszczak", "P. Paclı́k", "E. Pekalska", "D. de Ridder", "D. Tax", "S. Verzakov" ],
      "venue" : "2007.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A Public Domain Dataset for Human Activity Recognition Using Smartphones",
      "author" : [ "D. Anguita", "A. Ghio", "L. Oneto", "X. Parra", "J.L. Reyes-Ortiz" ],
      "venue" : "21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013, 2013.  PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS  14",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Conversely, unsupervised learning techniques try to discover the intrinsic structure of a completely unlabeled data set [1], [2].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "Conversely, unsupervised learning techniques try to discover the intrinsic structure of a completely unlabeled data set [1], [2].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "vised learning paradigms where both unlabeled and labeled data are taken into account in class or cluster formation and prediction process [3], [4].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "vised learning paradigms where both unlabeled and labeled data are taken into account in class or cluster formation and prediction process [3], [4].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]–[12].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]–[12].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]–[12].",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]–[12].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 8,
      "context" : "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]–[12].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 9,
      "context" : "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]–[12].",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 11,
      "context" : "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]–[12].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 12,
      "context" : "The labels are propagated to the whole graph using a particular optimization heuristic [13].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "Complex networks are large-scale graphs with nontrivial topology [14].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Such networks introduce a powerful tool to describe the interplay of topology, structure, and dynamics of complex systems [14], [15].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "Moreover, traditional graph-based techniques have high computational complexity, usually at cubic order [16].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "A common strategy to overcome this disadvantage is using a set of sparse prototypes derived from the data [12].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "The particle competition model was originally proposed in [17] and extended for the data clustering task in [18].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "The particle competition model was originally proposed in [17] and extended for the data clustering task in [18].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : "Later, it has been applied to semisupervised learning [19], [20] where the particle competition is formally represented by a nonlinear stochastic dynamical system.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "Later, it has been applied to semisupervised learning [19], [20] where the particle competition is formally represented by a nonlinear stochastic dynamical system.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "If the destination vertex is not a sink, its survival probability is 1− λσ̃ ij(t) where λ ∈ [0, 1] is the competition parameter.",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : "Thus, with the Jensen’s inequality [21], we have E [ σ̃ ij(t) ] ≥ 1− E [ ñij(t) ] + E [ ñji(t) ] ∑C q=1 E [ ñqij(t) ] + E [ ñqji(t) ] .",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 20,
      "context" : "The k-NN method, for example, has complexity order of O(D |V| log |V|) using multidimensional binary search tree [22].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 21,
      "context" : "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 22,
      "context" : "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).",
      "startOffset" : 259,
      "endOffset" : 263
    }, {
      "referenceID" : 23,
      "context" : "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).",
      "startOffset" : 296,
      "endOffset" : 300
    }, {
      "referenceID" : 24,
      "context" : "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).",
      "startOffset" : 323,
      "endOffset" : 327
    }, {
      "referenceID" : 16,
      "context" : "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).",
      "startOffset" : 350,
      "endOffset" : 354
    }, {
      "referenceID" : 25,
      "context" : "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).",
      "startOffset" : 415,
      "endOffset" : 419
    }, {
      "referenceID" : 25,
      "context" : "Only the proposed Edge Domination and Minimum Tree Cut [27] have linear time, though the latter must either receive or construct a spanning tree.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "The networks used for the analysis were generated by the following model: A complex network is constructed given a labeled vector y, a number m > 0 of edges by vertex, and a weight p ∈ [0, 1] that controls the preferential attachment between vertices of different classes.",
      "startOffset" : 185,
      "endOffset" : 191
    }, {
      "referenceID" : 26,
      "context" : "In our experiments, we use k-NN graph with Euclidean distance since it was proved to approximate the low-dimensional manifold of a set of points [28].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 27,
      "context" : "(The datasets were generated using the PRTools framework [29].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "Simulations on Benchmark Datasets We compare our model with 14 semi-supervised techniques tested on Chapelle’s benchmark [3].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "The datasets are described in [3].",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "), Transductive support vector machines (TSVM), Cluster kernels (ClusterKernel), Low-density separation (LDS), Laplacian regularized least squares (Laplacian RLS), Local and global consistency (LGC), Label propagation (LP), Linear neighborhood propagation (LNP), and Network-Based Stochastic Semisupervised Learning (Vertex Domination), The simulation results were collected from [3], except for LGC, LP, LNP, and Vertex Domination that are found in [18].",
      "startOffset" : 380,
      "endOffset" : 383
    }, {
      "referenceID" : 16,
      "context" : "), Transductive support vector machines (TSVM), Cluster kernels (ClusterKernel), Low-density separation (LDS), Laplacian regularized least squares (Laplacian RLS), Local and global consistency (LGC), Label propagation (LP), Linear neighborhood propagation (LNP), and Network-Based Stochastic Semisupervised Learning (Vertex Domination), The simulation results were collected from [3], except for LGC, LP, LNP, and Vertex Domination that are found in [18].",
      "startOffset" : 450,
      "endOffset" : 454
    } ],
    "year" : 2016,
    "abstractText" : "Abstract—The emergence of collective dynamics in neural networks is a mechanism of the animal and human brain for information processing. In this paper, we develop a computational technique of distributed processing elements, which are called particles. We observe the collective dynamics of particles in a complex network for transductive inference on semi-supervised learning problems. Three actions govern the particles’ dynamics: walking, absorption, and generation. Labeled vertices generate new particles that compete against rival particles for edge domination. Active particles randomly walk in the network until they are absorbed by either a rival vertex or an edge currently dominated by rival particles. The result from the model simulation consists of sets of edges sorted by the label dominance. Each set tends to form a connected subnetwork to represent a data class. Although the intrinsic dynamics of the model is a stochastic one, we prove there exists a deterministic version with largely reduced computational complexity; specifically, with subquadratic growth. Furthermore, the edge domination process corresponds to an unfolding map. Intuitively, edges “stretch” and “shrink” according to edge dynamics. Consequently, such effect summarizes the relevant relationships between vertices and uncovered data classes. The proposed model captures important details of connectivity patterns over the edge dynamics evolution, which contrasts with previous approaches focused on vertex dynamics. Computer simulations reveal that our model can identify nonlinear features in both real and artificial data, including boundaries between distinct classes and the overlapping structure of data.",
    "creator" : "LaTeX with hyperref package"
  }
}