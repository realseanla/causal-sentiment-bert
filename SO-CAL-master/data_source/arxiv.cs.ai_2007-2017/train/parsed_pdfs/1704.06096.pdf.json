{
  "name" : "1704.06096.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Dependent Doors Problem: An Investigation into Sequential Decisions without Feedback∗",
    "authors" : [ "Amos Korman", "Yoav Rodeh" ],
    "emails" : [ "amos.korman@irif.fr", "yoav.rodeh@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The fundamental distribution of a door describes the probability it opens in the best of conditions (with respect to other doors being open or closed). We show that if in two configurations of d doors corresponding doors share the same fundamental distribution, then these configurations have the same optimal running time up to a universal constant, no matter what are the dependencies between doors and what are the distributions. We also identify algorithms that are optimal up to a universal constant factor. For the case in which all doors share the same fundamental distribution we additionally provide a simpler algorithm, and a formula to calculate its running time. We furthermore analyse the price of lacking feedback for several configurations governed by standard fundamental distributions. In particular, we show that the price is logarithmic in d for memoryless doors, but can potentially grow to be linear in d for other distributions.\nWe then turn our attention to investigate precise bounds. Even for the case of two doors, identifying the optimal sequence is an intriguing combinatorial question. Here, we study the case of two cascading memoryless doors. That is, the first door opens on each knock independently with probability p1. The second door can only open if the first door is open, in which case it will open on each knock independently with probability p2. We solve this problem almost completely by identifying algorithms that are optimal up to an additive term of 1."
    }, {
      "heading" : "1 Introduction",
      "text" : "Often it is the case that one must accomplish multiple tasks whose success probabilities are dependent on each other. In many cases, failure to achieve one task will tend to have a more negative affect on the success probabilities of other tasks. In general, such dependencies may be quite complex, and balancing the work load between different tasks becomes a computational challenge. The situation is further complicated if the ability to detect whether a task has been accomplished is limited. For example, if task B highly depends on task A then until A is accomplished, all efforts invested in B may be completely wasted. How should one divide the effort between these tasks if feedback on the success of A is not available?\nIn this preliminary work we propose a setting that captures some of the fundamental challenges that are inherent to the process of decision making without feedback. We introduce the dependent doors\n∗This work has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 648032).\nar X\niv :1\n70 4.\n06 09\n6v 1\n[ cs\n.A I]\n2 0\nA pr\n2 01\nproblem, informally described as follows. There are d ≥ 2 doors (representing tasks) which are initially closed, and the aim is to open all of them as fast as possible. To open a door, the algorithm can “knock” on it and it might open or not according to some governing probability distribution, that may depend on other doors being open or closed1. We focus on settings in which doors are positively correlated, which informally means that the probability of opening a door is never decreased if another door is open. The governing distributions and their dependencies are known to the algorithm in advance. Crucially, however, during the execution, it gets no direct feedback on whether or not a door has opened unless all d doors have opened, in which case the task is completed.\nThis research has actually originated from our research on heuristic search on trees [4]. Consider a tree of depth d with a treasure placed at one of its leaves. At each step the algorithm can “check” a vertex, which is child of an already checked vertex. Moreover, for each level of the tree, the algorithm has a way to compare the previously checked vertices on that level. This comparison has the property that if the ancestor of the treasure on that level was already checked, then it will necessarily be considered as the “best” on that level. Note, however, that unless we checked all the vertices on a given level, we can never be sure that the vertex considered as the best among checked vertices in the level is indeed the correct one. With such a guarantee, and assuming that the algorithm gets no other feedback from checked vertices, any reasonable algorithm that is about to check a vertex on a given level, will always choose to check a child of the current best vertex on the level above it. Therefore, the algorithm can be described as a sequence of levels to inspect. Moreover, if we know the different distributions involved, then we are exactly at the situation of the dependent doors problem. See Appendix A for more details on this example.\nAnother manifestation of d dependent doors can arise in the context of cryptography. Think about a sequence of d cascading encryptions, and separate decryption protocols to attack each of the encryptions. Investing more efforts in decrypting the i’th encryption would increase the chances of breaking it, but only if previous encryptions where already broken. On the other hand, we get no feedback on an encryption being broken unless all of them are.\nThe case of two doors can serve as an abstraction for exploration vs. exploitation problems, where it is typically the case that deficient performances on the exploration part may result in much waste on the exploitation part [10, 17]. It can also be seen as the question of balance between searching and verifying in algorithms that can be partitioned thus [1, 15]. In both examples, there may be partial or even no feedback in the sense that we don’t know that the first procedure succeeded unless the second one also succeeds.\nFor simplicity, we concentrate on scenarios in which the dependencies are acyclic. That is, if we draw the directed dependency graph between doors, then this graph does not contain any directed cycles. The examples of searching and verifying and the heuristic search on trees can both be viewed as acyclic. Moreover, despite the fact that many configurations are not purely acyclic, one can sometimes obtain a useful approximation that is.\nTo illustrate the problem, consider the following presumably simple case of two dependent memoryless doors. The first door opens on each knock independently with probability 1/2. The second door can only open if the first door is open, in which case it opens on each knock independently, with probability 1/2. What is the sequence of knocks that minimizes the expected time to open both doors, remembering that we don’t know when door 1 opens? It is easy to see that the alternating sequence 1, 2, 1, 2, 1, 2, . . . results in 6 knocks in expectation. Computer simulations indicate that the best sequence gives a little more than 5.8 and starts with 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2. Applied to this particular scenario, our theoretical lower bound gives 5.747, and our upper bound gives a sequence with expected time 5.832."
    }, {
      "heading" : "1.1 Context and Related Work",
      "text" : "This paper falls under the framework of decision making under uncertainty, a large research subject that has received significant amount of attention from researchers in various disciplines, including computer science, operational research, biology, sociology, economy, and even psychology and cognition, see, e.g., [2, 3, 5, 6, 7, 8, 9, 16].\nPerforming despite limited feedback would fit the framework of reinforced learning [17] and is inherent to the study of exploration vs. exploitation type of problems, including Multi-Armed Bandit problems [10]. In this paper we study the impact of having no feedback whatsoever. Understanding this extreme scenario may serve as an approximation for cases where feedback is highly restricted, or limited in its impact. For\n1Actually, the distribution associated with some door i may depend on the state of other doors (being open or closed) not only at the current knock, but also at the time of each of the previous knocks on door i.\nexample, if it turns out that the price of lacking feedback is small, then it may well be worth to avoid investing efforts in complex methods for utilizing the partial feedback.\nOf particular interest is the case of two doors. As mentioned, difficulties resulting from the lack of feedback can arise when one aims to find a solution by alternating between two subroutines: Producing promising candidate solutions and verifying these candidates. Numerous strategies are based on this interplay, including heuristics based on brute force or trail and error approaches [1, 15], sample and predict approaches [11, 14, 17], iterative local algorithms [12, 13], and many others. Finding strategies for efficiently balancing these two tasks can be therefore applicable."
    }, {
      "heading" : "1.2 Setting",
      "text" : "There are d ≥ 2 doors and each door can be either open or closed. Doors start closed, and once a door opens it never closes. To open a door, an algorithm can knock on it and it might open or not according to some probability distribution. The goal is to minimize the expected number of knocks until all doors open. Crucially, the algorithm has no feedback on whether or not a door has opened, unless all doors have opened, in which case the task is completed.\nThe probability that a door opens may depend on the state of other doors (being open or closed) at the time of the current knock as well as on their state during each of the previous knocks on the door. For example, the probability that a certain knock at door i succeeds may depend on the number of previous knocks on door i, but counting only those that were made while some other specific door j was open. The idea behind this definition is that the more time we invest in opening a door the more likely it is to open, and the quality of each knock depends on what is the state of the doors it depends on at the time of the knock.\nBelow we provide a semi-formal description of the setting. The level of detail is sufficient to understand the content of the main text, which is mainly concerned with independent and cascading configurations. The reader interested in a more formal description of the model is deferred to Appendix B.\nA specific setting of doors is called a configuration (normally denoted C). This includes a description of all dependencies between doors and the resulting probability distributions. In this paper we assume that the dependency graph of the doors is acyclic, and so we may assume that a configuration describes an ordering of the doors, such that each door depends only on lower index doors. Furthermore, we assume that the correlation between doors is positive, i.e., a door being open can only improve the chances of other doors to open.\nPerhaps the simplest configuration is when all doors are independent of each other. In this case, door i can be associated with a function pi : N→ [0, 1], where pi(n) is the probability that door i is not open after knocking on it n times. Another family of acyclic configurations are cascading configurations. Here, door i cannot open unless all doors of lower index are already open. In this case, the configuration can again be described by a set of functions {pi}di=1, where pi(n) describes the probability that door i is not open after knocking on it n times, where the count starts only after door i− 1 is already open.\nIn general, given a configuration, each door i defines a non-decreasing function pi : N→ [0, 1], called the fundamental distribution of the door, where pi(n) is the probability that the door is not open after knocking on it n times in the best of conditions, i.e., assuming all doors of lower index are open. In the case of independent and cascading configurations, the fundamental distribution pi coincides with the functions mentioned above. Two doors are similar if they have the same fundamental distribution. Two configurations are similar if for every i, door i of the first configuration is similar to door i of the second.\nWhen designing an algorithm, we will assume that the configuration it is going to run in is known. As there is no feedback, a deterministic algorithm can be thought of as a possibly infinite sequence of door knocks. A randomized algorithm is therefore a distribution over sequences, and as all of them will have expected running time at least as large as that of an optimal sequence (if one exists), the expected running time of a randomized algorithm cannot be any better. Denote by TC(π), the expected time until all doors open when running sequence π in configuration C. We define TC = minπ TC(π). By Claim 23 in Appendix B.3, there exists a sequence achieving this minimum. Therefore, by the aforementioned arguments, we can restrict our discussion to deterministic algorithms only.\nIf we had feedback we would knock on each door until it opens, and then continue to the next. Denoting by Ei = ∑∞ n=0 pi(n) the expected time to open door i on its own, the expected running time\nthen does not depend on the specific dependencies between doors at all, and is ∑ iEi. Also, this value is clearly optimal. To evaluate the impact of lacking feedback for a configuration C, we therefore define:\nPrice(C) = TC∑ iEi\nObviously Price(C) ≥ 1, and for example, if all doors start closed and open after just 1 knock, it is in fact equal to 1. Claim 22 in Appendix B.2 shows that Price(C) ≤ d."
    }, {
      "heading" : "1.3 Our Results",
      "text" : "We have two main results. The first one, presented in Section 2, states that any two similar configurations have the same optimal running time up to a constant factor. We stress that this constant factor is universal in the sense that it does not depend on the specific distributions or on the number of doors d.\nFurthermore, given a configuration, we identify an algorithm that is optimal for it up to a constant factor. We then show that for configurations where all doors are similar, there is a much simpler algorithm which is optimal up to a constant factor, and describe a formula that computes its approximate running time. We conclude Section 2 by analysing the price of lacking feedback for several configurations governed by standard fundamental distributions. In particular, we show that the price is logarithmic in d for memoryless doors, but can potentially grow to be linear in d for other distributions.\nWe then turn our attention to identify exact optimal sequences. Perhaps the simplest case is the case of two cascading memoryless doors. That is, the first door opens on each knock independently with probability p1. The second door can only open if the first door is open, in which case it opens on each knock independently, with probability p2. In Section 3 we present our second main result: Algorithms for these configurations that achieve the precise optimal running time up to an additive term of 1.\nOn the technical side, to establish such an extremely competitive algorithm, we first consider a semi-fractional variant of the problem and find a sequence that achieves the precise optimal bound. We then approximate this semi-fractional sequence to obtain an integer solution losing only an additive term of 1 in the running time. A nice anecdote is that in the case where p1 = p2 and are very small, the ratio of 2-knocks over 1-knocks in the sequence we get approaches the golden ratio. Also, in this case, the optimal running time approaches 3.58/p1 as p1 goes to zero. It follows that in this case, the price of lacking feedback tends to 3.58/2 and the price of dependencies, i.e., the multiplicative gap between the cascading and independent settings, tends to 3.58/3."
    }, {
      "heading" : "2 Near Optimal Algorithms",
      "text" : "The following important lemma is proved in Appendix B.1 using a coupling argument:\nLemma 1. Consider similar configurations C,X and I, where X is cascading and I is independent. For every sequence π, TI(π) ≤ TC(π) ≤ TX (π). This also implies that TI ≤ TC ≤ TX .\nThe next theorem presents a near optimal sequence of knocks for a given configuration. In fact, by Lemma 1, this sequence is near optimal for any similar configuration, and so we get that the optimal running time for any two similar configurations is the same up to a universal multiplicative factor.\nTheorem 2. There is a polynomial algorithm2, that given a configuration C generates a sequence π such that TC(π) = Θ(TI). In fact, TC(π) ≤ 2 + 4TI ≤ 2 + 4TC.\nProof. Denote by p1, . . . , pd the fundamental distributions of the doors of C. For a finite sequence of knocks α, denote by SCC(α) the probability that after running α in configuration C, some of the doors are still closed. Note that if α is sorted, that is, if all knocks on door 1 are done first, followed by the knocks on doors 2, etc., then SCX (α) = SCI(α).\nWe start by showing that for any T , we can construct in polynomial time a finite sequence αT of length T that maximizes the probability that all doors will open, i.e., minimizes SCI(αT ). As noted above, if we sort the sequence, this is equal to SCX (αT ).\nThe algorithm follows a dynamic programming approach, and calculates a matrix A, where A[i, t] holds the maximal probability that a sequence of length t has of opening all of the doors 1, 2, . . . , i. All the entries A[0, ·] are just 1, and the key point is that for each i and t, knowing all of the entries in A[i, ·], it is easy to calculate A[i+ 1, t]:\nA[i+ 1, t] = t\nmax k=0\nA[i, t− k] · (1− pi+1(k))\n2A polynomial algorithm in our setting generates the next knock in the sequence in polynomial time in the index of the knock and in d, assuming that reading any specific value of any of the fundamental distributions of a door takes constant time.\nCalculating the whole table takes O(dT 2) time, and A[d, T ] will give us the highest probability a sequence of length T can have of opening all doors. Keeping tabs on the choices the max in the formula makes, we can get an optimal sequence αT , and can take it to be sorted.\nConsider the sequence π = α2 · α4 · · ·α2n · · · . The complexity of generating this sequence up to place T is O(dT 2), and so this algorithm is polynomial. Our goal will be to compare TX (π) with TI(π?), where π? is the optimal sequence for I.\nThe following observation stems from the fact that for any natural valued random variable X, E [X] = ∑∞ n=0 Pr [X > n] and Pr [X > n] is a non-increasing function of n. Observation 3. Let {an}∞n=1 be a strictly increasing sequence of natural numbers, and X be some natural valued random variable. Then:\n∞∑ n=1 (an+1 − an)Pr [X > an+1] ≤ E [X] ≤ a1 + ∞∑ n=1 (an+1 − an)Pr [X > an]\nFor a sequence π, denote by π[n] the prefix of π of length n. In this terminology, TC(π) =∑∞ n=0 SCC(π[n]). Setting an = 2 + 4 + . . . + 2\nn in the right side of Observation 3, and letting X be the number of rounds until all doors open when using π, we get:\nTX (π) ≤ 2 + ∞∑ n=1 2n+1 · SCX (π[2 + . . .+ 2n]) ≤ 2 + ∞∑ n=1 2n+1 · SCX (α2n)\n= 2 + ∞∑ n=1 2n+1 · SCI(α2n) ≤ 2 + ∞∑ n=1 2n+1 · SCI(π?[2n]) ≤ 2 + 4TI(π?)\nThe last step is using Observation 3 with an = 2 n−1. Theorem 2 concludes."
    }, {
      "heading" : "2.1 Configurations where all Doors are Similar",
      "text" : "In this section we focus on configurations where all doors have the same fundamental distribution p(n). We provide simple algorithms that are optimal up to a universal constant, and establish the price of lacking feedback with respect to a few natural distributions. Corresponding proofs appear in Appendix C."
    }, {
      "heading" : "2.1.1 Simple Algorithms",
      "text" : "Let us consider the following very simple algorithm Asimp. It runs in phases, where in each phase it knocks on each door once, in order. As a sequence, we can write Asimp = (1, 2, . . . , d)\n∞. Let X1, . . . , Xd be i.i.d. random variables taking positive integer values, satisfying Pr [Xi > n] = p(n). The following is straightforward:\nClaim 4. TI(Asimp) = Θ (d · E [max {X1, . . . , Xd}]) This one is less trivial:\nClaim 5. If all doors are similar then TI(Asimp) = Θ(TI)\nThe claim above states that Asimp is optimal up to a multiplicative constant factor in the independent case, where all doors are similar. As a result, we can also show:\nClaim 6. Denote by αn the sequence 1 2n , . . . , d2 n\n. If all doors are similar then for any configuration C, TC (α0 · α1 · α2 · · ·) = Θ(TC).\nIn plain words, the above claim states that the following algorithm is optimal up to a universal constant factor for any configuration where all doors are similar: Run in phases where phase n consists of knocking 2n consecutive times on each door, in order."
    }, {
      "heading" : "2.1.2 On the Price of Lacking Feedback",
      "text" : "By Claims 4 and 5, investigating the price of lacking feedback when all doors are similar boils down to understanding the expected maximum of i.i.d. random variables.\nPrice = Θ\n( E [max {X1, . . . , Xd}]\nE [X1]\n) (1)\nNote that we omitted dependency on the configuration, as by Theorem 2, up to constant factors, it is the same price as in the case where the doors are independent. Let us see a few examples of this value. First:\nLemma 7. If X1, . . . , Xd are i.i.d. random variables taking natural number values, then:\nE [max(X1, . . . , Xd)] = Θ\n( κ+ d\n∞∑ n=κ Pr [Xi > n]\n)\nWhere κ = min {n ∈ N | Pr [X1 > n] < 1/d}\nExample 8. After the first knock on it, each door opens with probability 1− 1/d and if it doesn’t, it will open at its d+ 1’st knock. The expected time to open each door on its own is 2. By Lemma 7, as κ = d+ 1, we get that Price = Ω(κ) = Ω(d). By Claim 22, Price = Θ(d).\nExample 9. If p(n) = qn for some 1/2 < q < 1, then Price = Θ(log(d)).\nExample 10. If for some c > 0 and a > 1, p(n) = min(1, c/na), then Price = Θ(d 1 a ).\nSometimes we know a bound on some moment of the distribution of opening a door. If E [X1] < M , then by Claim 22, T = O(d2M). Also,\nExample 11. If E [Xa1 ] < M for some a > 1, then T = O ( d1+ 1 aM1/a(1 + 1a−1 ) ) .\nFor example, if the second moment of the time to open a door on its own is bounded, we get an O(d3/2) algorithm."
    }, {
      "heading" : "3 Two Memoryless Cascading Doors",
      "text" : "One can say that by Theorem 2 we solved much of the dependent doors problem. There is an equivalence of the independent and cascading models, and we give an up to constant factor optimal algorithm for any situation. However, we still find the question of finding the true optimal sequences for cascading doors to be an interesting one. What is the precise cost of having no feedback, in numbers? Even the simple case of two doors, each opening with probability 1/2 on each knock, turns out to be quite challenging and has a not so intuitive optimal sequence.\nIn this section, we focus on a very simple yet interesting case of the cascading door problem, and solve it almost exactly. We have two doors. Door 1 opens with probability p1 each time we knock on it, and door 2 opens with probability p2. We further extend the setting to consider different durations. Specifically, we assume that a knock on door 1 takes one time unit, and a knock on door 2 takes c time units. Denote q1 = 1− p1 and q2 = 1− p2. For brevity, we will call a knock on door 1 a 1-knock, and a knock on door 2 a 2-knock.\nThe Semi-Fractional Model. As finding the optimal sequence directly proved to be difficult, we introduce a relaxation of our original model, termed the semi-fractional model. In this model, we allow 1-knocks to be of any length. A knock of length t, where t is a non-negative real number, will have probability of 1− qt1 of opening the door. In this case, a sequence consists of the alternating elements 1t and 2, where 1t describes a knock of length t on door 1. We call sequences in the semi-fractional model semi-fractional sequences, and to differentiate, we call sequences in the original model integer sequences.\nAs our configuration C will be clear from context, for a sequence π, we define E [π] = TC(π) to be the expected running time of the sequence. Clearly, every integer sequence has a similar semi-fractional sequence with the same expected running time. As we will see, the reverse is not far from being true. That being so, finding the optimal semi-fractional sequence will give an almost optimal integer sequence."
    }, {
      "heading" : "3.1 Equivalence of Models",
      "text" : "Theorem 12. Every semi-fractional sequence π has an integer sequence π′, s.t., E [π′] ≤ E [π] + 1.\nFor this purpose, in this subsection only, we describe a semi-fractional sequence π as a sequence of non-decreasing non-negative real numbers: π0, π1, π2, . . ., where π0 = 0. This sequence describes the following semi-fractional sequence (in our original terms):\n1π1−π0 · 2 · 1π2−π1 · 2 · · ·\nThis representation simplifies our proofs considerably. Here are some observations:\n• 1-knocks can be of length 0, yet we still consider them in our indexing.\n• The sequence is an integer sequence iff for all i, πi ∈ N.\n• The i-th 2-knock starts at time πi + c(i− 1) and ends at πi + ci.\n• The probability of door 1 being closed after the completion of the i-th 1-knock is qπi1 , and so the probability it opens at 1-knock i is q\nπi−1 1 − q πi 1\nLemma 13. For two sequences π = (π0, π1, . . .) and π ′ = (π′1, π ′ 2, . . .), if for all i, πi ≤ π′i ≤ πi + 1 then E [π′] ≤ E [π] + 1. Lemma 13 is the heart of our theorem. Indeed, once proven, Theorem 12 follows in a straightforward manner. Given a semi-fractional sequence π, define π′i = dπie. Then, π′ is an integer sequence, and it satisfies the conditions of the lemma, so we are done. The lemma makes sense, as the sequence π′ in which for all i > 0, π′i = πi + 1, can be thought of as adding a 1-knock of length one in the beginning of the sequence. Even if this added 1-knock did nothing, the running time would increase by at most 1. However, the proof is more involved, since in the lemma, while some of the 2-knocks may have an increased chance of succeeding, some may actually have a lesser chance.\nProof. Given a sequence π and an event X, we denote by E [π |X] the expected running time of π given the event X. Let Xi denote the event that door 1 opens at its i-th 1-knock. As already said:\nPr [Xi] = q πi−1 1 − q πi 1 = ∫ πi πi−1 qx1 ln(q1) dx\nWhere the last equality comes as no surprise, as it can be seen as modelling door 1 in a continuous fashion, having an exponential distribution fitting its geometrical one. Now:\nE [π] = ∞∑ i=1 Pr [Xi] E [π |Xi] = ∞∑ i=1 ∫ πi πi−1 qx1 ln(q1) dx · E [π |Xi] = ∫ ∞ 0 qx1 ln(q1) · E [ π ∣∣Xi(x)]dx\nWhere i(x) = maxi {x ≥ πi−1}, that is, the index of the 1-knock that x belongs to when considering only time spent knocking on door 1. Defining X ′i and i\n′(x) in an analogous way for π′, we want to show that for all x,\nE [ π′ ∣∣∣X ′i′(x)] ≤ 1 + E [π ∣∣Xi(x)]\nas using it with the last equality will prove the lemma. We need the following three claims:\n1. If j ≤ i, then E [π |Xj ] ≤ E [π |Xi]\n2. For all x, i′(x) ≤ i(x)\n3. For all i, E [π′ |X ′i] ≤ 1 + E [π |Xi] Together they give what we need:\nE [ π′ ∣∣∣X ′i′(x)] ≤ 1 + E [π ∣∣Xi′(x)] ≤ 1 + E [π ∣∣Xi(x)]\nThe first is actually true trivially for all sequences, as the sooner the first door opens, the better the expected time to finish. For the second, since for all i, π′i ≥ πi, then x ≥ π′i implies that x ≥ πi, and so:\ni′(x) = max i\n{ x ≥ π′i−1 } ≤ max\ni {x ≥ πi−1} = i(x)\nFor the third, denote by Yj the event that door 2 opens at the j’th 2-knock. Then:\nE [π |Xi] = ∞∑ j=i (πj + cj)Pr [Yj |Xi]\nLet us consider this same expression as it occurs in π′. First note that Pr [Yj |Xi] = Pr [ Y ′j ∣∣X ′i], as all that matters for its evaluation is j − i. Therefore:\nE [π′ |X ′i] = ∞∑ j=i (π′j + cj)Pr [ Y ′j ∣∣X ′i] ≤ ∞∑ j=i (πj + 1 + cj)Pr [Yj |Xi]\n= E [π |Xi] + ∞∑ j=i Pr [Yj |Xi] ≤ E [π |Xi] + 1"
    }, {
      "heading" : "3.2 The Optimal Semi-Fractional Sequence",
      "text" : "A big advantage of the semi-fractional model is that we can find an optimal sequence for it. For that we need some preparation:\nDefinition 14. For a semi-fractional sequence π, and some 0 ≤ x ≤ 1, denote by Ex [π] the expected running time of π when started with door 1 being closed with probability x. In this notation, E [π] = E1 [π].\nLemma 15. Let y = x/(q2 + p2x). Then:\nEx [ 1t · π ] = t+ Eqt1x [π] Ex [2 · π] = c+ x\ny Ey [π]\nProof. The first equation is clear, since starting with door 1 being closed with probability x, and then knocking on it for t rounds, the probability that this door is closed is qt1x.\nAs for the second equation, if door 1 is closed with probability x, then knocking on door 2, we have a probability of p2(1− x) of terminating, and so the probability we did not finish is:\n1− p2(1− x) = 1− p2 + p2x = q2 + p2x = x\ny\nIt remains to show that conditioning on the fact that we indeed continue, the probability that door 1 is closed is y. It is the following expression, evaluated after a 2-knock:\nPr [door 1 is closed]\nPr [door 1 is closed] + Pr [door 1 is open but not door 2] =\nx\nx+ (1− x)q2 = y\nApplying Lemma 15 iteratively on a finite sequence w, we get:\nEx [wπ] = a(x,w) + b(x,w)Eδ(x,w) [π] (2)\nOf specific interest is δ(x,w). It can be thought of as the state3 of our algorithm after running the sequence w, when we started at state x. Lemma 15 and Equation (2) give us the behaviour of δ(x,w):\nδ(x, 1t) = qt1x δ(x, 2) = x\nq2 + p2x δ(x, aw) = δ(δ(x, a), w)\nWe start with the state being 1, since we want to calculate E1 [π]. Except for this first moment, as we can safely assume any reasonable algorithm will start with a 1-knock, the state will always be in the interval (0, 1). A 1-knock will always decrease the state and a 2-knock will increase it.\nOur point in all this, is that we wish to exploit the fact that our doors are memoryless, and if we encounter a state we’ve already been at during the running of the sequence, then we should probably make the same choice now as we did then. The following definition and lemma capture this point.\nDefinition 16. We say a non-empty finite sequence w is x-invariant, if δ(x,w) = x.\nThe following Lemma is proved in Appendix D.2, and formalizes our intuition about how an optimal algorithm should behave.\nLemma 17. If w is x-invariant, and Ex [wπ] ≤ Ex [π] then Ex [w∞] ≤ Ex [wπ]."
    }, {
      "heading" : "3.2.1 The Actual Semi-Fractional Sequence",
      "text" : "Theorem 18. There is an optimal semi-fractional sequence π? of the form 1s(21t)∞, for some positive real values s and t, and its running time is:\nE [π?] = min z∈[0,1]\n( logq1(1− z) +\nc+ (1− p2z) logq1(1− p2z) p2z ) Proof. Claim 26 of Appendix D.1 says that there is an optimal semi-fractional sequence π. It clearly starts with a non-zero 1-knock, and so we can write π = 1s2π′. Intuitively, in terms of its state, this sequence starts at 1, goes down for some time with a 1-knock, and then jumps back up with a 2-knock.\nThe state it reaches now was already passed through on the first 1-knock, and so as this is an optimal sequence we can assume it will choose the same as it did before, and keep zig-zaging up and down.\nWe next prove that indeed there is an optimal sequence following the zig-zaging form above. Again, take some optimal π, and write π = 1s2π′. Denote x = δ(1, 1s) and y = δ(1, 1s2) = δ(x, 2) > x (see Figure 1). Taking r = logq1(y) < s, we get δ(1, 1\nr) = y. Denoting t = s − r, this means that 1t2 is y-invariant. Since π is optimal, then:\nE [π] = E [ 1r(1t2)π′ ] ≤ E [1rπ′] which implies: Ey [ 1t2π′ ] ≤ Ey [π′]\nSo by Lemma 17: Ey [ (1t2)∞ ] ≤ Ey [ 1t2π′ ] which implies: E [ 1r(1t2)∞ ] ≤ E [ 1r1t2π′ ] = E [π]\nTherefore, 1r(1t2)∞ = 1s(21t)∞ is optimal. We denote this sequence π?. Now for the analysis of the running time of this optimal sequence. We will use Lemma 15 many times in what follows. E1 [ 1s(21t)∞ ] = s+ Ex [ (21t)∞\n] Denote α = (21t)∞.\nEx [α] = Ex [ 21tα ] = c+ x y Ey [ 1tα ] = c+ x y (t+ Ex [α])\nSince t = s− r = logq1(x/y):\nEx [α] = c\n1− xy +\nx y\n1− xy logq1(x/y)\nBy Lemma 15, as our y is the state resulting from a 2-knock starting at state x, it follows that y = x/(q2 + p2x). Since x/y = q2 + p2x, then 1− x/y = p2(1− x) and then we get:\nc\np2(1− x) +\nq2 + p2x p2(1− x) logq1(q2 + p2x)\nAnd in total:\nE1 [ 1s(21t)∞ ] = logq1(x) + c+ (q2 + p2x) logq1(q2 + p2x)\np2(1− x) Changing variable to z = 1− x, results in q2 + p2x = 1− p2z, and we get the expression in the statement of the theorem.\n3There is an intuitive meaning behind this. Going through Lemma 15, we can see that δ(1, w) is actually the probability that after running w, door 1 is closed conditioned on door 2 being closed. Indeed, After running some finite sequence, the only feedback we have is that the algorithm did not finish yet. We can therefore calculate from our previous moves what is the probability that door 1 is closed, and that is the only information we need for our next steps."
    }, {
      "heading" : "3.3 Actual Numbers",
      "text" : "Theorem 18 gives the optimal semi-fractional sequence and a formula to calculate its expected running time. This formula can be approximated as accurately as we wish for any specific values of p1, p2 and c, but it is difficult to obtain a closed form formula from it. Lemma 27 in Appendix D.3 gives us a pretty good result when p1 ≈ p2, especially when they are small, as by Observation 25, we get log(1/(1− p1)) ≈ p1, and so the additive mistake in the formula is something like 1.\nIn general, when p1 is small, then θ (see Lemma 27) is approximately cp1/p2, which is the expected time to open door 2 on its own, divided by the time to open door 1 on its own - a natural measure of the system. Then, ignoring the additive mistake, we get that the lower bound is approximately F(θ)/p1, where F is some function not depending on the parameters of the system. For example F(1) = 3.58. So opening two similar doors without feedback when p is small takes about 3.58 times more time than opening one door as opposed to the case with feedback, where the factor is only 2.\nWe also note, that when the two doors are independent and similar, it is quite easy to see that the optimal expected running time is at most 3/p (see Claim 28 in Appendix D.4). As a last interesting point, in Appendix D.5 we show that if c = 1 and p = p1 = p2 approaches zero, then the ratio between the number of 2-knocks and the number of 1-knocks approaches 12 (1 + √ 5), which is the golden ratio."
    }, {
      "heading" : "3.4 Examples",
      "text" : "For p1 = p2 = 1/2 and c = 1, the lower bound is 5.747. Simulations show that the best algorithm for this case is slightly more than 5.8, so the lower bound is quite tight, but our upper bound is 6.747 which is pretty far. However, the sequence we get from the upper bound proof starts with:\n1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, . . .\nThe value it gives is about 5.832, which is very close to optimal. For p1 = p2 = 1/100 and c = 1, the sequence we get is:\n197, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, . . .\nAnd the value it gives is about 356.756, while the lower bound can be calculated to be approximately 356.754. As we see this is much tighter than the +1 that our upper bound promises."
    }, {
      "heading" : "A “Real Life” Example",
      "text" : "The following scenario, much simplified, is the focus of [4]. A treasure τ is placed at a leaf of a ∆-regular rooted tree of depth d. A mobile agent starting at the root wishes to find it as fast as possible and is allowed to move along edges. At each node there is an advice pointer to one of its neighbours. With probability p the advice is correct, i.e., directed towards the treasure, and with probability q = 1− p it is incorrect, i.e., points towards the one of the neighbours uniformly at random. The agent can move between two neighbouring nodes in one unit of time and look at the advice at the current node hosting it. Minimizing the expected running time until finding the treasure turns out to be not trivial, and the crux of the problem is that the advice is permanent, and so cannot be amplified by rechecking it. It is shown in [4] that if q & 1/ √ ∆, then no algorithm has running time which is polynomial in d. Modeling this problem as cascading doors gives a non-trivial solution for the cases where q is smaller. Consider door i as open once the algorithm visits τi, the ancestor of the treasure that is at distance i from the root. The purpose is then to open all doors. At any point in the algorithm, the candidates at level i are those unvisited vertices at that level, whose parent is already visited. Also denote the score of a vertex as the number of advice pointers that point towards it in the advice seen so far by the agent. A knock on door i consists of visiting the highest scoring candidate on level i, where symmetry is broken arbitrarily.\nThe difference in score between two candidates at the same level is affected by the advice on the path between them only, and as the algorithm moves on edges, all of the advice on this path is known. Consider a candidate at level i that is at distance l from τi. It will have a score that is at least as high as the treasure if the number of advice pointers on the path connecting them that point towards it is greater than the number of those pointing towards τi. The probability that this happens can be viewed as the probability that a random walk of length l sums up to at least 0, where each step is (−1, 0, 1) with respective probabilities (p+ q/∆, (1− 2/∆)q, q/∆). Denote this probability by α(l). It is shown in [4] that α(l) ≈ ql for q < 1/ √ ∆.\nDenote by Ci the number of such candidates at level i that “beat” τi. Even assuming all of the vertices at that level are now reachable:\nE [Ci] ≤ i∑\nj=1\n∆jα(2j − 1) ≈ 1 q i∑ j=1 (∆q2)j = O( √ ∆)\nThis is in fact an upper bound on the expected number of knocks until opening door i, assuming door i− 1 is already open. By Example 11, Asimp will need an expected O(d2 √ ∆) knocks to open all doors and find the treasure. As moving from one candidate to another takes O(d) moves, the running time of this algorithm is at most O(d3 √ ∆). If we were able to prove that E [ C2i ]\nis small, then by Example 11 we could have dropped d’s exponent to 2.5, but it turns out that this second moment is actually exponential in d and so this approach fails.\nOf course, assuming there is no feedback at all in this situation is an over approximation, and while it gives a non-trivial result, using much more sophisticated arguments, it is shown in [4] that there is an O( √ ∆d) algorithm, and that it is in fact optimal."
    }, {
      "heading" : "B General Dependencies",
      "text" : "In the main text of the paper we focus on two special cases, that of independent doors and that of cascading doors. In what follows we introduce the possibility of much more general dependencies, and show that in fact, the two cases above are the extreme ones and so proving their equivalence is enough to prove it for all cases. For that we need to revisit our basic definition of doors and knocks.\nAcyclic dependencies. We assume that the directed graph of dependencies between doors is acyclic. In such cases, the doors can be ordered in a topological order such that a door may depend only on lower index doors. In what follows, w.l.o.g., we shall always assume that doors are ordered in such an order.\nConfiguration. A configuration C for d doors indexed 1, . . . , d describes the probabilities of each door opening as a result of knocks on it. It relates a door i with the function:\nφCi : {(X1, . . . , Xn) |n ≥ 1,∀j.Xj ⊆ {1, . . . , i− 1}} → [0, 1]\nWhich given, for a sequence of n knocks on door i, the set of doors Xj that are open at the time of each of those knocks, returns the probability that door i was opened by one of these knocks. We will omit the superscript C when it is clear from context.\nMonotonicity. The more we knock on a door the better our chances of opening it. More precisely, the monotonicity property requires that if X = (X1, . . . , Xn), and X\n′ is a sub-sequence of X (possibly a non-consecutive one), then φi(X ′) ≤ φi(X).\nPositive Correlation. We focus on the case where the doors are positively correlated, namely, a door being open can never decrease the chances of other doors to open. Formally this means that for every i, if for all j, X ′j ⊆ Xj , then φi(X ′1, . . . , X ′n) ≤ φi(X1, . . . , Xn).\nFundamental Distribution. The fundamental distribution4 of door i in configuration C is the function pCi (again, we will omit the superscript) where pi(n) denotes the probability, in the best of conditions, i.e., when all doors it depends on are open, that door i remains closed after being knocked on n times. Formally, pi(n) = 1−φi({1, . . . , i− 1}n). So pi(0) = 1 for every door i, and by the monotonicity property pi is non-increasing. We also denote by Ei = ∑∞ n=0 pi(n) the expected time to open door i assuming all the doors it depends on are already open. We will always assume that for all i, Ei <∞.\nSimilarity. Two doors are similar if they have the same fundamental distribution. Two configurations are similar if for every i, door i of the first configuration is similar to door i of the second."
    }, {
      "heading" : "B.1 The Cascading and Independent Configurations",
      "text" : "In light of the definitions above we define the two main configurations:\n1. Independent doors. The distribution associated with a door is independent of whether or not other doors are open. Formally, φi(X1, . . . , Xk) = φi({1, . . . , i− 1}k).\n2. Cascading doors. Door i > 1 cannot open unless door i− 1 is already open. Only after door i− 1 opens we start counting knocks on door i. Formally, φi(X1, . . . , Xk) = φi({1, . . . , i− 1}t) where t is the number of Xj ’s that are equal to {1, . . . , i− 1}.\nDefinition 19. For configurations A and B, we say that A dominates B, if for every i and every X = (X1, . . . , Xn), we have: φ A i (X) ≥ φBi (X).\nFirst:\nClaim 20. For configuration C, similar independent configuration I, and similar cascading configuration X , I dominates C and C dominates X .\nProof. Denote n = |X|, and denote by k the number of elements of X that are equal to {1, . . . , i− 1}. We get the following series of inequalities:\nφXi (X) = 1− pi(k) = φCi ({1, . . . , i− 1} k )\n≤ φCi (X) ≤ φCi ({1, . . . , i− 1} n ) = 1− pi(n) = φIi (X)\nWhere we used, in order: the definition of cascading configuration, the fact that pi is the fundamental distribution of door i, monotonicity, positive correlation, the fact that p is the fundamental distribution of door i, and the definition of independent configuration.\nAn important property of dominance is:\nClaim 21. For any sequence π, if A dominates B then TA(π) ≤ TB(π).\nProof. A possible way to describe the random process governing the running of an algorithm in a particular configuration, is as follows:\n1. For each door i, choose uniformly at random a real number ai ∈ [0, 1]. Fix this number for the rest of the run.\n4This is actually not a distribution function, but rather the complement of an accumulative distribution function.\n2. Denote by X the history of open doors as usual. Start it as the empty sequence.\n3. Go over the knocks in the sequence in order, and when the knock is on door i, check if φi(X ′) > ai,\nwhere X ′ is part of X that is relevant to calculate φi (only the indices where there is a knock of door i, and only the information about the doors of {1, . . . , i− 1}). If it is then consider door i as open from this point on, and start marking it as such in X.\nThis way of describing the run is a little bizarre, but is in fact very natural, as our doors are described by an accumulative distribution function.\nFor two histories X = (X1, . . . Xn) and X ′ = (X ′1, . . . X ′ n), we write X ′ X if for all j, X ′j ⊆ Xj . We note that Definition 19 combined with positive correlation, gives us that if X ′ X then φA(X) ≤ φB(X ′).\nThis fact together with a simple argument finishes the proof: Use the same random coins to run the sequence π in both A and B. By induction and the fact above, the histories at any point in time satisfy XB XA, and so the run on A will always be at least as fast as the run on B. Since this is true no matter what ai’s we got, it is true in expectation.\nTogether these two claims prove the lemma we need for the paper:\nLemma 1. Consider similar configurations C,X and I, where X is cascading and I is independent. For every sequence π, TI(π) ≤ TC(π) ≤ TX (π). This also implies that TI ≤ TC ≤ TX ."
    }, {
      "heading" : "B.2 A Simple Upper Bound on the Price of Lacking Feedback",
      "text" : "Claim 22. For every configuration C, Price(C) ≤ d. Proof. Denote by X the cascading configuration that is similar to C. Denote π = (1, 2, . . . , d)∞. Using Lemma 1, TC ≤ TC(π) ≤ TX (π) The behaviour of door i in the cascading case can be described in a simple manner: It doesn’t open until all lower index doors are open, and from that time is behaves according to pi. Hence, the expected number of knocks on door i until it opens when starting the count after all doors j < i are open, is precisely Ei.\nIn sequence π, it takes dEi to guarantee that door i was knocked upon Ei times. Therefore, by linearity of expectation, it follows that the expected time until we open all doors in X is at most d ∑d i=1Ei.\nDividing by ∑d i=1Ei, we get the result."
    }, {
      "heading" : "B.3 Existence of an Optimal Sequence",
      "text" : "Claim 23. For any configuration C there is some sequence π such that for every π′, TC(π) ≤ TC(π′). Proof. Assume there is no optimal sequence. Recall we assume that the fundamental distribution of each door allows it to be opened in finite expected time. It is then easy to see that the sequence (1, 2, . . . d)∞ will open all doors in finite time no matter what the configuration is as long as it is acyclic. Therefore, I = infπ TC(π) exists.\nTake a sequence of sequences π(1), π(2), . . . where limn→∞ TC(π(n)) = I. W.l.o.g., we can assume that π(n+1) agrees with π(n) on all the first n places. How so? there is at least one door number that appears as the first knock in infinitely many of the sequences. Take one such number, and erase all sequences that don’t have it as a first knock. Of the remaining sequences, take the first one, fix it as α(1), and erase it. Starting with the sequence of sequences that remains, find a number that appears infinitely often in the second place. Erase all sequences not having it as the second knock, and then fix α(2) as the first of the remaining sequences, erase it and continue thus. We get that the α(i)’s are a sub-sequence of the π(i)’s, and satisfy the assumption.\nDefine πi = limn→∞ π (n) i . It is clearly defined for such a sequence of sequences. This is our π. Now:\nTC(π) = lim n→∞ n∑ i=0 Pr [π not finished by time i]\n= lim n→∞ n∑ i=0 Pr [ π(n) not finished by time i ] ≤ lim n→∞ TC(π(n)) = I\nWhere the second equality is because π = π(n) in the first n places."
    }, {
      "heading" : "C Proofs Related to Section 2",
      "text" : "C.1 Asimp is Optimal up to a Constant Factor for Identical Independent Doors\nClaim 5. If all doors are similar then TI(Asimp) = Θ(TI)\nProof. By Claim 23, there is some fixed sequence π such that TI(π) = TI . Denote by πi(t) the number of times door i has been knocked on by time t in π. Clearly ∑ i πi(t) = t.\nTI = TI(π) = ∞∑ t=0 Pr [some door is closed at time t]\n= ∞∑ t=0 1− Pr [all doors are open at time t] = ∞∑ t=0 1− ( d∏ i=1 (1− p(πi(t))) ) By time t, the number of doors that have been tried more than 2t/d is less than d/2. So at least half the doors have been tried at most t′ = b2t/dc times. Therefore, each such door i satisfies p(πi(t)) ≥ p(t′). We then have: ∏d i=1(1− p(πi(t))) ≤ (1− p(t′)) d 2 . So TI is at least:\n∞∑ t=0 1− (1− p (b2t/dc)) d 2\nIn general, for any x ≤ 1, as t traverses all integers from 0 to infinity, btxc takes every natural value at least b1/xc times. In our case we get:\nTI ≥ ⌊ d\n2 ⌋ · ∞∑ t=0 1− (1− p(t)) d2 (3)\nWe now turn to analyse the expected running time ofAsimp. By Claim 4, TI(Asimp) = O(d·E [max (X1, . . . , Xd)]), where Xi is the number of knocks on door i until it opens. Now:\nE [max (X1, . . . , Xd)] = ∞∑ t=0 1− Pr [Xi ≤ t]d = ∞∑ t=0 1− (1− p(t))d\nDenote x(t) = (1− p(t))d/2, and then the sum ∑∞ t=0(1− (1− p(t))d) becomes:\n∞∑ t=0 1− x(t)2 = ∞∑ t=0 (1− x(t))(1 + x(t)) ≤ 2 ∞∑ t=0 1− x(t) = 2 ∞∑ t=0 1− (1− p(t))d/2\nApplying this, and then using Equation (3) we get:\nTI(Asimp) = O ( d\n∞∑ t=0\n1− (1− p(t))d/2 ) = O(TI)"
    }, {
      "heading" : "C.2 A Simple Algorithm for General Configurations Where all Doors are Similar",
      "text" : "Claim 6. Denote by αn the sequence 1 2n , . . . , d2 n\n. If all doors are similar then for any configuration C, TC (α0 · α1 · α2 · · ·) = Θ(TC).\nProof. Denote π = α0 · α1 · · · , and note that |αn| = 2nd. By Lemma 1 we need only consider TX (π). Taking an = d+ 2d+ . . .+ 2\nnd, and using the right side of Observation 3 (where indices are shifted to account for the fact that a0 is the first element and not a1):\nTX (π) ≤ d+ ∞∑ n=0 2n+1d · SCX (π[d+ 2d+ . . .+ 2nd)]) ≤ d+ 2 ∞∑ n=0 2nd · SCX (αn)\n= d+ 4 ∞∑ n=0 2n−1d · SCI(Asimp[2nd]) ≤ d+ 4TI(Asimp)\nWhere for the last step we used the left side of Observation 3, taking an = 2 nd. Seeing as all doors start closed, TI(Asimp) ≥ d, and we get that:\nTC(π) = O(TI(Asimp)) = O(TI) = O(TC)\nWhere for the last two steps, we used Claim 5, and then Theorem 1."
    }, {
      "heading" : "C.3 Expected Maximum of iid Random Variables",
      "text" : "Lemma 7. If X1, . . . , Xd are i.i.d. random variables taking natural number values, then:\nE [max(X1, . . . , Xd)] = Θ\n( κ+ d\n∞∑ n=κ Pr [Xi > n]\n)\nWhere κ = min {n ∈ N | Pr [X1 > n] < 1/d}\nProof. Denote p(n) = Pr [Xi > n]. The expectation we are interested in is:\n∞∑ t=0 1− Pr [X ≤ t]d = ∞∑ t=0 1− (1− p(t))d = κ−1∑ t=0 1− (1− p(t))d + ∞∑ t=κ 1− (1− p(t))d (4)\nThe first term is at least: κ−1∑ t=0 1− ( 1− 1 d )d ≥ κ ( 1− 1 e ) and at most κ, and so is Θ(κ). For the second term, examine (1− a)d when a ≤ 1/d. We use 1 + x ≤ ex and Observation 24 (see below):\n(1− a)d ≤ e−ad ≤ 1− 1 2 ad\nHence the second term of (4) is Ω(d ∑∞ t=κ p(t)). On the other hand, by the same observation:\n(1− a)d ≥ e−2ad ≥ 1− 2ad\nAnd so the second term of (4) is O(d ∑∞ t=κ p(t)).\nObservation 24. Every 0 ≤ x ≤ 1 satisfies e−x ≤ 1− 12x.\nProof. Define:\nf(x) = 1− 1 2 x− e−x\nWhenever f is positive the required inequality is satisfied. We note that f(0) = 0 and f(1) = 1− 12− 1 e > 0. Now,\nf ′(x) = −1 2 + e−x\nIt is positive for x < ln(2) < 1, zero at ln(2), and negative for larger values. So f starts as 0 at 0, climbs up to reach its maximum at ln(2) and then decreases. Since f(1) > 0, it must be the case that for all 0 ≤ x ≤ 1, f is positive, which proves the lemma."
    }, {
      "heading" : "C.4 Proofs for the Examples of Subsection 2.1.2",
      "text" : "Example 9. If p(n) = qn for some 1/2 < q < 1, then Price = Θ(log(d)).\nProof. In this case, κ = dlog1/q(d)e, and E [Xi] = 1/(1− q), so by (1):\nPrice = Θ ( (1− q) ⌈ log1/q(d) ⌉ + d(1− q) ∞∑ i=κ qi )\nThe second term inside the brackets is equal to dqκ ≤ 1. The first term is at least:\n1− q ln(1/q) ln(d) ≥ q ln(d) ≥ 1 2 ln(d)\nWhere we used Observation 25 below. On the other hand, it is at most: (1− q) ⌈ ln(d)\n1− q\n⌉ ≤ (1− q) ( 1 + ln(d)\n1− q\n) ≤ 1 + ln(d) ≤ 3 ln(d)\nSince d ≥ 2 and so 2 ln(d) > 1. So we get the result.\nObservation 25. For 0 < q < 1,\n1− q ≤ ln ( 1\nq\n) ≤ 1− q\nq\nProof. The following is true for all x > −1:\nx\n1 + x ≤ ln(1 + x) ≤ x\nSo for 0 < x < 1: −x\n1− x ≤ ln(1− x) ≤ −x\nWhich is:\nx ≤ ln ( 1\n1− x\n) ≤ x\n1− x Taking x = 1− q we get the first result.\nExample 10. If for some c > 0 and a > 1, p(n) = min(1, c/na), then Price = Θ(d 1 a ).\nProof. In this case κ = ⌈ (dc)1/a ⌉ . In this proof we have many approximations (such as dropping the rounding above), and they all go into the constants.\nThe expected time to open just one door is (we assume c 1 a is an integer, again this will only cost a\nconstant factor):\n∞∑ n=0 p(n) = c 1 a + ∞∑ n=c1/a c na ≈ c 1a + c ∫ ∞ c1/a 1 xa dx ≈ c 1a + c (a− 1)c1− 1a = c 1 a ( 1 + 1 a− 1 ) On the other hand, in the terminology of Equation (1):\nE [max {X1, . . . , Xd}] = Θ ( (dc) 1 a + d\n∞∑ i=κ p(i)\n)\nWe approximate the sum in second term in the brackets by an integral:\nd ∞∑ i=κ c ia ≈ d ∫ ∞ κ c xa dx =\ndc (a− 1)κa−1 ≈ dc (a− 1)(dc)1− 1a = d\n1 a c 1 a a− 1\nSo the expectation of the maximum is:\nd 1 a c 1 a ( 1 + 1\na− 1 ) And we get the result.\nExample 11. If E [Xa1 ] < M for some a > 1, then T = O ( d1+ 1 aM1/a(1 + 1a−1 ) ) .\nProof.\np(n) = Pr [X1 > n] = Pr [X a 1 > n\na] < E [Xa1 ] na ≤ M na\nSo the current configuration dominates the independent door configuration where each door has fundamental distribution q(n) = M/na, and so by Claim 21 has algorithms with running time at least as good. Following the proof of Example 10, there is such an algorithm with running timeO ( d1+ 1 aM 1 a ( 1 + 1a−1 )) ."
    }, {
      "heading" : "D Proofs Related to Section 3",
      "text" : ""
    }, {
      "heading" : "D.1 The Existence of an Optimal Semi-Fractional Sequence",
      "text" : "Claim 26. There is an optimal semi-fractional sequence π. That is, for every semi-fractional sequence π′, E [π] ≤ E [π′]\nProof. Assume there is not. But clearly, I = infπ(E [π]) exists. Take a series π 1, π2, . . . where limn→∞ E [π n] = I.\nWe think of a sequence as its sequence of 1-knock lengths. That is, πni is the length of the i-th 1-knock in πn. We first show that we can assume that for every i, the set {πni |n ≥ 1} is bounded.\nFor this purpose, we first note that if for some semi-fractional sequence α, E [α] < M , then for every i, αi < Mq i−1 2 . That is because with probability at least q i−1 2 the algorithm will actually run the i-th 1-knock, and if it’s longer than stated, then E [α] ≥M , in contradiction. Since we can assume that for all n, E [πn] < 2I, then by the observation above, we get the boundedness property we were aiming for.\nNow, we claim that we can assume that for every i, πni converges as n goes to infinity. For this, start by taking a sub-series of the πn where πn1 converges (it exists, because these values are bounded, as we said). Erase all other πn. Take the first element of this series and put aside as the new first element. From the rest, take a sub-series where πn2 converges, and erase all others. Take the new first element, and put it aside as the new second element. Continuing this, we get an infinite series as required.\nDefine πi = limn→∞ π n i . We claim that π is optimal.\nE [π] = ∞∑ i=1  i∑ j=1 (πj + c)  Pr [π finishes at 2-knock i] Denoting by Xi the event that π finishes at or after 2-knock i, this is equal to:\n∞∑ i=1 (πi + c)Pr [Xi] = lim k→∞ k∑ i=1 (πi + c)Pr [Xi]\nFix some k. And denote by Xni the event that π n finishes at or after 2-knock i. Since Pr [Xi] is a continuous function of π1, . . . , πi, we get:\nk∑ i=1 (πi + c)Pr [Xi] = lim n→∞ k∑ i=1 (πni + c)Pr [X n i ] ≤ lim n→∞ TI(πn) = I\nSo E [π] ≤ I and we conclude."
    }, {
      "heading" : "D.2 Memoryless Doors Imply Memoryless Algorithms",
      "text" : "Lemma 17. If w is x-invariant, and Ex [wπ] ≤ Ex [π] then Ex [w∞] ≤ Ex [wπ].\nProof. As in (2), for any sequence α:\nEx [wα] = a+ bEx [α]\nWhere a and b are functions of x and w. Since w is not empty, and as 1-knocks decrease the state and 2-knocks increase it, there must be at least one 2-knock in w, and thus b < 1. So:\nEx [wπ] = a+ bEx [π] ≥ a+ bEx [wπ] =⇒ Ex [wπ] ≥ a\n1− b\nOn the other hand:\nEx [w ∞] = Ex [ww ∞] = a+ bEx [w ∞] =⇒ Ex [w∞] =\na\n1− b\nAnd we conclude."
    }, {
      "heading" : "D.3 Approximating the Optimal Semi-Fractional Running Time",
      "text" : "Theorem 18 gives a way to calculate the expectation of the best semi-fractional sequence π? for our configuration. Unfortunately, we were not able to obtain a close formula for this value. The following lemma can be used to approximate it. Lemma 27. Denoting θ = −c log(q1)/p2, and ψ = 12 ( √ θ2 + 4θ − θ), we have:\nE [π?] ∈ 1 log(1/q1)\n( log ( 1\n1− ψ\n) + θ\nψ + 1\n) − [ 0,\np2 log(1/q1) ] Proof. Recall the result of Theorem 18:\nE [π?] = min z∈[0,1]\n( logq1(1− z) +\nc+ (1− p2z) logq1(1− p2z) p2z ) By the definition of θ in the statement of the lemma, and denoting:\nY = − (1− p2z) log(1− p2z) p2z\nWe get:\nE [π?] = 1\nlog(q1) min z∈[0,1]\n( log(1− z)− θ\nz − Y ) = 1\nlog(1/q1) min z∈[0,1]\n( log ( 1\n1− z\n) + θ\nz + Y ) Next, since for x > −1,\nx\n1 + x ≤ log(1 + x) ≤ x\nThen for 0 < x < 1: −x ≤ log(1− x) ≤ − x 1− x Multiplying by −(1− x)/x (a positive number):\n1− x ≤ − (1− x) log(1− x) x ≤ 1\nTherefore, Y ∈ [1− p2z, 1] ⊆ [1− p2, 1]. It follows that:\nE [π?] ∈ 1 log(1/q1) ( min z∈[0,1] ( log ( 1 1− z ) + θ z ) + [1− p2, 1] ) (5)\nFor the minimization, we take the derivative and compare to 0\n1 1− z − θ z2 = 0 =⇒ z 2 θ + z − 1 = 0\n=⇒ z = √\n1 + 4/θ − 1 2/θ =\n√ θ2 + 4θ − θ\n2 = ψ\nWhere we took the root that is in [0, 1]. Assigning back in (5),\nE [π?] ∈ 1 log(1/q1)\n( log ( 1\n1− ψ\n) + θ\nψ + 1\n) − [ 0,\np2 log(1/q1)\n]"
    }, {
      "heading" : "D.4 Similar Independent Memoryless Doors",
      "text" : "The following simple claim implies that the expected time to open two similar memoryless doors is at most 3 times the expected time to open one of them. A generalization to d doors can easily be established based on the same idea.\nClaim 28. Consider the configuration I of two similar doors that open on each knock independently with probability p. Then TI(Asimp) = 3p − 1.\nProof. Until the first door opens (either door 1 or door 2), each knock has probability p to open. Therefore, the first door opens in expected time 1/p. From that time, every odd knock will be on the other door, and will succeed with probability p. So the expected time to open the second door after the first one has opened is 2/p− 1, and altogether we have expected time 3/p− 1."
    }, {
      "heading" : "D.5 The Golden Ratio",
      "text" : "Returning to the case where c = 1, and p1 = p2 are very small. As we said, θ of Lemma 27 tends to 1, and so ψ there tends to ( √ 5− 1)/2. This ψ is actually the value of z that minimizes the expression of Theorem 18. Looking in the proof of the theorem, the length of 1-knocks (except the first), is\nt = logq(x/y) = logq(q + px) = logq(1− pz) = log(1− pz) log(1− p)\nFor small x, log(1 + x) ≈ x and so, as p goes to zero, the above ratio tends to z, and in our case to ψ. So the length of 1-knocks is ψ, and that of the 2-knocks is 1. In the long run the length of the first 1-knock is insignificant, and the transformation of Theorem 12 will make the ratio of between the number of 2-knocks and the number of 1-knocks approach 1/ψ, which is the golden ratio."
    } ],
    "references" : [ {
      "title" : "On the complexity of trial and error",
      "author" : [ "Xiaohui Bei", "Ning Chen", "Shengyu Zhang" ],
      "venue" : "In Symposium on Theory of Computing Conference,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Regret in decision making under uncertainty",
      "author" : [ "David E. Bell" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1982
    }, {
      "title" : "The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well)",
      "author" : [ "Michael Ben-Or", "Avinatan Hassidim" ],
      "venue" : "Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Searching on trees with noisy memory",
      "author" : [ "Lucas Boczkowski", "Amos Korman", "Yoav Rodeh" ],
      "venue" : "CoRR, abs/1611.01403,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Decision making with and without feedback: The role of intelligence, strategies, executive functions, and cognitive styles",
      "author" : [ "Matthias Brand", "Christian Laier", "Mirko Pawlikowski", "Hans J. Markowitsch" ],
      "venue" : "Journal of Clinical and Experimental Neuropsychology,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Deterministic and probabilistic binary search in graphs",
      "author" : [ "Ehsan Emamjomeh-Zadeh", "David Kempe", "Vikrant Singhal" ],
      "venue" : "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Computing with noisy information",
      "author" : [ "Uriel Feige", "Prabhakar Raghavan", "David Peleg", "Eli Upfal" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1994
    }, {
      "title" : "Social Foraging Theory. Monographs in behavior and ecology",
      "author" : [ "L.A. Giraldeau", "T. Caraco" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Noisy binary search and its applications",
      "author" : [ "Richard M. Karp", "Robert Kleinberg" ],
      "venue" : "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "The multi-armed bandit problem: Decomposition and computation",
      "author" : [ "Michael N. Katehakis", "Arthur F. Veinott", "Jr." ],
      "venue" : "Math. Oper. Res.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1987
    }, {
      "title" : "An Introduction to Computational Learning Theory",
      "author" : [ "Michael J. Kearns", "Umesh V. Vazirani" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1994
    }, {
      "title" : "Toward more localized local algorithms: removing assumptions concerning global knowledge",
      "author" : [ "Amos Korman", "Jean-Sébastien Sereni", "Laurent Viennot" ],
      "venue" : "Distributed Computing,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "A simple parallel algorithm for the maximal independent set problem",
      "author" : [ "Michael Luby" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1986
    }, {
      "title" : "Machine Learning. McGraw-Hill, Inc., New York, NY, USA, 1 edition",
      "author" : [ "Thomas M. Mitchell" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "Design and Analysis of Experiments",
      "author" : [ "Douglas C. Montgomery" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Searching games with errors - fifty years of coping with liars",
      "author" : [ "Andrzej Pelc" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "This research has actually originated from our research on heuristic search on trees [4].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "exploitation problems, where it is typically the case that deficient performances on the exploration part may result in much waste on the exploitation part [10, 17].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "exploitation problems, where it is typically the case that deficient performances on the exploration part may result in much waste on the exploitation part [10, 17].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "It can also be seen as the question of balance between searching and verifying in algorithms that can be partitioned thus [1, 15].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "It can also be seen as the question of balance between searching and verifying in algorithms that can be partitioned thus [1, 15].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : ", [2, 3, 5, 6, 7, 8, 9, 16].",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : ", [2, 3, 5, 6, 7, 8, 9, 16].",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : ", [2, 3, 5, 6, 7, 8, 9, 16].",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : ", [2, 3, 5, 6, 7, 8, 9, 16].",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : ", [2, 3, 5, 6, 7, 8, 9, 16].",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : ", [2, 3, 5, 6, 7, 8, 9, 16].",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : ", [2, 3, 5, 6, 7, 8, 9, 16].",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : ", [2, 3, 5, 6, 7, 8, 9, 16].",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "Performing despite limited feedback would fit the framework of reinforced learning [17] and is inherent to the study of exploration vs.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "exploitation type of problems, including Multi-Armed Bandit problems [10].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Numerous strategies are based on this interplay, including heuristics based on brute force or trail and error approaches [1, 15], sample and predict approaches [11, 14, 17], iterative local algorithms [12, 13], and many others.",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "Numerous strategies are based on this interplay, including heuristics based on brute force or trail and error approaches [1, 15], sample and predict approaches [11, 14, 17], iterative local algorithms [12, 13], and many others.",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Numerous strategies are based on this interplay, including heuristics based on brute force or trail and error approaches [1, 15], sample and predict approaches [11, 14, 17], iterative local algorithms [12, 13], and many others.",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "Numerous strategies are based on this interplay, including heuristics based on brute force or trail and error approaches [1, 15], sample and predict approaches [11, 14, 17], iterative local algorithms [12, 13], and many others.",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "Numerous strategies are based on this interplay, including heuristics based on brute force or trail and error approaches [1, 15], sample and predict approaches [11, 14, 17], iterative local algorithms [12, 13], and many others.",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : "Numerous strategies are based on this interplay, including heuristics based on brute force or trail and error approaches [1, 15], sample and predict approaches [11, 14, 17], iterative local algorithms [12, 13], and many others.",
      "startOffset" : 201,
      "endOffset" : 209
    }, {
      "referenceID" : 12,
      "context" : "Numerous strategies are based on this interplay, including heuristics based on brute force or trail and error approaches [1, 15], sample and predict approaches [11, 14, 17], iterative local algorithms [12, 13], and many others.",
      "startOffset" : 201,
      "endOffset" : 209
    }, {
      "referenceID" : 0,
      "context" : "In this case, door i can be associated with a function pi : N→ [0, 1], where pi(n) is the probability that door i is not open after knocking on it n times.",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "In general, given a configuration, each door i defines a non-decreasing function pi : N→ [0, 1], called the fundamental distribution of the door, where pi(n) is the probability that the door is not open after knocking on it n times in the best of conditions, i.",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "n=1 2 · SCI(π[2]) ≤ 2 + 4TI(π)",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "E [π] = min z∈[0,1] ( logq1(1− z) + c+ (1− p2z) logq1(1− p2z) p2z )",
      "startOffset" : 14,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "The following scenario, much simplified, is the focus of [4].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "It is shown in [4] that if q & 1/ √ ∆, then no algorithm has running time which is polynomial in d.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "It is shown in [4] that α(l) ≈ q for q < 1/ √ ∆.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "Of course, assuming there is no feedback at all in this situation is an over approximation, and while it gives a non-trivial result, using much more sophisticated arguments, it is shown in [4] that there is an O( √ ∆d) algorithm, and that it is in fact optimal.",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : ", i− 1}} → [0, 1]",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "For each door i, choose uniformly at random a real number ai ∈ [0, 1].",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "E [π] = min z∈[0,1] ( logq1(1− z) + c+ (1− p2z) logq1(1− p2z) p2z )",
      "startOffset" : 14,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "By the definition of θ in the statement of the lemma, and denoting: Y = − (1− p2z) log(1− p2z) p2z We get: E [π] = 1 log(q1) min z∈[0,1] ( log(1− z)− θ z − Y )",
      "startOffset" : 131,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "= 1 log(1/q1) min z∈[0,1] ( log ( 1 1− z ) + θ z + Y )",
      "startOffset" : 20,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "It follows that: E [π] ∈ 1 log(1/q1) ( min z∈[0,1] ( log ( 1 1− z ) + θ z ) + [1− p2, 1] ) (5)",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "Where we took the root that is in [0, 1].",
      "startOffset" : 34,
      "endOffset" : 40
    } ],
    "year" : 2017,
    "abstractText" : "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of d doors that are initially closed, and the aim is to open all of them as fast as possible. To open a door, the algorithm knocks on it and it might open or not according to some probability distribution. This distribution may depend on which other doors are currently open, as well as on which other doors were open during each of the previous knocks on that door. The algorithm aims to minimize the expected time until all doors open. Crucially, it must act at any time without knowing whether or which other doors have already opened. In this work, we focus on scenarios where dependencies between doors are both positively correlated and acyclic. The fundamental distribution of a door describes the probability it opens in the best of conditions (with respect to other doors being open or closed). We show that if in two configurations of d doors corresponding doors share the same fundamental distribution, then these configurations have the same optimal running time up to a universal constant, no matter what are the dependencies between doors and what are the distributions. We also identify algorithms that are optimal up to a universal constant factor. For the case in which all doors share the same fundamental distribution we additionally provide a simpler algorithm, and a formula to calculate its running time. We furthermore analyse the price of lacking feedback for several configurations governed by standard fundamental distributions. In particular, we show that the price is logarithmic in d for memoryless doors, but can potentially grow to be linear in d for other distributions. We then turn our attention to investigate precise bounds. Even for the case of two doors, identifying the optimal sequence is an intriguing combinatorial question. Here, we study the case of two cascading memoryless doors. That is, the first door opens on each knock independently with probability p1. The second door can only open if the first door is open, in which case it will open on each knock independently with probability p2. We solve this problem almost completely by identifying algorithms that are optimal up to an additive term of 1.",
    "creator" : "LaTeX with hyperref package"
  }
}