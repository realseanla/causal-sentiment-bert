{
  "name" : "1511.04387.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode Resource-constrained Multi-project Scheduling Problem",
    "authors" : [ "Shahriar Asta", "Daniel Karapetyan", "Ahmed Kheiri", "Ender Özcan", "Andrew J. Parkes" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n04 38\n7v 2\n[ cs\n.D S]\n8 S\nMulti-mode resource and precedence-constrained project scheduling is a well-known challenging real-world optimisation problem. An important variant of the problem requires scheduling of activities for multiple projects considering availability of local and global resources while respecting a range of constraints. A critical aspect of the benchmarks addressed in this paper is that the primary objective is to minimise the sum of the project completion times, with the usual makespan minimisation as a secondary objective. We observe that this leads to an expected different overall structure of good solutions and discuss the effects this has on the algorithm design. This paper presents a carefully designed hybrid of Monte-Carlo tree search, novel neighbourhood moves, memetic algorithms, and hyper-heuristic methods. The implementation is also engineered to increase the speed with which iterations are performed, and to exploit the computing power of multicore machines. Empirical evaluation shows that the resulting information-sharing multi-component algorithm significantly outperforms other solvers on a set of “hidden” instances, i.e. instances not available at the algorithm design phase.\nKeywords: metaheuristics; hybrid heuristics; hyper-heuristics; Monte Carlo tree search; permutation based local search; multi-project scheduling"
    }, {
      "heading" : "1. Introduction",
      "text" : "Project scheduling has been of long-standing interest to academics as well as practitioners. Solving such a problem requires scheduling of interrelated activities (jobs), potentially each using or sharing scarce resources, subject to a set of constraints, and with one or several of a variety of objective functions. There are various project scheduling problems and many relevant surveys in the literature, e.g. see [4, 22, 21, 48, 39, 20, 65]. The best-known problem class is the Resource Constrained Project Scheduling Problem\n∗Corresponding author\nPreprint submitted to Elsevier September 9, 2016\n(RCPSP) in which activities have fixed usages of the resources, there are fixed precedence constraints between them, and often the objective is simple minimisation of the makespan (completion time of last activity). These problems have been proven to be NP-hard [2], and a well-known benchmark suite, PSPLIB, is provided in [29].\nA generalisation of the RCPSP is to also consider ‘Multi-mode RCPSP’ (MRCPSP) in which activities can be undertaken in one of a set of modes, with each mode potentially using different sets of resources. Furthermore, there are many options besides makespan for the objective function(s); a typical one is that a weighted sum of completion times is minimised. As common in optimisation problems, exact methods perform best on smaller instances and on larger instances heuristics and metaheuristics become necessary. Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].\nThis paper presents our winning approach submitted to MISTA 2013 challenge1 on a further extension called ‘multi-mode resource-constrained multi-project scheduling’ (MRCMPSP) and the results on the associated benchmark/competition instances. The full description of this problem domain can be found on the competition website and in [62]; however, for completeness we also summarise it in Section 2. The broad aim is to schedule a set of different and partially interacting projects, with each project consisting of a set of activities. There are no precedence constraints between the activities of different projects however they can compete for resources. Also, the objective function is extended to be a mix of a kind of weighted completion time and makespan. The MRCMPSP is hence interesting in that it has a mix of structures and requirements that are a step towards modelling the complexity of real-world scheduling problems. The high real-world relevance of the multi-project version of scheduling is well-known, e.g. a survey [35] found that “84% of the companies which responded to the survey indicated that they worked with multiple projects”. However, the majority of scheduling work is on the single project version, though there is some existing work on the multi-project case, e.g. see [35, 18, 32, 36].\nOur approach searches the space of sequences of activities, from which schedules are constructed and then the quality of each schedule is evaluated using the objective function. The search process on the set of sequences operates in two phases in a “construct and improve” fashion. In the first phase, a heuristic constructor creates initial sequences of activities. A novel proposal in this paper is to investigate the overall global structure of the solutions and use this to motivate constructing the initial sequences using a MonteCarlo Tree Search (MCTS) method, e.g. see [3]. This construction phase is followed by an improvement phase which makes use of a large and diverse set of heuristic neighbourhood moves. The search process during the improvement phase is carefully controlled by a combination of methods arising from a standard metaheuristic, namely memetic algorithm, and also an extension of existing hyper-heuristic components [27, 46].\nThere is an interesting potential for dual views of the overall problem. It is defined as a multi-project problem, but it can be also viewed as a single project (multi-mode) RCPSP, in which the precedence graph has a particular structure, consisting of disjoint clusters.\n1http://gent.cs.kuleuven.be/mista2013challenge/\nThere is a sense in which we work with both views together. Some neighbourhood moves treat the problem in a single-project fashion and work on the constituent activities; other neighbourhood operators explicitly consider the multi-project nature of the problem, and focus on moves of projects. Both views, and kinds of operators, are used and work together to improve the overall project-level structure as well as the detailed activity level structure. A discussion and a computational study on both approaches can be found in [35].\nThe primary contributions of this paper are:\n• Observation and investigation of how the primary objective function being essentially a “sum of project completion times” leads to good solutions having inherently different structure to those with makespan as the primary objective. In particular, minimisation of project completion times subject to limited global resource results in partial ordering of projects; this does somewhat reduce the effective size of the search space, but also may lead to good solutions being more widely separated. Understanding of this significantly affected our algorithm design, including an MCTS construction method aiming to create solutions having such structure.\n• Novel neighbourhood moves, including those that are designed specifically for smoother navigation through the search space of the multi-project extension of MRCPSP – reflecting our observation of the effect that the main objective function has on the solution structures.\n• An adaptive hybrid hyper-heuristic system to effectively control the usage of the rich set of neighbourhood moves.\n• Evidence of the effectiveness based on successful results on a range of benchmark problems. This includes winning a competition, in which some problems were hidden at the algorithm design/tuning phase. We also tested our algorithm on single-project instances from PSPLIB. Although our algorithm was not designed to work on single-project instances, it demonstrated good performance in these tests, and was competitive with the state-of-the-art methods tailored to the singleproject case. Furthermore, it improved 3 best solutions on these PSPLIB instances during these experiments.\nThese contributions are directed towards a system that is both robust and flexible; with the potential to be effective at handling a wide variety of problem requirements and instances. Arguably, one of the lessons of this paper is that greater complexity and richness of such scheduling problems needs to be matched with a greater complexity and richness of the associated algorithms; especially when not all instances are known in advance, and so algorithms should not over-specialise to a particular data set.\nRegarding the structure of the paper; in Section 2 we describe the problem to be solved. In Section 3 we discuss how we have carefully chosen the appropriate data structures and implemented algorithms operating with those data structures efficiently in order to construct the schedule from a given sequence as fast as possible. (To build an effective system, one has to pay attention to all of its components.) However, most of the contribution of this paper arises from choosing and combining the effective algorithmic components including the search control algorithm and then (partially) tuning the relevant parameters within the overall approach. These consist of the MCTS-based\nconstructor given in Section 4, the neighbourhoods given in Section 5, and the improvement phase given in Section 6. The computational experiments and competition results are presented and analysed in Section 7; including some reports of performance on a multi-mode, though single project, benchmark set from PSPLIB. Section 8 concludes the paper."
    }, {
      "heading" : "2. Problem Description",
      "text" : "The problem consists of a set P of projects, where each project p ∈ P is composed of a set of activities, denoted as Ap, a partition from all activities A. Each project p ∈ P has a release time ep, which is the earliest start time for the activities Ap.\nThe activities are interrelated by two different types of constraints: the precedence constraints, which force each activity j ∈ A to be scheduled to start no earlier than all the immediate predecessor activities in set Pred(j) are completed; and the resource constraints, in which the processing of the activities is subject to the availability of resources with limited capacities. There are three different types of the resources: local renewable, local non-renewable and global renewable. Renewable resources (denoted using the superscript ρ) have a fixed capacity per time unit. Non-renewable resources (denoted using the superscript ν) have a fixed capacity for the whole project duration. Global renewable resources are shared between all the projects while local resources are specified independently for each project.\nRenewable and non-renewable resources are denoted using the superscript ρ and ν, respectively. Rρp is the set of local renewable resources associated with a project p ∈ P , and Rρpk is the capacity of k ∈ R ρ p , i.e. the amount of the resource k available at each time unit. Rνp is the set of local non-renewable resources associated with a project p ∈ P , and Rνpk is the capacity of k ∈ R ν p , i.e. the amount of the resource k available for the whole duration of the project. G ρ is the set of the global renewable resources, and Gρk is the capacity of the resource k ∈ G ρ.\nEach activity j ∈ Ap, p ∈ P , has a set of execution modes Mj. Each mode m ∈ Mj determines the duration of the activity djm and the activity resource consumptions. For a local renewable resource k ∈ Rρp , the resource consumption is r ρ jkm; for a local nonrenewable resource k ∈ Rνp , the resource consumption is r ν jkm; for a global renewable resources k ∈ G ρ, the resource consumption is gρjkm. Schedule D = (T,M) is a pair of time and mode vectors, each of size n. For an activity j ∈ A, values Tj and Mj indicate the start time and the execution mode of j, respectively. Schedule D = (T,M) is feasible if:\n• For each p ∈ P and each j ∈ Ap, the project release time is respected: Tj ≥ ep;\n• For each project p ∈ P and each local non-renewable resource k ∈ Rνp , the total resource consumption does not exceed its capacity Rνpk.\n• For each project p ∈ P , each time unit t and each local renewable resource k ∈ Rρp , the total resource consumption at t does not exceed the resource capacity Rρpk.\n• For each time unit t and each global renewable resource k ∈ G ρp , the total resource consumption at t does not exceed the resource capacity Gρk.\n• For each j ∈ A, the precedence constraints hold: Tj ≥ maxj′∈Prec(j) Tj′ + dj′Mj′ .\nThe objective of the problem is to find a feasible schedule D = (T,M) such that it minimises the so-called total project delay (TPD), defined by using the time for total project completion (TPC)\nTPC = ∑\np∈P\nCp (1)\nand\nTPD ≡ fd(D) = TPC− L =\n\n\n∑\np∈P\nCp\n\n − L , (2)\nwhere Cp is the completion time of project p\nCp = max j∈Ap\n( Tj + djMj ) . (3)\nThe constant L is a lower bound calculated as\nL = ∑\np∈P\n(CPDp + ep) , (4)\nwith CPDp being a given pre-calculated value. Since L is a constant, then it does not affect the optimisation (it was presumably introduced in the competition just to make the output numbers smaller and easier to interpret). Specifically, since L is the lower bound (though not necessarily a tight bound), fd(D) ≥ 0 for any feasible solution D.\nNote that this primary objective is an instance of the standard “weighted completion time”, usually denoted by “ ∑\nj wjCj”, but specialised to the case, “ ∑ p wpCp”, in which\nonly the completion times of the projects are used2 (in the case of TPD all the weights are assigned to be one).\nThe tie-breaking secondary objective is to minimise the total makespan, (TMS), which is the finishing time of the last activity (or equivalently of the last project):\nTMS ≡ fm(D) = max j∈A\n( Tj + djMj )\n= max p∈P Cp . (5)\nIn our implementation, we combine the objective functions fd(D) and fm(D) into one function f(D) that gives the necessary ranking to the solutions:\nf(D) = fd(D) + γfm(D) , (6)\nwhere 0 < γ ≪ 1 is a constant selected so that γfm(D) < 1 for any solution D produced by the algorithm. In fact, we sometimes use γ = 0 to disable the second objective. For details, see Section 6.1.\nUnder the conventional “α|β|γ” labelling, it could perhaps be described as “MP+S | prec | ( ∑\np wpCp, Cmax )” using ‘MP +S’ to denote ‘Multi-mode Multi-Project Schedul-\ning’.\n2If there is no unique activity marking the end of a project, then a dummy empty end activity can always be added, without changing the problem."
    }, {
      "heading" : "3. Schedule Generator",
      "text" : "Designing an algorithm for solving the multi-mode resource-constrained multi-project scheduling problem requires an appropriate solution representation. There are two ‘natural’ solution representations in the scientific literature:\nSchedule-based: A direct representation using the assignment times, and also modes, of activities, i.e. vectors T and M .\nSequence-based: This is based on selecting a total order on all the activities. Given such a sequence, a time schedule is constructed by taking the activities one at a time in the order of the sequence and placing each one at the earliest time slot such that feasibility of the solution would be preserved. This approach is called serial schedule generation scheme (e.g., see [4]).\nThe schedule-based representation is perhaps the most natural one for a mathematical programming approach, but we believe that it could make the search process difficult for a metaheuristic method, in particular, generating a feasible solution at each step could become more challenging. As is common for heuristic approaches [62], we preferred the sequence-based representation, since it provides the ease of producing schedules that are both feasible and for which no activity can be moved to an earlier time without moving some other activities (the schedule is then said to be ‘active’).\nThe sequence-based representation is a pair S = (π,M), where π is a permutation of all the activities A, and M is a modes vector, same as in the direct representation. The permutation π has to obey all the precedence relations, i.e., π(j) > π(j′) for each j ∈ A and j′ ∈ Pred(j). The modes vector is feasible if Mj ∈ Mj for each j ∈ A and the local non-renewable resource constraints are satisfied for each project p ∈ P .\nIn order to evaluate a solution S, it has to be converted into the direct representation D. By definition, the sequence-based representation S = (π,M) corresponds to a schedule produced by consecutive allocation of activities π(1), π(2), . . . , π(n) to the earliest available slot. The corresponding procedure, which we call schedule generator, is formalised in Algorithms 1, 2 and 3. Note that the procedure guarantees feasibility of the resulting schedule as it schedules every activity in such a way that feasibility of the whole schedule is preserved.\nAlgorithm 1: Serial schedule generation scheme\n1 Let S = (π,M) be the sequence-based solution; 2 for i ← 1, 2, . . . , n do 3 Let j ← π(i); 4 Schedule j in mode Mj to the earliest available slot such that feasibility of the schedule is preserved; 5 end\nThe worst case time complexity of this implementation is O(n(ζ + Tρd)), where ζ = maxj∈A |Prec(j)| is the maximum length of the precedence relation list, T is the makespan, ρ = |G ρ| + maxp∈P |R ρ p | is the maximum number of local and global renewable resources and d = maxj∈A djMj is the maximum activity duration. The first term of\nAlgorithm 2: Scheduling an activity to the earliest available slot.\n1 Let j be the activity to be scheduled; 2 Let m be the mode associated with j; 3 Let p be an index such that j ∈ Ap;\n4 Calculate the earliest start time of j as t0 ← max\n{\nep, max j′∈Prec(j) (Tj′ + dj′m)\n}\n;\n5 t ← TestSlot(j, t0); 6 Allocate activity j at t in mode m and update the remaining capacities;\nAlgorithm 3: A naive implementation of the TestSlot(j, t) function. The function returns the earliest time slot at or after t feasible for scheduling activity j.\n1 Let m be the mode associated with j; 2 Let p be an index such that j ∈ Ap; 3 for t′ ← t, t+ 1, . . . , t+ djm − 1 do 4 for k ∈ Rρp do 5 Let a be the remaining capacity of k at t′; 6 if rρjkm > a then return TestSlot(j, t+ 1) 7 end 8 for k ∈ G ρ do 9 Let a be the remaining capacity of k at t′; 10 if rρjkm > a then return TestSlot(j, t+ 1) 11 end\n12 end 13 return t;\nthe sum corresponds to handling precedence relations, and the second term corresponds to scanning slots and testing resource availability. Note that ζ < n, and the maximum number ρ of resources is a constant in our benchmark instances. Also, T is typically linear in n, and, hence, the time complexity is quadratic. The schedule generator is the performance bottleneck of our solver (note that most of the local search moves described in this paper are no worse than linear time complexity). In our experiments, schedule generation was usually taking over 98% of the CPU time. By introducing several improvements (based on information sharing and caching of partial solutions) described below we reduced the running times of the schedule generator by a factor of around ten compared to our initial routine implementation. That significantly increased the number of iterations the higher level algorithm was able to run within a given time."
    }, {
      "heading" : "3.1. Issues in Efficient Implementation",
      "text" : "In this section, we briefly discuss algorithmic and implementational issues that do not directly affect the number of sequence evaluations, but that are designed to increase the rate that evaluations are performed. This is of importance for application of the methods to real-world problems. However, it can also be of potential importance to choices between different heuristics or other algorithmic components.\nThe methods for evaluation and comparison of algorithms are not necessarily clear. In particular, two aspects that arise with respect to this work, and scheduling in general, are “hidden instances”, and “termination criteria”. Hidden instances (meaning ones that are not available until after the implementation is finished) act against the danger that occurs with open instances of the techniques becoming tailored to the specific instances. For reliability and verification of results this generally means the implementations must be finalised before the release of the instances. In practice, this seems to be rarely applied outside of the context of a competition. In such cases, one might regard competitions are a way to enforce the scientific good practice of fully finalising the algorithm and implementation before the testing.\nThe other aspect is evaluating algorithms’ performance purely in terms of their (wallclock) runtimes, or instead using some attempt at implementing independent “counting” measure of steps taken. The advantages of the former “runtime” method is that it relates to what real world users would usually care about, and also might be the only real option when no sensible pure counting methods are available. The advantage of the counting is that it hides the implementation efficiency and hence allows to compare “pure algorithm designs”. In some research situations the classes of algorithms are sufficiently similar for a counting based comparison being viable, and then may well be standard e.g. in genetic algorithms the number of fitness evaluations is commonly used.\nHence, the counting-based approach encourages/supports rapid explorations of ideas for new algorithms, however exclusive usage would discourage developing new practical methods of improving algorithm performance. For example, counting fitness evaluations can miss the advantages of the incremental evaluation techniques routinely used in metaheuristics, and that are vital for their effectiveness. Runtime-based approach on contrary encourages the researchers to exploit methods that are practical in real circumstances, taking into account incremental evaluation, parallelism and other considerations crucial for real-world systems. We note here that the associated added complexity of algorithm engineering could potentially be partly addressed by hyper-heuristics, as they could provide feedback to the programmer regarding the extra value of low-level heuristics if their implementation were improved. This is particularly relevant in the context of a solver employing multiple neighbourhoods, like the one presented in this paper.\nWe believe that there is no simple answer to which of these two algorithm evaluation approaches is best in general and so both of them have their place. However in the context of the very well studied project scheduling we do believe that engineering questions need to be accounted for. As an example of the importance of such ‘engineering issues’ we refer to another well-studied area of solving propositional satisfiability (SAT) problems and that has been active for many decades3. An important part of development of SAT solvers was the development of ‘watched literals’ [42]. The technique only directly affected the standard and routine ‘unit propagation’ procedure in SAT solvers (which is the CPU-intensive portion, analogous to the schedule generator), but it did so in a fashion that meant new heuristics were then practical, leading to new algorithm designs.\nIn real-world usages of scheduling, an important aspect is the software engineering aspect of the time and cost of implementing and maintaining the software. An initial implementation of a neighbourhood is often relatively easy; however, the practical problems can arise when effective use requires that it is implemented in a fashion that uses\n3E.g. competitions have been held for many years, see http://www.satcompetition.org/\nincremental or delta evaluation (so that the objective function does not require a full re-evaluation). To support the incremental evaluation it is often necessary to implement appropriate data structures that are more sophisticated, and so harder to implement and maintain. With serial generation the majority of the CPU time is spent in the generation of the schedule from the sequence. Hence, we naturally found that significant improvements were achieved by modifying the generator algorithm. Good ‘engineering’ of the serial generation also has the important advantage that it helps all of the neighbourhoods. (If all the neighbourhoods were to rely on entirely separate implementations, then there would be much more pressure, for practical software engineering issues, to reduce to a smaller set.)\nObserve (see Algorithm 2) that the schedule generation algorithm spends most of the time finding the first available slot for an activity. To speed up this phase, we use a modification of the Knuth-Morris-Pratt substring search algorithm. By testing resource availability in the reversed order, we can use early exploration of insufficient resources to skip several values of t, see Algorithm 4.\nAlgorithm 4: An improved implementation of the TestSlot(j, t) function. The function returns the earliest time slot at or after t feasible for scheduling activity j.\n1 for t′ ← t+ djm − 1, t+ djm − 2, . . . , t do 2 for k ∈ Rρp do 3 Let a be the remaining capacity of k at t′; 4 if rρjkm > a then return TestSlot(j, t ′ + 1) 5 end 6 for k ∈ G ρ do 7 Let a be the remaining capacity of k at t′; 8 if rρjkm > a then return TestSlot(j, t ′ + 1) 9 end\n10 end 11 return t\nAnother speed-up heuristic is exploiting the nature of the neighbourhood moves; we noted that any two solutions tested consequently are likely to share a prefix. Let S1 = (π1,M1) be some solution, S2 = (π2,M2) be its neighbour, andD1 = (T 1,M1) andD2 = (T 2,M2) be their direct representations. According to our assumption, π1(i) = π2(i) = j and M1j = M 2 j for i = 1, 2, . . . , x, where x is the prefix length (which is likely to be significant). Then, by construction, T 1j = T 2 j for each j = π 2(1), π2(2), . . . , π2(x). Hence, knowing D1, we do not need to calculate the values T 2j for j = π 2(1), π2(2), . . . , π2(x). For details, see Algorithm 5. This gives a form of incremental evaluation that has the advantage that it applies to all the neighbourhoods used. Potentially, this engineering optimisation could have impact on the design of the improvement algorithm in that the selection of neighbourhood moves could benefit from exploiting changes at the end of the schedule being faster to evaluate than those at the beginning. This is something that can potentially be captured by a hyper-heuristic, as one of the long-term intentions of hyper-heuristics is that they should monitor the CPU times taken by different moves, and combine this with monitoring of\ntheir effects, in order to give better adaptive control of the improvement phase.\nAlgorithm 5: Serial schedule generation scheme with prefix detection.\n1 Let S2 = (π2,M2) be the new solution; 2 Let S1 = (π1,M1) be the previous solution and D1 = (T 1,M1) be the corresponding direct representation; 3 Let prefix ← true; 4 for i ← 1, 2, . . . , n do 5 Let j ← π2(i); 6 if prefix = true and π1(i) = j and M1j = M 2 j then 7 Allocate activity j to T 1j in mode M 2 j and update the remaining capacities; 8 else 9 Schedule j in mode M2j to the earliest available slot;\n10 prefix ← false ;\n11 end\n12 end\nIn addition to the algorithmic improvements, we used several standard programming techniques to optimise the implementation performance, with the primary ones being:\n• Incremental maintenance of auxiliary data: Note that the schedule generator has to maintain the amount of remaining resource for each renewable resource and each time unit, i.e. T · ( |G ρ| + ∑\np∈P |R ρ p | )\nvalues. Since T is not known in advance, the corresponding auxiliary data structure has to be large enough to fit a schedule of size Tmax, where Tmax is the upper bound of T . Considering the initialisation of this auxiliary data structure, the real time complexity of the algorithm is O(n(ζ + Tρd) + Tmaxρ ′), where ρ′ = |G ρ| + ∑ p∈P |R ρ p | = O(ρ|P |). Usually Tmax ≫ T and ρ ′ ≫ 1 and, hence, the last term of the generator complexity has a major impact on the real performance of the procedure. By reusing the same auxiliary data structures and only re-initialising the portion that was altered in the previous run of the generator, we speed up the initialisation phase by a factor of about Tmax/T .\n• Local memory access: Most of the solution evaluations happen in our metaheuristic within local search procedures. Each instance of the local search algorithm is assigned a CPU core, and a dedicated copy of the schedule generator is maintained for it. This is particularly important for the prefix reuse heuristic and CPU cache efficiency."
    }, {
      "heading" : "4. Solution Structures",
      "text" : "This section primarily explains why having “Total Project Delay” (TPD) as the main objective can be expected to lead to the general structure of solutions being very different to the structure obtained with the more standard makespan (TMS) objective. Our observations of the TPD-driven structure had an important influence on our design of the overall algorithm. The structure motivates many of the “project level” moves that are\nconsidered in Section 5. In this section, we also propose a heuristic method to construct the initial solutions which feed into the improvement phase later. As standard in such “construct and improve” optimisation, the concern was that the desirable structures might be difficult to achieve by the improvement unless the initial constructor heuristically tried to get close, by doing an appropriate “heuristically-guided global sampling” of the space of approximate structures. Accordingly, with the intent to increase the robustness of the solver, to handle such structures in unseen instances, a specialised MCTS-based constructor is proposed and developed."
    }, {
      "heading" : "4.1. TPD-driven Solution Structures",
      "text" : "In this subsection, we report on the investigation of the natural question of what constitutes a ‘good’ approximate structure. We firstly look at the structure of a high quality schedule for a benchmark instance, and then we elucidate the observed structures using some small examples. The critical message is that optimising TPD leads to different structures of the solutions than when optimising TMS; we believe that algorithm design needs to take account of this difference.4\nIn particular, we have observed that in many cases, dominance by the TPD objective frequently leads to approximate ordering of the projects. A typical example of this is\n4Similar effects are also observed in [63].\ngiven in Figure 1 using a good solution to instance B-1. Figure 1a shows the structure obtained when only the standard makespan is minimised, but the structure is very different in Figure 1b with the required TPD-dominated objective. That is, the pattern of completion times of each project, the Cp of (3), depends on whether they are driven by TPD, effectively minimising the average of the Cp, or instead driven by the makespan, reducing the maximum of the Cp. The example shows how, with TPD dominating, there are time periods in the schedule when the general focus is on relatively few projects and during the schedule this focus changes between projects.\nHence, the evidence from Figure 1, and other multi-project cases we have looked at, suggests that the approximate ordering is common. Overall, these structures naturally arises from the combination of the TPD (2) objective with the limited global resources. Suppose that some P is the final finishing project, and so its last activity determines the TMS and P’ is an earlier finishing project. It may well be that P’ can move some of its activities earlier by delaying activities of P, though without changing the last activity of P. Such moves will improve the TPD without worsening the TMS.\nThat is, in contrast to the makespan objective, the TPD objective, together with limited shared resources, has a natural side-effect of encouraging unfairness between the finish times of projects, and will drive some projects to finish as early as possible.\nTo illustrate the way in which an approximate ordering might arise, and TPD-driven and TMS-driven solutions can be quite different, we give a set of small example instances. We denote these example instances by “2[x–y–x]”, consisting of two projects. Each project consists of a precedence-constrained chain of just 3 activities of durations x, y and x, respectively. There are two (renewable) shared resources with activities 1 and 3 using the resource 2, and activity 2 using resource 1. We focus on values of x and y that lead to resource 2 being the bottleneck, and so the main driving force of the makespan – corresponding to the shared global resource in the MISTA benchmarks. Figure 2a shows four schedules of this parametrised instance, along with their corresponding values of C1 and C2. Solutions S4 and S3 arise from simply swapping the order of the projects in solutions S1 and S2 respectively.\nThe solutions S1 and S4 do not interleave the two projects, and so leave gaps in the bottleneck resource 2. In contrast, solutions S2 and S3 interleave the projects and so lead to full usage of the bottleneck resource, and are hence automatically the best solutions for the TMS objective. However, S2 and S3 are not always the best solutions for the TPD objective. Changing the solution leads to a tradeoff between C1 and C2, and this is illustrated, in Figure 2b for various choices of x and y. In this figure, the values of x and y are selected so that we always have 2x+ y = 100; this makes the effect of changing the relative balance of x and y clearer, and also means that the values can be interpreted as percentages of the project time.\nBefore proceeding further, we observe that we can also interpret the project completion times, Cp, from the point of view of ‘multi-objective optimisation’, by regarding each of them as separate objectives. In particular, in standard fashion, one can say that a set of values for Cp dominates another set provided that none of the Cp values are worse (larger) and at least one is better (smaller). One can hence discuss the cases in Figure 2b as being the Cp-Pareto Front, of non-dominated sets of Cp. We emphasise this view is for obtaining insight into the space of solutions, and is not used directly within our algorithm; we did not perform multi-objective optimisation over the Cp, though believe it would be worthy of future investigation. Also, note this view is different from\ntaking the pair (TPD,TMS) as a bi-objective problem, and having a (TPD,TMS)-Pareto Front (this would correspond again to a different class of algorithms than we consider here, but is again worthy of future investigation). Incidentally, there are two distinct ‘multi-objective views’ on the space of solutions:\n• Cp-Pareto Front, of dimension p, formed from the p completion times\n• (TPD,TMS)-Pareto Front, of dimension 2, and formed from the aggregates (sum and max) of the Cp\nHowever, it is important to realise that although the Cp and (TPD,TMS) views are different, they are tightly linked. Both the TPD, arising from ∑\np Cp, and the TMS, from maxp Cp, are monotone (non-decreasing) with respect to the Cp and so preserve Pareto dominance. That is, if solution Cp dominates solution C ′ p, then the resulting pair (TPD,TMS) dominates (TPD’,TMS’) – though note that the converse does not\napply. Hence, the (TPD,TMS)-Front can be extracted from the Cp-Front. However, this ‘projection’ loses information; the Cp-Front gives more insight into the space of solutions.\nConsequently, the “Cp-Pareto Front”, or set of non-dominated Cp values, can give some insight into the effect of TPD compared to TMS on the structure of the solutions. Hence, we can regard Figure 2b as showing the effect of the x–y balance on the Cp-Pareto Front. In particular, we see that for two of the cases, 2[44–12–44] and 2[48–4–48] the Cp-Pareto Front is concave in terms of the set of feasible Cp values (on the top right). When the Front is concave then the two solutions minimising the TPD are S1 and S4 and are at opposite ends of the Cp-Front. In contrast, the solutions S2 and S3 minimising the TMS are in the middle.\nGenerally, with a concave Cp-Pareto Front the solutions (locally) minimising ∑\np Cp are naturally at the ends of the Front, and furthermore solutions at the ends of the Front will be more likely to have some Cp values small and others large. This matches with the way that the observed solutions minimising TPD do indeed sequence the projects. That is, when the projects are roughly equal size, and the Cp Pareto Front is concave, then it becomes reasonable that the TPD objective prefers the end points, and so the candidate good solutions are more likely to be widely separated in the search space.\nAlso, notice that as the fraction of activities using the bottleneck resource 2 (that is the value of 2x) increases, then the Cp-Pareto Front becomes more concave, and the TPDdriven solutions become different from the TMS-driven ones. This is consistent with the our general experience of the properties of MISTA instances; the shared global resource tends to be a bottleneck, and consequently they show the approximate ordering as seen in Figure 1. Future work could well use such features related to the tightness of bottleneck resources to predict these effects, and so be used to select appropriate algorithm components. We also remark that the interesting structure of the tradeoff between the completion times does suggest that future work might well use multi-objective methods, e.g. see [10], and of course could study the effect of different weights for the completion times.\nThe simple example above is atypical in that the two projects are identical. However, if the projects are similar in size then it is reasonable to expect that the Cp-Pareto Front may be similar in overall structure but slightly distorted. When the front is concave but still roughly symmetric with respect to swapping the Cp, then the TPD-driven solutions are likely to still be towards the ends of the Front and so widely separated. However, because the front is not totally symmetric the ends are likely to have slightly different values of TPD. Hence, in such circumstances, it is reasonable to expect that there may be many widely separated local optima in TPD, and also that the optima correspond to different (approximate) orderings of the projects."
    }, {
      "heading" : "4.2. MCTS",
      "text" : "We have seen above that the TPD objective tends to favour a different solution structure than standard makespan minimisation, and in particular can drive a partial orderings of the projects, and there may be many good solutions that are widely separated in the solution space. Consequently, our construction method attempts to do a broad exploration of the solution space and to create initial sequences with a broadly similar partial order. The reason for including this was simply that we expected the partial ordering to lead to widely separated local minima in the search space, with large barriers between them – as swapping the order of two projects might mean going through\ninterleaved intermediate states in which the TPD is worse. We included a method to sample the space of approximate project orderings, with the intent to avoid starting in a poor ordering and being trapped.\nWe expect that only a partial ordering is needed because we can assume that the subsequent improvement phase can make small or medium size adjustments to the overall project ordering structure. However, the improvement phase could have more difficulty, and take more iterations, if the general structure of the project ordering were not close to the structure expected in best solutions. Consequently, and for simplicity, we decided that a reasonable approximation would be to use a 3-way partition of the projects taken to correspond to ‘start’, ‘middle’ and ‘end’ parts of the overall project time. We required the numbers of projects in each part (of the partition) to be equal – or with a difference of at most one when the total number of projects is not a multiple of 3.\nThe problem then is how to quickly select good partition of the projects, and the method we selected is a version of Monte-Carlo Tree Search (MCTS) methods [3]. The general idea of MCTS is to search a tree of possibilities, but the evaluation of leaves is not done using a predefined heuristic, but instead by sampling the space of associated solutions. The sampling is performed using multiple invocations of a “rollout” which is designed to be fast and unbiased. It needs to be fast so that multiple samples can be taken; also rather than trying to produce “best solutions” it is usually designed to be unbiased – the (now standard) idea being that it should provide reliable branching decisions in the tree, but is not directly trying to find good solutions.\nIn our case, the tree search corresponds to decisions about which projects should be placed in which part of the partition. The rollout is a fast way to sample the feasible activity sequences consistent with the candidate choice for the partition of the projects. Specifically, the tree search works in two levels; firstly to select the projects to be placed in the end part and then to select the partition between the start and middle parts.\nThe first stage considers5 100 random choices for the partition of the projects, and then selects between these using 120 samples or the rollout6. The rollout consists of two main stages:\n1. Randomly select a total ordering of the activities consistent with the precedences and with the candidate partitioning. Specifically, within each partition we effectively consider a dispatch policy that randomly selects between activities that are available to be scheduled because their preceding activities (if any) are already scheduled.\n2. Randomly select modes for the activities. If the result is not feasible then this can only be because of the mode selection causing a shortfall in some non-renewable resources. Hence, it is repaired using a local search on the space of mode selections. We use moves that randomly flip one mode at a time, and an objective function that measures the degree of infeasibility by the shortfall in resources. Since the non-renewable resources are not shared between projects, this search turned out to be fast and reliable. As a measure of precaution, if the procedure fails to obtain\n5The parameters for the MCTS were the result of some mild tuning, and used in the competition submission, but of course are adjustable.\n6The number 120 was selected so that the rollouts could be evenly distributed between 2, 4, 6, 8 or 12 cores of the machine\na feasible selection of modes after a certain number of local search iterations, we restart it with a random selection of modes.\nThe first stage ends by making a selection of the best partitioning, using the quality of the 25th percentile of the final solution qualities (the best quartile) of the results of the rollouts. The ‘end’ part is then fixed to that of the best partition. The decision to fix the ‘end’ part also arose out of the observation that in good solutions the end projects are least interleaved. The MCTS proceeds to the second stage, and follows the same rollout procedure but this time to select the contents of the middle (and hence start) parts. This entire process usually completes within only a few seconds, and was used as the construction stage before the much longer improvement phase. We emphasise that the TPD-structure observed earlier had an important influence of the design of the neighbourhoods used in the key improvement phase – in particular, the motivation that some moves should affect the project level structure."
    }, {
      "heading" : "5. Neighbourhood Operators",
      "text" : "In this section, we describe our neighbourhood operators, used by several components during the improvement phase. The operators (also referred to as low-level heuristics or simply moves, depending on the algorithm which makes use of them) are categorised into three groups. This categorisation is mainly based on the common nature of the strategy the operators employ while manipulating the solution. Some of the moves are similar to those used by other submissions; see [62], and [16] and [57] for few examples. To make the paper self-contained, below we provide brief descriptions of all the moves used in our algorithm.\nWe guarantee that all of our operators preserve feasibility of the solution. Also, all of the moves are randomised so that they could be repeatedly used in “simulated annealing”like improvement or applied as mutation operators. All the random selections are made at uniform unless specified otherwise."
    }, {
      "heading" : "5.1. Activity-level Operators",
      "text" : "Operators in this category involve basic operations, widely used in the literature, such as changing the mode of a single activity or swapping the positions of two activities. On top of that, we implemented a limited first improvement local search procedure for each of the basic moves.\nTo describe the moves in this category, we will need additional notations. Let pos(j) be the position of activity j ∈ A within a given solution. If activity j is shifted to a different position, we say that the feasible range of its new position is [ℓ(j), u(j)], where ℓ(j) and u(j) can be computed as follows:\nℓ(j1) =\n{\nmaxj∈Pred(j1) pos(j) + 1 if Pred(j1) 6= ∅, 1 otherwise (7)\nand\nℓ(j1) =\n{\nminj∈Succ(j1) pos(j)− 1 if Succ(j1) 6= ∅, n otherwise, (8)\nwhere Succ(j1) is the set of successors of j1.\n• Swap activities: swap two activities in the sequence. Select an activity j1 ∈ A randomly. Select another activity j2 6= j1 ∈ A randomly such that ℓ(j1) ≤ pos(j2) ≤ u(j1). If ℓ(j2) ≤ pos(j1) ≤ u(j2) then swap j1 and j2. Otherwise leave the solution intact.\n• Shift: shift an activity to a new location in the sequence. Shift a randomly selected activity j ∈ A to a new position randomly selected from pos(j) ∈ [ℓ(j), u(j)].\n• Change mode: change the mode of a single activity. Select an activity j randomly at uniform, and if |Mj | = 1 then leave the solution intact. Otherwise select a new mode m 6= Mj ∈ Mj and, if this does not lead to a violation of non-renewable resource constraints, update Mj to m.\n• FILS swap activities: apply the first improvement local search (FILS) procedure based on the swap move. The operator has one parameter: the width W > 1 of the window to be scanned. Select an activity j1 ∈ A randomly. Define the window [ℓ′, u′] for activity j2 as follows. If u(j) − ℓ(j) < W , let ℓ′ = ℓ(j) and u′ = u(j). Otherwise select ℓ′ ∈ [ℓ(j), u(j) − W + 1] and set u′ = ℓ′ + W − 1. Then, for every j2 ∈ A \\ {j1} such that ℓ\n′ ≤ pos(j2) ≤ u, attempt to swap j1 and j2 (see SwapActivities). If the attempt is successful and it reduces the objective value of the solution then accept it and stop the search. Otherwise roll back the move and proceed to the next j2 if any.\n• FILS shift: apply the first improvement local search (FILS) procedure based on the shift move. The operator is implemented very similarly to FILS swapActivities, i.e. it attempts to shift a randomly selected activity to an new position within a window of a given size.\n• FILS change mode: apply the first improvement local search (FILS) procedure based on the change mode move (the operator has no parameters). Select an activity j ∈ A randomly. For m ∈ Mj \\ {Mj}, produce a new solution by setting Mj = m. If the resulting solution is feasible and provides an improvement over the original solution, accept the move and stop the local search. Otherwise proceed to the next m ∈ Mj \\ {Mj} if any."
    }, {
      "heading" : "5.2. Ruin & Recreate Operators",
      "text" : "The Ruin & Recreate (R&R) operators are widely used in metaheuristics as mutations or strong local search moves but, to the best of our knowledge, they are relatively new to serial generation in scheduling. As the name suggests, such operators have two phases: the “ruin” phase removes some elements of the solution and the “create” phase reshuffles those elements (for example, randomly) and then inserts them back. The number of elements to remove and re-insert is a parameter of a R&R operator, which controls its strength (i.e. the average distance between the original and the resulting solutions).\nWe implemented three different types of R&R operators, and several strategies to select activities involved in the move. One can arbitrary combine any type of the R&R operator with any activity selection strategy.\nThe move types are Reshuffle Positions, Reshuffle modes and Reshuffle positions and modes. Reshuffle positions move removes all the selected activities\nA′ ⊂ A from the solution (leaving |A′| gaps in the sequence) and then re-inserts them in a random order while respecting the precedence relations. To determine a feasible order, we compute which activities V i can be placed in each gap i = 1, 2, . . . , |A′| (in terms of precedence relations between the activities in A′ and activities in A\\A′), and also produce a precedence relations sub-graph induced by A′. Then we apply a backtracking algorithm. In each iteration, it fills one gap, starting from the earliest gaps in the sequence. For gap i it randomly selects an activity j ∈ A′ such that there are no incoming arcs to j in the precedence sub-graph. If such an activity exists, the algorithm removes j from A′ and from the precedence sub-graph. Otherwise it rolls back to attempt another activity on the previous level of search. The depth of roll backs is unlimited, i.e., in the worst case, the algorithm will performs the depth first search of the whole search tree (observe that there is always at least one feasible arrangement of the activities).\nReshuffle modes move changes the modes of selected activities while keeping their positions intact. Each of the selected activities j ∈ A′ is assigned a randomly chosen mode, either equal or not to the previous mode Mj . Observe that the new mode selection may cause infeasibility in terms of non-renewable resources. Thus, we use the multi-start metaheuristic, simply repeating the above step until a feasible mode selection is found.\nReshuffle positions and modes combines the above two moves, which is trivial to implement as modes feasibility is entirely independent of the sequence feasibility.\nWe implemented several activity selection strategies some of which exploit our knowledge of the problem structure.\n• Uniform: as the name suggests, A′ ⊂ A is selected randomly at uniform. The number of elements in A′ is a parameter of the move.\n• Project: the activities A′ are selected within a single project, i.e. A′ ⊂ Ap. The project p ∈ P is selected randomly.\n• Local: the selection of activities is biased to those scheduled near a certain time slot in the direct representation D. We randomly sample the activities accepting an activity j with probability\nprobability(j) = 1\n|Tj−τ | width\n+ 1 , (9)\nwhere 0 ≤ τ ≤ fm(D) (see (5)) is the randomly selected time slot (the centre of the distribution) and width is a parameter defining the spread of the distribution. The sampling stops when A′ reaches the prescribed cardinality.\n• Global resource driven: the selection of activities is biased to the ones scheduled to time slots that under-utilise the global resources. The rationale is that global resources usually present a bottleneck in minimising the project completion times, and inefficiencies in the global resources consumption should be addressed when polishing the solution. In this selection strategy, as in Local selection, we use random sampling, but the acceptance probability for an activity j ∈ A is defined by\nprobability (j) =\n∑\nk∈G ρ remainingk(Tj) ∑\nk∈G ρ G ρ k\n, (10)\nwhere remainingk(t) is the remaining (unutilised) capacity of global resource k at the time slot t.\n• Ending biased: the selection of activities is biased to the last activities within the projects. The rationale is that, in an unpolished solution, the completion of a project might be improved by careful packing the last activities such that all the last activities end at roughly the same time, effectively maximising the utilisation of the local resources and redistributing the global resources between projects. As in Local selection and Global resource driven selection, we use random sampling of activities. The probability of accepting an activity j ∈ A is\nprobability (j) = project -pos(j)\n|Ap| , (11)\nwhere p is the project of activity j and project -pos(j) is the position of activity j among the activities of project p."
    }, {
      "heading" : "5.3. Project-level Operators",
      "text" : "It was shown in Section 4 that the TPD objective function tends to create a partial ordering of projects in high-quality solutions. The danger, however, is that the ordering to which the solution converges might be sub-optimal. Consider two well-polished solutions S1 and S2 having different project orderings. The distance between S1 and S2 is likely to be significant with respect to the activity-level and R&R operators as many activities need to be moved to convert S1 into S2. Moreover, the transitional solutions in such a conversion will have significantly poorer quality compared to that of S1 and S2. This indicates that, once the algorithm converges to a certain project ordering, it needs a lot of effort to leave the corresponding local minimum and change the project ordering.\nThe project-level operators are designed to overcome such barriers, allowing faster exploration of the rough landscape of the MRCMPSP. They perform on the project level and thus one move of any of operators from this category usually results into a change in the project ordering. That being said, the project-level moves are likely to corrupt the solution causing many activity-level inefficiencies that need to be treated with activity-level and R&R moves. Thus, they ought to be used rarely within local search, but they can serve well as mutation operators.\nIn some of the project-level moves we use the formal concept of project ordering P (S). To extract P (S) from a solution S, we compute the “centre of mass” centre(p) for each project p ∈ P as\ncentre(p) = 1\n|Ap|\n∑\nj∈Ap\npos(j)\nand define the ordered set P (S) of projects according to their centres of mass.\n• Swap two projects: swap two randomly selected projects in the sequence. Select two projects p1 6= p2 ∈ P randomly. Remove all the activities belonging to projects p1 and p2 from the sequence. Fill the gaps with all the p2 activities and then all the p1 activities preserving the original order within each of the projects. If, in the original solution, p1 was located earlier than p2 then the move swaps the projects (for instance, (1, 1, 1, 2, 1, 2, 2, 2) turns into (2, 2, 2, 2, 1, 1, 1, 1)). Otherwise\nit simply separates them more clearly (for instance, (2, 2, 2, 1, 2, 1, 1, 1) turns into (2, 2, 2, 2, 1, 1, 1, 1)).\n• Swap neighbour projects: swap two projects adjacent in the project ordering. Extract the project ordering P (S), randomly select 1 ≤ i < q and set p1 = P (S)i and p2 = P (S)i+1. Then apply the Swap two projects move.\n• Compress project: place all the activities of a project adjacently to a new location x in the sequence while preserving their original ordering. Randomly select a project p ∈ P and remove all the activities j ∈ Pp from the sequence, squeezing the gaps (the resulting sequence will contain n−|Pp| activities). Insert all the activities j ∈ Pp consecutively at position ⌈x(n − |Pp|)⌉, where the parameter 0 ≤ x ≤ 1 is the new relative location of Pp.\n• Shift project: shift all the activities of a project by some offset. Randomly select a project p ∈ P and calculate posmin = minj∈Pp posj and posmax = maxj∈Pp posj . Randomly select an offset −posmin < δn − posmax and shift every activity j ∈ Pp by δ positions toward the end of the sequence.\n• Flush projects: flush the activities of one or several projects to the beginning or ending of the sequence. Compute the project ordering P (S) and select x consecutive projects P ′ from P (S), where 1 ≤ x < |P | is a parameter. Flush all the activities in projects P to either the beginning or the ending of the sequence (defined by an additional parameter of the move)."
    }, {
      "heading" : "6. Improvement Phase",
      "text" : "Most of the time our algorithm spends on improving the initial solutions. We use a multi-threaded implementation of a simple memetic algorithm with a powerful local search procedure based on a hyper-heuristic which controls moves discussed in the previous section."
    }, {
      "heading" : "6.1. Memetic Algorithm",
      "text" : "A genetic algorithm is a population based metaheuristic combining principles of natural evolution and genetics for problem solving [54]. A pool of candidate solutions (individuals) for a given problem is evolved to obtain a high quality solution at the end. A fitness function is used to measure the quality of each solution. Mate/parent selection, recombination, mutation and replacement are the main operators of an evolutionary algorithm. However, the usefulness of recombination is still under debate in the research community [13, 38]. A recent study showed that recombination can be useful at a certain stage during the search process, if the mutations do not change the quality of resultant individuals leading to a population containing different individuals with the same fitness [56]. The choice of the recombination operator can influence the best setting for the rate of mutation depending on the problem dealt with. Although the study is rigorous, it is still limited considering that some benchmark functions, such as OneMax are used for the proofs. A memetic algorithm (MA) hybridises a genetic algorithm with local search which is commonly applied after mutation on the new individuals [40, 41]. Many improvements for MAs have been suggested, for example the population sizing [24] and\ninterleaved mode of operation [47]. MAs have been successfully applied to many different problems ranging from generalised travelling salesman [19] to nurse rostering [44].\nThe improvement phase of our algorithm is controlled by a simple multi-threaded MA which manages the solution pool and effectively utilises all the cores of the CPU. Our MA is based on quantitative adaptation at a local level according to the classification in [43].\nWithin the MA, we use a powerful local search procedure that takes a few seconds on a single core to converge to a good local minimum. (Running the local search for a longer time still improves the solution but the pace of improvements slows down.) As the local search procedure has to be applied to every solution of the population in each generation, and the local search is by far the most intensive time consumer in our algorithm, the total running time of the algorithm can be estimated as\nnum-gen · pop-size · ls-time\ncores ,\nwhere num-gen is the number of generations, pop-size is the size of the population, ls-time is the time taken by the local search procedure on one core per solution and cores is the number of available CPU cores. Our algorithm is designed to converge within a few minutes. This implies that, to have a sufficient number of generations, the size of the population ought to be of the same order as the number of cores. On the other hand, as the memetic algorithm requires a barrier synchronisation at each generation, and our local search procedure cannot utilise any more than one CPU core, the size of the population has to be pop-size = cores · i to fully utilise all the cores, where i is a positive integer. In our implementation, we fixed the running time of the local search procedure to five seconds, and the size of the population to the number of CPU cores available to the algorithm.\nBecause of the small population size and limited number of generations, we decided to use a simple version of the MA, see Algorithm 6. We denote the population as S = {S1, S2, . . . , Scores}, and the subroutines used in the algorithm are as follows:\n• Construct() returns a new random solution with the initial partial project sequence obtained during the construction phase (see Section 4).\n• Accept(Si) returns true if the solution Si is considered ‘promising’ and false otherwise. The function returns false in two cases: (1) f(Si) > 1.05f(Si′) for some i′ ∈ {1, 2, . . . , |S|} or (2) the solution was created at least three generations ago and Si is among the worst three solutions. Ranking of solutions is performed according to fd(Si) + idle , where idle is the number of consecutive generations that did not improve the solution Si.\n• Select(S) returns a solution from the population chosen with the tournament selection based on two randomly picked individuals.\n• Mutate(X) returns a new solution produced from solution X by applying a mutation operator. The mutation operator to be applied is selected randomly and uniformly among the available options:\n– Apply the Reshuffle positions and modes move with the Local selection strategy (see Section 5.2). The cardinality of the selection is |A′| = 3. Repeat\nthe procedure 20 times, each time randomly selecting the centre of distribution 1 ≤ τ ≤ fm(D), see (9). The width parameter is taken as width = 0.1fm(D).\n– Apply the Swap neighbour projects operator once (see Section 5.3).\n– Apply the Flush projects operator with the number of selected projects being one and flushing to the end of the sequence (see Section 5.3).\n– Apply the Flush projects operator with the number of selected projects being two and flushing to the beginning of the sequence. It was noted that the project ordering is usually less explicit at the beginning of the sequence in good solutions, and for that reason we did not use flushing single projects to the beginning in our mutations.\n– Same as the last mutation except that the number of selected projects is three.\nIt was noted in Section 4 that the components fd(D) and fm(D) of the objective function f(D) (see (6)) are competing. Indeed, minimisation of the total makespan favours solutions with projects running in parallel as such solutions are more likely to achieve higher utilisation of the global resources. At the same time, minimisation of the TPD favours solutions with the activities grouped by projects. Hence, the second objective creates a pressure for the local search that pushes the solutions away from the local minima with regards to the first (main) objective. To avoid this effect, we initially disable the second objective (γ ← 0, see Section 2) and re-enable it only after 70% of the given time is elapsed.\nThe parameters of the memetic algorithm (such as the ones used in the Accept(Si) function, or the number of solutions in the tournament in Select(S)) have been chosen using a parameter tuning procedure. It should be noted, however, that the algorithm is not very sensitive to any of those parameters, which makes it efficient on a wide range of instances, and this conclusion was supported by our empirical tests, see Section 7."
    }, {
      "heading" : "6.2. A Dominance based Hyper-heuristic Using an Adaptive Threshold Move Acceptance",
      "text" : "There is a growing interest towards self-configuring, self-tuning, adaptive and automated search methodologies. Hyper-heuristics are such high level approaches which explore the space of heuristics (i.e. move operators) rather than the solutions in problem solving [5]. There are two common types of hyper-heuristics in the scientific literature [7]: selection methodologies that choose/mix heuristics from a set of preset low-level heuristics (which can both improve or worsen the solution) and attempt to control those heuristics during the search process; and generation methodologies that aim to build new heuristics from a set of preset components. The main constituents of an iterative selection hyper-heuristic are heuristic selection and move acceptance methods. At each step, an input solution is modified using a selected heuristic from a set of low-level heuristics. Then the move acceptance method is used to decide whether to accept or reject the new solution. More on different types of hyper-heuristics, their components and application domains can be found in [5, 6, 53, 45].\nIn this study, we combine two selection hyper-heuristics under a single point based iterated local search framework by employing them adaptively in a staged manner. The approach extends the heuristic selection and move acceptance methods introduced in [46] and [28], respectively. The pseudocode of our adaptive iterated multi-stage hyperheuristic is given in Algorithm 7. lines 5–21 and 22–25 provide the high level design of the first and second stage hyper-heuristics, respectively.\nAlgorithm 6: Improvement Phase.\n1 γ ← 0; 2 for i ← 1, 2, . . . , |S| do 3 Si ← Construct(); 4 end 5 while elapsed -time ≤ given-time do 6 if elapsed -time ≥ 0.7given-time then 7 γ ← 0.000001 (enable secondary objective function); 8 end\n9 for i ← 1, 2, . . . , |S| (multi-threaded) do 10 Si ← LocalSearch(Si); 11 end 12 for i ← 1, 2, . . . , |S| do 13 if Accept(Si) = false then 14 X ← Select(S); 15 Si ← Mutate(X); 16 end\n17 end\n18 end"
    }, {
      "heading" : "6.2.1. First stage hyper-heuristic",
      "text" : "The first stage hyper-heuristic maintains an active pool of low-level heuristics LLH ⊆ LLH all and a score scoreh associated with each heuristic h ∈ LLH . In each iteration, it randomly selects a low-level heuristic from the active pool with probability of picking h ∈ LLH being proportional to scoreh (line 5). Then the selected heuristic is applied to the current solution (line 6). Initially, each heuristic has a score of 1, hence the selection probability of a heuristic is equally likely. The first stage hyper-heuristic always maintains the best solution found so far, denoted as Sbest (lines 9–11) and keeps track of the time since last improvement.\nThe move acceptance component of this hyper-heuristic (lines 7–17) is an adaptive threshold acceptance method controlled by a parameter ǫ accepting all improving moves (lines 7–12) and sometimes non-improving moves (lines 14–16). If the quality of a new solution S′ is better than (1 + ǫ)f(Sbest) (line 14), even if this is a non-improving move, S′ gets accepted becoming the current solution S. Whenever Sbest can no longer be improved for elapsed -time2 (in our implementation elapsed -time2 = 1 sec), the parameter ǫ gets updated as follows:\nǫ(Sbest) = ⌈log(f(Sbest))⌉+ rand\nf(Sbest) (12)\nwhere 1 ≤ rand ≤ ⌈log(x)⌉ is selected randomly at uniform. Note that 0 is a lower bound for f(S) (see Section 2) and, hence, the algorithm will terminate if f(Sbest) = 0."
    }, {
      "heading" : "6.2.2. Second stage hyper-heuristic",
      "text" : "The second stage hyper-heuristic dynamically starts operating (lines 22–25 of Algorithm 7) whenever there is no improvement in f(Sbest) for elapsed -time3 (in our imple-\nAlgorithm 7: LocalSearch(Si)\n1 Let LLH all = {LLH 1,LLH 2, . . . ,LLHM} represent set of all low level heuristics with each heuristic being associated with a score, initially set to 1; 2 Let Sbest represent the best schedule; 3 S ← Si;Sbest ← Si;LLH ← LLH all; ǫ ← ǫ(Sbest); 4 repeat 5 h ← SelectLowLevelHeuristic(LLH); 6 S′ ← h(S); 7 if f(S′) < f(S) then 8 S ← S ′; 9 if f(S′) < f(Sbest) then\n10 Sbest ← S′; 11 end\n12 end\n13 else 14 if f(S′) < (1 + ǫ)f(Sbest) then 15 S ← S′; 16 end\n17 end 18 if NoImprovement(elapsed -time2) then 19 S ← Sbest; 20 ǫ ← ǫ(Sbest); 21 end 22 if NoImprovement(elapsed -time3) then 23 ǫ ← ǫ(Sbest); 24 (S,LLH ) ← SecondStage(Sbest,LLH all, elapsed -time1); 25 end\n26 until timeLimitExceeded(elapsed -time1); 27 return Sbest;\nmentation elapsed -time3 = 3 sec) in line 22. The hyper-heuristic in this stage updates the active pool LLH of heuristics. LLH ⊆ LLHall is formed based on the idea of a dominance-based heuristic selection as introduced in [46] reflecting the trade-off between the objective value achieved by each low-level heuristic and number of steps involved. The method considers that a low-level heuristic producing a solution with a small improvement in a small number of steps has a similar performance to a low level heuristic generating a large improvement in large number of steps. This hyper-heuristic not only attempts to reduce the set of low-level heuristics but also assigns a score for each lowlevel heuristic in the reduced set, dynamically. Those scores are used as a basis for the heuristic selection probability of each low level heuristic to be used in the first stage hyper-heuristic.\nIn the second stage hyper-heuristic, firstly, ǫ is updated in the same manner as in the first stage hyper-heuristic and never gets changed during this phase. Then a greedy strategy is employed using all heuristics in LLH all for a certain number of steps. Each move in this stage is accepted using the same adaptive threshold acceptance method as described in Section 6.2.1. LLH all is partitioned into three subsets LLH small, LLHmedium, LLH large considering the number of activities processed (e.g., number of swaps) by a given heuristic. Small: Swap activities, Shift and Change mode. Medium:\n• Reshuffle modes and Reshuffle positions and modes with the Uniform selection strategy;\n• Reshuffle modes and Reshuffle positions and modes with the Local selection strategy;\n• Reshuffle positions and modes with the Global resource driven selection strategy;\n• Reshuffle positions and modes with the End biased selection strategy;\n• Reshuffle positions and modes with the Project selection strategy;\n• FILS swap activities, FILS shift and FILS change mode;\nLarge:\n• Flush projects applied to one project; the direction is picked randomly;\n• Swap two projects;\n• Compress project;\n• Shift project.\nAt each step, each low-level heuristic is applied to the same input solution for a fixed number of iterations (5n/q for small, n/q for medium and 1 for large heuristics).7 If\n7Similar to the memetic algorithm parameters, this partition of LLH all and the associated numbers of iterations were selected by intuition and parameter tuning.\na low-level heuristic produces a new solution identical to the input, that invocation is ignored. Otherwise, the objective of the new solution together with the low-level heuristic which produced that solution gets recorded. Once all heuristics are applied to the same input and get processed at a given step, the best resultant solution propagates as input to the next greedy step.\nA solution is considered to be a ‘non-dominated’ solution at a given step, if the quality of the solution is better than the best solution obtained in the previous steps. The active pool LLH of low-level heuristics is formed using the heuristics achieving the non-dominated solutions obtained at each step during the greedy phase. The score of a low-level heuristic is set to the number of non-dominated solutions that it produced. Note that a non-dominated solution could be generated by multiple low-level heuristics, in which case, their scores get increased.\nFigure 3 illustrates a run of the second stage hyper-heuristic with LLH all = {LLH 1, LLH 2, LLH 3, LLH 4} for four steps. The plot shows that the set of non-dominated solutions contains 3 points marked in black. The best solution achieved by LLH 3 in the last step is ignored, since that solution is dominated by the first three best solutions obtained in the first three steps. The first, second and third points in the set of nondominated solutions are associated with {LLH 1,LLH 2}, {LLH 1} and {LLH 1,LLH 3}, respectively. Hence, LLH becomes {LLH 1, LLH 2, LLH 3}. The scores of LLH 1, LLH 2 and LLH 3 are assigned to 3, 1 and 1, respectively, yielding a heuristic selection probability of 60%, 20% and 20% for each heuristic. More details on the components of the proposed hyper-heuristic can be found in [26, 25]."
    }, {
      "heading" : "7. Experiments",
      "text" : "In this section we report the results of our computational experiments as well as discuss the MISTA 2013 Challenge and its outcomes. For additional analysis of the performance of our and other submitted algorithms we refer to [62]."
    }, {
      "heading" : "7.1. Test Instances",
      "text" : "In our experiments, we used three sets of MRCPSP instances provided by the organisers of the MISTA 2013 Challenge. The first set (set A) was provided at the beginning of the competition. The second set (set B) was provided after the qualification phase. The\nthird set (set X) was hidden from the participants of the competition until the announcement of the results; it was used to evaluate the performance of the submitted algorithms. While designing and tuning our algorithm, we had no information on what properties would the hidden instances have, which drove our decisions towards self-adaptive methods and rich range of moves.\nAll the three sets of instances were produced by combining PSPLIB benchmark instances [29] and can be downloaded from allserv.kahosl.be/mista2013challenge. In Table 1 we provide some properties of all the A, B and X instances."
    }, {
      "heading" : "7.2. Experimental Results",
      "text" : "During the competition, the submitted algorithms were tested by running each of the algorithms on each of the B and X instances 10 times. The results for X (hidden) instances were used to rank the teams. The detailed discussion of the ranking scheme and the analysis of the competition results are reported in [62].\nTo further analyse the performance of our algorithm, we ran it for 5 minutes on each of the B and X instances 2500 times. We used a machine with similar configuration8 to that employed by the competition organisers.\nThe aggregated results of our experiment are reported in Table 2. The average and best solutions are obtained from our 2500 runs, while the competition (“Comp” column) result is the best solution found during the final phase of the competition by our or some other algorithm.\nIt is worth mentioning here that during the final phase of the competition where the submitted algorithms were tested on B and X instances, our approach outperformed, on average, all the other algorithms in 18 out of 20 cases, and produced the best solution for 17 instances. [62] note the consistency of the performance of our algorithm, and also provide ranks with several different ranking methodologies, showing that our approach would win the competition even if the rules of the competition were different.\nWe conducted an additional experiment to investigate the effect of the runtime on the ranking results, and so an indication of whether or not the code improvements as discussed in Section 3.1, might have been the predominant reason, as opposed to the algorithm itself, of achieving the first rank during the competition. In this experiment, we ran our algorithm (in the style which was used during the competition) with reduced time limits. After each run, we determined the rank of our algorithm among all other competitors (the data required for this ranking was received from competition organisers). Figure 4 shows the result of the experiment for various time limits ranging from 1 to 25 seconds. For the first two or three seconds the algorithm runs MCTS thus not generating high quality solutions (recall that the goal of MCTS is to find a high-quality partial ordering of projects). However, as soon as the memetic algorithm starts, the solution quality rapidly improves. In fact, our algorithm manages to achieve the first rank after just 12 seconds. Given that the time limit during the competition was 300 seconds, this means that the algorithm would have still ranked highest even if it had run 25 times slower. In contrast the improvements of Section 3.1 did not give that factor of a speed up. Assuming that the other submissions were reasonably implemented then we conclude that efficient implementation was not the only factor, but that the algorithm design also had a major role in the success of the algorithm.\n8Intel i7 3.2 GHz CPU, 16 GB of RAM, Microsoft Windows 7 x64 operating system\nTo illustrate the performance of our approach in time, two boxplots are provided in Figures 5a and 5b for instances B-1 and X-10, respectively. These instances have been chosen to demonstrate the performance of our algorithm on relatively small and large instances. However, our experiments show that the algorithm behaves in a similar manner with respect to the other instances. The central dividing line of each box in each of the figures presents the median objective value obtained by our algorithm. The\nedges of each box refer to 25th and 75th percentiles while the whiskers (demonstrated by a + marker) are the extreme objective values which are not considered as outliers. Also, the curve which passes through the plot demonstrates the average performance of the algorithm. Our approach continuously makes improvement indicating that the algorithm does not prematurely converge.\nIt would obviously be interesting to link together the performance of the algorithm(s) with the features of the instances; for example, see [50, 37] and others. There are many potential features that could be used to characterise the instances, and these have often been used as part of the process of generating instances, for example [30, 12]. For example, one can characterise the precedence graph in various ways [52, 23, 15] and associated measures such as ‘complexity’ and ‘Series-Parallel’ indices could also be included. This would be far outside the scope of this paper, and warrants a future study. However, we can make some initial observations by analysing the instance properties in Table 1 and the results in Table 2, and in more detail in Table 33 of the final competition summary [64]. In particular, we focus on the property of the number of local renewable resources in the column “avg |Rρ|”, and that there are none for instances B-2, B-8, B-10, X-1 and X-4. These seem to correlate with the performance in that the algorithm here only fails to rank first on instances B-2 and X-1, both of which have no local renewable resources. We remark that having no local renewables is a rather special case, and so if these were removed the relative performance would have further improved. However, it seems reasonably likely that the number of local renewable resources would be good instance feature for selecting algorithms, or for future studies comparing different algorithms in more details."
    }, {
      "heading" : "7.3. Results on Single Project (PSPLIB) Instances",
      "text" : "The main aim of this work is to develop methods for the MISTA benchmark problems having multiple projects. However, it is also of interest to study the methods on other scheduling benchmarks; in particular, the PSPLIB library9 of instances [29] has been widely used and studied in the academic scheduling community. Unfortunately, despite it being long known that the majority of real world problems are multi-project [35], it seems\n9http://www.om-db.wi.tum.de/psplib/\nthat much of the work in project scheduling has been on single project instances, and PSPLIB does not currently contain any multi-project instances. Despite this limitation of PSPLIB, it has functioned well as a standard set of benchmarks, and so it is likely to interest how well the methods of this paper perform on these instances. The most relevant set of instances in PSPLIB is provided by the multi-mode single project instances10. The study of those instances is also limited considering the number of tasks which goes up to 30 tasks, whereas the MISTA competition included instances with up to 600 tasks. (Other PSPLIB instances are either much simpler as they are just single mode, or else introduce extra constraints of objectives for the timing, and so are outside the scope of this current paper.) Specifically, we studied the J30 instances from PSPLIB; they all have precisely 3 modes for each of the 30 tasks, with 2 renewable and 2 non-renewable resources, and we use the 552 of them that are satisfiable (a feasible solution is known).\nFor the special case of single project, minimising the TPD objective becomes the same as minimising the makespan, TMS. Hence, the PSPLIB multi-mode instances can be regarded as a special single-project case of the MISTA formulation and so our methods could be directly applied to these instances to minimise the makespan. However, doing so would be rather inefficient as many portions of our methods are designed to handle multiple-projects and the associated TPD objective. Hence, we partially specialised our code in order to handle these simpler PSPLIB instances. In particular, we inactivated the Monte-Carlo Tree Search as it is only there in order to handle the structures arising from the TPD objective, replacing it with a simple random generation heuristic. We also inactivated the neighbourhoods that are specific to working on a specific project. Since making comparisons to previous work also needed a very limited time period and just a single thread, we also disabled the memetic component as it was designed to exploit\n10http://www.om-db.wi.tum.de/psplib/getdata.cgi?mode=mm\nmulti-cores and longer runs; to partially compensate for the resulting expected loss in diversity in the search, we also added a simple restart of the search every 10k generated schedules.\nThese instances have been studied in various works. [49] used a Genetic algorithm and also Forward-Backward Iterations (FBI) [34] along with a simple method to repair infeasible mode choices. [9] again used FBI along with a more expensive SAT-based method to ensure that only feasible mode decisions were made (recently, [51] reported that this was the best method on these instances). In these works, the standard fashion is to impose a limit on the number of schedules generated. The reported performance measure is then the gap, averaged over all instances, from either a known lower bound or else from the current “Best Known Solutions (BKS)”. Often, the number of schedules permitted is rather small, such a 5–10k, which would take our code less than a second, and so would not really match the intent of our methods. Hence, in Table 3 we compare with previous results at 50k and 500k from Table 4 of [9]. We also note that with 500k schedules, and under 2s, our heuristic found the best-known solutions in more than 85% of instances, and in 99% it was within 1 time unit of the BKS. We also performed much longer runs to exploit the full power of all the algorithm components: up to 500M generated schedules within the concurrent memetic, though still only taking up to about 400 seconds, due to the concurrency, and the more efficient implementations (see Section 3). We were able to find 3 new best solutions; these have been added to PSPLIB, and so giving the only improvements to these multi-mode J30 instances since 2011.\nHence, we conclude that the methods here perform competitively on these multi-mode instances. We believe this gives evidence of the power and flexibility of the overall approach. This is particularly noteworthy, as our methods were originally designed for the more general multi-project case with the extra delay based objective, which has quite different properties, as already discussed in Section 4; the difference between minimising TPD and TMS can lead to a significant difference in the nature of the solutions, and reasonable algorithms. For example, both of the compared approaches [49, 9] used the successful forward-backward iterations, however our methods for did not include it, because it is not naturally applicable to the primary objective being TPD. Including it in our suite of improvement operators, might well improve performance for the special case of improving makespan.\nFinally, we remark that recent work on these multi-mode single project instances by [37] has considered the problem of selecting algorithms. This is perhaps the closest in spirit to our goals, in that the aim is to take multiple options for algorithms and then to use intelligent or machine learning techniques in order to make the selection. In contrast to a more traditional approach in which the exploration of the combinations of many components is done manually."
    }, {
      "heading" : "8. Conclusions",
      "text" : "This paper described the components of a multi-component hybrid approach to the MRCMPSP. Broadly, our algorithm is built on the serial generation scheme, in which sequences of activities are used to construct final schedules for their quality evaluation. We use a two-phase construct-and-improve method to search the space of activity sequences, with a variety of novel contributions, including algorithm mechanisms specifically designed to handle the multi-project structure and associated objectives functions.\nFirstly, the primary objective is a sum of completion times of the individual projects, and we give evidence and arguments that this tends to lead to an imbalance in the completion times; some projects finish much earlier than others. Hence, the nature of the problem itself is such that an approximate partial ordering of projects occurs in good solutions, and this is quite different from the structure expected with makespan as the primary objective. We expect such approximate partial ordering may well be common in real-world multi-project problems as well. Accordingly, in the construction phase, a novel Monte-Carlo Tree Search method is given that creates and selects initial solutions with an approximately-similar structure.\nSecondly, in the improvement phase, multiple novel neighbourhoods are included that are designed to work at the ‘project-level’ and so enable any needed changing of the approximate partial ordering of the projects. These are complemented with a wide range of neighbourhood moves also designed to work at lower levels within projects. The improvement phase itself is organised and controlled using a novel hybrid of a memetic algorithm and a hyper-heuristic, and in a fashion that furthermore makes effective use of a multi-core machine when it is available.\nPerhaps a distinguishing characteristic of the methods here is that there are far more neighbourhoods than in other submissions to the competition, or indeed the literature in general. From Table 6 in [62], other submissions have around 1–4 neighbourhoods but we have 13–17 (though many neighbourhoods have different variants, and are used in different stages, and so there is not a single meaningful count). At first this might seem to be a step towards extra complexity, and that might make the algorithm more difficult to use in real-world practice. However, in many problem domains there is a drive towards a larger number of neighbourhoods, for example, in realistic educational timetabling where 10–15 neighbourhoods is not uncommon (for example [28, and others]). Having many neighbourhoods is a natural reaction to the increasing complexity of the structure of the problem. They are designed to work on each level of the roughly hierarchical structure of many projects with each project containing many activities. We only eliminated neighbourhoods in a stage when they clearly never helped; instead preferring to keep neighbourhoods, with the intention that the diversity would help in robustness and effectiveness (though admittedly at the cost of ‘neatness’). Also, the objective itself is a type of weighted completion time and so is sensitive to the internal structure of\nsolutions. Hence, it seems reasonable that many neighbourhoods are needed; and this seems to be be borne out by the much better performance. Furthermore, in the case of scheduling using the serial generation there is an advantage and opportunity in that the neighbourhoods are mostly ‘lightweight’ to implement. They work by modifying the activity sequence, and when the majority of the runtime is spent on the evaluation of a sequence, then designing and efficiently implementing many different neighbourhoods is much less onerous then many other problem domains.\nAnother significant difference to other submissions to the challenge, and as described in [62], it seems that many others in the competition used forward-backward iteration (FBI) methods. We did not use it due to the extra complexity, and also uncertainty of how to best use it for the TPD (weighted completion time) rather than the standard makespan. Also, the results in [51] said it did not “distinguish between a good or bad performing metaheuristic”. However, it would be interesting future work to also include FBI moves; possibly with appropriate modification for the TPD objective.\nThe effectiveness of the resulting hybrid algorithm was demonstrated when it convincingly won the MISTA 2013 challenge, outperforming, on average, the other algorithms on 18 of the 20 instances [62]. The many-component and many-neighbourhood structure was a deliberate decision on our part, and was aimed at giving a more robust solver. In particular, part of the design goal was that the methods would be robust and so work well on the hidden instances, and it did indeed do particular well on those; ranking first on 9 out of the 10 hidden instances. We also observed that the performance was weakest (though still high ranking) on those instances with no local renewable resources. Such instances are rather special, and arguably less realistic, however, it does suggest that there is good potential for future work to investigate the links between features of the instances and the the algorithm performance. Nevertheless, Section 7.3 gives additional evidence of the robustness of our algorithm as even a much-reduced version performs well on the simple single-project but multi-mode instances from PSPLIB.\nThe broader contribution of this work is to give evidence for the general approach and methodology of investigating algorithms that consist of many components (such as many neighbourhoods, but also other aspects) and that aims to exploit the potential benefits of simple, but carefully controlled, interactions between them. It is possible that the algorithm could be simplified (e.g. by reducing the number of neighbourhood moves); however, we suspect that doing so is not worth the risk of it only working well on the seen set of instances, and would lose its robustness and hence performance on the unseen instances. Hence, we believe that the work gives evidence that individual components should not be prematurely discarded. Instead, ultimately, the decision as to which combination works best over a particular suite should be automated; that is, as a form of algorithm assembly, for example in the style of [1]. Of course, such assembly can also use a form of algorithm selection for the appropriate scheduling algorithm, based on the features of instances (e.g. [50, 37]).\nSuch ‘automated assembly’ work would be outside the scope of this particular paper; however, by providing a broad range of options and components that have the proven potential to give a successful and robust solver, this work hence gives a good foundation for such studies in the MRCMSP, and other variants of project scheduling, and arguably intelligently-coordinated metaheuristic methods in general.\nAcknowledgements This work was supported in part by EPSRC Grants EP/F033214/1\nand EP/H000968/1."
    } ],
    "references" : [ {
      "title" : "Automatic design of evolutionary algorithms for multi-objective combinatorial optimization",
      "author" : [ "Leonardo C.T. Bezerra", "Manuel López-Ibáñez", "Thomas Stützle" ],
      "venue" : "PPSN 2014,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Scheduling subject to resource constraints: classification and complexity",
      "author" : [ "J. Blazewicz", "J.K. Lenstra", "A.H.G.Rinnooy Kan" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1983
    }, {
      "title" : "A survey of Monte Carlo tree search methods",
      "author" : [ "C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton" ],
      "venue" : "Computational Intelligence and AI in Games, IEEE Transactions on, 4(1):1–43",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Resourceconstrained project scheduling: Notation, classification, models, and methods",
      "author" : [ "Peter Brucker", "Andreas Drexl", "Rolf Mohring", "Klaus Neumann", "Erwin Pesch" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1999
    }, {
      "title" : "Hyper-heuristics: A survey of the state of the art",
      "author" : [ "E.K. Burke", "M. Gendreau", "M. Hyde", "G. Kendall", "G. Ochoa", "E. Özcan", "R. Qu" ],
      "venue" : "Journal of the Operational Research Society",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Hyper-heuristics: An emerging direction in modern search technology",
      "author" : [ "E.K. Burke", "E. Hart", "G. Kendall", "J. Newall", "P. Ross", "S. Schulenburg" ],
      "venue" : "F. Glover and G. Kochenberger, editors, Handbook of Metaheuristics, pages 457–474. Kluwer",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A classification of hyper-heuristics approaches",
      "author" : [ "Edmund K. Burke", "Matthew Hyde", "Graham Kendall", "Gabriela Ochoa", "Ender Özcan", "John R. Woodward" ],
      "venue" : "Handbook of Metaheuristics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "An efficient hybrid algorithm for resource-constrained project scheduling",
      "author" : [ "Wang Chen", "Yan-jun Shi", "Hong-fei Teng", "Xiao-ping Lan", "Li-chen Hu" ],
      "venue" : "Information Sciences,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Multi-mode resource-constrained project scheduling using rcpsp and sat solvers",
      "author" : [ "J. Coelho", "M. Vanhoucke" ],
      "venue" : "European Journal of Operational Research, 213(1):73–82",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A comprehensive survey of evolutionary-based multiobjective optimization techniques",
      "author" : [ "Carlos A. Coello Coello" ],
      "venue" : "Knowledge and Information Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Differential evolution for solving multi-mode resource-constrained project scheduling problems",
      "author" : [ "N. Damak", "B. Jarboui", "P. Siarry", "T. Loukil" ],
      "venue" : "Comput. Oper. Res., 36(9):2653–2659",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "RanGen: A random network generator for activity-on-the-node networks",
      "author" : [ "Erik Demeulemeester", "Mario Vanhoucke", "Willy Herroelen" ],
      "venue" : "Journal of Scheduling,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "Crossover can provably be useful in evolutionary computation",
      "author" : [ "Benjamin Doerr", "Edda Happ", "Christian Klein" ],
      "venue" : "In Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "A hybrid rank-based evolutionary algorithm applied to multi-mode resource-constrained project scheduling problem",
      "author" : [ "Sonda Elloumi", "Philippe Fortemps" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Exploring statistical and population aspects of network complexity",
      "author" : [ "Frank Emmert-Streib", "Matthias Dehmer" ],
      "venue" : "PLoS One,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Iterated variable neighborhood search for the resource constrained multi-mode multi-project scheduling problem",
      "author" : [ "Martin Josef Geiger" ],
      "venue" : "CoRR, abs/1310.0602,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Multiobjective metaheuristic algorithms for the resource-constrained project scheduling problem with precedence relations",
      "author" : [ "Helton Cristiano Gomes", "Francisco de Assis das Neves", "Marcone Jamilson Freitas Souza" ],
      "venue" : "Computers & Operations Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "A genetic algorithm for the resource constrained multi-project scheduling problem",
      "author" : [ "J.F. Gonçalves", "J.J.M. Mendes", "M.G.C. Resende" ],
      "venue" : "European Journal of Operational Research, 189(3):1171– 1190",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A memetic algorithm for the generalized traveling salesman problem",
      "author" : [ "Gregory Gutin", "Daniel Karapetyan" ],
      "venue" : "Natural Computing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "A survey of variants and extensions of the resource-constrained project scheduling problem",
      "author" : [ "Sönke Hartmann", "Dirk Briskorn" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Project scheduling under uncertainty: Survey and research potentials",
      "author" : [ "Willy Herroelen", "Roel Leus" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2005
    }, {
      "title" : "Resource-constrained project scheduling: A survey of recent developments",
      "author" : [ "Willy Herroelen", "Bert De Reyck", "Erik Demeulemeester" ],
      "venue" : "Computers & Operations Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    }, {
      "title" : "Minimizing the complexity of an activity",
      "author" : [ "Jerzy Kamburowski", "David J. Michael", "Matthias F.M. Stallmann" ],
      "venue" : "network. Networks,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2000
    }, {
      "title" : "A new approach to population sizing for memetic algorithms: a case study for the multidimensional assignment problem",
      "author" : [ "Daniel Karapetyan", "Gregory Gutin" ],
      "venue" : "Evolutionary computation,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Multi-stage Hyper-heuristics for Optimisation Problems",
      "author" : [ "Ahmed Kheiri" ],
      "venue" : "PhD thesis, University of Nottingham, School of Computer Science,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "An iterated multi-stage selection hyper-heuristic",
      "author" : [ "Ahmed Kheiri", "Ender Özcan" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "HySST: Hyper-heuristic search strategies and timetabling",
      "author" : [ "Ahmed Kheiri", "Ender Özcan", "Andrew J. Parkes" ],
      "venue" : "In Proceedings of the Ninth International Conference on the Practice and Theory of Automated Timetabling (PATAT",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "A stochastic local search algorithm with adaptive acceptance for high-school timetabling",
      "author" : [ "Ahmed Kheiri", "Ender Özcan", "Andrew J. Parkes" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "PSPLIB - a project scheduling problem library : OR software - ORSEP operations research software exchange program",
      "author" : [ "Rainer Kolisch", "Arno Sprecher" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1997
    }, {
      "title" : "Characterization and generation of a general class of resource-constrained project scheduling problems",
      "author" : [ "Rainer Kolisch", "Arno Sprecher", "Andreas Drexl" ],
      "venue" : "Management Science,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1995
    }, {
      "title" : "A particle swarm optimization based hyper-heuristic algorithm for the classic resource constrained project scheduling problem",
      "author" : [ "Georgios Koulinas", "Lazaros Kotsikas", "Konstantinos Anagnostopoulos" ],
      "venue" : "Information Sciences,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "A heuristic solution framework for the resource constrained (multi-)project scheduling problem with sequence-dependent transfer times",
      "author" : [ "Doreen Krüger", "Armin Scholl" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "MILP formulations for single- and multi-mode resource-constrained project scheduling problems",
      "author" : [ "Thomas S. Kyriakidis", "Georgios M. Kopanos", "Michael C. Georgiadis" ],
      "venue" : "Computers & Chemical Engineering,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    }, {
      "title" : "An iterative scheduling technique for resource-constrained project scheduling",
      "author" : [ "K.Y. Li", "R.J. Willis" ],
      "venue" : "European Journal of Operational Research, 56(3):370 – 379",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "A multicriteria heuristic method to improve resource allocation in multiproject scheduling",
      "author" : [ "Antonio Lova", "Concepci??n Maroto", "Pilar Tormos" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2000
    }, {
      "title" : "A random key based genetic algorithm for the resource constrained project scheduling problem",
      "author" : [ "J.J.M. Mendes", "J.F. Gon?alves", "M.G.C. Resende" ],
      "venue" : "Computers & Operations Research,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2009
    }, {
      "title" : "An automatic algorithm selection approach for the multi-mode resource-constrained project scheduling problem",
      "author" : [ "Tommy Messelis", "Patrick De Causmaecker" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2014
    }, {
      "title" : "When will a genetic algorithm outperform hill climbing",
      "author" : [ "Melanie Mitchell", "John H. Holland", "Stephanie Forrest" ],
      "venue" : null,
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1993
    }, {
      "title" : "Solving project scheduling problems by minimum cut computations",
      "author" : [ "Rolf H. Möhring", "Andreas S. Schulz", "Frederik Stork", "Marc Uetz" ],
      "venue" : "Management Science,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2003
    }, {
      "title" : "On evolution, search, optimization, genetic algorithms and martial arts: Towards memetic algorithms. Caltech concurrent computation program",
      "author" : [ "Pablo Moscato" ],
      "venue" : "C3P Report,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 1989
    }, {
      "title" : "A memetic approach for the traveling salesman problem implementation of a computational ecology for combinatorial optimization on message-passing systems",
      "author" : [ "Pablo Moscato", "Michael G Norman" ],
      "venue" : "Parallel Computing and Transputer Applications,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1992
    }, {
      "title" : "Ying Zhao",
      "author" : [ "M.W. Moskewicz", "C.F. Madigan" ],
      "venue" : "Lintao Zhang, and S. Malik. Chaff: engineering an efficient sat solver. In Design Automation Conference, 2001. Proceedings, pages 530–535",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Classification of adaptive memetic algorithms: A comparative study",
      "author" : [ "Yew-Soon Ong", "Meng-Hiot Lim", "Ning Zhu", "Kok-Wai Wong" ],
      "venue" : "Trans. Sys. Man Cyber. Part B,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2006
    }, {
      "title" : "A comprehensive analysis of hyperheuristics",
      "author" : [ "Ender Özcan", "Burak Bilgin", "Emin Erkan Korkmaz" ],
      "venue" : "Intelligent Data Analysis,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2008
    }, {
      "title" : "A hyper-heuristic based on random gradient, greedy and dominance",
      "author" : [ "Ender Özcan", "Ahmed Kheiri" ],
      "venue" : "Computer and Information Sciences II,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2012
    }, {
      "title" : "The interleaved constructive memetic algorithm and its application to timetabling",
      "author" : [ "Ender Özcan", "Andrew J. Parkes", "Alpay Alkan" ],
      "venue" : "Comput. Oper. Res.,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2012
    }, {
      "title" : "A survey on the resource-constrained project scheduling problem",
      "author" : [ "Linet Özdamar", "Gündüz Ulusoy" ],
      "venue" : "IIE Transactions,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1995
    }, {
      "title" : "A genetic algorithm for the preemptive and nonpreemptive multi-mode resource-constrained project scheduling problem",
      "author" : [ "Vincent Van Peteghem", "Mario Vanhoucke" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2010
    }, {
      "title" : "Using resource scarceness characteristics to solve the multi-mode resource-constrained project scheduling problem",
      "author" : [ "Vincent Van Peteghem", "Mario Vanhoucke" ],
      "venue" : "Journal of Heuristics,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2011
    }, {
      "title" : "An experimental investigation of metaheuristics for the multi-mode resource-constrained project scheduling problem on new dataset instances",
      "author" : [ "Vincent Van Peteghem", "Mario Vanhoucke" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2014
    }, {
      "title" : "On the use of the complexity index as a measure of complexity in activity networks",
      "author" : [ "Bert De Reyck", "Willy Herroelen" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 1996
    }, {
      "title" : "Hyper-heuristics",
      "author" : [ "P. Ross" ],
      "venue" : "E. K. Burke and G. Kendall, editors, Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques, chapter 17, pages 529–556. Springer",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Solving resource-constrained project scheduling problem with evolutionary programming",
      "author" : [ "M H Sebt", "Y Alipouri", "Y Alipouri" ],
      "venue" : "Journal of the Operational Research Society, 64:1327–1335",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Crossover speeds up building-block assembly",
      "author" : [ "Dirk Sudholt" ],
      "venue" : "In Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2012
    }, {
      "title" : "An integer programming approach to a generalized project scheduling problem",
      "author" : [ "Túlio Toffolo", "Haroldo Santos", "Marco Carvalho", "Janniele Soares", "Tony Wauters", "Greet Vanden Berghe" ],
      "venue" : "In Matheuristics. Hamburg,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2014
    }, {
      "title" : "An overview of recent research results and future research avenues using simulation studies in project management",
      "author" : [ "Mario Vanhoucke" ],
      "venue" : "ISRN Computational Mathematics,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2013
    }, {
      "title" : "An exact procedure for the resourceconstrained weighted earliness?tardiness project scheduling problem",
      "author" : [ "Mario Vanhoucke", "Erik Demeulemeester", "Willy Herroelen" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2001
    }, {
      "title" : "An effective shuffled frog-leaping algorithm for multi-mode resourceconstrained project scheduling problem",
      "author" : [ "Ling Wang", "Chen Fang" ],
      "venue" : "Information Sciences,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2011
    }, {
      "title" : "An effective estimation of distribution algorithm for the multi-mode resource-constrained project scheduling problem",
      "author" : [ "Ling Wang", "Chen Fang" ],
      "venue" : "Computers & Operations Research,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2012
    }, {
      "title" : "The multi-mode resource-constrained multi-project scheduling problem",
      "author" : [ "Tony Wauters", "Joris Kinable", "Pieter Smet", "Wim Vancroonenburg", "Greet Vanden Berghe", "Jannes Verstichel" ],
      "venue" : "Journal of Scheduling,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2016
    }, {
      "title" : "A learningbased optimization approach to multi-project scheduling",
      "author" : [ "Tony Wauters", "Katja Verbeeck", "Patrick De Causmaecker", "Greet Vanden Berghe" ],
      "venue" : "Journal of Scheduling,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2015
    }, {
      "title" : "A learning metaheuristic for the multi mode resource constrained project scheduling problem. In Learning and Intelligent OptimizatioN (LION",
      "author" : [ "Tony Wauters", "Jannes Verstichel", "Katja Verbeeck", "Greet Vanden Berghe" ],
      "venue" : null,
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2009
    }, {
      "title" : "Project scheduling with finite or infinite number of activity processing modes - a survey",
      "author" : [ "Jan Weglarz", "Joanna Józefowska", "Marek Mika", "Grzegorz Waligóra" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "see [4, 22, 21, 48, 39, 20, 65].",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "see [4, 22, 21, 48, 39, 20, 65].",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : "see [4, 22, 21, 48, 39, 20, 65].",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 46,
      "context" : "see [4, 22, 21, 48, 39, 20, 65].",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 38,
      "context" : "see [4, 22, 21, 48, 39, 20, 65].",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "see [4, 22, 21, 48, 39, 20, 65].",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 62,
      "context" : "see [4, 22, 21, 48, 39, 20, 65].",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "These problems have been proven to be NP-hard [2], and a well-known benchmark suite, PSPLIB, is provided in [29].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 28,
      "context" : "These problems have been proven to be NP-hard [2], and a well-known benchmark suite, PSPLIB, is provided in [29].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 32,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 56,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 58,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 13,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 228,
      "endOffset" : 240
    }, {
      "referenceID" : 52,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 228,
      "endOffset" : 240
    }, {
      "referenceID" : 16,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 228,
      "endOffset" : 240
    }, {
      "referenceID" : 30,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 7,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 286,
      "endOffset" : 293
    }, {
      "referenceID" : 57,
      "context" : "Recent works on the MRCPSP range from exact approaches, such as, MILP [33], and branch-and-bound [59], to metaheuristics, such as, differential evolution [11], estimation of distribution algorithms [61], evolutionary algorithms [14, 55, 17], swarm intelligence methods [31], and others [8, 60].",
      "startOffset" : 286,
      "endOffset" : 293
    }, {
      "referenceID" : 59,
      "context" : "The full description of this problem domain can be found on the competition website and in [62]; however, for completeness we also summarise it in Section 2.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 34,
      "context" : "a survey [35] found that “84% of the companies which responded to the survey indicated that they worked with multiple projects”.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 34,
      "context" : "see [35, 18, 32, 36].",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "see [35, 18, 32, 36].",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 31,
      "context" : "see [35, 18, 32, 36].",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 35,
      "context" : "see [35, 18, 32, 36].",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "see [3].",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "The search process during the improvement phase is carefully controlled by a combination of methods arising from a standard metaheuristic, namely memetic algorithm, and also an extension of existing hyper-heuristic components [27, 46].",
      "startOffset" : 226,
      "endOffset" : 234
    }, {
      "referenceID" : 44,
      "context" : "The search process during the improvement phase is carefully controlled by a combination of methods arising from a standard metaheuristic, namely memetic algorithm, and also an extension of existing hyper-heuristic components [27, 46].",
      "startOffset" : 226,
      "endOffset" : 234
    }, {
      "referenceID" : 34,
      "context" : "A discussion and a computational study on both approaches can be found in [35].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : ", see [4]).",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 59,
      "context" : "As is common for heuristic approaches [62], we preferred the sequence-based representation, since it provides the ease of producing schedules that are both feasible and for which no activity can be moved to an earlier time without moving some other activities (the schedule is then said to be ‘active’).",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : "An important part of development of SAT solvers was the development of ‘watched literals’ [42].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 60,
      "context" : "Similar effects are also observed in [63].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : "see [10], and of course could study the effect of different weights for the completion times.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 2,
      "context" : "The problem then is how to quickly select good partition of the projects, and the method we selected is a version of Monte-Carlo Tree Search (MCTS) methods [3].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 59,
      "context" : "Some of the moves are similar to those used by other submissions; see [62], and [16] and [57] for few examples.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "Some of the moves are similar to those used by other submissions; see [62], and [16] and [57] for few examples.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 54,
      "context" : "Some of the moves are similar to those used by other submissions; see [62], and [16] and [57] for few examples.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "However, the usefulness of recombination is still under debate in the research community [13, 38].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 37,
      "context" : "However, the usefulness of recombination is still under debate in the research community [13, 38].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 53,
      "context" : "A recent study showed that recombination can be useful at a certain stage during the search process, if the mutations do not change the quality of resultant individuals leading to a population containing different individuals with the same fitness [56].",
      "startOffset" : 248,
      "endOffset" : 252
    }, {
      "referenceID" : 39,
      "context" : "A memetic algorithm (MA) hybridises a genetic algorithm with local search which is commonly applied after mutation on the new individuals [40, 41].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 40,
      "context" : "A memetic algorithm (MA) hybridises a genetic algorithm with local search which is commonly applied after mutation on the new individuals [40, 41].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "Many improvements for MAs have been suggested, for example the population sizing [24] and 20",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 45,
      "context" : "interleaved mode of operation [47].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "MAs have been successfully applied to many different problems ranging from generalised travelling salesman [19] to nurse rostering [44].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 42,
      "context" : "Our MA is based on quantitative adaptation at a local level according to the classification in [43].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "move operators) rather than the solutions in problem solving [5].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "There are two common types of hyper-heuristics in the scientific literature [7]: selection methodologies that choose/mix heuristics from a set of preset low-level heuristics (which can both improve or worsen the solution) and attempt to control those heuristics during the search process; and generation methodologies that aim to build new heuristics from a set of preset components.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "More on different types of hyper-heuristics, their components and application domains can be found in [5, 6, 53, 45].",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "More on different types of hyper-heuristics, their components and application domains can be found in [5, 6, 53, 45].",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 51,
      "context" : "More on different types of hyper-heuristics, their components and application domains can be found in [5, 6, 53, 45].",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 43,
      "context" : "More on different types of hyper-heuristics, their components and application domains can be found in [5, 6, 53, 45].",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 44,
      "context" : "The approach extends the heuristic selection and move acceptance methods introduced in [46] and [28], respectively.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 27,
      "context" : "The approach extends the heuristic selection and move acceptance methods introduced in [46] and [28], respectively.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 44,
      "context" : "LLH ⊆ LLHall is formed based on the idea of a dominance-based heuristic selection as introduced in [46] reflecting the trade-off between the objective value achieved by each low-level heuristic and number of steps involved.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : "More details on the components of the proposed hyper-heuristic can be found in [26, 25].",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "More details on the components of the proposed hyper-heuristic can be found in [26, 25].",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 59,
      "context" : "For additional analysis of the performance of our and other submitted algorithms we refer to [62].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 28,
      "context" : "All the three sets of instances were produced by combining PSPLIB benchmark instances [29] and can be downloaded from allserv.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 59,
      "context" : "The detailed discussion of the ranking scheme and the analysis of the competition results are reported in [62].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 59,
      "context" : "[62] note the consistency of the performance of our algorithm, and also provide ranks with several different ranking methodologies, showing that our approach would win the competition even if the rules of the competition were different.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 55,
      "context" : "parallel’ nature [58].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 48,
      "context" : "It would obviously be interesting to link together the performance of the algorithm(s) with the features of the instances; for example, see [50, 37] and others.",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 36,
      "context" : "It would obviously be interesting to link together the performance of the algorithm(s) with the features of the instances; for example, see [50, 37] and others.",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 29,
      "context" : "There are many potential features that could be used to characterise the instances, and these have often been used as part of the process of generating instances, for example [30, 12].",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 11,
      "context" : "There are many potential features that could be used to characterise the instances, and these have often been used as part of the process of generating instances, for example [30, 12].",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 50,
      "context" : "For example, one can characterise the precedence graph in various ways [52, 23, 15] and associated measures such as ‘complexity’ and ‘Series-Parallel’ indices could also be included.",
      "startOffset" : 71,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "For example, one can characterise the precedence graph in various ways [52, 23, 15] and associated measures such as ‘complexity’ and ‘Series-Parallel’ indices could also be included.",
      "startOffset" : 71,
      "endOffset" : 83
    }, {
      "referenceID" : 14,
      "context" : "For example, one can characterise the precedence graph in various ways [52, 23, 15] and associated measures such as ‘complexity’ and ‘Series-Parallel’ indices could also be included.",
      "startOffset" : 71,
      "endOffset" : 83
    }, {
      "referenceID" : 61,
      "context" : "However, we can make some initial observations by analysing the instance properties in Table 1 and the results in Table 2, and in more detail in Table 33 of the final competition summary [64].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 28,
      "context" : "However, it is also of interest to study the methods on other scheduling benchmarks; in particular, the PSPLIB library of instances [29] has been widely used and studied in the academic scheduling community.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 34,
      "context" : "Unfortunately, despite it being long known that the majority of real world problems are multi-project [35], it seems",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 47,
      "context" : "[49] used a Genetic algorithm and also Forward-Backward Iterations (FBI) [34] along with a simple method to repair infeasible mode choices.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[49] used a Genetic algorithm and also Forward-Backward Iterations (FBI) [34] along with a simple method to repair infeasible mode choices.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "[9] again used FBI along with a more expensive SAT-based method to ensure that only feasible mode decisions were made (recently, [51] reported that this was the best method on these instances).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 49,
      "context" : "[9] again used FBI along with a more expensive SAT-based method to ensure that only feasible mode decisions were made (recently, [51] reported that this was the best method on these instances).",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "Hence, in Table 3 we compare with previous results at 50k and 500k from Table 4 of [9].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 47,
      "context" : "For example, both of the compared approaches [49, 9] used the successful forward-backward iterations, however our methods for did not include it, because it is not naturally applicable to the primary objective being TPD.",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "For example, both of the compared approaches [49, 9] used the successful forward-backward iterations, however our methods for did not include it, because it is not naturally applicable to the primary objective being TPD.",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 36,
      "context" : "Finally, we remark that recent work on these multi-mode single project instances by [37] has considered the problem of selecting algorithms.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 59,
      "context" : "From Table 6 in [62], other submissions have around 1–4 neighbourhoods but we have 13–17 (though many neighbourhoods have different variants, and are used in different stages, and so there is not a single meaningful count).",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 59,
      "context" : "Another significant difference to other submissions to the challenge, and as described in [62], it seems that many others in the competition used forward-backward iteration (FBI) methods.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 49,
      "context" : "Also, the results in [51] said it did not “distinguish between a good or bad performing metaheuristic”.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 59,
      "context" : "The effectiveness of the resulting hybrid algorithm was demonstrated when it convincingly won the MISTA 2013 challenge, outperforming, on average, the other algorithms on 18 of the 20 instances [62].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "Instead, ultimately, the decision as to which combination works best over a particular suite should be automated; that is, as a form of algorithm assembly, for example in the style of [1].",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 48,
      "context" : "[50, 37]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 36,
      "context" : "[50, 37]).",
      "startOffset" : 0,
      "endOffset" : 8
    } ],
    "year" : 2016,
    "abstractText" : "Multi-mode resource and precedence-constrained project scheduling is a well-known challenging real-world optimisation problem. An important variant of the problem requires scheduling of activities for multiple projects considering availability of local and global resources while respecting a range of constraints. A critical aspect of the benchmarks addressed in this paper is that the primary objective is to minimise the sum of the project completion times, with the usual makespan minimisation as a secondary objective. We observe that this leads to an expected different overall structure of good solutions and discuss the effects this has on the algorithm design. This paper presents a carefully designed hybrid of Monte-Carlo tree search, novel neighbourhood moves, memetic algorithms, and hyper-heuristic methods. The implementation is also engineered to increase the speed with which iterations are performed, and to exploit the computing power of multicore machines. Empirical evaluation shows that the resulting information-sharing multi-component algorithm significantly outperforms other solvers on a set of “hidden” instances, i.e. instances not available at the algorithm design phase.",
    "creator" : "LaTeX with hyperref package"
  }
}