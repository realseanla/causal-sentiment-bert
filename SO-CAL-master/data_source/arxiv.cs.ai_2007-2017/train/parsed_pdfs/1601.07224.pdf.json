{
  "name" : "1601.07224.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "yuraperov@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Федеральное государственное автономное образовательное учреждение\nвысшего профессионального образования\n«СИБИРСКИЙ ФЕДЕРАЛЬНЫЙ УНИВЕРСИТЕТ»\nИнститут математики и фундаментальной информатики\nБазовая кафедра математического моделирования и процессов управления\nБАКАЛАВРСКАЯ РАБОТА\nНаправление 010101.62 Математика\nПОРОЖДАЮЩЕЕ ВЕРОЯТНОСТНОЕ ПРОГРАММИРОВАНИЕ\nВыпускник – Юрий Перов 1 .\nНаучный руководитель в СФУ – Татьяна Валерьевна Крупкина.\nКембридж-Оксфорд-Красноярск 2012–2014\n1 Адрес для корреспонденции: yuraperov@gmail.com\n_ _ _ _ _ _ _ _ _\n/\nar X\niv :1\n60 1.\n07 22\n4v 1\n[ cs\n.A I]\n2 6\nJa n\n20 16\nРЕФЕРАТ\nВыпускная квалификационная бакалаврская работа по теме «Порождающее веро-\nятностное программирование» содержит 48 страниц текста, 50 использованных источников, 12 рисунков.\nКлючевые слова: ВЕРОЯТНОСТНОЕ ПРОГРАММИРОВАНИЕ, МАШИННОЕ\nОБУЧЕНИЕ, ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ВЕРОЯТНОСТНЫЕ МОДЕЛИ, АВТОМАТИЧЕСКОЕ МОДЕЛИРОВАНИЕ.\nРабота посвящена новому направлению в области машинного обучения и компью-\nтерных наук — вероятностному программированию. В работе дается краткое реферативное введение в языки вероятностного программирования Church/Venture/Anglican, а также описываются результаты первых экспериментов по автоматической генерации вероятностных программ.\nСОДЕРЖАНИЕ\nВведение 4"
    }, {
      "heading" : "1 Краткое введение в языки вероятностного программирования Church,",
      "text" : "Venture и Anglican 8 1.1 Первое знакомство с Church, Venture, Anglican . . . . . . . . . . . . . . . . . 8 1.2 Статистический вывод в вероятностных языках программирования с помо-\nщью алгоритма Метрополиса-Гастингса . . . . . . . . . . . . . . . . . . . . . 11 1.2.1 Метод «выборки с отклонением» . . . . . . . . . . . . . . . . . . . . . 11 1.2.2 Пространство историй выполнений вероятностных программ . . . . . 12 1.2.3 Апостериорное распределение историй выполнений программ . . . . . 14 1.2.4 Использование методов Монте-Карло по схеме Марковских цепей . . 15 1.2.5 Программная реализация статистического вывода . . . . . . . . . . . 19\n1.3 Эффективность вывода . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1.4 Порождающее вероятностное программирование в распознавании образов . 24 1.5 О различиях между Church, Venture, Anglican . . . . . . . . . . . . . . . . . . 26\n2 Автоматическая генерация вероятностных программ 27\n2.1 Обзор литературы . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.2 Описание подхода . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.3 Грамматика и порождающие правила . . . . . . . . . . . . . . . . . . . . . . . 33 2.4 Вероятности использования порождающих правил . . . . . . . . . . . . . . . 35 2.5 Эксперименты . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.5.1 Выборки из сгенерированных вероятностных программ . . . . . . . . 36 2.5.2 Вывод вероятностных программ, определяющих распределения, сов-\nпадающие или приближающие классические одномерные распределения . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n2.5.3 Обобщение произвольных эмпирических данных с помощью порож-\nдающего вероятностного программирования . . . . . . . . . . . . . . . 37\n2.5.4 «Компиляция» вероятностных программ . . . . . . . . . . . . . . . . . 39\n2.6 Обобщение порождающего вероятностного программирования . . . . . . . . 41\nЗаключение 44\nСписок использованных источников 45\nМоей любимой Маше и всем другим моим любимым родным\nВВЕДЕНИЕ\nДанная бакалаврская работа посвящена вероятностному программированию [1], но-\nвому направлению в областях машинного обучения, искусственного интеллекта и компьютерных наук, а именно реферативному краткому введению в вероятностное программирование, описанию языков вероятностного программирования Church [2], Venture [3] и Anglican [4], описанию подхода порождающего вероятностного программирования для решения задач распознавания образов [5], а также представлению полученных предварительных результатов по автоматизации вывода вероятностных моделей для вероятностного программирования [6].\nОписание структуры работы\nПервая часть данной работы начинается с общих сведений о вероятностном программировании, а затем в ней кратко описываются результаты, полученные коллегами автора и им самим в работе над научными проектами на протяжении двух лет в Массачусетском технологическом инситуте под руководством профессора Джошуа Тененбаума и доктора Викаша Мансингхи (Кембридж, штат Массачусетс, США) и Оксфордском университете под руководством профессора Френка Вуда (Великобритания, Оксфорд). Эта часть работы, являясь по сути переводом, представляет собой реферативную выдержку о вероятностном программировании и его приложениях на русском языке. Насколько автору известно, литературы о вероятностном программировании на русском языке практически нет, поэтому он надеется, что данная чисто реферативная часть работы принесет существенную пользу русскоговорящему научному сообществу, а особенно заинтересованным студентам и школьникам, которые впервые захотят познакомиться с развивающимся направлением вероятностного программирования.\nВо второй части данной работы автор описывает новые результаты в области ве-\nроятностного программирования, связанные с автоматизированным или полуавтоматизированным выводом вероятностных моделей для вероятностного программирования, полученные во время стажировки автора в Департаменте технических наук Оксфордского\nуниверситета в научной лаборатории профессора Френка Вуда и под его руководством. Вторая часть завершается рассуждениями автора об обобщении автоматизированного изучения и вывода компьютером вероятностных моделей в виде вероятностных программ, то есть о возможностях порождающего вероятностного программирования.\nВероятностное программирование\nВероятностное программирование можно определить как компактный, композиционный способ представления порождающих вероятностных моделей и проведения статистического вывода в них с учетом данных с помощью обобщенных алгоритмов.\nВероятностная модель является важным понятием машинного обучения, одного из\nосновных направлений искусственного интеллекта на сегодняшний день. В общем случае в рамках теории машинного обучения перед компьютером ставится задача произвести какое-то действие yi на основе входных данных xi, априорных знаний и возможности взаимодействовать со средой. Без ограничений общности целевым действием для компьютера можно считать производство ответа в виде выходных данных, представленных в виде информации. Например, в робототехнике эта информация может являться инструкциями для моторов и механических устройств для выполнения тех или иных физических действий роботом.\nПри использовании различных подходов в рамках машинного обучения использу-\nются модели, которые являются формальным «описанием компонентов и функций, отображающих существенные свойства моделируемого объема или процесса» [7]. В рамках машинного обучения используются вероятностные модели, так как свойства, элементы и связи между ними являются не фиксированными, а стохастическими.\nКлассическим примером одного из подходов в машинном обучении является способ\n«обучение с учителем», когда существует N прецедентов, то есть пар входных и выходных данных обучающей выборки X = {xi}, Y = {yi}, i = 1, N , и необходимо найти алгоритм описывающий зависимость между xi и yi, то есть алгоритм F , который позволяет на основе каждого элемента входных данных xi получать абсолютно или достаточно точный элемент выходных данных yi, то есть F (xi) → yi. С помощью данного алгоритма затем для M известных элементов входных данных {xi}, j = N + 1, N +M находятся значения M неизвестных элементов выходных данных {yi}, j = N + 1, N +M . В рамках машинного обучения эта проблема, в том числе, решается с помощью методов регрессионного\nанализа.\nВ только что приведенной как пример модели неизвестными (скрытыми) парамет-\nрами T = {tj}, j = 1, K будет информация о характеристиках модели F , которые подлежат выводу при данной обучающей выборке в виде пар (xi, yi).\nПриведем простой пример: в линейной регрессии (при L независимых переменных)\nзначениями независимых переменных будут xi,j, значениями зависимой переменной будут yi, параметрами модели будут t1, t2, . . . , tL+1, а алгоритмом F будет\nyi = Ft1,...,tL+1(xi) = t1 + t2 · xi,1 + . . .+ tL+1 · xi,L.\nДля простоты без ограничений общности будем считать, что мы можем всегда по-\nлучить yi детерминированно, зная T и xi.\nВ порождающих вероятностных моделях задается совместное распределение веро-\nятностей P (T,X), обычно сначала путем задания априорного распределения P (T ), а затем задания условного распределения P (X | T ). Это и называется моделью.\nПри заданной модели и известных X задачей будет являться поиск апостериор-\nного распределения на T , таким образом P (T | X). Одним из способов поиска данного апостериорного распределения является применение теоремы Байеса:\nP (T | X) = P (T )P (X | T ) P (X) ,\nгде P (X) теоретически можно найти как ∫ P (T )P (X | T ) dT , но при решении практических задач это часто невозможно, так как перебор всего пространства T не поддается аналитическому решению или решению с помощью численных методов за разумное время. Поэтому чаще всего при решении задач в рамках Байесовского подхода стоит задача поиска ненормированного значения P (T | X):\nP (T | X) ∝ P (T )P (X | T ),\nгде символ ∝, часто встречающийся в зарубежной литературе, но очень редко встречающийся у нас, означает «пропорционально». Нормировочную константу затем можно найти приближенно, но иногда ее значение даже не вычисляют, так как бывает достаточно найти и работать дальше с наиболее вероятными элементами T̂ из апостериорного распределения\nP (T | X).\nОбычно, особенно при решении практических задач с большим объемом данных\nи в рамках сложных моделей, апостериорное распределение P (T | X) находят не точно, а с помощью приближенных методов, в том числе с помощью методов Монте-Карло [8], которые позволяют сгенерировать выборку из интересующего нас распределения.\nКак отмечалось в самом начале данного подраздела, вероятностное программиро-\nвание позволяет:\n1. Композиционно и компактно записывать порождающую вероятностную модель\nс помощью задания априорных вероятностей P (T ) и P (X | T ) в виде алгоритма (вероятностной программы) с использованием стохастических функций (например, стохастическая функция Normal(µ, σ)) и вспомогательных детерминированных функций (например, + или ∗). 2. Снабжать модель данными X̂, таким образом теоретически определяя условное\nраспределение P (T | X̂).\n3. Производить статистический вывод для генерации выборки из условного распре-\nделения с помощью обобщенных алгоритмов статистического вывода, подходящих для всех или большого множества моделей. Для ознакомления с машинным обучением и искусственным интеллектом автор\nрекомендует следующие источники: [9, 10, 11]. Информацию о вероятностных моделях и Байесовских методах на русском языке можно найти в [12].\n1 Краткое введение в языки вероятностного программирования\nChurch, Venture и Anglican\nСуществует более 15 языков вероятностного программирования, перечень с кратким описанием каждого из них можно найти на [13]. В данной работе реферативно будут рассмотрены три языка вероятностного программирования: Church [2], Venture [3] и Anglican [4]. Языки Venture и Anglican являются продолжениями языка Church. Church в свою очередь основан на языке «обычного» программирования Lisp и Scheme. Заинтересованному читателю крайне рекомендуется ознакомиться с книгой [14], являющейся одним из лучших способов начать знакомство с языком «обычного» программирования Scheme."
    }, {
      "heading" : "1.1 Первое знакомство с Church, Venture, Anglican",
      "text" : "На текущий момент любой язык вероятностного программирования напрямую связан с методами статистического вывода, которые используются в нем, поэтому часто язык ассоциируется с платформой вероятностного программирования, то есть с его технической реализацией в виде компьютерной программы.\nРассмотрим задание простой вероятностной модели Байесовской линейной регрес-\nсии [10] на языке вероятностного программирования Venture/Anglican [15] в виде вероятностной программы:\n1 [ASSUME t1 ( normal 0 1 ) ] 2 [ASSUME t2 ( normal 0 1 ) ] 3 [ASSUME no i s e 0 . 0 1 ] 4 [ASSUME noisy_x ( lambda ( time ) ( normal (+ t1 (∗ t2 time ) ) no i s e ) ) ] 5 [OBSERVE ( noisy_x 1 . 0 ) 1 0 . 3 ] 6 [OBSERVE ( noisy_x 2 . 0 ) 1 1 . 1 ] 7 [OBSERVE ( noisy_x 3 . 0 ) 1 1 . 9 ] 8 [PREDICT t1 ] 9 [PREDICT t2 ]\n10 [PREDICT ( noisy_x 4 . 0 ) ]\nСкрытые искомые параметры — значения коэффициентов t1 и t2 линейной функции\nx (time) = t1 + t2 · time. У нас есть априорные предположения о данных коэффициентах,\nа именно мы предполагаем, что они распределены по закону нормального распределения Normal(0, 1) со средним 0 и стандартным отклонением 1. Таким образом, мы определили в первых двух строках вероятностной программы вероятность P (T ), описанную в предыдущем раздел. Инструкцию [ASSUME name expression] можно рассматривать как определение случайной величины с именем name, принимающей значение вычисляемого выражение (программного кода) expression, которое содержит в себе неопределенность.\nВероятностные языки программирования (здесь и далее будут иметься в виду кон-\nкретно Church, Venture, Anglican, если не указано иное), как и Lisp/Scheme, являются функциональными языками программирования, и используют польскую нотацию1 при записи выражений для вычисления. Это означает, что в выражении вызова функции сначала располагается оператор, а уже только потом аргументы: (+ 1 2), и вызов функции обрамляется круглыми скобками. На других языках программирования, таких как C++ или Python, это будет эквивалентно коду 1 + 2.\nВ вероятностных языках программирования выражение вызова функции принято\nразделять на три разных вида:\n1. Вызов детерминированных процедур (primitive-procedure arg1 ...argN), ко-\nторые при одних и тех же аргументах всегда возвращают одно и то же значение. К таким процедурам, например, относятся арифметические операции. 2. Вызов вероятностных (стохастических) процедур (stochastic-procedure arg1\n...argN), которые при каждом вызове генерируют случайным образом элемент из соответствующего распределения. Такой вызов определяет новую случайную величину. Например, вызов вероятностной процедуры (normal 1 10) определяет случайную величину, распределенную по закону нормального распределения Normal(1, √ 10), и результатом выполнения каждый раз будет какое-то вещественное число. 3. Вызов составных процедур (compound-procedure arg1 ...argN), где\ncompound-procedure — введенная пользователем процедура с помощью специального выражения lambda: (lambda (arg1 ...argN) body), где body — тело процедуры, состоящее из выражений. В общем случае составная процедура\nявляется стохастической (недетерминированной) составной процедурой, так как 1Venture имеет отдельный дополнительный вид синтаксиса VentureScript, использующий инфиксную нотацию и не требующий обрамления вызова функций скобками, то есть схожий по своей сути с привычными большинству людей языками программирования C, C++, Python и т.д.\nее тело может содержать вызовы вероятностных процедур.\nПосле этого мы хотим задать условную вероятность P (X | T ) наблюдаемых пе-\nременных x1, x2, x3 при заданных значениях скрытых переменных t1, t2 и параметра time.\nПеред вводом непосредственно самих наблюдений с помощью выражения [OBSERVE\n...] мы определяем общий закон для наблюдаемых переменных {xi} в рамках нашей модели, а именно мы предполагаем, что данные наблюдаемые случайные величины при заданных t1, t2 и заданном уровне шума noise распределены по закону нормального распределения Normal(t1+t2·time, √ noise) со средним t1+t2·time и стандартным отклонением noise. Данная условная вероятность определена на строках 3 и 4 данной вероятностной программы. noisy_x определена как функция, принимающая параметр time и возвращающая случайное значение, определенное с помощью вычисления выражение (normal (+ t1 (* t2 time)) noise) и обусловленное значениями случайных величин t1 и t2 и переменной noise. Отметим, что выражение (normal (+ t1 (* t2 time)) noise) содержит в себе неопределенность, поэтому каждый раз при его вычислении мы будем получать в общем случае разное значение.\nНа строках 5—7 мы непосредственно вводим известные значения x̂1 = 10.3, x̂2 =\n11.1, x̂3 = 11.9. Инструкция вида [OBSERVE expression value] фиксирует наблюдение о том, что случайная величина, принимающая значение согласно выполнению выражения expression, приняла значение value.\nПовторим на данном этапе всё, что мы сделали. На строках 1—4 с помощью ин-\nструкций вида [ASSUME ...] мы задали непосредственно саму вероятностную модель: P (T ) и P (X | T ). На строках 5—7 мы непосредственно задали известные нам значения наблюдаемых случайных величин X с помощью инструкций вида [OBSERVE ...].\nНа строках 8—9 мы запрашиваем у системы вероятностного программирования апо-\nстериорное распределение P (T | X) скрытых случайных величин t1 и t2. Как уже было сказано, при большом объеме данных и достаточно сложных моделях получить точное аналитическое представление невозможно, поэтому инструкции вида [PREDICT ...] генерируют выборку значений случайных величин из апостериорного распределения P (T | X) или его приближения. Инструкция вида [PREDICT expression] в общем случае генерирует один элемент выборки из значений случайной величины, принимающей значение\nсогласно выполнению выражения expression. Если перед инструкциями вида [PREDICT ...] расположены инструкции вида [OBSERVE ...], то выборка будет из апостериорного распределения2, обусловленного перечисленными ранее введенными наблюдениями.\nОтметим, что в завершении мы можем также предсказать значение функции\nx(time) в другой точке, например, при time = 4.0. Под предсказанием в данном случае понимается генерация выборки из апостериорного распределения новой случайной величины при значениях скрытых случайных величин t1, t2 и параметре time = 4.0.\nДля генерации выборки из апостериорного распределения P (T | X) в языке\nпрограммирования Church в качестве основного используется алгоритм МетрополисаГастингса, который относится к методам Монте-Карло по схеме Марковских цепей. В следующем подразделе будет произведено подробное описание применения данного алгоритма для обобщенного статистического вывода в вероятностных языках. Под «обобщенным» выводом в данном случае понимается то, что алгоритм может быть применен к любым вероятностным программам, написанным на данном вероятностном языке программирования.\n1.2 Статистический вывод в вероятностных языках программирования с по-\nмощью алгоритма Метрополиса-Гастингса\nОписание алгоритма Метрополиса-Гастингса в применении к «семейству» вероятностных языков Church впервые опубликовано в [2] и более подробно описано в [16].\nПолучить выборку из N элементов из априорного распределения скрытых пара-\nметров P (T ), наблюдаемых величин P (X | T ) или их совместного распределение P (T,X) какой-либо порождающей вероятностной модели, записанной в виде вероятностной программы, не составляет труда. Для этого достаточно выполнить вероятностную программу N раз. Отметим очевидный факт, что в данном случае вероятностная программа будет содержать лишь инструкции вида [ASSUME ...] (задание вероятностной модели) и [PREDICT ...] (перечисление случайных величин, выборку которых мы генерируем).\n1.2.1 Метод «выборки с отклонением»\nПри заданных наблюдениях с помощью инструкций вида [OBSERVE expression value] наиболее простым способом получения апостериорного распределения является метод 2Говоря точнее, конечно, из приближения апостериорного распределения.\n«выборки с отклонением» [17]. Для понимания рассмотрим следующую вероятностную программу:\n1 [ASSUME a ( uniform−d i s c r e t e 1 6 ) ] 2 [ASSUME b ( uniform−d i s c r e t e 1 6 ) ] 3 [OBSERVE (+ a b) 5 ] 4 [PREDICT (∗ a b ) ]\nОтметим, что стохастическая процедура (uniform-continuous a b) возвращает\nзначение случайной величины, распределенной по равномерному дискретному закону распределения с носителем {a, . . . , b}.\nТекстом задачу, записанную выше с помощью вероятностной программы, можно\nсформулировать следующим образом: подбрасываются два шестигранных игральных кубика, в сумме выпало 5; каково распределение произведения очков на этих двух кубиках?\nМетод «выборки с отклонением» заключается в том, чтобы генерировать значения\nочков на первом и втором кубиках из их априорного распределения (таким образом, два независимых дискретных равномерных распределения) и проверять, равна ли их сумма 5. Если нет, данная попытка отвергается и не учитывается. Если да, то произведение очков на кубиках добавляется во множество элементов выборки. Данная выборка и будет аппроксимацией апостериорного распределения значений произведения очков на двух кубиках, если известно, что сумма очков равна 5.\nДанный метод является неэффективным, и при решении более сложных задач про-\nсто вычислительно неосуществимым. Также отметим, что он хорошо подходит только для дискретных значений случайных и промежуточных переменных.\n1.2.2 Пространство историй выполнений вероятностных программ\nПри выполнении вероятностной программы каждая случайная величина принимает определенное значение. Например, следующая вероятностная программа содержит три случайных величины, каждая из которых распределена по закону нормального распределения:\n1 [ASSUME a ( normal 0 1 ) ] 2 [ASSUME b ( normal 0 1 ) ]\n3 [ASSUME c ( normal (+ a b) 1 ) ] 4 [PREDICT c ]\nПри выполнении данной вероятностной программы каждая из трех случайных ве-\nличин примет свое случайное значение. Назовем историей выполнения вероятностной программы отображение, сопоставляющее каждой случайной величине ее значение. Мощность истории выполнения совпадает с количеством случайных величин, которые были созданы при ее выполнении.\nДля каждой программы можно определить соответствующее вероятностное про-\nстранство, элементарными событиями которого будут являться все истории ее выполнения. Вероятностью каждого элементарного события будет являться произведение вероятностей принятие того или иного значения каждой случайные величины при данной истории выполнения.\nОчевидно, что история выполнения однозначным образом определяет выполнение\nвероятностной программы и принимаемые значения, как случайных, так и зависящих от них детерминированных выражений.\nДля примера рассмотрим еще более простую вероятностную программу.\n1 [ASSUME a ( b e r n ou l l i 0 . 7 ) ] 2 [ASSUME b ( b e r n ou l l i 0 . 7 ) ] 3 [PREDICT a ] 4 [PREDICT b ]\nВ ней обе случайных величины имеют свое название, что облегчает запись историй\nвыполнений (иначе необходимо вводить адресную схему для наименования случайных величин, чтобы однозначно идентифицировать их).\nДанная вероятностная программа имеет четыре различных возможных историй\nвыполнений, а именно {a : 0, b : 0}, {a : 0, b : 1}, {a : 1, b : 0} и {a : 1, b : 1} с вероятностями 0.09, 0.21, 0.21 и 0.49 соответственно.\nОтметим также, что количество «активных» случайных величин при выполнении\nодной и той же вероятностной программы может меняться, как, например, в следующей программе:\n[ASSUME geometr ic\n( lambda (p) ( i f (= ( b e r n ou l l i p ) 1) 0 (+ ( geometr ic p) 1 ) ) ) ]\n[PREDICT ( geometr ic 0 . 5 ) ]\nДанная программа генерирует элемент (т.е. одноэлементную выборку) из геомет-\nрического распределения с параметром 0.5 с помощью составной процедуры geometric, параметризованной параметром p. При выполнении данной вероятностной программы в истории ее выполнения количество «активных» случайных величин может быть любым.\n1.2.3 Апостериорное распределение историй выполнений программ\nПри добавлении наблюдений, то есть фиксации значения определенных случайных величин, можно считать, что рассматривается подмножество множества элементарных событий, а именно те элементы, в историях выполнений которых наблюдаемые случайные величины принимают желаемое значение.\nМожно описать другое вероятностное пространство, множеством элементарных со-\nбытий которого будет являться только что описанное подмножество. Вероятностная мера в новом вероятностном пространстве может быть «индуцирована» вероятностной мерой из первоначального вероятностного пространства с учетом нормировочной константы.\nТолько что описанный переход полностью сочетается с теоремой Байеса, которая в\nнашем случае записывается следующим образом:\nP (T | X = x̂) = P (T )P (X = x̂ | T ) P (X = x̂) ,\nгде X — множество случайных величин, для которых мы знаем фиксированные наблюдаемые значения; T — множество случайных величин, ассоциируемых со скрытыми параметрами, апостериорное распределение которых мы заинтересованы получить (грубо говоря, это всё остальные случайные величины); x̂ — наблюдаемые значения случайных величин.\nОтметим, что часто мы заинтересованы не в апостериорном распределении\nP (T | X = x̂), а в апостериорном распределении лишь части скрытых параметров P (T ′ ⊂ T | X = x̂) или даже функции f от них P (f(T ) | X = x̂), в общем случае не являющейся биекцией. С другой стороны, так как мы не ставим задачу получить аналитическое представление данных апостериорных распределений, а лишь выборку из них, то\nэто не играет большой роли в нашем случае при решении задач методами Монте-Карло: мы можем генерировать элементы выборки из P (T | X = x̂), и затем использовать только значения нужных нам скрытых случайных величин и/или действовать функцией f на них.\n1.2.4 Использование методов Монте-Карло по схеме Марковских цепей\nМатематическо-статистический аппарат методов Монте-Карло по схеме Марковских цепей кратко и «современно» изложен в [8].\nИнтуитивно опишем, что мы собираемся делать. У нас есть вероятностная про-\nграмма g, определяющая множество скрытых T и наблюдаемых X случайных величин. Вероятностная программа своей записью задает априорное распределение P (T ), P (X | T ), а значит и совместное распределение P (T,X). Каждый раз путем выполнения данной программы мы получаем одну из возможных реализаций данной программы, которая биективно описывается соответствующей историей выполнения hi ∈ H, где множество H — множество всех возможных историй выполнений данной вероятностной программы, которое можно рассматривать как множество элементарных событий. Для каждой истории выполнения определена ее вероятностная мера\nP (hi) = P (T = t̃, X = x̃) = P (T = t̃)P (X = x̃ | T = t̃).\nУ нас также есть «экспериментальное» значение каждой наблюдаемой случайной\nвеличины x̂i, то есть множество {x̂i} = x̂. Существует подмножество историй выполнений H ′ ⊂ H, в котором наблюдаемые случайные величины принимают желаемое значение. Данное подмножество можно рассматривать как множество элементарных событий другого вероятностного пространства, вероятностную меру на котором можно «индуцировать» из предыдущего путем деления на нормирующую постоянную P (X = x̂):\nP (h′) = P (T = t̃)P (X = x̂ | T = t̃)\nP (X = x̂) .\nБудем обозначать ненормированную вероятностную меру\n˜P (h′) = P (T = t̃)P (X = x̂ | T = t̃).\nРассмотрим цепь Маркова, исходами которой являются истории выполнений h′ ∈\nH ′. В качестве начального распределения можно выбрать априорное распределение P (T )P (X | T ), при котором значения наблюдаемых случайных величин не выбираются случайным образом согласно их распределению, а устанавливаются согласно их значениям. Например, в вероятностной программе\n1 [ASSUME a (gamma 1 1 ) ] 2 [ASSUME b ( lambda ( ) ( normal a 1 ) ) ] 3 [OBSERVE (b) 5 . 3 ]\nпервая случайная величина a, распределенная по закону Гамма-распределения Gamma(1, 1), будет являться скрытой, и будет сгенерирована согласно данному закону распределения. Наблюдаемая же случайная величина, распределенная по нормальному закону Normal(a, 1), не будет сгенерирована, а будет установлена в соответствии с ее наблюдаемым значением.\nМы хотим установить такие правила перехода по схеме Метрополиса-Гастингса из\nодного состояния (исхода) цепи Маркова в другое, чтобы стационарное распределение данной цепи Маркова совпадало с распределением P (h′). В таком случае для получения аппроксимации искомого апостериорного распределения в виде выборки нам будет достаточно имитировать данную цепь Маркова [8, 18].\nВ алгоритме Метрополиса-Гастингса вероятностная мера может быть известна с\nточностью до нормировочной константы, что и происходит в нашем случае. На каждом шаге алгоритма дано текущее состояние h′t и в соответствии с заданным заранее условным распределение предлагается новое состояние h′∗ ∼ Q( · | h′t). Таким образом, Q можно назвать распределением предлагаемых переходов. После этого подсчитывается коэффициент «принятия» нового состояния:\nα = min ( 1, P̃ (h′∗) Q(h′t | h′∗) P̃ (h′t) Q(h ′∗ | h′t) ) .\nСостояние h′∗ принимается в качестве следующего состояния h′t+1 с вероятностью\nα, в противном случае h′t+1 := h′t.\nНовое состояние предлагается следующим образом: 1. Случайным образом (равномерно) выбирается одна «активная» скрытая случай-\nная величина r ∈ domh′t для «вариации».\n2. Предлагается новое значение данной случайной величины r∗ ∼ κ( · | r = r̂, h′t).\nУсловная вероятность κ в данном случае будет локальным для r распределением предлагаемых переходов. 3. Если случайная величина r влияет на поток выполнения вероятностной програм-\nмы, и в результате ее нового значения должны быть исполнены другие ветви выполнения вероятностной программы, они исполняются и в общем случае происходит генерация новых случайных величин, которые прежде были неактивны.\nДля примера рассмотрим следующую вероятностную программу:\n1 [ASSUME a ( b e r n ou l l i 0 . 3 ) ] 2 [ASSUME b ( i f (= a 1) ( normal 0 1) (gamma 1 1 ) ) ] 3 [PREDICT c ]\nВ данной вероятностной программе три случайных величины, первая ξ1 распределена по закону Бернулли, вторая ξ2 по закону нормального распределения, третья ξ3 по закону Гамма-распределения. ξ1 можно также называть «a» (по имени переменной), хотя переменная «b» не является сама по себе случайной величиной, а зависит от значения ξ1, определяющей поток выполнения вероятностной программы, и от значения либо ξ2, либо ξ3. Отметим также, что при каждом выполнении данной вероятностной программы будет существовать только две активных случайных величины.\nПредположим, что текущим состоянием вероятностной программы в момент вре-\nмени t была история выполнения h′t = {ξ1 = 1, ξ2 = 3.2}. Это означает, что случайная величина ξ приняла значение 1, и поэтому для задания переменной b был сгенерирован элемент из нормального распределения, то есть была «реализована» случайная величина ξ2 ∼ Normal(0, 1). Очевидно, что\nP (h′t) = P (ξ1 = 1) · P (ξ2 = 3.2 | ξ1 = 1) = 0.3 · fNormal(0,1)(3.2) ≈ 0.0007152264603.\nПредложим новое состояние h′∗: 1. Выберем случайным образом одну из двух «активных» случайных величин,\nпусть это будет ξ1.\n2. Предложим случайным образом новое значение данной случайной величины со-\nгласно ее априорному распределению (то есть Bernoulli(0.3)), пусть это будет 0.\n3. Так как случайная величина ξ1 действительно влияет на поток выполнения ве-\nроятностной программы и при изменении ее значения в данном случае должна быть выполнена другая ветвь, выполним данную ветвь, генерируя значения «активирующихся» случайных величин (в нашем случае только ξ3). Предположим, что ξ3 стало равным 12.3.\nТогда\nQ(h′∗ | h′t) = 1\n2 · 0.7 · fGamma(1,1)(12.3).\nВ общем случае\nQ(h′∗ | h′t) = 1\n|h′t| · κ(r∗ | r = r̂, h′t) ·Rnew,\nгде |h′t| — количество активных случайных величин в текущей истории выполнения, κ(r∗ | r = r̂, h′t) — вероятность нового значения варьируемой случайной величины r, Rnew — совместная вероятность выбора своих значений «активирующихся» случайных величин.\nПри уже фиксированном h′∗ обратное Q(h′t | h′∗) для коэффициента принятия ал-\nгоритма Метрополиса-Гастингса в общем случае может быть посчитано аналогичным образом:\nQ(h′t | h′∗) = 1\n|h′∗| · κ(r̂ | r = r ∗, h′∗) ·Rold,\nгде Rold — совместная вероятность выбора своих значений случайных величин, активных в h′t, но не являющихся активными в h′∗.\nВ только что рассмотренном примере\nQ(h′t | h′∗) = 1\n2 · 0.3 · fNormal(0,1)(3.2).\nИ тогда с учетом того, что P (h′∗) = 0.7 · fGamma(1,1)(12.3) ≈ 0.000003186221124, мы\nполучаем, что\nα = min ( 1,\n0.7 · fGamma(1,1)(12.3) · 12 · 0.3 · fNormal(0,1)(3.2) 0.3 · fNormal(0,1)(3.2) · 12 · 0.7 · fGamma(1,1)(12.3)\n) = 1.\nПоэтому в данном конкретном примере h′t+1 = h′∗ в любом случае, то есть с веро-\nятностью один.\nВыбор локального для r распределения предлагаемых переходов может быть раз-\nным. В простейшем случае, если r — независимая случайная величина, данное распределение выбирается идентичным априорному распределению для r. Если же r — перестановочная случайная величина [19], а такие случайные величины поддерживаются рассматриваемыми языками вероятностного программирования, то ее распределение может быть выбрано с учетом уже накопленных значений.\nТаким образом, мы описали алгоритм для предложения нового состояния h′∗ при\nтекущем состоянии h′t и описали его в рамках метода Метрополиса-Гастингса. Чтобы получить выборку {h′j}Nj=1 из N элементов желаемого апостериорного распределения, нам необходимо имитировать данную цепь Маркова согласно описанному выше алгоритму и получить {h′j}M+N ·Kl=j элементов данной цепи. Затем необходимо отсеять первыеM элементов и из оставшихся выбрать каждый N -й элемент. Полученное множество будет являться аппроксимацией искомой выборки {h′j}Nj=1 [8], а так как любая история выполнений h′ однозначно определяет значение всех случайных величин в вероятностной программе, то и аппроксимацией выборки из P (T | X = x̂).\nK выбирается больше единицы, чтобы исключить автокорреляцию h′i, которая есте-\nственным образом возникает в цепи Маркова. M выбирается достаточно больши́м, чтобы независимо от выбора начальной точки h′1 цепь успела «забыть» данный первоначальный выбор; это особенно важно, когда начальное априорное распределение P (T,X) очень сильно отличается от апостериорного P (T | X = x̂). Какого-то общего правила для выбора данных величин нет, и обычно они выбираются эмпирически.\n1.2.5 Программная реализация статистического вывода\nВ предыдущем подпункте теоретически был описан алгоритм для обобщенного статистического вывода в вероятностных языках программирования.\nПростая программная реализация впервые достаточно подробно была описана в\n[16]:\n1. Для инициализации h′1 выбирается случайным образом путем выполнения ве-\nроятностной программы и фиксации наблюдаемых случайных величин в соответствии с имеющимися данными. При этом в памяти компьютера сохраняется база данных активных случайных величин вместе с их значениями, а также сохраняется P (h′1), которое подсчитывается во время первоначального выполнения программы с учетом вероятности наблюдаемых случайных величин принять то\nзначение, которое они приняли. В [16] описывается схема адресации, которая позволяет различать случайные величины между собой. 2. Затем для выбора последующего h′t+1 каждый раз из базы данных случайным\nравномерным образом выбирается случайная величина r и она случайным образом варьируется в соответствии со своим локальным распределением предлагаемых переходов κ( · | r = r̂, h′t), которое предварительно задается для всех используемых примитивных (несоставных) случайных величин. Создается копия базы данных, в которой значение случайной величины r заменяется на новое. 3. Вероятностная программа выполняется еще один раз, при этом случайные вели-\nчины, для которых уже в базе данных имеется запись (согласно схеме адресации), принимают соответствующие старые значения, а r принимает свое новое полученное значение. 4. В общем случае, если r влияет на поток выполнения программы, генерируются\nзначения для новых случайных величин, которые становятся активными. Для этих случайных величин прежде в базе данных не было записей. 5. Также если r влияет на поток выполнения программы, то некоторые случайные\nвеличины могут перестать быть активными. В данном случае соответствующие им записи в базе данных будут невостребованы. 6. После выполнения программы второй раз можно считать, что мы получили h′∗.\nТакже отметим, что при выполнении программы h′∗ мы получили и использовали всё необходимые компоненты для подсчета P (h′∗) и Q(h′∗ | h′t). 7. Вероятность обратного перехода Q(h′t | h′∗) может быть получена разными спо-\nсобами. Если в вероятностной программе есть только независимые случайные величины (и нет перестановочных), то первые две компоненты Q(h′t | h′∗) могут быть найдены тривиально, так как мы знаем количество случайных величин в h′∗ и можем посчитать κ(r̂ | r = r∗, h′∗). Для третьей компоненты нам нужно посчитать произведение вероятностей случайных величин, которые перестали быть активными в h′∗, то есть все невостребованные случайные величины в базе данных. В случае наличия и использования перестановочных случайных величин мы можем имитировать ситуацию, что h′∗ является нашим старым состоянием, а новое состояние мы получаем в точном соответствии с h′t [3].\n8. Имея P (h′t), P (h′∗), Q(h′∗ | h′t) и Q(h′t | h′∗), мы подсчитываем коэффициент при-\nнятия α для алгоритма Метрополиса-Гастингса, и либо принимаем h′∗ с вероятностью α, что означает, что в следующий раз мы будем использовать уже новую базу данных случайных величин со значением, соответствующими h′∗, либо отклоняем с вероятностью (1 − α), что означает, что на следующем шаге h′t+1 мы снова будем использовать старую базы данную от h′t.\n9. Алгоритм повторяется с шага № 2."
    }, {
      "heading" : "1.3 Эффективность вывода",
      "text" : "При использовании вероятностных языков программирования встает вопрос об эффективности статистического вывода, другими словами о том, как быстро мы генерируем выборку желаемой точности из апостериорного распределения. Описанная в секции 1.2.5 программная реализация статистического вывода является неэффективной, так как происходит перевыполнение всей вероятностной программы, хотя вариация случайной величины r обычно имеет только локальный эффект.\nРассмотрим более подробно данную проблему на примере следующей вероятност-\nной программы:\n1 [ASSUME rainy−season ( b e r n ou l l i 0 . 2 ) ] 2 [ASSUME cloudy 3 ( b e r n ou l l i ( i f ra iny−season 0 .8 0 . 3 ) ) ] 4 [ASSUME ra in 5 ( b e r n ou l l i ( i f c loudy 0 .8 0 . 2 ) ) ] 6 [ASSUME sp r i n k l e r 7 ( b e r n ou l l i ( i f c loudy 0 .1 0 . 5 ) ) ] 8 [ASSUME wet−g ra s s 9 ( b e r n ou l l i\n10 ( i f s p r i n k l e r ( i f r a in 0 .99 0 . 9 ) 11 ( i f r a in 0 .9 0 . 0 1 ) ) ) ]\nДанная вероятностная программа описывает статистически простую упрощенную\nмодель зависимости между сезоном, облачностью, дождем, работой разбрызгивателя и состоянием травы (мокрая или нет) в какой-то день. Значение переменных следующее:\nРисунок 1 — Пример Байесовской сети. Из [20].\nrainy-season: входит ли тот день в сезон дождей или нет?, cloudy: облачно в тот день или нет?; rain: был ли дождь в тот день?; sprinkler: работал ли разбрызгиватель в тот день или нет?; wet-grass: была ли трава мокрой в тот день? Данная модель может быть представлена с помощью Байесовской сети доверия (см. рис. 1) и таблицами условных вероятностей. Отметим, что любая Байесовская сеть доверия может быть представлена в виде вероятностной программы на языке Church/Venture/Anglican, но не любая вероятностная программа может быть представлена Байесовской сетью.\nВ случае, если во время очередной итерации алгоритма Метрополиса-Гастингса в\nкачестве варьируемой случайной величины выбрана случайная величина rain (соответствующий узел на рис. 1 выделен самым темным цветом), для подсчета коэффициента принятия достаточно лишь рассмотреть значения и вероятности при данных значениях узлов «Дождь» и «Трава мокрая». Все остальные значения и их вероятности останутся прежними. Иллюстрацию распространения возмущений в связи с вариацией случайной величины «Дождь» см. на рис. 2.\nВ подобном простом примере этот факт не играет большой роли, так как отношение\nслучайных величин, требующих «переучета», к общему количеству активных случайных величин невелико, однако при большом объеме данных это играет решающую роль при выполнении статистического вывода во многих моделях при помощи программных реализаций вероятностных языков программирования. Например, в скрытых марковских моделях [17, 2, 4] или в латентном размещении Дирихле [21, 20].\nНапример, в простой скрытой Марковской модели есть N скрытых и N наблю-\nдаемых величин. При реализации статистического вывода алгоритмом, описанным в сек-\nРисунок 2 — Байесовская сеть в виде «отпечатка» выполнения вероятностной программы. Из [20].\nции 1.2.5, одна итерация алгоритма Метрополиса-Гастингса имеет сложность (по времени) O(N), хотя желаемая и возможная сложность O(1) или по крайней мере O(logN).\nДля достижения желаемой сложности в каждой истории выполнений необходимо\nотслеживать зависимости между случайными величинами. Следует отметить, что эти зависимости в общем случае могут изменяться в вероятностных программах. Например, в следующей программе\n1 [ASSUME a ( b e r n ou l l i 0 . 5 ) ] 2 [ASSUME b (gamma 1 1 ) ] 3 [ASSUME c ( normal ( i f a b 3 . 0 ) 1 ) ]\nслучайная величина c, т.о. (normal ...), зависит от случайной величины b, т.о. (gamma ...), не всегда, а только если случайная величина a принимает значение «ИСТИНА».\nОписание структур данных и алгоритмов, необходимых для отслеживания зависи-\nмостей в режиме реального времени, были предварительно приведено в [20] и [22], а затем более обширно и подробно в [3]. При использовании данных структур данных и алгоритмов временная сложность одной итерации алгоритма Метрополиса-Гастингса в простой скрытой Марковской модели равна O(logN), при этом если использовать упорядоченный\nперебор случайных величин, то временная сложность снизиться до O(1), так как логарифмический фактор появляется в связи с необходимостью выбирать случайным образом следующий узел (т.е. случайную величину) для вариации.\nВ простой скрытой Марковской модели количество «активных» случайных вели-\nчин постоянно и вид зависимости тривиален (от каждой скрытой случайной величины напрямую зависит только одна наблюдаемая случайная величина и только следующая скрытая случайная величина). В более сложных моделях виды зависимостей более изощрены и количество случайных величин меняется от одной истории выполнений к другой. С другой стороны, для большого количества порождающих моделей, используемых в настоящее время в машинном обучении, необходимо, чтобы время на одну итерацию оставалось постоянным или росло хотя бы логарифмически вместе с линейным ростом количества наблюдений, иначе статистический вывод будет невозможен за разумное время.\nНа рис. 3 показаны результаты применения алгоритмов и структур данных, описан-\nных в [20, 22, 3]. При использовании старого подхода, описанного в [16] (см. секцию 1.2.5) время на N итераций алгоритма Метрополиса-Гастингса росло квадратично с линейным ростом размерности модели (N скрытых и N наблюдаемых случайных величин), а при использовании предлагаемого подхода время растет квазилинейно.\nНа этом же рисунке видно, что благодаря локализации выполнения одной итера-\nции Метрополиса-Гастингса стало возможным проводить приближенный статистический вывод параллельно.\nСледует отметить, что пространственная сложность алгоритма (объем памяти,\nнеобходимый для его применения) равна, грубо говоря, O(N + K), где O(K) — стоимость хранения в памяти зависимостей между случайными величинами. В общем случае эта величина может быть достаточно большой, и в работе [22] описаны возможности более эффективного расходования памяти.\n1.4 Порождающее вероятностное программирование в распознавании обра-\nзов\nКак было отмечено в начале, цель вероятностного программирования — облегчить задачу моделирования порождающих вероятностных моделей и проведения вывода в них. Примером иллюстрации успешного предварительного применения вероятностного программирования может служить работа [5], в которой вероятностное программирование использует-\n0\n20\n40\n60\n80\n100\n120\n140\n160\nС р\nе д\nн е е в\nр е\nм я н\nа N\nи те\nр а ц\nи й\nа л\nго р и\nтм а\nМ е тр\nо п о\nл и\nса -Г\nа ст\nи н гс\nа\nСтатистический вывод без оптимизации (красный) Предложенные алгоритмы и структуры данных (зел.)\n50 100 150 200 250\nКоличество наблюдений N в скрытой модели Маркова\n—//—, 2 потока (син.) —//—, 4 потока (гол.)\nРисунок 3 — Эффективность статистической вывода при его различных реализациях: красный график соответствует простейшему алгоритму, описанному в 1.2.5, при котором каждый раз происходит перевыполнение всей вероятностной программы. Зеленый график соответствует использованию новых алгоритмов и структур данных, что позволяет производить статистический вывод асимптотически более эффективно. Синий и голубой график показывают, что применение предлагаемого подхода также позволяет производить статистический вывод параллельно, по крайней мере приближенно. Рис. из [20].\nся для моделирования вероятностной модели находящихся на изображении объектов и их взаимодействия между собой. Байесовский подход к интерпретации изображений путем задания априорного распределения на расположение объектов и на связи между ними был предложен задолго до появления рассматриваемых языков программирования, однако именно с их появлением исследование и осуществление данного подхода стало проще, так как вероятностные языки программирования позволяют композиционно и компактно представлять вероятностные порождающие модели и проводить статистический вывод в них.\nВ работе рассматриваются два примера: проблема графической CAPTCHA [23] —\n«компьютерного теста, используемого для определения, кем является пользователь системы: человеком или компьютером», знакомого почти что каждому пользователю интернета; и проблема нахождения на изображении с камеры автомобиля дороги, разделительной полосы, и левого и правого оврагов. Полученные результаты по своей эффективности на рассматриваемых простых примерах не уступают другим современным подходам к решению этих задач, однако представление, моделирование и вывод проще осуществляется с\nпомощью вероятностного программирования."
    }, {
      "heading" : "1.5 О различиях между Church, Venture, Anglican",
      "text" : "Целью данной главы, очевидно, не было подробное описание этих вероятностных языков, а только краткое введение в них. Заинтересованному читателю мы можем порекомендовать продолжить свое знакомство с вероятностным языком Church с [2, 24, 25, 16], Venture — [3], Anglican — [26, 4, 27].\nХотя многое объединяет эти вероятностные языки, в некоторых принципиальных\nвещах они различаются. Например, Venture на данный момент больше позиционируется как универсальная платформа, включающая в себя разные виды алгоритмов и методов статистического вывода с запроектированной возможностью добавлять новые с помощью использования базовых компонент и методов. В рамках работы над Anglican развиваются методы обобщенного вывода с использованием методов фильтрации частиц. Church, с другой стороны, позволяет производить статистический запрос внутри другого статистического запроса, что Venture и Anglican пока делать не могут.\n2 Автоматическая генерация вероятностных программ\nПри использовании полных по Тьюрингу вероятностных языков программирования, включающих в себя функции высших порядков3, которыми в том числе являются языки Church, Venture и Anglican, вероятностная программа одновременно является и порождающей моделью, и записанной процедурой для генерации элементов выборки из этой модели путем выполнения исходного кода данной процедуры. Любая процедура в вероятностном программировании является формально программным кодом, который описывает процесс генерации элемента выборки при заданных аргументах данной функции. Таким образом, процедуры вероятностных программ являются конструктивным способом описания условных распределений.\nПолные по Тьюрингу и допускающие функции высших порядков вероятностные\nязыки программирования открывают возможность проведения вывода исходного текста самих вероятностных программ, если задано априорное распределение на множестве исходного текста, с помощью операторов eval и apply. Грубо говоря, необходимо представить вероятностную порождающую мета-модель, которая будет генерировать вероятностные модели в виде исходного кода вероятностных программ.\nДанная глава основана на работе [6], которая включает в себя первые предваритель-\nные результаты по этой амбициозной задаче вывода самих порождающих вероятностных моделей при наличии какой-либо информации об искомом распределении, которое определяется искомой вероятностной программой. Отметим, что статистический вывод в пространстве исходного кода сложен и нет какого-то простого подхода как к построению вероятностных порождающих моделей исходного кода [28], так и выводу в них [29].\nВ рамках нашей предварительной работы мы поставили задачу найти с помощью\nстатистического вывода и предлагаемой нами порождающей вероятностной мета-модели такие вероятностные программы, которые будут генерировать выборку элементов, схожую по своим статистическим характеристикам с каким-то заранее заданным распределением.\nЭта задача интересна сама по себе, так как нахождение эффективных алгоритмов\nгенерации (моделирования) случайных величин — нетривиальная задача и для людей3Функциями (процедурами) высших порядков называются функции, аргументами или значениями которых могут быть другие функции.\nученых, которой они занимаются на протяжении десятков лет [30, 31, 32].\nНаши предварительные результаты показывают, что подобный автоматизирован-\nный вывод порождающих вероятностных моделей в виде вероятностных программ действительно возможен. В частности, мы приводим результаты успешного эксперимента, в рамках которого мы автоматически нашли обобщенную вероятностную программу, которая подлинно (не приближенно) генерирует случайные величины, распределенные по закону Бернулли с произвольным параметром p."
    }, {
      "heading" : "2.1 Обзор литературы",
      "text" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений. Подходы к решению проблемы включают статистический вывод, поиск и оптимизацию, в том числе эволюционные алгоритмы и в особенности генетическое программирование.\nИдеи и методы использования вероятностного программирования для изучения и\nавтоматизированного представления вероятностных моделей предлагались и ранее [2, 42, 43, 3].\nНасколько автору известно, описываемый подход к порождению вероятностных\nпрограммам мета-вероятностной программой ранее не рассматривался в качестве отдельной проблемы достаточно подробно, хотя первые шаги в исследовании и формулировке проблемы были сделаны в работах [42, 3]."
    }, {
      "heading" : "2.2 Описание подхода",
      "text" : "Наш подход может быть описан в рамках приближенных Байесовских вычислений [44] с использованием метода Монте-Карло по схеме цепей Маркова с выбором в качестве искомого апостериорного распределения\nπ(X|X̂ )p(X̂ |T )p(T ), (1)\nгде π(X|X̂ ) — вероятностная мера, измеряющая расстояние между значения статистик, вычисленных соответственно на выборке X из искомого распределения и выборке X̂ , по-\n[ASSUME program−text ( product ions ‘ ( ) ‘ r e a l ) ] [ASSUME program ( eval ( l i s t ‘ lambda ‘ ( ) program−text ) ) ] [ASSUME samples ( apply−n−times program 10000 ’ ( ) ) ] [OBSERVE ( normal (mean samples ) no i s e− l e v e l ) 0 . 0 ] [OBSERVE ( normal ( var i ance samples ) no i s e− l e v e l ) 1 . 0 ] [OBSERVE ( normal ( skewness samples ) no i s e− l e v e l ) 0 . 0 ] [OBSERVE ( normal ( ku r t o s i s samples ) no i s e− l e v e l ) 0 . 0 ] [PREDICT program−text ] [PREDICT ( apply−n−times . . . ( program ) ) ] Рисунок 4 — Вероятностная программа для вывода исходного кода вероятностной программы для генерации случайных чисел, распределенных по закону стандартного нормального распределения Normal(0, 1).\nлученной путем выполнения исходного кода вероятностной программы-кандидата.\nПусть существует распределение F с параметрами θ, вероятностную программу\nдля генерации элементов выборки из которого мы хотим вывести. Пусть X = {xi}Ii=1, xi ∼ F (·|θ) будет выборкой из I элементов из распределения F при каком-то фиксированном значении параметров θ. Рассмотрим задачу вывода исходного кода вероятностной программы T , которая при ее неоднократном выполнении J раз сгенерирует выборку элементов X̂ = {x̂j}Jj=1, x̂j ∼ T (·), статистически близких к распределению F при заданных параметрах θ.\nПусть s будет статистикой выборки, и тогда s(X ) и s(X̂ ) — значение этой стати-\nстики на элементах выборки из F и из T (·) соответственно. Пусть вероятностная мера d(s(X ), s(X̂ )) = π(X|X̂ ), для простоты ненормированная, принимает бо́льшие значения, когда s(X ) ≈ s(X̂ ). d можно интерпретировать как расстояние или штраф.\nМы используем вероятностное программирование для представления мета-модели,\nпорождающей другие вероятностные программы в виде исходного текста, и для проведения статистического вывода в пространстве искомых вероятностных моделей. Для статистического вывода мы использовали программную реализацию вероятностного языка программирования Anglican, которая поддерживает [4] статистический вывод методом частиц и методом Метрополиса-Гастингса по методу Монте-Карло по схеме цепей Маркова.\nВероятностная мета-модель представлена на рис. 4, где на первой строке мы уста-\nнавливаем соответствие между T и переменной program-text, которая будет содержать один сгенерированный элемент из распределения на исходный код P (T ), определенное априорно через порождающую процедуру production с помощью адаптивной грамматики по типу [45] (см. подробнее в разделе 2.3).\nМы не указываем здесь θ, так как задача вывода в данном случае найти вероятност-\nную программу, генерирующую элементы из стандартного нормального распределения. Переменная samples на второй строке представляет описанную выше выборку X̂ из вероятностной программы-кандидата, и в этом примере J = 10000.\ns и d вычисляются на следующих четырех строках вероятностной программы, где\nстатистика s определяется как четырехмерный вектор, включающий в себя соответственно выборочные среднее, дисперсию, коэффициент асимметрии и коэффициент эксцесса выборки элементов из распределения, определенного вероятностной программой, полученной из распределения T . Мера расстояния d определяется через плотность многомерного нормального распределения со средними [0.0, 1.0, 0.0, 0.0]T и диагональной ковариационной матрицей σ2I. Отметим, что это означает, что мы ищем вероятностные программы, результат выполнения которых определяет распределение со средним равным 0, дисперсией — 1, коэффициентами асимметрии и эксцесса равными 0, и мы «штрафуем» отклонения от этих значений с помощью квадратичной экспоненциальной функции потерь с коэффициентом 1 σ2 , где σ определена как noise-level. Эта функция потерь представляет собой функцию плотности нормального распределения.\nДанный пример служит хорошей иллюстрацией основных особенностей нашего под-\nхода. Для поиска в виде вероятностной программы генератора случайных чисел из стандартного нормального распределения мы используем аналитическую информацию s(F ) о стандартном нормальном распределении при вычислении расстояния между статистиками d(s(F ), s(X̂ )). Существует по крайней мере три различных ситуации, включая данную, в которых s и d могут вычисляться различными способами:\n1. В рамках первой ситуации мы ищем вероятностную программу, которая эффек-\nтивно генерирует элементы из аналитически известных распределений. Под эффективностью в данном случае можно понимать вычислительную временную сложность и среднее количество использованной энтропии для генерации элемента выборки в среднем. Практически всегда в этой ситуации и при подобной постановке задачи статистики распределения F известны аналитически. 2. Вторая ситуация возникает, когда мы можем генерировать только элементы вы-\nборки из F . Например, подобная ситуация возникает, когда мы генерируем элементы из апостериорного распределения с помощью «дорогостоящего» (вычис-\n[ASSUME program−text ( product ions ‘ ( r e a l ) ‘ bool ) ] [ASSUME program ( eval ( l i s t ‘ lambda ‘ ( ) program−text ) ) ] [ASSUME J 100 ] [ASSUME samples−1 ( apply−n−times program J ’ ( 0 . 5 ) ) ] [OBSERVE ( f l i p (G−test−p−value\nsamples−1 ‘ Be rnou l l i ( l i s t 0 . 5 ) ) ) t rue ] . . .\n[ASSUME samples−N ( apply−n−times program J ’ ( 0 . 7 ) ) ] [OBSERVE ( f l i p (G−test−p−value samples−N ‘ Be rnou l l i ( l i s t 0 . 7 ) ) ) t rue ] [PREDICT program−text ] [PREDICT ( apply−n−times program J ’ ( 0 . 3 ) ) ] Рисунок 5 — Вероятностная программа для вывода исходного кода вероятностной программы, генерирующей случайные числа, распределенные по закону Бернулли Bernoulli(θ). На предпоследней строчке выводится текст вероятностной программыкандидата. На последней строчке вероятностная программа-кандидат выполняется J раз для генерации выборки из J элементов при параметризации p = 0.3, причем предыдущие θn (т.о. тренировочные значения параметров) не содержали p = 0.3.\nлительно) метода Монте-Карло по схеме Марковских цепей, и мы заинтересованы получить другое представление апостериорного распределения в виде вероятностной программы, чье априорное распределение будет точно или хотя бы приблизительно совпадать с искомым апостериорным. 3. При третьей ситуации нам заранее дана фиксированного размера выборка из\nF , и мы хотим найти вероятностную программу, которая позволит генерировать выборку произвольного размера из F в дальнейшем. Мы начали этот раздел с постановки задачи именно в рамках третьей ситуации. Следует отметить, что возможная польза представления генератора случайных чи-\nсел потенциально не только в том, чтобы эффективно и быстро генерировать выборку из желаемого распределения, но и чтобы получить формальное представление желаемого распределения или его приближения в виде исходного кода вероятностной программы, то есть в виде формальной сущности, дальнейшие действия с которой можно производить; в том числе проводить анализ над выведенным исходным кодом и использовать найденные блоки исходного кода для решения других задач.\nРис. 5 иллюстрирует другое важное обобщение решение задачи, сформулированной\nв самом начале с апостериорным распределением (1).\nПри постановке задачи на вывод генератора случайных чисел, распределенных со-\nгласно стандартному нормальному распределению, мы не параметризовали искомое рас-\n[ASSUME box−muller−normal ( lambda (mean std )\n(+ mean (∗ std (∗ ( cos (∗ 2 (∗ 3.14159 ( uniform−continuous 0 .0 1 . 0 ) ) ) ) ( sqrt (∗ −2\n( log ( uniform−continuous 0 .0 1 . 0 ) ) ) ) ) ) ) ) ]\n[ASSUME poi s son ( lambda ( ra t e ) ( begin ( d e f i n e L (exp (∗ −1 ra t e ) ) )\n( d e f i n e inner− loop ( lambda (k p) ( i f (< p L) ( dec k )\n( begin ( d e f i n e u ( uniform−continuous 0 1) )\n( inner− loop ( inc k ) (∗ p u ) ) ) ) ) ) ( inner− loop 1 ( uniform−continuous 0 1 ) ) ) ) ]\nРисунок 6 — Найденные и записанные людьми исходные коды вероятностных программ для (слева) общего нормального распределения Normal(µ, σ) [31] и (справа) распределения Пуассона Poisson(λ) [46]. Эти исходные коды входят в собранный нами корпус, с помощью которого мы определяем априорные вероятности для наших порождающих правил путем подсчета количества встречающихся констант, процедур разных видов и т.д.\nпределение никаким образом, так как у стандартного нормального распределения нет параметров, т.е. θ = ∅. В общем случае распределения, которые мы хотим представить в виде вероятностных программ, имеют нетривиальную параметризацию, и представляют собой по сути семейство распределений. Мы хотим найти вероятностную программу, входные аргументы которой как раз бы и являлись параметрами искомого распределения, таким образом, эта вероятностная программа позволяла бы генерировать случайные величины из всего семейства. Для наглядности рассмотрим алгоритм генерации случайных чисел, распределенных по закону нормального распределения, с помощью преобразования Бокса-Мюллера, представленный в виде вероятностной программы на рис. 6.\nЭта вероятностная процедура параметризована двумя параметрами: средним и\nстандартным отклонением. Для постановки обобщенной задачи вывода параметризованных вероятностных программам нам необходимо изменить наше искомое апостериорное распределение, включив в него параметр θ, параметризующий искомое распределение F :\nπ(X|X̂ , θ)p(X̂ |T , θ)p(T |θ)p(θ).\nНам бы хотелось вывести вероятностную программу, которая смогла бы обобщить\nвсе возможные значения параметра θ. С допущением, что если мы выберем конечное число N различных параметризаций θ̂i, мы получим обобщение всего семейства распределений в виде вероятностной программы, мы формулируем нашу задачу в виде следующего приближения с использованием приближенных Байесовских вычислений в рамках метода\nМонте-Карло по схеме цепей Маркова:\n1\nN N∑ n=1 π(Xn|X̂n, θn)p(X̂n|T , θn)p(T |θn) ≈ ∫ π(X|X̂ , θ)p(X̂ |T , θ)p(T |θ)p(θ)dθ.\nВероятностная программа для поиска параметризованной вероятностной програм-\nмы, генерирующей случайные числа, распределенных по закону Бернулли Bernoulli(θ), представлена на рис. 5, и наглядным образом иллюстрирует применение данного допущения и приближения. При выбранных N различных параметризациях параметра p распределения Бернулли мы каждый раз генерируем J элементов из вероятностной программыкандидата, аккумулируя расстояние (штраф) между искомым распределением и полученным распределением, представляющим вероятностную программу-кандидата. В каждом конкретном случае для θi мы высчитываем: 1) расстояние с использованием статистики G-теста (более «современный» аналог критерия согласия Пирсона и соответствующей статистики) в виде\nGn = 2 ∑ i∈0,1\n#[X̂n = i]ln (\n#[X̂n = i] θin(1− θn)(1−i) · |X̂n|\n) ,\nгде #[X̂n = i] — количество элементов выборки из X̂n, принимающих значение i; 2) а также соответствующее p-значение с нулевой гипотезой, утверждающей, что элементы выборки X̂n являются и элементами выборки из распределения Bernoulli(θn). Так как распределение статистики G-теста приблизительно распределено по закону распределения хи-квадрат, т.е. G ∼ χ2(1) в нашем примере, мы можем представить и находить расстояние d в данном случае путем вычисления вероятности ложного отклонения нулевой гипотезы H0 : X̂n ∼ Bernoulli(θn). Ложное отклонение нулевой гипотезы эквивалентно успеху в проведении испытания Бернулли с вероятностью успеха равной p-значению."
    }, {
      "heading" : "2.3 Грамматика и порождающие правила",
      "text" : "С учетом наличия в нашем распоряжении выразительного вероятностного языка программирования, допускающего функции высших порядков и полного по Тьюрингу, наше априорное распределение об исходном коде искомых вероятностных программ также достаточно выразительно. В общих чертах оно схоже с адаптивными грамматиками [45], используемыми в [29], но имеет отличия, в частности связанные с созданием сред с ло-\nкальными переменными. В виде псевдокода наше априорное распределение может быть представлено следующим образом (символ → означает «может перейти в»):\n1. Выражение exprtype|env → в имя переменной, случайно выбираемой из среды\nпеременных env с типом type.\n2. Выражение exprtype|env → в случайную константу типа type. Константы различ-\nных типов (целочисленные, вещественные и т.д.) генерируются из отдельного для каждого типа type процесса Дирихле4 DPtype(Htype, α), где базовое распределение Htype само по себе в общем случае являются смесью нескольких распределений. Например, для констант вещественного типа мы используем смесь нормального распределения (normal 0 10), равномерного непрерывного распределения (uniform-continuous -100 100) и равномерного дискретного распределения из множества {−1, 0, 1, . . .}. 3. Выражение exprtype|env → (proceduretype exprarg_1_type ... exprarg_N_type), где про-\nцедура procedure является случайно выбираемой примитивной детерминированной или стохастической (не составной) процедурой, определенной заранее в глобальной среде, с типом возвращаемого значения type.\n4. Выражение exprtype|env → (compound_proceduretype exprarg_1_type ... exprarg_N_type),\nгде compound_proceduretype является составной процедурой, также генерируемой в соответствии с процессом Дирихле DPtype(Gtype, β), где базовое распределение Gtype случайным образом генерирует составную процедуру с типом возвращаемого значения type и количеством входных аргументов, распределенным по закону Пуассона, где каждый входной аргумент имеет свой произвольный тип. Тело body составной процедуры генерируется также случайным образом согласно этим же порождающим правилам и грамматике, но с учетом введение локальной среды, включающей в себя входные аргументы процедуры. 5. Выражение exprtype|env → (let [(gensym) exprreal] exprtype|env ∪ gensym)), где\nenv ∪ gensym означает среду, дополненную переменной с именем (gensym) и со значением, вычисляемым в соответствии с генерируемым случайным образом выражением согласно этих же порождающих правил.\n6. Выражение exprtype|env → (if (exprbool) exprtype exprtype).\n7. Выражение exprtype|env → (recur exprarg_1_type ... exprarg_M_type), таким образом 4Не нужно путать с распределением Дирихле.\nрекурсивный вызов текущей составной процедуры.\nВо избежание вычислительных ошибок во время выполнения сгенерированных про-\nцедур мы заменяем примитивные функции их «защищенными» аналогами, например log(a) заменяется на safe-log(a), причем последний возвращает 0 если a < 0; или например uniform-continuous заменяется safe-uc(a, b), которая в случае если a > b меняет аргументы местами, а также возвращает просто a если аргументы равны: a = b.\nМножество типов, которые мы использовали в рамках наших экспериментов, вклю-\nчало вещественные и булевы типы, а общее множество примитивных процедур, включенных нами в глобальную среду, включало в себя такие функции как +, −, *, safe-div, safe-uc, safe-normal."
    }, {
      "heading" : "2.4 Вероятности использования порождающих правил",
      "text" : "Для задания априорных вероятностей порождающих правил, то есть вероятностей, с которой каждое из правил будет применяться в случае возможности его применения, мы вручную составили небольшой корпус вероятностных программ, которые повторяют найденные учеными [32] алгоритмы генераторов случайных чисел. Примеры таких программ представлены на рис. 6. Заметим, что все они требуют наличия только одной стохастической процедуры, а именно uniform-continuous, так что мы включили только ее в глобальную среду с положительной вероятностью для экспериментов, описанных в 2.5.2.\nИспользуя данный корпус, мы вычислили априорные вероятности каждого порож-\nдающего правила, при этом при выводе вероятностной программы для генерации случайных величин из искомого распределения F (например, распределения Бернулли), мы исключали из корпуса все элементы, которые генерируют случайные величины согласно закону распределения F . После этого вероятности использования порождающих правил были «смягчены» с помощью распределения Дирихле. В будущем можно использовать более обширные корпусы вероятностных программ, примером зарождающегося подобного корпуса может служить [25]."
    }, {
      "heading" : "2.5 Эксперименты",
      "text" : "Эксперименты были разработаны таким образом, чтобы проиллюстрировать все три вида возможных ситуаций, описанных ранее. Но в начале мы проиллюстрируем выразительность нашего априорного распределения исходного кода вероятностных программ. После\n−6 −5 −4 −3 −2 Y\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 P̂ (Y )\n−3.0 −2.5 −2.0 −1.5 −1.0 −0.5 0.0 Y\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 P̂ (Y )\n−4 −2 0 2 4 Y\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nP̂ (Y\n)\n−1.0 −0.5 0.0 0.5 1.0 Y\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 P̂ (Y )\n−14.56636 −7.28318 Y\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nP̂ (Y\n)\n−2.0 −1.5 −1.0 −0.5 0.0 Y\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nP̂ (Y\n)\nРисунок 7 — Гистограммы выборок из некоторых порожденных вероятностных программ из их априорного распределения. По форме распределений видно, что наши порождающие правила достаточно обширны, чтобы соответствующие порождающим вероятностным программам распределения были нетривиальны.\nэтого мы опишем постановку и результаты экспериментов в рамках использования нашего подхода во всех трех различных ситуациях возможности вычисления расстояния d.\n2.5.1 Выборки из сгенерированных вероятностных программ\nДля иллюстрации выразительности нашего априорного распределения исходных кодов вероятностных программ мы приводим выборки из случайным образом сгенерированных вероятностных программ из их априорного распределения. Эти шесть выборок в виде гистограмм из шести различных автоматически сгенерированных вероятностных программам расположены на рис. 7.\nИз рисунка видно, что разные случайным образом сгенерированные вероятностные\nпрограммы определяют в общем случае достаточно различные структурно распределения. В частности, можно заметить разнообразие в носителе, дисперсии и количестве «режимов» (т.е. островков носителя с высокой вероятностью по отношению к очень низковероятностным пространствам между островками).\n2.5.2 Вывод вероятностных программ, определяющих распределения, совпадающие или\nприближающие классические одномерные распределения\nКак уже было отмечено, для всех классических одномерных распределений существуют алгоритмы, позволяющие и точно, и приближенно генерировать любое количество элементов из данных параметризованных распределений, и они были аналитически выведены учеными. Эти алгоритмы в том числе, очевидно, могут быть записаны как вероятностные программы в виде их исходного кода.\nМы провели серию экспериментов, чтобы проверить возможность автоматическо-\nго вывода вероятностных программ, генерирующих выборки из классических одномерных распределений, а именно из распределения Бернулли Bernoulli(p), распределения Пуассона Poisson(λ), Гамма-распределения Gamma(a, 1.0), Бета-распределения Beta(a, 1), стандартного нормального распределения Normal(0, 1), и «общего» нормального распределения Normal(µ, σ).\nДля каждого семейства распределений мы проводили статистический вывод,\nвыбирая в качестве целевого апостериорного распределения частное распределение π(X|X̂ )p(X̂ |T )p(T ), маргинализированное по параметру θ. Для приближения в каждом случае мы выбирали малое множество значений параметров {θ1, . . . , θN} и определяли апостериорное распределение ограничениями по p−значению (для распределения Бернулли) и по близости моментам к ожидаемым (для всех других рассматриваемых распределений). Еще раз отметим, что при задании априорного распределения на порождающие правила в каждом конкретном случае из корпуса исключались все вероятностные программы, относящиеся к искомому распределению.\nОбразцы гистограмм выборок из лучших найденных в результате вывода вероят-\nностных программ представлены на рис. 8, где под лучшими мы понимаем вероятностные программы с наименьшим расстоянием d на тренировочных значениях параметров и соответствующих значениях моментов или максимального p-значения.\nСтоит особенно отметить результат эксперимента с распределением Бернулли, в\nрамках которого был найден исходный код вероятностной программы, статистически подлинно (точно) генерирующей выборку из всего семейства распределения Бернулли с параметром p. Найденная вероятностная программа представлена на рис. 9.\nНа рис. 10 представлен выведенный исходный текст вероятностной программы для\nгенерации элементов из Гамма-распределения Gamma(a, 1), параметризованного параметром a.\n2.5.3 Обобщение произвольных эмпирических данных с помощью порождающего вероят-\nностного программирования\nМы также проверили наш метод на выводе порождающих моделей в виде вероятностных программ для объяснения настоящих, не синтетических, данных, аналитическое распределение которых неизвестно.\n0 1 Y\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nP̂ (Y\n)\n40 50 60 70 80 90 100 110\nY\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nP̂ (Y\n)\n-2 -1 0 1 2 3 4 5 6 7 8 9 10 11 Y\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 P̂ (Y )\n0 2 4 6 8 10 12 14\nY\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 P̂ (Y )\n−4 −2 0 2 4 Y\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 P̂ (Y ) 0.0 0.5 1.0 1.5\nY\n0.0\n0.5\n1.0\n1.5\n2.0\nP̂ (Y\n)\nРисунок 8 — (Зеленые сплошные линии) Гистограммы выборок из распределений, соответствующих вероятностным программам, имеющим «высокую» вероятность в апостериорном распределении при поиске порождающих моделей, соответствующих распределениям (слева направо, сверху вниз): Bernoulli(p), Normal(µ, σ), Poisson(λ), Gamma(a, 1.0), Normal(0, 1), Beta(a, 1.0). (Синие пунктирные линии) Гистограммы выборок из «настоящих» распределений. Параметризация семейства распределений в каждом случае производилась другими значениями параметров, которые не входили в обучающее множество значений {θn}. Отметим, что для семейства распределений Бернулли Bernoulli(p) была выведена вероятностная программа, статистически точно генерирующая элементы выборки при любой параметризации (см. рис. 9). С другой стороны, не все распределения получили хорошее приближение найденными вероятностными программами за то время, что мы проводили вывод, как, например, в случае Бета-распределение Beta(a, 1.0).\n( lambda ( par stack− id ) ( i f (< ( uniform−continuous 0 .0 1 . 0 ) par ) 1 . 0 0 . 0 ) ) ( lambda ( par stack− id ) ( i f (< 1 .0 ( sa f e− sqr t ( safe−div par ( safe−uc par ( dec par ) ) ) ) ) 1 . 0 0 . 0 ) ) ( lambda ( par stack− id ) ( i f (< 1 .0 ( safe−uc ( sa f e− sq r t par ) (+ par ( cos par ) ) ) ) 1 . 0 0 . 0 ) ) Рисунок 9 — (сверху) Написанный человеком исходный код генератора случайных чисел, распределенных по закону Бернулли Bernoulli(p). (внизу, две программы) Выведенные исходные коды. Первая из двух выведенных вероятностных программ определяет настоящее семейство распределений Бернулли Bernoulli(p), параметризованное p. Вторая программа генерирует распределение, приближенное к распределению Бернулли, параметризованное p.\n( lambda ( par stack− id ) (∗ ( begin ( d e f i n e sym0 0 . 0 ) (exp ( safe−uc −1.0 ( sa f e− sqr t ( safe−uc ( safe−div ( safe−uc 0 .0 ( safe−uc 0 .0 3 . 14159 ) )\npar ) (+ 1 .0 ( safe−uc ( begin ( d e f i n e sym2 ( lambda ( var1 var2 stack− id ) ( dec var2 ) ) ) ( sym2 ( safe−uc −2.0 (∗ ( safe−uc 0 .0 ( begin ( d e f i n e sym4 ( safe−uc sym0 (∗ (+ ( begin ( d e f i n e sym5 ( lambda ( var1 var2 stack− id ) ( safe−div (+ ( sa fe− l og ( dec 0 . 0 ) ) −1.0) var1 ) ) ) ( sym5 (exp par ) 1 .0 0 ) ) 1 . 0 ) 1 . 0 ) ) ) ( i f (< ( safe−uc par sym4) 1 . 0 ) sym0 ( safe−uc 0 .0 −1 .0 ) ) ) ) sym0 ) ) ( safe−div sym0 (exp 1 . 0 ) ) 0 ) ) 0 . 0 ) ) ) ) ) ) ) par ) )\n( lambda ( stack− id ) (∗ 2 .0 (∗ (∗\n(∗ −1.0 ( safe−uc 0 .0 2 . 0 ) ) ( safe−uc ( safe−uc 4 .0\n(+ ( sa fe− l og 2 . 0 ) −1.0)) (∗ ( sa fe−div 2 .0 −55.61617747203855) ( i f (< ( safe−uc\n( safe−uc 27.396810474207317 ( safe−uc −1.0 2 . 0 ) ) 2 . 0 ) 2 . 0 ) 4 .0 −1 .0 ) ) ) ) −1 .0) ) )\nРисунок 10 — Исходный код выведенных вероятностных программ для (слева) Гаммараспределения Gamma(a, 1) и (справа) для третьей эмпирической выборки индикаторов, используемых для рассмотрения заявок на выдачу кредита. Выборки, сгенерированные с помощью выполнения этих программ, расположены соответственно на рис. 8 и на рис. 11 (последняя гистограмма из трех). Для экономии места исходный код был сокращен, где возможно; например, путем замены (* 1.0 0.0) на 0.0.\n10 20 30 40 50 60 70 80 90\nY\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nP̂ (Y\n)\n0 5 10 15 20 25 30\nY\n0.00\n0.05\n0.10\n0.15\n0.20\nP̂ (Y\n)\n−5 0 5 10 15 20 25 30 Y\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nP̂ (Y\n)\nРисунок 11 — Гистограммы (зеленые сплошные линии) выборок из распределений, определяемых найденными в процессе статистического вывода вероятностными программами для аппроксимации эмпирических данных (синие пунктирные линии) трех вещественных показателей из базы данных, используемой для анализа заявок на выдачу кредита.\nДля данного эксперимента мы выбрали три набора данных признаков обращающих-\nся в банк клиентов для получения кредита [47, 48], и производили вывод вероятностной программы, используя для сравнения p-значения двухвыборочного теста КолмогороваСмирнова аналогично тому, как мы использовали G-тест для дискретных распределений. Гистограммы выборок из лучших найденных вероятностных программ в сравнении с гистограммами истинных эмпирическими данными признаков клиентов приведены на рис. 11. Пример выведенной вероятностной программы показан на рис. 10 (справа).\n2.5.4 «Компиляция» вероятностных программ\nГенерация выборок из апостериорного распределения методом Монте-Карло по схеме Марковских цепей, особенно в случае Байесовского вывода, обычно достаточно дорогостояща. Под «компиляцией» вероятностных программ мы имеем в виду поиск вероятностных программ, априорное распределение которых согласовывалось бы точно или приблизительно с искомым апостериорным распределением.\nВ вероятностном программировании, где в общем случае генерация выборки из же-\n[ASSUME theta ( beta 1 .0 1 . 0 ) ] [OBSERVE ( f l i p theta ) True ] [OBSERVE ( f l i p theta ) True ] [OBSERVE ( f l i p theta ) True ] [OBSERVE ( f l i p theta ) True ] [PREDICT theta ] [PREDICT ( f l i p theta ) ]\n[ASSUME theta ( safe−beta 4 .440 1 . 0 ) ] [ASSUME theta ( sa f e− sqr t ( safe−beta ( sa fe− l og 11 .602) 1 . 0 ) ) ] [ASSUME theta ( safe−beta ( sa f e− sqr t 27 .810) 1 . 0 ) ]\n[ASSUME theta ( beta 5 .0 1 . 0 ) ] [PREDICT theta ]\nРисунок 12 — (слева) Вероятностная модель бета-биномиального распределения (в несжатой форме) в виде вероятностной программы. В рамках эксперимента была поставлена задача найти формализацию апостериорного распределения скрытого параметра θ в виде другой вероятностной программы, чье априорное распределение будет совпадать или приближать данное апостериорное. (справа, сверху) Найденные вероятностные программы (т.е., результат «компиляции» вероятностных программ), априорное распределение которых приближает заданное апостериорное распределение. (справа, внизу) Записанный человеком исходный код вероятностной программы, чье априорное распределение совпадает с априорным распределением. В данном конкретном случае данная вероятностная программа может быть просто выведена аналитически, так как Бета-распределение является сопряженным к биномиальному, но в общем случае это нетривиальная задача.\nлаемого апостериорного распределения путем использования методом Монте-Карло или аналогичным им является единственным доступным средством за разумное время, эта проблема стоит еще острее.\nВ качестве самых предварительных результатов мы провели эксперимент, в рам-\nках которого наша априорная модель была моделью бета-биномиального распределения (в несжатой форме) с априорным распределением на скрытый параметр θ ∼ Beta(1.0, 1.0), и использовали алгоритм Метрополиса-Гастингса для получения выборки X̂ из апостериорного распределения скрытого параметра θ с учетом проведения четырех успешных испытаний по схеме Бернулли Bernoulli(θ). Соответствующая вероятностная программа представлена на рис. 12.\nЗатем мы использовали наш подход к выводу вероятностных программ, априорное\nраспределение которых будет статистически схожим с полученной выборкой из желаемого апостериорного распределения. Примеры найденных вероятностных программ даны на рис. 12. В данном конкретном случае мы можем аналитически найти и записать в виде вероятностной программы апостериорное распределение, равное Beta(5.0, 1.0). Таким образом, полученные результаты показывают, что мы нашли хорошее приближение апостериорного распределения."
    }, {
      "heading" : "2.6 Обобщение порождающего вероятностного программирования",
      "text" : "Как было отмечено ранее, порождающие вероятностные модели определяют совместное распределение (T,X), часто задаваемое сначала с помощью распределения (T ), а затем с помощью условного распределения (X | T ). Порождающие вероятностные модели могут использоваться либо для генерации выборок напрямую из (T,X), либо в качестве промежуточного этапа для нахождения условного распределения скрытых параметров (T | X = x̂), обычно с помощью приближенных методов статистического вывода или оптимизации.\nВ рамках наших экспериментов, описанных выше, мы решали задачу вывода веро-\nятностной программы, которая определяет вероятностную модель (в общем случае параметризованную), априорное распределение которой соответствует или приближает искомое распределение, заданное аналитически, или в виде выборки, или в виде дорогостоящего генератора элементов выборки.\nНа языке вероятностного программирования Church, поддерживающего проведение\nстатистического запроса внутри другого статистического запроса, данная задача может быть сформулирована следующим образом (подобно тому, как мы ее формулировали в Anglican/Venture, см. 4 и 5):\n1 ( query 2 ( d e f i n e program−text ( product ions INPUT−TYPES OUTPUT−TYPE) ) 3 ( d e f i n e program 4 ( eval ( l i s t ‘ lambda ‘ ( arg1 . . . arg N) program−text ) ) ) 5 program−text 6 (and 7 ( noisy−distance−equal 8 ( g e t− s t a t i s t i c s 9 ( mult iple−query\n10 ; No d e f i n e s . 11 ( program arg1_1 . . . arg1_N) 12 true ) ) 13 expe c t ed− s t a t i s t i c s 1 ) 14 . . .\n15 ( noisy−distance−equal 16 ( g e t− s t a t i s t i c s 17 ( mult iple−query 18 ; No d e f i n e s . 19 ( program argM_1 . . . argM_N) 20 true ) ) 21 expected− s tat i s t i c sM ) ) )\nгде (query defines expression predicate) — задание запроса в Church (см. [2]), INPUT-TYPES — перечисление типов входных аргументов, OUTPUT-TYPE — тип выходного значения процедуры, noisy-distance-equal — функция сравнения статистик, get-statistics — функция извлечения статистик из распределения, multiple-query — аналог query, возвращающий несколько элементов из распределения вместо одного элемента, expected-statistics... — M значений статистик искомого распределения.\nАвтор считает, что более общо, для нахождения вероятностной модели, служащей\nвспомогательным инструментом в нахождении условного апостериорного распределения P (T | X = x̂), мы можем быть заинтересованы в постановке задачи следующим образом:\n1 ( query 2 ( d e f i n e l a t en t−var i ab l e s−c r ea to r ( la tent−product ions LATENT−TYPES) ) 3 ( d e f i n e program−text 4 ( product ions LATENT−TYPES INPUT−TYPES OUTPUT−TYPE) ) 5 ( d e f i n e program 6 ( eval ( l i s t ‘ lambda ‘ ( arg1 . . . argN ) program−text ) ) ) 7 ( l i s t l a t en t−var i ab l e s−c r ea to r program−text ) 8 (and 9 ( noisy− latents−equal\n10 ( multiple−query 11 ( d e f i n e l a t en t−va r i ab l e s ( l a t en t−var i ab l e s−c r ea to r ) ) 12 l a t en t−va r i ab l e s 13 ( noisy−outputs−equal 14 ( program la t en t−va r i ab l e s arg1_1 . . . arg1_N) output1 ) )\n15 expected− l a tents1 ) 16 . . . 17 ( noisy− latents−equal 18 ( multiple−query 19 ( d e f i n e l a t en t−va r i ab l e s ( l a t en t−var i ab l e s−c r ea to r ) ) 20 l a t en t−va r i ab l e s 21 ( noisy−outputs−equal 22 ( program la t en t−va r i ab l e s argM_1 . . . argM_N) outputM ) ) 23 expected− latentsM ) ) )\nгде latent-productions — правила порождения объекта, представляющего скрытые параметры типов LATENT-TYPES, «лениво» создаваемые с помощью процедуры mem (см. [2]); latent-variables — объект, представляющий скрытые параметры; arg..._... — M значений аргументов, output... — M значений наблюдаемых данных; expected-latents... — M значений ожидаемых скрытых параметров.\nТаким образом, в рамках данной формулировки мы ищем вероятностную програм-\nму, содержащую: скрытые переменные, определяемые latent-variables; наблюдаемые переменные, определяемые выполнением program; и вывод производится с учетом M тренировочных троек\n{ai, t̂i, x̂i},\nгде ai — дополнительная параметризация, x̂i — значения наблюдаемых переменных, t̂i — ожидаемые значения скрытых переменных.\nСледует отметить, что так как в настоящее время обобщенный статистический вы-\nвод с использованием вероятностных языков программирования производится достаточно медленно, то решение практических задач таким образом пока не имеет смысла. С другой стороны, с дальнейшим развитием [4, 3] обобщенных методов статистического вывода и других решений (например, оптимизации или поиска), с использовании более продуманных алгоритмов программной реализации и структур данных [3, 27], со специализированным аппаратным обеспечением [49, 50], автоматизированная генерация и использование порождающих вероятностных моделей может успешно и продуктивно применяться на практике.\nЗАКЛЮЧЕНИЕ\nВ первой части данной работы очень кратко и реферативно представлено введение\nв вероятностное программирование на примере языков Church/Venture/Anglican, первое подобное введение на русском языке, насколько известно автору.\nВо второй части данной работы представлен подход к порождению вероятност-\nных программ, обобщающих распределения, представленные либо в виде выборки, либо в виде аналитического представления (например, в виде значения статистик), причем метавероятностная модель вероятностных моделей также записывается в виде вероятностной программы. Используя данный подход, были получены предварительные положительные результаты статистического вывода из апостериорного распределения искомых вероятностных программ методами Монте-Карло по схеме цепей Маркова, в том числе была автоматически выведена вероятностная программа, генерирующая элементы выборки всего семейства распределений Бернулли.\nСПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ\n[1] Goodman Noah D. The principles and practice of probabilistic programming // ACM\nSIGPLAN Notices / ACM.— Vol. 48. — 2013. — P. 399–402.\n[2] Church: A language for generative models / Noah D. Goodman, Vikash K. Mansinghka,\nDaniel M. Roy et al. — 2008. — P. 220–229.\n[3] Mansinghka Vikash, Selsam Daniel, Perov Yura. Venture: a higher-order probabilistic\nprogramming platform with programmable inference // arXiv preprint arXiv:1404.0099. — 2014.\n[4] Wood Frank, van de Meent Jan Willem, Mansinghka Vikash. A new approach to\nprobabilistic programming inference // Proceedings of the 17th International conference on Artificial Intelligence and Statistics. — 2014.\n[5] Approximate Bayesian image interpretation using generative probabilistic graphics\nprograms / Vikash Mansinghka, Tejas D Kulkarni, Yura N Perov, Josh Tenenbaum // Advances in Neural Information Processing Systems. — 2013. — P. 1520–1528.\n[6] Perov Yura N, Wood Frank D. Learning probabilistic programs (статья отправлена на\nконференцию). — 2014.\n[7] Лопатников Леонид Исидорович. Экономико-математический словарь. — 5-е изд., пе-\nрераб. и доп. — М.: Дело, 2003.\n[8] Ветров Дмитрий Петрович, Кропотов Дмитрий Александрович. Байесовские мето-\nды машинного обучения (курс лекций). Методы Монте-Карло по схеме Марковских цепей. — 2011. — URL: http://www.machinelearning.ru/wiki/images/6/6b/BMMO11_ 10.pdf.\n[9] Рассел Стюарт, Норвиг Питер. Искусственный интеллект: современный подход (2-е\nиздание). — «Вильямс», 2007.\n[10] Bishop Christopher. Pattern Recognition and Machine Learning. — Springer, New York,\n2006.\n[11] Murphy Kevin P. Machine learning: a probabilistic perspective. — MIT Press, 2012.\n[12] Ветров Дмитрий Петрович, Кропотов Дмитрий Александрович. Байесовские методы\nмашинного обучения (курс лекций). — 2013. — URL: http://www.machinelearning. ru/.\n[13] Сайт о вероятностном программировании. — 2014. — URL: http://\nprobabilistic-programming.org/.\n[14] Абельсон Харольд, Сассман Джеральд Джей. Структура и интерпретация компью-\nтерных программ. — Добросвет, КДУ, 2010.\n[15] Freer Cameron, Mansinghka Vikash, Roy Daniel. When are probabilistic programs\nprobably computationally tractable? // NIPS 2010 Workshop on Monte Carlo Methods in Modern Applications. — 2010.\n[16] Wingate David, Stuhlmueller Andreas, Goodman Noah D. Lightweight implementations of\nprobabilistic programming languages via transformational compilation. — 2011. — P. 131.\n[17] Николенко Сергей Игоревич. Вероятностное обучение (курс лекций). — 2007. — URL:\nhttp://logic.pdmi.ras.ru/~sergey/index.php?page=mlbayes.\n[18] Chib Siddhartha, Greenberg Edward. Understanding the metropolis-hastings algorithm //\nThe American Statistician. — 1995. — Vol. 49, no. 4. — P. 327–335.\n[19] Зубков А. М., Шуваев Д. В. Вычисление моментов комбинаторных статистик от пе-\nрестановочных случайных величин // Дискретная математика. — 2005.\n[20] Perov Yura, Mansinghka Vikash. Exploiting Conditional Independence for Efficient,\nAutomatic Multicore Inference for Church. — 2012.\n[21] Blei David M, Ng Andrew Y, Jordan Michael I. Latent dirichlet allocation // the Journal\nof machine Learning research. — 2003. — Vol. 3. — P. 993–1022.\n[22] Wu Jeff. Reduced traces and JITing in Church : Ph.D. thesis / Jeff Wu ; Massachusetts\nInstitute of Technology. — 2013.\n[23] CAPTCHA: Using hard AI problems for security / Luis Von Ahn, Manuel Blum,\nNicholas J Hopper, John Langford // Advances in Cryptology—EUROCRYPT 2003. — Springer, 2003. — P. 294–311.\n[24] Goodman N. D., Tenenbaum J. B. Probabilistic Models of Cognition. — 2014. — URL:\nhttps://probmods.org/.\n[25] A Repository for Generative Models (composite authors, edited by Andreas Stuhlmüller). —\n2014. — URL: http://forestdb.org/.\n[26] Wood Frank et al. Probabilistic Programming Tutorial (Machine Learning Summer School,\nIceland). — 2014. — URL: http://www.robots.ox.ac.uk/~fwood/anglican/teaching/ mlss2014/.\n[27] Paige Brooks, Wood Frank. A Compilation Target for Probabilistic Programming\nLanguages. — 2014.\n[28] Maddison Chris J, Tarlow Daniel. Structured Generative Models of Natural Source Code. —\n2014.\n[29] Liang Percy, Jordan Michael I, Klein Dan. Learning programs: A hierarchical Bayesian\napproach. — 2010. — P. 639–646.\n[30] Marsaglia George, Bray Thomas A. A convenient method for generating normal\nvariables. — 1964. — Vol. 6, no. 3. — P. 260–264.\n[31] Box George EP, Muller Mervin E et al. A note on the generation of random normal\ndeviates. — 1958. — Vol. 29, no. 2. — P. 610–611.\n[32] Devroye Luc. Non-Uniform Random Variate Generation. — 1986.\n[33] Gulwani Sumit, Kitzelmann Emanuel, Schmid Ute. Approaches and Applications of\nInductive Programming (Dagstuhl Seminar 13502). — 2014. — Vol. 3, no. 12. — P. 43– 66. — URL: http://drops.dagstuhl.de/opus/volltexte/2014/4507.\n[34] Looks Moshe. Program evolution for general intelligence // FRONTIERS IN ARTIFICIAL\nINTELLIGENCE AND APPLICATIONS. — 2007. — Vol. 157. — P. 125.\n[35] Muggleton Stephen, De Raedt Luc. Inductive logic programming: Theory and methods. —\n1994. — Vol. 19. — P. 629–679.\n[36] Muggleton Stephen, Feng Cao. Efficient induction of logic programs. — 1992. — Vol. 38. —\nP. 281–298.\n[37] De Raedt Luc, Kersting Kristian. Probabilistic inductive logic programming. — Springer,\n2008.\n[38] Kersting Kristian. An inductive logic programming approach to statistical relational\nlearning / IOS Press. — 2005. — P. 1–228. — URL: http://people.csail.mit.edu/ kersting/FAIAilpsrl/.\n[39] Muggleton Stephen. Stochastic logic programs. — 1996. — Vol. 32. — P. 254–264.\n[40] Exploiting compositionality to explore a large space of model structures / Roger Grosse,\nRuslan R Salakhutdinov, William T Freeman, Joshua B Tenenbaum. — 2012.\n[41] Structure discovery in nonparametric regression through compositional kernel search /\nDavid Duvenaud, James Robert Lloyd, Roger Grosse et al. — 2013.\n[42] Hwang Irvin, Stuhlmüller Andreas, Goodman Noah D. Inducing probabilistic programs by\nBayesian program merging. — 2011.\n[43] Gordon Andrew D. An Agenda for Probabilistic Programming: Usable, Portable,\nand Ubiquitous // Workshop on Probabilistic Programming: Democratizing Machine Learning. — 2013.\n[44] Markov chain Monte Carlo without likelihoods / Paul Marjoram, John Molitor,\nVincent Plagnol, Simon Tavaré. — 2003. — Vol. 100, no. 26. — P. 15324–15328.\n[45] Johnson Mark, Griffiths Thomas L, Goldwater Sharon. Adaptor grammars: A framework\nfor specifying compositional nonparametric Bayesian models. — 2007. — Vol. 19. — P. 641.\n[46] Knuth Donald E. The Art of Computer Programming, 3rd edn., vol. 2. — 1998.\n[47] Quinlan J. Ross. Simplifying decision trees. — 1987. — Vol. 27, no. 3. — P. 221–234.\n[48] Bache K., Lichman M. UCI Machine Learning Repository. — 2013. — URL: http:\n//archive.ics.uci.edu/ml.\n[49] Tenenbaum Joshua B, Jonas Eric M, Mansinghka Vikash K. Stochastic Digital Circuits for\nProbabilistic Inference. — 2008.\n[50] Jonas Eric Michael. Stochastic architectures for probabilistic computation : Ph.D. thesis /\nEric Michael Jonas ; Massachusetts Institute of Technology. — 2014."
    } ],
    "references" : [ {
      "title" : "The principles and practice of probabilistic programming",
      "author" : [ "D. Goodman Noah" ],
      "venue" : "ACM SIGPLAN Notices / ACM.—",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Church: A language for generative models",
      "author" : [ "Noah D. Goodman", "Vikash K. Mansinghka", "Daniel M. Roy" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Venture: a higher-order probabilistic programming platform with programmable inference // arXiv preprint arXiv:1404.0099",
      "author" : [ "Mansinghka Vikash", "Selsam Daniel", "Perov Yura" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "A new approach to probabilistic programming inference",
      "author" : [ "Wood Frank", "van de Meent Jan Willem", "Mansinghka Vikash" ],
      "venue" : "Proceedings of the 17th International conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Approximate Bayesian image interpretation using generative probabilistic graphics programs",
      "author" : [ "Vikash Mansinghka", "Tejas D Kulkarni", "Yura N Perov", "Josh Tenenbaum" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Learning probabilistic programs (статья отправлена на конференцию)",
      "author" : [ "N Perov Yura", "D. Wood Frank" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "Bishop Christopher" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Machine learning: a probabilistic perspective",
      "author" : [ "P. Murphy Kevin" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "When are probabilistic programs probably computationally tractable",
      "author" : [ "Freer Cameron", "Mansinghka Vikash", "Roy Daniel" ],
      "venue" : "NIPS 2010 Workshop on Monte Carlo Methods in Modern Applications",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Lightweight implementations of probabilistic programming languages via transformational compilation",
      "author" : [ "Wingate David", "Stuhlmueller Andreas", "Goodman Noah D" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Understanding the metropolis-hastings algorithm // The American Statistician",
      "author" : [ "Chib Siddhartha", "Greenberg Edward" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1995
    }, {
      "title" : "М., Шуваев Д. В. Вычисление моментов комбинаторных статистик от перестановочных случайных величин // Дискретная математика",
      "author" : [ "А. Зубков" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Exploiting Conditional Independence for Efficient, Automatic Multicore Inference for Church",
      "author" : [ "Perov Yura", "Mansinghka Vikash" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Latent dirichlet allocation // the Journal of machine Learning research",
      "author" : [ "M Blei David", "Y Ng Andrew", "I. Jordan Michael" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2003
    }, {
      "title" : "Reduced traces and JITing in Church : Ph.D",
      "author" : [ "Wu Jeff" ],
      "venue" : "Jeff Wu ; Massachusetts Institute of Technology",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Probabilistic Models of Cognition",
      "author" : [ "D. Goodman N", "B. Tenenbaum J" ],
      "venue" : "URL: https://probmods.org/",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Probabilistic Programming Tutorial (Machine Learning Summer School, Iceland)",
      "author" : [ "Wood Frank" ],
      "venue" : "http://www.robots.ox.ac.uk/~fwood/anglican/teaching/",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "A Compilation Target for Probabilistic Programming Languages",
      "author" : [ "Paige Brooks", "Wood Frank" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "Structured Generative Models of Natural Source Code",
      "author" : [ "Maddison Chris J", "Tarlow Daniel" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "Learning programs: A hierarchical Bayesian approach",
      "author" : [ "Liang Percy", "Jordan Michael I", "Klein Dan" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "A convenient method for generating normal variables",
      "author" : [ "Marsaglia George", "Bray Thomas A" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1964
    }, {
      "title" : "A note on the generation of random normal deviates",
      "author" : [ "EP Box George", "E Muller Mervin" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1958
    }, {
      "title" : "Non-Uniform Random Variate Generation",
      "author" : [ "Devroye Luc" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1986
    }, {
      "title" : "Approaches and Applications of Inductive Programming (Dagstuhl Seminar 13502)",
      "author" : [ "Gulwani Sumit", "Kitzelmann Emanuel", "Schmid Ute" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    }, {
      "title" : "Program evolution for general intelligence // FRONTIERS IN ARTIFICIAL INTELLIGENCE AND APPLICATIONS",
      "author" : [ "Looks Moshe" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2007
    }, {
      "title" : "Inductive logic programming: Theory and methods",
      "author" : [ "Muggleton Stephen", "De Raedt Luc" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1994
    }, {
      "title" : "Efficient induction of logic programs",
      "author" : [ "Muggleton Stephen", "Feng Cao" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1992
    }, {
      "title" : "Probabilistic inductive logic programming",
      "author" : [ "De Raedt Luc", "Kersting Kristian" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2008
    }, {
      "title" : "An inductive logic programming approach to statistical relational 47  learning / IOS Press",
      "author" : [ "Kersting Kristian" ],
      "venue" : null,
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2005
    }, {
      "title" : "Stochastic logic programs",
      "author" : [ "Muggleton Stephen" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1996
    }, {
      "title" : "Exploiting compositionality to explore a large space of model structures",
      "author" : [ "Roger Grosse", "Ruslan R Salakhutdinov", "William T Freeman", "Joshua B Tenenbaum" ],
      "venue" : null,
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2012
    }, {
      "title" : "Structure discovery in nonparametric regression through compositional kernel search",
      "author" : [ "David Duvenaud", "James Robert Lloyd", "Roger Grosse" ],
      "venue" : null,
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2013
    }, {
      "title" : "Inducing probabilistic programs by Bayesian program merging",
      "author" : [ "Hwang Irvin", "Stuhlmüller Andreas", "Goodman Noah D" ],
      "venue" : null,
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2011
    }, {
      "title" : "An Agenda for Probabilistic Programming: Usable, Portable, and Ubiquitous // Workshop on Probabilistic Programming: Democratizing Machine Learning",
      "author" : [ "D. Gordon Andrew" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2013
    }, {
      "title" : "Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models",
      "author" : [ "Johnson Mark", "Griffiths Thomas L", "Goldwater Sharon" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2007
    }, {
      "title" : "Simplifying decision trees",
      "author" : [ "Quinlan J. Ross" ],
      "venue" : "Vol. 27,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1987
    }, {
      "title" : "Lichman M. UCI Machine Learning Repository",
      "author" : [ "K. Bache" ],
      "venue" : "URL: http: //archive.ics.uci.edu/ml",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2013
    }, {
      "title" : "Stochastic Digital Circuits for Probabilistic Inference",
      "author" : [ "B Tenenbaum Joshua", "M Jonas Eric", "K. Mansinghka Vikash" ],
      "venue" : null,
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2008
    }, {
      "title" : "Stochastic architectures for probabilistic computation : Ph.D",
      "author" : [ "Jonas Eric Michael" ],
      "venue" : "Massachusetts Institute of Technology",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Данная бакалаврская работа посвящена вероятностному программированию [1], новому направлению в областях машинного обучения, искусственного интеллекта и компьютерных наук, а именно реферативному краткому введению в вероятностное программирование, описанию языков вероятностного программирования Church [2], Venture [3] и Anglican [4], описанию подхода порождающего вероятностного программирования для решения задач распознавания образов [5], а также представлению полученных предварительных результатов по автоматизации вывода вероятностных моделей для вероятностного программирования [6].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "Данная бакалаврская работа посвящена вероятностному программированию [1], новому направлению в областях машинного обучения, искусственного интеллекта и компьютерных наук, а именно реферативному краткому введению в вероятностное программирование, описанию языков вероятностного программирования Church [2], Venture [3] и Anglican [4], описанию подхода порождающего вероятностного программирования для решения задач распознавания образов [5], а также представлению полученных предварительных результатов по автоматизации вывода вероятностных моделей для вероятностного программирования [6].",
      "startOffset" : 301,
      "endOffset" : 304
    }, {
      "referenceID" : 2,
      "context" : "Данная бакалаврская работа посвящена вероятностному программированию [1], новому направлению в областях машинного обучения, искусственного интеллекта и компьютерных наук, а именно реферативному краткому введению в вероятностное программирование, описанию языков вероятностного программирования Church [2], Venture [3] и Anglican [4], описанию подхода порождающего вероятностного программирования для решения задач распознавания образов [5], а также представлению полученных предварительных результатов по автоматизации вывода вероятностных моделей для вероятностного программирования [6].",
      "startOffset" : 314,
      "endOffset" : 317
    }, {
      "referenceID" : 3,
      "context" : "Данная бакалаврская работа посвящена вероятностному программированию [1], новому направлению в областях машинного обучения, искусственного интеллекта и компьютерных наук, а именно реферативному краткому введению в вероятностное программирование, описанию языков вероятностного программирования Church [2], Venture [3] и Anglican [4], описанию подхода порождающего вероятностного программирования для решения задач распознавания образов [5], а также представлению полученных предварительных результатов по автоматизации вывода вероятностных моделей для вероятностного программирования [6].",
      "startOffset" : 329,
      "endOffset" : 332
    }, {
      "referenceID" : 4,
      "context" : "Данная бакалаврская работа посвящена вероятностному программированию [1], новому направлению в областях машинного обучения, искусственного интеллекта и компьютерных наук, а именно реферативному краткому введению в вероятностное программирование, описанию языков вероятностного программирования Church [2], Venture [3] и Anglican [4], описанию подхода порождающего вероятностного программирования для решения задач распознавания образов [5], а также представлению полученных предварительных результатов по автоматизации вывода вероятностных моделей для вероятностного программирования [6].",
      "startOffset" : 436,
      "endOffset" : 439
    }, {
      "referenceID" : 5,
      "context" : "Данная бакалаврская работа посвящена вероятностному программированию [1], новому направлению в областях машинного обучения, искусственного интеллекта и компьютерных наук, а именно реферативному краткому введению в вероятностное программирование, описанию языков вероятностного программирования Church [2], Venture [3] и Anglican [4], описанию подхода порождающего вероятностного программирования для решения задач распознавания образов [5], а также представлению полученных предварительных результатов по автоматизации вывода вероятностных моделей для вероятностного программирования [6].",
      "startOffset" : 584,
      "endOffset" : 587
    }, {
      "referenceID" : 6,
      "context" : "Для ознакомления с машинным обучением и искусственным интеллектом автор рекомендует следующие источники: [9, 10, 11].",
      "startOffset" : 105,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : "Для ознакомления с машинным обучением и искусственным интеллектом автор рекомендует следующие источники: [9, 10, 11].",
      "startOffset" : 105,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "В данной работе реферативно будут рассмотрены три языка вероятностного программирования: Church [2], Venture [3] и Anglican [4].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "В данной работе реферативно будут рассмотрены три языка вероятностного программирования: Church [2], Venture [3] и Anglican [4].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "В данной работе реферативно будут рассмотрены три языка вероятностного программирования: Church [2], Venture [3] и Anglican [4].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "Рассмотрим задание простой вероятностной модели Байесовской линейной регрессии [10] на языке вероятностного программирования Venture/Anglican [15] в виде вероятностной программы:",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "Рассмотрим задание простой вероятностной модели Байесовской линейной регрессии [10] на языке вероятностного программирования Venture/Anglican [15] в виде вероятностной программы:",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "Описание алгоритма Метрополиса-Гастингса в применении к «семейству» вероятностных языков Church впервые опубликовано в [2] и более подробно описано в [16].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "Описание алгоритма Метрополиса-Гастингса в применении к «семейству» вероятностных языков Church впервые опубликовано в [2] и более подробно описано в [16].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 10,
      "context" : "В таком случае для получения аппроксимации искомого апостериорного распределения в виде выборки нам будет достаточно имитировать данную цепь Маркова [8, 18].",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 11,
      "context" : "Если же r — перестановочная случайная величина [19], а такие случайные величины поддерживаются рассматриваемыми языками вероятностного программирования, то ее распределение может быть выбрано с учетом уже накопленных значений.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "Простая программная реализация впервые достаточно подробно была описана в [16]: 1.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "В [16] описывается схема адресации, которая позволяет различать случайные величины между собой.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "В случае наличия и использования перестановочных случайных величин мы можем имитировать ситуацию, что h′∗ является нашим старым состоянием, а новое состояние мы получаем в точном соответствии с ht [3].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : "Из [20].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "Например, в скрытых марковских моделях [17, 2, 4] или в латентном размещении Дирихле [21, 20].",
      "startOffset" : 39,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "Например, в скрытых марковских моделях [17, 2, 4] или в латентном размещении Дирихле [21, 20].",
      "startOffset" : 39,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "Например, в скрытых марковских моделях [17, 2, 4] или в латентном размещении Дирихле [21, 20].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Например, в скрытых марковских моделях [17, 2, 4] или в латентном размещении Дирихле [21, 20].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Из [20].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : "Описание структур данных и алгоритмов, необходимых для отслеживания зависимостей в режиме реального времени, были предварительно приведено в [20] и [22], а затем более обширно и подробно в [3].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "Описание структур данных и алгоритмов, необходимых для отслеживания зависимостей в режиме реального времени, были предварительно приведено в [20] и [22], а затем более обширно и подробно в [3].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "Описание структур данных и алгоритмов, необходимых для отслеживания зависимостей в режиме реального времени, были предварительно приведено в [20] и [22], а затем более обширно и подробно в [3].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : "3 показаны результаты применения алгоритмов и структур данных, описанных в [20, 22, 3].",
      "startOffset" : 75,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "3 показаны результаты применения алгоритмов и структур данных, описанных в [20, 22, 3].",
      "startOffset" : 75,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "3 показаны результаты применения алгоритмов и структур данных, описанных в [20, 22, 3].",
      "startOffset" : 75,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "При использовании старого подхода, описанного в [16] (см.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "В общем случае эта величина может быть достаточно большой, и в работе [22] описаны возможности более эффективного расходования памяти.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Примером иллюстрации успешного предварительного применения вероятностного программирования может служить работа [5], в которой вероятностное программирование использует-",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "из [20].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "Заинтересованному читателю мы можем порекомендовать продолжить свое знакомство с вероятностным языком Church с [2, 24, 25, 16], Venture — [3], Anglican — [26, 4, 27].",
      "startOffset" : 111,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "Заинтересованному читателю мы можем порекомендовать продолжить свое знакомство с вероятностным языком Church с [2, 24, 25, 16], Venture — [3], Anglican — [26, 4, 27].",
      "startOffset" : 111,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "Заинтересованному читателю мы можем порекомендовать продолжить свое знакомство с вероятностным языком Church с [2, 24, 25, 16], Venture — [3], Anglican — [26, 4, 27].",
      "startOffset" : 111,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Заинтересованному читателю мы можем порекомендовать продолжить свое знакомство с вероятностным языком Church с [2, 24, 25, 16], Venture — [3], Anglican — [26, 4, 27].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Заинтересованному читателю мы можем порекомендовать продолжить свое знакомство с вероятностным языком Church с [2, 24, 25, 16], Venture — [3], Anglican — [26, 4, 27].",
      "startOffset" : 154,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "Заинтересованному читателю мы можем порекомендовать продолжить свое знакомство с вероятностным языком Church с [2, 24, 25, 16], Venture — [3], Anglican — [26, 4, 27].",
      "startOffset" : 154,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "Заинтересованному читателю мы можем порекомендовать продолжить свое знакомство с вероятностным языком Church с [2, 24, 25, 16], Venture — [3], Anglican — [26, 4, 27].",
      "startOffset" : 154,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "Данная глава основана на работе [6], которая включает в себя первые предварительные результаты по этой амбициозной задаче вывода самих порождающих вероятностных моделей при наличии какой-либо информации об искомом распределении, которое определяется искомой вероятностной программой.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "Отметим, что статистический вывод в пространстве исходного кода сложен и нет какого-то простого подхода как к построению вероятностных порождающих моделей исходного кода [28], так и выводу в них [29].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 19,
      "context" : "Отметим, что статистический вывод в пространстве исходного кода сложен и нет какого-то простого подхода как к построению вероятностных порождающих моделей исходного кода [28], так и выводу в них [29].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 20,
      "context" : "ученых, которой они занимаются на протяжении десятков лет [30, 31, 32].",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "ученых, которой они занимаются на протяжении десятков лет [30, 31, 32].",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : "ученых, которой они занимаются на протяжении десятков лет [30, 31, 32].",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 23,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 25,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 26,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 27,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 28,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 29,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 30,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 205,
      "endOffset" : 213
    }, {
      "referenceID" : 31,
      "context" : "Рассматриваемые нами идеи относятся к разным областям, в том числе к автоматизации процесса программирования [33, 34], индуктивному программированию [33, 35, 36, 37, 38, 39], автоматическому моделированию [40, 41], компьютерному определению и представлению плотности распределений.",
      "startOffset" : 205,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "Идеи и методы использования вероятностного программирования для изучения и автоматизированного представления вероятностных моделей предлагались и ранее [2, 42, 43, 3].",
      "startOffset" : 152,
      "endOffset" : 166
    }, {
      "referenceID" : 32,
      "context" : "Идеи и методы использования вероятностного программирования для изучения и автоматизированного представления вероятностных моделей предлагались и ранее [2, 42, 43, 3].",
      "startOffset" : 152,
      "endOffset" : 166
    }, {
      "referenceID" : 33,
      "context" : "Идеи и методы использования вероятностного программирования для изучения и автоматизированного представления вероятностных моделей предлагались и ранее [2, 42, 43, 3].",
      "startOffset" : 152,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "Идеи и методы использования вероятностного программирования для изучения и автоматизированного представления вероятностных моделей предлагались и ранее [2, 42, 43, 3].",
      "startOffset" : 152,
      "endOffset" : 166
    }, {
      "referenceID" : 32,
      "context" : "Насколько автору известно, описываемый подход к порождению вероятностных программам мета-вероятностной программой ранее не рассматривался в качестве отдельной проблемы достаточно подробно, хотя первые шаги в исследовании и формулировке проблемы были сделаны в работах [42, 3].",
      "startOffset" : 268,
      "endOffset" : 275
    }, {
      "referenceID" : 2,
      "context" : "Насколько автору известно, описываемый подход к порождению вероятностных программам мета-вероятностной программой ранее не рассматривался в качестве отдельной проблемы достаточно подробно, хотя первые шаги в исследовании и формулировке проблемы были сделаны в работах [42, 3].",
      "startOffset" : 268,
      "endOffset" : 275
    }, {
      "referenceID" : 3,
      "context" : "Для статистического вывода мы использовали программную реализацию вероятностного языка программирования Anglican, которая поддерживает [4] статистический вывод методом частиц и методом Метрополиса-Гастингса по методу Монте-Карло по схеме цепей Маркова.",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "4, где на первой строке мы устанавливаем соответствие между T и переменной program-text, которая будет содержать один сгенерированный элемент из распределения на исходный код P (T ), определенное априорно через порождающую процедуру production с помощью адаптивной грамматики по типу [45] (см.",
      "startOffset" : 284,
      "endOffset" : 288
    }, {
      "referenceID" : 21,
      "context" : "Рисунок 6 — Найденные и записанные людьми исходные коды вероятностных программ для (слева) общего нормального распределения Normal(μ, σ) [31] и (справа) распределения Пуассона Poisson(λ) [46].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "В общих чертах оно схоже с адаптивными грамматиками [45], используемыми в [29], но имеет отличия, в частности связанные с созданием сред с ло-",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "В общих чертах оно схоже с адаптивными грамматиками [45], используемыми в [29], но имеет отличия, в частности связанные с созданием сред с ло-",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : "Для задания априорных вероятностей порождающих правил, то есть вероятностей, с которой каждое из правил будет применяться в случае возможности его применения, мы вручную составили небольшой корпус вероятностных программ, которые повторяют найденные учеными [32] алгоритмы генераторов случайных чисел.",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 35,
      "context" : "Для данного эксперимента мы выбрали три набора данных признаков обращающихся в банк клиентов для получения кредита [47, 48], и производили вывод вероятностной программы, используя для сравнения p-значения двухвыборочного теста КолмогороваСмирнова аналогично тому, как мы использовали G-тест для дискретных распределений.",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 36,
      "context" : "Для данного эксперимента мы выбрали три набора данных признаков обращающихся в банк клиентов для получения кредита [47, 48], и производили вывод вероятностной программы, используя для сравнения p-значения двухвыборочного теста КолмогороваСмирнова аналогично тому, как мы использовали G-тест для дискретных распределений.",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "[2]), INPUT-TYPES — перечисление типов входных аргументов, OUTPUT-TYPE — тип выходного значения процедуры, noisy-distance-equal — функция сравнения статистик, get-statistics — функция извлечения статистик из распределения, multiple-query — аналог query, возвращающий несколько элементов из распределения вместо одного элемента, expected-statistics.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2]); latent-variables — объект, представляющий скрытые параметры; arg.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "С другой стороны, с дальнейшим развитием [4, 3] обобщенных методов статистического вывода и других решений (например, оптимизации или поиска), с использовании более продуманных алгоритмов программной реализации и структур данных [3, 27], со специализированным аппаратным обеспечением [49, 50], автоматизированная генерация и использование порождающих вероятностных моделей может успешно и продуктивно применяться на практике.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "С другой стороны, с дальнейшим развитием [4, 3] обобщенных методов статистического вывода и других решений (например, оптимизации или поиска), с использовании более продуманных алгоритмов программной реализации и структур данных [3, 27], со специализированным аппаратным обеспечением [49, 50], автоматизированная генерация и использование порождающих вероятностных моделей может успешно и продуктивно применяться на практике.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "С другой стороны, с дальнейшим развитием [4, 3] обобщенных методов статистического вывода и других решений (например, оптимизации или поиска), с использовании более продуманных алгоритмов программной реализации и структур данных [3, 27], со специализированным аппаратным обеспечением [49, 50], автоматизированная генерация и использование порождающих вероятностных моделей может успешно и продуктивно применяться на практике.",
      "startOffset" : 229,
      "endOffset" : 236
    }, {
      "referenceID" : 17,
      "context" : "С другой стороны, с дальнейшим развитием [4, 3] обобщенных методов статистического вывода и других решений (например, оптимизации или поиска), с использовании более продуманных алгоритмов программной реализации и структур данных [3, 27], со специализированным аппаратным обеспечением [49, 50], автоматизированная генерация и использование порождающих вероятностных моделей может успешно и продуктивно применяться на практике.",
      "startOffset" : 229,
      "endOffset" : 236
    }, {
      "referenceID" : 37,
      "context" : "С другой стороны, с дальнейшим развитием [4, 3] обобщенных методов статистического вывода и других решений (например, оптимизации или поиска), с использовании более продуманных алгоритмов программной реализации и структур данных [3, 27], со специализированным аппаратным обеспечением [49, 50], автоматизированная генерация и использование порождающих вероятностных моделей может успешно и продуктивно применяться на практике.",
      "startOffset" : 284,
      "endOffset" : 292
    }, {
      "referenceID" : 38,
      "context" : "С другой стороны, с дальнейшим развитием [4, 3] обобщенных методов статистического вывода и других решений (например, оптимизации или поиска), с использовании более продуманных алгоритмов программной реализации и структур данных [3, 27], со специализированным аппаратным обеспечением [49, 50], автоматизированная генерация и использование порождающих вероятностных моделей может успешно и продуктивно применяться на практике.",
      "startOffset" : 284,
      "endOffset" : 292
    }, {
      "referenceID" : 0,
      "context" : "[1] Goodman Noah D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Church: A language for generative models / Noah D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Mansinghka Vikash, Selsam Daniel, Perov Yura.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Wood Frank, van de Meent Jan Willem, Mansinghka Vikash.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Approximate Bayesian image interpretation using generative probabilistic graphics programs / Vikash Mansinghka, Tejas D Kulkarni, Yura N Perov, Josh Tenenbaum // Advances in Neural Information Processing Systems.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Perov Yura N, Wood Frank D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[10] Bishop Christopher.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[11] Murphy Kevin P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[15] Freer Cameron, Mansinghka Vikash, Roy Daniel.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[16] Wingate David, Stuhlmueller Andreas, Goodman Noah D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[18] Chib Siddhartha, Greenberg Edward.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[19] Зубков А.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[20] Perov Yura, Mansinghka Vikash.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[21] Blei David M, Ng Andrew Y, Jordan Michael I.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[22] Wu Jeff.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[24] Goodman N.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[26] Wood Frank et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[27] Paige Brooks, Wood Frank.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[28] Maddison Chris J, Tarlow Daniel.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[29] Liang Percy, Jordan Michael I, Klein Dan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[30] Marsaglia George, Bray Thomas A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[31] Box George EP, Muller Mervin E et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[32] Devroye Luc.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[33] Gulwani Sumit, Kitzelmann Emanuel, Schmid Ute.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[34] Looks Moshe.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[35] Muggleton Stephen, De Raedt Luc.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[36] Muggleton Stephen, Feng Cao.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[37] De Raedt Luc, Kersting Kristian.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[38] Kersting Kristian.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[39] Muggleton Stephen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[40] Exploiting compositionality to explore a large space of model structures / Roger Grosse, Ruslan R Salakhutdinov, William T Freeman, Joshua B Tenenbaum.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[41] Structure discovery in nonparametric regression through compositional kernel search / David Duvenaud, James Robert Lloyd, Roger Grosse et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[42] Hwang Irvin, Stuhlmüller Andreas, Goodman Noah D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[43] Gordon Andrew D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[45] Johnson Mark, Griffiths Thomas L, Goldwater Sharon.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[47] Quinlan J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[48] Bache K.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[49] Tenenbaum Joshua B, Jonas Eric M, Mansinghka Vikash K.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[50] Jonas Eric Michael.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : null,
    "creator" : "LaTeX with hyperref package"
  }
}