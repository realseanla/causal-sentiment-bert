{
  "name" : "1502.02643.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions",
    "authors" : [ "Alina Ene", "Huy L. Nguyễn" ],
    "emails" : [ "A.Ene@dcs.warwick.ac.uk", "hlnguyen@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14]. Despite this intense focus, the running times of these algorithms are high-order polynomials in the size of the data and designing faster algorithms remains a central and challenging direction in submodular optimization.\nAt the same time, technological advances have made it possible to capture and store data at an ever increasing rate and level of detail. A natural consequence of this “big data\" phenomenon is that machine learning applications need to cope with data that is quite large and it is growing at a fast pace. Thus there is an increasing need for algorithms that are fast and scalable.\nThe general purpose algorithms for submodular minimization are designed to provide worst-case guarantees even in settings where the only structure that one can exploit is submodularity. At the other extreme, graph cut algorithms are very efficient but they cannot handle more general submodular functions. In many applications, the functions strike a middle ground between these two extremes and it is becoming increasingly more important to use their special structure to obtain significantly faster algorithms.\nFollowing [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions. We use the term simple to refer to functions F for which there\nar X\niv :1\n50 2.\n02 64\n3v 1\n[ cs\n.L G\nis an efficient algorithm for minimizing F + w, where w is a linear function. We assume that we are given black-box access to these minimization procedures for simple functions.\nDecomposable functions are a fairly rich class of functions and they arise in several applications in machine learning and computer vision. For example, they model higher-order potential functions for MAP inference in Markov random fields, the cost functions in SVM models for which the examples have only a small number of features, and the graph and hypergraph cut functions in image segmentation.\nThe recent work of [6, 8, 19] has developed several algorithms with very good empirical performance that exploit the special structure of decomposable functions. In particular, [6] have shown that the problem of minimizing decomposable submodular functions can be formulated as a distance minimization problem between two polytopes. This formulation, when coupled with powerful convex optimization techniques such as gradient descent or projection methods, it yields algorithms that are very fast in practice and very simple to implement [6].\nOn the theoretical side, the convergence behaviour of these methods is not very well understood. Very recently, Nishihara et al. [12] have made a significant progress in this direction. Their work shows that the classical alternating projections method, when applied to the distance minimization formulation, converges at a linear rate.\nOur contributions. In this work, we use random coordinate descent methods in order to obtain algorithms for minimizing decomposable submodular functions with faster convergence rates and cheaper iteration costs. We analyze a standard and an accelerated random coordinate descent algorithm and we show that they achieve linear convergence rates. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they are faster by a factor equal to the number of simple functions. Moreover, our accelerated algorithm converges in a much smaller number of iterations. We experimentally evaluate our algorithms on image segmentation tasks and we show that they perform very well and they converge much faster than the alternating projection method.\nSubmodular minimization. The first polynomial time algorithm for submodular optimization was obtained by Grötschel et al. [4] using the ellipsoid method. There are several combinatorial algorithms for the problem [17, 5, 3, 14]. Among the combinatorial methods, Orlin’s algorithm [14] achieves the best time complexity of O(n5T +n6), where n is the size of the ground set and T is the maximum amount of time it takes to evaluate the function. Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12]. Stobbe and Krause [19] use gradient descent methods with sublinear convergence rates for minimizing sums of concave functions applied to linear functions. Nishihara et al. [12] give an algorithm based on alternating projections that achieves a linear convergence rate."
    }, {
      "heading" : "1.1 Preliminaries and Background",
      "text" : "Let V be a finite ground set of size n; without loss of generality, V = {1, 2, . . . , n}. We view each point w ∈ Rn as a modular set function w(A) = ∑ i∈A wi on the ground set V .\nA set function F : 2V → R is submodular if F (A)+F (B) ≥ F (A∩B)+F (A∪B) for any two sets A,B ⊆ V . A set function Fi : 2V → R is simple if there is a fast subroutine for minimizing Fi + w for any modular function w ∈ Rn.\nIn this paper, we consider the problem of minimizing a submodular function F : 2V → R of the form F = ∑r i=1 Fi, where each function Fi is a simple submodular set function:\nmin A⊆V F (A) ≡ min A⊆V r∑ i=1 Fi(A) (DSM)\nWe assume without loss of generality that the function F is normalized, i.e., F (∅) = 0. Additionally,\nwe assume we are given black-box access to oracles for minimizing Fi + w for each function Fi in the decomposition and each w ∈ Rn.\nThe base polytope B(F ) of F is defined as follows.\nB(F ) = {w ∈ Rn | w(A) ≤ F (A) for all A ⊆ V, w(V ) = F (V )}\nThe discrete problem (DSM)1 admits an exact convex programming relaxation based on the Lovász extension of a submodular function. The Lovász extension f of F can be written as the support function of the base polytope B(F ):\nf(x) = max w∈B(F ) 〈w, x〉 ∀x ∈ Rn\nEven though the base polytope B(F ) has exponentially many vertices, the Lovász extension f can be evaluated efficiently using the greedy algorithm of Edmonds (see for example [18]). Given any point x ∈ Rn, Edmonds’ algorithm evaluates f(x) using O(n logn) × T time, where T is the time needed to evaluate the submodular function F .\nLovász showed that a set function F is submodular if and only if its Lovász extension f is convex [9]. Thus we can relax the problem of minimizing F to the following non-smooth convex optimization problem:\nmin x∈[0,1]n f(x) ≡ min x∈[0,1]n r∑ i=1 fi(x)\nwhere fi is the Lovász extension of Fi.\nThe relaxation above is exact. Given a fractional solution x to the Lovász Relaxation, the best threshold set of x has cost at most f(x).\nAn important drawback of the Lovász relaxation is that its objective function is not smooth. Following previous work [6, 12], we consider a proximal version of the problem (‖· ‖ denotes the `2-norm):\nmin x∈Rn ( f(x) + 12 ‖x‖ 2 ) ≡ min x∈Rn r∑ i=1 ( fi(x) + 1 2r ‖x‖ 2 )\n(Proximal)\nGiven an optimal solution x to the proximal problem minx∈Rn ( f(x) + 12 ‖x‖ 2 ), we can construct an optimal solution to the discrete problem (DSM) by thresholding x at zero; more precisely, the set {v ∈ V : x(v) ≥ 0} is an optimal solution to (DSM) (Proposition 8.6 in [1]).\nLemma 1 ([6]). The dual of the proximal problem\nmin x∈Rn r∑ i=1 ( fi(x) + 1 2r ‖x‖ 2 )\nis the problem\nmax y(1)∈B(F1),...,y(r)∈B(Fr) −12 ∥∥∥∥∥ r∑ i=1 y(i) ∥∥∥∥∥ 2\nThe primal and dual variables are linked as x = − ∑r i=1 y (i).\nLemma 1 was proved in [6]; we include a proof in Section A for completeness. 1DSM stands for decomposable submodular function minimization.\nRCDM Algorithm for (Prox-DSM) 〈〈We can take the initial point y0 to be 0〉〉 Start with y0 = (y(1)0 , . . . , y (r) 0 ) ∈ Y In each iteration k (k ≥ 0) Pick an index ik ∈ {1, 2, . . . , r} uniformly at random 〈〈Update the block ik〉〉 y\n(ik) k+1 ← arg min\ny∈B(Fik )\n(〈 ∇ikg(yk), y − y (ik) k 〉 +Lik2\n∥∥∥y − y(ik)k ∥∥∥2 )\nFigure 1: Random block coordinate descent method for (Prox-DSM). It finds a solution to (Prox-DSM)\ngiven access to an oracle for miny∈B(Fi)\n(\n〈y, a〉+ ‖y‖2\n)\n.\nWe write the dual proximal problem in the following equivalent form:\nmin y(1)∈B(F1),...,y(r)∈B(Fr) ∥∥∥∥∥ r∑ i=1 y(i) ∥∥∥∥∥ 2\n(Prox-DSM)\nIt follows from the discussion above that, given an optimal solution y = (y(1), . . . , y(r)) to (Prox-DSM), we can recover an optimal solution to (DSM) by thresholding x = − ∑r i=1 y (i) at zero."
    }, {
      "heading" : "2 Random Coordinate Descent Algorithm",
      "text" : "In this section, we give an algorithm for the problem (Prox-DSM) that is based on the random coordinate gradient descent method (RCDM) of [10]. The algorithm is given in Figure 1. The algorithm is very easy to implement and it uses oracles for problems of the form miny∈B(Fi) ( 〈y, a〉+ ‖y‖2 ) , where i ∈ [r] and a ∈ Rn. Since each function Fi is simple, we have such oracles that are very efficient.\nIn the remainder of this section, we analyze the convergence rate of the RCDM algorithm. We emphasize that the objective function of (Prox-DSM) is not strongly convex and thus we cannot use as a black-box Nesterov’s analysis of the RCDM method for minimizing strongly convex functions. Instead, we exploit the special structure of the problem to achieve convergence guarantees that match the rate achievable for strong convex objectives with strong convexity parameter 1/(n2r). Our analysis shows that the RCDM algorithm is faster by a factor of r than the alternating projections algorithm from [12].\nOutline of the analysis: Our analysis has two main components. First, we build on the work of [12] in order to prove a key theorem (Theorem 2). This theorem exploits the special structure of the (Prox-DSM) problem and it allows us to overcome the fact that the objective function of (Prox-DSM) is not strongly convex. Second, we modify Nesterov’s analysis of the RCDM algorithm for minimizing strongly convex functions and we replace the strong convexity guarantee by the guarantee given by Theorem 2.\nWe start by introducing some notation; for the most part, we follow the notation of [10] and [12]. Let Rnr = ⊗r i=1 Rn. We write a vector y ∈ Rnr as y = (y(1), . . . , y(r)), where each block y(i) is an n-dimensional\nvector. Let Y = ⊗r\ni=1 B(Fi) be the constraint set of (Prox-DSM). Let g : Rnr → R be the objective function of (Prox-DSM): g(y) = ∥∥∑r i=1 y (i) ∥∥2. We use ∇g to denote the gradient of g, i.e., the (nr)-dimensional vector of partial derivatives. For each i ∈ {1, . . . , r}, we use ∇ig(y) ∈ Rn to denote the i-th block of coordinates of ∇g(y).\nLet S ∈ Rn×nr be the following matrix:\nS = 1√ r [ InIn · · · In︸ ︷︷ ︸ r times ]\nNote that g(y) = r ‖Sy‖2 and ∇g(y) = 2rSTSy. Additionally, for each i ∈ {1, 2, . . . , r}, ∇ig is Lipschitz continuous with constant Li = 2:\n‖∇ig(x)−∇ig(y)‖ ≤ Li ∥∥∥x(i) − y(i)∥∥∥ , (1)\nfor all vectors x, y ∈ Rnr that differ only in block i.\nOur first step is to prove the following key theorem that builds on the work of [12].\nTheorem 2. Let y ∈ Y be a feasible solution to (Prox-DSM). Let y∗ be an optimal solution to (Prox-DSM) that minimizes ‖y − y∗‖. We have\n‖S(y − y∗)‖ ≥ 1 nr ‖y − y∗‖ .\nThe proof of Theorem 2 uses the following key result from [13]. We will need the following definitions from [13].\nLet d(K1,K2) = inf {‖k1 − k2‖ : k1 ∈ K1, k2 ∈ K2} be the distance between sets K1 and K2. Let P and Q be two closed convex sets in Rd. Let E ⊆ P and H ⊆ Q be the sets of closest points\nE = {p ∈ P : d(p,Q) = d(P,Q)} H = {q ∈ Q : d(q,P) = d(P,Q)}\nSince P and Q are convex, for each point in p ∈ E, there is a unique point q ∈ H such that d(p, q) = d(P,Q) and vice versa. Let v = ΠQ−P0; note that H = E + v. Let Q′ = Q− v; Q′ is a translated version of Q and it intersects P at E. Let\nκ∗ = sup x∈(P∪Q′)\\E d(x,E) max {d(x,P), d(x,Q′)} .\nBy combining Corollary 5 and Proposition 11 from [13], we obtain the following theorem. Theorem 3 ([12]). If P is the polyhedron ⊗r i=1 B(Fi) and Q is the polyhedron { y ∈ Rnr : ∑r i=1 y (i) = 0 } , we have κ∗ ≤ nr.\nNow we are ready to prove Theorem 2. Let\nP = ⊗r\ni=1 B(Fi) = Y Q = { y ∈ Rnr : ∑r i=1 y(i) = 0 } = {y ∈ Rnr : Sy = 0}\nWe define Q′ and κ∗ as above.\nLet y and y∗ be the two points in the statement of the theorem. Note that y ∈ P and y∗ ∈ E, since E is the set of all optimal solutions to (Prox-DSM) (see Proposition 10 in Section B for a proof). We may assume that y /∈ E, since otherwise the theorem trivially holds. Since y ∈ P \\ E, we have\nκ∗ ≥ d(y,E) d(y,Q′)\nSince y∗ is an optimal solution that is closest to y, we have d(y,E) = ‖y − y∗‖. Using the fact that the rows of S form a basis for the orthogonal complement of Q, we can show that d(y,Q′) = ‖S(y − y∗)‖ (see Proposition 11 in Section B for a proof). Therefore\nκ∗ ≥ ‖y − y∗‖ ‖S(y − y∗)‖ .\nTheorem 2 now follows from Theorem 3.\nIn the remainder of this section, we use Nesterov’s analysis [10] in conjunction with Theorem 2 in order to show that the RCDM algorithm converges at a linear rate. Recall that E is the set of all optimal solutions to (Prox-DSM).\nTheorem 4. After (k + 1) iterations of the RCDM algorithm, we have\nE [ d(yk, E)2 + g(yk+1)− g(y∗) ] ≤ (\n1− 2 n2r2 + r\n)k+1 ( d(y0, E)2 + g(y0)− g(y∗) ) ,\nwhere y∗ = arg miny∈E ‖y − yk‖ is the optimal solution that is closest to yk.\nWe devote the rest of this section to the proof of Theorem 4. We recall the following well-known lemma, which we refer to as the first-order optimality condition.\nLemma 5 (Theorem 2.2.5 in [11]). Let f : Rd → R be a differentiable convex function and let Q ⊆ Rd be a closed convex set. A point x∗ ∈ Rd is a solution to the problem minx∈Q f(x) if and only if\n〈∇f(x∗), x− x∗〉 ≥ 0\nfor all x ∈ Q.\nIt follows from the first-order optimality condition for y(ik)k+1 that, for any z ∈ B(Fik ),〈 ∇ikg(yk) + Lik ( y (ik) k+1 − y (ik) k ) , z − y(ik)k+1 〉 ≥ 0 (2)\nWe have g(yk+1) = g(yk) + ∫ 1\n0 〈yk+1 − yk,∇g(yk + t(yk+1 − yk))〉dt\n= g(yk) + 〈∇g(yk), yk+1 − yk〉+ ∫ 1\n0\n〈 yk+1 − yk,∇g(yk + t(yk+1 − yk))−∇g(yk) 〉 dt\n= g(yk) + 〈 ∇ikg(yk), y (ik) k+1 − y (ik) k 〉 + ∫ 1\n0\n〈 y\n(ik) k+1 − y (ik) k ,∇ikg(yk + t(yk+1 − yk))−∇ikg(yk)\n〉 dt\n≤ g(yk) + 〈 ∇ikg(yk), y (ik) k+1 − y (ik) k 〉 + ∫ 1\n0 ∥∥∥y(ik)k+1 − y(ik)k ∥∥∥ ‖∇ikg(yk + t(yk+1 − yk))−∇ikg(yk)‖ dt (1) ≤ g(yk) + 〈 ∇ikg(yk), y (ik) k+1 − y (ik) k 〉 + ∫ 1\n0 Lik ∥∥∥y(ik)k+1 − y(ik)k ∥∥∥2 tdt = g(yk) + 〈 ∇ikg(yk), y (ik) k+1 − y (ik) k 〉 + Lik2\n∥∥∥y(ik)k+1 − y(ik)k ∥∥∥2 (3) On the third line, we have used the fact that yk and yk+1 agree on all coordinate blocks except the ik-th block. On the fourth line, we have used the Cauchy-Schwartz inequality. On the fifth line, we have used inequality (1).\nLet y∗ = arg miny∈E ‖y − yk‖ be the optimal solution that is closest to yk. We have\n‖yk+1 − y∗‖2 = ‖yk − y∗‖2 + ‖yk+1 − yk‖2 + 2〈yk − y∗, yk+1 − yk〉 = ‖yk − y∗‖2 − ‖yk+1 − yk‖2 + 2 〈yk+1 − y∗, yk+1 − yk〉\n= ‖yk − y∗‖2 − ∥∥∥y(ik)k+1 − y(ik)k ∥∥∥2 + 2〈y(ik)k+1 − (y∗)(ik), y(ik)k+1 − y(ik)k 〉\n(2) ≤ ‖yk − y∗‖2 − ∥∥∥y(ik)k+1 − y(ik)k ∥∥∥2 + 2Lik 〈 ∇ikg(yk), (y∗)(ik) − y (ik) k+1 〉 = ‖yk − y∗‖2 +\n2 Lik\n〈 ∇ikg(yk), (y∗)(ik) − y (ik) k 〉 − 2 Lik ( Lik 2\n∥∥∥y(ik)k+1 − y(ik)k ∥∥∥2 + 〈∇ikg(yk), y(ik)k+1 − y(ik)k 〉) (3) ≤ ‖yk − y∗‖2 +\n2 Lik\n〈 ∇ikg(yk), (y∗)(ik) − y (ik) k 〉 − 2 Lik (g(yk+1)− g(yk)) (4)\nOn the third line, we have used the fact that yk and yk+1 agree on all coordinate blocks except the ik-th block. On the fourth line, we have used the inequality (2) with z = (y∗)(ik). On the last line, we have used inequality (3).\nIf we rearrange the terms of the inequality (4), take expectation over ik, and substitute Lik = 2, we obtain\nEik [ ‖yk+1 − y∗‖2 + g(yk+1)− g(y∗) ] ≤ ‖yk − y∗‖2 + g(yk)− g(y∗) +\n1 r 〈∇g(yk), y∗ − yk〉 (5)\nWe can upper bound 〈∇g(yk), y∗ − yk〉 as follows.\n〈∇g(yk), y∗ − yk〉 = 2r 〈 STSyk, y ∗ − yk 〉\n= r 〈 STSyk + STSy∗, y∗ − yk 〉 + r 〈 STSyk − STSy∗, y∗ − yk 〉 = r 〈 STSyk + STSy∗, y∗ − yk 〉 − r ‖S(yk − y∗)‖2\n= r 〈S(yk + y∗), S(y∗ − yk)〉 − r ‖S(yk − y∗)‖2\n= (g(y∗)− g(yk))− r ‖S(yk − y∗)‖2 ≤ (g(y∗)− g(yk))− 1 n2r ‖yk − y∗‖2 (By Theorem 2) (6)\nOn the first and fifth lines, we have used the fact that ∇g(z) = 2rSTSz and g(z) = r ‖Sz‖2 for any z ∈ Rnr. On the last line, we have used Theorem 2.\nSince y∗ is an optimal solution to (Prox-DSM), the first-order optimality condition gives us that\n〈∇g(y∗), y∗ − yk〉 = 2r〈STSy∗, y∗ − yk〉 ≤ 0 (7)\nUsing the inequality above, we can also upper bound 〈∇g(yk), y∗ − yk〉 as follows.\n〈∇g(yk), y∗ − yk〉 = 2r〈STSyk, y∗ − yk〉 = 2r〈STSy∗, y∗ − yk〉+ 2r〈STSyk − STSy∗, y∗ − yk〉 = 2r〈STSy∗, y∗ − yk〉 − 2r ‖S(yk − y∗)‖2\n(7) ≤ −2r ‖S(yk − y∗)‖2 ≤ − 2 n2r ‖yk − y∗‖2 (By Theorem 2) (8)\nAPPROX algorithm applied to (Prox-DSM) Start with z0 = (z(1)0 , . . . , z (r) 0 ) ∈ Y θ0 ← 1r , u0 ← 0 In each iteration k (k ≥ 0) Generate a random set of blocks Rk where each block is included independently with probability 1r uk+1 ← uk, zk+1 ← zk For each i ∈ Rk\nt (i) k ← arg mint+z(i)\nk ∈B(Fik )\n(〈 ∇ig ( θ2kuk + zk ) , t 〉 + 2rθk ‖t‖2 )\nz (i) k+1 ← z (i) k + t (i) k u (i) k+1 ← u (i) k − 1−rθk θ2\nk\nt (i) k\nθk+1 = √ θ4 k +4θ2 k −θ2k\n2 Return θ2kuk+1 + zk+1\nFigure 2: The APPROX algorithm of [2] applied to (Prox-DSM). It finds a solution to (Prox-DSM) given\naccess to an oracle for miny∈B(Fi)\n(\n〈y, a〉+ ‖y‖2\n)\n. By taking 2n2r+1 × (6) + ( 1− 2n2r+1 ) × (8), we obtain\n〈∇g(yk), y∗ − yk〉 ≤ − 2\nn2r + 1\n( g(yk)− g(y∗) + ‖yk − y∗‖2 ) (9)\nBy (5) and (9),\nE ik\n[ ‖yk+1 − y∗‖2 + g(yk+1)− g(y∗) ] ≤ (\n1− 2 n2r2 + r\n)( g(yk)− g(y∗) + ‖yk − y∗‖2 ) Note that d(yk+1, E)2 ≤ ‖yk+1 − y∗‖2 and d(yk, E)2 = ‖yk − y∗‖2. Therefore\nE ik\n[ d(yk+1, E)2 + g(yk+1)− g(y∗) ] ≤ (\n1− 2 n2r2 + r\n)( d(yk, E)2 + g(yk)− g(y∗) ) By taking expectation over ξ = (i1, . . . , ik), we get\nE ξ\n[ d(yk+1, E)2 + g(yk+1)− g(y∗) ] ≤ (\n1− 2 n2r2 + r\n)k+1 ( d(y0, E)2 + g(y0)− g(y∗) ) Therefore the proof of Theorem 4 is complete."
    }, {
      "heading" : "3 Accelerated Coordinate Descent Algorithm",
      "text" : "In this section, we give an accelerated random coordinate descent (ACDM) algorithm for (Prox-DSM). The algorithm uses the APPROX algorithm of Fercoq and Richtárik [2] as a subroutine. The APPROX algorithm (Algorithm 2 in [2]), when applied to the (Prox-DSM) problem, yields the algorithm in Figure 2. The ACDM algorithm runs in a sequence of epochs (see Figure 3). In each epoch, the algorithm starts with the solution of the previous epoch and it runs the APPROX algorithm for Θ(nr3/2) iterations. The solution constructed by the APPROX algorithm will be the starting point of the next epoch. Note that, for each i, the gradient\nACDM Algorithm for (Prox-DSM) 〈〈We can take the initial point y0 to be 0〉〉 Start with y0 = (y(1)0 , . . . , y (r) 0 ) ∈ Y In each epoch ` (` ≥ 0) Run the algorithm in Figure 2 for (4nr3/2 + 1) iterations with y` as its starting point (z0 = y`) Let y`+1 be the vector returned by the algorithm\nFigure 3: Accelerated block coordinate descent method for (Prox-DSM). It finds a solution to (Prox-DSM)\ngiven access to an oracle for miny∈B(Fi)\n(\n〈y, a〉+ ‖y‖2\n)\n.\n∇ig(y) = 2 ∑ j y\n(j) can be easily maintained at a cost of O(n) per block update, and thus the iteration cost is dominated by the time to compute projection.\nIn the remainder of this section, we use the analysis of [2] together with Theorem 2 in order to show that the ACDM algorithm converges at a linear rate. We follow the notation used in Section 2.\nTheorem 6. After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have\nE[g(y`+1)− g(y∗)] ≤ 1\n2`+1 (g(y0)− g(y ∗))\nIn the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in [2] and thus the convergence analysis given in [2] can be applied to our setting.\nLemma 7. Let R ⊆ {1, 2, . . . , r} be a random subset of coordinate blocks with the property that each i ∈ {1, 2, . . . , r} is in R independently at random with probability 1/r. Let x and h be two vectors in Rnr. Let hR be the vector in Rnr such that (hR)(i) = h(i) for each block i ∈ R and (hR)(i) = 0 otherwise. We have\nE [g (x+ hR)] ≤ g(x) + 1 r 〈∇g(x), h〉+ 2 r ‖h‖2 .\nProof: We have E [g (x+ hR)] = E [ r ‖S(x+ hR)‖2 ] = E [ r ‖Sx‖2 + r ‖ShR‖2 + 2r 〈Sx, ShR〉\n] = E [ r ‖Sx‖2 + r ‖ShR‖2 + 2r 〈 STSx, hR\n〉] = E\ng(x) + ∥∥∥∥∥ r∑ i=1 h (i) R ∥∥∥∥∥ 2 + 〈∇g(x), hR〉  = g(x) + 1\nr2 ∑ i 6=j 〈h(i), h(j)〉+ 1 r r∑ i=1 ∥∥∥h(i)∥∥∥2 + 1 r 〈∇g(x), h〉\n≤ g(x) + 1 r2 ∑ i 6=j 1 2 (∥∥∥h(i)∥∥∥2 + ∥∥∥h(j)∥∥∥2)+ 1 r r∑ i=1 ∥∥∥h(i)∥∥∥2 + 1 r 〈∇g(x), h〉\n≤ g(x) + 2 r r∑ i=1 ∥∥∥h(i)∥∥∥2 + 1 r 〈∇g(x), h〉 = g(x) + 2 r ‖h‖2 + 1 r 〈∇g(x), h〉\nLemma 7 together with Theorem 3 in [2] give us the following theorem.\nTheorem 8 (Theorem 3 of [2]). Consider iteration k of the APPROX algorithm (see Figure 2). Let yk = θ2kuk+1 + zk+1. Let y∗ = arg miny∈E ‖y − yk‖ is the optimal solution that is closest to yk. We have\nE[g(yk)− g(y∗)] ≤ 4r2\n(k − 1 + 2r)2\n(( 1− 1\nr\n) (g(z0)− g(y∗)) + 2 ‖z0 − y∗‖2 ) Proof: It follows from Lemma 7 that the objective function g of (Prox-DSM) and the random blocks Rk used by the APPROX algorithm satisfy Assumption 1 in [2] with τ = 1 and νi = 4 for each i ∈ {1, 2, . . . , r}. Thus we can apply Theorem 3 in [2].\nConsider an epoch `. Let y`+1 be the solution constructed by the APPROX algorithm after 4nr3/2 + 1 iterations, starting with y`. Let y∗ = arg miny∈E ‖y − y`+1‖ be the optimal solution that is closest to y`+1. Let ξ` denote the random choices made during epoch `. By Theorem 8,\nE ξ`\n[g(y`+1)− g(y∗)] ≤ 4r2\n(4nr3/2 + 2r)2\n(( 1− 1\nr\n) (g(y`)− g(y∗)) + 2 ‖y` − y∗‖2 ) ≤ 1 (2nr1/2 + 1)2 ( g(y`)− g(y∗) + 2 ‖y` − y∗‖2\n) We also have\ng(y`) = g(y∗) + 〈∇g(y∗), y` − y∗〉+ ∫ 1\n0 〈∇g(y∗ + t(y` − y∗))−∇g(y∗), y` − y∗〉dt\n≥ g(y∗) + ∫ 1\n0 〈∇g(y∗ + t(y` − y∗))−∇g(y∗), y` − y∗〉dt\n= g(y∗) + ∫ 1\n0 2tr ‖S(y` − y∗)‖2 dt\n= g(y∗) + r ‖S(y` − y∗)‖2 ≥ g(y∗) + 1 n2r ‖y` − y∗‖2 (By Theorem 2)\nIn the second line, we have used the first-order optimality condition for y∗ (Lemma 5). In the last line, we have used Theorem 2.\nTherefore ‖y` − y∗‖2 ≤ n2r(g(y`)− g(y∗))\nand hence\nE ξ`\n[g(y`+1)− g(y∗)] ≤ 2n2r + 1 (2nr1/2 + 1)2 ( g(y`)− g(y∗) ) ≤ 12 ( g(y`)− g(y∗)\n) Let ξ = (ξ0, . . . , ξ`) be the random choices made during the epochs 0 to `. We have\nE ξ [g(y`+1)− g(y∗)] ≤ 1 2`+1\n( g(y0)− g(y∗) ) This completes the proof of Theorem 6 and the convergence analysis for the ACDM algorithm."
    }, {
      "heading" : "4 Experiments",
      "text" : "Algorithms. We empirically evaluate and compare the following algorithms: the RCDM described in Section 2, the ACDM described in Section 3, and the alternating projections (AP) algorithm of [12]. The AP algorithm solves the following best approximation problem that is equivalent to (Prox-DSM):\nmin a∈A,y∈Y\n‖a− y‖2 (Best-Approx)\nwhere A = { (a(1), a(2), . . . , a(r)) ∈ Rnr : ∑r i=1 a (i) = 0 } and Y = ⊗r i=1 B(Fi).\nThe AP algorithm starts with a point a0 ∈ A and it iteratively constructs a sequence {(ak, yk)}k≥0 by projecting onto A and Y: yk = ΠY(ak), ak+1 = ΠA(yk).\nΠK(· ) is the projection operator onto K, that is, ΠK(x) = arg minz∈K ‖x− z‖. Since A is a subspace, it is straightforward to project onto A. The projection onto Y can be implemented using the oracles for the projections ΠB(Fi) onto the base polytopes of the functions Fi.\nFor all three algorithms, the iteration cost is dominated by the cost of projecting onto the base polytopes\nB(Fi). Therefore the total number of such projections is a suitable measure for comparing the algorithms. In each iteration, the RCDM algorithm performs a single projection for a random block i and the ACDM algorithm performs a single projection in expectation. The AP algorithm performs r projections in each iteration, one for each block.\nImage Segmentation Experiments. We evaluate the algorithms on graph cut problems that arise in image segmentation or MAP inference tasks in Markov Random Fields. Our experimental setup is similar to that of [6]. We set up the image segmentation problems on a 8-neighbor grid graph with unary potentials derived from Gaussian Mixture Models of color features [16]. The weight of a graph edge (i, j) between pixels i and j is a function of exp(−‖vi − vj‖2), where vi is the RGB color vector of pixel i. The optimization problem that we solve for each segmentation task is a cut problem on the grid graph.\nFunction decomposition: We partition the edges of the grid into a small number of matchings and we decompose the function using the cut functions of these matchings. Note that it is straightforward to project onto the base polytopes of such functions using a sequence of projections onto line segments.\nDuality gaps: We evaluate the convergence behaviours of the algorithms using the following measures. Let y be a feasible solution to the dual of the proximal problem (Proximal). The solution x = − ∑r i=1 y\n(i) is a feasible solution for the proximal problem. We define the smooth duality gap to be the difference between the objective values of the primal solution x and the dual solution y: νs = ( f(x) + 12 ‖x‖ 2 ) − ( − r2 ‖Sy‖ 2 ) . Additionally, we compute a discrete duality gap for the discrete problem (DSM) and the dual of its Lovász relaxation; the latter is the problem maxz∈B(F )(z)−(V ), where (z)− = min {z, 0} applied elementwise [6]. The best level set Sx of the proximal solution x = − ∑r i=1 y\n(i) is a solution to the discrete problem (DSM). The solution z = −x = ∑r i=1 y\n(i) is a feasible solution for the dual of the Lovász relaxation. We define the discrete duality gap to be the difference between the objective values of these solutions: νd(x) = F (Sx) − (−x)−(V ).\nWe evaluated the algorithms on four image segmentation instances2 [7, 16]. Figure 5 shows the smooth and discrete duality gaps on the four instances. Figure 4 shows some segmentation results for one of the instances.\nAcknowledgements. We thank Stefanie Jegelka for providing us with some of the data used in our experiments.\n2The data is available at http://melodi.ee.washington.edu/~jegelka/cc/index.html and http://research.microsoft. com/en-us/um/cambridge/projects/visionimagevideoediting/segmentation/grabcut.htm"
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "By the definition of the Lovász extension, for each i ∈ [r], we have\nfi(x) = max y(i)∈B(Fi)\n〈y(i), x〉.\nTherefore\nmin x∈Rn r∑ i=1 ( fi(x) + 1 2r ‖x‖ 2 )\n= min x∈Rn r∑ i=1 ( max y(i)∈B(Fi) 〈y(i), x〉+ 12r ‖x‖ 2 )\n= min x∈Rn max y(1)∈B(F1),...,y(r)∈B(Fr) r∑ i=1 ( 〈y(i), x〉+ 12r ‖x‖ 2 )\n= max y(1)∈B(F1),...,y(r)∈B(Fr) min x∈Rn r∑ i=1 ( 〈y(i), x〉+ 12r ‖x‖ 2 )\n= max y(1)∈B(F1),...,y(r)∈B(Fr) −12 ∥∥∥∥∥ r∑ i=1 y(i) ∥∥∥∥∥ 2\nOn the third line, we have used the fact that the function 〈y, x〉 + (1/2r) ‖x‖2 is convex in x and linear in y, which allows us to exchange the min and the max (see for example Corollary 37.3.2 in Rockafellar [15]). On the fourth line, we have used the fact that the minimum is achieved at x = − ∑r i=1 y (i)."
    }, {
      "heading" : "B Proofs omitted from Section 2",
      "text" : "If x ∈ Rnr and X is a subspace of Rnr, we let ΠX (x) denote the projection of x on X , that is, ΠX (x) = arg minz∈Rnr ‖x− z‖. We let X⊥ denote the orthogonal complement of the subspace X .\nProposition 9. For any point x ∈ Rnr, ΠQ⊥(x) = STSx and thus ΠQ(x) = x− STSx.\nProof: Since Q is the null space of S, Q⊥ is the row space of S. Since the rows of S are orthonormal, they form a basis for Q⊥. Therefore, if we let v1, . . . , vn denote the rows of S, we have\nΠQ⊥(x) = n∑ i=1 〈x, vi〉vi = STSx.\nProposition 10. The set of all optimal solutions to (Prox-DSM) is equal to E.\nProof: We have\nd(P,Q) = min y∈P ‖y −ΠQ(y)‖\n= min y∈P ∥∥STSy∥∥ 〈〈By Proposition 9〉〉 = min\ny∈P ‖Sy‖\nSince (Prox-DSM) is the problem miny∈P r ‖Sy‖2, E is the set of all optimal solutions to (Prox-DSM).\nProposition 11. Let y ∈ Rnr and let p ∈ E. We have d(y,Q′) = ‖S(y − p)‖.\nProof: Since Q′ = Q− v, we have\nd(y,Q′) = d(y + v,Q) = ‖ΠQ⊥(y + v)‖ = ∥∥STS(y + v)∥∥ 〈〈By Proposition 9〉〉\n= ∥∥STS(y − STSp)∥∥ 〈〈Since v = −STSp〉〉\n= ∥∥STS(y − p)∥∥ 〈〈Since SST = In〉〉\n= ‖S(y − p)‖"
    } ],
    "references" : [ {
      "title" : "Learning with submodular functions: A convex optimization perspective",
      "author" : [ "Francis Bach" ],
      "venue" : "ArXiv preprint arXiv:1111.6453,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Accelerated, parallel and proximal coordinate descent",
      "author" : [ "Olivier Fercoq", "Peter Richtárik" ],
      "venue" : "ArXiv preprint arXiv:1312.5799,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "A push-relabel framework for submodular function minimization and applications to parametric optimization",
      "author" : [ "Lisa Fleischer", "Satoru Iwata" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "The ellipsoid method and its consequences in combinatorial optimization",
      "author" : [ "Martin Grötschel", "László Lovász", "Alexander Schrijver" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1981
    }, {
      "title" : "A faster scaling algorithm for minimizing submodular functions",
      "author" : [ "Satoru Iwata" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Reflection methods for user-friendly submodular optimization",
      "author" : [ "Stefanie Jegelka", "Francis Bach", "Suvrit Sra" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Submodularity beyond submodular energies: coupling edges in graph cuts",
      "author" : [ "Stefanie Jegelka", "Jeff Bilmes" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Minimizing a sum of submodular functions",
      "author" : [ "Vladimir Kolmogorov" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Submodular functions and convexity",
      "author" : [ "László Lovász" ],
      "venue" : "In Mathematical Programming The State of the Art,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1983
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course, volume 87",
      "author" : [ "Yurii Nesterov" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "On the convergence rate of decomposable submodular function minimization",
      "author" : [ "Robert Nishihara", "Stefanie Jegelka", "Michael I Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "On the convergence rate of decomposable submodular function minimization",
      "author" : [ "Robert Nishihara", "Stefanie Jegelka", "Michael I Jordan" ],
      "venue" : "ArXiv preprint arXiv:1406.6474,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "A faster strongly polynomial time algorithm for submodular function minimization",
      "author" : [ "James B Orlin" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Convex analysis",
      "author" : [ "R Tyrrell Rockafellar" ],
      "venue" : "Number 28 in Princeton Mathematical Series. Princeton university press,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1970
    }, {
      "title" : "Grabcut: Interactive foreground extraction using iterated graph cuts",
      "author" : [ "Carsten Rother", "Vladimir Kolmogorov", "Andrew Blake" ],
      "venue" : "ACM Transactions on Graphics (TOG),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "A combinatorial algorithm minimizing submodular functions in strongly polynomial time",
      "author" : [ "Alexander Schrijver" ],
      "venue" : "Journal of Combinatorial Theory, Series B,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2000
    }, {
      "title" : "Combinatorial optimization: polyhedra and efficiency, volume 24",
      "author" : [ "Alexander Schrijver" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].",
      "startOffset" : 163,
      "endOffset" : 180
    }, {
      "referenceID" : 16,
      "context" : "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].",
      "startOffset" : 163,
      "endOffset" : 180
    }, {
      "referenceID" : 4,
      "context" : "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].",
      "startOffset" : 163,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].",
      "startOffset" : 163,
      "endOffset" : 180
    }, {
      "referenceID" : 13,
      "context" : "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].",
      "startOffset" : 163,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "The recent work of [6, 8, 19] has developed several algorithms with very good empirical performance that exploit the special structure of decomposable functions.",
      "startOffset" : 19,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "The recent work of [6, 8, 19] has developed several algorithms with very good empirical performance that exploit the special structure of decomposable functions.",
      "startOffset" : 19,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "In particular, [6] have shown that the problem of minimizing decomposable submodular functions can be formulated as a distance minimization problem between two polytopes.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "This formulation, when coupled with powerful convex optimization techniques such as gradient descent or projection methods, it yields algorithms that are very fast in practice and very simple to implement [6].",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 11,
      "context" : "[12] have made a significant progress in this direction.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] using the ellipsoid method.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "There are several combinatorial algorithms for the problem [17, 5, 3, 14].",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "There are several combinatorial algorithms for the problem [17, 5, 3, 14].",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "There are several combinatorial algorithms for the problem [17, 5, 3, 14].",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "There are several combinatorial algorithms for the problem [17, 5, 3, 14].",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "Among the combinatorial methods, Orlin’s algorithm [14] achieves the best time complexity of O(n5T +n6), where n is the size of the ground set and T is the maximum amount of time it takes to evaluate the function.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "[12] give an algorithm based on alternating projections that achieves a linear convergence rate.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "The Lovász extension f of F can be written as the support function of the base polytope B(F ): f(x) = max w∈B(F ) 〈w, x〉 ∀x ∈ R Even though the base polytope B(F ) has exponentially many vertices, the Lovász extension f can be evaluated efficiently using the greedy algorithm of Edmonds (see for example [18]).",
      "startOffset" : 304,
      "endOffset" : 308
    }, {
      "referenceID" : 8,
      "context" : "Lovász showed that a set function F is submodular if and only if its Lovász extension f is convex [9].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "min x∈[0,1]n f(x) ≡ min x∈[0,1]n r ∑",
      "startOffset" : 6,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "min x∈[0,1]n f(x) ≡ min x∈[0,1]n r ∑",
      "startOffset" : 26,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "Following previous work [6, 12], we consider a proximal version of the problem (‖· ‖ denotes the `2-norm):",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "Following previous work [6, 12], we consider a proximal version of the problem (‖· ‖ denotes the `2-norm):",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "6 in [1]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "Lemma 1 ([6]).",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 5,
      "context" : "Lemma 1 was proved in [6]; we include a proof in Section A for completeness.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "In this section, we give an algorithm for the problem (Prox-DSM) that is based on the random coordinate gradient descent method (RCDM) of [10].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "Our analysis shows that the RCDM algorithm is faster by a factor of r than the alternating projections algorithm from [12].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "First, we build on the work of [12] in order to prove a key theorem (Theorem 2).",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "We start by introducing some notation; for the most part, we follow the notation of [10] and [12].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "We start by introducing some notation; for the most part, we follow the notation of [10] and [12].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "Our first step is to prove the following key theorem that builds on the work of [12].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "The proof of Theorem 2 uses the following key result from [13].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "We will need the following definitions from [13].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "By combining Corollary 5 and Proposition 11 from [13], we obtain the following theorem.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "Theorem 3 ([12]).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "In the remainder of this section, we use Nesterov’s analysis [10] in conjunction with Theorem 2 in order to show that the RCDM algorithm converges at a linear rate.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "5 in [11]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "Figure 2: The APPROX algorithm of [2] applied to (Prox-DSM).",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "The algorithm uses the APPROX algorithm of Fercoq and Richtárik [2] as a subroutine.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "The APPROX algorithm (Algorithm 2 in [2]), when applied to the (Prox-DSM) problem, yields the algorithm in Figure 2.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "In the remainder of this section, we use the analysis of [2] together with Theorem 2 in order to show that the ACDM algorithm converges at a linear rate.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have E[g(y`+1)− g(y∗)] ≤ 1 2`+1 (g(y0)− g(y ∗)) In the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in [2] and thus the convergence analysis given in [2] can be applied to our setting.",
      "startOffset" : 230,
      "endOffset" : 233
    }, {
      "referenceID" : 1,
      "context" : "After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have E[g(y`+1)− g(y∗)] ≤ 1 2`+1 (g(y0)− g(y ∗)) In the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in [2] and thus the convergence analysis given in [2] can be applied to our setting.",
      "startOffset" : 277,
      "endOffset" : 280
    }, {
      "referenceID" : 1,
      "context" : "Lemma 7 together with Theorem 3 in [2] give us the following theorem.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "Theorem 8 (Theorem 3 of [2]).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "Proof: It follows from Lemma 7 that the objective function g of (Prox-DSM) and the random blocks Rk used by the APPROX algorithm satisfy Assumption 1 in [2] with τ = 1 and νi = 4 for each i ∈ {1, 2, .",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "Thus we can apply Theorem 3 in [2].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "We empirically evaluate and compare the following algorithms: the RCDM described in Section 2, the ACDM described in Section 3, and the alternating projections (AP) algorithm of [12].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "Our experimental setup is similar to that of [6].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "We set up the image segmentation problems on a 8-neighbor grid graph with unary potentials derived from Gaussian Mixture Models of color features [16].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "Additionally, we compute a discrete duality gap for the discrete problem (DSM) and the dual of its Lovász relaxation; the latter is the problem maxz∈B(F )(z)−(V ), where (z)− = min {z, 0} applied elementwise [6].",
      "startOffset" : 208,
      "endOffset" : 211
    }, {
      "referenceID" : 6,
      "context" : "We evaluated the algorithms on four image segmentation instances2 [7, 16].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "We evaluated the algorithms on four image segmentation instances2 [7, 16].",
      "startOffset" : 66,
      "endOffset" : 73
    } ],
    "year" : 2015,
    "abstractText" : "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of “simple\" functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.",
    "creator" : "LaTeX with hyperref package"
  }
}