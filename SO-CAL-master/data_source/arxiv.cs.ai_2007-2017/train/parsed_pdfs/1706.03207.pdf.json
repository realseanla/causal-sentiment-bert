{
  "name" : "1706.03207.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards Statistical Reasoning in Description Logics over Finite Domains (Full Version)",
    "authors" : [ "Rafael Peñaloza", "Nico Potyka" ],
    "emails" : [ "rafael.penaloza@unibz.it", "npotyka@uni-osnabrueck.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 6.\n03 20\n7v 1\n[ cs\n.A I]\n1 0\nJu n\nproportions of the domain and are interested in the probabilistic-logical consequences of these proportions. After introducing some general reasoning problems and analyzing their properties, we present first algorithms and complexity results for reasoning in some fragments of StatisticalALC."
    }, {
      "heading" : "1 Introduction",
      "text" : "Probabilistic logics enrich classical logics with probabilities in order to incorporate uncertainty. In [5], probabilistic logics have been classified into three types that differ in\nthe way how they handle probabilities. Type 1 logics enrich classical interpretations\nwith probability distributions over the domain and are well suited for reasoning about statistical probabilities. This includes proportional statements like “2% of the population suffer from a particular disease.” Type 2 logics consider probability distributions\nover possible worlds and are better suited for expressing subjective probabilities or degrees of belief. For instance, a medical doctor might say that she is 90% sure about her diagnosis. Type 3 logics combine type 1 and type 2 logics allow to reason about both\nkinds of uncertainty.\nOne basic desiderata of probabilistic logics is that they generalize a classical logic.\nThat is, the probabilistic interpretation of formulas with probability 1 should agree with the classical interpretation. However, given that first-order logic is undecidable, a prob-\nabilistic first-order logic that satisfies our basic desiderata will necessarily be undecid-\nable. In order to overcome the problem, we can, for instance, restrict to Herbrand interpretations over a fixed domain [2,9,13] or consider decidable fragments like description\nlogics [3, 8, 10].\nProbabilistic type 2 extensions of description logics have been previously studied in [11]. In the unpublished appendix of this work, a type 1 extension of ALC is presented along with a proof sketch for EXPTIME-completeness of the corresponding satisfiability problem. This type 1 extension enriches classical interpretations with proba-\nbility distributions over the domain as suggested in [5]. We consider a similar, but more restrictive setting here. We are interested in anALC extension that allows statistical reasoning. However, we do not impose a probability distribution over the domain. Instead,\nwe are only interested in reasoning about the proportions of a population satisfying\nsome given properties. For instance, given statistical information about the relative fre-\nquency of certain symptoms, diseases and the relative frequency of symptoms given diseases, one can ask the relative frequency of a disease given a particular combination of symptoms. Therefore, we consider only classical ALC interpretations with finite domains and are interested in the relative proportions that are true in these interpretations.\nHence, interpretations in our framework can be regarded as a subset of the interpre-\ntations in [11], namely those with finite domains and a uniform probability distribution\nover the domain. These interpretations are indeed sufficient for our purpose. In particular, by considering strictly less interpretations, we may be able to derive tighter answer\nintervals for some queries. Our approach bears some resemblance to the random world approach from [4]. However, the authors in [4] consider possible worlds with a fixed domain size N and are interested in the limit of proportions as N goes to infinity. We are interested in all finite possible worlds that satisfy certain proportions and ask what statistical statements must be true in all these worlds.\nWe begin by introducing Statistical ALC in Section 2 together with three relevant reasoning problems. Namely, the Satisfiability Problem, the l-Entailment problem and\nthe p-Entailment problem. In Section 3, we will then discuss some logical properties of StatisticalALC . In Section 4 and 5, we present first computational results for fragments of Statistical ALC.\n2 Statistical ALC\nWe start by revisiting the classical description logic ALC. Given two disjoint sets NC of concept names and NR of role names, ALC concepts are built using the grammar rule C ::= ⊤ | A | ¬C | C ⊓ C | ∃r.C, where A ∈ NC and r ∈ NR. One can express disjunction, universal quantification and subsumption through the usual logical equivalences like C1 ⊔ C2 ≡ ¬(¬C1 ⊓ ¬C2). For the semantics, we focus on finite interpretations. An ALC interpretation I = (∆I , ·I) consist of a non-empty, finite domain ∆I and an interpretation function ·I that maps concept names A ∈ NC to sets AI ⊆ ∆I and roles names r ∈ NR to binary relations rI ⊆ ∆I × ∆I . Two ALC concepts C1, C2 are equivalent (C1 ≡ C2) iff CI1 = C I 2 for all interpretations I.\nHere, we consider a probabilistic extension of ALC. Statistical ALC knowledge bases consist of probabilistic conditionals that are built up overALC concepts.\nDefinition 1 (Conditionals, Statistical KB). A probabilistic ALC conditional is an expression of the form (C | D)[ℓ, u], where C,D are ALC concepts and ℓ, u ∈ Q are rational numbers such that 0 ≤ ℓ ≤ u ≤ 1. A statistical ALC knowledge base (KB) is a set K of probabilisticALC conditionals.\nFor brevity, we usually call probabilisticALC conditionals simply conditionals.\nExample 2. Let Kflu = {(∃has.fever | ∃has.flu)[0.9, 0.95], (∃has.flu | ⊤)[0.01, 0.03]}. Kflu states that 90 to 95 percent of patients who have the flu have fever, and that only 1 to 3 percent of patients have the flu.\nIntuitively, a conditional (C | D)[ℓ, u] expresses that the relative proportion of elements ofD that also belong to C is between ℓ and u. In order to make this more precise, consider a finite ALC interpretation I, and an ALC concept X . We denote the cardinality of XI by [X ]I , that is, [X ]I := |XI |. The interpretation I satisfies (C | D)[ℓ, u], written as I |= (C | D)[ℓ, u], iff either [D]I = 0 or\n[C ⊓D]I\n[D]I ∈ [ℓ, u]. (1)\nI satisfies a statisticalALC knowledge baseK iff it satisfies all conditionals inK. In this case, we call I a model of K and write I |= K. We denote the set of all models of K by Mod(K). As usual, K is consistent ifMod(K) 6= ∅ and inconsistent otherwise. We call two knowledge basesK1,K2 equivalent and writeK1 ≡ K2 iffMod(K1) = Mod(K2).\nExample 3. Consider again the KB Kflu from Example 2. Let I be an interpretation with 1000 individuals. 10 of these have the flu and 9 have both the flu and fever. Then I ∈ Mod(Kflu).\nIn classical ALC, knowledge bases are defined by a set of general concept inclusions (GCIs) C ⊑ D that express that C is a subconcept of D. An interpretation I satisfies C ⊑ D iff CI ⊆ DI . As shown next, GCIs can be seen as a special kind of conditionals, and hence statisticalALC KBs are a generalization of classicalALC KBs.\nProposition 4. For all statistical ALC interpretations I, we have I |= C ⊑ D iff I |= (D | C)[1, 1].\nProof. If I |= C ⊑ D then CI ⊆ DI and CI ∩ DI = CI . If CI = ∅, we have [C]I = 0. Otherwise [C⊓D] I\n[C]I = 1. Hence, I |= (D | C)[1, 1].\nConversely, assume I |= (D | C)[1, 1]. If [C]I = 0, then CI = ∅ and I |= C ⊑ D.\nOtherwise, [C⊓D]I\n[C]I = 1, that is, [C ⊓D] I = [C]I . If there was a d ∈ CI \\DI , we had\n[C ⊓D]I < [C]I , hence, we have CI ⊆ DI and I |= C ⊑ D. ⊓⊔\nGiven a statistical ALC knowledge base K, the first problem that we are interested in is deciding consistency of K. We define the satisfiability problem for statistical ALC knowledge bases as usual.\nSatisfiability Problem: Given a knowledge base K, decide whetherMod(K) 6= ∅.\nExample 5. Consider again the knowledge base Kflu from Example 2. The conditional (∃has.flu | ⊤)[0.01, 0.03] implies that [∃has.flu]I ≥ 0.01 for all models I ∈ Mod(Kflu). (∃has.fever | ∃has.flu)[0.9, 0.95] implies [∃has.fever⊓∃has.flu]I ≥ 0.9[∃has.flu]I . Therefore, [∃has.fever]I ≥ [∃has.fever ⊓ ∃has.flu]I ≥ 0.9[∃has.flu]I ≥ 0.009. Hence, adding the conditional (∃has.fever | ⊤)[0, 0.005]} renders Kflu inconsistent.\nIf K is consistent, we are interested in deriving (implicit) probabilistic conclusions. We can think of different reasoning problems in this context. First, we can define an entailment relation analogously to logical entailment. Then, the probabilistic conditional\n(C | D)[ℓ, u] is an l-consequence of the KB K iff Mod(K) ⊆ Mod({(C | D)[ℓ, u]}). In this case, we write K |=l (C | D)[ℓ, u]. In the context of type 2 probabilistic conditionals, this entailment relation has also been called just logical consequence [9].\nl-Entailment Problem: Given a knowledge base K and a conditional (C | D)[ℓ, u], decide whether K |=l (C | D)[ℓ, u].\nExample 6. Consider again the KB Kflu from Example 2. As explained in Example 5, [∃has.fever]I ≥ 0.009 holds for all models I ∈ Mod(K). Therefore, it follows that Kflu |=l (∃has.fever | ⊤)[0.009, 1]. That is, our statistical information suggests that at least 9 out of 1, 000 of our patients have fever.\nExample 7. Consider a domain with birds (B), penguins (P) and flying animals (F). We let Kbirds = {(B | ⊤)[0.5, 0.6], (F | B)[0.85, 0.9], (F | P )[0, 0]}. Note that the conditional (F | B)[0.85, 0.9] is actually equivalent to (¬F | B)[0.1, 0.15]. Furthermore, for all I ∈ Mod(Kbirds), (F | P )[0, 0] implies [P ⊓ F ]I = 0. Therefore, we have [P ⊓B]I = [B ⊓ P ⊓ F ]I + [B ⊓ P ⊓ ¬F ]I ≤ 0 + [B ⊓ ¬F ]I ≤ 0.15[B]I . Hence, Kbirds |=l (P | B)[0, 0.15]. That is, our statistical information suggests that at most 15 out of 100 birds in our population are penguins.\nAs usual, the satisfiability problem can be reduced to the l-entailment problem.\nProposition 8. K is inconsistent iff K |=l (⊤ | ⊤)[0, 0].\nProof. If K is inconsistent, thenMod(K) = ∅ and so K |=l (⊤ | ⊤)[0, 0].\nConversely, assume K |=l (⊤ | ⊤)[0, 0]. We have [⊤]I > 0 and [⊤⊓⊤]I\n[⊤]I = 1 for all\ninterpretations I. Hence,Mod({(⊤ | ⊤)[0, 0]}) = ∅ and since K |=l (⊤ | ⊤)[0, 0], we must haveMod(K) = ∅ as well. ⊓⊔\nOften, we do not want to check whether a specific conditional is entailed, but rather deduce tight probabilistic bounds for a statement. This problem is often referred to\nas the probabilistic entailment problem in other probabilistic logics, see [6, 9, 13] for instance. Consider a query of the form (C | D), where C,D are ALC concepts. We define the p-Entailment problem similar to the probabilistic entailment problem for\ntype 2 probabilistic logics.\np-Entailment Problem: Given knowledge base K and a query (C | D), find minimal and maximal solutions of the optimization problems\ninf I∈Mod(K) / sup I∈Mod(K)\n[C ⊓D]I\n[D]I\nsubject to [D]I > 0\nSince the objective function [C⊓D]I\n[D]I is bounded from below by 0 and from above by\n1, the infimum m and the maximum M are well-defined whenever there is a model I ∈ Mod(K) such that [D]I > 0. In this case, we say that K p-entails (C | D)[m,M ] and write K |=p (C | D)[m,M ]. In the context of type 2 probabilistic conditionals,\nthis entailment relation has also been called tight logical consequence [9]. If [D]I = 0 for all I ∈ Mod(K), the p-Entailment problem is infeasible, that is, there exists no solution.\nExample 9. In Example 7, we found that Kbirds |=l (P | B)[0, 0.15]. This bound is actually tight. Since 0 is always a lower bound and we showed that 0.15 is an upper bound, it suffices to give examples of interpretations that take these bounds. For the lower bound, let I0 be an interpretation with 200 individuals. 100 of these individuals are birds and 85 are birds that can fly. There are no penguins. Then I0 is a model of Kbirds with [B]I0 > 0 that satisfies (P | B)[0, 0]. Construct I1 from I0 by letting the 15 non-flying birds be penguins. Then I1 is another model of Kbirds and I1 satisfies (P | B)[0.15, 0.15]. Hence, we also have Kbirds |=p (P | B)[0, 0.15].\nIf K |=p (C | D)[m,M ], one might ask whether the values between m and M are actually taken by some model of K or whether there can be large gaps in between. For the probabilistic entailment problem for type 2 logics, we can show that the models of K do indeed yield a dense interval by noting that each convex combination of models is a model and applying the Intermediate Value Theorem from Real Analysis. However,\nin our framework, we do not consider probability distributions over possible worlds, but the worlds themselves, which are discrete in nature.We therefore cannot apply the same\ntools here. However, for each two models that yield different probabilities for a query,\nwe can find another model that takes the probability in the middle of these probabilities.\nLemma 10 (Bisection Lemma). Let C,D be two arbitrary ALC concepts. If there exist I0, I1 ∈ Mod(K) such that r0 = [C⊓D]I0 [D]I0 < [C⊓D] I1 [D]I1 = r1, then there is an I0.5 ∈ Mod(K) such that [C⊓D]I0.5\n[D]I0.5 = r0+r12 .\nProof. Given an interpretation I and n ∈ N, we construct the interpretation I(n) as follows. We set∆I (n) = {d1, . . . , dn | d ∈ ∆I}; that is, we make n different copies of the domain. For allA ∈ NC, we setA I(n) = {d1, . . . , dn | d ∈ A I}, and for all r ∈ NR, we set rI (n) = {(d1, e1), . . . , (d1, en), . . . (dn, e1), . . . , (dn, en) | (d, e) ∈ rI}. By induction on the shape of ALC concepts, we can show that [F ]I = n[F ]I (n) for all concepts F . Let now (Ci | Di)[ℓi, ui], i = 1, . . . , |K| be all the conditionals from K. Let ℓ be the least commonmultiple of all values from [D]I0 , [D]I1 , [D1] I0 , [D1] I1 , . . . , [Dn] I0 , [Dn] I1 that are non-zero, and k,K, k1,K1, . . . , kn,Kn be such that k[D]\nI0 = ℓ, K[D]I1 = ℓ, . . . , kn[Dn] I0 = ℓ, Kn[Dn] I1 = ℓ. Assume w.l.o.g. that I0 and I1 have different domains (just rename the elements of one domain if necessary). For n,N ∈ N, let In,N be the interpretation that is obtained from I (n) 0 and I (N) 1 by taking the union of the domains, concept and role interpretations. That is, ∆In,N = ∆I (n) 0 ∪ ∆I (N) 1 , AIn,N = AI (n) 0 ∪ AI (N) 1 and rIn,N = rI (n) 0 ∪ rI (N) 1 . Consider k′ = k\n∏n i=1 ki,\nk′−j = k ∏ i6=j ki,K ′ = K ∏n i=1 Ki andK ′ −j = K ∏n i6=j Ki. Then, for i = 1, . . . , |K|,\n[Ci ⊓Di] Ik′n,K′N\n[Di] Ik′n,K′N\n= [Ci ⊓Di]I\n(k′n) 0 + [Ci ⊓Di]I (K′N) 1\n[Di]I (k′n) 0 + [Di]I (K′N) 1\n= k′n[Ci ⊓Di]I0 +K ′N [Ci ⊓Di]I1\nk′n[Di]I0 +K ′N [Di]I1\n= k′n[Ci ⊓Di]I0 +K ′N [Ci ⊓Di]I1\n(k′−in+K ′ −iN)l\n= k′n\nk′−in+K ′ −iN\n[Ci ⊓Di]I0\nki[Di]I0 +\nK ′N\nk′−in+K ′ −iN\n[Ci ⊓Di]I1\nKi[Di]I1\n= k′n\nk′−in+K ′ −iN\n[Ci ⊓Di] I0\n[Di]I0 +\nK ′−iN\nk′−in+K ′ −iN\n[Ci ⊓Di] I1\n[Di]I1 .\nThe last equality shows that [Ci⊓Di]\nI k′n,K′N\n[Di] I k′n,K′N\nis a convex combination of [Ci⊓Di]\nI0\n[Di]I0 and\n[Ci⊓Di] I1\n[Di]I1 . Since, I0 and I1 satisfy the i-th conditional, Ik′n,K′N satisfies the condi-\ntional as well. In case that both [Di] I0 = 0 and [Di] I1 = 0, we have [Di] Ik′n,K′N = 0 as well and so the conditional is still satisfied. If only [Di] I0 = 0, we can see from the second inequality that [Ci⊓Di] I k′n,K′N\n[Di] I k′n,K′N\n= k ′n0+K′N [Ci⊓Di] I1 k′n0+K′N [Di]I1 = [Ci⊓Di] I1 [Di]I1 and\nthe conditional is still satisfied. The case [Di] I1 = 0 is analogous of course. Hence, Ik′n,K′N ∈ Mod(K) for all choices of n and N .\nLet k0 = ∏n i=1 ki andK0 = ∏n i=1 Ki. Then we can show completely analogously\nthat [C⊓D]\nI k′n,K′N\n[D] I k′n,K′N\n= k0nk0n+K0N [Ci⊓Di]\nI0\n[Di]I0 + K0Nk0n+K0N\n[Ci⊓Di] I1\n[Di]I1 . Letting n = K ′ and\nN = K ′, we have [C⊓D] I k′K′,K′k′\n[D] I k′K′,K′k′\n= 12 [Ci⊓Di]\nI0\n[Di]I0 + 12\n[Ci⊓Di] I1\n[Di]I1 = r0+r12 . ⊓⊔\nWe can now show that for each value between the lower and upper bound given by p-entailment, we can find a model that gives a probability arbitrarily close to this value.\nProposition 11 (Intermediate Values). Let K |=p (C | D)[m,M ]. Then for every x ∈ (m,M) (where (m,M) denotes the open interval between m and M ) and for all ǫ > 0, there is a Ix,ǫ ∈ Mod(K) such that | [C⊓D]Ix,ǫ\n[D]Ix,ǫ − x| < ǫ.\nProof. Since K |=p (C | D)[m,M ], there must exist an I0 ∈ Mod(K) such that m ≤ [C⊓D] I0\n[D]I0 ≤ x and an I1 ∈ Mod(K) such that x ≤\n[C⊓D]I1\n[D]I1 ≤ M .\nConsider the following bisection algorithm: we let I⊥0 = I0, I ⊤ 0 = I1. Then start-\ning from i = 1, we let I0.5i be the model of K that is obtained from I ⊥ i−1 and I ⊤ i−1 as explained in the bisection lemma. If [C⊓D]I 0.5 i\n[D]I 0.5 i\n= x, we are done. Otherwise, if\n[C⊓D]I 0.5 i\n[D]I 0.5 i\n< x, we let I⊥i = I ⊥ i−1 and I ⊤ i = I\n0.5. Otherwise, we have [C⊓D]I\n0.5 i\n[D]I 0.5 i\n< x,\nand we let I⊥i = I 0.5 i and I ⊤ i = I ⊤ i−1. By construction, we maintain the invariant [C⊓D]I ⊥ i\n[D]I ⊥ i\n≤ x ≤ [C⊓D] I ⊤ i\n[D]I ⊤ i\nand we have [C⊓D]I\n⊤ i\n[D]I ⊤ i\n− [C⊓D] I ⊥ i\n[D]I ⊥ i\n≤ M−m2n . Hence, after at\nmost i = ⌈ log ( M−m ǫ )⌉ iterations, I0.5i is a model of K that proves the claim. ⊓⊔"
    }, {
      "heading" : "3 Logical Properties",
      "text" : "We now discuss some logical properties of Statistical ALC. We already noted that Statistical ALC generalizes classical ALC in Proposition 4. Furthermore, p-entailment yields a tight and dense (Proposition 11) answer interval for all queries whose condi-\ntion can be satisfied by models of the knowledge base. Let us also note that statistical ALC is language invariant. That is, increasing the language by adding new concept or role names does not change the semantics of ALC. This can be seen immediately by observing that the interpretation of conditionals in (1) depends only on the concept and role names that appear in the conditional.\nStatistical ALC is also representation invariant in the sense that for all concepts C1, D1 and C2, D2, if C1 ≡ C2 and D1 ≡ D2 then (C1 | D1)[l, u] ≡ (C2 | D2)[l, u]. Hence, changing the syntactic representation of conditionals does not change their se-\nmantics. In particular, entailment results are independent of such changes.\nBoth l- and p-entailment satisfy the following independence property: whether or not K |=l (C | D)[ℓ, u] (K |=p (C | D)[m,M ]) depends only on the conditionals in K that are connected with the query. This may simplify answering the query by reducing the size of the KB. In order to make this more precise, we need some additional definitions. For an arbitraryALC concept C, Sig(C) denotes the set of all concept and role names appearing in C. The conditionals (C1 | D1)[ℓ1, u1] and (C2 | D2)[ℓ2, u2] are directly connected (written (C1 | D1)[ℓ1, u1] ⇋ (C2 | D2)[ℓ2, u2]) if and only if (Sig(C1) ∪ Sig(D1)) ∩ (Sig(C2) ∪ Sig(D2)) 6= ∅. That is, two conditionals are directly connected iff they share concept or role names. Let ⇋∗ denote the transitive closure of ⇋. We say that (C1 | D1)[ℓ1, u1] and (C2 | D2)[ℓ2, u2] are connected iff (C1 | D1)[ℓ1, u1] ⇋∗ (C2 | D2)[ℓ2, u2]. The restriction ofK to conditionals connected to (C | D)[ℓ, u] is the set {κ ∈ K | κ ⇋∗ (C | D)[ℓ, u]}. Using an analogous definition for queries (qualitative conditionals) (C1 | D1) and (C2 | D2), we get the following result.\nProposition 12 (Independence). If K is consistent, we have\n1. K |=l (C | D)[ℓ, u] iff {κ ∈ K | κ ⇋∗ (C | D)[ℓ, u]} |=l (C | D)[ℓ, u]. 2. K |=p (C | D)[m,M ] iff {κ ∈ K | κ ⇋∗ (C | D)} |=p (C | D)[m,M ].\nProof. For both claims, it suffices to show that for each model I1 of K, there is a model I2 of {κ ∈ K | κ ⇋∗ (C | D)} ({κ ∈ K | κ ⇋∗ (C | D)[ℓ, u]}) such that [D]I1 = [D]I2 and [C ⊓D]I1 = [C ⊓D]I2 and vice versa.\nIf I1 is a model of K, let I2 be the restriction of I1 to the concept and role names in {κ ∈ K | κ ⇋∗ (C | D)}. Then I2 is still a model of {κ ∈ K | κ ⇋∗ (C | D)}. In particular, [D]I1 = [D]I2 and [C ⊓D]I1 = [C ⊓D]I2 .\nConversely, let I2 be a model of {κ ∈ K | κ ⇋ ∗ (C | D)}. By consistency of K, there is a model I0 ofK. Let I1 be the interpretation defined as the disjoint union of I0 and I2. Since {κ ∈ K | κ ⇋∗ (C | D)} and K\\{κ ∈ K | κ ⇋∗ (C | D)} do not share any concept and role names by definition of connectedness, I1 satisfies conditionals in {κ ∈ K | κ ⇋∗ (C | D)} iff I2 does and conditionals in K \\ {κ ∈ K | κ ⇋∗ (C | D)} iff I0 does. Hence, I1 is a model of K. In particular, it holds that [D]I1 = [D]I2 and [C ⊓D]I1 = [C ⊓D]I2 . ⊓⊔\nAnother interesting property of probabilistic logics is continuity. Intuitively, continuity\nstates that minor changes in the knowledge base do not yield major changes in the\nderived probabilities. However, as demonstrated by Courtney and Paris, this condition is too strong when reasoningwith the maximum entropymodel of the knowledge base [14,\np. 90]. The same problem arises for the probabilistic entailment problem [16, Example 4]. While these logics considered subjective probabilities, the same problem occurs in\nour setting for statistical probabilities as we demonstrate now.\nExample 13. Consider the knowledge base\nK = {(B | A)[0.4, 0.5], (C | A)[0.5, 0.6], (B | C)[1, 1], (C | B)[1, 1]}.\nThe interpretation I = ({a, b}, ·I) with AI = {a, b}, BI = CI = {b} is a model of K, i.e., K is consistent. In particular, since A is interpreted by the whole domain of I we know that\nK |=p (A | ⊤)[m, 1]\nfor some m ∈ [0, 1]. As explained in Proposition 4, deterministic conditionals correspond to concept inclusions and so (B | C)[1, 1] and (C | B)[1, 1] imply that\nBI ′ = CI ′ for all models I ′ of K. Therefore, [B⊓A] I ′\n[A]I′ = [C⊓A]\nI ′\n[A]I′ . Let K′ denote\nthe knowledge base that is obtained from K by decreasing the upper bound of the first conditional in K by an arbitrarily small ǫ > 0. That is,\nK′ = {(B | A)[0.4, 0.5− ǫ], (C | A)[0.5, 0.6], (B | C)[1, 1], (C | B)[1, 1]}.\nThen the only way to satisfy the first two conditionals in K′ is by interpretingA by the empty set. Indeed, the interpretation I∅ that interprets all concept names by the empty set is a model of K′. So K′ is consistent and\nK′ |=p (A | ⊤)[0, 0].\nHence, a minor change in the probabilities in the knowledge base can yield a severe\nchange in the entailed probabilities. This means that the p-entailment relation that we consider here is not continuous in this way either.\nAs an alternative to this strong notion of continuity, Paris proposed to measure the\ndifference between KBs by the Blaschke distance between their models. Blaschke continuity says that if KBs are close with respect to the Blaschke distance, the entailed\nprobabilities are close. Blaschke continuity is satisfied by some probabilistic logics un-\nder maximum entropy and probabilistic entailment [14, 16]. In [14, 16], probabilistic interpretations are probability distributions over a finite number of classical interpre-\ntations and the distance between two interpretations is the distance between the corresponding probability vectors. We cannot apply this definition here because we interpret\nconditionals by means of classical interpretations. It is not at all clear what a reasonable\ndefinition for the distance between two classical interpretations is. We leave the search for a reasonable topology on the space of classical interpretations for future work."
    }, {
      "heading" : "4 Statistical EL",
      "text" : "Proposition 4 and the fact that reasoning in ALC is EXPTIME-complete, show that our reasoning problems are EXPTIME-hard. However, we did not find any upper bounds on the complexity of reasoning in ALC so far. We will therefore focus on some fragments of ALC now.\nTo begin with, we will focus on the sublogic EL [1] of ALC that does not allow for negation and universal quantification. Formally, EL concepts are constructed by the grammar ruleC ::= A | ⊤ | C⊓C | ∃r.C, whereA ∈ NC and r ∈ NR. A statistical EL KB is a statisticalALC KBwhere conditionals are restricted to EL concepts. Notice that, due to the upper bounds in conditionals, statistical EL KBs are capable of expressing some weak variants of negations. For instance, a statement (C | ⊤)[ℓ, u] with u < 1 restricts every model I = (∆I , ·I) to contain at least one element δ ∈ ∆I \\ CI . Thus, contrary to classical EL, statistical EL KBs may be inconsistent.\nExample 14. Consider the KB K1 = (∅, C1), where\nC1 = {(A | ⊤)[0, 0.2], (A | ⊤)[0.3, 1]}.\nSince ⊤I = ∆I 6= ∅, every model I = (∆I , ·I) of K1 must satisfy\n[A]I ≤ 0.2[⊤]I < 0.3[⊤]I ≤ [A]I ,\nwhich is clearly a contradiction. Thus, K1 is inconsistent.\nMore interestingly, though, it is possible to simulate valuations over a finite set of propo-\nsitional formulas wit the help of conditional statements. Thus, the satisfiability problem is at least NP-hard even for Statistical EL.\nTheorem 15. The satisfiability problem for Statistical EL is NP-hard.\nProof. We provide a reduction from the well-known coNP-complete problem of deciding validity of a 3DNF formula. Let ϕ = ∨n\ni=1 κi be a 3DNF formula; that is, each κi, 1 ≤ i ≤ n is a conjunction of three literals κi = λ1i ∧ λ 2 i ∧ λ 3 i . We construct a statistical EL KB as follows. Let V be the set of all variables appearing in ϕ. For every x ∈ V , we use two concept names Ax and A¬x. In addition, for every clause κi we introduce a concept name Bi, and create an additional concept name C.\nConsider the KB Kϕ = (Tϕ, Cϕ), where\nTϕ := { 3l\nj=1\nAλj i ⊑ Bi, Bi ⊑ C | 1 ≤ i ≤ n}\nCϕ := {(Ax | ⊤)[0.5, 1], (A¬x | ⊤)[0.5, 1], (A¬x | Ax)[0, 0]} ∪ {(C | ⊤)[0, 0.5]}.\nThen it holds that ϕ is valid iff Kϕ is inconsistent. ⊓⊔\nOn the other hand, consistency can be decided in non-deterministic exponential\ntime, through a reduction to integer programming. Before describing the reduction in detail, we introduce a few simplifications.\nRecall from Proposition 4 that a conditionals of the form (D | C)[1, 1] is equivalent to the classical GCI C ⊑ D. Thus, in the following we will often express statistical EL KBs as pairs K = (T , C), where T is a classical TBox (i.e., a finite set of GCIs), and C is a set of conditionals. A statistical EL KB K = (T , C) is said to be in normal form if all the GCIs in T are of the form\nA1 ⊓ A2 ⊑ B, A ⊑ ∃r.B, ∃r.A ⊑ B\nand all its conditionals are of the form\n(A | B)[ℓ, u]\nwhere A,B ∈ NC ∪ {⊤}, and r ∈ NR. Informally, a KB is in normal form if at most one constructor is used in any GCI, and all conditionals are atomic (i.e., between\nconcept names). Every KB can be transformed to an equivalent one (w.r.t. the original\nsignature) in linear time using the normalization rules from [1], and introducing new concept names for complex concepts appearing in conditionals. More precisely, we replace any conditional of the form (C | D)[ℓ, u] with the statement (A | B)[ℓ, u], whereA,B are two fresh concept names, and extend the TBox with the axiomsA ≡ C, and B ≡ D.\nThe main idea behind our consistency algorithm is to partition the finite domain of a model into the different types that they define, and use integer programming to verify that all the logical and conditional constraints are satisfied. LetNC(K) denote the set of all concept names appearing in the KB K. We call any subset θ ⊆ NC(K) a type for K. Intuitively, such a type θ represents all the elements of the domain that are interpreted to belong to all concept names A ∈ θ and no concept name A /∈ θ. We denote as Θ(K) the set of all types of K. To simplify the presentation, in the following we treat ⊤ as a concept name that belongs to all types.\nGiven a statistical EL KB K = (T , C) in normal form, we consider an integer variablexθ for every type θ ∈ Θ(K). These variableswill express the number of domain elements that belong to the corresponding type. In addition, x⊤ will be used to represent the total size of the domain.We build a system of linear inequalities over these variables as follows. First, we require that all variables have a value at least 0, and that the sizes of all types add exactly the size of the domain.\n∑\nθ∈Θ(K)\nxθ = x⊤ (2)\n0 ≤ xθ for all θ ∈ Θ(K) (3)\nThen, we ensure that all the conditional statements from the KB are satisfied by adding, for each statement (A | B)[ℓ, u] ∈ C the constraint\nℓ · ∑\nB∈θ\nxθ ≤ ∑\nA,B∈θ\nxθ ≤ u · ∑\nB∈θ\nxθ, (4)\nFinally, we must ensure that the types satisfy all the logical constraints introduced by the TBox. The GCI A1 ⊓A2 ⊑ B states that every element that belongs to bothA1 and\nA2 must also belong to B. This means that types containing A1, A2 but excluding B should not be populated. We thus introduce the inequality\nxθ = 0 if A1 ⊓ A2 ⊑ B ∈ T , A1, A2 ∈ θ, and B /∈ θ (5)\nDealing with existential restrictions requires checking different alternatives, which we solve by creating different linear programs. The GCI A ⊑ ∃r.B implies that, whenever there exists an element in A, there must also exist at least one element in B. Thus, to satisfy this axiom, either A should be empty (i.e., ∑\nA∈θ xθ = 0), or ∑\nB∈θ xθ ≥ 1. Hence, for every existential restriction of the form A ⊑ ∃r.B, we define the set\nEA,B := { ∑\nA∈θ\nxθ = 0, ∑\nB∈θ\nxθ ≥ 1}\nTo deal with GCIs of the form ∃r.A ⊑ B, we follow a similar approach, together with the ideas of the completion algorithm for classical EL. For every pair of existential restrictions A ⊑ ∃r.B, ∃r.C ⊑ D, we define the set\nFA,B,C,D := { ∑\nA∈θ,D/∈θ\nxθ = 0, ∑\nB∈θ,C/∈θ\nxθ ≥ 1}\nIntuitively, ∑\nA∈θ,D/∈θ xθ ≥ 1 whenever there exists an element that belongs to A but not toD. If this is the case, and the GCIs A ⊑ ∃r.B, ∃r.C ⊑ D belong to the TBox T , then there must exist some element that belongs to B but not to C.\nWe call the hitting sets of\n{EA,B | A ⊑ ∃r.B ∈ T } ∪ {FA,B,C,D | A ⊑ ∃r.B, ∃r.C ⊑ D ∈ T }\nchoices for T . A program forK is an integer program containing all the inequalities (2)– (5) and a choice for T . Then we get the following result.\nLemma 16. K is consistent iff there exists a program for K that is satisfiable.\nProof. The “only if” direction is straight-forward since the inequalities are sound w.r.t. the semantics of statistical KBs. We focus on the “if” direction only.\nGiven a solution of the integer program, we construct an interpretation I = (∆, ·I) as follows. We create a domain∆ with x⊤ elements, and partition it such that for every type θ ∈ Θ(K), there is a class [[θ]] containing exactly xθ elements. For every nonempty class, select a representative element δθ ∈ [[θ]].\nThe interpretation function ·I maps every concept name A to the set\nAI := ⋃\nA∈θ\n[[θ]].\nGiven a non-empty class [[θ]] such that A ∈ θ and A ⊑ ∃r.B ∈ T , let τ be a type such that B ∈ τ , xτ > 0, and for every ∃r.C ⊑ D ∈ T , if D /∈ θ, then C /∈ τ . Notice that such a τ must exist because the solution must satisfy at least one restriction in each FA,B,C,D. We define rθA,B := θ × {δτ} and set\nrI := ⋃\nA∈θ,A⊑∃r.B∈T\nrθA,B.\nIt remains to be shown that I is a model of K. Notice that for two concept names A,B, it holds that (A⊓B)I = ⋃\nA,B∈θ[[θ]] and\nhence [A⊓B]I | = ∑\nA,B∈θ xθ . Given a conditional statement (A | B)[ℓ, u] ∈ C, since the solution must satisfy the inequality (4), it holds that\nℓ · [B]I ≤ [A ⊓B]I ≤ u · [B]I .\nFor a GCI A1 ⊓ A2 ⊑ B ∈ T , by the inequality (5) it follows that for every type θ containing both A1, A2, but not B, [[θ]] = ∅. Hence AI1 ∩ A I 2 ⊆ B I . For every A ⊑ ∃r.B ∈ T , and every γ ∈ ∆, if γ ∈ AI then by construction there is an element γ′ such that (γ, γ′) ∈ rI .\nFinally, if (γ, γ′) ∈ rI , then by construction there exists a type θ and an axiom A ⊑ ∃r.B ∈ T such that γ ∈ [[θ]] and γ′ = δτ . Then, for every GCI ∃r.C ⊑ D ∈ T , γ′ ∈ CI implies C ∈ τ and henceD ∈ θ which means that γ ∈ DI . ⊓⊔\nNotice that the construction produces exponentially many integer programs, each of\nwhich uses exponentially many variables, measured on the size of the KB. Since satisfiability of integer linear programs is decidable in non-deterministic polynomial time\non the size of the program, we obtain a non-deterministic exponential time upper bound for deciding consistency of statistical EL KBs.\nTheorem 17. Consistency of statistical EL KBs is in NEXPTIME."
    }, {
      "heading" : "5 Reasoning with Open Minded KBs",
      "text" : "In order to regain tractability, we now further restrict statistical EL KBs by disallowing upper bounds in the conditional statements. We call such knowledge bases open minded.\nDefinition 18 (Open Minded KBs). A statistical EL KB K = (T , C) is open minded iff all the conditional statements (C | D)[ℓ, u] ∈ C are such that u = 1.\nFor the scope of this section, we consider only open minded KBs. The first obvious\nconsequence of restricting to this class of KBs is that negations cannot be simulated. In fact, every open minded KB is consistent and, as in classical EL, can be satisfied in a simple universal model.\nTheorem 19. Every open minded KB is consistent.\nProof. Consider the interpretationI = ({δ}, ·I)where the interpretation functionmaps every concept name A to AI := {δ} and every role name r to rI := {(δ, δ)}. It is easy to see that this interpretation is such that CI = {δ} holds for every EL concept C. Hence, I satisfies all ELGCIs and in addition [C ⊓D]I = [C]I = 1 which implies that all conditionals are also satisfied. ⊓⊔\nRecall that, intuitively, conditionals specify that a proportion of the population satisfies some given properties. One interesting special case of p-entailment is the question how likely it is to observe an individual that belongs to a given concept.\nDefinition 20. Let K be an open minded KB, C a concept, andm ∈ [0, 1]. C ism-necessary in K if K p-entails (C | ⊤)[m, 1]. The problem of m-necessity consists in deciding whether C ism-necessary in K.\nWe show that this problem can be solved in polynomial time. As in the previous sec-\ntion, we assume that the KB is in normal form and additionally, that all conditional statements (A | B)[ℓ, 1] ∈ C are such that ℓ < 1. This latter assumption is made w.l.o.g. since the conditional statement (A | B)[1, 1] can be equivalently replaced by the GCI B ⊑ A (see Proposition 4). Moreover, checking m-necessity of a complex concept C w.r.t. the KB (T , C) is equivalent to decidingm-necessity of a new concept name A w.r.t. the KB (T ∪ {A ≡ C}, C). Thus, in the following we consider w.l.o.g. only the problem of deciding m-necessity of a concept name w.r.t. to a KB in normal form.\nOur algorithm extends the completion algorithm for classification of EL TBoxes to in addition keep track of the lower bounds of necessity for all relevant concept names. The algorithm keeps as data structure a set S of tuples of the form (A,B) and (A, r,B) for A,B ∈ NC ∪ {⊤}. These intuitively express that the TBox T entails the subsumptions A ⊑ B and A ⊑ ∃r.B, respectively. Additionally, we keep a function L that maps every element A ∈ NC ∪ {⊤} to a number L(A) ∈ [0, 1]. Intuitively, L(A) = n expresses that K p-entails (A | ⊤)[n, 1].\nThe algorithm initializes the structures S and L as\nS := {(A,A), (A,⊤) | A ∈ NC(K) ∪ {⊤}}\nL(A) :=\n{\n0 if A ∈ NC(K)\n1 if A = ⊤.\nThese structures are then updated using the rules from Table 1. In each case, a rule is only applied if its execution extends the available knowledge; that is, if either S is extended to include one more tuple, or a lower bound in L is increased. In the latter case, only the larger value is kept through the function L.\nThe first three rules in Table 1 are the standard completion rules for classical EL. The remaining rules update the lower bounds for the likelihood of all relevant concept names, taking into account their logical relationship, as explained next.\nRule L1 applies the obvious inference associated to conditional statements: from all the individuals that belong to B, (A | B)[ℓ, 1] states that at least 100ℓ% belong also to A. Thus, assuming that L(B) is the lowest proportion of elements in B possible, the\nproportion of elements inAmust be at least ℓ ·L(B). L3 expresses that if every element of B must also belong to A, then there must be at least as many elements in A as there are in B. Finally, L2 deals with the fact that two concepts that are proportionally large must necessarily overlap. For example, if 60% of all individuals belong to A and 50% belong to B, then at least 10% must belong to both A and B; otherwise, together they would cover more than the whole domain.\nThe algorithm executes all the rules until saturation; that is, until no rule is applicable. Once it is saturated, we can decide m-necessity from the function L as follows: A is m-necessary iff m ≤ L(A). Before showing the correctness of this algorithm, we show an important property.\nNotice that the likelihood information from L is never transferred through roles. The reason for this is that an existential restriction ∃r.B only guarantee the existence of one element belonging to the conceptB. Proportionally, the number of elements that belong to B tends to 0.\nExample 21. Consider the KB ({⊤ ⊑ ∃r.A}, ∅). For any n ∈ N, construct the interpretation In := ({0, . . . , n}, ·In), where AIn = {0} and rIn = {(k, 0) | 0 ≤ k ≤ n}. It is easy to see that In is a model of the KB and [A]\nIn/[⊤]In < 1/n. Thus, the best lower bound form-necessity of A is 0, as correctly given by the algorithm.\nTheorem 22 (correctness). Let L be the function obtained by the application of the rules until saturation and A0 ∈ NC . Then A0 ism-necessary iffm ≤ L(A).\nProof (sketch). It is easy to see that all the rules are sound, which proves the “if” direction. For the converse direction, we consider a finite domain∆ and an interpretation ·I of the concept names such that [A]I/|∆| = L(A) and the post-conditions of the rules L1–L3 are satisfied. Such interpretation can be obtained recursively by considering the last rule application that updatedL(A). Assume w.l.o.g. that the domain is large enough so that c/|∆| < m − L(A0), where c is the number of concept names appearing in K. It is easy to see that this interpretation satisfies all conditional statements and the GCIs A1 ⊓ A2 ⊑ B ∈ T . For every concept name A, create a new domain element δA and extend the interpretation I such that δA ∈ B iff (A,B) ∈ S. Given a role name r, we define rI := {(γ, δB) | A ⊑ ∃r.B, γ ∈ AI}. Then, this interpretation satisfies the KB K, and [A0]I/|∆| ≤ L(A0) + c/|∆| < m. ⊓⊔\nThus, the algorithm can correctly decide m-necessity of a given concept name. It remains only to be shown that the process terminates after polynomially many rule ap-\nplications. To guarantee this, we impose an ordering in the rule applications. First, we apply all the classical rules C1–C3, and only when no such rules are applicable, we update the function L through the rules L1–L3. In this case, the rule that will update to the largest possible value is applied first. It is known that only polynomially many classical rules (on the size of T ) can be applied [1]. Deciding which bound rule to apply next requires polynomial time on the number of concept names in K. Moreover, since the largest update is applied first, the value of L(A) is changed at most once for every concept name A. Hence, only linearly many rules are applied. Overall, this means that the algorithm terminates after polynomially many rule applications, which yields the following result.\nTheorem 23. Decidingm-necessity is in P."
    }, {
      "heading" : "6 Related Work",
      "text" : "Over the years, various probabilistic extensions of description logics have been investi-\ngated, see, for instance, [3,7,8,10,12,15,17]. The one that is closest to our approach is the type 1 extension of ALC proposed in the appendix of [11]. Briefly, [11] introduces probabilistic constraints of the form P (C | D) ≤ p, P (C | D) = p, P (C | D) ≥ p for ALC conceptsC,D. These correspond to the conditionals (C | D)[0, p], (C | D)[p, p], (C | D)[p, 1], respectively. Conversely, each conditional can be rewritten as such a probabilistic constraint. However, there is a subtle but fundamental difference in the semantics. While the definition in [11] allows for probability distributions over arbi-\ntrary domains, we do not consider uncertainty over the domain. This comes down to\nallowing only finite domains and only the uniform distribution over this domain; that is, our approach further restricts the class of models of a KB. One fundamental difference\nbetween the two approaches is that Proposition 4 does not hold in [11]: the reason is that the conditional (C | D)[1, 1] can be satisfied by an interpretation I that contains an element x ∈ (C ⊓ ¬D)I , where x has probability 0.\nThis difference is the main reason why the EXPTIME algorithm proposed by Lutz and Schröder cannot be transferred to our setting. It does not suffice to consider the\nsatisfiable types independently, but other implicit subsumption relations may depend\non the conditionals only.\nExample 24. Consider the statistical EL KB K = (T , C) with\nT := {⊤ ⊑ ∃r.A, ∃r.B ⊑ C}\nC := {(B | ⊤)[0.5, 1], (A | B)[0.5, 1], (A | ⊤)[0, 0.25]}\nFrom C it follows that every element of A must also belong to B, and hence every domain element must be an element of C. However,¬C defines a satisfiable type (w.r.t. T ) which will be interpreted as non-empty in the model generated by the approach in [11]."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have introduced Statistical ALC, a new probabilistic extension of the description logic ALC for statistical reasoning. We analyzed the basic properties of this logic and introduced some reasoning problems that we are interested in. As a first step towards effective reasoning in Statistical ALC, we focused on EL, a well-known sublogic of ALC that, in its classical form, allows for polynomial-time reasoning. We showed that upper bounds in conditional constraints make the satisfiability problem in statistical EL NP-hard and gave an NEXPTIME algorithm to decide satisfiability. We showed that tractability can be regained by disallowing strict upper bounds in the conditional\nstatements.\nWe are going to provide more algorithms and a more complete picture of the complexity of reasoning for StatisticalALC and its fragments in future work. A combination of integer programming and the inclusion-exclusion principle may be fruitful to design first algorithms for reasoning in full Statistical ALC."
    } ],
    "references" : [ {
      "title" : "Pushing the EL envelope",
      "author" : [ "F. Baader", "S. Brandt", "C. Lutz" ],
      "venue" : "Kaelbling, L.P., Saffiotti, A. (eds.) Proc. of the 19th Int. Joint Conf. on Artificial Intelligence (IJCAI’05). pp. 364–369. Morgan-Kaufmann",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Extending and completing probabilistic knowledge and beliefs without bias",
      "author" : [ "C. Beierle", "G. Kern-Isberner", "M. Finthammer", "N. Potyka" ],
      "venue" : "KI-Künstliche Intelligenz 29(3), 255–262",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The bayesian ontology language BEL",
      "author" : [ "İ.İ. Ceylan", "R. Peñaloza" ],
      "venue" : "J. Autom. Reasoning 58(1), 67–95",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Random worlds and maximum entropy",
      "author" : [ "A.J. Grove", "J.Y. Halpern", "D. Koller" ],
      "venue" : "Logic in Computer Science, 1992. LICS’92., Proceedings of the Seventh Annual IEEE Symposium on. pp. 22–33. IEEE",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "An analysis of first-order logics of probability",
      "author" : [ "J.Y. Halpern" ],
      "venue" : "Artificial intelligence 46(3), 311–350",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Probabilistic satisfiability",
      "author" : [ "P. Hansen", "B. Jaumard" ],
      "venue" : "Kohlas, J., Moral, S. (eds.) Handbook of Defeasible Reasoning and Uncertainty Management Systems, vol. 5, pp. 321–367. Springer Netherlands",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Pronto: A practical probabilistic description logic reasoner",
      "author" : [ "P. Klinov", "B. Parsia" ],
      "venue" : "Uncertainty Reasoning for the Semantic Web II, pp. 59–79. Springer",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "P-classic: a tractable probablistic description logic",
      "author" : [ "D. Koller", "A. Levy", "A. Pfeffer" ],
      "venue" : "AAAI/IAAI 1997, 390–397",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Probabilistic logic programming with conditional constraints",
      "author" : [ "T. Lukasiewicz" ],
      "venue" : "ACM Trans. Comput. Logic 2(3), 289–339",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Managing uncertainty and vagueness in description logics for the semantic web",
      "author" : [ "T. Lukasiewicz", "U. Straccia" ],
      "venue" : "JWS 6(4), 291–308",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Probabilistic description logics for subjective uncertainty",
      "author" : [ "C. Lutz", "L. Schröder" ],
      "venue" : "Proc. KR 2010. AAAI Press",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Log-linear description logics",
      "author" : [ "M. Niepert", "J. Noessner", "H. Stuckenschmidt" ],
      "venue" : "IJCAI. pp. 2153–2158",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic logic",
      "author" : [ "N.J. Nilsson" ],
      "venue" : "Artificial Intelligence 28, 71–88",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "The Uncertain Reasoner’s Companion – AMathematical Perspective",
      "author" : [ "J.B. Paris" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Probabilistic reasoning in the description logic ALCP with the principle of maximum entropy",
      "author" : [ "R. Peñaloza", "N. Potyka" ],
      "venue" : "International Conference on Scalable Uncertainty Management. pp. 246–259. Springer",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Probabilistic reasoning with inconsistent beliefs using inconsistency measures",
      "author" : [ "N. Potyka", "M. Thimm" ],
      "venue" : "IJCAI. pp. 3156–3163",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Probabilistic description logics under the distribution semantics",
      "author" : [ "F. Riguzzi", "E. Bellodi", "E. Lamma", "R. Zese" ],
      "venue" : "Semantic Web 6(5), 477–501",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In [5], probabilistic logics have been classified into three types that differ in the way how they handle probabilities.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "In order to overcome the problem, we can, for instance, restrict to Herbrand interpretations over a fixed domain [2,9,13] or consider decidable fragments like description logics [3, 8, 10].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "In order to overcome the problem, we can, for instance, restrict to Herbrand interpretations over a fixed domain [2,9,13] or consider decidable fragments like description logics [3, 8, 10].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "In order to overcome the problem, we can, for instance, restrict to Herbrand interpretations over a fixed domain [2,9,13] or consider decidable fragments like description logics [3, 8, 10].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "In order to overcome the problem, we can, for instance, restrict to Herbrand interpretations over a fixed domain [2,9,13] or consider decidable fragments like description logics [3, 8, 10].",
      "startOffset" : 178,
      "endOffset" : 188
    }, {
      "referenceID" : 7,
      "context" : "In order to overcome the problem, we can, for instance, restrict to Herbrand interpretations over a fixed domain [2,9,13] or consider decidable fragments like description logics [3, 8, 10].",
      "startOffset" : 178,
      "endOffset" : 188
    }, {
      "referenceID" : 9,
      "context" : "In order to overcome the problem, we can, for instance, restrict to Herbrand interpretations over a fixed domain [2,9,13] or consider decidable fragments like description logics [3, 8, 10].",
      "startOffset" : 178,
      "endOffset" : 188
    }, {
      "referenceID" : 10,
      "context" : "Probabilistic type 2 extensions of description logics have been previously studied in [11].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "This type 1 extension enriches classical interpretations with probability distributions over the domain as suggested in [5].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "Hence, interpretations in our framework can be regarded as a subset of the interpretations in [11], namely those with finite domains and a uniform probability distribution over the domain.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "Our approach bears some resemblance to the random world approach from [4].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "However, the authors in [4] consider possible worlds with a fixed domain size N and are interested in the limit of proportions as N goes to infinity.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "For all statistical ALC interpretations I, we have I |= C ⊑ D iff I |= (D | C)[1, 1].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "For all statistical ALC interpretations I, we have I |= C ⊑ D iff I |= (D | C)[1, 1].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Hence, I |= (D | C)[1, 1].",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "Hence, I |= (D | C)[1, 1].",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "Conversely, assume I |= (D | C)[1, 1].",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Conversely, assume I |= (D | C)[1, 1].",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "In the context of type 2 probabilistic conditionals, this entailment relation has also been called just logical consequence [9].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "This problem is often referred to as the probabilistic entailment problem in other probabilistic logics, see [6, 9, 13] for instance.",
      "startOffset" : 109,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "This problem is often referred to as the probabilistic entailment problem in other probabilistic logics, see [6, 9, 13] for instance.",
      "startOffset" : 109,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "This problem is often referred to as the probabilistic entailment problem in other probabilistic logics, see [6, 9, 13] for instance.",
      "startOffset" : 109,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "this entailment relation has also been called tight logical consequence [9].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "6], (B | C)[1, 1], (C | B)[1, 1]}.",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "6], (B | C)[1, 1], (C | B)[1, 1]}.",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "6], (B | C)[1, 1], (C | B)[1, 1]}.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "6], (B | C)[1, 1], (C | B)[1, 1]}.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "for some m ∈ [0, 1].",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "As explained in Proposition 4, deterministic conditionals correspond to concept inclusions and so (B | C)[1, 1] and (C | B)[1, 1] imply that B ′ = C ′ for all models I ′ of K.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "As explained in Proposition 4, deterministic conditionals correspond to concept inclusions and so (B | C)[1, 1] and (C | B)[1, 1] imply that B ′ = C ′ for all models I ′ of K.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "As explained in Proposition 4, deterministic conditionals correspond to concept inclusions and so (B | C)[1, 1] and (C | B)[1, 1] imply that B ′ = C ′ for all models I ′ of K.",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "As explained in Proposition 4, deterministic conditionals correspond to concept inclusions and so (B | C)[1, 1] and (C | B)[1, 1] imply that B ′ = C ′ for all models I ′ of K.",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "6], (B | C)[1, 1], (C | B)[1, 1]}.",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "6], (B | C)[1, 1], (C | B)[1, 1]}.",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "6], (B | C)[1, 1], (C | B)[1, 1]}.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "6], (B | C)[1, 1], (C | B)[1, 1]}.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "Blaschke continuity is satisfied by some probabilistic logics under maximum entropy and probabilistic entailment [14, 16].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "Blaschke continuity is satisfied by some probabilistic logics under maximum entropy and probabilistic entailment [14, 16].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "In [14, 16], probabilistic interpretations are probability distributions over a finite number of classical interpretations and the distance between two interpretations is the distance between the corresponding probability vectors.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "In [14, 16], probabilistic interpretations are probability distributions over a finite number of classical interpretations and the distance between two interpretations is the distance between the corresponding probability vectors.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "To begin with, we will focus on the sublogic EL [1] of ALC that does not allow for negation and universal quantification.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "Recall from Proposition 4 that a conditionals of the form (D | C)[1, 1] is equivalent to the classical GCI C ⊑ D.",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "Recall from Proposition 4 that a conditionals of the form (D | C)[1, 1] is equivalent to the classical GCI C ⊑ D.",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "the original signature) in linear time using the normalization rules from [1], and introducing new concept names for complex concepts appearing in conditionals.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "Let K be an open minded KB, C a concept, andm ∈ [0, 1].",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "since the conditional statement (A | B)[1, 1] can be equivalently replaced by the GCI B ⊑ A (see Proposition 4).",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "since the conditional statement (A | B)[1, 1] can be equivalently replaced by the GCI B ⊑ A (see Proposition 4).",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Additionally, we keep a function L that maps every element A ∈ NC ∪ {⊤} to a number L(A) ∈ [0, 1].",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "It is known that only polynomially many classical rules (on the size of T ) can be applied [1].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "Over the years, various probabilistic extensions of description logics have been investigated, see, for instance, [3,7,8,10,12,15,17].",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "Over the years, various probabilistic extensions of description logics have been investigated, see, for instance, [3,7,8,10,12,15,17].",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "Over the years, various probabilistic extensions of description logics have been investigated, see, for instance, [3,7,8,10,12,15,17].",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "Over the years, various probabilistic extensions of description logics have been investigated, see, for instance, [3,7,8,10,12,15,17].",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "Over the years, various probabilistic extensions of description logics have been investigated, see, for instance, [3,7,8,10,12,15,17].",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "Over the years, various probabilistic extensions of description logics have been investigated, see, for instance, [3,7,8,10,12,15,17].",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "Over the years, various probabilistic extensions of description logics have been investigated, see, for instance, [3,7,8,10,12,15,17].",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "The one that is closest to our approach is the type 1 extension of ALC proposed in the appendix of [11].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Briefly, [11] introduces probabilistic constraints of the form P (C | D) ≤ p, P (C | D) = p, P (C | D) ≥ p for ALC conceptsC,D.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : "While the definition in [11] allows for probability distributions over arbitrary domains, we do not consider uncertainty over the domain.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "One fundamental difference between the two approaches is that Proposition 4 does not hold in [11]: the reason is that the conditional (C | D)[1, 1] can be satisfied by an interpretation I that contains an element x ∈ (C ⊓ ¬D) , where x has probability 0.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "One fundamental difference between the two approaches is that Proposition 4 does not hold in [11]: the reason is that the conditional (C | D)[1, 1] can be satisfied by an interpretation I that contains an element x ∈ (C ⊓ ¬D) , where x has probability 0.",
      "startOffset" : 141,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "One fundamental difference between the two approaches is that Proposition 4 does not hold in [11]: the reason is that the conditional (C | D)[1, 1] can be satisfied by an interpretation I that contains an element x ∈ (C ⊓ ¬D) , where x has probability 0.",
      "startOffset" : 141,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "T ) which will be interpreted as non-empty in the model generated by the approach in [11].",
      "startOffset" : 85,
      "endOffset" : 89
    } ],
    "year" : 2017,
    "abstractText" : "We present a probabilistic extension of the description logic ALC for reasoning about statistical knowledge. We consider conditional statements over proportions of the domain and are interested in the probabilistic-logical consequences of these proportions. After introducing some general reasoning problems and analyzing their properties, we present first algorithms and complexity results for reasoning in some fragments of StatisticalALC.",
    "creator" : "LaTeX with hyperref package"
  }
}