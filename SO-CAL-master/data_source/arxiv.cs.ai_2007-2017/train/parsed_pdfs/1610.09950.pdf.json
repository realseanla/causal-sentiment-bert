{
  "name" : "1610.09950.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "From Node Embedding To Community Embedding",
    "authors" : [ "Vincent W. Zheng", "Sandro Cavallari", "Hongyun Cai", "Kevin Chen-Chuan Chang", "Erik Cambria" ],
    "emails" : [ "hongyun.c}@adsc.com.sg,", "sandro001@e.ntu.edu.sg,", "kcchang@illinois.edu,", "cambria@ntu.edu.sg" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Graph data is becoming increasingly popular, thanks to the proliferation of various social media (e.g., blogs, Flickr, Twitter) and many other kinds of information networks (e.g., DBLP, knowledge graphs). To effectively process and analyze the graph data, we often need to consider how to appropriately represent the graph. Graph embedding is a mainstream graph representation framework (Roweis and Saul 2000; Chang et al. 2015; Niepert, Ahmed, and Kutzkov 2016; Xie et al. 2016), which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on.\nConventionally, graph embedding focuses on nodes – it tries to output a vector representation for each node in the graph, such that two nodes being “close” on the graph have similar vector representations (i.e., close in the lowdimensional space). There are different ways to measure the closeness between two nodes in the graph. Most of the existing graph embedding methods measure the closeness by:\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1) first-order proximity (Tenenbaum, de Silva, and Langford 2000; Belkin and Niyogi 2001), which considers two nodes as close if they are direct neighbors to each other in the graph; 2) second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016), which considers two nodes as close if they share similar neighbors in the graph; 3) combination of first-order proximity and second-order proximity (Tang et al. 2015; Wang, Cui, and Zhu 2016), which considers two nodes as close if they are directly linked and also share similar neighbors.\nDespite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing in the literature. Embedding community is useful; it helps to\n• Support community-level applications: for example, we can visualize the communities in a low-dimensional space to help generate insights about the graph structure. We can also enable community recommendation by predicting the most likely community to a node according to their closeness in the embedded space. We will quantitatively evaluate this community prediction task in the experiment.\n• Preserve community structure in graph embedding: the state of the art such as DeepWalk (Perozzi, Al-Rfou, and Skiena 2014), LINE (Tang et al. 2015) and node2vec (Grover and Leskovec 2016) is unable to preserve communities in the embedded space. In Fig. 1, we visualize the graph embedding of these methods for the Zachary’s Karate Club data set1. The graph has 34 nodes. Each node has a color2, and there are in total four colors, indicating four different communities. Fig. 1(b)–1(d) show that, they cannot clearly separate the nodes of different colors apart.\nCommunity embedding. In this paper, we introduce a new concept of community embedding. A community embedding is a latent representation for a community in the graph. To represent a community, we are motivated by the Gaussian Mixture Model (Bishop 2006) to see each community as a Gaussian component. Thus, we represent each community with: 1) where its center is; 2) how its member nodes are spreaded. As to be defined later in Def. 1, we formulate a community embedding as a tuple of a mean vector indicat-\n1https://networkdata.ics.uci.edu/data.php?id=105 2Same colors as used in (Perozzi, Al-Rfou, and Skiena 2014).\nar X\niv :1\n61 0.\n09 95\n0v 1\n[ cs\n.S I]\n3 1\nO ct\n2 01\n6\ning the center of a community and a covariance matrix indicating the spread of its members in a low-dimensional space.\nCommunity embedding is significant. First of all, it provides a “direct” representation for each community in the low-dimensional space, in contrast with representing a community as a collection of nodes. Second, it captures a higherorder proximity in graph embedding, such that the multihop-away nodes within the same community can also be close in the embedded space. There is few graph embedding work that considers higher-order proximity, and their definitions of higher-order proximity are not based on communities (Cao, Lu, and Xu 2016; Ou et al. 2016). More discussions are in the related work section.\nOur solution. Our major insight to learn the community embedding is hinged upon the mutual reinforcement between node embedding and community embedding. On one hand, a good community embedding helps to get a good node embedding, because of the higher-order proximity. On the other hand, a good node embedding also helps to get a good community embedding, since the nodes can then be better clustered together. As a result, we shall jointly optimize the community embedding and the node embedding. Note that, having a better node embedding also enables us to do better predictions (e.g., node classification) on the graph. We will also evaluate the node classification task in the experiment.\nBased on our above insight, we propose ComEmbed, the first community embedding method for graph analytics. Specifically, in ComEmbed we iteratively optimize the community embedding and the node embedding. Given the node embedding, we fit the community embedding by the Gaussian Mixture Model. Given the community embedding, we fit the node embedding by novelly preserving all of the firstorder, second-order and higher-order proximities. We emphasize that, the idea of joint modeling node embedding and community embedding is new. There is some work on using community to improve the node embedding (Yang et al. 2016), or using node embedding to achieve better community detection (Tian et al. 2014). But there is no study on the reinforcement of node embedding and community embedding. Besides, the joint modeling is also effective. In Figure 1(e), we first show some promising result: thanks to the com-\nmunity embedding, ComEmbed can well separate the nodes with different colors in the embedded space. More evaluations are in the experiment section.\nWe summarize our contributions as follows.\n• To the best of our knowledge, we are the first to introduce the concept of community embedding to graph analytics.\n• We propose ComEmbed, which jointly learns the community embedding and the node embedding together.\n• We evaluate ComEmbed on real-world data sets. We improve the state-of-the-art baselines by at least 4.0%–5.5% (conductance) and 5.3%–11.2% (NMI) in community prediction, 14.1%–91.8% (macro-F1) and 7.6%–10.2% (micro-F1) in node classification."
    }, {
      "heading" : "Related Work",
      "text" : "There is no existing graph embedding work that considers community embedding. Most of the graph embedding methods focus on generating node embedding. For example, earlier methods, such as MDS (Cox and Cox 2000), LLE (Roweis and Saul 2000), IsoMap (Tenenbaum, de Silva, and Langford 2000) and Laplacian eigenmap (Belkin and Niyogi 2001), typically aim to solve the leading eigenvectors of graph affinity matrices as node embedding. Recent methods typically rely on neural networks to learn the representation for each node, with either shallow architectures (Yang, Cohen, and Salakhutdinov 2016; Xie et al. 2016; Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al. 2015). Other than node embedding, there is some attempt to learn edge embedding (Luo et al. 2015), which aims to learn the embedding for both entities (i.e., nodes) and relations (i.e., edges) in a knowledge graph.\nThere is little existing graph embedding work that considers high-order proximity and their definitions of highorder proximity are also different from ours. Most existing methods focus on first-order proximity and/or second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016; Tang et al. 2015; Wang, Cui, and Zhu 2016), as discussed in the introduction. Some recent attempts to consider higher-order proximity are based on the facts that:\n1) DeepWalk is a realization of SkipGram (Mikolov et al. 2013) on the graph data, where DeepWalk treats each node as a “word” and each path as a “sentence”; 2) SkipGram implicitly factorizes a matrix based on the word-word coocurrence (Levy and Goldberg 2014). Thus, DeepWalk’s secondorder proximity stems from factorizing the matrix about node-node coocurrence. Both (Ou et al. 2016) and (Cao, Lu, and Xu 2016) design and factorize a higher-order node-node proximity matrix by PageRank or Katz index. Thus their higher-order proximity is based on the graph reachability via random walk, where the notion of community is missing.\nThe mutual reinforcement of community embedding and node embedding has never been exploited. It is common to use node embedding results for community detection (Tian et al. 2014; Kozdoba and Mannor 2015), but they do not have the notion of community in their node embedding. There is some work that allows community feedback to guide the node embedding (Yang et al. 2016), but again it lacks the concept of community embedding and its community feedback requires extra supervision on must-links."
    }, {
      "heading" : "Problem Formulation",
      "text" : "As input, we are given a graph G = (V,E), where V is the node set and E is the edge set. Traditional graph embedding aims to learn a node embedding for each vi ∈ V as φi ∈ Rd. In this paper, we introduce the concept of community embedding. Suppose there are K communities on the graphG. For each node vi, we denote its community assignment as zi ∈ {1, ...,K}. Motivated by Gaussian Mixture Model (GMM), we represent each community as a Gaussian component, which is characterized by a mean vector indicating the community center and a covariance matrix indicating its member nodes’ spread. Formally, we define:\nDefinition 1 (Community Embedding) A community embedding for a community k in a d-dimensional space is a tuple (ψk,Σk), where ψk ∈ Rd is a Gaussian mean vector and Σk ∈ Rd×d is a Gaussian covariance matrix.\nAs output, we aim to learn both the community embedding (ψk,Σk) for each community k ∈ {1, ...,K} and the node embedding φi for each node vi ∈ V on the graph G.\nConsidering the mutual reinforcement between node embedding and community embedding, we propose to learn two embeddings together. First of all, to learn the node embedding, we consider both first-order and second-order proximity. In addition, we leverage the community embedding to achieve higher-order proximity, by requiring all the nodes within the same community to share similar embeddings. More precisely, we require all the nodes to have their node embeddings “close” to the corresponding community embedding, where we measure the closeness by a Gaussian distribution. Finally, we jointly optimize the node embedding and the community embedding, by maximizing the likelihood of using both embeddings to preserve all the first-order, second-order and higher-order proximities on the graph. Next, we give the model details.\nFirst-order proximity. To preserve the first-order proximity, we require two nodes that are direct neighbors on the\ngraph to have similar node embeddings. Specifically, for each edge (vi, vj) ∈ E, we follow LINE (Tang et al. 2015) to model the likelihood of first-order proximity as\np1(vi, vj) = σ(φ T j φi), (1)\nwhere σ(x) = 1/1 + exp(−x) is a sigmoid function. Then we define the objective function for the first-order proximity node embedding as\nO1 = − ∑\n(vi,vj)∈E log p1(vi, vj). (2)\nBy minimizing O1, we make φi close to each of its direct neighbors φj , ∀(vi, vj) ∈ E. Second-order proximity. To preserve the second-order proximity, we require two nodes that share similar neighbors on the graph to have similar node embeddings. We follow DeepWalk (Perozzi, Al-Rfou, and Skiena 2014) to consider a general sense of “neighbors” as the nodes that are reachable by the target node within ζ steps in a random walk on the graph. Formally, we consider the neighbors of vi as the context for vi, and we denote C(vi) as the context nodes for vi. Then, to preserve the second-order proximity, we require node vi to have similar node embedding to each of its context uj ∈ C(vi). We follow LINE to introduce an extra context embedding φ′j ∈ Rd for each node uj . Thus we define the likelihood of second-order proximity as\np(C(vi)|vi;φ,φ′) = ∏ uj∈C(vi) p(uj |vi;φ,φ ′), (3)\nwhere p(uj |vi;φ,φ′) in general is a softmax function\np(uj |vi;φ,φ′) = exp(φ′Tj φi)∑ uk exp(φ′Tk φi) .\nAs the summarization in the softmax function is time consuming, we follow (Mikolov et al. 2013) to use negative sampling to replace the summation term in the softmax function. By taking the logarithm over p(uj |vi;φ,φ′), we then define the new log-likelihood with negative sampling as\nlog p2(uj |vi;φ,φ′) = log σ(φ′ T j φi) + ∑m l=1 Euk∼Pn(u)[log σ(−φ ′T kφi)], (4)\nwhere uk ∼ Pn(u) means sampling a node uk (other than vi and any of its context) from V as a negative context of vi according to a probability Pn(u). We follow (Tang et al. 2015) to define Pn(u) ∝ r3/4u , where ru is node u’s degree. Euk∼Pn(u)[·] is the expectation over Pn(u). In total, we sample m negative context nodes to evaluate p(uj |vi;φ,φ′). Finally, we define the objective function for the second-order proximity node embedding as\nO2 = −α ∑ vi∈V ∑ uj∈C(vi) log p2(uj |vi;φ,φ ′), (5)\nwhere α > 0 is a trade-off parameter. By minimizing O2, we make φi close to each of its context φj , ∀φj ∈ C(vi). Community embedding. To preserve the higher-order proximity, we require all the nodes within the same community to be close to the corresponding community center in the embedded space. In other words, the community\nstructure is preserved after embedding the nodes into a lowdimensional space. To model the node distribution for each community in the embedded space, we choose GMM to model the likelihood of higher-order proximity as∏N\ni=1 ∑K k=1 p(vi|zi = k;φ,ψ,Σk)p(zi = k), (6)\nwhere each node vi belongs to a community k with a probability p(zi = k), and vi’s “closeness” to community k is measured by a probability p(vi|zi = k;φ,ψ,Σk). We define p(vi|zi = k;φ,ψ) as a multivariate normal distribution with the mean ψk and the covariance Σk:\np(vi|zi = k;φ,ψ) = N (φi|ψk,Σk). (7)\nFor notation simplicity, we further denote πik = p(zi = k), where πik ∈ [0, 1] and ∑K k=1 πik = 1. Finally, we define the objective function for using community embedding to enforce the higher-order proximity as\nO3 = − βK ∑N i=1 log ∑K k=1 πikN (φi|ψk,Σk), (8)\nwhere β > 0 is a trade-off parameter. By minimizing O3, we: 1) find the community assignment πik’s for each node vi; 2) optimize the community embedding (ψk,Σk)’s to best explain their corresponding community member nodes; 3) achieve the higher-order proximity by making the nodes within the same community to have similar embeddings.\nJoint modeling. Given all the first-order, second-order and higher-order proximities, we now jointly optimize the node embedding and the community embedding. Denote Φ = [φ1, ...,φN ] ∈ Rd×N , Φ′ = [φ ′ 1, ...,φ ′ N ] ∈ Rd×N , Ψ = [ψ1, ...,ψK ] ∈ Rd×K and Σ = {Σ1, ...,ΣK}. Then we aim to minimize the overall objective function for ComEmbed:\nL(Φ,Φ′,Ψ,Σ) = O1(Φ) +O2(Φ,Φ′) +O3(Φ,Ψ,Σ). (9)\nWe make some interesting connections of the ComEmbed objective function with the existing graph embedding methods. Firstly,O2 alone is the objective function for DeepWalk. Secondly, LINE trains O1 and O2 separately, but it also suggests jointly training O1 and O2 to combine two proximity. Our O1 + O2 can be seen as an extension of LINE. Thirdly, as shown in next section, we solve Eq. 9 by iteratively optimizing node embedding and community embedding. In contrast, first using LINE to learn node embedding and then using GMM to learn community embedding is a pipeline approach (to be evaluated in the community prediction task of our experiment). Finally, as GMM is known as a probabilistic version of K-means clustering (Bishop 2006), Eq. 9 can also be seen a probabilistic and joint optimization version of “first doing LINE then doing K-means”."
    }, {
      "heading" : "Inference",
      "text" : "To solve Eq. 9, we do coordinate descent between (Φ,Φ′) and (Ψ,Σ). Given (Φ,Φ′), optimizing (Ψ,Σ) equals to detecting communities in the embedded space. Given (Ψ,Σ), optimizing (Φ,Φ′) equals to learning node embedding with all the first-order, second-order and higher-order proximities. By iteratively optimizing (Φ,Φ′) and (Ψ,Σ), we keep\nminimizing the objective function. As the objective function is bounded, we eventually reach the convergence.\nFixing (Φ,Φ′), optimize (Ψ,Σ). In this case, we simplify L(Φ,Φ′,Ψ,Σ) as the negative likelihood of a GMM. According to (Bishop 2006), we can easily optimize (Ψ,Σ) by expectation maximization, and fortunately we have closedform update for each parameter as:\nψk = 1 Nk ∑N i=1 γikφi, (10)\nΣk = 1 Nk ∑N i=1 γik(φi −ψk)(φi −ψk)T , (11) πik = Nk N , (12)\nwhere γik = πikN (φi|ψk,Σk)∑K\nk′=1 πik′N (φi|ψk′ ,Σk′ ) and Nk =\n∑N i=1 γik.\nFixing (Ψ,Σ), optimize (Φ,Φ′). In this case, we simplify L(Φ,Φ′,Ψ,Σ) as optimizing LINE with community embedding regularization. Because of the summation within the logarithm term in L, it is inconvenient to compute the gradient of φi. As a result, we use variational inference and minimize an upper bound of L(Φ,Ψ,Σ). We first define\nO′3 = − β K ∑N i=1 ∑K k=1 πik logN (φi|ψk,Σk). (13)\nIt is easy to prove that\nO′3(Φ,Ψ,Σ) ≥ O3(Φ,Ψ,Σ), (14)\ndue to the log-concavity N∑ i=1 log K∑ k=1\nπikN (φi|ψk,Σk) ≥ N∑ i=1 K∑ k=1 log πikN (φi|ψk,Σk). As a result, we define\nL′(Φ,Φ′) = O1(Φ) +O2(Φ,Φ′) +O′3(Φ,Ψ,Σ),\nand we have L′(Φ,Φ′) ≥ L(Φ,Φ′). Finally, we optimize L′(Φ,Φ′) by stochastic gradient descent (SGD). For each vi ∈ V , we have its gradient as\n∂O1 ∂φi\n= − ∑\n(i,j)∈E σ(−φTj φi)φj , (15)\n∂O2 ∂φi\n= −α ∑\nuj∈C(vi)\n[ σ(−φ′j T φi)φ ′ j\n+ ∑m\nl=1 Euk∼Pn(u)[σ(φ\n′ k T φi)(−φ ′ k)]\n] , (16)\n∂O′3 ∂φi = β\nK ∑K k=1 πikΣ −1 k (φi −ψk). (17)\nThus, we have the gradient for φi as\n∂L′ ∂φi = ∂O1∂φi + ∂O2∂φi + ∂O′3 ∂φi . (18)\nWe also compute the gradient for context embedding as\n∂O2 ∂φ′j = −α ∑ vi∈V ∑ uj∈C(vi) [ δ(uj = vj)σ(−φ′j T φi)φi\n+ ∑m l=1 Euk∼Pn(u)[δ(uk = vj)σ(φ ′ k T φi)(−φi)] ] ,\n(19)\nAlgorithm 1 ComEmbed Require: graph G = (V,E), number of communities K,\nnumber of paths per node γ, walk length `, context window size ζ, embedding dimension d, size of negative context m, parameters α and β. Ensure: node embedding Φ, context embedding Φ′, community embedding (Ψ,Σ).\n1: Initialize a path set P = ∅; 2: for all vi ∈ V do 3: for j = 1 : γ do 4: path p← SamplePath(G, vi, `); 5: Vp ← GetNodeSubset(V, p); 6: for all vk exists in p do 7: Cp(vk)← GetContextSet(p, vk, ζ); 8: C̄p(vk)← SampleNegativeContext(G, vk,m); 9: Ep(vk)← GetEdgeSubset(E, p, vk);\n10: end for 11: P ← P ∪ p; 12: end for 13: end for 14: Initialize Φ, Φ′, Ψ and Σ; 15: for iter = 1 : T1 do 16: for subiter = 1 : T2 do 17: Updateψk by Eq. 10; update Σk by Eq. 11; update πik by Eq. 12; 18: end for 19: Shuffle the paths in P; 20: for all path p ∈ P do 21: for all vi ∈ Vp do 22: Do SGD to optimizeφi based onEp(vi),Cp(vi) and C̄p(vi) by Eq. 18; 23: end for 24: for all vj exists as a context for Vp do 25: Do SGD to optimize φ′j based on Vp, Cp(vi)’s and C̄p(vi)’s by Eq. 20; 26: end for 27: end for 28: end for\nFinally we have the gradient for φ′j as\n∂L′ ∂φ′j = ∂O2∂φ′j . (20)\nAlgorithm and complexity. We summarize our ComEmbed in Alg. 1. In lines 1–13, we sample paths on the graph and then prepare the positive context set Cp(vk), negative context set C̄p(vk) and edge set Ep(vk) for each node in these paths. In lines 16–18, we fix (Φ,Φ′) and optimize (Ψ,Σ). In lines 19–27, we fix (Ψ,Σ) and optimize (Φ,Φ′).\nWe analyze the time complexity for Alg. 1. Lines 1–13 takeO(γN`) to sample paths and node context in each path. Line 14 takesO(N+K) to initialize the parameters. Line 17 takes O(NK) to compute a GMM. Line 19 takes O(γN). Lines 21–23 take O(`2 + `K) to compute the node embedding gradients. Lines 24–26 take O(`2) to compute the context embedding gradients. Thus, the overall complexity for Alg. 1 is O(γN`+N +K+T1(T2NK+γN + `2 + `K)),\nwhich is linear to N . Hence Alg. 1 is efficient. Moreover we can easily parallelize Alg. 1 for each path in lines 20–27."
    }, {
      "heading" : "Experiments",
      "text" : "Data sets. We use two real-world data sets3: BlogCatalog and Flickr. The BlogCatalog data set has 10,312 nodes, 333,983 edges and 39 node labels. The Flickr data set has 80,513 nodes, 5,899,882 edges and 195 node labels.\nTasks. We evaluate ComEmbed in two tasks: 1) community prediction; 2) node classification. In community prediction, our goal is to predict the most likely community assignment for each node. Since our data sets are labeled, we set the number of communities K as the number of distinct labels in the data set. As an unsupervised task, we use the whole graph for learning embeddings and then predicting communities for each node. In node classification, our goal is to classify each node into one of the multiple labels. We follow (Perozzi, Al-Rfou, and Skiena 2014) to first train the embeddings on the whole graph, then we randomly split 10% (BlogCatalog) and 90% (Flickr) of the nodes as test data, respectively. We use the remaining nodes to train a classifier by LibSVM (c = 1 for all methods) (Chang and Lin 2011). We repeat 10 splits and report the average performance.\nEvaluation. In community prediction, we use both conductance (Kloster and Gleich 2014) and normalized mutual information (NMI) (Tian et al. 2014) as the evaluation metrics. Conductance is basically a ratio between the number of edges leaving a community and that within the community. NMI measures the closeness between the predicted communities with ground truth based on the node labels.\nIn node classification, we use micro-F1 and macro-F1 as the evaluation metrics (Perozzi, Al-Rfou, and Skiena 2014). Micro-F1 is the overall F1 w.r.t. all kinds of labels. MacroF1 is the average of F1 scores w.r.t. each kind of label.\nBaselines. To show the advantages of community embedding and its joint modeling with node embedding, we compare with several state-of-the-art node embedding methods.\n• DeepWalk (Perozzi, Al-Rfou, and Skiena 2014): it models the second-order proximity for node embedding.\n• LINE (Tang et al. 2015): it models both the first-order and second-order proximities for node embedding.\n• node2vec (Grover and Leskovec 2016): it extends DeepWalk to exploit homophily and structural roles for node embedding. It also models the second-order proximity.\nFor all the baselines, we use the publicly available codes released by their authors for experiments. We try to compare with all the baselines on both BlogCatalog and Flickr data sets. However, we are unable to produce results for node2vec on Flickr due to unmanageable out-of-memory errors on a machine with 64GB memory. Thus we exclude node2vec from comparison on Flickr.\nParameters and environment. For DeepWalk, node2vec and ComEmbed, we set γ = 10, ` = 80, ζ = 10, m = 5.\n3http://socialcomputing.asu.edu/pages/datasets\nFor LINE, we use concatenation of two embeddings for firstorder proximity and second-order proximity. In node2vec, we set p = 0.25, q = 0.25. We set the embedding dimension d = 128 for all methods. We always set K as the number of unique labels in the data sets. We run our experiments on the Linux machines equipped with eight 3.50GHz Intel Xeon(R) CPUs and 16GB memory.\nTask 1. Community Prediction. We compare ComEmbed with the baselines for community prediction. We set α = 1, β = 0.1 in ComEmbed for both data sets. As the baselines do not consider community in their embeddings, for fair comparison, we apply GMM over all the methods’ node embedding outputs for community prediction. As shown in Fig. 2, ComEmbed is consistently better than the baselines in terms of both conductance and NMI. Specifically, ComEmbed improves the best baseline (i.e., DeepWalk) in both data sets by relative 4.0%–5.5% (conductance) and 5.3%–11.2% (NMI). This improvement suggests that, for community prediction, modeling community together with node embedding is better than doing them separately.\nTask 2. Node Classification. We compare ComEmbed with the baselines for node classification. We set α = 1, β = 0.01 for BlogCatalog and α = 0.1, β = 0.01 for Flickr. We vary the number of training data to build the classifiers for each method’s node embeddings. As shown in Fig. 3, ComEmbed is generally better than the baselines in terms of both macro-F1 and micro-F1. Specifically, ComEmbed improves the best baselines (i.e., node2vec on BlogCatalog and DeepWalk on Flickr) in both data sets by relative 14.1%– 91.8% (macro-F1) and 7.6%–10.2% (micro-F1), when using 90% (BlogCatalog) and 10% (Flickr) of nodes for training. Our student t-tests show that all the above relative improvements are significant over the 10 data splits, with onetailed p-values always less than 0.01. It is interesting to see ComEmbed improves the baselines on node classification, since community embedding is after all unsupervised and it does not directly optimize the classification loss. This suggests that the higher-order proximity from community embedding does contribute to a better node embedding.\nImpact of parameters. We test different values for the model parameters α and β. As shown in Fig. 4, generally α = 0.01 gives the best results for community prediction and node classification. This suggests keeping an appropriate trade off for the second-order proximity in the objective function is necessary. β = 0.01 is generally the best for both tasks. In general, we can see when α and β are within the\nrange of [0.001, 1], the model performance is quite robust."
    }, {
      "heading" : "Conclusion",
      "text" : "In this paper, we study the problem of embedding communities on the graph. The problem is new because most of the existing graph embedding methods focus on individual nodes, instead of a group of nodes. We observe that community embedding and node embedding reinforce each other. On one hand, a good community embedding helps to get a good node embedding, thanks to its preserving the community structure during embedding. On the other hand, a good node embedding also helps to get a good community embedding, as clustering is then done over the nodes with good representations. We jointly optimize node embedding and community embedding. We evaluate our method on the real-world data sets, and show that it outperforms the state-of-the-art baselines by at least 4.0%–5.5% (conductance) and 5.3%–11.2% (NMI) in community prediction, 14.1%–91.8% (macro-F1) and 7.6%–10.2% (micro-F1) in node classification.\nIn the future, we plan to include node features in the community embedding. Besides, we also wish to extend our method to handle an infinite number of communities."
    } ],
    "references" : [ {
      "title" : "Laplacian eigenmaps and spectral techniques for embedding and clustering",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "NIPS, 585–591.",
      "citeRegEx" : "Belkin and Niyogi,? 2001",
      "shortCiteRegEx" : "Belkin and Niyogi",
      "year" : 2001
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Secaucus, NJ, USA: Springer-Verlag New York, Inc.",
      "citeRegEx" : "Bishop,? 2006",
      "shortCiteRegEx" : "Bishop",
      "year" : 2006
    }, {
      "title" : "Deep neural networks for learning graph representations",
      "author" : [ "S. Cao", "W. Lu", "Q. Xu" ],
      "venue" : "AAAI, 1145–1152.",
      "citeRegEx" : "Cao et al\\.,? 2016",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2016
    }, {
      "title" : "Libsvm: A library for support vector machines",
      "author" : [ "Chang", "C.-C.", "Lin", "C.-J." ],
      "venue" : "ACM Trans. Intell. Syst. Technol. 2(3):27:1–27:27.",
      "citeRegEx" : "Chang et al\\.,? 2011",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2011
    }, {
      "title" : "Heterogeneous network embedding via deep architectures",
      "author" : [ "S. Chang", "W. Han", "J. Tang", "G. Qi", "C.C. Aggarwal", "T.S. Huang" ],
      "venue" : "KDD, 119–128.",
      "citeRegEx" : "Chang et al\\.,? 2015",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2015
    }, {
      "title" : "Multidimensional Scaling, Second Edition",
      "author" : [ "T.F. Cox", "M. Cox" ],
      "venue" : "Chapman and Hall/CRC, 2 edition.",
      "citeRegEx" : "Cox and Cox,? 2000",
      "shortCiteRegEx" : "Cox and Cox",
      "year" : 2000
    }, {
      "title" : "node2vec: Scalable feature learning for networks",
      "author" : [ "A. Grover", "J. Leskovec" ],
      "venue" : "KDD.",
      "citeRegEx" : "Grover and Leskovec,? 2016",
      "shortCiteRegEx" : "Grover and Leskovec",
      "year" : 2016
    }, {
      "title" : "Heat kernel based community detection",
      "author" : [ "K. Kloster", "D.F. Gleich" ],
      "venue" : "KDD, 1386–1395.",
      "citeRegEx" : "Kloster and Gleich,? 2014",
      "shortCiteRegEx" : "Kloster and Gleich",
      "year" : 2014
    }, {
      "title" : "Community detection via measure space embedding",
      "author" : [ "M. Kozdoba", "S. Mannor" ],
      "venue" : "NIPS, 2890–2898.",
      "citeRegEx" : "Kozdoba and Mannor,? 2015",
      "shortCiteRegEx" : "Kozdoba and Mannor",
      "year" : 2015
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "O. Levy", "Y. Goldberg" ],
      "venue" : "NIPS, 2177–2185.",
      "citeRegEx" : "Levy and Goldberg,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg",
      "year" : 2014
    }, {
      "title" : "Contextdependent knowledge graph embedding",
      "author" : [ "Y. Luo", "Q. Wang", "B. Wang", "L. Guo" ],
      "venue" : "EMNLP, 1656– 1661.",
      "citeRegEx" : "Luo et al\\.,? 2015",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "NIPS, 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning convolutional neural networks for graphs",
      "author" : [ "M. Niepert", "M. Ahmed", "K. Kutzkov" ],
      "venue" : "ICML, 2014– 2023.",
      "citeRegEx" : "Niepert et al\\.,? 2016",
      "shortCiteRegEx" : "Niepert et al\\.",
      "year" : 2016
    }, {
      "title" : "Asymmetric transitivity preserving graph embedding",
      "author" : [ "M. Ou", "P. Cui", "J. Pei", "Z. Zhang", "W. Zhu" ],
      "venue" : "KDD, 1105–1114.",
      "citeRegEx" : "Ou et al\\.,? 2016",
      "shortCiteRegEx" : "Ou et al\\.",
      "year" : 2016
    }, {
      "title" : "Deepwalk: Online learning of social representations",
      "author" : [ "B. Perozzi", "R. Al-Rfou", "S. Skiena" ],
      "venue" : "KDD, 701–710.",
      "citeRegEx" : "Perozzi et al\\.,? 2014",
      "shortCiteRegEx" : "Perozzi et al\\.",
      "year" : 2014
    }, {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "S.T. Roweis", "L.K. Saul" ],
      "venue" : "Science 290(5500):2323–2326.",
      "citeRegEx" : "Roweis and Saul,? 2000",
      "shortCiteRegEx" : "Roweis and Saul",
      "year" : 2000
    }, {
      "title" : "Line: Large-scale information network embedding",
      "author" : [ "J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei" ],
      "venue" : "WWW, 1067–1077.",
      "citeRegEx" : "Tang et al\\.,? 2015",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "A global geometric framework for nonlinear dimensionality reduction",
      "author" : [ "J.B. Tenenbaum", "V. de Silva", "J.C. Langford" ],
      "venue" : "Science",
      "citeRegEx" : "Tenenbaum et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Tenenbaum et al\\.",
      "year" : 2000
    }, {
      "title" : "Learning deep representations for graph clustering",
      "author" : [ "F. Tian", "B. Gao", "Q. Cui", "E. Chen", "T. Liu" ],
      "venue" : "AAAI, 1293–1299.",
      "citeRegEx" : "Tian et al\\.,? 2014",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2014
    }, {
      "title" : "Structural deep network embedding",
      "author" : [ "D. Wang", "P. Cui", "W. Zhu" ],
      "venue" : "KDD, 1225–1234.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Representation learning of knowledge graphs with entity descriptions",
      "author" : [ "R. Xie", "Z. Liu", "J. Jia", "H. Luan", "M. Sun" ],
      "venue" : "AAAI, 2659–2665.",
      "citeRegEx" : "Xie et al\\.,? 2016",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2016
    }, {
      "title" : "Modularity based community detection with deep learning",
      "author" : [ "L. Yang", "X. Cao", "D. He", "C. Wang", "X. Wang", "W. Zhang" ],
      "venue" : "IJCAI, 2252–2258.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Revisiting semi-supervised learning with graph embeddings",
      "author" : [ "Z. Yang", "W.W. Cohen", "R. Salakhutdinov" ],
      "venue" : "ICML, 40–48.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Graph embedding is a mainstream graph representation framework (Roweis and Saul 2000; Chang et al. 2015; Niepert, Ahmed, and Kutzkov 2016; Xie et al. 2016), which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on.",
      "startOffset" : 63,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "Graph embedding is a mainstream graph representation framework (Roweis and Saul 2000; Chang et al. 2015; Niepert, Ahmed, and Kutzkov 2016; Xie et al. 2016), which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on.",
      "startOffset" : 63,
      "endOffset" : 155
    }, {
      "referenceID" : 20,
      "context" : "Graph embedding is a mainstream graph representation framework (Roweis and Saul 2000; Chang et al. 2015; Niepert, Ahmed, and Kutzkov 2016; Xie et al. 2016), which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on.",
      "startOffset" : 63,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "1) first-order proximity (Tenenbaum, de Silva, and Langford 2000; Belkin and Niyogi 2001), which considers two nodes as close if they are direct neighbors to each other in the graph; 2) second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016), which considers two nodes as close if they share similar neighbors in the graph; 3) combination of first-order proximity and second-order proximity (Tang et al.",
      "startOffset" : 25,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "1) first-order proximity (Tenenbaum, de Silva, and Langford 2000; Belkin and Niyogi 2001), which considers two nodes as close if they are direct neighbors to each other in the graph; 2) second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016), which considers two nodes as close if they share similar neighbors in the graph; 3) combination of first-order proximity and second-order proximity (Tang et al.",
      "startOffset" : 209,
      "endOffset" : 270
    }, {
      "referenceID" : 16,
      "context" : "1) first-order proximity (Tenenbaum, de Silva, and Langford 2000; Belkin and Niyogi 2001), which considers two nodes as close if they are direct neighbors to each other in the graph; 2) second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016), which considers two nodes as close if they share similar neighbors in the graph; 3) combination of first-order proximity and second-order proximity (Tang et al. 2015; Wang, Cui, and Zhu 2016), which considers two nodes as close if they are directly linked and also share similar neighbors.",
      "startOffset" : 420,
      "endOffset" : 463
    }, {
      "referenceID" : 16,
      "context" : "• Preserve community structure in graph embedding: the state of the art such as DeepWalk (Perozzi, Al-Rfou, and Skiena 2014), LINE (Tang et al. 2015) and node2vec (Grover and Leskovec 2016) is unable to preserve communities in the embedded space.",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "2015) and node2vec (Grover and Leskovec 2016) is unable to preserve communities in the embedded space.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "To represent a community, we are motivated by the Gaussian Mixture Model (Bishop 2006) to see each community as a Gaussian component.",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "There is few graph embedding work that considers higher-order proximity, and their definitions of higher-order proximity are not based on communities (Cao, Lu, and Xu 2016; Ou et al. 2016).",
      "startOffset" : 150,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "There is some work on using community to improve the node embedding (Yang et al. 2016), or using node embedding to achieve better community detection (Tian et al.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : "2016), or using node embedding to achieve better community detection (Tian et al. 2014).",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "For example, earlier methods, such as MDS (Cox and Cox 2000), LLE (Roweis and Saul 2000), IsoMap (Tenenbaum, de Silva, and Langford 2000) and Laplacian eigenmap (Belkin and Niyogi 2001), typically aim to solve the leading eigenvectors of graph affinity matrices as node embedding.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "For example, earlier methods, such as MDS (Cox and Cox 2000), LLE (Roweis and Saul 2000), IsoMap (Tenenbaum, de Silva, and Langford 2000) and Laplacian eigenmap (Belkin and Niyogi 2001), typically aim to solve the leading eigenvectors of graph affinity matrices as node embedding.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "For example, earlier methods, such as MDS (Cox and Cox 2000), LLE (Roweis and Saul 2000), IsoMap (Tenenbaum, de Silva, and Langford 2000) and Laplacian eigenmap (Belkin and Niyogi 2001), typically aim to solve the leading eigenvectors of graph affinity matrices as node embedding.",
      "startOffset" : 161,
      "endOffset" : 185
    }, {
      "referenceID" : 20,
      "context" : "Recent methods typically rely on neural networks to learn the representation for each node, with either shallow architectures (Yang, Cohen, and Salakhutdinov 2016; Xie et al. 2016; Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al.",
      "startOffset" : 126,
      "endOffset" : 259
    }, {
      "referenceID" : 16,
      "context" : "Recent methods typically rely on neural networks to learn the representation for each node, with either shallow architectures (Yang, Cohen, and Salakhutdinov 2016; Xie et al. 2016; Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al.",
      "startOffset" : 126,
      "endOffset" : 259
    }, {
      "referenceID" : 6,
      "context" : "Recent methods typically rely on neural networks to learn the representation for each node, with either shallow architectures (Yang, Cohen, and Salakhutdinov 2016; Xie et al. 2016; Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al.",
      "startOffset" : 126,
      "endOffset" : 259
    }, {
      "referenceID" : 4,
      "context" : "2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al. 2015).",
      "startOffset" : 54,
      "endOffset" : 132
    }, {
      "referenceID" : 10,
      "context" : "Other than node embedding, there is some attempt to learn edge embedding (Luo et al. 2015), which aims to learn the embedding for both entities (i.",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "Most existing methods focus on first-order proximity and/or second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016; Tang et al. 2015; Wang, Cui, and Zhu 2016), as discussed in the introduction.",
      "startOffset" : 83,
      "endOffset" : 187
    }, {
      "referenceID" : 16,
      "context" : "Most existing methods focus on first-order proximity and/or second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016; Tang et al. 2015; Wang, Cui, and Zhu 2016), as discussed in the introduction.",
      "startOffset" : 83,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : "1) DeepWalk is a realization of SkipGram (Mikolov et al. 2013) on the graph data, where DeepWalk treats each node as a “word” and each path as a “sentence”; 2) SkipGram implicitly factorizes a matrix based on the word-word coocurrence (Levy and Goldberg 2014).",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "2013) on the graph data, where DeepWalk treats each node as a “word” and each path as a “sentence”; 2) SkipGram implicitly factorizes a matrix based on the word-word coocurrence (Levy and Goldberg 2014).",
      "startOffset" : 178,
      "endOffset" : 202
    }, {
      "referenceID" : 13,
      "context" : "Both (Ou et al. 2016) and (Cao, Lu, and Xu 2016) design and factorize a higher-order node-node proximity matrix by PageRank or Katz index.",
      "startOffset" : 5,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "It is common to use node embedding results for community detection (Tian et al. 2014; Kozdoba and Mannor 2015), but they do not have the notion of community in their node embedding.",
      "startOffset" : 67,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "It is common to use node embedding results for community detection (Tian et al. 2014; Kozdoba and Mannor 2015), but they do not have the notion of community in their node embedding.",
      "startOffset" : 67,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : "There is some work that allows community feedback to guide the node embedding (Yang et al. 2016), but again it lacks the concept of community embedding and its community feedback requires extra supervision on must-links.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "Specifically, for each edge (vi, vj) ∈ E, we follow LINE (Tang et al. 2015) to model the likelihood of first-order proximity as",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "As the summarization in the softmax function is time consuming, we follow (Mikolov et al. 2013) to use negative sampling to replace the summation term in the softmax function.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "We follow (Tang et al. 2015) to define Pn(u) ∝ r u , where ru is node u’s degree.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "Finally, as GMM is known as a probabilistic version of K-means clustering (Bishop 2006), Eq.",
      "startOffset" : 74,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "According to (Bishop 2006), we can easily optimize (Ψ,Σ) by expectation maximization, and fortunately we have closedform update for each parameter as:",
      "startOffset" : 13,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "In community prediction, we use both conductance (Kloster and Gleich 2014) and normalized mutual information (NMI) (Tian et al.",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "In community prediction, we use both conductance (Kloster and Gleich 2014) and normalized mutual information (NMI) (Tian et al. 2014) as the evaluation metrics.",
      "startOffset" : 115,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "• LINE (Tang et al. 2015): it models both the first-order and second-order proximities for node embedding.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "• node2vec (Grover and Leskovec 2016): it extends DeepWalk to exploit homophily and structural roles for node embedding.",
      "startOffset" : 11,
      "endOffset" : 37
    } ],
    "year" : 2016,
    "abstractText" : "Most of the existing graph embedding methods focus on nodes, which aim to output a vector representation for each node in the graph such that two nodes being “close” on the graph are close too in the low-dimensional space. Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing. Embedding communities is useful, not only for supporting various community-level applications, but also to help preserve community structure in graph embedding. In fact, we see community embedding as providing a higher-order proximity to define the node closeness, whereas most of the popular graph embedding methods focus on first-order and/or second-order proximities. To learn the community embedding, we hinge upon the insight that community embedding and node embedding reinforce with each other. As a result, we propose ComEmbed, the first community embedding method, which jointly optimizes the community embedding and node embedding together. We evaluate ComEmbed on real-world data sets. We show it outperforms the state-of-the-art baselines in both tasks of node classification and community prediction.",
    "creator" : "TeX"
  }
}