{
  "name" : "1312.4637.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Constraint Reduction using Marginal Polytope Diagrams for MAP LP Relaxations",
    "authors" : [ "Zhen Zhang", "Qinfeng Shi", "Chunhua Shen", "Anton van den Hengel" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a unified formulation under which existing MAP LP relaxations may be compared and analysed. Furthermore, we propose a new tool called Marginal Polytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited such as node redundancy and edge equivalence. We show that using Marginal Polytope Diagrams allows the number of constraints to be reduced without loosening the LP relaxations. Then, using Marginal Polytope Diagrams and constraint reduction, we develop three novel message passing algorithms, and demonstrate that two of these show a significant improvement in speed over state-of-art algorithms while delivering a competitive, and sometimes higher, quality of solution.\nKeywords Constraint Reduction · Higher Order Potential · Message Passing · Probabilistic Graphical Models · MAP inference\n1 Introduction\nLinear Programming (LP) relaxations have been used to approximate the maximum a posteriori (MAP) inference of Probabilistic Graphical Models (PGMs)\n1. School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China, 710129 Shaanxi Provincial Key Laboratory of Speech & Image Information Processing, Xi’an, China, 710129 2. School of Computer Science, the University of Adelaide, Adelaide, Australia, SA, 5005\nar X\niv :1\n31 2.\n46 37\nv2 [\ncs .C\nV ]\n2 1\nA pr\n2 01\n2 Z. Zhanget al.\n[7] by enforcing local consistency over edges or clusters. An attractive property of this approach is that it is guaranteed to find the optimal MAP solution when the labels are integers. This is particularly significant in light of the fact that Kumar et al. showed that LP relaxation provides a better approximation than Quadratic Programming relaxation and Second Order Cone Programming relaxation [12]. Despite their success, there remain a variety of large-scale problems that off-the-shelf LP solvers can not solve [25]. Moreover, it has been shown [25, 19] that LP relaxations have a large gap between the dual objective and the decoded primal objective and fail to find the optimal MAP solution in many real-world problems.\nIn response to this shortcoming a number of dual message passing methods have been proposed including Dual Decompositions [10, 20, 21] and Generalised Max Product Linear Programming (GMPLP) [3]. These methods can still be computationally expensive when there are a large number of constraints in the LP relaxations. It is desirable to reduce the number of constraints in order to reduce computational complexity without sacrificing the quality of the solution. However, this is non-trivial, because for a MAP inference problem the dimension of the primal variable can be different in various LP relaxations. This also presents a barrier for effectively comparing the quality of two LP relaxations and their corresponding message passing methods. Furthermore, these message-passing methods may get stuck in non-optimal solutions due to the non-smooth dual objectives [18, 4, 16].\nOur contributions are: 1) we propose a unified form for MAP LP relaxations, under which existing MAP LP relaxations can be rewritten as constrained optimisation problems with variables of the same dimension and objective; 2) we present a new tool which we call the Marginal Polytope Diagram to effectively compare different MAP LP relaxations. We show that any MAP LP relaxation in the above unified form has a Marginal Polytope Diagram, and vice versa. We establish propositions to conveniently show the equivalence of seemingly different Marginal Polytope Diagrams; 3) Using Marginal Polytope Diagrams, we show how to safely reduce the number of constraints (and consequently the number of messages) without sacrificing the quality of the solution, and propose three new message passing algorithms in the dual; 4) we show how to perform message passing in the dual without computing and storing messages (via updating the beliefs only and directly); 5) we propose a new cluster pursuit strategy.\n2 MAP Inference and LP Relaxations\nWe consider MAP inference over factor graphs with discrete states. For generality, we will use higher order potentials (where possible) throughout the paper.\nConstraint Reduction using Marginal Polytope Diagrams 3\n2.1 MAP inference\nAssume that there are n variables X1, · · · , Xn, each taking discrete states xi ∈ Vals(Xi). Let V = {1, · · · , n} denote the node set, and let C be a collection of subsets of V. C has an associated group of potentials θ = {θc(xc) ∈ R|c ∈ C}, where xc = [xi]i∈c. Given a graph G = (V,C) and potentials θ, we consider the following exponential family distribution [22]:\np(x |θ) = 1 Z exp\n(∑\nc∈C θc(xc)\n) , (1)\nwhere x = [x1, x2, . . . , xn] ∈ X, and Z = ∑ x∈X exp( ∑ c∈C θc(xc)) is known as a normaliser, or partition function. The goal of MAP inference is to find the MAP assignment, x∗, that maximises p(x |θ). That is\nx∗ = argmax x\n∑ c∈C θc(xc). (2)\nHere we slightly generalise the notation of xc to xs = [xi]i∈s, xt = [xi]i∈t and xf = [xi]i∈f where s, t, f are subsets of V reserved for later use.\n2.2 Linear Programming Relaxations\nBy introducing\nµ = (µc(xc))c∈C, (3)\nthe MAP inference problem can be written as an equivalent Linear Programming (LP) problem as follows\nµ∗ = argmax µ∈M(G)\n∑\nc∈C\n∑\nxc\nµc(xc)θc(xc), (4)\nin which the feasible set, M(G), is known as the marginal polytope [22], defined as follows\n  µ ∣∣∣∣∣∣ q(x) > 0, ∀x∑ x q(x) = 1∑ xV \\c q(x) = µc(xc),∀c ∈ C,xc    . (5)\nHere the first two groups of constraints specify that q(x) is a distribution over X, and we refer to the last group of constraints as the global marginalisation constraint, which guarantees that for arbitrary µ in M(G), all µc(xc), c ∈ C can be obtained by marginalisation from a common distribution q(x) over X. In general, exponentially many inequality constraints (i.e. q(x) > 0, ∀x) are required to define a marginal polytope, which makes the LP hard to solve.\n4 Z. Zhanget al.\nThus (4) is often relaxed with a local marginal polytope ML(G) to obtain the following LP relaxation\nµ∗ = argmax µ∈ML(G)\n∑\nc∈C\n∑\nxc\nµc(xc)θc(xc). (6)\nDifferent LP relaxation schemes define different local marginal polytopes. A typical local marginal polytope defined in [20, 4] is as follows:\n  µ ∣∣∣∣∣∣ ∑ xc\\{i} µc(xc) = µi(xi),∀c ∈ C, i ∈ c, xi µc(xc) > 0,\n∑ xc µc(xc) = 1,∀c ∈ C,xc\n   . (7)\nCompared to the marginal polytope, for arbitrary µ in a local marginal polytope, all µc(xc) may not be the marginal distributions of a common distribution q(x) over X, but there are much fewer constraints in local marginal polytope. As a result, the LP relaxation can be solved more efficiently. This is of particular practical significance because state-of-the-art interior point or simplex LP solvers can only handle problems with up to a few hundred thousand variables and constraints while many real-world datasets demand far more variables and constraints [25, 12].\nSeveral message passing-based approximate algorithms [3, 19, 21] have been proposed to solve large scale LP relaxations. Each of them applies coordinate descent to the dual objective of an LP relaxation problem with a particular local marginal polytope. Different local marginal polytopes use different local marginalisation constraints, which leads to different dual problems and hence different message updating schemes.\n2.3 Generalised Max Product Linear Programming\nGloberson and Jaakkola [3] showed that LP relaxations can also be solved by message passing, known as Max Product LP (MPLP) when only node and edge potentials are considered, or Generalised MPLP (GMPLP) (see Section 6 of [3]) when potentials over clusters are considered.\nIn GMPLP, they define I = {s|s = c ∩ c′; c, c′ ∈ C}, and µg = (µc(xc), µs(xs))c∈C,s∈I. (8)\nThen they consider the following LP relaxation\nµg∗ = argmax µg∈MgL(G)\n∑\nc∈C\n∑\nxc\nµc(xc)θc(xc), (9)\nwhere the local marginal polytope MgL(G) is defined as\n   µg ∣∣∣∣∣∣∣∣ µc(xc) > 0,∀c ∈ C,xc∑ xc µc(xc) = 1, ∀c ∈ C ∑ xc\\s µc(xc) = µs(xs),∀c ∈ C, s ∈ Sg(c),xs   \n(10)\nConstraint Reduction using Marginal Polytope Diagrams 5\nwith Sg(c) = {s|s ∈ I, s ⊆ c}. To derive a desirable dual formulation, they replace the third group of constraints with the following equivalent constraints\nµc\\s,s(xc\\s,xs) = µc(xc), ∀c ∈ C, s ∈ Sg(c),xc,∑\nxc\\s\nµc\\s,s(xc\\s,xs) = µs(xs), ∀c ∈ C, s ∈ Sg(c),xs\nwhere µc\\s,s(xc\\s,xs) is known as the copy variable. Let βc\\s,s(xc\\s,xs) be the dual variable associated with the first group of the new constraints above, using standard Lagrangian yields the following dual problem:\nmin β\n∑ s∈I max xs\n∑\nc∈C,s∈Sg(c) max xc\\s βc\\s,s(xc\\s,xs)\ns.t. θc(xc) = ∑\ns∈Sg(c) βc\\s,s(xc\\s,xs), ∀c ∈ C,xc . (11)\nLet λc→s(xs) = maxxc\\s βc\\s,s(xc\\s,xs), they use a coordinate descent method to minimise the dual by picking up a particular c ∈ C and updating all λ∗c→s(xs) as following:\nλ∗c→s(xs) = −λ−cs (xs)+ 1 |Sg(c)| max xc\\s [ θc(xc)+ ∑\nŝ∈Sg(c) λ−cŝ (xŝ)\n] ,∀s ∈ Sg(c),xs (12)\nwhere λ−cs (xs) = ∑ ĉ∈{c̄|c̄∈C,c̄ 6=c,s∈Sg(c̄)} λĉ→s(xs). At each iteration the dual objective always decreases, thus guaranteeing convergence. Under certain conditions GMPLP finds the exact solution. Sontag et al. [19] extended this idea by iteratively adding clusters and reported faster convergence empirically.\n2.4 Dual Decomposition\nDual Decomposition [10, 20] explicitly splits node potentials (those potentials of order 1) from cluster potentials with order greater than 1, and rewrites the MAP objective (2) as ∑\ni∈V θi(xi) +\n∑ f∈F θf (xf ), (13)\nwhere F = {f |f ∈ C, |f | > 1}. By defining µd = (µi(xi), µf (xf ))i∈V,f∈F, they consider the following LP relaxation:\nmax µd∈MdL(G)\nfd(µ d)\nfd(µ d) =\n∑\ni∈V\n∑\nxi\nµi(xi)θi(xi) + ∑\nf∈F\n∑\nxf\nµf (xf )θf (xf ) (14)\n6 Z. Zhanget al.\nwith a different local marginal polytope MdL(G) defined as\n{ µd > 0 ∣∣∣∣ ∑ xi µi(xi) = 1,∀i ∈ V∑\nxf/{i} µf (xf ) = µi(xi),∀f ∈ F, i ∈ f, xi\n} . (15)\nLet λfi(xi) be the Lagrangian multipliers corresponding to each ∑\nxf\\{i} µf (xf ) =\nµi(xi) for each f ∈ F, i ∈ f, xi, one can show that the standard Lagrangian duality is\nL(λ) = ∑\ni∈V max xi\n( θi(xi) +\n∑\nf∈{f ′|f ′∈F,i∈f ′} λfi(xi)\n)\n+ ∑\nf∈F max xf\n( θf (xf )− ∑\ni∈f λfi(xi)\n) . (16)\nSubgradient or coordinate descent can be used to minimise the dual objective. Since the Dual Decomposition using coordinate descent is closely related to GMPLP and the unified form which we will present, we give the update rule derived by coordinate descent below,\nλ∗fi(xi) = −θi(xi)− λ−fi (xi)+ 1\n|f | maxxf\\{i} [ θf (xf )+ ∑\nî∈f θî(xî)+\n∑ î∈f λ−f î (xî) ] ,∀i∈f,xi (17)\nwhere f is a particular cluster from F, and λ−fi (xi) = ∑ f̂∈{f̄ |f̄∈F,f̄ 6=f,i∈f̄} λf̂ i(xi).\nCompared to GMPLP, the local marginal polytope in the Dual Decomposition has much fewer constraints. In general for an arbitrary graph G = (V,C), MdL(G) is looser than M g L(G) (i.e. M d L(G) ⊇MgL(G)) .\n2.5 Dual Decomposition with cycle inequalities\nRecently, Sontag et al. [21] proposed a Dual Decomposition with cycle inequalities considering the following LP relaxation\nµd∗ = max µ∈MoL(G) fd(µ d) (18)\nwith a local marginal polytope MoL(G),    µd ∣∣∣∣∣∣∣∣∣∣∣ ∑ xi µi(xi) = 1,∀i ∈ V, xi∑ xf/{i} µf (xf )=µi(xi),∀f ∈ F, i ∈ f, xi ∑ xf/e µf (xf )=µe(xe),∀f, e∈F, e⊂f,xe, |e|=2, |f |≥3\nµd > 0\n  \nThey added cycle inequalities to tighten the problem. Reducing the primal feasible set may reduce the maximum primal objective, which reduces the\nConstraint Reduction using Marginal Polytope Diagrams 7\nminimum dual objective. They showed that finding the “tightest” cycles, which maximise the decrease in the dual objective, is NP-hard. Thus, instead, they looked for the most “frustrated” cycles, which correspond to the cycles with the smallest LHS of their cycle inequalities. Searching for “frustrated” cycles, adding the cycles’ inequalities and updating the dual is repeated until the algorithm converges.\n3 A Unified View of MAP LP Relaxations\nIn different LP relaxations, not only the formulations of the objective, but also the dimension of primal variable may vary, which makes comparison difficult. By way of illustration, note that the primal variable in GMPLP is µg = (µc(xc), µs(xs))c∈C,s∈I, while in Dual Decomposition the primal variable is µd = {µi(xi), µf (xf )}i∈V,f∈F. Although µd can be reformulated to (µc(xc))c∈C if C = {{i}|i ∈ V} ∪ F, the variables {µs(xs)}s∈I (corresponding to intersections) in GMPLP still do not appear in Dual Decomposition. This shows that the dimensions of the primal variables in GMPLP and Dual Decomposition are different.\n3.1 A Unified Formulation\nWhen using the local marginal polytope the objective of the LP relaxation depends only on those µc(xc), c ∈ C. We thus reformulate the LP Relaxation into a unified formulation as follows:\nµ∗ = argmax µ∈ML(G,C′,S(C′))\n∑\nc∈C\n∑\nxc\nµc(xc)θc(xc), (19)\nwhere µ is defined in (3). The local marginal polytope, ML(G,C ′, S(C′)), can be defined in a unified formulation as\n   µ ∣∣∣∣∣∣∣∣ µc(xc) > 0,∀c ∈ C′,xc∑ xc µc(xc) = 1,∀c ∈ C′ ∑ xc\\s µc(xc) = µs(xs),∀c ∈ C′, s ∈ S(c),xs    . (20)\nHere C′ is what we call an extended cluster set, where each c ∈ C′ is called an extended cluster. S(C′) = (S(c))c∈C′ , where each s ∈ S(c) is a subset of c, which we refer to as a sub-cluster. The choices of C′ and S(C′) correspond to existing or even new inference algorithms, which will be shown later, and when specifying C′ and S(C′), we require C′ ∪(∪c∈C′ S(c)) ⊇ C.\nThe first two groups of constraints in ML(G,C ′, S(C′)) ensure that ∀c ∈ C′, µc(xc) is a distribution over Vals(xc). We refer to the third group of constraints as local marginalisation constraints.\n8 Z. Zhanget al.\nRemarks The LP formulation in (1) and (2) of [24] may look similar to ours. However, the work of [24] is in fact a special case of ours. In their work, an additional restriction s ⊂ c for (20) must be satisfied (see (4) in [24]). As a result, their work does not cover the LP relaxations in [19] and GMPLP, where redundant constraints like µc(xc) = µc(xc) are used to derive a message from one cluster to itself (see Figure 1 of [19]). Our approach, however, is in fact a generalisation of [19], GMPLP and [24].\n3.2 Reformulating GMPLP and Dual Decomposition\nHere we show that both GMPLP and Dual Decomposition can be reformulated by (19).\nLet us start with GMPLP first. Let C′ be C and S(C′) be Sg(C) = (Sg(c))c∈C. GMPLP (9) can be reformulated as follows\nµ∗ = argmax µ∈ML(G,C,Sg(C))\n∑\nc∈C\n∑\nxc\nµc(xc)θc(xc), (21)\nwhere ML(G,C, Sg(C)) is defined as\n   µ ∣∣∣∣∣∣∣∣ µc(xc) > 0,∀c ∈ C,xc∑ xc µc(xc) = 1,∀c ∈ C ∑ xc\\s µc(xc) = µs(xs),∀c ∈ C, s ∈ Sg(c),xs    . (22)\nWe can see that (22) and (10) only differ in the dimensions of their variables µ and µg (see (3) and (8)). Since the objectives in (21) and (9) do not depend on µs(xs), s ∈ I directly, the solutions of the two optimisation problems (21) and (9) are the same on µ.\nFor Dual Decomposition, we let Cd = {{i}|i ∈ V} ∪ F, and\nSd(c) = { ∅, |c| = 1 {{i}|i ∈ C} |c| > 1 . (23)\nLet C′ be Cd and S(C ′) be Sd(Cd) = (Sd(c))c∈Cd . Dual Decomposition (14) can be reformulated as\nµ∗ = argmax µ∈ML(G,Cd,Sd(Cd))\n∑\nc∈C\n∑\nxc\nµc(xc)θc(xc), (24)\nwhere ML(G,Cd, Sd(Cd)) is defined as\n   µ ∣∣∣∣∣∣∣∣ µc(xc) > 0, ∀c ∈ Cd,xc∑ xc µc(xc) = 1, ∀c ∈ Cd ∑ xc\\s µc(xc) = µs(xs), ∀c ∈ Cd, s ∈ Sd(c),xs    . (25)\nConstraint Reduction using Marginal Polytope Diagrams 9\nSimilarly, for Dual Decomposition with cycle inequalities in (18), we define So(c) as follows\nSo(c) =    ∅, |c| = 1 {{i, j}|{i, j} ⊂ c} |c| = 3 {{i}} |c| > 1, |c| 6= 3 . (26)\nLet C′ be Cd and S(C ′) be So(Cd) = (So(c))c∈Cd , we reformulate the problem in (18) as\nµ∗ = argmax µ∈ML(G,Cd,So(Cd))\n∑\nc∈C\n∑\nxc\nµc(xc)θc(xc). (27)\n3.3 Generalised Dual Decomposition\nNote that MdL(G) and M o L(G) in Dual Decomposition are looser than M g L(G). This suggests that for some θ Dual Decomposition may achieve a lower quality solution or slower convergence (in terms of number of iterations) than GMPLP 1. We show using the unified formulation of LP Relaxation in (19), Dual Decomposition can be derived on arbitrary local marginal polytopes (including those tighter than MgL(G), M d L(G) and M o L(G)). We refer to this new type of Dual Decomposition as Generalised Dual Decomposition (GDD), which forms a basic framework for more efficient algorithms to be presented in Section 6.\n3.3.1 GDD Message Passing\nLet λc→s(xs) be the Lagrangian multipliers (dual variables) corresponding to the local marginalisation constraints ∑ xc\\s\nµc(xc) = µs(xs) for each c ∈ C′, s ∈ S(c),xs. Define\nT = C′ ∪[ ∪ ĉ∈C′ S(ĉ)], (28)\nand the following variables ∀t ∈ T,xt:\nθ̂t(xt) = 1(t ∈ C)θt(xt) , (29a) γt(xt) = 1(t ∈ C′)\n∑ ŝ∈S(t)\\{t} λt→ŝ(xŝ) , (29b)\nλt(xt) = ∑\nc∈{c′|c′∈C′,t∈S(c′)\\{c′}} λc→t(xt) , (29c)\nbt(xt) = θ̂t(xt) + λt(xt)− γt(xt) , (29d) 1 This does not contradict the result reported in [21], where Dual Decomposition with cycle inequalities converges faster in terms of running time than GMPLP. In [21], on all their datasets MoL(G) = M g L(G) as the order of clusters are at most 3. Dual Decomposition with cycle inequalities runs faster because it has a better cluster pursuit strategy. On datasets with higher order potentials, it may have worse performance than GMPLP.\n10 Z. Zhanget al.\nwhere 1(S) is the indicator function, which is equal to 1 if the statement S is true and 0 otherwise. Define λ = (λc→s(xs))c∈C′,s∈S(c), we have the dual problem (see derivation in Section 1 of the supplementary).\ng(λ) = max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n[∑\nc∈C\n∑\nxc\nµc(xc)θc(xc)+\n∑\nc∈C′\n∑\ns∈S(c)\n∑\nxs\n( µs(xs)− ∑\nxc\\s\nµc(xc) ) λc→s(xs) ]\n= ∑\nt∈T max xt bt(xt). (30)\nIn (30), if c ∈ S(c) for some c ∈ C′, the variable λc→c(xc) will always be cancelled out2. As a result, λc→c(xc) can be set to arbitrary value. To optimise (30), we use coordinate descent. For any c ∈ C′ fixing all λc′→s(xs), c′ ∈ C′, s ∈ S(c′) except λc,S(c) = (λc→s(xs))s∈S(c)\\{c} yeilds a sub-optimisation problem,\nargmin λc,S(c) gc(λc,S(c)) ,\ngc(λc,S(c))= [ max xc [ θ̂c(xc)− ∑ s∈S(c)\\{c} λc→s(xs)+λc(xc) ] +\n∑ s∈S(c)\\{c} max xs [ θ̂s(xs)−γs(xs) +λ−cs (xs) +λc→s(xs)\n]] , (31)\nwhere ∀s ∈ S(c) \\ {c},xs\nλ−cs (xs)= ∑\nĉ∈{c′|c′∈C,c′ 6=c,s∈S(c′)\\{c′}} λĉ→s(xs). (32)\nA solution is provided in the proposition below.\nProposition 1 ∀s ∈ S(c) \\ {c},xs, let\nλ∗c→s(xs) = −θ̂s(xs) + γs(xs)− λ−cs (xs)\n+ 1\n| S(c) \\ {c}| maxxc\\s\n[ θ̂c(xc) + λc(xc)+\n∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ) )] , (33)\nthen λ∗c,S(c) = (λ ∗ c→s(xs))s∈S(c)\\{c} is a solution of (31).\n2 In dual objective other than (30), λc→c(xc) may not be cancelled out (e.g. the dual objective used in GMPLP).\nConstraint Reduction using Marginal Polytope Diagrams 11\nAlgorithm 1: GDD Message Passing\ninput : G = (V,C), ML(G,C ′,S(C′)), C′, S(C′), θ, Tg , Kmax output: λ = (λc→s(xs))c∈C′,s∈S(c)\\{c}\n1 k = 0, g0(λ) = +∞, λ = 0; 2 repeat 3 k = k + 1; 4 for c ∈ C′ do 5 Compute λ∗c,S(c) using (33); update λc,S(c) = λ ∗ c,S(c); 6 Computing b using (29);\n7 gk(λ) = ∑ t∈T maxxt bt(xt);\n/* By Proposition 3, g(λ) always converges. */\n8 until |gk(λ)− gk−1(λ)| < Tg or k > Kmax;\nThe derivation of (30) and (31), and the proof of Proposition 1 are provided in Section 1 in the supplementary material. The bt(xt) are often referred to as beliefs, and λc→s(xs) messages (see [3, 19]). In (33), λc(xc) and γs(xs), λ−cs (xs),∀s ∈ S(c) \\ {c} are known, and they do not depend on λc→s(xs). We summarise the message updating procedure in Algorithm 1. Dual Decomposition can be seen as a special case of GDD with a specific local marginal polytope ML(G,C, Sd(C)) in (25).\nDecoding The beliefs bt(xt), t ∈ T are computed via (29d) to evaluate the dual objective and decode an integer solution of the original MAP problem. For a g(λ) obtained via GDD based message passing, we find x∗ (so called decoding) via\nx∗t ∈ argmax xt bt(xt),∀t ∈ T . (34)\nHere we use ∈ instead of = is because there may be multiple maximisers. In fact, if a node i ∈ V is also an extended cluster or sub-cluster (i.e. ∃t ∈ T, s.t. t = {i}), then we perform more efficient decoding via\nx∗i ∈ argmax xi bi(xi). (35)\nFurther discussion on decoding is deferred to Proposition 4 and Section 3.5.\n3.3.2 Convergence and Decoding Consistency\nIn this part we analyse the convergence and decoding consistency of GDD message passing.\nGDD essentially iterates over c ∈ C′, and updates the messages via (33). The dual decrease defined below\nd(c) =gc(λc,S(c))− gc(λ∗c,S(c)) (36)\nplays a role in the analysis of GDD.\n12 Z. Zhanget al.\nProposition 2 (Dual Decrease) For any c ∈ C′, the dual decrease d(c) = max\nxc bc(xc) +\n∑\ns∈S(c)\\{c} max xs bs(xs)\n−max xc\n[ bc(xc) + ∑\ns∈S(c)\\{c} bs(xs)\n] > 0. (37)\nThe proof is provided in Section 2 of the supplementary. A natural question is whether GDD is convergent, which is answered by the following proposition.\nProposition 3 (Convergence) GDD always converges.\nProof According to duality and LP relaxation, we have for arbitrary λ,\ng(λ) > max x\n∑ c∈C θc(xc). (38)\nBy Proposition 2 in each single step of coordinate descent, the dual decrease d(c) is non-negative. Thus GDD message passing produces a monotonically decreasing sequence of g(λ). Since the sequence has a lower bound, the sequence must converge.\nNote that Proposition 3 does not guarantee g(λ) reaches the limit in finite steps in GDD (GMPLP and Dual Decomposition have the same issue). However, in practice we observe that GDD often converges in finite steps. The following proposition in part explains why the decoding in (35) is reasonable.\nProposition 4 (Decoding Consistency) If GDD reaches a fixed point in finite steps, then ∀c ∈ C′, s ∈ S(c) \\ {c}, there exist x̂c ∈ argmaxxc bc(xc), and x̄s ∈ argmaxxs bs(xs), s.t. x̂s = x̄s. Proof If GDD reaches a fixed point, ∀c ∈ C′, d(c) = 0 (see (37) ). Otherwise a non-zero dual decrease means GDD would not stop. Thus ∀c ∈ C′,\nmax xc\nbc(xc) + ∑\ns∈S(c)\\{c} max xs bs(xs) =\nmax xc\n[ bc(xc) + ∑\ns∈S(c)\\{c} bs(xs)\n] , (39)\nwhich completes the proof.\nProposition 4 essentially states that there exist two maximisers that agree on xs. This in part justifies decoding via (35) (e.g. s is a node). Further discussion on decoding is provided in Section 3.5.\nIt’s obvious that the solution of GDD is exact, if the gap between the dual objective and the decoded primal objective is zero. Here we show that the other requirements for the exact solution also hold.\nProposition 5 If there exists x that maximises bt(xt),∀t ∈ T, the solution of GDD is exact.\nThe proof is provided in Section 3 of the supplementary. Proposition 4 and 5 generalise the results of Section 1.7 in [20].\nConstraint Reduction using Marginal Polytope Diagrams 13\nAlgorithm 2: Belief Propagation Without Messages\ninput : G = (V,C), ML(G,C ′,S(C′)), C′, S(C′), θ, Tg , Kmax, output: b = (bt(xt))t∈T ;\n1 b = (θ̂t(xt))t∈T , k = 0, g 0(λ) = +∞; 2 repeat 3 k = k + 1; 4 for c ∈ C′ do 5 Compute b∗c,S(c) using (40); update bc,S(c) = b ∗ c,S(c);\n6 gk(λ) = ∑ t∈T maxxt bt(xt);\n/* By Proposition 3, g(λ) always converges. */\n7 until |gk(λ)− gk−1(λ)| < Tg or k > Kmax;\n3.4 Belief Propagation Without Messages\nGDD involves updating many messages (e.g. λc(xc) and γs(xs), λ −s c (xs),∀c ∈ C′, s ∈ S(c) \\ {c}). These messages are then used to compute the beliefs bt(xt),∀t ∈ T (see (29d)). Here we show that we can directly update the beliefs without computing and storing messages.\nWhen optimising (31), bc(xc) and bs(xs), s ∈ S(c) \\ {c} are determined by λc,S(c)(xc) (see (78) in supplementary). Thus let bc,S(c) = (bc(xc), bs(xs))s∈S(c)\\{c} be the beliefs determined by λc,S(c), and b ∗ c,S(c) be the beliefs determined by λ∗c,S(c). We have the following proposition. Proposition 6 When optimising (31), the beliefs b∗c,S(c) can be computed from a bc,S(c) determined by arbitrary λc,S(c) as following:\nb∗s(xs)= max xc\\s\n[ bc(xc)+ ∑ ŝ∈S(c)\\{c}bŝ(xŝ) ]\n| S(c) \\ {c}| ,∀s∈S(c)\\{c},xs\nb∗c(xc) =bc(xc)+ ∑\nŝ∈S(c)\\{c} bŝ(xŝ)−\n∑\nŝ∈S(c)\\{c} b∗ŝ(xŝ),∀xc . (40)\nThe proof is provided in Section 4 of the supplementary. The reason that b∗c,S(c) can be computed using bc,S(c) from an arbitrary λc,S(c), is because λc→s(xs), s ∈ S(c) \\ {c} is cancelled out (e.g. see (86) in supplementary) during the calculation. In the first iteration, beliefs are initialised via bt(xt) = θ̂t(xt),∀t ∈ T, and then we can use bc,S(c) from previous iterations to compute b∗c,S(c) without messages and the potentials.\nMemory Conservation For message updating based methods such as GDD message passing in Algorithm 1, Max-Sum Diffusion [11, 24], GMPLP[3] and etc, both messages and potentials need to be stored in order to compute new messages (see (33) for example). However, the proposed belief propagation without messages (summarised in Algorithm 2) only needs to store beliefs. The beliefs can simple take the space of potentials (i.e. initialisation), and then update. For a graph G, we assume that each node takes k = | Vals(Xi)| states, and let ML(G,C ′, S(C′)) be a local marginal polytope with specific C′\n14 Z. Zhanget al.\nand S(C′). Then we need the following space to store beliefs, potentials and messages:\nMembeliefs = ∑\nt∈T k|t|, (41a)\nMempotentials = ∑\nc∈C k|c|, (41b)\nMemmessages = ∑\nc∈C\n∑\ns∈S(c)\\{c} k|s|. (41c)\nRecall the definition of T in (28), it is easy to show that\nMembeliefs = ∑\nc∈C k|c| +\n∑\ns∈T \\C k|s|\n= ∑\nc∈C k|c| +\n∑\ns∈(C∪(∪c∈C S(c))\\C k|s|\n6 ∑\nc∈C k|c| +\n∑\ns∈∪c∈C S(c) k|s|\n6 ∑\nc∈C k|c| +\n∑\nc∈C\n∑\ns∈S(c)\\{c} k|s|\n=Mempotentials + Memmessages. (42)\nThis means that belief propagation without messages always uses less memory than message passing. If we choose T = C, and S(t) = {t′|t′ ⊂ t}, the memory for message passing has a simpler form\nMempotentials + Memmessages = ∑\nt∈T k|t| +\n∑\nt∈T\n∑ s⊂t k|s|,\n= ∑\nt∈T k|t| +\n∑\nt∈T\n[ (1 + k)|t| − k|t| − 1 ]\n= ∑\nt∈T\n[ (1 + k)|t| − 1 ] . (43)\nLet k = 2, |t| = 10, we have k|t| = 1024, and (k + 1)|t| − 1 = 59048, where message updating based methods uses approximately 59 times memory as belief propagation without messages.\n3.5 “Stealth” Cluster Pursuit\nIf GDD does not find the exact solution, then there exists a gap between the dual objective of GDD and the decoded primal objective. Various approaches, including [9, 23, 19, 1], try to tighten the gap. These approaches typically involve two steps: 1) creating a dictionary of clusters, and then 2) search for a best cluster in the dictionary w.r.t. some score function. They typically\nuse a fixed dictionary of clusters. For example, Sontag et al [19] consider the dictionary as all possible triplets; Werner [23] considers all order 4 cycles for grids type graphs.\nDual decrease is a popular choice of the score function. For example, Sontag et al [19], brutal force search over the dictionary for the cluster with maximum dual decrease. Batra et al [1] accelerate the process by computing primal dual gap for all clusters in the dictionary first (computing primal dual gap is much cheaper than computing dual decrease), and then only search over the clusters with non-zero primal dual gaps for the cluster with maximum dual decrease.\nIn this section, we show a new cluster pursuit strategy, which dynamically generates a dictionary of clusters instead of using a fixed one. The new cluster pursuit strategy is based on a special type of cluster which we call “stealth” clusters.\nDefinition 1 (“Stealth” cluster) ∀c1, c2 ∈ C′, we say c = c1 ∪ c2 is a “stealth” cluster if\n1. ∃s ∈ T s.t. s ∈ S(c1) \\ {c1}, s ∈ S(c2) \\ {c2}, and 2. @ĉ ∈ C′ s.t. c1 ∈ S(ĉ) \\ {ĉ}, c2 ∈ S(ĉ) \\ {ĉ}.\nProposition 4 essentially says there exist two maximisers that agree on xs. There is however a situation where the decoding from different clusters may disagree. Let us consider two clusters c1, c2 in Definition 1. According to Proposition 4 there exists a maximiser x′s of bs(xs) that agrees with bc1(xc1), and a maximiser x′′s of bs(xs) that agrees with bc2(xc2). However, if bs(xs) has multiple maximisers, the maximisers x′s and x ′′ s could be different. This means bc1(xc1) and bc2(xc2) may disagree on their overlap. Adding c1∪c2 into C′ with S(c1 ∪ c2) = {s|s ∈ T, s ⊂ (c1 ∪ c2)} at least one maximiser of bc1(xc1) and bc2(xc2) will become the same according to Proposition 4. This observation yields a strategy to dynamically generate dictionary of clusters to tighten the relaxation.\nWith similar derivation as in [19], adding a new “stealth” cluster c = c1∪c2 with S(c) = {s|s ∈ T, s ⊂ c}, the dual decrease after one message updating for\n16 Z. Zhanget al.\nAlgorithm 3: GDD with “Stealth” Cluster Pursuit\ninput : G = (V,C), ML(G,C ′,S(C′)), C′, S(C′), θ,\nthreshold Tg, Ta, max iterations K 1 max and K 2 max,\nmax running timeTmax, cluster addition size n output: x∗\n1 l = 0; 2 Initialise λ0 = 0; b0 := (b0t (xt))t∈T = (θ̂t(xt))t∈T ; 3 repeat 4 l = l + 1; T = C′ ∪(∪c∈C′ S(c)); P = ∅; 5 K = 1(l = 1)K1max + 1(l > 1)K 2 max; 6 Run Algo. 1 (λl,bl) =GDD(G,ML,C ′,S(C′), θ, Tg, K,λ l−1); 7 or Algo. 2 bl =BP(G,ML,C ′,S, θ, Tg, K,b l−1); 8 for t ∈ T do 9 for c1, c2 ∈ {c|c ∈ C′, t ∈ S(c) \\ {c}} do\n10 {x̄c1} = argmaxxc1 b l c1 (xc1 ); 11 {x̂c2} = argmaxxc1 b l c2 (xc2 ); 12 if @x̄c1 , x̂c2 , s.t. x̂t = x̄t then 13 P = P ∪ {c1 ∪ c2}; 14 S(c1 ∪ c2) = {s|s ∈ T, s ⊂ (c1 ∪ c2)}; 15 Compute d1(c) according to (44);\n16 Add the n clusters in P with largest d1(c) to C ′; 17 For all new added c, ∀s ∈ S(c) \\ {c},xs, λlc→s(xs) = 0; 18 Decode x∗ using (34) or (35); 19 g(λ) = ∑ t∈T maxxt bt(xt); 20 until |g(λ)− p(x∗ |θ)| 6 Ta or running time> Tmax;\nc is\nd1(c) =gc(λc,S(c))− gc(λ∗c,S(c)) = ∑\ns∈S(c)\\{c} max xs bs(xs)−max xc\n∑\ns∈S(c)\\{c} bs(xs). (44)\nIn practice, we add clusters that will lead to largest dual decrease. With these observations, a new cluster pursuit strategy, which we call “stealth” cluster pursuit strategy, is summarised in Algorithm 3. In a nutshell, we search for disagreeing clusters which maximise the dual decrease. In the worst case, this can be slow if too many disagreeing “stealth” clusters exist. However, in practice it is very fast. As we can see from Figure 1, the number of disagreeing “stealth” clusters is far less (about 1% to 10%) than the total number of “stealth” clusters, which leads to a significant speed up. More importantly, “stealth” cluster pursuit makes our feasible set tighter than that of LP relaxation in (7) and GMPLP, which in turn are tighter than Dual Decomposition [20].\n“Stealth” cluster pursuit may get bigger and bigger clusters which would become prohibitively expensive to solve. In our experiments, it’s always computationally affordable. When it isn’t, one can use low order terms to approximate bt. Furthermore both the frustrated cycle search strategy in [21] and acceleration via evaluating primal dual gap first in [1] are applicable to GDD as well.\nConstraint Reduction using Marginal Polytope Diagrams 17\nConvergence and Consistency “Stealth” cluster pursuit can be seen as adding new constraints only. No matter which clusters are added to C′, the dual decrease d1(c) is always no-negative. Thus GDD with “stealth” cluster pursuit still have the same convergence and consistency properties as original GDD presented in Section 3.3.2.\n4 Marginal Polytope Diagrams\nLP relaxation based message passings can be slow if there are too many constraints. This motivates us to seek ways of reducing the number of constraints to reduce computational complexity without sacrificing the quality of the solution. Here we first propose a new tool which we call marginal polytope diagrams. Then with this tool, we show how to reduce constraints without loosening the optimisation problem.\nDefinition 2 (Marginal Polytope Diagram) Given a graph G = (V,C), GM = (VM ,EM ) with node set VM and edge set EM is said to be a marginal polytope diagram of G if\n1. C ⊆ VM ⊆ 2V and 2. a directed edge from c to s deonted by (c→ s), belongs to EM only nodes\nif c, s ∈ VM , s ⊆ c.\nRemarks Previous work in this vein includes Region graphs [26, 7] and Hasse diagrams (a.k.a poset diagrams) [17, 22, 15]. What distinguishes marginal polytope diagrams, however, is the fact that the receivers of an edge can be a subset of the senders, where Region graphs and Hasse diagrams require that the edge’s receivers must be a proper subset of the senders. For example, the definition of region graph in Page 419 of [7], requires that a receiver must be a proper subset of a sender in region graph. Likewise in Page 16 of [26], the authors state that a region graph must be a directed acyclic graph, which means that the edge’s receivers must be a proper subset of the senders (otherwise there would be a loop from a region to itself). This is particularly significant since some dual message passing algorithms (including GMPLP) send messages from a cluster c to itself. Hasse diagrams [17] have a further restriction, which corresponds to a special case of Marginal Polytope Diagram, where for arbitrary v1, v2 ∈ VM , v1 ⊂ v2 edge (v1 → v2) ∈ EM if and only if @v3 ∈ VM , s.t. v2 ⊂ v3 ⊂ v1. In some LP relaxation based message passing algorithms, some local marginalisation constraints from a cluster to itself are required, thus violating the proper subset requirement. Both Hasse diagrams and Region graphs are inapplicable in this case. For example, in Section 6 of [3], GMPLP sends messages from one cluster to itself, which requires a local marginalisation constraint from one cluster to itself. In [19], the message λij→ij (in their Figure 1) is from the edge ij to itself, which requires a local marginalisation constraint from the edge to itself. The proposed marginal polytope diagram not only handles the above situations, but also provides\na natural correspondence among marginal polytope diagram, local marginal polytope and MAP message passing.\nIn marginal polytope diagrams, we use rectangles to represent nodes (to differentiate from a graph of graphical models) similar to Hasse diagrams (see Section 4.2.1 in [22]). An example is given in Figure 2. For the marginal polytope diagram associated with GMPLP (in Figure 2 middle) has edges from a node to itself, which are not allowed in both Region graphs and Hasse diagrams. Also edges like ({2, 3, 4} → {3}) are not allowed in Hasse diagrams. We choose to use the term diagram instead of graph in order to distinguish from graphs in graphical models.\nFrom local marginal polytope to diagram Given a graph G = (V,C) and arbitrary local marginal polytope ML(G,C\n′, S(C′)), the corresponding marginal polytope diagram GM = (VM ,EM ) can be constructed as:\n1. VM = T (T is defined in (28)), 2. ∀c ∈ C′, if s ∈ S(c), then (c→ s) ∈ EM .\nFrom diagram to local marginal polytope Given a graph G = (V,C) and a marginal polytope diagram GM = (VM ,EM ), the corresponding local marginal polytope ML(G,C ′, S(C′)) can be recovered as follows:\nC′ = {c|c ∈ VM ,∃(c→ s) ∈ EM}, S(c) = {s|(c→ s) ∈ EM},∀c ∈ C′ . (45)\n4.1 Equivalent Edges\nDefinition 3 (Edge equivalence) For arbitrary G = (V,C), and marginal polytope diagram GM = (VM ,EM ) of G, ∀c1, c2, t ∈ VM , t ⊆ c1, t ⊆ c2, given\nU = {∑\nxc\\s\nµc(xc) = µs(xs),∀(c→ s) ∈\n(EM \\{(ĉ→ t)|ĉ ∈ VM , t ⊆ ĉ}),xs } , (46)\nConstraint Reduction using Marginal Polytope Diagrams 19\nFig. 3 Two types of edge equivalence in Proposition 7. Left: Type 1 (c → t) ⇔ (s → t). Right: Type 2 (s1 → t)⇔ (s2 → t).\nFig. 4 A counter example for the simpler edge equivalence definition.\nedges (c1 → t) and (c2 → t) are said to be equivalent w.r.t GM denoted by (c1 → t)⇔ (c2 → t), if the following holds:\n{ µ |U∪{ ∑\nxc1\\t\nµc1(xc1) = µt(xt),∀xt} }\n= { µ |U∪{ ∑\nxc2\\t\nµc2(xc2) = µt(xt),∀xt} } . (47)\nNote that the definition of edge equivalence does not require (c1 → t) and (c2 → t) from EM . Checking whether two edges are equivalent via Definition 3 might be inconvenient. In fact, edge equivalence can be read directly from a marginal polytope diagram.\nProposition 7 Given a graph G = (V,C) and a marginal polytope diagram GM = (VM ,EM ) of G, we have\n1. ∀c, s, t ∈ VM , t ⊂ s ⊂ c, if (c→ s) ∈ EM , then (c→ t)⇔ (s→ t); 2. If (c → s1), (c → s2) ∈ EM , then ∀t ∈ VM , t ⊂ s1, t ⊂ s2, (s1 → t) ⇔\n(s2 → t). The proof is provided in Section 5 of supplementary. In Figure 3 we give examples of the two types of edge equivalence in Proposition 7. Furthermore, the following proposition always holds.\nProposition 8 For arbitrary G = (V,C), and marginal polytope diagram GM = (VM ,EM ) of G, edge equivalence w.r.t. GM is an equivalence relation.\nThe proof is provided in Section 6 of supplementary.\nSimpler edge equivalence definition? Readers may wonder why in Definition 3, all edges sent to t are removed (see (46)), instead of only removing two edges (c1 → t), (c2 → t). The answer is that if we did so, the resulting edge equivalence (we call it simpler edge equivalence) would no longer be an equivalence relation. To see this, we can replace EM \\{(ĉ → t)|ĉ ∈ VM} with EM \\{(c1 → t), (c2 → t)} in (46), and see a counter example in Figure 4. In considering whether (s1 → t) /∈ EM and (s4 → t) /∈ EM are equivalent, we need to consider two constraint sets Ua = { ∑ xc\\s\nµc(xc) = µs(xs),∀(c → s) ∈ EM ∪{(s1 → t)},xs} and Ub = { ∑ xc\\s\nµc(xc) = µs(xs),∀(c → s) ∈ EM ∪{(s4 → t)},xs}. Then by the fact (s1 → t) ⇔ (s2 → t), (s3 → t) ⇔ (s4 → t), (s2 → t) ∈\n20 Z. Zhanget al.\nEM , (s3 → t) ∈ EM , we have {µ |Ua} = {µ |Ub}. Thus (s1 → t) ⇔ (s4 → t) by the simpler edge equivalence definition. However, by the simpler edge equivalence definition (s2 → t), (s3 → t) are not equivalent in general. This means transitivity does not hold. Thus the simpler edge equivalence is not an equivalence relation. Figure 4 is not a counter example for Definition 3, because (s1 → t), (s4 → t) are not equivalent by Definition 3.\nWith edge equivalence, we can see that given a marginal polytope diagram GM = (VM ,EM ) of a graph G = (V,C), the following two operations would not change the corresponding local marginal polytope.\n1. Adding a new edge that is equivalent to an existing edge in EM ; 2. Removing one of two existing equivalent edges in EM .\nBy composing the two operations above, we are able to derive a series of operations which would not change the corresponding local marginal polytope. For illustration, using Operation 1) first and then using Operation 2) would lead to an operation: replacing an existing edge in EM with an equivalent edge. Repeating Operation 2) we can merge several equivalent edges in EM to one edge. Repeating Operation 2) and then using Operation 1) we can replace a group of equivalent edges in EM with a new equivalent edge.\n4.2 Redundant Nodes\nThere is a type of node, the removal of which from a marginal polytope diagram does not change the local marginal polytope.\nDefinition 4 (Redundant Node) For any graph G = (V,C), and marginal polytope diagram GM = (VM ,EM ) of G, we say v ∈ VM \\C is a redundant node w.r.t. GM , if\n{ µ | ∑\nxc\\s\nµc(xc) = µs(xs),∀(c→ s) ∈ EM ,xs }\n= { µ | ∑\nxc\\s\nµc(xc) = µs(xs),∀(c→ s) ∈ Ê M ,xs\n}\nwhere\nÊ M = [ EM \\({(c→ v) ∈ EM} ∪ {(v → s) ∈ EM}) ]\n∪ { (c→ s)|(c→ v) ∈ EM , (v → s) ∈ EM } .\nThe following proposition provides an easy way to find redundant nodes in a marginal polytope diagram.\nProposition 9 Given a graph G = (V,C) and a marginal polytope diagram GM = (VM ,EM ), v ∈ VM \\C is a redundant node w.r.t. GM if either of the following statements is true:\n1. There is only one (c→ v) ∈ EM ; 2. All (c→ v) ∈ EM are equivalent w.r.t. GM according to Definition 3.\nProof Let us consider the first case where there is only one (c→ v) ∈ EM . It is easy to check that\n   µ ∣∣∣∣∣∣∣ ∀(v → s) ∈ EM ,xs, ∑ xc\\s µc(xc) = µs(xs); ∀xv, ∑ xc\\v µc(xc) = µv(xv)   \n(48a)\n=    µ ∣∣∣∣∣∣∣ ∀xv, ∑ xc\\v µc(xc) = µv(xv); ∀(v → s) ∈ EM ,xs, ∑ xv\\s µv(xv) = µs(xs)    . (48b)\nSince v /∈ C, µv(xv) is not part of µ in (3). Note that removing ∑\nxc\\v µc(xc) =\nµv(xv), ∀xv from (48a) would not change the set of µ described by (48a) as no other variables depend on µv(xv). That is,\n{ µ | ∑\nxĉ\\ŝ\nµĉ(xĉ) = µŝ(xŝ),∀(ĉ→ ŝ) ∈ EM1 ,xŝ }\n= { µ | ∑\nxĉ\\ŝ\nµĉ(xĉ) = µŝ(xŝ),∀(ĉ→ ŝ) ∈ EM2 ,xŝ } ,\nwhere EM1 = {(c→ s)|(v → s) ∈ EM}, and EM2 = {(c→ v)}∪{(v → s) ∈ EM}. Let A = {µ |∑xĉ\\ŝ µĉ(xĉ) = µŝ(xŝ),∀(ĉ→ ŝ) ∈ E M \\EM2 ,xŝ}. We have\n{ µ | ∑\nxĉ\\ŝ\nµĉ(xĉ) = µŝ(xŝ),∀(ĉ→ ŝ) ∈ EM1 ,xŝ } ∩A\n= { µ | ∑\nxĉ\\ŝ\nµĉ(xĉ) = µŝ(xŝ),∀(ĉ→ ŝ) ∈ EM2 ,xŝ } ∩A.\nSince EM = (EM \\EM2 ) ∪ EM2 and Ê M\n= (EM \\EM2 ) ∪ EM1 , { µ | ∑\nxĉ\\ŝ\nµĉ(xĉ) = µŝ(xŝ),∀(ĉ→ ŝ) ∈ Ê M ,xŝ\n}\n= { µ | ∑\nxĉ\\ŝ\nµĉ(xĉ) = µŝ(xŝ),∀(ĉ→ ŝ) ∈ EM ,xŝ } .\nHence v is a redundant node by definition. Now let us consider the second case, where there are multiple equivalent (c → v) ∈ EM . By edge equivalence we can keep one of (c → v) ∈ EM and remove the rest without changing the local marginal polytope, which becomes the first case.\n5 Constraint Reduction\nEquivalent edges offer an effective way to reduce constraints. By composing the two basic operations in Section 4.1, we can get a series of operations which would not change the corresponding local marginal polytope. For illustration we can partition EM into several equivalent classes by equivalence between edges, and we can simply pick up arbitrarily many edges in each equivalent class to get a new marginal polytope diagram with fewer edges. As each edge corresponds to a local marginalisation constraint, the number of constraints can be efficiently reduced by the above operations. As shown in Figure 5, all edges to node {3} are equivalent, thus we can keep just one of them in the diagram to keep the local marginal polytope unaltered yet with fewer constraints.\nGiven a graph G = (V,C), and a marginal polytope diagram GM = (VM ,EM ) of G, if a node v ∈ VM is a redundant node, we can reduce the number of constraints by removing v from VM to obtain a marginal polytope diagram as follows:\nGMR =(V M R ,E M R ),V M R = V M \\{v}, EMR =[E\nM \\({(c→ v) ∈ EM} ∪ {(v → s) ∈ EM})] ∪ {(c→ s)|(c→ v), (v → s) ∈ EM}, (49)\nConstraint Reduction using Marginal Polytope Diagrams 23\nand according to the definition of redundant nodes, GMR and G M correspond to the same local marginal polytope. Figure 6 gives an example of redundant node removal. In the middle diagram, one can see that nodes {2, 4, 5}, {2, 5, 6}, {4, 5, 8}, and {5, 6, 8} are redundant because their indegrees are 1. For node {5}, as all edges to {5} are equivalent, node {5} is also redundant. Removal of these redundant nodes leads to fewer constraints without changing the local marginal polytope.\nIt is worth mentioning that [8] is perhaps closest idea to ours in spirit. However, [8, Proposition 2.1] considers the removal of edges only, whereas ours considers the removal of both edges and nodes.\n5.1 Is the Minimal Number of Constraints Always a Good Choice?\nUsing marginal polytope diagrams one can safely reduce the number of constraints without altering the local marginal polytope. On one hand, fewer constraints means fewer belief updates on bt(xt), t ∈ T, which leads to lower run time per iteration. On the other hand, fewer constraints means fewer coordinates (i.e. search directions), and thus that the algorithm is more likely to get stuck at corners due to the non-smoothness of the dual objective and the nature of coordinate descent (this has been often observed empirically too). This means that minimal number of constraints is not always a good choice. As we shall see in the next section, a trade-off between the minimal number of constraints and the maximal number of constraints performs best.\n6 From Constraint Reduction To New Message Passing Algorithms\nHere we propose three new efficient algorithms for MAP inference, using different constraint reduction strategies (via marginal polytope diagrams). All three algorithms are based on the GDD belief propagation procedure which is equivalent to GDD message passing, thus the theoretical properties in Section 3.3.2 also hold for these three algorithms.\nTo derive new algorithms, we first construct a local marginal polytope as an initial local marginal polytope, and then by different constraint reduction strategies we get three different algorithms. For arbitrary graph G = (V,C), we let C′ be C0 = {c′|c′ ⊆ c, c ∈ C} and S(C′) be S0(C0) = (S0(c))c∈C0 with S0(c) = {s|s ∈ C0, s ⊂ c} to construct a local marginal polytope ML(G,C0, S0(C0)) as a initial local marginal polytope. Thus the marginal polytope diagram is GM0 = (V M 0 ,E M 0 ) where\nVM0 = {v|v ⊆ c, c ∈ C}, EM0 = {(c→ s)|c, s ∈ VM0 , s ⊂ c}. (50)\nThe marginal polytope diagram GM0 and GDD provide the base for all three algorithms.\n24 Z. Zhanget al.\n6.1 Power Set Algorithm\nIn the first algorithm which we call Power Set algorithm, we do not remove any redundant nodes in GM0 . One can see that ∀(c → t) ∈ EM0 , |c| − |t| > 1, ∃s ∈ VM0 , s.t.|s| = |t| + 1, (c → s) ∈ EM0 , which suggests (c → t) ⇔ (s → t). Using equivalent edges we get a marginal polytope diagram GMp = (V M p ,E M p ) as follows\nVMp = V M 0 , EMp = {(c→ s)|c, s ∈ VMp , s ⊂ c, |c| = |s|+ 1}, (51)\nwhich corresponds to the same local marginal polytope as GM0 . Thus we define Cp = {c|c ⊆ ĉ, ĉ ∈ C}, Sp(c) = {s|(c → s) ∈ EMp }, and let C′ be Cp, and S(C′) be Sp(Cp) = (Sp(c))c∈C′ , the corresponding local marginal polytope becomes ML(G,Cp, Sp(Cp)). By the fact that c /∈ Sp(c),∀c ∈ Cp, applying belief propagation without messages to ML(G,Cp, Sp(Cp)) yields the following belief propagation form ∀c ∈ Cp:\nb∗s(xs) = 1 |c| maxxc\\s [bc(xc) + ∑\nŝ∈Sp(c) bŝ(xŝ)],∀s ∈ Sp(c),xs\nb∗c(xc) = bc(xc)+ ∑\nŝ∈Sp(c) bŝ(xŝ)−\n∑\nŝ∈Sp(c) b∗ŝ(xŝ),∀xc . (52)\n6.2 π-System Algorithm\nThe second algorithm which we call π-System algorithm, is based on the πsystem [5] extended from C. Such a π-system denoted by Cπ has the following properites:\n1. if c ∈ C, then c ∈ Cπ; 2. if c1, c2 ∈ Cπ, then c1 ∩ c2 ∈ Cπ. We can construct Cπ using the properties above by assigning all elements in C to Cπ and adding intersections repeatedly to Cπ.\nProposition 10 All v ∈ VM0 \\Cπ are redundant nodes w.r.t GM0 .\nProof Since C ⊆ Cπ, for any v ∈ VM0 \\Cπ, we have v ∈ VM0 \\C. Now we prove the proposition by proving that all edges to v are equivalent.\nLet Pv = {p|(p→ v) ∈ EM0 }, and Cv = {c|c ∈ C, v ⊂ c}. We let s = ∩c∈Cvc, and we must have v ⊆ s. Moreover, if v = s we have v = ∩c∈Cvc ∈ Cπ, this contradicts the fact that v ∈ VM \\Cπ. Thus we must have v ⊂ s. Then, by the definition of VM0 and E M 0 in (50), ∀p ∈ Pv, ∃c ∈ Cv, s.t. p ⊆ c. By the fact that s = ∩ĉ∈Cv ĉ, we have s ⊆ c. Thus if p = c = s, then (p → v) ⇔ (s → v) naively holds. If only one of p and s is equal to c, we have (p → v) ⇔ (s → v) by Proposition 7 (the first case). If both p and s are not equal to c, by\nConstraint Reduction using Marginal Polytope Diagrams 25\nProposition 7 (the second case) we have (p → v) ⇔ (s → v). As a result, all (p → v), p ∈ Pv are equivalent, which implies that v is redundant node w.r.t. GM0 by Proposition 9.\nBy edge equivalence, we construct a marginal polytope diagram GMπ = (VMπ ,E M π ) below,\nVMπ = Cπ, (53) EMπ = {(c→ s)|c, s ∈ Cπ, s ⊂ c,@t ∈ Cπ, s.t. s ⊂ t ⊂ c}.\nThus we define Sπ(c) = {s|(c→ s) ∈ EMπ }. Let C′ be Cπ, and S(C′) be Sπ(Cπ) = (Sπ(c))c∈Cπ the corresponding marginal polytope becomes ML(G,Cπ, Sπ(Cπ)). By the fact that c /∈ Sπ(c),∀c ∈ Cπ, applying belief propagation without messages on ML(G,Cπ, Sπ(Cπ)) results in the following belief propagation form ∀c ∈ Cπ:\nb∗s(xs)= 1\n| Sπ(c)| max xc\\s\n[bc(xc)+ ∑\nŝ∈Sπ(c) bŝ(xŝ)],∀s ∈ Sπ(c),xs\nb∗c(xc)=bc(xc) + ∑\nŝ∈Sπ(c) bŝ(xŝ)−\n∑\nŝ∈Sπ(c) b∗ŝ(xŝ),∀xc . (54)\nNote that a node in the π-system may still be a redundant node.\n6.3 Maximal-Cluster Intersection algorithm\nHere we remove more redundant nodes by introducing the notion of maximal clusters.\nDefinition 5 (maximal cluster) Given a graph G = (V,C), a cluster c is said to be a maximal cluster of C, if c ∈ C,@ĉ ∈ C, s.t. c ⊂ ĉ. The intersection of all maximal clusters is\nIm = {s|s = c ∩ c′, c, c′ ∈ Cm}, (55)\nwhere\nCm = {c|c ∈ C,@ĉ ∈ C, s.t. c ⊂ ĉ}. (56)\nProposition 11 All v ∈ VM0 \\{C∪ Im} are redundant nodes w.r.t GM0 .\nThe proof is provided in Section 7 of the supplementary material. By edge equivalence, we construct a marginal polytope diagram GMm = (VMm ,E M m ) with\nVMm = C∪ Im EMm = {(c→ s)|c ∈ Cm, s ∈ C∪ Im, s ⊂ c}. (57)\n26 Z. Zhanget al.\nThus we define Sm(c) = {s|(c → s) ∈ EMm }. Let C′ be Cm, and S(C) be Sm(Cm) = (Sm(c))c∈Cm , the corresponding local marginal polytope becomes ML(G,Cm, Sm(Cm)). By the fact that c /∈ Sm(c),∀c ∈ Cm, applying belief propagation without messages to ML(G,Cm, Sm(Cm)) yields the following belief propagation ∀c ∈ CM :\nb∗s(xs)= 1\n| Sm(c)| max xc\\s\n[bc(xc)+ ∑\nŝ∈Sm(c) bŝ(xs)],∀s ∈ Sm(c),xs\nb∗c(xc)=bc(xc)+ ∑\nŝ∈Sm(c) bŝ(xs)−\n∑\nŝ∈Sm(c) b∗ŝ(xŝ),∀xc . (58)\n7 Experiments\nMAP LP relaxation can be solved using standard LP solvers such as CPLEX, Gurobi, LPSOLVE etc.. However, for the inference problems in our experiments the LP relaxations typically have more than 105 variables and 106 constraints. It is very slow to use standard LP solvers in this case. Even stateof-the-art commercial LP solvers such as CPLEX have been reported to be slower than message passing based algorithms [25]. Thus we only compare our methods against message passing based algorithms.\nWe compare the proposed algorithms (all with “stealth” cluster pursuit) which are Power Set algorithm (PS), π-System algorithm (π-S) and MaximalCluster Intersection algorithm (MI), with 3 competitors: GMPLP [3] with “triplet” cluster pursuit [19] (GMPLP+T), GMPLP with our “stealth” cluster pursuit (GMPLP+S), and Dual Decomposition with “triplet” and “cycle” cluster pursuit [21] (Sontag12). All algorithms run belief propagation/message passing with all original constraints (including the ones with higher order potentials). After several iterations of belief propagation, if there is a gap between the dual and decoded primal, different cluster pursuit strategy are applied to tighten the LP relaxations. A brief summary of these methods is provided in Table 1. Max-Sum Diffusion (MSD) [23, 24] has been shown empirically inferior to GMPLP [see 20, Figure 1.5]. Similarly TRW-S of [8] has been shown to be inferior to GMPLP in the higher order potential case [see 8, Section 5]. Thus we compare primarily with Sontag12 and GMPLP. Note that Sontag12 is considered the state-of-the-art.\nWe implement our algorithms and GMPLP in C++. For Sontag12, we use their released C++ code3. As all algorithms use a framework similar to Algorithm 3 (with different the message updating and cluster pursuit), we can describe the termination criteria for these algorithms using the notation from Algorithm 3. For all algorithms, the threshold of inner loop Tg is set to 10\n−8, and the maximum number of iterations K1max = 1000. We adopt K 2 max = 20 in light of the faster convergence observed empirically in [19]. The threshold for the outer loop Ta is set to 10\n−6. In each cluster pursuit we add n = 20 new clusters, and the maximum running time Tmax is set to 1 hour.\nGDD based algorithms can be implemented as either a message passing procedure or a belief propagation procedure without messages. We implemented both, and observed that both have similar speed (see Section 8.5 in the supplementary material). Of course, the latter uses less storage. For presentation clarity, we only report the result of GDD using belief propagation without messages here.\n7.1 Synthetic data\nWe generate a synthetic graphical model with a structure commonly used in image segmentation and denoising. The structure is a 128 × 128 grid shown in Figure 7(a) with 3 types of potentials: node potentials, edge potentials and higher order potentials. We consider the problem below,\nmax x\n[∑\ni∈V θi(xi) +\n∑ ij∈E θij(xi, xj) + ∑ f∈F θf (xf ) ] ,\nwhere each xi ∈ {1, 2, 3} and |f | = 4. All potentials are generated from normal distribution N(0, 1). For clusters with order ≥ 4, Sontag12 only enforces local marginalisation constraints from clusters to nodes, thus its initial local\n3 For computational efficiency, we optimised Sontag et al.’s released code (achieving the same output but with 2-3 times speed up). This is done for GDD and GMPLP as well to ensure a fair comparison. All algorithms are compiled with option “-O3 -fomit-frame-pointer -pipe -ffast-math” using “clang-4.2”, and all experiments are running in single thread with I7 3610QM and 16GB RAM.\nmarginal polytope is looser than that of GMPLP and our methods. As shown in Figure 7(b), the proposed methods, converge much faster than GMPLP+S, GMPLP+T and Sontag12. Also Sontag12’s gap between the dual objective and the decoded primal objective is much larger than that of our methods and GMPLP (even with cluster pursuit to tighten the local marginal polytope).\n7.2 Protein-Protein Interaction\nHere we consider 8 Protein-Protein Interaction (PPI) inference problems (from protein1 to protein8 ) used in [21]. In each problem, there are typically over 14000 nodes, and more than 42000 potentials defined on nodes, edges and triplets. Since the highest order of the potentials is only 3 (triplets), the local marginal polytopes (without cluster pursuit) of all methods here are the same tight. Thus performance difference here is mainly due to different cluster pursuit strategies and computational complexity per iteration.\nWe test all methods on all 8 problems. The average running time for one iteration of updating all beliefs or messages in Table 2 (i.e. steps 4-7 in Algorithm 2 for ours, and the counterpart for the competitors similar to steps 4-8 in Algorithm 1). We can see that the proposed methods have the smallest average running time, followed by Sontag12, and then by GMPLP.\nConstraint Reduction using Marginal Polytope Diagrams 29\nWe present dual objective plots on two problems in Figure 8 here, and provide the results for all problems in the supplementary (Section 8.2). Overall, the proposed methods converge fastest and two of them (π-S and MI) find exact solutions on 3 problems: protein2, protein4 and protein8. Sontag12 finds exact solutions on protein4 and protein8, and achieved the smallest dual objective values on the problems where all methods failed to find the exact solutions. In terms of convergence, Sontag12 converges slower than proposed methods and faster than GMPLP. GMPLP+T does not find an exact solution on any of the 8 problems, and GMPLP+S finds the exact solution on protein2 only.\n7.3 Image Segmentation\nImage segmentation is often seen as a MAP inference problem over PGMs. Following [6], we consider the MAP problem below\nmax x\n[∑\ni∈V θi(xi) +\n∑ ij∈E θij(xi, xj) + ∑ f∈F θf (xf ) ] .\nwhere |f | = 4. We use the same graph structure as in Figure 7(a), and follow the potentials in [6], where the colour terms in θi(xi) are computed as in [2]. More details including parameter settings are provided in the supplementary material. Here we segment three images: banana1, book and bool in the MSRC Grabcut dataset 4. The resolution of the images varies from 520 × 450 to 640 × 480, and each of the inference problems has about 2 × 105 to 3 × 105 nodes and more than 106 potentials. Sontag12 failed to find exact solutions in all 3 images. GMPLP+S and GMPLP+T find exact solution on book only. The proposed methods, PS, π-S and MI, find the exact solution on all three problems. The result is shown in Figure 9. From the third row of Figure 9 (primal-dual objetives), we can see that the proposed method converges much faster than the competitors. From the fourth row of Figure 9, we can see that the inference error rate (against the exact solution) reduced quickest to zero for the proposed methods.\n7.4 Image Matching\nHere we consider key point based image matching between two images (a source image and a destination image). First we detect key points from both images via SIFT [14] detector. Assume that there are m key points from the source image and n key points from the destination image. Let {p(i) ∈ R2}i=1,2,...,m and {q(i) ∈ R2}i=1,2,...,n be the coordinates of key points in the source and the destination images respectively. Let {h(i)}i=1,2,...,m and\n4 http://research.microsoft.com/en-us/um/cambridge/projects/visionimagevideoediting/ segmentation/grabcut.htm\n{g(i)}i=1,2,...,n be the SIFT feature vectors of the source and the destination images respectively. Assume m ≤ n (otherwise swap the source image and the destination image to guarantee so). The task is for each key point i ∈ {1, 2, . . . ,m} in the source image, to find a corresponding key point xi ∈ {1, 2, . . . , n} in the destination image. When there are a large number of points involved, a practical way is to restrict a corresponding key point xi ∈ {1, . . . , k}∪{−1}. Here if i has a corresponding point, we restrict it from its k-nearest neighbours of the SIFT feature vector h(i) in {q(i) ∈ R2}i=1,2,...,n. If i has no corresponding point, we let xi = −1.\nLet V = {1, 2, . . .m}, and the matching problem can be formulated as a MAP problem similar to [13],\nmax x\n{∑\ni∈V θi(xi) +\n∑ f∈F θf (xf ) } , (59)\nConstraint Reduction using Marginal Polytope Diagrams 31\nwhere |f | = 4, xi ∈ {1, . . . , k}∪{−1}, and constructing of F is provided in the supplementary. In [13], key points in the source image are filtered and reduced to less than 100 (see Section 3 of [13]), thus they often have corresponding key points in the destination image. As a result they did not use −1 to handle the no correspondence case. However, this strategy gives rise to a danger that potentially important key points may be removed too. Also the small scale of their problem allows them to let xi take all n states. In our experiment, we keep all key points (often over 103 in both source and destination images). In that situation, we face two issues: 1) each f ∈ F needs O((k + 1)4) storage for potentials and beliefs; 2) some key points in the source image have no corresponding points in the destination image. For the first issue, we set k = 4 for computational efficiency. For the second issue, we extend the models in [13] to handle the potential lack of correspondence. The node and higher order potentials are defined as follows:\nθi(xi) =\n{ −η xi = −1\n−δi||h(i)− g(xi)||22 otherwise\nθf (xf ) =\n{ 0 ∃i ∈ f, s.t.xi = −1\n−||PxfWf ||1 otherwise\nwhere η and δi are user specified parameters, Pxf = [q(xi)]i∈f ∈ R2×4, and Wf ∈ R4 is a column vector computed via solving (5) in [13] (details provided in supplementary).\nWe set η = −25, and δi = 100/maxxi ||h(i) − g(xi)||22,∀i ∈ V. We test all algorithms on 6 image sequences from Affine Covariant Regions Datasets 5. Each inference problem has about 1 × 103 to 3 × 103 nodes and 2 × 103 to 6 × 103 potentials. All algorithms find exact solutions. GMPLP converges before using cluster pursuit, thus GMPLP+S and GMPLP+T became the same (reported as GMPLP). From Figure 10 we can see that our π-S converges fastest among all methods in all images, followed by our MI. Both the total running time and the number of iterations required to reach an exact solution for the matching problems are reported in Tables 3 and 4. In several cases π-S takes an abnormally long time because it was trapped at a local optimum and cluster pursuit had to be applied to escape its basin of attraction. Two of the proposed methods, PS and π-S, take less running time and iterations in most cases as number of constraints and variables is sufficiently reduced without loosening the local marginal polytope.\n8 Conclusion\nWe have proposed a unified formulation of MAP LP relaxations which allows to conveniently compare different LP relaxation with different formulations of objectives and different dimensions of primal variables. With the unified formulation, a new tool, the Marginal Polytope Diagram, is proposed to describe\n5 http://www.robots.ox.ac.uk/~vgg/data/data-aff.html\nLP relaxations. With a group of propositions, we can easily find equivalence between different marginal polytope diagrams. Thus constraint reduction can be carried out via the removal of redundant nodes and replacement of equivalent edges in the marginal polytope diagram. Together with the unified formulation and constraint reduction, we have also proposed three new message\nConstraint Reduction using Marginal Polytope Diagrams 33\n1. Batra D, Nowozin S, Kohli P (2011) Tighter relaxations for map-mrf inference: A local primal-dual gap based separation algorithm. In: International Conference on Artificial Intelligence and Statistics, pp 146–154 2. Blake A, Rother C, Brown M, Perez P, Torr P (2004) Interactive image segmentation using an adaptive gmmrf model. In: Computer Vision-ECCV 2004, Springer, pp 428–441 3. Globerson A, Jaakkola T (2007) Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In: NIPS, vol 21 4. Hazan T, Shashua A (2010) Norm-product belief propagation: Primal-dual message-passing for approximate inference. Information Theory, IEEE Transactions on 56(12):6294–6316 5. Kallenberg O (2002) Foundations of modern probability. Springer Verlag\n34 Z. Zhanget al.\n6. Kohli P, Ladickỳ L, Torr PH (2009) Robust higher order potentials for enforcing label consistency. IJCV 82(3):302–324 7. Koller D, Friedman N (2009) Probabilistic graphical models: principles and techniques. MIT press 8. Kolmogorov V, Schoenemann T (2012) Generalized sequential treereweighted message passing. arXiv preprint arXiv:12056352 9. Komodakis N, Paragios N (2008) Beyond loose lp-relaxations: Optimizing mrfs by repairing cycles. In: Computer Vision–ECCV 2008, Springer, pp 806–820 10. Komodakis N, Paragios N, Tziritas G (2007) MRF optimization via dual decomposition: Message-passing revisited. In: ICCV, IEEE, pp 1–8 11. Kovalevsky V, Koval V (1975) A diffusion algorithm for decreasing energy of max-sum labeling problem. Glushkov Institute of Cybernetics, Kiev, USSR 12. Kumar MP, Kolmogorov V, Torr PH (2009) An analysis of convex relaxations for MAP estimation of discrete MRFs. The Journal of Machine Learning Research 10:71–106\nConstraint Reduction using Marginal Polytope Diagrams 35\n13. Li H, Kim E, Huang X, He L (2010) Object matching with a locally affineinvariant constraint. In: CVPR, IEEE, pp 1641–1648 14. Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503–510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795–804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R© in Machine Learning 1(1-2):1–305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1–8 24. Werner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. PAMI, IEEE Transactions on 32(8):1474–1488 25. Yanover C, Meltzer T, Weiss Y (2006) Linear Programming Relaxations and Belief Propagation–An Empirical Study. JMLR 7:1887–1907 26. Yedidia J, Freeman W, Weiss Y (2005) Constructing free-energy approximations and generalized belief propagationalgorithms. Information Theory, IEEE Transactions on 51(7):2282–2312\n1 Derivation of GDD Message updating\nFor convenience, we add several redundant constraints to reformulate (20) as\n   µ ∣∣∣∣∣∣∣∣ µt(xt) > 0,∀t ∈ T,xt∑ xt µt(xt) = 1,∀t ∈ T ∑ xc\\s µc(xc) = µs(xs),∀c ∈ C′, s ∈ S(c),xs    . (58)\nNow we keep the first two groups of constraints (thus not correspond to any Lagrangian multipliers), and introduce Lagrangian multipliers {λc→s(xs)|∀c ∈ C′, s ∈ S(c),xs} to the third group of the constraints. By standard Lagrangian duality, we have the dual objective below,\ng(λ) = max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n[∑\nc∈C\n∑\nxc\nµc(xc)θc(xc) + ∑\nc∈C′\n∑\ns∈S(c)\n∑\nxs\n( µs(xs)− ∑\nxc\\s\nµc(xc) ) λc→s(xs) ]\n= max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n[∑\nc∈C\n∑\nxc\nµc(xc)θc(xc) + ∑\nc∈C′\n∑\ns∈S(c)\\{c}\n∑\nxs\n( µs(xs)− ∑\nxc\\s\nµc(xc) ) λc→s(xs) ] . (59)\nHere the last equation holds because if c ∈ S(c) for some c ∈ C′, λc→c(xc) is cancelled out. Rearranging variables in (59), the dual objective of GDD becomes:\ng(λ) = max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n[∑\nc∈C\n∑\nxc\nµc(xc)θc(xc) + ∑\nc∈C′\n∑\ns∈S(c)\\{c}\n∑\nxs\nµs(xs)λc→s(xs)− ∑\nc∈C′\n∑\ns∈S(c)\\{c}\n∑\nxc\nµc(xc)λc→s(xs)\n]\n= max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n[∑\nc∈C\n∑\nxc\nµc(xc)θc(xc) + ∑\ns∈ [ ∪c′∈C′ ( S(c′)\\{c′}\n)] ∑\nc∈ { c′|c′∈C′,s∈S(c′)\\{c′}\n} ∑\nxs\nµs(xs)λc→s(xs)\n− ∑\nc∈C′\n∑\ns∈S(c)\\{c}\n∑\nxc\nµc(xc)λc→s(xs)\n] . (60)\nBy definition of θ̂t(xt), t ∈ T in (29a) and the fact that C ⊆ T, we have\n∀t ∈ T \\C, θ̂t(xt) = 1(t ∈ C)θt(xt) = 0,∀xt ∀t ∈ C, θ̂t(xt) = 1(t ∈ C)θt(xt) = θt(xt),∀xt (61)\nThus the first term in the most RHS of (60) can be reformulated as:\n∑\nc∈C\n∑\nxc\nµc(xc)θc(xc) = ∑\nc∈C\n∑\nxc\nµc(xc)θ̂c(xc) + ∑\nt∈T\\C\n∑\nxt\nµt(xt)θ̂t(xt)\n= ∑\nt∈T\n∑\nxt\nµt(xt)θ̂t(xt). (62)\nBy the definition of T in (28) it is easy to verify that:\n∀t ∈ T \\ [ ∪c′∈C′ ( S(c′) \\ {c′} )] ,@c ∈ C′, s.t. t ∈ S(c) \\ {c}. (63)\nThus we have ∑\nt∈T \\ [ ∪c′∈C′ ( S(c′)\\{c′}\n)] ∑\nc∈ { c′|c′∈C′,t∈S(c′)\\{c′}\n} ∑\nxt\nµt(xt)λc→t(xt) = 0. (64)\n1\nAs a result, the second term in the most RHS of (60) can be reformulated as:\n∑\ns∈ [ ∪c′∈C′ ( S(c′)\\{c′}\n)] ∑\nc∈ { c′|c′∈C′,s∈S(c′)\\{c′}\n} ∑\nxs\nµs(xs)λc→s(xs)\n= ∑\ns∈ [ ∪c′∈C′ ( S(c′)\\{c′}\n)] ∑\nc∈ { c′|c′∈C′,s∈S(c′)\\{c′}\n} ∑\nxs\nµs(xs)λc→s(xs)\n+ ∑\nt∈T \\ [ ∪c′∈C′ ( S(c′)\\{c′}\n)] ∑\nc∈ { c′|c′∈C′,t∈S(c′)\\{c′}\n} ∑\nxt\nµt(xt)λc→t(xt)\n= ∑\nt∈T\n∑\nc∈ { c′|c′∈C′,t∈S(c′)\\{c′}\n} ∑\nxt\nµt(xt)λc→t(xt). (65)\nFor the third term in the most RHS of (60), we simply reformulate it as:\n∑\nc∈C′\n∑\ns∈S(c)\\{c}\n∑\nxc\nµc(xc)λc→s(xs) = ∑\nt∈T\n∑\ns∈S(t)\\{t}\n∑\nxt\n1(t ∈ C′)µt(xt)λt→s(xs) (66)\nUsing (62), (65) and (66), we have\ng(λ) = max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n[∑\nt∈T\n∑\nxt\nµt(xt)θ̂t(xt) + ∑\nt∈T\n∑\nc∈ { c′|c′∈C′,t∈S(c′)\\{c′}\n} ∑\nxt\nµt(xt)λc→t(xt)\n− ∑\nt∈T\n∑\ns∈S(t)\\{t}\n∑\nxt\n1(t ∈ C′)µt(xt)λt→s(xs) ] .\n= max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n∑\nt∈T\n∑\nxt\nµt(xt) [ θ̂t(xt) +\n∑\nc∈ { c′|c′∈C′,t∈S(c′)\\{c′}\n}λc→t(xt)− 1(t ∈ C ′)\n∑\ns∈S(t)\\{t} λt→s(xs)\n]\n(67a)\n= max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n∑\nt∈T\n∑\nxt\nµt(xt) [ θ̂t(xt) + λt(xt)− γt(xt) ] (67b)\n= max ∀t ∈ T,xt,µt(xt) > 0,∑\nxt µt(xt) = 1\n∑\nt∈T\n∑\nxt\nµt(xt)bt(xt) (67c)\n= ∑\nt∈T max xt bt(xt). (67d)\nHere from (67a) to (67c) we use the definition of γt(xt), λt(xt) and bt(xt), t ∈ T in (29). From (67c) to (67d), as ∀t ∈ T,xt, µt(xt) > 0, ∑ xt µt(xt) = 1, the maximum can be attained by letting µt(x ∗ t ) = 1 for some x ∗ t ∈ argmaxxt bt(xt). When applying coordinate descent to optimise the above problem, we pick a particular c ∈ C ′ and then fix all λ except those λc→s(xs), s ∈ S(c). Recall the definition of T in (28), we can reformulate (28) as\nT = ∪ c′∈C′\n({c′} ∪ S(c′)) = [ ({c} ∪ ( S(c) \\ {c} )] ∪ (T \\({c} ∪ S(c))). (68)\n2\nThus by definition of bt(xt) in (29d), g(λ) can be decomposed to three parts as follows:\ng(λ) = ∑\nt∈T max xt\n[ θ̂t(xt)− γt(xt) + λt(xt) ]\n= max xc\n[ θ̂c(xc)− γc(xc) + λc(xc) ] + ∑\ns∈S(c)\\{c} max xs\n[ θ̂s(xs)− γs(xs) + λs(xs) ]\n+ ∑\nt∈T \\({c}∪S(c)) max xt\n[ θ̂t(xt)− γt(xt) + λt(xt) ]\n= max xc\n[ θ̂c(xc)− ∑\ns∈S(c)\\{c} λc→s(xs) + λc(xc)\n] (69a)\n+ ∑\ns∈S(c)\\{c} max xs\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) + λc→s(xs) ] (69b)\n+ ∑\nt∈T \\({c}∪S(c)) max xt\n[ θ̂t(xt)− γt(xt) + λt(xt) ] (69c)\nNote that only (69a) and (69b) depend on λc→s(xs), s ∈ S(c), thus minimising g(λ) over all λc→s(xs), s ∈ S(c) is equivalent to the sub-optimisation problem in (31).\nAn optimal solution of (31) is provided in the following proposition.\nProposition 1 ∀s ∈ S(c) \\ {c},xs, let\nλ∗c→s(xs) = −θ̂s(xs) + γs(xs)− λ−cs (xs)\n+ 1\n| S(c) \\ {c}| maxxc\\s\n[ θ̂c(xc) + λc(xc) + ∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ) )] ,\nthen λ∗c,S(c) = (λ ∗ c→s(xs))s∈S(c)\\{c} is a solution of (31).\nProof Considering the objective of (31), we have:\ngc(λc,S(c)) = max xc\n[ θ̂c(xc)− ∑\ns∈S(c)\\{c} λc→s(xs) + λc(xc)\n] (70a)\n+ ∑\ns∈S(c)\\{c} max xs\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) + λc→s(xs) ] (70b)\n>max xc\n{[ θ̂c(xc)− ∑\ns∈S(c)\\{c} λc→s(xs) + λc(xc)\n]\n+ ∑\ns∈S(c)\\{c}\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) + λc→s(xs)\n]}\n= max xc\n[ θ̂c(xc) + λc(xc) + ∑\ns∈S(c)\\{c}\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) ]] . (70c)\nClearly the RHS of (70c) is a lower bound of gc(λc,S(c)) for arbitrary λc,S(c). Now we show that lower bound is attained when λc,S(c) = λ ∗ c,S(c).\n3\nWhen λc→s(xs) = λ∗c→s(xs),∀xs for each s ∈ S(c) \\ {c}, we have\nθ̂s(xs)− γs(xs) + λ−cs (xs) + λ∗c→s(xs) =θ̂s(xs)− γs(xs) + λ−cs (xs)+{ − θ̂s(xs) + γs(xs)− λ−cs (xs)\n+ 1\n| S(c) \\ {c}| maxxc\\s\n[ θ̂c(xc) + λc(xc) + ∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ)\n)]}\n= 1\n| S(c) \\ {c}| maxxc\\s\n[ θ̂c(xc) + λc(xc) + ∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ) )] ,∀xs . (71)\nThus when λc,S(c) = λ ∗ c,S(c), (70b) becomes:\n∑\ns∈S(c)\\{c} max xs\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) + λ∗c→s(xs) ]\n= ∑\ns∈S(c)\\{c} max xs\n1\n| S(c) \\ {c}| maxxc\\s\n[ θ̂c(xc) + λc(xc) + ∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ)\n)]\n= max xc\n[ θ̂c(xc) + λc(xc) + ∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ) )] . (72)\nThus gc(λc,S(c)) becomes:\ngc(λ ∗ c,S(c)) = max\nxc\n[ θ̂c(xc)− ∑\ns∈S(c)\\{c} λ∗c→s(xs) + λc(xc)\n] + ∑\ns∈S(c)\\{c} max xs\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) + λ∗c→s(xs) ]\n= max xc\n[ θ̂c(xc)− ∑\ns∈S(c)\\{c} λ∗c→s(xs) + λc(xc)\n]\n+ max xc\n[ θ̂c(xc) + λc(xc) + ∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ) )] . (73)\nAs the RHS of (70c) is a lower bound of gc(λc,S(c)) for arbitrary λc,S(c), thus we must have\nmax xc\n[ θ̂c(xc)− ∑\ns∈S(c) λ∗c→s(xs) + λc(xc)\n] > 0, (74)\nwhich implies that the RHS of (70a) is non-negative.\n4\nNow we show that the RHS of (70a) is also non-positive.\nmax xc\n[ θ̂c(xc)− ∑\ns∈S(c)\\{c} λ∗c→s(xs) + λc(xc)\n]\n= max xc\n[ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c} λ∗c→s(xs)\n]\n= max xc\n{ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c}\n[ − θ̂s(xs) + γs(xs)− λ−cs (xs)\n+ 1\n| S(c) \\ {c}| maxxc\\s\n[ ∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ) ) + θ̂c(xc) + λc(xc)\n]]}\n= 1 | S(c) \\ {c}| maxxc ∑\ns∈S(c)\\{c}\n{ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c}\n[ − θ̂s(xs) + γs(xs)− λ−cs (xs) ]\n−max xc\\s\n[ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c}\n[ − θ̂s(xs) + γs(xs)− λ−cs (xs) ]]}\n6 1| S(c) \\ {c}| ∑\ns∈S(c)\\{c} max xc\n{ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c}\n[ − θ̂s(xs) + γs(xs)− λ−cs (xs) ]\n−max xc\\s\n[ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c}\n[ − θ̂s(xs) + γs(xs)− λ−cs (xs) ]]}\n= 1 | S(c) \\ {c}| ∑\ns∈S(c)\\{c} max xs\n{ max xc\\s [ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c}\n[ − θ̂s(xs) + γs(xs)− λ−cs (xs) ]]\n−max xc\\s\n[ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c}\n[ − θ̂s(xs) + γs(xs)− λ−cs (xs) ]]}\n=0. (75)\nThus using (74) and (75) we have\nmax xc\n[ θ̂c(xc)− ∑\ns∈S(c)\\{c} λ∗c→s(xs) + λc(xc)\n] = 0. (76)\nThus using (73) and (76), we have\ngc(λ ∗ c,S(c)) = max\nxc\n[ θ̂c(xc) + λc(xc) + ∑\ns∈S(c)\\{c}\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) ]] , (77)\nwhich implies by specifying λc,S(c) = λ ∗ c,S(c), gc(λ ∗ c,S(c)) achieves the lower bound shown in RHS of (70c).\n2 Dual Decrease in a single coordinate descent step\nProposition 2 (Dual Decrease) For any c ∈ C′, the dual decrease\nd(c) = max xc\nbc(xc) + ∑\ns∈S(c)\\{c} max xs bs(xs)\n−max xc\n[ bc(xc) + ∑\ns∈S(c)\\{c} bs(xs)\n] > 0.\n5\nProof Considering the sub-optimisation problem in (31), by definition of bt(xt), t ∈ T we have\nbc(xc) = θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c} λc→s(xs),∀xc, (78a)\nbs(xs) = θ̂s(xs)− γs(xs) + ∑\nĉ∈ { c′|c′∈C′,s∈S(c′)\\{c′}\n}λĉ→s(xs)\n= θ̂s(xs)− γs(xs) + ∑\nĉ∈ { c′|c′∈C′,c′ 6=c,s∈S(c′)\\{c′}\n}λĉ→s(xs) + λc→s(xs)\n= θ̂s(xs)− γs(xs) + λ−cs (xs) + λc→s(xs),∀s ∈ S(c) \\ {c},xs . (78b)\nWhen considering the sub-optimisation problem in (31), only λc→s(xs), s ∈ S(c) in (78) are flexible. Thus bc(xc) and bs(xs), s ∈ S(c) can be determined by λc,S(c), and the following equation always holds:\ngc(λc,S(c)\\{c}) = max xc\n[ θ̂c(xc)− ∑\ns∈S(c)\\{c} λc→s(xs) + λc(xc)\n] + ∑\ns∈S(c)\\{c} max xs\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) + λc→s(xs) ]\n= max xc\nbc(xc) + ∑\ns∈S(c)\\{c} max xs bs(xs). (79)\nNow we evaluate the optimal objective. By (77) and definition of bt(xt), t ∈ T in (29d) we have\ngc(λ ∗ c,S(c)) = max\nxc\n[ θ̂c(xc) + λc(xc) + ∑\ns∈S(c)\\{c}\n[ θ̂s(xs)− γs(xs) + λ−cs (xs)\n]]\n= max xc\n[ θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c} λc→s(xs) +\n∑\ns∈S(c)\\{c}\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) + λc→s(xs)\n]]\n= max xc\n[ bc(xc) + ∑\ns∈S(c)\\{c} bs(xs)\n] . (80)\nThus the dual decrease in a single coordinate descent step is\nd(c) =gc(λc,S(c))− gc(λ∗c,S(c))\n= max xc\nbc(xc) + ∑\ns∈S(c)\\{c} max xs bs(xs)−max xc\n[ bc(xc) + ∑\ns∈S(c)\\{c} bs(xs)\n]\n>0. (81)\n3 Proof of Proposition 5\nProposition 5 If there exists x that maximises bt(xt),∀t ∈ T, the solution of GDD is exact.\nProof Under the above assumptions, we have\ng(λ) = ∑\nt∈T max xc bc(xc) = max x\n∑ t∈T bt(xt)\n= max x\n[∑\nt∈T θ̂t(xt) +\n∑ t∈T λt(xt)− ∑ t∈T γt(xt) ]\n= max x\n[∑\nt∈T 1(t ∈ C)θt(xt) +\n∑\nt∈T\n∑\nc∈{c′|c′∈C′,t∈S(c′)\\{c′}} λc→t(xt)−\n∑ t∈T 1(t ∈ C′) ∑ ŝ∈S(t)\\{t} λt→ŝ(xŝ) ] .\n6\nAs it is easy to verify that\n∑\nt∈T\n∑\nc∈{c′|c′∈C′,t∈S(c′)\\{c′}} λc→t(xt) =\n∑\nc∈∪t∈T{c′|c′∈C′,t∈S(c′)\\{c′}}\n∑\ns∈S(c)\\{c} λc→s(xs)\n= ∑\nc∈C′\n∑\ns∈S(c)\\{c} λc→s(xs)\n= ∑\nt∈T 1(t ∈ C′)\n∑\nŝ∈S(t)\\{t} λt→ŝ(xŝ).\nThus we have\ng(λ) = max x\n[∑\nt∈T 1(t ∈ C)θt(xt)\n] = max\nx\n∑ c∈C θc(xc),\nwhich completes the proof.\n4 Derivation of Belief Propagation Without Messages\nProposition 6 When optimising (31), the beliefs b∗c,S(c) can be computed from a bc,S(c) determined by arbitrary λc,S(c) as following:\nb∗s(xs) = max xc\\s\n[ bc(xc) + ∑ ŝ∈S(c)\\{c} bŝ(xŝ) ]\n| S(c) \\ {c}| ,∀s ∈ S(c) \\ {c},xs\nb∗c(xc) =bc(xc) + ∑\nŝ∈S(c)\\{c} bŝ(xŝ)−\n∑\nŝ∈S(c)\\{c} b∗ŝ(xŝ),∀xc .\nProof By (78) we have:\nb∗c(xc) = θ̂c(xc) + λc(xc)− ∑\ns∈S(c)\\{c} λ∗c→s(xs),∀xc . (82a)\nb∗s(xs) = θ̂s(xs)− γs(xs) + λ−cs (xs) + λ∗c→s(xs), s ∈ S(c) \\ {c},xs (82b)\nAccording to (82b) and (78b), we have:\nb∗s(xs)− bs(xs) = λ∗c→s(xs)− λc→s(xs),∀s ∈ S(c) \\ {c},xs . (83)\nAccording to (82a) and (78a), we have:\nb∗c(xc)− bc(xc) = ∑\ns∈S(c)\\{c} λc→s(xs)−\n∑\ns∈S(c)\\{c} λ∗c→s(xs),∀xc . (84)\nRearranging (78b) yields:\nbs(xs)− λc→s(xs) = θ̂s(xs)− γs(xs) + λ−cs (xs),∀s ∈ S(c) \\ {c},xs . (85)\nBy (78a) and (78b), we have: bc(xc) + ∑\ns∈S(c)\\{c} bs(xs) =θ̂c(xc) + λc(xc)−\n∑\ns∈S(c)\\{c} λc→s(xs) +\n∑\ns∈S(c)\\{c}\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) + λc→s(xs) ]\n=θ̂c(xc) + λc(xc) + ∑\ns∈S(c)\\{c}\n[ θ̂s(xs)− γs(xs) + λ−cs (xs) ] ,∀xc . (86)\n7\nUsing (85) and (86), we can reformulate (33) to:\nλ∗c→s(xs) =− θ̂s(xs) + γs(xs)− λ−cs (xs) + 1\n| S(c) \\ {c}| maxxc\\s\n[ ∑\nŝ∈S(c)\\{c}\n( θ̂ŝ(xŝ)− γŝ(xŝ) + λ−cŝ (xŝ) ) + θ̂c(xc) + λc(xc)\n]\n=− ( bs(xs)− λc→s(xs) ) +\n1\n| S(c) \\ {c}| maxxc\\s\n[ bc(xc) + ∑\nŝ∈S(c)\\{c} bŝ(xŝ)\n] , ∀s ∈ S(c) \\ {c},xs . (87)\nReplacing λ∗c→s(xs) in (83) with the most RHS of (87) results in:\nb∗s(xs) = bs(xs) + λ ∗ c→s(xs)− λc→s(xs)\n= bs(xs)− ( bs(xs)− λc→s(xs) ) +\n1\n| S(c) \\ {c}| maxxc\\s\n[ bc(xc) + ∑\nŝ∈S(c)\\{c} bŝ(xŝ)\n] − λc→s(xs)\n= 1\n| S(c) \\ {c}| maxxc\\s\n[ bc(xc) + ∑\nŝ∈S(c)\\{c} bŝ(xŝ)\n] ,∀s ∈ S(c) \\ {c},xs, (88)\nand by reformulating (84) using (83) we get\nb∗c(xc) =bc(xc) + ∑\nŝ∈S(c)\\{c} λc→ŝ(xŝ)−\n∑\nŝ∈S(c)\\{c} λ∗c→ŝ(xŝ)\n=bc(xc) + ∑\nŝ∈S(c)\\{c} bŝ(xŝ)−\n∑\nŝ∈S(c)\\{c} b∗ŝ(xŝ),∀xc . (89)\nTogether with (88) and (89) we finished the proof.\n5 Proof of Proposition 7\nProposition 9 Given a graph G = (V,C) and a marginal polytope diagram GM = (VM ,EM ) of G, we have\n1. ∀c, s, t ∈ VM , t ⊂ s ⊂ c, if (c→ s) ∈ EM , then (c→ t)⇔ (s→ t); 2. If (c→ s1), (c→ s2) ∈ EM , then ∀t ∈ VM , t ⊂ s1, t ⊂ s2, (s1 → t)⇔ (s2 → t).\nProof To show the first case, we let Ua be\nUa = {∑\nxĉ\\ŝ\nµĉ(xĉ) = µŝ(xŝ), ∀xŝ |(ĉ→ ŝ) ∈ (EM \\{(c̄→ t)|c̄ ∈ VM}) } . (90)\nBy the fact that (c → s) ∈ EM , we know that (c → s) ∈ EM \\{(c̄ → t)|c̄ ∈ VM}. Thus for arbitrary µ̄ ∈ {µ |Ua}, constraints\n∑\nxc\\s\nµ̄c(xc) = µ̄s(xs),∀xs (91)\nmust be satisfied. Thus if a µ̄ ∈ {µ |Ua} also satisfies ∑\nxc\\t µ̄c(xc) = µ̄t(xt),∀xt, we must have\n∑\nxs\\t\nµ̄s(xs) = ∑\nxs\\t\n∑\nxc\\s\nµ̄c(xc) = ∑\nxc\\t\nµ̄c(xc) = µ̄t(xt),∀xt, (92)\nwhich implies such a µ̄ also satisfies ∑\nxs\\t µ̄s(xs) = µ̄t(xt),∀xt.\nOn the other hand, if a µ̄ ∈ {µ |Ua} also satisfies ∑\nxs\\t µ̄s(xs) = µ̄t(xt),∀xt, we must have\n∑\nxc\\t\nµ̄c(xc) = ∑\nxs\\t\n∑\nxc\\s\nµ̄c(xc) = ∑\nxs\\t\nµ̄s(xs) = µ̄t(xt),∀xt, (93)\n8\nwhich implies such a µ̄ also satisfies ∑\nxc\\t µ̄c(xc) = µ̄t(xt),∀xt. Thus we have\n{ µ |Ua ∪{ ∑\nxc\\t\nµc(xc) = µt(xt),∀xt} } = { µ |Ua ∪{ ∑\nxs\\t\nµs(xs) = µt(xt),∀xt} } ,\nwhich shows that edges (c→ t) and (s→ t) are equivalent by definition. For the second case, we define\nUb = {∑\nxĉ\\ŝ\nµĉ(xĉ) = µŝ(xŝ), ∀xŝ |(ĉ→ ŝ) ∈ (EM \\{(c̄→ t)|c ∈ VM}) } . (94)\nBy the fact that (c → s1), (c → s2) ∈ EM , we must have (c → s1), (c → s2) ∈ EM \\{(c̄ → t)|c ∈ VM}. Thus for arbitrary µ̄ ∈ {µ|Ub}, constraints\n∑\nxc\\s1\nµ̄c(xc) = µ̄s1(xs1), ∑\nxc\\s2\nµ̄c(xc) = µ̄s2(xs2),∀xs1 ,xs2\nmust be satisfied. Thus if a µ̄ ∈ {µ |Ub} also satisfies ∑\nxs1\\t µ̄s1(xs1) = µ̄t(xt),∀xt, we have\n∑\nxs2\\t\nµ̄s2(xs2) = ∑\nxs2\\t\n∑\nxc\\s2\nµ̄c(xc) = ∑\nxc\\t\nµ̄c(xc) = ∑\nxc\\s1\n∑\nxs1\\t\nµ̄c(xc) = µ̄t(xt),∀xt, (95)\nwhich implies such µ̄ also satisfies ∑\nxs2\\t µ̄s2(xs2) = µ̄t(xt).\nOn the other hand, if a µ̄ ∈ {µ |Ub} also satisfies ∑\nxs2\\t µ̄s2(xs2) = µ̄t(xt), we have\n∑\nxs1\\t\nµ̄s1(xs1) = ∑\nxs1\\t\n∑\nxc\\s1\nµ̄c(xc) = ∑\nxc\\t\nµ̄c(xc) = ∑\nxs2\\t\n∑\nxc\\s2\nµ̄c(xc) = µ̄t(xt), (96)\nwhich implies such µ̄ also satisfies ∑\nxs1\\t µ̄s1(xs1) = µ̄t(xt). Thus we have\n{ µ |Ub ∪{ ∑\nxs1\\t\nµs1(xs1) = µt(xt),∀xt} } = { µ |Ub ∪{ ∑\nxs2\\t\nµs2(xs2) = µt(xt),∀xt} } ,\nwhich shows that (s1 → t) and (s2 → t) are equivalent by definition.\n6 Proof of Proposition 8\nProposition 10 For arbitrary G = (V,C), and marginal polytope diagram GM = (VM ,EM ) of G, edge equivalence w.r.t. GM is an equivalence relation.\nProof By definition, reflexivity and symmetry naively holds. Thus we only prove the transitivity by proving the claim that ∀c1, c2, c3, t ∈ VM , t ⊆ c1, t ⊆ c2, t ⊆ c3, if (c1 → t)⇔ (c2 → t) and (c2 → t)⇔ (c3 → t) are true, then (c1 → t)⇔ (c3 → t) must be true.\nNow we prove the claim. Let U be\nU = {∑\nxc\\s\nµc(xc) = µs(xs),∀(c→ s) ∈ (EM \\{(ĉ→ t)|ĉ ∈ VM , t ⊆ ĉ}),xs } .\nThen by definition of edge equivalence, we must have { µ |U∪{ ∑\nxc1\\t\nµc1(xc1) = µt(xt),∀xt} } = { µ |U∪{ ∑\nxc2\\t\nµc2(xc2) = µt(xt),∀xt} } ,\n{ µ |U∪{ ∑\nxc2\\t\nµc2(xc2) = µt(xt),∀xt} } = { µ |U∪{ ∑\nxc3\\t\nµc3(xc3) = µt(xt),∀xt} } , (97)\n9\nwhich implies that\n{ µ |U∪{ ∑\nxc1\\t\nµc1(xc1) = µt(xt),∀xt} } = { µ |U∪{ ∑\nxc3\\t\nµc3(xc3) = µt(xt).∀xt} } . (98)\nThus we must have (c1 → t)⇔ (c3 → t) by definition. Over all, edge equivalence w.r.t. GM is an equivalence relation.\n7 Proof of Proposition 11\nProposition 15 All v ∈ VM0 \\(C∪ Im) are redundant nodes w.r.t. GM0 .\nProof Since C ⊆ (C∪ Im), for any v ∈ VM0 \\(C∪ Im), we have v ∈ VM0 \\C. Now we prove the proposition by proving that all edges to v are equivalent. Let Pv = {p|(p → v) ∈ EM0 }. By definition of Cm, we have ∀p1, p2 ∈ Pv, ∃c1, c2 ∈ Cm, s.t. p1 ⊆ c1, p2 ⊆ c2. Thus let s = c1 ∩ c2, by the fact that v ⊆ p1 ⊆ c1 and v ⊆ p2 ⊆ c2, we must have v ⊆ s. Moreover, if v = s we must have v = c1 ∩ c2 ∈ Im, which contradict to the fact that v ∈ VM \\(C∪ Im). Thus we must have v ⊂ s. By the fact s = c1 ∩ c2, we have s ⊆ ci, i ∈ {1, 2}. Thus ∀i ∈ {1, 2}, if pi = ci = s, (pi → v) ⇔ (s → v) naively holds; if only one of pi and s is equal to ci, we have (pi → v) ⇔ (s → v) by Proposition 7 (the first case); if both pi and s are not equal to ci, by Proposition 7 (the second case) we have (pi → v) ⇔ (s → v). Thus by transitivity we have all (p→ v), p ∈ Pv are equivalent, which implies that v is redundant node w.r.t. GM0 by Proposition 9.\n8 Experiment\nWe present more experiment here.\n8.1 Results on Synthetic Data\nAdditional results on the synthetic data for the convergence in terms of both running time and the number of iterations are provided in Figure 1, which consistently shows faster convergence of the proposed methods.\n8.2 Results on PPI dataset\nAdditional results on PPI dataset for the convergence in terms of both running time and the number of iterations are provided in Figure 2.\n10\n8.3 Image segmentation\nHere the node potentials θi(xi), i ∈ V are computed according to (5) in Kohli et al [2009] as follows,\nθi(xi) = θTϕT (xi) + θcolϕcol(xi) + θlϕl(xi). (99)\nWe choose θT = 0, θcol = 1 and θl = 0, thus we have θi(xi) = ϕcol(xi). We learn ϕcol(xi) from the data using Gaussian Mixture Models as in Blake et al [2004]. We follow (12) and (10) in Kohli et al [2009] to compute edge potentials and high order potentials (with θα = 0, θ h p = 0, θ h v = 25 and θ h β being set to the reciprocal of variance of all pixels i.e. grey value in [0, 255]). Additional results are provided in Figures 3 and 4. We can see that the proposed methods, PS, π-S and MI, not only find the exact solution on all three problems, but also converge much faster than others.\n11\n12\n8.4 Image Matching\nWe first detect key points from source images and destination images by using SIFT detectors implemented in OpenCV with default parameters. As in Li et al [2010], for every p(i), i ∈ V, if its nearest 3 neighbours p(i1), p(i2), p(i3) are not in a line, there must exist a column vector W ′ f in R3 s.t.\np(i) = [p(i1), p(i2), p(i3)]W ′ f . (100)\nThen let Wf = [W ′ f ,−1] we must have\n[p(i1), p(i2), p(i3), p(i)]Wf = 0, (101)\nand the equation is invariant to affine transformation Li et al [2010]. Thus for every p(i), i ∈ V, if its nearest 3 nearest neighbours p(i1), p(i2), p(i3) are not in a line, there is an order-4 cluster f = {i1, i2, i3, i}. Let Pxf = [q(xi1), q(xi2), q(xi3), q(xi)]. ||PxfWf ||1 can be used as a geometry cost.\nAdditional results are provided in the following figures. Although all algorithms achieves exact solutions on all data sets, the proposed methods often show better convergence rate in terms of both iterations and running time. The plot of dataset ubc45 is excluded since all algorithms achieve the exact solution at the first iteration.\n13\n14\n15\n16\n8.5 Comparing Belief Propagation and Message Passing\nOur GDD based algorithms can be implemented as either a message passing (MP) procedure or a belief propagation procedure without messages. We implement both, and observe that both have similar speed as shown in Figure 15. Of course, the latter uses less storage.\n17\nReferences\nBatra D, Nowozin S, Kohli P (2011) Tighter relaxations for map-mrf inference: A local primal-dual gap based separation algorithm. In: International Conference on Artificial Intelligence and Statistics, pp 146–154\nBlake A, Rother C, Brown M, Perez P, Torr P (2004) Interactive image segmentation using an adaptive gmmrf model. In: Computer Vision-ECCV 2004, Springer, pp 428–441\nGloberson A, Jaakkola T (2007) Fixing max-product: Convergent message passing algorithms for MAP LPrelaxations. In: NIPS, vol 21\nHazan T, Shashua A (2010) Norm-product belief propagation: Primal-dual message-passing for approximate inference. Information Theory, IEEE Transactions on 56(12):6294–6316\nKallenberg O (2002) Foundations of modern probability. Springer Verlag\nKohli P, Ladickỳ L, Torr PH (2009) Robust higher order potentials for enforcing label consistency. IJCV 82(3):302– 324\nKoller D, Friedman N (2009) Probabilistic graphical models: principles and techniques. MIT press\nKolmogorov V, Schoenemann T (2012) Generalized sequential tree-reweighted message passing. arXiv preprint arXiv:12056352\n18\nKomodakis N, Paragios N (2008) Beyond loose lp-relaxations: Optimizing mrfs by repairing cycles. In: Computer Vision–ECCV 2008, Springer, pp 806–820\nKomodakis N, Paragios N, Tziritas G (2007) MRF optimization via dual decomposition: Message-passing revisited. In: ICCV, IEEE, pp 1–8\nKovalevsky V, Koval V (1975) A diffusion algorithm for decreasing energy of max-sum labeling problem. Glushkov Institute of Cybernetics, Kiev, USSR\nKumar MP, Kolmogorov V, Torr PH (2009) An analysis of convex relaxations for MAP estimation of discrete MRFs. The Journal of Machine Learning Research 10:71–106\nLi H, Kim E, Huang X, He L (2010) Object matching with a locally affine-invariant constraint. In: CVPR, IEEE, pp 1641–1648\nLowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157\nMcEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299\nMeshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031\nPakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873\nSchwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS\nSontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using messagepassing. In: UAI, AUAI Press, pp 503–510\nSontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press\nSontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795–804\nWainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R© in Machine Learning 1(1-2):1–305\nWerner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1–8\nWerner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. PAMI, IEEE Transactions on 32(8):1474–1488\nYanover C, Meltzer T, Weiss Y (2006) Linear Programming Relaxations and Belief Propagation–An Empirical Study. JMLR 7:1887–1907\nYedidia J, Freeman W, Weiss Y (2005) Constructing free-energy approximations and generalized belief propagationalgorithms. Information Theory, IEEE Transactions on 51(7):2282–2312\n19"
    } ],
    "references" : [ {
      "title" : "Tighter relaxations for map-mrf inference: A local primal-dual gap based",
      "author" : [ "D Batra", "S Nowozin", "P Kohli" ],
      "venue" : null,
      "citeRegEx" : "Batra et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Batra et al\\.",
      "year" : 2011
    }, {
      "title" : "Interactive image segmentation using an adaptive gmmrf",
      "author" : [ "A Blake", "C Rother", "M Brown", "P Perez", "P Torr" ],
      "venue" : null,
      "citeRegEx" : "Blake et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Blake et al\\.",
      "year" : 2004
    }, {
      "title" : "Fixing max-product: Convergent message passing algorithms for MAP LP",
      "author" : [ "A Globerson", "T Jaakkola" ],
      "venue" : null,
      "citeRegEx" : "Globerson and Jaakkola,? \\Q2007\\E",
      "shortCiteRegEx" : "Globerson and Jaakkola",
      "year" : 2007
    }, {
      "title" : "A (2010) Norm-product belief propagation: Primal-dual message-passing for approximate infer",
      "author" : [ "Hazan T", "Shashua" ],
      "venue" : null,
      "citeRegEx" : "T and Shashua,? \\Q2010\\E",
      "shortCiteRegEx" : "T and Shashua",
      "year" : 2010
    }, {
      "title" : "Foundations of modern probability",
      "author" : [ "O Kallenberg" ],
      "venue" : null,
      "citeRegEx" : "Kallenberg,? \\Q2002\\E",
      "shortCiteRegEx" : "Kallenberg",
      "year" : 2002
    }, {
      "title" : "Robust higher order potentials for enforcing label consistency",
      "author" : [ "P Kohli", "L Ladickỳ", "PH Torr" ],
      "venue" : null,
      "citeRegEx" : "Kohli et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kohli et al\\.",
      "year" : 2009
    }, {
      "title" : "Probabilistic graphical models: principles and techniques",
      "author" : [ "D Koller", "N Friedman" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedman,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "Generalized sequential tree-reweighted message passing",
      "author" : [ "V Kolmogorov", "T Schoenemann" ],
      "venue" : null,
      "citeRegEx" : "Kolmogorov and Schoenemann,? \\Q2012\\E",
      "shortCiteRegEx" : "Kolmogorov and Schoenemann",
      "year" : 2012
    }, {
      "title" : "Beyond loose lp-relaxations: Optimizing mrfs by repairing cycles",
      "author" : [ "N Komodakis", "N Paragios" ],
      "venue" : "Vision–ECCV",
      "citeRegEx" : "Komodakis and Paragios,? \\Q2008\\E",
      "shortCiteRegEx" : "Komodakis and Paragios",
      "year" : 2008
    }, {
      "title" : "MRF optimization via dual decomposition: Message-passing revisited",
      "author" : [ "N Komodakis", "N Paragios", "G Tziritas" ],
      "venue" : "In: ICCV,",
      "citeRegEx" : "Komodakis et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Komodakis et al\\.",
      "year" : 2007
    }, {
      "title" : "A diffusion algorithm for decreasing energy of max-sum labeling problem",
      "author" : [ "V Kovalevsky", "V Koval" ],
      "venue" : "Glushkov Institute of Cybernetics,",
      "citeRegEx" : "Kovalevsky and Koval,? \\Q1975\\E",
      "shortCiteRegEx" : "Kovalevsky and Koval",
      "year" : 1975
    }, {
      "title" : "An analysis of convex relaxations for MAP estimation of discrete MRFs",
      "author" : [ "MP Kumar", "V Kolmogorov", "PH Torr" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "Kumar et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2009
    }, {
      "title" : "Object matching with a locally affine-invariant constraint",
      "author" : [ "H Li", "E Kim", "X Huang", "L He" ],
      "venue" : "In: CVPR,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Object recognition from local scale-invariant features",
      "author" : [ "DG Lowe" ],
      "venue" : "ICCV 1999, IEEE,",
      "citeRegEx" : "Lowe,? \\Q1999\\E",
      "shortCiteRegEx" : "Lowe",
      "year" : 1999
    }, {
      "title" : "Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance",
      "author" : [ "RJ McEliece", "M Yildirim" ],
      "venue" : null,
      "citeRegEx" : "McEliece and Yildirim,? \\Q2003\\E",
      "shortCiteRegEx" : "McEliece and Yildirim",
      "year" : 2003
    }, {
      "title" : "Convergence rate analysis of map coordinate minimization algorithms",
      "author" : [ "O Meshi", "T Jaakkola", "A Globerson" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Meshi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Meshi et al\\.",
      "year" : 2012
    }, {
      "title" : "Estimation and marginalization using the kikuchi approximation methods",
      "author" : [ "P Pakzad", "V Anantharam" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "Pakzad and Anantharam,? \\Q2005\\E",
      "shortCiteRegEx" : "Pakzad and Anantharam",
      "year" : 2005
    }, {
      "title" : "Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins",
      "author" : [ "AG Schwing", "T Hazan", "M Pollefeys", "R Urtasun" ],
      "venue" : null,
      "citeRegEx" : "Schwing et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schwing et al\\.",
      "year" : 2012
    }, {
      "title" : "Tightening LP relaxations for MAP using messagepassing",
      "author" : [ "D Sontag", "T Meltzer", "A Globerson", "Y Weiss", "T Jaakkola" ],
      "venue" : null,
      "citeRegEx" : "Sontag et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sontag et al\\.",
      "year" : 2008
    }, {
      "title" : "Introduction to dual decomposition for inference",
      "author" : [ "D Sontag", "A Globerson", "T Jaakkola" ],
      "venue" : "SJ (eds) Optimization for Machine Learning,",
      "citeRegEx" : "Sontag et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sontag et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficiently Searching for Frustrated Cycles in MAP Inference",
      "author" : [ "D Sontag", "DK Choe", "Y Li" ],
      "venue" : null,
      "citeRegEx" : "Sontag et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sontag et al\\.",
      "year" : 2012
    }, {
      "title" : "Graphical models, exponential families, and variational inference. Foundations and Trends R",
      "author" : [ "MJ Wainwright", "MI Jordan" ],
      "venue" : null,
      "citeRegEx" : "Wainwright and Jordan,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright and Jordan",
      "year" : 2008
    }, {
      "title" : "Linear Programming Relaxations and Belief Propagation–An",
      "author" : [ "C Yanover", "T Meltzer", "Y Weiss" ],
      "venue" : null,
      "citeRegEx" : "Yanover et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Yanover et al\\.",
      "year" : 2006
    }, {
      "title" : "Constructing free-energy approximations and generalized belief propaga",
      "author" : [ "W Freeman", "Y Weiss" ],
      "venue" : null,
      "citeRegEx" : "Yedidia et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Yedidia et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Kallenberg O (2002) Foundations of modern probability.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets.",
      "startOffset" : 0,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms.",
      "startOffset" : 0,
      "endOffset" : 344
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods.",
      "startOffset" : 0,
      "endOffset" : 518
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins.",
      "startOffset" : 0,
      "endOffset" : 680
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing.",
      "startOffset" : 0,
      "endOffset" : 840
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503–510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference.",
      "startOffset" : 0,
      "endOffset" : 974
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503–510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference.",
      "startOffset" : 0,
      "endOffset" : 1142
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503–510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795–804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference.",
      "startOffset" : 0,
      "endOffset" : 1272
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503–510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795–804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R © in Machine Learning 1(1-2):1–305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf).",
      "startOffset" : 0,
      "endOffset" : 1419
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503–510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795–804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R © in Machine Learning 1(1-2):1–305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1–8 24. Werner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction.",
      "startOffset" : 0,
      "endOffset" : 1607
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503–510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795–804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R © in Machine Learning 1(1-2):1–305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1–8 24. Werner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. PAMI, IEEE Transactions on 32(8):1474–1488 25. Yanover C, Meltzer T, Weiss Y (2006) Linear Programming Relaxations and Belief Propagation–An Empirical Study.",
      "startOffset" : 0,
      "endOffset" : 1812
    }, {
      "referenceID" : 13,
      "context" : "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150–1157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275–299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023–3031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836–1873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503–510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795–804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R © in Machine Learning 1(1-2):1–305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1–8 24. Werner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. PAMI, IEEE Transactions on 32(8):1474–1488 25. Yanover C, Meltzer T, Weiss Y (2006) Linear Programming Relaxations and Belief Propagation–An Empirical Study. JMLR 7:1887–1907 26. Yedidia J, Freeman W, Weiss Y (2005) Constructing free-energy approximations and generalized belief propagationalgorithms.",
      "startOffset" : 0,
      "endOffset" : 1944
    } ],
    "year" : 2014,
    "abstractText" : "LP relaxation-based message passing algorithms provide an effective tool for MAP inference over Probabilistic Graphical Models. However, different LP relaxations often have different objective functions and variables of differing dimensions, which presents a barrier to effective comparison and analysis. In addition, the computational complexity of LP relaxation-based methods grows quickly with the number of constraints. Reducing the number of constraints without sacrificing the quality of the solutions is thus desirable. We propose a unified formulation under which existing MAP LP relaxations may be compared and analysed. Furthermore, we propose a new tool called Marginal Polytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited such as node redundancy and edge equivalence. We show that using Marginal Polytope Diagrams allows the number of constraints to be reduced without loosening the LP relaxations. Then, using Marginal Polytope Diagrams and constraint reduction, we develop three novel message passing algorithms, and demonstrate that two of these show a significant improvement in speed over state-of-art algorithms while delivering a competitive, and sometimes higher, quality of solution.",
    "creator" : "LaTeX with hyperref package"
  }
}