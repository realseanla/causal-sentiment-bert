{
  "name" : "1604.00923.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning",
    "authors" : [ "Philip S. Thomas", "Emma Brunskill" ],
    "emails" : [ "PHILIPT@CS.CMU.EDU", "EBRUN@CS.CMU.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The ability to predict the performance of a policy without actually having to use it is crucial to the responsible use of reinforcement learning algorithms. Consider the setting where the user of a reinforcement learning algorithm has already deployed some policy, e.g., for determining which advertisement to show a user visiting a website (Theocharous et al., 2015), for determining which medical treatment to suggest for a patient (Thapa et al., 2005), or for suggesting a personalized curriculum for a student (Mandel et al., 2014). In these examples, using a bad policy can be costly or dangerous, so it is important that the user of a reinforcement learning algorithm be able to accurately predict how well a new policy will perform without having to deploy it.\nIn this paper we propose a new algorithm for tackling this performance prediction problem, which is called the offpolicy policy evaluation (OPE) problem. The primary objective in OPE problems is to produce estimates that minimize some notion of error. We select mean squared error, a\npopular notion of error for estimators, as our loss function. This is in line with previous works that all use (root) mean squared error when empirically validating their methods (Precup et al., 2000; Dudı́k et al., 2011; Mahmood et al., 2014; Thomas, 2015b; Jiang & Li, 2015).\nGiven this goal, an estimator should be strongly consistent—its mean squared error should converge almost surely to zero as the amount of available data increases.1 In this paper we introduce a new strongly consistent estimator, MAGIC, that directly optimizes mean squared error. Our empirical results show that MAGIC can produce estimates with orders of magnitude lower mean squared error than the estimates produced by existing algorithms.\nOur new algorithm comes from the synthesis of two new contributions. The first contribution is an extension of the recently proposed doubly robust (DR) OPE algorithm (Jiang & Li, 2015). We present a novel derivation of their algorithm that removes the assumption that the horizon is finite and known. We also give conditions under which the DR estimator is strongly consistent. We then show how we can significantly reduce the variance of the DR estimator by introducing a small amount of bias—an effective tradeoff when attempting to minimize the mean squared error of the estimates. We call our extension of the DR estimator the weighted doubly robust (WDR) estimator.\nOur second major contribution is a new estimator, which we call the blending IS and model (BIM) estimator, that combines two different OPE estimators not just by selecting between them, but by blending them together in a way that minimizes the mean squared error. The combination of these two contributions results in a particularly powerful new OPE algorithm that we call the model and guided importance sampling combining (MAGIC) estimator, which uses BIM to combine a purely model-based estimator with WDR. In our simulations, MAGIC has the best general per-\n1A formal definition of what it means for an estimator to be strongly consistent is provided in Appendix A, where, in Lemma 3, we also elucidate the relationship between strong consistency and mean squared error.\nar X\niv :1\n60 4.\n00 92\n3v 1\n[ cs\n.L G\n] 4\nA pr\n2 01\nformance, often exhibiting orders of magnitude lower mean squared error than prior state-of-the-art estimators."
    }, {
      "heading" : "2. Notation",
      "text" : "We assume that the reader is familiar with reinforcement learning (Sutton & Barto, 1998) and adopt notational standard MDPNv1 for Markov decision processes (Thomas, 2015a, MDPs). For simplicity, our notation assumes that the state, action, and reward sets are finite, although our results carry over to more general settings.2 Let H := (S0, A0, R0, S1, . . . ) be a trajectory,3\nand g(H) := ∑∞\nt=0 γ tRt denote the return of a trajec-\ntory. We assume that the (possibly unknown) minimum and maximum rewards, rmin and rmax, are finite and that γ ∈ [0, 1] for the finite-horizon setting and γ ∈ [0, 1) for the indefinite and infinite horizon settings so that g(H) is bounded. We use the discounted objective function, v(π) := E[g(H)|H ∼ π], where H ∼ π denotes that H was generated using the policy π. When dealing with multiple trajectories, we use superscripts to denote which trajectory a term comes from. For example, we write SHt to denote the state at time t during trajectory H . Let vπ and qπ be the state value function and state-action value function for policy π—for all (π, s, a) ∈ Π × S × A, let vπ(s) := E [ ∑∞ t=0 γ\ntRt|S0 = s, π] and qπ(s, a) := E [ ∑∞\nt=0 γ tRt|S0 = s,A0 = a, π]. Notice that v without\na superscript denotes the objective function, while vπ denotes a value function.\nWe will assume that historical data, D, is provided. Formally this historical data is a set of n ∈ N>0 trajectories and the known policies, called behavior policies, that were used to generate them. That is, D := {(Hi, πi)}ni=1, where Hi ∼ πi. Importantly, when we write Hi, we always mean that Hi ∼ πi. Let ρt(H,πe, πb) := ∏t i=0 πe ( AHi ∣∣SHi ) /πb ( AHi ∣∣SHi ) , be an importance weight, which is the probability of the first t steps of H under the evaluation policy, πe, divided by its probability under the behavior policy πb (Precup et al., 2000, Section 2). For brevity, we write ρit as shorthand for ρt(Hi, πe, πi) and ρt as shorthand for ρt(H,πe, πb). To\n2We have verified that our results carry over to the setting where the states, actions, and rewards are continuous random variables with density functions. This result is relatively straightforward—summations are replaced with integrals, probability mass functions with probability density functions, and probabilities with probability densities, where appropriate. We have included Assumption 4, which is implied when the setting is restricted to finite state, action, and reward sets, since it is necessary for the continuous setting. We have not verified our results for the setting where the states, actions, and rewards come from distributions that do not have probability mass or density functions, e.g., if the rewards come from the Cantor distribution.\n3For alliteration, one might think ofH as denoting the history of an episode.\nsimplify later expressions, let ρi−1 := 1 for all i. One of the primary challenges will be to combat the high variance and large range of the importance weights, ρt.\nSome of the methods that we describe use an approximate model of an MDP. Let r̂π(s, a, t) denote the model’s prediction of the reward t steps later, Rt, if S0 = s, A0 = a, and the policy π is used to generate the subsequent actions, A1, A2, . . . . For example, r̂π(s, a, 0) is a prediction of the immediate reward after taking action a in state s and is thus the same for all policies, π. We assume that these predictions are bounded by finite (possibly unknown) constants rmodelmin and r model max , i.e., r̂ π(s, a, t) ∈ [rmodelmin , rmodelmax ]. Let\nr̂π(s, t) := ∑\na∈A π(a|s)r̂π(s, a, t), (1)\nbe a prediction of Rt if S0 = s and the policy π is used to generate actions A0, A1, . . . , for all (s, t, π) ∈ S × N≥0 × Π. Let v̂π(s) := ∑∞ t=0 γ\ntr̂π(s, t) and q̂π(s, a) :=∑∞ t=0 γ\ntr̂π(s, a, t) be the model’s estimates of vπ(s) and qπ(s, a). We assume that if a terminal absorbing state, ∞ s , is reached, the model’s predictions of rewards that occur thereafter are always zero: r̂π( ∞ s , a, t) = 0 for all (π, a, t) ∈ Π × A × N≥0. Although better models will tend to improve our estimates, we make no assumptions about the veracity of the approximate model’s predictions, r̂π(s, a, t)."
    }, {
      "heading" : "3. Off-Policy Policy Evaluation (OPE)",
      "text" : "The problem of off-policy policy evaluation (OPE) is defined as follows. We are given an evaluation policy, πe, historical data, D, and an approximate model. Our goal is to produce an estimator, v̂(D), of v(πe) that has low mean squared error (MSE): MSE(v̂(D), v(πe)) := E [ (v̂(D)− v(πe))2 ] .We use capital letters to denote ran-\ndom variables, and so the random terms in expected values are always the capitalized letters (e.g. D is a random variable). We assume that the process producing states, actions, and rewards is an MDP with an unknown initial state distribution, transition function, and reward function. We assume that the evaluation policy, πe, the behavior policies, πi, i ∈ {1, . . . , n}, and the discount parameter, γ, are known. For a review of OPE methods, see the works of Precup et al. (2000) or Thomas (2015b, Chapter 3). More recent methods can be found in the works of Jiang & Li (2015) and Mandel et al. (2016)."
    }, {
      "heading" : "4. Doubly Robust (DR) Estimator",
      "text" : "The doubly robust (DR) estimator (Jiang & Li, 2015) is a new unbiased estimator of v(πe) that achieves promising empirical and theoretical results by leveraging an approximate model of an MDP to decrease the variance of the unbiased estimates produced by ordinary importance sam-\npling (Precup et al., 2000). It is doubly robust in that it will provide “good” estimates if either 1) the model is accurate or 2) the behavior policies are known. By “good” it is meant that if the former does not hold then the estimator will remain unbiased (although it might have high variance and thus high mean squared error), and if the latter does not hold then if the model has low error the doubly robust estimator will also tend to have low error. Doubly robust estimators were introduced and remain popular in the statistics community (Rotnitzky & Robins, 1995; Heejung & Robins, 2005).\nThe work that introduced the DR estimator for MDPs (Jiang & Li, 2015) derived it as a generalization of a doubly robust estimator for bandits (Dudı́k et al., 2011). This may be why the DR estimator was derived only for the finite horizon setting where the horizon is known (every trajectory must terminate within L < ∞ time steps, and L must be known). It also resulted in a recursive definition of the DR estimator that can be difficult to interpret. In Appendix B we instead derive the DR estimator for MDPs as an application of control variates. Our new derivation holds without assumptions on the horizon and gives the intuitive non-recursive definition, where wit = ρ i t/n:\nDR(D) := n∑\ni=1\n∞∑\nt=0\nγtwitR Hi t (2)\n− n∑\ni=1\n∞∑\nt=0\nγt ( witq̂ πe ( SHit , A Hi t ) − wit−1v̂πe ( SHit )) .\nIn Appendix B we show that this definition is equivalent to that of Jiang & Li (2015) when the horizon is finite and known, and we provide several new theoretical results pertaining to the DR estimator. Specifically, we give conditions for DR to be an unbiased estimator without assumptions on the horizon, and we give the first proofs that it is a strongly consistent estimator. Although these are important properties to establish, we relegate them to an appendix due to space limitations.\nThe non-recursive definition of the DR estimator presented in (2) also reveals the close relationship of the DR estimator to advantage sum estimators. Advantage sum estimators were introduced as a way to lower the variance of on-policy Monte Carlo performance estimates for a setting that is a generalization of the (partially observable) MDP setting (Zinkevich et al., 2006; White & Bowling, 2009). The DR estimator for the on-policy setting can be found in the work of Zinkevich et al. (2006, Equation 8). One may therefore view the DR estimator (Jiang & Li, 2015) as the extension of the advantage sum estimator (Zinkevich et al., 2006) to the off-policy setting or as the extension of the doubly robust estimator for bandits (Dudı́k et al., 2011) to the sequential setting. We are therefore not the first to\nshow that the DR estimator can be viewed as an application of control variates, since Veness et al. (2011, Section 3.1) point out that the advantage sum estimator is an application of control variates. Still, our derivation in Appendix B of the DR estimator is novel.\nThe DR estimator is not purely model based, since it uses importance weights. However, it is also not a model-free importance sampling method, since it uses an approximate model to decrease the variance of its estimates. We therefore refer to it as a guided importance sampling method, since the approximate model is used to guide, but not completely replace, the importance sampling estimates."
    }, {
      "heading" : "5. Weighted Doubly Robust (WDR) Estimator",
      "text" : "Empirical and theoretical results show that the DR estimator developed, analyzed, and tested by Jiang & Li (2015) can significantly reduce the variance of ordinary importance sampling without introducing bias. The fact that it does not introduce bias can be particularly important when the estimator is used to produce confidence bounds on v(πe) (Thomas, 2015b). However, in practice these confidence bounds often require an impractical amount of data before they are tight enough to be useful, and so approximate confidence bounds (e.g., bootstrap confidence bounds) are used instead (Theocharous et al., 2015; Thomas, 2015b). When using these approximate confidence bounds, the strict requirement that an OPE estimator be an unbiased estimator of v(πe) is not necessary. Furthermore, often the goal of OPE is not to produce confidence bounds, but to produce the best estimate of v(πe) possible, in order to determine whether πe should be used instead of the current behavior policy or as an internal mechanism in a policy search algorithm (Levine & Koltun, 2013). In these cases, the “best” estimator is typically defined as the one that has the lowest mean squared error (MSE). For example, in their experiments, Precup et al. (2000), Dudı́k et al. (2011), Mahmood et al. (2014), Thomas (2015b), and Jiang & Li (2015) all use the (root) MSE when evaluating OPE methods.\nAlthough unbiasedness might seem like a desirable property of an estimator, when the goal is to minimize MSE, it often is not. In general, the MSE of an estimator, θ̂, of a statistic, θ, can be decomposed into its variance and its squared bias: MSE(θ̂, θ) = E[(θ − θ̂)2] = Var(θ̂) + Bias(θ̂)2, where Bias(θ̂) := E[θ̂] − θ. The optimal estimator in terms of MSE is typically one that balances this bias-variance trade-off, not one with zero bias. Therefore, in the context of minimizing MSE, strong asymptotic consistency, which requires the MSE of an estimator to almost surely converge to zero as the amount of available data increases, is a more desirable property than unbiasedness.\nIn this section we propose a new OPE estimator that we call the weighted doubly robust (WDR) estimator. The WDR estimator comes from applying a simple well-known extension to importance sampling estimators to the DR estimator to produce a new guided importance sampling method. This extension does not directly optimize the bias-variance trade-off, but it does tend to significantly better balance it while maintaining asymptotic consistency. More specifically, WDR is based on weighted importance sampling (Powell & Swann, 1966) as opposed to ordinary importance sampling (Hammersley & Handscomb, 1964). For further discussion of the benefits of weighted importance sampling over ordinary importance sampling, see the work of Thomas (2015b, Section 3.8). Weighted importance sampling has been used before for OPE (Precup et al., 2000), but not in conjunction with the DR estimator.\nOur WDR estimator is defined as the DR estimator in (2), except where wit := ρ i t/ ∑n j=1 ρ j t . 4 Intuitively it is clear that this estimator is asymptotically correct because E[ρjt ] = 1, and so by the law of large numbers the denominator of wit will converge to n. Although WDR is not an unbiased estimator of v(πe), its bias follows a pattern that is both predictable and also sometimes desirable. When there is only a single trajectory, i.e., n = 1, WDR(D) is an unbiased estimator of the performance of the behavior policy, since w1t = 1 for all t. If there is a single behavior policy, πb, as the number of trajectories increases, the expected value ofWDR(D) shifts away from v(πb) towards v(πe).\nBefore presenting theoretical results about the WDR estimator, we introduce assumptions that they will require. These assumptions are only included if they are explicitly mentioned in a theorem—most theorems only rely on few of these assumptions. Even when these assumptions are not satisfied, it does not mean that the result does not hold or that the WDR estimator will perform poorly—it merely means that the theoretical results that we provide are not guaranteed by our proofs.\nAssumption 1 ensures that all trajectories of interest when evaluating πe will be produced by all of the behavior policies. This is a standard assumption in OPE and typically precludes the use of deterministic behavior policies.5\nAssumption 1 (Absolute continuity). For all (s, a, i) ∈ 4Just as DR-v2 extends the DR estimator (Jiang & Li, 2015, Section 4.4), one can create the WDR-v2 estimator by replacing q̂πe(St, At) with r̂πe(St, At, 0) + γv̂πe(St+1) in (2). Empirical results for DR-v2 and WDR-v2 are included in the spreadsheet in the supplemental documents. For the domains presented here, these variants did not outperform the original DR and WDR estimators.\n5Assumption 1 could be replaced with a less-restrictive assumption like that used by Thomas (2015b, Section 3.5). We use Assumption 1 because it allows for simplified proofs.\nS ×A× {1, . . . , n}, if πi(a|s) = 0 then πe(a|s) = 0.\nAssumption 2 requires all of the behavior policies to be identical. This assumption is trivially satisfied if data is collected from one behavior policy. Also, this assumption is often satisfied for applications where data is abundant, so that evaluation can be performed using only the data from the most recent behavior policy. Also, Assumption 3 requires the horizon, L, to be finite.\nAssumption 2 (Single behavior policy). For all (i, j) ∈ {1, . . . , n}2, πi = πj . Assumption 3. L is finite.\nAssumption 4 requires the importance weights, ρit, to be bounded above by a finite constant, β ∈ R (they are always bounded below by zero). It is trivially satisfied in the common setting where the horizon is finite and the state and action sets are finite. Although Assumption 4 requires β to exist, none of our results depend on how large β is. So, in the non-finite state, action, and horizon settings one may ensure that evaluation policies are only considered if they satisfy Assumption 4 for some arbitrarily large β.\nAssumption 4 (Bounded importance weight). There exists a constant β < ∞ such that for all (t, i) ∈ N≥0 × {1, . . . , n}, ρit ≤ β surely.\nIn the following theorems, Theorems 1 and 2, we give two different sets of assumptions that are sufficient to show that WDR is a strongly consistent estimator of v(πe). Notice that if the sets of states and actions are finite and the horizon is finite, then Assumption 4 holds, and so Theorem 2 means that WDR will be strongly consistent given only Assumption 1.\nTheorem 1 (WDR – strongly consistent estimator for one behavior policy, finite horizon). If Assumptions 1, 2, and 3 hold thenWDR(D) a.s.−→ v(πe). Proof See Appendix C.1. Theorem 2 (WDR – strongly consistent estimator for many behavior policies). If Assumptions 1 and 4 hold then WDR(D) a.s.−→ v(πe). Proof See Appendix C.2."
    }, {
      "heading" : "6. Empirical Studies (WDR)",
      "text" : "In order to both show the empirical benefits of WDR over existing importance sampling estimators and better motivate our second major contribution, in this section we present an empirical comparison of different OPE methods.6 We compare to a broad sampling of model-free importance sampling estimators, definitions of which can be found in the work of Thomas (2015b, Chapter 3): importance sampling (IS), per-decision importance sampling\n6The raw data for all of the plots provided in this paper (as well as additional plots) are available in the spreadsheet included in the supplemental material.\nIS PDIS WIS CWPDIS DR AM WDR\n(PDIS), weighted importance sampling (WIS), and consistent weighted per-decision importance sampling (CWPDIS). We also compare to the guided importance sampling doubly robust (DR) estimator (Jiang & Li, 2015).\nLastly, we compare to the approximate model (AM) estimator, which uses all of the available data to construct an approximate model of the MDP.7 The performance of the evaluation policy on the approximate model is typically easy to compute and can be used as an estimate of v(πe). For example, in our experiments the approximate model maintains an estimate, d̂0, of the initial state distribution, and so we define AM := ∑ s∈S d̂0(s)v̂\nπe(s). Notice that unlike the importance sampling based methods, AM does not include any importance weights (ρt terms).\nIn Appendix D we provide detailed descriptions of the experimental setup and results. Here we provide only an overview. We used three domains: a 4 × 4 gridworld previously constructed specifically for evaluating OPE methods (Thomas, 2015b, Section 2.5), as well as two simple domains that we developed to exemplify the settings where different methods excel and fail. In our simulations, WDR dominated the other importance sampling and guided importance sampling estimators (but not AM). Not only did WDR always achieve the lowest mean squared error of these estimators, but no other single (guided) importance sampling estimator was able to always achieve mean squared errors within an order of WDR’s. Figure 1a is an example using the gridworld where no other method achieved mean squared errors within an order of WDR’s.\nThe second notable trend is that WDR often significantly outperformed AM. We constructed a simple MDP and experimental setup that we callModelFail, which exemplifies this. In the ModelFail experiments, the approximate model\n7This model-based estimator has been called the direct method in previous work (Dudı́k et al., 2011), however, in other previous work direct methods are model-free while indirect methods are model-based (Sutton & Barto, 1998, Section 9.2).\nuses function approximation, which causes it to converge to the wrong MDP. This results in AM’s mean squared error plateauing at a non-zero value, as shown in Figure 1b. Since WDR remains strongly consistent in this setting, its MSE converges almost surely to zero.\nThe third notable trend is that when an accurate approximate model is available, WDR does not always outperform AM.We constructed a simple MDP that we callModelWin, which exemplifies this. In the ModelWin MDP, the approximate model quickly converges to the correct MDP, and so AM outperforms WDR. This is depicted in Figure 1c.\nOne might wonder why DR and WDR can do worse than AM even though they incorporate the approximate model. Notice that we can write the DR and WDR estimators as:\nWDR(D) := 1\nn\nn∑\ni=1\nv̂πe(SHi0 )\n︸ ︷︷ ︸ (a)\n(3)\n+\nn∑\ni=1\n∞∑\nt=0\nγtwit [ RHit − q̂πe ( SHit , A Hi t ) + γv̂πe ( SHit+1 )\n︸ ︷︷ ︸ (b)\n] .\nIf the approximate model is perfect, then (a) is both a low variance and unbiased estimator of v(πe). If the approximate model is perfect and Rt and St+1 are deterministic functions of St and At, then (b) is zero, and so the second term is always zero andWDR is an excellent estimator. However, ifRt or St+1 is not a deterministic function of St and At—if the state transitions or rewards are stochastic— then (b) is not necessarily zero. If the importance weights, wit, have high variance, then even slightly non-zero values of (b) can cause DR and WDR to have high variance.\nIn summary, in our experiments, WDR dominated the other (guided) importance sampling estimators, sometimes achieving orders of magnitude lower MSE. However, the experiments also show that WDR is not always the best\nestimator—sometimes AM can produce estimates with an order lower MSE. This trend is also visible in the results of Jiang & Li (2015), where AM performs better than DR (although they did not compare to WDR, since it had not yet been introduced). Ideally we would like an estimator that combines WDR and AM or switches between them automatically, to always achieve the performance of the better estimator. In the following sections we show how this can be done."
    }, {
      "heading" : "7. Blending IS and Model (BIM) Estimator",
      "text" : "In this section we show how two OPE estimators can be merged into a single estimator that exhibits the desirable properties of both. Before doing so, we establish some terminology. We divide OPE estimators into three classes. The first class we call importance sampling estimators. We define this class to include all estimators that, when L is finite, are defined using all of the importance weights ρ0, ρ1, . . . , ρL−1. Notice that this includes IS, PDIS, WIS, and CWPDIS, as well as the guided importance sampling methods, DR and WDR.\nThe second class we call purely model-based estimators. We define this class to include all estimators that do not contain any ρt terms for t ≥ 0. The only purely modelbased estimator in this paper is AM. Finally, we call the third class partial importance sampling estimators. These estimators are those that do not fall into either of the other two classes—estimators that use importance weights, ρt, but only for t < L− 1. We will introduce one such estimator later in this section.\nWe contend that importance sampling estimators and purely model-based estimators are two extremes on a spectrum of estimators. Importance sampling estimators tend to be strongly consistent. That is, as more historical data becomes available, their estimates become increasingly accurate. However, their use of importance weights means that they all (including DR and WDR) also can have high variance relative to purely model-based estimators. This is evident in the results on the ModelWin domain.\nOn the other end of the spectrum, purely model-based estimators like AM are often not strongly consistent. If the approximate model uses function approximation or if there is some partial observability, then the approximate model may not converge to the true MDP. So, as more historical data becomes available, the estimates of AMmay converge to a value other than v(πe). Thus, purely model-based estimators tend to have high bias, even asymptotically, as evidenced by the AM curve in Figure 1b. However, purely model-based methods also tend to have low variance because they do not contain any ρt terms.\nBetween these two extremes lies a range of partial im-\nportance sampling estimators. Estimators that are close to the purely model-based estimators use ρt terms only for small t, while estimators that are close to importance sampling estimators use ρt terms with large t approaching L − 1. Before formally defining one such partial importance sampling estimator, we present a few additional definitions. First, let IS(j)(D) denote an estimate of E[ ∑j\nt=0 γ tRt|H ∼ πe], constructed from D using an im-\nportance sampling method like PDIS or WDR, which uses importance weights up to and including ρt. Similarly, let AM(j)(D) denote a primarily model-based prediction from D of E[ ∑∞ t=j γ\ntRt|H ∼ πe] that may not use ρt terms with t ≥ j. We can now define a partial importance sampling estimator that we call the off-policy j-step return, g(j)(D), which uses an importance sampling based method to predict the outcome of using πe up until Rj is generated, and the approximate model estimator to predict the outcomes thereafter. That is, for all j ∈ N≥−1, let8\ng(j)(D) := IS(j)(D) + AM(j+1)(D)\ng(∞)(D) := lim j→∞ g(j)(D). (4)\nWe refer to j as the length of the j-step return.\nNotice that g(−1)(D) is a purely model-based estimator, g(∞)(D) is an importance sampling estimator, and the other off-policy j-step returns are partial importance sampling estimators that blend between these two extremes. When j is small, the off-policy j-step return is similar to AM, using importance sampling to predict only a few early rewards. When j is large, it uses importance sampling to predict most of the rewards and the model only for a few rewards at the end of a trajectory. So, as j increases, we expect the variance of the return to increase, but the bias to decrease.\nWe propose a new estimator, which we call the blending IS and model (BIM) estimator, that leverages this spectrum of estimators to blend together the IS and AM estimators in a way that minimizes MSE. It does this by computing a weighted average of the different length returns: BIM(D) := x⊺g(D), where x := (x−1, x0, x1, . . . )⊺ is an infinite-dimensional weight vector and g(D) is an infinitedimensional vector of different length returns, g(D) := (g(−1)(D), g(0)(D), . . . , )⊺. Although in theory x can be infinite, in practice there is always finite data, so x is finite. The remaining question is then: how should we select the weights, x?\nA similar question has been studied before in reinforce-\n8If prior knowledge about d0 is available, then one might consider adding g(−2)(D) to denote the model’s prediction of v(πe), which might differ from g(−1)(D).\nment learning research when deciding how to weight jstep returns (not off-policy), as reviewed by Sutton & Barto (1998, Section 7.2). The most common solution, a complex return called the λ-return, uses x−1 = 0 and xj = (1− λ)λj for all other j. The λ-return is the foundation of the entire TD(λ) family of algorithms, which includes the original linear-time algorithm (Sutton, 1988), least-squares formulations (Bradtke & Barto, 1996; Mahmood et al., 2014), methods for adapting λ (Downey & Sanner, 2010), true-online methods (van Hasselt et al., 2014), and the recent emphatic methods (Mahmood et al., 2015).\nRecent work has suggested that the λ-return could be replaced by more statistically principled complex returns like the γ-return (Konidaris et al., 2011) or Ωreturn (Thomas et al., 2015). For the finite-horizon setting and for j ∈ {0, . . . , L − 1} the γ-return uses xj := ( ∑j i=0 γ 2i))−1/ ∑L−1 ĵ=0 ( ∑ĵ i=0 γ 2i)−1, and the Ωreturn uses xj = ∑L−1 i=0 Ω −1 n (j, i)/ ∑L−1 ĵ,i=0 Ω−1n (ĵ, i), where Ωn is the L×L covariance matrix where Ωn(i, j) = Cov(g(i)(D), g(j)(D)), and where both the γ and Ωreturns use xj = 0 for j 6∈ {0, . . . , L− 1}. The advantage of the γ-return over the λ-return is that it uses a more accurate model of how variance increases with the length of a return, which also eliminates the λ hyperparameter used by the λ-return. The advantages of the Ωreturn over the γ-return are that it both uses a yet moreaccurate estimate of how variance grows with the length of the return, which is computed from historical data, and that it better accounts for the fact that different length returns are not independent, i.e., g(i)(D) and g(j)(D) are not independent even if i 6= j. However, none of these weighting schemes are sufficient for our needs because they do not cause BIM to necessarily be a strongly consistent estimator.9 This is likely because they were all designed for the setting where only one trajectory is available, i.e., n = 1, while strong consistency is a property that deals with performance as n→∞. Furthermore, they were designed for on-policy policy evaluation.\nWe therefore propose a new weighting scheme (a new complex return for multiple trajectories) that directly optimizes our primary objective: the mean squared error. This new weighting scheme is x⋆ := argminx∈R∞ MSE(x⊺g(D), v(πe)). Unfortunately, we typically cannot compute x⋆, because we do not know MSE(x⊺g(D), v(πe)) for any x. Instead, we propose estimating x⋆ by minimizing an approximation of MSE(x⊺g(D), v(πe)). First, dealing with an infinite number of different return lengths is challenging. To avoid this, we propose only using a subset of the returns, {g(j)(D)},\n9The λ-return with λ = 1 is defined to be g(∞)(D) and is consistent, but it does not mix the two OPE methods at all.\nfor j ∈ J , where |J | < ∞. For all j 6∈ J , we assign xj = 0. We suggest including −1 and∞ in J . To simplify later notation, let gJ (D) ∈ R|J | be the elements of g(D) whose indexes are in J—the returns that will not necessarily be given weights of zero. Also let Jj denote the jth element in J . We can then estimate x⋆ by:\nx̂⋆ ∈ arg min x∈R|J | MSE(x⊺gJ (D), v(πe)),\nwhere our estimate of x⋆j is zero if j 6∈ J and our estimate of x⋆Jj is x̂ ⋆ j for j ∈ {1, . . . , |J |}.\nNext, to avoid searching all of R|J | and also to serve as a form of regularization on x̂⋆, we limit the set of x that we consider to the |J |-simplex, i.e., we require xj ≥ 0 for all j ∈ {1, . . . , |J |} and ∑|J |j=1 xj = 1. We write ∆|J | to denote this set of weight vectors—the |J |-simplex. Using the bias-variance decomposition of MSE, we therefore have that\nx̂⋆ ∈ arg min x∈∆|J | Bias(x⊺gJ (D))2 +Var(x⊺gJ (D))\n= arg min x∈∆|J |\nx⊺[Ωn + bnb⊺n]x,\nwhere n remains the number of trajectories in D, Ωn is the |J | × |J | covariance matrix where Ωn(i, j) = Cov(g(Ji)(D),g(Jj)(D)) and bn is the |J |-dimensional vector with bn(j) = E[g(Jj)(D)] − v(πe) for all j ∈ {1, . . . , |J |}.10 This simplifies the problem of estimating the MSE for all possible x into estimating two terms: the bias vector, bn, and the covariance matrix, Ωn.\nLet b̂n and Ω̂n be the estimates of bn and Ωn when there are n trajectories in D. The exact scheme used to estimate bn and Ωn depends on the definitions of IS(j)(D) and AM(j)(D). In general, both terms are easier to estimate for unweighted importance sampling estimators like PDIS and DR than for weighted estimators like CWPDIS or WDR.\nTo make the dependence of BIM on the estimates of Ωn and bn explicit, and to summarize the approximations we have made, we redefine the BIM estimator as:\nBIM(D, Ω̂n, b̂n) := (x̂ ⋆)⊺gJ (D),\nwhere\nx̂⋆ ∈ arg min x∈∆|J | x⊺[Ω̂n + b̂nb̂⊺n]x.\nIn the next section we propose using WDR as the importance sampling method, IS, and show how bn and Ωn can\n10Since bn (similarly, Ωn) already has a subscript, we write bn(j) to denote the j th element of bn.\nbe approximated in this setting. First we show that if at least one of the returns included in J is a strongly consistent estimator of v(πe), and if the estimates of bn and Ωn are themselves strongly consistent, then BIM is a strongly consistent estimator of v(πe): Theorem 3. If Assumption 4 holds, there exists at least one j ∈ J such that g(j)(D) is a strongly consistent estimator of v(πe), b̂n − bn a.s.−→ 0, and Ω̂n −Ωn a.s.−→ 0, then BIM(D, Ω̂n, b̂n) a.s.−→ v(πe). Proof See Appendix E."
    }, {
      "heading" : "8. Model and Guided Importance Sampling Combining (MAGIC) Estimator",
      "text" : "In this section we propose using the BIM estimator with WDR as the importance sampling estimator. The resulting estimator combines purely model based estimates with the estimates of the guided importance sampling algorithm WDR, and so we call it the model and guided importance sampling combining (MAGIC) estimator.\nAlthough the derivation of how to properly define IS(j)(D) and AM(j)(D) in order to blend WDR with the approximate model is less obvious than one might expect and therefore an important technical detail, we relegate it to Appendix F due to space restrictions. The resulting definition of an off-policy j-step return is, for all j ∈ N≥−1:\ng(j)(D) := n∑\ni=1\nj∑\nt=0\nγtwitR Hi t\n︸ ︷︷ ︸ (a)\n+ n∑\ni=1\nγj+1wij v̂ πe(SHij+1) ︸ ︷︷ ︸ (b)\n− n∑\ni=1\nj∑\nt=0\nγt ( witq̂ πe ( SHit , A Hi t ) − wit−1v̂πe ( SHit ))\n︸ ︷︷ ︸ (c)\n.\nwhere (c) is the combined control variate for both the importance sampling based term, (a), and the model-based term, (b), and where we use WDR’s definition of wit.\nWe estimateΩn from the n trajectories inD using a sample covariance matrix, Ω̂n. See Appendix G for details and pseudocode for MAGIC.\nEstimating the bias vector, bn, is challenging because it has a strong dependence on the value that we wish we knew, v(πe). We cannot use AM’s estimate as a stand-in for v(πe) because it would cause us to assume that AM’s greatest weakness—its high bias—is negligible. We cannot use WDR’s estimate (or any other importance sampling estimator’s estimate) because our estimate of bn would then conflate the high variance of importance sampling estimates with the bias that we wish to estimate.\nWhen n, the number of trajectories inD, is small, variance tends to be the root cause of high MSE. We therefore propose using an estimate of bn that is initially conservative— initially it underestimates the bias—but which becomes\ncorrect as n increases. LetCI(g(∞)(D), δ) be a 1−δ confidence interval on the expected value of the random variable g(∞)(D) = WDR(D). Intuitively, as n increases we expect that this confidence interval will converge to g(∞)(D), which in turn converges to v(πe). So, we estimate bn(j), the bias of the off-policy j-step return, by its distance from the 10% confidence interval. That is, we estimate bn(j) as\nb̂n(j) := dist ( g(Jj)(D),CI(g(∞)(D), 0.5) ) ,\nwhere dist(y,Z) is the distance between y ∈ R and the set Z ⊆ R: dist(y,Z) = minz∈Z |y−z|.We use both the percentile bootstrap confidence interval (Efron & Tibshirani, 1993) and Chernoff-Hoeffding’s inequality—whichever is tighter—for CI in our experiments.\nIn Theorem 4 we show that the MAGIC estimator is a strongly consistent estimator of v(πe) given one set of assumptions that we used to show that WDR is strongly consistent and that WDR is included as one of the off-policy j-step returns.\nTheorem 4 (MAGIC - strongly consistent). If Assumptions 1 and 4 hold and ∞ ∈ J then MAGIC(D) a.s.−→ v(πe). Proof See Appendix H."
    }, {
      "heading" : "9. Empirical Studies (MAGIC)",
      "text" : "Appendix I provides detailed experiments using MAGIC. In this section we provide an overview of these results. The first three plots in Figure 2 correspond to those in Figure 1, but include MAGIC. In general MAGIC does very well, tracking or exceeding the best performance of WDR and AM. However, in Figure 2c MAGIC does not perfectly track AM. The scale is logarithmic, so the difference between MAGIC and AM is small in comparison to the benefit of MAGIC over WDR. We hypothesize that the reason MAGIC does not match AM may be due to error in our estimates of Ωn and bn.\nFigure 2d is for an experimental setup that we call Hybrid, where early in trajectories there is partial observability (e.g., initial uncertainty about a student’s knowledge in an intelligent tutoring system, or uncertainty about the state of the world in a robotic application). In these settings MAGIC outperforms all other estimators, even AM and WDR, by automatically leveraging WDR for the parts of trajectories where partial observability causes the model to be inaccurate, and AM for parts of the trajectories where the model is accurate. To emphasize this, we include MAGIC-B (B for binary) where J = {−1,∞}, so that BIM can only blend AM and WDR by placing weights on them. The relatively poor performance of MAGIC-B supports our use of off-policy j-step returns.\nDR AM WDR MAGIC MAGIC-B"
    }, {
      "heading" : "10. Conclusion",
      "text" : "We have proposed several new OPE estimators and showed empirically that they outperform existing estimators. While previous OPE estimators that use importance sampling often failed to outperform the approximate model estimator (which does not use importance sampling), our new estimators often do, frequently by orders of magnitude. In cases where the approximate model estimator remains the best estimator, one of our new estimators, MAGIC, performs similarly. In other cases, MAGIC meets or exceeds the performance of state-of-the-art prior estimators."
    }, {
      "heading" : "A. Preliminaries",
      "text" : "In this section we present additional notation, definitions, properties, (known) theorems, corollaries, and lemmas that are useful when we prove theorems later.\nLet Ht := (S0, A0, R0, S1, . . . , St−1, At−1, Rt−1, St) be the first t transitions in the episode H . We call Ht a partial trajectory of length t. Notice that we use subscripts on trajectories to denote the trajectory’s index inD and superscripts to denote partial trajectories—Hti is the first t transitions of the ith trajectory in D. Let Ht be the set of all possible partial trajectories of length t.\nFor all (π, s) ∈ Π × S , let supps(π) be the set of actions that have non-zero probability when the policy π is used to select an action in state s, i.e., supps(π) := {a ∈ A : π(a|s) 6= 0}. Similarly, let supp(π, t) := {ht ∈ Ht : Pr(Ht = ht|π) 6= 0}. Later we will need to bound terms like ρitR i t for some t and i. Notice that even if ρit < β, it is possible for ρitR i t > βrmax if rmax is negative, since ρ i t could be zero. Additionally, sometimes we may deal with rmax terms and other times rmodelmax . To avoid explicitly handling these cases, we will bound terms using loose bounds that depend on a new term: r⋆max := max{|rmin|, |rmax|, |rmodelmin |, |rmodelmax |}. Definition 1 (Almost Sure Convergence). A sequence of random variables, (Xn)∞n=1, converges almost surely to the random variable X if\nPr ( lim n→∞ Xn = X ) = 1.\nWe write Xn a.s.−→ X to denote that the sequence (Xn)∞n=1 convergences almost surely to X .\nDefinition 2. Let θ be a real number and (θ̂n)∞n=1 be an infinite sequence of random variables. We call θ̂n, a (strongly) consistent estimator of θ if and only if θ̂n a.s.−→ θ.\nNotice that an estimator being unbiased does not mean that it is also strongly consistent—estimators can be any combination of biased/unbiased and consistent/inconsistent. Next we present several known properties of almost sure convergence (Mittelhammer, 1996, Section 5.5).\nProperty 1. [Continuous mapping theorem] Xn a.s.−→ X implies that f(Xn) a.s.−→ f(X) for every continuous function f .\nProperty 2. Let Xn and Yn be sequences of random variables and X and Y be random variables. If Xn\na.s.−→ X , Yn\na.s.−→ Y , and if Pr(Y = 0) = 0, then XnYn a.s.−→ XY .\nProperty 3. If {Xin}mi=1 arem <∞ sequences of random variables such that Xin\na.s.−→ Xi for all i ∈ {1, . . . ,m}, then ∑m i=1X i n a.s.−→∑mi=1Xi.\nWe will require an additional property of almost sure convergence that is similar to Property 3, but which allows for the sum over a countably infinite number of sequences of random variables, i.e., m = ∞. In order to establish this property we begin with Lebesgue’s dominated convergence theorem:\nTheorem 5 (Lebesgue’s Dominated Convergence Theorem). Let (fn)∞n=1 be a sequence of integrable functions that converges almost everywhere to a real-valued measurable function f . If there exists an integrable function11 g such that |fn| ≤ g for all n, then\nlim n→∞\n∫ fn dµ = ∫ f dµ.\nProof. See the work of (Bartle, 2014, Theorem 5.6).\nNext we use Lebesgue’s dominated convergence theorem to show conditions under which we can reverse the order of a limit and an infinite summation:\nLemma 1. Let {xin}∞i=0 be a countably infinite number of real-valued sequences indexed by i, such that limn→∞ xin = x\ni for all i ∈ N≥0. If there exists a function g : N≥0 → R such that |xin| ≤ g(i) for all n ∈ N>0 and i ∈ N≥0, and ∑∞ i=0 g(i) <∞, then\nlim n→∞\n∞∑\ni=0\nxin =\n∞∑\ni=0\nlim n→∞\nxin.\nProof. We apply Lebesgue’s dominated convergence theorem (Theorem 5), where for all (n, i) ∈ N>0 × N≥0, fn(i) = X i n, f(i) = x\ni, and µ is the counting measure on the measure space (N≥0,P(N≥0)), where P(N≥0) is the power set of N≥0.\nWe can now establish our desired property about almost sure convergence:\nProperty 4. Let {Xin}∞i=0 be a countably infinite number of sequences of random variables such thatXin\na.s.−→ Xi for all i ∈ N≥0. If there exists a function g : N≥0 → R such that |Xin| ≤ g(i) surely for all (n, i) ∈ N>0 × N≥0, and∑∞\ni=0 g(i) <∞, then ∑∞ i=0X i n a.s.−→∑∞i=0Xi. 11To conform to standard notations elsewhere, here we reuse the symbol g, which was previously used to denote the return of a trajectory, g(H). The two uses of g are sufficiently dissimilar that this reuse should not cause confusion."
    }, {
      "heading" : "Proof.",
      "text" : "Pr ( lim n→∞ ∞∑\ni=0\nXin =\n∞∑\ni=0\nXi )\n(a) ≥Pr   ∞⋂\ni=0\n( lim n→∞ Xin = X i )⋂( ∞∑\ni=0\nlim n→∞\nXin = ∞∑\ni=0\nXi )\n︸ ︷︷ ︸ (b)\n \n(c) ≥Pr   ∞⋂\ni=0\n( lim n→∞ Xin = X i )⋂( ∞⋂\ni=0\n( lim n→∞ Xin = X i ))\n︸ ︷︷ ︸ (d)\n \n=Pr\n( ∞⋂\ni=0\n( lim n→∞ Xin = X i ))\n=1− Pr   ∞⋃\ni=0\n( lim n→∞ Xin 6= Xi ) ︸ ︷︷ ︸ (e)\n \n=1,\nwhere (a) comes from Lemma 1 which ensures that\n∞⋂\ni=0\n( lim n→∞ Xin = X i )\n=⇒ (\nlim n→∞\n∞∑\ni=0\nXin = ∞∑\ni=0\nlim n→∞\nXin ) ,\n(c) holds because (d) =⇒ (b), and (e) has zero measure because it is the countable union of zero measure sets by the assumption that Xin a.s.−→ Xi for all i ∈ N≥0.\nNext we show that if a sequence of random variables, Xn, converges almost surely to a random variable, X , then the expected value of Xn converges to the expected value of X .\nLemma 2. If (Xi)∞i=1 is a sequence of uniformly bounded real-valued random variables and if Xn\na.s.−→ X , then limn→∞E[Xn] = E[X].\nProof. Let Xn (for all n) and X be random variables on the probability space (Ω,Σ, P ) and let A = {ω ∈ Ω : limn→∞Xn = X}. Then:\nlim n→∞ E [Xn] = lim n→∞\n∫\nΩ\nXndP\n(a) =\n∫\nΩ\nlim n→∞ XndP\n=\n∫\nA lim n→∞ XndP ︸ ︷︷ ︸\n(b)\n+\n∫\nΩ\\A lim n→∞ XndP ︸ ︷︷ ︸ (c) ,\nwhere (a) comes from the bounded convergence theorem. For term (b), notice that for all ω ∈ A, limn→∞Xn = X . For term (c), notice that by the assumption thatXn\na.s.−→ X , we have that Ω \\ A has measure zero. So:\nlim n→∞ E [Xn] =\n∫\nA XdP\n=\n∫\nA XdP +\n∫\nΩ\\A XdP\n=E[X].\nNext we present a lemma that relates almost sure convergence of estimators to mean squared error. Let θ̂ be an estimator of θ. Recall that:\nMSE(θ̂, θ) := E [ (θ̂ − θ)2 ] .\nWe show that a sequence, (Xn)∞n=1 converges almost surely to X if and only if limn→∞MSE(Xn, X) = 0.\nLemma 3. If (Xi)∞i=1 is a sequence of uniformly bounded real-valued random variables, then Xn\na.s.−→ X if and only if limn→∞MSE(Xn, X) = 0.\nProof. We show each direction separately. First we show that Xn a.s.−→ X implies limn→∞MSE(Xn, X) = 0.\nMSE(Xn, X) =E[(Xn −X)2] =E[Yn],\nwhere Yn := (Xn − X)2. By the continuous mapping theorem we have that Yn\na.s.−→ (X − X)2 = 0. So, by Lemma 2 (applied to E[Yn]) we have that\nlim n→∞ MSE(Xn, X) =E[0]\n= 0.\nNext we show the other direction: that limn→∞MSE(Xn, X) = 0 implies Xn\na.s.−→ X . Let X and allXn be random variables on the probability space (Ω,Σ, P ), A = {ω ∈ Ω : limn→∞MSE(Xn, X) = 0}, and B = {ω ∈ A : limn→∞Xn 6= X}. If limn→∞MSE(Xn, X) = 0, then by the definition of\nMSE we have that:\n0 = lim n→∞\n∫\nΩ\n(Xn −X)2 dP\n(a) =\n∫\nΩ\n( lim n→∞ Xn −X )2 dP\n=\n∫\nB\n( lim n→∞ Xn −X )2\ndP ︸ ︷︷ ︸\n(b)\n+\n∫\nA\\B\n( lim n→∞ Xn −X )2 dP\n︸ ︷︷ ︸ (c)\n+\n∫\nΩ\\A\n( lim n→∞ Xn −X )2 dP\n︸ ︷︷ ︸ (d)\n,\nwhere we get (a) by using the bounded convergence theorem to pass the limit inside the integral and the fact that (Xn − X)2 is a continuous function of Xn to then move the limit to the Xn term. Notice that (b), (c), and (d) are all positive, and so they must all be zero for the equality with zero to hold. We have that (d) is necessarily zero due to the definition of A and our assumption that limn→∞MSE(Xn, X) = 0. Similarly, (c) is zero because, from the definition of B, A \\ B causes limn→∞Xn = X . However, in (b), by the definition of B, limn→∞Xn−X is non-zero, and so for the equality with zero to hold, B must have measure zero. That is, Pr(limn→∞Xn 6= X) = 0, and thus Pr(limn→∞Xn = X) = 1.\nNext we show that if two sequences of random variables converge to the same random variable, then any sequence of random variables bounded between the two sequences must also converge to the same random variable.\nLemma 4. If Xn a.s.−→ X , Zn a.s.−→ X , and for all n, Xn ≤ Yn ≤ Zn, then Yn a.s.−→ X ."
    }, {
      "heading" : "Proof.",
      "text" : "Pr ( lim n→∞ Yn = X ) = Pr (( lim n→∞ Yn ≤ X )\n(5) ⋂(\nlim n→∞\nYn ≥ X ))\nSince\nPr ( lim n→∞ Yn ≥ X ) ≥Pr ( lim n→∞ Xn ≥ X )\n≥Pr ( lim n→∞ Xn = X )\n=1,\nand\nPr ( lim n→∞ Yn ≤ X ) ≥Pr ( lim n→∞ Zn ≤ X )\n≥Pr ( lim n→∞ Zn = X )\n=1,\nwe have that (5) is the probability of the joint occurance of two probability one events, and so\nPr ( lim n→∞ Yn = X ) = 1.\nNext we show that if the difference between two sequences converges almost surely to zero, then we can substitute one sequence for the other as an input to a continuous function without changing the almost sure convergence properties of the function: Lemma 5. If f is a continuous function, f(Xn) a.s.−→ X , and Yn −Xn a.s.−→ 0, then f(Yn) a.s.−→ X ."
    }, {
      "heading" : "Proof.",
      "text" : "Pr ( lim n→∞ f(Yn) = X ) = Pr ( lim n→∞ f(Yn −Xn +Xn) = X )\n(a) =Pr ( f ( lim n→∞ Yn −Xn +Xn ) = X )\n(b) ≥Pr (( lim n→∞ Yn −Xn = 0 )\n⋂( f ( lim n→∞ Xn ) = X\n))\n=Pr (( lim n→∞ Yn −Xn = 0 )\n⋂( lim n→∞ f(Xn) = X ))\n(c) =1,\nwhere (a) holds because f is a continuous function, and where (b) holds because it gives sufficient conditions for the event in the line above to hold, and (c) holds because under our assumptions the two events both occur with probability one. So we can conclude that f(Yn) a.s.−→ X .\nNext we review two standard forms of the strong law of large numbers.\nTheorem 6 (Khintchine Strong Law of Large Numbers). Let {Xi}∞i=1 be independent and identically distributed random variables. Then ( 1n ∑n i=1Xi) ∞ n=1 is a sequence of random variables that converges almost surely to E[X1].\nProof. See the work of Sen & Singer (1993, Theorem 2.3.13).\nTheorem 7 (Kolmogorov Strong Law of Large Numbers). Let {Xi}∞i=1 be independent (not necessarily identically distributed) random variables. If all Xi have the same mean and bounded variance (i.e., there is a finite constant b such that for all i ≥ 1, Var(Xi) ≤ b), then ( 1n ∑n i=1Xi) ∞ n=1 is a sequence of random variables that converges almost surely to E[X1].\nProof. See the work of Sen & Singer (1993, Theorem 2.3.10 with Proposition 2.3.10).\nIn Corollary 1 we present a simple extension of Kolmogorov’s strong law of large numbers that we often still refer to as Kolmogorov’s strong law of large numbers:\nCorollary 1. Let {Xi}∞i=1 be independent (not necessarily identically distributed) random variables. If all Xi have the same mean and are uniformly bounded by a finite constant b, then ( 1n ∑n i=1Xi) ∞ n=1 is a sequence of random variables that converges almost surely to E[X1].\nProof. For all i ∈ N>0 we have that |Xi| ≤ b surely, so from Popoviciu’s inequality, Var(Xi) ≤ b2, and so we can apply Theorem 7.\nWe now turn to results that are more specific to reinforcement learning and off-policy policy evaluation. Lemma 6 establishes a relationship between the expected values of r̂πe(s, i) and r̂πe(s,A, i) for all i ifA is generated by some policy π.\nLemma 6. Let (πe, π) ∈ Π2, where (π(a|s) = 0) =⇒ (πe(a|s) = 0) for all (a, s) ∈ A × S . Then for all (s, i) ∈ S × N≥0,\nr̂πe(s, i) = E [ πe(A|s) π(A|s) r̂ πe(s,A, i) ∣∣∣∣A ∼ π ] .\nProof. First, recall from (1) that for all (s, i) ∈ S ×\n{1, . . . , n}:\nr̂πe(s, i) := ∑\na∈A πe(a|s)r̂πe(s, a, i)\n= ∑\na∈supps(πe) πe(a|s)r̂πe(s, a, i)\n(a) =\n∑\na∈supps(π) πe(a|s)r̂πe(s, a, i)\n= ∑\na∈supps(π)\nπ(a|s) π(a|s)πe(a|s)r̂ πe(s, a, i)\n= ∑ a∈supps(π) π(a|s)πe(a|s) π(a|s) r̂ πe(s, a, i)\n=E [ πe(A|s) π(A|s) r̂ πe(s,A, i) ∣∣∣∣A ∼ π ] .\nwhere (a) holds by the assumption that (π(a|s) = 0) =⇒ (πe(a|s) = 0) for all (a, s) ∈ A× S .\nCorollary 2 extends Lemma 6 to show a relationship between v̂πe (s) and the expected value of q̂πe (s,A, i) if A is generated by some policy π:\nCorollary 2. Let (πe, π) ∈ Π2, where (π(a|s) = 0) =⇒ (πe(a|s) = 0) for all (a, s) ∈ A× S . Then for all s ∈ S ,\nv̂πe(s) = E [ πe(A|s) π(A|s) q̂ πe (s,A) ∣∣∣∣A ∼ π ] .\nProof. We have from Lemma 6 that for all i ∈ N≥0,\nr̂πe(s, i) = E [ πe(A|s) π(A|s) r̂ πe(s,A, i) ∣∣∣∣A ∼ π ] .\nSumming both sides over t and multiplying by γt we have that: ∞∑\nt=0\nγtr̂πe(s, t)\n︸ ︷︷ ︸ =v̂πe (s)\n=\n∞∑\nt=0\nγtE [ πe(A|s) π(A|s) r̂ πe(s,A, t) ∣∣∣∣A ∼ π ]\nv̂πe(s) =E [ πe(A|s) π(A|s) ∞∑\nt=0\nγtr̂πe(s,A, t)\n︸ ︷︷ ︸ =q̂πe (s,A)\n∣∣∣∣∣A ∼ π ]\n=E [ πe(A|s) π(A|s) q̂ πe (s,A) ∣∣∣∣A ∼ π ] .\nBefore presenting the next theorem, notice that we can express the DR estimator, (2), asDR(D) = 1n ∑n i=1DRi(D)\nif\nDRi(D) := ∞∑\nt=0\nγtρitR Hi t\n− ∞∑\nt=0\nγt ( ρitq̂ πe ( SHit , A Hi t ) − ρit−1v̂πe ( SHit )) .\nLemma 7 gives conditions under which the DR estimator is an unbiased estimator of v(πe) when using only one trajectory. This lemma is the bulk of the proof that the full DR estimator is unbiased—we have placed it in a separate lemma because it is also a useful result when showing that the DR estimator is strongly consistent.\nLemma 7. If Assumption 1 holds then E[DRi(D)] = v(πe) for all i ∈ {1, . . . , n}.\nProof. Recall that\nDRi(D) := ∞∑\nt=0\nγtρitR Hi t\n− ∞∑\nt=0\nγt ( ρitq̂ πe ( SHit , A Hi t ) − ρit−1v̂πe ( SHit )) .\nFirst, notice that ∑∞\nt=0 γ tρHit R Hi t is the per-decision im-\nportance sampling (PDIS) estimator, which is known to be an unbiased estimator of v(πe) (Precup et al., 2000; Thomas, 2015b). So, we need only show that the remaining terms in the definition of DRi(D) have expected value zero, i.e., that\nE\n[ ∞∑\nt=0\nγtρitq̂ πe ( SHit , A Hi t )] = E [ ∞∑\nt=0\nγtρit−1v̂ πe ( SHit )] .\nBy Corollary 2 (which requires Assumption 1) we have that\nE\n[ ∞∑\nt=0\nγtρit−1v̂ πe ( SHit\n)]\n=E\n[ ∞∑\nt=0\nγtρit−1 πe\n( AHit |SHit )\nπi ( AHit |SHit\n) q̂πe ( SHit , A Hi t\n)]\n=E\n[ ∞∑\nt=0\nγtρitq̂ πe ( SHit , A Hi t )] .\nFor completeness, next we show formally the obvious result that Assumption 1 implies that partial trajectories that occur under the evaluation policy must occur under the behavior policy.\nLemma 8. Assumption 1 implies that if Pr(Ht=ht|πi) = 0, then Pr(Ht = ht|πe) = 0 for all i ∈ {1, . . . , n}, ht := (s0, a0, r0, s1, . . . , st−1, at−1, rt−1, st) ∈ Ht, and 0 ≤ t <∞.\nProof. If t = 0 then ht = (s0), which does not depend on the policy, so clearly if Pr(H0 = h0|πi) = 0 then Pr(H0 = h0|πe) = 0. Hereafter we assume 1 ≤ t < ∞. Notice that for any π ∈ Π,\nPr(Ht=ht|π) (a) =Pr(S0=s0) Pr(A0=a0|S0 = s0, π)\n× ( t−1∏\ni=1\nPr(Si=si|Si−1 = si−1, Ai−1 = ai−1)\n× Pr(Ri−1=ri−1|Si−1=si−1, Ai−1=ai−1, Si=si) × Pr(Ai=ai|Si=si, π) )\n× Pr(St=st|St−1 = st−1, At−1 = at−1) × Pr(Rt−1 = rt−1|St−1 = st−1, At−1 = at−1, St = st)\n(b) =d0(s0)π(a0|s0)P (st|st−1, at−1)R(rt−1|st−1, at−1, st)\n× t−1∏\ni=1\nP (si|si−1ai−1)R(ri−1|si−1, ai−1, si)π(ai|si).\nwhere (a) comes from repeated application of the rule that, for any random variables X and Y , Pr(X = x, Y = y) = Pr(X =x) Pr(Y = y|X =x) and the Markov property for state transitions, actions, and rewards, and (b) comes from the definitions of d0, π, R and P in MDPNv1.\nSo, if Pr(Ht = ht|πi) = 0, then one of the terms in the product above (using πi for π) must be zero. If that term is not a πi term, then it also shows up in Pr(Ht = ht|πe), and so Pr(Ht = ht|πe) = 0. If the term is a πi term, then by Assumption 1, the corresponding πe term must also be zero, and so Pr(Ht = ht|πi) = 0.\nNext, recall the known result that the ratio of partial trajectory probabilities under two different policies can be written in terms of the two policies:\nLemma 9. Let πe and πb be any two policies and t ∈ N>0. Let ht be any history of length t that has non-zero probability under πb, i.e., Pr(Ht=ht|πb) 6= 0. Then\nPr(Ht=ht|πe) Pr(Ht=ht|πb) = t−1∏\ni=0\nπe (ai|si) πb (ai|si) .\nProof. See the works of (Precup et al., 2000) or (Thomas, 2015b, Lemma 1).\nNext we establish Lemma 10, which states that we can use importance sampling to generate unbiased estimates of any function of partial trajectories in D. Recall that whenever\nwe writeHi (orHti ) we always mean a trajectory generated by πi, so Hi ∼ πi. Lemma 10. If Assumption 1 holds, then for all (t, i) ∈ N≥−1 × {1, . . . , n}:\nE[ρitf(H t+1 i )] = E[f(H t+1)|Ht+1 ∼ πe],\nfor any real-valued function f .\nProof. If t = −1 then Ht−1 = (S0), which does not depend on the policy, so the result is immediate. If t ≥ 0:\nE[ρitf(H t+1 i )] =E\n  t∏\nj=0\nπe ( AHij |SHij ) πi ( AHij |SHij ) f(Ht+1i )  \n(a) =E\n[ Pr ( Ht+1i = h t+1 i ∣∣πe )\nPr ( Ht+1i = h t+1 i ∣∣πi ) f(Ht+1i )\n]\n= ∑\nsupp(πi,t+1)\nPr ( Ht+1 = ht+1 ∣∣πi )\n× Pr ( Ht+1 = ht+1 ∣∣πe )\nPr (Ht+1 = ht+1|πi) f(Ht+1)\n= ∑\nsupp(πi,t+1)\nPr ( Ht+1 = ht+1 ∣∣πe ) f(Ht+1)\n(b) = ∑\nsupp(πe,t+1)\nPr ( Ht+1 = ht+1 ∣∣πe ) f(Ht+1)\n=E[f(Ht+1)|Ht+1 ∼ πe],\nwhere (a) comes from Lemma 9 and (b) comes from Lemma 8, which requires Assumption 1.\nWe can use Lemma 10 to show the well-known result that the expected value of an importance weight is one:\nLemma 11. For all πi and t ∈ N≥−1, if Assumption 1 holds, then E[ρit] = 1.\nProof. This follows from Lemma 10 with f(Ht+1) := 1.\nNext we establish a lemma that will be crucial to showing that the WDR estimator is strongly consistent:\nLemma 12. For all t ∈ N≥0, let ft : Ht+1 → R. If Assumption 1 holds, ft = 0 for all t ∈ N≥L, and either: • Case 1: Assumptions 2 and 3 hold.\nor • Case 2: Assumption 4 holds and there is a finite fmax such that for all t ∈ N≥0 and ht+1 ∈ Ht+1, |ft(ht+1)| < fmax.\nthen ∞∑\nt=0\nγt n∑\ni=1 ρit∑n j=1 ρ j t ft(H t+1 i ) (6)\na.s.−→ E [ ∞∑\nt=0\nγtft(H t+1) ∣∣∣∣∣H ∼ πe ] .\nProof. Let\nXtn := n∑\ni=1 ρit∑n j=1 ρ j t γtft(H t+1 i ),\nso that the left side of (6) can be written as ∑∞\nt=0X t n. First\nwe multiply the numerator and denominator ofXtn by 1 n to get:\nXtn = 1 n\n∑n i=1 γ tρitft(H t+1 i )\n1 n ∑n i=1 ρ i t\n. (7)\nWe will show that the numerator of (7) converges almost surely to the desired value:\n1\nn\nn∑\ni=1\nρitγ tft(H t+1 i ) a.s.−→ E[γtft(Ht+1)|Ht+1 ∼ πe].\n(8) By Lemma 10, which relies on Assumption 1, we have that E[ρitγ tft(H t+1 i )] = E[γ tft(H t+1)|Ht+1 ∼ πe]. Consider the two cases from the statement of the lemma:\n1. Case 1: Ht+1i is independent and identically distributed for all i, so ρitγ tft(H t+1 i ) is also indepen-\ndent and identically distributed for all i. Therefore by Khintchine’s strong law of large numbers, Theorem 6, we have (8). 2. Case 2: Ht+1i are not necessarily identically distributed since there may be multiple behavior policies, so we cannot directly apply Khintchine’s strong law of large numbers. Instead notice that ρit is bounded by β due to Assumption 4, and so |ρitγtft(Ht+1i )| ≤ βγtfmax. So, we can apply Kolmogorov’s strong law of large numbers, Corollary 1, to get (8).\nNext we show that the denominator of (7) converges almost surely to one:\n1\nn\nn∑\ni=1\nρit a.s.−→ 1. (9)\nBy Lemma 11, which relies on Assumption 1, we have that E[ρit] = 1. Again consider the two possible settings:\n1. Case 1: Ht+1i is independent and identically distributed for all i, so ρit is also independent and identically distributed for all i. Therefore by Khintchine’s strong law of large numbers we have (9). 2. Case 2: Since ρit ≤ β, we can apply Kolmogorov’s strong law of large numbers to get (9).\nBy applying Property 2 to (8) and (9) we have that for all t, Xtn a.s.−→ E [ γtft(H t+1) ∣∣Ht+1 ∼ πe ] . So,\n1. Case 1: Since Xtn = 0 for t ≥ L and by Property 3, ∞∑\nt=0\nXtn = L−1∑\nt=0\nXtn\na.s.−→E [ L−1∑\nt=0\nγtft(H t+1) ∣∣∣∣∣H t+1 ∼ πe ]\n=E\n[ ∞∑\nt=0\nγtft(H t+1) ∣∣∣∣∣H ∼ πe ] .\n2. Case 2: In order to apply Property 4 we must show that there exists a function g : N≥0 → R such that∑∞\nt=0 g(t) < ∞ and for all n ∈ N>0 and t ∈ N≥0, |Xtn| ≤ g(t). The following definition of g satisfies these requirements:\ng(t) := { γtfmax if t < L, 0 otherwise.\nThat is,\n∞∑\nt=0\ng(t) ≤ {\nfmax 1−γ if γ < 1, Lfmax otherwise,\n<∞,\nsince we have assumed that γ can only be 1 in the finite-horizon setting, where L 6= ∞. Also, |Xtn| = 0 = g(t) by definition if t ≥ L and if t < L then:\n|Xtn| := ∣∣∣∣∣ n∑\ni=1 ρit∑n j=1 ρ j t γtft(H t+1 i )\n∣∣∣∣∣\n≤γtfmax n∑\ni=1 ρit∑n j=1 ρ j t\n=γtfmax\n=g(t).\nSo, by Property 4, we have (6).\nFinally, we establish an extension of Lemma 12 that will facilitate its use with sequences that are not quite in the form that it is defined for:\nLemma 13. For all t ∈ N≥0, let ft : Ht → R. If Assumption 1 holds, ft = 0 for all t ∈ N≥L, and either: • Case 1: Assumptions 2 and 3 hold. or\n• Case 2: Assumption 4 holds and there is a finite fmax such that for all t ∈ N≥0 and ht ∈ Ht, |ft(ht)| < fmax.\nthen\n∞∑\nt=0\nγt n∑\ni=1 ρit−1∑n j=1 ρ j t−1 ft(H t i )\na.s.−→ E [ ∞∑\nt=0\nγtft(H t) ∣∣∣∣∣H ∼ πe ] .\n(10)\nProof. By removing the first term of the sum and shifting the variable that the sum uses by one, we can rewrite the left side of (10) as\n1\nn\nn∑\ni=1\nf0(H 0 i ) +\n∞∑\nt=0\nγt n∑\ni=1 ρit∑n j=1 ρ j t γft+1(H t+1 i ).\nWe have that\n1\nn\nn∑\ni=1\nf0(H 0 i ) a.s.−→ E[f0(H0)], (11)\nby Khintchine’s strong law of large numbers in Case 1, and Kolmogorov’s strong law of large numbers in Case 2 (since f0 is bounded). Also, by Lemma 12 (where the definition of ft+1 in this lemma is used for ft in our application of Lemma 12) we have that\n∞∑\nt=0\nγt n∑\ni=1 ρit∑n j=1 ρ j t γft+1(H t+1 i ) a.s.−→ E [ ∞∑\nt=0\nγt+1ft+1(H t+1) ∣∣∣∣∣H ∼ πe ] . (12)\nSo by applying Property 3 to (11) and (12) we have:\n1\nn\nn∑\ni=1\nf0(H 0 i ) +\n∞∑\nt=0\nγt n∑\ni=1 ρit∑n j=1 ρ j t γft+1(H t+1 i )\na.s.−→E[f0(H0)] +E [ ∞∑\nt=0\nγt+1ft+1(H t+1) ∣∣∣∣∣H ∼ πe ]\n= 0∑\nt=0\nE[γtft(H t)] +E\n[ ∞∑\nt=1\nγtft(H t) ∣∣∣∣∣H ∼ πe ]\n=E\n[ ∞∑\nt=0\nγtft(H t) ∣∣∣∣∣H ∼ πe ] ."
    }, {
      "heading" : "B. Doubly Robust Derivation and Proofs",
      "text" : "In this appendix we provide an alternate derivation of the DR estimator using control variates. The idea behind control variates is as follows. Suppose that we would like to estimate θ := E[X] given a sample of X . The obvious estimator would be θ̂1 := X . However, if we have a sample of another random variable, Y , with known expected value, E[Y ], then the estimator θ̂2 := X − Y + E[Y ] may have\nlower variance. Specifically, while Var(θ̂1) = Var(X), we have thatVar(θ̂2) = Var(X)+Var(Y )−2Cov(X,Y ). So, θ̂2 has lower variance than θ̂1 if 2Cov(X,Y ) > Var(Y ). Often Y is referred to as the control variate. Notice that the optimal control variate is Y := X , since then Var(θ̂2) = 0. Furthermore, notice that θ̂2 remains an unbiased estimator of θ as long as the expected value of Y exists—E[θ̂2] = E[X − Y +E[Y ]] = E[X]−E[Y ] +E[Y ] = E[X] = θ. Control variates have been used before in reinforcement learning to reduce the variance of policy gradient estimates (Bhatnagar et al., 2009), where the control variate was referred to as a baseline.\nRecall that we have defined the DR estimator in (2) as\nDR(D) := n∑\ni=1\n∞∑\nt=0\nγtwitR Hi t\n︸ ︷︷ ︸ X\n− n∑\ni=1\n∞∑\nt=0\nγt ( witq̂ πe ( SHit , A Hi t ) − wit−1v̂πe ( SHit ))\n︸ ︷︷ ︸ Y\n.\nIn this definition theX term is the per-decision importance sampling (PDIS) estimator, which is known to be an unbiased and strongly consistent estimator of v(πe) (Precup et al., 2000; Thomas, 2015b). Also, the control variate, Y , is mean zero, i.e., E[Y ] = 0. To see why this control variate is reasonable, notice that all of the terms that are multiplied by γtwit approximately cancel:\nq̂πe ( SHit , A Hi t ) ≈ RHit + γv̂πe ( SHit+1 ) .\nSo, Y is a decent approximation of X , and therefore DR(D) will have low variance.\nOur derivation of the control variate used by the DR estimator is based on an alternate view of control variates. If we do not know the expected value of the control variate, Y , but we have another random variable, Z, such that E[Z] = E[Y ], then we can use the unbiased estimator θ̂3 = X − Y + Z. The variance of this estimator is given by Var(θ̂3) = Var(X)+Var(Y −Z)−2Cov(X,Y −Z). So, if Y ≈ X and Z has low variance, then this estimator may have lower variance than θ1. Technically, this is an ordinary application of control variates using Y −Z as the mean-zero control variate. We derive DR using this alternate view.\nWe begin with the per-decision importance sampling (PDIS) estimator, which is known to be an unbiased and strongly consistent estimator of v(πe) (Precup et al., 2000;\nThomas, 2015b). The PDIS estimator is given by:\nPDIS(D) := 1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tRHit .\nIn order to reduce the variance of this estimator we will subtract a control variate that we expect to be highly correlated with the PDIS estimator, and then add back in the expected value of the control variate:\n1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tRHit\n︸ ︷︷ ︸ PDIS estimator,X\n− 1 n\nn∑\ni=1\n∞∑\nt=0\nρitγ tr̂πe(SHit , A Hi t , 0)\n︸ ︷︷ ︸ control variate, Y\n+E\n[ 1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tr̂πe(SHit , A Hi t , 0)\n]\n︸ ︷︷ ︸ E[control variate]=E[Y ]\n. (13)\nHere we expect the control variate to be similar to the PDIS estimator if the model’s reward predictions are accurate, i.e., if RHit ≈ r̂πe(SHit , AHit , 0). If it could be used, (13) would be an extremely lowvariance estimator of v(πe) sinceX − Y would usually be near-zero and E[Y ] is a constant that is near v(πe). However, E[control variate] is not known, and so we cannot use (13) directly. Although estimating E[Y ] is nearly as hard as estimating v(πe), it is marginally easier. It is easier because v(πe) uses the unknown transition and reward functions of the MDP to produce the distribution of rewards at each time step, while E[Y ] uses the known approximate model’s transition and reward function for the last transition before each reward occurs. We can therefore estimate E[Y ] using an unbiased estimator that typically has lower variance than the control variate. In the alternate view of control variates this new term will be Z:\n1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tRHit\n︸ ︷︷ ︸ PDIS estimator,X\n− 1 n\nn∑\ni=1\n∞∑\nt=0\nρitγ tr̂πe ( SHit , A Hi t , 0 )\n︸ ︷︷ ︸ control variate, Y\n+ 1\nn\nn∑\ni=1\n∞∑\nt=0\nρit−1γ tr̂πe ( SHit , 0 )\n︸ ︷︷ ︸ Z\n. (14)\nHere we expect the Z term to have lower variance than the Y term because for each i and t it only depends on actions AHi1 , . . . , A Hi t−1 and not A Hi t . This is reflected in its use of ρit−1 rather than ρ i t. Before continuing our derivation we\nverify that E[Y ] = E[Z] if Assumption 1 holds:\nE[Z] =E\n[ 1\nn\nn∑\ni=1\n∞∑\nt=0\nρit−1γ tr̂πe ( SHit , 0\n)]\n(a) =E   1 n n∑\ni=1\n∞∑\nt=0\nρit−1γ t πe ( AHit |SHit )\nπi ( AHit |SHit\n) r̂πe ( SHit , A Hi t , 0\n)  \n=E\n[ 1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tr̂πe ( SHit , A Hi t , 0\n)]\n=E[Y ],\nwhere (a) comes from Lemma 6.\nSo far, in (14), we have introduced a control variate into PDIS that we expect might reduce the variance of the estimator a little without introducing bias. However, it will still have high variance because Z is a high-variance estimator ofE[Y ]. To overcome this, we can introduce another control variate into Z to make it a lower-variance estimator of E[Y ]. So, we introduce another control variate:\n1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tRHit\n︸ ︷︷ ︸ X\n− 1 n\nn∑\ni=1\n∞∑\nt=0\nρitγ tr̂πe ( SHit , A Hi t , 0 )\n︸ ︷︷ ︸ Y\n+ 1\nn\nn∑\ni=1\n∞∑\nt=0\nρit−1γ tr̂πe ( SHit , 0 )\n︸ ︷︷ ︸ Z\n− 1 n\nn∑\ni=1\n∞∑\nt=0\nρit−1γ tr̂πe ( SHit−1, A Hi t−1, 1 )\n︸ ︷︷ ︸ new control variate,Y ′\n+ 1\nn\nn∑\ni=1\n∞∑\nt=0\nρit−2γ tr̂πe ( SHit−1, 1 )\n︸ ︷︷ ︸ Z′\n.\nHere E[Z ′] = E[Y ′] (although we omit to proof of this claim), Y ′ is similar to Z and so it serves as a good control variate therefor, and Z ′ will usually have lower variance than Y ′ because it uses ρit−2 rather than ρ i t−1. However, now Z ′ is a high-variance estimator ofE[Y ′]. We therefore introduce a control variate for Z ′, and this process repeats. This process of introducing control variates eventually terminates when the new control variate is not random. The resulting estimator is (we call this estimator DR(D) be-\ncause we will show that it is equivalent to (2)):\nDR(D) = 1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tRHit (15)\n− 1 n\nn∑\ni=1\n∞∑\nt=0\nγt t∑\nτ=0\nρiτ r̂ πe ( SHiτ , A Hi τ , t− τ )\n+ 1\nn\nn∑\ni=1\n∞∑\nt=0\nγt t∑\nτ=0\nρiτ−1r̂ πe ( SHiτ , t− τ ) .\nNext we will combine the r̂ terms into v̂ and q̂ terms to get a more succinct expression. To this end, we will use the property that ∑∞ i=0 ∑i j=0 f(i, j) = ∑∞ j=0 ∑∞ i=j f(i, j) to change the order of the sums over t and τ . We also split γt into γτγt−τ :\nDR(D) = 1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tRHit\n− 1 n\nn∑\ni=1\n∞∑\nτ=0\nρiτγ τ ∞∑\nt=τ\nγt−τ r̂πe ( SHiτ , A Hi τ , t− τ )\n+ 1\nn\nn∑\ni=1\n∞∑\nτ=0\nρiτ−1γ τ ∞∑\nt=τ\nγt−τ r̂πe ( SHiτ , t− τ ) .\nNext we perform a change of variable using j = t − τ to replace t:\nDR(D) = 1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tRHit\n− 1 n\nn∑\ni=1\n∞∑\nτ=0\nρiτγ τ ∞∑\nj=0\nγj r̂πe ( SHiτ , A Hi τ , j )\n+ 1\nn\nn∑\ni=1\n∞∑\nτ=0\nρiτ−1γ τ ∞∑\nj=0\nγj r̂πe ( SHiτ , j )\n= 1\nn\nn∑\ni=1\n∞∑\nt=0\nρitγ tRHit\n− 1 n\nn∑\ni=1\n∞∑\nτ=0\nρiτγ τ q̂πe ( SHiτ , A Hi τ )\n+ 1\nn\nn∑\ni=1\n∞∑\nτ=0\nρiτ−1γ τ v̂πe ( SHiτ ) .\nReplacing the variable τ with t and using wit = ρit n we get\nthat:\nDR(D) = n∑\ni=1\n∞∑\nt=0\nγtwitR Hi t\n− n∑\ni=1\n∞∑\nt=0\nγt ( witq̂ πe ( SHit , A Hi t ) − wit−1v̂πe ( SHit )) ,\nwhich is (2).\nThe original derivation of the DR estimator (Jiang & Li, 2015) required the horizon to be finite and known. Our derivation makes neither of these assumptions. That is, it allows for infinite or indefinite horizons and for finite horizons where the horizon is not known. If the horizon, L, is finite and known, then one should ensure that the model uses all of the available information, including the known horizon and time step. In the next section we show that if L is finite and known, then our non-recursive definition of the DR estimator is equivalent to the recursive form of (Jiang & Li, 2015)."
    }, {
      "heading" : "B.1. Equivalence of DR Definitions",
      "text" : "In this section we show that our non-recursive definition of the DR estimator is equivalent to the recursive definition provided by Jiang & Li (2015) when the horizon is finite and known. Theorem 8. (2) is equivalent to the DR estimator presented by Jiang & Li (2015) if the finite horizon, L, of the MDP is known.\nProof. Jiang & Li (2015) define theDR estimator for a single trajectory (i.e., n = 1) as the last element, XL, of a sequence, (Xi)Li=0. This sequence is defined by the following recurrence relation. LetX0 := 0 and for all k ∈ {1, . . . , L} let\nXk :=v̂ πe (SL−k) + πe (AL−k|SL−k) π1 (AL−k|SL−k)\n( RL−k + γXk−1\n− q̂πe (SL−k, AL−k) ) .\nAs in the definition of DR(D) in (2), Jiang & Li (2015) define the DR estimator for multiple trajectories to be the average of the estimator for each trajectory individually. So, to show that their recursive definition and our definition are equivalent, we need only show that they are equivalent when there is a single trajectory.\nSince hereafter in this proof we deal with only a single trajectory, we drop the superscripts that we use to specify the trajectory, i.e., we write ρt rather than ρ1t . Also let πb := π1 denote the single behavior policy. For further brevity, let\nπeb(t) := πe (At|St) πb (At|St) .\nFirst, notice that we can rewrite (2) for the single-trajectory finite-horizon setting as:\nDR(D) =\nL−1∑\nt=0\nγtρtRt − L−1∑\nt=0\nγtρtq̂ πe (St, At)\n+ L−1∑\nt=0\nγtρt−1v̂ πe (St)) , (16)\nsince SL is surely the absorbing state and so Rt, q̂πe (St, At), and v̂πe (St) are all zero for t ≥ L. To verify that this definition is equivalent to XL, we will define another sequence, (Yi)Li=1, such that Xi = Yi for all i ∈ {1, . . . , L} and such that YL = DR(D) trivially. Let\nYk :=\n∑L−1 t=L−k γ t [ ρt (Rt − q̂πe (St, At)) + ρt−1v̂πe (St) ]\nγL−kρL−k−1 .\nNotice that YL is identical to (16) since γL−LρL−L−1 = 1. So, all that remains is to show that Yk = Xk for all k ∈ {1, . . . , L}. We will show this using a proof by induction. For the base case, k = 1, it is straightforward to verify that X1 = Y1. For the inductive step we assume the inductive hypothesis that Xk−1 = Yk−1 and show that then Xk = Yk:\nXk :=v̂ πe (SL−k) + π e b(L− k) ( RL−k + γXk−1\n− q̂πe (SL−k, AL−k) )\n=v̂πe (SL−k) + π e b(L− k) ( RL−k + γYk−1\n− q̂πe (SL−k, AL−k) ) .\nSubstituting in the definition of Yk−1 and performing algebraic manipulations we have that:\nXk =v̂ πe (SL−k) + π e b(L− k)RL−k + πeb(L− k) γL−kρL−k\n× L−1∑ t=L−k+1 γt [ ρt (Rt − q̂πe (St, At)) + ρt−1v̂πe (St) ]\n− πeb(L− k)q̂πe (SL−k, AL−k) ,\nwhere× denotes that a line was split into multiple lines (we do not use cross-products anywhere in this paper). Since\nπeb(L− k) ρL−k = 1 ρL−k−1 ,\nand by reordering terms, we have that\nXk =π e b(L− k)(RL−k − q̂πe (SL−k, AL−k)) + v̂πe (SL−k)\n+\n∑L−1 t=L−k+1 γ t [ ρt (Rt − q̂πe (St, At)) + ρt−1v̂πe (St) ]\nγL−kρL−k−1 .\nAdding one more element to the summation so that it starts at t = L − k, and then explicitly subtracting off this additional term we have that:\nXk =π e b(L− k)(RL−k − q̂πe (SL−k, AL−k)) + v̂πe (SL−k)\n+\n∑L−1 t=L−k γ t [ ρt (Rt − q̂πe (St, At)) + ρt−1v̂πe (St) ]\nγL−kρL−k−1\n− γ L−k\nγL−kρL−k−1\n[ ρL−k (RL−k − q̂πe (SL−k, AL−k))\n+ ρL−k−1v̂ πe (SL−k) ] .\nCanceling several γ and ρ terms, we have that:\nXk =\n∑L−1 t=L−k γ t [ ρt (Rt − q̂πe (St, At)) + ρt−1v̂πe (St) ]\nγL−kρL−k−1\n=Yk."
    }, {
      "heading" : "B.2. DR is Unbiased",
      "text" : "While Jiang & Li (2015) showed that the DR estimator (with finite horizon) is an unbiased estimator of v(πe), in this section we show that the DR estimator (without assumptions about the horizon) is an unbiased estimator of v(πe). Theorem 9 (DR – unbiased estimator). If Assumption 1 holds, then E[DR(D)] = v(πe).\nProof. This result was shown previously for the known finite horizon setting (Jiang & Li, 2015), but has not been shown before for the other settings. Because we will use some steps of this proof in later proofs, the majority of this proof is relegated to a lemma.\nE [DR(D)] =E\n[ 1\nn\nn∑\ni=1\nDRi(D)\n]\n(a) = 1\nn\nn∑\ni=1\nv(πe)\n=v(πe),\nwhere (a) comes from Lemma 7."
    }, {
      "heading" : "B.3. Conditions for Consistency of DR",
      "text" : "In this section we show that the DR estimator is a strongly consistent estimator of v(πe) given mild technical assumptions and that there is only one behavior policy (Theorem 10) or that the importance weights are bounded (Theorem 11).\nTheorem 10 (DR – strongly consistent estimator for one behavior policy). If Assumptions 1 and 2 hold then DR(D) a.s.−→ v(πe).\nProof. This proof is a relatively straightforward application of the law of large numbers.\nWe have from Lemma 7 that E[DRi(D)] = v(πe) for all i ∈ {1, . . . , n}. By Assumption 2, {DRi(D)}ni=1 is a set of n independent and identically distributed random variables (sinceHi ∼ π1 for all i, andDRi(D) only depends onHi). We can therefore conclude by Khintchine’s strong law of large numbers, Theorem 6, that DR(D) a.s.−→ v(πe).\nTheorem 11 (DR – strongly consistent estimator for many behavior policies). If Assumptions 1 and 4 hold then DR(D) a.s.−→ v(πe).\nProof. We have from Lemma 7 that E[DRi(D)] = v(πe) for all i ∈ {1, . . . , n}. However, {DRi(D)}ni=1 is a set of n independent but not necessarily identically distributed random variables, so we cannot apply Khintchine’s strong law of large numbers. Instead, we will apply Kolmogorov’s strong law of large numbers, which requires each random variable, DRi(D), to be bounded.\nWe have that:\nDRi(D) = ∞∑\nt=0\nγtρitR Hi t −\n∞∑\nt=0\nγtρitq̂ πe ( SHit , A Hi t )\n+ ∞∑\nt=0\nγtρit−1v̂ πe ( SHit )\n=\n∞∑\nt=0\nγtρitR Hi t\n− ∞∑\nt=0\nγtρit\n∞∑\nτ=0\nγτ r̂πe ( SHit , A Hi t , τ )\n︸ ︷︷ ︸ =:q̂πe ( S Hi t ,A Hi t )\n+\n∞∑\nt=0\nγtρit−1\n∞∑\nτ=0\nγτ r̂πe ( SHit , τ )\n︸ ︷︷ ︸ =:v̂πe ( S Hi t )\n.\nSo,\n|DRi(D)| ≤3βr⋆max L∑\nt=0\nγt L∑\nτ=0\nγτ\n<∞,\nsince either L < ∞ or γ ∈ [0, 1). So, DRi(D) is bounded above and below and thus we can apply Kolmogorov’s strong law of large numbers (Corollary 1) to conclude that DR(D) a.s.−→ v(πe)."
    }, {
      "heading" : "C. Weighted Doubly Robust Proofs",
      "text" : ""
    }, {
      "heading" : "C.1. Proof of Theorem 1",
      "text" : "In this section we prove Theorem 1, which states that WDR(D) is a strongly consistent estimator of v(πe) if Assumptions 1, 2, and 3 hold.\nFirst, notice that we can rewrite the WDR estimator as:\nWDR(D) :=\n∞∑\nt=0\nγt n∑\ni=1 ρit∑n j=1 ρ j t RHit\n︸ ︷︷ ︸ =:CWPDIS(D)\n(17)\n− ∞∑\nt=0\nγt n∑\ni=1 ρit∑n j=1 ρ j t\nq̂πe ( SHit , A Hi t )\n︸ ︷︷ ︸ =:Xn\n+\n∞∑\nt=0\nγt n∑\ni=1 ρit−1∑n j=1 ρ j t−1\nv̂πe ( SHit )\n︸ ︷︷ ︸ =:Yn\n.\nWe have from Lemma 12 that\nCWPDIS(D) a.s.−→E\n[ ∞∑\nt=0\nγtRHt |H ∼ πe ]\n=v(πe), (18)\nwhich has been shown before (Thomas, 2015b, Theorem 13). Also by Lemma 12 we have that\nXn a.s.−→ E\n[ ∞∑\nt=0\nγtq̂πe ( SHt , A H t ) ∣∣∣H ∼ πe ] , (19)\nand by Lemma 13 we have that\nYn a.s.−→E\n[ ∞∑\nt=0\nγtv̂πe ( SHt ) ∣∣∣H ∼ πe ]\n=E [ ∞∑\nt=0\nγt ∞∑\nj=0\nγj r̂πe ( SHt , j )\n︸ ︷︷ ︸ =v̂πe(SHt )\n∣∣∣H ∼ πe ]\n=E [ ∞∑\nt=0\nγt ∞∑\nj=0\nγj ∑ a∈A πe ( a|SHt ) r̂πe ( SHt , a, j )\n︸ ︷︷ ︸ =r̂πe(SHt ,j)\n∣∣∣H ∼ πe ]\n=E [ ∞∑\nt=0\nγt ∞∑\nj=0\nγj r̂πe ( SHt , A H t , j )\n︸ ︷︷ ︸ =q̂πe (SHt ,A H t )\n∣∣∣H ∼ πe ]\n=E [ ∞∑\nt=0\nγtq̂πe ( SHt , A H t ) ∣∣∣H ∼ πe ] . (20)\nSo, by applying Property 3 to (18), (19), and (20) we have thatWDR(D) a.s.−→ v(πe)."
    }, {
      "heading" : "C.2. Proof of Theorem 2",
      "text" : "In this section we prove Theorem 2, which states that if Assumptions 1 and 4 hold then\nWDR(D) a.s.−→ v(πe).\nRecall that WDR can be defined as in (17). First we apply Lemma 12 to the CWPDIS(D) term, which uses ft(H t+1 i ) = R Hi t , which is bounded since |RHit | ≤ r⋆max. The result of Lemma 12 is that\nCWPDIS(D) a.s.−→E\n[ ∞∑\nt=0\nγtRHt |H ∼ πe ]\n=v(πe). (21)\nNext we apply Lemma 12 to the Xn term, which uses ft(H t+1 i ) = q̂ πe ( SHit , A Hi t ) , which is bounded since\n∣∣∣q̂πe ( SHit , A Hi t )∣∣∣ ≤ {\nr⋆max 1−γ if L =∞ Lrmax otherwise.\nThe result of applying Lemma 12 to Xn is that\nXn a.s.−→ E\n[ ∞∑\nt=0\nγtq̂πe ( SHt , A H t ) ∣∣∣H ∼ πe ] . (22)\nLastly, we apply Lemma 13 to the Yn term, which uses ft(H t i ) = v̂ πe ( SHit ) , which is bounded since\n∣∣∣v̂πe ( SHit )∣∣∣ ≤ {\nr⋆max (1−γ) if L =∞ Lr⋆max otherwise.\nThe result of applying Lemma 13 to Yn is that\nYn a.s.−→E\n[ ∞∑\nt=0\nγtv̂πe ( SHt ) ∣∣∣H ∼ πe ]\n(a) =E\n[ ∞∑\nt=0\nγtq̂πe ( SHt , A H t ) ∣∣∣H ∼ πe ] , (23)\nwhere (a) comes from the same derivation that was used in (20). So, by applying Property 3 to (21), (22), and (23) we have thatWDR(D) a.s.−→ v(πe)."
    }, {
      "heading" : "D. Extended Empirical Studies (WDR)",
      "text" : "In this section we provide a detailed description of our experiments comparing the WDR estimator to various importance sampling estimators (IS, PDIS, WIS, CWPDIS), as well as DR and AM. We performed experiments using three domains: ModelFail, ModelWin, and a gridworld. We will describe each domain, then describe the experimental setup, and then present empirical results. All three domains have a finite horizon and use γ = 1.0."
    }, {
      "heading" : "D.1. The ModelFail Domain",
      "text" : "The ModelFail domain was constructed so that the model would fail to converge to the true MDP. One way that this can happen is if the model uses function approximation, so that it cannot represent the true MDP. Another way that this can happen is if there is some partial observability, which is common in real applications. We therefore construct a domain where the true underlying MDP has three states (plus the terminal absorbing state), but where the agent cannot tell the difference between any of the states.\nThe MDP used by ModelFail is depicted in Figure 3. Although the MDP has three states (denoted by circles) plus the terminal absorbing state (denoted by the double-circle), the agent does not observe which state it is in—it only sees a single state. The agent begins in the left-most state, where it has two actions available. The first action always takes it to the upper state, while the second always takes in to the lower state. In both cases, the agent receives no reward.\nAt time t = 1, the agent is always in the upper or lower state (although it cannot tell the difference between them and the initial state), and it must select between two possible actions. Both actions always have the same effect—the agent transitions to the terminal absorbing state. However, if the agent was in the upper state, R1 = 1, whileR1 = −1 if the agent was in the lower state. The horizon is L = 2 since S2 = ∞ s always.\nThe behavior policy selects a1 with probability approximately 0.88 and a2 with probability approximately 0.12 (these probabilities were chosen arbitrarily by using weights of 1 and −1 with softmax action selection,\nand were not optimized). The evaluation policy does the opposite—it selects a1 with probability approximately 0.12 and a2 with probability approximately 0.88.\nConsider what happens when we try to model this MDP based on the observations produced by running the behavior policy to produce an infinite number of trajectories (without trying to infer anything about the true underlying structure of the MDP). Recall that we observe only a single state. First consider the transition dynamics: half of the time either action causes a transition back to the single state, while half of the time the agent transitions to the absorbing state. Next consider the rewards: half of the time the agent receives no reward, with probability 0.88/2 it receives a reward of 1, and with probability 0.12/2 it receives a reward of −1, and these rewards appear completely uncorrelated with the action that was selected (since non-zero rewards occur at time t = 1 and A1 has no bearing on rewards or state transitions). So, from the model’s point of view, the actions have no impact on state transitions or rewards, and so every policy is equally good and will produce an expected return of 0.38, while in reality an optimal policy will produce an expected return of 0.5 and a pessimal policy will produce an expected return of −0.5. We provided the model with the true horizon, L = 2, so that its predictions of Rt are zero for t ≥ 2."
    }, {
      "heading" : "D.2. The ModelWin Domain",
      "text" : "This domain was constructed so that the approximate model of the MDP would quickly converge to the true MDP, while importance sampling based approaches like DR and WDR would continue to have high variance. Recall from our discussion in Section 6 that DR and WDR will be equal to a simple model-based approach if the approximate MDP is perfect and state transition and rewards are deterministic. To avoid this, the ModelWin domain has stochastic state transitions that cause the (b) term in (3) to not necessarily be zero.\nThe ModelWin MDP is depicted in Figure 4. Unlike the ModelFail domain, the agent observes the true underlying states of the ModelWin MDP, of which there are three, plus a terminal absorbing state (not pictured). The agent always begins in s1, where it must select between two actions. The\nfirst action, a1, causes the agent to transition to s2 with probability 0.4 and s3 with probability 0.6. The second action, a2, does the opposite: the agent transitions to s2 with probability 0.6 and s3 with probability 0.4. If the agent transitions to s2, then it receives a reward of 1, and if it transitions to s3 it receives a reward of −1. In states s2 and s3, the agent has two possible actions, but both always produce a reward of zero and a deterministic transition back to s1. The horizon is set to L = 20, so, S20 = ∞ s always.12\nTo see why DR and WDR struggle on this domain, consider what happens if the approximate model is perfect and the agent takes action a1 in state s1. In our discussion of (3) we concluded that DR and WDR will perform well if R1 = q\nπe(s1, a1) − γv̂πe(S′), where S′ is the state that the agent transitions to after taking action a1 in state s1, which is a random variable. Consider the two values that the right side can take, depending on whether S′ = s2 or S′ = s3. It can be either q̂πe(s1, a1) − γv̂πe(s2) or q̂πe(s1, a1) − γv̂πe(s3). Since v̂πe(s2) = v̂πe(s3), these two statements are equal—the prediction of R1 will be the same regardless of whether the agent transitions to s2 or s3, and so its prediction must sometimes be wrong (since the rewards differ depending on whether the agent transitions to s2 or s3). So, term (b) in (3) will not be zero—the control variate used by DR and WDR does not perfectly cancel with the PDIS (or CWPDIS) term. If wit is large, then this will produce high variance. In order to make wit large, we need only make the horizon long and the behavior and evaluation policies dissimilar.\nThe behavior and evaluation policies both select actions uniformly randomly in states s2 and s3. However, in s1 the behavior policy takes action a1 with probability approximately 0.73 and action a2 with probability approximately 0.27, while the evaluation policy does the opposite—it takes action a1 with probability approximately 0.27 and action a2 with probability approximately 0.73 (these probabilities come from using softmax action selection with weights of 1 and 0).\n12Technically, implementing the horizon of L = 20 requires the states to be augmented to include the current time step so that state transitions are Markovian. The approximate model is provided with the time step and the horizon.\nAs in the ModelFail domain, for the ModelWin domain we provided the approximate model with the true horizon of the MDP, L = 20, so that its predictions of Rt were zero for t ≥ 20."
    }, {
      "heading" : "D.3. The Gridworld Domain",
      "text" : "The third domain that we used was the gridworld domain developed by Thomas (2015b, Section 2.5) for evaluating OPE algorithms. It is a 4 × 4 gridworld with four actions, L = 100, and deterministic transition and reward functions. This domain was developed specifically for evaluating different OPE methods. Thomas (2015b) proposed five policies, π1, . . . , π5, that can serve as the behavior and evaluation policies.\nAlthough this setup was developed for evaluating OPE methods, it was not developed with DR and WDR in mind (since they were introduced later). Specifically, its use of deterministic state-transition and reward functions means that when the model is accurate, AM, DR, and WDR will all perform similarly (due the the (b) term in (3) being nearzero).\nWe therefore performed experiments with two variants of this gridworld. In the first variant the approximate model was provided with the horizon, L = 100. However, in the second variant we introduced some partial observability by providing the model with the incorrect horizon: L = 101. This has a significant impact for value predictions close to the end of a trajectory because the model incorrectly predicts when the rewards will necessarily be zero. We write Gridworld-TH and Gridworld-FH to denote the gridworld where the agent is provided with the true horizon and false horizon, respectively."
    }, {
      "heading" : "D.4. Experimental Setup",
      "text" : "For each domain we generated n trajectories (for various n) and computed the sample mean squared error between the predictions of the various OPE methods and the true performance of the evaluation policy (estimated using a large number of on-policy Monte-Carlo rollouts). For each value of n and each OPE algorithm, we performed this experiment 128 times and report the average sample mean squared error over these 128 trials. All plots include standard error bars and use logarithmic scales for both the horizontal and vertical axes.\nPerhaps surprisingly, it is not obvious how to fairly compare the different OPE algorithms. Clearly IS, PDIS, WIS, and CWPDIS should use all of the trajectories in D, since they do not require an approximate model. Similarly, AM should use all of the data to construct an approximate model. However, how should the available data be split for DR, WDR, and the MAGIC estimators? We believe that\nthere are at least three reasonable answers:\n1. DR, WDR, and MAGIC should be provided with additional trajectories not available to IS, PDIS, WIS, and CWPDIS, and these trajectories should be used to construct an approximate model. This setup would emulate the setting where prior domain knowledge (not necessarily trajectories) can be used to construct an approximate model, which IS, PDIS, WIS, and CWPDIS ignore. 2. DR, WDR, and MAGIC should use all of the available data, D, to construct an approximate model. They should then reuse this same data to compute their estimates. This approach is reasonable, but the reuse of data invalidates our theoretical guarantees. Still, empirically we find that this approach causes DR, WDR, and MAGIC to perform at their best. 3. DR, WDR, and MAGIC should partition D into two sets. The first set should be used to construct the approximate model, and the second set should be used to compute the DR, WDR, and MAGIC estimates using the approximate model.\nSince there is not necessarily a “correct” answer to which way of performing experiments is best, we show our results using both the second and third approach. For each domain, the “full-data” variant uses the second approach while the “half-data” variant uses the third approach, whereD is partitioned into two sets of equal size.\nSince all of the domains that we use have finite state and action sets, we use a simple maximum-likelihood approximate model. That is, we predict that the probability of transitioning from s to s′ given action a is the number of times this transition was observed divided by the number of times action a was taken in state s. If D contains no examples of action a being taken in state s, then we assume that taking action a in state s always causes a transition to the terminal absorbing state.\nIn this appendix, we present empirical results from four previous importance sampling methods, definitions of which can be found in the work of Thomas (2015b, Chapter 3): importance sampling (IS), per-decision importance sampling (PDIS), weighted importance sampling (WIS), and consistent weighted per-decision importance sampling (CWPDIS). We also show results for the guided importance sampling methods DR and WDR and the purely modelbased method, AM. The legend used by all of the plots in this appendix is provided in Figure 5."
    }, {
      "heading" : "D.5. ModelFail Results",
      "text" : "Figure 1b in Section 6 depicts the result on the ModelFail domain in the full-data setting. We reproduce this plot in Figure 6. Here the weighted importance sampling methods, WIS and CWPDIS, are obscured by the curve for WDR, while the unweighted importance sampling methods, IS and PDIS, are obscured by the curve for DR. Notice that WDR outperforms AM by orders of magnitude and DR by approximately an order of magnitude. Also notice that even though the approximate model is not accurate, which means that the control variates used by DR and WDR may be poor, the DR andWDR estimators do not perform worse than PDIS and CWPDIS, respectively.\nIn Figure 7 we reproduce this experiment in the half-data setting. Since AM does not use any data for importance sampling, in both settings (half-data and full-data) it is identical. Similarly, IS, PDIS, WIS, and CWPDIS do not use an approximate model, so they always use all of the data and are therefore also identical in both settings. However, DR and WDR are not the same—they use half of the data to construct the approximate model and the other half to compute their estimates. This means that, for DR and WDR, the approximate model tends to be worse, and the importance sampling estimate also tends to be worse. As a result, the DR and WDR curves are shifted up slightly. Still, the same general trends are evident—WDR outperforms AM by orders and DR by an order."
    }, {
      "heading" : "D.6. ModelWin Results",
      "text" : "Figure 1c in Section 6 depicts the result of running importance sampling and guided importance sampling methods as well as the approximate model estimator on the ModelWin experimental setup in the full-data setting. We reproduce this plot in Figure 8. Here AM has approximately an order of magnitude lower MSE than all of the other methods, including WDR, and was our motivation for combining AM and WDR using BIM.\nIn Figure 9 we reproduce this experiment in the half-data setting. As with the ModelWin setup, this only hurts DR and WDR. When there are few trajectories, it appears to impact DR more than WDR, although this may be due to noise (notice the large standard error bars on the DR curve when n is small."
    }, {
      "heading" : "D.7. Gridworld Results",
      "text" : "Figure 1a in Section 6 depicts the results of using the fourth gridworld policy, π4, as the behavior policy and the fifth, π5, as the evaluation policy for the Gridworld-FH domain in the full-data setting. We reproduce it in Figure 10. Notice that WDR outperforms all other methods by at least an order of magnitude.\nIn Figure 11 we reproduce this experiment in the half-data setting. As before there is little change, except that the DR and WDR curves shift up. WDR remains the bestperforming estimator, by approximately an order of magnitude.\nNext we reproduced Figures 10 and 11 for Gridworld-TH as opposed to Gridworld-FH. The results are in Figures 12 and 13 respectively. Notice that, when given the true horizon, AM excels. In the full-data setting DR and WDR both lie directly on top of the curve for AM. This makes sense because the transition function and reward function are deterministic, and so, given the way that we constructed our approximate model, both methods degenerate to exactly AM. In the half-data setting DR and WDR lag slightly behind the curve for AM since they can only use half as much data.\nNext we reproduced these four figures using the first gridworld policy, π1, as the behavior policy and the second, π2, as the evaluation policy. Whereas π4 and π5 are nearly deterministic and produce long trajectories, π1 and π2 are far from deterministic and tend to produce shorter trajectories. Notably, the behavior policy, π1, selects actions uniformly randomly, and so this presents a very different setting for OPE. The results are provided in Figures 14–17. In this example, DR and WDR perform similarly—significantly better than the importance sampling algorithms IS, PDIS, WIS, and CWPDIS, and marginally better than AM given enough data. Also, when the true horizon is provided to the model, DR and WDR again degenerate to AM."
    }, {
      "heading" : "D.8. Summary",
      "text" : "The key takeaways from these experiments are that WDR tends to outperform the other importance sampling estimators, IS, PDIS, WIS, and CWPDIS, as well as the guided importance sampling method, DR. None of these methods achieved mean squared errors within an order of magnitude of WDR’s across all of our experiments. This shows the power of WDR as a guided importance sampling method.\nHowever, WDR did not always win—in the ModelFail setting, AM outperformed WDR by an order of magnitude. Similar results have been observed by others. For example, in the experiments of Jiang & Li (2015), AM tended to outperform DR (although they did not compare to WDR, since it had not yet been introduced). This motivated our introduction of the BIM estimator as a way to blend together WDR and AM.\nNotice that, if the transition function and reward function are deterministic and there is no partial observability (as in the gridworld experiments using the true horizon), then, given the way that we constructed our approximate model, DR andWDR degenerate to AM. This degeneration (which\nis not bad, but suggests that importance sampling methods are not necessary) would also not occur if the approximate model used function approximation.\nLastly, notice that DR and WDR performed better in the full-data setting than in the half-data setting. This suggests that, in practice, one should use all of the available data both to produce an approximate model and to compute the DR and WDR estimates. Even though this violates the assumptions used by our theoretical guarantees, this does not mean, for example, that MAGIC will not still be a strongly consistent estimator for the application at hand."
    }, {
      "heading" : "E. Consistency of BIM",
      "text" : "In this appendix we prove Theorem 3, which states that if Assumption 4 holds, there exists at least one j ∈ J such that g(j)(D) is a strongly consistent estimator of v(πe), and b̂n − bn a.s.−→ 0, and Ω̂n − Ωn a.s.−→ 0, then BIM(D, Ω̂n, b̂n)\na.s.−→ v(πe). We begin by showing that BIM converges almost surely to v(πe) if it were to use the true Ωn and bn, rather than estimates thereof. Let j⋆ ∈ J be an index such that g(j ⋆)(D) a.s.−→ v(πe), which exists by assumption. Let y ∈ ∆|J | be the weight vector that places a weight of one on g(j\n⋆)(D) and a weight of zero on the other returns, such that y⊺gJ (D) = g(j ⋆)(D) a.s.−→ v(πe). So, by Lemma 3 (which requires that g(j)(D) is uniformly bounded for all j ∈ J , which holds by Assumption 4 and the fact that rewards and reward predictions are bounded), we have that limn→∞MSE(y⊺g(D), v(πe)) = 0.\nRecall thatBIM(D,Ωn,bn) uses the weight vector, x⋆ that minimizes the MSE:\nx⋆ ∈ arg min x∈∆|J | MSE(x⊺gJ (D),Ωn,bn).\nSince y ∈ ∆|J |, we have that for all n\nMSE((x⋆)⊺gJ (D), v(πe)) ≤ MSE(y⊺gJ (D), v(πe)).\nSince limn→∞MSE(y⊺gJ (D), v(πe)) = 0 we have that\nlim n→∞\nMSE((x⋆)⊺gJ (D), v(πe)) ≤0,\nand since MSE is always greater than or equal to zero, we can replace the ≤ above with an equality. Since (x⋆)⊺gJ (D) = BIM(D,Ωn,bn) this can be rewritten as\nlim n→∞ MSE(BIM(D,Ωn,bn), v(πe)) = 0.\nBy Lemma 3 we have that this implies that BIM(D,Ωnbn)\na.s.−→ v(πe). So far we have shown that BIM, when using the true covariance matrix and bias vector, converges almost surely to\nv(πe). By Lemma 5 we can therefore conclude that if b̂n− bn\na.s.−→ 0 and Ω̂n − Ωn a.s.−→ 0, then BIM(D, Ω̂nb̂n) a.s.−→ v(πe).\nF. Derivation of g(j)(D) using WDR In this appendix we derive a reasonable definition for g(j)(D), the off-policy j-step return, when using WDR for the importance sampling estimator. We assume that the reader is familiar with our use of control variates in Appendix B. First, consider what control variate should be added to the j-step PDIS or CWPDIS estimator:\nn∑\ni=1\nj∑\nt=0\nγtwitR Hi t ,\nwhere the definition of wit determines whether this is PDIS or CWPDIS. Reproducing our arguments from Appendix B, we find that a reasonable definition for IS(j)(D) is similar to (15), but with the time index, t, summing only to t = j and using wit terms rather than ρ i t terms for generality:\nIS(j)(D) := n∑\ni=1\nj∑\nt=0\nwitγ tRHit\n− n∑\ni=1\nj∑\nt=0\nγt t∑\nτ=0\nwiτ r̂ πe ( SHiτ , A Hi τ , t− τ )\n+\nn∑\ni=1\nj∑\nt=0\nγt t∑\nτ=0\nwiτ−1r̂ πe ( SHiτ , t− τ ) .\nNotice that this definition is not equivalent to what one would get if (2) were modified only so that the sum goes from time t = 0 to t = j, since that definition would include reward predictions beyond Rj in v̂ and q̂ terms. Instead, this definition is equivalent to the definition of (2) if it were applied to a modified MDP where every episode terminates after Rj is produced.\nNext, consider the definition of AM(j)(D). We might use importance sampling to correct for the distribution of Sj , and the model to predict the remaining rewards:13\nAM(j)(D) =γj n∑\ni=1\nwij−1v̂ πe(SHij )\n=γj n∑\ni=1\nwij−1\n∞∑\nτ=0\nγτ r̂πe(SHij , τ).\n13This is just one possible definition of AM(j). We also experimented with a definition that is purely model based: AM(j)(D) := ∑ s∈S d̂0(s) ∑∞ t=j γ\ntr̂πe(s, t). Since this definition does not include any importance weights, it does not require an additional control variate. We found that this variant performed similarly to the definition that we present.\nNotice that AM(j) is not a purely model-based estimator if j ≥ 0 since it uses importance weights. Furthermore, this use of importance sampling can result in high variance. To partially mitigate this variance, we can introduce a control variate to get a new definition:\nAM(j)(D) =γj n∑\ni=1\nwij−1\n∞∑\nτ=0\nγτ r̂πe(SHij , τ)\n− γj n∑\ni=1\nwij−1\n∞∑\nτ=0\nγτ r̂πe(SHij−1, A Hi j−1, τ + 1)\n+ γj n∑\ni=1\nwij−2\n∞∑\nτ=0\nγτ r̂πe(SHij−1, τ + 1).\nAs in our derivation of the DR estimator in Appendix B, we can repeat this process by continuing to add control variates until the control variate is not random to get our final definition of AM(j)(D):\nAM(j)(D) :=γj n∑\ni=1\nwij−1\n∞∑\nτ=0\nγτ r̂πe(SHij , τ)\n− γj j∑\nk=1\nn∑\ni=1\nwij−k\n∞∑\nτ=0\nγτ r̂πe(SHij−k, A Hi j−k, τ + k)\n+ γj j∑\nk=1\nn∑\ni=1\nwij−k−1\n∞∑\nτ=0\nγτ r̂πe(SHij−k, τ + k).\nCombining the IS and AM definitions to produce a offpolicy j-step return as defined in (4) we have:\ng(j)(D) := IS(j)(D) + AM(j+1)(D)\n= n∑\ni=1\nj∑\nt=0\nwitγ tRHit + γ\nj+1 n∑\ni=1\nwij\n∞∑\nτ=0\nr̂πe(SHij+1, τ)\n− n∑\ni=1\nj∑\nt=0\nγt t∑\nτ=0\nwiτ r̂ πe ( SHiτ , A Hi τ , t− τ )\n︸ ︷︷ ︸ (a)\n+ n∑\ni=1\nj∑\nt=0\nγt t∑\nτ=0\nwiτ−1r̂ πe ( SHiτ , t− τ )\n︸ ︷︷ ︸ (b)\n− γj+1 j+1∑\nk=1\nn∑\ni=1\nwij+1−k\n∞∑\nτ=0\nγτ r̂πe(SHij+1−k, A Hi j+1−k, τ + k)\n︸ ︷︷ ︸ (c)\n+ γj+1 j+1∑\nk=1\nn∑\ni=1\nwij−k\n∞∑\nτ=0\nγτ r̂πe(SHij+1−k, τ + k)\n︸ ︷︷ ︸ (d)\n.\nNotice that the terms (a) and (b) use predictions of rewards up until and including Rj , while the terms (c) and (d) use predictions of rewards beginning with Rj+1 and going to infinity. So, with algebraic manipulations we can combine (a) and (c) to get\nn∑\ni=1\nj∑\nt=0\nγtwitq̂ πe ( SHit , A Hi t )\nand we can combine (b) and (d) to get:\nn∑\ni=1\nj∑\nt=0\nγtwit−1v̂ πe ( SHit ) .\nSo, we have that\ng(j)(D) := n∑\ni=1\nj∑\nt=0\nγtwitR Hi t +\nn∑\ni=1\nγj+1wij v̂ πe(SHij+1)\n− n∑\ni=1\nj∑\nt=0\nγt ( witq̂ πe ( SHit , A Hi t ) − wit−1v̂πe ( SHit )) ."
    }, {
      "heading" : "G. MAGIC Details",
      "text" : "In this section we provide additional details about the MAGIC algorithm. Specifically, we describe exactly how we estimate Ωn and bn before presenting pseudocode for MAGIC."
    }, {
      "heading" : "G.1. Estimating Ωn",
      "text" : "We can write g(j)(D) as the sum of n terms:\ng(j)(D) = n∑\ni=1\ng (j) i (D), (24)\nwhere\ng (j) i (D) :=\n( j∑\nt=0\nγtwitR Hi t ) + γj+1wij v̂ πe(SHij+1)\n− j∑\nt=0\nγt ( witq̂ πe ( SHit , A Hi t ) − wit−1v̂πe ( SHit )) .\nSo,\nCov(g(i)(D), g(j)(D)) =Cov\n( n∑\nk=1\ng (i) k (D),\nn∑\nk=1\ng (j) k (D)\n) .\nNotice that g(j)i (D) really is a function of all of D, not just Hi, since wit = ρ i t/ ∑n j=1 ρ j t . This means that, although the terms in the sum, ∑n\nk=1 g (i) k (D), are identically\ndistributed, they are not independent, due to their shared\nreliance on D. However, notice that the g(i)k (D) terms, for various k, become less dependent as n → ∞ because the only dependence of g(i)k (D) on trajectories other than Hk comes from the denominator of wit, which converges almost surely to n (we established this in our proofs that WDR is strongly consistent).\nWe therefore propose an approximation of Ωn that comes from the assumption that the g(i)k (D) terms, for various k, are independent:\nCov(g(i)(D), g(j)(D))\n= ∑\nk∈{1,...,n}\n∑\nl∈{1,...,n} Cov\n( g (i) k (D), g (j) l (D) )\n(a)≈ ∑\nk∈{1,...,n} Cov\n( g (i) k (D), g (j) k (D) )\n(b) =nCov ( g (i) (·) (D), g (j) (·) (D) ) ,\nwhere (a) comes from the assumption that g(i)k (D) and g (j) l (D) are independent for all i, j, k, and l where k 6= l, (b) comes from the assumption that they are identically distributed, and where g(i)(·) (D) uses (·) to denote that any subscript in {1, . . . , n} could be used since the random variables are independent and identically distributed.\nWe therefore approximateΩn using the sample covariance:\nΩ̂n(i, j) := n n− 1 n∑\nk=1\n( g (Ji) k (D)− ḡ (Ji) k (D) ) (25) × ( g (Jj) k (D)− ḡ (Jj) k (D) ) ,\nwhere\nḡ (Ji) k (D) :=\n1\nn\nn∑\nk=1\ng (Ji) k (D).\nThe above scheme for estimating Ωn is the one that we use in our pseudocode and experiments. However, we also experimented with bootstrap estimates ofΩn. They yielded similar performance at significantly higher computational cost."
    }, {
      "heading" : "G.2. Estimating bn",
      "text" : "As described previously, we use a confidence interval, CI(g(∞)(D), δ), when computing bn. We stated that the confidence interval that we use is a combination of the percentile bootstrap and the Chernoff-Hoeffding inequality. Specifically, we compute the confidence interval produced by both methods, and return the tighter of the two. In practice, this is nearly always the confidence interval produced by the percentile bootstrap, and so practical implementations of MAGIC may just use the percentile bootstrap. We include the loose Chernoff-Hoeffding bound be-\ncause it allows for easier theoretical analysis of the MAGIC algorithm."
    }, {
      "heading" : "G.3. Pseudocode",
      "text" : "Pseudocode for the MAGIC algorithm is provided in Algorithm 1. It takes as inputD, πe, and an approximate model, all of which are defined in Section 2. It also takes as input J , which is defined in Section 7, and a positive integer κ, that we have not defined previously. We use κ to denote the number of times the bootstrap algorithm should resample the trajectories. In our experiments we used κ = 200. In general, it should be made as large as possible given any runtime constraints. Other literature has suggested that it should be chosen to be approximately κ = 2000 (Efron & Tibshirani, 1993; Davison & Hinkley, 1997).\nLine 2 calls for the |J | × |J | matrix, Ω̂n, to be computed according to (25).\nLine 3 specifies that a structure,D, should be created. This structure will be used to store the bootstrap resamplings, such that Di is the ith resampling of D. That is, Di is a set of n trajectories and the behavior policies that generated them, sampled with replacement from D (this resampling is done on lines 4–6).\nLine 7 calls for the creation of a vector, v, to store the offpolicy j-step return for j = ∞ (recall that this is just the WDR estimator) for each bootstrap sample, sorted into ascending order. Lines 8 and 9 then compute the percentile bootstrap 10% confidence interval, [l, u], for the mean of g(∞)(D), which we ensure includes WDR(D). For our theoretical analysis, we add a line after this that sets\nl← max { l,WDR(D)− ξ √ ln(2/δ)\n2n\n} (26)\nand\nu← min { l,WDR(D) + ξ √ ln(2/δ)\n2n\n} , (27)\nwhere ξ is a bound on the range of g(i)(D). In practice, these lines almost never change the values of l and u and can be ignored.\nLines 10–12 then show how the bias vector can be computed from the already defined terms. Notice that the order of g(Jj)(D) and l or u does not matter since the bias term in the decomposition of mean squared error is squared. The order that we use facilitates a simple consistency proof for MAGIC. Given that the covariance matrix and bias vector have been approximated, Line 13 sets x to be the solution of a constrained quadratic program (in our experiments we solved this quadratic program using the Gurobi library). Finally, line 14 returns the weighted combination\nof the different off-policy j-step returns (recall that gJ (D) is defined in Section 7).\nAlgorithm 1MAGIC(D) 1: Input:\n• D: Historical data. • πe: Evaluation policy. • Approximate model that allows for computation of r̂πe(s, a, t). • J : The set of return lengths to consider. The first element, J1, should be −1 and the last, J|J |, should be∞.\n• κ: The number of bootstrap resamplings. 2: Compute Ω̂n according to (25). 3: Allocate D(·) so that for all i ∈ {1, . . . , κ}, Di can\nhold n trajectories. 4: for i = 1 to κ do 5: Load Di with n uniform random samples drawn from D with replacement. 6: end for 7: v = sort ( g(∞)(D(·)) )\n8: l← min {WDR(D),v (⌊0.05n⌋)} 9: u← max {WDR(D),v (⌈0.95n⌉)} 10: for j = 1 to |J | do 11:\nb̂n(j)←    g(Jj)(D)− u if g(Jj)(D) > u g(Jj)(D)− l if g(Jj)(D) < l 0 otherwise.\n12: end for 13: x← argminx∈∆|J | x⊺[Ω̂n + b̂nb̂⊺n]x 14: return x⊺gJ (D)"
    }, {
      "heading" : "H. Consistency of MAGIC",
      "text" : "In this section we prove Theorem 4, which states that if Assumptions 1 and 4 hold and∞ ∈ J , thenMAGIC(D) a.s.−→ v(πe). This result follows immediately from Theorem 3 if Ω̂n − Ωn a.s.−→ 0 and b̂n − bn a.s.−→ 0, since Assumptions 1 and 4 are sufficient to ensure that g(∞)(D) = WDR(D)\na.s.−→ v(πe). In Appendix H.3 we show that Ω̂n − Ωn a.s.−→ 0, and then in Appendix H.4 we show that b̂n − bn a.s.−→ 0. However, first we establish two useful properties of the off-policy j-step returns.\nH.1. Convergence of Off-Policy j-Step Return\nRecall that the off-policy j-step return used by MAGIC is given by:\ng(j)(D) := n∑\ni=1\nj∑\nt=0\nγtwitR Hi t +\nn∑\ni=1\nγj+1wij v̂ πe(SHij+1)\n− n∑\ni=1\nj∑\nt=0\nγt ( witq̂ πe ( SHit , A Hi t ) − wit−1v̂πe ( SHit )) ,\nwhich can be written as:\ng(j)(D) = n∑\ni=1\nj∑\nt=0\nγtwitX i t +\n1\nn\nn∑\ni=1\nv̂πe(SHi0 ),\nwhere\nXit =R Hi t − q̂πe ( SHit , A Hi t ) + γv̂πe ( SHit+1 ) . (28)\nNotice thatXit is a bounded random variable since rewards and reward predictions are bounded. So, by Lemma 12 we have that\nn∑\ni=1\nj∑\nt=0\nγtwitX i t\na.s.−→ E [ j∑\nt=0\nγtXt ∣∣∣∣∣H ∼ πe ] . (29)\nAlso, since v̂πe(SHi0 ) is bounded, we have from the Kolmogorov strong law of large numbers that\n1\nn\nn∑\ni=1\nv̂πe(SHi0 ) a.s.−→ E[v̂πe(S0)]. (30)\nSo, combining (29) and (30) we have from Property 3 that\ng(j)(D) a.s.−→ E [ v̂πe(SH0 ) + j∑\nt=0\nγtXt ∣∣∣∣∣H ∼ πe ] .\nLet cj := E [ v̂πe(SH0 ) + ∑j t=0 γ tXt ∣∣∣H ∼ πe ] denote this constant value that g(j)(D) converges to.\nH.2. Convergence of Component of Off-Policy j-Step Return\nRecall from (24) that the off-policy j-step return can be written as:\ng(j)(D) = n∑\ni=1\ng (j) i (D),\nwhere\ng (j) i (D) :=\n( j∑\nt=0\nγtwitR Hi t ) + γj+1wij v̂ πe(SHij+1)\n− j∑\nt=0\nγt ( witq̂ πe ( SHit , A Hi t ) − wit−1v̂πe ( SHit )) .\nhere we will show that for any i and j, g(j)i (D) a.s.−→ 0.\nNotice that g(j)i (D) can be written as:\ng (j) i (D) =\nj∑\nt=0\nγt ρitX i t∑n\nk=1 ρ k t\n=\nj∑\nt=0\nγtY it ,\nwhere Xit is as defined in (28), and\nY it := ρitX i t∑n\nk=1 ρ k t\n= 1 nρ i tX i t\n1 n ∑n k=1 ρ k t\nSince Xit and ρ i t are bounded, we have that limn→∞ 1nρ i tX i t = 0. Also, by Lemma 11 and Kolmogorov’s strong law of large numbers, we have that 1 n ∑n k=1 ρ k t a.s−→ 1. So, Y it a.s.−→ 0 for all t and i. Furthermore, Y it is bounded since 0 ≤ ρ i t∑n\nk=1 ρ k t ≤ 1 and Xit is\nbounded. So, by Property 4, we have that g(j)i (D) a.s.−→ 0."
    }, {
      "heading" : "H.3. Consistency of Ω̂n",
      "text" : "Here we establish that Ω̂n − Ωn a.s.−→ 0. There are two steps to this result. First we will show that limn→∞ Ωn = 0—the true covariance matrix converges to the zero matrix. We then show that Ω̂n\na.s.−→ 0 as well, which means that Ω̂n − Ωn a.s.−→ 0. Recall from Appendix H.1 that g(j)(D) a.s.−→ cj . We can write Ωn(i, j) =E [ (g(i)(D)−E[g(i)(D)])(g(j)(D)−E[g(j)(D)]) ]\n=E[Yn], (31)\nwhere\nYn := ( g(i)(D)−E[g(i)(D)] )( g(j)(D)−E[g(j)(D)] ) .\nRecall that g(j)(D) a.s.−→ cj . By Lemma 2 we therefore have that for all j, limn→∞E[g(j)(D)] = cj . So, by the continuous mapping theorem,\nYn a.s.−→(ci − ci)(cj − cj) =0.\nSo, by applying Lemma 2 to (31) we have that limn→∞ Ωn(i, j) = limn→∞E[Yn] = 0.\nNext we show that Ω̂n a.s.−→ 0. First, recall from Appendix H.2 that for all j ∈ J and k ∈ {1, . . . , n},\ng (j) k (D) a.s.−→ 0.\nSo, by Property 3 we have that ḡ(j)k (D) a.s.−→ 0 as well. So, g (j) k (D) − ḡ (j) k (D)\na.s.−→ 0, and so by Property 3 and the definition of Ω̂n, we have that\nΩ̂n(i, j) a.s.−→ 0\nfor all (i, j) ∈ J 2."
    }, {
      "heading" : "H.4. Consistency of b̂n",
      "text" : "Here we show that b̂n − bn a.s.−→ 0. We have from the definitions of b̂n, l, and u that:\nb̂n(j)− bn(j) ≤g(Jj)(D)− l −E[g(Jj)(D)] + v(πe) (32)\nand\nb̂n(j)− bn(j) ≥g(Jj)(D)− u−E[g(Jj)(D)] + v(πe). (33)\nWe will show that both of the right hand sides above converge almost surely to zero, which, by Lemma 4, implies that b̂n(j)−bn(j) converges almost surely to zero as well. First consider (32). We have from Appendix H.1 that 1) g(Jj)(D)\na.s.−→ cJj . So, by Lemma 2 we have that 2) limn→∞E[g(Jj)(D)] = E[cJj ] = cJj . We also have that u − l ≤ 1√\nn\n√ 2ξ2 ln(2/δ), by (26) and (27). Since\nWDR(D) ∈ [l, u], we have that\n|WDR(D)− l| ≤ 1√ n\n√ 2ξ2 ln(2/δ).\nSince ξ is a constant, the right side is a sequence of constants (not random variables) that converges to zero. The left side is positive and less than the right, and so it too must converge (surely, not just almost surely) to zero: limn→∞ |WDR(D)− l| = 0. So,\nPr( lim n→∞ l = v(πe)) =Pr( lim n→∞\nl +WDR(D)− l = v(πe))\n=Pr( lim n→∞ WDR(D) = v(πe))\n=1,\nwhere the last step comes from Theorem 2. This means that 3) l a.s.−→ v(πe). Combining 1), 2), and 3), we have that the right side of (32) converges almost surely to zero. This same argument, using the upper bound, u, rather than the lower bound, l, shows that the right side of (33) converges almost surely to zero as well, and so we can conclude."
    }, {
      "heading" : "I. Extended Empirical Studies (MAGIC)",
      "text" : "Here we present detailed results concerning theMAGIC estimator. These results will use the same three domains and two experimental setups (full-data and half-data) that were introduced in Appendix D, as well as one additional domain, which we call theHybrid domain. We begin by introducing the Hybrid domain, we then discuss minor changes to the experimental setup and then present results."
    }, {
      "heading" : "I.1. The Hybrid Domain",
      "text" : "The purpose of this domain is to showcase a common problem type: domains where early in a trajectory there is partial observability, but as time passes within each trajectory, the partial observability decays. This happens, for example, in robotics applications where there may be some uncertainty about the position or pose of a robot. However, as the trajectory progresses the robot may be able to better localize itself, removing or diminishing the uncertainty.\nWe emulate this setting by concatenating the ModelFail and ModelWin domains. That is, the agent begins in the ModelFail domain. Whenever it would transition to the absorbing state, it instead transitions to the initial state of the ModelWin domain."
    }, {
      "heading" : "I.2. Experimental Setup",
      "text" : "We performed these experiments in the same way as those in Appendix D, except that we compared different estimators. Specifically, we introduce curves for the MAGIC estimator, but remove the curves for the poorly-performing importance sampling estimators, IS, PDIS, WIS, and CWPDIS. So, the plots contain curves for DR, WDR, AM, and MAGIC. The legend used by all of the plots in this appendix is provided in Figure 18.\nDR AM WDR MAGIC MAGIC-B\nFigure 18: The legend used by all plots in Appendix I.\nAlso, for the hybrid domain we included a curve for binary MAGIC (MAGIC-B), which uses J = {−1,∞}. Whereas MAGIC blends between AM and WDR using off-policy jstep returns of various lengths, binary MAGIC only places weights on AM and WDR. Our comparison to MAGICB shows the importance of including the off-policy j-step returns rather than merely trying to switch between, or directly weight, AM and WDR.\nLastly, since all of the domains have finite horizons, we used J = {−1, . . . , L} for MAGIC. This means that it uses all of the possible off-policy j-step returns."
    }, {
      "heading" : "I.3. ModelFail Results",
      "text" : "Figure 2b in Section 9 depicts the results for the ModelFail domain in the full-data setting. We reproduce this plot in Figure 19. In Figure 20 we show the results for ModelFail in the half-data setting. There is little difference between the plots—in both cases MAGIC properly tracks WDR, so that both WDR and MAGIC outperform AM an DR by at least an order of magnitude for most n."
    }, {
      "heading" : "I.4. ModelWin Results",
      "text" : "Figure 2c in Section 9 depicts the results for the ModelWin domain in the full-data setting. We reproduce this plot in Figure 21. In Figure 22 we show the results for ModelFail in the half-data setting. In both cases MAGIC tracks AM, although it drifts away a little as n increases. This suggests that there may be room for improvement in our estimates of Ωn and bn. However, also notice that due to the logarithmic scale, the difference between MAGIC and AM is small in comparison to the distance between MAGIC and DR."
    }, {
      "heading" : "I.5. Gridworld Results",
      "text" : "Figures 23 through 30 depict the results for the GridworldFH and Gridworld-TH domains in both the full and halfdata settings. The same general trends are visible. First, WDR tends to outperform DR, sometimes by an order of magnitude. Also, MAGIC tends to track WDR, since in these experiments it is usually the best-performing algorithm. Lastly, for the Gridworld-TH, full-data setting, DR, WDR, and MAGIC all degenerate to AM, while in the Gridworld-TH, half-data setting they degenerate to approximately AM using half as much data."
    }, {
      "heading" : "I.6. Hybrid Results",
      "text" : "Last, but not least, Figures 31 and 32 show the results on the Hybrid domain in the full-data and half-data settings, respectively. Notice that in MAGIC significantly outperforms all other methods, including WDR and AM. MAGIC also outperforms MAGIC-B, which shows the importance of using off-policy j-step returns for various values of j."
    }, {
      "heading" : "I.7. Summary",
      "text" : "Overall, MAGIC acts as desired—it tracks WDR or AM, whichever is better for the application at hand. However, notice that it does not do this perfectly, particularly when there is little data available. This is likely because when there is little data it is difficult to estimate Ωn, and the confidence interval used when estimating bn will be loose. In some cases, even when there is a large amount of data, MAGIC struggles to properly track AM. However, this tends to be when both methods perform well, and may be\n0.0001\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n2 20 200 2,000\nM e\na n\nS q\nu a\nre d\nE rr\no r\nNumber of Episodes, n\nFigure 29: Gridworld-TH, full-data. p1p2\n0.0001\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n2 20 200 2,000\nM e\na n\nS q\nu a\nre d\nE rr\no r\nNumber of Episodes, n\nFigure 30: Gridworld-TH, half-data. p1p2\ndue to an increased difficulty of determining which method to favor when they both are improving rapidly with n.\nWe also showed in Figures 31 and 32 an example where MAGIC outperformed MAGIC-B by an order of magnitude, and all previous methods (including DR) by 2–3 orders of magnitude. This exemplifies 1) the importance of blending between importance sampling methods and purely model-based estimators using off-policy j-step returns, as opposed to selecting between or directly weighting WDR and AM and 2) the power of MAGIC relative to existing estimators."
    }, {
      "heading" : "J. Future Work",
      "text" : "Several avenues of future work remain. Good performance of MAGIC is contingent on our ability to efficiently estimate Ωn and bn, and so improved estimators for these terms could yield even better performance. For instance, if the sample mean importance weight is near zero, then the importance sampling estimators have high variance that is not captured by the sample covariance matrix that we use.\nAnother possible avenue of future work would be to consider how MAGIC could be applied when our fundamental assumptions are violated. For example, what should be done if the transition and reward functions of the MDP are nonstationary? Can our estimators be extended to the average reward setting? What should be done if the behavior policies are not known exactly? If the approximate model is not provided initially, but constructed from the same data that is used to produce the DR,WDR, orMAGIC estimates, will DR, WDR, and MAGIC remain strongly consistent estimators? If there are multiple approximate models available, is there a way to detect which one will work best with DR, WDR, and MAGIC?"
    } ],
    "references" : [ {
      "title" : "The elements of integration and Lebesgue measure",
      "author" : [ "Bartle", "Robert G" ],
      "venue" : null,
      "citeRegEx" : "Bartle and G.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bartle and G.",
      "year" : 2014
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning",
      "author" : [ "S.J. Bradtke", "A.G. Barto" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bradtke and Barto,? \\Q1996\\E",
      "shortCiteRegEx" : "Bradtke and Barto",
      "year" : 1996
    }, {
      "title" : "Bootstrap Methods and their Application",
      "author" : [ "A.C. Davison", "D.V. Hinkley" ],
      "venue" : null,
      "citeRegEx" : "Davison and Hinkley,? \\Q1997\\E",
      "shortCiteRegEx" : "Davison and Hinkley",
      "year" : 1997
    }, {
      "title" : "Temporal difference Bayesian model averaging: A Bayesian perspective on adapting lambda",
      "author" : [ "C. Downey", "S. Sanner" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "Downey and Sanner,? \\Q2010\\E",
      "shortCiteRegEx" : "Downey and Sanner",
      "year" : 2010
    }, {
      "title" : "Doubly robust policy evaluation and learning",
      "author" : [ "M. Dudı́k", "J. Langford", "L. Li" ],
      "venue" : "In Proceedings of the TwentyEighth International Conference on Machine Learning,",
      "citeRegEx" : "Dudı́k et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudı́k et al\\.",
      "year" : 2011
    }, {
      "title" : "An Introduction to the Bootstrap",
      "author" : [ "B. Efron", "R.J. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Efron and Tibshirani,? \\Q1993\\E",
      "shortCiteRegEx" : "Efron and Tibshirani",
      "year" : 1993
    }, {
      "title" : "Monte carlo methods, methuen & co",
      "author" : [ "J.M. Hammersley", "D.C. Handscomb" ],
      "venue" : "Ltd., London, pp",
      "citeRegEx" : "Hammersley and Handscomb,? \\Q1964\\E",
      "shortCiteRegEx" : "Hammersley and Handscomb",
      "year" : 1964
    }, {
      "title" : "Doubly robust estimation in missing data and causal inference models",
      "author" : [ "H. Heejung", "J.M. Robins" ],
      "venue" : null,
      "citeRegEx" : "Heejung and Robins,? \\Q2005\\E",
      "shortCiteRegEx" : "Heejung and Robins",
      "year" : 2005
    }, {
      "title" : "Doubly robust off-policy evaluation for reinforcement learning",
      "author" : [ "N. Jiang", "L. Li" ],
      "venue" : "ArXiv, arXiv:1511.03722v1,",
      "citeRegEx" : "Jiang and Li,? \\Q2015\\E",
      "shortCiteRegEx" : "Jiang and Li",
      "year" : 2015
    }, {
      "title" : "TDγ : Re-evaluating complex backups in temporal difference learning",
      "author" : [ "G.D. Konidaris", "S. Niekum", "P.S. Thomas" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Konidaris et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Konidaris et al\\.",
      "year" : 2011
    }, {
      "title" : "Guided policy search",
      "author" : [ "S. Levine", "V. Koltun" ],
      "venue" : "In Proceedings of The 30th International Conference on Machine Learning, pp",
      "citeRegEx" : "Levine and Koltun,? \\Q2013\\E",
      "shortCiteRegEx" : "Levine and Koltun",
      "year" : 2013
    }, {
      "title" : "Weighted importance sampling for off-policy learning with linear function approximation",
      "author" : [ "A.R. Mahmood", "H. Hasselt", "R.S. Sutton" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mahmood et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mahmood et al\\.",
      "year" : 2014
    }, {
      "title" : "Offline policy evaluation across representations with applications to educational games",
      "author" : [ "T. Mandel", "Y. Liu", "S. Levine", "E. Brunskill", "Z. Popović" ],
      "venue" : "In Proceedings of the 13th International Conference on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Mandel et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mandel et al\\.",
      "year" : 2014
    }, {
      "title" : "Offline evaluation of online reinforcement learning algorithms",
      "author" : [ "T. Mandel", "Y. Liu", "E. Brunskill", "Z. Popović" ],
      "venue" : "In Proceedings of the Thirtieth Conference on Artificial Intelligence,",
      "citeRegEx" : "Mandel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mandel et al\\.",
      "year" : 2016
    }, {
      "title" : "Mathematical statistics for economics and business, volume 78",
      "author" : [ "R.C. Mittelhammer" ],
      "venue" : null,
      "citeRegEx" : "Mittelhammer,? \\Q1996\\E",
      "shortCiteRegEx" : "Mittelhammer",
      "year" : 1996
    }, {
      "title" : "Weighted uniform sampling: a Monte Carlo technique for reducing variance",
      "author" : [ "M.J.D. Powell", "J. Swann" ],
      "venue" : "Journal of the Institute of Mathematics and its Applications,",
      "citeRegEx" : "Powell and Swann,? \\Q1966\\E",
      "shortCiteRegEx" : "Powell and Swann",
      "year" : 1966
    }, {
      "title" : "Eligibility traces for off-policy policy evaluation",
      "author" : [ "D. Precup", "R.S. Sutton", "S. Singh" ],
      "venue" : "In Proceedings of the 17th International Conference on Machine Learning,",
      "citeRegEx" : "Precup et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2000
    }, {
      "title" : "Semiparametric regression estimation in the presence of dependent censoring",
      "author" : [ "A. Rotnitzky", "J.M. Robins" ],
      "venue" : null,
      "citeRegEx" : "Rotnitzky and Robins,? \\Q1995\\E",
      "shortCiteRegEx" : "Rotnitzky and Robins",
      "year" : 1995
    }, {
      "title" : "Large Sample Methods in Statistics An Introduction With Applications",
      "author" : [ "P.K. Sen", "J.M. Singer" ],
      "venue" : null,
      "citeRegEx" : "Sen and Singer,? \\Q1993\\E",
      "shortCiteRegEx" : "Sen and Singer",
      "year" : 1993
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "Agent based decision support system using reinforcement learning under emergency circumstances",
      "author" : [ "D. Thapa", "I. Jung", "G. Wang" ],
      "venue" : "Advances in Natural Computation,",
      "citeRegEx" : "Thapa et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Thapa et al\\.",
      "year" : 2005
    }, {
      "title" : "Personalized ad recommendation systems for life-time value optimization with guarantees",
      "author" : [ "G. Theocharous", "P.S. Thomas", "M. Ghavamzadeh" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Theocharous et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Theocharous et al\\.",
      "year" : 2015
    }, {
      "title" : "A notation for Markov decision processes",
      "author" : [ "P.S. Thomas" ],
      "venue" : "ArXiv, arXiv:1512.09075v1,",
      "citeRegEx" : "Thomas,? \\Q2015\\E",
      "shortCiteRegEx" : "Thomas",
      "year" : 2015
    }, {
      "title" : "Safe Reinforcement Learning",
      "author" : [ "P.S. Thomas" ],
      "venue" : "PhD thesis, University of Massachusetts Amherst,",
      "citeRegEx" : "Thomas,? \\Q2015\\E",
      "shortCiteRegEx" : "Thomas",
      "year" : 2015
    }, {
      "title" : "Policy evaluation using the Ω-return",
      "author" : [ "P.S. Thomas", "S. Niekum", "G. Theocharous", "G.D. Konidaris" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Thomas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Thomas et al\\.",
      "year" : 2015
    }, {
      "title" : "Offpolicy TD(λ) with true online equivalence",
      "author" : [ "H. van Hasselt", "A.R. Mahmood", "R.S. Sutton" ],
      "venue" : "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2014
    }, {
      "title" : "Variance reduction in monte-carlo tree search",
      "author" : [ "J. Veness", "M. Lanctot", "M. Bowling" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Veness et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning a value analysis tool for agent evaluation",
      "author" : [ "M. White", "M. Bowling" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "White and Bowling,? \\Q1976\\E",
      "shortCiteRegEx" : "White and Bowling",
      "year" : 1976
    }, {
      "title" : "Optimal unbiased estimators for evaluating agent performance",
      "author" : [ "M. Zinkevich", "M. Bowling", "N. Bard", "M. Kan", "D. Billings" ],
      "venue" : "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2006
    }, {
      "title" : "theDR estimator for a single trajectory (i.e., n = 1) as the last element",
      "author" : [ "Jiang", "Li" ],
      "venue" : "XL, of a sequence,",
      "citeRegEx" : "Jiang and Li,? \\Q2015\\E",
      "shortCiteRegEx" : "Jiang and Li",
      "year" : 2015
    }, {
      "title" : "DR is Unbiased While Jiang & Li (2015) showed that the DR estimator (with finite horizon) is an unbiased estimator of v(πe)",
      "author" : [ "Yk. B" ],
      "venue" : null,
      "citeRegEx" : "B.2.,? \\Q2015\\E",
      "shortCiteRegEx" : "B.2.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : ", for determining which advertisement to show a user visiting a website (Theocharous et al., 2015), for determining which medical treatment to suggest for a patient (Thapa et al.",
      "startOffset" : 72,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : ", 2015), for determining which medical treatment to suggest for a patient (Thapa et al., 2005), or for suggesting a personalized curriculum for a student (Mandel et al.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : ", 2005), or for suggesting a personalized curriculum for a student (Mandel et al., 2014).",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "This is in line with previous works that all use (root) mean squared error when empirically validating their methods (Precup et al., 2000; Dudı́k et al., 2011; Mahmood et al., 2014; Thomas, 2015b; Jiang & Li, 2015).",
      "startOffset" : 117,
      "endOffset" : 214
    }, {
      "referenceID" : 4,
      "context" : "This is in line with previous works that all use (root) mean squared error when empirically validating their methods (Precup et al., 2000; Dudı́k et al., 2011; Mahmood et al., 2014; Thomas, 2015b; Jiang & Li, 2015).",
      "startOffset" : 117,
      "endOffset" : 214
    }, {
      "referenceID" : 11,
      "context" : "This is in line with previous works that all use (root) mean squared error when empirically validating their methods (Precup et al., 2000; Dudı́k et al., 2011; Mahmood et al., 2014; Thomas, 2015b; Jiang & Li, 2015).",
      "startOffset" : 117,
      "endOffset" : 214
    }, {
      "referenceID" : 14,
      "context" : "For a review of OPE methods, see the works of Precup et al. (2000) or Thomas (2015b, Chapter 3).",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "For a review of OPE methods, see the works of Precup et al. (2000) or Thomas (2015b, Chapter 3). More recent methods can be found in the works of Jiang & Li (2015) and Mandel et al.",
      "startOffset" : 46,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "More recent methods can be found in the works of Jiang & Li (2015) and Mandel et al. (2016).",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning pling (Precup et al., 2000).",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "The work that introduced the DR estimator for MDPs (Jiang & Li, 2015) derived it as a generalization of a doubly robust estimator for bandits (Dudı́k et al., 2011).",
      "startOffset" : 142,
      "endOffset" : 163
    }, {
      "referenceID" : 29,
      "context" : "Advantage sum estimators were introduced as a way to lower the variance of on-policy Monte Carlo performance estimates for a setting that is a generalization of the (partially observable) MDP setting (Zinkevich et al., 2006; White & Bowling, 2009).",
      "startOffset" : 200,
      "endOffset" : 247
    }, {
      "referenceID" : 29,
      "context" : "One may therefore view the DR estimator (Jiang & Li, 2015) as the extension of the advantage sum estimator (Zinkevich et al., 2006) to the off-policy setting or as the extension of the doubly robust estimator for bandits (Dudı́k et al.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : ", 2006) to the off-policy setting or as the extension of the doubly robust estimator for bandits (Dudı́k et al., 2011) to the sequential setting.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : ", bootstrap confidence bounds) are used instead (Theocharous et al., 2015; Thomas, 2015b).",
      "startOffset" : 48,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "For example, in their experiments, Precup et al. (2000), Dudı́k et al.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "(2000), Dudı́k et al. (2011), Mahmood et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "(2000), Dudı́k et al. (2011), Mahmood et al. (2014), Thomas (2015b), and Jiang & Li (2015) all use the (root) MSE when evaluating OPE methods.",
      "startOffset" : 8,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "(2000), Dudı́k et al. (2011), Mahmood et al. (2014), Thomas (2015b), and Jiang & Li (2015) all use the (root) MSE when evaluating OPE methods.",
      "startOffset" : 8,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "(2000), Dudı́k et al. (2011), Mahmood et al. (2014), Thomas (2015b), and Jiang & Li (2015) all use the (root) MSE when evaluating OPE methods.",
      "startOffset" : 8,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Weighted importance sampling has been used before for OPE (Precup et al., 2000), but not in conjunction with the DR estimator.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "In the ModelFail experiments, the approximate model This model-based estimator has been called the direct method in previous work (Dudı́k et al., 2011), however, in other previous work direct methods are model-free while indirect methods are model-based (Sutton & Barto, 1998, Section 9.",
      "startOffset" : 130,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "The λ-return is the foundation of the entire TD(λ) family of algorithms, which includes the original linear-time algorithm (Sutton, 1988), least-squares formulations (Bradtke & Barto, 1996; Mahmood et al.",
      "startOffset" : 123,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "The λ-return is the foundation of the entire TD(λ) family of algorithms, which includes the original linear-time algorithm (Sutton, 1988), least-squares formulations (Bradtke & Barto, 1996; Mahmood et al., 2014), methods for adapting λ (Downey & Sanner, 2010), true-online methods (van Hasselt et al.",
      "startOffset" : 166,
      "endOffset" : 211
    }, {
      "referenceID" : 9,
      "context" : "Recent work has suggested that the λ-return could be replaced by more statistically principled complex returns like the γ-return (Konidaris et al., 2011) or Ωreturn (Thomas et al.",
      "startOffset" : 129,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : ", 2011) or Ωreturn (Thomas et al., 2015).",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "First, notice that ∑∞ t=0 γ tρi t R Hi t is the per-decision importance sampling (PDIS) estimator, which is known to be an unbiased estimator of v(πe) (Precup et al., 2000; Thomas, 2015b).",
      "startOffset" : 151,
      "endOffset" : 187
    }, {
      "referenceID" : 16,
      "context" : "See the works of (Precup et al., 2000) or (Thomas, 2015b, Lemma 1).",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "In this definition theX term is the per-decision importance sampling (PDIS) estimator, which is known to be an unbiased and strongly consistent estimator of v(πe) (Precup et al., 2000; Thomas, 2015b).",
      "startOffset" : 163,
      "endOffset" : 199
    }, {
      "referenceID" : 16,
      "context" : "We begin with the per-decision importance sampling (PDIS) estimator, which is known to be an unbiased and strongly consistent estimator of v(πe) (Precup et al., 2000; Thomas, 2015b).",
      "startOffset" : 145,
      "endOffset" : 181
    }, {
      "referenceID" : 23,
      "context" : "The Gridworld Domain The third domain that we used was the gridworld domain developed by Thomas (2015b, Section 2.5) for evaluating OPE algorithms. It is a 4 × 4 gridworld with four actions, L = 100, and deterministic transition and reward functions. This domain was developed specifically for evaluating different OPE methods. Thomas (2015b) proposed five policies, π1, .",
      "startOffset" : 89,
      "endOffset" : 343
    } ],
    "year" : 2016,
    "abstractText" : "In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods—it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang & Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.",
    "creator" : "LaTeX with hyperref package"
  }
}