{
  "name" : "1402.0565.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lifted Variable Elimination: Decoupling the Operators from the Constraint Language",
    "authors" : [ "Nima Taghipour", "Daan Fierens", "Jesse Davis", "Hendrik Blockeel" ],
    "emails" : [ "nima.taghipour@cs.kuleuven.be", "daan.fierens@cs.kuleuven.be", "jesse.davis@cs.kuleuven.be", "hendrik.blockeel@cs.kuleuven.be" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Statistical relational learning or SRL (Getoor & Taskar, 2007; De Raedt, Frasconi, Kersting, & Muggleton, 2008) focuses on combining first-order logic with probabilistic graphical models, which permits algorithms to reason about complex, uncertain, structured domains. A major challenge in this area is how to perform inference efficiently. First-order logic can reason on the level of logical variables: if a model states that for all X, P (X) implies Q(X), then whenever P (X) is known to be true, one can infer Q(X), without knowing what X stands for. Many approaches to SRL, however, transform their knowledge into a propositional graphical model before performing inference. By doing so, they lose the capacity to reason on the level of logical variables: standard inference methods for graphical models can reason only on the “ground” level, repeating the same inference steps for each different value x of X, instead of once for all x.\nTo address this problem, Poole (2003) introduced the concept of lifted inference for graphical models. The idea is to group together interchangeable objects, and perform the inference operations once for each group instead of once for each object. Multiple different algorithms have been proposed, based on variable elimination (Poole, 2003; de Salvo Braz, Amir, & Roth, 2005; Milch, Zettlemoyer, Kersting, Haimes, & Kaelbling, 2008; Sen, Deshpande, & Getoor, 2009b, 2009a; Choi, Hill, & Amir, 2010; Apsel & Brafman,\nc©2013 AI Access Foundation. All rights reserved.\n2011), belief propagation (Kersting, Ahmadi, & Natarajan, 2009; Singla & Domingos, 2008), or various other approaches (Van den Broeck, Taghipour, Meert, Davis, & De Raedt, 2011; Jha, Gogate, Meliou, & Suciu, 2010; Gogate & Domingos, 2011).\nA group of interchangeable objects is typically defined by means of a constraint that an object must fulfill in order to belong to that group. The type of constraints that are allowed, and the way in which they are handled, directly influence the granularity of the grouping, and hence, the efficiency of the subsequent lifted inference (Kisynski & Poole, 2009a). Among the approaches based on variable elimination, the most advanced system (C-FOVE) uses a specific class of constraints, namely, conjunctions of pairwise (in)equalities. This is the bare minimum required to be able to perform lifted inference. However, as we will show, it unnecessarily limits the symmetries the model can capture and exploit.\nIn this article, we present an algorithm for lifted variable elimination that is based on C-FOVE, but uses a constraint language that is extensionally complete, that is, for any group of variables a constraint exists that defines exactly that group. To this aim, C-FOVE’s constraint manipulation is redefined in terms of relational algebra operators. This decouples the lifted inference algorithm from the constraint representation mechanism. Consequently, any constraint language that is closed under these operators can be plugged into the algorithm to obtain a working system. Apart from redefining existing operators, we also define a novel operator, called lifted absorption, in this way. Furthermore, we propose a concrete mechanism for constraint representation that is extensionally complete, and briefly discuss how the operators can be implemented for this particular mechanism. The new lifted inference algorithm, with this constraint representation mechanism, can perform lifted inference with a much coarser granularity than its predecessors. Due to this, it outperforms existing systems by several orders of magnitude on some problems, and solves inference problems that until now could only be solved by approximate inference methods.\nThe basic ideas behind our approach have been explained in an earlier conference paper (Taghipour, Fierens, Davis, & Blockeel, 2012). This article extends that paper by providing precise and motivated definitions for the operators, up to the level where they can be implemented. These definitions, at the same time, may help understand on a more intuitive and semantic level how lifted variable elimination works, and can serve as a kind of gold standard for other implementations of lifted variable elimination, as they provide a semantics based reference point.\nThe paper is structured as follows. Section 2 illustrates the principles of lifted variable elimination by example, and briefly states how this work improves upon the state of the art, C-FOVE (Milch et al., 2008). Section 3 introduces formal notation and terminology, and Section 4 provides a high-level outline of our lifted variable elimination algorithm. Section 5 describes in detail all the operators that the algorithm uses. Section 6 briefly discusses an efficient representation for the constraints themselves. Section 7 empirically compares our system’s performance with that of C-FOVE, and Section 8 concludes."
    }, {
      "heading" : "2. Lifted Variable Elimination by Example",
      "text" : "Although lifted variable elimination builds on simple intuitions, it is relatively complicated, and an accurate description of it requires a level of technical detail that is not conducive to a clear understanding. For this reason, we first illustrate the basic principles of lifted inference\non a simple example, and without referring to the technical terminology that is introduced later. We start with describing the example; next, we illustrate variable elimination on this example, and show how it can be lifted."
    }, {
      "heading" : "2.1 The Workshop Example",
      "text" : "This example is from Milch et al. (2008). Suppose a new workshop is organized. If the workshop is popular (that is, many people attend), it may be the start of a series. Whether a person is likely to attend depends on the topic.\nWe introduce a random variable T , indicating the topic of the workshop, and a random variable S, indicating whether the workshop becomes a series. We consider N people, and for each person i, we include a random variable Ai that indicates whether i attends. Each random variable has a finite domain from which it takes on values, i.e., {ai,ml, . . . } for T , {yes, no} for S, and {true, false} for each Ai.\nThe joint probability distribution of these variables can be specified by an undirected graphical model. A set of factors captures dependencies between the random variables in such a model. In our model, there are two kinds of factors. For each person i, there is a factor φ1(Ai, S) that states how having a series depends on whether person i attends, and a factor φ2(T,Ai) that states how i’s attendance depends on the topic. Note that all N factors of the first type have the same potential function φ1, and all factors of the second type have potential function φ2. This imposes a certain symmetry on the model: it implies that S depends on each person’s attendance in exactly the same way, and all people have the same topic preferences.\nThe model defines a joint probability distribution over the variables that is the normalized product of the factors (normalized such that all joint probabilities sum to one):\nPr(T, S,A1, . . . , AN ) = 1\nZ\nn ∏\ni=1\nφ1(Ai, S)\nn ∏\ni=1\nφ2(T,Ai)\nwhere Z is the normalization constant.\nUndirected graphical models can be visualized as factor graphs (Kschischang, Frey, & Loeliger, 2001), which have a node for each random variable and each factor, and an edge between a factor and a random variable if that variable occurs in the factor. Figure 1 shows a factor graph for our example."
    }, {
      "heading" : "2.2 Variable Elimination",
      "text" : "From now on, we refer to the values taken by a variable by the corresponding lowercase symbols (e.g., ai as shorthand for Ai = ai).\nSuppose we want to compute the marginal probability distribution Pr(S).\nPr(S) = ∑\nT\n∑\nA1\n· · · ∑\nAN\nPr(T, S,A1, . . . , AN ) (1)\n= 1\nZ\n∑\nT\n∑\nA1\n· · · ∑\nAN\nN ∏\ni=1\nφ1(Ai, S)\nN ∏\ni=1\nφ2(T,Ai) (2)\nUsually, the normalization constant Z is ignored during the computations, and normalization happens only at the very end. So, we can focus on how to compute\nP̃r(S) = ∑\nT\n∑\nA1\n· · · ∑\nAN\nN ∏\ni=1\nφ1(Ai, S) N ∏\ni=1\nφ2(T,Ai). (3)\nA straightforward way of computing P̃r(S) is to compute P̃r(s) for each possible value s of S, and tabulate the results. We can compute P̃r(true) by iterating over all possible value combinations (t, a1, . . . , an) of (T,A1, . . . , An) and computing ∏N i=1 φ1(ai, true) ∏N i=1 φ2(t, ai) for each combination, and similarly for P̃r(false). If all variables are binary, there are 2N+1 such combinations, and for each combination 2N − 1 multiplications are performed. This clearly does not scale.\nHowever, we can improve efficiency by rearranging the computations. In the above computation, the same multiplications are performed repeatedly. Since φ1(A1, S) and φ2(T,A1) are constant in all Ai except A1, they can be moved out of the summations over Ai, i > 1, so the right hand side of Equation 3 becomes:\n∑\nT\n∑\nA1\nφ1(A1, S)φ2(T,A1) ∑\nA2\n· · · ∑\nAN\nN ∏\ni=2\nφ1(Ai, S)\nN ∏\ni=2\nφ2(T,Ai) (4)\nConversely, the factor starting with ∑\nA2 is independent of A1, so it can be moved outside\nof the summation over A1, giving\n∑\nT\n\n\n∑\nA2\n· · · ∑\nAN\nN ∏\ni=2\nφ1(Ai, S) N ∏\ni=2\nφ2(T,Ai)\n\n\n\n\n∑\nA1\nφ1(A1, S)φ2(T,A1)\n\n (5)\nRepeating this for each Ai eventually yields\n∑\nT\n\n\n∑\nA1\nφ1(A1, S)φ2(T,A1)\n\n . . .\n\n\n∑\nAN\nφ1(AN , S)φ2(T,AN )\n\n (6)\nwhich shows that for each Ai, the product φ1(Ai, S)φ2(T,Ai) needs to be computed only once for each combination of values for (T, S,Ai). When T is binary, there are eight such combinations, reducing the total number of multiplications to 8N .\nNote that the result of Formula 6 is a function of S; it can yield a different value for each value s of S. In other words, it is a factor over S. Similarly, the result of φ1(A1, S)·φ2(T,A1) depends on the values of S, T and A1 (is a factor over these variables), and after summation over A1 a factor over S and T is obtained. Thus, the multiplications and summations in Formula 6 are best seen as operating on factors, not individual numbers. Figure 2 illustrates the process of multiplying and summing factors.\nThe result of Formula 6 can be computed as follows. First, multiply the factors φ1(A1, S) and φ2(T,A1) for each value of A1, and sum out A1 from the product. This is exactly the computation illustrated in Figure 2. After summing over all values of A1, the result depends on T and S only; A1 no longer occurs in this factor, nor in any other factors. We say that A1 has been eliminated. Note that the elimination consisted of first gathering all factors containing A1, multiplying them, then summing over all possible values of A1.\nAfter eliminating A1, we can repeat the process for all other Ai, each time obtaining a factor over T and S. All those factors can then be multiplied and the result summed over T , at which point a single factor over S is obtained. This factor equals P̃r(S).\nThe above computation is exactly what Variable Elimination (VE) does. Generally, VE works as follows. It considers one variable at a time, in an order called the elimination order. For each considered variable X, VE first retrieves all factors that involve X, then multiplies these factors together into a single joint factor, and finally sums out X, thereby\neliminating X from the factor. Hence, in each step, the number of remaining variables strictly decreases (by 1) and also the number of factors decreases (because the set of factors involving X is replaced by a single factor).\nThe elimination order can heavily influence runtime. Unfortunately, finding the optimal order is NP-hard. In the above example, the elimination order was A1, A2, . . . , AN , T , and this resulted in a computation with 8N multiplications, which is O(N)."
    }, {
      "heading" : "2.3 Lifted Inference: Exploiting Symmetries Among Factors",
      "text" : "In the above example, by avoiding many redundant computations, VE obtained an exponential speedup compared to the naive computation discussed before, reducing computation time from O(2N ) to O(N). N can still be large. Even more efficiency can be gained when we know that certain factors have the same potential function.\nIn our example, VE computes the same product N times: in Expression 6, factors φ1(Ai, S) and φ2(T,Ai) are the same for all i, and so is their product φ12(Ai, S, T ) = φ1(Ai, S)φ2(T,Ai). It also computes the sum ∑\nAi φ12(Ai, S, T ) N times. This redundancy\narises because in our probabilistic model all N people behave in the same way, i.e., all Ai are interchangeable. The idea behind lifted inference is to exploit such symmetries, and compute the product and sum only once. From the algorithmic perspective, lifted variable elimination eliminates only one Ai variable, then exponentiates the resulting factor (see formula below), and then sums out T . Mathematically, Expression 6 is computed as follows:\n∑\nT\n\n\n∑\nA1\n(φ1(A1, S)φ2(T,A1))\n\n\nN\n(7)\nThe way in which lifted variable elimination manipulates the set of variables {A1, . . . , AN} is called lifted multiplication and lifted summing-out (a.k.a. lifted elimination). Note that the number of operations required is now constant in N . Assuming N is already known, the main operation here is computing the N -th power, which is O(logN) (logarithmic in N if exact arithmetic is used, constant for floating point arithmetic). Thus, lifted variable elimination runs in O(logN) time in this case."
    }, {
      "heading" : "2.4 Lifted Inference: Exploiting Symmetries within Factors",
      "text" : "Now consider a second elimination order, where we first eliminate T and then the Ai:\nP̃r(S) = ∑\nA1,...,AN\n∑\nT\nN ∏\ni=1\nφ1(Ai, S)\nN ∏\ni=1\nφ2(T,Ai) = ∑\nA1,...,AN\nN ∏\ni=1\nφ1(Ai, S)\n(\n∑\nT\nN ∏\ni=1\nφ2(T,Ai)\n)\n(8) With this order, regular variable elimination works as follows. The inner summation (elimination of T ) first multiplies all factors φ2(T,Ai) into a factor φ3(T,A1, . . . , AN ), and then sums out T :\n∑\nT\nN ∏\ni=1\nφ2(T,Ai) = ∑\nT\nφ3(T,A1, . . . , AN ) = φ ′ 3(A1, . . . , AN )\nNote that φ3 is a function of N +1 binary variables, so its tabular representation has 2 N+1 entries, which makes the cost of this elimination O(2N+1). Substituting the computed φ′3 into Equation (8) yields:\nP̃r(S) = ∑\nA1,...,AN\n(\nN ∏\ni=1\nφ1(Ai, S)\n)\nφ′3(A1, . . . , AN )\nNow we can multiply φ′3(A1, . . . , AN ) by φ1(A1, S) and sum out A1, then multiply the result by φ1(A2, S) and sum out A2, and so on, until we obtain a factor φ ′ 4(S):\nP̃r(S) = φ′4(S)\nThis again involves N multiplications and summations with exponential complexity. In summary, variable elimination computes the result in O(2N+1).\nThis elimination order also has symmetries that lifted inference can exploit. Let us examine φ3(T,A1, . . . , AN ), the product of factors φ2(T,Ai). For each assignment T = t and (A1, . . . , AN ) = (a1, . . . aN ) ∈ {true, false} N :\nφ3(t, a1, . . . , aN ) = φ2(t, a1) . . . φ2(t, aN )\nNote that, since each ai ∈ {true, false}, the multiplicands on the right hand side can have only one of two values, φ2(t, true) or φ2(t, false). That is, for each ai = true there is a φ2(t, true), and similarly for each ai = false, a φ2(t, false). This means that, with At = {Ai|ai = true} and Af = {Ai|ai = false}, we can rewrite the above expression as:\nφ3(t, a1, . . . , aN ) = ∏\nai∈At\nφ2(t, true) ∏\nai∈Af\nφ2(t, false) = φ2(t, true) |At| φ2(t, false) |Af |.\nThis shows that to evaluate φ3(T,A1, . . . , AN ) it suffices to know how many Ai are true (call this number nt) and false (nf ); we do not need to know the value of each individual Ai. We can therefore restate φ3 in terms of a new variable #[A], called a counting variable, the value of which is the two-dimensional vector (nt, nf ). Generally, #[A] can take any value (x, y) with x, y ∈ N and x+ y = N . We call such a value a histogram. It captures the distribution of values among A = {A1, . . . , AN}. The reformulation of a factor in terms of a counting variable is called counting conversion. Rewriting φ3(T,A1, . . . , AN ) as φ ∗ 3(T,#[A]), we have\nφ∗3(t, (nt, nf )) = φ2(t, true) nt φ2(t, false) nf .\nφ∗3 has 2(N +1) possible input combinations (two values for t and N +1 values for (nt, nf ), since nt+nf = N). It can be tabulated in time O(N), using the recursive formula φ ∗ 3(t, (nt+ 1, nf − 1)) = φ ∗ 3(t, (nt, nf )) ·φ1(t, true)/φ2(t, false). Note that VE’s computation of φ3 was O(2N ). Because φ∗3 has only 2(N+1) possible input states, instead of 2\nN+1, we can now eliminate T in O(N):\n∑\nT\nN ∏\ni=1\nφ2(T,Ai) = ∑\nT\nφ∗3(T,#[A]) = φ ′ 3(#[A])\nUsing this result, we continue with the elimination:\nP̃r(S) = ∑\nA1,...,AN\nN ∏\ni=1\nφ1(Ai, S) φ ′ 3(#[A])\nUsing counting conversion a second time, we can reformulate the result of ∏N\ni=1 φ1(Ai, S) as φ4(#[A], S), which gives:\nP̃r(S) = ∑\nA1,...,AN\nφ4(#[A], S) φ ′ 3(#[A]) =\n∑\nA1,...,AN\nφ43(#[A], S) (9)\nIn itself, the final summation still enumerates all 2N joint states of variables A, computes the histogram (nt, nf ) and φ43((nt, nf ), S) for each state, and adds up all the φ43. But we can do better: all states that result in the same histogram (nt, nf ) have the same value for φ43((nt, nf ), S), and we know exactly how many such joint states there are, namely (\nN nt\n)\n= N ! nt!nf ! . We will call this the multiplicity of the histogram (nt, nf ), denoted Mul((nt, nf )). Thus, we can compute φ43((nt, nf ), S) just once for each histogram (nt, nf ) and multiply it by its multiplicity:\n∑\nA1,...,AN\nφ43(#[A], S) = ∑\n#[A]\nMul(#[A]) · φ43(#[A], S)\nThis way we enumerate over N + 1 possible values of #[A] instead of 2N possible states of A. To summarize, we can reformulate Equation (9) as\nP̃r(S) = ∑\nA1,...,AN\nφ43(#[A], S) = ∑\n#[A]\nMul(#[A]) · φ43(#[A], S) = φ5(S)\nwhich shows that #[A] can be eliminated with O(N) operations.\nThe whole computation of P̃r(S) thus has complexity O(N), instead of O(2N ) for VE with this elimination order. This reduction in complexity is possible due to symmetries in the model that allow us to treat all variables A as one unit #[A]."
    }, {
      "heading" : "2.5 Capturing the Symmetries",
      "text" : "It is clear that lifting can yield important speedups, if certain symmetries among factors or among the inputs of a single factor are present. To exploit these, it is essential that one can indicate which variables are interchangeable and hence induce these symmetries.\nIn our workshop example, assume, for instance, that not every person has the same preferences with respect to topics, but there are two types of people, and different potentials (φ2a and φ2b) are associated with each type of person. It is clear that instead of Formula 7,\n∑\nT\n\n\n∑\nA1\nφ1(A1, S)φ2(T,A1)\n\n\nN\n,\nwe then need to compute\n∑\nT\n\n\n∑\nAk\nφ1(Ak, S)φ2a(T,Ak)\n\n\nNa \n\n∑\nAl\nφ1(Al, S)φ2b(T,Al)\n\n\nNb\nwhere Ak and Al are random members from the first and second group, and Na and Nb the cardinality of these groups. In order to do this, we need to be able to state for which Ai φ2a is relevant, and for which φ2b is. (For this particular computation, it actually suffices to know the size of each group, but that is not true in general; for instance, to compute the marginal distribution of A5, we need to know which group A5 is in.)\nOur main contribution is related to this particular point. At the time of writing, the C-FOVE system (Milch et al., 2008) is considered the state of the art in lifted variable elimination. By introducing counting variables, it can capture within-factor symmetries better than its predecessor, FOVE. However, as it turns out, C-FOVE is less good at capturing symmetries among multiple factors, compared to FOVE. This is because groups of variables or factors are defined by means of constraints, and C-FOVE uses a constraint language that is more limited than FOVE’s; essentially, it only allows for conjunctive constraints.\nThere are two reasons why it is important to be able to group variables with as much flexibility as possible. First, it gives more flexibility to the user who has to specify the graphical model itself. Second, during inference, it may become necessary to “split up” groups into subgroups.\nWe cannot go into detail about the constraint based representation at this point (we will do that later), but basically, during lifted inference, one may have a set of interchangeable variables that could in principle be treated as one group, but are not because the system cannot represent this group. It then needs to partition the group into smaller groups, possibly up to the level of individuals. For instance, assume the groups in our above example are {A1, A2, A5, A6, A7} and {A3, A4, A8}. Further assume that the constraint language is such that sets of variables Ai are defined using constraints of the form {Ai|l ≤ i ≤ u}. Neither group can be represented using one single constraint. For instance, the first group consists of the union of {Ai|1 ≤ i ≤ 2} and {Ai|5 ≤ i ≤ 7}. Using this constraint language, we get four groups of size 2, 3, 2 and 1 instead of two groups of size 5 and 3. As a result, the computation actually performed will contain four exponentiated factors instead of two:\n∑\nT\n\n\n∑\nA1\nφ1(A1, S)φ2a(T,A1)\n\n\n2 \n\n∑\nA5\nφ1(A5, S)φ2a(T,A5)\n\n\n3\n\n\n∑\nA3\nφ1(A3, S)φ2b(T,A3)\n\n\n2\n\n∑\nA8\nφ1(A8, S)φ2b(T,A8)\n\n\n1\nGenerally, during lifted inference, groups may be split repeatedly. Unnecessary splits can substantially hurt efficiency, as each one causes a duplication of work. Since the duplicated work may include further splitting, the overall effect can be exponential in the number of consecutive splits.\nIdeally, the constraint language should have the property that for each group of variables, there exists a constraint that represents exactly that group of variables. In that case, it is\nnever necessary to split a group into subgroups just because the group cannot be represented. We call such a language “extensionally complete”. The main contribution of this article is that it shows how to perform lifted variable elimination with an extensionally complete constraint language. To this aim, first, a lifted variable elimination algorithm is defined in a way that is independent of the constraint representation mechanism, by defining its operators in terms of relational algebra expressions. We call this algorithm GC-FOVE. To make GC-FOVE operational, some kind of constraint representation mechanism is of course needed. Any constraint language L can be plugged into GC-FOVE, as long as it is closed with respect to the relational algebra operators used by GC-FOVE. Second, we propose an extensionally complete constraint representation language that is based on trees. Such a language is necessarily closed with respect to the relational algebra operators, and therefore suitable for GC-FOVE. The resulting system, GC-FOVETREES, can perform inference at a higher level of granularity, and therefore more efficiently, than C-FOVE, which does not use an extensionally complete constraint language. The effect of this is visible in particular when evidence is given (which breaks symmetries and hence causes group splitting); in such cases, GC-FOVE achieves exponential speedups compared to C-FOVE.\nThis ends our informal introduction to lifted variable elimination and the main contribution this articles makes to it. In the following sections, we first introduce formal notation and terminology, then present our contributions in more detail."
    }, {
      "heading" : "3. Representation",
      "text" : "Lifted inference exploits symmetries in a probabilistic model. Such symmetries often occur in models that have repeating structures, such as plates (Getoor & Taskar, 2007, Ch. 7), or, more generally, in probabilistic-logical models. Probabilistic-logical modeling languages (also called probabilistic-relational languages) combine the representational and inferential aspects of first-order logic with that of probability theory.\nFirst-order logic languages refer to objects (possibly of various types) in some universe, and properties of, or relationships between, these objects. Formulas in these languages can express that some property holds for a particular object, or for an entire set of objects. For instance, the fact that all humans are mortal could be written as ∀x : Human(x) → Mortal(x). Probabilistic-logical models can, in a similar way, express probabilistic knowledge about all objects. For instance, they could state that for each human, there is a prior probability of 20% that he or she smokes: ∀x : P (Smokes(x)|Human(x)) = 0.2. It is this ability to make (probabilistic) statements about entire sets of objects that allows these languages to compactly express symmetries in a model. Many different languages exist for representing probabilistic-logical models (e.g., see Getoor & Taskar, 2007). We use a representation formalism based on undirected graphical models that is closely related to the one used in earlier work on lifted variable elimination (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).\nThe concepts introduced in this section have also been introduced in earlier work (de Salvo Braz, 2007; Milch et al., 2008). Differences arise in terminology and notation as we emphasize the constraint part."
    }, {
      "heading" : "3.1 A Constraint-based Representation Formalism",
      "text" : "An undirected model is a factorization of a joint distribution over a set of random variables (Kschischang et al., 2001). Given a set of random variables X = {X1,X2, . . . ,Xn}, a factor consists of a potential function φ and an assignment of a random variable to each of φ’s inputs. For instance, the factorization f(X1,X2,X3) = φ(X1,X2)φ(X2,X3) contains two different factors (even if their potential functions are the same).\nLikewise, in our probabilistic-logical representation framework, a model is a set of factors. The random variables they operate on are properties of, and relationships between, objects in the universe. We now introduce some terminology to make this more concrete. We assume familiarity with set and relational algebra (union ∪, intersection ∩, difference \\, set partitioning, selection σC , projection πX , attribute renaming ρ, join ⊲⊳); see, for instance, the work of Ramakrishnan and Gehrke (2003).\nThe term “variable” can be used in both the logical and probabilistic context. To avoid confusion, we use the term logvar to refer to logical variables, and randvar to refer to random variables. We write variable names in uppercase, and their values in lowercase. Sets or sequences of logvars are written in boldface, sets or sequences of randvars in calligraphic; their values are written in boldface lowercase.\nThe vocabulary of our representation includes a finite set of predicates and a finite set of constants. A constant represents an object in our universe. A term is either a constant or a logvar. A predicate P has an arity n and a finite range (range(P )); it is interpreted as a mapping from n-tuples of objects (constants) to the range. An atom is of the form P (t1, t2, . . . , tn), where the ti are terms. A ground atom is an atom where all ti are constants. A ground atom represents a random variable; this implies that its interpretation, an element of range(P ), corresponds to the assignment of a value to the random variable. Hence, the range of a predicate corresponds to the range of the random variables it can represent, and is not limited to {true, false} as in logic.\nLogvars have a finite domain, which is a set of constants. The domain of a logvar X is denoted D(X). A constraint is a relation defined on a set of logvars, i.e., it is a pair (X, CX), where X = (X1,X2, . . . ,Xn) is a tuple of logvars, and CX is a subset of D(X) = ×iD(Xi) (Dechter, 2003). Hence, CX is a set, whose elements (tuples) indicate the allowed combinations of value assignments for the variables in X. For ease of exposition, we identify a constraint with its relation CX, and write C instead of CX when X is apparent from the context. We assume an implicit ordering of values in CX’s tuples according to the order of logvars in X. For instance with X = (X1,X2), the constraint CX = {(a, b), (c, d)} indicates that there are two possibilities: either X1 = a and X2 = b, or X1 = c and X2 = d. A constraint that contains only one tuple is called singleton.\nA constraint may be defined extensionally, by listing the tuples that satisfy it, or intensionally, by means of some logical condition, expressed in a constraint language. We call a constraint language L extensionally complete if it can express any relation over logvars X, i.e., for any subset of D(X), there is a constraint CX ∈ L whose extension is exactly that subset.\nA constrained atom is of the form P (X)|C, where P (X) is an atom and C is a constraint on X. A constrained atom P (X)|C represents a set of ground atoms {P (x)|x ∈ C}, and hence a set of randvars. For consistency with the literature, we call such a constrained atom\na parametrized randvar (PRV), and use calligraphic notation to denote it. Given a PRV V, we use RV (V) to denote the set of randvars it represents; we also say these randvars are covered by V.\nA valuation of a randvar (set of randvars) is an assignment of a value to the randvar (an assignment of values to all randvars in the set).\nExample 1. The PRV V = Smokes(X)|C, with C = {x1, . . . , xn}, represents n randvars {Smokes(x1), . . . Smokes(xn)}.\nA factor f = φf (Af ) consists of a sequence of randvars Af = (A1, . . . , An) and a potential function φf : × n i=1range(Ai) → R\n+. The product of two factors, f1 ⊗ f2, is defined as follows. Factor f = φ(A) is the product of f1 = φ1(A1) and f2 = φ2(A2) if and only if A = A1 ∪ A2 and for all a ∈ D(A): φ(a) = φ1(a1)φ2(a2) with πAi(a) = ai for i = 1, 2. That is, a assigns to each randvar in Ai the same value as ai. We use ∏\nto denote multiplication of multiple factors. Multiplying a factor by a scalar c means replacing its potential φ by φ′ : x 7→ c · φ(x).\nAn undirected model is a set of factors F . It represents a probability distribution PF on randvars A = ⋃\nf∈F Af as follows: PF (A) = 1 Z\n∏\nf∈F φf (Af ), with Z a normalization constant such that ∑\na∈range(A) PF (a) = 1. A parametric factor or parfactor has the form φ(A)|C, with A = {Ai} n i=1 a sequence of atoms, φ a potential function on A, and C a constraint on the logvars appearing in A.1 The set of logvars occurring in A is denoted logvar(A); the set of logvars in C is denoted logvar(C). A factor φ(A′) is a grounding of a parfactor φ(A) if A′ can be obtained by instantiating X = logvar(A) with some x ∈ C. The set of groundings of a parfactor g is denoted gr(g).\nExample 2. Parfactor g1 = φ1(Smokes(X))|X ∈ {x1, . . . , xn} represents the set of factors gr(g1) = {φ1(Smokes(x1)), . . . , φ1(Smokes(xn))}.\nA set of parfactors G is a compact way of defining a set of factors F = {f |f ∈ gr(g)∧g ∈ G} and the corresponding probability distribution PG(A) = 1 Z ∏ f∈F φf (Af )."
    }, {
      "heading" : "3.2 Counting Formulas",
      "text" : "Milch et al. (2008) introduced the idea of counting formulas and (parametrized) counting randvars.\nA counting formula is a syntactic construct of the form #Xi∈C [P (X)], where Xi ∈ X is called the counted logvar.\nA grounded counting formula is a counting formula in which all arguments of the atom P (X), except for the counted logvar, are constants. It defines a counting randvar (CRV), the meaning of which is as follows. First, we define the set of randvars it covers as RV (#X∈C [P (X)]) = RV (P (X)|X ∈ C). The value of the CRV is determined by the values of the randvars it covers. More specifically, it is a histogram that indicates, given a valuation of RV (P (X)|X ∈ C), how many different values of X occur for each r ∈ range(P ). Thus, its value is of the form {(r1, n1), (r2, n2), . . . , (rk, nk)}, with ri ∈ range(P ) and ni the\n1. We use the definition of Kisynski and Poole (2009a) for parfactors, as it allows us to simplify the notation.\ncorresponding count. Given a histogram h, we will also write h(v) for the count of v in h. Note that the range of a CRV, i.e., the set of all possible histograms it can take as a value, is determined by k = |range(P )| and |C|.\nExample 3. #X∈{x1,x2,x3}[P (X, y, z)] is a grounded counting formula. It covers the randvars P (x1, y, z), P (x2, y, z) and P (x3, y, z). It defines a CRV, the value of which is determined by the values of these three randvars; if P (x1, y, z) = true, P (x2, y, z) = false and P (x3, y, z) = true, the CRV takes the value {(true, 2), (false, 1)}.\nThe concept of a CRV is somewhat complicated. A CRV behaves like a regular randvar in some ways, but not all. It is a construct that can occur as an argument of a factor, like regular randvars, but in that role it actually stands for a set of randvars, all of which are arguments of the factor. A factor of the form φ∗(· · · ,#X∈C [P (X)], · · · ) is equivalent to a factor of the form φ(· · · , P (X1), P (X2), . . . , P (Xk), · · · ), with P (Xi) all the instantiations of X obtainable by instantiating X with a value from C, and with φ returning for any valuation of the P (Xi) the value that φ ∗ returns for the corresponding histogram.\nExample 4. The factor φ∗(#X∈{x1,x2,x3}[P (X, y, z)]) is equivalent to a factor φ(P (x1, y, z), P (x2, y, z), P (x3, y, z)). If φ\n∗({(true, 2), (false, 1)}) = 0.3, this implies that φ(false, true, true) = φ(true, false, true) = φ(true, true, false) = 0.3.\nAs illustrated in Section 2.4, counting formulas are useful for capturing symmetries within a potential function. Recall the workshop example. Whether a person attends a workshop depends on its topic, and this dependence is the same for each person. We can represent this with a single parfactor φ(T,A(X))|X ∈ {x1, . . . , xn} that represents n ground factors. Eliminating T requires multiplying these n factors into a single factor φ′(T,A(x1), A(x2), . . . , A(xn)) before summing out T . The potential function φ\n′ is highdimensional, so a tabular representation for it would be very costly. However, it contains a certain symmetry: φ′ depends only on how many times each possible value for A(xi) occurs, not on where exactly these occur. By representing the factor using a potential function φ∗ that has only two arguments, T and the CRV #X∈{x1,...,xn}[A(X)], it can be represented more concisely, and computed more efficiently. For instance, to sum out A(X), we do not need to enumerate all possible (2n) value combinations of the A(xi) and sum the corresponding φ′(T,A(x1), . . . , A(xn)), we just need to enumerate all possible (n + 1) values for the histogram of #X∈{x1,...,xn}[A(X)] and sum the corresponding φ\n∗(T,#X∈{x1,...,xn}[A(X)]), each multiplied by its multiplicity.\nNote the complementarity between PRVs and CRVs. While the randvars covered by a PRV occur in different factors, the randvars covered by a CRV occur in one and the same factor. Thus, PRVs impose a symmetry among different factors, whereas CRVs impose a symmetry within a single factor.\nA parametrized counting randvar (PCRV) is of the form #X [P (X)] |CX. In this notation we write the constraint on the counted logvar X as part of the constraint CX on all variables in X. Similar to the way in which a PRV defines a set of randvars through its groundings, a PCRV defines a set of CRVs through its groundings of all variables in X \\ {X}.\nExample 5. #Y [Friend(X,Y )] |C represents a set of CRVs, one for each x ∈ πX(C), indicating the number of friends x has. If C = D(X) × D(Y ) with D(X) = D(Y ) =\n{ann, bob, carl}, we might for instance have #Y [Friend(ann, Y )]|C = {(true, 1), (false, 2)} (Ann has one friend, and two people are not friends with her).\nSome definitions from the previous section need to be extended slightly in order to accommodate PCRVs. First, because CRVs are not regular randvars, they are not included in the set of randvars covered by the PRCV; that is, RV (#Xi [P (X)]|C) = RV (P (X)|C). Second, since a counting formula “binds” the counted logvar (it is no longer a parameter of the resulting PCRV), we define logvar(#Xi [P (X)]) = X\\{Xi}. Thus, generally, logvar(A) refers to all the logvars occurring in A, excluding the counted logvars. Note that logvar(C) remains unchanged: it refers to all logvars in C, whether they appear as counted or not.\nWe end this section with two definitions that will be useful later on.\nDefinition 1 (Count function) Given a constraint CX, for any Y ⊆ X and Z ⊆ X−Y, the function CountY|Z : CX → N is defined as follows:\nCountY|Z(t) = |πY(σZ=πZ(t)(CX))|\nThat is, for any tuple t, this function tells us how many values for Y co-occur with t’s value for Z in the constraint. We define CountY|Z(t) = 1 when Y = ∅.\nDefinition 2 (Count-normalized constraint) For any constraint CX, Y ⊆ X and Z ⊆ X−Y, Y is count-normalized w.r.t. Z in CX if and only if\n∃n ∈ N : ∀t ∈ CX : CountY|Z(t) = n.\nWhen such an n exists, we call it the conditional count of Y given Z in CX, and denote it CountY|Z(CX).\nExample 6. LetX be {P,C} and let the constraint CX be (P,C) ∈ {(ann, eric), (bob, eric), (carl, f inn), (debbie, f inn), (carl, gemma), (debbie, gemma)}. SupposeCX indicates the parent relationship: Ann is a parent of Eric, etc. Then {P} is count-normalized w.r.t. {C} because all children (i.e., all instantiations of C in CX: Eric, Finn and Gemma) have two parents according to CX, or formally, for all tuples t ∈ CX it holds that Count{P}|{C}(t) = 2. Conversely, {C} is not count-normalized w.r.t. {P} because not all parents have equally many children. For instance, Count{C}|{P}((ann, eric)) = 1 (Ann has 1 child), but Count{C}|{P}((carl, f inn)) = 2 (Carl has 2 children)."
    }, {
      "heading" : "4. The GC-FOVE Algorithm: Outline",
      "text" : "We now turn to the problem of performing lifted inference on models specified using the above representation. The algorithm we introduce for this is called GC-FOVE (for Generalized C-FOVE). At a high level, it is similar to C-FOVE (Milch et al., 2008), the current state-of-the-art system in lifted variable elimination, but it differs in the definition and implementation of its operators.\nRecall how standard variable elimination works. It eliminates randvars one by one, in a particular order called the elimination order. Elimination consist of multiplying all factors the randvar occurs in into one factor, then summing out the randvar.\nSimilarly, GC-FOVE visits PRVs (as opposed to individual randvars) in a particular order. Ideally, it eliminates each PRV by multiplying the parfactors in which it occurs into one parfactor, then summing out the PRV, using the lifted multiplication and lifted summing-out operators. However, these operators are not always immediately applicable: it may be necessary to refine the involved parfactors and PRVs to make them so. This is done using other operators, which we call enabling operators.2\nA high-level description of GC-FOVE is shown in Algorithm 1. Like C-FOVE, it makes use of a number of operators, and repeatedly selects and performs one of the possible operators on one or more parfactors. It uses the same greedy heuristic as C-FOVE for this selection, choosing the operation with the minimum cost, where the cost of each operation is defined as the total size (number of rows in tabular form) of all the potentials it creates.\nThe main difference between C-FOVE and GC-FOVE is in the operators used. Four of GC-FOVE’s operators (multiply, sum-out, count-convert and ground-logvar) are a straightforward generalization of a similar operator in C-FOVE, the difference being that we provide definitions that work for any constraint representation language that is closed under relational algebra, instead of definitions that are specific for the constraint language used by C-FOVE. Three other operators (expand, count-normalize and split) also have counterparts in C-FOVE, but need to be redefined more substantially because they directly concern constraint manipulation. The lifted absorption operator (absorb) is completely new.\nGC-FOVE in itself does not specify a particular constraint language. In practice, constraints have to be represented one way or another, so some constraint representation mechanism has to be plugged in. In this article, we propose a tree-based representation mechanism for constraints. Important advantages of this mechanism are that, on the one hand, any extensional set can be represented by these trees, and on the other hand, constraints can still be manipulated efficiently.\nThe generalization of the operators, the new absorption operator, and the tree-based constraint language are the main contributions of this paper. Together, they greatly improve the efficiency of inference, as will be clear from the experimental section. Before describing the operators in detail, we illustrate the importance of using an expressive constraint language."
    }, {
      "heading" : "4.1 Constraint Language",
      "text" : "In C-FOVE, a constraint is a set of pairwise (in)equalities between a single logvar and a constant, or between two logvars. Thus, in a single parfactor, C-FOVE can represent, for instance, Friend(X,Y )|X 6= ann, but not Friend(X,Y )|(X,Y ) ∈ {(ann, bob), (bob, carl)}). Table 1 provides some more examples of PRVs that C-FOVE can/cannot represent, and Figure 3 illustrates this visually. Basically, C-FOVE can only use conjunctive constraints, not disjunctive ones, and C-FOVE’s operators are defined to operate directly on this representation. GC-FOVE, on the other hand, allows a constraint to be any relation on the logvars, and can therefore handle all these PRVs. Because it has no restrictions whatsoever regarding the constraints it can handle, it can maximally exploit opportunities for lifting.\n2. Technically speaking, multiplication is also an enabling operator as summing-out can only be applied after multiplication.\nThe expressiveness of the constraint representation language, and the way the constraints are handled by the operators, are crucial to the efficiency of lifted variable elimination. The reason is that variables continuously need to be re-grouped (i.e., constraints need to be rewritten) during inference. For instance, we can multiply φ1(P (X))|{x1, x2, x3} and φ2(P (X))|{x1, x2, x3} directly, resulting in a parfactor of the form φ12(P (X))|{x1, x2, x3}, but we cannot multiply φ1(P (X))|{x1, x2, x3} and φ2(P (X))|{x2, x3, x4, x5} into a single parfactor because their PRVs do not match. The solution is to split constraints and parfactors so that matching parfactors arise. In this particular case, a model with three parfactors arises: φ1(P (x1)), φ12(P (X))|{x2, x3} and φ2(P (X))|{x4, x5}. GC-FOVE’s operations result in this model. C-FOVE, however, when splitting constraints, separates off one tuple at a time (“splitting based on substitution”, Milch et al., 2008), which here results in four parfactors: φ1(P (x1)); φ12(P (X))|X 6= x1,X 6= x4,X 6= x5; φ2(P (x4)); and φ2(P (x5)) (assuming the domain of X is {x1, x2, . . . , x5}). In this case, C-FOVE could in fact represent the separate factors φ2(P (x4)) and φ2(P (x5)) as one parfactor φ2(P (X))|X 6= x1,X 6= x2,X 6= x3, but it does not do so (only the intersection of two constraints is kept on the lifted level),\nand in general, for non-unary predicates, this is not possible, as Table 1 shows. Because of its restricted constraint language, C-FOVE often has to create finer-grained partitions than necessary. GC-FOVE, because it uses an extensionally complete constraint language, does not suffer from this problem."
    }, {
      "heading" : "4.2 Lifted Absorption",
      "text" : "Absorption (van der Gaag, 1996) is an additional operator in VE that is known to increase efficiency. It consists of removing a random variable from a model when its valuation is known, and rewriting the model into an equivalent one that does not contain the variable. C-FOVE, like its predecessors, does not use absorption, and including it might in fact have detrimental effects due to breaking of symmetries. GC-FOVE’s extensionally complete constraint language, however, not only makes it possible to use absorption more effectively, it even allows for lifting it."
    }, {
      "heading" : "4.3 Summary of Contributions",
      "text" : "We are now at a point where we can summarize the contributions of this work more precisely.\n1. We present the first description of lifted variable elimination that decouples the lifted inference algorithm from the constraint representation it uses. This is done by taking the C-FOVE algorithm and redefining its operators so that they become independent from the underlying constraint mechanism. This is achieved by defining the operators\nin terms of relational algebra operators. This redefinition generalizes the operators and clarifies on a higher level how they work.\n2. We present a mechanism for representing constraints that is extensionally complete. It is closed under the relational algebra operators, and allows for executing them efficiently. In itself, this is a minor contribution, but it is necessary in order to obtain an operational system.\n3. We present a new operator, called lifted absorption.\n4. We experimentally demonstrate the practical impact of the above contributions.\n5. We contribute the software itself.\nContributions 1 and 3 (our main contributions) are the subject of Section 5. Contribution 2 is detailed in Section 6, and Contribution 4 in Section 7. Contribution 5 is at http://dtai.cs.kuleuven.be/ml/systems/gc-fove."
    }, {
      "heading" : "5. GC-FOVE’s Operators",
      "text" : "This section provides detailed information on GC-FOVE’s operators. These can conceptually be split into two categories: operators that manipulate potential functions, and operators that refine the model so that the first type of operators can be applied. We will start with three operators that belong to the first category: lifted multiplication, lifted summing-out and counting conversion. These can be seen as generalized versions of the corresponding C-FOVE operators; algorithmically, they are similar. Next, we discuss splitting, shattering, expansion, and count normalization. Because they operate specifically on the constraints, these differ more strongly from C-FOVE’s operators. We will systematically compare them to the latter, showing each time that C-FOVE’s constraint language and operators force it to create more fine-grained models than necessary, while GC-FOVE, because of its extensionally complete constraint language, can always avoid this: whatever the set of interchangeable randvars is, this set can be represented by one constraint. Finally, we discuss lifted absorption, which is completely new, and grounding, which is again similar to its C-FOVE counterpart.\nIn the following, G refers to a model (i.e., a set of parfactors), and G1 ∼ G2 means that models G1 and G2 define the same probability distribution."
    }, {
      "heading" : "5.1 Lifted Multiplication",
      "text" : "The lifted multiplication operator multiplies whole parfactors at once, instead of separately multiplying the ground factors they cover (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008). Figure 4 illustrates this for two parfactors g1 = φ1(S(X))|C and g2 = φ2(S(X), A(X))|C, where C = (X ∈ {x1, . . . , xn}). Lifted multiplication is equivalent to n multiplications on the ground level.\nThe above illustration is deceptively simple, for several reasons. First, the naming of the logvars suggests that logvar X in g1 corresponds to X in g2. In fact, g2 could have multiple logvars, with different names. An alignment between the parfactors is necessary, showing how logvars in different parfactors correspond to each other (de Salvo Braz, 2007).\nThe alignment must constrain the aligned logvars to exactly the same values in g1 and g2 (otherwise, they cannot give identical PRVs in both parfactors). We formalize this as follows.\nDefinition 3 (substitution) A substitution θ = {X1 → t1, . . . ,Xn → tn} = {X → t} maps each logvar Xi to a term ti, which can be a constant or a logvar. When all ti are constants, θ is called a grounding substitution, and when all are different logvars, a renaming substitution. Applying a substitution θ to an expression α means replacing each occurrence of Xi in α with ti; the result is denoted αθ.\nDefinition 4 (alignment) An alignment θ between two parfactors g = φ(A)|C and g′ = φ′(A′)|C ′ is a one-to-one substitution {X → X′}, with X ⊆ logvar(A) and X′ ⊆ logvar(A′), such that ρθ(πX(C)) = πX′(C ′) (with ρ the attribute renaming operator).\nAn alignment tells the multiplication operator that two atoms in two different parfactors represent the same PRV, so it suffices to include it in the resulting parfactor only once. Including it twice is not wrong, but less efficient: some structure in the parfactor is then lost. For this reason, it is useful to look for “maximal” alignments which map as many PRVs to each other as possible.\nExample 7. Consider g1 = φ1(S(X), F (X,Y ))|CX,Y and g2 = φ2(S(X ′), F (X ′, Y ′))|CX′,Y ′ with CX,Y = CX′,Y ′ = {xi} n 1 × {yj} m 1 . Using the maximal alignment {X → X ′, Y →\nY ′)}, we get the product parfactor φ3(S(X), F (X,Y ))|CX,Y . This alignment establishes a 1:1 association between each ground factor φ1(S(xi), F (xi, yj)) and the corresponding φ2(S(xi), F (xi, yj)). If, however, we multiply g1 and g2 with the alignment {X → X\n′}, the result is a parfactor φ′3(S(X), F (X,Y ), F (X,Y ′))|(X,Y, Y ′) ∈ {xi} n 1 × {yj} m 1 × {yk} m 1 , which for each xi unnecessarily multiplies each factor φ1(S(xi), F (xi, yj)) with all factors φ2(S(xi), F (xi, yk)), k = 1, . . . ,m. In other words, it unnecessarily creates a direct dependency between all pairs of randvars F (xi, yj), F (xi, yk).\nA second complication is that a single randvar may participate in multiple factors within a certain parfactor, and the number of such factors it appears in may differ across parfactors. Consider parfactors g1 = φ1(S(X))|X ∈ {xi} n 1 and g2 = φ2(S(X), F (X,Y ))|(X,Y ) ∈ {xi} n 1×{yi} m 1 . For each xi, φ1(S(xi)) shares randvar S(xi) withm factors φ2(S(xi), F (xi, yj)), j = 1, . . . ,m. Multiplication should result in a single parfactor φ3(S(xi), F (xi, Y ))|Y ∈ {yi} m 1 that covers m factors φ3(S(xi), F (xi, yj)), and is equivalent to the product of one factor φ1(S(xi)) and m factors φ2(S(xi), F (xi, yj)). This means we must find a φ3 such that ∀v,w : φ3(v,w) m = φ1(v) ∏m i=1 φ2(v,w). This gives φ3(v,w) = φ1(v) 1/mφ2(v,w). The exponentiation of φ1 to the power 1/m is called scaling. The result of this multiplication for a single xi is the same regardless of xi, so finally, the product of the parfactors g1 and g2 will be the parfactor\nφ3(S(X), F (X,Y )) = φ1(S(X)) 1/m · φ2(S(X), F (X,Y )) | (X,Y ) ∈ {xi} n 1 × {yj} m 1 .\nFigure 5 illustrates this multiplication graphically. An alignment between parfactors is called 1 : 1 if all non-counted logvars in the parfactors are mapped to each other, and is called m:n otherwise. Multiplication based on an m:n alignment involves scaling, and requires that the non-aligned logvars be count-normalized (Definition 2, p. 406) with respect to the aligned logvars in the constraints (otherwise there is no single scaling exponent that is valid for the whole parfactor).\nOperator 1 formally defines the lifted multiplication. Note that this definition does not assume any specific format for the constraints."
    }, {
      "heading" : "5.2 Lifted Summing-Out",
      "text" : "Once a PRV occurs in only one parfactor, it can be summed out from that parfactor (Milch et al., 2008). We begin with an example of lifted summing-out, which will help motivate the formal definition of the operator.\nExample 8. Consider parfactor g = φ(S(X), F (X,Y ))|C, in which C = {(xi, yi,j) : i ∈ {1, . . . , n}, j ∈ {1, . . . ,m}} (Figure 6). Note that Y is count-normalized w.r.t X in C. Assume we want to sum out randvars F (xi, yi,j) ∈ RV (F (X,Y )|C) on the ground level. Each randvar F (xi, yi,j) appears in exactly one ground factor φ(S(xi), F (xi, yi,j)) (see Figure 6 (middle)). We can therefore sum out each F (xi, yi,j) from its factor independently from the others, obtaining a factor φ′(S(xi)) = ∑\nF (xi,yi,j) φ(S(xi), F (xi, yi,j)). Since the m ground\nfactors φ(S(xi), F (xi, yi,j)) have the same potential φ, summing out their second argument always results in the same potential φ′, so we can compute φ′ just once and, instead of storing m copies of the resulting factor φ′(S(xi)), store a single factor φ ′′(S(xi)) = φ ′(S(xi))\nm. In the end, we obtain n such factors, one for each S(xi), i = 1, . . . , n. We can represent\nthis result using a single parfactor g′ = φ′′(S(X))|C ′, with C ′ = {x1, . . . , xn} = πX(C). Lifted summing-out directly computes g′ from g in one operation. Note that to have a single exponent for all φ′′, Y must be count-normalized w.r.t. X in C.\nLike its C-FOVE counterpart, our lifted summing-out operator requires a one-to-one mapping between summed-out randvars and factors; that is, each summed-out randvar appears in exactly one factor, and all these factors are different. This is guaranteed when the eliminated atom contains all the logvars of the parfactor, since there is a different ground factor for each instantiation of the logvars. Further, lifted summing-out may result in identical factors on the ground level, which is exploited by computing one factor and exponentiating. This is the case when there is a logvar that occurs only in the eliminated atom, but not in the other atoms (such as Y in F (X,Y ) in the above example).\nAs already illustrated in Section 2.4, counting randvars require special attention in lifted summing-out. A formula like φ(#X [P (X)])|X ∈ {x1, . . . , xk} is really a shorthand for a factor φ(P (x1), P (x2), . . . , P (xk)) whose value depends only on how many arguments take particular values. In principle, we need to sum out over all combinations of values of P (Xi). We can replace this by summing out over all values of #X [P (X)], on the condition that we take the multiplicities of the latter into account. The multiplicity of a histogram\nOperator multiply Inputs: (1) g1 = φ1(A1)|C1: a parfactor in G (2) g2 = φ2(A2)|C2: a parfactor in G (3) θ = {X1 → X2}: an alignment between g1 and g2 Preconditions: (1) for i = 1, 2: Yi = logvar(Ai) \\Xi is count-normalized w.r.t. Xi in Ci Output: φ(A)|C, with (1) C = ρθ(C1) ⊲⊳ C2. (2) A = A1θ ∪A2, and (3) for each valuation a of A, with a1 = πA1θ(a) and a2 = πA2(a) :\nφ(a) = φ 1/r2 1 (a1) · φ 1/r1 2 (a2), with ri = CountYi|Xi(Ci)\nPostcondition: G ∼ G \\ {g1, g2} ∪ {multiply(g1, g2, θ)}\nOperator 1: Lifted multiplication. The definition assumes, without loss of generality, that the logvars in the parfactors are standardized apart, i.e., the two parfactors do not share variable names (this can always be achieved by renaming logvars).\nh = {(r1, n1), (r2, n2), . . . , (rk, nk)} is a multinomial coefficient, defined as\nMul(h) = n!\n∏k i=1 ni!\n.\nAs multiplicities should only be taken into account for (P)CRVs, never for regular PRVs, we define for each PRV A and for each value v ∈ range(A): Mul(A, v) = 1 if A is a regular PRV, and Mul(A, v) = Mul(v) if A is a PCRV. This Mul function is identical to Milch et al.’s (2008) num-assign.\nWith all this in mind, the formal definition of the lifted summing-out in Operator 2 is mostly self-explanatory. Precondition (1) ensures that all randvars in the summed-out P(C)RV occur exclusively in this parfactor. Precondition (2) ensures that each summed out randvar occurs in exactly one, separate, ground factor. Precondition (3) ensures that logvars occurring exclusively in the eliminated PRV are count-normalized with respect to the other logvars in that PRV, so that there is one unique exponent for exponentiation."
    }, {
      "heading" : "5.3 Counting Conversion",
      "text" : "Counting randvars may be present in the original model, but they can also be introduced into parfactors by an operation called counting conversion (Milch et al., 2008) (see also Section 2.4). To see why this is useful, consider a parfactor g = φ(S(X), F (X,Y ))|C, with C = {xi} n i=1×{yj} m j=1, and assume we want to eliminate S(X)|C. To do that, we first need to make sure each S(xi) occurs in only one factor. On the ground level, this can be achieved for a given S(xi) by multiplying all factors φ(S(xi), F (xi, yj)) in which it occurs. This results in a single factor φ′(S(xi), F (xi, y1), . . . , F (xi, ym)) = ∏\nj φ(S(xi), F (xi, yj)) (see Figure 7). This is a high-dimensional factor, but because it equals a product of identical potentials φ, its F (xi, yj) arguments are mutually interchangeable: all that matters is how often values v1, v2, . . . occur among them, not where they occur. This is exactly the kind of symmetry\nthat CRVs aim to exploit. The factor φ′(S(xi), F (xi, y1), . . . , F (xi, ym)) can therefore be replaced by a two-dimensional φ′′(S(xi), h) with h a histogram that indicates how often each possible value in the range of F (xi, yj) occurs. Thus, by introducing a CRV, we can define a two-dimensional φ′′ with that CRV as an argument, as opposed to the high-dimensional φ′. As argued in Section 2.4, this reduces the size of the potential function, and hence computational complexity, exponentially.\nIn many situations where lifted elimination cannot immediately be applied, counting conversion makes it applicable. The conditions of the sum-out operator (Section 5.2) state that an atom Ai can only be eliminated from a parfactor g if Ai has all the logvars in g. When an atom has fewer logvars than the parfactor, counting conversion modifies the parfactor by replacing another atom Aj by a counting formula, which removes this counted logvar from logvar(A). For instance, in the above example, S(X) does not have the logvar Y in g = φ(S(X), F (X,Y ))|C and cannot be eliminated from the original parfactor g, but a counting conversion on Y replaces F (X,Y ) with #Y [F (X,Y )], allowing us to sum out S(X) from the new parfactor g′ = φ(S(X),#Y [F (X,Y )])|C.\nOperator sum-out Inputs: (1) g = φ(A)|C: a parfactor in G (2) Ai: an atom in A, to be summed out from g1 Preconditions (1) For all PRVs V, other than Ai|C, in model G: RV (V) ∩RV (Ai|C) = ∅ (2) Ai contains all the logvars X ∈ logvar(A) for which πX(C) is not singleton. (3) Xexcl = logvar(Ai) \\ logvar(A \\Ai) is count-normalized w.r.t. Xcom= logvar(Ai) ∩ logvar(A \\ Ai) in C Output: φ′(A′)|C ′, such that (1) A′ = A \\ Ai (2) C ′ = πXcom(C) (3) for each assignment a′ = (. . . , ai−1, ai+1, . . . ) to A ′,\nφ ′ (. . . , ai−1, ai+1, . . . ) = ∑\nai∈range(Ai) Mul(Ai, ai) φ(. . . , ai−1, ai, ai+1, . . . )\nr\nwith r = CountXexcl|Xcom(C)\nPostcondition: PG\\{g}∪{sum-out(g,Ai)} = ΣRV (Ai|C)PG\nOperator 2: The lifted summing-out operator.\nOperator 3 formally defines counting conversion. It is mostly self-explanatory, apart from the preconditions. Precondition 1 makes sure that counting conversion, on the ground level, corresponds to multiplying factors that only differ in one randvar (i.e., are the same up to their instantiation of the counted logvar). Precondition 2 guarantees that the resulting histograms have the same range. Precondition 3 is more difficult to explain. It imposes a kind of independence between the logvar to be counted and already occurring counted logvars. Though not explicitly mentioned there, this precondition is also required for CFOVE’s counting operation; it implies that no inequality constraint should exist between X and any counted logvar X#. A similar condition for FOVE’s counting elimination is mentioned by de Salvo Braz (2007).\nTo see why precondition 3 is necessary, consider the parfactor g = φ(S(X),#Y [A(Y )]) |(X,Y ) ∈ {(x1, y2), (x1, y3), (x2, y1), (x2, y3), (x3, y1), (x3, y2)}, which does not satisfy it. This parfactor represents three factors of the form φ(S(xi),#Y [A(Y )])|Y ∈ {y1, y2, y3}\\{yi}, which contribute to the joint distribution with the product\nφ(S(x1),#Y ∈{y2,y3}[A(Y )]) · φ(S(x2),#Y ∈{y1,y3}[A(Y )]) · φ(S(x3),#Y ∈{y1,y2}[A(Y )]).\nCounting conversion on logvar X turns g into a factor of the form\nφ′(#X [S(X)],#Y [A(Y )])\nthat should be equivalent. Note that φ′ depends only on #X [S(X)] and #Y [A(Y )]. Now consider valuations V1: [S(x1), S(x2), S(x3), A(y1), A(y2), A(y3)] = [t, t, f, t, t, f ] and V2: [S(x1), S(x2), S(x3), A(y1), A(y2), A(y3)] = [t, t, f, t, f, t]. For both valuations, #X [S(X)] = (2, 1) and #Y [A(Y )] = (2, 1), so φ\n′(#X [S(X)],#Y [A(Y )]) must return the same value under V1 and V2. The original parfactor, however, returns φ(S(t), (1, 1)) · φ(S(t), (1, 1)) · φ(S(f), (2, 0)) under V1, and φ(S(t), (1, 1)) · φ(S(t), (2, 0)) · φ(S(f), (1, 1))\nunder V2, which may be different. Since the original parfactor can distinguish valuations that no factor of the form φ′(#X [S(X)],#Y [A(Y )]) can, counting conversion cannot be applied in this case.\nIn contrast, consider g′ = φ(S(X),#Y [A(Y )])|(X,Y ) ∈ {x1, x2, x3}×{y1, y2, y3}, which is similar to g, except that its constraint satisfies precondition 3. All three factors represented by g′ differ only in their first argument, randvar S(xi); they have the same counting randvar #Y [A(Y )]|Y ∈ {y1, y2, y3} as their second argument (this was not the case for g). Their product, thus, can be represented by a parfactor φ′(#X [S(X)],#Y [A(Y )])|(X,Y ) ∈ {x1, x2, x3} × {y1, y2, y3}, which is derived from g ′ by a counting conversion."
    }, {
      "heading" : "5.4 Splitting and Shattering",
      "text" : "When the preconditions for lifted multiplication, lifted summing-out and counting conversion are not fulfilled, it is necessary to reformulate the model in terms of parfactors that do fulfill them. For instance, if g1 = φ1(S(X))|X ∈ {x1, x2, x3} and g2 = φ2(S(X))|X ∈ {x1, x2, x3, x4, x5}, we cannot multiply g1 and g2 directly without creating unwanted de-\nOperator count-convert Inputs: (1) g = φ(A)|C: a parfactor in G (2) X: a logvar in logvar(A) Preconditions (1) there is exactly one atom Ai ∈ A with X ∈ logvar(Ai) (2) X is count-normalized w.r.t logvar(A) \\ {X} in C (3) for all counted logvars X# in g: πX,X#(C) = πX(C)× πX#(C) Output: φ′(A′)|C, such that (1) A′ = A \\ {Ai} ∪ {A ′ i} with A ′ i = #X [Ai] (2) for each assignment a′ to A′ with a′i = h:\nφ′(. . . , ai−1, h, ai+1, . . . ) = ∏\nai∈range(Ai) φ(. . . , ai−1, ai, ai+1, . . . )\nh(ai)\nwith h(ai) denoting the count of ai in histogram h Postcondition: G ∼ G \\ {g} ∪ {count-convert(g,X)}.\nOperator 3: The counting conversion operator.\npendencies. However, we can replace g2 with g2a = φ2(S(X))|X ∈ {x1, x2, x3} and g2b = φ2(S(X))|X ∈ {x4, x5}. The resulting model is equivalent, but in this new model, we can multiply g1 with g2a, resulting in g3 = φ3(S(X))|X ∈ {x1, x2, x3}.\nThe above is a simple case of splitting parfactors (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008). Basically, splitting two parfactors partitions each parfactor into a part that is shared with the other parfactor, and a part that is disjoint. The goal is to rewrite the P(C)RVs and parfactors into a proper form. Two P(C)RVs (V1,V2) are proper if RV (V1) and RV (V2) are either identical or disjoint; two parfactors are proper if all their P(C)RVs are proper. A pair of parfactors can be written into proper form by applying the following procedure, until all their P(C)RVs are proper. Choose a P(C)RV V1 from one parfactor, compare it to a P(C)RV V2 from the other, and rewrite the first parfactor such that V1 is split into two parts: one that is disjoint from V2 and one that is shared with V2. All the parfactors in the model can be made proper w.r.t. each other by repeatedly applying this rewrite until convergence. This is called shattering the model.\nIt is simpler to rewrite a PRV into the proper form than a PCRV. We describe the operator that handles PRVs, namely split, in this section and discuss the operator that handles PCRVs, namely expand, in the following section. Before defining the split operator, we provide the following auxiliary definitions, which will also be used later on.\nDefinition 5 (Splitting on overlap) Splitting a constraint C1 on its Y-overlap with C2, denoted C1/YC2, partitions C1 into two subsets, containing all tuples for which the Y part occurs or does not occur, respectively, in C2. C1/YC2 = {{t ∈ C1|πY(t) ∈ πY(C2)}, {t ∈ C1|πY(t) /∈ πY(C2)}}.\nDefinition 6 (Parfactor partitioning) Given a parfactor g = φ(A)|C and a partition C = {Ci} n i=1 of C, partition(g,C) = {φ(A)|Ci} n i=1.\nOperator 4 defines splitting of parfactors. Note that, in the operator definition, for simplicity, we assume that A = A′ = P (Y), which means that the logvars used in A and\nOperator split Inputs: (1) g = φ(A)|C: a parfactor in G (2) A = P (Y): an atom in A (3) A′ = P (Y)|C ′ or #Y [P (Y)]|C ′ Output: partition(g,C), with C = C/YC ′ \\ {∅} Postcondition G ∼ G \\ {g} ∪ split(g,A,A′)\nOperator 4: The split operator.\nA′ must be the same, in the same order. We can always rewrite the model such that any two PRVs with the same predicate are in this form. For this, we rewrite the parfactors as follows: (i) if the parfactors share logvars, we first standardize apart the logvars between two parfactors, (ii) linearize each atom in which some logvar occurs more than once, i.e., rewrite it such that it has a distinct logvar in each argument, and (iii) apply a renaming substitution on the logvars such that the concerned atoms have the same logvars. For instance, consider the two parfactors g1 = φ1(P (X,X))|X ∈ C1 and g2 = φ2(P (Y,Z))|(Y,Z) ∈ C2. The logvars of the two parfactors are already different, so there is no need for standardizing them apart. However, the atom P (X,X) in g1 is not linearized yet. To linearize it, we rewrite g1 into the form φ1(P (X,X ′))|(X,X ′) ∈ C ′1, where C ′ 1 = {(x, x)|x ∈ C1}. Finally, we rename the logvarsX andX ′ to Y and Z, respectively, to derive φ1(P (Y,Z))|(Y,Z) ∈ C ′ 1. This brings the atom P (X,X) into the desired form P (Y,Z). For ease of exposition, we will not explicitly mention this linearization and renaming; whenever two PRVs from different parfactors are compared, any notation suggesting that they have the same logvars is to be interpreted as “have the same logvars after linearization and renaming”.\nWhen GC-FOVE wants to multiply two parfactors, it first checks for all pairs A1|C1, A2|C2 (one from each parfactor) whether they are proper. If a pair is found that is not proper, this means A1 and A2 are both of the form P (Y), with different (but overlapping) instantiations for Y in C1 and C2. The pair is then split on Y.\nExample 9. Consider g1 = φ1(N(X,Y ), R(X,Y,Z))|C1 with C1 = (X,Y,Z) ∈ {xi} 50 i=1 × {yi} 50 i=1 × {zi} 5 i=1, and g2 = φ2(N(X,Y ))|C2 with C2 = (X,Y ) ∈ {x2i} 25 i=1 × {yi} 50 i=1. First, we compare the PRVs N(X,Y )|C1 and N(X,Y )|C2. These PRVs partially overlap, so splitting is necessary. To split the parfactors, we split C1 and C2 on their (X,Y)-overlap. This partitions C1 into two sets: C com 1 = {x2i} 25 i=1 × {yi} 50 i=1 × {zi} 5 i=1, and C excl 1 = C1 \\ Ccom1 = {x2i−1} 25 i=1 × {yi} 50 i=1 × {zi} 5 i=1. C2 does not need to be split, as it has no tuples for which the (X,Y)-values do not occur in C1. After splitting the constraints, we split the parfactors accordingly: g1 is split into two parfactors g com 1 = φ(N(X,Y ), R(X,Y,Z))|C com 1 and gexcl1 = φ(N(X,Y ), R(X,Y,Z))|C excl 1 , and parfactor g2 remains unmodified.\nOur splitting procedure splits any two PRVs into at most two partitions each. Similarly, the involved parfactors are split into at most two partitions each. This strongly contrasts with C-FOVE’s approach to splitting. C-FOVE operates per logvar, and splits off each value in a separate partition (splitting based on substitution) (Poole, 2003; Milch et al., 2008). Thus, it may require many splits where GC-FOVE requires just one. In\nthe above example, instead of gexcl1 = φ(N(X,Y ), R(X,Y,Z))|C excl 1 , C-FOVE ends up with 1250 parfactors φ(N(x1, y1), R(x1, y1, Z))|{zi} 5 i=1, φ(N(x1, y2), R(x1, y2, Z))|{zi} 5 i=1, . . . , φ(N(x3, y1), R(x3, y1, Z))|{zi} 5 i=1, . . . , φ(N(x49, y50), R(x49, y50, Z))|{zi} 5 i=1.\nThe reason why GC-FOVE can always split into at most two parfactors, yielding much coarser partitions than C-FOVE, is that it assumes an extensionally complete constraint language, whereas C-FOVE allows only pairwise (in)equalities, forcing it to split off each element separately."
    }, {
      "heading" : "5.5 Expansion of Counting Formulas",
      "text" : "When handling parfactors with counting formulas, to rewrite a P(C)RV into the proper from, we employ the operation of expansion (Milch et al., 2008). When we split one group of randvars RV (V) into a partition {RV (Vi)} m i=1, any counting randvar γ that counts the values of RV (V) needs to be expanded, i.e., replaced by a group of counting randvars {γi} m i=1, where each γi counts the values of randvars in RV (Vi). In parallel with this, the potential that originally had V as an argument must be replaced by a potential that has all the Vi as arguments; we call this potential expansion.\nExample 10. Suppose we need to split g1 = φ1(#X [S(X)])|C1 and g2 = φ2(S(X))|C2, with C1 = {x1, . . . , x100} and C2 = {x1, . . . , x40}. C1 is split into C com 1 = C1 ∩C2 = {x1, . . . x40} and Cexcl1 = C1 \\ C2 = {x41, . . . x100}. Consequently, the original group of randvars in parfactor g1, namely {S(x1), . . . S(x100)}, is partitioned into V com 1 = {S(x1), . . . S(x40)} and Vexcl1 = {S(x41), . . . S(x100)}. To preserve the semantics of the original counting formula, we now need two separate counting formulas, one for Vcom1 and one for V excl 1 , and we need to replace the original potential φ1(#X [S(X)]) by φ ′ 1(#Xcom [S(Xcom)],#Xexcl [S(Xexcl)]), where φ′1() depends only on the sum of the two new counting randvars #Xcom[S(Xcom)] and #Xexcl [S(Xexcl)]. The end effect is that the parfactor g1 is replaced by the new parfactor φ′1(#Xcom[S(Xcom)],#Xexcl [S(Xexcl)])|C ′ 1, where C ′ 1 = C com 1 × C excl 1 .\nTo explain expansion, we begin with the case of (non-parametrized) CRVs and then move to the general case of expansion for PCRVs."
    }, {
      "heading" : "5.5.1 Expansion of CRVs",
      "text" : "First consider the simplest possible type of CRV: #X [P (X)]|C. It counts for how many values of X in C, P (X) has a certain value. When C is partitioned, X must be counted within each subset of the partition.\nIn the following, we assume C is partitioned into two non-empty subsets C1 and C2. If one of them is empty, the other equals C, which means the CRV can be kept as is and no expansion is needed.\nIn itself, splitting #X [P (X)]|C into #X [P (X)]|C1 and #X [P (X)]|C2 is trivial, but a problem is that both of the resulting counting formulas will occur in one single parfactor, and a constraint is always associated with a parfactor, not with a particular argument of a parfactor. Thus, we need to transform φ(#X [P (X)])|C into a parfactor of the form φ′(#X1 [P (X1)],#X2 [P (X2)])|C\n′, where the single constraint C ′ expresses that X1 can take only values in C1, and X2 only values in C2. It is easily seen that C ′ = ρX→X1C1×ρX→X2C2\nsatisfies this condition. Further, to preserve the semantics, φ′ should, for any count of X1 and X2, give the same result as φ with the corresponding count of X. The function\nφ′(h1, h2) = φ(h1 ⊕ h2),\nwith ⊕ denoting summation of histograms, has this property. Indeed, the histogram for X1 (resp. X2) in C\n′ is equal to that for X in C1 (resp. C2), and since {C1, C2} is a partition of C, the sum of these histograms equals the histogram for X in C.\nMore generally, consider a non-parametrized CRV #X [P (X)]|C, with X ∈ X meaning that πX\\{X}(C) is singleton. The constraint C\n′ = πX\\{X}(C) × (πX1(ρX→X1C1) × πX2(ρX→X2C2)) joins this singleton with the Cartesian product of πX(C1) and πX(C2), and is equivalent to the constraint ρX→X1(C1) ⊲⊳ ρX→X2(C2). The result is again such that counting X1 (X2) in C\n′ is equivalent to counting X in C1 (C2), while the constraint on all other variables remains unchanged. This shows that a parfactor φ(A,#X [P (X)])|C, for any partition {C1, C2} of C with C1 and C2 non-empty, can be rewritten in the form φ′(A,#X1 [P (X)],#X2 [P (X)])|C\n′, where C ′ = ρX→X1(C1) ⊲⊳ ρX→X2(C2). Note that the ranges of the counting formulas in φ′ (the hi arguments) depend on the\ncardinality of C1 and C2, which we will further denote as n1 and n2 respectively."
    }, {
      "heading" : "5.5.2 Expansion of PCRVs",
      "text" : "Consider the case where πX\\{X}(C) is not a singleton, i.e., we have a parametrized CRV V that represents a group of CRVs, each counting the values of a subset of RV (V). Given a partitioning of the constraint C, we need to expand each underlying CRV and the corresponding potential. The constraint C ′ = ρX→X1(C1) ⊲⊳ ρX→X2(C2) remains correct (for non-empty C1, C2), even when πX\\{X}(C) is no longer singleton: it associates the correct values of X1 and X2 with each tuple in πX\\{X}(C). However, because the result of potential expansion depends on the size of the partitions, n1 and n2, only those CRVs that have the same (n1,n2) result in identical potentials after expansion, and can be grouped in one parfactor. To account for this, PCRV expansion first splits the PCRV into groups of CRVs that have the same “joint count” (n1, n2), then applies for each group the corresponding potential expansion.\nTo formalize this, we first provide the following auxiliary definitions.\nDefinition 7 (Group-by) Given a constraint C and a function f : C → R, GroupBy(C, f) = C/ ∼f , with x ∼f y ⇔ f(x) = f(y) and / denoting set quotient. That is, Group-By(C, f) partitions C into subsets of elements that have the same result for f .\nDefinition 8 (Joint-count) Given a constraint C over variables X, partitioned into {C1, C2}, and a counted logvar X ∈ X; then for any t ∈ C, with L = X \\ {X} and l = πL(t),\njoint-countX,{C1,C2}(t) = (|πX(σL=l(C1))|, |πX (σL=l(C2))|).\nWhen a PCRV V = #Xi [P (X)] |C in a parfactor g partially overlaps with another PRV A′|C ′ in the model, expansion performs the following on g: (1) partition C on its X-overlap with C ′, resulting in C/XC\n′; (2) partition C into C = group-by(C, joint-countX,C/XC′) (this corresponds to a partition of V into CRVs that have the same number of randvars in each of the common and exclusive partitions in C/XC ′); (3) split g, based on\nOperator expand Inputs: (1) g = φ(A)|C: a parfactor in G (2) A = #X [P (X)]: a counting formula in A (3) A′ = P (X)|C ′ or #Y [P (X)]|C ′ Output: {gi = φ ′ i(A ′ i)|C ′ i} n i=1 where (1) C/XC ′ = {Ccom, Cexcl} (2) {C1, . . . , Cn} = group-by(C, joint-countX,C/XC′) (3) for all i where Ci ⊲⊳ C com = ∅ or Ci ⊲⊳ C excl = ∅: φ′i = φ, A ′ i = A, C ′ i = Ci (4) for all other i: (5) C ′i = πlogvar(A)(Ci) ⊲⊳ (ρX→Xcom(C com) ⊲⊳ ρX→Xexcl(C excl))\n(6) A′i = A \\ {A} ∪ {Aθcom, Aθexcl} with θcom = {X → Xcom}, θexcl = {X → Xexcl} (7) for each valuation (l, hcom, hexcl) of A ′ i, φ ′ i(l, hcom, hexcl) = φ(l, hcom ⊕ hexcl)\nPostcondition G ∼ G \\ {g} ∪ expand(g,A,A′)\nOperator 5: The expansion operator.\nC = {C1, . . . , Cn}, resulting in parfactors g1, . . . , gn that each require a distinct expanded potential; (4) in each gi, replace potential φ with its expanded version. The formal definition of expansion is given in Operator 5.\nExample 11. Suppose we need to split parfactors g = φ(#Y [F (X,Y )])|C and g ′ = φ′(F (X,Y ))|C ′, with C = {ann, bob, carl} × {dave, ed, fred, gina} and C ′ = {ann, bob} × {dave, ed}. Assume F stands for friendship; #Y [F (X,Y )]|C counts the number of friends and non-friends each X has in C. The random variables covered by PCRV #Y [F (X,Y )] |C partially overlap with those of F (X,Y ) |C ′. If we need to split C on overlap with C ′, yielding Ccom and Cexcl, we need to replace the original PCRV with separate PCRVs for Ccom and Cexcl. But PCRVs require count-normalization, and the fact that Y is count-normalized w.r.t. X in C does not necessarily imply that the same holds in Ccom and Cexcl. That is why, in addition to the split on overlap, we need an orthogonal partitioning of C according to the joint counts. Within a subset Ci of this partitioning, Y will be count-normalized w.r.t. X in Ccomi and in C excl i .\nWe follow the four steps outlined above. Figure 8 illustrates these steps. First, we find the partition C/X,Y C\n′ = {Ccom, Cexcl} with Ccom = {ann, bob} × {dave, ed} and Cexcl = {ann, bob} × {fred, gina} ∪ {carl} × {dave, ed, fred, gina}. Inspecting the joint counts, we see that Ccom contains 2 possible friends for Ann or Bob (namely Dave and Ed), but 0 for Carl, whereas Cexcl contains 2 possible friends for Ann or Bob and 4 for Carl. Formally, joint-countY,C/X,Y C′(t) equals (2,2) for πX(t) = ann or πX(t) = bob, and equals (0,4) for πX(t) = carl. So, within C com and Cexcl, Y is no longer count-normalized with respect to X. We therefore partition C into subsets {C1, C2} = group-by(C, joint-countY,C/X,Y C′), which gives C1 = {ann, bob}×{dave, ed, fred, gina} and C2 = {carl}×{dave, ed, fred, gina}. For each Ci, we can now construct a C ′ i that allows for counting the friends in Ccomi and in C excl i separately, using the series of joins discussed earlier. Where both Ccomi and C excl i are non-empty, the original PCRV #Y [F (X,Y )] |C is\nresults in subsets in which Y is no longer count-normalized w.r.t. X: the joint counts of Y for both subsets are (2,2) for Ann and Bob, and (0,4) for Carl. To obtain count-normalized subsets, we need to partition C into a subset C1 for Ann and Bob, and C2 for Carl; this is what the Group-By construct does. For each of the subsets, a split on overlap with C ′ will yield subsets in which Y is count-normalized w.r.t. X. C ′1 is the result of joining the common and exclusive parts according to the join construct motivated earlier. C ′2 equals C2 because C2 has no overlap with C ′ and hence need not be split.\nreplaced by two PCRVs per Ci, #Ycom[F (X,Ycom)] |Ci and #Yexcl [F (X,Yexcl)] |Ci, and the new potential φ′ is defined such that φ′(hcom, hexcl) = φ(hcom ⊕ hexcl).\nGC-FOVE’s expansion improves over C-FOVE’s in the following way. C-FOVE uses expansion based on substitution (Milch et al., 2008). For instance, in Example 10, C-FOVE splits off all the elements of Cexcl individually from C, adding each of these elements as a separate argument of the parfactor and the involved potential function. This yields a\npotential function φ′1() with 61 arguments, namely the counting randvar #Xcom[S(Xcom)] and the 60 randvars S(x41), . . . S(x100). This causes an extreme blow up in the size (number of entries) of the potential function, which does not happen using our approach. In general, C-FOVE’s expansion yields a potential function of size O(rk · (n− k)r), with n = |C1|, k = |Cexcl1 |, and r the cardinality of the range of the considered randvars (e.g., r = |range(S(.))| in Example 10). In contrast, GC-FOVE’s expansion yields a potential function of size O(kr · (n − k)r). In the likely scenario that r ≪ k, this is exponentially smaller than C-FOVE’s potential function. Given that this potential function will later be used for multiplication or summing-out, it is clear that GC-FOVE can yield large efficiency gains over C-FOVE."
    }, {
      "heading" : "5.6 Count Normalization",
      "text" : "Lifted multiplication, summing-out and counting conversion all require certain variables to be count-normalized (recall Definition 2, p. 406). When this property does not hold, it can be achieved by normalizing the involved parfactor, which amounts to splitting the parfactor into parfactors for which the property does hold (Milch et al., 2008). Concretely, when Y is not count-normalized given Z in a constraint C, then C is simply partitioned into C = Group-By(C,CountY|Z), with CountY|Z as defined in Definition 1; next, the parfactor is split according to C. The formal definition of count normalization is shown in Operator 6.\nOperator count-normalize Inputs: (1) g = φ(A)|C: a parfactor in G (2) Y|Z: sets of logvars indicating the desired normalization property in C Preconditions (1) Y ⊂ logvar(A) and Z ⊆ logvar(A) \\Y Output: partition(g, group-by(C,CountY|Z)) Postconditions G ∼ G \\ {g} ∪ count-normalize(g,Y|Z)\nOperator 6: The count-normalization operator.\nExample 12. Consider the parfactor g with A = (Prof(P ), Supervises(P, S)) and constraint C = {(p1, s1), (p1, s2), (p2, s2), (p2, s3), (p3, s5), (p4, s3), (p4, s4), (p5, s6)}. Lifted elimination of Supervises(P, S) requires logvar S (student) to be count-normalized with respect to logvar P (professor). Intuitively, we need to partition the professors into groups such that all professors in the same group supervise the same number of students. In our example, C needs to be partitioned into two, namely C1 = σP∈{p3,p5}(C) = {(p3, s5), (p5, s6)} (tuples involving professors with 1 student) and C2 = σP∈{p1,p2,p4}(C) = {(p1, s1), (p1, s2), (p2, s2), (p2, s3), (p4, s3), (p4, s4)} (professors with 2 students). Next, the parfactor g is split accordingly into two parfactors g1 and g2 with constraints C1 and C2. These parfactors are now ready for lifted elimination of Supervises(P, S).\nC-FOVE requires a stronger normalization property to hold. For every pair of logvars X and Y it requires either (1) πX,Y (C) = πX(C)× πY (C) or (2) πX(C) = πY (C) and πX,Y (C) =\n(πX(C)×πY (C))\\{〈xi, xi〉 : xi ∈ πX(C)}. To enforce this, C-FOVE requires finer partitions than our approach does. In our example, C-FOVE requires C to be split into 5 subsets {Ci} 5 i=1 with Ci = σP∈{pi}(C), i.e., one group per professor. The coarser partitioning used in our approach cannot be represented using C-FOVE’s constraint language."
    }, {
      "heading" : "5.7 Absorption: Handling Evidence",
      "text" : "When the value of a randvar is observed, this usually makes probabilistic inference more efficient: the randvar can be removed from the model, which may introduce extra independencies in the model. However, in lifted inference, there is also an adverse effect: observations can break symmetries among randvars. For this reason, it is important to handle observations in a manner that preserves as much symmetry as possible. In order to effectively handle observations in a lifted manner, we introduce the novel operator of lifted absorption.\nIn the ground setting, absorption works as follows (van der Gaag, 1996). Given a factor φ(A) and an observation Ai = ai with Ai ∈ A, absorption replaces φ(A) with a factor φ\n′(A′), with A′ = A \\ {Ai} and φ\n′(a1, . . . , ai−1, ai+1, . . . , am) = φ(a1, . . . , ai−1, ai, ai+1, . . . , am). This reduces the size of the factor and may introduce extra independencies in the model, which is always beneficial.\nIf n randvars (built from the same predicate) have the same observed value, we can perform absorption on the lifted level by treating these n randvars as one single group. Consider a parfactor g = φ(S(X), F (X,Y ))|(X,Y ) ∈ {(x1, y1), . . . , (x1, y50)}. Assume that evidence atoms F (x1, y1) to F (x1, y10) all have the value true. This can be represented by adding an evidence parfactor gE to the model: gE = φE(F (X,Y ))|(X,Y ) ∈ {x1} × {yj} 10 1 , with φE(true) = 1 (the observed value) and φE(false) = 0. To absorb the evidence, g needs to be split into two, namely g1 with C1 = {(x1, y1), . . . , (x1, y10)} (the parfactor about which we have evidence) and g2 with C2 = {(x1, y11), . . . , (x1, y50)} (no evidence). Then, we can absorb the evidence about F into g1. Performing absorption on the ground level would result in ten identical factors φ′(S(x1)) (the logvar Y disappears in the absorption). Lifted absorption computes the same φ′ once, and raises it to the tenth power. Generally, with Xexcl the logvars that occur exclusively in the atom being absorbed, the exponent is the number of values Xexcl can take, so Xexcl must be count-normalized with respect to the other logvars. Further, all logvars in Xexcl can be removed from the constraint C as they disappear in the absorption.\nFor parfactors with counting formulas, essentially the same reasoning is used, but now the exponent is determined by the non-counted logvars occurring exclusively in the atom (Xnce). These logvars, together with the counted logvar, can be removed from C. The value for the absorbed counting formula, to be filled in in φ, is a histogram indicating how many times each possible value has been observed in the absorbed PRV. Since there is only one observed value in the evidence parfactor, this histogram maps that value to the number of randvars being absorbed, and other values to zero. Lifted absorption is formally defined in Operator 7. We provide a correctness proof for this operator in Appendix A, and analyze its complexity in Appendix B.\nGC-FOVE handles evidence by absorption as follows. It first creates one evidence parfactor per observed value for each predicate. Next, it compares each evidence parfactor with\nOperator absorb Inputs: (1) g = φ(A)|C: a parfactor in G (2) Ai ∈ A with Ai = P (X) or Ai = #Xi [P (X)] (3) gE = φE(P (X))|CE : an evidence parfactor Let Xexcl = X \\ logvar(A \\ Ai);\nXnce = Xexcl \\ {Xi} if Ai = #Xi [P (X)], X excl otherwise; L′ = logvar(A) \\Xexcl; o = the observed value for P (X) in gE ;\nPreconditions (1) RV (Ai|C) ⊆ RV (Ai|CE) (2) Xnce is count-normalized w.r.t. L′ in C. Output: g′ = φ′(A′)|C ′, with (1) A′ = A \\ {Ai} (2) C ′ = πlogvar(C)\\Xexcl (C) (3) φ′(. . . , ai−1, ai+1, . . . ) = φ(. . . , ai−1, e, ai+1, . . . ) r, with r = CountXnce|L′(C), and\nwith e = o if Ai = P (X) and e a histogram with e(o) = CountXi|logvar(A)(C), e(.) = 0 elsewhere, otherwise\n(namely if Ai = #Xi [P (X)]) Postcondition G ∪ {gE} = G \\ {g} ∪ {gE ,absorb(g,Ai, gE)}\nOperator 7: Lifted absorption.\neach PRV in the model, applying absorption when possible. Where necessary, parfactors in the model are split to allow for absorption. (It is never necessary to split evidence parfactors, see precondition 1.) When no more absorptions are possible with a given evidence parfactor, it is removed from the model: the evidence has been incorporated completely.\nLike the sum-out operator, the absorb operator has the effect of eliminating PRVs from the model. As the operator’s definitions show, however, absorb requires weaker preconditions than sum-out, which means that it can be applied in more situations. Also, the absorb operator easily lends itself to a splitting as needed constraint processing strategy (Kisynski & Poole, 2009a), which keeps the model at a much higher granularity, by requiring fewer splits on the parfactors compared to a preemptive shattering strategy. In the presence of observations, which is often the case in real-world problems, these effects can result in large computational savings.\nOur approach to dealing with evidence differs from C-FOVE’s in two important ways. First, C-FOVE introduces a separate evidence factor for each ground observation A = a. This causes extensive splitting: if there are n randvars with the same observed value, there will be n separate factors, and C-FOVE will perform (at least) n eliminations on these randvars. In addition, the splitting may cause further splitting as C-FOVE continues, destroying even more opportunities for lifting. We show in Section 7 that this can make inference impossible with C-FOVE in the presence of evidence.\nSecond, C-FOVE does not use absorption; during inference, the evidence factors are used for multiplication and summing-out like any other factors. Absorption is advantageous\nOperator ground-logvar Inputs: (1) g = φ(A)|C: a parfactor in G (2) X: a logvar in logvar(A) Output: partition(g,group-by(C, πX)) Postcondition G ∼ G \\ {g} ∪ ground-logvar(g,X)\nOperator 8: Grounding.\nbecause it eliminates randvars from the model, so they no longer need to be summed out. As a result, in our approach, evidence reduces the number of summing-out and multiplication operations, while in C-FOVE it increases that number."
    }, {
      "heading" : "5.8 Grounding a Logvar",
      "text" : "There is no guarantee that the enabling operators eventually result in PRVs and parfactors that allow for any of the lifted operators. To illustrate this, consider a model consisting of a single parfactor φ(R(X,Y ), R(Y,Z), R(X,Z))|C, which expresses a probabilistic variant of transitivity. Since there is only one factor, no multiplications are needed before starting to eliminate variables. Yet, because of the structure of the parfactor, no single PRV can be eliminated (the preconditions for lifted summing out and counting conversion are not fulfilled, and none of the other operators can change that).\nIn cases like this, when no other operators can be applied, lifted VE can always resort to a last operator: grounding a logvarX in a parfactor g (de Salvo Braz, 2007; Milch et al., 2008). Given a parfactor g = φ(A)|C and a logvar X ∈ logvar(A) with πX(C) = {x1, . . . , xn}, grounding X replaces g with the set of parfactors {g1, . . . , gn} with gi = φ(A)|σX=xi(C). This is equivalent to splitting g based on the partition group-by(C, πX), which yields the definition shown in Operator 8. Note that in each resulting parfactor gi, logvar X can only take on a single value xi, so in practice X can be replaced by the constant xi and removed from the set of logvars.\nGrounding can significantly increase the granularity of the model and decrease the opportunities for performing lifted inference: in the extreme case where all logvars are grounded, inference is performed at the propositional level. It is therefore best used only as a last resort. In practice, (G)C-FOVE’s heuristic for selecting operators, which relies on the size of the resulting factors, automatically has this effect.\nCalling the ground-logvar operator should not be confused with the event of obtaining a ground model. ground-logvar grounds only one logvar, and does not necessarily result in a ground model. Conversely, one may arrive at a ground model without ever calling ground-logvar, simply because the splitting continues up to the singleton level."
    }, {
      "heading" : "6. Representing and Manipulating the Constraints",
      "text" : "We have shown that using an extensionally complete constraint language instead of allowing only pairwise (in)equalities can potentially yield large efficiency gains by allowing more opportunities for lifting. The question remains how we can represent these constraints.\nIn principle, we could represent them extensionally, as lists of tuples. This allows any constraint to be represented, but is inefficient when we have many logvars. Instead, we employ a constraint tree, as also used in First Order Bayes-Ball (Meert, Taghipour, & Blockeel, 2010). Hence, the lower-level operations on constraints (projection, splitting, counting) must be implemented in terms of constraint trees. Below, we briefly explain how this is done.\nA constraint tree on logvars X is a tree in which each internal (non-leaf) node is labeled with a logvar X ∈ X, each leaf is labeled with a terminal label ⊤, and each edge e = (Xi,Xj) is labeled with a (sub-)domain D(e) ⊆ D(Xi). See Figure 9 for an example. We use ordered trees, where all nodes in the same level of the tree are labeled with the same logvar, and each logvar occurs on only one level. Each path from the root to a leaf through edges e1, . . . , e|X| represents the tuples in the Cartesian product ×iD(ei). For example, in Figure 9, the left most path represents the tuples {x1, x2, x3} × {y1, . . . , y10} × {z1, . . . , z5}. The constraint represented by the tree is the union of tuples represented by each root-to-leaf path.\nGiven a constraint (in terms of the set of tuples that satisfy it), we construct the corresponding tree in a bottom-up manner by merging compatible edges. Different logvar orders can result in trees of different sizes. A tree can be re-ordered by interchanging nodes in two adjacent levels of the tree and applying the possible merges at those levels. We employ re-ordering to simplify the various constraint handling operations. For projection of a constraint, we move the projected logvars to the top of the tree and discard the parts below these logvars. For splitting, we perform a pairwise comparison of the two involved constraint trees. First, we re-order each tree such that the logvars involved in the split are at the top of the trees. Then we process the trees top-down by comparing the edges leaving the root in the two trees and partition their domains based on their overlap. We recursively repeat this for their children until we reach the last logvar involved in the split. For count normalization, we also first apply this re-ordering. Then we partition the tree based on the number of tuples of counted logvars in each branch. For counting this number,\nwe only need to consider the size of the domains associated with the edges. Finally, the join of two constraints is computed by reordering the trees so that the join variables occur at the top, merging the levels of the join variables in the same way as is done for splitting, and extending each leaf in the resulting tree with the cross-product of the corresponding subtrees of the original trees.\nConstraint trees (and the way they are constructed) are close to the hypercube representation used in lifted belief propagation (Singla, Nath, & Domingos, 2010). However, for a given constraint, the constraint tree is typically more compact. The constraint tree of Figure 9 corresponds to a set of five hypercubes, one for each leaf. The hypercube representation does not exploit the fact that the first and second hypercube, for instance, share the part {x1, x2, x3}. In the constraint tree, this is explicit, which makes it more compact.\nWe stress that GC-FOVE can use any extensionally complete constraint representation language. Constraint trees are just one such representation. Other representations can be more compact in some cases, but in the choice of a representation we need to consider also the tradeoff between compactness and ease of constraint processing. Consider a constraint graph, which is similar to our trees, but in which parent nodes can share child nodes. This representation is more compact than a constraint tree, but also requires more complicated constraint handling operations. For instance, consider splitting, in which we might need to split a child node for one parent but not for the others. Such operations become more complicated on graphs, while they are trivial on trees."
    }, {
      "heading" : "7. Experiments",
      "text" : "Using an extensionally complete constraint language, we can capture more symmetries in the model, which potentially offers the ability to perform more operations at a lifted level. However, this comes at a cost, as manipulating more expressive constraints is more computationally demanding. We hypothesize that the ability to perform fewer computations by capturing more symmetries will far outweigh this cost in typical inference tasks. In this section, we compare the performances of C-FOVE and GC-FOVETREES (GC-FOVE using the tree representation from Section 6) to empirically validate this hypothesis. In particular, we study how the performances vary as a function of two parameters: (i) the domain size, and (ii) the amount of evidence. We also empirically study whether GC-FOVETREES can solve inference tasks that are beyond the reach of C-FOVE.\nThroughout this section, GC-FOVE stands for GC-FOVETREES."
    }, {
      "heading" : "7.1 Methodology and Datasets",
      "text" : "We compare C-FOVE and GC-FOVE on several inference tasks with synthetic and realworld data. We use the version of C-FOVE extended with general parfactor multiplication (de Salvo Braz, 2007).3 For implementing GC-FOVE, we started from the publicly available C-FOVE code (Milch, 2008), so the implementations are maximally comparable.4 In all experiments, the undirected model has parfactors whose constraints are all representable\n3. This allows C-FOVE to handle some tasks in an entirely lifted way, where otherwise it would have to resort to grounding, e.g., on the social network domain (Jha et al., 2010). 4. GC-FOVE is available from http://dtai.cs.kuleuven.be/ml/systems/gc-fove.\nby C-FOVE. Thus, GC-FOVE has no initial advantage, which makes the comparison conservative.\nIn each experiment we compute the marginal probability of a query randvar given some evidence. The query randvar is selected at random from the non-observed atoms. The evidence is generated by randomly selecting randvars of a particular predicate and giving them a value chosen randomly and uniformly from their domain. All the reported results are averaged over multiple runs for different query and evidence sets."
    }, {
      "heading" : "7.1.1 Experiments with Synthetic Data",
      "text" : "In terms of synthetic data, we evaluate our algorithm on three standard benchmark problems. The first domain is called workshop attributes (Milch et al., 2008). Here, m different attributes (e.g., topic, date, etc.) describe the workshop, and a corresponding factor for each attribute shows the dependency between the attendance of each person and the attribute. The theory contains the following parfactors.\nφ1(Attends(X), Attr1)\n...\nφm(Attends(X), Attrm)\nφm+1(Attends(X), Series)\nThe second domain is called competing workshops (Milch et al., 2008). It models the fact that people are more likely to attend a workshop if it is on a “hot topic” and that the number of attendees influences whether the workshop becomes a series. The theory contains the following parfactors.\nφ1(Attends(X),Hot(Y ))\nφ2(Attends(X), Series)\nIn our experiments on both of the above domains, the query variable is Series, and all evidence randvars are of the form Attends(x).\nThe third domain is called social network (Jha et al., 2010) and it models people’s smoking habits, their chance of having asthma, and the dependence of a persons habits and diseases on their friendships. The theory contains the following parfactors.\nφ1(Smokes(X))\nφ2(Asthma(X))\nφ3(Friends(X,Y ))\nφ4(Asthma(X), Smokes(X))\nφ5(Asthma(X), F riends(X,Y ), Smokes(Y ))\nIn this domain, the evidence randvars will be a mix of randvars of the form Smokes(x) or Asthma(x), and the query randvar can be any randvar that is unobserved."
    }, {
      "heading" : "7.1.2 Experiments with Real-World Data",
      "text" : "We also used two other datasets from the field of statistical relational learning. The first, WebKB (Craven & Slattery, 1997), contains data about more than 1200 webpages, including their class (e.g., “course page”), textual content (set of words), and the hyperlinks between the pages. The model consists of multiple parfactors, stating for instance how the classes of two linked pages depend on each other. Our inference task concerns link prediction. Here, the class information is observed for a subset of all pages and the task is to compute the probability of having a hyperlink between a pair of pages. We use one Pageclass predicate in the model for each run, and average the runtime over multiple runs for each class. We used the following set of parfactors.\nφ1(Pageclass(P ))\nφ2(Pageclass(P ),HasWord(P,W ))\nφ3(Pageclass(P1), Link(P1, P2), Pageclass(P2))\nThe second dataset, Yeast (Davis, Burnside, de Castro Dutra, Page, & Costa, 2005), contains data about more than 7800 yeast genes, their functions and locations, and the interactions between these genes. The model and task are similar to those in WebKB (gene functions correspond to page classes, gene-to-gene interactions to hyperlinks). In this task, we observe the function information for a subset of all genes and query the existence of an interaction between two genes. Similar to WebKB, we also use one function in the model in each run and average the results over multiple runs. Here, we used the following set of parfactors.\nφ1(Function(G))\nφ2(Location(G,L))\nφ3(Function(G), Location(G,L))\nφ4(Function(G1), Interaction(G1, G2), Function(G2))\nMotivation for evidence randvars. In all experiments, evidence randvars correspond to atoms of a unary predicate; we call them “unary randvars”. This is done on purpose because introducing evidence randomly for binary randvars, e.g., randvars of the type P (X,Y ), can quickly break so many symmetries that lifted inference is not possible anymore. In fact, there are recent theoretical results that show that lifted inference in the presence of arbitrary evidence on binary randvars is simply not possible. This limitation is not unique to our approach, but is true of any possible exact lifted inference approach (Van den Broeck & Davis, 2012). Because of this, random insertion of evidence on binary randvars can quickly cause any lifted inference algorithm to resort to ground inference, which would blur the distinction between C-FOVE, GC-FOVE, and ground inference. We avoid this by placing evidence only on unary randvars."
    }, {
      "heading" : "7.2 Influence of the Domain Size",
      "text" : "In the first set of experiments, we use the synthetic datasets to measure the effect of domain size (number of objects) on runtime. We vary the domain size from 50 to 1000 objects while holding the proportion of observed randvars (relative to the number of observable randvars) constant at 20%. Figures 10(a) through 10(c) show the performance on all three synthetic datasets. On all three models, GC-FOVE outperforms C-FOVE on all domain sizes. As the number of objects in the domain increases, the runtimes increase for both algorithms. GC-FOVE’s runtime increases at a much lower rate than C-FOVE’s on all three models. On the first two tasks, GC-FOVE is between one and two orders of magnitude faster than C-GOVE, for the largest domain sizes. On the social network domain, the difference in performance becomes more striking: C-FOVE cannot handle domain sizes of 100 objects or more, while GC-FOVE handles the largest domain (1000 objects) in about 200 seconds. The improvement in performance arises as GC-FOVE better preserves the symmetries present in the model by treating all indistinguishable elements, observed or not, as a single unit.\nThe gain is more pronounced for larger domains. C-FOVE makes a separate partition (and a separate evidence factor) for each observed randvar, thus, with a fixed evidence ratio, the number of partitions induced by C-FOVE grows linearly with the domain size. Moreover, it has a costly elimination operation for each partition. In contrast, GC-FOVE, which employs lifted absorption, keeps the model at a higher granularity by grouping the observations and handles whole groups of observations with a single lifted operation."
    }, {
      "heading" : "7.3 Influence of the Amount of Evidence",
      "text" : "In the second set of experiments, we measure the effect of the proportion of observed randvars on runtime, using the synthetic datasets. We fix the domain size, and vary the percentage of observed randvars from 0% to 100%. Note that this is a percentage of all “observable” randvars (e.g., all randvars of the form Smokes(x)), not of all randvars of any type (so 100% does not mean there are no unobserved variables left). Figures 11(a) through 11(c) show the performance on all three synthetic domains with domain size of 1000 objects. To better demonstrate C-FOVE’s behavior on the social networks domain, Figure 11(d) shows the performances on a domain with only 25 objects. Both algorithms display similar trends across the three domains. Without evidence, GC-FOVE is comparable\nto C-FOVE. This is the best scenario for C-FOVE as (i) the initial model only contains (in)equality constraints, and (ii) there is no evidence, so no symmetries are broken when the inference operators are applied. In this case, the only difference in runtime between the two algorithms is the overhead associated with constraint processing, which is almost negligible. As the proportion of observations increases, and the symmetries between the objects are broken, GC-FOVE maintains a much coarser grouping, and so performs inference much more efficiently, than C-FOVE. In all domains, C-FOVE’s runtime increases dramatically with an increase in the percentage of observations. As more evidence is added, C-FOVE induces more partitions, which results in finer groupings of objects and leaves fewer opportunities for lifting. GC-FOVE performs significantly better in comparison, due to coarser grouping of observations and employing absorption for their elimination from the model. GC-FOVE’s runtime experiences a bump as the initial set of evidence is added, but then levels out or gradually decreases (the more evidence, the more randvars are efficiently eliminated by absorption). GC-FOVE consistently finishes in under 200 seconds, regardless of the setting. In contrast, on the social network domain (Figure 11(c)) C-FOVE cannot handle portions of evidence greater than 1% (it runs out of memory on machine configured with 30GB of memory).\nThese results confirm that both the coarser groupings and the use of lifted absorption contribute to the much better performance of GC-FOVE."
    }, {
      "heading" : "7.4 Performance on Real-World Data",
      "text" : "In the final set of experiments, we compared the algorithms on the two real-world datasets, WebKB and Yeast. On both datasets, we varied the percentage of observed page classes or functions from 0% to 100% in steps of 10%. Figures 12(a) and 12(b) illustrate the results. C-FOVE could solve only the zero-evidence problems in these experiments; for the other cases, it typically ran out of memory after up to an hour of computation time on a machine configured with 30 GB of memory. Its failure is primarily due to the large number of observations, which often forces it to resort to inference at the ground level for a large number of objects. GC-FOVE, on the other hand, runs successfully for all experimental conditions. Furthermore, GC-FOVE can consistently solve the problems in a few seconds. As on the synthetic data, GC-FOVE’s performance improves with increasing number of observations. In these cases more randvars can be eliminated through absorption, instead of the more expensive operations of multiplication and summation."
    }, {
      "heading" : "8. Conclusions",
      "text" : "Constraints play a crucial role in lifted probabilistic inference as they determine the degree of lifting that takes places. Surprisingly, most lifted inference algorithms use the same class of constraints based on pairwise (in)equalities (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011); the main exception is the work on approximate inference using lifted belief propagation (Singla & Domingos, 2008). In this paper we have shown that this class of constraints is overly restrictive. We proposed using extensionally complete constraint languages, which can capture more symmetries among the objects and allow for more operations to occur on a lifted level. We defined the relevant constraint handling operations (e.g., splitting and normalization) to work with extensionally complete constraint languages and implemented them for performing lifted variable elimination. We made use of constraint trees to efficiently represent and manipulate the constraints. We empirically evaluated our system on several domains. Our approach resulted in up to three orders of magnitude improvement in runtime, as compared to C-FOVE. Furthermore, GC-FOVE can solve several tasks that are intractable for C-FOVE.\nFuture work includes generalizing other lifted inference algorithms that currently use only inequality constraints, e.g., the works of Jha et al. (2010) and Van den Broeck et al. (2011), and further optimizing constraint handling. With respect to the latter, an interesting direction is the recent work of de Salvo Braz, Saadati, Bui, and OReilly (2012) that employs a logical representation for constraints, which is extensionally complete, and presents specialized constraint processing methods for this representation. Finally, it is possible to extend lifted absorption such that it works not only with evidence parfactors, but more generally with deterministic parfactors. This is another promising direction for future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Daan Fierens is supported by the Research Foundation of Flanders (FWO-Vlaanderen). Jesse Davis is partially supported by the Research Fund KULeuven (CREA/11/015 and OT/11/051), and EU FP7 Marie Curie Career Integration Grant (#294068). This work was funded by GOA/08/008 “Probabilistic Logic Learning” of the Research Fund KULeuven. The authors thank Maurice Bruynooghe and Guy Van den Broeck for interesting discussions and comments on this work and text. They also thank the reviewers for their constructive comments and very concrete suggestions to improve the article."
    }, {
      "heading" : "Appendix A. Correctness Proof for Lifted Absorption",
      "text" : "In this appendix, we prove the correctness of the novel lifted absorption operator. We begin by providing some lemmas.\nRecall that a set of parfactors G is a compact way of defining a set of factors gr(G) = {f |f ∈ gr(g) ∧ g ∈ G} and the corresponding probability distribution\nPG(A) = 1\nZ\n∏\nf∈gr(G)\nφf (Af ).\nFurther, G ∼ G′ means G and G′ define the same probability distribution. Thus, formally:\nG ∼ G′ ⇔ PG(A) = PG′(A) ⇔ 1\nZ\n∏\nf∈gr(G)\nφf (Af ) = 1\nZ ′\n∏\nf∈gr(G′)\nφf (Af ).\nThe following lemmas are easily proven by applying the above definition and keeping in mind that gr(G ∪G′) = gr(G) ∪ gr(G′).\nLemma 1 For all models G,G′, G′′: G′ ∼ G′′ ⇒ G ∪G′ ∼ G ∪G′′.\nLemma 2 Given a factor f = φ(A1, A2, . . . , An) and an evidence factor fE = φE(A1) with φE(a1) = 1 if a1 = o (the observed value) and φE(a1) = 0 otherwise, {f, fE} ∼ {f\n′, fE} with f ′ = φ′(A2, . . . , An) and φ ′(a2, . . . , an) = φ(o, a2, . . . , an).\nLemma 3 A model that consists of m identical factors, G = {φ(A1, . . . , An)} m i=1, is equivalent to a model with a single factor G′ = {φ′(A1, . . . , An)} where φ ′(a1, . . . , an) = φ(a1, . . . , an) m.\nWe now prove that the Absorb operator is correct, i.e., its postconditions hold, given the preconditions.\nTheorem 1 Given a model G, a parfactor g ∈ G and an evidence parfactor gE, if the preconditions of the absorb operator are fulfilled, then\nG ∪ {gE} ∼ G \\ {g} ∪ {absorb(g,Ai, gE), gE}.\nProof: With G′ = G \\ {g}, we can rewrite the above equivalence as\nG′ ∪ {g, gE} ∼ G ′ ∪ {absorb(g,Ai, gE), gE}.\nBecause of Lemma 1, it suffices to prove\n{g, gE} ∼ {absorb(g,Ai, gE), gE}.\nLet g = φ(A)|C with A = {A1(X1), . . . , Ak(Xk)}, let gE = φE(P (X))|CE , and let L = logvar(A) (non-counted logvars in A), Xexcl = X\\ logvar(A\\{Ai}) (logvars occurring exclusively in Ai) and L\n′ = logvar(A) \\Xexcl (non-counted logvars occurring (also) outside Ai). The operator returns a parfactor of the form φ\n′(A′)|C ′ where A′ = {A2, . . . , Ak} and C ′ = πlogvar(C)\\Xexcl(C) (see the operator definition). We need to prove that φ\n′ is such that the above equivalence holds. For ease of exposition, from now we assume that the atom or counting formula that is to be absorbed (Ai in the operator’s input) is A1. We first consider the case where A1 is an atom P (X1), and then the case where A1 is a counting formula #X [P (X1)]. Absorption for atoms. In this case, we have φ′(a2, . . . , ak) = φ(o, a2, . . . , ak) r with r = CountXexcl|L′(C) (see the operator definition, observing that X nce = Xexcl).\nBy definition, gr(g) = {φ(P (x1), A2(x2), . . . , Ak(xk))}l∈πL(C), with xi = πXi(l). Precondition 1 guarantees that for each φ(P (x1), A2(x2), . . . , Ak(xk)), there exists an evidence factor φE(P (x1)) in gr(gE). By Lemma 2, we can therefore rewrite each factor in gr(g) into the form φ∗(A2(x2), . . . ), with φ\n∗(a2, . . . ak) = φ(o, a2, . . . , ak), with o the observed value for P (x1).\nThe potential function φ∗ is the same for all factors, since there is only one observed value o for the whole evidence parfactor. Therefore, any two factors φ(P (x1), A2(x2), . . . , Ak(xk)) and φ(P (x′1), A2(x2), . . . , Ak(xk)) that differ only in their first argument are rewritten to the same factor. Because of Precondition 2, the number of factors rewritten to the same factor is constant and equals CountXexcl|L′(C) = r. By Lemma 3, each set of identical factors can therefore be replaced by a single factor with potential function\nφ′(a2, . . . , ak) = φ ∗(a2, . . . , ak) r = φ(o, a2, . . . , ak) r,\nwhich is exactly how φ′ is defined by the operator. Absorption for counting formulas. In this case, we have\nφ′(a2, . . . , ak) = φ(e, a2, . . . , ak) r\nand r = CountXnce|L′(C) with X nce = Xexcl \\ {X} (see the operator definition).\nWe define X′1 = X1 \\ {X}, and use (x ′ 1,X) to denote X1 with all logvars instantiated\nexcept the counted logvar X. Now, by definition,\ngr(g) = {φ(#X∈Cl [P (x ′ 1,X)], A2(x2), . . . , Ak(xk))}l∈πL(C),\nwith xi = πXi(l), x ′ 1 = πX′1(l) and Cl = πX(σL=l(C)). Each Cl is of the form {x1, . . . , xn}, where n = CountX|L(C) (n exists because PCRV’s are by definition count-normalized).\nWe show correctness of the operator in this case by showing that for each factor f in gr(g), the evidence parfactor gE can be rewritten to contain an evidence factor that has the same CRV as f , such that the same reasoning as above can be applied on f .\nPrecondition 1 guarantees that for each factor\nf = φ(#X∈{x1,...,xn}[P (x ′ 1,X)], A2(x2), . . . , Ak(xk)),\ngr(gE) contains the group of evidence factors\nEf = {φE(P (x ′ 1, x1)), . . . , φE(P (x ′ 1, xn))}.\nWe can multiply all factors in Ef into\nφ′E(P (x1, x ′ 1), . . . , P (x1, x ′ n)),\nwith φ′E(o, o, . . . , o) = 1 and φ ′ E(.) = 0 elsewhere, and then rewrite this as\nfE = φ ∗ E(#X∈{x1,...,xn}[P (x ′ 1,X)]),\nwhere φ∗E is such that (i) φ ∗ E(e) = 1, for e the histogram with e(o) = n and e(.) = 0 elsewhere, and (ii) φ∗E(e ′) = 0 for e′ 6= e.\nHaving formed fE, we can rewrite f into the form φ ′(A2(x2), . . . ), with φ ′(a2, . . . ak) = φ(e, a2, . . . , ak)\nr and r = CountXnce|L′(C), with the same argumentation as for regular atoms. After this, we can replace fE with the equivalent Ef , thus restoring gE . Repeating this for each f preserves equivalence and eventually yields the model that the operator returns."
    }, {
      "heading" : "Appendix B. Computational Complexity of Lifted Absorption",
      "text" : "Applying lifted absorption on a parfactor g = φ(A)|C, has complexity O(|C|)+O(Size(φ) · log |C|), where |C| is the cardinality (number of tuples) of the constraint C, and Size(φ) equals the product of range sizes of the arguments A, i.e., Size(φ) = ∏\nAi∈A |range(Ai)|.\nThe first term in the complexity, O(|C|), arises because absorption involves a projection of the constraint C, which in the worst case (with an extensional representation) has complexity O(|C|). The second term, O(Size(φ) · log |C|), is the complexity of computing the new potential function, which involves manipulating φ, which has Size(φ) entries (in a tabular representation), and exponentiating it, which has complexity O(log |C|)."
    } ],
    "references" : [ {
      "title" : "Extended lifted inference with joint formulas",
      "author" : [ "U. Apsel", "R.I. Brafman" ],
      "venue" : "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Apsel and Brafman,? \\Q2011\\E",
      "shortCiteRegEx" : "Apsel and Brafman",
      "year" : 2011
    }, {
      "title" : "Lifted inference for relational continuous models",
      "author" : [ "J. Choi", "D. Hill", "E. Amir" ],
      "venue" : "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Choi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2010
    }, {
      "title" : "Relational learning with statistical predicate invention: Better models for hypertext",
      "author" : [ "M. Craven", "S. Slattery" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Craven and Slattery,? \\Q1997\\E",
      "shortCiteRegEx" : "Craven and Slattery",
      "year" : 1997
    }, {
      "title" : "An integrated approach to learning Bayesian networks of rules",
      "author" : [ "J. Davis", "E.S. Burnside", "I. de Castro Dutra", "D. Page", "V.S. Costa" ],
      "venue" : "In Proceedings of 16th European Conference on Machine Learning (ECML),",
      "citeRegEx" : "Davis et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Davis et al\\.",
      "year" : 2005
    }, {
      "title" : "Probabilistic inductive logic programming: Theory and applications",
      "author" : [ "L. De Raedt", "P. Frasconi", "K. Kersting", "S. Muggleton" ],
      "venue" : null,
      "citeRegEx" : "Raedt et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Raedt et al\\.",
      "year" : 2008
    }, {
      "title" : "Lifted first-order probabilistic inference",
      "author" : [ "R. de Salvo Braz" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Braz,? \\Q2007\\E",
      "shortCiteRegEx" : "Braz",
      "year" : 2007
    }, {
      "title" : "Lifted first-order probabilistic inference",
      "author" : [ "R. de Salvo Braz", "E. Amir", "D. Roth" ],
      "venue" : "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Braz et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Braz et al\\.",
      "year" : 2005
    }, {
      "title" : "Lifted arbitrary constraint solving for lifted probabilistic inference",
      "author" : [ "R. de Salvo Braz", "S. Saadati", "H. Bui", "C. OReilly" ],
      "venue" : "In Proceedings of the 2nd International Workshop on Statistical Relational AI (StaRAI),",
      "citeRegEx" : "Braz et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Braz et al\\.",
      "year" : 2012
    }, {
      "title" : "Constraint processing",
      "author" : [ "R. Dechter" ],
      "venue" : null,
      "citeRegEx" : "Dechter,? \\Q2003\\E",
      "shortCiteRegEx" : "Dechter",
      "year" : 2003
    }, {
      "title" : "An Introduction to Statistical Relational Learning",
      "author" : [ "L. Getoor", "B. Taskar" ],
      "venue" : null,
      "citeRegEx" : "Getoor and Taskar,? \\Q2007\\E",
      "shortCiteRegEx" : "Getoor and Taskar",
      "year" : 2007
    }, {
      "title" : "Probabilistic theorem proving",
      "author" : [ "V. Gogate", "P. Domingos" ],
      "venue" : "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Gogate and Domingos,? \\Q2011\\E",
      "shortCiteRegEx" : "Gogate and Domingos",
      "year" : 2011
    }, {
      "title" : "Lifted inference seen from the other side : The tractable features",
      "author" : [ "A. Jha", "V. Gogate", "A. Meliou", "D. Suciu" ],
      "venue" : "In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Jha et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jha et al\\.",
      "year" : 2010
    }, {
      "title" : "Counting belief propagation",
      "author" : [ "K. Kersting", "B. Ahmadi", "S. Natarajan" ],
      "venue" : "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Kersting et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kersting et al\\.",
      "year" : 2009
    }, {
      "title" : "Constraint processing in lifted probabilistic inference",
      "author" : [ "J. Kisynski", "D. Poole" ],
      "venue" : "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Kisynski and Poole,? \\Q2009\\E",
      "shortCiteRegEx" : "Kisynski and Poole",
      "year" : 2009
    }, {
      "title" : "Lifted aggregation in directed first-order probabilistic models",
      "author" : [ "J. Kisynski", "D. Poole" ],
      "venue" : "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Kisynski and Poole,? \\Q2009\\E",
      "shortCiteRegEx" : "Kisynski and Poole",
      "year" : 2009
    }, {
      "title" : "Factor graphs and the sum-product algorithm",
      "author" : [ "F.R. Kschischang", "B.J. Frey", "Loeliger", "H.-A" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Kschischang et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kschischang et al\\.",
      "year" : 2001
    }, {
      "title" : "First-order bayes-ball",
      "author" : [ "W. Meert", "N. Taghipour", "H. Blockeel" ],
      "venue" : "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD),",
      "citeRegEx" : "Meert et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Meert et al\\.",
      "year" : 2010
    }, {
      "title" : "Lifted probabilistic inference with counting formulas",
      "author" : [ "B. Milch", "L.S. Zettlemoyer", "K. Kersting", "M. Haimes", "L.P. Kaelbling" ],
      "venue" : "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Milch et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2008
    }, {
      "title" : "First-order probabilistic inference",
      "author" : [ "D. Poole" ],
      "venue" : "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Poole,? \\Q2003\\E",
      "shortCiteRegEx" : "Poole",
      "year" : 2003
    }, {
      "title" : "Database management systems (3",
      "author" : [ "R. Ramakrishnan", "J. Gehrke" ],
      "venue" : null,
      "citeRegEx" : "Ramakrishnan and Gehrke,? \\Q2003\\E",
      "shortCiteRegEx" : "Ramakrishnan and Gehrke",
      "year" : 2003
    }, {
      "title" : "Bisimulation-based approximate lifted inference",
      "author" : [ "P. Sen", "A. Deshpande", "L. Getoor" ],
      "venue" : "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Sen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sen et al\\.",
      "year" : 2009
    }, {
      "title" : "Prdb: managing and exploiting rich correlations in probabilistic databases",
      "author" : [ "P. Sen", "A. Deshpande", "L. Getoor" ],
      "venue" : "VLDB Journal,",
      "citeRegEx" : "Sen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sen et al\\.",
      "year" : 2009
    }, {
      "title" : "Lifted first-order belief propagation",
      "author" : [ "P. Singla", "P. Domingos" ],
      "venue" : "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Singla and Domingos,? \\Q2008\\E",
      "shortCiteRegEx" : "Singla and Domingos",
      "year" : 2008
    }, {
      "title" : "Approximate Lifted Belief Propagation",
      "author" : [ "P. Singla", "A. Nath", "P. Domingos" ],
      "venue" : "In Proceedings of the 1st International Workshop on Statistical Relation AI (StaRAI),",
      "citeRegEx" : "Singla et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Singla et al\\.",
      "year" : 2010
    }, {
      "title" : "Lifted variable elimination with arbitrary constraints",
      "author" : [ "N. Taghipour", "D. Fierens", "J. Davis", "H. Blockeel" ],
      "venue" : "In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Taghipour et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Taghipour et al\\.",
      "year" : 2012
    }, {
      "title" : "Conditioning in first-order knowledge compilation and lifted probabilistic inference",
      "author" : [ "G. Van den Broeck", "J. Davis" ],
      "venue" : "In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Broeck and Davis,? \\Q2012\\E",
      "shortCiteRegEx" : "Broeck and Davis",
      "year" : 2012
    }, {
      "title" : "Lifted Probabilistic Inference by First-Order Knowledge Compilation",
      "author" : [ "G. Van den Broeck", "N. Taghipour", "W. Meert", "J. Davis", "L. De Raedt" ],
      "venue" : "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Broeck et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Broeck et al\\.",
      "year" : 2011
    }, {
      "title" : "On evidence absorption for belief networks",
      "author" : [ "L.C. van der Gaag" ],
      "venue" : "Int. J. Approx. Reasoning,",
      "citeRegEx" : "Gaag,? \\Q1996\\E",
      "shortCiteRegEx" : "Gaag",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "To address this problem, Poole (2003) introduced the concept of lifted inference for graphical models.",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "Section 2 illustrates the principles of lifted variable elimination by example, and briefly states how this work improves upon the state of the art, C-FOVE (Milch et al., 2008).",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 17,
      "context" : "1 The Workshop Example This example is from Milch et al. (2008). Suppose a new workshop is organized.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "At the time of writing, the C-FOVE system (Milch et al., 2008) is considered the state of the art in lifted variable elimination.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "We use a representation formalism based on undirected graphical models that is closely related to the one used in earlier work on lifted variable elimination (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).",
      "startOffset" : 158,
      "endOffset" : 212
    }, {
      "referenceID" : 17,
      "context" : "We use a representation formalism based on undirected graphical models that is closely related to the one used in earlier work on lifted variable elimination (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).",
      "startOffset" : 158,
      "endOffset" : 212
    }, {
      "referenceID" : 17,
      "context" : "The concepts introduced in this section have also been introduced in earlier work (de Salvo Braz, 2007; Milch et al., 2008).",
      "startOffset" : 82,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "1 A Constraint-based Representation Formalism An undirected model is a factorization of a joint distribution over a set of random variables (Kschischang et al., 2001).",
      "startOffset" : 140,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : ",Xn) is a tuple of logvars, and CX is a subset of D(X) = ×iD(Xi) (Dechter, 2003).",
      "startOffset" : 65,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "1 A Constraint-based Representation Formalism An undirected model is a factorization of a joint distribution over a set of random variables (Kschischang et al., 2001). Given a set of random variables X = {X1,X2, . . . ,Xn}, a factor consists of a potential function φ and an assignment of a random variable to each of φ’s inputs. For instance, the factorization f(X1,X2,X3) = φ(X1,X2)φ(X2,X3) contains two different factors (even if their potential functions are the same). Likewise, in our probabilistic-logical representation framework, a model is a set of factors. The random variables they operate on are properties of, and relationships between, objects in the universe. We now introduce some terminology to make this more concrete. We assume familiarity with set and relational algebra (union ∪, intersection ∩, difference \\, set partitioning, selection σC , projection πX , attribute renaming ρ, join ⊲⊳); see, for instance, the work of Ramakrishnan and Gehrke (2003). The term “variable” can be used in both the logical and probabilistic context.",
      "startOffset" : 141,
      "endOffset" : 975
    }, {
      "referenceID" : 15,
      "context" : "2 Counting Formulas Milch et al. (2008) introduced the idea of counting formulas and (parametrized) counting randvars.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "We use the definition of Kisynski and Poole (2009a) for parfactors, as it allows us to simplify the notation.",
      "startOffset" : 25,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "At a high level, it is similar to C-FOVE (Milch et al., 2008), the current state-of-the-art system in lifted variable elimination, but it differs in the definition and implementation of its operators.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "1 Lifted Multiplication The lifted multiplication operator multiplies whole parfactors at once, instead of separately multiplying the ground factors they cover (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).",
      "startOffset" : 160,
      "endOffset" : 214
    }, {
      "referenceID" : 17,
      "context" : "1 Lifted Multiplication The lifted multiplication operator multiplies whole parfactors at once, instead of separately multiplying the ground factors they cover (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).",
      "startOffset" : 160,
      "endOffset" : 214
    }, {
      "referenceID" : 17,
      "context" : "2 Lifted Summing-Out Once a PRV occurs in only one parfactor, it can be summed out from that parfactor (Milch et al., 2008).",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 17,
      "context" : "3 Counting Conversion Counting randvars may be present in the original model, but they can also be introduced into parfactors by an operation called counting conversion (Milch et al., 2008) (see also Section 2.",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 17,
      "context" : "This Mul function is identical to Milch et al.’s (2008) num-assign.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "A similar condition for FOVE’s counting elimination is mentioned by de Salvo Braz (2007). To see why precondition 3 is necessary, consider the parfactor g = φ(S(X),#Y [A(Y )]) |(X,Y ) ∈ {(x1, y2), (x1, y3), (x2, y1), (x2, y3), (x3, y1), (x3, y2)}, which does not satisfy it.",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "The above is a simple case of splitting parfactors (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).",
      "startOffset" : 51,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "The above is a simple case of splitting parfactors (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).",
      "startOffset" : 51,
      "endOffset" : 105
    }, {
      "referenceID" : 18,
      "context" : "C-FOVE operates per logvar, and splits off each value in a separate partition (splitting based on substitution) (Poole, 2003; Milch et al., 2008).",
      "startOffset" : 112,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "C-FOVE operates per logvar, and splits off each value in a separate partition (splitting based on substitution) (Poole, 2003; Milch et al., 2008).",
      "startOffset" : 112,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "5 Expansion of Counting Formulas When handling parfactors with counting formulas, to rewrite a P(C)RV into the proper from, we employ the operation of expansion (Milch et al., 2008).",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "C-FOVE uses expansion based on substitution (Milch et al., 2008).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "When this property does not hold, it can be achieved by normalizing the involved parfactor, which amounts to splitting the parfactor into parfactors for which the property does hold (Milch et al., 2008).",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 17,
      "context" : "In cases like this, when no other operators can be applied, lifted VE can always resort to a last operator: grounding a logvarX in a parfactor g (de Salvo Braz, 2007; Milch et al., 2008).",
      "startOffset" : 145,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : ", on the social network domain (Jha et al., 2010).",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "The first domain is called workshop attributes (Milch et al., 2008).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "φm(Attends(X), Attrm) φm+1(Attends(X), Series) The second domain is called competing workshops (Milch et al., 2008).",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "The third domain is called social network (Jha et al., 2010) and it models people’s smoking habits, their chance of having asthma, and the dependence of a persons habits and diseases on their friendships.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : "Surprisingly, most lifted inference algorithms use the same class of constraints based on pairwise (in)equalities (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011); the main exception is the work on approximate inference using lifted belief propagation (Singla & Domingos, 2008).",
      "startOffset" : 114,
      "endOffset" : 247
    }, {
      "referenceID" : 17,
      "context" : "Surprisingly, most lifted inference algorithms use the same class of constraints based on pairwise (in)equalities (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011); the main exception is the work on approximate inference using lifted belief propagation (Singla & Domingos, 2008).",
      "startOffset" : 114,
      "endOffset" : 247
    }, {
      "referenceID" : 11,
      "context" : "Surprisingly, most lifted inference algorithms use the same class of constraints based on pairwise (in)equalities (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011); the main exception is the work on approximate inference using lifted belief propagation (Singla & Domingos, 2008).",
      "startOffset" : 114,
      "endOffset" : 247
    }, {
      "referenceID" : 10,
      "context" : ", the works of Jha et al. (2010) and Van den Broeck et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : ", the works of Jha et al. (2010) and Van den Broeck et al. (2011), and further optimizing constraint handling.",
      "startOffset" : 15,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "With respect to the latter, an interesting direction is the recent work of de Salvo Braz, Saadati, Bui, and OReilly (2012) that employs a logical representation for constraints, which is extensionally complete, and presents specialized constraint processing methods for this representation.",
      "startOffset" : 84,
      "endOffset" : 123
    } ],
    "year" : 2013,
    "abstractText" : "Lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. More specifically, they identify groups of interchangeable variables and perform inference once per group, as opposed to once per variable. The groups are defined by means of constraints, so the flexibility of the grouping is determined by the expressivity of the constraint language. Existing approaches for exact lifted inference use specific languages for (in)equality constraints, which often have limited expressivity. In this article, we decouple lifted inference from the constraint language. We define operators for lifted inference in terms of relational algebra operators, so that they operate on the semantic level (the constraints’ extension) rather than on the syntactic level, making them language-independent. As a result, lifted inference can be performed using more powerful constraint languages, which provide more opportunities for lifting. We empirically demonstrate that this can improve inference efficiency by orders of magnitude, allowing exact inference where until now only approximate inference was feasible.",
    "creator" : "dvips(k) 5.992 Copyright 2012 Radical Eye Software"
  }
}