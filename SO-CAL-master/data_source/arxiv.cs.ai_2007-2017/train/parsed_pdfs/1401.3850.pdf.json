{
  "name" : "1401.3850.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Model-Based Active Testing Approach to Sequential Diagnosis",
    "authors" : [ "Alexander Feldman", "Gregory Provan", "Arjan van Gemund" ],
    "emails" : [ "a.b.feldman@tudelft.nl", "g.provan@cs.ucc.ie", "a.j.c.vangemund@tudelft.nl" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Combinational Model-Based Diagnosis (MBD) approaches (de Kleer & Williams, 1987) often lead to a large number of diagnoses, which is exponential in the number of components, in the worst-case. Combining multiple sensor readings (observation vectors) (Pietersma & van Gemund, 2006) helps in a limited number of cases because the approach is inherently passive, i.e., there are situations in which the observations repeat themselves (for example, in systems that are stationary, pending a reconfiguration).\nSequential diagnosis algorithms (Shakeri, 1996) can be used as an alternative to the above passive approach, with better decay of the number of diagnostic hypotheses. The decay rate depends on the tests and test dictionary matrix, and is bounded from below\nc©2010 AI Access Foundation. All rights reserved.\nby results for tests with binary outcomes. Algorithms for sequential diagnosis suffer from a number of other limitations. Early approaches assume single-faults while multiple-fault sequential diagnosis is super-exponential (Σp2 or harder) (Shakeri, Raghavan, Pattipati, & Patterson-Hine, 2000).\nAs observations (test outcomes) are not known in advance, the goal of a diagnostician is to create a policy that minimizes the diagnostic uncertainty on average, i.e., one aims at minimizing the average depth of a test tree. Pattipati and Alexandridis (1990) have shown that under certain conditions (e.g., unit test costs and equal prior fault probabilities) a onestep look-ahead policy leads to an optimal average depth of the test tree; de Kleer, Raiman, and Shirley (1992) have shown that one-step look-ahead delivers good practical results for a range of combinational circuits.\nThis paper proposes a framework, called Fractal (FRamework for ACtive Testing ALgorithms) for comparing different computational vs. optimality trade-offs in various techniques for reducing the diagnostic uncertainty. All Fractal algorithms start from an initial set of multiple-fault diagnostic hypotheses (this initial set can contain all possible hypotheses) and compute actions for reducing this initial set to, if possible, a single diagnostic hypothesis (candidate). In the case of probing (de Kleer & Williams, 1987), this action consists of measuring an internal (hidden) variable. In the case of sequential diagnosis (ATPG), the action consists of applying a set of input (control) assignments that disambiguate the health state of the component that appears faulty in most of the initial diagnostic hypotheses. In the case of active testing the action consists of applying a set of input (control) assignments that optimally reduce the initial set of hypotheses. In our framework the active testing and sequential diagnosis approaches differ only in how they compute the input (control) assignments. We measure the optimality of the algorithms by computing the speed with which they decay the initial set of hypotheses and the computational efficiency.\nIn Fractal, we study the influence of not just the input (IN) and output (OUT) variables, but also control (CTL) variables. Controls are similar to inputs, except they can be modified by users while the system is connected to its environment. Use of models from first principles and controls in Fractal allows us to eliminate the need of designing explicit tests and test dictionaries. Our algorithms implicitly create test matrices leading to optimal decay based on the built-in testing capabilities of the system. We call the above approach active testing; it is a technique using models from first principles and controls for creating test sequences that reduce the diagnostic uncertainty of a system. The architecture in which we use Fractal and active testing is shown in Fig. 1.\nAs reliable component failure rates may be problematic to obtain, we assume equally likely and small prior probabilities of failure and measure the diagnostic uncertainty as the number of Minimal Cardinality (MC) diagnoses. Fractal can be modified to use arbitrary failure probabilities and even components that are more likely to be faulty than healthy. This would necessitate modifications of some of the algorithms (e.g., change of bias in the importance sampling, etc.). In addition to simplifying the modeling, the equiprobable failure rates assumption also has computational advantages. It can be shown that with equal and small prior probabilities of failure, the diagnostic entropy, e.g., as used by de Kleer and Williams (1987), can be computed directly from the number of MC diagnoses.\nThe computational complexity of deterministic algorithms for sequential diagnosis increases with respect to both the fault-cardinality and the number of tests (the size of the test dictionary). To enable performance to scale up to real-world problems, which may have high fault-cardinality and a large number of tests, we propose FractalG–a low-cost greedy stochastic approach that maintains exponential decay of the number of MC diagnoses. Instead of assuming single faults or timing out, FractalG may result in suboptimal but still exponential decay.\nWe study the performance of FractalG compared to two alternatives: (1) FractalATPG, which implements sequential diagnosis based on Automated Test Pattern Generation (ATPG), and (2) FractalP, which implements probing (de Kleer & Williams, 1987). ATPG has been successfully used in the electronic industry to compute sets of inputs that test each component in a VLSI circuit. We have considered an ATPG-based approach because it is natural to attempt to reduce the diagnostic ambiguity by computing inputs that can disambiguate the status of the single component that appears in the majority of the diagnostic hypotheses.\nFractalATPG is derived from sequential testing, is deterministic and myopic, and allows us to evaluate how well a single-step lookahead approach works on the given model. Although probing is not classified as a technique for sequential diagnosis, it can be viewed as a process for generating tests using additional control circuitry (machine or human) to execute a probe such that some output reveals the internal variable. Its significance is that it shows a lower bound on the number of diagnoses achievable for a model extended with unlimited CTL circuitry.\nOur contributions are as follows:\n• We devise an approach for reducing the diagnostic uncertainty, called active testing, that generalizes sequential diagnosis and MBD, allows combination of multiple passive sensor readings, and does not require explicit tests and test dictionaries.\n• We design FractalATPG—a single-step look-ahead algorithm based on ATPG—for solving the active testing problem.\n• We design and implement FractalG–a greedy approximation algorithm for active testing that overcomes the limitations of FractalATPG and offers a trade-off in computational complexity vs. optimality for reducing the diagnostic uncertainty. We compare FractalG and FractalATPG.\n• We implement FractalP and use it as a computationally efficient, myopic (one-step lookahead), easy-to-analyze baseline technique for reducing diagnostic uncertainty.\nAlthough FractalP is technically not an active testing algorithm, the implementation of probing and active testing in a common framework and the unified experimentation help to understand the cost vs. performance trade-offs in (active and passive) testing vs. probing strategies.\n• We present extensive empirical data on 74XXX/ISCAS85 circuits, which enable us to evaluate FractalATPG, FractalG, and FractalP in terms of their ability to reduce the number of remaining diagnoses according to a geometric decay function.\nThis paper is organized as follows. Section 2 introduces related work. Section 3 presents basic MBD notions, the concept of remaining number of diagnoses and a framework for sequential diagnosis. Section 4 introduces a stochastic sampling-based algorithm for computing the expected number of cardinality-minimal diagnoses. Section 5 describes the FractalATPG, FractalG, and FractalP algorithms. Section 6 shows experimental results. Finally, Sec. 7 summarizes this paper and discusses future work."
    }, {
      "heading" : "2. Related Work",
      "text" : "Early work aimed at diagnostic convergence by de Kleer and Williams (1987) compute a probe sequence for reducing diagnostic entropy using a myopic search strategy. Unlike their work, in active testing we assume that probes are not available, other than indirectly exposed through diagnosis based on test vectors, which offers an automated solution.\nGenerating test vectors to deduce faults has received considerable attention. Automatic test pattern generation (ATPG) aims at verifying particular, single-faults (Stephan, Brayton, & Sangiovanni-Vincentelli, 1996). ATPG differs from active testing in that the vectors are specific for particular single-faults, whereas active testing generates a sequence of vectors to isolate unknown, multiple-faults, a much harder problem.\nTable 1 summarizes the properties of the various techniques for sequential diagnosis discussed in this paper. Fractal eliminates the need for using tools for building tests and test dictionaries, such as the ones proposed by Deb, Ghoshal, Malepati, and Kleinman (2000). In our approach tests and test dictionaries are automatically constructed from design speci-\nfications and models. At the same time, Fractal delivers comparable or better diagnostic convergence at reasonable computational price.\nActive testing bears some resemblance with sequential diagnosis, which also generates a sequence of test vectors (Pattipati & Alexandridis, 1990; Raghavan, Shakeri, & Pattipati, 1999; Tu & Pattipati, 2003; Kundakcioglu & Ünlüyurt, 2007). The principal difference is that in sequential diagnosis a fault dictionary is used (“fault matrix”). This pre-compiled dictionary has the following drawback: in order to limit the (exponential) size of the dictionary, the number of stored test vectors is extremely small compared to the test vector space. This severely constrains the optimality of the vector sequence that can be generated; in contrast, active testing computes arbitrary test vectors on the fly using a model-based approach. Furthermore, the matrix specifies tests that only have a binary (pass/fail) outcome, whereas active testing exploits all the system’s outputs, leading to faster diagnostic convergence. In addition, we allow the inputs to be dynamic, which makes our framework suitable for online fault isolation.\nThe sequential diagnosis problem studies optimal trees when there is a cost associated with each test (Tu & Pattipati, 2003). When costs are equal, it can be shown that the optimization problem reduces to a next best control problem (assuming one uses information entropy). In this paper a diagnostician who is given a sequence S and who tries to compute the next optimal control assignment would try to minimize the expected number of remaining diagnoses |Ω(S)|.\nOur task is harder than that of Raghavan et al. (1999), since the diagnosis task is NPhard, even though the diagnosis lookup uses a fault dictionary; in our case we compute a new diagnosis after every test. Hence we have an NP-hard sequential problem interleaved with the complexity of diagnostic inference at each step (in our case the complexity of diagnosis is Σp2-hard). Apart from the above-mentioned differences, we note that optimal test sequencing is infeasible for the size of problems in which we are interested.\nModel-Based Testing (MBT) (Struss, 1994) is a generalization of sequential diagnosis. The purpose of MBT is to compute inputs manifesting a certain (faulty) behavior. The main differences from our active testing approach are that MBT (1) assumes that all inputs are controllable and (2) MBT aims at confirming single-fault behavior as opposed to maximally decreasing the diagnostic uncertainty.\nBrodie, Rish, Ma, and Odintsova (2003) cast their models in terms of Bayesian networks. Our notion of entropy is the size of the diagnosis space, whereas Brodie et al. use decisiontheoretic notions of entropy to guide test selection. Brodie et al. extend their past Bayesian diagnostic approach (Rish, Brodie, & Ma, 2002) with sequential construction of probe sets (probe sets are collections of, for example, pings to a subset of the nodes in a computer network). The approach of Brodie et al. is limited to networks although it can be extended by modifying the type of Bayesian network shown by Rish et al.; such a modification, however, would necessitate more computationally expensive Bayesian reasoning for achieving good approximation results for the most probable explanations.\nThe approach of Brodie et al. (2003) does not compute modifications in the target network topology and does not propose control actions (for example, a network server that fails to respond can be dialed-up through a modem or checked by a technician at a higher cost). The similarity between Fractal and active probing is that both approaches attempt at reducing the diagnostic uncertainty by analyzing the future state of the system\nas a function of some action (sending a set of probes for active probing or an application of control inputs for FractalG and FractalATPG).\nWe solve a different problem than that of Heinz and Sachenbacher (2008), Alur, Courcoubetis, and Yannakakis (1995). Both of these approaches assume a non-deterministic model defined as an automaton. In contrast, our framework assumes a static system (plant model) for which we must compute a temporal sequence of tests to best isolate the diagnosis.\nEsser and Struss (2007) also adopt an automaton framework for test generation, except that, unlike Heinz and Sachenbacher (2008) or Alur et al. (1995), they transform this automaton to a relational specification, and apply their framework to software diagnosis. This automaton-based framework accommodates more general situations than does ours, such as the possibility that the system’s state after a transition may not be uniquely determined by the state before the transition and the input, and/or the system’s state may be associated with several possible observations. In our MBD framework, a test consists of an instantiation of several variables, which corresponds to the notion of test sequence within the automaton framework of Heinz and Sachenbacher. The framework of Esser and Struss requires modeling of the possible faults, whereas Fractal works both with weak and strong-fault models1. Interestingly, as shown by Esser and Struss, modeling of abnormal software behavior can be derived to some extent from software functional requirements. This makes their framework suitable for software systems.\nA recent approach to active diagnosis is described by Kuhn, Price, de Kleer, Do, and Zhou (2008), where additional test vectors are computed to optimize the diagnosis while the system (a copier) remains operational. Their work differs from ours in that plans (roughly analogous to test sequences) with a probability of failure T are computed statically, and a plan remains unmodified even if it fails to achieve its desired goal (a manifestation of a failure with probability close to T ). Conversely, Fractal dynamically computes next-best control settings in a game-like manner. The biggest difference between Fractal and the approach of Kuhn et al. is in the use of models. Fractal is compatible with traditional MBD (de Kleer & Williams, 1987) and can reuse existing models from first principles while the pervasive approach of Kuhn et al. uses an automaton and a set of possible actions.\nThe approach of Kuhn et al. (2008) uses existing MBD and planning algorithms, and as such integrates existing approaches; in contrast, Fractal introduces new control algorithms and reuses an external diagnostic oracle. An advantage of the pervasive diagnosis approach is that the use of a planning engine generates a complete sequence of actions, as opposed to the one-step lookahead of FractalG. Depending on the planning formalism, the complexity of pervasive diagnosis can be dominated by the planning module, while the most complex computational task in Fractal is that of diagnosis. Both pervasive diagnosis and this paper, however, report good average-case computational efficiency for benchmark problems. Last, the paper of Kuhn et al. is limited to single-fault diagnoses, although the pervasive diagnosis framework can be generalized to multiple faults.\nFeldman, Provan, and van Gemund (2009a) introduce an early version of FractalG. This paper (1) generalizes the Fractal framework, (2) introduces FractalATPG and FractalP, (3) extends the experimental results, and (4) provides a comparison of the different Fractal approaches.\n1. Weak-fault models (also known as models with ignorance of abnormal behavior) and strong-fault models are discussed by Feldman, Provan, and van Gemund (2009b)."
    }, {
      "heading" : "3. Concepts and Definitions",
      "text" : "Our discussion starts by introducing relevant MBD notions. Central to MBD, a model of an artifact is represented as a propositional Wff over a set of variables V . We will define four subsets of these variables: assumable, observable2, control, and internal variables. This gives us our initial definition:\nDefinition 1 (Active Testing System). An active testing system ATS is defined as ATS = 〈SD, COMPS, CTL, OBS〉, where SD is a propositionalWff over a variable set V , COMPS∪ OBS ∪ CTL ⊆ V , and COMPS, OBS, and CTL are subsets of V containing assumable, observable, and control variables, respectively.\nThe set of internal variables is denoted as INT, INT = V \\ {COMPS ∪ OBS ∪ CTL}. Throughout this paper we assume that OBS, COMPS, and CTL are disjoint, and SD 6|=⊥. Sometimes it is convenient (but not necessary) to split OBS into non-controllable inputs IN and outputs OUT (OBS = IN ∪OUT, IN ∩OUT = ∅)."
    }, {
      "heading" : "3.1 Running Example",
      "text" : "We will use the Boolean circuit shown in Fig. 2 as a running example for illustrating all notions and the algorithm shown in this paper. The 2-to-4 line demultiplexer consists of four Boolean inverters and four and-gates.\nThe expression h ⇒ (o⇔ ¬i) models an inverter, where the variables i, o, and h represent input, output, and health respectively. Similarly, an and-gate is modeled as h ⇒ (o⇔ i1 ∧ i2 ∧ i3). The above propositional formulae are copied for each gate in Fig. 2 and their variables subscripted and renamed in such a way as to ensure a proper disambiguation\n2. In the MBD literature the assumable variables are also referred to as “component”, “failure-mode”, or “health” variables. Observable variables are also called “measurable” variables.\nand to connect the circuit. The result is the following propositional model:\nSD =\n\n      \n       [h1 ⇒ (a⇔ ¬p)] ∧ [h2 ⇒ (p⇔ ¬r)] [h3 ⇒ (b⇔ ¬q)] ∧ [h4 ⇒ (q ⇔ ¬s)] h5 ⇒ (o1 ⇔ i ∧ p ∧ q) h6 ⇒ (o2 ⇔ i ∧ r ∧ q) h7 ⇒ (o3 ⇔ i ∧ p ∧ s) h8 ⇒ (o4 ⇔ i ∧ r ∧ s)\n(1)\nThe set of assumable variables is COMPS = {h1, h2, . . . , h8}, the observable variables are OBS = {a, b, o1, o2, o3, o4}, and the set of control variables is the singleton CTL = {i}. Note the conventional selection of the sign of the “health” variables h1, h2, . . . , hn. Other authors use “ab” for abnormal."
    }, {
      "heading" : "3.2 Diagnosis",
      "text" : "The traditional query in MBD computes terms of assumable variables, which are explanations for the system description and an observation.\nDefinition 2 (Diagnosis). Given a system ATS, an observation α over some variables in OBS, and an assignment ω to all variables in COMPS, ω is a diagnosis iff SD ∧ α ∧ ω 6|=⊥.\nThe set of all diagnoses of SD and an observation α is denoted as Ω(SD, α). The cardinality of a diagnosis, denoted as |ω|, is defined as the number of negative literals in ω.\nContinuing our running example, consider an observation vector α1 = ¬a ∧ ¬b ∧ i ∧ o4. There are a total of 256 possible assignments to all variables in COMPS and |Ω(SD, α1)| = 200. Example diagnoses are ω1 = h1 ∧ h2 ∧ . . . ∧ h7 ∧ ¬h8 and ω2 = ¬h1 ∧ h2 ∧ h3 ∧ ¬h4 ∧ h5 ∧ h6 ∧ h7 ∧ h8. We will write sometimes a diagnosis in a set notation, specifying the set of negative literals only. Thus ω2 would be represented as D2 = {¬h1,¬h4}.\nDefinition 3 (Minimal-Cardinality Diagnosis). A diagnosis ω≤ is defined as MinimalCardinality (MC) if no diagnosis ω̃≤ exists such that |ω̃≤| < |ω≤|.\nOur selection of minimality criterion is such that it is impossible to compute all diagnoses from the set of all MC diagnoses without further inference. MC diagnoses, however, are often used in practice due to the prohibitive cost of computing a representation of all diagnoses of a system and an observation (e.g., all subset-minimal diagnoses).\nConsider an observation vector α2 = ¬a ∧ ¬b ∧ i ∧ ¬o1 ∧ o4. There are 6 MC diagnoses of cardinality 2 consistent with SD ∧ α2, and counting these MC diagnoses is a common problem in MBD.\nThe number of MC diagnoses of a system ATS and an observation α is denoted as |Ω≤(SD, α)|, where Ω≤(SD, α) is the set of all MC diagnoses of SD ∧ α. Given a system ATS, an observation sequence S is defined as a k-tuple of terms S = 〈α1, α2, . . . , αk〉, where αi (1 ≤ i ≤ k) is an instantiation of variables in OBS.\nThroughout this paper, we assume that the health of the system under test does not change during the test (i.e., the same inputs and a fault produce the same outputs) and call this assumption stationary health.\nLemma 1. Given a system ATS, a stationary health state for its components ω, and an observation sequence S, it follows that ω ∈ Ω(SD, α1) ∩ Ω(SD, α2) ∩ . . . ∩ Ω(SD, αk).\nProof. The above statement follows immediately from the stationary health assumption and Def. 2.\nLemma 1 can be applied only in the cases in which all diagnoses are considered. If we compute subset-minimal diagnoses in a weak-fault model, for example, the intersection operator has to be redefined to handle subsumptions. To handle non-characterizing sets of diagnoses3 (e.g., MC or first m diagnoses), we provide the following definition.\nDefinition 4 (Consistency-Based Intersection). Given a set of diagnoses D of SD∧α, and an a posteriori observation α′, the intersection of D with the diagnoses of SD∧α′, denoted as Ω∩(D,α′), is defined as the set D′ (D′ ⊆ D) such that for each ω ∈ D′ it holds that SD ∧ α′ ∧ ω 6|=⊥.\nIt is straightforward to generalize the above definition to an observation sequence S.\nDefinition 5 (Remaining Minimal-Cardinality Diagnoses). Given a diagnostic system ATS and an observation sequence S, the set of remaining diagnoses Ω(S) is defined as Ω(S) = Ω∩(Ω∩(· · ·Ω∩(Ω≤(SD, α1), α2), · · · ), αk).\nWe use |Ω(S)| instead of the more precise diagnostic entropy as defined by de Kleer and Williams (1987) and subsequent works, as this allows low-complexity estimations (discussed in Sec. 4). In particular, if all diagnoses are of minimal-cardinality and the failure probability of each component is the same, then the gain in the diagnostic entropy can be directly computed from |Ω(S)|."
    }, {
      "heading" : "4. Computing the Expected Number of MC Diagnoses",
      "text" : "Active testing aims to minimize the expected number of diagnoses that result from the possible set of outputs that may occur from a given control vector. In this section we present an algorithm to approximate this expectation.\nWe will compute the expected number of diagnoses for a set of observable variables M (M ⊆ OBS). The initial observation α and the set of MC diagnoses D = Ω≤(SD, α) modify the probability density function of subsequent outputs (observations), i.e., a subsequent observation α′ changes its likelihood. The (non-normalized) a posteriori probability of an observation α′, given a function Ω≤ that computes the set of MC diagnoses and an initial observation α, is:\nPr(α′|SD, α) = |Ω ∩(Ω≤(SD, α), α′)| |Ω≤(SD, α)| (2)\nThe above formula computes the probability of a given a priori set of diagnoses restricting the possible outputs, i.e., we assume that the probability is the ratio of the number of\n3. A characterizing set of diagnoses, for example the set of all subset-minimal diagnoses, is loosely defined as a set of diagnoses from which the (complete) set of all diagnoses can be constructed without using the system description or any other information.\nremaining diagnoses to the number of initial diagnoses. In practice, there are many α for which Pr(α′|SD, α) = 0, because a certain fault heavily restricts the possible outputs of a system (i.e., the set of the remaining diagnoses in the numerator is empty).\nThe expected number of remaining MC diagnoses for a variable set M , given an initial observation α, is then the weighted average of the intersection sizes of all possible instantiations over the variables in M (the weight is the probability of an output):\nE≤(SD,M |α) =\n∑\nα′∈M∗ |Ω∩(D,α′)| · Pr(α′|SD, α) ∑\nα′∈M∗\nPr(α′|SD, α) (3)\nwhere D = Ω≤(SD, α) and M∗ is the set of all possible assignments to the variables in M . Replacing (2) in (3) and simplifying gives us the following definition:\nDefinition 6 (Expected Minimal-Cardinality Diagnoses Intersection Size). Given a system ATS and an initial observation α, the expected remaining number of MC diagnoses E≤(SD,OBS|α) is defined as:\nE≤(SD,OBS|α) =\n∑\nα′∈OBS∗\n|Ω∩(Ω≤(SD, α), α′)|2\n∑\nα′∈OBS∗\n|Ω∩(Ω≤(SD, α), α′)| (4)\nwhere OBS∗ is the set of all possible assignments to all variables in OBS.\nTwo of the algorithms presented in this paper compute the expected number of remaining MC diagnoses for one variable. As a result the expectation expression in (4) simplifies to:\nE≤(SD, v|α) = |Ω ∩(Ω≤(SD, α), v)|2 + |Ω∩(Ω≤(SD, α),¬v)|2\n|Ω∩(Ω≤(SD, α), v)| + |Ω∩(Ω≤(SD, α),¬v)| (5)\nThe complexity of computing (5) depends only on the length of the sequence S, the complexity of the MC oracle computing Ω≤(SD, α), and the complexity of the intersection algorithm."
    }, {
      "heading" : "4.1 Computing the Expectation Using Importance Sampling",
      "text" : "To overcome the computational complexity of evaluating an expectation, we employ a stochastic algorithm based on importance sampling. The key insight that allows us to build a fast method for computing the expected number of remaining diagnoses is that the prior observation (and respectively the set of MC diagnoses) shifts the probability of the outputs. Hence, an algorithm that samples the possible input assignments (recall that it is a basic modeling assumption that inputs are equally likely) and counts the number of different observations, given the set of prior diagnoses, can produce a good approximation.\nWe next introduce an algorithm for approximating the expected number of remaining diagnoses.\nAlgorithm 1 Approximate expectation of Ω(S)\n1: function Expectation(ATS, γ,D) returns a real\ninputs: ATS (active testing system): model γ (term): control vector D (set of diagnoses): prior diagnoses local variables: α, β, ω (terms): observation s (integer): sum of the remaining diagnoses, initially 0 q (integer): sum of squares of the remaining diagnoses, initially 0 Z (set of terms): samples\nÊ (real): expectation\n2: Z ← ∅ 3: repeat 4: α← RandomInputs(SD, IN) 5: for all ω ∈ D do 6: β ← InferOutputs(SD,OUT, α ∧ γ, ω) 7: if α ∧ β 6∈ Z then 8: Z ← Z ∪ {α ∧ β} 9: q ← q + |Ω∩(D,α ∧ β ∧ γ)|2\n10: s← s+ |Ω∩(D,α ∧ β ∧ γ)| 11: Ê ← q/s 12: end if\n13: end for 14: until Terminate(Ê) 15: return Ê 16: end function\nAlgorithm 1 uses a couple of auxiliary functions: RandomInputs assigns random values to all inputs and InferOutputs computes all outputs from the system model, all inputs and a diagnosis.4 The computation of the intersection size |Ω∩(D,α∧β∧γ)| can be implemented by counting those ω ∈ D for which SD ∧ α ∧ β ∧ γ ∧ ω 6|=⊥.\nThe algorithm terminates when a termination criterion (checked by Terminate) is satisfied. In our implementation, Terminate returns success when the last n iterations (where n is a small constant) leave the expected number of diagnoses, Ê, unchanged, in terms of its integer representation. Our experiments show that for all problems considered, n < 100 yields a negligible error.\nThe complexity of Alg. 1 is determined by the complexity of consistency checking (line 9 – 10) and the size of D. If we denote the complexity of a single consistency check with Υ, then the complexity of Alg. 1 becomes O(|D|Υ). Although consistency checking for diagnostic problems is NP -hard in the worst case, for average-case problems it is easy. In our implementation of Expectation we overcome the complexity of consistency checking\n4. This is not always possible in the general case. In our framework, we have a number of assumptions, i.e., a weak-fault model, well-formed circuit, etc. The complexity of InferOutputs thus depends on the framework and the assumptions.\nby using an incomplete Logic-Based Truth Maintenance System (LTMS) (Forbus & de Kleer, 1993)."
    }, {
      "heading" : "5. Algorithms for Reducing the Diagnostic Uncertainty",
      "text" : "In this section we introduce three algorithms: FractalATPG, FractalG, and FractalP."
    }, {
      "heading" : "5.1 Problem Definition and Exhaustive Search",
      "text" : "Our AT problem is defined as follows:\nProblem 1 (Optimal Control Sequence). Given a system ATS, a sequence (of past observations and controls) S = 〈α1 ∧ γ1, α2 ∧ γ2, · · · , αk ∧ γk〉, where αi (1 ≤ i ≤ k) are OBS assignments and γj (1 ≤ j ≤ k) are CTL assignments, compute a new CTL assignment γk+1, such that:\nγk+1 = argmin γ∈CTL⋆\nE≤(Ω∩(SD, S), {IN ∪OUT}|γ) (6)\nwhere CTL⋆ is the space of all possible control assignments.\nProblem 1 is different from the general sequential testing problem, as formulated by Shakeri (1996). In the Shakeri formulation, there are different test costs and different prior failure probabilities, where Problem 1 assumes equal costs and equal small prior probabilities of failure. Pattipati and Alexandridis (1990) show that under those assumptions, minimizing the test cost at each step constitutes an optimal policy for minimizing the expected test cost. Hence, solving Problem 1 is solving the lesser problem of generating an optimal test strategy given unit costs and equal prior failure probability. Note that we can use an algorithm that optimizes Problem 1 as a heuristic algorithm for solving the sequential testing problem. In this case the expected cost would be arbitrarily far from the optimum one, depending on the cost distribution and the tests.\nConsider our running example with an initial observation vector (and control assignment) α3 ∧ γ3 = a ∧ b ∧ i ∧ o1 ∧ ¬o2 ∧ ¬o3 ∧ ¬o4, where γ3 = i is chosen as the initial control input. The four MC diagnoses of SD ∧ α3 ∧ γ3 are Ω≤ = {{¬h1,¬h3}, {¬h2,¬h5}, {¬h4,¬h5}, {¬h5,¬h8}}.\nAn exhaustive algorithm would compute the expected number of diagnoses for each of the 2|CTL| next possible control assignments. In our running example we have one control variable i and two possible control assignments (γ5 = i and γ6 = ¬i). To compute the expected number of diagnoses, for each possible control assignment γ and for each possible observation vector α, we have to count the number of initial diagnoses which are consistent with α ∧ γ.\nComputing the intersection sizes for our running example gives us Table 2. Note that, in order to save space, Table 2 contains rows only for those α ∧ γ for which Pr(α ∧ γ) 6= 0, given the initial diagnoses Ω≤ (and, as a result, |Ω∩(Ω≤(SD, α3 ∧ γ3), α ∧ γ)| 6= 0). It is straightforward to compute the expected number of diagnoses for any control assignment with the help of this marginalization table. In order to do this we have to (1) filter out those lines which are consistent with the control assignment γ and (2) compute the sum and the sum of the squares of the intersection sizes (the rightmost column of Table 2).\nTo compute E(SD,OBS|α3 ∧ ¬i), we have to find the sum and the sum of the squares of the intersection sizes of all rows in Table 2 for which column i is F. It can be checked that E(SD,OBS|α3,¬i) = 24/16 = 1.5. Similarly, E(SD,OBS|α3 ∧ i) = 34/16 = 2.125. Hence an optimal diagnostician would consider a second measurement with control setting γ = i.\nThe obvious problem with the above brute-force approach is that the size of the marginalization table is, in the worst-case, exponential in |OBS|. Although many of the rows in the marginalization table can be skipped as the intersections are empty (there are no consistent prior diagnoses with the respective observation vector and control assignment), the construction of this table is computationally so demanding that we will consider an approximation algorithm (to construct Table 1 for our tiny example, the exhaustive approach had to perform a total of 512 consistency checks).\n5.2 FractalATPG\nConsider the running example from Sec. 3 and an observation α4 = a∧ b∧ i∧o1∧¬o4. This leads to the 6 double-fault MC diagnoses, shown in Fig. 3.\nInstead of searching through the space of all possible control assignments, we directly compute a control assignment that tests a specific component c by using an approach from\nATPG. We choose this component c to be the one that most decreases the expected number of remaining MC diagnoses by minimizing E≤(SD, c|α∧ γ). If we look at Fig. 3 we can see that knowing the health of h1 and h3 leads to E\n≤ ≈ 3.33, for h2, h4, h5, and h7, we have E≤ ≈ 4.33, and for h6 and h7 we have E≤ = 6. Choosing a control setting that computes the state of h1 or h3 is intuitive as the state of this component makes the most balanced partition of the prior diagnoses.\nWe next present the FractalATPG algorithm that uses the approach illustrated above.\nAlgorithm 2 ATPG-Based active testing algorithm\n1: function FractalATPG(ATS, α, γ) returns a control term\ninputs: ATS (active testing system): model α (term): initial (non-modifiable) observation γ (term): initial control local variables: c (variable): component f (integer): remaining diagnoses d (term): diagnosis γ (term): control setting H (set of pairs): component/expectation pairs D (set of terms): diagnoses\n2: D ← Ω≤(SD, α ∧ γ) 3: for all c ∈ COMPS do 4: f ← 0 5: for all d ∈ D do 6: if c ∈ d then 7: f ← f + 1 8: end if\n9: end for\n10: H ← 〈c, f2 + (|D| − f)2〉 11: end for 12: H ← SortByExpectation(H) 13: for i = 1 . . . |H| do 14: if γ ← ATPG(ATS, α,Hi〈c〉) then 15: return γ 16: end if\n17: end for 18: return RandomControls() 19: end function\nAlgorithm 2 counts the number of prior diagnoses that each component appears in (lines 4 - 8) and the result is saved in the variable f . This number is then used to compute the expected number of remaining MC diagnoses given the component health (line 10). For each component the expected number of diagnoses is stored in the set H (line 10). The set H is then sorted in increasing order of expectation (line 12). We then iterate over the set of components in order of expectation (lines 13 – 17). For each component we try to compute\nan ATPG vector that tests it. In some cases such a vector may not exist. In the worst case there is no ATPG vector that can test any component, and Alg. 2 has no better strategy but to return a random control assignment (line 18).\nThe time complexity of Alg. 2 is determined by the complexity of the diagnostic search (line 2) and the complexity of ATPG (line 14). If we denote the former with Ψ and the latter with Φ then the complexity of FractalATPG becomes O(ΦΨ|COMPS|). As the complexity of ATPG is usually lower than that of diagnosis (abductive reasoning) (Φ < Ψ), the complexity of FractalATPG is determined by the time for computing MC diagnoses.\nComputing ATPG vectors has been extensively studied (Bushnell & Agrawal, 2000) and although it is known to be an NP -hard problem (Ibarra & Sahni, 1975), there exists evidence that ATPG is easy for practical problems (Prasad, Chong, & Keutzer, 1999). Some efficient ATPG algorithms integrate randomized approach and Boolean difference (Bushnell & Agrawal, 2000). The former approach efficiently computes test vectors for the majority of components, while the latter computes test vectors for the remaining components by using a DPLL-solver.\nWe implement ATPG as follows. First we duplicate the system description SD by renaming each variable v : v 6∈ {IN ∪ CTL} to v′, thus generating SD′ (SD and SD′ share the same input and control variables). Then we create the all healthy assignment (for all assumable variables) ω0 and the single fault assignment ωI such that ω0 and ωI differ only in the sign of the literal whose component we want to test. Finally, we construct the following propositional expression:\n∆ ≡ α ∧ SD ∧ SD′ ∧ ω0 ∧ ωI ∧ [ ∨\no∈OUT\no⊕ o′ ]\n(7)\nwhere the operator ⊕ denotes an exclusive or, hence o⊕ o′ ≡ (¬o ∧ o′) ∨ (o ∧ ¬o′). The propositional expression in (7) leaves unconstrained only the controls γ that we need. There are two “instances” of the system: healthy (SD and ω0) and faulty (SD and ωI). The last term in ∆ forces the output of the healthy and the faulty system to be different in at least one bit. To compute an ATPG control vector we need one satisfiable solution of ∆. Note that an ATPG control vector may not exist (∆ |=⊥), i.e., a component may not be testable given CTL and SD ∧ α. Often there are multiple satisfying control assignments. In this case FractalATPG chooses an arbitrary one. The latter does not mean that all satisfiable ATPG control vectors achieve the same uncertainty reduction. FractalATPG becomes suboptimal when there is no control testing a given component, or when there are multiple controls. FractalATPG becomes completely random when there are no components that can be tested with the given choice of controls.\nThere are two problems with FractalATPG. First, FractalATPG assumes stationary inputs, i.e., FractalATPG ignores a source of uncertainty. The non-modifiable inputs, however, can only help in the decay process, hence FractalATPG is “conservative” in choosing the control assignments–a feature that leads to suboptimality. A bigger problem is that FractalATPG decreases the expected number of remaining MC diagnoses by computing the exact health of one component. Here, the problem is not that FractalATPG tests one component per step, but that it tries to compute a control assignment that computes the exact state of this component. An active testing algorithm can decrease the diagnostic uncertainty by computing a probability distribution function for the state of each component.\nA natural extension of FractalATPG is an algorithm that computes the state of k components simultaneously. The latter approach assumes that the system is k-component testable–an unrealistic assumption. In our experiments we have seen that systems are often even not single-component testable. Note that computing the exact states of components is not a requirement for decreasing the diagnostic uncertainty. Instead of computing the exact state of one or more components, the algorithm shown in the next section implicitly builds a probability density function for the health state of each component, and does not suffer from the problems of FractalATPG.\n5.3 FractalG\nConsider SD from the example started in Sec. 3, input variables IN = {i}, control variables CTL = {a, b}, initial input values β = i, and an initial observation α3 = β ∧ (¬a ∧ ¬b) ∧ (¬o1 ∧ o2 ∧ o3 ∧ o4). The initial observation α3 leads to 5 triple-fault MC diagnoses: Ω≤(SD, α3) = {{¬h1, ¬h4,¬h7}, {¬h1,¬h7,¬h8}, {¬h2,¬h3,¬h6}, {¬h2,¬h4,¬h5}, {¬h3,¬h6,¬h8}}. We also write D = Ω≤(SD, α3) and choose one of the faults in D to be the truly injected fault ω∗ (let ω∗ = {¬h1,¬h7,¬h8}).\nThe left and right parts of Fig. 4 show two possible scenarios for locating ω∗. On the left we have an exhaustive approach which considers all the 2|CTL| control assignments, hence it cannot be used to solve practical problems. The greedy scenario on the right side of Fig. 4 decreases the number of computations of expected number of remaining MC diagnoses from 2|CTL| to |CTL|. The idea is to flip one control variable at a time, to compute the expected number of remaining MC diagnoses and to keep the flip (shown in bold in Fig. 4) if E≤ decreases. Given an initial control assignment γ we consider the space of possible control flips. This space can be visualized as a lattice (Fig. 5 shows a small example). Figure 5\nshows the expected number of MC diagnoses for each control assignment. Note that probing can be visualized in a similar way.\nIn practice, control literals are mostly independent and even though the space of control assignments is not continuous in general, it has large continuous subspaces. The greedy approach is shown in Alg. 3, which computes a control assignment for a given active testing system and a prior observation.\nAlgorithm 3 Greedy active testing algorithm\n1: function Fractal(ATS, α) returns a control term\ninputs: ATS (active testing system): model α (term): initial observation local variables: γ, γ′ (terms): control configurations E,E′ (reals): expectations D (set of terms): diagnoses l (literal): control literal\n2: D ← Ω≤(SD, α) 3: E ← Expectation(ATS, γ,D) 4: for all l ∈ γ do 5: γ′ ← FlipLiteral(γ, l) 6: E′ ← Expectation(ATS, γ′,D) 7: if E′ < E then 8: γ ← γ′ 9: E ← E′\n10: end if\n11: end for 12: return γ 13: end function\nThe set of initial diagnoses is computed from the initial observation in line 2. In line 5, Alg. 3 “flips” the next literal in the current control assignment. The auxiliary FlipLiteral subroutine simply changes the sign of a specified literal in a term. After each “flip” the expected intersection size is computed with a call to Expectation (cf. Alg. 1). If the new expected intersection size is smaller than the current one, then the proposed control assignment is accepted as the current control assignment, and the search continues from there.\nThe complexity of FractalG is determined by the complexity of the diagnostic search (line 2) and the complexity of Expectation (line 3 and line 6). If we denote the former with Ψ and the latter with Ξ then the complexity of FractalG becomes O(ΦΞ|CTL|). As Φ ∼ Ξ, the complexity of FractalG is the same as FractalG. In practice FractalG requires more computation to compute a sufficient decay. This is due to the design of Expectation (Alg. 1).\nWhile the active-testing problem is worst-case NP -hard (it can be reduced to computing a diagnosis), as we will see in the experimentation section, it is possible to achieve very good average-case performance by choosing an appropriate MBD oracle. The advantage of the greedy approach, in particular, is that the number of computations of the expected number of diagnoses is linear in the number of literals in the control assignment. This is done at the price of some optimality (i.e., the effect of combinations of controls is neglected).\n5.4 FractalP\nProbing is related to active testing as measuring internal variables can be thought of as revealing internal control circuits. Alternatively, one can add control circuitry to a model that reveals the values of internal variables. To reveal this hidden control potential we implement GDE probing (de Kleer & Williams, 1987) in FractalP. Our approach is different from GDE in two ways. First, we compute the expected number of remaining MC diagnoses instead of expected diagnostic entropy. Second, Fractal does not use an Assumption-Based Truth Maintenance System (ATMS) (de Kleer, 1986).\nConsider the running example from Sec. 3 and an observation α5 = ¬a∧ ¬b∧ i ∧ ¬o1 ∧ o2 ∧ o3 ∧ o4. This leads to 5 triple-fault MC diagnoses: Ω≤(SD, α3) = {{¬h1, ¬h4,¬h7}, {¬h1,¬h7,¬h8}, {¬h2,¬h3,¬h6}, {¬h2,¬h4,¬h5}, {¬h3,¬h6,¬h8}}. Subsequent measurement of p gives us |Ω∩(Ω≤(SD, α3), p)| = 3 if p is positive and |Ω∩(Ω≤(SD, α5),¬p)| = 2 otherwise. The expected number of MC diagnoses is E≤(SD, {p}|α3) = 2.6. Repeating this for the remaining internal variables results in E≤(SD, {q}|α3) = 2.6, E≤(SD, {r}|α3) = 3.4, and E≤(SD, {s}|α3) = 3.4. As a result we can see that measuring p and q is less informative than measuring r and s, which is intuitive as r and s give a more balanced partitioning of the circuit.\nProblem 2 (Probe Sequencing). Given a system ATS, an observation α and a partial assignment to the internal variables ψ, choose a variable p∗ from the set U of unassigned internal variables ψ, such that:\np∗ = argmin p∈U E≤(SD, p|α ∧ ψ) (8)\nAlgorithm 4 solves Problem 2. Algorithm 4 computes the expected number of diagnoses for each unobserved variable (lines 3 - 11). Starting from the set D of initial diagnoses\n(computed in line 2), Alg. 4 perform a total of 2|D||V \\{OBS∪COMPS}| consistency checks (lines 4 and 5) to determine the expected number of MC diagnoses for each unobserved variable.\nWe next show the probing algorithm as introduced by de Kleer and Williams (1987) and adapted for the Fractal framework.\nAlgorithm 4 Probing algorithm\n1: function FractalP(ATS, α) returns a variable\ninputs: ATS (active testing system): model α (term): observation local variables: v,R (variables): probes E,E′ (reals): expectations p, q (reals): remaining diagnoses D (set of terms): diagnoses\n2: D ← Ω≤(SD, α) 3: for all v ∈ V \\ {COMPS ∪OBS} do 4: p← |Ω∩(D,α ∧ v)| 5: q ← |Ω∩(D,α ∧ ¬v)| 6: E′ ← (p2 + q2)/(p + q) 7: if E′ < E then 8: R← v 9: E ← E′\n10: end if\n11: end for 12: return R 13: end function\nInstead of computing the expected number of remaining MC diagnoses for a single variable p, it is possible to consider measuring all pairs of variables 〈p1, p2〉, or in general, all k-tuples of internal variables 〈p1, p2, . . . , pm〉 form ≤ |V \\{OBS∪COMPS}|. We will refer to probing involving more than 1 variable as k-probing. Although it has been shown that users do not significantly benefit in terms of diagnostic uncertainty by performing k-probing (de Kleer et al., 1992), we can easily modify FractalP to consider multiple probes. Note that for m = |V \\ {OBS ∪ COMPS}| there is no probing problem, as there is only one way to pick all internal variables.\nThe most complex operation in FractalP is again computing the initial set of MC diagnoses. In addition to that, we have |V \\ {COMPS ∪ OBS}| consistency checks. Consistency checking is, in general, easier than diagnosis. Note that all Fractal algorithms (FractalATPG, FractalG, and FractalP) start with computing the set of initial MC diagnoses. Hence, the difference in their performance is determined by the complexity of reducing the initial set Ω≤(SD, α). According to this criterion, the fastest algorithm is FractalP as it only performs a small number of consistency checks, followed closely by FractalATPG (computing ATPG vectors). The slowest algorithm is FractalG, as it computes the expected number of MC diagnoses given multiple variables."
    }, {
      "heading" : "6. Experimental Results",
      "text" : "We have implemented Fractal in approximately 3 000 lines of C code (excluding the diagnostic engine and the Logic Based Truth Maintenance System). All experiments were run on a 64-node dual-CPU cluster (each node configured with two 2.4 GHz AMD Opteron DP 250 processors and 4 Gb of RAM)."
    }, {
      "heading" : "6.1 Experimental Setup",
      "text" : "We have experimented on the well-known benchmark models of ISCAS85 combinational circuits (Brglez & Fujiwara, 1985). As models derived from the ISCAS85 circuits are computationally intensive (from a diagnostic perspective), we have also considered four mediumsized circuits from the 74XXX family (Hansen, Yalcin, & Hayes, 1999). In order to use the same system model for both MC diagnosis counting and simulation, the fault mode of each logic gate is “stuck-at-opposite”, i.e., when faulty, the output of a logic gate assumes the opposite value from the nominal. Without loss of generality, only gates are allowed to fail in our models. This is different from ATPG where gates typically do not fail but wires are modeled as components that can fail with failure modes “stuck-at-zero” and “stuck-at-one”. The ATPG and MBD modeling approaches achieve the same results.\nIn addition to the original 74XXX/ISCAS85 models, we have performed cone reductions as described by Siddiqi and Huang (2007) and de Kleer (2008). Recall that from the perspective of the MBD diagnostic engine, faults inside a cone (where a cone is a set of components) cannot be distinguished, hence it is enough to provide a single health variable per cone. We call models with a single health variable per cone “reduced”. Table 3 describes all models.\nBoth initial observation vectors and control settings are used in the first step of the Fractal inference. To illustrate the significant diagnostic convergence that is possible, we use initial observations leading to high numbers of initial MC diagnoses.\nTo average over the diagnostic outcomes of the observations, we repeat each experiment with a range of initial observation vectors. The cardinality of the MC diagnosis is of no significance to Fractal, but it produces a significant burden on the diagnostic oracle (Feldman, Provan, & van Gemund, 2008). In order to overcome this computational difficulty, we have limited our experiments to observation vectors leading to double faults only.\nFor each circuit we have generated 1 000 non-masking double-faults, and for each observation we have computed the number of initial MC diagnoses. From the 1 000 observation vectors we have taken the 100 with the largest number of MC diagnoses. The resulting observations are summarized in Table 4. For example, we can see a staggering number of 46 003 double faults for the most under-constrained c7552 observation.\nSince the 74XXX/ISCAS85 circuits have no control variables we “abuse” the benchmark by designating a fraction of the input variables as controls.\nWe define two policies for generating next inputs: random and stationary. The latter input policy (where the input values do not change in time) is a typical diagnostic worst-case for system environments which are, for example, paused pending diagnostic investigation, and it provides us with useful bounds for analyzing Fractal’s performance.\nNote that the use of non-characterizing sets of diagnoses (see Def. 4) may lead to a situation in which the real (injected) fault is not in the initial set of diagnoses. In such a case the set of remaining diagnoses Ω(S) may become an empty set after some number of Fractal steps. Although this gives us some diagnostic information, this is an undesirable situation and non-characterizing sets of diagnoses should represent most of the diagnostic probability mass to minimize the likelihood of such cases. We have constructed our experimental benchmark of initial observations in such a way as to avoid such cases."
    }, {
      "heading" : "6.2 Expected Number of MC Diagnoses",
      "text" : "We have observed that the error of Alg. 1 is insensitive to the number or the composition of the input variables. It can be seen that the value of the expected number of diagnoses Ê approaches the exact value E when increasing the number of samples n. In particular, Ê is equal to the exact value of the expected number of MC diagnoses E, when all possible input values are considered. Figure 6 shows examples of Ê approaching E for three of our benchmark models.\nTerminate approximates the intermediate value of Ê by computing the sequence E = 〈Ê1, Ê2, . . ., Ên〉. The standard error of the mean of E is defined as:\nSEME = s√ n , (9)\nwhere s is the standard deviation of E. We have set Terminate to terminate Alg. 1 when n > 15 and SEME < θ, where θ is a circuit-dependent threshold constant. Table 5 shows θ for the various circuits we have experimented on.\nWe have determined θ using the following procedure. First, for each circuit, we choose an arbitrary initial observation and a small set IN of input variables (|IN| = 8). The small cardinality of IN allows us to compute true values of E. Next, for each circuit we run 10 pseudo-random experiments. For θ we choose the smallest value of SEME such that its corresponding Ê is within 95% of E. Table 5 shows the average and maximum number of steps in which Alg. 1 reaches this value. In all cases an upper bound of n = 100 is a safe termination criterion."
    }, {
      "heading" : "6.3 Comparison of Algorithms",
      "text" : "Consider a weak-fault model of a chain of n inverters and a set of MC diagnoses D (initially, |D| = n). At each step single-variable probing can eliminate at most 0.5|D| diagnoses. It can also be shown that halving the expected number of remaining MC diagnoses is a theoretical bound of any one-step lookahead strategy. As a result we use the geometric decay curve\nN(k) = N0p k +N∞ (10)\nas a model of the diagnosis decay. In this case, N0 is the initial number of diagnoses, N∞ is the value to which |Ω(S)| converges, and p is the decay rate constant. For probing, N∞ = 1. In all our experiments we will fit both the expected number of remaining MC diagnoses E and the actual number or remaining MC diagnoses Ω(S) to Eqn. 10.\n6.3.1 FractalATPG\nFigure 7 shows the reduction of the expected number of MC diagnoses as a function of (1) the number of control variables |CTL| and (2) the time k. One can easily see that a global optimum is reached quickly on both independent axes. This decay is shown for both c432 (Fig. 7, left) and the reduced c880 (Fig. 7, right). The number of control variables |CTL| varies from 0 to 36 for c432 (|IN| = 36) and from 0 to 60 for c880 (|IN| = 60).\nUsing |Ω(S)| instead of E results in similar plots (there is high correlation between E and |Ω(S)|), hence we have omitted the |Ω(S)| plots. The minimum, maximum and mean Pearson’s linear correlation coefficient between E from Fig. 7 and the respective |Ω(S)| for each number of control variables in c432 is ρmin = 0.713, ρmax = 0.999, and ρavg = 0.951,\nrespectively. The corresponding correlation coefficients for the reduced c880 are ρmin = 0.834, ρmax = 1, and ρavg = 0.972.\nIt can be seen that the expected number of remaining diagnoses E quickly reaches a global optimum when increasing |CTL|, which means that turning even a small number of input variables into controls allows for a geometric decay of the diagnostic entropy. The results for the reduced c880 are similar to the non-reduced c432. Hence, identification of cones helps the performance of the diagnostic oracle, but does not change the convergence behavior or the effect of the control variables.\nFitting geometric decay curves (Eqn. 10) on the |CTL| axes of Fig. 7 produces better fits for c880 than for c432. Similarly, the values of N∞ for fits alongside the k-axis are larger for c432 than for c880. The reason for that is the small number of outputs in c432 (cf. Table 3). In circuits with few outputs, randomly turning a limited number of inputs into controls may not lead to a fast decay or a small N∞, as the control-output connectivity of a model is essential for decreasing the diagnostic uncertainty.\nTable 6 and Table 7 summarize a total of 14 000 FractalATPG experiments over the whole 74XXX/ISCAS85 benchmark. Table 6 shows the correlation between the expected number of remaining MC diagnoses and the actual number of remaining MC diagnoses. In the second and third columns of Table 6 we can see the minimum and average correlations between E and Ω≤(S). The third and fourth cases specify the fraction of observations for which we have ρ > 0.95 and ρ > 0.975, respectively. Columns 6 – 9 repeat this data for the reduced 74XXX/ISCAS85 circuits.\nTable 7 summarizes the parameters of the geometric decay curves fitted to Ω(S). We can see that although Ω(S) is well approximated by a geometric decay curve (the average goodness-\nof-fit criterion R2 is 0.84) the average decay constant p is low (0.13 for the non-reduced and 0.22 for the reduced 74XXX/ISCAS85 circuits).\nThe decay rate p depends mostly on the circuit topology, hence the large variance in Table 7. Consider, for example, an artificial topology, where there are n components, and n output variables that produce the health-state of each component for a specific control assignment (e.g., a self-test). In this topology p would be very small as a diagnostician needs at most one test (control assignment) to decrease the number of MC diagnoses to one.\nThe performance of FractalATPG is determined by the size of the model and the diagnostic oracle. In the above experiments the overall time for executing a single scenario varied from 3.4 s for 74182 to 1 015 s for c6288. The satisfiability problems in the ATPG part were always easy and the DPLL solver spent milliseconds in computing control assignments.\nThe decay rate of FractalATPG depends on the number and composition of controls. In what follows we will see that FractalG can achieve a similar decay rate with a smaller number of control variables.\n6.3.2 FractalG\nFigure 8 shows the decay in the expected number of remaining MC diagnoses for FractalG. While the reduction is similar for c432, we can see a steeper reduction in the number of remaining MC diagnoses on both independent axes. Hence, the greedy algorithm is better than FractalATPG in identifying control combinations of small size, thereby leading to a better decay rate.\nTable 8 and Table 9 summarize the whole 74XXX/ISCAS85 benchmark. Table 8 shows that FractalG, similar to FractalATPG, results in high average correlation between Ω(S) and Ê (ρavg > 0.79 for all circuits).\nThe decay rates of FractalATPG and FractalG are similar (cf. Table 7 and Table 9), but, as is visible from Fig. 8, FractalG reduces the number of remaining MC diagnoses more quickly, with fewer control variables. The c432 combinational circuit is difficult for active testing because it has a small number of outputs compared to the number of inputs (cf. Table 3), hence reducing the diagnostic utility.\nTo summarize the effect of the number of controls on the diagnostic convergence, we again fit the geometric decay curve (Eqn. 10) to Ω(S) for each of the 100 initial observation\nvectors and various |CTL|. In this case, N0 is the initial number of diagnoses, N∞ is the value to which |Ω(S)| converges, and p is the decay constant (the most important parameter of our fits). For an “easy” circuit with chain topology, for p = 12 , N0 halves every k steps, as in binary search, hence p corresponds to one bit. For p = 14 , p corresponds to two bits.\nTable 10 shows the average p over all initial observations and for various numbers of control bits b = lg |CTL|. Table 10 does not include data for the 74XXX circuits as they do not have enough inputs (we need circuits with at least 32 inputs). From Table 10 it is visible\nthat an exponential increase in the number of control variables does not lead to a significant decrease in p. Hence, for ISCAS85, even turning a small number of the input variables into controls leads to a near-optimal decrease in the number of remaining MC diagnoses.\nThe performance of FractalG was worse than that of FractalATPG due to the multivariable expectation. The running time varied between 7.1 s for 74182 and 2 382 s for c6288. Most of the CPU time was spent in the Expectation subroutine (cf. Alg. 1). Each consistency check was computationally easy, but for each circuit there were thousands of them. Hence, improving the performance of LTMS would lead to an increase of the performance of FractalG.\n6.3.3 FractalP\nWe next discuss FractalP. As mentioned earlier, probing is different from active testing as it assumes full observability of the model, i.e., all internal variables can be measured (cf. Sec. 5). Furthermore, probing considers one internal variable per step, while active testing assigns value to all control variables.5\nThe value of the decay rate p depends on (1) the topology of the circuit, (2) the initial observation and (3) the values of the subsequent probes. For probing in ISCAS85 we see that the values of the decay rate p are close to 0.5 for both Ω(S) and E. Figure 9 shows the actual and expected number of remaining MC diagnoses (Ω≤(S) and E, respectively) and a geometric fit to E for three probing scenarios.\nEach plot in Fig. 9 shows a single probing session with a single initial observation. Figure 10 shows the goodness-of-fit criterion R2 vs. the decay rate constant p for all 100 observations and each of the 10 multiple runs of the Fig. 9 circuits.\nIt is visible from Fig. 10 that the absolute values of R2 are (in most of the cases) close to 1. This is an indicator that the probing experiments fit the geometric decay model given in Eqn. 10 well. Figure 10 shows a “bad” topology (c432 on the left), and a “good” topology (c5315 on the right) that achieves decay rate p close to 0.5 (0.38 < p < 0.58) with very high accuracy of the fit (0.9896 ≤ R2 ≤ 1). The expected number of remaining MC diagnoses is a good predictor of the actual number of MC diagnoses for all ISCAS85 circuits, as is shown in Table 11. The absolute values, again\n5. There exist multi-probe generalizations of probing (de Kleer et al., 1992).\ndepend on the topology, and we can see a smaller correlation ρ for some c432 observations. In most of the cases, however, the correlation is significant, e.g., for all circuits and observations except c432 we have ρ > 0.95.\nIn the second and third columns of Table 11 we can see the minimum and average correlations between E and Ω≤(S). The third and fourth cases specify the fraction of observations for which we have ρ > 0.95 and ρ > 0.975, respectively. Columns 6 – 9 repeat this data for the reduced 74XXX/ISCAS85 circuits.\nTable 12 summarizes the decay rate p and the goodness-of-fit criterion R2 for all observations and circuits. For c432, the values of p and R2 are more dispersed, while in the other experiments p strongly resembles that of “chained-elements” (i.e., p is close to 0.5). The minimum, maximum and average values of p (per circuit) are given in columns pmin, pmax, and pavg, respectively."
    }, {
      "heading" : "6.4 Experimental Summary",
      "text" : "If we compare Table 6 and Table 11 we can see that the average correlation ρavg decreases significantly. Hence, assuming limited observability (i.e., assuming that not all internals are measurable) decreases the quality of E as a predictor of Ω(S). The increased statistical dispersion of ρ is visible from the increased range ρmax − ρmin (cf. Table 6, where ρmax is always 1). For example, if we consider c2670, the standard deviation of all E vs. Ω(S) correlation coefficients ρ is σρ = 0.0031 for Fractal P and σρ = 0.0783 for Fractal ATPG. The difference in dispersion of correlation coefficients is significant for all circuits, with smallest values for c432, where it is 0.0038 for FractalP and 0.0825 for FractalATPG.\nBy comparing Table 7, Table 9, and Table 12 we can see that the mean decay rates of FractalATPG, FractalG, and FractalP are similar (the average p of FractalG is 0.7 while the average p of FractalATPG is 0.73). The average goodness-of-fit criterion R2 for exponential decays is always good (0.88 for FractalG, 0.84 for FractalATPG), and almost perfect in probing (0.97).\nThe summary of our experiments is best shown in Fig. 11. To factor out sampling error and to be able to perform exhaustive computations, we have chosen the smallest 74182 circuit. The original 74182 (a 4-bit carry-lookahead generator) has 19 components, 9 inputs, and 5 outputs. We have turned four of the inputs into controls (hence, |IN| = 4 and |CTL| = 4).\nWe have considered a random control policy in addition to FractalP, FractalATPG, and FractalG. With a random control policy, at each step, a random value is assigned to each control variable. We have also shown an exhaustive control search where the expected\nnumber of remaining MC diagnoses is computed at each step, and for each possible control combination. This works with 74182 but leads to a combinatorial blow-up with any other (larger) circuit.\nTo reduce the stochastic error when plotting Fig. 11, we have replaced the sampling (for computing an expected number of remaining MC diagnoses) with an exhaustive method; this is possible as |IN| = 5. The only randomized decision is to choose the actual fault from the initial ambiguity group. To reduce the error due to this stochastic fault injection, we have tested each of the 5 control policies 100 times.\nWe can see in Fig. 11 that the least informed control policy (the random control policy simply does not use E) shows the worst decay in the number of remaining diagnoses. On the other extreme, the exhaustive control policy achieves the best decay. The price for this policy in terms of computational effort, however, is prohibitive. FractalG achieves decay rates comparable to the exhaustive policy with affordable average-case complexity. FractalATPG has better complexity than FractalG, but the whole decay rate curve of FractalATPG is bounded from below by the one computed by FractalG.\nProbing does not compare to active testing as both approaches have different assumptions on the observability of the model. Figure 11 shows the decay rate of probing to illustrate the different decay curves depending on the observability assumptions. In this experiment the probing decay rate geometric fit with p = 12 almost perfectly fits the actual number of remaining MC diagnoses."
    }, {
      "heading" : "7. Conclusions",
      "text" : "We have devised an algorithm, FractalG, for active testing that is (1) computationally efficient and (2) rapidly reduces the diagnostic uncertainty (measured as the number of remaining MC diagnoses) by manipulating a set of control variables. As fully optimizing (2) leads to a combinatorial blow-up, FractalG achieves a compromise between (1) and (2) by using a greedy approximation approach for searching over the space of control assignments and a stochastic sampling method for computing the number of remaining MC diagnoses. The result is a fast algorithm (optimizing a whole Fractal scenario takes between 1 s\nfor 74182 and 40 min for c6288) that decreases the diagnostic uncertainty according to a geometric decay curve. This geometric decay curve fits the Fractal data well (the goodness-of-fit criterion R2 is 0.88 on average) and provides steep decay (the average decay rate p is 0.7).\nWe have applied FractalG to the real-world problem of reducing the diagnostic uncertainty of a heavy-duty printer (Feldman, 2010). For that purpose, we have modeled the Paper Input Module (PIM). In the PIM case-study, FractalG computed the most informative tests in troubleshooting multiple sensor and component failures. This happens even with a coarse-grained device model (only a few constraints per component), which shows an unexpected benefit of Fractal: trade-off of modeling complexity vs. test effort.\nThe optimality of FractalG depends on the topology of and constraints on the input model. We can create models leading to arbitrarily bad optimality of FractalG by, for example, directly encoding truth tables in SD. In practical situations, however, controls are independent. That means that applying a single control rarely “undoes” the effect of the previous ones. This also happens when arbitrary inputs are converted to controls, as in our experimentation benchmark. Consider, for example, a multiplier (c6288). Leaving out some of the inputs leads to “don’t cares” in the output and hence some components (full-adders, and-gates) will remain untested. Subsequently assigning values to these left-out inputs will unambiguously exonerate or blame these untested components, which will help narrowing down the set of diagnostic hypotheses.\nThe most important benefit in applying Fractal to industrial cases is that active testing “trade-offs” modeling fidelity for computational complexity and extra testing. This enables users to achieve good diagnostic certainty without the large cost traditionally associated with developing high fidelity models based on physics of failure and other precision approaches.\nWe have compared the optimality and performance of FractalG to an ATPG-based algorithm for sequential diagnosis, FractalATPG. While the average decay rate of both algorithms is similar (average p of FractalATPG is 0.73), the average goodness-of-fit criterion R2 of FractalATPG is lower (0.84), which means that FractalG is consistently closer to the optimal solution than is FractalATPG. FractalG has achieved better exponential decay compared to all algorithms except exhaustive control search. For example, the difference in the decay rate p between FractalG and exhaustive search for 74182 is 5.4%. The exhaustive control approach, however, takes minutes to complete even for a circuit as simple as 74182, and times-out with any model having more than 20 controls. As a result, we can conclude that FractalG trades off a small decrease in p for a significant performance speedup."
    } ],
    "references" : [ {
      "title" : "Distinguishing tests for nondeterministic and probabilistic machines",
      "author" : [ "R. Alur", "C. Courcoubetis", "M. Yannakakis" ],
      "venue" : "In Proc. ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Alur et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Alur et al\\.",
      "year" : 1995
    }, {
      "title" : "A neutral netlist of 10 combinational benchmark circuits and a target translator in Fortran",
      "author" : [ "F. Brglez", "H. Fujiwara" ],
      "venue" : "In Proc. ISCAS’85,",
      "citeRegEx" : "Brglez and Fujiwara,? \\Q1985\\E",
      "shortCiteRegEx" : "Brglez and Fujiwara",
      "year" : 1985
    }, {
      "title" : "Active probing strategies for problem diagnosis in distributed systems",
      "author" : [ "M. Brodie", "I. Rish", "S. Ma", "N. Odintsova" ],
      "venue" : "In Proc. IJCAI’03,",
      "citeRegEx" : "Brodie et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Brodie et al\\.",
      "year" : 2003
    }, {
      "title" : "Essentials of Electronic Testing for Digital, Memory and Mixed-Signal VLSI Circuits",
      "author" : [ "M.L. Bushnell", "V.D. Agrawal" ],
      "venue" : null,
      "citeRegEx" : "Bushnell and Agrawal,? \\Q2000\\E",
      "shortCiteRegEx" : "Bushnell and Agrawal",
      "year" : 2000
    }, {
      "title" : "Problem solving with the ATMS",
      "author" : [ "J. de Kleer" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Kleer,? \\Q1986\\E",
      "shortCiteRegEx" : "Kleer",
      "year" : 1986
    }, {
      "title" : "An improved approach for generating Max-Fault Min-Cardinality diagnoses",
      "author" : [ "J. de Kleer" ],
      "venue" : "In Proc",
      "citeRegEx" : "Kleer,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleer",
      "year" : 2008
    }, {
      "title" : "One step lookahead is pretty good. In Readings in Model-Based Diagnosis, pp. 138–142",
      "author" : [ "J. de Kleer", "O. Raiman", "M. Shirley" ],
      "venue" : null,
      "citeRegEx" : "Kleer et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Kleer et al\\.",
      "year" : 1992
    }, {
      "title" : "Diagnosing multiple faults",
      "author" : [ "J. de Kleer", "B. Williams" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Kleer and Williams,? \\Q1987\\E",
      "shortCiteRegEx" : "Kleer and Williams",
      "year" : 1987
    }, {
      "title" : "Tele-diagnosis: Remote monitoring of large-scale systems",
      "author" : [ "S. Deb", "S. Ghoshal", "V.N. Malepati", "D.L. Kleinman" ],
      "venue" : "In Proc. AEROCONF’00,",
      "citeRegEx" : "Deb et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Deb et al\\.",
      "year" : 2000
    }, {
      "title" : "Fault-model-based test generation for embedded software",
      "author" : [ "M. Esser", "P. Struss" ],
      "venue" : "In Proc. IJCAI’07,",
      "citeRegEx" : "Esser and Struss,? \\Q2007\\E",
      "shortCiteRegEx" : "Esser and Struss",
      "year" : 2007
    }, {
      "title" : "Approximation Algorithms for Model-Based Diagnosis",
      "author" : [ "A. Feldman" ],
      "venue" : "Ph.D. thesis, Delft University of Technology",
      "citeRegEx" : "Feldman,? \\Q2010\\E",
      "shortCiteRegEx" : "Feldman",
      "year" : 2010
    }, {
      "title" : "Computing observation vectors for Max-Fault Min-Cardinality diagnoses",
      "author" : [ "A. Feldman", "G. Provan", "A. van Gemund" ],
      "venue" : "In Proc. AAAI’08,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2008
    }, {
      "title" : "FRACTAL: Efficient fault isolation using active testing",
      "author" : [ "A. Feldman", "G. Provan", "A. van Gemund" ],
      "venue" : "In Proc. IJCAI’09,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2009
    }, {
      "title" : "Solving strong-fault diagnostic models by model relaxation",
      "author" : [ "A. Feldman", "G. Provan", "A. van Gemund" ],
      "venue" : "In Proc. IJCAI’09,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2009
    }, {
      "title" : "Building Problem Solvers",
      "author" : [ "K. Forbus", "J. de Kleer" ],
      "venue" : null,
      "citeRegEx" : "Forbus and Kleer,? \\Q1993\\E",
      "shortCiteRegEx" : "Forbus and Kleer",
      "year" : 1993
    }, {
      "title" : "Unveiling the ISCAS-85 benchmarks: A case study in reverse engineering",
      "author" : [ "M. Hansen", "H. Yalcin", "J. Hayes" ],
      "venue" : "IEEE Design & Test,",
      "citeRegEx" : "Hansen et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hansen et al\\.",
      "year" : 1999
    }, {
      "title" : "Using model counting to find optimal distinguishing tests",
      "author" : [ "S. Heinz", "M. Sachenbacher" ],
      "venue" : "In Proc. of COUNTING’08,",
      "citeRegEx" : "Heinz and Sachenbacher,? \\Q2008\\E",
      "shortCiteRegEx" : "Heinz and Sachenbacher",
      "year" : 2008
    }, {
      "title" : "Polynomially complete fault detection problems",
      "author" : [ "O.H. Ibarra", "S.K. Sahni" ],
      "venue" : "IEEE Trans. on Computers,",
      "citeRegEx" : "Ibarra and Sahni,? \\Q1975\\E",
      "shortCiteRegEx" : "Ibarra and Sahni",
      "year" : 1975
    }, {
      "title" : "Pervasive diagnosis: Integration of active diagnosis into production plans",
      "author" : [ "L. Kuhn", "B. Price", "J. de Kleer", "M. Do", "R. Zhou" ],
      "venue" : "In Proc",
      "citeRegEx" : "Kuhn et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kuhn et al\\.",
      "year" : 2008
    }, {
      "title" : "Bottom-up construction of minimum-cost and/or trees for sequential fault diagnosis",
      "author" : [ "O.E. Kundakcioglu", "T. Ünlüyurt" ],
      "venue" : "IEEE Trans. on SMC,",
      "citeRegEx" : "Kundakcioglu and Ünlüyurt,? \\Q2007\\E",
      "shortCiteRegEx" : "Kundakcioglu and Ünlüyurt",
      "year" : 2007
    }, {
      "title" : "Application of heuristic search and information theory to sequential fault diagnosis",
      "author" : [ "K. Pattipati", "M. Alexandridis" ],
      "venue" : "IEEE Trans. on SMC,",
      "citeRegEx" : "Pattipati and Alexandridis,? \\Q1990\\E",
      "shortCiteRegEx" : "Pattipati and Alexandridis",
      "year" : 1990
    }, {
      "title" : "Temporal versus spatial observability in modelbased diagnosis",
      "author" : [ "J. Pietersma", "A. van Gemund" ],
      "venue" : "Systems, Man and Cybernetics,",
      "citeRegEx" : "Pietersma and Gemund,? \\Q2006\\E",
      "shortCiteRegEx" : "Pietersma and Gemund",
      "year" : 2006
    }, {
      "title" : "Why is ATPG easy",
      "author" : [ "M.R. Prasad", "P. Chong", "K. Keutzer" ],
      "venue" : "In Proc. DAC’99,",
      "citeRegEx" : "Prasad et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 1999
    }, {
      "title" : "Optimal and near-optimal test sequencing algorithms with realistic test models",
      "author" : [ "V. Raghavan", "M. Shakeri", "K. Pattipati" ],
      "venue" : "IEEE Trans. on SMC,",
      "citeRegEx" : "Raghavan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Raghavan et al\\.",
      "year" : 1999
    }, {
      "title" : "Accuracy vs. efficiency trade-offs in probabilistic diagnosis",
      "author" : [ "I. Rish", "M. Brodie", "S. Ma" ],
      "venue" : "In Proc. AAAI’02,",
      "citeRegEx" : "Rish et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Rish et al\\.",
      "year" : 2002
    }, {
      "title" : "Advances in System Fault Modeling and Diagnosis",
      "author" : [ "M. Shakeri" ],
      "venue" : "Ph.D. thesis, University of Connecticut",
      "citeRegEx" : "Shakeri,? \\Q1996\\E",
      "shortCiteRegEx" : "Shakeri",
      "year" : 1996
    }, {
      "title" : "Sequential testing algorithms for multiple fault diagnosis",
      "author" : [ "M. Shakeri", "V. Raghavan", "K.R. Pattipati", "A. Patterson-Hine" ],
      "venue" : "IEEE Trans. on SMC,",
      "citeRegEx" : "Shakeri et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Shakeri et al\\.",
      "year" : 2000
    }, {
      "title" : "Hierarchical diagnosis of multiple faults",
      "author" : [ "S. Siddiqi", "J. Huang" ],
      "venue" : "In Proc. IJCAI’07,",
      "citeRegEx" : "Siddiqi and Huang,? \\Q2007\\E",
      "shortCiteRegEx" : "Siddiqi and Huang",
      "year" : 2007
    }, {
      "title" : "Combinational test generation using satisfiability",
      "author" : [ "P. Stephan", "R. Brayton", "A. Sangiovanni-Vincentelli" ],
      "venue" : "IEEE Trans. on CAD of Integrated Circuits and Systems,",
      "citeRegEx" : "Stephan et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Stephan et al\\.",
      "year" : 1996
    }, {
      "title" : "Testing physical systems",
      "author" : [ "P. Struss" ],
      "venue" : "In Proc. AAAI’94,",
      "citeRegEx" : "Struss,? \\Q1994\\E",
      "shortCiteRegEx" : "Struss",
      "year" : 1994
    }, {
      "title" : "Rollout strategies for sequential fault diagnosis",
      "author" : [ "F. Tu", "K. Pattipati" ],
      "venue" : "IEEE Trans. on SMC,",
      "citeRegEx" : "Tu and Pattipati,? \\Q2003\\E",
      "shortCiteRegEx" : "Tu and Pattipati",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Sequential diagnosis algorithms (Shakeri, 1996) can be used as an alternative to the above passive approach, with better decay of the number of diagnostic hypotheses.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "Pattipati and Alexandridis (1990) have shown that under certain conditions (e.",
      "startOffset" : 0,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : ", unit test costs and equal prior fault probabilities) a onestep look-ahead policy leads to an optimal average depth of the test tree; de Kleer, Raiman, and Shirley (1992) have shown that one-step look-ahead delivers good practical results for a range of combinational circuits.",
      "startOffset" : 138,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : ", as used by de Kleer and Williams (1987), can be computed directly from the number of MC diagnoses.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Early work aimed at diagnostic convergence by de Kleer and Williams (1987) compute a probe sequence for reducing diagnostic entropy using a myopic search strategy.",
      "startOffset" : 49,
      "endOffset" : 75
    }, {
      "referenceID" : 29,
      "context" : "Model-Based Testing (MBT) (Struss, 1994) is a generalization of sequential diagnosis.",
      "startOffset" : 26,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "Our task is harder than that of Raghavan et al. (1999), since the diagnosis task is NPhard, even though the diagnosis lookup uses a fault dictionary; in our case we compute a new diagnosis after every test.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "Our task is harder than that of Raghavan et al. (1999), since the diagnosis task is NPhard, even though the diagnosis lookup uses a fault dictionary; in our case we compute a new diagnosis after every test. Hence we have an NP-hard sequential problem interleaved with the complexity of diagnostic inference at each step (in our case the complexity of diagnosis is Σp2-hard). Apart from the above-mentioned differences, we note that optimal test sequencing is infeasible for the size of problems in which we are interested. Model-Based Testing (MBT) (Struss, 1994) is a generalization of sequential diagnosis. The purpose of MBT is to compute inputs manifesting a certain (faulty) behavior. The main differences from our active testing approach are that MBT (1) assumes that all inputs are controllable and (2) MBT aims at confirming single-fault behavior as opposed to maximally decreasing the diagnostic uncertainty. Brodie, Rish, Ma, and Odintsova (2003) cast their models in terms of Bayesian networks.",
      "startOffset" : 32,
      "endOffset" : 957
    }, {
      "referenceID" : 2,
      "context" : "Our notion of entropy is the size of the diagnosis space, whereas Brodie et al. use decisiontheoretic notions of entropy to guide test selection. Brodie et al. extend their past Bayesian diagnostic approach (Rish, Brodie, & Ma, 2002) with sequential construction of probe sets (probe sets are collections of, for example, pings to a subset of the nodes in a computer network). The approach of Brodie et al. is limited to networks although it can be extended by modifying the type of Bayesian network shown by Rish et al.; such a modification, however, would necessitate more computationally expensive Bayesian reasoning for achieving good approximation results for the most probable explanations. The approach of Brodie et al. (2003) does not compute modifications in the target network topology and does not propose control actions (for example, a network server that fails to respond can be dialed-up through a modem or checked by a technician at a higher cost).",
      "startOffset" : 66,
      "endOffset" : 734
    }, {
      "referenceID" : 11,
      "context" : "We solve a different problem than that of Heinz and Sachenbacher (2008), Alur, Courcoubetis, and Yannakakis (1995).",
      "startOffset" : 42,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "We solve a different problem than that of Heinz and Sachenbacher (2008), Alur, Courcoubetis, and Yannakakis (1995). Both of these approaches assume a non-deterministic model defined as an automaton.",
      "startOffset" : 42,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "Esser and Struss (2007) also adopt an automaton framework for test generation, except that, unlike Heinz and Sachenbacher (2008) or Alur et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "Esser and Struss (2007) also adopt an automaton framework for test generation, except that, unlike Heinz and Sachenbacher (2008) or Alur et al.",
      "startOffset" : 0,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "Esser and Struss (2007) also adopt an automaton framework for test generation, except that, unlike Heinz and Sachenbacher (2008) or Alur et al. (1995), they transform this automaton to a relational specification, and apply their framework to software diagnosis.",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "Esser and Struss (2007) also adopt an automaton framework for test generation, except that, unlike Heinz and Sachenbacher (2008) or Alur et al. (1995), they transform this automaton to a relational specification, and apply their framework to software diagnosis. This automaton-based framework accommodates more general situations than does ours, such as the possibility that the system’s state after a transition may not be uniquely determined by the state before the transition and the input, and/or the system’s state may be associated with several possible observations. In our MBD framework, a test consists of an instantiation of several variables, which corresponds to the notion of test sequence within the automaton framework of Heinz and Sachenbacher. The framework of Esser and Struss requires modeling of the possible faults, whereas Fractal works both with weak and strong-fault models1. Interestingly, as shown by Esser and Struss, modeling of abnormal software behavior can be derived to some extent from software functional requirements. This makes their framework suitable for software systems. A recent approach to active diagnosis is described by Kuhn, Price, de Kleer, Do, and Zhou (2008), where additional test vectors are computed to optimize the diagnosis while the system (a copier) remains operational.",
      "startOffset" : 132,
      "endOffset" : 1208
    }, {
      "referenceID" : 0,
      "context" : "Esser and Struss (2007) also adopt an automaton framework for test generation, except that, unlike Heinz and Sachenbacher (2008) or Alur et al. (1995), they transform this automaton to a relational specification, and apply their framework to software diagnosis. This automaton-based framework accommodates more general situations than does ours, such as the possibility that the system’s state after a transition may not be uniquely determined by the state before the transition and the input, and/or the system’s state may be associated with several possible observations. In our MBD framework, a test consists of an instantiation of several variables, which corresponds to the notion of test sequence within the automaton framework of Heinz and Sachenbacher. The framework of Esser and Struss requires modeling of the possible faults, whereas Fractal works both with weak and strong-fault models1. Interestingly, as shown by Esser and Struss, modeling of abnormal software behavior can be derived to some extent from software functional requirements. This makes their framework suitable for software systems. A recent approach to active diagnosis is described by Kuhn, Price, de Kleer, Do, and Zhou (2008), where additional test vectors are computed to optimize the diagnosis while the system (a copier) remains operational. Their work differs from ours in that plans (roughly analogous to test sequences) with a probability of failure T are computed statically, and a plan remains unmodified even if it fails to achieve its desired goal (a manifestation of a failure with probability close to T ). Conversely, Fractal dynamically computes next-best control settings in a game-like manner. The biggest difference between Fractal and the approach of Kuhn et al. is in the use of models. Fractal is compatible with traditional MBD (de Kleer & Williams, 1987) and can reuse existing models from first principles while the pervasive approach of Kuhn et al. uses an automaton and a set of possible actions. The approach of Kuhn et al. (2008) uses existing MBD and planning algorithms, and as such integrates existing approaches; in contrast, Fractal introduces new control algorithms and reuses an external diagnostic oracle.",
      "startOffset" : 132,
      "endOffset" : 2039
    }, {
      "referenceID" : 0,
      "context" : "Esser and Struss (2007) also adopt an automaton framework for test generation, except that, unlike Heinz and Sachenbacher (2008) or Alur et al. (1995), they transform this automaton to a relational specification, and apply their framework to software diagnosis. This automaton-based framework accommodates more general situations than does ours, such as the possibility that the system’s state after a transition may not be uniquely determined by the state before the transition and the input, and/or the system’s state may be associated with several possible observations. In our MBD framework, a test consists of an instantiation of several variables, which corresponds to the notion of test sequence within the automaton framework of Heinz and Sachenbacher. The framework of Esser and Struss requires modeling of the possible faults, whereas Fractal works both with weak and strong-fault models1. Interestingly, as shown by Esser and Struss, modeling of abnormal software behavior can be derived to some extent from software functional requirements. This makes their framework suitable for software systems. A recent approach to active diagnosis is described by Kuhn, Price, de Kleer, Do, and Zhou (2008), where additional test vectors are computed to optimize the diagnosis while the system (a copier) remains operational. Their work differs from ours in that plans (roughly analogous to test sequences) with a probability of failure T are computed statically, and a plan remains unmodified even if it fails to achieve its desired goal (a manifestation of a failure with probability close to T ). Conversely, Fractal dynamically computes next-best control settings in a game-like manner. The biggest difference between Fractal and the approach of Kuhn et al. is in the use of models. Fractal is compatible with traditional MBD (de Kleer & Williams, 1987) and can reuse existing models from first principles while the pervasive approach of Kuhn et al. uses an automaton and a set of possible actions. The approach of Kuhn et al. (2008) uses existing MBD and planning algorithms, and as such integrates existing approaches; in contrast, Fractal introduces new control algorithms and reuses an external diagnostic oracle. An advantage of the pervasive diagnosis approach is that the use of a planning engine generates a complete sequence of actions, as opposed to the one-step lookahead of Fractal. Depending on the planning formalism, the complexity of pervasive diagnosis can be dominated by the planning module, while the most complex computational task in Fractal is that of diagnosis. Both pervasive diagnosis and this paper, however, report good average-case computational efficiency for benchmark problems. Last, the paper of Kuhn et al. is limited to single-fault diagnoses, although the pervasive diagnosis framework can be generalized to multiple faults. Feldman, Provan, and van Gemund (2009a) introduce an early version of Fractal.",
      "startOffset" : 132,
      "endOffset" : 2906
    }, {
      "referenceID" : 10,
      "context" : "Weak-fault models (also known as models with ignorance of abnormal behavior) and strong-fault models are discussed by Feldman, Provan, and van Gemund (2009b).",
      "startOffset" : 118,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "We use |Ω(S)| instead of the more precise diagnostic entropy as defined by de Kleer and Williams (1987) and subsequent works, as this allows low-complexity estimations (discussed in Sec.",
      "startOffset" : 78,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : "Problem 1 is different from the general sequential testing problem, as formulated by Shakeri (1996). In the Shakeri formulation, there are different test costs and different prior failure probabilities, where Problem 1 assumes equal costs and equal small prior probabilities of failure.",
      "startOffset" : 85,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "Pattipati and Alexandridis (1990) show that under those assumptions, minimizing the test cost at each step constitutes an optimal policy for minimizing the expected test cost.",
      "startOffset" : 0,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "We next show the probing algorithm as introduced by de Kleer and Williams (1987) and adapted for the Fractal framework.",
      "startOffset" : 55,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : "In addition to the original 74XXX/ISCAS85 models, we have performed cone reductions as described by Siddiqi and Huang (2007) and de Kleer (2008).",
      "startOffset" : 100,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "In addition to the original 74XXX/ISCAS85 models, we have performed cone reductions as described by Siddiqi and Huang (2007) and de Kleer (2008). Recall that from the perspective of the MBD diagnostic engine, faults inside a cone (where a cone is a set of components) cannot be distinguished, hence it is enough to provide a single health variable per cone.",
      "startOffset" : 132,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : "We have applied Fractal to the real-world problem of reducing the diagnostic uncertainty of a heavy-duty printer (Feldman, 2010).",
      "startOffset" : 113,
      "endOffset" : 128
    } ],
    "year" : 2010,
    "abstractText" : "Model-based diagnostic reasoning often leads to a large number of diagnostic hypotheses. The set of diagnoses can be reduced by taking into account extra observations (passive monitoring), measuring additional variables (probing) or executing additional tests (sequential diagnosis/test sequencing). In this paper we combine the above approaches with techniques from Automated Test Pattern Generation (ATPG) and Model-Based Diagnosis (MBD) into a framework called Fractal (FRamework for ACtive Testing ALgorithms). Apart from the inputs and outputs that connect a system to its environment, in active testing we consider additional input variables to which a sequence of test vectors can be supplied. We address the computationally hard problem of computing optimal control assignments (as defined in Fractal) in terms of a greedy approximation algorithm called Fractal. We compare the decrease in the number of remaining minimal cardinality diagnoses of Fractal to that of two more Fractal algorithms: Fractal and Fractal. Fractal is based on ATPG and sequential diagnosis while Fractal is based on probing and, although not an active testing algorithm, provides a baseline for comparing the lower bound on the number of reachable diagnoses for the Fractal algorithms. We empirically evaluate the trade-offs of the three Fractal algorithms by performing extensive experimentation on the ISCAS85/74XXX benchmark of combinational circuits.",
    "creator" : "dvips(k) 5.95a Copyright 2005 Radical Eye Software"
  }
}