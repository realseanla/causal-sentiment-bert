{
  "name" : "1511.05662.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Discovering Underlying Plans Based on Distributed Representations of Actions",
    "authors" : [ "Xin Tian", "Hankz Hankui Zhuo", "Subbarao Kambhampati" ],
    "emails" : [ "tianxin1860@gmail.com,", "zhuohank@mail.sysu.edu.cn,", "rao@asu.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "As computer-aided cooperative work scenarios become increasingly popular, human-in-the-loop planning and decision support has become a critical planning chellenge (c.f. (Cohen et al. 2015; Dong et al. 2004; Manikonda et al. 2014)). An important aspect of such a support (Kambhampati and Talamadupula 2015) is recognizing what plans the human in the loop is making, and provide appropriate suggestions about their next actions. Although there is a lot of work on plan recognition, much of it has traditionally depended on the availability of a complete domain model (Ramı́rez and Geffner 2009a; Zhuo, Yang, and Kambhampati 2012). As has been argued elsewhere (Kambhampati and Talamadupula 2015), such models are hard to get in human-in-the-loop planning scenarios. Here, the decision support systems have to make themselves useful without insisting on complete action models of the domain. The situation here is akin to that faced by search engines and other tools for computer supported cooperate work, and is thus a significant departure for the “planning as pure inference” mindset of the automated planning community. As such, the\nCopyright c© 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nproblem calls for plan recognition with “shallow” models of the domain (c.f. (Kambhampati 2007)), that can be easily learned automatically.\nThere has been very little work on learning such shallow models to support human-in-the-loop planning. Some examples include the work on Woogle system (Dong et al. 2004) that aimed to provide support to humans in web-service composition. That work however relied on very primitive understanding of the actions (web services in their case) that consisted merely of learning the input/output types of individual services.\nIn this paper, we focus on learning more informative models that that can help recognize the plans under construction by the humans, and provide active support by suggesting relevant actions. To drive this process, we need to learn shallow models of the domain. We propose to adapt the recent successes of word-vector models (Mikolov et al. 2013) in language to our problem. Specifically, we assume that we have access to a corpus of previous plans that the human user has made. Viewing these plans as made up of action words, we learn word vector models for these actions. These models provide us a way to induce the distribution over the identity of each unobserved action. Given the distributions over individual unobserved actions, we use an expectationmaximization approach to infer the joint distribution over all unobserved actions. This distribution then forms the basis for action suggestions.\nWe will present the details of our approach, and will also empirically demonstrate that it does capture a surprising amount of structure in the observed plan sequences, leading to effective plan recognition. We further compare its performance to traditional plan recognition techniques, including one that uses the same plan traces to learn the STRIPS-style action models, and use the learned model to support plan recognition."
    }, {
      "heading" : "Problem Definition",
      "text" : "A plan library, denoted by L, is composed of a set of plans {p}, where p is a sequence of actions, i.e., p = 〈a1, a2, . . . , an〉 where ai, 1 ≤ i ≤ n, is an action name (without any parameter) represented by a string. For example, a string unstack-A-B is an action meaning that a robot unstacks block A from block B. We denote the set of all possible actions by Ā which is assumed to be known before-\nar X\niv :1\n51 1.\n05 66\n2v 1\n[ cs\n.A I]\n1 8\nN ov\n2 01\n5\nhand. For ease of presentation, we assume that there is an empty action, Ø, indicating an unknown or not observed action, i.e., A = Ā ∪ {Ø}. An observation of an unknown plan p̃ is denoted by O = 〈o1, o2, . . . , oM 〉, where oi ∈ A, 1 ≤ i ≤ M , is either an action in Ā or an empty action Ø indicating the corresponding action is missing or not observed. Note that p̃ is not necessarily in the plan library L, which makes the plan recognition problem more challenging, since matching the observation to the plan library will not work any more.\nWe assume that the human is making a plan of at most length M . We also assume that at any given point, the planner is able to observe M − k of these actions. The k unobserved actions might either be in the suffiix (i.e., yet to be formed part) of the plan, or in the middle (due to observational gaps). Our aim is to suggest, for each of the k unobserved actions, m possible choices–from which the user can select the action. (Note that we would like to keep m small, ideally close to 1, so as not to overwhelm the user) Accordingly, we will evaluate the effectiveness of the decision support in terms of whether or not the user’s best/intended action is within the suggested m actions.\nSpecifically, our recognition problem can be represented by a triple < = (L,O,A). The solution to < is to discover the unknown plan p̃ that best explains O given L and A. An example of our plan recognition problem in the blocks1 domain is shown below.\nExample: A plan library L in the blocks domain is assumed to have four plans as shown below:\nplan 1: pick-up-B stack-B-A pick-up-D stack-D-C plan 2: unstack-B-A put-down-B unstack-D-C put-downD plan 3: pick-up-B stack-B-A pick-up-C stack-C-B pickup-D stack-D-C plan 4: unstack-D-C put-down-D unstack-C-B put-downC unstack-B-A put-down-B\nAn observation O of action sequence is shown below:\nobservation: pick-up-B Ø unstack-D-C put-down-D Ø stack-C-B Ø Ø\nGiven the above input, our DUP algorithm outputs plans as follows:\npick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C\nOur DUP Algorithm Our DUP approach to the recognition problem < functions by two phases. We first learn vector representations of actions using the plan library L. We then iteratively sample actions for unobserved actions oi by maximizing the probability of the unknown plan p̃ via the EM framework. We present DUP in detail in the following subsections.\n1http://www.cs.toronto.edu/aips2000/\nLearning Vector Representations of Actions Since actions are denoted by a name strings, actions can be viewed as words, and a plan can be viewed as a sentence. Furthermore, the plan library L can be seen as a corpus, and the set of all possible actions A is the vocabulary. We thus can learn the vector representations for actions using the Skip-gram model with hierarchical softmax, which has been shown an efficient method for learning high-quality vector representations of words from unstructured corpora (Mikolov et al. 2013).\nThe objective of the Skip-gram model is to learn vector representations for predicting the surrounding words in a sentence or document. Given a corpus C, composed of a sequence of training words 〈w1, w2, . . . , wT 〉, where T = |C|, the Skip-gram model maximizes the average log probability\n1\nT T∑ t=1 ∑ −c≤j≤c,j 6=0 log p(wt+j |wt) (1)\nwhere c is the size of the training window or context. The basic probability p(wt+j |wt) is defined by the hierarchical softmax, which uses a binary tree representation of the output layer with the K words as its leaves and for each node, explicitly represents the relative probabilities of its child nodes (Mikolov et al. 2013). For each leaf node, there is an unique path from the root to the node, and this path is used to estimate the probability of the word represented by the leaf node. There are no explicit output vector representations for words. Instead, each inner node has an output vector v′n(w,j), and the probability of a word being the output word is defined by\np(wt+j |wt) = L(wt+j)−1∏\ni=1\n{ σ(I(n(wt+j , i+ 1) =\nchild(n(wt+j , i))) · vn(wt+j ,i) · vwt) } , (2)\nwhere σ(x) = 1/(1 + exp(−x)).\nL(w) is the length from the root to the word w in the binary tree, e.g., L(w) = 4 if there are four nodes from the root to w. n(w, i) is the ith node from the root to w, e.g., n(w, 1) = root and n(w,L(w)) = w. child(n) is a fixed child (e.g., left child) of node n. vn is the vector representation of the inner node n. vwt is the input vector representation of word wt. The identity function I(x) is 1 if x is true; otherwise it is -1.\nWe can thus build vector representations of actions by maximizing Equation (1) with corpora or plan libraries L as input. We will exploit the vector representations to discover the unknown plan p̃ in the next subsection."
    }, {
      "heading" : "Maximizing Probability of Unknown Plan p̃",
      "text" : "With the vector representations learnt in the last subsection, a straightforward way to discover the unknown plan p̃ is to explore all possible actions in Ā such that p̃ has the highest\nprobability, which can be defined similar to Equation (1), i.e.,\nF(p̃) = M∑ k=1 ∑ −c≤j≤c,j 6=0 log p(wk+j |wk) (3)\nwhere wk denotes the kth action of p̃ and M is the length of p̃. As we can see, this approach is exponentially hard with respect to the size of Ā and number of unobserved actions. We thus design an approximate approach in the ExpectationMaximization framework to estimate an unknown plan p̃ that best explains the observation O.\nTo do this, we introduce new parameters to capture “weights” of values for each unobserved action. Specifically speaking, assuming there are X unobserved actions in O, i.e., the number of Øs inO isX , we denote these unobserved actions by ā1, ..., āx, ..., āX , where the indices indicate the order they appear in O. Note that each āx can be any action in Ā. We associate each possible value of āx with a weight, denoted by Γ̄āx,x. Γ̄ is a |Ā| ×X matrix, satisfying∑\no∈Ā\nΓ̄o,x = 1 ∧ Γ̄o,x ≥ 0,\nfor each x. For the ease of specification, we extend Γ̄ to a bigger matrix with a size of |Ā|×M , denoted by Γ, such that Γo,y = Γ̄o,x if y is the index of the xth unobserved action in O, for all o ∈ Ā; otherwise, Γo,y = 1 and Γo′,y = 0 for all o′ ∈ Ā ∧ o′ 6= o. Our intuition is to estimate the unknown plan p̃ by selecting actions with the highest weights. We thus introduce the weights to Equation (2), as shown below,\np(wk+j |wk) = L(wk+j)−1∏\ni=1\n{ σ(I(n(wk+j , i+ 1) =\nchild(n(wk+j , i))) · avn(wk+j ,i) · bvwk) } , (4)\nwhere a = Γwk+j ,k+j and b = Γwk,k. We can see that the impact of wk+j and wk is penalized by weights a and b if they are unobserved actions, and stays unchanged, otherwise (since both a and b equal to 1 if they are observed actions). We redefine the objective function as shown below,\nF(p̃,Γ) = M∑ k=1 ∑ −c≤j≤c,j 6=0 log p(wk+j |wk), (5)\nwhere p(wk+j |wk) is defined by Equation (4). The only parameters needed to be updated are Γ, which can be easily done by gradient descent, as shown below,\nΓo,x = Γo,x + δ ∂F ∂Γo,x , (6)\nif x is the index of unobserved action in O; otherwise, Γo,x stays unchanged, i.e., Γo,x = 1. Note that δ is a learning constant.\nWith Equation (6), we can design an EM algorithm by repeatedly sampling an unknown plan according to Γ and updating Γ based on Equation (6) until reaching convergence (e.g., a constant number of repetitions is reached).\nAn overview of our DUP algorithm is shown in Algorithm 1. In Step 2 of Algorithm 1, we initialize Γo,k = 1/M for all o ∈ Ā, if k is an index of unobserved actions in O; and otherwise, Γo,k = 1 and Γo′,k = 0 for all o′ ∈ Ā ∧ o′ 6= o. In Step 4, we view Γ·,k as a probability distribution, and sample an action from Ā based on Γ·,k if k is an unobserved action index in O. In Step 5, we only update Γ·,k where k is an unobserved action index. In Step 6, we linearly project all elements of the updated Γ to between 0 and 1, such that we can do sampling directly based on Γ in Step 4. In Step 8, we simply select āx based on\nāx = arg max o∈Ā Γo,x,\nfor all unobserved action index x.\nAlgorithm 1 Framework of our DUP algorithm Input: plan library L, observed actions O Output: plan p̃\n1: learn vector representation of actions 2: initialize Γo,k with 1/M for all o ∈ Ā, when k is an\nunobserved action index 3: while the maximal number of repetitions is not reached\ndo 4: sample unobserved actions in O based on Γ 5: update Γ based on Equation (6) 6: project Γ to [0,1] 7: end while 8: select actions for unobserved actions with the largest\nweights in Γ 9: return p̃"
    }, {
      "heading" : "Experiments",
      "text" : "In this section, we evaluate our DUP algorithms in three planning domains from International Planning Competition, i.e., blocks1, depots2, and driverlog2. To generate training and testing data, we randomly created 5000 planning problems for each domain, and solved these planning problems with a planning solver, such as FF3, to produce 5000 plans. We then randomly divided the plans into ten folds, with 500 plans in each fold. We ran our DUP algorithm ten times to calculate an average of accuracies, each time with one fold for testing and the rest for training. In the testing data, we randomly removed actions from each testing plan (i.e., O) with a specific percentage ξ of the plan length. Features of datasets are shown in Table 1, where the second column is the number of plans generated, the third column is the total number of words (or actions) of all plans, and the last column is the size of vocabulary used in all plans.\nWe define the accuracy of our DUP algorithm as follows. For each unobserved action āx DUP suggests a set of possible actions Sx which have the highest value of Γāx,x for all āx ∈ Ā. If Sx covers the truth action atruth, i.e.,\n2http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume20/long03ahtml/JAIRIPC.html\n3https://fai.cs.uni-saarland.de/hoffmann/ff.html\natruth ∈ Sx, we increase the number of correct suggestions g by 1. We thus define the accuracy acc as shown below:\nacc = 1\nT T∑ i=1 #〈correct-suggestions〉i Ki ,\nwhere T is the size of testing set, #〈correct-suggestions〉i is the number of correct suggestions for the ith testing plan, Ki is the number of unobserved actions in the ith testing plan. We can see that the accuracy acc may be influenced by Sx. We will test different size of Sx in the experiment.\nState-of-the-art plan recognition approaches with plan libraries as input aim at finding a plan from plan libraries to best explain the observed actions (Geib and Steedman 2007), which we denote by MatchPlan. We develop a MatchPlan system based on the idea of (Geib and Steedman 2007) and compare our DUP algorithm to MatchPlan with respect to different percentage of unobserved actions ξ and different size of suggestion set Sx. Another baseline is action-models based plan recognition approach (Ramirez and Geffner 2009b) (denoted by PRP, short for Plan Recognition as Planning). Since we do not have action models as input in our DUP algorithm, we exploited the action model learning system ARMS (Yang, Wu, and Jiang 2007) to learn action models from the plan library and feed the action models to the PRP approach. We call this hybrid plan recognition approach ARMS+PRP. To learn action models, ARMS requires state information of plans as input. We thus added\nextra information, i.e., initial state and goal of each plan in the plan library, to ARMS+PRP. In addition, PRP requires as input a set of candidate goals G for each plan to be recognized in the testing set, which was also generated and fed to PRP when testing. In summary, the hybrid plan recognition approach ARMS+PRP has more input information, i.e., initial states and goals in plan library and candidate goals G for each testing example, than our DUP approach."
    }, {
      "heading" : "Accuracy w.r.t. Percentage of Unobserved Actions",
      "text" : "We first evaluate our DUP algorithm with respect to different percentage of unobserved actions ξ in O. We set the window of training context c in Equation (1) to be three and the size of recommendations to be ten. We compare our DUP algorithm to both MatchPlan and ARMS+PRP. To make fair comparison (to MatchPlan), we set the matching window MatchPlan to be three as well when searching plans from plan libraries L. In other words, to estimate an unobserved action āx in O, MatchPlan matches previous three actions and subsequent three actions of āx to plans in L, and recommends ten actions with maximal number of matched actions, considering unobserved actions (Ø in the context of āx) and actions in L as a successful matching. For ARMS+PRP, we generated 20 candidate goals for each testing example including the ground-truth goal which corresponds to the ground-truth plan to be recognized. The results are shown in Figure 1.\nFrom Figure 1, we can see that in all three domains, the accuracy of our DUP algorithm is generally higher than MatchPlan and ARMS+PRP, which verifies that our DUP algorithm can indeed capture relations among actions better than previous matching approaches. The rationale is that we explore global plan information from the plan library to learn a “shallow” model (distributed representations of actions) and use this model with global information to best\nexplain the observed actions. In contrast, MatchPlan just utilizes local plan information when matching the observed actions to the plan library which results in lower accuracies. Although ARMS+PRP tries to leverage global plan information from the plan library to learn action models and uses the models to recognize observed actions, it enforces itself to extract “exact” models represented by planning models which are often with noise. When feeding those noisy models to PRP, since PRP that uses planning techniques to recognize plans is very sensitive to noise of planning models, the recognition accuracy is lower than DUP, even though ARMS+PRP has more input information (i.e., initial states and candidate goals) than our DUP algorithm.\nLooking at the changes of accuracies with respect to the percentage of unobserved actions, we can see that our DUP algorithm performs fairly well even when the percentage of unobserved action reaches 25%. In contrast, ARMS+PRP is sensitive to the percentage of unobserved actions, i.e., the accuracy goes down when more actions are unobserved. This is because the noise of planning models induces more uncertain information, which harms the recognition accuracy, when the percentage of unobserved actions becomes larger. Comparing accuracies of different domains, we can see that our DUP algorithm functions better in the blocks domain than the other two domains. This is because the ratio of #word over #vocabulary in the blocks domain is much larger than the other two domains, as shown in Table 1. We would conjecture that increasing the ratio could improve the accuracy of DUP."
    }, {
      "heading" : "Accuracy w.r.t. Size of Recommendation Set",
      "text" : "We next evaluate the performance of our DUP algorithm with respect to the size of recommendation set Sx. Likewise, we set the context window c used in Equation (1) to be three, which was also set when matching the observed actionsO to plan librariesL in the MatchPlan approach. For\nARMS+PRP, the number of candidate goals for each testing example is set to 20. ARMS+PRP aims to recognize plans that are optimal with respect to the cost of actions. We relax ARMS+PRP to output |Sx| optimal plans, some of which might be suboptimal. We varied the number of actions recommended by DUP (or MatchPlan) from 1 to 10. The results are shown in Figure 2.\nFrom Figure 2, we find that accuracies of the three approaches generally become larger when the size of the recommended action set increases in all three domains. This is consistent with our intuition, since the larger the recommended action set is, the higher the possibility for the truth action to be in the recommended action set. We can also see that the accuracy of our DUP algorithm are generally larger than both MatchPlan and ARMS+PRP in all three domains, which verifies that our DUP algorithm can indeed better capture relations among actions and thus recognize unobserved actions better than the matching approach MatchPlan and the planning model learning approach ARMS+PRP. The reason is similar to the one given for Figure 1 in the previous section. That is, the “shadow” model learnt by our DUP algorithm is better for recognizing plans than both the “exact” planning model learnt by ARMS for recognizing plans with planning techniques and the local matching approach MatchPlan. On the other hand, we can also see the accuracy of ARMS+PRP is generally higher than MatchPlan. This verifies that the additional information of initial states and candidate goals exploited by ARMS+PRP can indeed help improve the accuracy. Furthermore, the advantage of DUP becomes even larger when the size of recommended action set increases, which suggests our vector representation based learning approach can better capture action relations when the size of recommended action set is larger. The possibility of actions correctly recognized by DUP becomes much larger than the other two approaches when the size of recommendations increases."
    }, {
      "heading" : "Related work",
      "text" : "Kautz and Allen proposed an approach to recognizing plans based on parsing observed actions as sequences of subactions and essentially model this knowledge as a context-free rule in an “action grammar” (Kautz and Allen 1986). All actions, plans are uniformly referred to as goals, and a recognizer’s knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events. Lesh and Etzioni further presented methods in scaling up activity recognition to scale up his work computationally (Lesh and Etzioni 1995). They automatically constructed plan-library from domain primitives, which was different from (Kautz and Allen 1986) where the plan library was explicitly represented. In these approaches, the problem of combinatorial explosion of plan execution models impedes its application to real-world domains. Kabanza and Filion (Kabanza et al. 2013) proposed an anytime plan recognition algorithm to reduce the number of generated plan execution models based on weighted model counting. These approaches are, however, difficult to represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed.\nInstead of using a library of plans, Ramirez and Geffner (Ramirez and Geffner 2009b) proposed an approach to solving the plan recognition problem using slightly modified planning algorithms, assuming the action models were given as input. Except previous work (Kautz and Allen 1986; Bui 2003; Geib and Goldman 2009; Ramirez and Geffner 2009b) on the plan recognition problem presented in the introduction section, Note that action models can be created by experts or learnt by previous systems, such as ARMS (Yang, Wu, and Jiang 2007) and LAMP (Zhuo et al. 2010). Saria and Mahadevan presented a hierarchical multi-agent markov processes as a framework for hierarchical probabilistic plan recognition in cooperative multi-agent systems (Saria and Mahadevan 2004). Singla and Mooney proposed an approach to abductive reasoning using a first-order probabilistic logic to recognize plans (Singla and Mooney 2011). Amir and Gal addressed a plan recognition approach to recognizing student behaviors using virtual science laboratories (Amir and Gal 2011). Ramirez and Geffner exploited offthe-shelf classical planners to recognize probabilistic plans (Ramirez and Geffner 2010).\nEarly work on human-in-the-loop planning scenarios in automated planning went under the name of “mixedinitiative planning” (e.g. (Ferguson, Allen, and Miller 1996)). An important limitation of that work was that the humans in the loop were helping the automated planner (with a complete action model) navigate its search space of plans more efficiently. In contrast, we are interested in planning technology that helping humans develop plans, even in the absence of complete formal models of the planning domain. While some work in web-service composition (c.f. (Dong et al. 2004)) did focus on this type of planning support, they were hobbled by being limited to simple input/output\ntype comparison. In contrast, we believe that DUP learns and uses a model that captures more of the structure of the planning domain (while still not insisting on complete action models).\nWhile DUP focuses on learning models from plan corpora, some recent work looked at using crowdsourcing to acquire domain models. For example, Lasecki et al. (Lasecki et al. 2013) introduce Legion:AR, which combines the benefits of automatic and human activity labeling for robust and deployable activity recognition. The system exploits an active learning approach (Zhao, Sukthankar, and Sukthankar 2011) in which automatic activity recognition is augmented with on-demand activity labels from the crowd when an observed activity cannot be confidently classified. By engaging a group of people, Legion:AR is able to label activities as they occur more reliably than a single person can, especially in complex domains with multiple actors performing activities quickly. Lasecki et al. (Lasecki et al. 2014) built a crowdsourcing based system called ARchitect, using the crowd to capture the dependency structure of the actions that make up activities. Such crowd-sourcing methods can complement the plan-corpus based approach proposed in DUP."
    }, {
      "heading" : "Conclusion and Discussion",
      "text" : "In this paper we present a novel plan recognition approach DUP based on vector representation of actions. We first learn the vector representations of actions from plan libraries using the Skip-gram model which has been demonstrated to be effective. We then discover unobserved actions with the vector representations by repeatedly sampling actions and optimizing the probability of potential plans to be recognized. We also empirically exhibit the effectiveness of our approach.\nWhile we focused on a one-shot recognition task in this paper, in practice, human-in-the-loop planning will consist of multiple iterations, with DUP recognizing the plan and suggesting action addition alternatives; the human making a selection and revising the plan. The aim is to provide a form of flexible plan completion tool, akin to auto-completers for search engine queries. To do this efficiently, we need to make the DUP recognition algorithm “incremental.”\nThe word-vector based domain model we developed in this paper provides interesting contrasts to the standard precondition and effect based action models used in automated planning community. One of our future aims is to provide a more systematic comparison of the tradeoffs offered by these models. Although we have focused on the “plan recognition” aspects of this model until now, and assumed that “planning support” will be limited to suggesting potential actions to the humans. In future, we will also consider “critiquing” the plans being generated by the humans (e.g. detecting that an action introduced by the human is not consistent with the model learned by DUP), and “explaining/justifying” the suggestions generated by humans. Here, we cannot expect causal explanations of the sorts that can be generated with the help of complete action models (e.g. (Petrie 1992)), and will have to develop justifications analogous to those used in recommendation systems."
    } ],
    "references" : [ {
      "title" : "Y",
      "author" : [ "O. Amir", "Gal" ],
      "venue" : "K.",
      "citeRegEx" : "Amir and Gal 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "H",
      "author" : [ "Bui" ],
      "venue" : "H.",
      "citeRegEx" : "Bui 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "R",
      "author" : [ "P.R. Cohen", "E.C. Kaiser", "M.C. Buchanan", "S. Lind", "M.J. Corrigan", "Wesson" ],
      "venue" : "M.",
      "citeRegEx" : "Cohen et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A",
      "author" : [ "Dong, X.", "Halevy" ],
      "venue" : "Y.; Madhavan, J.; Nemes, E.; and Zhang, J.",
      "citeRegEx" : "Dong et al. 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Trains-95: Towards a mixed-initiative planning assistant",
      "author" : [ "Allen Ferguson", "G. Miller 1996] Ferguson", "J. Allen", "B. Miller" ],
      "venue" : "In Proceedings of the Third Conference on Artificial Intelligence Planning Systems",
      "citeRegEx" : "Ferguson et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Ferguson et al\\.",
      "year" : 1996
    }, {
      "title" : "2009",
      "author" : [ "C.W. Geib", "Goldman", "R. P" ],
      "venue" : "A probabilistic plan recognition algorithm based on plan tree grammars. Artificial Intelligence 173(11):1101–",
      "citeRegEx" : "Geib and Goldman 2009",
      "shortCiteRegEx" : null,
      "year" : 1132
    }, {
      "title" : "and Steedman",
      "author" : [ "C.W. Geib" ],
      "venue" : "M.",
      "citeRegEx" : "Geib and Steedman 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A",
      "author" : [ "F. Kabanza", "J. Filion", "Benaskeur" ],
      "venue" : "R.; and Irandoust, H.",
      "citeRegEx" : "Kabanza et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Talamadupula",
      "author" : [ "S. Kambhampati" ],
      "venue" : "K.",
      "citeRegEx" : "Kambhampati and Talamadupula 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain models",
      "author" : [ "S. Kambhampati" ],
      "venue" : "In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July",
      "citeRegEx" : "Kambhampati,? \\Q2007\\E",
      "shortCiteRegEx" : "Kambhampati",
      "year" : 2007
    }, {
      "title" : "J",
      "author" : [ "H.A. Kautz", "Allen" ],
      "venue" : "F.",
      "citeRegEx" : "Kautz and Allen 1986",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "J",
      "author" : [ "W.S. Lasecki", "Y.C. Song", "H.A. Kautz", "Bigham" ],
      "venue" : "P.",
      "citeRegEx" : "Lasecki et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "J",
      "author" : [ "W.S. Lasecki", "L. Weingard", "G. Ferguson", "Bigham" ],
      "venue" : "P.",
      "citeRegEx" : "Lasecki et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Etzioni",
      "author" : [ "N. Lesh" ],
      "venue" : "O.",
      "citeRegEx" : "Lesh and Etzioni 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "G",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "Corrado" ],
      "venue" : "S.; and Dean, J.",
      "citeRegEx" : "Mikolov et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "1992",
      "author" : [ "Petrie", "C. J" ],
      "venue" : "Constrained decision revision. In Proceedings of the 10th National Conference on Artificial Intelligence. San Jose, CA, July 12-16,",
      "citeRegEx" : "Petrie 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Plan recognition as planning",
      "author" : [ "Ramı́rez", "M. Geffner 2009a] Ramı́rez", "H. Geffner" ],
      "venue" : "IJCAI",
      "citeRegEx" : "Ramı́rez et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ramı́rez et al\\.",
      "year" : 2009
    }, {
      "title" : "Plan recognition as planning",
      "author" : [ "Ramirez", "M. Geffner 2009b] Ramirez", "H. Geffner" ],
      "venue" : "In Proceedings of IJCAI,",
      "citeRegEx" : "Ramirez et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ramirez et al\\.",
      "year" : 2009
    }, {
      "title" : "and Geffner",
      "author" : [ "M. Ramirez" ],
      "venue" : "H.",
      "citeRegEx" : "Ramirez and Geffner 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and Mahadevan",
      "author" : [ "S. Saria" ],
      "venue" : "S.",
      "citeRegEx" : "Saria and Mahadevan 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "and Mooney",
      "author" : [ "P. Singla" ],
      "venue" : "R.",
      "citeRegEx" : "Singla and Mooney 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning action models from plan examples using weighted MAX-SAT",
      "author" : [ "Wu Yang", "Q. Jiang 2007] Yang", "K. Wu", "Y. Jiang" ],
      "venue" : "Artificial Intelligence Journal",
      "citeRegEx" : "Yang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2007
    }, {
      "title" : "Robust active learning using crowdsourced annotations for activity recognition",
      "author" : [ "Sukthankar Zhao", "L. Sukthankar 2011] Zhao", "G. Sukthankar", "R. Sukthankar" ],
      "venue" : "In AAAI workshop",
      "citeRegEx" : "Zhao et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2011
    }, {
      "title" : "D",
      "author" : [ "H.H. Zhuo", "Q. Yang", "Hu" ],
      "venue" : "H.; and Li, L.",
      "citeRegEx" : "Zhuo et al. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "S",
      "author" : [ "H.H. Zhuo", "Q. Yang", "Kambhampati" ],
      "venue" : "2012. Action-model based multiagent plan recognition. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Zhuo. Yang. and Kambhampati 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either discover plans by maximally “matching” observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming complete domain models are available. In real world applications, however, target plans are often not from plan libraries and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Our approach is capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains.",
    "creator" : "LaTeX with hyperref package"
  }
}