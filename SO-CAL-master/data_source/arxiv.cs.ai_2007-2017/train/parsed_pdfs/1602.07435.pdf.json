{
  "name" : "1602.07435.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Parametric Prediction from Parametric Agents",
    "authors" : [ "Yuan Luo", "Nihar B. Shah", "Jianwei Huang", "Jean Walrand" ],
    "emails" : [ "yluo@ie.cuhk.edu.hk,", "nihar@eecs.berkeley.edu", "jwhuang@ie.cuhk.edu.hk", "walrand@berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Parametric Prediction from Parametric Agents\nYuan Luo Department of Information Engineering, The Chinese University of Hong Kong,\nyluo@ie.cuhk.edu.hk,\nNihar B. Shah Department of Electrical Engineering and Computer Sciences, UC Berkeley,\nnihar@eecs.berkeley.edu\nJianwei Huang Department of Information Engineering, The Chinese University of Hong Kong,\njwhuang@ie.cuhk.edu.hk\nJean Walrand Department of Electrical Engineering and Computer Sciences, UC Berkeley,\nwalrand@berkeley.edu\nWe consider a problem of prediction based on opinions elicited from heterogeneous rational agents with private information. Making an accurate prediction with a minimal cost requires a joint design of the incentive mechanism and the prediction algorithm. Such a problem lies at the nexus of statistical learning theory and game theory, and arises in many domains such as consumer surveys and mobile crowdsourcing. In order to elicit heterogeneous agents’ private information and incentivize agents with different capabilities to act in the principal’s best interest, we design an optimal joint incentive mechanism and prediction algorithm called COPE (COst and Prediction Elicitation), the analysis of which offers several valuable engineering insights. First, when the costs incurred by the agents are linear in the exerted effort, COPE corresponds to a “crowd contending” mechanism, where the principal only employs the agent with the highest capability. Second, when the costs are quadratic, COPE corresponds to a “crowd-sourcing” mechanism that employs multiple agents with different capabilities at the same time. Numerical simulations show that COPE improves the principal’s profit and the network profit significantly (larger than 30% in our simulations), comparing to those mechanisms that assume all agents have equal capabilities. Key words : mechanism design, aggregated prediction, crowdsourcing"
    }, {
      "heading" : "1. Introduction",
      "text" : ""
    }, {
      "heading" : "1.1. Background and Motivations",
      "text" : "Prediction markets, which aggregate information elicited from people with private beliefs, have\nserved as a reliable tool for estimating the outcome of specific future events (see Berg and Rietz\n1\nar X\niv :1\n60 2.\n07 43\n5v 1\n[ cs\n.G T\n] 2\n4 Fe\nb 20\n16\n(2003)). For example, these markets have been used to predict the winners of election (see Wolfers\nand Rothschild (2011)), future demand for a product (see Hayes (1998)), and stock prices and\nreturns (see Gottschlich and Hinz (2014)). In these markets, values of traded information depend\non future outcomes, and the accuracy of prediction can be verified based on the realized outcomes.\nWith the emergence of several commercial crowdsourcing platforms such as Amazon Mechanical\nTurk and Microworkers, collecting information from people to make prediction has become much\ncheaper, easier and faster. However, the information collected from people (“the agents”) can be\nhighly unreliable due to the agents’ insufficient expertise and the lack of appropriate incentives.\nMore specifically, in a crowdsourcing platform, the agents are heterogeneous as they come from\ndifferent countries and have different skills, which leads to significant variations of the work quality\n(see Karger et al. (2014)). Furthermore, agents may exert different levels of effort to finish the\nallocated task based on different levels of payments, and different agents may react differently to\nthe same level of payment (see Liu et al. (2014)). The chosen level of effort affects their performance\ndramatically. Due to these issues, an appropriate incentive mechanism that exploits the agents’\nheterogeneity whilst incentivizes appropriate effort levels is crucial to a successful prediction system.\nBesides eliciting high quality of information, the prediction performance also depends on the\nprediction behaviour of the surveyor (“the principal”). Without an appropriate prediction rule,\nthe principal may not be able to effectively utilize the collected information and may obtain an\ninaccurate prediction results. This motivates us to study the incentive mechanism design together\nwith the prediction rule optimization.\nThe resultant problem of joint design poses a significant challenge due to the following reasons.\nFirst, due to the incorporation of the prediction problem, the objective of incentive mechanism\nchanges from eliciting agents’ information truthfully to minimizing the prediction error. As the\nprediction error is a result of the agents’ information and actions, the designed mechanisms not\nonly needs to motivate agents to report their truthful estimation information, but also needs to\nmake sure that agents take appropriate actions. Hence, we cannot directly implement existing\nstrictly proper scoring rules, e.g., the quadratic scoring rule (see Brier (1950), Selten (1998)), that only promotes truthfulness among agents to address the joint problem.\nSecond, the designed mechanism needs to solve both moral hazard and adverse selection problems simultaneously. Moral hazard results from the inability of the principal to observe an agent’s actions (i.e., effort exerted by the agent), while adverse selection corresponds to the inability of knowing an agent’s private information (i.e., the expertise of an agent). This is different from most incentive mechanisms designed in the existing works, which separate the mechanism design from the prediction problem and address either “hidden actions” (i.e., moral hazard) (see e.g., Fang et al. (2007), Ioannidis and Loiseau (2013)) or “hidden information” (i.e., adverse selection) (see e.g., Frongillo et al. (2015), Abernethy et al. (2015)).\nNevertheless, we formulate and optimally solve a “parametric” form of this joint design problem. More specifically, the principal desires to predict a parameter of a known distribution. Each agent is modeled in a parametric fashion, with her expertise governed by a single parameter that is the agent’s private information. We assume that agents are heterogeneous as they have different levels of expertise. While each agent aims to maximize her own expected payoff (i.e., the revenue minus the effort cost), the principal optimizes a joint utility that trades off the prediction error and the monetary costs. For ease of exposition, we refer to the principal as “she” and each agent as “he”."
    }, {
      "heading" : "1.2. Results and Contributions",
      "text" : "We focus on the interactions among a principal and multiple agents, and design an appropriate incentive mechanism to facilitate the parametric prediction process. Specifically, we design a mechanism, which we call “COPE” (standing for “COst and Prediction Elicitation”), that jointly optimizes the principal’s payoff in terms of the payments made to the agents and the prediction error incurred. COPE provides a systematic way for the principal to incentivize all participating agents to report their estimations truthfully and exert appropriate amounts of effort based on their respective capabilities.\nWe summarize our key results and the main contributions as follows.\n• Theoretic significance: We relax several critical assumptions that are common in papers in the\nrelated literature, i.e., the costs incurred by the agents are all known to the principal, the agents do\nnot incur costs for efforts, and the agents are all homogeneous. Hence, the proposed model pushes\nthis line of theoretical focused research into more realistic settings.\n• Optimal incentive mechanism design: We study a generic incentive mechanism design problem\nsituated in a prediction process. To study the optimal prediction solution, we design the COPE\nmechanism, which ensures that all participating agents report their estimations truthfully and\nexert appropriate amounts of effort based on their respective capabilities, meanwhile maximizes\nthe principal’s expected payoff.\n• Observations and insights: Our results show that, with Gaussian estimation noise, when the\nagent’s marginal cost is independent of his amount of exerted effort, the principal should conduct\na crowd-tender mechanism, by soliciting service only from the agent with the lowest reported cost.\nOn the other hand, when the marginal costs depends on the exerted effort, the optimal mechanism\nis in the form of crowd-sourcing, where the principal recruits multiple agents to complete the task.\n• Numerical results: Simulation in Section 5 show that COPE improves both the principal’s\nprofit and the network profit, comparing to those mechanisms that assume all agents have equal\ncapabilities and incentivize agents exert the same amount of effort. Moreover, the performance gap\nbetween COPE and the centralized benchmark solution is less than 3% under the quadratic cost\nfunction and 10% under the linear cost function.\nThe rest of the paper is organized as follows. After reviewing the literature in Section 2, we\ndescribe the system model in Section 3, and design the incentive mechanism in Section 4. In Section\n5, we provide the simulations results. We conclude in Section 6."
    }, {
      "heading" : "2. Related Work",
      "text" : "Mechanism design for truthful elicitation of agents’ opinions is an extensively studied problem, most\nrecently investigated are in the context of crowdsourcing (see, e.g., Cavallo and Jain (2015), Miller\net al. (2005), Prelec (2004), Shah et al. (2015), Shah and Zhou (2015)). In contrast to our work, this\nline of literature does not consider the prediction aspect, and only focuses on the elicitation problem\nalone. Mechanism design for truthful elicitation of agents’ opinions is also studied in the context\nof prediction markets (see, e.g., Wolfers and Zitzewitz (2004), Conitzer (2009)). These results,\nhowever, study the scenario where the agents take the responsibility of aggregating information. Our\npaper concerns a different setting and objective, in which the principal is in charge of information\ngathering and making the final prediction. Hence, the mechanism we design should not only elicit\nthe agents’ information but also incentivize agents to exert appropriate effort.\nThe scenario becomes quite different when prediction must be done by taking incentives into\naccount, and calls for the design of new procedures catering to both aspects. The recent studies (see,\ne.g., Fang et al. (2007), Ioannidis and Loiseau (2013), Frongillo et al. (2015), Cai et al. (2015),\nAbernethy et al. (2015)) address problems in this space. However, the models assumed in these\nworks are different, and generally more restrictive than the models considered in our paper in many\nrespects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation\nand the quality of each agent, under the assumption that the agents are homogeneous with the\nsame cost type. In contrast, we consider the more general and realistic setting where agents can\nhave different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative\ngame and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study\nhow a principal can make predictions by eliciting the agents’ confidences, again without considering\nthe costs that may be incurred by the agents. Abernethy et al. (2015) consider a model where the\nagents cannot fabricate their observation, but may lie about their costs, and design a mechanism\nto ensure that agents truthfully report their costs. In contrast, we assume a more general scenario\nwhere the agents can be strategic in choosing and reporting their respective observations. Cai et al.\n(2015) propose a monetary mechanism to collect data and to perform an estimation of a function at\none random point. However, they assume that the agent always reports truthfully once he makes an\nobservation. In contrast, our work considers strategic agents and proposes an optimal mechanism\nto ensure truth-telling by the agents.\nWe begin with a formal description of the problem formulation. Through this description, we will set up notations to capture the behavior of the agents, the objective of the principal, the prediction problem, and the mechanism-design problem."
    }, {
      "heading" : "3.1. System Model",
      "text" : "We consider a setting where the principal wishes to make a parametric prediction, that is, to form an informed estimate about a parameter x∗ ∈ X ⊆ R. Predicting the winner of an election and predicting box office results for movies are two motivating examples. We assume that x∗ has a prior distribution that is publicly known, for instance, from the results of an earlier election. We assume that the principal will come to know the precise value of x∗ sometime in the future, for instance, upon completion of the election. In order to make a prediction, the principal queries a set A= {1, . . . ,N} of N agents to report their observations.\nFigure 1 pictorially depicts the interaction between agents and the principal, including the agents’ reporting strategy and the principal’s prediction and payment decision. Before we explain each individual components of the figure, we first introduce some notations used to characterize the agents’ strategies and types in Figure 1.\nEffort Level and Cost Type: When queried by the principal, an agent can put in some effort to form an “observation” whose value is known only to that agent. We assume that the observation of any agent is noisy, and the distribution of the observation yn comes from a parameterised family of distributions φ(x∗, qn), where qn represents the effort exerted by agent n to make observation. The higher value of qn, the more effort agent n exerts, and thus the better quality of agent n’s\nobservation. An example that we focus on subsequently in the paper is additive Gaussian noise (see, e.g., Fang et al. (2007), Cai et al. (2015)), i.e., yn ∼N (x∗, 1qn ), where yn follows from Gaussian distribution with mean x∗ and variance 1/qn. Conditioned on x ∗, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn,∀n ∈A) be the observation vector and q= (qn,∀n∈A) be the effort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others.\nIt is costly for each agent to exert a high level effort when making an observation, and this cost not only depends on the effort chosen by the agent, but also is affected by the agent’s costtype parameter θn. The cost types of different agents are allowed to be different, capturing the heterogeneity of the agents. A smaller value of θn implies a higher capability of agent n. Specifically, we consider a publicly known cost function C : R+×R+→R+, and assume that the cost incurred by an agent n∈A with the cost type θn when exerting an effort qn is C(qn, θn). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2.\nThe cost types {θn}n∈A are assumed to be randomly, independently and identically distributed on support [θ, θ̄] for some 0≤ θ < θ̄ <∞. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [θ, θ̄]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.\nNext, we will discuss each individual components of Figure 1 sequentially. Reporting Observations and Making Payments: The principal employs monetary incentives to ensure that agents make their observations and report them to the principal. In order to incentivize agents to participate the prediction task, the payment to an agent must, at the least, cover the cost incurred by that agent in putting effort to make the observation. However, since each agent’s cost parameter is known only to that agent, the principal needs to ask each agent to\nreport his own cost type (Figure 1a). The agents are strategic, and any agent n∈A may report a cost type θ̂n that is different from his true cost type θn. Let θ̂ = (θ̂n, n ∈A) be the reporting cost type vector.\nAs we will see subsequently, incentivizing different agents to put different levels of effort depend-\ning on their respective cost types allows for a significantly better prediction performance. The\nprincipal must incentivize these different effort levels, and the choice of these effort levels is based on the agents’ reported cost types θ̂ (Figure 1b). Let function Qp : [θ, θ̄]N → R+ denote the effort that the principal requires an agent to exert. The function Qp depends on the cost types reported by the agents: Qp(θ̂n, θ̂−n) represents the effort required from agent n ∈ A, where θ̂−n = [θ̂1, . . . , θ̂n−1, θ̂n+1, . . . , θ̂N ] T is the reported cost parameters of all agents except agent n. Here (and elsewhere in the paper), we use the superscript “P” to represent the principal. We let Qp(θ̂) = (Qp(θ̂n, θ̂−n),∀n∈A) be the effort vector required by the principal.\nEach agent n∈A is strategic and may exert an effort qn 6=Qp(θ̂n, θ̂−n) to suit his own interests. When choosing the effort to exert, the agent may also exploit the fact that the principal cannot\ndirectly observe the actual effort exerted. Upon exerting the chosen effort qn, the agent obtains an observation yn (Figure 1c). The principal seeks to know the value of the observation yn, but the agent may report a strategically chosen value ŷn 6= yn to the principal (Figure 1d) that suits the agent’s own interests. We adopt the shorthand ŷ = (ŷn, n ∈ A) as the reporting vector. Based on the information obtained, the principal must make a prediction for the value of x∗ (Figure 1e).\nThe principal makes payment to each agent once she observes the true value of x∗. Specifically, we define the payment function as R : R× R× [θ, θ̄]N×→ R+; the payment to agent n is R(x∗, ŷn, θ̂n, θ̂−n), which depends on the value of x ∗, the agent n’s reported observation ŷn, and all agents’ reported cost parameters θ̂.\nAs indicated above, the model considered is a one-shot model, i.e., once the agents simultaneously\nreport their observations, the principal determines the payments based on the agents’ reported cost\nparameters and observations only, with no further adjustments on reported values or the payments.\nPayoff of Agent: Given the payment function announced by the principal, each agent n’s payoff Ua :R× [θ, θ̄]×R+×R× [θ, θ̄]N →R is defined as the difference between the payment received from the principal and the cost incurred in making the observation, and is given as\nUa(x∗, θ̂n, qn, ŷn, θn, θ̂−n) =R(x ∗, ŷn, θ̂n, θ̂−n)−C ( qn, θn ) , (1)\nHere the superscript “A” indicates a term associated to the agents. Equation (1) shows that agent n∈A’s payoff also depends on other agents’ reported cost parameters θ̂−n. When each agent n∈A chooses his strategy, i.e., his cost reported value θ̂n, exerted effort qn, and the reported observation ŷn, to maximize his expected payoff. The expected payoff of the agent n is calculated as\nE[Ua(x∗, θ̂n, qn, ŷn, θn, θ̂−n)] =E[R(x∗, ŷn, θ̂n, θ̂−n)]−C(qn, θn), (2)\nwhere the expectation is taken with respect to the distributions of x∗ and all agents’ cost parameters\nθ−n. Recall that each agent n only knows his own cost parameter θn, and only has distributional information about other agents’ cost parameters.\nPayoff of the Principal: Let function x̂ :R×R+→R characterize the prediction made by the principal based on the agents’ reported observations ŷ and the effort assumed to be exerted by the agents Qp(θ̂). Due to the inability of the principal to observe the agents’ exerted effort, the\nprincipal makes prediction based on her own knowledge (i.e., the agents reported observation ŷ and the effort Qp(θ̂) required from agents). Later in Section 4, we will show that the agents’ true\neffort level are the same as that required by the principal by employing our proposed mechanism. Let `p : R×R→ R+ be the loss function that characterizes the penalty term for mistakes in the prediction. For instance, one could consider the squared loss `p(x∗, x̂(ŷ,qp)) = (x∗ − x̂(ŷ,qp))2 as the penalty for the principal, where qpn = Q p(θ̂n, θ̂−n) and q p = (qpn,∀n ∈ A). We use qp as the shorthand notation for Qp(θ̂n, θ̂−n), in order to show that q p n, for any n∈A, is a decision variable for the principal. In Section 4.2.1, we will show how the principal determines the desired effort from the agents by taking the first order derivative of her Bayes risk with respect to qpn.\nWe measure the utility gained by the principal through the prediction in terms of the Bayes risk incurred under this loss function. The reason that we use Bayes risk is that it yields a real number (not a function of x̂ or ŷ) for each prediction, and the principal’s posterior expected prediction loss is equivalent to the Bayes risk (see Robert (2007), Berger (2013)). If all agents report their true observations (i.e., ŷ = y) and exert efforts as desired by the principal (i.e., q = qp =Qp(θ̂)), then the Bayes risk Bp :R×R→R+ is (see Robert (2007), Berger (2013))\nBp(qp) = inf x̂ E[`p(x∗, x̂(y,qp))], (3)\nwhere the expectation is taken with respect to x∗ and y. By assuming that the principal’s utility with perfect prediction is zero, the Bayes risk chacterzies the penality for the principal’s mistakes in the prediction, and the prinicial’s expected utility is just −Bp(qp). Correspondingly, the payoff of the principal U p : R × RN × [θ, θ̄]N → R is then defined as the difference between her utility obtained from prediction and the monetary payments to all agents:\nU p(x∗,qp,y, θ̂) =−Bp(qp)− ∑ n∈A R(x∗, yn, θ̂n, θ̂−n). (4)\nHere, we assumed without loss of generality that the monetary payment and the prediction loss is normalized to be on the same scale.\nFor clarity, we list the key notations in Table 1."
    }, {
      "heading" : "3.2. Design Objective",
      "text" : "Before explaining the design objective, we begin by defining two standard game-theoretic terms that are required for subsequent discussions.\nDefinition 1. (BIC: Bayesian Incentive Compatibility) A mechanism satisfies the Bayesian incentive compatibility (BIC) if for every agent n∈A, his expected payoff satisfies (see Fudenberg and Tirole (1991), Myerson (1979))\nE [ Ua(x∗, θn,Q p(θn,θ−n), yn, θn,θ−n) ] ≥E [ Ua(x∗, θ̂n, qn, ŷn, θn,θ−n) ] ,\n∀(θ̂n, qn, ŷn) 6=(θn,Qp(θn,θ−n), yn), (5)\nwhere the expectation is taken with respect to x∗ and all other agents cost parameters θ−n.\nBIC means that for any agent n, reporting the true cost parameter, exerting the effort requested by the principal, and reporting true observation will maximize his expected payoff, given common knowledge about the distribution on agents cost parameters and when other agents are truthfully report their cost parameters.\nDefinition 2. (BIR: Bayesian Individual Rationality) A mechanism satisfies the Bayesian incentive rationality (BIR), if the expected payoff of every agent n ∈ A is non-negative, given that he reports truthfully, exerts effort as the principal desires, and assumes that all other agents report their cost parameters truthfully, that is (see Fudenberg and Tirole (1991), Myerson (1979)),\nE [ Ua(x∗, θn,Q p(θn,θ−n), yn, θn,θ−n) ] ≥ 0, ∀n∈A, (6)\nwhere the expectation is taken with respect to x∗ and all other agents’ cost parameters θ−n.\nAssuming (without loss of generality) that the payoff of an agent not participating in this process equals zero, BIR means that an agent will participate only if his expected payoff is at least as much as that of a non-participating agent.\nBased on the revelation principal (see Myerson (1979)), the problem of finding a mechanism that maximizes the principal’s expected payoff can be restricted to the set of mechanisms where agents are willing to reveal their private information to the principal. Moreover, the principal cannot force agents to accept the task. Hence, the problem that we want to solve is formalized as follows. The goal is to design a mechanism, say M, that maximizes the principal’s expected while ensuring truthful reports from the agents:\nsup M\nE [ U p(x∗,y,θ) ] subject to: BIC and BIR in (5) and (6),\n(7)\nwhere the expectation is taken with respect to x∗, y and θ, and the BIR condition makes sure that the agents are willing to participate in the game. In words, the goal is to design a mechanism such that: (i) the principal’s payoff is maximized in expectation; (ii) the principal can elicit truthful information from all agents; and (iii) the principal can incentivise suitable effort from the agents based on their respective cost parameters."
    }, {
      "heading" : "4. The COPE Mechanism",
      "text" : "In this section, we present our mechanism “COPE” (COst and Prediction Elicitation) that provides an optimal solution to the problem (7) of parametric prediction from parametric agents. We will first consider two specific settings in order to illustrate the key ideas behind COPE, and to obtain some concrete engineering insights. We will then proceed to present COPE in full generality."
    }, {
      "heading" : "4.1. Two Example Settings",
      "text" : "We consider the following specific setting in this section. We consider the Gaussian case, where we assume the prior x∗ ∼N (µ0, σ20), and the observation of every agent n follows the distribution yn ∼N (x∗, 1qn ), independent of all other events. The values of µ0 and σ0 are assumed to be public knowledge. We assume θn ∼Uniform[θ, θ̄], independent for every n ∈A. We consider the squared `2-loss to measure the prediction error, namely, ` p(x∗, x̂) = (x∗− x̂)2. In what follows, we consider two cost functions: (i) the linear cost function C(q, θ) = qθ, and (ii) the quadratic cost function C(q, θ) = 1 2 θq2.\n4.1.1. Linear Cost Function C(q, θ) = qθ We first consider the linear cost function C(q, θ) = qθ and discuss the corresponding COPE mechanism. Algorithm 1 presents the higher-level structure of the mechanism which corresponds to the steps in Figure 1. The optimality of the mechanism crucially relies on the careful construction of specific functions referred to in the algorithm, and these constructions are described below.\nRecall that the function Qp : [θ, θ̄]× [θ, θ̄]N−1→R+ specifies the effort that the principal requires an agent to exert, based on the cost parameters reported by all agents. In Theorem 1 subsequently, we show that when the cost function is linear, the principal requires only one agent to exert effort.\nAlgorithm 1 COPE Step 1: The principal announces a payment function R. Step 2: Every agent n∈A independently reports a cost type θ̂n ∈ [θ, θ̄] to the principal. Step 3: The principal sends each agent n ∈ A a contract, which specifies the effort level Qp(θ̂n, θ̂−n) along with values of functions π(θ̂n, θ̂−n), K(θ̂n, θ̂−n), and S(θ̂n, θ̂−n) that comprise the function R. Step 4: Each agent n∈A exerts effort qn and makes an observation yn. Step 5: Each agent n∈A reports an estimate ŷn. Step 6: The principal makes prediction x̂. Step 7: The true value x∗ is realized. Step 8: The principal makes the payment R(x∗, ŷn, θ̂n, θ̂−n) to every agent n∈A.\nIf there are multiple agents achieving the same minimum value of θ̂n, the principal would randomly choose one agent to exert effort. This property is reflected in the following choice of function Qp:\nQp(θ̂n, θ̂−n) =\n{ max{(2θ̂n− θ)− 1 2 −σ−20 ,0} if n= arg minm∈A θ̂m\n0 otherwise. (8)\nThe function Qp is designed to strike an optimal balance between the prediction error and the monetary expenditure, accommodating the fact that the agents are heterogeneous. We define n0 = argminm∈Aθ̂m, that is, n0 is the agent with the lowest reported cost parameter.\nWe now characterize the function R that governs the payment made by the principal to the agents. The payments to all agents other than agent n0 are zero, since these agents are not involved in the observation and prediction procedure. The payment made to agent n0 is\nR(x∗, ŷn0 , θ̂n0 , θ̂−n0) = π(θ̂n0 , θ̂−n0)− (x ∗− ŷn0) 2 ·K(θ̂n0 , θ̂−n0) +S(θ̂n0 , θ̂−n0), (9)\nwhere\nπ(θ̂n0 , θ̂−n0) = π(θ̂n0) = θ̂n0(2θ̂n0 − θ) − 12 − θ̄σ−20 + 2[(2θ̄− θ) 1 2 − (2θ̂n− θ) 1 2 ],\nK(θ̂n0 , θ̂−n0) =K(θ̂n0) = θ̂n0(2θ̂n0 − θ) −1, and S(θ̂n0 , θ̂−n0) = S(θ̂n0) = θ̂n0(2θ̂n0 − θ) − 12 . (10)\nLet us explain the main ideas behind the above choices. The detailed proof can be found in Appendix. First, The term (x∗− ŷn0)2 in (9) ensures that the agent reports his observation truthfully. This is because no other terms in (9) depend on ŷn0 , and the value of calculated by K(θ̂n0)\nis always positive. Hence, when the agent chooses the reporting observation strategy to maximize his expected payoff, he focuses on minimizing the term Ex∗ [(x∗ − ŷn0)2] whose value is minimum only when the agent reports his truthful observation, i.e., ŷn0 = yn0 . Second, we can verify that the expected payoff of the agent is maximized only when the agent truthfully reports his cost param-\neter, given the agent reports his true observation. Third, the choices of functions K and S ensure that the agent exerts an effort as desired by the principal. As the term (x∗− ŷn0)2 makes the agent reports his true observation, we can verify that the expected payoff of the agent is maximized only when the agent chooses qn0 =Q p(θn0 ,θ−n0). Finally, the function π is designed to ensure that the principal’s expected payoff defined in (4) is maximized while ensuring BIC and BIR condition is satisfied. As we shown that the term (x∗− ŷn0)2 and the choices of functions K and S guarantee the truthful behavior of the agent, i.e., ŷn0 = ŷn and qn0 =Q p(θn0 ,θ−n0), the expected value of R simply equals to the value of the function π. In such case, we can focus on deriving the function of\nπ to maximize the expected payoff the principal.\nThen we characterize the prediction decision made by the principal. After collecting all agents\nreported observations, the principal makes the prediction x̂ based on the following equation:\nx̂(ŷn0 , q p n0\n) = µ0 · 1/σ20 + qpn0 · g(ŷn0)\n1/σ20 + q p n0\n, (11)\nwhere qpn0 =Q p(θ̂n0 , θ̂−n0), and function g :R→R is defined as g(ŷn0) = ŷn0 + (ŷn0−µ0) (qpn0 ·σ20) .\nHere, the predictor x̂ employed by the principal is the standard Bayes estimator operating on\nthe agents’ responses and this predictor does not affect the payoff of the agents.\nWe prove that the proposed COPE mechanism is optimal.\nTheorem 1. Under the linear cost function C(q, θ) = qθ, COPE satisfies the BIC and BIR con-\ndition defined in (5) and (6) and maximizes the principal’s expected payoff defined in (7).\nWe provide the detailed proof in Appendix A. An important consequence of the theorem is that\nthe optimal mechanism in the case of linear costs awards the task to the single agent with the\nlowest bid. This corresponds to what we call a “crowd-tender” system where all agents submit their\ncost parameters, and the lowest bidder is awarded the task. We will discuss the intuition behind such a result in more details in Section 4.1.3.\n4.1.2. Quadratic Cost Function C(q, θ) = 1 2 θq2 We now consider a quadratic cost function C(q, θ) = 1 2 θq2 and present COPE for this setting. The higher level structure of COPE is again given by Algorithm 1, and the specific functions referred to in the algorithm is provided below.\nUnder COPE, the function Qp : [θ, θ̄]× [θ, θ̄]N−1→R+ that governs the effort that the principal\nrequires an agent to exert is given as\nQp(θ̂n, θ̂−n) = (2θ̂n− θ)−1(W (θ̂))−2, (12)\nwhere W is the solution of the equation [W (θ)]3− 1 σ20\n[W (θ)]2 = ∑\nm∈A 1 2θm−θ . An explicit (although\ncumbersome) solution of W is provided in (103) of Appendix B.\nAs in the case of linear costs, the function Qp is designed to optimally harness the heterogenity of the agents in order to minimize the prediction error with a small enough payment. Note that in contrast to the linear case (8), here the principal requires every agent to exert a positive effort.\nWe define the function R that governs the payment to any agent n as\nR(x∗, ŷn, θ̂n, θ̂−n) = π(θ̂n, θ̂−n)− (x∗− ŷn)2 ·K(θ̂n, θ̂−n) +S(θ̂n, θ̂−n), (13)\nwhere\nπ(θ̂n, θ̂−n) = 1\n2\n( θ̂n · [ Qp(θ̂n, θ̂−n) ]2 + ∫ θ̄ θ̂n [ Qp(z, θ̂−n) ]2 dz ) ,\nK(θ̂n, θ̂−n) = [ Qp(θ̂n, θ̂−n) + 1/σ 2 0 ]2 θ̂n ·Qp(θ̂n, θ̂−n),\nS(θ̂n, θ̂−n) = [ Qp(θ̂n, θ̂−n) + 1/σ 2 0 ] θ̂n ·Qp(θ̂n, θ̂−n). (14)\nThese functions have a form similar to those in the case of linear costs (9), except that these functions depend on the reported cost parameters of all N agents, whereas the corresponding functions in the linear cost setting depended only on the reported cost parameter of one agent. The remaining higher level intuition behind this construction is similar to that behind the linear-cost case described in the previous section.\nThe principal uses the Bayes estimate as her predictor:\nx̂(ŷ,qp) = (1−N)µ0/σ20 +\n∑ n∈A (1/σ 2 0 + q p n) · ŷn\n1/σ20 + ∑ n∈A q p n\n, (15)\nwhere qpn =Q p(θ̂n, θ̂−n).\nThe following theorem establishes the optimality guarantee of COPE under quadratic costs.\nTheorem 2. Under the quadratic cost function C(q, θ) = 1 2 θq2, COPE satisfies the BIC and BIR condition defined in (5) and (6) and maximizes the principal’s expected payoff defined in (7).\nThe detailed proof is provided in Appendix B. As COPE under the quadratic cost function requires all agents to exert certain effort and to report their observation, this corresponds to a “crowd-sourcing” system.\n4.1.3. Engineering Takeaways Our results show that interestingly, it is optimal for the principal to call for a crowd-tender when the cost function is linear, while it is optimal to design a crowd-sourcing mechanism when the cost function is quadratic. Informally, the cost function acts as a regularizer on the choice of effort levels qp, and the dichotomy of these two cost functions is related to the sparsity inducing properties of the `1-regularizer, and the lack thereof of the (squared) `2-regularizer."
    }, {
      "heading" : "4.2. General Setting",
      "text" : "In this section, we will present COPE under more general forms of the cost function, the noise distribution, the prior distribution, and the prediction loss function. Under these general conditions, the structure of the mechanism remains identical to Algorithm 1. We will show that COPE is optimal and feasible under certain regularity conditions.\n4.2.1. Assumptions Cost Function We first define the general cost function of the agent n ∈ A. Specifically, for\nagent n∈A, his cost function C :R+×R+→R+ is C(qn, θn) = ∫ qn\n0 c(z, θn)dz, where c(z, θn) is the\nthe marginal cost function. We assume that the marginal cost function c :R+×R+→R+ satisfies:\n∂c(q, θ)\n∂q > 0,\n∂c(q, θ)\n∂θ > 0,\n∂2c(q, θ)\n∂θ2 > 0,\n∂2c(q, θ)\n∂q∂θ > 0. (16)\nwhere the first inequality shows a nondecreasing marginal cost in agent’s effort level, the second and third inequalities show that the marginal cost is monotonically increasing and convex in the cost parameter θn, the last inequality implies that the marginal cost with respect to cost parameter θn is increasing in effort q. These assumptions are widely used to model the cost function (see, e.g., Che (1993), Chen et al. (2008) and references therein).\nThe cost types {θn}n∈A are assumed to be randomly, independently and identically distributed with a cumulative distribution function F : [θ, θ̄]→R+ and probability density function f : [θ, θ̄]→ R+. The functions F and f are public knowledge to all agents. We also assume that the c.d.f. function F is continuous, differentiable, and log concave in [θ, θ̄]. This is a regularity condition often assumed in auction contexts (see, e.g., Myerson (1981)). This assumption is satisfied by a wide range of distributions, such as the uniform, gamma, and beta distributions. See Rosling (2002) for an extensive discussion on log concave probability distributions.\nObservation and Loss Function We assume that the distribution of the observation yn of any agent n∈A comes from a parameterized family of distributions φ(x∗, qn), where qn represents the effort exerted by agent n to make observation. A typical parameterized distribution, for example, is the Gaussian distribution with mean x∗ and variance 1/qn.\nLet x̂ : RN × RN+ → RN be the prediction function that characterizes the prediction made by the principal, and `p : R× R→ R+ be the loss function that characterizes the penalty term for mistakes in the principal’s prediction. A typical loss function, for example, is the squared loss `p(x∗, x̂(y,qp)) = [x∗ − x̂(y,qp)]2, where qpn = Qp(θ̂n, θ̂−n) and qp = (qpn,∀n ∈ A). We measure the utility gained by the principal through the prediction in terms of the Bayes risk. Here, the Bayes risk is calculated under the loss function `p. Specifically, if all agents report their true observations (i.e., ŷ= y) and exert efforts as desired by the principal (i.e., q= qp =Qp(θ̂)), then the Bayes risk Bp :R×R→R+ is\nBp(qp) = inf x̂ E[`p(x∗, x̂(y,qp))], (17)\nwhere the expectation is taken with respect to x∗ and y. We assume that such a Bayes estimator minimizing the Bayes risk exists, and that the Bayes risk is finite.1\nWe will also assume the existence of a function `a : R×R→R+ using which the principal may measure the accuracy of an agent’s report. Specifically, we assume that if yn is generated according to agent n’s observation distribution, then we assume that `a satisfies\nyn ∈ arg inf y † n E[`a(x∗, y†n(yn))], (18)\nwhere function y†n : R→ R characterizes the reporting strategy of the agent n ∈ A given his true observation is yn, the infimum is over all measurable functions of the observation yn, and the assumption says that the identity function is a minimizer of the expected value of `a when its first argument is x∗. For instance, we considered `a(x∗, ŷn) = (x ∗ − ŷn)2 earlier in the two motivating examples involving the Gaussian distribution.\nWe let Ba :R×R→R+ denote the associated Bayes risk:\nBa(qn) = inf yn E[`a(x∗, yn)], (19)\nwhere the expectation is taken with respect to x∗ and yn, and the distribution of yn depends on the agent’s exerted effort qn. The Bayes risk of the principal, i.e., B p(qp), characterizes the principle’s expected payoff loss due to the difference between the true value x∗ and her own prediction x̂; while the Bayes risk of the agent, i.e., Ba(qn), characterizes the agent n∈A’s expected payoff loss due to the difference between the true value x̂ and his reporting prediction ŷn.\nWe assume that the Bayes risk of the principal and the agent satisfy the following monotonicity\nconditions:\n∂Bp(qp)\n∂qpn 6 0,\ndBa(qn)\ndqn 6 0,\n∂2Bp(qp)\n∂qpn 2 > 0,\nd2Ba(qn)\ndqn2 > 0,\n∂2Bp(qp)\n∂qpn∂q p m\n> 0, ∀m 6= n. (20)\nIn Section 4.1, we can verify that under the Gaussian distribution, the Bayes risk of the principal\nis Bp(qp) = 1 1/σ20+ ∑ n∈A q p n and the Bayes risk of the agent is Ba(qn) = 1 1/σ20+qn , both satisfy (20).\nWe assume that the principal has designed a mechanism that can control the agents’ exerted effort and elicit agents to report their observation truthfully. Hence, from the principal’s point of view, qpn, for any n∈A is a decision variable for the principal, and we can take the derivative of the principal’s Bayes risk with respect to qpn in (20). However, in reality, the agents would strategically\nchoose their exerted effort. Hence, we need to design a mechanism that involves a carefully designed function Qp, so that the agents would put the effort as the principal expected and truthfully report their observations.\nGiven these preliminaries, we now present our mechanism for this setting.\n4.2.2. Mechanism Our proposed mechanism COPE for the general setting also follows Algo-\nrithm 1, with the specific functions detailed below.\nThe function Qp : [θ, θ̄]× [θ, θ̄]N−1→ R+ that governs the effort that the principal requires an\nagent to exert is given as the solution of the equation\nmaxQp Eθ [ −Bp ( Qp(θ̂) ) − ∑ n∈A C ( Qp(θ̂n, θ̂−n), θn ) − ∑ n∈A ∂C ( Qp(θ̂n, θ̂−n), θn ) ∂θn · F (θn) f(θn) ] s.t. Qp(θ̂n, θ̂−n) is nonincreasing in θ̂n,∀n∈A. (21)\nWe define the function R(·) that governs the payment to any agent n as\nR(x∗, ŷn, θ̂n, θ̂−n) = π(θ̂n, θ̂−n)− `a(x∗, ŷn) ·K(θ̂n, θ̂−n) +S(θ̂n, θ̂−n), (22)\nwhere\nπ(θ̂n, θ̂−n) =C ( Qp(θn,θ−n), θn ) + ∫ θ̄ θn ∂C(Qp(z,θ−n), η) ∂η dz, (23)\nK(θ̂n, θ̂−n) = − c(Qp(θ̂n, θ̂−n), θ̂n)\ndBa(qn)/dqn ∣∣∣∣∣ qn=Qp(θ̂n,θ̂−n) , (24)\nS(θ̂n, θ̂−n) = − c(Qp(θ̂n, θ̂−n), θ̂n) ·Ba(qn)\ndBa(qn)/dqn\n∣∣∣∣∣ qn=Qp(θ̂n,θ̂−n) , (25)\nThe principal uses the Bayes estimate as her predictor: x̂(ŷ,qp) = arg inf x̂E [ `p ( x∗, x̂(ŷ,qp) )] . The detailed form of the principal’s estimation depends on the distribution of each agent’s observation φ(x∗, qn) defined in Section 3.1 and the loss function ` p. With the Gaussian distribution and quadratic loss function adopted in Section 4, the principal’s predictor is calculated as x̂(ŷ,qp) = (1−|Ap|)µ0/σ20+ ∑ n∈Ap (1/σ 2 0+q p n)·ŷn\n1/σ20+ ∑ n∈Ap q p n\n, where qpn =Q p(θ̂n, θ̂−n) is the decision variable of the principal, Ap\nis the set of agents recruited by the principal to report their estimation, and |Ap| is the number of agents in the set Ap.\n4.2.3. Guarantees The following theorem establishes the optimality guarantees of COPE.\nTheorem 3. COPE satisfies the BIC and BIR condition defined in (5) and (6) and maximizes the principal’s expected payoff defined in (7) if\n∂c ( Qp(θn,θ−n), θn ) ∂θn ≤ 0, (26)\nwhere function c characterizes the agent’s magical cost and is defined in (16).\nCondition (26) implies that the marginal cost of the agent should decrease with the agent’s cost type, so that COPE can induce the truthful behavior of the agents. We note that condition (26) is satisfied under the Gaussian case when θ follows from uniform distribution, as discussed in Appendix A and B. 5. Simulations\nWe conduct numerical studies to evaluate the performance of COPE. In particular, we investigate the amount of gain that can be achieved by (optimally) exploiting the heterogeneity of the agents. We first consider an integrated system, where the principal and all agents act as an integrated decision maker to maximize their aggregate profit (called network profit). We denote the network profit achieved under the integrated system as the centralized benchmark.2 Then we describe the details of the homogeneous benchmark mechanism under both the linear and quadratic cost function. Finally we compare the performance of COPE to the homogeneous benchmark in terms of principal’s expected payoff, expected prediction error and total payment made by the principal to the agents."
    }, {
      "heading" : "5.1. Centralized Mechanism",
      "text" : "In the integrated system, the integrated player (the principal and all agents) knows the precise value of θ (i.e., all agents’ cost types). Moreover, all participated agents would exert the effort that maximizes the network profit. Specifically, the expected network profit is defined as the difference between her utility obtained from prediction and the cost of all agents:\nUnp(q,θ) =−Bp(q)− ∑ n∈A C ( qn, θn ) , (27)\nwhere the utility gained through the prediction is measured in terms of the Bayes risk given in (17). Let function qo : R+× [θ, θ̄]→R+ be the solution that maximizes the network profit defined in (27). By focusing on the case x∗ ∼ N (µ0, σ20), we have the optimal solution under linear cost function as:\nqon =\n{ max{1/ √ θn− 1/σ20,0}, if n= arg minm∈A θm,\n0, otherwise. (28)\nThe optimal solution under quadratic cost function is\nqon = 1 θn · 1[ W o(θ) ]2 , (29) where the function W o : [θ, θ̄]N →R+ is the solution of the below equation:\n[ W o(θ) ]3− 1 σ20 · [ W o(θ) ]2−∑ m∈A 1 θm = 0. (30)\nIn the integrated system, the integrated player makes the prediction as\nx̂= µ0/σ\n2 0 + ∑ n∈A y o n · qon\n1/σ20 + ∑ n∈A q o n , (31)\nwhere yon is agent n∈A’s true observation."
    }, {
      "heading" : "5.2. Homogenous Mechanism",
      "text" : "The homogenous mechanism assumes all agents to be identical (although in practice they are not), and hence does not elicit the cost parameters of individual agents. In the absence of this knowledge, the principal operates under the belief that every agent’s cost parameter equals θ† ∈ [θ, θ̄]. The principal thus chooses payment function Rhom := α(θ †) − β(θ†) · (x∗ − ŷn)2, where the function α : [θ, θ̄]→ R+ and the function β : [θ, θ̄]→ R+ are chosen to incentivize every agent n to exert optimal effort and report observations truthfully in a manner that maximizes the principal’s payoff.\nWe focus on the case where x∗ ∼N (µ0, σ20). Let function q† : R+× [θ, θ̄]→R+ be the effort that the principal requires every agent to exert, based on the principal’s belief that every agent’s cost parameter equals to θ†. Then the principal makes the prediction as\nx̂= µ0/σ 2 0 + q\n†∑ n∈A g(ŷn)\n1/σ20 +N · q† , (32)\nwhere ŷn is the agent n’s reported observation, and the function g :R→R is defined as\ng(ŷn) = ŷn + ŷn−µ0 q† ·σ20 . (33)\nLinear Cost Function: Under the linear cost function, the choices of functions q†, α, and β\nare\nq†(N,θ†) = 1\nN ( 1√ θ† − 1 σ20 ) ,\nα(θ†) = ( 1/σ20 + q †) · θ†q†+ θ†q†, β(θ†) = (1/σ20 + q†)2 · θ† The principal chooses the function α(·) to make sure that the agent n with the cost type θ† is willing to participate the prediction task, and chooses the function β(·) to make sure that the agent n exerts the effort qn = q †(N,θ†) as the principal desires.\nRecall that the actual cost parameter of the agent n ∈ A is θn. Hence, the agent n will exert\neffort qn = √ β(θ†)/θn−1/σ20 and report ŷn = µ0/σ 2 0+yn·qn\n1/σ20+qn to maximize his expected payoff. Besides,\nif the expected payoff of the agent n is negative, he will not participate this prediction task.\nAlso recall that the principal knows the prior information of x∗ ∼N (µ0, σ20). Hence, the principal can always achieve a payoff of −1/σ20 by not making any payments, and simply choosing the prior mean has her prediction. Hence, the principal does not pay anything and simply sets x̂= µ0 if her expected payoff is smaller than −1/σ20.\nQuadratic Cost Function: Under the quadratic cost function, q† is the solution of the following\nequation:\n1( 1/σ20 +N · q† )2 − θ† · q† = 0 The functions α(·) and β(·) are :\nα(θ†) = ( 1/σ20 + q †) · θ†q†+ 1 2 θ† [ q† ]2 , β(θ†) = ( 1/σ20 + q †)2 · θ†q†. Recall that the actually cost parameter of the agent n ∈A is θn. Hence, the agent n will exert\neffort qn to maximize his own expected payoff, where qn is the solution of\nβ( 1/σ20 + qn )2 − θnqn = 0.\nBesides, if the expected payoff of the agent n is negative, he will not participate this prediction task. Also, the principal does not pay anything and simply sets x̂ = µ0 if her expected payoff is smaller than −1/σ20."
    }, {
      "heading" : "5.3. Numerical Results",
      "text" : "In the simulations, we draw x∗ ∼N (0,1), and set θ = 0 and θ̄ = 1. We vary the number of agents from N = 3 to N = 19. Each point in the plots is an average across 50000 trials. Without loss of generality, we have normalized the principal’s payoff (see (4)) so that it equals zero in the ideal (unachievable) case of zero prediction error and a zero payment. Note that the principal can always achieve a payoff of −1 by not making any payments, and simply choosing the prior mean has her prediction.\nFigure 2 depicts the principal’s expected payoff achieved under COPE and under the homogeneous mechanism for different values of θ† when the cost function is linear and quadratic, respectively. We use the red line with circle markers to denote COPE, the blue dash line with square markers to denote homogeneous mechanism with θ† = 0.2, the dark dash line with diamond markers to denote homogeneous mechanism with θ† = 0.5, and the magenta line with right-pointing triangle markers to denote homogeneous mechanism with θ† = 0.8.\nFigure 2 shows that COPE can improve the principal’s payoff by exploring the heterogeneity of users. The improvement is at least 10% under the linear cost function and 5% under the quadratic cost function.\nBy comparing Figure 2a to Figure 2b, we can see that the value of the belief of principal (i.e., θ†) under the homogeneous mechanism will affect the final result. Under the linear cost function, a lower value of θ† results in a better performance in terms of the principal’s payoff. However, under the quadratic cost function, a higher value of θ† results in a better performance. The reasons are as follows.\nUnder the homogeneous mechanism, the value of θ† will determine the number and types of agents joining the task. Having a higher value of θ† would incentivize more agents to participate.\nThis is because, for an agent n∈A whose cost parameter θn < θ†, he can put less effort to achieve the same performance as the agent with cost parameter θ† can.\nUnder the linear cost function, similar as COPE, finding the most capable one would be optimal for the principal, as the marginal cost is nonnegative even the agent does not put any effort. Hence, having a lower value of θ† would eliminate more agents, and have a higher chance to find the agent with θn ≤ θ†.\nOn the contrary, under the quadratic cost function, it would be optimal to recruit as many agents as possible to improve the prediction accuracy. The benefit brought by the accuracy improvement would be higher than the additional payment to agents. Hence, having a higher value of θ† would help the principal recruit more agents.\nFigure 3 depicts the network profit achieved under COPE and under the homogeneous mechanism for different values of θ† when the cost function is linear and quadratic, respectively. We use the dash brown line to denote the benchmark solution where the principal and all agents acted as an integrated player.\nFrom Figure 3, we have the following observations. • COPE can achieve a network profit close to the integrated benchmark solution, e.g., the gap\nis less than 10% under the linear cost function and 3% under the quadratic cost function.\n• Network profit achieved under COPE increases with the number of agents. This is because\nthe increasing number of agents allows the principal to have a higher chance to incentivize agents\nwith high capability to improve the prediction accuracy. This is true under both costs, even though\nonly one agent will be recruited under the linear cost.\n• COPE leads to a much higher network profit, compared to the homogeneous mechanism. The\nreason is that COPE explores the heterogeneity of agents by eliciting their cost type information.\nRecall that the principal makes the prediction based on (32) in the homogeneous mechanism.\nAs the actually effort exerted by the agents are different from that desired by the principal (i.e., qn 6= q†), the prediction made by the principal is inaccurate. On the contrary, COPE elicits the cost types of agents as well as incentivizes each agent to exert the appropriate level of effort, which\nresults in a better performance than the homogeneous mechanism. Due to the joint effect of the number of agents and the value of θ†, the performance of homogeneous mechanism is close to that\nof COPE (e.g., N < 4 under the linear cost function and N < 6 under the quadratic cost function) in terms of expected network profit. However, it is difficult to determine the proper choice of θ† in terms of the number of agents. Finding the optimal value of θ† given the number of agents will be\nan interesting future work."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We study the parametric prediction market under information asymmetry. To elicit the truthful information of participating agents and exploit the heterogeneity in the agents, we propose mechanism COPE, which ensures agents to exert effort desired by the principal and report their true observation. Our analysis indicate that, under the Gaussian estimation noise scenario, when the costs incurred by the agents are linear in the amount of exerted effort, the principal should require service from only one agent with the lowest reported cost. On the other hand, when the costs are quadratic in the exerted effort, it is optimal for the principal to recruit multiple agents to complete the task. We also present the general form of COPE that is optimal for a wide variety of settings (e.g., general cost function and the noise distribution).\nIn this work, we have focused on the parametric setting, where the principal recruits agents to estimate a parameter (e.g., the winner of a election) that the realized value can be observed in the future. As in some cases, such as rating the quality of a book, the true outcome cannot be easily observed or verified. In the future, we will consider how to incentivize agents’ behaviour when collecting subjective data. Moreover, in order to give theoretical insights into the problem of estimation from strategic agents, we use one parameter, i.e., the cost type θ to characterize the heterogeneity of the agents in terms of their cost. Relaxing the parametric assumption (e.g., characterizing agents’ types with random functions) and designing mechanisms with theoretical guarantees is extremely challenging. In practice, heuristics (e.g., Brousseau and Glachant (2002), Chiappori and Salanié (1997)) are employed to circumvent the parametric assumption when using these mechanisms. In the future, we would consider how to relax such parametric assumption."
    }, {
      "heading" : "Appendix. Full Proofs",
      "text" : ""
    }, {
      "heading" : "A. Proof of Theorem 1: Linear Costs",
      "text" : "The proof will proceed in four steps. The first three steps show that our mechanism incentivizes the agents to be truthful, and the fourth step proves optimality of our mechanism. First, we show that\nirrespective of what an agent reports as his cost parameter, and irrespective of the effort he exerts, the agent is always incentivized to report his true observation. We follow this up and show that irrespective of the effort that an agent exerts, he is always incentivized to report his cost parameter correctly. The third step completes the proof of truthfulness, showing that under truthful reporting of the cost parameter and the observation, in our mechanism, an agent is always incentivized to exert precisely the effort as desired by the principal. Finally, we show that among all mechanisms that ensure truthful reports, our mechanism maximizes the principal’s expected utility.\nWe assume that the random variables {θn}n∈A are independently and identically distributed on support [θ, θ̄], with a cumulative distribution function F : [θ, θ̄]→ R+ and a probability density function f : [θ, θ̄]→ R+. We further assume that the c.d.f. function F is continues, differentiable, and log concave in [θ, θ̄]. This assumption is satisfied by a wide range of distributions, such as uniform, gamma, and beta distributions.\nStep 1. Truthful reporting of observation under COPE We will analyze the strategies of the agent who is recruited by the principal and the agents who\nare not recruited by the principal, respectively.\nWe first study the observation reporting strategy of the agent n0 who is recruited and rewarded\nby the principal, where n0 = argminm∈Aθ̂m.\nWe will show that the agent n0 will choose\nŷn0 = µ0 · 1/σ20 + yn0 · qn0\n1/σ20 + qn0 (34)\nto maximize his expected payoff given his exerting effort qn0 and own observation yn0 .\nAs shown in (9), π(θ̂n0), K(θ̂n0) and S(θ̂n0) are independent of ŷn0 . Moreover, the value of calculated by K(θ̂n0) is always positive. Hence, when the agent n0 makes reporting observation strategy to maximize his expected payoff, i.e.,\nŷn0 ∈ argmaxE [ π(θ̂n0)−K(θ̂n0) · (x ∗− ŷn0) 2 +S(θ̂n0) ] −C ( qn0 , θn0 ) ,\nwhere the expectation is taken with respect to x∗ and cost parameters θ−n0 =\n[θ1, . . . , θn0−1, θn0+1, . . . , θN ] T except agent n0, it is equivalent for the agent n0 to choose reporting strategy such that\nŷn0 ∈ argminEx∗ [(x ∗− ŷn0) 2]. (35)\nBased on theory of Bayesian estimation (see Myerson (1979), Lehmann and Casella (1998)), only\nwhen ŷn0 = µ0·1/σ20+yn0 ·qn0\n1/σ20+qn0 , the value of Ex∗ [(x∗− ŷn0)2] is minimized and the expected value is\nEx∗ [(x∗− ŷn0) 2] =\n1\n1/σ20 + qn0 .\nWe then study the observation reporting strategy of other agents who are not recruited and rewarded by the principal. For agent n∈A, n 6= n0, he will put zero effort as he does not receive any reward from the principal. In this case, only when reporting his observation ŷn = µ0 can minimize Ex∗ [(x∗− ŷn)2]. The expected value of Ex∗ [(x∗− ŷn)2] is\nEx∗ [(x∗− ŷn)2] = 1\n1/σ20 , n∈A, n 6= n0.\nStep 2. Truthful reporting of the cost parameter under COPE We first show that the agent n0 will truthfully reveals his cost type. We first rewrite functions\nπ, K, and S as follows.\nπ(θ̂n0 ,θ−n0) =θ̂n0 ·Q p(θ̂n0 ,θ−n0) + ∫ θ̄ θ̂n0 Qp(z,θ−n0)dz,\nK(θ̂n0 ,θ−n0) = [ Qp(θ̂n0 ,θ−n0) + 1/σ 2 0 ]2 · θ̂n0 , S(θ̂n0 ,θ−n0) = [ Qp(θ̂n0 ,θ−n0) + 1/σ 2 0 ] · θ̂n0 .\nThe expected payoff of the agent who has a cost type θn0 but reports θ̂n0 is: E{x∗,yn0 ,θ−n0} [ Ua(x∗, θ̂n0 , qn0 , yn0 , θn0 ,θ−n0) ] =Eθ−n0 [ π(θ̂n0 ,θ−n0)−K(θ̂n0 ,θ−n0) · 1\n1/σ20 + qn0 +S(θ̂n0 ,θ−n0)− qn0θn0\n] .\n(36)\nFor notation convenience, we define the function Uae :R× [θ, θ̄]×R+× [θ, θ̄]N →R+ as\nUae(θ̂n0 , qn0 , θn0 ,θ−n0) = [ π(θ̂n0 ,θ−n0)−K(θ̂n0 ,θ−n0)\n1\n1/σ20 + qn0 +S(θ̂n0 ,θ−n0)− qn0θn0\n] , (37)\nwhere θ−n0 are the random variables of all agents’ cost type except that of agent n0. By comparing (36) to (37), the expected payoff of the agent n is\nE{x∗,yn0 ,θ−n0} [ Ua(x∗, θ̂n0 , qn0 , yn0 , θn0 ,θ−n0) ] =Eθ−n0 [ Uae(θ̂n0 , qn0 , θn0 ,θ−n0) ] .\nBy the mean value theorem, we have:\nE [ Uae(θn0 , qn0 , θn0 ,θ−n0) ] −E [ Uae(θ̂n0 , qn0 , θn0 ,θ−n0) ] =E\n[ ∂Uae(η, qn0 , θn0 ,θ−n0)\n∂η\n] · (θn0 − θ̂n0),\n(38)\nwhere the expectation is taken with respect to θ−n0 , and η lies between θn0 and θ̂n0 .\nWe further have:\nEθ−n0\n[ ∂Uae(η, qn0 , θn0 ,θ−n0)\n∂η ] =Eθ−n0 [ ∂\n∂η\n( ηQp(η,θ−n0) + ∫ θ̄ η Qp(z,θ−n0)dz− [ Qp(η,θ−n0) + 1/σ 2 0 ]2 1/σ20 + qn0 η\n+ [ Qp(η,θ−n0) + 1/σ 2 0 ] η− qn0θn0 )] =Eθ−n0 [ 2η ∂Qp(η,θ−n0) ∂η − [ Qp(η,θ−n0) + 1/σ 2 0 ]2 1/σ20 + qn0\n− 2[Q p(η,θ−n0) + 1/σ 2 0] 1/σ20 + qn0 · ∂Q p(η,θ−n0) ∂η · η+\n[ Qp(η,θ−n0) + 1/σ 2 0 ]] =Eθ−n0 [( 1− Q p(η,θ−n0) + 1/σ 2 0\nqn0 + 1/σ 2 0\n) · ( 2η · ∂Q p(η,θ−n0)\n∂η +Qp(η,θ−n0) + 1/σ 2 0\n)] .\n(39)\nIf we have\n− ∂Qp(η,θ−n0)/\n( Qp(η,θ−n0) + 1/σ 2 0 ) ∂θn0/θn0 ≥ 1 2 , (40)\nthen we have\n2η · ∂Q p(η,θ−n0)\n∂η +Qp(η,θ−n0) + 1/σ 2 0 ≤ 0.\nLemma 1. If θn ∼Uniform[θ, θ̄] which is independent for every n∈A, then (40) is satisfied.\nProof : First consider the case N = 1. Since there is only one agent, hence the principal can only\nselect that agent. So Qp is simply Q of that agent:\nQ(θ) = 1/ √ θ+F (θ)/f(θ)− 1/σ20,\nhence\n−∂Q(θ) ∂θ\nθ\nQ(θ) + 1/σ20 =\n1 + ∂ ∂θ\n( F (θ)\nf(θ) ) 2[θ+F (θ)/f(θ)] 1√ θ+F (θ)/f(θ)\nθ 1/ √ θ+F (θ)/f(θ)\n= 1\n2\n1 + ∂ ∂θ\n( F (θ)\nf(θ) ) 1 + 1\nθ\n( F (θ)\nf(θ)\n) ≥ 1 2 ,\nwhere the final inequality holds for uniform distribution.\nWe now consider N > 1. Observe that the calculation above will be violated only when the cost\nparameter of some other agent is infinitesimally close to θn0 (since in that case, ∂Qp(θn0 ,θ−n0 )\n∂θn0 is\ndifferent from that calculated above). However given our assumptions that the distribution of θ\nfollows some known distribution such as uniform and normal distributions, and given that the\nnumber of agents N is finite, θn0 will be well separated from the cost types of all other agents with probability 1.\nAs the agent is selfish, he will exert effort qn0 to maximize his expected payoff. Hence, the agent’s exerted effort can be obtained by taking the first order derivative of (36) with respect to qn0 and setting it to zero, which is\n(1/σ20 + q p n0 )2 · θ̂n0 = (1/σ 2 0 + qn0) 2 · θn0 , (41)\nwhere qpn0 is the shorthand notation for Q p(θ̂n0 ,θ−n0).\nBased on (41), we have (i) if θ̂n0 > θn0 , q p n0 < qn0 , (ii) if θ̂n0 < θn0 , q p n0 > qn0 , and (iii) if θ̂n0 = θn0 ,\nqpn0 = qn0 .\nHence, If θ̂n0 > θn0 , the equation (39) is negative and (38) is positive. This inequality also holds for θ̂n0 < θn0 , by a similar argument. Therefore, agent n0 will truthfully report his own cost parameter.\nWe then show that an agent n ∈ A, n 6= n0 will truthfully reveal his cost type. Recall that the principal does not recruit and reward the agent n ∈ A, n 6= n0. Hence, the payment to the agent n∈A, n 6= n0 is zero. The we have\nEθ−n [ Uae(θn, qn, θn,θ−n) ] −Eθ−n [ Uae(θ̂n, qn, θn,θ−n) ] = 0,∀n∈A, n 6= n0,\nwhich shows that there is no difference between truthfully reporting cost type or not in terms of expected payoff for the agent n. Without loss of generality, we assume that in this case, the agent will truthfully report their cost types.\nStep 3. Incentivize agent to exert precisely the effort as desired by the principal\nunder COPE\nAs we have proved in Step 2 that the agent n0 would truthfully report his cost type (θ̂n = θn),\nthen we will show that the agent n0 exerts an effort level such that qn0 = q p n0 would maximize his expected payoff, which is given as\nE [ Uae(θn0 , q p n0 , θn0 ,θ−n0) ] = π(θn0 ,θ−n0)−K(θn0 ,θ−n0)\n1\n1/σ20 + qn0 +S(θn0 ,θ−n0)− qn0θn0 ,\n(42)\nwhere the expectation is taken with respect to θ−n0 .\nIt can be verified that (42) is concave in qn0 . Hence, by taking the first order derivative of (42)\nwith respect to qn0 , we have\n∂ ∂qn0 E [ Uae(θn0 , q p n0 , θn0 ,θ−n0) ] =\n[ 1/σ20 + q p n0\n1/σ20 + qn0\n]2 · θn0 − θn0 . (43)\nWe can verify that the value of (43) equals to zero only when qn0 = q p n0 . Hence, agent n0 will exert the effort as the principal desires to maximize his expected payoff. Then (34) is rewritten as\nŷn0 = µ0 · 1/σ20 + yn0 · qpn0\n1/σ20 + q p n0\n. (44)\nBecause the principal knows the value of µ0, σ 2 0, and q p n0 , she can infer the agent n0’s truth\nobservation yn0 from (44).\nStep 4. Maximize the principal’s expected utility under COPE Then we look at the expected payoff of the principal. The following lemma describes that COPE\nis the optimal mechanism that maximizes the principal’s expected utility. Lemma 2. The optimal predictor x̂= µ0·1/σ20+ ∑ n∈A yn·q p n\n1/σ20+ ∑ n∈A q p n\ndefined in COPE maximizes the principal’s expected utility, and the Bayes risk of the principal’s prediction is Bp ( qp )\n= 1 1/σ20+ ∑ n∈A q p n .\nProof : Recall that qpn =Q p(θn,θ−n). Given all agents’ observation y and agents’ exert effort q p,\nthe principal’s updated belief on the realization of x∗ can be expressed as\nx∗|(y,qp)∼N ( µ0 · 1/σ20 + ∑ n∈A yn · qpn\n1/σ20 + ∑ n∈A q p n\n, 1 1/σ20 + ∑ n∈A q p n\n) .\nTo maximize the expected utility for the prediction, the principal solves\nmax x̂\nE [ v− (x∗− x̂)2|(y,qp) ] = max\nx̂\n( v− { E [ x∗2|(y,qp) ] − 2x̂E [ x∗|(y,qp) ] + x̂2 }) = max\nx̂\n( v− [ x̂− µ0 · 1/σ20 + ∑\nn∈A yn · qpn 1/σ20 + ∑ n∈A q p n\n]2 − 1\n1/σ20 + ∑ n∈A q p n ) ≤ v− 1\n1/σ20 + ∑ n∈A q p n\nThe equality holds only when\nx̂= µ0 · 1/σ20 +\n∑ n∈A yn · qpn\n1/σ20 + ∑ n∈A q p n\n. (45)\nHence, the optimal predictor that maximizes the principal’s expected utility is\nx̂ ( y,qp ) = µ0 · 1/σ20 + ∑ n∈A yn · qpn\n1/σ20 + ∑ n∈A q p n\n(46)\nand the Bayes risk is\nBp ( qp )\n= inf x̂ E[(x∗− x̂)2] = 1 1/σ20 + ∑ n∈A q p n ,\nwhere the expectation is taken with respect to x∗ and y.\nRecall that under the linear cost function, the principal only recruits agent n0 to exert effort, in such case, qpn = 0, ∀n ∈ A, n 6= n0. Also recall that the principal can infer the true observation of the agent n0 through the function g : R→R, and such an observation is defined as yn0 = g(ŷn0) = ŷn0 + (ŷn0 −µ0)/(qpn0σ 2 0). Then putting back to (46) we can get the conclusion.\nWe then show that the desired effort level Qp(θn,θ−n) defined in (8) and the function π(θn,θ−n)\ndefined in (9) can maximize the principal’s expected payoff and satisfy BIC and BIR conditions.\nNotice that as the agent n0 exerts effort such that qn0 = q p n0 and reports ŷn0 = µ0·1/σ20+yn0 ·q p n0\n1/σ20+q p n0\n, the\nexpected payment function is reduced to\nE{x∗,yn0 ,θ−n0} [ R(x∗, yn0 , qn0 , θn0 ,θ−n0) ] =Eθ−n0 [ π(θn0 ,θ−n0)−K(θn0 ,θ−n0) ·\n1\n1/σ20 + qn0 +S(θn0 ,θ−n0)\n] =Eθ−n0 [ π(θn0 ,θ−n0) ] . (47)\nFor other agent n∈A, n 6= n0, as the principal does not require him to do the observation, we first assume that the expected payment to him is as follows,\nE{x∗,yn0 ,θ−n0}[R(x ∗, yn, θn,θ−n)] =Eθ−n0\n[ π(θn,θ−n) ] ,∀n∈A, n 6= n0. (48)\nLater we will show that π(θn,θ−n) = 0,∀n 6= n0.\nThe expected payoff of agent n∈A is\nEθ−n [ Uae ( θ̂n0 , q p n0 , θn0 ,θ−n0 )] =Eθ−n [ π(θ̂n,θ−n)− θnQp(θ̂n,θ−n) ] ,\nwhere qpn0 is the shorthand notation of Q p(θ̂n,θ−n). For notation convenience, we adopt\nUae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn ) in the later proof of Theorem 1, where the function Uae is rewritten as\nEθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] =Eθ−n [ π(θ̂n,θ−n)− θnQp(θ̂n,θ−n) ] . (49)\nCorrespondingly, BIC and BIR conditions, i.e., (5) and (6), can be rewritten as\nEθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥Eθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] , ∀θ̂n 6= θn, (50) Eθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥ 0, ∀θn ∈ [θ, θ̄]. (51)\nBase on Lemma 2, (47), and (48), the expected payoff of the principal is\nEx∗,y,θ[U p(x∗,qp,y, θ̂)] =−Bp ( qp ) −Ex∗,y,θ [∑ n∈A R(x∗, yn, θn,θ−n) ]\n=− 1 1/σ20 + ∑ n∈A q p n −Eθ [∑ n∈A π(θn,θ−n) ] .\nRecall that qpn =Q p(θn,θ−n), the principal’s optimal problem defined in (7) can be rewritten as\nsup {Qp(θ),π(θ)},∀θn∈θ,∀n∈A\nE[U p(x∗,qp,y, θ̂)],\nsubject to : BIC and BIR in (50) and (51).\n(52)\nIn the following lemmas, we characterize an equivalent formulation for the feasible region defined by BIC and BIR. Using these lemmas, we show that Qp(θn,θ−n) defined in (8) and π(θn,θ−n) defined in (9) are the optimal solution that solves the principal’s problem in (52).\nLemma 3. The solution of (52) is feasible if and only if it satisfies the following conditions for all θn ∈ [θ, θ̄], ∀n∈A:\n• the expected payoff of agent n is\nEθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] =Eθ−n [∫ θ̄ θn Qp(x,θ−n)dx ] ; (53)\n• Qp(θn,θ−n) is non-increasing in θn.\nProof : The proof of Lemma 3 is as follows. We first show that BIC and BIR imply the condition\nin (53).\nNotice that the first derivative of (49) is:\n∂Eθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )]\n∂θn =Eθ−n\n[ −Qp(θ̂n,θ−n) ] ≤ 0. (54)\nThen, for any θ1n > θ 2 n, we have Eθ−n [ Uae(π(θ1n,θ−n),Q p(θ1n,θ−n), θ 1 n) ] ≤Eθ−n [ Uae(π(θ1n,θ−n),Q p(θ1n,θ−n), θ 2 n) ]\n≤Eθ−n [ Uae(π(θ2n,θ−n),Q p(θ2n,θ−n), θ 2 n) ] ;\n(55)\nwhere the first inequality is because (54) and the second is from the BIC condition defined in (50). In other words, for the agent n∈A whose cost parameter θ≤ θn ≤ θ̄, we have\nEθ−n [ Uae(π(θ̄,θ−n),Q p(θ̄,θ−n), θ̄) ] ≤Eθ−n [ Uae(π(θn,θ−n),Q p(θn,θ−n), θn) ]\n≤Eθ−n [ Uae(π(θ,θ−n),Q p(θ,θ−n), θ) ] . (56)\nRecall that the BIR condition is\nEθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥ 0, ∀θn ∈ [θ, θ̄], (57)\nwhich implies that, for the agent n ∈ A with any value θn ∈ [θ, θ̄], his expected payoff should at least be zero. Then the expected payoff of the agent n with cost parameter θ̄ must be binding at zero. Otherwise, the principal can reduce the π(θ̄,θ−n) by a small value of δ > 0, which does not violate the constraint of (57) but raises the principal’s expected payoff. Hence, we have\nEθ−n [ Uae(π(θ̄,θ−n),Q p(θ̄,θ−n), θ̄) ] = 0. (58)\nLet Uae(θn,θ−n) =U ae ( π(θn,θ−n),Q p(θn,θ−n), θn ) . From the BIC condition, we have\nEθ−n [ Uae(θn,θ−n) ] = max\nθ̂n\nEθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] .\nBy using the envelope theorem, we have:\n∂Eθ−n [ Uae(θn,θ−n) ] ∂θn = ∂Eθ−n [ Uae(π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn) ] ∂θn ∣∣∣∣∣ θ̂n=θn =Eθ−n [ −Qp(θn,θ−n) ] ,\n(59)\nwhere θn is a parameter. By integrating both sides from the value of θn to θ̄ and using (58) and the assumption that the random variable θn of the agent n is independent for every n∈A , we get\nEθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] =Eθ−n [∫ θ̄ θn Qp(x,θ−n)dx ] . (60)\nWe prove that Qp(θn,θn) is nonincreasing in θn by contradiction. Let pn be the shorthand\nnotation for π(θn,θ−n). Suppose for any θ 1 n > θ 2 n, we have Q p(θ1n,θ−n)>Q p(θ2n,θ−n). Because\n∂2Uae ( pn, q p n, θn ) ∂qpn∂θn =−1< 0, (61)\n∂2Uae ( pn, q p n, θn ) ∂qpn 2 = 0, (62)\nwe have\n0 = ∂Uae\n( pn, q p n, θ 1 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ1n,θ−n)\n= ∂Uae\n( pn, q p n, θ 1 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n)\n< ∂Uae\n( pn, q p n, θ 2 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n) , (63)\nwhere the first equality is due to BIC when the agent n’s cost parameter θn has the value of θ 1 n, the second equality is due to (62), and the inequality is due to (61).\nHowever, based on the BIC condition, if the agent n’s cost parameter θn has the value of θ 2 n,\nthen we should have\n∂Uae ( pn, q p n, θ 2 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n) = 0,\nwhich holds true for all scalar values of pn. Hence, for any θ 1 n > θ 2 n, Q p(θ1n,θ−n)≤Qp(θ2n,θ−n).\nThen we need to prove that (53) implies BIC and BIR defined in (50) and (51).\nBIR is verified by putting θn back to (53). Besides, by putting θn = θ̄ back to (53), we have\nEθ−n [ Uae ( π(θ̄,θ−n),Q p(θ̄,θ−n), θ̄ )] = 0.\nThen we prove that (53) implies BIC. Notice that we have:\nEθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )]\n1 =Eθ−n [ − ∫ θ̄ θn ∂Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), z ) ∂z dz ] 2 =Eθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θ̂n ) − ∫ θ̂n θn ∂Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), z ) ∂z dz\n] 3 =Eθ−n [∫ θ̄ θ̂n Qp(η,θ−n)dη− ∫ θ̂n θn ∂Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), z ) ∂z dz\n] 4 =Eθ−n [ − ∫ θn θ̄ Qp(η,θ−n)dη− ∫ θ̂n θn Qp(η,θ−n)dη+ ∫ θ̂n θn Qp(θ̂n,θ−n)dz\n] 5 =Eθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn ) + ∫ θ̂ θn ( Qp(θ̂n,θ−n)−Qp(η,θ−n) ) dη ] ,\nwhere the third equality and the fifth equality are obtained by (53).\nIf θ̂n > θn, then the above equation is non-positive (because Q p(η,θ−n) is non-increasing in η),\nhence\nEθ−n [ Uae(π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn) ] <Eθ−n [ Uae(π(θn,θ−n),Q p(θn,θ−n), θn) ] .\nThis inequality also holds for θ̂n < θn by a similar argument. Therefore, the two condition imply BIC.\nThen based on Lemma 3, we have the following Lemma.\nLemma 4. The optimisation problem in (52) has the following equivalent formulation:\nmax {Qp(θ)},∀θn∈θ\nEθ [ − 1\n1/σ20 + ∑ n∈AQ p(θn,θ−n) − ∑ n∈A Qp(θn,θ−n) · θn− ∑ n∈A Qp(θn,θ−n) · F (θn) f(θn) ] ,\ns.t. Qp(θn,θ−n) is nonincreasing in θn, (64)\nwhere the expectation is taken with respect to θ.\nProof : The proof of Lemma 4 is as follows. The expected payoff of the principal can be written\nas: Eθ [ − 1\n1/σ20 + ∑ n∈AQ p(θn,θ−n) − ∑ n∈A Qp(θn,θ−n) · θn− ∑ n∈A Ua ( π(θn,θ−n),Q p(θn,θ−n), θn )]\n=Eθ [ − 1\n1/σ20 + ∑ n∈AQ p(θn,θ−n) − ∑ n∈A Qp(θn,θ−n) · θn− ∑ n∈A ∫ θ̄ θn Qp(x,θ−n)dx ] ,\n(65)\nwhere the expectation is taken with respect to θ. Notice that Eθn [∫ θ̄\nθn\nQp(x,θ−n)dx ] = ∫ θ̄ θ ∫ θ̄ z Qp(x,θ−n)dx · f(z)dz = ∫ θ̄ θ F (z)Qp(z,θ−n)dz\n= ∫ θ̄ θ F (z) f(z) Qp(z,θ−n)f(z)dz =Eθn [F (θn) f(θn) Qp(θn,θ−n) ] ,\nwhere the first equation is obtained by using integration by parts. Then by applying the above equation to (126) and the fact that {θn}n∈A are assumed to be random, independently and identically distributed on support [θ, θ̄], we can get the conclusion.\nBased on Lemma 4, the principal’s problem reduces to choosing the desired effort qpn = Qp(θn,θ−n) for each agent n ∈A. We first consider the problem in (64) without the constraint. If the optimal solution to this unconstrained problem is increasing, then it is also an optimal solution to the constrained problem.\nLemma 5. Qp(θn,θ−n) defined in (8) and π(θn,θ−n) defined in (9) are the optimal solution that solves the principal’s problem in (52)\nProof : We first prove that for the agent ∀n∈A,\nQp(θn,θ−n) =\n{ max{1/ √ γ(θn)− 1/σ20,0}, if n0 = argminm∈Aθm,\n0, otherwise,\nis the optimal solution of (64) by contradiction.\nAs qpn =Q p(θn,θ−n), the problem in (64) is equivalent to\nmin qp≥0\n1 1/σ20 + ∑ n∈A q p n + ∑ n∈A qpn · γ(θn),\ns.t. qpn is nonincreasing in θn, (66)\nwhere γ(θn) = θn +F (θn)/f(θn).\nWithout loss of generality, let γ(θ1)≤ γ(θ2) . . .≤ γ(θN). If the principal’s desired efforts from all\nagents are positive, then the solution is\nqp1 = q p,∗ 1 , q p 2 = q p,∗ 2 , . . . , q p N = q p,∗ N . (67)\nSuppose that there is another solution such that qp,†1 = q p 1 + q p j , qp,†i = q p i , ∀i 6= 1, j, i∈A,\nqp,†j = 0.\n(68)\nThen we can verify that\n∑ n∈A qp,†n = ∑ n∈A qpn and ∑ n∈A ( qp,†n · γ(θn) ) ≤ ∑ n∈A ( qpn · γ(θn) ) .\nHence, (67) is not an optimal solution. Then we let qpn = 0,∀n> 1, the problem in (66) becomes\nmin qp1\n1\n1/σ20 + q p 1\n+ qp1 · γ(θ1),\ns.t. qp1 ≥ 0. (69)\nBy solving the above problem we can get that qp1 = max{1/ √ γ(θ1)−1/σ20,0}. As we define n0 =\nargminm∈Aθm and the assumption that F is log-concave in [θ, θ̄], we have q p n0 = max{1/ √ γ(θn0)− 1/σ20,0}.\nAccording to (53), we have\nEθ−n [ π(θn,θ−n)−Qp(θn,θ−n) · θn ] =Eθ−n [∫ θ̄ θn Qp(x,θ−n)dx ] .\nThen the optimal payment function given the agent n0 and 1/ √ γ(θn0)− 1/σ20 ≥ 0 is\nπ(θn0) = θn0/ √ γ(θn0)− θn0/σ 2 0 + ∫ θ̄ θn0 ( 1√ γ(z) − 1 σ20 ) dz = θn0/ √ γ(θn0)− θ̄/σ 2 0 + ∫ θ̄ θn0 1√ γ(z) dz,\nand the payment will be zero if 1/ √ γ(θn0)− 1/σ20 < 0.\nFor other agents, the payments will be zero as they are not involved in the observation and\nprediction.\nAs in Theorem 1 , we assume that θn ∼ Uniform[θ, θ̄], which is independent for every n ∈ A.\nPutting the expression of F and f back to the above equations, we can have the conclusion."
    }, {
      "heading" : "B. Proof of Theorem 2: Quadratic Costs",
      "text" : "The proof is similar to that in Section A. The difference is as follows.\nFirst, the function π : [θ, θ̄]N →R+, K,S : [θ, θ̄]N ×R+→R+ are defined as\nπ(θ̂n,θ−n) = 1 2 · [ θ̂n · [ Qp(θ̂n,θ−n) ]2 + ∫ θ̄ θ̂n ([ Qp(z,θ−n) ]2) dz ] , (70)\nK(θ̂n,θ−n) = [ Qp(θ̂n,θ−n) + 1/σ 2 0 ]2 θ̂n ·Qp(θ̂n,θ−n), (71)\nS(θ̂n,θ−n) = [ Qp(θ̂n,θ−n) + 1/σ 2 0 ] θ̂n ·Qp(θ̂n,θ−n). (72)\nStep 1. Truthful reporting of observation under COPE We will analyze the strategies of the agent n, ∀n∈A. We will show that the agent n will choose\nŷn = µ0 · 1/σ20 + yn · qn\n1/σ20 + qn (73)\nto maximize his expected payoff given his exerting effort qn and own observation yn.\nAs π(θ̂n,θ−n), K(θ̂n,θ−n) and S(θ̂n,θ−n) are independent of ŷn and the value of calculated by K(θ̂n,θ−n) is always positive. Hence, when the agent n makes reporting observation strategy to maximize his expected payoff, i.e.,\nŷn ∈ argmaxE [ π(θ̂n,θ−n)−K(θ̂n,θ−n)(x∗− ŷn)2 +S(θ̂n,θ−n) ] −C ( qn, θn ) ,\nwhere the expectation is taken with respect to x∗ and cost parameters θ−n = [θ1, . . . , θn−1, θn+1, . . . , θN ] T except agent n, it is equivalent for the agent n to choose reporting strategy such that\nŷn ∈ argminEx∗ [(x∗− ŷn)2]. (74)\nThe value of Ex∗ [(x∗ − ŷn)2] is minimized when ŷn = µ0·1/σ 2 0+yn·qn\n1/σ20+qn . The expected value in this\ncase is\nEx∗ [(x∗− ŷn)2] = 1\n1/σ20 + qn .\nStep 2. Truthful reporting of cost parameter under COPE We will show that the agent n, ∀n∈A will truthfully reveals his cost type. The expected payoff\nof the agent who has a cost type is θn but reports θ̂n is: E{x∗,yn,θ−n} [ Ua(x∗, θ̂n, qn, yn, θn,θ−n) ] =Eθ−n [ π(θ̂n,θ−n)−K(θ̂n,θ−n) · 1\n1/σ20 + qn +S(θ̂n,θ−n)−\n1 2 θnq 2 n\n] .\n(75)\nFor notation convenience, we define\nUa(θ̂n, qn, θn,θ−n) = [ π(θ̂n,θ−n)− K(θ̂n,θ−n)\n1/σ20 + qn +S(θ̂n,θ−n)−\n1 2 θnq 2 n ] By the mean value theorem, we have:\nE [ Ua(θn, qn, θn,θ−n) ] −E [ Ua(θ̂n, qn, θn,θ−n) ] =Eθ−n [∂Ua(η, qn, θn,θ−n) ∂η ] (θn− θ̂n), (76)\nwhere the expectation is taken with respect to θ−n, and η lies between θn and θ̂n.\nWe further have\nEθ−n\n[ ∂Ua(η, qn, θn,θ−n)\n∂η ] =Eθ−n0 [ ∂\n∂η\n( 1\n2 · η [ Qp(η,θ−n) ]2 + ∫ θ̄ η [ Qp(z,θ−n) ]2 dz\n+ [ Qp(η,θ−n0) + 1/σ 2 0 ] η [ Qp(η,θ−n) ] − [ Qp(η,θ−n) + 1/σ 2 0 ]2 1/σ20 + qn η [ Qp(η,θ−n) ] − 1 2 θnq 2 n )]\n=Eθ−n\n[( 1− Q p(η,θ−n) + 1/σ 2 0\nqn + 1/σ20\n)( 2Qp(η,θ−n) · η · ∂Qp(η,θ−n) ∂η + [ Qp(η,θ−n) + 1/σ 2 0 ] ·Qp(η,θ−n)\n+ [ Qp(η,θ−n) + 1/σ 2 0 ] · η · ∂\n∂η\n[ Qp(η,θ−n) ]) − 1\n2\n[ Qp(η,θ−n) ]2] . (77)\nWe can check that if\n−∂Q p(η,θ−n)/(Q p(η,θ−n) + 1/σ 2 0) ∂θn/θn ≥ 1 2 ,∀n∈A, (78)\nwe have\n2Qp(η,θ−n) · η · ∂Qp(η,θ−n) ∂η + [ Qp(η,θ−n) + 1/σ 2 0 ] ·Qp(η,θ−n)≤ 0.\nLater we will show that Qp(η,θ−n) is non-increasing in η, ∀n∈A (i.e., Lemma 7), hence,\n2Qp(η,θ−n)η ∂Qp(η,θ−n) ∂η + [ Qp(η,θ−n) + 1/σ 2 0 ] Qp(η,θ−n) + η\nσ20\n∂\n∂η\n[ Qp(η,θ−n) ] ≤ 0.\nLemma 6. If θn ∼Uniform[θ, θ̄] which is independent for every n∈A, then (78) is satisfied.\nProof : First, for an agent n∈A,\nQp(θn,θ−n) = 1\nθn + F (θn)\nf(θn)\n· 1[ W (θ) ]2 , (79) where the function W : [θ, θ̄]N →R+ is the solution of the below equation:\n[ W (θ) ]3− 1 σ20 · [ W (θ) ]2−∑ m∈A\n1\nθm + F (θm)\nf(θm)\n= 0. (80)\nhence\n−∂Q p(θn,θ−n)\n∂θn\nθn Qp(θn,θ−n) + 1/σ20\n≥ 1 + ∂ ∂θ\n( F (θ)\nf(θ) ) 1 + 1\nθ\n( F (θ)\nf(θ)\n) · 1 1 + [W (θ)] 2\nσ20\n( θn + F (θn)\nf(θn) ) ≥ 1\n2 ,\nwhere the final inequality holds for uniform distribution.\nAs the agent is selfish, he will exert effort qn0 to maximize his expected payoff. Hence, the agent’s exerted effort can be obtained by taking the first order derivative of (75) with respect to qn and setting it to zero, which is\n(1/σ20 + q p n) 2 · qpn · θ̂n = (1/σ20 + qn)2 · qn · θn, (81)\nwhere qpn is a shorthand for Q p(θ̂n,θ−n).\nBased on (81), we have (i) if θ̂n > θn, q p n < qn, (ii) if θ̂n < θn, q p n > qn, and (iii) if θ̂n = θn, q p n = qn. Then if θ̂n > θn, the equation (77) is negative and (76) is positive. This inequality also holds for\nθ̂n < θn, by a similar argument. Therefore, the agent n will truthfully report his own cost type.\nStep 3. Incentivize agents to exert precisely the efforts as desired by the principal\nunder COPE\nThen we will show that an agent n, ∀n ∈A exerts effort such that qn = qpn would maximize his\nexpected payoff as follows. E{x∗,yn,θ−n} [ Ua(x∗, qn, yn, θn,θ−n) ] =Eθ−n [ π(θn,θ−n)−K(θn,θ−n) ·\n1\n1/σ20 + qn +S(θn,θ−n)−\n1 2 θnq 2 n\n] ,\n(82)\nwhere the expectation is taken with respect to θ−n, x ∗, and yn.\nIt can be verified that (82) is concave in qn. Hence, by taking the first order derivative of (82)\nwith respect to qn, we have\n∂ ∂qn E [ Ua(x∗, qn, yn, θn,θ−n) ] =\n[ 1/σ20 + q p n\n1/σ20 + qn\n]2 · θn · qpn− θn0 · qn. (83)\nWe can verify that the value of (83) equals to zero only when qn = q p n. Hence, agent n will exert\nthe effort as the principal desires to maximize his expected payoff. Then (73) is rewritten as\nŷn = µ0 · 1/σ20 + yn · qpn\n1/σ20 + q p n\n. (84)\nBecause the principal knows the value of µ0, σ 2 0, and q p n, she can infer the agent n’s truth\nobservation yn from (84).\nStep 4. Maximize the principal’s expected utility under COPE Then we look at the expected payoff of the principal. We will show that the desired effort level Qp(θn,θ−n) defined in (12) and the function π(θn,θ−n) defined in (70) can maximize the principal’s expected payoff and satisfy BIC and BIR condition.\nNotice that when an agent n, ∀n∈A exerts effort such that qn = qpn and reports ŷn = µ0·1/σ20+yn·q p n\n1/σ20+q p n\n,\nthe expected payment function is reduced to\nE{x∗,yn,θ−n} [ R(x∗, yn, qn, θn,θ−n) ]\n=Eθ−n [ π(θn,θ−n)−K(θn,θ−n)(x∗− ŷn)2 +S(θn,θ−n) ] =Eθ−n [ π(θn,θ−n) ] . (85)\nThe expected payoff of the agent n is rewritten as\nEθ−n [ Ua ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] =Eθ−n [ π(θ̂n,θ−n)− 1 2 θn · [ Qp(θ̂n,θ−n) ]2] , (86)\nand the BIC and BIR conditions, i.e., (5) and (6), can be rewritten as\nEθ−n [ Ua ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥Eθ−n [ Ua ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] , ∀θ̂n 6= θn, (87)\nEθ−n [ Ua ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥ 0, ∀θn. (88)\nBase on Lemma 2 and (85), the expected payoff of the principal is\nEx∗,y,θ[U p(x∗,qp,y, θ̂)] =−Bp ( qp ) −Ex∗,y,θ [∑ n∈A R(x∗, yn, θn,θ−n) ]\n=− 1 1/σ20 + ∑ n∈A q p n −Eθ [∑ n∈A π(θn,θ−n) ] .\nRecall that qpn =Q p(θn,θ−n), the principal’s optimal problem defined in (7) can be rewritten as\nsup {Qp(θ),π(θ)},∀θn∈θ,∀n∈A\nE[U p(x∗,qp,y, θ̂)],\nsubject to : BIC and BIR in (87) and (88).\n(89)\nIn the following lemmas, we characterize an equivalent formulation for the feasible region defined by BIC and BIR. Using these lemmas, we show that Qp(θn,θ−n) defined in (12) and π(θn,θ−n) defined in (70) are the optimal solution that solves the principal’s problem in (89).\nLemma 7. The solution of (89) is feasible if and only if it satisfies the following conditions for all θn ∈ [θ, θ̄]:\n• The expected payoff of the agent n, ∀n∈A is\nEθ−n [ Ua ( π(θn,θ−n),Q p(θn,θ−n), θn )] = 1\n2 Eθ−n [∫ θ̄ θn [ Qp(x,θ−n) ]2 dx ] , (90)\n• Qp(θn,θ−n) is non-increasing in θn.\nProof : The proof of Lemma 7 is as follows. We first show that BIC and BIR imply the condition\nin (90).\nNotice that the first derivative of (86) is:\n∂Eθ−n [ Ua ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )]\n∂θn =Eθ−n\n[ − 1\n2\n[ Qp(θ̂n,θ−n) ]2]≤ 0. (91) Then, for any θ1n > θ 2 n, we have\nEθ−n [ Ua(π(θ1n,θ−n),Q p(θ1n,θ−n), θ 1 n) ] ≤Eθ−n [ Ua(π(θ1n,θ−n),Q p(θ1n,θ−n), θ 2 n) ]\n≤Eθ−n [ Ua(π(θ2n,θ−n),Q p(θ2n,θ−n), θ 2 n) ] ,\n(92)\nwhere the first inequality is due to (91) and the second is due to the BIC condition defined in (87).\nRecall that the BIR condition is\nEθ−n [ Ua ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥ 0, ∀θn ∈ [θ, θ̄], (93)\nwhich implies that, for an agent n ∈ A with any value θn ∈ [θ, θ̄], his expected payoff should be nonnegative. Then the expected payoff of the agent n with cost parameter θ̄ must be binding at zero. Otherwise, the principal can raise the π(θ̄,θ−n) by a small value of δ > 0, which does not violate the constraint of (93) but raises the principal’s expected payoff. Hence, we have\nEθ−n [ Ua(π(θ̄,θ−n),Q p(θ̄,θ−n), θ̄) ] = 0. (94)\nLet Ua(θn,θ−n) =U a ( π(θn,θ−n),Q p(θn,θ−n), θn ) . From BIC condition, we have\nEθ−n [ Ua(θn,θ−n) ] = max\nθ̂n\nEθ−n [ Ua ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] .\nBy using the envelope theorem, we have: ∂Eθ−n [ Ua(θn,θ−n) ] ∂θn = ∂Eθ−n [ Ua(π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn) ] ∂θn ∣∣∣∣∣ θ̂n=θn =Eθ−n [ − 1 2 [ Qp(θn,θ−n) ]2] , where θn is a parameter. By integrating both sides from the value of θn to θ̄ and using (94), we get\nEθ−n [ Ua ( π(θn,θ−n),Q p(θn,θ−n), θn )] = 1\n2 Eθ−n [∫ θ̄ θn [ Qp(x,θ−n) ]2 dx ] (95)\nWe prove thatQp(θn,θn) is nonincreasing in θn by contradiction. Let pn as the shorthand notation\nfor π(θn,θ−n). Suppose for any θ 1 n > θ 2 n, we have Q p(θ1n,θ−n)>Q p(θ2n,θ−n). Because\n∂2Ua ( pn, q p n, θn ) ∂qpn∂θn =−qpn < 0, and (96)\n∂2Ua ( pn, q p n, θn ) ∂qpn 2 =−θn ≤ 0, (97)\nwe have\n0 = ∂Ua\n( pn, q p n, θ 1 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ1n,θ−n)\n≤ ∂Ua\n( pn, q p n, θ 1 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n)\n< ∂Ua\n( pn, q p n, θ 2 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n) ,\nwhere the first equality is due to BIC when the agent n’s cost parameter θn has the value of θ 1 n, the second equality is due to(96), and the inequality is due to (97).\nHowever, based on the BIC condition, if the agent n’s cost parameter θn has the value of θ 2 n,\nthen we should have\n∂Ua ( pn, q p n, θ 2 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n) = 0,\nwhich holds true for all scalar value of pn. Hence, for any θ 1 n > θ 2 n, Q p(θ1n,θ−n)≤Qp(θ2n,θ−n).\nThen we need to prove that (90) implies BIC and BIR defined in (87) and (88). Notice that we\nhave:\nEθ−n [ Ua ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )]\n=Eθ−n [ − ∫ θ̄ θn ∂Ua ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), z ) ∂z dz ] =Eθ−n [ 1\n2 ∫ θ̄ θ̂n [ Qp(η,θ−n) ]2 dη− ∫ θ̂n θn ∂Ua ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), z ) ∂z dz ] =Eθ−n [ − 1\n2 ∫ θn θ̄ [ Qp(η,θ−n) ]2 dη− 1 2 ∫ θ̂n θn [ Qp(η,θ−n) ]2 dη+ 1 2 ∫ θ̂n θn [ Qp(θ̂n,θ−n) ]2 dz ] =Eθ−n [ Ua ( π(θn,θ−n),Q p(θn,θ−n), θn ) + 1\n2 ∫ θ̂ θn ([ Qp(θ̂n,θ−n) ]2− [Qp(η,θ−n)]2)dη], where the second equality and the forth equality is obtained by (90).\nIf θ̂n > θn, then the above equation is non-positive (because Q p(η,θ−n) is non-increasing in η),\nhence\nEθ−n [ Ua(π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn) ] <Eθ−n [ Ua(π(θn,θ−n),Q p(θn,θ−n), θn) ] .\nThis inequality also holds for θ̂n < θn by a similar argument. Therefore, the two condition imply BIC.\nBIR is verified by putting θn back to (90). Then based on Lemma 7, we have the following lemma.\nLemma 8. The optimisation problem in (89) has the following equivalent formulation:\nmax {Qp(θ)},∀θn∈θ\nEθ [ − 1\n1/σ20 + ∑ n∈AQ p(θn,θ−n) − 1 2 ∑ n∈A [ Qp(θn,θ−n) ]2 θn− 1 2 ∑ n∈A [ Qp(θn,θ−n) ]2F (θn) f(θn) ] ,\ns.t. Qp(θn,θ−n) is nonincreasing in θn. (98)\nProof : The proof of Lemma 8 is as follows. The expected payoff of the principal can be written\nas: Eθ [ − 1\n1/σ20 + ∑ n∈AQ p(θn,θ−n) − 1 2 ∑ n∈A [ Qp(θn,θ−n) ]2 · θn−∑ n∈A Ua ( π(θn,θ−n),Q p(θn,θ−n), θn )]\n=Eθ [ − 1\n1/σ20 + ∑ n∈AQ p(θn,θ−n) − 1 2 ∑ n∈A [ Qp(θn,θ−n) ]2− 1 2 ∑ n∈A ∫ θ̄ θn [ Qp(x,θ−n) ]2 dx ] Using integration by parts and Lemma 7, we can get the conclusion.\nBased on Lemma 8, the principal’s problem thus reduces to choosing the desired effort\nQp(θn,θ−n) for each agent n∈A.\nLet qpn =Q p(θn,θ−n) and\nM ( qp1, . . . , q p N ) =− 1\n1/σ20 + ∑ n∈A q p n − 1 2 ∑ n∈A [ qpn ]2 · θn− 1 2 ∑ n∈A [ qpn ]2F (θn) f(θn) .\nLet G= [∂2M/∂qpi ∂q p j ] be the matrix of second order derivatives, and it is a symmetric matrix with negative diagonal terms as\n∂2M\n∂qpi ∂q p j =− 2[ 1/σ20 + ∑ n∈A q p n ]3 , j 6= i, ∂2M ∂qpi 2 =− 2[ 1/σ20 + ∑ n∈A q p n\n]3 − θi− F (θi)f(θi) . As we can verify that, for k = 1, . . . ,N , the kth leading principal minors of G alternate in sign, hence G is negative definite and M is strictly concave. Thus, the principal’s desired effort levels from agents qpn =Q p(θn,θ−n), ∀n∈N are the solution of the below equations:\n1[ 1/σ20 + ∑ n∈A q p n ]2 −Qp(θn,θ−n) · θn−Qp(θn,θ−n) · F (θn)f(θn) = 0, n= 1,2, . . . ,N (99)\nUsing Cramer’s rule and the assumption that the function F is log concave in θ , we can verify\nthat\n∂Qp(θn,θ−n)\n∂θn =−∂ 2M/∂qpn∂θn ∂2M/∂qpn 2 ≤ 0, (100)\nwhich shows that Qp(θn,θ−n) derived from (99) is non-increasing in θn, so that it is the feasible solution of (98).\nThe solution of (99) is\nQp(θn,θ−n) = 1\nθn + F (θn)\nf(θn)\n· 1[ W (θ) ]2 , (101) where the function W : [θ, θ̄]N →R+ is the solution of the below equation:\n[ W (θ) ]3− 1 σ20 · [ W (θ) ]2−∑ m∈A\n1\nθm + F (θm)\nf(θm)\n= 0. (102)\nThe real root of the above cubic equation is as follows.\nW (θ̂) = 1\n3σ20 + 3 √√√√ 1 27σ60 + 1 2 [∑ m∈A\n1\nθ̂m + F (θ̂m)\nf(θ̂m)\n] + √ λ(θ̂) + 3 √√√√ 1 27σ60 + 1 2 [∑ m∈A\n1\nθ̂m + F (θ̂m)\nf(θ̂m)\n] − √ λ(θ̂),\n(103)\nwhere function λ : [θ, θ̄]N →R+ is given as\nλ(θ̂) = 1\n27σ60 [∑ m∈A\n1\nθ̂m + F (θ̂m)\nf(θ̂m)\n] + 1\n4 [∑ m∈A\n1\nθ̂m + F (θ̂m)\nf(θ̂m)\n]2 .\nAccording to (90), we have\nEθ−n [ π(θn,θ−n)− 1\n2 · [Qp(θn,θ−n)]2 · θn\n] = 1\n2 Eθ−n [∫ θ̄ θn [ Qp(x,θ−n) ]2 dx ] .\nFrom the above equation, we can derive the optimal payment function as given in (70)."
    }, {
      "heading" : "C. Proof of Theorem 3: General Setting",
      "text" : "The proof will proceed in four steps. The first three steps show that our mechanism incentivizes the\nagents to be truthful, and the fourth step proves optimality of our mechanism. First, we show that\nirrespective of what an agent reports as his cost parameter, and irrespective of the effort he exerts,\nthe agent is always incentivized to report his true observation. We follow this up and show that\nirrespective of the effort that an agent exerts, he is always incentivized to report his cost parameter\ncorrectly. The third step completes the proof of truthfulness, showing that under truthful reporting\nof the cost parameter and the observation, in our mechanism, an agent is always incentivized to\nexert precisely the effort as desired by the principal. Finally, we show that among all mechanisms\nthat ensure truthful reports, our mechanism maximizes the principal’s expected utility.\nStep 1. Truthful reporting of observation under COPE\nWe first show that the agent will choose\nyn = arg infŷn(yn)E [ `a(x∗, ŷn) ] . (104)\nto maximize his expected payoff, given his exerting effort qn and own observation yn.\nAs shown in (22), π(θ̂n,θ−n), K(θ̂n,θ−n) and S(θ̂n,θ−n) are independent of ŷn. Moreover, the value ofK(θ̂n,θ−n) is always positive. Hence, when the agent n determines his reporting observation strategy to maximize his expected payoff, i.e.,\nŷn ∈ argmaxE [ π(θ̂n,θ−n)−K(θ̂n,θ−n) · `a(x∗, ŷn) +S(θ̂n,θ−n) ] −C ( qn, θn ) ,\nwhere the expectation is taken with respect to x∗ and cost parameters θ−n = [θ1, . . . , θn−1, θn+1, . . . , θN ] T of all the agents except agent n, it is equivalent for the agent n to choose the reporting strategy such that\nŷn ∈ argminEx∗ [`a(x∗, ŷn)]. (105)\nAccording to (18) and (19) in Section 4.2.1, the value of Ex∗ [`a(x∗, ŷn)] is minimized when ŷn = yn,\nand the expected value is\nEx∗ [`a(x∗, ŷn)] =Ba ( qn ) .\nStep 2. Truthful reporting of cost parameter under COPE\nWe now show that the agent will truthfully reveals its cost type. The expected payoff of the\nagent whose cost type is θn0 but reports θ̂n0 is: E{x∗,yn,θ−n} [ Ua(x∗, θ̂n, qn, yn, θn,θ−n) ] =Eθ−n [ π(θ̂n,θ−n)−K(θ̂n,θ−n) ·Ba ( qn ) +S(θ̂n,θn)−C(qn, θn) ] .\n(106)\nFor notational convenience, we define the function Uae :R× [θ, θ̄]×R+× [θ, θ̄]N →R+ as\nUae(θ̂n, qn, θn,θ−n) = [ π(θ̂n,θ−n)−K(θ̂n,θ−n) ·Ba ( qn ) +S(θ̂n,θn)−C(qn, θn) ] , (107)\nwhere θ−n are the random variables of all agents cost type except that of agent n. By comparing (106) to (107), the expected payoff of the agent n is\nE{x∗,yn,θ−n} [ Ua(x∗, θ̂n, qn, yn, θn,θ−n) ] =Eθ−n [ Uae(θ̂n, qn, θn,θ−n) ] .\nThen, by the mean value theorem, we have:\nEθ−n [ Uae(θn, θn,θ−n) ] −Eθ−n [ Uae(θ̂n, θn,θ−n) ] =Eθ−n [ ∂Uae(η, θn,θ−n)\n∂η\n] · (θn− θ̂n)\nWe further have:\nEθ−n\n[ ∂Uae(η, θn,θ−n)\n∂η ] =E [ Ba(qn)\ndBa(qpn)/dq p n\n· ( 1− B a(qpn)\nBa(qn)\n) · ( ∂c(qpn, η)\n∂η − c(q p n, η)\ndBa(qpn)/dq p n\n· d 2Ba(qpn)\ndqpn 2 · ∂qpn ∂η )] (108) As the agent is selfish, he will exert effort qn to maximize his expected payoff. Hence, the agent’s exerted effort can be obtained by taking the first order derivative of (107) with respect to qn and setting it to zero, which is\ndBa ( qn )\ndqn · c(qpn, θ̂n) =\ndBa ( qpn )\ndqpn · c(qn, θn) (109)\nAs we assume that dB a(z)\ndz ≤ 0, then based on (109), we have (i) if θ̂n > θn, qpn < qn, (ii) if θ̂n < θn,\nqpn > qn, and (iii) if θ̂n = θn, q p n = qn.\nAs we assume that dBa(qpn)/dq p n ≤ 0, d2Ba(qpn)/dqpn 2 > 0, and later we will prove in Lemma 10 that Qp(θn,θ−n) is nonincreasing in θn, if (26) holds, i.e., ∂c (Q p(η,θ−n), η)/∂η 6 0, then the\nabove equation (108) is negative when θ̂n > θn. Based on this, we have Eθ−n [ Uae(θn, θn,θ−n) ] >\nEθ−n [ Uae(θ̂n, θn,θ−n) ] . This inequality also holds for θ̂n < θn, by a similar argument. Therefore, an agent will truthfully report his own cost parameter.\nStep 3. Incentivize agent to exert precisely the effort as desired by the principal under COPE As we have proved in Step 2 that the agent n would truthfully report his cost type (θ̂n = θn). Next we will show that the agent n exerts effort such that qn = q p n would maximize his expected payoff as follows.\nE [ Uae(x∗, qpn, yn, θ̂n, θ̂−n) ] = π(θ̂n, θ̂−n)−K(θ̂n) ·Ba ( qn ) +S(θ̂n)−C(qn, θn), (110)\nwhere the expectation is taken with respect to θ−n.\nIt can be verified that (110) is concave in qn as we assume that d2Ba ( qn )\ndqn 2 ≥ 0.\nHence, by taking the first order derivative of (110) with respect to qn, we have\n∂ ∂qn E [ Uae(x∗, qpn, yn, θ̂nθ̂−n) ] =\nc ( qpn, θ̂n ) dBa ( qpn ) /dqpn · dBa ( qn ) dqn − c(qn, θn). (111)\nWe can verify that the value of (111) equals to zero when qn = q p n. Hence, agent n will exert the\neffort as the principal desires to maximize his expected payoff.\nStep 4. Maximize the principal’s expected utility under COPE Then we look at the expected payoff of the principal. To maximize the expected utility for the\nprediction, the principal solves\nmax x̂\nE [ − `p ( x∗, x̂(y,qp)|(ŷn,qp) ] . (112)\nThe principal employs the Bayes estimate x̂ as follows:\nx̂ ( ŷ,q∗ ) = arg infx̂E [ `p ( x∗, x̂(ŷ,qp) )] . (113)\nIt follows that the expected utility of the principal is\nBp ( qp ) = inf x̂ E [ `p ( x∗, x̂(y,qp) )] . (114)\nWe then show that the desired effort level Qp(θ̂n, θ̂−n) defined as the solution of (125) and the function π(θ̂n, θ̂−n) defined in (23) can maximize the principal’s expected payoff and satisfy BIC and BIR conditions.\nNotice that agent n exerts effort such that qn = q p n and reports yn ∈ argminEx∗ [`a(x∗, ŷn)], the\nexpected payment function defined in (22) is reduced to\nE[R(x∗, yn, θn,θ−n)] = π(θn,θ−n),\nwhere the expectation is taken with respect to x∗ and yn.\nThen the expected payoff of agent n is rewritten as\nEθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] =Eθ−n [ π(θ̂n,θ−n)−C ( Qp(θ̂n,θ−n), θn )] , (115)\nand the BIC and BIR conditions, i.e., (5) and (6) can be rewritten as\nEθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥Eθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] ∀θ̂n 6= θn, (116) Eθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥ 0, ∀θn. (117)\nThen the expected payoff of the principal is\nE[U p(x∗,qp,y,θ)] =−Bp ( qp ) − ∑ n∈A π(θn,θ−n)\nwhere the expectation is taken with respect to x∗ and y.\nRecall that qpn =Q p(θn,θ−n), we then rewrite (7) as\nsup {Qp(θ),π(θ)},∀θn∈θ\nE[U p(x∗,qp,y, θ̂)],\nsubject to : BIC and BIR in (116) and (117).\n(118)\nFor the feasible region defined by BIC and BIR, we can characterise an equivalent formulation\nin the following lemma:\nLemma 9. The solution of (118) is feasible if and only if it satisfies the following conditions for all θn ∈ [θ, θ̄]:\n• The expected payoff of agent n is\nEθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] =Eθ−n [∫ θ̄\nθn\n∂C ( Qp(z,θ−n), η ) ∂η dz ] , (119)\n• Qp(θn,θ−n) is non-increasing in θn.\nProof : We first show that BIC and BIR imply the condition 119. The first derivative of (115) is\n∂Eθ−n [ Uae(π(θ̂,θ−n),Q p(θ̂,θ−n), η) ]\n∂η =Eθ−n\n[ − ∂C(Q p(θ̂,θ−n), η)\n∂η\n] ≤ 0. (120)\nThen, for any θ1n > θ 2 n, we have Eθ−n [ Uae(π(θ1n,θ−n),Q p(θ1n,θ−n), θ 1 n) ] ≤Eθ−n [ Uae(π(θ1n,θ−n),Q p(θ1n,θ−n), θ 2 n) ]\n≤Eθ−n [ Uae(π(θ2n,θ−n),Q p(θ2n,θ−n), θ 2 n) ] ,\n(121)\nwhere the first inequality is due to (120) and the second is from the BIC condition defined in (116).\nRecall that the BIR condition is\nEθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] ≥ 0, ∀θn ∈ [θ, θ̄], (122)\nwhich implies that, for the agent n ∈ A with any value θn ∈ [θ, θ̄], his expected payoff should be nonnegative. Then the expected payoff of the agent n with cost parameter θ̄ must be binding at zero. Otherwise, the principal can reduce the π(θ̄,θ−n) by a small value of δ > 0, which does not violate the constraint of (122) but raises the principal’s expected payoff. Hence, we have\nEθ−n [ Uae(π(θ̄,θ−n),Q p(θ̄,θ−n), θ̄) ] = 0. (123)\nLet Uae(θn,θ−n) =U ae ( π(θn,θ−n),Q p(θn,θ−n), θn ) . From BIC condition, we have\nEθ−n [ Uae(θn,θ−n) ] = max\nθ̂n\nEθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )] .\nBy using the envelope theorem, we have: ∂Eθ−n [ Uae(θn,θ−n) ] ∂η = ∂Eθ−n [ Uae(π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn) ]\n∂θn\n∣∣∣∣∣ θ̂n=θn\n=Eθ−n\n[ − ∂C(Q p(θn,θ−n), θn)\n∂θn\n] ,\nwhere θn is a parameter. By integrating both sides from θn to θ̄ and using (123) and the assumption that the random variable θn of the agent n is independent for every n∈A, we get\nEθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn )] =Eθ−n [∫ θ̄ θn ∂C(Qp(z,θ−n), η) ∂η dz ] . (124)\nWe prove thatQp(θn,θn) is nonincreasing in θn by contradiction. Let pn as the shorthand notation\nfor π(θn,θ−n). Suppose for any θ 1 n > θ 2 n, we have Q p(θ1n,θ−n)>Q p(θ2n,θ−n). Because\n∂2Uae ( pn, q p n, θn ) ∂qpn∂θn =−∂c(q, θn) ∂θn < 0, and\n∂2Uae ( pn, q p n, θn ) ∂qpn 2 =− ∂c(q, θn) ∂q < 0,\nwe then have\n0 = ∂Uae\n( pn, q p n, θ 1 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ1n,θ−n)\n< ∂Uae\n( pn, q p n, θ 1 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n)\n< ∂Uae\n( pn, q p n, θ 2 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n) ,\nwhere the equality is due to BIC when the agent n’s cost parameter θn has the value of θ 1 n, the second equality is due to (C), and the inequality is due to (C).\nHowever, based on the BIC condition, if the agent n’s cost parameter θn has the value of θ 2 n,\nthen we should have\n∂Uae ( pn, q p n, θ 2 n ) ∂qpn ∣∣∣∣∣ qpn=Q p(θ2n,θ−n) = 0,\nwhich holds true for all scalar values of pn. Hence, for any θ 1 n > θ 2 n, Q p(θ1n,θ−n)≤Qp(θ2n,θ−n).\nThen we need to prove that ((119)) implies BIC and BIR defined in ((116)) and ((117)).\nBIR is verified by putting θn back to (119). Besides, by putting θn = θ̄ back to (119), we have\nEθ−n [ Uae ( π(θ̄,θ−n),Q p(θ̄,θ−n), θ̄ )] = 0.\nThen we prove that ((119)) implies BIC. Notice that we have:\nEθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn )]\n1 =Eθ−n [ − ∫ θ̄ θn ∂Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), z ) ∂z dz ] 2 =Eθ−n [ Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), θ̂n ) − ∫ θ̂n θn ∂Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), z ) ∂z dz\n] 3 =Eθ−n [∫ θ̄ θ̂n ∂C(Qp(η,θ−n), z) ∂z dη− ∫ θ̂n θn ∂Uae ( π(θ̂n,θ−n),Q p(θ̂n,θ−n), z ) ∂z dz\n] 4 =Eθ−n [ − ∫ θn θ̄ ∂C(Qp(η,θ−n), z) ∂z dη− ∫ θ̂n θn ∂C(Qp(η,θ−n), z) ∂z dη+ ∫ θ̂n θn ∂C(Qp(θ̂n,θ−n), z) ∂z dz\n] 5 =Eθ−n [ Uae ( π(θn,θ−n),Q p(θn,θ−n), θn ) + ∫ θ̂ θn ( ∂C(Qp(θ̂n,θ−n), z) ∂z − ∂C(Q p(η,θ−n), z) ∂z ) dη ] ,\nwhere the third equality and the fifth equality is obtained by (119).\nIf θ̂n > θn, then the above equation is non-positive. This is because Q p(η,θ−n) is non-increasing\nin η and ∂C(q, θ)/∂θ > 0. Hence,\nEθ−n [ Uae(π(θ̂n,θ−n),Q p(θ̂n,θ−n), θn) ] <Eθ−n [ Uae(π(θn,θ−n),Q p(θn,θ−n), θn) ] .\nThis inequality also holds for θ̂n < θn by a similar argument. Therefore, the two condition imply BIC.\nThen based on Lemma 9 and let qpn =Q p(θn,θ−n), we have the following Lemma.\nLemma 10. The optimisation problem in (118) has the following equivalent formulation:\nmax qp\nEθ [ −Bp ( qp ) − ∑ n∈A C ( qpn, θn ) − ∑ n∈A ∂C ( qpn, θn ) ∂θn · F (θn) f(θn) ] s.t. qpn is nonincreasing in θn,∀n∈A. (125)\nwhere the expectation is taken with respect to θ.\nproof : The proof of Lemma 4 is as follows. The expected payoff of the principal can be written\nas:\nEθ [ −Bp ( qp ) − ∑ n∈A C ( qpn, θn ) − ∑ n∈A Ua ( π(θn,θ−n), q p n, θn )]\n=Eθ [ −Bp ( qp ) − ∑ n∈A C ( qpn, θn ) − ∑ n∈A ∫ θ̄ θn ∂C ( qpn, η ) ∂η dx ] ,\n(126)\nwhere the expectation is taken with respect to θ. Notice that Eθn [∫ θ̄\nθn\n∂C ( qpn, η ) ∂η dx ] = ∫ θ̄ θ ∫ θ̄ z ∂C ( qpn, η ) ∂η dx · f(z)dz = ∫ θ̄ θ F (z) ∂C ( qpn, η ) ∂η dz\n= ∫ θ̄ θ F (z) f(z) ∂C ( qpn, η ) ∂η f(z)dz =Eθn [ F (θn) f(θn) ∂C ( qpn, η ) ∂η ] ,\nwhere the first equation is obtained by using integration by parts. Then by applying the above equation to (126) and the fact that {θn}n∈A are assumed to be random, independently and identically distributed on support [θ, θ̄], we can get the conclusion.\nBased on Lemma 10, the principal’s problem thus reduces to choosing the desired effort qpn = Qp(θn,θ−n) for each agent n∈A. We first consider the problem in (125) without the constraint. If the optimal solution to this unconstrained problem is increasing, then it is also an optimal solution to the constrained problem.\nLet qpn =Q p(θn,θ−n) and\nM ( qp1, . . . , q p N ) =−Bp ( qp ) − ∑ n∈A C ( qpn, θn ) − ∑ n∈A\n∂C ( qpn, θn ) ∂θn · F (θn) f(θn) .\nAs we assume that ∂2Bp(qp)/∂qpi ∂q p j > 0,∀j 6= i, ∂C(qpn, θn)/∂qpn > 0 and ∂2C(qpn, θn)/∂qpn∂θn ≥ 0,\nwe can check that\n∂2M\n∂qpi ∂q p j\n=− ∂2Bp\n( qp )\n∂qpi ∂q p j\n≤ 0, j 6= i,\n∂2M ∂qpi 2 =−\n∂2Bp ( qp )\n∂qpi 2 − c(q p i , θi)−\n∂c(qpi , θi) ∂θi · F (θi) f(θi) ≤ 0. (127)\nLet G = [∂2M/∂qpi ∂q p j ], i, j = 1, . . . ,N , be the matrix of second order derivatives. Matrix G is symmetric with negative diagonal terms as shown in (127).\nAs we can verify that, for k = 1, . . . ,N , the kth leading principal minors of G alternate in sign, so that G is negative definite and W is strictly concave. The computational complexity of finding the optimal solution of (125) will depend on the specific structure of the functions.\nUsing Cramer’s rule, the assumption that the c.d.f. function F is log concave in θ, and the\nassumption that ∂C(qpi , θi)/∂q p i > 0 and ∂ 2c(qpi , θi)/∂q p i ∂θi ≥ 0, we can verify that\n∂Qp(θn,θ−n)\n∂θn =−∂ 2M/∂qpn∂θn ∂2M/∂qpn 2 ≤ 0,∀n∈A, (128)\nwhich shows that Qp(θn,θ−n) derived by solving (125) is nonincreasing in θn, so that it is a feasible solution of (125).\nAccording to (119), we have\nEθ−n [ π(θn,θ−n)−C ( Qp(θn,θ−n), θn )] =Eθ−n [∫ θ̄ θn ∂C ( Qp(z,θ−n), η ) ∂η dz ] .\nFrom the above equation, we can derive the optimal payment function as given in (23).\nEndnotes\n1. We want to emphasize that whether the Bayes risk exists is a open problem. Scharf (1991),\nFigueiredo (2004) consider some special case such as Gaussian distribution. Characterizing the\ngeneral condition for the existence and uniqueness of Bayes risk will be interesting future work.\n2. To the best of our knowledge, this is the first paper that studies how to incentivize all agents to\nreport their truthful estimations and exert appropriate amounts of effort based on their respective\ncapabilities during a prediction process. Hence, we have not found an algorithm in the existing\nliterature as a fair benchmark to compare with the COPE performance."
    } ],
    "references" : [ {
      "title" : "Low-cost learning via active data",
      "author" : [ "Abernethy", "Jacob", "Yiling Chen", "Chien-Ju Ho", "Bo Waggoner" ],
      "venue" : null,
      "citeRegEx" : "Abernethy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical decision theory and Bayesian analysis",
      "author" : [ "Berger", "James O" ],
      "venue" : null,
      "citeRegEx" : "Berger and O.,? \\Q2013\\E",
      "shortCiteRegEx" : "Berger and O.",
      "year" : 2013
    }, {
      "title" : "The economics of contracts: theories and applications",
      "author" : [ "Brousseau", "Eric", "Jean-Michel" ],
      "venue" : "Glachant",
      "citeRegEx" : "Brousseau et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brousseau et al\\.",
      "year" : 2002
    }, {
      "title" : "Optimum statistical estimation with",
      "author" : [ "Press) Cai", "Yang", "Constantinos Daskalakis", "Christos H Papadimitriou" ],
      "venue" : null,
      "citeRegEx" : "Cai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2015
    }, {
      "title" : "Sourcing through auctions and audits",
      "author" : [ "Chen", "Ying-Ju", "Sridhar Seshadri", "Eitan Zemel" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2008
    }, {
      "title" : "A Kash (2015) Elicitation for aggregation",
      "author" : [ "Rafael M", "Yiling Chen", "Ian" ],
      "venue" : "Proceedings of the 29th",
      "citeRegEx" : "Frongillo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Frongillo et al\\.",
      "year" : 2015
    }, {
      "title" : "Decision support systems",
      "author" : [ "Hayes", "Bob E" ],
      "venue" : null,
      "citeRegEx" : "Hayes and E.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hayes and E.",
      "year" : 1998
    }, {
      "title" : "Linear regression as a non-cooperative game",
      "author" : [ "Ioannidis", "Stratis", "Patrick Loiseau" ],
      "venue" : "International Journal of Quality & Reliability Management",
      "citeRegEx" : "Ioannidis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ioannidis et al\\.",
      "year" : 2013
    }, {
      "title" : "Budget-optimal task allocation for reliable",
      "author" : [ "Economics. Springer", "277–290. Karger", "David R", "Sewoong Oh", "Devavrat Shah" ],
      "venue" : null,
      "citeRegEx" : "Springer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Springer et al\\.",
      "year" : 2014
    }, {
      "title" : "Parametric Prediction from Parametric Agents",
      "author" : [ "Lehmann", "Erich Leo", "George Casella" ],
      "venue" : null,
      "citeRegEx" : "Lehmann et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Lehmann et al\\.",
      "year" : 1998
    }, {
      "title" : "Incentive compatibility and the bargaining problem",
      "author" : [ "Myerson", "Roger B" ],
      "venue" : "method. Management Science",
      "citeRegEx" : "Myerson and B.,? \\Q1979\\E",
      "shortCiteRegEx" : "Myerson and B.",
      "year" : 1979
    }, {
      "title" : "Statistical signal processing(Addison-Wesley, Reading Massachusetts)",
      "author" : [ "1007–1017. Scharf", "Louis L" ],
      "venue" : null,
      "citeRegEx" : "Scharf and L.,? \\Q1991\\E",
      "shortCiteRegEx" : "Scharf and L.",
      "year" : 1991
    }, {
      "title" : "Approval voting and incentives in crowdsourcing",
      "author" : [ "Shah", "Nihar B", "Dengyong Zhou", "Yuval Peres" ],
      "venue" : null,
      "citeRegEx" : "Shah et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : ", Frongillo et al. (2015), Abernethy et al.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "(2015), Abernethy et al. (2015)).",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "(2005), Prelec (2004), Shah et al. (2015), Shah and Zhou (2015)).",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "(2005), Prelec (2004), Shah et al. (2015), Shah and Zhou (2015)).",
      "startOffset" : 23,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "(2007), Ioannidis and Loiseau (2013), Frongillo et al. (2015), Cai et al.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "(2015), Cai et al. (2015), Abernethy et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "(2015), Abernethy et al. (2015)) address problems in this space.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type.",
      "startOffset" : 8,
      "endOffset" : 246
    }, {
      "referenceID" : 0,
      "context" : "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium.",
      "startOffset" : 8,
      "endOffset" : 545
    }, {
      "referenceID" : 0,
      "context" : "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study how a principal can make predictions by eliciting the agents’ confidences, again without considering the costs that may be incurred by the agents.",
      "startOffset" : 8,
      "endOffset" : 688
    }, {
      "referenceID" : 0,
      "context" : "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study how a principal can make predictions by eliciting the agents’ confidences, again without considering the costs that may be incurred by the agents. Abernethy et al. (2015) consider a model where the agents cannot fabricate their observation, but may lie about their costs, and design a mechanism to ensure that agents truthfully report their costs.",
      "startOffset" : 8,
      "endOffset" : 865
    }, {
      "referenceID" : 0,
      "context" : "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study how a principal can make predictions by eliciting the agents’ confidences, again without considering the costs that may be incurred by the agents. Abernethy et al. (2015) consider a model where the agents cannot fabricate their observation, but may lie about their costs, and design a mechanism to ensure that agents truthfully report their costs. In contrast, we assume a more general scenario where the agents can be strategic in choosing and reporting their respective observations. Cai et al. (2015) propose a monetary mechanism to collect data and to perform an estimation of a function at one random point.",
      "startOffset" : 8,
      "endOffset" : 1198
    }, {
      "referenceID" : 3,
      "context" : "(2007), Cai et al. (2015)), i.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "(2007), Cai et al. (2015)), i.e., yn ∼N (x∗, 1 qn ), where yn follows from Gaussian distribution with mean x∗ and variance 1/qn. Conditioned on x ∗, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn,∀n ∈A) be the observation vector and q= (qn,∀n∈A) be the effort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others. It is costly for each agent to exert a high level effort when making an observation, and this cost not only depends on the effort chosen by the agent, but also is affected by the agent’s costtype parameter θn. The cost types of different agents are allowed to be different, capturing the heterogeneity of the agents. A smaller value of θn implies a higher capability of agent n. Specifically, we consider a publicly known cost function C : R+×R+→R+, and assume that the cost incurred by an agent n∈A with the cost type θn when exerting an effort qn is C(qn, θn). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2. The cost types {θn}n∈A are assumed to be randomly, independently and identically distributed on support [θ, θ̄] for some 0≤ θ < θ̄ <∞. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [θ, θ̄]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.",
      "startOffset" : 8,
      "endOffset" : 1635
    }, {
      "referenceID" : 3,
      "context" : "(2007), Cai et al. (2015)), i.e., yn ∼N (x∗, 1 qn ), where yn follows from Gaussian distribution with mean x∗ and variance 1/qn. Conditioned on x ∗, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn,∀n ∈A) be the observation vector and q= (qn,∀n∈A) be the effort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others. It is costly for each agent to exert a high level effort when making an observation, and this cost not only depends on the effort chosen by the agent, but also is affected by the agent’s costtype parameter θn. The cost types of different agents are allowed to be different, capturing the heterogeneity of the agents. A smaller value of θn implies a higher capability of agent n. Specifically, we consider a publicly known cost function C : R+×R+→R+, and assume that the cost incurred by an agent n∈A with the cost type θn when exerting an effort qn is C(qn, θn). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2. The cost types {θn}n∈A are assumed to be randomly, independently and identically distributed on support [θ, θ̄] for some 0≤ θ < θ̄ <∞. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [θ, θ̄]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.",
      "startOffset" : 8,
      "endOffset" : 1657
    }, {
      "referenceID" : 3,
      "context" : "(2007), Cai et al. (2015)), i.e., yn ∼N (x∗, 1 qn ), where yn follows from Gaussian distribution with mean x∗ and variance 1/qn. Conditioned on x ∗, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn,∀n ∈A) be the observation vector and q= (qn,∀n∈A) be the effort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others. It is costly for each agent to exert a high level effort when making an observation, and this cost not only depends on the effort chosen by the agent, but also is affected by the agent’s costtype parameter θn. The cost types of different agents are allowed to be different, capturing the heterogeneity of the agents. A smaller value of θn implies a higher capability of agent n. Specifically, we consider a publicly known cost function C : R+×R+→R+, and assume that the cost incurred by an agent n∈A with the cost type θn when exerting an effort qn is C(qn, θn). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2. The cost types {θn}n∈A are assumed to be randomly, independently and identically distributed on support [θ, θ̄] for some 0≤ θ < θ̄ <∞. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [θ, θ̄]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.",
      "startOffset" : 8,
      "endOffset" : 1678
    }, {
      "referenceID" : 4,
      "context" : ", Che (1993), Chen et al. (2008) and references therein).",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : ", Che (1993), Chen et al. (2008) and references therein). The cost types {θn}n∈A are assumed to be randomly, independently and identically distributed with a cumulative distribution function F : [θ, θ̄]→R+ and probability density function f : [θ, θ̄]→ R+. The functions F and f are public knowledge to all agents. We also assume that the c.d.f. function F is continuous, differentiable, and log concave in [θ, θ̄]. This is a regularity condition often assumed in auction contexts (see, e.g., Myerson (1981)).",
      "startOffset" : 14,
      "endOffset" : 507
    }, {
      "referenceID" : 4,
      "context" : ", Che (1993), Chen et al. (2008) and references therein). The cost types {θn}n∈A are assumed to be randomly, independently and identically distributed with a cumulative distribution function F : [θ, θ̄]→R+ and probability density function f : [θ, θ̄]→ R+. The functions F and f are public knowledge to all agents. We also assume that the c.d.f. function F is continuous, differentiable, and log concave in [θ, θ̄]. This is a regularity condition often assumed in auction contexts (see, e.g., Myerson (1981)). This assumption is satisfied by a wide range of distributions, such as the uniform, gamma, and beta distributions. See Rosling (2002) for an extensive discussion on log concave probability distributions.",
      "startOffset" : 14,
      "endOffset" : 643
    } ],
    "year" : 2016,
    "abstractText" : "Yuan Luo Department of Information Engineering, The Chinese University of Hong Kong, yluo@ie.cuhk.edu.hk, Nihar B. Shah Department of Electrical Engineering and Computer Sciences, UC Berkeley, nihar@eecs.berkeley.edu Jianwei Huang Department of Information Engineering, The Chinese University of Hong Kong, jwhuang@ie.cuhk.edu.hk Jean Walrand Department of Electrical Engineering and Computer Sciences, UC Berkeley, walrand@berkeley.edu",
    "creator" : "LaTeX with hyperref package"
  }
}