{
  "name" : "1306.0686.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Online Learning under Delayed Feedback",
    "authors" : [ "Pooria Joulani" ],
    "emails" : [ "pooria@ualberta.ca", "gyorgy@ualberta.ca", "szepesva@ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n06 86\nv2 [\ncs .L\nG ]\n5 J\nun 2\n01 3"
    }, {
      "heading" : "1. Introduction",
      "text" : "In this paper we study sequential learning when the feedback about the predictions made by the forecaster are delayed. This is the case, for example, in web advertisement, where the information whether a user has clicked on a certain ad may come back to the engine in a delayed fashion: after an ad is selected, while waiting for the information if the user clicks or not, the engine has to provide ads to other users. Also, the click information may be aggregated and then periodically sent to the module that decides about the ads, resulting in further delays. (Li et al., 2010; Dudik et al., 2011). Another example is parallel, distributed learning, where propagating information among nodes causes delays (Agarwal & Duchi, 2011).\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nWhile online learning has proved to be successful in many machine learning problems and is applied in practice in situations where the feedback is delayed, the theoretical results for the non-delayed setup are not applicable when delays are present. Previous work concerning the delayed setting focussed on specific online learning settings and delay models (mostly with constant delays). Thus, a comprehensive understanding of the effects of delays is missing. In this paper, we provide a systematic study of online learning problems with delayed feedback. We consider the partial monitoring setting, which covers all settings previously considered in the literature, extending, unifying, and often improving upon existing results. In particular, we give general meta-algorithms that transform, in a blackbox fashion, algorithms developed for the non-delayed case into algorithms that can handle delays efficiently. We analyze how the delay effects the regret of the algorithms. One interesting, perhaps somewhat surprising, result is that the delay inflates the regret in a multiplicative way in adversarial problems, while this effect is only additive in stochastic problems. While our general meta-algorithms are useful, their time- and spacecomplexity may be unnecessarily large. To resolve this problem, we work out modifications of variants of the UCB algorithm (Auer et al., 2002) for stochastic bandit problems with delayed feedback that have much smaller complexity than the black-box algorithms.\nThe rest of the paper is organized as follows. The problem of online learning with delayed feedback is defined in Section 2. The adversarial and stochastic problems are analyzed in Sections 3.1 and 3.2, while the modification of the UCB algorithm is given in Section 4. Some proofs, as well as results about the KL-UCB algorithm (Garivier & Cappé, 2011) under delayed feedback, are provided in the appendix."
    }, {
      "heading" : "2. The delayed feedback model",
      "text" : "We consider a general model of online learning, which we call the partial monitoring problem with side in-\nformation. In this model, the forecaster (decision maker) has to make a sequence of predictions (actions), possibly based on some side information, and for each prediction it receives some reward and feedback, where the feedback is delayed. More formally, given a set of possible side information values X , a set of possible predictions A, a set of reward functions R ⊂ {r : X ×A → R}, and a set of possible feedback values H, at each time instant t = 1, 2, . . ., the forecaster receives some side information xt ∈ X ; then, possibly based on the side information, the forecaster predicts some value at ∈ A while the environment simultaneously chooses a reward function rt ∈ R; finally, the forecaster receives reward rt(xt, at) and some time-stamped feedback set Ht ⊂ N×H. In particular, each element of Ht is a pair of time index and a feedback value, the time index indicating the time instant whose decision the associated feedback corresponds to.\nNote that the forecaster may or may not receive any direct information about the rewards it receives (i.e., the rewards may be hidden). In standard online learning, the feedback-set Ht is a singleton and the feedback in this set depends on rt, at. In the delayed model, however, the feedback that concerns the decision at time t is received at the end of the time period t+τt, after the prediction is made, i.e., it is delayed by τt time steps. Note that τt ≡ 0 corresponds to the non-delayed case. Due to the delays multiple feedbacks may arrive at the same time, hence the definition of Ht.\nThe goal of the forecaster is to maximize its cumulative reward ∑n t=1 rt(xt, at) (n ≥ 1). The performance of the forecaster is measured relative to the best static strategy selected from some set F ⊂ {f | f : X → A} in hindsight. In particular, the forecaster’s performance is measured through the regret, defined by\nRn = sup a∈F\nn∑\nt=1\nrt(xt, a(xt))− n∑\nt=1\nrt(xt, at).\nA forecaster is consistent if it achieves, asymptotically, the average reward of the best static strategy, that is E [Rn] /n → 0, and we are interested in how fast the average regret can be made to converge to 0.\nThe above general problem formulation includes most scenarios considered in online learning. In the full information case, the feedback is the reward function itself, that is, H = R and Ht = {(t, rt)}) (in the non-delayed case). In the bandit case, the forecaster only learns the rewards of its own prediction, i.e., H = R and Ht = {(t, rt(xt, at))}. In the partial monitoring case, the forecaster is given a reward function r : X × A × B → R and a feedback function h : X × A × B → H, where B is a set of choices (outcomes) of the environment. Then, for each time instant the environment picks an outcome bt ∈ B, and the reward becomes rt(xt, at) = r(xt, at, bt), while Ht = {(t, h(xt, at, bt))}. This interaction protocol is shown in Figure 1 in the delayed case. Note that the bandit and full information problems can also be treated as special partial monitoring problems. Therefore, we will use this last formulation of the problem. When no stochastic assumption is made on how the sequence bt is generated, we talk about the adversarial model. In the stochastic setting we will consider the case when bt is a sequence of independent, identically distributed (i.i.d.) random variables. Side information may or may not be present in a real problem; in its absence X is a singleton set. Finally, we may have different assumptions on the delays. Most often, we will assume that (τt)t≥1 is an i.i.d. sequence, which is independent of the past predictions (as)s≤t of the forecaster. In the stochastic setting, we also allow the distribution of τt to depend on at.\nNote that the delays may change the order of observing the feedbacks, with the feedback of a more recent prediction being observed before the feedback of an earlier one."
    }, {
      "heading" : "2.1. Related work",
      "text" : "The effect of delayed feedback has been studied in the recent years under different online learning scenarios\nG∗n ≤ τmax when the delays have an upper bound τmax, and we show that G ∗ n = O\n(\nE [τt] +\n√\nE [τt] logn+ logn\n)\nwhen the delays τt are i.i.d. The new bounds for the partial monitoring problem are automatically applicable in the other, spacial, cases, and give improved results in most cases.\nand different assumptions on the delay. A concise summary, together with the contributions of this paper, is given in Table 1.\nTo the best of our knowledge, Weinberger & Ordentlich (2002) were the first to analyze the delayed feedback problem; they considered the adversarial full information setting with a fixed, known delay τconst. They showed that the minimax optimal solution is to run τconst + 1 independent optimal predictors on the subsampled reward sequences: τconst + 1 prediction strategies are used such that the ith predictor is used at time instants t with (t mod (τconst + 1)) + 1 = i. This approach forms the basis of our method devised for the adversarial case (see Section 3.1). Langford et al. (2009) showed that under the usual conditions, a sufficiently slowed-down version of the mirror descent algorithm achieves optimal decay rate of the average regret. Mesterharm (2005; 2007) considered another variant of the full information setting, using an adversarial model on the delays in the label prediction setting, where the forecaster has to predict the label corresponding to a side information vector\nxt. While in the full information online prediction problem Weinberger & Ordentlich (2002) showed that the regret increases by a multiplicative factor of τconst, in the work of Mesterharm (2005; 2007) the important quantity becomes the maximum/average gap defined as the length of the largest time interval the forecaster does not receive feedback. Mesterharm (2005; 2007) also shows that the minimax regret in the adversarial case increases multiplicatively by the average gap, while it increases only in an additive fashion in the stochastic case, by the maximum gap. Agarwal & Duchi (2011) considered the problem of online stochastic optimization and showed that, for i.i.d. random delays, the regret increases with an additive factor of order E [ τ2 ] .\nQualitatively similar results were obtained in the bandit setting. Considering a fixed and known delay τconst, Dudik et al. (2011) showed an additive O(τconst √ log n) penalty in the regret for the stochastic setting (with side information), while (Neu et al., 2010) showed a multiplicative regret for the adversarial bandit case. The problem of delayed feedback has also been studied for Gaussian process bandit optimization\n(Desautels et al., 2012), resulting in a multiplicative increase in the regret that is independent of the delay and an additive term depending on the maximum delay.\nIn the rest of the paper we generalize the above results to the partial monitoring setting, extending, unifying, and often improving existing results."
    }, {
      "heading" : "3. Black-Box Algorithms for Delayed Feedback",
      "text" : "In this section we provide black-box algorithms for the delayed feedback problem. We assume that there exists a base algorithm Base for solving the prediction problem without delay. We often do not specify the assumptions underlying the regret bounds of these algorithms, and assume that the problem we consider only differs from the original problem because of the delays. For example, in the adversarial setting, Base may build on the assumption that the reward functions are selected in an oblivious or non-oblivious way (i.e., independently or not of the predictions of the forecaster). First we consider the adversarial case in Section 3.1. Then in Section 3.2, we provide tighter bounds for the stochastic case."
    }, {
      "heading" : "3.1. Adversarial setting",
      "text" : "We say that a prediction algorithm enjoys a regret or expected regret bound f : [0,∞) → R under the given assumptions in the non-delayed setting if (i) f is nondecreasing, concave, f(0) = 0; and (ii) supb1,...,bn∈B Rn ≤ f(n) or, respectively, supb1,...,bn∈B E [Rn] ≤ f(n) for all n. The algorithm of Weinberger & Ordentlich (2002) for the adversarial full information setting subsamples the reward sequence by the constant delay τconst+1, and runs a base algorithm Base on each of the τconst + 1 subsampled sequences. Weinberger & Ordentlich (2002) showed that if Base enjoys a regret bound f then their algorithm in the fixed delay case enjoys a regret bound (τconst+1)f(n/(τconst+1)). Furthermore, when Base is minimax optimal in the non-delayed setting, the subsampling algorithm is also minimax optimal in the (full information) delayed setting, as can be seen by constructing a reward sequence that changes only in every τconst + 1 times. Note that Weinberger & Ordentlich (2002) do not require condition (i) of f . However, these conditions imply that yf(x/y) is a concave function of y for any fixed x (a fact which will turn out to be useful in the analysis later), and are satisfied by all regret bounds we are aware of (e.g., for multi-armed bandits, contextual bandits, partial monitoring, etc.),\nwhich all have a regret upper bound of the form Õ(nα) for some 0 ≤ α ≤ 1, with, typically, α = 1/2 or 2/3.1. In this section we extend the algorithm of Weinberger & Ordentlich (2002) to the case when the delays are not constant, and to the partial monitoring setting. The idea is that we run several instances of a non-delayed algorithm Base as needed: an instance is “free” if it has received the feedback corresponding to its previous prediction – before this we say that the instance is “busy”, waiting for the feedback. When we need to make a prediction, we use one of existing instances that is free, and is hence ready to make another prediction. If no such instance exists, we create a new one to be used (a new instance is always “free”, as it is not waiting for the feedback of a previous prediction). The resulting algorithm, which we call Black-Box Online Learning under Delayed feedback (BOLD) is shown below (note that when the delays are constant, BOLD reduces to the algorithm of Weinberger & Ordentlich (2002)):\nAlgorithm 1 Black-box Online Learning under Delayed feedback (BOLD)\nfor each time instant t = 1, 2, . . . , n do Prediction:\nPick a free instance of Base (independently of past predictions), or create a new instance if all existing instances are busy. Feed the instance picked with xt and use its prediction. Update: for each (s, hs) ∈ Ht do Update the instance used at time instant s with the feedback hs. end for\nend for\nClearly, the performance of BOLD depends on how many instances of Base we need to create, and how many times each instance is used. Let Mt denote the number of Base instances created by BOLD up to and including time t. That is, M1 = 1, and we create a new instance at the beginning of any time instant when all instances are waiting for their feedback. Let Gt = ∑t−1 s=1 I {s+ τs ≥ t} be the total number of outstanding (missing) feedbacks when the forecaster is making a prediction at time instant t. Then we have Gt algorithms waiting for their feedback, and so Mt ≥ Gt + 1. Since we only introduce new instances when it is necessary (and each time instant at most\n1un = Õ(vn) means that there is a β ≥ 0 such that limn→∞ un/(vn log β n) = 0.\none new instance is created), it is easy to see that\nMt = G ∗ t + 1 (1)\nfor any t, where G∗t = max1≤s≤t Gt.\nWe can use the result above to transfer the regret guarantee of the non-delayed base algorithm Base to a guarantee on the regret of BOLD.\nTheorem 1. Suppose that the non-delayed algorithm Base used in BOLD enjoys an (expected) regret bound fBase. Assume, furthermore, that the delays τt are independent of the forecaster’s prediction at. Then the expected regret of BOLD after n time steps satisfies\nE [Rn] ≤ E [ (G∗n + 1)fBase ( n\nG∗n + 1\n)]\n≤ (E [G∗n] + 1)fBase (\nn\nE [G∗n] + 1\n) .\nProof. As the second inequality follows from the concavity of y 7→ yfBase(x/y) (x, y > 0), it remains to prove the first one.\nFor any 1 ≤ j ≤ Mn, let Lj denote the list of time instants in which BOLD has used the prediction chosen by instance j, and let nj = |Lj| be the number of time instants this happens. Furthermore, let Rjnj denote the regret incurred during the time instants t with t ∈ Lj:\nRjnj = sup a∈F\n∑\nt∈Lj\nrt(xt, a(xt))− ∑\nt∈Lj\nrt(xt, at),\nwhere at is the prediction made by BOLD (and instance j) at time instant t. By construction, instance j does not experience any delays. Hence, Rjnj is its regret in a non-delayed online learning problem. 2 Then,\nRn = sup a∈F\nn∑\nt=1\nrt(xt, a(xt))− n∑\nt=1\nrt(xt, at)\n= sup a∈F\nMn∑\nj=1\n∑\nt∈Lj\nrt(xt, a(xt)) − Mn∑\nj=1\n∑\nt∈Lj\nrt(xt, at)\n≤ Mn∑\nj=1\n\nsup a∈F\n∑\nt∈Lj\nrt(xt, a(xt))− ∑\nt∈Lj\nrt(xt, at)\n\n\n=\nMn∑\nj=1\nRjnj .\n2Note that Lj is a function of the delay sequence and is not a function of the predictions (at)t≥1. Hence, the reward sequence that instance j is evaluated on is chosen obliviously whenever the adversary of BOLD is oblivious.\nNow, using the fact that fBase is an (expected) regret bound, we obtain\nE [Rn|τ1, . . . , τn] ≤ Mn∑\nj=1\nE [ Rjnj |τ1, . . . , τn ]\n≤ Mn∑\nj=1\nfBase(nj) = Mn\nMn∑\nj=1\n1\nMn fBase(nj)\n≤ MnfBase\n  Mn∑\nj=1\n1\nMn nj\n  = MnfBase ( n\nMn\n) ,\nwhere the first inequality follows since Mn is a deterministic function of the delays, while the last inequality follows from Jensen’s inequality and the concavity of fBase. Substituting Mn from (1) and taking the expectation concludes the proof.\nNow, we need to bound G∗n to make the theorem meaningful. When all delays are the same constants, for n > τconst we get G ∗ n = τt = τconst, and we get back the regret bound\nE [Rn] ≤ (τconst + 1)fBase (\nn\nτconst + 1\n)\nof Weinberger & Ordentlich (2002), thus generalizing their result to partial monitoring. We do not know whether this bound is tight even when Base is minimax optimal, as the argument of Weinberger & Ordentlich (2002) for the lower bound does not work in the partial information setting (the forecaster can gain extra information in each block with the same reward functions).\nAssuming the delays are i.i.d., we can give an interesting bound on G∗n. The result is based on the fact that although Gt can be as large as t, both its expectation and variance are upper bounded by E [τ1].\nLemma 2. Assume τ1, . . . , τn is a sequence of i.i.d. random variables with finite expected value, and let B(n, t) = t+ 2 logn+ √ 4t logn. Then\nE [G∗n] ≤ B(n,E [τ1]) + 1.\nProof. First consider the expectation and the variance of Gt. For any t,\nE [Gt] = E\n[ t−1∑\ns=1\nI {s+ τs ≥ t} ] = t−1∑\ns=1\nP {s+ τs ≥ t}\n=\nt−2∑\ns=0\nP {τ1 > s} ≤ E [τ1] ,\nand, similarly\nσ2 [Gt] =\nt−1∑\ns=1\nσ2 [I {s+ τs ≥ t}] ≤ t−1∑\ns=1\nP {s+ τs ≥ t} ,\nso σ2 [Gt] ≤ E [τ1] in the same way as above. By Bernstein’s inequality (Cesa-Bianchi & Lugosi, 2006, Corollary A.3), for any 0 < δ < 1 and any t we have, with probability at least 1− δ,\nGt − E [Gt] ≤ log 1δ + √ 2σ2 [Gt] log 1 δ .\nApplying the union bound for δ = 1/n2, and our previous bounds on the variance and expectation of Gt, we obtain that with probability at least 1− 1/n,\nmax 1≤t≤n\nGt ≤ E [τ1] + 2 logn+ √ 4E [τ1] logn.\nTaking into account that max1≤t≤n Gt ≤ n, we get the statement of the lemma.\nCorollary 3. Under the conditions of Theorem 1, if the sequence of delays is i.i.d, then\nE [Rn] ≤ (B(n,E [τ1]) + 2)fBase (\nn\nB(n,E [τ1]) + 2\n) .\nNote that although the delays can be arbitrarily large, whenever the expected value is finite, the bound only increases by a logn factor."
    }, {
      "heading" : "3.2. Finite stochastic setting",
      "text" : "In this section, we consider the case when the prediction set A of the forecaster is finite; without loss of generality we assume A = {1, 2, . . . ,K}. We also assume that there is no side information (that is, xt is a constant for all t, and, hence, will be omitted; the results can be extended easily to the case of a finite side information set, where we can repeat the procedures described below for each value of the side information separately). The main assumption in this section is that the outcomes (bt)t≥1 form an i.i.d. sequence, which is also independent of the predictions of the forecaster. When B is finite, this leads to the standard i.i.d. partial monitoring (IPM) setting, while the conventional multi-armed bandit (MAB) setting is recovered when the feedback is the reward of the last prediction, that is, ht = rt(at, bt). As in the previous section, we will assume that the feedback delays are independent of the outcomes of the environment. The main result of this section shows that under these assumptions, the penalty in the regret grows in an additive fashion due to the delays, as opposed to the multiplicative penalty that we have seen in the adversarial case.\nBy the independence assumption on the outcomes, the sequences of potential rewards rt(i) . = r(i, bt) and feedbacks ht(i) . = h(i, bt) are i.i.d., respectively, for the same prediction i ∈ A. In this setting we also assume that the feedback and reward sequences of different predictions are independent of each other. Let µi = E [rt(i)] denote the expected reward of predicting i, µ∗ = maxi∈A µi the optimal reward and i ∗ with µi∗ = µ ∗ the optimal prediction. Moreover, let\nTi(n) = ∑n\nt=1 I {at = i} denote the number of times i is predicted by the end of time instant n. Then, defining the “gaps” ∆i = µ\n∗−µi for all i ∈ A, the expected regret of the forecaster becomes\nE [Rn] = n∑\nt=1\nµ∗ − µat = K∑\ni=1\n∆iE [Ti(n)] . (2)\nSimilarly to the adversarial setting, we build on a base algorithm Base for the non-delayed case. The advantage in the IPM setting (and that we consider expected regret) is that here Base can consider a permuted order of rewards and feedbacks, and so we do not have to wait for the actual feedback; it is enough to receive a feedback for the same prediction. This is the idea at the core of our algorithm, Queued Partial Monitoring with Delayed Feedback (QPM-D):\nAlgorithm 2 Queued Partial Monitoring with Delays (QPM-D)\nCreate an empty FIFO buffer Q[i] for each i ∈ A. Let I be the first prediction of Base. for each time instant t = 1, 2, . . . , n do Predict:\nwhile Q[I] is not empty do Update Base with a feedback from Q[I]. Let I be the next prediction of Base. end while There are no buffered feedbacks for I, so predict at = I at time instant t to get a feedback. Update: for each (s, hs) ∈ Ht do Add the feedback hs to the buffer Q[as]. end for\nend for\nHere we have a Base partial monitoring algorithm for the non-delayed case, which is run inside the algorithm. The feedback information coming from the environment is stored in separate queues for each prediction value. The outer algorithm constantly queries Base: while feedbacks for the predictions made are available in the queues, only the inner algorithm Base runs (that is, this happens within a single time instant\nin the real prediction problem). When no feedback is available, the outer algorithm keeps sending the same prediction to the real environment until a feedback for that prediction arrives. In this way Base is run in a simulated non-delayed environment. The next lemma implies that the inner algorithm Base actually runs in a non-delayed version of the problem, as it experiences the same distributions:\nLemma 4. Consider a delayed stochastic IPM problem as defined above. For any prediction i, for any s ∈ N let h′i,s denote the sth feedback QPM-D receives for predicting i. Then the sequence (h′i,s)s∈N is an i.i.d. sequence with the same distribution as the sequence of feedbacks (ht,i)t∈N for prediction i.\nTo relate the non-delayed performance of Base and the regret of QPM-D, we need a few definitions. For any t, let Si(t) denote the number of feedbacks for prediction i that are received by the end of time instant t. Then the number of missing feedbacks for i when making a prediction at time instant t is Gi,t = Ti(t − 1)− Si(t − 1). Let G∗i,n = max1≤t≤n Gi,t. Furthermore, for each i ∈ A, let T ′i (t′) be the number of times algorithm Base has predicted i while being queried t′ times. Let n′ denote the number of steps the inner algorithm Base makes in n steps of the real IPM problem. Next we relate n and n′, as well as the number of times QPM-D and Base (in its simulated environment) make a specific prediction.\nLemma 5. Suppose QPM-D is run for n ≥ 1 time instants, and has queried Base n′ times. Then n′ ≤ n and\n0 ≤ Ti(n)− T ′i (n′) ≤ G∗i,n. (3)\nProof. Since Base can take at most one step for each feedback that arrives, and QPM-D has to make at least one step for each arriving feedback, n′ ≤ n. Now, fix a prediction i ∈ A. If Base, and hence, QPM-D, has not predicted i by time instant n, (3) trivially holds. Otherwise, let tn,i denote the last time instant (up to time n) when QPM-D predicts i. Then Ti(n) = Ti(tn,i) = Ti(tn,i − 1) + 1. Suppose Base has been queried n′′ ≤ n times by time instant tn,i (inclusive). At this time instant, the buffer Q[i] must be empty and Base must be predicting i, otherwise QPM-D would not predict i in the real environment. This means that all the Si(tn,i−1) feedbacks that have arrived before this time instant have been fed to the base algorithm, which has also made an extra step, that is, T ′i (n ′) ≥ T ′i (n′′) = Si(tn,i − 1) + 1. Therefore,\nTi(n)− T ′i (n′) ≤ Ti(tn,i − 1) + 1− (Si(tn,i − 1) + 1) ≤ Gi,tn,i ≤ G∗i,n.\nWe can now give an upper bound on the expected regret of Algorithm 2.\nTheorem 6. Suppose the non-delayed Base algorithm is used in QPM-D in a delayed stochastic IPM environment. Then the expected regret of QPM-D is upper-bounded by\nE [Rn] ≤ E [ RBasen ] +\nK∑\ni=1\n∆iE [ G∗i,n ] , (4)\nwhere E [ RBasen ] is the expected regret of Base when run in the same environment without delays.\nWhen the delay τt is bounded by τmax for all t, we also have G∗i,n ≤ τmax, and E [Rn] ≤ E [ RBasen ] +O(τmax). When the sequence of delays for each prediction is i.i.d. with a finite expected value but unbounded support, we can use Lemma 2 to bound G∗i,n, and obtain a bound E [ RBasen ] +O(E [τ1] + √ E [τ1] logn+ logn).\nProof. Assume that QPM-D is run longer so that Base is queried for n times (i.e., it is queried n − n′ more times). Then, since n′ ≤ n, the number of times i is predicted by the base algorithm, namely T ′i (n), can only increase, that is, T ′i (n\n′) ≤ T ′i (n). Combining this with the expectation of (3) gives\nE [Ti(n)] ≤ E [T ′i (n)] + E [ G∗i,n ] ,\nwhich in turn gives,\nK∑\ni=1\n∆iE [Ti(n)] ≤ K∑\ni=1\n∆iE [T ′ i (n)] +\nK∑\ni=1\n∆iE [ G∗i,n ] .\n(5)\nAs shown in Lemma 4, the reordered rewards and feedbacks h′i,1, h ′ i,2, . . . , h ′ i,T ′\ni (n′), . . . h ′ i,Ti(n) are i.i.d. with\nthe same distribution as the original feedback sequence (ht,i)t∈N. The base algorithm Base has worked on the first T ′i (n) of these feedbacks for each i (in its extended run), and has therefore operated for n steps in a simulated environment with the same reward and feedback distributions, but without delay. Hence, the first summation in the right hand side of (5) is in fact E [ RBasen ] , the expected regret of the base algorithm in a nondelayed environment. This concludes the proof."
    }, {
      "heading" : "4. UCB for the Multi-Armed Bandit Problem with Delayed Feedback",
      "text" : "While the algorithms in the previous section provide an easy way to convert algorithms devised for the nondelayed case to ones that can handle delays in the feedback, improvements can be achieved if one makes modifications inside the existing non-delayed algorithms\nwhile retaining their theoretical guarantees. This can be viewed as a ”white-box” approach to extending online learning algorithms to the delayed setting, and enables us to escape the high memory requirements of black-box algorithms that arises for both of our methods in the previous section when the delays are large. We consider the stochastic multi-armed bandit problem, and extend the UCB family of algorithms (Auer et al., 2002; Garivier & Cappé, 2011) to the delayed setting. The modification proposed is quite natural, and the common characteristics of UCB-type algorithms enable a unified way of extending their performance guarantees to the delayed setting (up to an additive penalty due to delays).\nRecall that in the stochastic MAB setting, which is a special case of the stochastic IPM problem of Section 3.2, the feedback at time instant t is ht = r(at, bt), and there is a distribution νi from which the rewards of each prediction i are drawn in an i.i.d. manner. Here we assume that the rewards of different predictions are independent of each other. We use the same notation as in Section 3.2.\nSeveral algorithms devised for the non-delayed stochastic MAB problem are based on upper confidence bounds (UCBs), which are optimistic estimates of the expected reward of different predictions. Different UCB-type algorithms use different upper confidence bounds, and choose, at each time instant, a prediction with the largest UCB. Let Bi,s,t denote the UCB for prediction i at time instant t, where s is the number of reward samples used in computing the estimate. In a non-delayed setting, the prediction of a UCB-type algorithm at time instant t is given by at = argmaxi∈A Bi,Ti(t−1),t. In the presence of delays, one can simply use the same upper confidence bounds only with the rewards that are observed, and predict\nat = argmaxi∈A Bi,Si(t−1),t (6)\nat time instant t (recall that Si(t − 1) is the number of rewards that can be observed for prediction i before time instant t). Note that if the delays are zero, this algorithm reduces to the corresponding non-delayed version of the algorithm.\nThe algorithms defined by (6) can easily be shown to enjoy the same regret guarantees compared to their non-delayed versions, up to an additive penalty depending on the delays. This is because the analyses of the regrets of UCB algorithms follow the same pattern of upper bounding the number of trials of a suboptimal prediction using concentration inequalities suitable for the specific form of UCBs they use.\nAs an example, the UCB1 algorithm (Auer et al.,\n2002) uses UCBs of the form Bi,s,t = µ̂i,s +√ 2 log(t)/s, where µ̂i,s = 1 s ∑s t=1 h ′ i,t is the average of the first s observed rewards. Using this UCB in our decision rule (6), we can bound the regret of the resulting algorithm (called Delayed-UCB1) in the delayed setting:\nTheorem 7. For any n ≥ 1, the expected regret of the Delayed-UCB1 algorithm is bounded by\nE [Rn] ≤ ∑\ni:∆i>0\n[ 8 logn\n∆i + 3.5∆i\n] + K∑\ni=1\n∆iE [ G∗i,n ] .\nNote that the last term in the bound is the additive penalty, and, under different assumptions, it can be bounded in the same way as after Theorem 6. The proof of this theorem, as well as a similar regret bound for the delayed version of the KL-UCB algorithm (Garivier & Cappé, 2011) can be found in Appendix B."
    }, {
      "heading" : "5. Conclusion and future work",
      "text" : "We analyzed the effect of feedback delays in online learning problems. We examined the partial monitoring case (which also covers the full information and the bandit settings), and provided general algorithms that transform forecasters devised for the non-delayed case into ones that handle delayed feedback. It turns out that the price of delay is a multiplicative increase in the regret in adversarial problems, and only an additive increase in stochastic problems. While we believe that these findings are qualitatively correct, we do not have lower bounds to prove this (matching lower bounds are available for the full information case only).\nIt also turns out that the most important quantity that determines the performance of our algorithms is G∗n, the maximum number of missing rewards. It is interesting to note that G∗n is the maximum number of servers used in a multi-server queuing system with infinitely many servers and deterministic arrival times. It is also the maximum deviation of a certain type of Markov chain. While we have not found any immediately applicable results in these fields, we think that applying techniques from these areas could lead to an improved understanding of G∗n, and hence an improved analysis of online learning under delayed feedback."
    }, {
      "heading" : "6. Acknowledgements",
      "text" : "This work was supported by the Alberta Innovates Technology Futures and NSERC."
    }, {
      "heading" : "A. Proof of Lemma 4",
      "text" : "In this appendix we prove Lemma 4 that was used in the i.i.d. partial monitoring setting (Section 3.2). To that end, we will first need two other lemmas. The first lemma shows that the i.i.d. property of a sequence of random variables is preserved under an independent random reordering of that sequence.\nLemma 8. Let (Xt)t∈N, be a sequence of independent, identically distributed random variables. If we reorder this sequence according to an independent random permutation, then the resulting sequence is i.i.d. with the same distribution as (Xt)t∈N.\nProof. Let the reordered sequence be denoted by (Zt)t∈N. It is sufficient to show that for all n ∈ N, for all y1, y2, . . . , yn, we have\nP {Z1 ≤ y1, Z2 ≤ y2, . . . , Zn ≤ yn} = P {X1 ≤ y1, X2 ≤ y2, . . . , Xn ≤ yn} .\nSince (Xt)t∈N is i.i.d., for any fixed permutation the equation above holds as both sides are equal to Πnt=1P {X1 ≤ yt}. Since the permutations are independent of the sequence (Xt)t∈N, using the law of total probability this extends to the general case as well.\nWe also need the following result (Doob, 1953, Page 145, Chapter III, Theorem 5.2).\nLemma 9. Let (Xt)t∈N be a sequence of i.i.d. random variables, and (X ′ t)t∈N be a subsequence of it such that the decision whether to include Xt in the subsequence is independent of future values in the sequence, i.e., of Xs for s ≥ t. Then the sequence (X ′t)t∈N is an i.i.d. sequence with the same distribution as (Xt)t∈N.\nWe can now proceed to the proof of Lemma 4.\nProof of Lemma 4. Let (Zi,t)t∈N be the sequence resulting from sorting the variables hi,t by their possible observation times t+ τi,t (that is, Zi,1 is the earliest feedback that can be observed if i is predicted at the appropriate time, and so on). Since delays are independent of the outcomes, they define an independent reordering on the sequence of feedbacks. Hence, by Lemma 8, (Zi,t)t∈N is an i.i.d. sequence with the same distribution as (hi,t)t∈N. Note that (h′i,s)s∈N, the sequence of feedbacks (sorted by their observation times) that the agent observes for predicting i, is a subsequence of (Zi,t)t∈N where the decision whether to include each Zi,t in the subsequence cannot depend on future possible observations Zi,t′ , t\n′ ≥ t. Also, the feedbacks of other predictions that are used in this decision were assumed to be independent of (Zi,t)t∈N. Hence, by Lemma 9, (h ′ i,s)s∈N is an i.i.d. sequence with the same distribution as (Zi,t)t∈N, which in turn has the same distribution as (hi,t)t∈N."
    }, {
      "heading" : "B. UCB for the Multi-Armed Bandit Problem with Delayed Feedback",
      "text" : "This appendix details the framework we described in Section 4 for analyzing UCB-type algorithms in the delayed settings, and provides the missing proofs. The regret of a UCB algorithm is usually analyzed by upper bounding the (expected) number of times a suboptimal prediction is made, and then using Equation (2) to get an expected regret bound. Consider a UCB algorithm with upper confidence bounds Bi,s,t, and fix a suboptimal prediction i. The typical analysis (e.g., by Auer et al. (2002)) considers the case when this prediction is made for at least ℓ > 1 times (for a large enough ℓ), and uses concentration inequalities suitable for the specific form of the upper-confidence bound to show that it is unlikely to make this suboptimal prediction more than ℓ times because observing ℓ samples from its reward distribution suffices to distinguish it from the optimal prediction with high confidence. This value ℓ thus gives an upper bound on the expected number of times i is predicted. Examples of such concentration inequalities include Hoeffding’s inequality (Hoeffding, 1963) and Theorem 10 of Garivier & Cappé (2011), which are used for the UCB1 and KL-UCB algorithms, respectively.\nMore precisely, the general analysis of UCB-type algorithms in the non-delayed setting works as follows: for ℓ > 1, we have Ti(n) ≤ ℓ+ ∑n t=1 I {at = i, Ti(t) > ℓ}, where the sum on the right hand side captures how much larger than ℓ the value of Ti(n) is (recall that Ti(t) is the number of times i is predicted up to and including time t). Whenever i is predicted, its UCB, Bi,Ti(t−1),t, must have been greater than that of an optimal prediction,\nBi∗, Ti∗ (t−1),t, which implies\nTi(n) ≤ ℓ+ n∑\nt=1\nI{Bi,Ti(t−1),t ≥ Bi∗,Ti∗(t−1),t, at = i, Ti(t− 1) ≥ ℓ}. (7)\nThe expected value of the summation on the right-hand-side is then bounded using concentration inequalities as mentioned above.\nIn the delayed-feedback setting, if we use upper confidence bounds Bi,Si(t−1),t instead (where Si(t) was defined to be the number of rewards observed up to and including time instant t), in the same way as above we can write\nTi(n) ≤ ℓ+ n∑\nt=1\nI{Bi,Si(t−1),t ≥ Bi∗,Si∗(t−1),t, at = i, Ti(t− 1) ≥ ℓ}.\nSince Ti(t− 1) = Gi,t + Si(t− 1), with ℓ′ = ℓ−G∗i,n we get\nTi(n) ≤ ℓ′ +G∗i,n + n∑\nt=1\nI{Bi,Si(t−1),t ≥ Bi∗,Si∗(t−1),t, at = i, Si(t− 1) ≥ ℓ′}. (8)\nNow the same concentration inequalities used to bound (7) in the analysis of the non-delayed setting can be used to upper bound the expected value of the sum in (8). Putting this into (2), we see that one can reuse the same upper confidence bound in the delayed setting (with only the observed rewards) and get a performance similar to the non-delayed setting, with only an additive penalty that depends on the delays. The following two sections demonstrate the use of this method on two UCB-type algorithms.\nB.1. UCB1 under delayed feedback: Proof of Theorem 7\nBelow comes the proof of Theorem 7 for the Delayed-UCB1 algorithm (Section 4).\nProof of Theorem 7. Following the outline of the previous section, we can bound the summation in (8) using the same analysis as in the original UCB1 paper (Auer et al., 2002). In particular, for any prediction i we can write\nn∑\nt=1\nI { Bi,Si(t−1),t ≥ Bi∗,Si∗(t−1),t, Si(t− 1) ≥ ℓ′ }\n≤ n∑\nt=1\nI { Bi∗,Si∗(t−1),t ≤ µi∗ , Si(t− 1) ≥ ℓ′ } +\nn∑\nt=1\nI { Bi,Si(t−1),t ≥ µi∗ , Si(t− 1) ≥ ℓ′ } . (9)\nThe event in the second summation implies that either µi + 2\n√ 2 log(t)\nSi(t− 1) > µi∗ or µ̂i,Si(t−1) −\n√ 2 log(t)\nSi(t− 1) ≥ µi\n(otherwise we will have Bi,Si(t−1),t < µi∗). Hence,\n(9) ≤ n∑\nt=1\nI { µ̂i∗,Si∗(t−1) + √ 2 log(t)\nSi∗(t− 1) ≤ µi∗\n} +\nn∑\nt=1\nI { µ̂i,Si(t−1) − √ 2 log(t)\nSi(t− 1) ≥ µi\n} +\nn∑\nt=1\nI { µi + 2 √ 2 log(t)\nSi(t− 1) > µi∗ , Si(t− 1) ≥ ℓ′\n} . (10)\nChoosing ℓ′ =\n⌈ 8 log(n)\n∆2i\n⌉ makes the events in the last summation above impossible, because Si(t − 1) ≥ ℓ′ ≥\n8 log(n)\n∆2i which implies 2\n√ 2 log(t)\nSi(t− 1) ≤ 2\n√ 2 log(n)\nℓ′ ≤ ∆i. Therefore, combining with (8), we can write\nTi(n) ≤ ⌈ 8 log(n)\n∆2i\n⌉ +G∗i,n + n∑\nt=1\nt∑\ns=1\n( I { µ̂i∗,s + √ 2 log(t)\ns ≤ µi∗\n} + I { µ̂i,s − √ 2 log(t)\ns ≥ µi\n}) .\nTaking expectation gives\nE [Ti(n)] ≤ ⌈ 8 log(t)\n∆2i\n⌉ + E [ G∗i,n ] + n∑\nt=1\nt∑\ns=1\n( P { µ̂i∗,s + √ 2 log(t)\ns ≤ µi∗\n} + P { µ̂i,s − √ 2 log(t)\ns ≥ µi\n}) .\nAs in the original analysis, Hoeffding’s inequality (Hoeffding, 1963) can be used to bound each of the probabilities in the summation, to get\nP { µ̂i∗,s + √ 2 log(t)\ns ≤ µi∗\n} ≤ e−4 log(t) = t−4,\nP { µ̂i,s − √ 2 log(t)\ns ≥ µi\n} ≤ e−4 log(t) = t−4.\nTherefore, we have\nE [Ti(n)] ≤ ⌈ 8 log(t)\n∆2i\n⌉ + E [ G∗i,n ] + ∞∑\nt=1\n2t−3 ≤ 8 log(n) ∆2i + 1 + E [ G∗i,n ] + 2ζ(3),\nwhere ζ(3) < 1.21 is the Riemann Zeta function.3 Combining with (2) proves the theorem.\nB.2. KL-UCB under delayed feedback\nThe KL-UCB algorithm was introduced by Garivier & Cappé (2011). The upper confidence bound used by KL-UCB for predicting i at time t is Bi,Ti(t−1),t , where Bi,s,t is\nmax {q ∈ [µ̂i,s, 1] : sd(µ̂i,s, q) ≤ log t+ 3 log(log t)} ,\nwith d(p, q) = p log(p q ) + (1− p) log(1−p1−q ) the KL-divergence of two Bernoulli random variables with parameters p and q. In their Theorem 2, Garivier & Cappé (2011) show that there exists a constant C1 ≤ 10, as well as functions 0 ≤ C2(ǫ) = O(ǫ−2) and 0 ≤ β(ǫ) = O(ǫ2), such that for any ǫ > 0, the expected regret of the KL-UCB algorithm (in the non-delayed setting) satisfies\nE [Rn] ≤ ∑\n∆i>0\n∆i\n[ log(n)\nd(µi, µi∗) (1 + ǫ) + C1 log(log n) +\nC2(ǫ) nβ(ǫ)\n] . (11)\nUsing this upper confidence bound with (6), we arrive at the Delayed-KL-UCB algorithm. For this algorithm, we can prove the following regret bound using the general scheme described above together with the same techniques used by Garivier & Cappé (2011), again obtaining an additive penalty compared to the non-delayed setting.\nTheorem 10. For any ǫ > 0, the expected regret of the Delayed-KL-UCB algorithm after n time instants satisfies\nE [Rn] ≤ ∑\ni:∆i>0\n∆i\n( log(n)\nd(µi, µi∗) (1 + ǫ) + C1 log(log(n))\n) + K∑\ni=1\n∆i\n( C2(ǫ)\nnβ(ǫ) E [ G∗i,n ] + E [ G∗i,n ] + 1\n) ,\nwhere C1, C2, and β are the same as in (11).\n3For properties and theory of the Riemann Zeta function, see the book of Titchmarsh & Heath-Brown (1987).\nIn this case, working out the proof and reusing the analysis is somewhat more complicated compared to UCB1. In particular, we will need an adaptation of Lemma 7 of Garivier & Cappé (2011), which is captured by the following lemma.\nLemma 11. Let d+(x, y) = d(x, y)I {x < y}. Then for any n ≥ 1, n∑\nt=1\nI { at = i, µi∗ ≤ Bi∗,Si∗ (t−1),t, Si(t− 1) ≥ ℓ′ } ≤ G∗i,n\nn∑\ns=ℓ′\nI { sd+(µ̂i,s, µi∗) < log(n) + 3 log(log(n)) } .\nProof of Lemma 11. We start in the same way as the original proof. Note that d+(p, q) is non-decreasing in its second parameter, and that at = i and µi∗ ≤ Bi∗,Si∗ (t−1),t together imply Bi,Si(t−1),t ≥ Bi∗,Si∗(t−1),t ≥ µi∗ , which in turn gives\nSi(t− 1)d+(µ̂i,Si(t−1), µi∗) ≤ Si(t− 1)d(µ̂i,Si(t−1), Bi,Si(t−1),t) ≤ log(t) + 3 log(log(t)).\nTherefore, we have\nn∑\nt=1\nI { at = i, µi∗ ≤ Bi∗,Si∗ (t−1),t, t > Si(t− 1) ≥ ℓ′ }\n≤ n∑\nt=ℓ′\nI { at = i, Si(t− 1)d+(µ̂i,Si(t−1), µi∗) ≤ log(t) + 3 log(log(t)), Si(t− 1) ≥ ℓ′ }\n≤ n∑\nt=ℓ′\nI { at = i, Si(t− 1)d+(µ̂i,Si(t−1), µi∗) ≤ log(n) + 3 log(log(n)), Si(t− 1) ≥ ℓ′ }\n≤ n∑\nt=ℓ′\nt∑\ns=ℓ′\nI {at = i, Si(t− 1) = s} × I { sd+(µ̂i,s, µi∗) ≤ log(n) + 3 log(log(n)) }\n=\nn∑\ns=ℓ′\nn∑\nt=s\nI {at = i, Si(t− 1) = s} × I { sd+(µ̂i,s, µi∗) ≤ log(n) + 3 log(log(n)) }\n=\nn∑\ns=ℓ′\nI { sd+(µ̂i,s, µi∗) ≤ log(n) + 3 log(log(n)) } × ( n∑\nt=s\nI {at = i, Si(t− 1) = s} ) .\nBut note that the second summation is bounded by G∗i,n, because for each s, there cannot be more than G ∗ i,n time instants at which i is predicted and Si(t) = s remained constant; otherwise for some t ′ ∈ {s, . . . , n} we would have Ti(t ′ − 1) − Si(t′ − 1) = Gi,t′ > G∗i,n, which is not possible. Substituting this bound in the last expression proves the lemma.\nWe also recall the following two results from the original paper.\nTheorem 12 (Theorem 10 of Garivier & Cappé (2011)). Let (Yt), t ≥ 1 be a sequence of independent random variables bounded in [0, 1], with common expectation µ = E [Yt]. Consider a sequence (ǫt), t ≥ 1 of Bernoulli variables such that for all t > 0, ǫt is a random function of Y1, . . . , Yt−1\n4, and is independent of Ys, s ≥ t. Let δ > 0 and for every 1 ≤ t ≤ n, let\nSt = t∑\ns=1\nǫs and µ̂t = ∑t s=1 ǫsYs St ,\nwith µ̂t = 0 when St = 0, and Bn = max {q > µ̂n : Snd(µ̂n, q) ≤ δ} .\nThen P {Bn < µ} ≤ e⌈δ log(n)⌉e−δ.\n4That is, a function of Y1, . . . , Yt−1 together with possibly an extra, independent randomization.\nLemma 13 (Lemma 8 of Garivier & Cappé (2011)). For a suboptimal prediction i, for every ǫ > 0, let Kn =⌊ 1 + ǫ\nd+(µi, µi∗)\n( log(n) + 3 log(log(n)) )⌋ . Then there exist C2(ǫ) > 0 and β(ǫ) > 0 such that\n∞∑\ns=Kn+1\nP { d+(µ̂i,s, µi∗) < d(µi, µi∗)\n1 + ǫ\n} ≤ C2(ǫ)\nnβ(ǫ) .\nNow, we are ready to prove Theorem 10 by reusing the same techniques as in the original paper.\nProof of Theorem 10. For a suboptimal prediction i, bounding the terms in (8) gives\nn∑\nt=1\nI { at = i, Bi,Si(t−1),t ≥ Bi∗,Si∗(t−1),t, Si(t− 1) ≥ ℓ′ }\n≤ n∑\nt=1\nI { Bi∗,Si∗ (t−1),t < µi∗ } +\nn∑\nt=1\nI { at = i, µi∗ ≤ Bi∗,Si∗ (t−1),t, Si(t− 1) ≥ ℓ′ }\n≤ n∑\nt=1\nI { Bi∗,Si∗ (t−1),t < µi∗ } +G∗i,n n∑\ns=ℓ′\nI { sd+(µ̂i,s, µi∗) < log(n) + 3 log(log(n)) } , (12)\nwhere the last inequality follows from Lemma 11. Let\nKn =\n⌊ 1 + ǫ\nd(µi, µi∗)\n( log(n) + 3 log(log(n)) )⌋ , (13)\nand note that d(µi, µi∗) = d +(µi, µi∗). Let ℓ ′ = 1 +Kn. Then we have:\nn∑\ns=ℓ′\nI { sd+(µ̂i,s, µi∗) ≤ log(n) + 3 log(log(n)) }\n≤ ∞∑\ns=Kn+1\nI { (Kn + 1)d +(µ̂i,s, µi∗) ≤ log(n) + 3 log(log(n)) }\n≤ ∞∑\ns=Kn+1\nI { d+(µ̂i,s, µi∗) < d(µi, µi∗)\n1 + ǫ\n} . (14)\nPutting the value of ℓ′ and inequalities (13) and (14) back into (12) and combining with (8), we get\nE [Ti(n)] ≤ 1 + ǫ\nd(µi, µi∗)\n( log(n) + 3 log(log(n)) ) + E [ G∗i,n ] + 1+\nn∑\nt=1\nP { Bi∗,Si∗(t−1),t < µi∗ } + E [ G∗i,n ] ∞∑\ns=Kn+1\nP { d+(µ̂i,s, µi∗) < d(µi, µi∗)\n1 + ǫ\n} ,\nwhere the last term is a result of the delays being independent of the rewards. The first summation can be bounded using Theorem 12, for which it suffices to set ǫt = 1, 1 ≤ t ≤ n, and use the sequence of observed rewards (h′i,t) for the arm under consideration as the sequence (Yt) in the theorem. In the same way as the analysis of Garivier & Cappé (2011), this gives an upper bound of the form C′1 log(logn) with the same value of C′1 ≤ 7 as in the non-delayed setting. The second summation can be bounded by Lemma 13. Therefore, the expected number of times a suboptimal prediction is made is bounded by:\nE [Ti(n)] ≤ 1 + ǫ\nd(µi, µi∗)\n( log(n) + 3 log(log(n)) ) + C′1 log(log(n)) + C2(ǫ)\nnβ(ǫ) E [ G∗i,n ] + E [ G∗i,n ] + 1.\nCombining this with (2) and letting C1 = C ′ 1 + 3 finishes the proof."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolò", "Fischer", "Paul" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Cesa-Bianchi", "Nicolò", "Lugosi", "Gábor" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2006
    }, {
      "title" : "Stochastic Processes",
      "author" : [ "Doob", "Joseph L" ],
      "venue" : null,
      "citeRegEx" : "Doob and L.,? \\Q1953\\E",
      "shortCiteRegEx" : "Doob and L.",
      "year" : 1953
    }, {
      "title" : "The KL-UCB algorithm for bounded stochastic bandits and beyond",
      "author" : [ "Garivier", "Aurélien", "Cappé", "Olivier" ],
      "venue" : "In Proceedings of the 24th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Garivier et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Garivier et al\\.",
      "year" : 2011
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "Hoeffding", "Wassily" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hoeffding and Wassily.,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding and Wassily.",
      "year" : 1963
    }, {
      "title" : "Slow learners are fast",
      "author" : [ "Langford", "John", "Smola", "Alexander", "Zinkevich", "Martin" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Langford et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2009
    }, {
      "title" : "Improving on-line learning",
      "author" : [ "Mesterharm", "Chris J" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Mesterharm and J.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mesterharm and J.",
      "year" : 2007
    }, {
      "title" : "The Theory of the Riemann ZetaFunctions",
      "author" : [ "Titchmarsh", "Edward Charles", "Heath-Brown", "David Rodney" ],
      "venue" : null,
      "citeRegEx" : "Titchmarsh et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Titchmarsh et al\\.",
      "year" : 1987
    }, {
      "title" : "On delayed prediction of individual sequences",
      "author" : [ "Weinberger", "Marcelo J", "Ordentlich", "Erik" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Weinberger et al\\.,? \\Q1959\\E",
      "shortCiteRegEx" : "Weinberger et al\\.",
      "year" : 1959
    }, {
      "title" : "UCB1 under delayed feedback: Proof of Theorem 7 Below comes the proof of Theorem 7 for the Delayed-UCB1 algorithm (Section 4). Proof of Theorem 7. Following the outline of the previous section, we can bound the summation in (8) using the same analysis as in the original UCB1 paper",
      "author" : [ "algorithms. B" ],
      "venue" : "(Auer et al.,",
      "citeRegEx" : "B.1.,? \\Q2002\\E",
      "shortCiteRegEx" : "B.1.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "To resolve this problem, we work out modifications of variants of the UCB algorithm (Auer et al., 2002) for stochastic bandit problems with delayed feedback that have much smaller complexity than the black-box algorithms.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Stochastic Feedback General (Adversarial) Feedback L No R(n) ≤ R(n) +O(E [ τ t ] ) R(n) ≤ O(τconst)×R(n/τconst) Side (Agarwal & Duchi, 2011) (Weinberger & Ordentlich, 2002) Full Info Info (Langford et al., 2009) (Agarwal & Duchi, 2011) L L Side Info R(n) ≤ R(n) +O(D) R(n) ≤ O(D̄)×R′(n/D̄) (Mesterharm, 2007) (Mesterharm, 2007) No Side R(n) ≤ C1R(n) + C2τmax log(τmax) R(n) ≤ O(τconst)×R(n/τconst) Bandit Info (Desautels et al.",
      "startOffset" : 188,
      "endOffset" : 211
    }, {
      "referenceID" : 5,
      "context" : "Langford et al. (2009) showed that under the usual conditions, a sufficiently slowed-down version of the mirror descent algorithm achieves optimal decay rate of the average regret.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "Langford et al. (2009) showed that under the usual conditions, a sufficiently slowed-down version of the mirror descent algorithm achieves optimal decay rate of the average regret. Mesterharm (2005; 2007) considered another variant of the full information setting, using an adversarial model on the delays in the label prediction setting, where the forecaster has to predict the label corresponding to a side information vector xt. While in the full information online prediction problem Weinberger & Ordentlich (2002) showed that the regret increases by a multiplicative factor of τconst, in the work of Mesterharm (2005; 2007) the important quantity becomes the maximum/average gap defined as the length of the largest time interval the forecaster does not receive feedback.",
      "startOffset" : 0,
      "endOffset" : 519
    }, {
      "referenceID" : 5,
      "context" : "Langford et al. (2009) showed that under the usual conditions, a sufficiently slowed-down version of the mirror descent algorithm achieves optimal decay rate of the average regret. Mesterharm (2005; 2007) considered another variant of the full information setting, using an adversarial model on the delays in the label prediction setting, where the forecaster has to predict the label corresponding to a side information vector xt. While in the full information online prediction problem Weinberger & Ordentlich (2002) showed that the regret increases by a multiplicative factor of τconst, in the work of Mesterharm (2005; 2007) the important quantity becomes the maximum/average gap defined as the length of the largest time interval the forecaster does not receive feedback. Mesterharm (2005; 2007) also shows that the minimax regret in the adversarial case increases multiplicatively by the average gap, while it increases only in an additive fashion in the stochastic case, by the maximum gap. Agarwal & Duchi (2011) considered the problem of online stochastic optimization and showed that, for i.",
      "startOffset" : 0,
      "endOffset" : 1021
    }, {
      "referenceID" : 0,
      "context" : "We consider the stochastic multi-armed bandit problem, and extend the UCB family of algorithms (Auer et al., 2002; Garivier & Cappé, 2011) to the delayed setting.",
      "startOffset" : 95,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "As an example, the UCB1 algorithm (Auer et al., 2002) uses UCBs of the form Bi,s,t = μ̂i,s + √ 2 log(t)/s, where μ̂i,s = 1 s ∑s t=1 h ′ i,t is the average of the first s observed rewards.",
      "startOffset" : 34,
      "endOffset" : 53
    } ],
    "year" : 2013,
    "abstractText" : "Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.",
    "creator" : "LaTeX with hyperref package"
  }
}