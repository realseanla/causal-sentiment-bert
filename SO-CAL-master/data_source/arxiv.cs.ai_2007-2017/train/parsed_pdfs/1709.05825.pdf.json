{
  "name" : "1709.05825.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Yuyi Wang", "Jesse Davis", "Steven Schockaert" ],
    "emails" : [ "KuzelkaO@cardiff.ac.uk", "yuwang@ethz.ch", "jesse.davis@cs.kuleuven.be", "SchockaertS1@cardiff.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 9.\n05 82\n5v 1\n[ cs\n.A I]\n1 8\nSe p\n20 17"
    }, {
      "heading" : "Introduction",
      "text" : "Statistical Relational Learning (SRL, Getoor and Taskar, eds., 2007) is concerned with learning probabilistic models of relational data. Many popular SRL frameworks, such as Markov Logic Networks (MLNs, Richardson and Domingos 2006), use weighted logical formulas to encode statistical regularities that hold for the considered problem. Typically, the maximum (pseudo-)likelihood weights of the formulas are estimated from training data, which is usually a single large example (e.g. a social network). This is problematic for two reasons. First, the weights that are learned from this single training example are in general not optimal for examples of different sizes. This turns out to be a fundamental problem, which can not simply be solved by rescaling the weights (Shalizi and Rinaldo 2013). Second, without making further assumptions, it is difficult to provide any statistical guarantees about the learned weights. In this paper, we approach parameter estimation in SRL from a novel direction, by introducing the notion of a relational marginal problem. In the propositional\nCopyright c© 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ncase (Wainwright and Jordan 2008), marginal problems entail finding a maximum-entropy distribution which has the givenmarginal probabilities. A well-known property of such problems is that they are dual to the maximum-likelihood estimation of the parameters of an undirected graphical model (where “dual” is in the sense of convex optimization). In relational marginal problems, we are similarly looking for a maximum-entropy distribution which satisfies some given statistics – relational marginals. However, we also need to define what these relational marginals are. Thus, first, we describe two different types of relational marginals, which differ in the kinds of statistics that are provided. The first type is based on relational marginal distributions (Kuželka, Davis, and Schockaert 2017) and the second is based on Halpern-style random substitution semantics (Bacchus et al. 1992). Second, for both types of statistics, we establish a relational counterpart of the duality between maximum-likelihood estimation and max-entropy marginal problems. Interestingly, for the latter model, the corresponding dual is MLNs.\nThird, the relational marginal perspective allows us to learn parameters for domains that have different sizes (i.e., number of constants) than the training data. The basic idea to achieve this is simple. We assume that the training data is a sample of the data that we want to model. For example, imagine trying to model all of Facebook based on a sampled subset of Facebook users along with all relations among them. Assuming the sample is a large enough and was obtained in a suitable way (which is not always the case in practice – we discuss this issue later), the parameters of the marginals estimated from the sample should be close to the respective parameters for the whole network. Then, instead of using a model learned by optimizing the likelihood on the training data, we use a model obtained as a solution of the corresponding relational marginal problemwith a domain of the required size. We may end up with estimated parameters for which the relational marginal problem has no solution. Therefore, we propose a method for adjusting the estimated parameters that enables a solution and characterize its effect on the estimates. Then we also the relational marginal polytopes, which allows us to provide conditions under which the unbiased unadjusted estimate will be valid (“realizable”) for domains of any size.\nIn addition, the relational marginal view of the param-\neter learning problem, can be thought of as consisting of two decoupled problems: estimation of the parameters of marginals and optimization to obtain the max-entropy distributions. Thus, to better understand parameter learning from relational data, it is important to characterize how accurate the estimates are. Assuming that all subsamples of the data being modeled are sampled with the same probability, we derive bounds on expected error, that is, the expected difference between the parameters obtained from the subsample and parameters that could be theoretically computed if the whole dataset were accessible (e.g. the whole Facebook). From this, we can also obtain lower-bounds on the effective sample size for relational data. The paper is structured around addressing the following four questions about relational marginal problems:\n1. What should the relational marginals be? (Section Two Types of Relational Marginals)\n2. What are the max-entropy distributions with the given relational marginals, and how can we find them? (Section Max-Entropy Models)\n3. When are relational marginal problems realizable, and how can we adjust them when they are not? How can we adjust learning to account for differences in domain sizes? (Section Realizability)\n4. How accurate are the parameter estimates of relational marginals, and what are the links with realizability? (Sections Relational Marginal Polytopes and Estimation)\nProofs of all propositions stated in the main text and details of the duality derivations are in the appendix located in the supplementary material."
    }, {
      "heading" : "Preliminaries",
      "text" : "This paper considers a function-free first-order logic language L, which is built from a set of constants Const, variables Var and predicates Rel = ⋃ i Reli, where Reli contains the predicates of arity i. We assume an untyped language and use the domain size to refer to the |Const|. For a1, ..., ak ∈ Const∪Var andR ∈ Relk, we callR(a1, ..., ak) an atom. If a1, .., ak ∈ Const, this atom is called ground. A literal is an atom or its negation. A clause is a disjunction of literals. We use vars(α) to denote the variables that appear in a formula α. The formula α0 is called a grounding of α if α0 can be obtained by replacing each variable in α with a constant from Const. A formula is called closed if all variables are bound by a quantifier. A possible world ω is defined as a set of ground atoms. The satisfaction relation |= is defined in the usual way. A substitution is a mapping from variables to terms. An injective substitution is a substitution which does not map any two variables to the same term. As commonly done in statistical relational learning, we use the unique names assumption, meaning that c1 6= c2 whenever c1 and c2 are different constants. A first-order universally quantified formula α is said to be proper if αϑ is trivially false whenever ϑ is not injective. For instance, the formula ∀X,Y : fr(X,Y ) is not proper whereas the formula ∀X,Y : fr(X,Y ) ∧X 6= Y is proper. In what follows we sometimes omit writing the\nuniversal quantifiers explicitly for universally quantified formulas and write simply, e.g. fr(X,Y ) ∧X 6= Y . A (global) example is a pair (A, C), with C a set of constants and A a set of ground atoms which only use constants from C. Let Υ = (A, C) be an example and S ⊆ C. The fragment Υ〈S〉 = (B,S) is defined as the restriction of Υ to the constants in S, i.e. B is the set of all atoms fromAwhich only contain constants from S. Two examples Υ1 = (A1, C1) and Υ2 = (A2, C2) are isomorphic, denoted as Υ1≈Υ2, if there exists a bijection σ : C1 → C2 such that σ(A1) = A2, where σ is extended to ground atoms in the usual way.When C is a set of constants andΦ0 a set of closed formulas, Π(C,Φ0) denotes the set of all Υ = (A, C) such that Υ |= Φ0 (we can think of Φ0 as a set of constraints)."
    }, {
      "heading" : "Two Types of Relational Marginals",
      "text" : "Typically, parameters for a statistical relational model are estimated from a single example of a relational structure that consists of a large set of ground atoms A. Intuitively, the goal is to learn a probability distribution of such relational structure. The challenge is how to estimate the distribution from a single example. One solution is based on the assumption that the relational structure has repeated regularities. Then, statistics about these regularities can be computed for small substructures of the train example and used to construct a distribution over large relational structures. Thus, the next issue is how to construct the fragments and compute statistics on them. Next, we discuss two possible ways to do so, which we will refer to as Model A and Model B."
    }, {
      "heading" : "Model A",
      "text" : "The first approach to constructing fragments is from (Kuželka, Davis, and Schockaert 2017). It repeatedly samples subsets S ⊆ C of the constants from the given example Υ = (A, C) and then builds one training example Υ〈S〉 for each S. However, the training examples must consider isomorphic classes of constants to account for the fact that each fragment will contain different constants. This is accomplished by using the notion of local examples.\nDefinition 1 (Local example). Let k ∈ N. A local example of width k is a pair ω = (A, {1, ..., k}), where A is a set of ground atoms that contain only constants from the set {1, 2, . . . , k}. For an example Υ = (A, C) and S ⊆ C, we write Υ[S] for the set of all local examples of width |S| which are isomorphic to Υ〈S〉. To distinguish local examples from global examples, we will denote them using lower case Greek letters such as ω instead of upper case letters such as Υ.\nExample 1. For Υ = ({fr(alice, bob), fr(bob, alice), fr(bob, eve), fr(eve, bob), sm(alice)}, {alice, bob, eve}), we have: Υ〈{alice, bob}〉= ({sm(alice), fr(alice, bob), fr(bob, alice)}, {alice, bob}), Υ[{alice, bob}]= {({fr(1, 2), fr(2, 1), sm(1)}, {1, 2}), {({fr(2, 1), fr(1, 2), sm(2)}, {1, 2})}. This leads to a natural definition of a probability distribution over local examples of width k.\nDefinition 2 (Relational marginal distribution of a global example). Let Υ = (A, C) be an example and k ∈ N. The relational marginal distribution of Υ of width k is a distribution PΥ,k over local examples, where PΥ,k(ω) is defined as the probability that ω is sampled by the following process: (i) Uniformly sample a subset S of k constants from C. (ii) Uniformly sample a local example ω from the set Υ[S]. For a closed formula α without constants, we also define, its probability: PΥ,k(α) = ∑ ω:ω|=α PΥ,k(ω).\nWe will call a pair (α, p), where α is a constant-free closed formula and p ∈ [0; 1], a relational marginal constraint. We may also interpret the probability PΥ,k(α) of a closed constant-free formula α as the probability that α is true in a restriction Υ〈S〉 of Υ to a randomly sampled subset S of k constants fromΥ. Thus, if we are only interested in the probabilities of closed constant-free formulas, we do not have to refer to local examples. Local examples are important because relational marginal distributions defined using them are themselves probability distributions on possible worlds, which is both nice conceptually and convenient (as it means that we can model relational marginals of Model A using any standard propositional probabilistic model).\nGlobal examplesmay also be assumed to be sampled from some distribution and we define the corresponding marginal distributions induced by such distributions accordingly. When P (Υ) is a distribution over finite global examples from a possibly countably infinite set Ω, then the marginal distribution of width k is a distribution Pk over local examples where Pk(ω) is defined as Pk(ω) = ∑ Υ∈Ω P (Υ) · PΥ,k(ω). For a closed formula α without constants, we also analogically define: Pk(α) = ∑ ω:ω|=α Pk(ω). In other words, a relational marginal distribution is a mixture of (possibly countably many) relational marginal distributions of global examples.\nProposition 2. Let P (Υ) be a distribution on domain size n and k ≤ n be an integer. Let Ωα = {Υ〈S〉 : S ⊆ C, |S| = k,Υ〈S〉 |= α} where Υ = (A, C) is sampled according to the distribution P (Υ). Then, for a closed formula α, p̂α = |Ωα| · (\nn k\n)−1 is an unbiased estimate of Pk(α)\nfor Model A."
    }, {
      "heading" : "Model B",
      "text" : "The second approach is to consider random substitutions, which is in the spirit of existing works (Bacchus et al. 1992; Schulte et al. 2014). Here, the statistics that we collect about Υ are defined as follows.\nDefinition 3 (Probability of formulas under Model B). Let Υ = (A, C) be a global example and α be a universally quantified formula. Let Pϑ be a uniform distribution on injective substitutions from the set Θα = {ϑ|ϑ : vars(α) → C and ϑ is injective}. Then the probability Q(α) of the formula α under model B is defined as\nQΥ(α) = ∑\nϑ∈Θα\n1(Υ |= αϑ)Pϑ(ϑ) = 1 |Θ| ∑\nϑ∈Θα\n1(Υ |= αϑ).\nJust like for Model A, we extend the definition of the probability of formulas straightforwardly to the case where Υ is not fixed but sampled from some distribution over a countable set Ω: Q(α) = ∑ Υ∈ΩQΥ(α) · P (Υ). Example 3. Let Υ be as in Example 1. Let\nα = ∀A,B : ¬fr(A,B) ∨ sm(B), β = ∀A,B : ¬fr(A,B) ∨ sm(A) ∨ sm(B).\nAssuming that the relation fr(., .), “friends”, is symmetric, the formula α is classically true if all people who are friends with someone who smokes, and the formula β is classically true if for every pair of people A and B who are friends, at least one of them smokes. Computing the respective probabilities, we get PΥ,2(α) = 1 3 , PΥ,2(β) = 2 3 , QΥ(α) = 1 2 , QΥ(β) = 2 3 , which illustrates that in general the “marginal” probabilities given by the two models will differ. The first model might be slightly easier to interpret as the marginal probabilities P (γ) correspond to fraction of the width-k fragments of Υ in which γ is true as a classical logic formula.\nWe note that the straightforward analogue of Proposition 2 also holds for Model B."
    }, {
      "heading" : "Max-Entropy Models",
      "text" : "In this section we show how to compute models of given relational marginals under Model A and Model B.\nDefinition 4 (Model of relational marginals). Let us have a set of pairs Φ = {(α1, θ1), . . . , (αh, θh)} of relational marginals, where αi is a closed formula and θi ∈ [0; 1]. Let Φ0 be a set of hard rules. We say that a probability distribution P (Υ) over worlds satisfying the hard rules from Φ0 is a width-k model of Φ iff Pk(αi) = θi (Qk(αi) = θi, respectively) for all (αi, θi) ∈ Φ. We will use standard duality arguments from convex optimization (Boyd and Vandenberghe 2004), essentially following (Singh and Vishnoi 2014). The results will in both cases be exponential-family models and the one for Model B will turn out to be equivalent to MLNs."
    }, {
      "heading" : "Model A",
      "text" : "Let C be a set of constants, A be the set of all atoms over C based on some given first-order language and let Ck denote the set of all k-element subsets of C. Next, let Φ be a set of relational marginals and Φ0 be a set of hard constraints, i.e. formulas α such that if Υ 6|= α then P (Υ) = 0. We assume that there exists at least one distribution P ′ which is a model of Φ and which satisfies the following positivity condition: P ′(Υ) > 0 for all Υ satisfying the hard constraints (i.e. those for which Υ |= Φ0).\nsup {PΥ:Υ∈Π(C,Φ0)}\n∑\nΥ∈Πn(C,Φ0)\nPΥ log 1\nPΥ s.t. (1)\n∀(αi, θi) ∈ Φ : 1\n|Ck| ∑\nS∈Ck\n∑\nΥ∈Π(C,Φ0)\n1(Υ〈S〉 |= αi) · PΥ = θi (2)\n∀Υ ∈ Πn(C,Φ0) : PΥ ≥ 0, ∑\nΥ∈Π(C,Φ0)\nPΥ = 1 (3)\nNext we define #k(α,Υ) = |{S ∈ Ck : Υ〈S〉 |= α}|. That is #k(α,Υ) is the number of sets S ∈ Ck such that the formula α is classically true in the restriction of Υ to constants in S. We show in the appendix that the distribution, if it exists, that is a solution to the optimization problem has the following form:\nPΥ = 1\nZ exp\n( ∑\nαi∈Φ\nλi #k(αi,Υ)\n|Ck|\n) . (4)\nThe Lagrangian dual problem of the maximum entropy problem is to maximize (where λi ∈ R): ∑\nαi∈Φ\nλiθi − log ∑\nΥ∈Π(C,Φ0)\ne ∑ αi∈Φ λi #k(αi,Υ) |Ck| (5)\nDue to the positivity assumption, Slater’s condition (Boyd and Vandenberghe 2004) is satisfied and strong duality holds. Consequently, instead of solving the original problem, which has an intractable number of constraints and variables (one variable for each world Υ ∈ Π(C,Φ0)), we can solve the dual problem, which only has |Φ| variables. On the other hand, the optimization criterion of the dual problem may still be computationally hard to solve as it requires weighted counting over worlds in Π(C,Φ0). However, in many restricted, but non-trivial, cases, we can exploit lifted weighted model counting techniques in the same way as they were used for maximum-likelihood estimation in (Van Haaren et al. 2016). Let us perform a change of variableswi := λi/|Cki |. This gives us\nP (Υ) = 1\nZ exp\n( ∑\nαi∈Φ\nwi ·#k(αi,Υ) )\n(6)\nfor the distribution and∑\nαi∈Φ\nwiθi|Cki | − log ∑\nΥ∈Π(C,Φ0)\ne ∑ αi∈Φ wi·#k(αi,Υ) (7)\nfor the optimization criterion of the dual problem. Assuming that the marginals were estimated from a global example Υ̂ ∈ Π(C,Φ0) (note that here the domain C is the same as the domain of the global examples over which the distribution is defined) and that they still satisfy the positivity assumption, we can also rewrite (7) as\n∑\nαi∈Φ\nwi ·#k(αi, Υ̂)−log ∑\nΥ∈Π(C,Φ0)\ne ∑ αi∈Φ wi·#k(αi,Υ) (8)\nIt is straightforward to check that this is the same\nas directly optimizing the log-likelihood of Υ̂. Thus, here we have a relational analogue of the well-known duality of maximum likelihood and maximum entropy (Wainwright and Jordan 2008). Note that it is important for the duality of maximum likelihood and maximum entropy\nthat both the Υ̂, from which we estimated the parameters, and the global examples over which the distribution is computed have the same domain size. The Section Realizability will address cases where the domain sizes of the training and testing data differ."
    }, {
      "heading" : "Model B",
      "text" : "Like for Model A, we can construct a convex optimization problem to obtain a maximum-entropy distribution with the given relational marginals under Model B. This problem’s optimization criterion is the same as (1). To obtain the constraints enforcing the marginals, we can replace in (2) the summation over subsets of constants in C by a summation over substitutions fromΘαi , whereΘαi is defined as in Definition 3, which gives us the following set of constraints for all (αi, θi) ∈ Φ:\n∑\nϑ∈Θαi\n1 |Θαi | ∑\nΥ∈Π(C,Φ0)\n1(Υ |= αiϑ) · PΥ = θi (9)\nUsing basically the same reasoning as for Model A, we arrive at the following form of the probability distribution\nP (Υ) = 1\nZ exp\n( ∑\nαi∈Φ\nwi · n(αi,Υ) )\n(10)\nwhere n(αi,Υ) is the number of groundings αiϑ of the formula αi, and all ϑ’s are injective, which are true in Υ. This is distribution is identical to the one for MLNs which only contain proper formulas1 (because of the injectivity requirement in the definition of Model B). The only difference to the distribution of Model A is the use of n(αi,Υ) instead of #k(αi,Υ). The dual optimization criterion for Model B then becomes to maximize\n∑\nαi∈Φ\nwiθi|Θαi |− log ∑\nΥ∈Π(C,Φ0)\ne ∑ αi∈Φ wi·n(αi,Υ) (11)\nwhich can be rewritten, when θi’s are estimated from some Υ̂ = (Â, Ĉ) with |Ĉ| = |Ĉ|, as ∑αi∈Φwi · n(αi, Υ̂) − log ∑\nΥ∈Π(C,Φ0) e ∑ αi∈Φ wi·n(αi,Υ) which is the same as\nlog-likelihood of Υ̂ w.r.t. the MLN given by (11), again assuming that all the formulas in the MLN are proper. Thus,\nwhen the size of the domain of the training example Υ̂ is the same as the cardinality of the domain of the modeled distribution, the max-entropy relational marginal problem in Model B is the same as maximum-likelihood estimation in MLNs."
    }, {
      "heading" : "Realizability",
      "text" : "Not all relational marginals have a model for a given domain size (or even any model at all). This is a problem if we want to estimate relational marginals on some given global exam-\nple Υ̂ and then use them to obtain a distribution on global examples that have a different domain size. The duality between maximum-likelihood estimation and max-entropy relational marginal problems discussed in the previous section only holds when the training data and the distribution that we want to model have the same domain size. In this section, we will show how to obtain relational marginals for any domain size. Accomplishing this requires\n1It has been shown (Poole et al. 2012) that the setting with proper formulas is equivalent in expressivity to general MLNs.\nreplacing the consistency of relational marginal estimators with a weaker notion. However, we can still bound the difference between the unbiased and the adjusted estimates. More importantly, the difference will tend to zero as the domain size of the examples from which the respective marginals were estimated increases. First, it is easy to see that if a model exists for a given set of marginals and domain size, then there is also such a model for smaller domain sizes, as the next proposition asserts.2\nProposition 4. For Model A, if there is a width-k model of Φ on domain of size n then there is also a width-k model of Φ on domain of size m for any m satisfying n ≥ m ≥ k. For Model B, if there is a model of Φ on domain of size n then there is also a model of Φ on domain of size m for any m satisfying n ≥ m ≥ maxα∈Φ |vars(α)|. Next, we give an example of a relational marginal distributions that does not have a model for arbitrary domain cardinalities.\nExample 5. Let L consist of predicate symbols e/2, r/1, g/1, b/1 and Υ = (A, C) be a global example where\nA = {e(v1, v2), e(v2, v1), e(v2, v3), e(v3, v2), e(v1, v3), e(v3, v1), r(v1), g(v2), b(v3)}, C = {v1, v2, v3}.\nLet k = 2 be the width of local examples. And let F (X1, X2) def = X1 6= X2 ∧ ¬e(X1, X1) ∧ e(X1, X2) ∧ e(X2, X1) ∧ ¬e(X2, X2). Then we can estimate, for instance, the following marginals from Υ under Model A:\nP [∃X1, X2 : F (X1, X2) ∧ r(X1) ∧ ¬g(X1) ∧ ¬b(X1)∧\n∧¬r(X2) ∧ g(X2) ∧ ¬b(X2)] = 1\n3 P [∃X1, X2 : F (X1, X2) ∧ r(X1) ∧ ¬g(X1) ∧ ¬b(X1)∧\n∧¬r(X2) ∧ ¬g(X2) ∧ b(X2)] = 1\n3 P [∃X1, X2 : F (X1, X2) ∧ ¬r(X1) ∧ g(X1) ∧ ¬b(X1)∧\n∧¬r(X2) ∧ ¬g(X2) ∧ b(X2)] = 1\n3\n(which can also be rewritten as probabilities of clauses by negating the existentially quantified conjunctions). The global example Υ can be imagined as a complete directed graph (without self-loops) on 3 vertices v1, v2, v3 where each of the vertices is colored by one of the “colors” r, g, b. We claim that no distribution on global examples satisfies the above marginal probabilities for domain size greater than 3. This can be shown as follows. Using the intuitive view of Υ as a colored directed graph, the distribution on global examples of domain size e.g. 4 would be a distribution on graphs with 4 vertices. Such graphs would have to contain either two vertices not connected by an edge or two vertices connected by an edge but labeled with the same color or a vertex with no color. However, two such vertices\n2Proposition 4 is one of the results which would not hold for Model B if we did not require injectivity of the randomly sampled substitutions in the definition of Model B.\nwould correspond to a local example which would otherwise have zero probability under the marginals estimated fromΥ, which are shown above. While the above reasoning is for Model A, a similar argument can be used to show the same issues for Model B.\nOne of the consequences of the above example is that the unbiased estimates of relational marginals from Proposition 2 cannot always be used for defining distributions of arbitrary domain sizes (we will show later in the paper conditions under which such unbiased estimates do exist, using the concept of relational marginal polytopes). In order to construct distributions for arbitrary domain sizes, which have relational marginals that are close to the relational marginals given by some global example Υ, we will rely on the following construction which we call expansion of global example.\nDefinition 5 (Expansion of a global example). Let Υ = (A, C) be a global example where C = {c1, c2, . . . , cn}, and let l be a positive integer. Then the l-level expansion of Υ is a global example Υ′ = (A′, C′) given by: A′ = {aθ : a ∈ A, θ ∈ Θ}, C′ = {c1, c2, . . . , cn, cn+1, . . . , cl·n}. Here, constants ci, cj are said to be congruent if i ≡ j mod n. Here, cn+1, . . . , cl·n are some arbitrary new constants and Θ is a set of all substitutions θ which satisfy the requirement that cθ is congruent with c for each c ∈ C. Next we illustrate the notion of expansion of examples. Example 6. Let Υ = (A, C) be given by A = {e(c1, c2), e(c2, c3)}, C = {c1, c2, c3}. Then the 2-level expansion of Υ is Υ′ = (A′, C′) where A = {e(c1, c2), e(c2, c3), e(c4, c5), e(c5, c6), e(c1, c5), e(c2, c6), e(c4, c2), e(c5, c3)}, C = {c1, c2, c3, c4, c5, c6}. The width-2 marginal probabilities on Υ and Υ′ are:\nPΥ,2(ω1) = 1\n3 PΥ′,2(ω1) =\n7\n15 ω1 = ({}, {1, 2})\nPΥ,2(ω2) = 1\n3 PΥ′,2(ω2) =\n4\n15 ω2 = ({e(1, 2)}, {1, 2})\nPΥ,2(ω3) = 1\n3 PΥ′,2(ω3) =\n4\n15 ω3 = ({e(2, 1)}, {1, 2})\nPΥ,2(ω) = 0 PΥ′,2(ω) = 0 ω 6∈ {ω1, ω2, ω3} The differences between the marginal probabilities given by Υ and Υ′ are at most 215 in this case, which is quite high. However, it follows from what we show in turn that this is mostly because of the small size of Υ. For larger global examples, the difference between the marginals obtained from them and from their expansions will tend to be smaller.\nImportantly, it is possible to bound the difference of the parameters obtained on expansions of global examples and the unbiased estimates obtained on the original examples. Proposition 7. Let Υ = (A, C) be a global example andΥ′ its l-level expansion and let n = |A| and k be the width of local examples. Then for any formula α:\n|PΥ,k(α) − PΥ′,k(α)| ≤ 1 − ( n− k + 1\nn\n)k−1\nfor Model A, and\n|QΥ(α)−QΥ′(α)| ≤ 1− ( n− |vars(α)| + 1\nn\n)|vars(α)|−1\nfor Model B.\nNote that the difference between the true and modeled probabilities of a fixed formula α decays as O ( 1 n ) .\nThe techniques described in this section have the following limitation. If we have a set of hard rules Φ0 which are satisfied by a given Υ, these rules may not be satisfied in an expansion of Υ. This is not just a limitation of our method though. There are cases where it is not possible to extend a given Υ while satisfying the constraints (this is because we allow the use of equality in the formulas and because we use the unique names assumption). However, if the example Υ is large enough and it satisfies the hard rules, then the number of violations of these rules will be small, which follows again from Proposition 7.\nIn fact, it seems to be a desirable property that formulas α satisfying PΥ,k(α) = 1 do not have to be treated as completely hard rules but as rules that “mostly” hold if they are learned from Υ, since it may be that they are not really rules that should always hold. Yet, if we actually took them as hard rules we would be forced to assign probability 0 to any example that violates them. It is possible to use the idea of expansions to obtain a distribution in which any formula α has nonzero probability and all properties of expansions are still preserved (such as those from Proposition 7). This can be achieved by randomly sampling additional atoms containing only congruent constants and adding them to the respective expansion. If we use a sufficiently high level of the expansion (at least k for Model A and at least maxα∈Φ |vars(α)| for Model B) then the probability of any formula will be nonzero and not equal to one w.r.t. the distribution induced by the expansions with the sampled atoms."
    }, {
      "heading" : "Relational Marginal Polytopes",
      "text" : "In this section, we define another important concept called relational marginal polytope, which will be used in the next section where we deal with estimation errors.\nDefinition 6 (Relational marginal polytope for Model A). Let k,m ∈ N and Φ = {α1, . . . , αl} be a set of formulas and Φ0 be a set of hard rules. Let C = {1, . . . ,m} and Ck be the set of size-k subsets of C. Then, for Model A, the relational marginal polytope ofΦ of width k and cardinality m w.r.t. the hard rules from Φ0 is the convex hull of the set {(\n#k(α1,Υ)\n|Ck| , . . . ,\n#k(αl,Υ)\n|Ck|\n)∣∣∣∣Υ ∈ Π(C,Φ0) } .\nLet Θαi be the set of all injective substitutions from variables of αi to constants from C. Then, for Model B, the relational marginal polytope of Φ of cardinality m w.r.t. the hard rules from Φ0 is the convex hull of the set\n{( n(α1,Υ)\n|Θα1 | , . . . ,\nn(αl,Υ)\n|Θαl |\n)∣∣∣∣Υ ∈ Π(C,Φ0) } .\nAny realizable set of relational marginals for Model A and Model B naturally corresponds to a point in the respective polytope. In the remainder of this paper, we only consider the cases when the relational marginal polytope is full-dimensional, that is, it does not live in a lower dimensional subspace which could happen if some of the relational marginals that define it were linearly dependent. We will also need the concept of η-interior of a relational marginal. We say that a point y is in the η-interior of a relational marginal polytope P if P contains a ball of radius η with center in y. Using Proposition 4 and Proposition 7, we can show the following both for Model A and Model B.\nProposition 8. Let θ be a vector representing the values of a set of relational marginals given by formulas from a set Φ = {α1, . . . , αl}. Let k be the width of the relational marginals of Model A or k = maxαi |vars(αi)| for Model B. Let the set of hard rules Φ0 be empty. If θ is in\nthe ( η + √ l ( 1− ( m−k+1\nm\n)k−1)) -interior of the relational\nmarginal polytope of Φ of domain-size m then it is also in the η-interior of the relational marginal polytope of Φ for any domain size m′."
    }, {
      "heading" : "Estimation",
      "text" : "In this section, we present error bounds for the estimation of relational marginals. We start by defining the learning setting. Clearly, we need some assumptions on the training and test data and their relationship (otherwise one could always come up with a setting in which the error can be arbitrarily large). In order to stay close to realistic settings we assume that there is some large global example ℵ = (Aℵ, Cℵ) that is not available and that represents the ground truth. That is what we essentially want to estimate for a given formula α is Pℵ,k(α), but we do not have access to whole ℵ. Imagine for instance that ℵ is the human gene regulatory network or Facebook. We assume that there is a process that samples size-m subsets of Cℵ uniformly and that we have access to one such sample CΥ and also to the respective induced Υ = ℵ〈CΥ〉. So, for a given formula α, we need to estimate Pℵ,k(α) using the available example Υ and the estimate needs to be realizable (otherwise the optimization problem would have no solution and the duality would also not hold). This is a reasonably realistic setting3 as in practice we also do not have a distribution over different Facebooks but there is one Facebook and we want to model it based on a sample that is available to us. We now provide theoretical upper bounds for the expected error of the estimates of Pℵ,k(α) assuming the just described learning setting. However, we first need to describe the estimators. Based on the results from the previous sections, the estimator works as follows. Given a global example Υ = (AΥ, CΥ) and an integer n, which is the size of the domain of the modelled distribution (e.g. n can be size of ℵ’s domain if it is known), we construct the l-level expansion Υ(l) of Υ, where l = ⌈n/|CΥ|⌉, and we use it to estimate the parameters as P̂α = PΥ(l),k(α) for Model A and\n3What might differ in realistic settings is the sampling process. We briefly discuss this in section Conclusions.\nas Q̂α = QΥ(l)(α) for Model B. The following proposition introduces an upper bound for the expected error of the estimated parameters.\nProposition 9. Let m and n be positive integers, α a closed formula and let k be the width of local examples. Let ℵ = (Aℵ, Cℵ) be a global example, CΥ be sampled uniformly among all size-m subsets of Cℵ and Υ = ℵ〈CΥ〉. Let Âℵ = Pℵ,k(α). Let B̂Υ be an estimate computed from the l-level expansion of CΥ. Then\nE [∣∣∣Âℵ − B̂Υ ∣∣∣ ] ≤ 1− ( m− k + 1\nm\n)k−1 + √ 1 + 2 log 2\n4⌊m/k⌋ .\nIn the case of model B, the same upper bound holds if we choose k = |vars(α)| and Âℵ = Qℵ(α). The proof of this proposition is based on a deviation bound\nfor a randomized estimator of B̂Υ, which we prove in the appendix located in the supplemantary material. It is possible to improve the estimation in some cases. If the vector corresponding to the marginals Φ estimated from Υ is guaranteed to be in the ( η + √ l ( 1− ( m−k+1\nm\n)k−1)) -\ninterior of the relational marginal polytope of domain of size m = |CΥ| for some η > 0, where k is the width of local examples for Model A (or maxα∈Φ |vars(α)| for Model B), then, by Proposition 8, we can estimate the parameters directly from Υ without constructing its expansion. We then have the following improved bound:\nE [∣∣∣Âℵ − B̂Υ ∣∣∣ ] ≤ √ (1 + 2 log 2)/(4⌊m/k⌋).\nIt is interesting to note that the lower-bound on effective sample size obtained from these bounds is ⌊m/k⌋, which is also the maximum number of non-overlapping size-k subsets of CΥ. A consequence for learning the parameters of models such as MLNs (which corresponds to relational marginal problems inModel B) is that this bound is inversely proportional to the number of variables in the used formulas, which also suggests an explanation for why learning with longer formulas is difficult."
    }, {
      "heading" : "Related Work",
      "text" : "The relational marginals from Model A were recently introduced (Kuželka, Davis, and Schockaert 2017). However, they were only studied in a possibilistic setting, which differs substantially from the probabilistic maximum-entropy setting that we considered. The idea of using random substitutions (Model B) goes back to (Bacchus et al. 1992) who, however, only considered unary predicates. Schulte et al. (2014) used the random substitutions semantics to define a relational Bayesian network model for population statistics. However, their model is, not based on any underlying ground model, and it is unclear whether the distributions are always realizable by a ground model. In the more restricted setting of exponential random graph models (ERGMs, Chatterjee and Diakonis 2013), a formally similar duality, based on densities of graph homomorphisms, has previously been established. To the best of our knowledge, however, such a duality has never been established in\nan SRL setting. In fact, even for ERGMs this duality has not yet been exploited for estimating parameters for models of different domain sizes, which is one of the key contributions of our work. Certain statistical properties of learning have been already studied for SRL models. Xiang and Neville (2011) studied consistency but postulated rather strong assumptions4, as a result of which their results are not comparable with ours. Their approach also differs in that it only considers distributions of labels conditioned on the underlying graph structure. It is interesting that a statistical estimation problem equivalent to the estimation of parameters in Model A has also been studied in the literature. In (Nandi and Sen 1963), the variance of an estimator, equivalent to the unbiased estimator for Model A, was given. However, we are not aware of any work showing a deviation bound in the same setting, which was needed to establish the bound on expected error in Proposition 9. Interestingly, the effective sample size m/k stemming from the work (Nandi and Sen 1963) for variance of the estimator is almost the same as the effective sample size ⌊m/k⌋ stemming from our deviation and expected-error bounds. This actually suggests that our bounds are rather tight. There were many works on Ustatistics (Hoeffding 1948) which are related as well but they rely on assumptions that are generally not realistic for SRL and, in particular, are not applicable to our setting; the work (Nandi and Sen 1963) is an exception."
    }, {
      "heading" : "Conclusions",
      "text" : "In this paper, we have introduced and studied relational marginal problems. Interestingly, this perspective enables learning a model that is applicable to data sets whose domain sizes differ from that of the training data. We established a relational counterpart of the classical duality between maximum-likelihood and max-entropy marginal problems. Then, we showed how to estimate and adjust parameters of the marginals in order to guarantee their realizability. We also complemented these results by providing bounds on the expected errors of the estimates in a reasonable learning setting. We believe that due to the simplicity and transparency of the learning setting that we introduced, this setting could play a similar role for SRL as the standard i.i.d. statistical learning setting plays for learning in propositional domains (Vapnik 1995). That is, as an idealized setting that is suitable for theoretical study, but that is not too far from settings that one encounters in reality. Still, it would be possible to extend the learning setting to make it more realistic. In particular, the sampling process that creates the training examples could be replaced by another sampling process that would take into account the structure of the relational data. That would probably make estimation of parameters and derivation of error bounds significantly more complex, and hence arguably less illuminating, which is why we leave it for future work.\n4The assumptions used in their work were weak dependency and bounded degree of graph nodes."
    }, {
      "heading" : "Appendix: Duality",
      "text" : "In this section we prove the duality results from the main text. First we recall the setting and notations. Let C be a set of constants, A the set of all atoms over C based on some given first-order language and let Ck denote the set of all k-element subsets of C. Next, let Φ be a set of relational marginals and Φ0 be a set of hard constraints, i.e. formulas α such that if Υ 6|= α then P (Υ) = 0. Let us also assume that there exists at least one distribution P ′ which is a model of Φ and which satisfies the positivity condition: P ′(Υ) > 0 for all Υ satisfying the hard constraints. The optimization problem is then given by:\nsup {PΥ:Υ∈Π(C,Φ0)}\n∑\nΥ∈Πn(C,Φ0)\nPΥ log 1\nPΥ s.t. (12)\n∀(αi, θi) ∈ Φ : 1\n|Ck| ∑\nS∈Ck\n∑\nΥ∈Π(C,Φ0)\n1(Υ〈S〉 |= αi) · PΥ = θi (13)\n∀Υ ∈ Πn(C,Φ0) : PΥ ≥ 0, ∑\nΥ∈Π(C,Φ0)\nPΥ = 1 (14)\nChanging maximization of entropy to minimization of negative entropy, we obtain a convex optimization problem. Next, we construct the Lagrangian L(P, λ, z) (here, P denotes the vector of all PΥ ∈ Π(C,Φ0), λ the vector of all λi’s, and we will ignore the non-negativity constraints for the moment as it will turn out that they are enforced implicitly):\nL(P, λ, z) = ∑\nΥ∈Π(C,Φ0)\nPΥ · logPΥ−\n− 1|Ck| ∑\nαi∈Φ\n∑\nS∈Ck\n∑\nΥ∈Π(C,Φ0)\nλi · 1(Υ〈S〉 |= αi) · PΥ+\n+ ∑\nαi∈Φ\nλi · θi + z − z ∑\nΥ∈Π(C,Φ0)\nPΥ (15)\nTo find the stationary points of L(P, λ, z) w.r.t. P , we take the partial derivatives of the Lagrangian w.r.t. PΥj and set them equal to zero:\nlogPΥj − 1\n|Ck|\n∑\nαi∈Φ\n∑\nS∈Ck\nλi ·1(Υj〈S〉 |= αi)− z− 1 = 0\n(16)\nNext we define #k(α,Υ) = |{S ∈ Ck : Υ〈S〉 |= α}|. That is #k(α,Υ) is a function counting the number of sets S ∈ Ck such that the formula α is true in the restriction of Υ to constants in S (i.e. those for which Υ〈S〉 |= α). This allows us to rewrite (16) as follows:\nlogPΥj − ∑\nαi∈Φ\nλi #k(αi,Υj)\n|Ck| − z − 1 = 0 (17)\nFrom this, we get:\nPΥj = exp\n( z + 1 + ∑\nαi∈Φ\nλi #k(αi,Υj)\n|Ck|\n) (18)\nSince we have ∑\nΥ∈Π(C,Φ0) PΥ = 1, exp (z + 1) is a nor-\nmalization constant. By further algebraic manipulations, combining (15) and (18) we obtain the Lagrangian dual problem (where λi ∈ R) is to maximize:\n∑\nαi∈Φ\nλiθi − log ∑\nΥ∈Π(C,Φ0)\ne ∑ αi∈Φ λi #k(αi,Υj) |Ck| (19)\nDue to the positivity assumption, Slater’s condition (Boyd and Vandenberghe 2004) is satisfied and strong duality holds.\nRemark 10. The same reasoning can be also applied when we have multiple sets of relational marginals Φ1, Φ2, . . . , Φn (Φ = Φ1 ∪ · · · ∪Φn) containing relational marginals of widths 1, 2, . . . , n, respectively. The only difference is that the denominators |Ck| will vary according to widths (ki) of the respective marginals (αi) and we will denote the respective sets of subsets of C by Cki . Let us perform a change of variables wi := λi/|Cki | and denote the normalization constant of the distribution as Z . This gives us\nP (Υ) = 1\nZ exp\n( ∑\nαi∈Φ\nwi ·#k(αi,Υ) )\n(20)\nfor the distribution and ∑\nαi∈Φ\nwiθi|Cki |−log ∑\nΥ∈Π(C,Φ0)\ne ∑ αi∈Φ wi·#k(αi,Υ) (21)\nfor the optimization criterion of the dual problem. Assuming that the marginals were estimated from a global example Υ̂ ∈ Π(C,Φ0) (note that here the domain C is the same as the domain of the global examples over which the distribution is defined) and that they still satisfy the positivity assumption, we can also rewrite (21) as\n∑\nαi∈Φ\nwi · #k(αi, Υ̂) − log ∑\nΥ∈Π(C,Φ0)\ne ∑ αi∈Φ wi·#k(αi,Υ)\n(22)\nThe reasoning is completely analogical for Model B. The only difference is that #k(α,Υ) is replaced by n(α,Υ) which counts the number of true groundings of α in Υ."
    }, {
      "heading" : "Appendix: Proofs",
      "text" : "In this section we give proofs of the propositions from the main text. Proposition 2. Let P (Υ) be a distribution on domain size n and k ≤ n be an integer. Let Ωα = {Υ〈S〉 : S ⊆ C, |S| = k,Υ〈S〉 |= α} where Υ = (A, C) is sampled according to the distribution P (Υ). Then, for a closed formula α,\np̂α = |Ωα| · (\nn k\n)−1\nis an unbiased estimate of Pk(α).\nProof. We have:\nE[p̂α] = ∑\nΥ=(A,C)∈Ω\nP (Υ) · (\nn k\n)−1 ·\n· |{S : S ⊆ C, |S| = k,Υ〈S〉}| = ∑\nΥ∈Ω\nP (Υ) · PΥ,k(α) =\n= ∑\nω:ω|=α\n∑\nΥ∈Ω\nP (Υ) · PΥ,k(ω) = P (α).\nProposition 4. For Model A, if there is a width-k model ofΦ on domain of size n then there is also a width-k model of Φ on domain of size m for any m satisfying n ≥ m ≥ k. For Model B, if there is a model of Φ on domain of size n then there is also a model of Φ on domain of size m for any m satisfying n ≥ m ≥ maxα∈Φ |vars(α)|.\nProof. Let P (.) be a model of Φ on domain of size n. Let P ′(.) be a distribution given by the following process: (i) sample Υ = (A, C) according to P (.), (ii) sample a subset C′ from C uniformly among all subsets of C of cardinality m, and (iii) set Γ := Υ〈C′〉. We claim that P ′(.) is also a width-k model of M. To show that this is true it is enough to demonstrate that Pk(ω) = P ′ k(ω) for all width-k local examples ω, where Pk(.) and P ′ k(.) are relational marginal distributions induced by P (.) and P ′(.), respectively. This is straightforward to show. It suffices to notice that Pk(ω) does not change when we modify the relational marginal sampling process so that it first samples Υ = (A, C), then it samples uniformly a subset C′ of C of cardinalitym, then it samples uniformly a subset S of C′ and then a local example fromΥ[S]. But this process is equivalent to sampling local examples from the global examples sampled according to the distribution P ′(.). Hence, Pk(ω) = P ′ k(ω).\nThe argument is completely analogical for Model B5.\nIt is worth noting here why the Proposition 4 would not hold for Model B if we did not require the sampled substitutions of Model B to be injective. In such a case, the distribution of local examples ω would not be the same if we first sampled a subset C′ of size m from C. For instance, for a formula α with two variables A and B, the proportion of samples mapping A and B to the same constant would be 1 n for sampling directly from C but 1m when first sampling C′ from C and then sampling the substitutions for C′. So the two distributions would be no longer equivalent.\nLemma 1. Let Υ = (A, C) be a global example and Υ′ = (A′, C′) its l-level expansion. Let n = |C|. Then there exists a distribution λ(.) such that PΥ′,k(ω) = (1− γ)PΥ,k(ω) + γλ(ω). Here γ is the probability that a randomly sampled subset of k constants contains at least two congruent con-\n5Here, the condition n ≥ m ≥ maxα∈Φ |vars(α)| is here so that we could actually sample injective substitutions.\nstants, i.e.\nγ = 1−\n( n k ) · lk\n( n · l k )\nProof. We prove this proposition for Model A; the arguments for Model B are essentially the same and therefore we omit them. For notational convenience, let us define an indicator function (S) which equals 1 when S contains two congruent constants and is 0 otherwise. We decompose the probability PΥ′,k(ω) as:\nPΥ′,k(ω) = PS [S = ω| (S) = 1] · PS [ (S) = 1]+ + PS [S = ω| (S) = 0] · PS [ (S) = 0]\nwherePS [.] denotes probability under sampling of S according to Model A. Using elementary combinatorial reasoning, we have PS [ (S) = 1] = γ, PS [ (S) = 0] = 1− γ and we can set λ(ω) = PS [S = ω| (S) = 1]. It remains to analyze PS [S = ω| (S) = 0]. First, we observe that this is a distribution on local examples “induced” by a uniform distribution over size-k subsets of C′ which do not contain congruent constants. The same distribution can also be obtained as follows. First, pick uniformly at random a size-k subset S ′ of C and then assign, again uniformly and independently at random, to each of the constants in S ′ a constant sampled from the constants congruent to it in C′. Thenwemay notice that it follows from the construction of expansions that the isomorphism class of the induced local example ω to be sampled is already determined when we sample the set S ′ and it is equal to Υ[S ′]. It follows that PS [S = ω| (S) = 0] = PΥ,k(ω) which finishes the proof.\nLemma 2. Let P and λ be distributions over Ω and α be a closed and constant-free formula. Then\n∣∣∣∣∣∣ ∑\nω∈Ω:ω|=α\nP (ω)− λ(ω) ∣∣∣∣∣∣ ≤ 1\nProof. If ∑\nω∈Ω:ω|=α P (ω)− λ(ω) ≥ 0 then we have ∑\nω∈Ω:ω|=α\nP (ω)− λ(ω) ≤ 1− ∑\nω∈Ω:ω|=α\nλ(ω) ≤ 1\nwhere the first and second inequality follow from the fact that P (ω) and λ(ω) most sum up to 1 over Ω and that they are positive. We can reason completely analogically for∑\nω∈Ω:ω|=α P (ω)−λ(ω) ≤ 0. The correctness of the lemma follows easily.\nProposition 7. Let Υ = (A, C) be a global example and Υ′ its l-level expansion and let n = |A| and k be the width of local examples. Then for any formula α:\n|PΥ,k(α) − PΥ′,k(α)| ≤ 1 − ( n− k + 1\nn\n)k−1\nfor Model A, and\n|QΥ(α)−QΥ′(α)| ≤ 1− ( n− |vars(α)| + 1\nn\n)|vars(α)|−1\nfor Model B.\nProof. We first prove the proposition for Model A. Let γ be as in Lemma 1. We have |PΥ,k(α)− PΥ′,k(α)| =\n∣∣∣∣∣∣ ∑\nω:ω|=α\n(PΥ,k(ω)− (1− γ) · PΥ,k(ω)− γ · λ(ω)) ∣∣∣∣∣∣\nwhere λ(.) is some suitable probability distribution. This can be rewritten as\n|PΥ,k(α)−PΥ′,k(α)| = γ · ∣∣∣∣∣∣ ∑\nω:ω|=α\n(PΥ,k(ω)− λ(ω)) ∣∣∣∣∣∣ ≤\n≤ γ = 1− l k · n · (n− 1) · · · · · (n− k + 1)\nn · l · (n · l − 1) · · · · · (n · l− k + 1)\nwhere the first inequality follows from Lemma 2. Next, we can verify that\n1− l k · n · (n− 1) · · · · · (n− k + 1)\nn · l · (n · l − 1) · · · · · (n · l − k + 1) =\n= 1− n · (n− 1) · · · · · (n− k + 1) n · (n− 1l ) · · · · · (n− k+1l )\nis non-decreasing with l. Taking the limit l → ∞ therefore preserves the inequality and we then get\n|PΥ,k(α)−PΥ′,k(α)| ≤ 1− n− 1 n · n− 2 n · · · · · n− k + 1 n .\nNow, to prove the proposition also for Model B, we proceed as follows. It is easy to verify that QΥ(α) is equivalently given by the following process. First, we uniformly sample a local example ω of width k = |vars(α)| and then we uniformly sample a substitution ϑ from the set Θk of injective substitutions from vars(α) to the set {1, 2, . . . , k}. QΥ(α) is then also equal to the probability that ω |= αϑ. We can then exploit the arguments used to prove the result for Model A. We have |QΥ(α) −QΥ′(α)| =\n= ∣∣∣∣∣∣ 1 |Θk| ∑\nϑ∈Θk\n∑\nω:ω|=αϑ\nPΥ,k(ω)− PΥ′,k(ω) ∣∣∣∣∣∣\n≤ 1− ( n− k + 1\nn\n)k−1\n= 1− ( n− |vars(α)|+ 1\nn\n)|vars(α)|−1\nwhere the inequality follows from the same algebraic manipulations that we performed above for Model A.\nProposition 8. Let θ be a vector representing the values of a set of relational marginals given by formulas from a set Φ = {α1, . . . , αl}. Let k be the width of the relational marginals of Model A or k = maxαi |vars(αi)| for Model B. Let the set of hard rules Φ0 be empty. If θ is in\nthe ( η + √ l ( 1− ( m−k+1\nm\n)k−1)) -interior of the relational\nmarginal polytope of Φ of domain-size m then it is also in the η-interior of the relational marginal polytope of Φ for any domain size m′.\nProof. Let us denote by S(Υi) the vector of values of the relational marginals corresponding to the formulas α1, . . . , αl computed from Υi. Let B be a point on the boundary of the relational marginal polytope. It follows from the definitions that we can write B as a linear combination6 B = a1 · S(Υ1) + · · · + al · S(Υl) where all ai’s are positive and a1 + · · · + al = 1 for some suitable Υ1, . . . ,Υl on domain of size m. Let Υ′1, . . . ,Υ ′ l be the expansions of the respective Υ1, . . . ,Υl (we pick the level of these expansions sufficient to make the cardinality of their domains greater than the desired cardinality m′) and let B′ = a1 · S(Υ′1) + · · · + al · S(Υ′l). For notational convenience, let us denote\nξ = 1− ( n− k + 1\nn\n)k−1 .\nWith this notation we have that θ is in the (η + ξ)-interior of the marginal polytope of domain of size cardinality n. Then, using Proposition 7, we have (for both Model A and Model B):\n‖S(Υi)− S(Υ′i)‖2 ≤ ξ √ l\nwhere we note that l is the number of formulas in Φ. Using triangle inequality, it follows that\n‖B −B′‖2 = = ‖a1(S(Υ1)− S(Υ′1)) + · · ·+ al(S(Υl)− S(Υ′l))‖2 ≤ a1‖S(Υ1)− S(Υ′1)‖2 + · · ·+ al‖S(Υl)− S(Υ′l)‖2\n≤ (a1 + · · ·+ al) · ξ · √ l = ξ √ l\nIntuitively this means that any point on the boundary of the polytope for domain size m will have a point on the boundary or inside the polytope for domain sizem′ in distance not greater than ξ. Since the point θ is in the (η + ξ)-interior of the polytope for domain size m, it follows that it must also be in the η-interior of the polytope for domain size m′ (for anym′), which is what we needed to show.\nNext we prove Proposition 9 using a series of lemmas.We first state and prove the lemmas for Model A and after that, separately, also their analogoues for Model B.\nLemma 3 (Model A Case). Let ℵ = (Aℵ, Cℵ), where Cℵ is a set of constants and Aℵ is a set of ground atoms involving only constants from Cℵ and let 0 ≤ n ≤ |Cℵ|\n6In particular, this follows from the fact that relational marginal polytope is a convex hull of a finite set of points in l-dimensional space where the points are given by S(Υ) for Υ ∈ Π(C, ∅).\nand 0 ≤ k ≤ n be integers. Let CΥ be sampled uniformly from all size-n subsets of Cℵ and let Υ = ℵ〈CΥ〉. Let X = (S1,S2, . . . ,S⌊n\nk ⌋) be a vector of subsets of Cℵ, each\nsampled uniformly and independently of the others from all size-k subsets of Cℵ. Next let Y = (S ′1,S ′2, . . . ,S ′⌊n\nk ⌋) be a\nvector sampled by the following process:\n1. Let I ′ = {1, 2, . . . , |Cℵ|}. 2. Sample subsets I ′1, . . . , I ′⌊n k ⌋ of size k uniformly from I ′.\n3. Sample an injective function g : ⋃\nI′ i ⊆I′ I ′i → CΥ uni-\nformly from all such functions.\n4. Let S ′i = g(I ′i) for all 0 ≤ i ≤ ⌊nk ⌋. ThenX andY have the same distribution.\nProof. (Sketch) Let h be a bijection from I to Cℵ sampled uniformly from the set of all such bijections. If we sample I1, . . . , I⌊n\nk ⌋ of size k uniformly from I then the dis-\ntribution of (h(I1), . . . , h(I⌊n k ⌋)) will be the same as that of X. In fact, most of h is actually irrelevant and we can replace sampling h by uniformly sampling an injective function g : ⋃ Ii → Cℵ after we sample the sets Ii. Finally, it is easy to realize that the function g can also be sampled as follows. First, sample a subset C of Cℵ of size n uniformly and then sample uniformly an injective function g such that Range(g) ⊆ C. The reason why we can do this is because |⋃ Ii| ≤ n. This finishes the proof of the lemma as this is actually the same as the process of samplingY.\nLemma 4 (Model A Case). Let ℵ = (Aℵ, Cℵ), where Cℵ is a set of constants andAℵ is a set of ground atoms involving only constants from Cℵ and let 0 ≤ n ≤ |Cℵ| and 0 ≤ k ≤ n be integers. Let CΥ be sampled uniformly from all size-n subsets of Cℵ and let Υ = ℵ〈CΥ〉. Let Y be sampled as in Lemma 3 (i.e. Y is sampled only using Υ and not directly ℵ). Let α be a closed and constant-free formula. Let\nÂΥ = 1 ⌊nk ⌋ ∑\nS′ i ∈Y\n1(Υ〈S ′i〉 |= α)\nand let Aℵ = Pℵ,k(α). Then we have\nP [∣∣∣ÂΥ −Aℵ ∣∣∣ ≥ ǫ ] ≤ 2 exp ( −2 ⌊n k ⌋ ǫ2 ) .\nProof. We define an estimator on the set X which is sampled as in Lemma 3\nÂ = 1⌊ n k\n⌋ ∑\nSi∈X\n1(ℵ〈Si〉 |= α).\nBy Lemma 3, we know that\nP [∣∣∣Â−Aℵ ∣∣∣ ≥ ǫ ] = P [∣∣∣ÂΥ −Aℵ ∣∣∣ ≥ ǫ ] ,\nso we only need to show\nP [∣∣∣Â−Aℵ ∣∣∣ ≥ ǫ ] ≤ 2 exp ( −2 ⌊n k ⌋ ǫ2 ) .\nSince Si inX are independent, the inequality above follows immediately from the fact that E [ Â ] = Aℵ (which follows E [1(ℵ〈Si〉 |= α)] = Aℵ and the linearity property of expectation) and the Chernoff-Hoeffding theorem.\nLemma 5 (Model A Case). Let ℵ = (Aℵ, Cℵ), where Cℵ is a set of constants andAℵ is a set of ground atoms involving only constants from Cℵ and let 0 ≤ n ≤ |Cℵ| and 0 ≤ k ≤ n be integers. Let CΥ be sampled uniformly from all size-n subsets of Cℵ and let Υ = ℵ〈CΥ〉. Let Y be sampled as in Lemma 3 (i.e. Y is sampled only using Υ and not directly ℵ). Let α be a closed and constant-free formula. Let\nÂΥ = 1 ⌊nk ⌋ ∑\nS′ i ∈Y\n1(Υ〈S ′i〉 |= α)\nand let Aℵ = Pℵ,k(α). Then we have\nE [∣∣∣ÂΥ −Aℵ ∣∣∣ ] ≤ √ 1 + 2 log 2\n4⌊n/k⌋ .\nProof. Let Z := ∣∣∣ÂΥ −Aℵ ∣∣∣ . First, notice that\nE [Z] ≤ √ E [Z2],\nso we now try to bound E [ Z2 ] . To bound E [ Z2 ] , note that for any u > 0,\nE [ Z2 ] =\n∫ ∞\n0\nP [ Z2 ≥ t ] dt\n=\n∫ ∞\n0\nP [ Z ≥ √ t ] dt\n≤ u+ ∫ ∞\nu\nP [ Z ≥ √ t ] dt\n≤ u+ 2 ∫ ∞\nu\nexp ( −2 ⌊n k ⌋ t ) dt\n= u+ exp ( −2 ⌊n k ⌋ u ) / ⌊n k ⌋ .\nSince u was arbitrary, we may choose it to minimize the obtained bound. The optimal choice is u = log 22⌊n/k⌋ , which yields E [ Z2 ] ≤ 1+2 log 24⌊n/k⌋ .\nLemma 6 (Model A Case). Let ℵ = (Aℵ, Cℵ), where Cℵ is a set of constants andAℵ is a set of ground atoms involving only constants from Cℵ and let 0 ≤ n ≤ |Cℵ| and 0 ≤ k ≤ n be integers. Let CΥ be sampled uniformly from all size-n subsets of Cℵ and let Υ = ℵ〈CΥ〉. Let α be a closed and constant-free formula and let Ck denote all size-k subsets of CΥ. Let\nÃΥ = ( n k )−1 ∑\nS∈Ck\n1(Υ〈S〉 |= α)\nand let Aℵ = Pℵ,k(α). Then we have\nE [∣∣∣ÃΥ −Aℵ ∣∣∣ ] ≤ √ 1 + 2 log 2\n4⌊n/k⌋ .\nProof. First we define an auxiliary estimator Ã (q) Υ . Let Yq be a vector of n · q size-k subsets of CΥ where the subsets\nof CΥ are sampled in the same way as the elements of the vectorY in Lemma 3. Let us define\nÃ (q) Υ =\n1 q · n ∑\nS∈Yq\n1(Υ〈S〉 |= α).\nUsing the triangle inequality and Lemma 5, we can show that\nE [∣∣∣Ã(q)Υ −Aℵ ∣∣∣ ] ≤ √ 1 + 2 log 2\n4⌊n/k⌋ .\nWe have (again using triangle inequality)\nE [∣∣∣ÃΥ −Aℵ ∣∣∣ ] = E [∣∣∣ÃΥ − Ã(q)Υ + Ã (q) Υ −Aℵ ∣∣∣ ]\n≤ E [∣∣∣ÃΥ − Ã(q)Υ ∣∣∣ ] + E [∣∣∣Ã(q)Υ −Aℵ ∣∣∣ ]\n≤ E [∣∣∣ÃΥ − Ã(q)Υ ∣∣∣ ] +\n√ 1 + 2 log 2\n4⌊n/k⌋ .\nIt follows from the strong law of large numbers (which holds for anyΥ) and Proposition 2 that P [limq→∞ Ã (q) Υ = ÃΥ] = 1. Since q was arbitrary, we can use this to conclude\nE [∣∣∣ÃΥ −Aℵ ∣∣∣ ] ≤ √ 1 + 2 log 2\n4⌊n/k⌋ .\nNext we state the analogues of the above lemmas also for Model B (for the first reading, we recommend skipping these lemmas and proceeding directly to the proof of Proposition 9). We omit the proofs as they are completely analogical to their respective counterparts for Model A (we just replace sampling of subsets by sampling of injective substitutions).\nLemma 7 (Model B Case). Let ℵ = (Aℵ, Cℵ), where Cℵ is a set of constants and Aℵ is a set of ground atoms involving only constants from Cℵ and let 0 ≤ n ≤ |Cℵ| and 0 ≤ k ≤ n be integers. Let CΥ be sampled uniformly from all size-n subsets of Cℵ and let Υ = ℵ〈CΥ〉. Let X = (ϑ1, ϑ2, . . . , ϑ⌊n\nk ⌋) be a vector of injective substi-\ntutions from a given size-k set of variables V to Cℵ, each sampled uniformly and independently of the others. Next let Y = (ϑ′1, ϑ ′ 2, . . . , ϑ ′ ⌊n\nk ⌋) be a vector sampled by the follow-\ning process:\n1. Let I ′ = {1, 2, . . . , |Cℵ|}. 2. Sample ordered subsets I ′1, . . . , I ′⌊n\nk ⌋ of size k uniformly\nfrom I ′. 3. Sample an injective function g :\n⋃ I′ i ⊆I′ I ′i → CΥ uni-\nformly from all such functions.\n4. Let ϑ′i = {Vj 7→ cj|Vj ∈ V and (c1, . . . , c|V|) = g(I ′i)} for all 0 ≤ i ≤ ⌊nk ⌋, i.e. ϑ′i is an injective substitution from V to CΥ.\nThenX andY have the same distribution.\nLemma 8 (Model B Case). Let ℵ = (Aℵ, Cℵ), where Cℵ is a set of constants andAℵ is a set of ground atoms involving only constants from Cℵ and let 0 ≤ n ≤ |Cℵ| and 0 ≤ k ≤ n be integers. Let CΥ be sampled uniformly from all sizen subsets of Cℵ and let Υ = ℵ〈CΥ〉. Let α be a closed and constant-free formula. LetY be sampled as in Lemma 7 (i.e. Y is sampled only usingΥ and not directly ℵ), where we set V = vars(α). Let\nÂΥ = 1 ⌊nk ⌋ ∑\nϑ′ i ∈Y\n1(Υ |= αϑ′i)\nand let Aℵ = Qℵ(α). Then we have\nP [∣∣∣ÂΥ −Aℵ ∣∣∣ ≥ ǫ ] ≤ 2 exp ( −2 ⌊n k ⌋ ǫ2 ) .\nLemma 9 (Model B Case). Let ℵ = (Aℵ, Cℵ), where Cℵ is a set of constants andAℵ is a set of ground atoms involving only constants from Cℵ and let 0 ≤ n ≤ |Cℵ| and 0 ≤ k ≤ n be integers. Let CΥ be sampled uniformly from all size-n subsets of Cℵ and let Υ = ℵ〈CΥ〉. Let Y be sampled as in Lemma 7 (i.e.Y is sampled only usingΥ and not directly ℵ), where we set V = vars(α). Let α be a closed and constantfree formula. Let\nÂΥ = 1 ⌊nk ⌋ ∑\nϑ′ i ∈Y\n1(Υ |= αϑ′i)\nand let Aℵ = Qℵ(α). Then we have\nE [∣∣∣ÂΥ −Aℵ ∣∣∣ ] ≤ √ 1 + 2 log 2\n4⌊n/k⌋ .\nLemma 10 (Model B Case). Let ℵ = (Aℵ, Cℵ), where Cℵ is a set of constants and Aℵ is a set of ground atoms involving only constants from Cℵ and let 0 ≤ n ≤ |Cℵ| and 0 ≤ k ≤ n be integers. Let CΥ be sampled uniformly from all size-n subsets of Cℵ and letΥ = ℵ〈CΥ〉. Let α be a closed and constant-free formula and Θα denote all injective substitutions from vars(α) to CΥ. Let\nÃΥ = 1 |Θα| ∑\nϑ∈Θα\n1(Υ |= αϑ)\nand let Aℵ = Qℵ(α). Then we have\nE [∣∣∣ÃΥ −Aℵ ∣∣∣ ] ≤ √ 1 + 2 log 2\n4⌊n/|vars(α)|⌋ .\nNow we have all the ingredients needed to prove Proposition 9. Proposition 9 Let m and n be positive integers, α a closed formula and let k be the width of local examples. Let ℵ = (Aℵ, Cℵ) be a global example, CΥ be sampled uniformly among all size-m subsets of Cℵ and Υ = ℵ〈CΥ〉. Let Âℵ = Pℵ,k(α). Let B̂Υ be an estimate computed from the l-level expansion of CΥ. Then\nE [∣∣∣Âℵ − B̂Υ ∣∣∣ ] ≤ 1− ( m− k + 1\nm\n)k−1 + √ 1 + 2 log 2\n4⌊m/k⌋ .\nIn the case of model B, the same upper bound holds if we choose k = |vars(α)| and Âℵ = Qℵ(α).\nProof. Let ÂΥ = PΥ,k(α) (ÂΥ = QΥ(α), respectively). Then we have\nE [∣∣∣Âℵ − B̂Υ ∣∣∣ ] = E [∣∣∣Âℵ − ÂΥ + ÂΥ − B̂Υ ∣∣∣ ]\n≤ E [∣∣∣ÂΥ − B̂Υ ∣∣∣ ] + E [∣∣∣Âℵ − ÂΥ ∣∣∣ ]\n≤ 1− ( m− k + 1\nm\n)k−1 + √ 1 + 2 log 2\n4⌊m/k⌋\nwhere the last inequality follows from Proposition 7 and Lemma 6 for Model A, and Lemma 10 for Model B, respectively."
    } ],
    "references" : [ {
      "title" : "1992",
      "author" : [ "F. Bacchus", "A.J. Grove", "D. Koller", "J. Y Halpern" ],
      "venue" : "From statistics to beliefs. In Proceedings of the 10th National Conference on Artificial Intelligence. San Jose, CA, July 12-16,",
      "citeRegEx" : "Bacchus et al. 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "and Vandenberghe",
      "author" : [ "S. Boyd" ],
      "venue" : "L.",
      "citeRegEx" : "Boyd and Vandenberghe 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "and Diaconis",
      "author" : [ "S. Chatterjee" ],
      "venue" : "P.",
      "citeRegEx" : "Chatterjee and Diaconis 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Taskar",
      "author" : [ "L. Getoor" ],
      "venue" : "B.",
      "citeRegEx" : "Getoor and Taskar 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Induction of interpretable possibilistic logic theories from relational data",
      "author" : [ "Davis Kuželka", "O. Schockaert 2017] Kuželka", "J. Davis", "S. Schockaert" ],
      "venue" : "In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Kuželka et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kuželka et al\\.",
      "year" : 2017
    }, {
      "title" : "and Sen",
      "author" : [ "H. Nandi" ],
      "venue" : "P.",
      "citeRegEx" : "Nandi and Sen 1963",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Aggregation and population growth: The relational logistic regression and markov logic cases",
      "author" : [ "Poole" ],
      "venue" : "In 2nd International Workshop on Statistical Relational AI (StarAI",
      "citeRegEx" : "Poole,? \\Q2012\\E",
      "shortCiteRegEx" : "Poole",
      "year" : 2012
    }, {
      "title" : "and Domingos",
      "author" : [ "M. Richardson" ],
      "venue" : "P.",
      "citeRegEx" : "Richardson and Domingos 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A",
      "author" : [ "O. Schulte", "H. Khosravi", "Kirkpatrick" ],
      "venue" : "E.; Gao, T.; and Zhu, Y.",
      "citeRegEx" : "Schulte et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Rinaldo",
      "author" : [ "C.R. Shalizi" ],
      "venue" : "A.",
      "citeRegEx" : "Shalizi and Rinaldo 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "N",
      "author" : [ "M. Singh", "Vishnoi" ],
      "venue" : "K.",
      "citeRegEx" : "Singh and Vishnoi 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Lifted generative learning of markov logic networks. Machine Learning 103(1):27–55",
      "author" : [ "Van Haaren" ],
      "venue" : null,
      "citeRegEx" : "Haaren,? \\Q2016\\E",
      "shortCiteRegEx" : "Haaren",
      "year" : 2016
    }, {
      "title" : "V",
      "author" : [ "Vapnik" ],
      "venue" : "N.",
      "citeRegEx" : "Vapnik 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "M",
      "author" : [ "M.J. Wainwright", "Jordan" ],
      "venue" : "I.",
      "citeRegEx" : "Wainwright and Jordan 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "and Neville",
      "author" : [ "R. Xiang" ],
      "venue" : "J.",
      "citeRegEx" : "Xiang and Neville 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In the propositional setting, the marginal problem is to find a (maximum-entropy) distribution that has some given marginals. We study this problem in a relational setting and make the following contributions. First, we compare two different notions of relational marginals. Second, we show a duality between the resulting relational marginal problems and the maximum likelihood estimation of the parameters of relational models, which generalizes a well-known duality from the propositional setting. Third, by exploiting the relational marginal formulation, we present a statistically sound method to learn the parameters of relational models that will be applied in settings where the number of constants differs between the training and test data. Furthermore, based on a relational generalization ofmarginal polytopes, we characterize cases where the standard estimators based on feature’s number of true groundings needs to be adjusted and we quantitatively characterize the consequences of these adjustments. Fourth, we prove bounds on expected errors of the estimated parameters, which allows us to lower-bound, among other things, the effective sample size of relational training data.",
    "creator" : "LaTeX with hyperref package"
  }
}