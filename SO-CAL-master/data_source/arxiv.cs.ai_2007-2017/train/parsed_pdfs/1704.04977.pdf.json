{
  "name" : "1704.04977.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilistic programs for inferring the goals of autonomous agents",
    "authors" : [ "Marco F. Cusumano-Towner", "Alexey Radul", "David Wingate", "Vikash K. Mansinghka" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Intelligent systems sometimes need to infer the probable goals of people, cars, and robots, based on partial observations of their motion. This paper introduces a class of probabilistic programs for formulating and solving these problems. The formulation uses randomized path planning algorithms as the basis for probabilistic models of the process by which autonomous agents plan to achieve their goals. Because these path planning algorithms do not have tractable likelihood functions, new inference algorithms are needed. This paper proposes two Monte Carlo techniques for these “likelihood-free” models, one of which can use likelihood estimates from neural networks to accelerate inference. The paper demonstrates efficacy on three simple examples, each using under 50 lines of probabilistic code."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Intelligent systems sometimes need to infer the probable goals of people, cars, and robots, based on partial observations of their motion. These problems are central to autonomous driving and driver assistance [Franke et al., 1998; Urmson et al., 2008; Aufrère et al., 2003], but also arise in aerial robotics, reconnaissance, and security applications [Kumar and Michael, 2012; Liao et al., 2006; Tran and Davis, 2008]. In these settings, knowledge of the beliefs and goals of an agent makes it possible to infer their probable future actions.\nBecause the mental state of another agent is inherently unobservable and uncertain, it is natural to take a Bayesian approach to inferring it. Probabilistic models can be used to describe how an agent’s latent high-level goals and beliefs about the environment interact to yield its probable actions. Most existing work along these\nlines has focused on modeling goal-directed behavior using Markov decision processes and related approaches from stochastic control [Baker et al., 2007; Ziebart et al., 2009]. While promising, these approaches involve significant task-specific engineering. They also calculate policies that prescribe actions for every possible state of the world, sometimes in the inner loop of an inference algorithm. This leads to fundamental scaling challenges, even for simple environments and goal priors.\nThis paper introduces a class of probabilistic programs that formulate goal inference problems as approximate inference in generative models of goal-directed behavior. The proposed approach reflects three contributions: First, agents are assumed to follow paths generated by fast randomized path planning code that can incorporate heuristics drawn from video game engines and robotics. This can scale to larger environments than approaches based on optimal control. Second, hierarchical models for goals and paths are represented as probabilistic programs. This allows one to formulate a broad class of single- and multi-agent problems with common modeling and inference machinery. Ordinary probabilistic programming constructs can handle complex maps, hierarchical goal priors, and partially observed environments. Third, this paper proposes an approach to real-time approximate inference, using neural networks to learn proposals for the internal choices made by any path planners. Together, these contributions lead to a practical proposal for goal inference that has the potential to scale to a broad class of real-world problems and real-time applications. We demonstrate the efficacy of prototype implementations of these algorithms on three simple examples, each written in under 50 lines of probabilistic code.\nNote that this proposal does not require planning algorithms to be rewritten as probabilistic programs, but instead allows optimized, low-level, or legacy planning codes to be treated as black boxes. This avoids the implementation and performance cost of rewriting an existing path planner in a high-level probabilistic programming\nar X\niv :1\n70 4.\n04 97\n7v 1\n[ cs\n.A I]\n1 7\nA pr\n2 01\nlanguage, and exposing the thousands of random choices it might make to generic inference algorithms. One difficulty is that such optimized black-box planners may well make too many internal random choices to have tractable input-output likelihoods. This paper proposes two novel Monte Carlo techniques for these “likelihood-free” models, each extending Metropolis-Hastings: (i) a cascading resimulation algorithm that makes joint proposals to ensure cancellation of the unknown likelihoods, and (ii) a nested inference algorithm that uses estimated likelihoods derived from inference over the internal random choices of the planner. Cascading resimulation is simple to implement, but nested inference enables use of a broad class of Monte Carlo, variational, and neural network mechanisms to handle the intractable likelihoods."
    }, {
      "heading" : "2 MODELING GOAL-DIRECTED BEHAVIOR USING RANDOMIZED PATH PLANNERS",
      "text" : "This paper defines probabilistic models of goal-directed behavior using randomized path-planning algorithms. Algorithm 1 describes one such planner, called AGENTPATH. This planner can be applied to a broad class of environments with complex obstacles. The planner assumes a bounded two-dimensional space (e.g., the square [0, 1]2) and a world map M that is a set of polygonal obstacles. The planner takes as input a start location s ∈ [0, 1]2, a goal location g ∈ [0, 1]2, the map M , and a sequence of T time points t = (t1, . . . , tT ), and returns either a sequence of locations z ∈ [0, 1]2T on a path from s to g at each time ti, or ‘no-path-found’. The planner operates by growing a rapidly-exploring random tree (RRT) [LaValle, 1998] from the start location s to fill the space, searching for a clear line of sight between the tree and the goal. If a path is found, it is then refined to minimize its length using local optimization. Finally, the agent walks the path at a constant speed, producing the output locations z. See Appendix A for more details.\nMany variations of this planner are possible, including\nversions that take into account costs other than path length, and spaces encoding configurations other than geographic position (e.g., configuration spaces of an articulated robot). The planner parameters N and R trade off the cost of planning with the (probable) optimality of the paths (see Figure 1). Figure 2 and Figure 4 show this planner being used as a modeling primitive in the Venture probabilistic programming platform [Mansinghka et al., 2014]. The planner was implemented in C and imported as a foreign modeling primitive into Venture. Venture supports likelihood-free primitives and design of custom inference strategies, including those of Section 3.\nAlgorithm 1 Model of an agent’s path given destination\nRequire:  World map M ; Start, goal s, g ∈ [0, 1]2 Time points t ∈ RT+ Refinement amount N ; Restarts R Max. # tree nodes J ;Min. # tree nodes S\n1: procedure RRT(M , s, g) 2: V ← {s} . Initialize tree with start s 3: for j ← 1 to J do . J tree growth iterations 4: a ∼ Uniform([0, 1]× [0, 1]) . Random point 5: if M.VALID-STATE(a) then 6: b← NEAREST-VERTEX(V, a) 7: ∼ Uniform([0, 1]) 8: c← a+ (1− )b . Propose new vertex 9: if M.CLEAR-LINE(b, c) then 10: V.ADD-EDGE(b→ c) . Extend tree 11: if M.CLEAR-LINE(c, g) ∧ j > S then 12: V.ADD-EDGE(c→ g) 13: return PATH-IN-TREE(V, s, g) 14: return ‘no-path-found’ 15: procedure PLAN-PATH(M , s, g; R, N ) 16: for r ← 1 to R do . Generate R paths 17: p(r) ∼ RRT(M, s, g) 18: p(r) ← SIMPLIFY-PATH(p(r)) 19: p(r) ∼ REFINE-PATH(M, s, g,p(r)) 20: d(r) ← PATH-LENGTH(p(r), s, g) 21: r∗ ← ARGMIN(d) . Select best of R paths 22: return p(r ∗) 23: procedure AGENT-PATH(M , s, g, t; R, N ) 24: p ∼ PLAN-PATH(M, s, g;R,N) . Abstract path 25: z← WALK-PATH(p, t) . Locations at times t 26: return z"
    }, {
      "heading" : "3 INFERENCE IN PROBABILISTIC PROGRAMS WITH LIKELIHOOD-FREE PRIMITIVES",
      "text" : "The path planner AGENT-PATH of Algorithm 1 can be used in a probabilistic program either by implementing the planner in a probabilistic programming language, or by treating the planner as a primitive random choice. We treat the planner as a random choice, as this allows use of an optimized C implementation of the planner. However, probabilistic programming languages such as Church, Stan, BLOG, and Figaro all require random choices to have tractable marginal likelihoods [Goodman et al., 2012; Carpenter et al., 2016; Milch et al., 2007; Pfeffer, 2009]. Computing the marginal likelihood of AGENT-PATH for outputs z and inputs M , s, g, and t would involve an intractable intregral over the (thousands of) internal random choices made in AGENT-PATH.\nThis section introduces two Monte Carlo strategies for inference in probabilistic programs that include random choices with intractable marginal likelihoods, referred to as “likelihood-free” primitives. The first strategy, shown in Algorithm 2, is called Cascading Resimulation Metropolis-Hastings; it makes block proposals to likelihood-free random choices, exploiting cancellation of the unknown likelihoods. The second, shown in Algorithm 3, is called Nested Inference Metropolis-Hastings; it uses Monte Carlo estimates of the unknown likelihoods in place of the likelihoods themselves. Although sim-\nple techniques like likelihood-weighting can also be used in the presence of likelihood-free primitives, they tend to work well only when a global proposal that is wellmatched to the posterior is available. The algorithms we introduce do not have this limitation.\nWe first introduce notation. Let T be the set of primitive random choices available to a probabilistic program (e.g. {FLIP, UNIFORM_CONTINUOUS, AGENT-PATH}). For t ∈ T , let Xt denote the set of valid arguments for the primitive, let Zt denote the set of possible outputs, and let pt(z;x) denote the marginal likelihood of output z ∈ Zt given arguments x ∈ Xt, where ∫ pt(z;x)dz = 1 for all x ∈ Xt. We do not require evaluation of pt(z;x) to be computationally tractable.\nFollowing Wingate et al. [2011], for a probabilistic program P , we assume there is a name i ∈ I assigned to every possible random choice, for some countable I. We assume that distinct random choices are assigned unique names within every execution of P . The set of names used in an execution is some finite set I ⊆ I. We require that all random choices with name i are of the same type ti ∈ T . Each unique completed execution of P can therefore be represented as the finite set of names I ⊆ I of those random choices made in the execution, together with the result values. We denote these results z ∈×i∈I Zti , and denote this complete package ρ = (I, z). The tuple ρ is called an execution trace of the probabilistic program P .\nThis paper focuses on probabilistic programs where I is the same for all executions — that is, the set of random choices made is not affected by any of those choices. Relaxations of this are left for future work; more general formalizations of probabilistic programs can be found in [Wingate et al., 2011; Mansinghka et al., 2014].\nWe consider random choice j to depend on random choice i if changing the result zi of i can lead to a change in the inputs xj of j, even if all other results zI\\{i} are held fixed. We assume that it is possible to construct a directed acyclic dependency graph G = (I, E) among random choices I , where an edge (i, j) ∈ E ⊂ I × I exists if and only if random choice j depends on random choice i in the above sense. The parents of a random choice j are denoted by πG(j) := {i ∈ I : (i, j) ∈ E}. The arguments xj of each random choice j are then a (deterministic) function fj of the results of random choices in πG(j), which are denoted zπG(j); we write xj = fj(πG(j)). Let cG(i) := {j ∈ I : (i, j) ∈ E} denote the ‘children’ of choice i. Also, let F ⊆ I denote the random choices with intractable likelihoods (the “likelihood-free” choices). Let C ⊆ (I \\ F ) denote the random choices that are constrained based on data, which must have tractable likelihoods. Let zC ∈×i∈C Zi denote the values we are constraining those random choices to. The joint probability density of an execution trace ρ = (I, z) is:\np(z) := ∏ i∈I pti(zi; fi(zπG(i))) (1)\nwhere we have omitted the dependence on I because it is the same for all executions."
    }, {
      "heading" : "3.1 CASCADING RESIMULATION METROPOLIS-HASTINGS",
      "text" : "How can a probabilistic program cope with complex, likelihood-free primitives? Our core insight is that if the proposal distribution m(z′i; ·) for a random choice zi is equal to the prior pti(z ′ i; fi(zπG(i))), then the likelihoods will cancel in a Metropolis-Hastings (MH) acceptance ratio and therefore do not need to be explicitly computed. Sampling from the prior is achieved simply by simulating the random choice. A (prototypical) acceptance ratio looks like this:\nα = pti (z\n′ i;·)\npti (zi;·) · m(zi;·)m(z′i;·) =\npti (z ′ i;·) pti (zi;·) · pti (zi;·)pti (z′i;·) = 1\ntarget proposal target proposal\nWe use blocked proposals in which a change to a likelihood-free choice is proposed from the prior whenever a proposal is made to any of its parents. A likelihood-free choice that is proposed may itself have\nlikelihood-free choices as children, in which case these children are also proposed, generating a cascade of proposals. Algorithm 2 shows the Cascading Resimulation MH transition operator, which extends an initial custom proposalm(z′i; z) to random choice i (which must not be likelihood-free) to also include any likelihood-free random choices H in the cascade, such that the intractable likelihoods cancel.\nAlgorithm 2 Single-site Cascading Resimulation Metropolis-Hastings transition\nRequire:  Prob. program with dep. graph G = (I, E) Likelihood-free random choices F ⊆ I Proposed-to random-choice i ∈ (I \\ F ) Custom proposal density m(z′i; z) for choice i Previous values z for all random choices\n1: z′i ∼ m(·; z) . Propose a new value for choice i 2: z′I\\{i} ← zI\\{i} . Initially, no change to other choices 3: `← 1 . Unnormalized target density for previous values 4: `′ ← 1 . Unnormalized target density for proposed values 5: B ← {i} ∪ cG(i) . Ask for likelihoods from j ∈ B 6: H ← {} . Likelihood-free cascade participants 7: A← {i} . Visited choices with tractable likelihoods 8: while |B| > 0 do 9: j ← POP(B) . Pop in topological order 10: if j ∈ F then . Choice j is likelihood-free 11: z′j ∼ ptj (·; fj(z′πG(j))) . Propose from prior 12: INSERT(B, cG(j)) . Ask for child likelihoods 13: H ← H ∪ {j} 14: else . Choice j has tractable likelihood 15: `← ` · ptj (zj ; fj(zπG(j))) 16: `′ ← `′ · ptj (z′j ; fj(z′πG(j))) 17: A← A ∪ {j} 18: α← (`′/`) · (m(zi; z′)/m(z′i; z)) . MH ratio 19: s ∼ Uniform(0, 1) 20: if s ≤ α then 21: z ← z′ . Accept\nAlgorithm 2 is a Metropolis-Hastings transition over the random choices {i} ∪ H with target density equal to the local posterior p ( z{i}∪H |zI\\{{i}∪H} ) , and with proposal density:\nm(z′i; z) ∏ j∈H ptj (z ′ j ; fj(z ′ πG(j) )) (2)\nThe Metropolis-Hastings acceptance ratio is:\nα =\n(∏ j∈H∪A ptj (z ′ j ; fj(z ′ πG(j)\n))∏ j∈H∪A ptj (zj ; fj(zπG(j)))\n· m(zi; z\n′) ∏ j∈H ptj (zj ; fj(zπG(j)))\nm(z′i; z) ∏ j∈H ptj (z ′ j ; fj(z ′ πG(j) ))\n)\n= m(zi; z\n′) ∏ j∈A ptj (z ′ j ; fj(z ′ πG(j) ))\nm(z′i; z) ∏ j∈A ptj (zj ; fj(zπG(j)))\n(3)\nWe illustrate Cascading Resimulation MH in Figure 2, on the task of inferring the goal of a simulated drone in an observed environment."
    }, {
      "heading" : "3.2 NESTED INFERENCE METROPOLIS-HASTINGS",
      "text" : "In some problems, Cascading Resimulation MH will generate many expensive simulations of likelihood-free choices, most of which will be rejected. For these problems, and for real-time applications, we propose an alternative Metropolis-Hastings algorithm, called Nested Inference MH, that uses Monte Carlo estimates of the intractable likelihoods in the acceptance ratio. The likelihood estimates are obtained using auxiliary “nested inference” algorithms, which sample probable values for the internal random choices made by a likelihood-free choice (e.g. a randomized planning algorithm) given its inputs and outputs, and calculate a weight that can be used to form an importance sampling estimate of the unknown likelihood.\nNested Inference MH is based on an interpretation of likelihood-free random choices like AGENT-PATH as probabilistic programs in their own right. Let u ∈ Ut be an execution trace of a likelihood-free random choice of type t. We denote the joint density on execution traces u and return values z of the random choice, given input arguments x, by pt(u, z;x). The marginal likelihood of the random choice is given by the (intractable) integral pt(z;x) = ∫ pt(u, z;x)du. We denote the conditional trace density for arguments x and output z by pt(u|z;x) := pt(u, z;x)/pt(z;x).\nNested inference assumes the existence of a nested inference algorithm that samples execution traces u according to some density qt(u;x, z) that approximates the conditional density on traces of the likelihood-free choice, i.e., qt(u;x, z) ≈ pt(u|z;x). We require that qt(u;x, z) > 0 for all u where pt(u|z;x) > 0. Using the nested inference algorithm as an importance sampler, we produce an unbiased importance sampling estimate p̂t(z;x) of the random choice’s intractable likelihood for arguments x and output z by sampling K times uk ∼ qt(·;x, z) from the inference algorithm, as follows:\np̂t(z;x) := 1\nK K∑ k=1 pt(uk, z;x) qt(uk;x, z) for uk ∼ qt(·;x, z).\n(4)\nNested inference also assumes that the ratio pt(u, z;x)/qt(u;x, z) can be evaluated. While in principle the nested inference algorithm can be produced by recoding the likelihood-free primitive in a high-level probabilistic programming language, this is by no means required, nor do we expect it to be the common case. In this paper, we focus on nested inference algorithms that use learned neural networks.\nThe accuracy of the likelihood estimate is determined by\nthe accuracy of the nested inference algorithm. Specifically, for K = 1 the variance of the estimate is:\nVaru∼q(·;x,z)\n[ pt(u, z;x)\nqt(u;x, z) ] ∝ Dχ2 (pt(u|z;x)||qt(u;x, z)) , (5)\nwhere Dχ2 denotes the chi-square divergence [Nielsen and Nock, 2014], and where pt(u|z;x) and qt(u;x, z) on the right-hand side represent density functions over u, not specific density values. Similarly, we can view log(pt(u, z;x)/qt(u;x, z)) for u ∼ q(·;x, z) as a (biased) estimator of log pt(z;x), where the bias is:\nEu∼qt(·;x,z)\n[ log pt(u, z;x)\nqt(u;x, z)\n] − log pt(z;x)\n= −DKL(qt(u;x, z)||pt(u|z;x)), (6)\nwhere DKL denotes the Kullback-Leibler (KL) divergence [Kullback and Leibler, 1951]."
    }, {
      "heading" : "3.2.1 Nested Inference Metropolis-Hastings",
      "text" : "Algorithm 3 describes a Nested Inference MH transition in which a custom proposal is made to a likelihood-free random choice i that uses estimated likelihoods produced using a nested inference algorithm. It assumes that all children of i also have nested inference algorithms themselves. Heterogeneous configurations are also possible.\nAlgorithm 3 Single-site Nested Inference MetropolisHastings transition\nRequire:  Prob. program with dep. graph G = (I, E) Proposed-to random choice i Custom proposal density m(z′i; z) Previous values z for all random choices Previous likelihood estimates ` for all choices\n1: z′i ∼ m(·; z) . Propose a new value for choice i 2: z′I\\{i} ← zI\\{i} . No change to other choices 3: for k ← 1 to K do 4: u′i,k ∼ qti(·;xi, z′i) . Choice i nested inference 5: `′i ← 1K ∑K k=1 pti (u ′ i,k,z ′ i;xi)\nqti (u ′ i,k ;xi,z ′ i)\n. Estimate pti(z ′ i;xi)\n6: for j ∈ cG(i) do 7: for k ← 1 to K do 8: u′j,k ∼ qtj (·;x′j , zj) . Choice j nested inference\n9: `′j ← 1K ∑K k=1 ptj (u ′ j,k,zj ;x ′ j)\nqtj (u ′ j,k ;x′j ,zj) . Estimate ptj (zj ;x\n′ j) 10: α← (∏\nj∈{i}∪cG(i) `′j `j\n) · ( m(zi;z\n′) m(z′i;z) ) 11: s ∼ Uniform(0, 1) 12: if s ≤ α then 13: zi ← z′i . Accept 14: for j ∈ {i} ∪ cG(i) do 15: `j ← `′j . Update density estimates\nAlthough this transition uses Monte Carlo estimates of likelihoods in the acceptance ratio, it is a standard Metropolis-Hastings transition on an extended state\nspace that includes the result zi of the proposed-to random choice i, K traces ui,k of the proposed-to random choice, andK traces uj,k of each child j of the proposedto random choice. The target density on the extended space is:\np(zi|zI\\{i}) ∏\nj∈{i}∪cG(i)\n1\nK K∑ k=1 ptj (uj,k|zj ;xj) K∏ r=1 r 6=k qtj (uj,r;xj , zj)\n(7) The proposal density on the extended space is:\nm(z′i; z) ∏\nj∈{i}∪cG(i)\nK∏ k=1 qtj (u ′ j,k;xj , zj) (8)\nThe values zj of other random choices j 6∈ {i}∪cG(i) are constant. See Appendix C for derivation. The marginal density of zi in the extended target density is the local posterior p(zi|zI\\{i}) for the result of random choice i given the values of all other random choices. Single-site Nested Inference MH transitions that propose to different random choices i but use the same database of nestedinference likelihood estimates ` can be composed to form Markov chains that converge to the posterior p(zI\\C |zC).\nOur use of unbiased likelihood estimates in place of the true likelihoods when computing the MetropolisHastings acceptance ratio in Algorithm 3 is closely related to pseudo-marginal MCMC [Andrieu and Roberts, 2009] and particle MCMC [Andrieu et al., 2010]. Indeed, each single-site Nested Inference MH transition can be seen as a compositional variant of a ‘grouped independence MH’ transition [Beaumont, 2003] in which several pseudo-marginal likelihoods (one for each random choice j ∈ {i} ∪ cG(i)) are used in the same update. The database of nested-inference likelihood estimates ` stores the ‘recycled’ pseudo-marginal likelihood estimates from previous transitions.\nThe convergence rate of a Markov chain based on Nested Inference MH transition operators depends on the accuracy of the nested inference algorithm and K. In the limit of exact nested inference algorithm (qt(u;x, z) = pt(u|z;x)) the likelihood estimates are exact, and the algorithm is identical to standard Metropolis-Hastings. If the nested inference algorithm is very inaccurate, it may routinely propose traces u that are incompatible with the output z of the random choice, resulting in low acceptance rates. Better characterizing how the convergence rate depends on the accuracy of the nested inference algorithms and on K is an important area for future work."
    }, {
      "heading" : "3.2.2 Learning a nested inference algorithm",
      "text" : "It is possible to learn a nested inference algorithm qt(u;x, z) that approximates pt(u|z;x). The idea of\nlearned inference for probabilistic generative models goes back at least to Morris [2001] and has also been used in Stuhlmüller et al. [2013] and Kingma and Welling [2013]. We apply this idea to nested inference as follows. Let qt,θ(u;x, z) denote a nested inference algorithm that is parameterized by θ — for example, θ might be the weights of a neural network used as part of the inference algorithm. We establish a training distribution dt(x) over the arguments to the primitive t, and approximately solve the following optimization problem:\nmin θ { Ex∼dt(·) z|x∼pt(·;x) [DKL(pt(u|z;x)||qt,θ(u;x, z))] }\n= min θ Ex∼dt(·)z|x∼pt(·;x) u|x,z∼pt(·|z;x) [ log pt(u|z;x) qt,θ(u;x, z) ] The goal is for qt,θ(u;x, z) to approximate pt(u|z;x) well (i.e., have small KL divergence) for typical input arguments x ∼ dt(·). We approximate this objective function by drawing M independent sets of input arguments x(i) from the training distribution, and running a traced execution of the likelihood-free random choice (e.g. planner) on each set of arguments, recording1 the trace u(i) and output z(i):\nx(i) ∼ dt(·) Sample planner arguments\nu(i), z(i) ∼ pt(·, ·;x(i)) Run likelihood-free planner, record trace u(i), output z(i)\nWe use the resulting dataset D = {(x(i), z(i), u(i)) : i = 1 . . .M} to define an approximate objective function JD(θ) that is an unbiased estimate of the original objective function:\nJD(θ) := 1\nM M∑ i=1 log pt(u (i)|z(i), x(i)) qt,θ(u(i);x(i), z(i))\n(9)\n= C − 1 M M∑ i=1 log qt,θ(u (i);x(i), z(i)) (10)\nwhere C does not depend on θ. Note that minimizing JD(θ) over θ is equivalent to maximizing the loglikelihood of the data D. Because we use forward simulations to produce u(i), z(i) jointly from pt(·, ·;x(i)), we have one exact conditional sample u(i)|z(i) ∼ pt(·|z(i);x(i)) for each training example.\n1This training regime cannot be applied to a true black-box path planner, since a recording of its internal randomness is now necessary. However, such recordings can be produced from a straightforwardly instrumented version of the algorithm. The likelihood estimator for the planner can still be treated as a black-box by the Nested Inference MH transition."
    }, {
      "heading" : "4 EXAMPLE APPLICATIONS",
      "text" : "We have implemented four example applications, designed to illustrate the flexibility of our framework:\n1. Inferring the probable goal of a simulated drone. This example shows that small changes to the environment, such as including an additional doorway, can yield large changes in the inferred goals.\n2. Inferring the probable goal of a simulated drone with a more complex planner. Specifically, we model the drone as following a multi-part path produced by a planner that first chooses a waypoint uniformly at random and then recursively solves the two path planning problems induced by the choice of waypoint. This example shows (a) applicability of the framework to more complex models of goaldirected behavior, and (b) that Nested Inference MH with a learned neural network can outperform Cascading Resimulation MH.\n3. Inferring whether or not two people walking around tables in a room are headed for the same goal or different goals. This example demonstrates applicability to simple hierarchical models for goals and also demonstrates applicability to real-world (as opposed to synthetic) data.\n4. Jointly inferring a simulated agent’s goals and its beliefs about an obstacle in the map whose location, size, and orientation is unknown to the probabilistic program. This example is described in the appendix due to space constraints."
    }, {
      "heading" : "4.1 EXAMPLE 1: SENSITIVITY OF GOAL INFERENCE TO SMALL MAP CHANGES",
      "text" : "Figure 2 shows a comparison of goal inference in two different maps given the same observations. The map for the scenario on the left has an enclosure with two openings, one on the top and one on the bottom, while the map for the scenario on the right has a single opening. In the map on the left, the inferred goal samples fall outside the enclosure, because if the drone intended to go inside the enclosure, it could have taken a much shorter path. In the map on the right, a significant fraction of goal samples fall inside the enclosure, as relatively efficient paths into the enclosure go through the partial trajectory that has been observed so far. Samples shown are the final states of 480 independent replicates of a Markov chain initialized from the prior, with 1000 Cascading Resimulation MH transitions (Algorithm 2) using the prior as the proposal. Planner parameters are R = 10, N = 1000, = 0.01, v = 0.5, J = 10000, S = 2000."
    }, {
      "heading" : "4.2 EXAMPLE 2: HANDLING PATH PLANNERS WITH WAYPOINTS VIA",
      "text" : "NESTED INFERENCE\nNext, we used a model where the agent may choose a waypoint and separately plan a path to the waypoint and a path from the waypoint to the goal (AGENTWAYPOINT-PATH, Algorithm 4). Unlike the simpler AGENT-PATH model, which typically samples from a small number of modes concentrated at efficient routes from the start to the goal, AGENT-WAYPOINT-PATH yields paths that are unpredictable without knowledge of the waypoint. Parameters R and N of PLAN-PATH are omitted for simplicity. We consider the same goal inference task as in Example 1 but with the alternative planner. Cascading Resimulation MH performs poorly on this task, because the prior is a poor proposal for the internal random choices of AGENT-WAYPOINT-PATH.\nAlgorithm 4 Pseudo-code for a likelihood-free primitive that models observed motion of an agent with known goal but optional unknown waypoint\nRequire: {\nWorld map M ; Start, goal s, g ∈ [0, 1]2 Time points t ∈ RT+\n1: procedure AGENT-WAYPOINT-PATH(M , s, g, t) 2: g′ ∼ Uniform([0, 1]× [0, 1]) . Pick waypoint 3: w ∼ Bernoulli(0.5) . Use waypoint? 4: if w then 5: p1 ∼ PLAN-PATH(M, s, g′) . Start-waypoint 6: p2 ∼ PLAN-PATH(M, g′, g) . Waypoint-goal 7: p = (p1,p2) . Concatenate paths 8: else 9: p ∼ PLAN-PATH(M, s, g) . Start to goal 10: z̃← WALK-PATH(p, t) . Locations at times t 11: z ∼ ADD-NOISE(z̃) . Add noise to locations 12: return z . Return noisy agent locations\nAlgorithm 5 shows a nested inference algorithm for AGENT-WAYPOINT-PATH that uses a neural network to propose the waypoint (g′) and whether the waypoint is used (w), given the goal and observations, and then executes the rest of the planner, conditioned on w and g′. The network was trained on 10,000 runs of AGENT-WAYPOINT-PATH with random goal input g ∼ Uniform([0, 1]2) and fixed world mapM and start s. The nested inference algorithm splits the trace u of AGENTWAYPOINT-PATH into u1 = (w, g′) and u2 (the random choices made within executions of PLAN-PATH), so that u = (u1, u2). The density of the nested inference algorithm is then qt(u;x, z) = qθ(u1;x, z)pt(u2;x), and the density ratio pt(u, x, z)/qt(u;x, z), which is used by Nested Inference MH when estimating the planner likelihoods, simplifies to pt(z|u;x)/qθ(u1;x, z). To evaluate this ratio, we separately evaluate the density pt(z|u; , x) of ADD-NOISE and the density qθ(u1;x, z) of the neural network’s stochastic outputs.\nSource: https://github.com/probcomp/mimh-2017/commit/68d9079c5ef7b34d0bf7c6a1d1ddcfc3f 26aaf3e python run_experiment.py python render_results.py\nAlgorithm 5 Using a neural network for nested inference in the AGENT-WAYPOINT-PATH path planner\nRequire: {\nArguments to planner x = (M, s, g, t) Hypothetical output of planner z\n1: w, g′ ∼ qθ(·;x, z) . Sample waypoint from neural net. 2: if w then 3: p1 ∼ PLAN-PATH(M, s, g′) . Start to waypoint 4: p2 ∼ PLAN-PATH(M, g′, g) . Waypoint to goal 5: return (p1,p2) 6: else 7: return PLAN-PATH(M, s, g) . Start to goal\nWe compared three strategies for goal inference: Nested Inference MH using Algorithm 5 and K = 1, Cascading Resimulation MH, and Nested Inference MH using a “resimulation” nested inference algorithm (qt(u;x, z) = pt(u;x)) and K = 2, 10. Figure 3 shows that neural Nested Inference MH converges faster than the other strategies. Planner parameters were the same as in Example 1. All inference strategies were implemented using a custom Python inference library. Integration of Nested Inference MH with Venture is left for future work."
    }, {
      "heading" : "4.3 EXAMPLE 3: MODELING REAL-WORLD HUMAN MOTION",
      "text" : "The Venture program of Figure 4(c) defines a model with two agents whose destinations may or may not be the same. The environment (world) and the start locations of the agents are known. The is_common_goal flag determines whether the agents share the same goal destination. The paths of both agents are modeled using AGENT-PATH. The corresponding Bayesian network is shown in Figure 4(e). We collected video of two col-\nlaborators walking in a scene containing tables, for two conditions—one in which the they meet at a common location, and one where they diverge. For the commongoal condition we constructed short and extended sequences of observed locations (Figure 4(a) and (b)). We used Cascading Resimulation MH for inference, initialized from the prior, with a joint prior proposal over all latent variables. We ran 60 chains of 200 transitions each, and rendered the final states in Figures 4(a-b). The speed for each individual was set to their average speed along the observed path. The estimated probabilities of is_common_goal=True for the short and extended sequences are 0.63 and 0.82 respectively. This trend qualitatively matches human judgments, shown in Figure 4(d) (the model was not calibrated to match human judgments). See Appendix B for additional results."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "This paper introduced a class of probabilistic programs for formulating goal inference as approximate inference in probabilistic generative models of goal-directed behavior. The technical contributions are (i) a probabilistic programming formulation that makes complex goal and map priors easy to specify; (ii) the use of randomized path planning algorithms as the backbone of generative models; and (iii) the introduction of Monte Carlo techniques that can handle the intractable likelihoods of these path planners. The experiments showed that it is possible for short probabilistic programs to make meaningful inferences about goal-directed behavior.\nFrom the standpoint of robotics, autonomous driving, or reconnaissance, the examples in this paper are quite pre-\nliminary. More experiments are needed to explore the accuracy of approximate inference in these models, as well as the accuracy of the models themselves, especially on real-world problems. The probabilistic programming formulation makes it easy to explore variations of models, environments, and inference strategies.\nThe problem of inferring the mental states of autonomous agents is central to probabilistic artificial intelligence. It may also be a natural application for structured generative models and for probabilistic programming, but only if sufficiently fast and flexible inference schemes can be developed. We hope this paper helps to encourage the use of probabilistic programming for building intelligent software that can draw meaningful inferences about goal-directed behavior."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank Feras Saad for obtaining human judgment data. Tree and car 3D models in figures are from http://www.f-lohmueller.de/ [Lohmüller, 2016]. This research was supported by DARPA (PPAML program, contract number FA875014-2-0004), IARPA (under research contract 2015- 15061000003), the Office of Naval Research (under research contract N000141310333), the Army Research Office (under agreement number W911NF-13-1-0212), and gifts from Analog Devices and Google. Marco Cusumano-Towner is supported by the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program."
    }, {
      "heading" : "A PLANNER DETAILS",
      "text" : "We now describe details of the planner omitted from the main text, including the procedures SIMPLIFY-PATH, REFINE-PATH, and WALK-PATH, which are defined in Algorithm 6. Paths p are represented as sequences of points, with lines connecting the points. The path p begins with start s and ends with goal g. To be a valid path with respect to map M , no point in the path may lie within an obstacle (polygon) of M (i.e. M.IS-VALID(pi)), and no line between two adjacent path points may intersect an obstacle of M (i.e. M.CLEAR-LINE(pi,pi+1)).\nAlgorithm 6 Additional details of the AGENT-PATH model of goal-directed behavior.\nRequire:  World map M ; Start, goal s, g ∈ [0, 1]2 Time points t ∈ RT+ Refinement amount N ; Restarts R Agent speed v ∈ R+\n1: procedure SIMPLIFY-PATH(M , p, s, g) 2: p′1 ← s . Initialize simplified path 3: j ← 2 4: for i← 2 to NUM-POINTS(p)− 1 do 5: if not M.CLEAR-LINE(pi−1,pi+1) then 6: p′j ← pi . Point pi is needed, keep it 7: j ← j + 1 8: else 9: pass . Point pi is not needed, skip it 10: p′j ← g . Add goal g to simplified path 11: return p′ 12: procedure REFINE-PATH(M , s, g, p) 13: for i← 1 to N do 14: d← PATH-LENGTH(p, s, g) 15: for l← 1 to L do . Iterate over L path dims. 16: ∼ N (0, σ2) 17: p′ ← p+ · el . Change path dim. l 18: d′ ← PATH-LENGTH(p′, s, g) 19: if d′ < d ∧M.CLEAR-PATH(p′, s, g) then 20: (d,p)← (p′, d′) . Accept 21: return p 22: procedure WALK-TO(p, t, v) 23: d← 0.0 . Path dist. from s traveled so far 24: d∗ ← tv . Desired path distance from s 25: for j ← 1 to NUM-POINTS(p)− 1 do 26: δ ← ||pj − pj+1||2 . Dist. to next point 27: if d+ δ > d∗ then 28: e← d∗ − d 29: return δ−e\nδ pj +\n( 1− δ−e\nδ\n) pj+1"
    }, {
      "heading" : "30: d← d+ δ",
      "text" : "31: return g . Once reached goal, stay forever 32: procedure WALK-PATH(p, t, v) 33: for i← 1 to T do 34: zi ← WALK-TO(p, ti, v) 35: return z"
    }, {
      "heading" : "B ADDITIONAL EXPERIMENTS",
      "text" : "B.1 JOINTLY INFERRING THE BELIEF AND GOAL OF AN AGENT\nThe Venture program of Figure 6(a) defines a model in which the belief of an agent about its environment, upon which the agent’s motion plan depends, is uncertain. The environment contains two, static objects (known_objects): a tree and a central divider wall that divides the [0, 1] × [0, 1] square into a left and right side. There are passageways between the left and right side that go above and below the divider. However, the agent has knowledge of (or belief in) an additional obstacle wall (obstacle), and the agent plans their path to the destination (goal) taking this additional obstacle into account. Figure 6(a) also shows a Bayesian network representation of this model. We seek to infer both the agent’s goal and the agent’s beliefs about the location, orientation, and size of the obstacle.\nWe used Cascading Resimulation Metropolis-Hastings (Algorithm 2) with a single repeated transition operator based on an independent joint proposal to goal (Uniform([0, 1]2)) and to the unknown parameters of obstacle (start post location, orientation, and length, proposed from the prior). We initialized from the prior. Parameters of the planner AGENT-PATH were R = 10, N = 1000, = 0.01, v = 0.5, J = 10000, S = 2000. We ran several independent Markov chains of 1000 iterations each, on a synthetic dataset in which the agent takes a path from the right to the left of the map by going below the divider. The final state of four such chains are visualized in Figure 6(b). For this dataset, the goal destination of the agent is revealed with certainty because the agent reaches and stops in the upper left corner. The obstacle inferences indicate that agent believes the upper route to its goal is blocked, because otherwise the agent would have taken the shorter, upper route, to its goal. However, the specific details of how the obstacle blocks the upper passageway remain uncertain.\nB.2 GOAL INFERENCE IN A DRIVING SCENARIO\nFigure 7 shows an application of the multi-agent common-goal model of Figure 4 to a driving scenario. We show 60 independent replicates of 3000 iterations of Cascading Resimulation Metropolis-Hastings each. The results illustrate that this model can be used with varied environments.\nB.3 REAL-WORLD HUMAN MOTION, ALTERNATE SEQUENCE\nWe extended the experiment described in Section 4.3 and shown in Figure 4 by running Cascading Resimulation Metropolis-Hastings on an alternate sequence of observed person locations in which the individals diverge to separate individual goal destination. The inferences, shown in Figure 5, confirm the expectations, with all samples indicating is_common_goal = False. Samples were obtained from the final state of 120 independent Markov chains, with initialization from the prior, followed by 1200 iterations of Cascading Resimulation Metropolis-Hastings.\nB.4 INFERENCE WITH WAYPOINT PLANNER\nFigure 8 compares waypoints and paths proposed by Nested Inference MH with a neural nested inference algorithm with waypoints and paths proposed by Cascading Resimulation MH on an illustrative example data set. The poor quality of the prior as the proposal, as used by Cascading Resimulation, results in unecessary rejections, and slow convergence. The neural network proposes waypoints near the bend in the path.\nThe KL divergence estimates of Figure 3(d) were obtained by binning 960 independent reference samples (30,000 transitions of Cascading Resimulation MH, initialized from the prior) and binning 960 independent approximate inference samples for each inference algorithm evaluated. The world unit square was binned into 25 squares (5-by-5), and a discrete distribution was estimated for each sampler by counting the number of samples falling into each bin, adding a pseudocount of 0.1 to each bin, and normalizing. The KL divergence from the resulting reference sampler histogram was computed to each resulting approximate inference algorithm histogram. For each inference strategy (Cascading Resimu-\nlation MH, Neural Nested Inference MH with K = 1, Resimulation Nested Inference MH with K = 2 and Resimulation Nested Inference MH with K = 10), the number of MH transitions was varied over several orders of magnitude, and the final state in each chain was recorded, to obtain samples for each inference algorithm evaluated. The number of MH transitions used to obtain samples shown in Figures 3(a,b,c) are 10, 1, and 10, respectively. Figure 9 shows additional samples comparing Nested Inference Metropolis-Hastings with a neural nested inference algorithm with Cascading Resimulation Metropolis-Hastings.\n(c) Rendered approximate posterior inference samples for inference over common_goal, goal, and goal_a, and goal_b in the model of (a), given known start locations for two argents (start_a and start_b), a known world, and observed path locations for the two agents. E\nSource: https://github.com/probcomp/mimh-2017/commit/392598f910bf47db 17ef4d8f00722e906060d647 python run_experiment.py python render_results.py"
    }, {
      "heading" : "C NESTED INFERENCE DERIVATIONS",
      "text" : "The variance of the likelihood estimate with K = 1 is:\nVaru∼q(·;x,z)\n[ pt(u, z;x)\nqt(u;x, z) ] = pt(x; z) 2 · ∫ (\npt(u|z;x) qt(u;x, z)\n− 1 )2 q(u;x, z)du\n∝ ∫ (\npt(u|z;x) qt(u;x, z)\n− 1 )2 q(u;x, z)du\n= Dχ2(pt(u|z;x)||qt(u;x, z))\nThe bias of the log likelihood estimate with K = 1 is:\nEu∼qt(·;x,z)\n[ log pt(u, z;x)\nqt(u;x, z)\n] − log pt(z;x)\n= Eu∼qt(·;x,z)\n[ log\npt(u|z;x) qt(u;x, z) ] = −DKL(qt(u;x, z)||pt(u|z;x))\nAlgorithm 3 can be intuitively understood as an approximation to a single-site MH update where a single value zi is being updated with proposal m(z′i; z) and target density p(zi|zI\\{i}), and where estimates of likelihoods are used in place of actual likelihoods when computing the\nMH acceptance ratio. However, Algorithm 3 is theoretically justified by recognizing that it is a standard joint MH transition on an extended state space that consists of zi (the value of the proposed-to random choice), ui,k for k = 1 . . .K (a set of K traces for the proposed-to random choice), and uj,k for j ∈ cG(i) and k = 1 . . .K (a set ofK traces for each of the children of the proposed-to random choice). The extended target density is:\np(zi|zI\\{i}) ∏\nj∈{i}∪cG(i)\n1\nK K∑ k=1 ptj (uj,k|zj ;xj) K∏ r=1 r 6=k qtj (uj,r;xj , zj)\n(11) Note that the marginal target density of zi is the original target density p(zi|zI\\{i}), which is proportional to pti(zi;xi) ∏ j∈cG(i) ptj (zj ;xj). Substituting\npti(zi;xi) ∏ j∈cG(i) ptj (zj ;xj) for p(zi|zI\\{i}) in the extended target density expression and simplifying gives the following unnormalized extended target density:\n∏ j∈{i}∪cG(i) 1 K K∑ k=1 ptj (uj,k, zj ;xj) K∏ r=1 r 6=k qtj (uj,r;xj , zj)\n(12)\nThe extended proposal density is:\nm(z′i; z) ∏\nj∈{i}∪cG(i)\nK∏ k=1 qtj (u ′ j,k;xj , zj) (13)\nThe ratio of the unnormalized extended target density over the extended proposal density, for proposed values z′i and u ′ j,k for all j ∈ {i} ∪ cG(i) and k = 1 . . .K, with all other z′I\\{i}∪cG(i) = zI\\{i}∪cG(i) is:\n1\nm(z′i; z) ∏ j∈{i}∪cG(i) 1 K K∑ k=1 ptj (u ′ j,k, z ′ j ;x ′ j) qtj (u ′ j,k;x ′ j , z ′ j)\n(14)\nNote that each factor 1K ∑K k=1 ptj (uj,k,zj ;xj)\nqtj (uj,k;xj ,zj) within this\nratio takes the form of a nested inference likelihood estimate. Composing the full MH acceptance ratio for the extended target and proposal densities gives the acceptance ratio used in Algorithm 3. Note that Algorithm 3 samples the proposed joint state zi, ui,k for k = 1 . . .K, and uj,k for j ∈ cG(i) and k = 1 . . .K precisely according to the extended proposal density. Finally, note that the nested-inference likelihood estimates ` for accepted proposals are retained between updates in Algorithm 3. These estimate values serve as summaries of the previous iterates for the traces ui,k and uj,k. Although the transition is an MH transition on the extended space including the traces, the previous estimates ` are sufficient for evaluating the extended MH acceptance ratio and retaining the previous trace iterates themselves is not necessary."
    } ],
    "references" : [ {
      "title" : "The pseudomarginal approach for efficient monte carlo computations",
      "author" : [ "Christophe Andrieu", "Gareth O Roberts" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Andrieu and Roberts.,? \\Q2009\\E",
      "shortCiteRegEx" : "Andrieu and Roberts.",
      "year" : 2009
    }, {
      "title" : "Particle markov chain monte carlo methods",
      "author" : [ "Christophe Andrieu", "Arnaud Doucet", "Roman Holenstein" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Andrieu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Andrieu et al\\.",
      "year" : 2010
    }, {
      "title" : "Perception for collision avoidance and autonomous driving",
      "author" : [ "Romuald Aufrère", "Jay Gowdy", "Christoph Mertz", "Chuck Thorpe", "Chieh-Chih Wang", "Teruko Yata" ],
      "venue" : null,
      "citeRegEx" : "Aufrère et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Aufrère et al\\.",
      "year" : 2003
    }, {
      "title" : "Goal inference as inverse planning",
      "author" : [ "Chris L Baker", "Joshua B Tenenbaum", "Rebecca R Saxe" ],
      "venue" : "In Proceedings of the Cognitive Science Society,",
      "citeRegEx" : "Baker et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 2007
    }, {
      "title" : "Estimation of population growth or decline in genetically monitored populations",
      "author" : [ "Mark A Beaumont" ],
      "venue" : "Genetics, 164(3):1139–1160,",
      "citeRegEx" : "Beaumont.,? \\Q2003\\E",
      "shortCiteRegEx" : "Beaumont.",
      "year" : 2003
    }, {
      "title" : "Stan: A probabilistic programming language",
      "author" : [ "Bob Carpenter", "Andrew Gelman", "Matt Hoffman", "Daniel Lee", "Ben Goodrich", "Michael Betancourt", "Michael A Brubaker", "Jiqiang Guo", "Peter Li", "Allen Riddell" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "Carpenter et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Carpenter et al\\.",
      "year" : 2016
    }, {
      "title" : "Autonomous driving goes downtown",
      "author" : [ "Uwe Franke", "Dariu Gavrila", "Steffen Gorzig", "Frank Lindner", "F Puetzold", "Christian Wohler" ],
      "venue" : "IEEE Intelligent Systems and Their Applications,",
      "citeRegEx" : "Franke et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Franke et al\\.",
      "year" : 1998
    }, {
      "title" : "Church: a language for generative models",
      "author" : [ "Noah Goodman", "Vikash Mansinghka", "Daniel M Roy", "Keith Bonawitz", "Joshua B Tenenbaum" ],
      "venue" : "arXiv preprint arXiv:1206.3255,",
      "citeRegEx" : "Goodman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Goodman et al\\.",
      "year" : 2012
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "On information and sufficiency",
      "author" : [ "Solomon Kullback", "Richard A Leibler" ],
      "venue" : "The annals of mathematical statistics,",
      "citeRegEx" : "Kullback and Leibler.,? \\Q1951\\E",
      "shortCiteRegEx" : "Kullback and Leibler.",
      "year" : 1951
    }, {
      "title" : "Opportunities and challenges with autonomous micro aerial vehicles",
      "author" : [ "Vijay Kumar", "Nathan Michael" ],
      "venue" : "The International Journal of Robotics Research,",
      "citeRegEx" : "Kumar and Michael.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kumar and Michael.",
      "year" : 2012
    }, {
      "title" : "Rapidly-exploring random trees: A new tool for path planning",
      "author" : [ "Steven M LaValle" ],
      "venue" : null,
      "citeRegEx" : "LaValle.,? \\Q1998\\E",
      "shortCiteRegEx" : "LaValle.",
      "year" : 1998
    }, {
      "title" : "Location-based activity recognition",
      "author" : [ "Lin Liao", "Dieter Fox", "Henry Kautz" ],
      "venue" : null,
      "citeRegEx" : "Liao et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2006
    }, {
      "title" : "Venture: a higher-order probabilistic programming platform with programmable inference",
      "author" : [ "Vikash Mansinghka", "Daniel Selsam", "Yura Perov" ],
      "venue" : "arXiv preprint arXiv:1404.0099,",
      "citeRegEx" : "Mansinghka et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mansinghka et al\\.",
      "year" : 2014
    }, {
      "title" : "blog: Probabilistic models with unknown objects",
      "author" : [ "Brian Milch", "Bhaskara Marthi", "Stuart Russell", "David Sontag", "Daniel L Ong", "Andrey Kolobov" ],
      "venue" : "Statistical relational learning,",
      "citeRegEx" : "Milch et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2007
    }, {
      "title" : "Recognition networks for approximate inference in bn20 networks. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pages 370–377",
      "author" : [ "Quaid Morris" ],
      "venue" : null,
      "citeRegEx" : "Morris.,? \\Q2001\\E",
      "shortCiteRegEx" : "Morris.",
      "year" : 2001
    }, {
      "title" : "On the chi square and higher-order chi distances for approximating fdivergences",
      "author" : [ "Frank Nielsen", "Richard Nock" ],
      "venue" : "IEEE Signal Processing Letters,",
      "citeRegEx" : "Nielsen and Nock.,? \\Q2014\\E",
      "shortCiteRegEx" : "Nielsen and Nock.",
      "year" : 2014
    }, {
      "title" : "Figaro: An object-oriented probabilistic programming language. Charles River Analytics",
      "author" : [ "Avi Pfeffer" ],
      "venue" : "Technical Report,",
      "citeRegEx" : "Pfeffer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Pfeffer.",
      "year" : 2009
    }, {
      "title" : "Learning stochastic inverses",
      "author" : [ "Andreas Stuhlmüller", "Jacob Taylor", "Noah Goodman" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Stuhlmüller et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Stuhlmüller et al\\.",
      "year" : 2013
    }, {
      "title" : "Event modeling and recognition using markov logic networks",
      "author" : [ "Son Tran", "Larry Davis" ],
      "venue" : "Computer vision– ECCV",
      "citeRegEx" : "Tran and Davis.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tran and Davis.",
      "year" : 2008
    }, {
      "title" : "Lightweight implementations of probabilistic programming languages via transformational compilation",
      "author" : [ "David Wingate", "Andreas Stuhlmüller", "Noah D Goodman" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Wingate et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wingate et al\\.",
      "year" : 2011
    }, {
      "title" : "Planning-based prediction for pedestrians",
      "author" : [ "Brian D Ziebart", "Nathan Ratliff", "Garratt Gallagher", "Christoph Mertz", "Kevin Peterson", "J Andrew Bagnell", "Martial Hebert", "Anind K Dey", "Siddhartha Srinivasa" ],
      "venue" : "In Intelligent Robots and Systems,",
      "citeRegEx" : "Ziebart et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ziebart et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "These problems are central to autonomous driving and driver assistance [Franke et al., 1998; Urmson et al., 2008; Aufrère et al., 2003], but also arise in aerial robotics, reconnaissance, and security applications [Kumar and Michael, 2012; Liao et al.",
      "startOffset" : 71,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "These problems are central to autonomous driving and driver assistance [Franke et al., 1998; Urmson et al., 2008; Aufrère et al., 2003], but also arise in aerial robotics, reconnaissance, and security applications [Kumar and Michael, 2012; Liao et al.",
      "startOffset" : 71,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : ", 2003], but also arise in aerial robotics, reconnaissance, and security applications [Kumar and Michael, 2012; Liao et al., 2006; Tran and Davis, 2008].",
      "startOffset" : 86,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : ", 2003], but also arise in aerial robotics, reconnaissance, and security applications [Kumar and Michael, 2012; Liao et al., 2006; Tran and Davis, 2008].",
      "startOffset" : 86,
      "endOffset" : 152
    }, {
      "referenceID" : 19,
      "context" : ", 2003], but also arise in aerial robotics, reconnaissance, and security applications [Kumar and Michael, 2012; Liao et al., 2006; Tran and Davis, 2008].",
      "startOffset" : 86,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "Most existing work along these lines has focused on modeling goal-directed behavior using Markov decision processes and related approaches from stochastic control [Baker et al., 2007; Ziebart et al., 2009].",
      "startOffset" : 163,
      "endOffset" : 205
    }, {
      "referenceID" : 21,
      "context" : "Most existing work along these lines has focused on modeling goal-directed behavior using Markov decision processes and related approaches from stochastic control [Baker et al., 2007; Ziebart et al., 2009].",
      "startOffset" : 163,
      "endOffset" : 205
    }, {
      "referenceID" : 11,
      "context" : "The planner operates by growing a rapidly-exploring random tree (RRT) [LaValle, 1998] from the start location s to fill the space, searching for a clear line of sight between the tree and the goal.",
      "startOffset" : 70,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "Figure 2 and Figure 4 show this planner being used as a modeling primitive in the Venture probabilistic programming platform [Mansinghka et al., 2014].",
      "startOffset" : 125,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "However, probabilistic programming languages such as Church, Stan, BLOG, and Figaro all require random choices to have tractable marginal likelihoods [Goodman et al., 2012; Carpenter et al., 2016; Milch et al., 2007; Pfeffer, 2009].",
      "startOffset" : 150,
      "endOffset" : 231
    }, {
      "referenceID" : 5,
      "context" : "However, probabilistic programming languages such as Church, Stan, BLOG, and Figaro all require random choices to have tractable marginal likelihoods [Goodman et al., 2012; Carpenter et al., 2016; Milch et al., 2007; Pfeffer, 2009].",
      "startOffset" : 150,
      "endOffset" : 231
    }, {
      "referenceID" : 14,
      "context" : "However, probabilistic programming languages such as Church, Stan, BLOG, and Figaro all require random choices to have tractable marginal likelihoods [Goodman et al., 2012; Carpenter et al., 2016; Milch et al., 2007; Pfeffer, 2009].",
      "startOffset" : 150,
      "endOffset" : 231
    }, {
      "referenceID" : 17,
      "context" : "However, probabilistic programming languages such as Church, Stan, BLOG, and Figaro all require random choices to have tractable marginal likelihoods [Goodman et al., 2012; Carpenter et al., 2016; Milch et al., 2007; Pfeffer, 2009].",
      "startOffset" : 150,
      "endOffset" : 231
    }, {
      "referenceID" : 20,
      "context" : "Following Wingate et al. [2011], for a probabilistic program P , we assume there is a name i ∈ I assigned to every possible random choice, for some countable I.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "Relaxations of this are left for future work; more general formalizations of probabilistic programs can be found in [Wingate et al., 2011; Mansinghka et al., 2014].",
      "startOffset" : 116,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "Relaxations of this are left for future work; more general formalizations of probabilistic programs can be found in [Wingate et al., 2011; Mansinghka et al., 2014].",
      "startOffset" : 116,
      "endOffset" : 163
    }, {
      "referenceID" : 16,
      "context" : "where Dχ2 denotes the chi-square divergence [Nielsen and Nock, 2014], and where pt(u|z;x) and qt(u;x, z) on the right-hand side represent density functions over u, not specific density values.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "where DKL denotes the Kullback-Leibler (KL) divergence [Kullback and Leibler, 1951].",
      "startOffset" : 55,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "Our use of unbiased likelihood estimates in place of the true likelihoods when computing the MetropolisHastings acceptance ratio in Algorithm 3 is closely related to pseudo-marginal MCMC [Andrieu and Roberts, 2009] and particle MCMC [Andrieu et al.",
      "startOffset" : 187,
      "endOffset" : 214
    }, {
      "referenceID" : 1,
      "context" : "Our use of unbiased likelihood estimates in place of the true likelihoods when computing the MetropolisHastings acceptance ratio in Algorithm 3 is closely related to pseudo-marginal MCMC [Andrieu and Roberts, 2009] and particle MCMC [Andrieu et al., 2010].",
      "startOffset" : 233,
      "endOffset" : 255
    }, {
      "referenceID" : 4,
      "context" : "Indeed, each single-site Nested Inference MH transition can be seen as a compositional variant of a ‘grouped independence MH’ transition [Beaumont, 2003] in which several pseudo-marginal likelihoods (one for each random choice j ∈ {i} ∪ cG(i)) are used in the same update.",
      "startOffset" : 137,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "The idea of learned inference for probabilistic generative models goes back at least to Morris [2001] and has also been used in Stuhlmüller et al.",
      "startOffset" : 88,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "The idea of learned inference for probabilistic generative models goes back at least to Morris [2001] and has also been used in Stuhlmüller et al. [2013] and Kingma and Welling [2013].",
      "startOffset" : 88,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "[2013] and Kingma and Welling [2013]. We apply this idea to nested inference as follows.",
      "startOffset" : 11,
      "endOffset" : 37
    } ],
    "year" : 2017,
    "abstractText" : "Intelligent systems sometimes need to infer the probable goals of people, cars, and robots, based on partial observations of their motion. This paper introduces a class of probabilistic programs for formulating and solving these problems. The formulation uses randomized path planning algorithms as the basis for probabilistic models of the process by which autonomous agents plan to achieve their goals. Because these path planning algorithms do not have tractable likelihood functions, new inference algorithms are needed. This paper proposes two Monte Carlo techniques for these “likelihood-free” models, one of which can use likelihood estimates from neural networks to accelerate inference. The paper demonstrates efficacy on three simple examples, each using under 50 lines of probabilistic code.",
    "creator" : "LaTeX with hyperref package"
  }
}