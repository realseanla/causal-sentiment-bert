{
  "name" : "1508.04522.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "arnabb@csa.iisc.ernet.in", "palash@csa.iisc.ernet.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 8.\n04 52\n2v 2\n[ cs\n.C C\n] 7\nS ep\n2 01\n5\nWe investigate the problem of winner determination from computational social choice theory in the data stream model. Specifically, we consider the task of summarizing an arbitrarily ordered stream of n votes on m candidates into a small space data structure so as to be able to obtain the winner determined by popular voting rules. As we show, finding the exact winner requires storing essentially all the votes. So, we focus on the problem of finding an ε-winner, a candidate who could win by a change of at most ε fraction of the votes. We show non-trivial upper and lower bounds on the space complexity of ε-winner determination for several voting rules, including k-approval, k-veto, scoring rules, approval, maximin, Bucklin, Copeland, and plurality with run off."
    }, {
      "heading" : "1 Introduction",
      "text" : "A common and natural way to aggregate preferences of agents is through an election. In a typical election, we have a set of m candidates and a set of n voters, and each voter reports his ranking of the candidates in the form of a vote. A voting rule selects one candidate as the winner once all voters provide their votes. Determining the winner of an election is one of the most fundamental problems in social choice theory.\nWe consider elections held in an online setting where voters vote in arbitrary order, and we would like to find the winner at any point in time. A very natural scenario where this occurs is an election conducted over the Internet. For instance, websites often ask for rankings of restaurants in a city and would like to keep track of the “best” restaurant according to some fixed voting rule. Traditionally, social choice theory addresses settings where the number of candidates is much smaller than the number of voters. However, we now often have situations where both the candidate set and voter set are very large. For example, the votes may be the result of high-frequency measurements made by sensors in a network [26], and a voting rule could be used to aggregate the measurements (as argued in [7]). Also, in online participatory democracy systems, such as [wid, syn], the number of candidates can be as large as the number of voters. The näıve way to conduct an online election is to store all the vote counts in a database and recompute the winner whenever it is needed. The space complexity of this approach becomes infeasible if the number of candidates or the number of votes is too large. Can we do better? Is it possible to compress the votes into a short summary that still allows for efficient recovery of the winner?\nThis question can be naturally formulated in the data stream model [3, 20]. Votes are interpreted as items in a data stream, and the goal is to devise an algorithm with minimum space requirement to determine the election winner. In the simplest setting of the plurality voting\nrule, where each vote is simply an approval for a single candidate and the winner is the one who is approved by the most, our problem is closely related to the classic problem of finding heavy hitters [9, 12] in a stream. For other popular voting rules, such as Borda, Bucklin or Condorcet consistent voting rules, the questions become somewhat different.\nRegardless of the voting rule, if the goal is to recover only the winner and the stream of votes is arbitrary, then it becomes essentially impossible to do anything better than the above-mentioned näıve solution (even when the algorithm is allowed to be randomized). Although we prove this formally, the reason should be intuitively clear: the winner may be winning by a very tiny margin thereby making every vote significant to the final outcome. We therefore consider a natural relaxation of the winner determination problem, where the algorithm is allowed to output any candidate who could have been the winner, according to the voting rule under consideration, by a change of at most εn votes. We call such a candidate an ε-winner; similar notions were introduced in [25, 36]. Note that if the winner wins by a margin of victory [36] of more than εn, there is a unique ε-winner. In this work, we study streaming algorithms to solve the (ε, δ)-WINNER DETERMINATION problem, i.e. the task of determining, with probability at least 1 − δ, an ε-winner of any given vote stream according to popular voting rules. Our algorithms are necessarily randomized."
    }, {
      "heading" : "1.1 Our Contributions",
      "text" : "We initiate the study of streaming algorithms for the (ε, δ)–WINNER DETERMINATION problem with respect to various voting rules. The results for the (ε, δ)–WINNER DETERMINATION problem, when both ε and δ are positive, are summarized in Table 1. (When ε or δ equals 0, we prove that the space requirements are much larger.)\nWe also exhibit algorithms, having space complexity nearly same as Table 1, for the more general sliding window model, introduced by Datar et. al. in [13]. In this setting, for some parameter N , we want to find an ε-winner with respect to the N most recent votes in the stream, clearly a very well motivated scenario in online elections."
    }, {
      "heading" : "1.2 Related Work",
      "text" : ""
    }, {
      "heading" : "1.2.1 Social Choice",
      "text" : "To the best of our knowledge, our work is the first to systematically study the approximate winner determination problem in the data stream model. A conceptually related work is that of Conitzer and Sandholm [10] who study the communication complexity of common voting rules. They consider n parties each of whom knows only their own vote but, through a communication protocol, would like to compute the winner according to a specific voting rule. Observe that a streaming algorithm for exact winner determination using s bits of memory space immediatelyI implies a one-way communication protocol where each party transmits s bits. However, it turns out that their results only imply weak lower bounds for the space complexity of streaming algorithms. Moreover, [10] does not study determination of ε-winners. The communication complexity of voting rules was also highlighted by Caragiannis and Procaccia in [7].\nIn a recent work, we [15] studied the problem of determining election winners from a random sample of the vote distribution. Since we can randomly sample from a stream of votes using a small amount of extra storage, the bounds from [15] are also useful in the streaming context.\nIEach party can input its vote into the stream and then communicate the memory contents of the streaming\nalgorithm to the next party.\nIn that work, the goal was to find the winner who was assumed to have a margin of victory [36] of at least ε, but the same arguments also work for finding ε-winners."
    }, {
      "heading" : "1.2.2 Streaming",
      "text" : "The field of streaming algorithms has been the subject of intense research over the past two decades in both the algorithms and database communities. The theoretical foundations for the area were laid by [3, 20]. A stream is a sequence of data items σ1, σ2, . . . , σn, drawn from the universe [m], such that on each pass through the stream, the items are read once in that order. The frequency vector associated with the stream f = (f1,⋯, fm) ∈ Zm is defined as fj being the number of times j occurs as an item in the stream. In this definition, the stream is insertiononly; more generally, in the turnstile model, items can both be inserted and deleted from the stream, in which case the frequency vector maintains the cumulative count of each element in [m]. General surveys of the area can be found in [32, 33]. Algorithms for the insertion-only case were discovered before the formulation of the data streaming model. Consider the point-query problem: for a stream of n items from a universe of size m and a parameter ε > 0, the goal is to output, for any item j ∈ [m], an estimate f̂j such that ∣f̂j − fj ∣ ≤ εn. Misra and Gries [30] gaveII an elegant but simple deterministic algorithm requiring only O(min{m,1/ε} ⋅(logm+ logn)) space in bit complexity. Since to find an ε-winner\nIIThe algorithm can be viewed as a generalization of the Boyer-Moore [5, 17] algorithm for ε = 1/2. It was also rediscovered 20 years later by [14, 22].\nfor the plurality voting rule, it’s enough to solve the point query problem and output the j with maximum f̂j, Misra-Gries automatically implies O(min{m,1/ε} ⋅(logm+ logn)) space complexity for plurality. We use sampling to improve the dependence on n and prove tightness in terms of ε and n. Our algorithms for many of the other voting rules are also based on the Misra-Gries algorithm. We note that in place of Misra-Gries, there are several other deterministic algorithms which could have been used, such as Lossy Counting [27] and Space Saving [28], but they would not change the asymptotic space complexity bounds. A thorough overview of the point query, or frequency estimation, problem can be found in [11].\nFor the more general turnstile model, the point query problem for such streams is that of finding f̂j, for every j, such that ∣f̂j − fj ∣ ≤ ε∥f∥1. The best result for this problem is due to Cormode and Muthukrishnan, the randomized count-min sketch [12], which has space complexity O(1 ε logm logn) in bits. The space bound was proved to be essentially tight by Jowhari et al. in [21]. In our context, the stream is a sequence of votes; so, our problems are mostly, just by definition, insertion-only. However, the count-min sketch becomes useful in our applications (i) if voters can issue retractions of their votes, and (ii) to maintain counts of random samples drawn from streams of unknown length."
    }, {
      "heading" : "1.3 Technical Overview",
      "text" : "Upper Bounds. The streaming algorithms that achieve the upper bounds shown in Table 1 are obtained through applying frequency estimation algorithms, such as Misra-Gries or countmin sketch, appropriately on a subsampled stream. The number of samples needed to obain ε-winners for the various voting rules was previously analyzed in [15].\nLower Bounds. Our main technical novelty is in the proofs of the lower bounds for the (ε, δ)winner determination problem. Usually, in the “heavy hitters” problem in the algorithms literature, the task is roughly to determine the set of items with frequency above εn. Since there can be 1/ε such items, a space lower bound of log (m 1/ε) = Ω(1ε log(εm)) immediately follows for m≫ 1/ε. In contrast, we wish to determine only one ε-winner, so that just logm bits are needed to output the result. In order to obtain stronger lower bounds that depend on ε, we need to resort to other techniques. Moreover, note that our lower bounds are in the insertion-only stream model, whereas previous lower bounds for frequency estimation problems are usually for the more general turnstile model.\nWe prove these bounds through new reductions from fundamental problems in communication complexity. To give a flavor of the reductions, let us sketch the proof for the plurality voting rule. Consider each additive term separately in the lower bound.\n• log logn: Suppose Alice has a number 1 ≤ a ≤ n and Bob a number 1 ≤ b ≤ n, and Bob wishes\nto know whether a > b through a protocol where communication is one way from Alice to Bob. It is known [29, 34] that Alice is required to send Ω(logn) bits to Bob. We can reduce this problem to finding a 1/3-winner in a plurality election among two candidates by having Alice push 2a approvals for candidate 1 into the stream and Bob pushing 2b approvals for\ncandidate 2; the Ω(log logn) lower bound follows. • (1/ε) log(1/ε) when m ≥ 1/ε: Consider the INDEXING problem over an arbitrary alphabet:\nAlice has a vector x ∈ [t]m and Bob an index i ∈ [m], and Bob wants to find xi through a oneway protocol from Alice to Bob. Ergün et al [16], extending [29]’s proof for the case of t = 2,\nshow Alice needs to send Ω(m log t) bits. For t = m = 1/√ε, we reduce INDEXING to ε-winner determination for a plurality election. Let the candidate set be [t] × [m]. Alice (given her input x) pushes n/2 votes into the stream with √εn/2 votes to each (xj , j) for all j ∈ [m] and\nsends over the memory content of the streaming algorithm to Bob who (given his input i) pushes another n/2 votes into the stream with √εn/2 votes to each (a, i) for all a ∈ [t]. Note that candidate (xi, i) is the unique √ε/4-winner of this plurality election! Using [16]’s lower bound Ω(1/√ε log(1/ε)) on the communication complexity of the INDEXING problem yields our result. • m log(1/ε)whenm ≤ 1/ε: Suppose Alice has a vector a ∈ [t]m and Bob a vector b ∈ [t]m, and Bob wants to findIII i = argmaxj(aj + bj) through a one-way protocol. We show by reducing from the AUGMENTED INDEXING problem [16, 29] that Alice needs to send Ω(m log t) bits to Bob. Suppose t = 1/ε. Alice imagines her vector a as being the vote count for a plurality election amongm candidates, streams in a and runs the streaming algorithm for the problem,\nand passes the memory output to Bob who also streams in his vector b. The maximum entry in a + b corresponds to a candidate winning by margin at least ε2n, hence yielding the Ω(m log(1/ε)) lower bound."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Voting and Voting Rules",
      "text" : "Let V = {v1, . . . , vn} be the set of all voters and C = {c1, . . . , cm} the set of all candidates. If not mentioned otherwise, V, C, n and m denote set of voters, the set of candidates, the number of voters and the number of candidates respectively. Each voter vi’s vote is a complete order ≻i over the candidate set C. For example, for two candidates a and b, a ≻i b means that the voter vi prefers a to b. We denote the set of all complete orders over C by L(C). Hence, L(C)n denotes the set of all n-voters’ preference profiles (≻1, . . . ,≻n). A map r ∶ ⊎n,∣C∣∈N+L(C)n Ð→ 2C is called a voting rule. Given a vote profile ≻∈ L(C)n, we call the candidates in r(≻) the winners. Given an election E = (V,C), we can construct a weighted graph GE , called weighted majority graph, from E . The set of vertices in GE is the set of candidates in E . For any two candidates x and y, the weight on the edge (x, y) is DE(x, y) = NE(x, y) −NE(y,x), where NE(x, y) (respectively NE(y,x)) is the number of voters who prefer x to y (respectively y to x). A candidate x is called the Condorcet winner in an election E if DE(x, y) > 0 for every other candidate y ≠ x. A voting rule is called Condorcet consistent if it selects the Condorcet winner as the winner of the election whenever it exists. Some examples of common voting rules are: • Positional scoring rules: A collection of m-dimensional vectors s⃗m = (α1, α2, . . . , αm) ∈ Rm with α1 ≥ α2 ≥ ⋅ ⋅ ⋅ ≥ αm and α1 > αm for every m ∈ N naturally defines a voting rule – a\ncandidate gets score αi from a vote if it is placed at the i th position. The score of a candidate is the sum of the scores it receives from all the votes. The winners are the candidate with maximum score. The vector α that is 1 in the first k coordinates and 0 in other coordinates gives the k-approval voting rule. The vector α that is 1 in the last k coordinates and 0 in other coordinates is called k-veto voting rule. Observe that the score of a candidate in the k-approval (respectively k-veto) voting rule is the number of approvals (and respectively vetoes) that the candidate receives. 1-approval is called the plurality voting rule, and 1-veto\nis called the veto voting rule. The score vector (m − 1,m − 2, . . . ,1,0) gives the Borda rule. • Generalized plurality: In generalized plurality voting, each voter approves or disapprove\none candidate. The score of a candidate is the number of approvals it receives minus number of disapprovals it receives. The candidates with highest score are the winners. We introduce this rule and consider it to be interesting particularly in an online setting where every voter\nIIIAssume the maximum is unique.\neither likes or dislikes an item; hence each vote is either an approval for a candidate or a disapproval for a candidate.\n• Approval: In approval voting, each voter approves a subset of candidates. The winners are\nthe candidates which are approved by the maximum number of voters.\n• Maximin: The maximin score of a candidate x is miny≠xDE(x, y). The winners are the candidates with maximum maximin score. • Copeland: The Copeland score of a candidate x is ∣{y ≠ x ∶ DE(x, y) > 0}∣. The winners are the candidates with maximum Copeland score.\n• Bucklin: A candidate x’s Bucklin score is the minimum number ℓ such that more than half of\nthe voters rank x in their first ℓ positions. The winners are the candidates with lowest Bucklin score.\n• Plurality with runoff: The top two candidates according to plurality score are selected first.\nThe pairwise winner of these two candidates is selected as the winner of the election. This rule is often called the runoff voting rule.\nAmong the above, only the maximin and Copeland rules are Condorcet consistent."
    }, {
      "heading" : "2.2 Model of Input Data",
      "text" : "In the basic model, the input data is an insertion only stream of elements from some universe U . We note that, in the context of voting in an online scenario, the natural model of input data is the insertion only streaming model over the universe of all possible votes L(C). The basic model can be generalized to the more sophisticated sliding window model where the only active items are the last n items, for some parameter n. In this work, we focus on winner determination algorithms for insertion only stream of votes in both basic and sliding window models. The basic input model can also be generalized to another input model, called turnstile model, where the input data is a sequence from U × {1,−1}; every element in the stream corresponds to either a unit increment or a unit decrement of frequency of some element from U . We will use the turnstile streaming model (over some different universe) only to design efficient winner determination algorithms for the insertion only stream of votes. We note that, the algorithms for the streaming data can make only one pass over the input data. These one pass algorithms are also called streaming algorithms."
    }, {
      "heading" : "2.3 Communication Complexity",
      "text" : "We will use lower bounds on communication complexity of certain functions to prove space complexity lower bounds for our problems. Communication complexity of a function measures the number of bits that need to be exchanged between two players to compute a function whose input is split among those two players [37]. In a more restrictive one-way communication model, the first player sends only one message to the second player and the second player outputs the result. A protocol is a method that the players follow to compute certain functions of their input. Also the protocols can be randomized; in that case, the protocol needs to output correctly with probability at least 1− δ, for some parameter δ ∈ [0,1] (the probability is taken over the random coin tosses of the protocol). The randomized one-way communication complexity of a function f with error δ is denoted by R 1−way δ\n(f). Classically the first player is named Alice and the second player is named Bob and we also follow the same convention here. [24] is a standard reference for communication complexity."
    }, {
      "heading" : "2.4 Chernoff Bound",
      "text" : "We will use the following concentration inequality:\nTheorem 1. Let X1, . . . ,Xℓ be a sequence of ℓ independent random variables in [0,1] (not necessarily identical). Let S = ∑iXi and let µ = E [S]. Then, for any 0 ≤ δ ≤ 1:\nPr[∣S − µ∣ ≥ δℓ] < 2exp(−2ℓδ2) and Pr[∣S − µ∣ ≥ δµ] < 2exp(−δ2µ/3) The first inequality is called an additive bound and the second multiplicative."
    }, {
      "heading" : "2.5 Problem Definition",
      "text" : "The basic winner determination problem is defined as follows.\nDefinition 1. (WINNER DETERMINATION) Given a voting profile ≻ over a set of candidates C and a voting rule r, determine the winners r(≻). We show a strong space complexity lower bound for the WINNER DETERMINATION problem for the plurality voting rule in Theorem 12. To overcome this theoretical bottleneck, we focus on determining approximate winner of an election. Below we define the notion of ε-approximate winner which we also call ε-winner.\nDefinition 2. (ε-WINNER) Given an n-voter voting profile ≻ over a set of candidates C and a voting rule r, a candidate w is called an ε–winner if w can be made winner by changing at most εn votes in ≻.\nNotice that there always exist an ε-winner in every election since a winner is also an ε-winner. We show that finding even an ε-winner deterministically requires large space when the number of votes is large [see Theorem 14]. However, we design space efficient randomized algorithms which outputs an ε-winner of an election with probability at least 1 − δ. The problem that we study here is called (ε, δ)-WINNER DETERMINATION problem and is defined as follows. Definition 3. ((ε, δ)-WINNER DETERMINATION) Given a voting profile ≻ over a set of candidates C and a voting rule r, determine an ε–winner with probability at least 1 − δ. (The probability is taken over the internal coin tosses of the algorithm.)"
    }, {
      "heading" : "3 Upper Bounds",
      "text" : "In this section, we present the algorithms for the (ε, δ)-Winner Determination problem for various voting rules. Before embarking on specific algorithms, we first prove a few supporting results that will be used crucially in our algorithms later. We begin with the following space efficient algorithm for picking an item uniformly at random from a universe of size n below.\nObservation 1. There is an algorithm for choosing an item with probability 1 n that uses O(log logn) bits of memory and uses fair coin as its only source of randomness.\nProof. First let us assume, for simplicity, that n is a power of 2. We toss a fair coin log2 n many times and choose the item, say x, only if the coin comes head all the times. Hence the probability that the item x gets chosen is 1 n . We need O(log logn) space to toss the fair coin log2 n times (to keep track of the number of times we have tossed the coin so far). If n is not a power of 2 then, toss the fair coin ⌈log2 n⌉ many times and we choose the item x only if the coin comes head in all the tosses conditioned on some event E. The event E contains exactly n outcomes including the all heads outcome.\nWe remark that Observation 1 is tight in terms of space complexity. We state the claim formally below, as it may be interesting in its own right.\nProposition 1. Any algorithm that chooses an item from a set of size n with probability p, for 0 < p ≤ 1 n , using a fair coin as its only source of randomness, must use Ω(log logn) bits of memory.\nProof. The algorithm tosses the fair coin some number of times (the number of times it tosses the coin may also depend on the outcome of the previous tosses) and finally picks an item from the set. Consider a run R of the algorithm where it chooses the item, say x, with smallest number of coin tosses; say it tosses the coin t many times in this run R. This means that in any other run of the algorithm where the item x is chosen, the algorithm must toss the coin at least t number of times. Let the outcome of the coin tosses in R be r1,⋯, rt. Let si be the memory content of the algorithm immediately after it tosses the coin ith time, for i ∈ [t], in the run R. First notice that if t < log2 n, then the probability with which the item x is chosen is more than 1 n , which would be a contradiction. Hence, t ≥ log2 n. Now we claim that all the si’s must be different. Indeed otherwise, let us assume si = sj for some i < j. Then the algorithm chooses the item x after tossing the coin t − (j − i) (which is strictly less than t) many times when the outcome of the coin tosses are r1,⋯, ri, rj+1,⋯, rt. This contradicts the assumption that the run R we started with chooses the item x with smallest number of coin tosses.\nAn essential ingredient in our algorithms is calculating the approximate frequencies of all the elements in a universe in an input data stream. The following result (due to [30]) provides a space efficient algorithm for that job.\nTheorem 2. Given an insertion only stream of length n over a universe of size m, there is a deterministic one pass algorithm to find the frequencies of all the items in the stream within an additive approximation of εn using O (min{1 ε (logm + logn) ,m log n}) bits of memory, for every ε > 0.\nProof. The O (1 ε (logm + logn)) space algorithm is due to [30]. On the other hand, notice that with spaceO (m logn), we can exactly count the frequency of every element, even in the turnstile model of stream, by simply keeping an array of length m (indexed by ids of the elements from the universe) each entry of which is capable of storing integers up to n.\nWe now describe streaming algorithms for the (ε, δ)–WINNER DETERMINATION problem for various voting rules. The general idea is to sample certain number of votes uniformly at random from the stream of votes using the algorithm of Observation 1 and generate another stream of elements over some different universe. The number of votes sampled and the universe of the stream generated depend on the specific voting rule we are considering. After that, we approximately calculate the frequencies of the elements in the generated stream using Theorem 2. For simplicity, we assume that the number of votes in known in advance up to some constant factor (only to be able to apply Observation 1). We will see in Section 3.1 how to get rid of this assumption, without affecting space complexity of any of the algorithms much. We begin with the k-approval and k-veto voting rules below.\nTheorem 3. Assume that the number of votes is known to be within [c1n, c2n] for some constants c1 and c2 in advance. Then there is a one pass algorithm for the (ε, δ)–WINNER DETERMINATION problem for the k-approval voting rule that uses O (min{k\nε (logm + log 1 ε + log log 1 δ ) ,m(log log(k+1) ε + log log 1 δ )} + log logn) bits of memory and\nfor the k-veto voting rule that uses O(min{k ε (logm + log 1 ε + log log 1 δ ) ,m( log log(m−k+1) ε + log log 1 δ )} + log logn) bits of memory.\nProof. Let us first consider the case of the k-approval voting rule. We pick the current vote in the stream with probability p (the value of p will be decided later) independent of other votes. Suppose we sample ℓ many votes; let S = {vi ∶ i ∈ [ℓ]} be the set of votes sampled. From the set of sampled votes S, we generate a stream T over the universe C as follows. For i ∈ [ℓ], let the vote vi be c1 ≻ c2 ≻ ⋯ ≻ cm. From the vote vi, we add k candidates c1,⋯, ck in the stream T . We know that there is a ℓ = O( log(k+1) ε2 log 1 δ ) (and thus a corresponding p = Ω( 1 n )) which ensures that for every candidate x ∈ C, ∣s(x) n − ŝ(x) ℓ ∣ < ε 3 with probability at least 1 − δ 2 [15], where s(⋅) and ŝ(⋅) are the scores of the candidates in the input stream of votes and in S respectively. Now we count ŝ(x) for every candidate x ∈ C within an additive approximation of εℓ\n3 and the result\nfollows from Theorem 2 (notice that the length of the stream T is kℓ).\nFor the k-veto voting rule, we approximately calculate the number of vetoes that every candidate gets using the same technique as above. However, for the k-veto voting rule, the corresponding bound for ℓ is O( log(m−k+1) ε2 log 1 δ ) which implies the result.\nBy similar techniques, we have the following algorithm for the generalized plurality rule.\nTheorem 4. Assume that the number of votes is known to be within [c1n, c2n] for any constants c1 and c2 in advance. Then there is a one pass algorithm for the (ε, δ)–WINNER DETERMINATION problem for the generalized plurality voting rule that uses O (1\nε (logm + log 1 ε + log log 1 δ ) + log logn)\nbits of memory.\nProof. We sample ℓ = O( 1 ε2 log 1 δ ) many votes uniformly at random from the input stream of votes using the technique used in the proof of Theorem 3. For every candidate, we count both the number of approvals and disapprovals that it gets within an additive approximation of εℓ 10 which is enough to get an ε-winner. Now the space complexity follows form Theorem 2.\nWe generalize Theorem 3 to the class of scoring rules next. We need the following result in the subsequent proof which is due to [15].\nLemma 1. Let α = (α1,⋯, αm) be an arbitrary score vector and w the winner of an α–election E . Let x be any candidate which is not a ε–winner. Then, s(w) − s(x) ≥ α1εn. With Lemma 1 at hand, we now present the algorithm for the scoring rules.\nTheorem 5. Assume that the number of votes is known to be within [c1n, c2n] for any constants c1 and c2 in advance. Let α = (α1,⋯, αm) be a score vector such that αi ≥ 0 for every i ∈ [m]. Then there is a one pass algorithm for the (ε, δ)–WINNER DETERMINATION problem for the α-scoring rule that uses O (∑mi=1 αi\nα1 (log logm + log 1 ε + log log 1 δ ) + log logn), which is\nO (m (log logm + log 1 ε + log log 1 δ ) + log logn), bits of memory. Proof. Let α = (α1,⋯, αm) be an arbitrary score vector with αi ≥ 0 for every i ∈ [m]. We define α′i =\nαi ∑mi=j αj (which is in [0,1]), for every i ∈ [m]. Since scoring rules remain same even if we multiply every αi with any positive constant λ, the score vectors α and α ′ correspond to same\nvoting rule. We pick the current vote in the stream with probability p (the value of p will be decided later) independent of other votes. Suppose we sample ℓmany votes; let S = {vi ∶ i ∈ [ℓ]} be the set of votes sampled. For i ∈ [ℓ], let the vote vi be c1 ≻ c2 ≻ ⋯ ≻ cm. We pick the candidate ci from the vote vi with probability α ′ i and define it to be ai. We compute the frequencies of the candidates in the stream S̄ = {ai ∶ i ∈ [ℓ]} within an additive factor of ε′n, where ε′ = ε3 . For every candidate x ∈ C, let s(x) be the α′–score of the candidate x in the input stream of votes and ŝ(x) be n\nℓ times the α′–score of the candidate x in the sampled votes S. We know that there\nexists an ℓ = O( 1 ε2 log m δ ) (and thus a corresponding p = Ω( 1 n )) which ensures that, for every candidate x ∈ C, ∣s(x) − ŝ(x)∣ < α′1ε′n with probability at least 1 − δ2 [15]. Let s̄(x) be nℓ times the frequency of the candidate x ∈ C in the stream S̄. We now prove the following claim from which the result follows immediately.\nClaim 1.\nPr[∀x ∈ C, ∣s̄(x) − ŝ(x)∣ ≤ α′1ε′n] ≥ 1 − δ 2\nProof. For every candidate x ∈ C and every i ∈ [ℓ], we define a random variable Xi(x) to be 1 if ai = x and 0 otherwise. Then, s̄(x) = nℓ ∑i∈[ℓ]Xi(x). We have, E [s̄(x)] = ŝ(x). Now using Chernoff bound from Theorem 1, we have the following:\nPr[∣s̄(x) − ŝ(x)∣ > α′1ε′n] = Pr[∣nℓ ∑ i∈[ℓ] Xi(x) − ŝ(x)∣ > α′1ε′n]\n= Pr[∣ ∑ i∈[ℓ] Xi(x) α′1 − ℓŝ(x) α′1n ∣ > ε′ℓ] ≤ 2exp{−ε2α′1nℓ 3ŝ(x) } ≤ 2exp{−ε2ℓ 3\n} The fourth inequality follows from the fact that ŝ(x) ≤ α′1n for every candidate x ∈ C. Now we use the union bound to get the following.\nPr[∀x ∈ C, ∣s̄(x) − ŝ(x)∣ ≤ α′1ε′n] ≥ 1 − ∑ x∈C 2exp{−ε2ℓ 3 } ≥ 1 − δ 2\nThe second inequality follows from an appropriate choice of ℓ = O( 1 ε2 log m δ ).\nWe estimate the frequency of every candidate in S̄ within an additive approximation ratio of α′1εℓ and output the candidate w with maximum estimated frequency as the winner of the election. The candidate w is an ε– winner (follows from Lemma 1) with probability at least 1−δ (follows from Claim 1). The space complexity of this algorithm follows from Theorem 2 (since 1 α′ 1 = ∑mi=1 αi α1 ≤ mα1 α1 =m) and Observation 1.\nWe present next the streaming algorithm for the approval voting rule. It is again obtained by running a frequency estimation algorithm on samples from a stream.\nTheorem 6. Assume that the number of votes is known to be within [c1n, c2n] in advance, for some constants c1 and c2. Then there is a one pass algorithm for the (ε, δ)–WINNER DETERMINATION problem for the approval voting rule that uses O (m (log logm + log 1\nε + log log 1 δ ) + log logn) bits\nof memory.\nProof. We sample ℓ many votes using the algorithm described in Observation 1 and technique described in the proof of Theorem 5. The total number of approvals in those sampled votes is at most mℓ and we estimate the number of approvals that every candidate receives within an additive approximation of εℓ 2 . The result now follows from the upper bound on ℓ [15] and Theorem 2.\nNow we move on to maximin, Copeland, Bucklin, and plurality with run off voting rules. We provide two algorithms for these voting rules, which trade off between the number of candidates m and the approximation factor ε. The algorithm in Theorem 7 below, which has better space complexity when 1 ε is small compared to m, simply stores all the sampled votes. Theorem 7. Assume that the number of votes is known to be within [c1n, c2n] in advance, for some constants c1 and c2. Then there is a one pass algorithm for the (ε, δ)–WINNER DETERMINATION problem for the maximin, Bucklin, and plurality with run off voting rules that use O (m log2 m log 1δ ε2\n+ log logn) bits of memory and for the Copeland voting rule that uses O (m log4 m log 1δ\nε2 + log logn) bits of memory.\nProof. We sample ℓ many votes from the input stream of votes uniformly at random and simply store all of them. Notice that we can store a vote using space O(m logm). The result now follows from the upper bound on ℓ [15] and Observation 1.\nNext we consider the case when 1 ε is large compared to m. Theorem 8. Assume that the number of votes is known to be within [c1n, c2n] in advance, for some constants c1 and c2. Then there is a one pass algorithm for the (ε, δ)–WINNER DETERMINATION problem for the maximin, Copeland, Bucklin, and plurality with runoff voting rules that uses O (m2 (log logm + log 1 ε + log log 1 δ ) + log logn) bits of memory. Proof. For each voting rule mentioned in the statement, we sample ℓmany votes S = {vi ∶ i ∈ [ℓ]} uniformly at random from the input stream of votes using the algorithm used in Observation 1 and the technique used in the proof of Theorem 5. From S, we generate another stream S̄ of elements belonging to a different universe U (which depends on the voting rule under consideration). Finally, we calculate the frequencies of the elements of S̄, using Theorem 2, within an additive approximation of εℓ 2 for maximin, Bucklin, and plurality with runoff voting rules and εℓ 2 logm for the Copeland voting rule. The difference of approximation factor is due to [15]. We know that ℓ = O ( log mδ ε2\n) for maximin, Bucklin, and plurality with run off voting rules and ℓ = O ( log3 mδ\nε2 ) for the Copeland voting rule [15]. This bounds on ℓ prove the result once we\ndescribe S̄ and U . Below, we describe the stream S̄ and the universe U for individual voting rules. Let the vote vi be c1 ≻ c2 ≻ ⋯ ≻ cm.\n• maximin, Copeland: U = C × C. From the vote vi, we put (cj , ck) in S̄ for every j < k. • Bucklin: U = C × [m]. From the vote vi, we put (cj , k) in S̄ for every j ≤ k. • plurality with runoff: U = C × C. From the vote vi, we put (cj , ck) in S̄ for every j < k and(c1, c1). In the plurality with runoff voting rule, we need to estimate the plurality score of\nevery candidate which we do by estimating the frequencies of the elements of the (x,x) in S̄. We also need to estimate DE(x, y) for every candidate x, y ∈ C which we do by estimating the frequencies of the elements of the form (x, y)."
    }, {
      "heading" : "3.1 Unknown stream length",
      "text" : "Now we consider the case when the number of voters is not known beforehand. The idea is to use reservoir sampling ([35]) along with approximate counting ([18, 31]) to pick an element from the stream almost uniformly at random. The following result shows that we can do so in a space efficient manner.\nTheorem 9. (Theorem 7 of [19]) Given an insertion only stream of length n (n is not known to the algorithm beforehand) over a universe of sizem, there is a randomized one pass algorithm that outputs, with probability at least 1 − δ, the element at a random position X ∈ [n] such that, for every i ∈ [n], ∣Pr{X = i}− 1\nn ∣ ≤ ε n using O(log 1 δ + log 1 ε + log logn+ logm) bits of memory, for every\nε ∈ (0,1] and δ > 0. Recall that Theorem 2 only works for insertion only streams. However, as the stream progresses, the element chosen by Theorem 9 changes; so, we cannot invoke Misra-Gries to do frequency estimation on a set of samples given by Theorem 9. For streams with both insertions and deletions, we have the following result which is due to count-min sketch [12].\nTheorem 10. Given a turnstile stream of length n over a universe of size m, there is a randomized one pass algorithm to find the frequencies of the items in the stream within an additive approximation of εn with probability at least 1 − δ using O ( logm ε log(1 δ ) (logm + logn)) bits of memory, for every ε > 0 and δ > 0.\nFrom Theorem 9 and 10 and from the proofs of Theorem 2, 4 to 6 and 8, we get the following.\nCorollary 1. Assume that the number of votes n is not known beforehand. Then there is a one pass algorithm for the (ε, δ)–WINNER DETERMINATION problem for k-approval, k-veto, generalized plurality, approval, maximin, Copeland, Bucklin, and plurality with run off voting rules that uses logm log 1 δ times more space than the corresponding algorithms when n is known beforehand upto a constant factor.\nProof. We use reservoir sampling with approximate counting from Theorem 9. The resulting stream that we generate have both positive and negative updates (since in reservoir sampling, we sometimes replace an item we previously sampled). Now we approximately estimate the frequency of every item in the generated stream using Theorem 10.\nAgain from Theorem 7 and 9, we get the following result which provides a better space upper bound than Corollary 1 when the number of candidates m is large.\nCorollary 2. Assume that the number of votes n is not known beforehand. Then there is a one pass algorithm for the (ε, δ)–WINNER DETERMINATION problem for the maximin, Bucklin, and plurality with run off voting rules that use O (m log2 m log 1δ\nε2 + log logn) bits of memory and for the\nCopeland voting rule that uses O (m log4 m log 1δ ε2 + log logn) bits of memory."
    }, {
      "heading" : "3.2 Sliding Window Model",
      "text" : "Suppose we want to compute an ε-winner of the last n many votes in an infinite stream of votes for various voting rules. The following result shows that there is an algorithm, with space complexity same as Theorem 9, to sample a vote from the last n votes in a stream.\nTheorem 11. ([6]) Given an insertion only stream over a universe of sizem, there is a randomized one pass algorithm that outputs, with probability at least 1 − δ, the element at a random position X from last n positions such that, for every i ∈ [n], ∣Pr{X = i} − 1 n ∣ ≤ ε n using O(log 1 δ + log 1 ε + log logn + logm) bits of memory, for every ε ∈ (0,1] and δ > 0. Theorem 11 immediately provides results same as Corollary 1 and 2, where n is the window size."
    }, {
      "heading" : "4 Lower Bounds",
      "text" : "In this section, we prove space complexity lower bounds for the (ε, δ)–WINNER DETERMINATION problem for various voting rules. We reduce certain communication problems to the (ε, δ)–WINNER DETERMINATION problem for proving space complexity lower bounds. Let us first introduce those communication problems with necessary results."
    }, {
      "heading" : "4.1 Communication Complexity",
      "text" : "Definition 4. (AUGMENTED-INDEXINGm,t) Let t and m be positive integers. Alice is given a string x = (x1,⋯, xt) ∈ [m]t. Bob is given an integer i ∈ [t] and (x1,⋯, xi−1). Bob has to output xi. The following communication complexity lower bound result is due to [16] by a simple extension of the arguments of Bar-Yossef et al [4].\nLemma 2. R 1−way δ (AUGMENTED-INDEXINGm,t) = Ω((1 − δ)t logm) for any δ < 1 − 32m . Also, we recall the multi-party version of the set-disjointness problem.\nDefinition 5. (DISJ promise m,t ) We have t sets X1,⋯,Xt each a subset of [m]. We have t players and player i is holding the set Xi. We are also given the promise that either Xi ∩Xj = ∅ for every i ≠ j or there exist an element y ∈ [m] such that y ∈ Xi for every i ∈ [t] and (Xi∖{y})∩(Xj ∖{y}) = ∅ for every i ≠ j. The output DISJ\npromise m,t (X1,⋯,Xt) is 1 if Xi ∩Xj = ∅ for every i ≠ j and 0 else.\nLemma 3 (Proved in [4, 8].). R 1−way δ (DISJpromisem,t ) = Ω(mt ), for any δ ∈ [0,1) and t. The following communication problem is very useful for us.\nDefinition 6. (MAX-SUMm,t) Alice is given a string x = (x1, x2,⋯, xt) ∈ [m]t of length t over universe [m]. Bob is given another string y = (y1, y2,⋯, yt) ∈ [m]t of length t over the same universe [m]. The strings x and y is such that the index i that maximizes xi + yi is unique. Bob has to output the index i ∈ [t] which satisfies xi + yi =maxj∈[t]{xj + yj}. We establish the following one way communication complexity lower bound for the MAX-SUMm,t problem by reducing it from the AUGMENTED-INDEXING2,t log m problem.\nLemma 4. R 1−way δ (MAX-SUMm,t) = Ω(t logm), for every δ < 14 .\nProof. We reduce the AUGMENTED-INDEXING2,t log m problem to MAX-SUM8m,t+1 problem thereby proving the result. Let the inputs to Alice and Bob in the AUGMENTED-INDEXING2,t log m instance be (a1, a2,⋯, at logm) ∈ {0,1}t logm and (a1,⋯, ai−1) respectively. The idea is to construct a corresponding instance of the MAX-SUM8m,t+1 problem that outputs t + 1 if and only if ai = 0. We achieve this as follows. Alice starts execution of the MAX-SUM8m,t+1 protocol using the vector x = (x1, x2,⋯, xt+1) ∈ [8m]t+1 which is defined as follows: the binary representation of xj is (0,0, a(j−1) logm+1, a(j−1) logm+2, a(j−1) logm+3,⋯, aj logm,0)2, for every j ∈ [t], and xt+1 is 0. Bob participates in the MAX-SUM8m,t+1 protocol with the vector y = (y1, y2,⋯, yt+1) ∈ [8m]t+1 which is defined as follows. Let us define λ = ⌈ i logm\n⌉. We define yj = 0, for every j ∉ {λ, t + 1}. The binary representation of yλ is (1,0, a(λ−1) logm+1, a(λ−1) logm+2,⋯, ai−1,1,0,0,⋯,0, 0, 1)2 . Let us define an integer T whose binary representation is (0,0, a(λ−1) logm+1, a(λ−1) logm+2,⋯, ai−1,0,1,1,⋯,1)2 . We define yt+1 to be T + yλ. First notice that the output of the MAX-SUM8m,t+1 instance is either λ or t + 1, by the construction of y. Now observe that if ai = 1 then, xλ > T and thus the output of the MAXSUM8m,t+1 instance should be λ. On the other hand, if ai = 0 then, xλ < T and thus the output of the MAX-SUM8m,t+1 instance should be t + 1.\nFinally, we also consider the GREATER-THAN problem.\nDefinition 7. (GREATER-THANn) Alice is given an integer x ∈ [n] and Bob is given an integer y ∈ [n], y ≠ x. Bob has to output 1 if x > y and 0 otherwise.\nThe following result is due to [29, 34]. We provide a simple proof of it that seems to be missingIV in the literature.\nLemma 5. R 1−way δ (GREATER-THANn) = Ω(logn), for every δ < 14 . Proof. We reduce the AUGMENTED-INDEXING2,⌈log n⌉+1 problem to the GREATER-THANn problem thereby proving the result. Alice runs the GREATER-THANn protocol with its input number whose representation in binary is a = (x1x2⋯x⌈logn⌉1)2. Bob participates in the GREATER-THANn protocol with its input number whose representation in binary is b = (x1x2⋯xi−11 0⋯0±\n(⌈logn⌉−i+1) 0′s )2. Now xi = 1 if and only if a > b."
    }, {
      "heading" : "4.2 Reductions",
      "text" : ""
    }, {
      "heading" : "4.2.1 The cases ε = 0 and δ = 0",
      "text" : "We begin with the problem where we have to find the winner (i.e., 0-winner) for a plurality election. Notice that, we can find the winner by exactly computing the plurality score of every candidate. This requiresO(m log n) bits of memory. We prove below that, when n is much larger than m, this space complexity is almost optimal even if we are allowed to use randomization, by reducing it from the MAX-SUMn,m problem. This strengthens a similar result proved in Karp et al. [22] only for deterministic algorithms.\nTheorem 12. Any one pass (0, δ)–WINNER DETERMINATION algorithm for the plurality and generalized plurality election must use Ω(m log(n/m)) bits of memory, for any δ ∈ [0, 1\n4 ).\nIVA similar proof appears in [23] but theirs gives a weaker lower bound.\nProof. We prove the result for (0, δ)–WINNER DETERMINATION problem for the plurality election. This gives the result for the generalized plurality election since every plurality election is also a generalized plurality election. Consider the MAX-SUMn,m problem where Alice is given a string x = (x1,⋯, xm) ∈ [n]m and Bob is given another string y = (y1,⋯, ym) ∈ [n]m. The candidate set of our election is [m]. The votes would be such that the only winner will be the candidate i such that i ∈ argmaxj∈[m]{xj + yj}. Moreover, the winner would be known to Bob, thereby proving the result. Thus Bob can output xi correctly whenever our (0, δ)–WINNER DETERMINATION algorithm outputs correctly. Alice generates xj many plurality votes for the candidate j, for every j ∈ [m]. Alice now sends the memory content to Bob. Bob resumes the run of the algorithm by generating yj many plurality votes for the candidate j, for every j ∈ [m]. The plurality score of candidate j is (xj + yj) and thus the plurality winner will be a candidate i such that i ∈ argmaxj∈[m]{xj + yj}. Notice that the total number of votes is at most 2mn. The result now follows from Lemma 4.\nFor the case when m and n are comparable, the following result is stronger. We prove this by exhibiting a reduction from the DISJ promise m,3 problem. Theorem 13. Any one pass (0, δ)–WINNER DETERMINATION algorithm for the plurality and generalized plurality election must use Ω(min{m,n}) bits of memory, for any δ ∈ [0,1). Proof. Suppose we have a one pass (0, δ)–WINNER DETERMINATION algorithm for the plurality election that uses s bits of memory. We will demonstrate a one-way three party protocol to compute DISJ promise m,3 function using 2s bits of communication thus proving the result. We have the candidate set [m + 1]. The protocol is as follows. Player 1 starts running the one pass (0, δ)–WINNER DETERMINATION algorithm on the input X1 ∪ {m + 1}. Once player 1 is done reading all its input, it sends its memory content to player 2. This needs at most s bits of communication. Player 2 resumes the run of the algorithm with input X2 ∪ {m + 1} and sends its memory content to player 3. Again this needs at most s bits of communication. Player 3 resumes the run of the algorithm on input X3 and output 1 if and only if the winner is m + 1 and 0 else. Notice that, if the Xi ∩ Xj = ∅ for every i ≠ j then, the only winner of the votes (X1,m + 1,X2,m + 1,X3) is the candidate m + 1 with a plurality score of two. On the other hand, if there exist an element y ∈ [m] such that y ∈ Xi for every i ∈ [t] and (Xi ∖ {y}) ∩ (Xj ∖ {y}) = ∅ for every i ≠ j then, the only winner of the votes(X1,m + 1,X2,m + 1,X3) is the candidate y with a plurality score of three. The number of candidates in the election above is m + 1 and the number of votes n is ∣X1∣ +∣X2∣+ ∣X3∣+ 2(m + 1) = Θ(m). This gives a space complexity lower bound of Ω(min{m,n}). Theorem 12 and 13 give space complexity lower bounds for the case ε = 0. Next, we consider the other extreme case: deterministically find an ε-winner, corresponding to δ = 0.\nTheorem 14. Assume ε < 1 5 . Then any one pass (ε,0)–WINNER DETERMINATION algorithm for the plurality election must use Ω(logn) bits of memory, even if the number of voters is known up to a factor of 2 and the number of candidates is only 2. The same applies for generalized plurality, scoring rules, maximin, Copeland, Bucklin, and plurality with run off voting rules.\nProof. For the sake of contradiction, we assume that the number of possible memory contents of the algorithm is o(n), since otherwise the algorithm uses Ω(logn) space and we have nothing to prove. Our candidate set is {0,1}. We will generate two vote streams, say R1 and R2, in such a way that the final state of the algorithm would be same; however ε–winner would be different for the two streams thus providing the contradiction we are looking for.\nLet s0 be the starting state of the algorithm. Consider the stream of votes for 1 and let the algorithm repeats its state for the first time after reading i many 1 votes. Let the state of the algorithm after reading ith 1 vote be same as the state the algorithm was after it read jth 1 vote. Let us call µ = i − j. Clearly µ = o(n). Then there exist δ1, δ2 = o(n) such that the state the algorithm will be after reading n\n4 − δ1 many votes for 1 is same as the state it will be after\nreading 3n 4 + δ2 many votes for 1. Let R1 be the stream of n 4 − δ1 many votes for 1 followed by n 2 many votes for 0. Let R2 be the stream of 3n 4 + δ2 many votes for 1 followed by n 2 many votes for 0. By construction the output of the algorithm is same for both the streams R1 and R2. However, candidate 1 is only ε-winner in R1 and candidate 0 is only ε-winner in R2. For elections with two candidates, scoring rules, maximin, Copeland, Bucklin, and plurality with run off voting rules are same as the plurality voting rule."
    }, {
      "heading" : "4.2.2 Lower Bounds for Approximate and Randomized algorithms",
      "text" : "Now we move on and show space complexity lower bounds for general (ε, δ)–WINNER DETERMINATION problem for various voting rules. The observation below immediately follows from the fact that the algorithm has to output a candidate as an ε-winner.\nObservation 2. Every (ε, δ)–WINNER DETERMINATION algorithm, for all the voting rules considered in this paper, needs Ω(logm) bits of memory. We show next a space complexity lower bound of Ω(1\nε log 1 ε ) bits for the (ε, δ)–WINNER DETER-\nMINATION problem for various voting rules.\nTheorem 15. Suppose the number of candidatesm is at least 1 ε . Any one pass (ε, δ)–WINNER DETERMINATION algorithm for approval, k-approval, for k = O(mλ) for every λ ∈ [0,1), generalized plurality, Borda, maximin, Copeland, and plurality with run off elections must use Ω((1−δ)1\nε log 1 ε )\nbits of memory, even when the number of votes are exactly known beforehand, for every 1 − δ > 3ε 2 . Proof. We will show that, when m ≥ 1 ε , we need Ω( 1√ ε log 1 ε ) bits of memory for solving the (√ε 8 , δ)–WINNER DETERMINATION problem, thereby proving the result. Consider the AUGMENTED-INDEXING1/√ε,1/√ε problem where Alice is given a string x = (x1, x2,⋯, x1/√ε) ∈ [1/√ε]1/√ε and Bob is given an integer i ∈ [1/√ε] and (x1,⋯, xi−1). The candidate set of the election, that we generate, is [1/√ε] × [1/√ε]. The overview of the technique is as follows: Alice generates a stream of votes and runs the algorithm, then sends the memory content to Bob, and Bob resumes the run of the algorithm with another stream of votes (both the streams of votes depend on the voting rule under consideration) in such a way that the only √ ε\n8 –winner will be\nthe candidate (xi, i). Thus Bob can output xi correctly if and only if the (√ε/8, δ)–WINNER DETERMINATION algorithm outputs correctly. Now the result follows from Lemma 2. The elections for specific voting rules are as follows. Let n be the number of votes.\n• k-approval for k = O(mλ) for every λ ∈ [0,1), approval, and generalized plurality: It is enough to prove the result for the k-approval voting rule for k = O(mλ) for every λ ∈ [0,1), since every k-approval election is also an approval election. For k = 1, we get the result\nfor the plurality voting rule and thus for the generalized plurality voting rule, since every plurality election is also a generalized plurality election. – Case 1: k ≤ √ m: Alice generates a stream of n\n2 votes in such a way that the k-approval score of every candidate in {(xj , j) ∶ j ∈ [1/√ε]} is at least ⌊k√εn/2⌋ and the k-approval score of any other candidate is 0. Alice now sends the memory content of the algorithm to Bob. Bob resumes the run of the algorithm by generating another stream of n/2 votes\nin such a way that the k-approval score of every candidate in {(j, i) ∶ j ∈ [1/√ε]} is at least ⌊k√εn/2⌋ and the k-approval score of any other candidate is 0. The score of the candidate (xi, i) is at least ⌊k√εn⌋ where as the score of every other candidate is at most ⌈k√εn/2⌉. Hence the only √ ε/8–winner is (xi, i).\n– Case 2: k > √ m and k = O(mλ) for any λ ∈ [0.5,1): Alice generates a stream of n\n2 votes\nin such a way that the k-approval score of every candidate in {(xj , j) ∶ j ∈ [ 1√ε]} is at least n 2 and the k-approval score of any other candidate is at most ⌈(k − 1√ ε )n/ ( 2√ ε ( 1√ ε − 1))⌉, which is at most n 2 − √ ε 2 n for sufficiently small constant ε (depending on λ). Alice now sends the memory content of the algorithm to Bob. Bob resumes the run of the algorithm by generating another stream of n 2 votes in such a way that the k-approval score of every candidate in {(j, i) ∶ j ∈ [ 1√ ε ]} is at least n 2 and the k-approval score of any other candidate is ⌈(k − 1√ ε ) n2√\nε ( 1√ ε −1)⌉. In this case also the only\n√ ε\n8 –winner is (xi, i).\n• Borda, Bucklin: Alice generates a stream of n 2 votes where the candidates in {(xℓ, ℓ), ℓ ∈[1/√ε]} are uniformly distributed in top 1/√ε positions of the votes and the rest of the candi-\ndates are uniformly distributed in bottom 1/ε − 1/√ε positions of the votes. Alice now sends the memory content to Bob and Bob resumes the run of the algorithm by generating another stream of n/2 votes where the candidates in {(ℓ, i), ℓ ∈ [1/√ε]} are uniformly distributed in top 1/√ε positions of the votes and the rest of the candidates are uniformly distributed in bottom 1/ε − 1/√ε positions of the votes. The Borda score of the candidate (xi, i) is (1/ε − 1/2√ε)n whereas the Borda score of every other candidate is at most (1/2ε − 1/4√ε)n. Hence, the only√ ε/8–winner for the Borda voting rule is (xi, i), since each vote change can reduce or increase the Borda score of any candidate by at most 1/ε. The candidate (xi, i) is ranked within top 2/3√ε positions in 2n/3 many votes, whereas any other candidate is ranked within top 2/3√ε positions in at most n/3 many votes. Hence the only √ ε/8–winner for the Bucklin voting rule is (xi, i). • Any Condorcet consistent voting rule, Plurality with runoff: Let us define X = {(xℓ, ℓ) ∶ ℓ ∈ [1/√ε]}, Y = [1/√ε] × [1/√ε] ∖X. Suppose Ð→X and Ð→Y are arbitrary but fixed ordering of the candidates in X and Y respectively. For every ℓ ∈ [1/√ε], Alice generates √εn/4 votes of the form (xℓ, ℓ) ≻ ÐÐÐÐÐÐÐÐ→X ∖ {(xℓ, ℓ)} ≻ Ð→Y and another √εn/4 votes of the form ←ÐÐÐÐÐÐX ∖ (xℓ, ℓ) ≻ (xℓ, ℓ) ≻ Ð→Y , where ←ÐX is the reverse order of Ð→X . Alice now sends the memory content to Bob. Let us define A = {(ℓ, i) ∶ ℓ ∈ [1/√ε]} and B = [1/√ε] × [1/√ε] ∖A. Suppose Ð→A and Ð→B are arbitrary but fixed ordering of A and B respectively. Bob resumes the run of the algorithm\nby generating another √ εn/4 votes of the form (ℓ, i) ≻ ÐÐÐÐÐ→A ∖ (ℓ, i) ≻ Ð→B and another √εn/4 votes of the form ←ÐÐÐÐÐ A ∖ (ℓ, i) ≻ (ℓ, i) ≻Ð→B for every ℓ ∈ [1/√ε], where ←ÐA is the reverse order of Ð→A . The candidate (xi, i) defeats every other candidate in pairwise election by a margin of at least n4 . Also the plurality score of the candidate (xi, i) is more than the plurality score of every other candidate by at least √ εn. Hence the only √ ε/8–winner is (xi, i). We can prove a space lower bound of Ω(mε log 1 ε ) for one pass (ε, δ)–WINNER DETERMINATION algorithms for Borda, Bucklin, Copeland, and maximin voting rules by reducing it from AUGMENTED-INDEXING1/ε,m in the proof of Theorem 15. We summarize this observation below. Corollary 3. Suppose the number of candidates m is at least 1 ε . Any one pass (ε, δ)–WINNER DETERMINATION algorithm for Borda, maximin, Copeland, and plurality with run off elections must use Ω((1 − δ)m log 1 ε ) bits of memory, even when the number of votes are exactly known beforehand, for every 1 − δ > 3ε 2 .\nFor the k-veto voting rule, we prove below, again by reducing from AUGMENTED-INDEXING, a slightly weaker space complexity lower bound compared to the bounds of Theorem 15.\nTheorem 16. Suppose the number of candidates m is at least 1 ε . Any one pass (ε, δ)–WINNER DETERMINATION algorithm for the k-veto voting rule for k = O(mλ), for every λ ∈ [0,1), must use Ω( 1\nεµ log 1 ε ), for every constant µ < 1, bits of memory, even when the number of votes are exactly\nknown beforehand, for every 1 − δ > 3ε 2 . Proof. We prove the result for ( ε 5 , δ)–WINNER DETERMINATION problem. Consider the AUGMENTED-INDEXING 1 ε1−µ , 1 εµ problem where the first player Alice is given a string x ∈ [ 1\nε1−µ ] 1εµ , while the second player Bob is given an integer i ∈ [ 1\nεµ ] and xj for every j < i. The candidate\nset of our election is [ 1 ε1−µ ] × [ 1εµ ]. The votes would be such that the only ε5–winner will be the candidate (xi, i), thereby proving the result. Thus Bob can output xi correctly whenever our (ε, δ)–WINNER DETERMINATION algorithm outputs correctly. Alice generates a stream of n\n2\nvotes (assume n to be sufficiently large) in such a way that for every a, b ∈ {(xj , j) ∶ j ∈ 1εµ} and x, y ∈ [ 1\nε1−µ ] × [ 1εµ ] ∖ {(xj , j) ∶ j ∈ 1εµ}, we have s(a) − s(x) ≥ εn2 , s(b) − 1 ≤ s(a) ≤ s(b) + 1, and s(y) − 1 ≤ s(x) ≤ s(y) + 1, where s(⋅) is the number of vetoes that a candidate receives (which is always negative or zero). This is possible since k = O(mλ) for λ ∈ [0,1). Alice now sends the memory content of the algorithm. Bob resumes the run of the algorithm by generating another stream of n 2 votes in such a way that for every a′, b′ ∈ {(z, i) ∶ z ∈ 1\nε1−µ } and x′, y′ ∈ [ 1\nε1−µ ] × [ 1εµ ] ∖ {(z, i) ∶ z ∈ 1ε1−µ }, we have s(a′) − s(x′) ≥ εn2 , s(b′) − 1 ≤ s(a′) ≤ s(b′) + 1, and s(y′) − 1 ≤ s(x′) ≤ s(y′) + 1. Now the score of (xi, i) is more than the score of every other candidate by at least εn\n2 . Hence, the candidate (xi, i) is the unique ε5–winner.\nFor the k-approval voting rule, we provide a stronger space complexity lower bound of Ω(k ε log 1 ε ), when the number of candidates m is at least k ε2 , by reducing from AUGMENTEDINDEXING 1 ε , k ε . Theorem 17. Assume that the number of candidates m is at least k ε2 . Then any one pass (ε, δ)– WINNER DETERMINATION algorithm for the k-approval voting rule must use Ω(k ε log 1 ε ) bits of memory.\nProof. We prove the result for ( ε 5 , δ)–WINNER DETERMINATION problem. Consider the AUGMENTED-INDEXING 1 ε , k ε problem where Alice is given (x1,⋯, x k ε ) ∈ [1 ε ]kε and Bob is given (x1,⋯, xi−1). We will create a k-approval election in such a way that the ε5 -winner will reveal xi to Bob. The candidate set of our election is [1\nε ]× [k ε ]. For every j ∈ [k], Alice generates εn 2 many\nvotes approving candidates in {(xk(j−1)+1, k(j − 1) + 1), (xk(j−1)+2, k(j − 1) + 2),⋯, (xkj , kj)}. Alice now sends the memory content to Bob. Let X = {(j, i) ∶ j ∈ [1\nε ]}. If k ≤ 1 ε then, Bob\ngenerates n 2 votes in such a way that every candidate in X gets at least kεn 2 many approvals and the candidates in [1 ε ] × [k ε ] ∖ X does not get any approval from the votes that Bob generates. Now, the k-approval score of the candidate (xi, i) is at least (k + 1)εn2 , whereas every other candidate gets at most kεn\n2 many approvals. Hence, (xi, i) is the unique ε5 -winner. If k > 1ε\nthen, Bob generates n 2 votes in such a way that every candidate in X gets n 2 many approvals and every candidate in [1 ε ] × [k ε ] ∖ X gets at most (k − 1 ε )n 2 1 k/ε2−1/ε ≤ n 2 ε2 many approvals from the votes that Bob generates. Here again the k-approval score of the candidate (xi, i) is at least(1 + ε)n 2 , where as the k-approval score of every other candidate is at most εn 2 . Hence, (xi, i) is the unique ε 5 -winner.\nFor the generalized plurality voting rule, we provide a Ω( 1√ ε logm) space complexity lower bound, again by reducing from AUGMENTED-INDEXINGm, 1√ ε . This bound is better than the lower bound of Theorem 15 when m is exponentially larger compared to 1 ε . Theorem 18. Suppose the number of candidates m is at least 1√ ε . Any one pass (ε, δ)–WINNER DETERMINATION algorithm for the generalized plurality rule must use Ω( 1√ ε logm) bits of memory, for every 1 − δ > 3ε 2 . Proof. We prove the result for ( ε 5 , δ)–WINNER DETERMINATION problem. Consider the\nAUGMENTED-INDEXINGm, 1√ ε problem where Alice is given a string x = (x1,⋯, x 1√ ε ) ∈ [m] 1√ε and Bob is given an integer i ∈ [ 1√\nε ] and (x1,⋯, xi−1). The candidate set of our election is [m]×[ 1√ε].\nThe votes would be such that the only ε 5 –winner will be the candidate (xi, i), thereby proving the result. Thus Bob can output xi correctly whenever our ( ε5 , δ)–WINNER DETERMINATION algorithm outputs correctly. Alice generates ( 1√\nε − j)εn many approvals for candidate (xj , j), for\nevery j < 1√ ε . Alice now sends the memory content of the algorithm. Bob resumes the run of the algorithm by generating ( 1√ ε − j)εn many approvals for candidate (xj, j), for every j < i. Notice that, the only ε 5 -winner is the candidate (xi, i). Now the space complexity lower bound follows from Lemma 2.\nThe space complexity lower bound in Theorem 15 for the plurality voting rule matches with the upper bound of Theorem 3, when 1 ε ≤ m ≤ 1 εO(1) . For the case when m ≤ 1 ε , we now show a matching space complexity lower bound for the plurality voting rule. We prove this result by exhibiting a reduction from the MAX-SUM 1 ε ,m problem. Theorem 19. Assume that the number of candidates m is at most 1 ε . Then any one pass (ε, δ)– WINNER DETERMINATION algorithm for the plurality, generalized plurality, approval, k-approval for k = O(mλ), for any λ ∈ [0,1), maximin, Copeland, Bucklin, plurality with run off voting rules must use Ω(m log 1\nε ) bits of memory.\nProof. First, let us prove the result for the plurality voting rule. Suppose we have a one pass (ε, δ)–WINNER DETERMINATION algorithm for the plurality election which uses s(n, ε) bits of memory. Consider the communication problem MAX-SUM 1\nε ,m. Let the inputs to Alice and Bob\nin the MAX-SUM 1 ε ,m instance be x = (x1, x2,⋯, xm) ∈ [1ε ]m and y = (y1, y2,⋯, ym) ∈ [1ε ]m respectively. The candidate set of the election is [m]. Alice generates xi many plurality vote for the candidate i, for every i ∈ [m]. Alice now sends the memory content of the algorithm to Bob. Bob resumes the run of the algorithm by generating yi many plurality votes for the candidate i, for every i ∈ [m]. Suppose i = argmaxj∈[m]{xj +yj} (recall from Definition 6 that there exist unique element i that maximizes xi + yi) and ℓ ≠ argmaxj∈[m]{xj + yj}. Then we have the following:\n(xi + yi) − (xℓ + yℓ) ∑j∈[m](xj + yj) ≥ ε 2m ≥ ε2 2\nThe first inequality follows from the fact that (xi + yi) − (xℓ + yℓ) ≥ 1 and ∑j∈[m] xj + yj ≤ 2mε . The second inequality follows from the assumption that m ≤ 1\nε . Hence, whenever the (ε2 5 , δ)–\nWINNER DETERMINATION algorithm outputs an ε 2\n5 -winner, Bob also outputs correctly in the\nMAX-SUM 1 ε ,m problem instance.\nFor the other voting rules, the idea is the same as above: we will generate votes in such a way that ensures that the candidate i wins if i = argmaxj∈[m]{xj + yj} by a margin of at least one. Below, we only specify the votes to be generated for other voting rules.\n• Generalized plurality, approval: Follows immediately from the fact that every plurality\nelection is a valid generalized plurality and approval election too.\n• k-approval for k = O(mλ), for any λ ∈ [0,1): Alice (respectively Bob) generates xi (respectively yi) many votes such that candidate i gets xi many approvals and the rest (k − 1)xi many approvals are equally distributed among other m − 1 candidates.\n• Borda, maximin, Copeland, Bucklin, plurality with run off: Alice (respectively Bob) gen-\nerates xi (respectively yi) many votes of the form i ≻ Ð→ C−i and another xi (respectively yi)\nmany votes of the form i ≻ ←Ð C−i, where Ð→ C−i is an arbitrary but fixed order of the candidates in\nC ∖ {i} and ←ÐC−i is the reverse order of Ð→C−i. Now we show space complexity lower bounds that depend on the number of votes n. The result below is obtained by reducing from the GREATER-THANn problem. The lower bound is tight in the number of votes n.\nTheorem 20. Any one pass (ε, δ)–WINNER DETERMINATION algorithm for the plurality voting rule must use Ω(log logn) memory bits, even if the number of candidates is only 2, for every δ < 1\n4 .\nThe same applies for generalized plurality, scoring rules, maximin, Copeland, Bucklin, and plurality with run off voting rules.\nProof. Suppose we have a one pass (ε, δ)–WINNER DETERMINATION algorithm for the plurality election which uses s(n) bits of space. Using this algorithm, we will show a communication protocol for the GREATER-THANn problem whose communication coplexity is s(2n) thereby proving the statement. The candidate set is {0,1}. Alice generates a stream of 2x many plurality votes for the candidate 1. Alice now sends the memory content of the algorithm. Bob resumes the run of the algorithm by generating a stream of 2y many plurality votes for the candidate 0. If x > y then the candidate 1 is the only ε-winner; whereas if x < y then the candidate 0 is the only ε-winner.\nFor elections with two candidates, generalized plurality, scoring rules, maximin, Copeland, Bucklin, and plurality with run off voting rules are same as the plurality voting rule."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "In this work, we studied the space complexity for determining approximate winners in the setting where votes are inserted continually into a data stream. We showed that allowing randomization and approximation indeed allows for much more space-efficient algorithms. Moreover, our bounds are tight in certain parameter ranges.\nThe most immediate open question is to close the gaps between the upper and lower bounds. In particular, even for plurality, the dependence on m and ε is not tight when m is large. Also, for the other voting rules, are there more sophisticated algorithms which improve our upper bounds? In a different vein, it may be interesting to implement these streaming algorithms for use in practice (say, for participatory democracy experiments or for online social networks) and investigate how they perform. Finally, instead of having the algorithm be passive, could we improve performance by having the algorithm actively query the voters as they appear in the stream?\nAcknowledgement: We thank David Woodruff for helpful conversations about the heavy hitters problem."
    } ],
    "references" : [ {
      "title" : "The space complexity of approximating the frequency moments",
      "author" : [ "N. Alon", "Y. Matias", "M. Szegedy" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1999
    }, {
      "title" : "An information statistics approach to data stream and communication complexity",
      "author" : [ "Z. Bar-Yossef", "T. Jayram", "R. Kumar", "D. Sivakumar" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "MJRTY- a fast majority vote algorithm",
      "author" : [ "R.S. Boyer", "J.S. Moore" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1991
    }, {
      "title" : "Optimal sampling from sliding windows",
      "author" : [ "V. Braverman", "R. Ostrovsky", "C. Zaniolo" ],
      "venue" : "In Proceedings of the Twenty-eighth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Voting almost maximizes social welfare despite limited communication",
      "author" : [ "I. Caragiannis", "A.D. Procaccia" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Near-optimal lower bounds on the multiparty communication complexity of set disjointness",
      "author" : [ "A. Chakrabarti", "S. Khot", "X. Sun" ],
      "venue" : "In Computational Complexity,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Finding frequent items in data streams",
      "author" : [ "M. Charikar", "K. Chen", "M. Farach-Colton" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "Communication complexity of common voting rules",
      "author" : [ "V. Conitzer", "T. Sandholm" ],
      "venue" : "In Proceedings of the 6th ACM conference on Electronic commerce,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Finding frequent items in data streams",
      "author" : [ "G. Cormode", "M. Hadjieleftheriou" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "An improved data stream summary: the count-min sketch and its applications",
      "author" : [ "G. Cormode", "S. Muthukrishnan" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "Maintaining stream statistics over sliding windows",
      "author" : [ "M. Datar", "A. Gionis", "P. Indyk", "R. Motwani" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Frequency estimation of internet packet streams with limited space",
      "author" : [ "E.D. Demaine", "A. López-Ortiz", "J.I. Munro" ],
      "venue" : "In ESA,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Sample complexity for winner prediction in elections",
      "author" : [ "P. Dey", "A. Bhattacharyya" ],
      "venue" : "In Proceeding of the 14th International Conference on Autonomous Systems and Multiagent Systems (AAMAS-15)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Periodicity in streams",
      "author" : [ "F. Ergün", "H. Jowhari", "M. Sağlam" ],
      "venue" : "In APPROX and RANDOM,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Finding a majority among n votes: Solution to problem 81-5",
      "author" : [ "M.J. Fischer", "S.L. Salzburg" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1982
    }, {
      "title" : "Approximate counting: a detailed analysis",
      "author" : [ "P. Flajolet" ],
      "venue" : "BIT Numerical Mathematics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1985
    }, {
      "title" : "Applying approximate counting for computing the frequency moments of long data streams",
      "author" : [ "A. Gronemeier", "M. Sauerhoff" ],
      "venue" : "Theory Comput. Syst.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "External memory algorithms. chapter Computing on Data Streams, pages 107–118",
      "author" : [ "M.R. Henzinger", "P. Raghavan", "S. Rajagopalan" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "Tight bounds for lp samplers, finding duplicates in streams, and related problems",
      "author" : [ "H. Jowhari", "M. Sağlam", "G. Tardos" ],
      "venue" : "In Proceedings of the Thirtieth ACM SIGMODSIGACT-SIGART Symposium on Principles of Database Systems,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "A simple algorithm for finding frequent elements in streams and bags",
      "author" : [ "R.M. Karp", "S. Shenker", "C.H. Papadimitriou" ],
      "venue" : "ACM Transactions on Database Systems (TODS),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2003
    }, {
      "title" : "On randomized one-round communication complexity",
      "author" : [ "I. Kremer", "N. Nisan", "D. Ron" ],
      "venue" : "Computational Complexity,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1999
    }, {
      "title" : "Communication Complexity",
      "author" : [ "E. Kushilevitz", "N. Nisan" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1997
    }, {
      "title" : "Crowdsourcing for participatory democracies: Efficient elicitation of social choice functions",
      "author" : [ "D.T. Lee", "A. Goel", "T. Aitamurto", "H. Landemore" ],
      "venue" : "In Proceedings of the Seconf AAAI Conference on Human Computation and Crowdsourcing,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Distributed sensor networks: A multiagent perspective, volume 9",
      "author" : [ "V. Lesser", "C.L. Ortiz Jr.", "M. Tambe" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Approximate frequency counts over data streams",
      "author" : [ "G.S. Manku", "R. Motwani" ],
      "venue" : "In Proceedings of the 28th international conference on Very Large Data Bases,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2002
    }, {
      "title" : "Efficient computation of frequent and top-k elements in data streams",
      "author" : [ "A. Metwally", "D. Agrawal", "A. El Abbadi" ],
      "venue" : "In Database Theory-ICDT",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2005
    }, {
      "title" : "On data structures and asymmetric communication complexity",
      "author" : [ "P.B. Miltersen", "N. Nisan", "S. Safra", "A. Wigderson" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1998
    }, {
      "title" : "Counting large numbers of events in small registers",
      "author" : [ "R. Morris" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1978
    }, {
      "title" : "Data streams: Algorithms and applications",
      "author" : [ "S. Muthukrishnan" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2005
    }, {
      "title" : "Sketching and streaming algorithms for processing massive data. XRDS: Crossroads, The ACM Magazine for Students, 19(1):14–19",
      "author" : [ "J. Nelson" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    }, {
      "title" : "Shannon’s information methods for lower bounds for probabilistic communication",
      "author" : [ "D. Smirnov" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1988
    }, {
      "title" : "Random sampling with a reservoir",
      "author" : [ "J.S. Vitter" ],
      "venue" : "ACM Trans. Math. Softw.,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1985
    }, {
      "title" : "Computing the margin of victory for various voting rules",
      "author" : [ "L. Xia" ],
      "venue" : "In Proceedings of the 13th ACM Conference on Electronic Commerce,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2012
    }, {
      "title" : "Some complexity questions related to distributive computing (preliminary report)",
      "author" : [ "Yao", "A.C.-C" ],
      "venue" : "In Proceedings of the eleventh annual ACM symposium on Theory of computing,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1979
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We investigate the problem of winner determination from computational social choice theory in the data stream model. Specifically, we consider the task of summarizing an arbitrarily ordered stream of n votes on m candidates into a small space data structure so as to be able to obtain the winner determined by popular voting rules. As we show, finding the exact winner requires storing essentially all the votes. So, we focus on the problem of finding an ε-winner, a candidate who could win by a change of at most ε fraction of the votes. We show non-trivial upper and lower bounds on the space complexity of ε-winner determination for several voting rules, including k-approval, k-veto, scoring rules, approval, maximin, Bucklin, Copeland, and plurality with run off.",
    "creator" : "LaTeX with hyperref package"
  }
}