{
  "name" : "1609.05140.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Option-Critic Architecture",
    "authors" : [ "Pierre-Luc Bacon", "Jean Harb", "Doina Precup" ],
    "emails" : [ "dprecup}@cs.mcgill.ca" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Temporal abstraction allows representing knowledge about course of actions that take place at different time scales. In reinforcement learning, options (Sutton, Precup, and Singh 1999; Precup 2000) provide a framework for defining such courses of action and for seamlessly learning and planning with them. Discovering temporal abstractions autonomously has been the subject of extensive research efforts in the last 15 years (McGovern and Barto 2001; Stolle and Precup 2002; Menache, Mannor, and Shimkin 2002; Şimşek and Barto 2009; Silver and Ciosek 2012), but approaches that can be used naturally with continuous state and/or action spaces have only recently started to become feasible (Konidaris et al. 2011; Niekum and Barto 2011; Mann, Mannor, and Precup ; Mankowitz, Mann, and Mannor 2016; Kulkarni et al. 2016; Vezhnevets et al. 2016; Daniel et al. 2016).\nThe majority of the existing work has focused on finding subgoals (useful states that an agent should reach) and subsequently learning policies to achieve them. This idea has lead to interesting methods but ones which are also difficult to scale up given their “combinatorial” flavor. Additionally, learning policies associated with subgoals can be expensive in terms of data and computation time; in the worst case, it can be as expensive as solving the entire task.\nWe present an alternative view, which blurs the line between the problem of discovering options from that of learning options. Based on the policy gradient theorem (Sutton et al. 2000), we derive new results which enable a gradual\nlearning process of the intra-option policies and termination functions, simultaneously with the policy over them. This approach works naturally with both linear and non-linear function approximators, under discrete or continuous state and action spaces. Existing methods for learning options are considerably slower when learning from a single task: much of the benefit will come from re-using the learned options in similar tasks. In contrast, we show that our approach is capable of successfully learning options within a single task without incurring any slowdown and while still providing re-use speedups.\nWe start by reviewing background related to the two main ingredients of our work: policy gradient methods and options. We then describe the core ideas of our approach: the intra-option policy and termination gradient theorems. Additional technical details are included in the appendix. We present experimental results showing that our approach learns meaningful temporally extended behaviors in an effective manner. As opposed to other methods, we only need to specify the number of desired options; it is not necessary to have subgoals, extra rewards, demonstrations, multiple problems or any other special accommodations (however, the approach can work with pseudo-reward functions if desired). To our knowledge, this is the first end-to-end approach for learning options that scales to very large domains at comparable efficiency."
    }, {
      "heading" : "Preliminaries and Notation",
      "text" : "A Markov Decision Process consists of a set of states S, a set of actionsA, a transition function P : S×A → (S → [0, 1]) and a reward function r : S × A → R. For convenience, we develop our ideas assuming discrete state and action sets. However, our results extend to continuous spaces using usual measure-theoretic assumptions (some of our empirical results are in continuous tasks). A (Markovian stationary) policy is a probability distribution over actions conditioned on states, π : S × A → [0, 1]. In discounted problems, the value function of a policy π is defined as the expected return: Vπ(s) = Eπ [ ∑∞ t=0 γ\ntrt+1 | s0 = s] and its action-value function as Qπ(s, a) = Eπ [ ∑∞ t=0 γ\ntrt+1 | s0 = s, a0 = a], where γ ∈ [0, 1) is the discount factor. A policy π is greedy with respect to a given action-value function Q if π(s, a) > 0 iff a = argmaxa′ Q(s, a\n′). In a discrete MDP, there is at least one optimal policy which is greedy with re-\nar X\niv :1\n60 9.\n05 14\n0v 1\n[ cs\n.A I]\n1 6\nSe p\n20 16\nspect to its own action-value function. Policy gradient methods (Sutton et al. 2000; Konda and Tsitsiklis 2000) address the problem of finding a good policy by performing stochastic gradient descent to optimize a performance objective over a given family of parametrized stochastic policies, πθ. The policy gradient theorem (Sutton et al. 2000) provides expressions for the gradient of the average reward and discounted reward objectives with respect to θ. In the discounted setting, the objective is defined with respect to a designated start state (or distribution) s0: ρ(θ, s0) = Eπθ [ ∑ t=0 γ\ntrt+1 | s0]. The policy gradient theorem shows that: ∂ρ(θ,s0)∂θ = ∑ s µπθ (s | s0) ∑ a ∂πθ(a|s)\n∂θ Qπθ (s, a), where µπθ (s | s0) = ∑∞ t=0 γ\nt P (st = s | s0) is a discounted weighting of the states along the trajectories starting from s0. In practice, the policy gradient is estimated from samples along the on-policy stationary distribution. (Thomas 2014) showed that neglecting the discount factor in this stationary distribution makes the usual policy gradient estimator biased. However, correcting for this discrepancy also reduces data efficiency. For simplicity, we build on the framework of (Sutton et al. 2000) and discuss how to extend our results according to (Thomas 2014).\nThe options framework (Sutton, Precup, and Singh 1999; Precup 2000) formalizes the idea of temporally extended actions. A Markovian option ω ∈ Ω is a triple (Iω, πω, βω) in which Iω ⊆ S is an initiation set, πω is an intra-option policy, and βω : S → [0, 1] is a termination function. We also assume that ∀s ∈ S,∀ω ∈ Ω : s ∈ Iω (i.e., all options are available everywhere), an assumption made in the majority of options discovery algorithms. We will discuss how to dispense with this assumption in the final section. (Sutton, Precup, and Singh 1999; Precup 2000) show that an MDP endowed with a set of options becomes a Semi-Markov Decision Process (Puterman 1994, chapter 11), which has a corresponding optimal value function over options VΩ(s) and option-value function QΩ(s, ω). Learning and planning algorithms for MDPs have their counterparts in this setting. However, the existence of the underlying MDP offers the possibility of learning about many different options in parallel : the idea of intra-option learning, which we leverage in our work."
    }, {
      "heading" : "Learning Options",
      "text" : "We adopt a continual perspective on the problem of learning options. At any time, we would like to distill all of the available experience into every component of our system: value function and policy over options, intra-option policies and termination functions. To achieve this goal, we focus on learning option policies and termination functions, assuming they are represented using differentiable parameterized function approximators.\nWe consider the call-and-return option execution model, in which an agent picks option ω according to its policy over options πΩ , then follows the intra-option policy πω until termination (as dictated by βω), at which point this procedure is repeated. Let πω,θ denote the intra-option policy of option ω parametrized by θ and βω,ϑ, the termination function of\nω parameterized by ϑ. We present two new results for learning options, obtained using as blueprint the policy gradient theorem (Sutton et al. 2000). Both results are derived under the assumption that the goal is to learn options that maximize the expected return in the current task. However, if one wanted to add extra information to the objective function, this could readily be done so long as it comes in the form of an additive differentiable function.\nSuppose we aim to optimize directly the discounted return, expected over all the trajectories starting at a designated state s0 and option ω0, then: ρ(Ω, θ, ϑ, s0, ω0) = EΩ,θ,ω [ ∑∞ t=0 γ\ntrt+1 | s0, ω0]. Note that this return depends on the policy over options, as well as the parameters of the option policies and termination functions. We will take gradients of this objective with respect to θ and ϑ. In order to do this, we will manipulate equations similar to those used in intra-option learning (Sutton, Precup, and Singh 1999, section 8). Specifically, the definition of the option-value function can be written as:\nQΩ(s, ω) = E Ω,θ,ϑ [ ∞∑ t=0 γtrt+1 ∣∣∣∣∣ s0 = s, ω0 = ω ]\n= ∑ a πω,θ (a | s)QU (s, ω, a) , (1)\nwhere QU : S × Ω × A → R is the value of executing an action in the context of a state-option pair:\nQU (s, ω, a) = r(s, a) + γ ∑ s′ P (s′ | s, a)U(ω, s′) . (2)\nNote that (s, ω) can be viewed as defining an augmented state space, cf. (Levy and Shimkin 2011). However, we will not work explicitly with this space; it is used only to simplify the derivation. The function U : Ω × S → R is called the option-value function upon arrival, (Sutton, Precup, and Singh 1999, equation 20). The value of executing ω upon entering a state s′ is given by:\nU(ω, s′) = (1− βω,ϑ(s′))QΩ(s′, ω) + βω,ϑ(s′)VΩ(s′) (3)\nNote that QU and U both depend on θ and ϑ, but we do not include these in the notation for clarity. The last ingredient required to derive policy gradients is the Markov chain along which the performance measure is estimated. The natural approach is to consider the chain defined in the augmented state space, because state-option pairs now play the role of regular states in a usual Markov chain. If option ωt has been initiated or is executing at time t in state st, then the probability of transitioning to (st+1, ωt+1) in one step is:\nP (st+1, ωt+1 | st, ωt) =∑ a πωt,θ (a | st) P (st+1 | st, a) ( (1− βωt,ϑ(st+1))1ωt=ωt+1\n+ βωt,ϑ(st+1)πΩ (ωt+1 | st+1) ) . (4)\nClearly, the process given by (4) is homogeneous. Under mild conditions, and with options available everywhere, it is\nin fact ergodic, and a stationary distribution over state-option pairs exists.\nWe will now compute the gradient of this objective, assuming the intra-option policies are stochastic and differentiable in θ. From (1 , 2), it follows that:\n∂QΩ(s, ω)\n∂θ = (∑ a ∂πω,θ (a | s) ∂θ QU (s, ω, a) )\n+ ∑ a πω,θ (a | s) ∑ s′ γ P (s′ | s, a) ∂U(ω, s ′) ∂θ .\nWe can further expand the right hand side using (3) and (4), which yields the following theorem: Theorem 1 (Intra-Option Policy Gradient Theorem). Given a set of Markov options with stochastic intra-option policies differentiable in their parameters θ, the gradient of the expected discounted return with respect to θ and initial condition (s0, ω0) is:∑\ns,ω µΩ (s, ω | s0, ω0) ∑ a ∂πω,θ (a | s) ∂θ QU (s, ω, a) ,\nwhere µΩ (s, ω | s0, ω0) is a discounted weighting of stateoption pairs along trajectories starting from (s0, ω0): µΩ (s, ω | s0, ω0) = ∑∞ t=0 γ\nt P (st = s, ωt = ω | s0, ω0). The proof is in the appendix. This gradient describes the effect of a local change at the primitive level on the global expected discounted return. In contrast, subgoal or pseudoreward methods assume the objective of an option is simply to optimize its own reward function, ignoring how a proposed change would propagate in the overall objective.\nWe now turn our attention to computing gradients for the termination functions, assumed again to be stochastic and differentiable in ϑ. From (1, 2, 3), we have: ∂QΩ(s, ω)\n∂ϑ = ∑ a πω,θ (a | s) ∑ s′ γ P (s′ | s, a) ∂U(ω, s ′) ∂θ\nHence, the key quantity is the gradient of U . This is a natural consequence of the call-and-return execution, in which the “goodness” of termination functions can only be evaluated upon entering the next state. The relevant gradient can be further expanded as: ∂U(ω, s′)\n∂ϑ = −∂βω,ϑ(s\n′)\n∂ϑ AΩ(s\n′, ω) +\nγ ∑ ω′ ∑ s′′ P (s′′, ω′ | s′, ω) ∂U(ω ′, s′′) ∂ϑ . (5)\nExpanding ∂U(ω ′,s′′)\n∂ϑ recursively leads to a similar form as in theorem (1) but where the weighting of state-option pairs is now according to a Markov chain shifted by one time step: µΩ (st+1, ωt | st, ωt−1) (details are in the appendix). Theorem 2 (Termination Gradient Theorem). Given a set of Markov options with stochastic termination functions differentiable in their parameters ϑ, the gradient of the expected discounted return objective with respect to ϑ is:\n− ∑ s′,ω µΩ (s ′, ω | s1, ω0) ∂βω,ϑ(s ′) ∂ϑ AΩ(s ′, ω) ,\nwhere AΩ is the advantage function (Baird 1993) over options AΩ(s′, ω) = QΩ(s′, ω) − VΩ(s′) and µΩ (s\n′, ω | s1, ω0) is a discounted weighting of stateoption pairs from (s1, ω0): µΩ (s, ω | s1, ω0) =∑∞ t=0 γ\nt P (st+1 = s, ωt = ω | s1, ω0). The advantage function often appears in policy gradient methods (Sutton et al. 2000) when forming a baseline to reduce the variance in the gradient estimates. Its presence in that context has to do mostly with algorithm design. It is interesting that in our case, it follows as a direct consequence of the derivation, and gives the theorem an intuitive interpretation: when the option choice is suboptimal with respect to the expected value over all options, the advantage function is negative and it drives the gradient corrections up, which increases the odds of terminating. After termination, the agent has the opportunity to pick a better option using πΩ. A similar idea also underlies the interrupting execution model of options (Sutton, Precup, and Singh 1999), in which termination is forced whenever the value of QΩ(s′, ω) for the current option ω is less than VΩ(s′). (Mann, Mankowitz, and Mannor 2014) recently studied interrupting options through the lens of an interrupting Bellman Operator in a valueiteration setting. The termination gradient theorem can be interpreted as providing a gradient-based interrupting Bellman operator.\nAlgorithms and Architecture\nBased on theorems 1 and 2, we can now design a gradient ascent procedure for learning options. Since µΩ,QU andAΩ will generally be approximate, we will use a stochastic gradient procedure. Using a two-timescale framework (Konda and Tsitsiklis 2000), we propose to learn the values at a fast timescale while updating the intra-option policies and termination functions at a slower rate.\nWe refer to the resulting system as an option-critic architecture, in reference to the actor-critic architectures (Sutton 1984). The intra-option policies, termination functions and\npolicy over options belong to the actor part of the system while the critic consists of QU and AΩ. The option-critic architecture does not prescribes how to obtain πΩ, and a variety of approaches would apply: using policy gradient methods at the SMDP level, with a planner over the options models, or using temporal difference updates. If πΩ is the greedy policy over options, it follows from (2) that the corresponding one-step off-policy update target g(1)t is:\ng (1) t = rt+1+ γ(1− βωt,ϑ(st+1)) ∑ a πωt,θ (a | st+1)QU (st+1, ωt, a)\nγβωt,ϑ(st+1) max ω ∑ a πω,θ (a | st+1)QU (st+1, ω, a) ,\nwhich also is the update target of the intra-option Q-learning algorithm of (Sutton, Precup, and Singh 1999). LearningQU in addition to QΩ is wasteful, both computationally and in terms of the number of parameters and samples. A practical solution is to only learn QΩ and derive an estimate of QU from it. Because QU is an expectation over next states, QU (s, ω, a) = Es′∼P [r(s, a) + γU(ω, s′) | s, ω, a], it follows that g(1)t is an appropriate estimator. We chose this approach in our tasks using a deep neural network to represents QΩ. A prototypical option-critic implementation in the tabular case, using intra-option Q-learning in the critic is given in the appendix."
    }, {
      "heading" : "Experiments",
      "text" : "We first experiment with a navigation task in the fourrooms domain (Sutton, Precup, and Singh 1999). The goal is to evaluate the ability of an option set learned fully autonomously to recover from a sudden change in the environment. (Sutton, Precup, and Singh 1999) presented a similar experiment for a set of pre-specified options; the options in our results have not been specified a priori.\nInitially, the goal is in the east doorway and the initial state is drawn uniformly from all the other cells. After 1000 episodes, the goal moves to a random location in the lower right room. Primitive movements can fail with probability 1/3, in which case the agent transitions randomly to one of the empty adjacent cells. The discount factor was 0.99, and the reward was +1 at the goal and 0 otherwise.\nWe chose to parametrize the intra-option policies with Boltzmann distributions and the terminations with sigmoid functions. The policy over options was learned using intraoption Q-learning. We also implemented primitive actorcritic (denoted AC-PG) using a Boltzmann policy. We also compared option-critic to a primitive SARSA agent using Boltzmann exploration and no eligibility traces. All the weights were initialized to zero.\nAs can be seen in Figure 2, when the goal suddenly changes, the option-critic agent recovers faster. Furthermore, the initial set of options is learned from scratch at a rate comparable to primitive methods. Despite the simplicity of the domain, we are not aware of other methods which could have solved this first task without incurring a cost\nmuch larger than when using primitive actions alone (McGovern and Barto 2001; Şimşek and Barto 2009). In the two temporally extended settings, with 4 options and 8 options, termination events seem to occur near the doorways (see appendix for illustration). As opposed to (Sutton, Precup, and Singh 1999), we did not encode this knowledge ourselves but simply let the agents find options that would maximize the expected discounted return."
    }, {
      "heading" : "Pinball Domain",
      "text" : "In the Pinball domain (Konidaris and Barto 2009), a ball must be guided through a maze of arbitrarily-shaped polygons to a designated target location. The state space is continuous and corresponds to the position and velocity of the ball in the x-y plane. At every step, the agent must choose among five discrete primitive actions: moves the ball faster or slower, in the vertical or horizontal direction, or take the null action. Collisions with obstacles are elastic and can be used to the advantage of the agent.\nTo learn the critic, we used intra-option Q-learning with linear function approximation and Fourier bases (Konidaris and Barto 2009) of order 3. We experimented with 2, 3 or 4 options. All other choices were as before.\nIn (Konidaris and Barto 2009), an option can only be\nused and updated after a gestation period of 10 episodes. As learning is fully integrated in option-critic, by 40 episodes a near optimal set of options had already been learned in all settings (learning curves can be found in the appendix). From a qualitative point of view, the options exhibit temporal extension and specialization (fig. 3). We also observed that across many successful trajectories to the goal, the same option (the red one) brings the ball to the goal when starting sufficiently close."
    }, {
      "heading" : "Arcade Learning Environment",
      "text" : "We applied the option-critic architecture in the Arcade Learning Environment (ALE) (Bellemare et al. 2013) using a deep neural network to approximate the critic, intra-option policies and termination functions. We used the same configuration as (Mnih et al. 2013) for the first 3 convolutional layers of the network. We then fed the output of the third layer into a dense shared layer of 512 neurons, as depicted in Figure 4.\nWe represented the intra-option policies as linear-softmax of the fourth (dense) layer, so as to output a probability distribution of selecting each action under the current observation. The termination functions were similarly defined as sigmoid functions, with one neuron per termination.\nThe critic network was trained using intra-option Qlearning with experience replay. Option policies and terminations were updated on-line. We used an -greedy policy over options with = 0.05 during the test phase (Mnih et al. 2013).\nAs a consequence of optimizing for the return, the termination gradient tend to shrink options over time. This is expected since in theory primitive actions are sufficient for solving any MDP. We tackled this issue by adding a small ξ = 0.01 term to the advantage function, used by the termination gradient:AΩ(s, ω)+ξ = QΩ(s, ω)−VΩ(s)+ξ. This term has a regularization effect, by imposing an ξ-margin between the value estimate of an option and that of the “optimal” one reflected in VΩ. This allows the advantage function to be positive if the value of an option is near the optimal, thereby stretching it. A similar regularizer was proposed in (Mann, Mankowitz, and Mannor 2014).\nAs in (Mnih et al. 2016), we observed that the intra-option policies would quickly become deterministic. This problem seems to pertain to the use of policy gradient methods with deep neural networks in general, and not from option-critic itself. We applied the regularizer prescribed by (Mnih et al. 2016), by penalizing for low-entropy intra-option policies.\nFinally, a baseline was added to the intra-option policy gradient to reduce the variance in the intra-option policy gradient estimates. The option-value function QΩ is a natural choice of baseline. This change provided substantial improvements in the quality of the intra-option policy distributions and the overall agent performance (see appendix for details).\nWe evaluated option-critic in Asterisk, Ms. Pacman, Seaquest and Zaxxon. For comparison, we allowed the system to learn for the same number of episodes as (Mnih et al. 2013) and fixed the parameters to the same values in all four domains. Despite having more parameters to learn, optioncritic was capable of learning a set of options to achieve the goal in all games, from the ground up, within 200 episodes. In Asterisk, Seaquest and Zaxxon, option-critic surpassed the performance of the DQN architecture based on primitive actions. The eight options learned in each game are learned fully end-to-end, in tandem with the feature representation, with no prior specification of a subgoal or pseudo-reward structure.\nThe solution found by option-critic was easy to interpret in the game of Seaquest when learning with only two options. We found that each option specialized in a behavior sequence which would include either the up or the down button. Figure 6 shows a typical transition from one option to the other, first going upward with option 0 then switching to option 1 downward."
    }, {
      "heading" : "Related Work",
      "text" : "As option discovery has received a lot of attention recently, we now discuss in more detail the place of our approach with respect to others. (Comanici and Precup 2010) also used a gradient-based approach for improving only the termination function for semi-Markov options; termination was modeled by a logistic distribution over a cumulative measure of the features observed since initiation. (Silver and Ciosek 2012) dynamically chained options into longer temporal sequences by relying on compositionality properties. Earlier work on linear options (Sorg and Singh 2010) also used compositionality to plan using linear expectation models for options. As in these works, we rely on Bellman equations and compositionality, but in conjunction with policy gradient. (Levy and Shimkin 2011) also built on policy gradient methods by constructing explicitly the augmented state space and treating stopping events as additional control actions. In contrast, we do not need to construct this (very large) space directly.\nSeveral very recent papers also attempt to formulate option discovery as an optimization problem, and provide solutions that work with function approximation. (Daniel et al. 2016) learn return-optimizing options by treating the termination functions as hidden variables, and using EM to learn them. (Vezhnevets et al. 2016) consider the problem of learning options that have open-loop intra-option policies, also called macro-actions. As in classical planning, the more frequent action sequences are cached. A mapping from states to action sequences is learned along with a commitment module, which triggers re-planning when necessary. In contrast, we use closed-loop policies throughout, which\nare reactive to state information and can provide better solutions. (Mankowitz, Mann, and Mannor 2016) propose a gradient-based option learning algorithm, assuming a particular structure for the initiation sets and termination functions. Under this framework, exactly one option is active in any partition of the state space. (Kulkarni et al. 2016) use the DQN framework to implement a gradient-based option learner, which uses intrinsic rewards to learn the internal policies of options, and extrinsic rewards to learn the policy over options. Descriptions of the subgoals are given as inputs to the option learners. In contrast with this approach, our framework is conceptually general, and does not require intrinsic motivation for learning the options."
    }, {
      "heading" : "Discussion",
      "text" : "We developed a general, gradient-based approach for learning simultaneously the intra-option policies and termination conditions, as well as the policy over options, in order to optimize a performance objective for the task at hand. Our ALE experiments demonstrate successful end-to-end training of the options in the presence of nonlinear function approximation. As noted, our approach only requires specifying the number of options. However, if one wanted to use additional pseudo-rewards, the option-critic framework would easily accommodate it. The internal policies and termination function gradients would simply need to be taken with respect to the pseudo-rewards instead of the task reward. A simple instance of this idea, which we used in some of the experiments, is to use additional rewards to encourage options that are indeed temporally extended, by adding a penalty whenever a switching event occurs. Our approach can work seam-\nlessly with any other heuristic for biasing the set of options towards some desirable property (e.g. compositionality or sparsity), as long as it can be expressed as an additive reward structure. However, as seen in the results, such biasing is not necessary to produce good results.\nThe option-critic architecture relies on the policy gradient theorem, and as discussed in (Thomas 2014), the gradient estimators can be biased in the discounted case. By introducing factors of the form γt ∏t i=1(1 − βi) in our updates (Thomas 2014, eq (3)), it would be possible to obtain unbiased estimates. However, we do not recommend this approach, since the sample complexity of the unbiased estimators is generally too high and the biased estimators performed well in our experiments.\nPerhaps the biggest remaining limitation of our work is the assumption that all options apply everywhere. In the case of function approximation, a natural extension to initiation sets is to use a classifier over features, or some other form of function approximation. As a result, determining which options are allowed may have similar cost to evaluating a policy over options (unlike in the tabular setting, where options with sparse initiation sets lead to faster decisions). This is akin to eligibility traces, which are more expensive than using no trace in the tabular case, but have the same complexity with function approximation. If initiation sets are to be learned, the main constraint that needs to be added is that the options and the policy over them lead to an ergodic chain in the augmented state-option space. This can be expressed as a flow condition that links initiation sets with terminations. The precise description of this condition, as well as sparsity regularization for initiation sets, is left for future work."
    }, {
      "heading" : "Augmented Process",
      "text" : "If option ωt has been initiated or is executing at time t in state st, then the discounted probability of transitioning to (st+1, ωt+1) in one step is:\nP(1)γ (st+1, ωt+1 | st, ωt) =∑ a πωt (a | st) γ P (st+1 | st, a) ( (1− βωt(st+1))1ωt=ωt+1 + βωt(st+1)πΩ (ωt+1 | st+1) ) .\nWhen conditioning the process from (st, ωt−1), the discounted probability of transitioning to st+1, ωt is:\nP(1)γ (st+1, ωt | st, ωt−1) =( (1− βωt−1(st))1ωt=ωt−1 + βωt−1(st)πΩ (ωt | st) )∑ a πωt (a | st) γ P (st+1 | st, a) .\nMore generally, the k-steps discounted probabilities can be expressed recursively as follows: P(k)γ (st+k, ωt+k | st, ωt) = ∑ st+1 ∑ ωt+1 P(1)γ (st+1, ωt+1 | st, ωt) P (k−1) γ (st+k, ωt+k | st+1, ωt+1) ,\nP(k)γ (st+k, ωt+k−1 | st, ωt−1) = ∑ st+1 ∑ ωt P(1)γ (st+1, ωt | st, ωt−1) P (k−1) γ (st+k, ωt+k−1 | st+1, ωt) ."
    }, {
      "heading" : "Proof of the Intra-Option Policy Gradient Theorem",
      "text" : "Taking the gradient of the option-value function:\n∂QΩ(s, ω)\n∂θ =\n∂\n∂θ ∑ a πω,θ (a | s)QU (s, ω, a)\n= ∑ a ( ∂πω,θ (a | s) ∂θ QU (s, ω, a) + πω,θ (a | s) ∂QU (s, ω, a) ∂θ )\n= ∑ a\n( ∂πω,θ (a | s)\n∂θ QU (s, ω, a) + πω,θ (a | s) ∑ s′ γ P (s′ | s, a) ∂U(ω, s ′) ∂θ\n) , (6)\n∂U(ω, s′)\n∂θ = (1− βω,ϑ(s′))\n∂QΩ(s ′, ω)\n∂θ + βω,ϑ(s\n′) ∂VΩ(s\n′)\n∂θ\n= (1− βω,ϑ(s′)) ∂QΩ(s\n′, ω)\n∂θ + βω,ϑ(s ′) ∑ ω′ πΩ (ω ′ | s′) ∂QΩ(s ′, ω′) ∂θ (7)\n= ∑ ω′ ( (1− βω,ϑ(s′))1ω′=ω + βω,ϑ(s′)πΩ (ω′ | s′) ) ∂QΩ(s′, ω′) ∂θ . (8)\nwhere (8) follows from the assumption that θ only appears in the intra-option policies. Substituting (8) into (6) yields a recursion which, using the previous remarks about augmented process can be transformed into:\n∂QΩ(s, ω) ∂θ = ∑ a ∂πω,θ (a | s) ∂θ QU (s, ω, a)\n+ ∑ a πω,θ (a | s) ∑ s′ γ P (s′ | s, a) ∑ ω′ ( (1− βω,ϑ(s′))1ω′=ω + βω,ϑ(s′)πΩ (ω′ | s′) )∂QΩ(s′, ω′) ∂θ\n= ∑ a ∂πω,θ (a | s) ∂θ QU (s, ω, a) + ∑ s′ ∑ ω′ P(1)γ (s ′, ω′ | s, ω) ∂QΩ(s ′, ω′) ∂θ\n= ∞∑ k=0 ∑ s′ ∑ ω′ P(k)γ (s ′, ω′ | s, ω) ∑ a ∂πω′,θ (a | s′) ∂θ QU (s ′, ω′, a)\nThe gradient of the expected discounted return with respect to θ is then:\n∂QΩ(s0, ω0) ∂θ = ∑ s,ω ∞∑ k=0 P(k)γ (s, ω | s0, ω0) ∑ a ∂πω,θ (a | s) ∂θ QU (s, ω, a)\n= ∑ s,ω µΩ(s, ω|s0, ω0) ∑ a ∂πω,θ (a | s) ∂θ QU (s, ω, a) ."
    }, {
      "heading" : "Proof of the Termination Gradient Theorem",
      "text" : "The expected sum of discounted rewards starting from (s1, ω0) is given by:\nU(ω0, s1) = E [ ∞∑ t=1 γt−1rt ∣∣∣∣∣ s1, ω0 ] .\nWe start by expanding U as follows:\nU(ω, s′) = (1− βω,ϑ(s′))QΩ(s′, ω) + βω,ϑ(s′)VΩ(s′)\n= (1− βω,ϑ(s′)) ∑ a πω,θ (a | s′)\n( r(s′, a) +\n∑ s′′ γ P (s′′ | s′, a)U(ω, s′′)\n)\n+ βω,ϑ(s ′) ∑ ω′ πω,θ (ω ′ | s′) ∑ a πω′,θ (a | s′)\n( r(s′, a) +\n∑ s′′ γ P (s′′ | s′, a)U(ω′, s′′)\n) .\nThe gradient of U is then:\n∂U(ω, s′)\n∂ϑ = ∂βω,ϑ(s\n′)\n∂ϑ (VΩ(s ′)−QΩ(s′, ω))︸ ︷︷ ︸ −AΩ(s′,ω) + (1− βω,ϑ(s′)) ∑ a πω,θ (a | s′) ∑ s′′ γ P (s′′ | s′, a) ∂U(ω, s ′′) ∂ϑ\n+ βω,ϑ(s ′) ∑ ω′ πΩ (s ′ | ω′) ∑ a πω′,θ (a | s′) ∑ s′′ γ P (s′′ | s′, a) ∂U(ω ′, s′′) ∂ϑ = −∂βω,ϑ(s ′)\n∂ϑ AΩ(s\n′, ω)\n+ ∑ ω′ ∑ s′′ ( (1− βω,ϑ(s′))1ω=ω′ + βω,ϑ(s′)πΩ (ω′ | s′) ) ∑ a πω′,θ (a | s′) γ P (s′′ | s′, a) ∂U(ω′, s′′) ∂ϑ\nUsing the structure of the augmented process:\n∂U(ω, s′)\n∂ϑ = −∂βω,ϑ(s\n′)\n∂ϑ AΩ(s ′, ω) + ∑ ω′ ∑ s′′ P(1)γ (s ′′, ω′ | s′, ω) ∂U(ω ′, s′′) ∂ϑ\n= − ∑ ω′,s′′ ∞∑ k=0 P(k)γ (s ′′, ω′ | s′, ω) ∂βω ′,ϑ(s ′′) ∂ϑ AΩ(s ′′, ω′) .\nWe finally obtain:\n∂U(ω0, s1)\n∂ϑ = − ∑ ω,s′ ∞∑ k=0 P(k)γ (s ′, ω | s1, ω0) ∂βω,ϑ(s ′) ∂ϑ AΩ(s ′, ω)\n= − ∑ ω,s′ µΩ(s ′, ω|s1, ω0) ∂βω,ϑ(s ′) ∂ϑ AΩ(s ′, ω) ."
    }, {
      "heading" : "Algorithm Outline",
      "text" : "We write α, αθ and αϑ for the learning rates of the critic, intra-option policies and termination functions respectively. The tabular setting is assumed only for clarity of presentation."
    }, {
      "heading" : "Four-Rooms Domain",
      "text" : "For all Boltzmann policies, we set the temperature parameter to 0.001. We also used the following methodology for selecting the learning rates and have a fair comparison across algorithms. We first chose the highest possible learning rate (0.5) for the SARSA agent which would be stable and have low variance. We would then use the same learning rate for the critic and perform a search over smaller values of the learning rates for the actor components: 0.5/(2k) for k = 1 to 5. We obtained 0.5 for the critic, 0.25 for the intra-option policy gradient, and 0.25 for the termination gradient."
    }, {
      "heading" : "Pinball",
      "text" : "In this domain, a drag coefficient of 0.995 effectively stops ball movements after a finite number of steps when the null action is chosen repeatedly. Each thrust action incurs a penalty of −5 while taking no action costs −1. The episode terminates with +10000 reward when the agent reaches the target. We interrupted any episode taking more than 10000 steps and set the discount factor to 0.99. We used Boltzmann policies for the intra-option policies and linear-sigmoid functions for the termination functions. The learning rates were set to of 0.01 for the critic and 0.001 for both the intra and termination gradients. We used an epsilon-greedy policy over options with = 0.01."
    }, {
      "heading" : "Arcade Learning Environments",
      "text" : "We used 32 convolutional filters of size 8 × 8 and stride of 4 in the first layer, 64 filters of size 4 × 4 with a stride of 2 in the second and 64 3 × 3 filters with a stride of 1 in the third layer. We fixed the learning rate for the intra-option policies and termination gradient to 0.00025 and used RMSProp or the critic.\nThe behavior specialization when using 2 options in the game of Seaquest is visible in Figure 9 showing that actions 10 (PLAYER A UPFIRE in ALE), is dominant in option 1 while action 13 (PLAYER A DOWNFIRE) only belongs to option 2. The distribution over the primitive actions is also complementary across the two options."
    } ],
    "references" : [ {
      "title" : "L",
      "author" : [ "Baird" ],
      "venue" : "C.",
      "citeRegEx" : "Baird 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "M",
      "author" : [ "Bellemare" ],
      "venue" : "G.; Naddaf, Y.; Veness, J.; and Bowling, M.",
      "citeRegEx" : "Bellemare et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Precup",
      "author" : [ "G. Comanici" ],
      "venue" : "D.",
      "citeRegEx" : "Comanici and Precup 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A",
      "author" : [ "O. Şimşek", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "Şimşek and Barto 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Probabilistic inference for determining options in reinforcement learning. Machine Learning, Special Issue 104(2):337–357",
      "author" : [ "Daniel" ],
      "venue" : null,
      "citeRegEx" : "Daniel,? \\Q2016\\E",
      "shortCiteRegEx" : "Daniel",
      "year" : 2016
    }, {
      "title" : "J",
      "author" : [ "V.R. Konda", "Tsitsiklis" ],
      "venue" : "N.",
      "citeRegEx" : "Konda and Tsitsiklis 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "and Barto",
      "author" : [ "G. Konidaris" ],
      "venue" : "A.",
      "citeRegEx" : "Konidaris and Barto 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A",
      "author" : [ "G. Konidaris", "S. Kuindersma", "R.A. Grupen", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "Konidaris et al. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
      "author" : [ "Kulkarni" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kulkarni,? \\Q2016\\E",
      "shortCiteRegEx" : "Kulkarni",
      "year" : 2016
    }, {
      "title" : "and Shimkin",
      "author" : [ "K.Y. Levy" ],
      "venue" : "N.",
      "citeRegEx" : "Levy and Shimkin 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "T",
      "author" : [ "Mankowitz, D.J.", "Mann" ],
      "venue" : "A.; and Mannor, S.",
      "citeRegEx" : "Mankowitz. Mann. and Mannor 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "D",
      "author" : [ "Mann, T.A.", "Mankowitz" ],
      "venue" : "J.; and Mannor, S.",
      "citeRegEx" : "Mann. Mankowitz. and Mannor 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A",
      "author" : [ "A. McGovern", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "McGovern and Barto 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Q-cut - dynamic discovery of sub-goals in reinforcement learning",
      "author" : [ "Mannor Menache", "I. Shimkin 2002] Menache", "S. Mannor", "N. Shimkin" ],
      "venue" : null,
      "citeRegEx" : "Menache et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Menache et al\\.",
      "year" : 2002
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "A. M" ],
      "venue" : "CoRR abs/1312.5602",
      "citeRegEx" : "M.,? \\Q2013\\E",
      "shortCiteRegEx" : "M.",
      "year" : 2013
    }, {
      "title" : "T",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "Lillicrap" ],
      "venue" : "P.; Harley, T.; Silver, D.; and Kavukcuoglu, K.",
      "citeRegEx" : "Mnih et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A",
      "author" : [ "S. Niekum", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "Niekum and Barto 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "M",
      "author" : [ "Puterman" ],
      "venue" : "L.",
      "citeRegEx" : "Puterman 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "and Ciosek",
      "author" : [ "D. Silver" ],
      "venue" : "K.",
      "citeRegEx" : "Silver and Ciosek 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "S",
      "author" : [ "J. Sorg", "Singh" ],
      "venue" : "P.",
      "citeRegEx" : "Sorg and Singh 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and Precup",
      "author" : [ "M. Stolle" ],
      "venue" : "D.",
      "citeRegEx" : "Stolle and Precup 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Y",
      "author" : [ "R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Mansour" ],
      "venue" : "2000. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12. 1057–",
      "citeRegEx" : "Sutton et al. 2000",
      "shortCiteRegEx" : null,
      "year" : 1063
    }, {
      "title" : "S",
      "author" : [ "R.S. Sutton", "D. Precup", "Singh" ],
      "venue" : "P.",
      "citeRegEx" : "Sutton. Precup. and Singh 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "R",
      "author" : [ "Sutton" ],
      "venue" : "S.",
      "citeRegEx" : "Sutton 1984",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Bias in natural actor-critic algorithms",
      "author" : [ "P. Thomas" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Thomas,? \\Q2014\\E",
      "shortCiteRegEx" : "Thomas",
      "year" : 2014
    }, {
      "title" : "A",
      "author" : [ "Vezhnevets" ],
      "venue" : "S.; Mnih, V.; Agapiou, J.; Osindero, S.; Graves, A.; Vinyals, O.; and Kavukcuoglu, K.",
      "citeRegEx" : "Vezhnevets et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "References [Baird 1993] Baird, L.",
      "startOffset" : 11,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "[Bellemare et al. 2013] Bellemare, M.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "[Comanici and Precup 2010] Comanici, G.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "[Şimşek and Barto 2009] Şimşek, O.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "[Konda and Tsitsiklis 2000] Konda, V.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "[Konidaris and Barto 2009] Konidaris, G.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "[Konidaris et al. 2011] Konidaris, G.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : "[Levy and Shimkin 2011] Levy, K.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "[Mankowitz, Mann, and Mannor 2016] Mankowitz, D.",
      "startOffset" : 0,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "[Mann, Mankowitz, and Mannor 2014] Mann, T.",
      "startOffset" : 0,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "[McGovern and Barto 2001] McGovern, A.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "[Mnih et al. 2016] Mnih, V.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 16,
      "context" : "[Niekum and Barto 2011] Niekum, S.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "[Puterman 1994] Puterman, M.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 18,
      "context" : "[Silver and Ciosek 2012] Silver, D.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "[Sorg and Singh 2010] Sorg, J.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "[Stolle and Precup 2002] Stolle, M.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 21,
      "context" : "[Sutton et al. 2000] Sutton, R.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "[Sutton, Precup, and Singh 1999] Sutton, R.",
      "startOffset" : 0,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "[Sutton 1984] Sutton, R.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 24,
      "context" : "[Thomas 2014] Thomas, P.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 25,
      "context" : "[Vezhnevets et al. 2016] Vezhnevets, A.",
      "startOffset" : 0,
      "endOffset" : 24
    } ],
    "year" : 2016,
    "abstractText" : "Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup & Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework. Introduction Temporal abstraction allows representing knowledge about course of actions that take place at different time scales. In reinforcement learning, options (Sutton, Precup, and Singh 1999; Precup 2000) provide a framework for defining such courses of action and for seamlessly learning and planning with them. Discovering temporal abstractions autonomously has been the subject of extensive research efforts in the last 15 years (McGovern and Barto 2001; Stolle and Precup 2002; Menache, Mannor, and Shimkin 2002; Şimşek and Barto 2009; Silver and Ciosek 2012), but approaches that can be used naturally with continuous state and/or action spaces have only recently started to become feasible (Konidaris et al. 2011; Niekum and Barto 2011; Mann, Mannor, and Precup ; Mankowitz, Mann, and Mannor 2016; Kulkarni et al. 2016; Vezhnevets et al. 2016; Daniel et al. 2016). The majority of the existing work has focused on finding subgoals (useful states that an agent should reach) and subsequently learning policies to achieve them. This idea has lead to interesting methods but ones which are also difficult to scale up given their “combinatorial” flavor. Additionally, learning policies associated with subgoals can be expensive in terms of data and computation time; in the worst case, it can be as expensive as solving the entire task. We present an alternative view, which blurs the line between the problem of discovering options from that of learning options. Based on the policy gradient theorem (Sutton et al. 2000), we derive new results which enable a gradual learning process of the intra-option policies and termination functions, simultaneously with the policy over them. This approach works naturally with both linear and non-linear function approximators, under discrete or continuous state and action spaces. Existing methods for learning options are considerably slower when learning from a single task: much of the benefit will come from re-using the learned options in similar tasks. In contrast, we show that our approach is capable of successfully learning options within a single task without incurring any slowdown and while still providing re-use speedups. We start by reviewing background related to the two main ingredients of our work: policy gradient methods and options. We then describe the core ideas of our approach: the intra-option policy and termination gradient theorems. Additional technical details are included in the appendix. We present experimental results showing that our approach learns meaningful temporally extended behaviors in an effective manner. As opposed to other methods, we only need to specify the number of desired options; it is not necessary to have subgoals, extra rewards, demonstrations, multiple problems or any other special accommodations (however, the approach can work with pseudo-reward functions if desired). To our knowledge, this is the first end-to-end approach for learning options that scales to very large domains at comparable efficiency. Preliminaries and Notation A Markov Decision Process consists of a set of states S, a set of actionsA, a transition function P : S×A → (S → [0, 1]) and a reward function r : S × A → R. For convenience, we develop our ideas assuming discrete state and action sets. However, our results extend to continuous spaces using usual measure-theoretic assumptions (some of our empirical results are in continuous tasks). A (Markovian stationary) policy is a probability distribution over actions conditioned on states, π : S × A → [0, 1]. In discounted problems, the value function of a policy π is defined as the expected return: Vπ(s) = Eπ [ ∑∞ t=0 γ rt+1 | s0 = s] and its action-value function as Qπ(s, a) = Eπ [ ∑∞ t=0 γ rt+1 | s0 = s, a0 = a], where γ ∈ [0, 1) is the discount factor. A policy π is greedy with respect to a given action-value function Q if π(s, a) > 0 iff a = argmaxa′ Q(s, a ′). In a discrete MDP, there is at least one optimal policy which is greedy with rear X iv :1 60 9. 05 14 0v 1 [ cs .A I] 1 6 Se p 20 16 spect to its own action-value function. Policy gradient methods (Sutton et al. 2000; Konda and Tsitsiklis 2000) address the problem of finding a good policy by performing stochastic gradient descent to optimize a performance objective over a given family of parametrized stochastic policies, πθ. The policy gradient theorem (Sutton et al. 2000) provides expressions for the gradient of the average reward and discounted reward objectives with respect to θ. In the discounted setting, the objective is defined with respect to a designated start state (or distribution) s0: ρ(θ, s0) = Eπθ [ ∑ t=0 γ rt+1 | s0]. The policy gradient theorem shows that: ∂ρ(θ,s0) ∂θ = ∑ s μπθ (s | s0) ∑ a ∂πθ(a|s) ∂θ Qπθ (s, a), where μπθ (s | s0) = ∑∞ t=0 γ t P (st = s | s0) is a discounted weighting of the states along the trajectories starting from s0. In practice, the policy gradient is estimated from samples along the on-policy stationary distribution. (Thomas 2014) showed that neglecting the discount factor in this stationary distribution makes the usual policy gradient estimator biased. However, correcting for this discrepancy also reduces data efficiency. For simplicity, we build on the framework of (Sutton et al. 2000) and discuss how to extend our results according to (Thomas 2014). The options framework (Sutton, Precup, and Singh 1999; Precup 2000) formalizes the idea of temporally extended actions. A Markovian option ω ∈ Ω is a triple (Iω, πω, βω) in which Iω ⊆ S is an initiation set, πω is an intra-option policy, and βω : S → [0, 1] is a termination function. We also assume that ∀s ∈ S,∀ω ∈ Ω : s ∈ Iω (i.e., all options are available everywhere), an assumption made in the majority of options discovery algorithms. We will discuss how to dispense with this assumption in the final section. (Sutton, Precup, and Singh 1999; Precup 2000) show that an MDP endowed with a set of options becomes a Semi-Markov Decision Process (Puterman 1994, chapter 11), which has a corresponding optimal value function over options VΩ(s) and option-value function QΩ(s, ω). Learning and planning algorithms for MDPs have their counterparts in this setting. However, the existence of the underlying MDP offers the possibility of learning about many different options in parallel : the idea of intra-option learning, which we leverage in our work. Learning Options We adopt a continual perspective on the problem of learning options. At any time, we would like to distill all of the available experience into every component of our system: value function and policy over options, intra-option policies and termination functions. To achieve this goal, we focus on learning option policies and termination functions, assuming they are represented using differentiable parameterized function approximators. We consider the call-and-return option execution model, in which an agent picks option ω according to its policy over options πΩ , then follows the intra-option policy πω until termination (as dictated by βω), at which point this procedure is repeated. Let πω,θ denote the intra-option policy of option ω parametrized by θ and βω,θ, the termination function of ω parameterized by θ. We present two new results for learning options, obtained using as blueprint the policy gradient theorem (Sutton et al. 2000). Both results are derived under the assumption that the goal is to learn options that maximize the expected return in the current task. However, if one wanted to add extra information to the objective function, this could readily be done so long as it comes in the form of an additive differentiable function. Suppose we aim to optimize directly the discounted return, expected over all the trajectories starting at a designated state s0 and option ω0, then: ρ(Ω, θ, θ, s0, ω0) = EΩ,θ,ω [ ∑∞ t=0 γ rt+1 | s0, ω0]. Note that this return depends on the policy over options, as well as the parameters of the option policies and termination functions. We will take gradients of this objective with respect to θ and θ. In order to do this, we will manipulate equations similar to those used in intra-option learning (Sutton, Precup, and Singh 1999, section 8). Specifically, the definition of the option-value function can be written as: QΩ(s, ω) = E Ω,θ,θ [ ∞ ∑ t=0 γrt+1 ∣∣∣∣ s0 = s, ω0 = ω ]",
    "creator" : "LaTeX with hyperref package"
  }
}