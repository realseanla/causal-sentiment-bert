{
  "name" : "1511.08456.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small Strategies in POMDPs",
    "authors" : [ "Krishnendu Chatterjee" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n08 45\n6v 1\n[ cs\n.A I]\n2 6"
    }, {
      "heading" : "1 Introduction",
      "text" : "The de facto model for dynamic systems with probabilistic and nondeterministic behavior are Markov decision processes (MDPs) [27]. MDPs provide the appropriate model to solve control and probabilistic planning problems [26, 39], where the nondeterminism represents the choice of the control actions for the controller (or planner), while the stochastic response of the system to control actions is represented by the probabilistic behavior. In perfect-observation (or perfect-information) MDPs, to resolve the nondeterministic choices among control actions the controller observes the current state of the system precisely, whereas in partially observable MDPs (POMDPs) the state space is partitioned according to observations that the controller can observe, i.e., the controller can only view the observation of the current state (the partition the state belongs to), but not the precise state [36]. POMDPs are widely used in several applications, such as in computational biology [23], speech processing [35], image processing [22], software verification [12], robot planning [31, 28], reinforcement learning [29], to name a few.\nReachability objectives and their computational problems. We consider POMDPs with one of the most basic and fundamental objectives, namely, reachability objectives. Given a set of target states, the reachability objective requires that some state in the\ntarget set is visited at least once. The main computational problems for POMDPs with reachability objectives are as follows: (a) the quantitative problem asks for the existence of a policy (that resolves the choice of control actions) that ensures the reachability objective with probability at least 0 < λ ≤ 1; and (b) the qualitative problem is the special case of the quantitative problem with λ = 1 (i.e., it asks that the objective is satisfied almost-surely).\nSignificance of qualitative problems. The qualitative problem is of great importance as in several applications it is required that the correct behavior happens with probability 1, e.g., in the analysis of randomized embedded schedulers, the important question is whether every thread progresses with probability 1. Also in applications where it might be sufficient that the correct behavior happens with probability at least λ < 1, the correct choice of the threshold λ can be still challenging, due to simplifications and imprecisions introduced during modeling. For example, in the analysis of randomized distributed algorithms it is common to require correctness with probability 1 (e.g., [38]). Finally, it has been shown recently [13] that for the important problem of minimizing the total expected cost to reach the target set [6, 10, 30] (under positive cost functions), it suffices to first compute the almost-sure winning set, and then apply any finite-horizon algorithm for approximation. Besides its importance in practical applications, almost-sure convergence, like convergence in expectation, is a fundamental concept in probability theory, and provides the strongest probabilistic guarantee [24].\nPrevious results. The quantitative analysis problem for POMDPs with reachability objectives is undecidable [37] (and the undecidability result even holds for any approximation [33]). In contrast, the qualitative analysis problem is EXPTIME-complete [15, 2]. The main algorithmic idea to solve the qualitative problem (that originates from [16]) is as follows: first construct the belief-support MDP explicitly (which is an exponential size perfect-information MDP where every state is the support of a belief), and then solve the qualitative analysis on the perfect-information MDP. Solving the qualitative analysis problem on the resulting MDP can be done using any one of several known polynomial-time algorithms, which are based on discrete graph theoretic approaches [19, 18, 17]. This yields the EXPTIME upper bound for the qualitative analysis of POMDPs, and the EXPTIME lower bound has been established in [15].\nDrawbacks. There are two major drawbacks of the present solution for the qualitative problem for POMDPs with reachability objectives. First, the algorithm requires to explicitly construct an exponential-size MDP, and there is no symbolic algorithm (that avoids the explicit construction) for the problem. Second, even though in practice a small amount of memory in policies might suffice, the construction of the beliefsupport MDP always searches for an exponential size policy (which is only required in the worst case). There is no algorithmic approach for small-memory policies for the problem.\nOur contributions. In this work our main contributions are as follows. First, we consider the qualitative analysis problem with respect to the special case of observationstationary (i.e., memoryless) policies. This problem is NP-complete. Motivated by the impressive performance of state-of-the-art SAT solvers in applications from AI as well as many other fields [7, 41, 8], we present an efficient reduction of our problem to SAT. This results in a practical, symbolic algorithm for the almost-sure reachability problem\nin POMDPs. We then show how our encoding to SAT can be extended to search for policies that use only a small amount of memory. Thus we present a symbolic SATbased algorithm that determines the existence of small-memory policies in POMDPs that can ensure that a target set is reached almost-surely. Our encoding is efficient: in the worst case it uses a quadratic number of variables and a cubic number of clauses, as compared to a naive encoding that uses a quartic (fourth power) number of clauses; and in practice our encoding uses just a linear number of variables and a quadratic number of clauses. Moreover, our encoding is incremental (it incrementally searches over lengths of paths), which may be further exploited by incremental SAT solvers (see Remark 1 for details). An important consequence of our result is that any improvement in SAT-solvers (improved solvers or parallel solvers), which is an active research area, carries over to the qualitative problem for POMDPs. We have implemented our approach and our experimental results show that our approach scales much better, and can solve large POMDP instances where the previous method fails.\nComparison with contingent or strong planning. We consider the qualitative analysis problem which is different as compared to strong or contingent planning [34, 21, 1]. The strong planning problem has been also considered under partial observation in [5, 40, 11]. The key difference of strong planning and qualitative analysis is as follows: in contingent planning the probabilistic aspect is treated as an adversary, whereas in qualitative analysis though the precise probabilities do not matter, still the probabilistic aspect needs to be considered. For a detailed discussion with illustrative examples see Appendix A.\nComparison with strong cyclic planning. The qualitative analysis problem is equivalent to the strong cyclic planning problem. The strong cyclic problem was studied in the perfect information setting in [21] and later extended to the partial information setting in [4]. However, there are two crucial differences of our work wrt [4]: (i) We consider the problem of finding small strategies as compared to general strategies. We show that our problem is NP-complete. In contrast, it is known that the qualitative analysis problem for POMDPs with general strategies is EXPTIME-complete [15, 2]. Thus the strong cyclic planning with general strategies considered in [4] is also EXPTIMEcomplete, whereas our problem is NP-complete. Thus there is a significant difference in the complexity of the problem we consider. (ii) The work of [4] presents a BDD-based implementation, whereas we present a SAT-based implementation. Note that since [4] considers an EXPTIME-complete problem in general there is no efficient reduction to SAT. (iii) Finally, the equivalence of strong cyclic planning and qualitative analysis of POMDPs imply that our results present an efficient SAT-based implementation to obtain small strategies in strong cyclic planning (also see Appendix B for a detailed discussion)."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Definition 1 POMDPs. A Partially Observable Markov Decision Process (POMDP) is defined as a tuple P = (S,A, δ,Z,O, I) where:\n• (i) S is a finite set of states; • (ii) A is a finite alphabet of actions;\n• (iii) δ : S ×A → D(S) is a probabilistic transition function that given a state s and an action a ∈ A gives the probability distribution over the successor states, i.e., δ(s, a)(s′) denotes the transition probability from s to s′ given action a; • (iv) Z is a finite set of observations; • (v) I ∈ S is the unique initial state; • (vi) O : S → Z is an observation function that maps every state to an ob-\nservation. For simplicity w.l.o.g. we consider that O is a deterministic function (see [14, Remark 1]).\nPlays and Cones. A play (or a path) in a POMDP is an infinite sequence (s0, a0, s1, a1, s2, a2, . . .) of states and actions such that s0 = I and for all i ≥ 0 we have δ(si, ai)(si+1) > 0. We write Ω for the set of all plays. For a finite prefix w ∈ (S · A)∗ · S of a play, we denote by Cone(w) the set of plays with w as the prefix (i.e., the cone or cylinder of the prefix w), and denote by Last(w) the last state of w. For a finite prefix w = (s0, a0, s1, a1, . . . , sn) we denote by O(w) = (O(s0), a0,O(s1), a1, . . . ,O(sn)) the observation and action sequence associated with w.\nStrategies (or policies). A strategy (or a policy) is a recipe to extend prefixes of plays and is a function σ : (S · A)∗ · S → D(A) that given a finite history (i.e., a finite prefix of a play) selects a probability distribution over the actions. Since we consider POMDPs, strategies are observation-based, i.e., for all histories w = (s0, a0, s1, a1, . . . , an−1, sn) and w′ = (s′0, a0, s ′ 1, a1, . . . , an−1, s ′ n) such that for all 0 ≤ i ≤ n we have O(si) = O(s′i) (i.e., O(w) = O(w ′)), we must have σ(w) = σ(w′). In other words, if the observation sequence is the same, then the strategy cannot distinguish between the prefixes and must play the same. Equivalently, we can define a POMDP strategy as a function σ : (Z · A)∗ · Z → D(A).\nObservation-Stationary (Memoryless) Strategies. A strategy σ is observationstationary (or memoryless) if it depends only on the current observation, i.e., whenever for two histories w and w′, we have O(Last(w)) = O(Last(w′)), then σ(w) = σ(w′). Therefore, a memoryless strategy is just a mapping from observations to a distribution over actions: σ : Z → D(A). We may also define a memoryless strategy as a mapping from states to distributions over actions (i.e., σ : S → D(A)), as long as σ(s) = σ(s′) for all states s, s′ ∈ S such that O(s) = O(s′). All three definitions are equivalent, so we will use whichever definition is most intuitive. We define the set of states that can be reached using a memoryless strategy recursively: I ∈ Rσ, and if s ∈ Rσ then s′ ∈ Rσ for all s′ such that there exists an action a where δ(s, a)(s′) > 0 and σ(s)(a) > 0. Let πk(s, s\n′) = (s1, a1, ..., sk) be a path of length k from s1 = s to sk = s′. We say that πk(s, s ′) is compatible with σ if σ(si)(ai) > 0 for all 1 ≤ i < k.\nStrategies with Memory. A strategy with memory is a tuple σ = (σu, σn,M,m0) where: (i) M is a finite set of memory states. (ii) The function σn : M → D(A) is the action selection function that given the current memory state gives the probability distribution over actions. (iii) The function σu : M ×Z × A → D(M) is the memory update function that given the current memory state, the current observation and action, updates the memory state probabilistically. (iv) The memory state m0 ∈ M is the initial memory state.\nProbability Measure. Given a strategy σ and a starting state I , the unique probability measure obtained given σ is denoted as PσI (·). We first define a measure ρ σ I (·) on cones. For w = I we have ρσI (Cone(w)) = 1, and for w = s ′ where I 6= s′ we have ρσI (Cone(w)) = 0; and for w ′ = w · a · s we have ρσI (Cone(w\n′)) = ρσI (Cone(w)) · σ(w)(a) · δ(Last(w), a)(s). By Carathéodory’s extension theorem, the function ρσI (·) can be uniquely extended to a probability measure PσI (·) over Borel sets of infinite plays [9].\nGiven a set of target states, the reachability objective requires that a target state is visited at least once.\nDefinition 2 Reachability Objective. Given a set T ⊆ S of target states, the reachability objective is Reach(T ) = {(s0, a0, s1, a1, ...) ∈ Ω|∃i ≥ 0 : si ∈ T }.\nIn the remainder of the paper, we assume that the set of target states contains a single goal state, i.e., T = {G} ⊆ S. We can assume this w.l.o.g. because it is always possible to add an additional state G with transitions from all target states in T to G.\nDefinition 3 Almost-Sure Winning. Given a POMDP P and a reachability objective Reach(T ), a strategy σ is almost-sure winning iff PσI (Reach(T )) = 1.\nIn the sequel, whenever we refer to a winning strategy, we mean an almost-sure winning strategy."
    }, {
      "heading" : "3 Almost-Sure Reachability with Memoryless Strategies",
      "text" : "In this section we present our results concerning the complexity of almost-sure reachability with memoryless strategies. First, we show that memoryless strategies for almostsure reachability take a simple form. The following proposition states that it does not matter with which positive probability an action is played.\nProposition 1 A POMDP P with a reachability objective Reach(T ) has a memoryless winning strategy if and only if it has a memoryless winning strategy σ such that for all a, a′ ∈ A and s ∈ S, if σ(s)(a) > 0 and σ(s)(a′) > 0 then σ(s)(a) = σ(s)(a′).\nIntuitively, σ only distinguishes between actions that must not be played, and therefore have probability 0, and those that may be played (having probabilities > 0). This proposition implies that we do not need to determine precise values for the positive probabilities when designing a winning strategy. In the following, we will for simplicity slightly abuse terminology: when we refer to a strategy or distribution as being uniform, we actually mean a distribution of this type.\nThe following result shows that determining whether there is a memoryless winning strategy reduces to finding finite paths from states to the target set.\nProposition 2 A memoryless strategy σ is a winning strategy if and only if for each state s ∈ Rσ, there is a path πk(s,G) compatible with σ, for some finite k ≤ |S|.\nIntuitively, the strategy must prevent the agent from reaching a state from which the target states can not be reached. It follows that determining whether there exists a memoryless, almost-sure winning strategy is in the complexity class NP. An NP-hardness result was established for a similar problem, namely, memoryless strategies in twoplayer games with partial-observation, in [20, Lemma 1]. The reduction constructed a game that is a DAG (directed acyclic graph), and replacing the adversarial player with a uniform distribution over choices shows that the almost-sure reachability problem under memoryless strategies in POMDPs is also NP-hard.\nTheorem 1 The problem of determining whether there exists a memoryless, almostsure winning strategy for a POMDP P and reachability objective Reach(T ) is NPcomplete.\nThe complexity of the almost-sure reachability problem for memoryless strategies suggests a possible approach to solve this problem in practice. We propose to find a memoryless winning strategy by encoding the problem as an instance of SAT, and then executing a state-of-the-art SAT solver to find a satisfying assignment or prove that no memoryless winning strategy exists."
    }, {
      "heading" : "3.1 SAT Encoding for Memoryless Strategies",
      "text" : "Next, we show how to encode the almost-sure reachability problem for memoryless strategies as a SAT problem. We will define a propositional formula Φk for an integer parameter k ∈ N, in Conjunctive Normal Form, such that Φk (for a sufficiently large k) is satisfiable if and only if the POMDP P has a memoryless, almost-sure winning strategy for reachability objective Reach(G).\nBy Propositions 1 and 2, we seek a function from states to subsets of actions, σ : S → P(A) (where P(A) is the powerset of actions) such that for each state s ∈ Rσ , there is a path πk(s,G) compatible with σ for some k ≤ |S|. The value of k will be a parameter of the SAT encoding. If we take k to be sufficiently large, e.g., k = |S| then one call to the SAT solver will be sufficient to determine if there exists a winning strategy. If k = |S| and the SAT solver determines that Φk is unsatisfiable, it will imply that there is no memoryless winning strategy.\nWe describe the CNF formula Φk by first defining all of its Boolean variables, followed by the clausal constraints over those variables.\nBoolean Variables. The Boolean variables of Φk belong to three groups, which are defined as follows:\n1. {Aij}, 1 ≤ i ≤ |S|, 1 ≤ j ≤ |A|. The Boolean variable Aij is the proposition that the probability of playing action j in state i is greater than zero, i.e., that σ(i)(j) > 0. 2. {Ci}, 1 ≤ i ≤ |S|. The Boolean variable Ci is the proposition that state i is reachable using σ (i.e, these variables define Rσ). 3. {Pij}, 1 ≤ i ≤ |S|, 0 ≤ j ≤ k. The Boolean variable Pij represents the proposition that from state i ∈ S there is a path to the goal of length at most j, that is compatible with the strategy.\nLogical Constraints. The following clause is defined for each i ∈ S, to ensure that at least one action is chosen in each state:\n∨\nj∈A\nAij\nTo ensure that the strategy is observation-based, it is necessary to ensure that if two states have the same observations, then the strategy behaves identically. This is achieved by adding the following constraint for all pairs of states i 6= j such that O(i) = O(j), and all actions r ∈ A:\nAir ⇐⇒ Ajr\nThe following clauses ensure that the {Ci} variables will be assigned True, for all states i that are reachable using the strategy defined by the {Aij} variables:\n¬Ci ∨ ¬Aij ∨ Cℓ\nSuch a clause is defined for each pair of states i, ℓ ∈ S and action j ∈ A for which δ(i, j)(ℓ) > 0. Furthermore, the initial state is reachable by the strategy, which is expressed by adding the single clause:\nCI\nWe introduce the following unit clauses, which say that from the goal state, the goal state is reachable using a path of length at most 0:\n(PG,j) for all 0 ≤ j ≤ k\nFor each state i ∈ S, we introduce the following clause that ensures if i is reachable, then there is a path from i to the goal that is compatible with the strategy.\n(¬Ci ∨ Pik)\nFinally, we use the following constraints to define the value of the {Pij} variables in terms of the chosen strategy.\nPij ⇐⇒ ∨\na∈A\n\nAia ∧\n\n\n∨\ni′∈S:δ(i,a)(i′)>0\nPi′,j−1\n\n\n\n\nThis constraint is defined for each i ∈ S, and 1 ≤ j ≤ k. We translate this constraint to clauses using the standard Tseitin encoding [45], which introduces additional variables in order to keep the size of the clausal encoding linear.\nThe conjunction of all clauses defined above forms the CNF formula Φk.\nTheorem 2 If Φk is satisfiable, for any k, then a memoryless winning strategy σ : S → D(A) can be extracted from the truth assignment to the variables {Aij}. If Φk is unsatisfiable for k = |S| then there is no memoryless winning strategy.\nThe number of variables in Φk is O(|S| · |A|+ |S| ·k), and the number of clauses is O(|S|2 · |A| ·k). Note that the number of actions, |A|, is usually a small constant, while the size of the state space, |S|, is typically large. The number of variables is quadratic in the size of the state space, while the number of clauses is cubic (recall k ≤ |S|).\nRemark 1 A naive SAT encoding would introduce a Boolean variable Xijℓ for each i, ℓ ∈ S, 1 ≤ j ≤ k, to represent the proposition that the jth state along a path from state i to the goal is ℓ ∈ S. However, using such variables to enforce the existence of paths from every reachable state to the goal, instead of the variables {Pij} which we used above, results in a formula with a cubic number of variables and a quartic (fourth power) number of clauses. Thus our encoding has the theoretical advantage of being considerably smaller than the naive encoding. Our encoding also offers two main practical advantages. First, it is possible to find a winning strategy, if one exists, using k ≪ |S|, by first generating Φk for small values of k. If the SAT solver finds Φk to be unsatisfiable, then we can increase the value of k and try again. Otherwise, if the formula is satisfiable, we have found a winning strategy and we can stop immediately. In this way, we are usually able to find a memoryless winning strategy (if one exists) very quickly, using only small values of k. So in practice, the size of Φk is actually only quadratic in |S|. Second, our encoding allows to take advantage of SAT solvers that offer an incremental interface, which supports the addition and removal of clauses between calls to the solver (though this is not exploited in our experimental results)."
    }, {
      "heading" : "4 Almost-Sure Reachability with Small-Memory Strategies",
      "text" : "For some POMDPs, a memoryless strategy that wins almost-surely may not exist. However, in some cases giving the agent a small amount of memory may help. We extend our SAT approach to the case of small-memory strategies in this section.\nDefinition 4 A small-memory strategy is a strategy with memory, σ = (σu, σn,M,m0), such that |M | = µ for some small constant µ.\nWe will refer to the number of memory states, µ, as the size of the small-memory strategy. Propositions 1 and 2 and Theorem 1 carry over to the case of small-memory strategies.\nProposition 3 A POMDP P with reachability objective Reach(T ) has a smallmemory winning strategy of size µ if and only if it has a small-memory winning strategy of size µ where both the action selection function and the memory update function are uniform.\nWe must modify the definition of a compatible path in the case of small-memory strategies, to also keep track of the sequence of memory states. Let πk(s,m, s′,m′) = (s1,m1, a1, ..., sk,mk, ak) be a finite sequence where s = s1, m = m1, s′ = sk and m′ = mk, and for all 1 ≤ i ≤ k, si ∈ S, mi ∈ M and ai ∈ A. Then we say that\nπk(s,m, s ′,m′) is a path compatible with small-memory strategy σ if for all 1 ≤ i < k, we have δ(si, ai)(si+1) > 0, σn(mi)(ai) > 0, and σu(mi,O(si), ai)(mi+1) > 0. Let Rσ be the set of all pairs (s,m) ∈ S ×M such that there exists a finite-length path πk(I,m0, s,m) that is compatible with σ.\nProposition 4 A small-memory strategy σ is winning if and only if for each (s,m) ∈ Rσ there is a path πk(s,m,G,m′) for some k ≤ |S| · |M | and some m′ ∈ M , that is compatible with σ.\nTheorem 3 The problem of determining whether there exists a winning, small-memory strategy of size µ, where µ is a constant, is NP-complete.\nTherefore, we may also find small-memory winning strategies using a SAT-based approach. We remark that Theorem 3 holds even if µ is polynomial in the size of the input POMDP."
    }, {
      "heading" : "4.1 SAT Encoding for Small-Memory Strategies",
      "text" : "The SAT encoding from Section 3.1 can be adapted for the purpose of finding smallmemory winning strategies. Given a POMDP P , reachability objective Reach(G), a finite set of memory states M of size µ, an initial memory state m0 ∈ M , and a path length k ≤ |S| · |M |, we define the CNF formula Φk,µ as follows.\nBoolean Variables. We begin by defining variables to encode the action selection function σn. We introduce a Boolean variable Ama for each memory-state m ∈ M and action a ∈ A, to represent that action a is among the possible actions that can be played by the strategy, given that the memory-state is m, i.e., that σn(m)(a) > 0.\nThe next set of Boolean variables encodes the memory update function. We introduce a Boolean variable Mm,z,a,m′ for each pair of memory-states m,m′ ∈ M , observation z ∈ Z and action a ∈ A. If such a variable is assigned to True, it indicates that if the current memory-state is m, the current observation is z, and action a is played, then it is possible that the new memory-state is m′, i.e., σu(m, z, a)(m′) > 0.\nSimilarly to the memoryless case, we also introduce the Boolean variables Ci,m for each state i ∈ S and memory state m ∈ M , that indicate which (state, memory-state) pairs are reachable by the strategy.\nWe define variables {Pi,m,j} for all i ∈ S, m ∈ M , and 0 ≤ j ≤ k, similarly to the memoryless case. The variable Pi,m,j corresponds to the proposition that there is a path of length at most j from (i,m) to the goal, that is compatible with the strategy.\nLogical Constraints. We introduce the following clause for each m ∈ M , to ensure that at least one action is chosen for each memory state:\n∨\nj∈A\nAmj\nTo ensure that the memory update function is well-defined, we introduce the following clause for each m ∈ M , a ∈ A and z ∈ Z .\n∨\nm′∈M\nMm,z,a,m′\nThe following clauses ensure that the {Ci,m} variables will be assigned True, for all pairs (i,m) that are reachable using the strategy.\n¬Ci,m ∨ ¬Am,a ∨ ¬Mm,z,a,m′ ∨ Cj,m′\nSuch a clause is defined for each pair of memory-states m,m′ ∈ M , each pair of states i, j ∈ S, each observation z ∈ Z , and each action a ∈ A, such that δ(i, a)(j) > 0 and z = O(j).\nClearly, the initial state and initial memory state are reachable. This is enforced by adding the single clause:\n(CI,m0)\nWe introduce the following unit clause for each m ∈ M and 0 ≤ j ≤ k, which says that the goal state with any memory-state is reachable from the goal state and that memory-state, using a path of length at most 0:\n(PG,m,j)\nNext, we define the following binary clause for each i ∈ S and m ∈ M , so that if the (state, memory-state) pair (i,m) is reachable, then the existence of a path from (i,m) to the goal is enforced.\n¬Ci,m ∨ Pi,m,k\nFinally, we use the following constraints to define the value of the Pi,m,j variables in terms of the chosen strategy.\nPi,m,j ⇐⇒\n∨\na∈A\n\n      Ama ∧\n\n     \n∨\nm′∈M,z∈Z, i′∈S:δ(i,a)(i′)>0\nand O(i′)=z\n[Mm,z,a,m′ ∧ Pi′,m′,j−1]\n\n     \n\n     \nThis constraint is defined for each i ∈ S, m ∈ M and 1 ≤ j ≤ k. We use the standard Tseitin encoding to translate this formula to clauses. The conjunction of all clauses defined above forms the CNF formula Φk,µ.\nTheorem 4 If Φk,µ is satisfiable then there is a winning, small-memory strategy of size µ, and such a strategy is defined by the truth assignment to the {Ama} and {Mm,z,a,m′} variables. If Φk,µ is unsatisfiable, and k ≥ |S| · µ, then there is no small-memory strategy of size µ that is winning.\nThe number of variables in Φk,µ is O(|S| · µ · k + µ2 · |Z| · |A|). The number of clauses is O(|S|2 · µ2 · |Z| · |A| · k). The number of actions, |A|, and the number of observations, |Z|, are usually constants. We also expect that the number of memory states, µ, is small. Since k ≤ |S| · µ, the number of variables is quadratic and the number of clauses is cubic in the size of the state space, as for memoryless strategies.\nThe comments in Remark 1 also carry over to the small-memory case. In practice, we can often find a winning strategy with small values for k and µ (see Section 5).\nRemark 2 Our encoding can be naturally extended to search for deterministic strategies, for details see Appendix C."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In this section we present our experimental results, which show that small-memory winning strategies do exist for several realistic POMDPs that arise in practice. Our experimental results clearly demonstrate the scalability of our SAT-based approach, which yields good performance even for POMDPs with large state spaces, where the previous explicit approach performs poorly.\nWe have implemented the encoding for small-memory strategies, described in Section 4.1, as a small python program. We compare against the explicit graph-based algorithm presented in [14]. This is the state-of-the-art explicit POMDP solver for almostsure reachability based on path-finding algorithms of [18] with a number of heuristics. We used the SAT solver Minisat, version 2.2.0 [25]. The experiments were conducted on a Intel(R) Xeon(R) @ 3.50GHz with a 30 minute timeout. We do not report the time taken to generate the encoding using our python script, because it runs in polynomial time, and more efficient implementations can easily be developed. Also, we do not exploit incremental SAT in our experimental results (this will be part of future work). We consider several POMDPs that are similar to well-known benchmarks. We generated several instances of each POMDP, of different sizes, in order to test the scalability of our algorithm.\nHallway POMDPs. We considered a family of POMDP instances, inspired by the Hallway problem introduced in [32] and used later in [43, 42, 10, 14]. In the Hallway POMDPs, a robot navigates on a rectangular grid. The grid has barriers where the robot cannot move, as well as trap locations that destroy the robot. The robot must reach a specified goal location. The robot has three actions: move forward, turn left, and turn right. The robot can see whether there are barriers around its grid cell, so there are two observations (wall or no wall) for each direction. The actions may all fail, in which case the robot’s state remains the same. The state is therefore comprised of the robot’s location in the grid, and its orientation. Initially, the robot is randomly located somewhere within a designated subset of grid locations, and the robot is oriented to the south (the goal is also to the south). We generated several Hallway instances, of sizes shown in Table 1. The runtimes for the SAT-based approach and the explicit approach are also given in the table. Timeouts (of 30 minutes) are indicated by “-”. In all cases, the number of memory states required for there to be a winning strategy is 2. Therefore, the runtimes reported for µ = 1 correspond to the time required by the SAT solver to prove that Φk,µ is unsatisfiable, while runs where µ = 2 resulted in the SAT solver finding a solution. We set k to a sufficiently large value by inspection of the POMDP instance.\nEscape POMDPs. The problem is based on a case study published in [44], where the goal is to compute a strategy to control a robot in an uncertain environment. Here, a robot is navigating on a square grid. There is an agent moving around the grid, and the robot must avoid being captured by the agent, forever. The robot has four actions: move north, move south, move east, move west. These actions have deterministic effects, i.e.,\nthey always succeed. The robot can observe whether or not there are barriers in each direction, and it can also observe the position of the agent if the agent is currently on an adjacent cell. The agent moves randomly. We generated several instances of the Escape POMDPs, of sizes shown in Table 2. The runtimes for the SAT-based approach and the explicit approach are also given in the table, with timeouts indicated by “-”. The number of memory states was set to µ = 5, which is sufficient for there to be a small-memory winning strategy. For these POMDPs, there is always a path directly to the goal state, so setting k = 2 was sufficient to find a winning strategy. In order to prove that there is no smaller winning strategy, we increased k to 8 = 2 × µ, where µ = 4. The runtimes for the resulting unsatisfiable formulas are also shown in Table 2.\nRockSample POMDPs. We consider a variant of the RockSample problem introduced in [42] and used later in [10, 14]. The RockSample instances model rover science exploration. The positions of the rover and the rocks are known, but only some of the rocks have a scientific value; we will call these rocks good. The type of the rock is not known to the rover, until the rock site is visited. Whenever a bad rock is sampled the rover is destroyed and a losing absorbing state is reached. If a sampled rock is sampled for the second time, then with probability 0.5 the action has no effect. With the remaining probability the sample is destroyed and the rock needs to be sampled one more time. An instance of the RockSample problem is parametrized with a parameter [n]: n is the number of rocks on a grid of size 3×3. The goal of the rover is to obtain two samples of good rocks. In this problem we have set µ = 2 and k = 8, which is sufficient to find a winning strategy for each instance. However, memoryless strategies are not sufficient as in some situations sampling is prohibited whereas in other situations it is required (hence we did not consider µ = 1). The results are presented in Table 3.\nRemark 3 In the unsatisfiable (UNSAT) results of the Hallway and Escape POMDPs, we have computed, based on the diameter of the underlying graph and the number of memory elements µ, a sufficiently large k to disprove the existence of an almost-sure\nwinning strategy of the considered memory size. It follows, that there is no memoryless strategy for the Hallway POMDPs, and no almost-sure winning strategy for the Escape POMDPs, that uses only 4 memory elements.\nMemory requirements. In all runs of the Minisat solver, at most 5.6 GB of memory was used. The runs of the explicit solver consumed around 30 GB of memory at the timeout."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this work we present the first symbolic SAT-based algorithm for almost-sure reachability in POMDPs. We have illustrated that the symbolic algorithm significantly outperforms the explicit algorithm, on a number of examples similar to problems from the literature. In future work we plan to investigate the possibilities of incremental SAT solving. Incremental SAT solvers can be beneficial in two ways: First, they may im-\nprove the efficiency of algorithms to find the smallest almost-sure winning strategy. Such an approach can be built on top of our encoding. Second, incremental SAT solving could help in the case that the original POMDP is modified slightly, in order to efficiently solve the updated SAT instance. Investigating the practical impact of incremental SAT solvers for POMDPs is the subject of future work."
    }, {
      "heading" : "A Appendix: Detailed comparison of qualitative analysis and contingent planning.",
      "text" : "We present examples that distinguish almost-sure winning from contingent planning. We first explain the conceptual difference and then illustrate the difference with examples. In the contingent planning setting it is required that all paths reach the goal state. In other words, contingent planning treats the probabilistic choice as an adversarial choice. In almost-sure winning, although it is true that the precise probabilities do not matter, it is still different than treating the probabilistic choice as adversarial. We first illustrate the difference with examples of Markov chains.\nExample 1 (Markov chains.) In Figure 1 we depict a Markov Chain M1 (which is a perfect-information MDP with a single action) with two states: the initial state s0 and the goal state G. The probabilistic transition function in state s0 selects the next state to be s0 with probability 12 , and the goal state G with the remaining probability 1 2 . In this example, the probability to reach G in n steps is ∑n\ni=1( 1 2 ) i. Since we consider the infinite-horizon setting, by taking the limit of n to ∞ we obtain that the probability of eventually reaching the goal state G is 1, i.e,\nlim n→∞\nn ∑\ni=1\n(\n1\n2\n)i\n= lim n→∞\n(1− 1\n2n ) = 1\nHence in this example, the goal state is reached almost-surely (with probability 1). However, in the contingent planning setting, there is no plan, since there exists a path that stays in s0 forever (namely, sω0 ) that does not reach the goal state G. Hence the answers are different: the answer to almost-sure winning is YES, whereas the answer to contingent planning is NO. Note that in the Markov chain example, if the probabilities change from (12 , 1 2 ) to ( 2 3 , 1 3 ) or ( 3 4 , 1 4 ) the answer to the almost-sure winning still remains same (the probability to reach within n steps changes to (1 − (23 ) n) and (1− (34 ) n), respectively, however the limits are still 1). For almost-sure winning the precise probability values of transitions do not matter, nevertheless this is still different from treating the probabilistic choices as adversarial choice (as considered in contingent planning).\nNext we consider the example M2 shown in Figure 2, where from s0, the next state is one of the three states L, G, and s0, with probabilities 13 for each. Here, the answer to both the almost-sure winning and contingent planning questions is NO. However, the path sω0 that stays in s0 forever is a witness to show that the answer to the contingent planning problem is NO, but the same witness is not valid for almost-sure winning. In\nother words, even when the answers to almost-sure winning and contingent planning are the same, a witness to show that the answer for contingent planning is NO, is not necessarily a witness to show the answer to almost-sure winning is NO.\nExample 2 (Perfect Information MDPs.) We now illustrate the situation in a perfectinformation MDP. Note that the situation would be only more complicated in the general POMDP setting. Consider the MDP shown in Figure 3. The initial state is s0, and there are two actions available at s0, to either go to state V with action a or to state U with action b. In state V , with probability 13 the next state is U , s0, or G (irrespective of the actions). In state U , with probability 12 the next state is U or s0 (irrespective of the actions). In this example, a strategy that always chooses action a at s0 is an almost-sure winning strategy, but there is no strategy that ensures that the answer to the contingent planning problem is YES. Also note that in this example, a strategy that always chooses b at s0 is not an almost-sure winning strategy. Thus even when almost-sure winning strategies exist, not all strategies are almost-sure winning.\nIn summary, the above examples illustrate the following: 1. The answer to the almost-sure winning question on a given input can be very dif-\nferent from contingent planning, even in very special cases of POMDPs, namely, in perfect-information Markov chains. 2. Even in cases when the answer to the almost-sure winning and contingent planning questions is the same, not every witness for the contingent planning problem is a valid witness for the almost-sure winning problem (even for Markov chains). 3. In case of perfect-information MDPs, the almost-sure winning strategy construction can be quite involved, and different from contingent planning. The key difference of the two setting is as follows: in the contingent planning since the requirement is for all paths, this effectively means treating the probabilistic choice as\nan adversary. For almost-sure winning, if a probabilistic choice is available infinitely often, then it must be chosen infinitely often. Note that almost-sure winning is the classical probability theory counterpart of almost-sure convergence, which is the strongest probabilistic guarantee, yet it does not require convergence for all points."
    }, {
      "heading" : "B Appendix: Detailed comparison of qualitative analysis and strong cyclic planning.",
      "text" : "The qualitative analysis problem is equivalent to the strong cyclic planning problem. The strong cyclic problem was studied in the perfect information setting in [21] and later extended to the partial information setting in [4]. However, there are two crucial differences of our work wrt [4]:\n1. We consider the problem of finding small strategies as compared to general strategies. We show that our problem is NP-complete. In contrast, it is known that the qualitative analysis problem for POMDPs with general strategies is EXPTIMEcomplete [15, 2]. Thus the strong cyclic planning with general strategies considered in [4] is also EXPTIME-complete, whereas we establish that our problem is NP-complete. Thus there is a significant difference in the complexity of the problem finding small strategies as compared to general strategies.\n2. The work of [4] presents a BDD-based implementation, whereas we present a SAT-based implementation. Note that since [4] considers an EXPTIMEcomplete problem in general there is no efficient (polynomial-time) reduction to SAT. In contrast not only we show that our problem is NP-complete, we present an efficient (cubic for constant-size memory) reduction to SAT.\nTo the best of our knowledge, there is no publicly available implementation for strong cyclic planning under partial observation.\nSignificance of our result. Finally, the equivalence of strong cyclic planning and qualitative analysis of POMDPs implies a greater significance of our result. First, our results become applicable also for strong cyclic planning. Second, our approach gives a way to compute small strategies (if they exist) for strong cyclic planning. Finally, we present\nan an efficient SAT-based implementation to obtain small strategies in strong cyclic planning. Previous works consider BDD-based approach to compute general strategies. Developing fast SAT-solvers (or incremental SAT-solvers) is an active research area, and our results imply that faster solvers for SAT can then be used both for qualitative analysis of POMDPs as well as for strong cyclic planning, for computing small strategies when they exist. In Table 4 we present the comparison of the existing approaches for strong planning and strong cyclic planning for MDPs (perfect-information setting) as well as POMDPs. Note that as described in the previous section, the strong planning problem is different from the qualitative analysis of POMDPs (or strong cyclic planning). In the perfect-information setting there exists SAT-based solvers both for strong planning as well as strong cyclic planning, whereas for general strategies in POMDPs there exists no SAT-based implementation. We present the first SAT-based implementation to compute small strategies for strong cyclic planning in the partial information setting."
    }, {
      "heading" : "C Appendix: Deterministic Strategies",
      "text" : "In this part we present a simple extension of our encoding that handles the case for deterministic strategies.\nDeterministic Strategies. A strategy with memory σ = (σu, σn,M,m0) is deterministic if both functions σn and σu assign only Dirac probability distributions and can be written as:\n• The action selection function is of type σn : M → A.\n• The memory update function is of type σu : M ×Z ×A → M .\nWe will present the modification only for the more complicated case of smallmemory strategies, the modifications for the memoryless case are analogous.\nNext-action Selection Function. The part of the encoding that codes for the next-action selection function σn is:\n∨\nj∈A\nAmj\nIt ensures for every m ∈ M , that at least one variable Amj is set to true, i.e., at least one action is chosen. In a deterministic strategy the requirements are stronger, it is required that exactly one action is chosen. This can be enforced by adding the following clause for every memory element m, and two distinct actions i, j:\nAmi ⊕Amj\nMemory Update Function. The part of the encoding that codes for the memory update function σu is:\n∨\nm′∈M\nMm,z,a,m′\nAs in the previous case, it is sufficient to add the following clause for every memory element m, observation z, action a, and two distinct memory element m′ and m”:\nMm,z,a,m′ ⊕Mm,z,a,m”"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "<lb>POMDPs are standard models for probabilistic planning problems, where an<lb>agent interacts with an uncertain environment. We study the problem of almost-<lb>sure reachability, where given a set of target states, the question is to decide<lb>whether there is a policy to ensure that the target set is reached with probability<lb>1 (almost-surely). While in general the problem is EXPTIME-complete, in many<lb>practical cases policies with a small amount of memory suffice. Moreover, the<lb>existing solution to the problem is explicit, which first requires to construct explic-<lb>itly an exponential reduction to a belief-support MDP. In this work, we first study<lb>the existence of observation-stationary strategies, which is NP-complete, and then<lb>small-memory strategies. We present a symbolic algorithm by an efficient encod-<lb>ing to SAT and using a SAT solver for the problem. We report experimental results<lb>demonstrating the scalability of our symbolic (SAT-based) approach.",
    "creator" : "LaTeX with hyperref package"
  }
}