{
  "name" : "1703.08041.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Palash Dey" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n08 04\n1v 1\n[ cs\n.D S]\n2 3\nM ar\n2 01\n7\nResolving the Complexity of Some Fundamental Problems\nin Computational Social Choice"
    }, {
      "heading" : "A THESIS",
      "text" : "SUBMITTED FOR THE DEGREE OF\nDoctor of Philosophy"
    }, {
      "heading" : "IN THE",
      "text" : "Faculty of Engineering\nBY\nPalash Dey\nComputer Science and Automation\nIndian Institute of Science\nBangalore – 560 012 (INDIA)\nAugust 2016\nc© Palash Dey\nAugust 2016\nAll rights reserved\nDEDICATED TO\nMy teachers"
    }, {
      "heading" : "Acknowledgements",
      "text" : "I am grateful to my Ph.D. advisors Prof. Y. Narahari and Prof. Arnab Bhattacharyya for providing me a golden opportunity to work with them. I thank them for the all encompassing support they ushered upon me throughout my Ph.D. I convey my special thanks to Prof. Y. Narahari for all his academic and nonacademic supports which made my Ph.D. life very easy and enjoyable. I have throughly enjoyed working with Prof. Arnab Bhattacharyya. I convey my very special thanks to Prof. Neeldhara Misra for many excellent and successful research collaborations, useful discussions, and playing an all encompassing super-active role in my Ph.D. life. Prof. Neeldhara Misra was always my constant source of inspiration and she guided me throughout my Ph.D. as a supervisor, advisor, friend, and elder sister. I am indebted to Prof. Dilip P. Patil for teaching me how the practice of writing statements rigorously and precisely in mathematics automatically clarifies many doubts. I am extremely grateful to have a teacher like him at the very early stage of my stay in IISc who not only significantly improved my mathematical thinking process but also acted like a friend, philosopher, and guide throughout my stay in IISc. I throughly enjoyed all the courses that he taught during my stay in IISc and learned a lot from those courses. Specially his courses on linear algebra, Galois theory, commutative algebra have immensely influenced my thinking process.\nI thank Prof. Sunil Chandran for teaching me the art of proving results in graph theory in particular and theoretical computer science in general. I am grateful to Dr. Deeparnab Chakrabarty and Prof. Arnab Bhattacharyya for teaching me approximation algorithms and randomized algorithms, Prof. Manjunath Krishnapur for teaching me probability theory and martingales, Prof. Chandan Saha for teaching me complexity theory and algebraic geometry, Prof. Y. Narahari for teaching me game theory. I thank all the faculty members of Department of Computer Science and Automation (CSA) for all of their supports. I convey my special thank to all the staffs of CSA specially Ms. Suguna, Ms. Meenakshi, Ms. Kushael for their helping hands. I thank Prof. Ashish Goel for giving me an excellent opportunity to work with\ni\nAcknowledgements\nhim and for hosting me in Stanford University for three months. I thank Dr. David Woodruff for collaborating with me on important problems.\nI feel myself extremely lucky to get such a wonderful and friendly lab members in the Game Theory lab. I want to thank all the members of the Game Theory lab with special mention to Dr. Rohith D. Vallam, Satyanath Bhat, Ganesh Ghalme, Shweta Jain, Swapnil Dhamal, Divya Padmanabhan, Dr. Pankaj Dayama, Praful Chandra, Shourya Roy, Tirumala Nathan, Debmalya Mandal, Arupratan Ray, and Arpita Biswas. I want to thank Dr. Minati De, Abhiruk Lahiri, Achintya Kundu, Aniket Basu Roy, Prabhu Chandran, Prasenjit Karmakar, R Kabaleeshwaran, Rohit Vaish, Sayantan Mukherjee, Srinivas Karthik, Suprovat Ghoshal, Vineet Nair for all the important discussions. I greatly acknowledge support from Ratul Ray and Uddipta Maity during my internship in Stanford University.\nI thank Google India for providing me fellowship during second and third years of my Ph.D. I acknowledge financial support from Ministry of Human Resource Development (MHRD) in India during my first year of Ph.D.\nLast but not the least, I express my gratitude to my parents, brother, wife, and all my\nfriends for their love, blessing, encouragement, and support.\nii\nAbstract\nIn many real world situations, especially involving multiagent systems and artificial intelligence, participating agents often need to agree upon a common alternative even if they have differing preferences over the available alternatives. Voting is one of the tools of choice in these situations. Common and classic applications of voting in modern applications include collaborative filtering and recommender systems, metasearch engines, coordination and planning among multiple automated agents etc. Agents in these applications usually have computational power at their disposal. This makes the study of computational aspects of voting crucial. This thesis is devoted to a study of computational complexity of several fundamental algorithmic and complexity-theoretic problems arising in the context of voting theory.\nThe typical setting for our work is an “election”; an election consists of a set of voters or agents, a set of alternatives, and a voting rule. The vote of any agent can be thought of as a ranking (more precisely, a complete order) of the set of alternatives. A voting profile comprises a collection of votes of all the agents. Finally, a voting rule is a mapping that takes as input a voting profile and outputs an alternative, which is called the “winner” or “outcome” of the election. Our contributions in this thesis can be categorized into three parts and are described below.\nPart I: Preference Elicitation. In the first part of the thesis, we study the problem of eliciting the preferences of a set of voters by asking a small number of comparison queries (such as who a voter prefers between two given alternatives) for various interesting domains of preferences.\nWe commence with considering the domain of single peaked preferences on trees in Chapter 3. This domain is a significant generalization of the classical well studied domain of single peaked preferences. The domain of single peaked preferences and its generalizations are\niii\nAbstract\nhugely popular among political and social scientists. We show tight dependencies between query complexity of preference elicitation and various parameters of the single peaked tree, for example, number of leaves, diameter, path width, maximum degree of a node etc.\nWe next consider preference elicitation for the domain of single crossing preference profiles in Chapter 4. This domain has also been studied extensively by political scientists, social choice theorists, and computer scientists. We establish that the query complexity of preference elicitation in this domain crucially depends on how the votes are accessed and on whether or not any single crossing ordering is a priori known.\nPart II: Winner Determination. In the second part of the thesis, we undertake a study of the computational complexity of several important problems related to determining winner of an election.\nWe begin with a study of the following problem: Given an election, predict the winner of the election under some fixed voting rule by sampling as few votes as possible. We establish optimal or almost optimal bounds on the number of votes that one needs to sample for many commonly used voting rules when the margin of victory is at least εn (n is the number of voters and ε is a parameter). We next study efficient sampling based algorithms for estimating the margin of victory of a given election for many common voting rules. The margin of victory of an election is a useful measure that captures the robustness of an election outcome. The above two works are presented in Chapter 5.\nIn Chapter 6, we design an optimal algorithm for determining the plurality winner of an election when the votes are arriving one-by-one in a streaming fashion. This resolves an intriguing question on finding heavy hitters in a stream of items, that has remained open for more than 35 years in the data stream literature. We also provide near optimal algorithms for determining the winner of a stream of votes for other popular voting rules, for example, veto, Borda, maximin etc.\nVoters’ preferences are often partial orders instead of complete orders. This is known as the incomplete information setting in computational social choice theory. In an incomplete information setting, an extension of the winner determination problem which has been studied extensively is the problem of determining possible winners. We study the kernelization complexity (under the complexity-theoretic framework of parameterized complexity) of the possible winner problem in Chapter 7. We show that there do not exist kernels of size that is polynomial in the number of alternatives for this problem for commonly used voting rules\niv\nAbstract\nunder a plausible complexity theoretic assumption. However, we also show that the problem of coalitional manipulation which is an important special case of the possible winner problem admits a kernel whose size is polynomially bounded in the number of alternatives for common voting rules.\nPart III: Election Control. In the final part of the thesis, we study the computational complexity of various interesting aspects of strategic behavior in voting.\nFirst, we consider the impact of partial information in the context of strategic manipulation in Chapter 8. We show that lack of complete information makes the computational problem of manipulation intractable for many commonly used voting rules.\nIn Chapter 9, we initiate the study of the computational problem of detecting possible instances of election manipulation. We show that detecting manipulation may be computationally easy under certain scenarios even when manipulation is intractable.\nThe computational problem of bribery is an extensively studied problem in computational social choice theory. We study computational complexity of bribery when the briber is “frugal” in nature. We show for many common voting rules that the bribery problem remains intractable even when the briber’s behavior is restricted to be frugal, thereby strengthening the intractability results from the literature. This forms the subject of Chapter 10.\nv\nPublications based on this Thesis\n1. Palash Dey, Neeldhara Misra, and Yadati Narahari. “Kernelization Complexity of Possible\nWinner and Coalitional Manipulation Problems in Voting”. In Theoretical Computer Science, volume 616, pages 111-125, February 2016. Remark: A preliminary version of this work was presented in the 14th International Conference on Autonomous Systems and Multiagent Systems (AAMAS-15), 2015.\n2. Palash Dey, Neeldhara Misra, and Yadati Narahari. “Frugal Bribery in Voting”. Accepted\nin Theoretical Computer Science, March 2017. Remark: A preliminary version of this work was presented in the 30th AAAI Conference on Artificial Intelligence (AAAI-16), 2016.\n3. Arnab Bhattacharyya, Palash Dey, and David P. Woodruff. “An Optimal Algorithm for ℓ1-\nHeavy Hitters in Insertion Streams and Related Problems”. Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS-16), pages 385-400, San Francisco, USA, 26 June - 1 July, 2016.\n4. Palash Dey and Neeldhara Misra. “Elicitation for Preferences Single Peaked on Trees”.\nProceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI16), pages 215-221, New York, USA, 9-15 July, 2016.\n5. Palash Dey and Neeldhara Misra. “Preference Elicitation For Single Crossing Domain”.\nProceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI16), pages 222-228, New York, USA, 9-15 July, 2016.\n6. Palash Dey, Neeldhara Misra, and Yadati Narahari. “Complexity of Manipulation with\nPartial Information in Voting”. Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16), pages 229-235, New York, USA, 9-15 July, 2016.\nvii\nPublications based on this Thesis\n7. Palash Dey, Neeldhara Misra, and Yadati Narahari. “Frugal Bribery in Voting”. Pro-\nceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16), vol. 4, pages 2466-2672, Phoenix, Arizona, USA, 2016.\n8. Palash Dey and Yadati Narahari. “Estimating the Margin of Victory of Elections using Sam-\npling”. Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI-15), pages 1120-1126, Buenos Aires, Argentina, 2015.\n9. Palash Dey and Arnab Bhattacharyya. “Sample Complexity for Winner Prediction in Elec-\ntions”. Proceedings of the 14th International Conference on Autonomous Systems and Multiagent Systems (AAMAS-15), pages 1421-1430, Istanbul, Turkey, 2015.\n10. Palash Dey, Neeldhara Misra, and Yadati Narahari. “Detecting Possible Manipulators in\nElections”. Proceedings of the 14th International Conference on Autonomous Systems and Multiagent Systems(AAMAS-15), pages 1441-1450, Istanbul, Turkey, 2015.\n11. Palash Dey, Neeldhara Misra, and Yadati Narahari “Kernelization Complexity of Possible\nWinner and Coalitional Manipulation Problems in Voting”. Proceedings of the 14th International Conference on Autonomous Systems and Multiagent Systems (AAMAS-15), pages 87-96, Istanbul, Turkey, 2015.\nviii\nContents\nAcknowledgements i\nAbstract iii\nPublications based on this Thesis vii\nContents ix\nList of Figures xv\nList of Tables xvi"
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Important Aspects of Voting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Applications and Challenges of Voting Theory Relevant to Computer Science . 4 1.3 Thesis Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3.1 Part I: Preference Elicitation . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3.2 Part II: Winner Determination . . . . . . . . . . . . . . . . . . . . . . . 7 1.3.3 Part III: Election Control . . . . . . . . . . . . . . . . . . . . . . . . . . 9"
    }, {
      "heading" : "2 Background 13",
      "text" : "2.1 Voting and Elections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.1.1 Basic Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.2 Axioms of Voting Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.1.3 Majority Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.1.4 Condorcet Paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.1.5 Incomplete Information Setting . . . . . . . . . . . . . . . . . . . . . . 16\nix\nCONTENTS\n2.1.6 Voting Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.2 Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.2.1 Parameterized Complexity . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2.2 Approximation Factor of an Algorithm . . . . . . . . . . . . . . . . . . 20 2.2.3 Communication Complexity . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.3 Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.3.1 Statistical Distance Measures . . . . . . . . . . . . . . . . . . . . . . . 20 2.3.2 Concentration Inequalities of Probability . . . . . . . . . . . . . . . . . 21\n2.4 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.5 Relevant Terminology for Trees . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.6 Universal Family of Hash Functions . . . . . . . . . . . . . . . . . . . . . . . . 25"
    }, {
      "heading" : "I Preference Elicitation 27",
      "text" : ""
    }, {
      "heading" : "3 Preference Elicitation for Single Peaked Profiles on Trees 29",
      "text" : "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.2 Domain of Single Peaked Profiles on Trees . . . . . . . . . . . . . . . . . . . . 36 3.3 Problem Definitions and Known Results . . . . . . . . . . . . . . . . . . . . . 36 3.4 Results for Preference Elicitation . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.4.1 Algorithmic Results for PREFERENCE ELICITATION . . . . . . . . . . . . 40 3.4.2 Lower Bounds for PREFERENCE ELICITATION . . . . . . . . . . . . . . . 41\n3.5 Results for Weak Condorcet Winner . . . . . . . . . . . . . . . . . . . . . . . . 45\n3.5.1 Algorithmic Results for Weak Condorcet Winner . . . . . . . . . . . . . 45 3.5.2 Lower Bounds for Weak Condorcet Winner . . . . . . . . . . . . . . . . 47\n3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49"
    }, {
      "heading" : "4 Preference Elicitation for Single Crossing Profiles 51",
      "text" : "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.1.2 Single Crossing Domain . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.1.3 Single Crossing Width . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.1.4 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\nx\nCONTENTS\n4.2 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.1 Model of Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.3 Results for Preference Elicitation . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.3.1 Known Single Crossing Order . . . . . . . . . . . . . . . . . . . . . . . 56 4.3.2 Unknown Single Crossing Order . . . . . . . . . . . . . . . . . . . . . 62 4.3.3 Single Crossing Width . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.4 Results for Condorcet Winner . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68"
    }, {
      "heading" : "II Winner Determination 69",
      "text" : ""
    }, {
      "heading" : "5 Winner Prediction and Margin of Victory Estimation 71",
      "text" : "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n5.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n5.1.1.1 Winner Prediction . . . . . . . . . . . . . . . . . . . . . . . . 74 5.1.1.2 Estimating Margin of Victory . . . . . . . . . . . . . . . . . . 77\n5.1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n5.2 Results for Winner Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.1 Results on Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . 80 5.2.2 Results on Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.2.2.1 Approval Voting Rule . . . . . . . . . . . . . . . . . . . . . . 85 5.2.2.2 Scoring Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 5.2.2.3 Maximin Voting Rule . . . . . . . . . . . . . . . . . . . . . . 90 5.2.2.4 Copeland Voting Rule . . . . . . . . . . . . . . . . . . . . . . 91 5.2.2.5 Bucklin Voting Rule . . . . . . . . . . . . . . . . . . . . . . . 92 5.2.2.6 Plurality with Runoff Voting Rule . . . . . . . . . . . . . . . . 93 5.2.2.7 STV Voting Rule . . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.3 Results for Estimating Margin of Victory . . . . . . . . . . . . . . . . . . . . . 97\n5.3.1 Results on Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . 97 5.3.2 Results on Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . 98\n5.3.2.1 Scoring Rules and Approval Voting Rule . . . . . . . . . . . . 98 5.3.2.2 Bucklin Voting Rule . . . . . . . . . . . . . . . . . . . . . . . 102 5.3.2.3 Maximin Voting Rule . . . . . . . . . . . . . . . . . . . . . . 104\nxi\nCONTENTS\n5.3.2.4 Copelandα Voting Rule . . . . . . . . . . . . . . . . . . . . . 104\n5.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107"
    }, {
      "heading" : "6 Streaming Algorithms for Winner Determination 108",
      "text" : "6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n6.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 6.1.2 Motivation for Variants of Heavy Hitters . . . . . . . . . . . . . . . . . 114\n6.2 Problem Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.3 Our Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.3.1 List Heavy Hitters Problem . . . . . . . . . . . . . . . . . . . . . . . . 119\n6.3.1.1 A Simpler, Near-Optimal Algorithm . . . . . . . . . . . . . . . 119 6.3.1.2 An Optimal Algorithm . . . . . . . . . . . . . . . . . . . . . . 122\n6.3.2 ε-Maximum Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 6.3.3 ε-Minimum Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 6.3.4 Problems for the Borda and Maximin Voting Rules . . . . . . . . . . . 132 6.3.5 Algorithms with Unknown Stream Length . . . . . . . . . . . . . . . . 133\n6.4 Results on Space Complexity Lower Bounds . . . . . . . . . . . . . . . . . . . 135\n6.4.1 Communication Complexity . . . . . . . . . . . . . . . . . . . . . . . . 135 6.4.2 Reductions from Problems in Communication Complexity . . . . . . . 137\n6.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141"
    }, {
      "heading" : "7 Kernelization for Possible Winner and Coalitional Manipulation 142",
      "text" : "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.1.1 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n7.2 Problem Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 7.3 Kernelization Hardness of the Possible Winner Problem . . . . . . . . . . . . . 146\n7.3.1 Results for the Scoring Rules . . . . . . . . . . . . . . . . . . . . . . . 146 7.3.2 Results for the Maximin Voting Rule . . . . . . . . . . . . . . . . . . . 149 7.3.3 Results for the Copeland Voting Rule . . . . . . . . . . . . . . . . . . . 154 7.3.4 Results for the Bucklin Voting Rule . . . . . . . . . . . . . . . . . . . . 157 7.3.5 Results for the Ranked Pairs Voting Rule . . . . . . . . . . . . . . . . . 160\n7.4 Polynomial Kernels for the Coalitional Manipulation Problem . . . . . . . . . 162 7.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\nxii\nCONTENTS"
    }, {
      "heading" : "III Election Control 166",
      "text" : ""
    }, {
      "heading" : "8 Manipulation with Partial Votes 168",
      "text" : "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\n8.1.1 Motivation and Problem Formulation . . . . . . . . . . . . . . . . . . . 170 8.1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 8.1.3 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 8.1.4 Problem Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 8.1.5 Comparison with Possible Winner and Coalitional Manipulation Problems176\n8.2 Hardness Results for WEAK MANIPULATION, OPPORTUNISTIC MANIPULATION,\nand STRONG MANIPULATION Problems . . . . . . . . . . . . . . . . . . . . . . 178 8.2.1 Weak Manipulation Problem . . . . . . . . . . . . . . . . . . . . . . . . 178\n8.2.1.1 Result for the k-Approval Voting Rule . . . . . . . . . . . . . 179 8.2.1.2 Result for the k-Veto Voting Rule . . . . . . . . . . . . . . . . 180 8.2.1.3 Result for the Bucklin Voting Rule . . . . . . . . . . . . . . . 182\n8.2.2 Strong Manipulation Problem . . . . . . . . . . . . . . . . . . . . . . . 184 8.2.3 Opportunistic Manipulation Problem . . . . . . . . . . . . . . . . . . . 186\n8.2.3.1 Result for the k-Approval Voting Rule . . . . . . . . . . . . . 186 8.2.3.2 Result for the k-Veto Voting Rule . . . . . . . . . . . . . . . . 188 8.2.3.3 Result for the Borda Voting Rule . . . . . . . . . . . . . . . . 190 8.2.3.4 Result for the Maximin Voting Rule . . . . . . . . . . . . . . . 192 8.2.3.5 Result for the Copelandα Voting Rule . . . . . . . . . . . . . 194 8.2.3.6 Result for the Bucklin Voting Rule . . . . . . . . . . . . . . . 196\n8.3 Polynomial Time Algorithms for WEAK MANIPULATION, STRONG MANIPULA-\nTION, and OPPORTUNISTIC MANIPULATION Problems . . . . . . . . . . . . . . . 199 8.3.1 Weak Manipulation Problem . . . . . . . . . . . . . . . . . . . . . . . . 199 8.3.2 Strong Manipulation Problem . . . . . . . . . . . . . . . . . . . . . . . 199 8.3.3 Opportunistic Manipulation Problem . . . . . . . . . . . . . . . . . . . 204\n8.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205"
    }, {
      "heading" : "9 Manipulation Detection 206",
      "text" : "9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n9.1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 9.1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\nxiii\nCONTENTS\n9.1.3 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n9.2 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n9.2.1 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n9.3 Results for the CPMW, CPM, CPMSW, and CPMS Problems . . . . . . . . . . . 214\n9.3.1 Scoring Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 9.3.2 Maximin Voting Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 9.3.3 Bucklin Voting Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 9.3.4 STV Voting Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n9.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224"
    }, {
      "heading" : "10 Frugal Bribery 226",
      "text" : "10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\n10.1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 10.1.2 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 10.1.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n10.2 Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 10.3 Results for Unweighted Elections . . . . . . . . . . . . . . . . . . . . . . . . . 235 10.4 Results for Weighted Elections . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 10.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250"
    }, {
      "heading" : "11 Summary and Future Directions 251",
      "text" : "11.1 Summary of Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\n11.1.1 Part I: Preference Elicitation . . . . . . . . . . . . . . . . . . . . . . . . 251 11.1.2 Part II: Winner Determination . . . . . . . . . . . . . . . . . . . . . . . 252 11.1.3 Part III: Election Control . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n11.2 Future Directions of Research . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\nReferences 257\nxiv\nList of Figures\n1.1 Structural overview of this dissertation . . . . . . . . . . . . . . . . . . . . . . 6\n2.1 Condorcet paradox. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2 Depicting classes of trees: (a) a path, (b) a star, (c) a (balanced) subdivision\nof a star, (d) a caterpillar. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n8.1 An example of a partial profile. Consider the plurality voting rule with one\nmanipulator. If the favorite candidate is A, then the manipulator simply has to place A on the top his vote to make A win in any extension. If the favorite candidate is B, there is no vote that makes B win in any extension. Finally, if the favorite candidate is C, then with a vote that places C on top, the manipulator can make C win in the only viable extension (Extension 2). . . . . . . . . . . . 172\nxv\nList of Tables\n1.1 Example of an election. Chocolate is the winner if the plurality voting rule is\nused. Kulfi is the winner if the Borda voting rule is used. Butterscotch is the Condorcet winner. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n3.1 Summary of query complexity bounds for eliciting preferences which are single\npeaked on trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.1 Summary of Results for preference elicitation for single crossing profiles. . . . 54\n5.1 Sample complexity of the (ε, δ)-WINNER DETERMINATION problem for common\nvoting rules. †–The lower bound of Ω((lnm/ε2) (1 − δ)) also applies to any voting rule that is Condorcet consistent. ∗– The lower bound of (1/4ε2) ln(1/8e√πδ) holds for any voting rule that reduces to the plurality voting rule for elections with two candidates. §– The lower bound holds for k 6 .999m. . . . . . . . . 76\n5.2 Sample complexity for the (c, ε, δ)–MARGIN OF VICTORY problem for common\nvoting rules. †The result holds for any c ∈ [0, 1). . . . . . . . . . . . . . . . . . 78\n6.1 The bounds hold for constant success probability algorithms and for n sufficiently\nlarge in terms of ε. For the (ε,φ)-Heavy Hitters problem and the ε-Maximum problem, we also achieve O(1) update time and reporting time which is linear in the size\nof the output. The upper bound for ε-Borda (resp. ε-Maximin) is for returning ev-\nery item’s Borda score (resp. Maximin score) up to an additive εmn (resp. additive\nεm), while the lower bound for ε-Borda (resp. ε-Maximin) is for returning only the approximate Borda score (resp. Maximin score) of an approximate maximum. . . . . 112\nxvi\nLIST OF TABLES\n8.1 Summary of Results (ℓ denotes the number of manipulators). The results in\nwhite follow immediately from the literature (Observation 8.1 to 8.3). Our results for the Copelandα voting rule hold for every rational α ∈ [0, 1] \\ {0.5} for the WEAK MANIPULATION problem and for every α ∈ [0, 1] for the OPPORTUNISTIC MANIPULATION and STRONG MANIPULATION problems. . . . . . . . . 175\n8.2 DP∪Q∪v(z, ·) and DP∪Q∪v(c, ·) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n9.1 Results for CPM and CPMW (k denotes coalition size). The ‘?’ mark means\nthat the problem is open. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n10.1 ⋆- The result holds for k > 5.\n•- The result holds for k > 3. †- The result holds for a much wider class of scoring rules. ⋄- The results do not hold for the plurality voting rule. ?- The problem is open. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\nxvii\nChapter 1\nIntroduction\nIn this chapter, we provide an informal introduction to the area called social choice theory\nand motivate its computational aspects. We then provide an overview of the thesis along\nwith further motivations for the problems we study in this thesis.\nPeople always have different opinions on almost everything and hence they often have to come to a decision collectively on things that affect all of them. There are various ways for people with different opinions to come to a decision and the use of voting is the most natural and common method for this task. Indeed, the use of voting can be traced back to Socrates (470-399 BC) who was sentenced to death by taking a majority vote. We also see the mention of voting rules in Plato’s (424-348 BC) unfinished book “Laws”. Then, in the Middle Ages, the Catalan philosopher, poet, and missionary Ramon Llull (1232-1316) mentioned Llull voting in his manuscripts Ars notandi, Ars eleccionis, and Alia ars eleccionis, which were lost until 2001. The German philosopher, theologian, jurist, and astronomer Nicholas of Cusa (1401-1464) suggested the method for electing the Holy Roman Emperor in 1433. The first systematic study of the theory of voting was carried out by two French mathematicians, political scientists, and philosophers Marie Jean Antoine Nicolas de Caritat, marquis de Condorcet (1743-1794) and Jean-Charles, chevalier de Borda (1733-1799). A more involved study of voting was pioneered by Kenneth Arrow (a co-recipient of the Nobel Memorial Prize in Economics in 1972). The celebrated Arrow’s impossibility theorem in 1951 along with Gibbard-Satterthwaite’s impossibility theorem in 1973 and 1977 laid the foundation stone of modern voting theory.\n1"
    }, {
      "heading" : "1.1 Important Aspects of Voting",
      "text" : "In a typical setting for voting, we have a set of alternatives or candidates, a set of agents called voters each of whom has an ordinal preference called votes over the candidates, and a voting rule which chooses one or a set of candidates as winner(s). We call a set of voters along with their preferences over a set of candidates and a voting rule an election. In Table 1.1 we exhibit an election with a set of voters {Satya, Shweta, Swapnil, Divya, Ganesh} and a set of five ice cream flavors {Chocolate, Butterscotch, Pesta, Vanilla, Kulfi} as a set of candidates. Suppose we use the plurality voting rule which chooses the candidate that is placed at the first positions of the votes most often as winner. Then the winner of the plurality election in Table 1.1 is Chocolate. Plurality voting rule is among the simplest voting rules and only observes the first position of every vote to determine the winner. Mainly due to its simplicity, the plurality voting rule frequently finds its applications in voting scenarios where human beings are involved, for example, political elections. Indeed many countries including India, Singapore, United Kingdom, Canada use plurality voting at various levels of elections [plu].\nHowever, there are more sophisticated voting rules which takes into account the entire preference ordering of the voters to choose a winner. One such voting rule which finds its widespread use specially in sports and elections in many educational institutions is the Borda voting rule [bor]. The Borda voting rule, first proposed by Jean-Charles, chevalier de Borda in 1770, gives a candidate a score equal to the number of other candidates it is preferred over for every vote. In Table 1.1 for example, the candidate Kulfi receives a score of 3 from Satya, 3 from Shweta, 2 from Swapnil, 2 from Divya, and 4 from Ganesh. Hence the Borda score of Kulfi is 14. It can be verified that the Borda score of every other candidate is less than 14\n2\nwhich makes Kulfi the Borda winner of the election. We can already observe how two widely used and intuitively appealing voting rules select two different winners for the same set of votes.\nA closer inspection of Table 1.1 reveals something peculiar about what the plurality and Borda voting rules choose as winners. The plurality voting rule chooses Chocolate as the winner whereas a majority (three out of five) of the voters prefer Butterscotch over Chocolate. The same phenomenon is true for the Borda winner (Kulfi) too — a majority of the voters prefer Butterscotch over Kulfi. More importantly Butterscotch is preferred over every other alternative (ice cream flavors) by a majority of the voters. Such an alternative, if it exists, which is preferred over every other alternative by a majority of the voters, is called the Condorcet winner of the election, named after the French mathematician and philosopher Marie Jean Antoine Nicolas de Caritat, marquis de Condorcet. We provide a more detailed discussion to all these aspects of voting including others in Chapter 2.\nNow let us again consider the example in Table 1.1. Suppose Shweta misreports her preference as “Butterscotch ≻ Pesta ≻ Chocolate ≻ Vanilla ≻ Kulfi.” Observe that the misreported vote of Shweta when tallied with the other votes makes Butterscotch the Borda winner with a Borda score of 12. Moreover, misreporting her preference results in an outcome (that is Butterscotch) which Shweta prefers over the honest one (that is Kulfi). This example demonstrate a fundamental notion in voting theory called manipulation — there can be instances where an individual voter may have an outcome by misreporting her preference which she prefers over the outcome of honest voting (according to her preference). Obviously manipulation is undesirable since it leads to suboptimal societal outcome. Hence one would like to use voting rules that do not create such possibilities of manipulation. Such voting rules are called strategy-proof voting rules. When we have only two candidates, it is known that the majority voting rule, which selects the majority candidate (breaking ties arbitrarily) as the winner, is a strategy-proof voting rule. There exist strategy-proof voting rules even if we have more than two candidates. For example, consider the voting rule fv that always outputs the candidate which is preferred most by some fixed voter say v. Obviously fv is strategy-proof since any voter other than v cannot manipulate simply because they can never change the outcome of the election and the winner is already the most preferred candidate of v. However, voting rules like fv are undesirable intuitively because it is not performing the basic job of voting which is aggregating preferences of all the voters. The voting rule fv is called a dictatorship since it always selects the candidate preferred most by some fixed voter v; such\n3\na voter v is called a dictator. Hence an important question to ask is the following: do there exist voting rules other than dictatorship that are strategy-proof with more than two candidates? Unfortunately classical results in social choice theory by Gibbard and Satterthwaite prove that there does not exist any such onto voting rule [Gib73, Sat75] if we have more than two candidates. A voting rule is onto if every candidate is selected for some set of votes. We formally introduce all these fundamental aspects of voting including others in Chapter 2."
    }, {
      "heading" : "1.2 Applications and Challenges of Voting Theory Relevant",
      "text" : "to Computer Science\nWith rapid improvement of computational power, the theory of voting found its applications not only in decision making among human beings but also in aggregating opinions of computational agents. Indeed, voting is commonly used whenever any system with multiple autonomous agents wish to take a common decision. Common and classic applications of voting in multiagent systems in particular and artificial intelligence in general include collaborative filtering and recommender systems [PHG00], planning among multiple automated agents [ER93], metasearch engines [DKNS01], spam detection [CSS99], computational biology [JSA08], winner determination in sports competitions [BBN14], similarity search and classification of high dimensional data [FKS03]. The extensive use of voting by computational agents makes the study of computational aspects of voting extremely important. In many applications of voting in artificial intelligence, one often has a huge number of voters and candidates. For example, voting has been applied in the design of metasearch engines [DKNS01] where the number of alternatives the agents are voting for is in the order of trillions if not more — the alternatives are the set of web pages in this case. In a commercial recommender system, for example Amazon.com, we often have a few hundred millions of users and items. In these applications, we need computationally efficient algorithms for quickly finding a winner of an election with so many candidates and voters.\nMoreover, the very fact that the agents now have computational power at their disposal allows them to easily strategize their behavior instead of acting truthfully. For example, the problem of manipulation is much more severe in the presence of computational agents since the voters can easily use their computational power to find a manipulative vote. The field known as computational social choice theory (COMSOC) studies various computational aspects\n4\nin the context of voting. We refer the interested readers to a recent book on computational social choice theory for a more elaborate exposition [MBC+16].\nOther than practical applications, the theory of voting turns out to be crucial in proving various fundamental theoretical results in computer science. Indeed, a notion known as noise stability of majority voting has been beautifully utilized for proving a lower bound on the quality of any polynomial time computable approximate solution of the maximum cut problem in a undirected graph [KKMO07, MOO05]. We refer interested reader to [O’D14] for a more detailed discussion."
    }, {
      "heading" : "1.3 Thesis Overview",
      "text" : "In this thesis, we resolve computational questions of several fundamental problems arising in the context of voting theory. We provide a schematic diagram of the contribution of the thesis in Figure 1.1. We begin our study with the first step in any election system – eliciting the preferences of the voters. We consider the problem of preference elicitation in the first part of the thesis and develop (often) optimal algorithms for learning the preferences of a set of voters. In the second part of the thesis, we develop (often) optimal sublinear time algorithms for finding the winner of an election. The problem of determining winner of an election is arguably the most fundamental problem that comes to our mind once the preferences of all the voters have been elicited. In the third part of the thesis, we exhibit complexity-theoretic results for various strategic aspects of voting."
    }, {
      "heading" : "1.3.1 Part I: Preference Elicitation",
      "text" : "The first step in any election system is to receive preferences of the voters as input. However, it may often be impractical to simply ask the voters for their preferences due to the presence of a huge number of alternatives. For example, asking users of Amazon.com to provide a complete ranking of all the items does not make sense. In such scenarios, one convenient way to know these agents’ preferences is to ask the agents to compare (a manageable number of) pairs of items. However, due to the existence of a strong query complexity lower bound (from sorting), we can hope to reduce the burden of eliciting preference from the agents only by assuming additional structure on the preferences. Indeed, in several application scenarios commonly considered, it is rare that preferences are entirely arbitrary, demonstrating no\n5\npatterns whatsoever. For example, the notion of single peaked preferences forms the basis of several studies in the analytical political sciences. Intuitively, preferences of a set of agents is single peaked if the alternatives and the agents can be arranged in a linear order and the preferences “respect” this order. We defer the formal definition of single peaked profiles to Chapter 3.\nWe show in Chapter 3 and 4 that the query complexity for preference elicitation can be reduced substantially by assuming practically appealing restrictions on the domain of the preferences.\nChapter 3: Preference Elicitation for Single Peaked Preferences on Trees\nSuppose we have n agents and m alternatives. Then it is known that if the preferences are single peaked, then elicitation can be done using Θ(mn) comparisons [Con09]. The notion of single peaked preferences has been generalized to single peaked preferences in a tree — intuitively, the preferences should be single peaked on every path of the tree. We show that if the single peaked tree has ℓ leaves, then elicitation can be done using Θ(mn log ℓ) comparisons [DM16a]. We also show that other natural parameters of the single peaked tree, for example, its diameter, path width, minimum degree, maximum degree do not decide the query complexity for preference elicitation.\n6\nChapter 4: Preference Elicitation for Single Crossing Preferences\nAnother well studied domain is the domain of single crossing profiles. Intuitively, a set of preferences is single crossing if the preferences can be arranged in a linear order such that all the preferences who prefer an alternative x over another alternative y are consecutive, for every x and y. We consider in [DM16b] two distinct scenarios: when an ordering of the voters with respect to which the profile is single crossing is known a priori versus when it is unknown. We also consider two different access models: when the votes can be accessed at random, as opposed to when they arise in any arbitrary sequence. In the sequential access model, we distinguish two cases when the ordering is known: the first is that the sequence in which the votes appear is also a single-crossing order, versus when it is not. The main contribution of our work is to provide polynomial time algorithms with low query complexity for preference elicitation in all the above six cases. Further, we show that the query complexities of our algorithms are optimal up to constant factor for all but one of the above six cases."
    }, {
      "heading" : "1.3.2 Part II: Winner Determination",
      "text" : "Once we have elicited the preferences of the voters, the next important task in an election is to determine the winner of this election. We show interesting complexity theoretic results on determining the winner of an election under a variety of application scenarios.\nChapter 5: Winner Prediction\nWe begin by studying the following problem: Given an election, predict the winner of the election under some fixed voting rule by sampling as few preferences as possible. Our results show that the winner of an election can be determined (with high probability) by observing only a few preferences that are picked uniformly at random from the set of preferences. We show this result for many common voting rules when the margin of victory is at least εn [DB15], where n is the number of voters and ε is a parameter. The margin of victory of an election is the smallest number of preferences that must be modified to change the winner of an election. We also establish optimality (in terms of the number of preferences our algorithms sample) of most of our algorithms by proving tight lower bounds on the sample complexity of this problem.\nAnother important aspect of an election is the robustness of the election outcome. A popular measure of robustness of an election outcome is its margin of victory. We develop\n7\nefficient sampling based algorithms for estimating the margin of victory of a given election for many common voting rules [DN15b]. We also show optimality of our algorithms for most of the cases under appealing practical scenarios.\nChapter 6: Winner Determination in Streaming\nAn important, realistic model of data in big data research is the streaming model. In the setting that we consider, the algorithms are allowed only one pass over the data. We give the first optimal bounds for returning the ℓ1-heavy hitters in an insertion only data stream, together with their approximate frequencies, thus settling a long line of work on this problem. For a stream of m items in {1, 2, . . . ,n} and parameters 0 < ε < φ 6 1, let fi denote the frequency of item i, i.e., the number of times item i occurs in the stream in [BDW16]. With arbitrarily large constant probability, our algorithm returns all items i for which fi > φm, returns no items j for which fj 6 (φ− ε)m, and returns approximations f̃i with |f̃i − fi| 6 εm for each item i that it returns. Our algorithm uses O(ε−1 logφ−1 +φ−1 logn+ log logm) bits of space, processes each stream update in O(1) worst-case time, and can report its output in time linear in the output size. We also prove a lower bound, which implies that our algorithm is optimal up to a constant factor in its space complexity. A modification of our algorithm can be used to estimate the maximum frequency up to an additive εm error in the above amount of space, resolving an open question on algorithms for data streams for the case of ℓ1-heavy hitters. We also introduce several variants of the heavy hitters and maximum frequency problems, inspired by rank aggregation and voting schemes, and show how our techniques can be applied in such settings. Unlike the traditional heavy hitters problem, some of these variants look at comparisons between items rather than numerical values to determine the frequency of an item.\nChapter 7: Kernelization Complexity for Determining Possible Winners\nVoters’ preferences are often partial orders instead of complete orders. This is known as the incomplete information setting in computational social choice theory. In an incomplete information setting, an extension of the winner determination problem which has been studied extensively is the problem of determining possible winners. An alternative x is a possible winner in a set of incomplete preferences if there exists a completion of these incomplete preferences where the alternative x wins. Previous work has provided, for many common\n8\nvoting rules, fixed parameter tractable algorithms for the Possible winner problem, with number of candidates as the parameter. However, the corresponding kernelization question is still open, and in fact, has been mentioned as a key research challenge [BCF+14a]. In this work, we settle this open question for many common voting rules.\nWe show in [DMN15b, DMN16c] that the Possible winner problem for maximin, Copeland, Bucklin, ranked pairs, and a class of scoring rules that includes the Borda voting rule do not admit a polynomial kernel with the number of candidates as the parameter. We show however that the Coalitional manipulation problem which is an important special case of the Possible winner problem does admit a polynomial kernel for maximin, Copeland, ranked pairs, and a class of scoring rules that includes the Borda voting rule, when the number of manipulators is polynomial in the number of candidates. A significant conclusion of this work is that the Possible winner problem is computationally harder than the Coalitional manipulation problem since the Coalitional manipulation problem admits a polynomial kernel whereas the Possible winner problem does not admit a polynomial kernel."
    }, {
      "heading" : "1.3.3 Part III: Election Control",
      "text" : "When agents in a multiagent system have conflicting goals, they can behave strategically to make the system produce an outcome that they favor. For example, in the context of voting, there can be instances where an agent, by misreporting her preference, can cause the election to result in an alternative which she prefers more than the alternative that would have been the outcome in case she had reported her preference truthfully. This phenomenon is called manipulation. Certainly, we would like to design systems that are robust to such manipulation. Unfortunately, classical results in social choice theory establish that any reasonable voting rule will inevitably create such opportunities for manipulation. In this context, computer scientists provide some hope by showing that the computational problem of manipulation can often be intractable. Indeed, if a computationally bounded agent has to solve an intractable problem to manipulate the election system, then the agent would, in all likelihood, fail to manipulate.\nChapter 8: Manipulation with Partial Information\nThe computational problem of manipulation has classically been studied in a complete information setting – the manipulators know the preference of every other voter. We extend\n9\nthis line of work to the more practical, incomplete information setting where agents have only a partial knowledge about the preferences of other voters [DMN16a]. In our framework, the manipulators know a partial order for each voter that is consistent with the true preference of that voter. We say that an extension of a partial order is viable if there exists a manipulative vote for that extension. We propose the following notions of manipulation when manipulators have incomplete information about the votes of other voters.\n1. WEAK MANIPULATION: the manipulators seek to vote in a way that makes their preferred\ncandidate win in at least one extension of the partial votes of the non-manipulators.\n2. OPPORTUNISTIC MANIPULATION: the manipulators seek to vote in a way that makes\ntheir preferred candidate win in every viable extension of the partial votes of the nonmanipulators.\n3. STRONG MANIPULATION: the manipulators seek to vote in a way that makes their pre-\nferred candidate win in every extension of the partial votes of the non-manipulators.\nWe consider several scenarios for which the traditional manipulation problems are easy (for instance, the Borda voting rule with a single manipulator). For many of them, the corresponding manipulative questions that we propose turn out to be computationally intractable. Our hardness results often hold even when very little information is missing, or in other words, even when the instances are very close to the complete information setting. Our overall conclusion is that computational hardness continues to be a valid obstruction or deterrent to manipulation, in the context of the more realistic setting of incomplete information.\nChapter 9: Manipulation Detection\nEven if the computational problem of manipulation is almost always intractable, there have been many instances of manipulation in real elections. In this work, we initiate the study of the computational problem of detecting possible instances of manipulation in an election [DMN15a]. We formulate two pertinent computational problems in the context of manipulation detection - Coalitional Possible Manipulators (CPM) and Coalitional Possible Manipulators given Winner (CPMW), where a suspect group of voters is provided as input and we have to determine whether they can form a potential coalition of manipulators. In the absence of any suspect group, we formulate two more computational problems namely Coalitional Possible Manipulators Search (CPMS) and Coalitional Possible Manipulators Search\n10\ngiven Winner (CPMSW). We provide polynomial time algorithms for these problems, for several popular voting rules. For a few other voting rules, we show that these problems are NP-complete. We observe that detecting possible instances of manipulation may be easy even when the actual manipulation problem is computationally intractable, as seen for example, in the case of the Borda voting rule.\nChapter 10: Frugal Bribery\nAnother fundamental problem in the context of social choice theory is bribery. Formally, the computational problem of bribery is as follows: given (i) a set of votes, (ii) a cost model for changing the votes, (iii) a budget, and (iv) a candidate x, is it possible for an external agent to bribe the voters to change their votes (subject to the budget constraint) so that x wins the election? We introduce and study two important special cases of the classical $BRIBERY problem, namely, FRUGAL-BRIBERY and FRUGAL-$BRIBERY where the briber is frugal in nature in [DMN16b]. By this, we mean that the briber is only able to influence voters who benefit from the suggestion of the briber. More formally, a voter is vulnerable if the outcome of the election improves according to her own preference when she accepts the suggestion of the briber. In the FRUGAL-BRIBERY problem, the goal of the briber is to make a certain candidate win the election by changing only the vulnerable votes. In the FRUGAL-$BRIBERY problem, the vulnerable votes have prices and the goal is to make a certain candidate win the election by changing only the vulnerable votes, subject to a budget constraint. We further formulate two natural variants of the FRUGAL-$BRIBERY problem namely UNIFORM-FRUGAL-$BRIBERY and NONUNIFORM-FRUGAL-$BRIBERY where the prices of the vulnerable votes are, respectively, all same or different.\nWe observe that, even if we have only a small number of candidates, the problems are intractable for all voting rules studied here for weighted elections, with the sole exception of the FRUGAL-BRIBERY problem for the plurality voting rule. In contrast, we have polynomial time algorithms for the FRUGAL-BRIBERY problem for plurality, veto, k-approval, k-veto, and plurality with runoff voting rules for unweighted elections. However, the FRUGAL-$BRIBERY problem is intractable for all the voting rules studied here barring the plurality and the veto voting rules for unweighted elections. These intractability results demonstrate that bribery is a hard computational problem, in the sense that several special cases of this problem continue to be computationally intractable. This strengthens the view that bribery, although a possible attack on an election in principle, may be infeasible in practice.\n11\nWe believe that the thesis work has attempted to break fresh ground by resolving the computational complexity of many long-standing canonical problems in computational social choice. We finally conclude in Chapter 11 with future directions of research.\n12\nChapter 2\nBackground\nThis chapter provides an overview of selected topics required to understand the technical\ncontent in this thesis. We cover these topics, up to the requirement of following this the-\nsis, in the following order: Voting and Elections, Computational Complexity, Probability\nTheory, Information Theory, Relevant Terminology for Trees, Universal Family of Hash\nFunctions.\nWe present the basic preliminaries in this chapter. We denote, for any natural number ℓ ∈ N, the set {1, 2, . . . , ℓ} by [ℓ]. We denote the set of permutations of [ℓ] by Sℓ. For a set X, we denote the set of subsets of X of size k by Pk(X) and the power set of X by 2 X."
    }, {
      "heading" : "2.1 Voting and Elections",
      "text" : "In this section, we introduce basic terminologies of voting."
    }, {
      "heading" : "2.1.1 Basic Setting",
      "text" : "Let C = {c1, c2, . . . , cm} be a set of candidates or alternatives and V = {v1, v2, . . . , vn} a set of voters. If not mentioned otherwise, we denote the set of candidates by C, the set of voters by V, the number of candidates by m, and the number of voters by n. Every voter vi has a preference or vote ≻i which is a complete order over C. A complete order over any set X is a relation on X which is reflexive, transitive, anti-symmetric, and total. A relation R on X is a subset of X × X. A relation R is called reflexive if (x, x) ∈ R for every x ∈ X, transitive if (x,y) ∈ R and (y, z) ∈ R implies (x, z) ∈ R for every x,y, z ∈ X, anti-symmetric\n13\nis (x,y) ∈ R and (y, x) ∈ R implies x = y for every x,y ∈ X, and total is either (x,y) ∈ R or (y, x) ∈ R for every x,y ∈ X. We denote the set of complete orders over C by L(C). We call a tuple of n preferences (≻1,≻2, · · · ,≻n) ∈ L(C)n an n-voter preference profile. Given a preference ≻= c1 ≻ c2 ≻ · · · ≻ cm ∈ L(C), we call the order cm ≻ cm−1 ≻ · · · ≻ c2 ≻ c1 the reverse order of ≻ and denote it by ←−≻ . We say that a candidate x ∈ C is at the ith position of a preference ≻ if there exists exactly i − 1 other candidates in C who are preferred over x in ≻, that is |{y ∈ C \\ {x} : y ≻ x}| = i − 1. It is often convenient to view a preference as a subset of C × C — a preference ≻ corresponds to the subset A = {(x,y) ∈ C × C : x ≻ y}. For a preference ≻ and a subset A ⊆ C of candidates, we define ≻ (A) be the preference ≻ restricted to A, that is ≻ (A) =≻ ∩(A×A).\nA voting correspondence is a function r̃ : ∪n∈NL(C)n −→ 2C \\ {∅} which selects, from a preference profile, a nonempty set of candidates as the winners. A tie breaking rule τ : 2C \\ {∅} −→ C with τ(A) ∈ A for every A ∈ 2C \\ {∅} selects one candidate from a nonempty set of (tied) candidates. A voting rule r : ∪n∈NL(C)n −→ C selects one candidate as the winner from a preference profile. An important class of tie breaking rules is the class of lexicographic tie breaking rules where the ties are broken in a fixed complete order of candidates. More formally, given a complete order ≻∈ L(C), we can define a tie breaking rule τ≻ as follows: for every nonempty subset A ⊆ C of C, τ≻(A) = x such that x ∈ A and x ≻ y for every y ∈ A \\ {x}. A natural way for defining a voting rule is to compose a voting correspondence r̃ with a tie breaking rule τ; that is r = τ ◦ r̃ is a voting rule. We call a preference profile over a set of candidates along with a voting rule an election. The winner (or winners) of an election is often called the outcome of the election."
    }, {
      "heading" : "2.1.2 Axioms of Voting Rules",
      "text" : "Let us now look at important axiomatic properties of voting rules that one may wish to satisfy.\n⊲ Onto: A voting rule r is called onto if every candidate wins for at least one voting\nprofile. More formally, a voting rule r is called onto if for every x ∈ C there exists a (≻1,≻2, . . . ,≻n) ∈ L(C)n such that r(≻1,≻2, . . . ,≻n) = x.\n⊲ Anonymity: A voting rule is anonymous if the “names” of the voters does not affect\nthe outcome. More formally, a voting rule r is said to satisfy anonymity if for every (≻1,≻2, . . . ,≻n) ∈ L(C)n, we have r(≻1,≻2, . . . ,≻n) = r(≻σ(1),≻σ(2), . . . ,≻σ(n)) for every permutation σ ∈ Sn.\n14\n⊲ Neutrality: A voting rule is called neutral if the “names” of the candidates are immaterial\nfor determining election outcome. That is, a voting rule r is called neutral if for every (≻1,≻2, . . . ,≻n) ∈ L(C)n, we have r(≻1,≻2, . . . ,≻n) = σ(r(σ(≻1),σ(≻2), . . . ,σ(≻n))) for every permutation σ ∈ Sm. Given a preference ≻= c1 ≻ c2 ≻ · · · ≻ cm and a permutation σ ∈ Sm, we denote the preference cσ(1) ≻ cσ(2) ≻ · · · ≻ cσ(m) by σ(≻).\n⊲ Homogeneity: A voting rule is called homogeneous if the outcome of the election solely\ndepends on the fraction of times (of the number of voters n) every complete order ≻∈ L(C) appears in the preference profile.\n⊲ Dictatorship: A voting rule r is called a dictatorship if there exists an integer i ∈ [n] such that r(≻1,≻2, . . . ,≻n) =≻i(1) for every (≻1,≻2, . . . ,≻n) ∈ L(C)n, where ≻i(1) the candidate placed at the first position of the preference ≻i.\n⊲ Manipulability: A voting rule r is called manipulable if there exists a preference profile\n(≻1,≻2, . . . ,≻n) ∈ L(C)n and a preference ≻∈ L(C) such that the following holds.\nr(≻1,≻2, . . . ,≻i−1,≻,≻i+1, . . . ,≻n) ≻i r(≻1,≻2, . . . ,≻n)\nThat it, voter vi prefers the election outcome if she reports ≻ to be her preference than the election outcome if she reports her true preference ≻i.\n⊲ Condorcet consistency: A candidate is called the Condorcet winner of an election if it\ndefeats every other candidate in pairwise election. More formally, given an election E, let us define NE(x,y) = |{i : x ≻i y}| for any two candidates x,y ∈ C. A candidate c is called the Condorcet winner of an election if NE(c, x) > n/2 for every candidate x ∈ C \\ {c} other than c. A voting rule is called Condorcet consistent is it selects the Condorcet winner as the outcome of the election whenever such a candidate exists.\n⊲ Weak Condorcet consistency: A candidate is called a weak Condorcet winner of an\nelection if it does not lose to any other candidate in pairwise election. More formally, given an election E, let us defineNE(x,y) = |{i : x ≻i y}| for any two candidates x,y ∈ C. A candidate c is called the Condorcet winner of an election if NE(c, x) > n/2 for every candidate x ∈ C \\ {c} other than c. A voting rule is called weak Condorcet consistent is it selects a weak Condorcet winner as the outcome of the election whenever one such candidate exists.\n15\nGiven an election E = (≻,C) and two candidates x,y ∈ C, we say that the candidate x defeats the candidate y in pairwise election if NE(x,y) > NE(y, x)."
    }, {
      "heading" : "2.1.3 Majority Graph",
      "text" : "Given an election E = (≻= (≻1,≻2, . . . ,≻n),C), we can construct a weighted directed graph GE = (U = C,E) as follows. The vertex set of the graph GE is the set of candidates C. For any two candidates x,y ∈ C with x 6= y, let us define the margin DE(x,y) of x from y to be NE(x,y) − NE(y, x). We have an edge from x to y in GE if DE(x,y) > 0. Moreover, in that case, the weight w(x,y) of the edge from x to y is DE(x,y). Observe that, a candidate c is the Condorcet winner of an election E if and only if there is an edge from c to every other vertices in the weighted majority graph GE."
    }, {
      "heading" : "2.1.4 Condorcet Paradox",
      "text" : "Given an election E = (≻,C), there may exist candidates c1, c2, . . . , ck ∈ C for some integer k > 3 such that the candidate ci defeats ci+1 mod k in pairwise election for every i ∈ [k]. This phenomenon was discovered by Marie Jean Antoine Nicolas de Caritat, marquis de Condorcet and is called Condorcet paradox or Condorcet cycles. Figure 2.1 shows an example of a Condorcet paradox where a defeats b, b defeats c, and c defeats a."
    }, {
      "heading" : "2.1.5 Incomplete Information Setting",
      "text" : "A more general setting is an election where the votes are only partial orders over candidates. A partial order is a relation that is reflexive, antisymmetric, and transitive. A partial vote can be extended to possibly more than one linear votes depending on how we fix the order for the unspecified pairs of candidates. For example, in an election with the set of candidates C = {a,b, c}, a valid partial vote can be a ≻ b. This partial vote can be extended to three\n16\nlinear votes namely, a ≻ b ≻ c, a ≻ c ≻ b, c ≻ a ≻ b. However, the voting rules always take as input a set of votes that are complete orders over the candidates."
    }, {
      "heading" : "2.1.6 Voting Rules",
      "text" : "Examples of common voting correspondences are as follows. We can use any tie breaking rule, for example, any lexicographic tie breaking rule, with these voting correspondences to get a voting rule.\n⊲ Positional scoring rules: A collection of m-dimensional vectors −→sm = (α1,α2, . . . ,αm) ∈ Rm with α1 > α2 > . . . > αm and α1 > αm for every m ∈ N naturally defines a voting rule — a candidate gets score αi from a vote if it is placed at\nthe ith position, and the score of a candidate is the sum of the scores it receives from all the votes. The winners are the candidates with maximum score. Scoring rules remain unchanged if we multiply every αi by any constant λ > 0 and/or add any constant µ. Hence, we assume without loss of generality that for any score vector −→sm, there exists a j such that αj−αj+1 = 1 and αk = 0 for all k > j. We call such a\n−→sm a normalized score vector. A scoring rule is called strict if α1 > α2 > · · · > αm for every natural number m. If αi is 1 for i ∈ [k] and 0 otherwise, then we get the k-approval voting rule. For the k-veto voting rule, αi is 0 for i ∈ [m− k] and −1 otherwise. 1-approval is called the plurality voting rule and 1-veto is called the veto voting rule. For the Borda voting rule, we have αi = m − i for every i ∈ [m].\n⊲ Bucklin and simplified Bucklin: Let ℓ be the minimum integer such that at least one\ncandidate gets majority within top ℓ positions of the votes. The winners under the simplified Bucklin voting rule are the candidates having more than n/2 votes within top ℓ positions. The winners under the Bucklin voting rule are the candidates appearing within top ℓ positions of the votes highest number of times. However, for brevity, other than Chapter 8, we use Bucklin to mean simplified Bucklin only.\n⊲ Fallback and simplified Fallback: For these voting rules, each voter v ranks a subset\nXv ⊂ C of candidates and disapproves the rest of the candidates [BS09]. Now for the Fallback and simplified Fallback voting rules, we apply the Bucklin and simplified Bucklin voting rules respectively to define winners. If there is no integer ℓ for which at least one candidate gets more than n/2 votes, both the Fallback and simplified Fallback\n17\nvoting rules output the candidates with most approvals as winners. We assume, for simplicity, that the number of candidates each partial vote approves is known.\n⊲ Maximin: The maximin score of a candidate x in an election E is miny 6=xDE(x,y). The\nwinners are the candidates with maximum maximin score.\n⊲ Copelandα: Given α ∈ [0, 1], the Copelandα score of a candidate x is |{y 6= x : DE(x,y) > 0}| + α|{y 6= x : DE(x,y) = 0}|. The winners are the candidates with maximum Copelandα score. If not mentioned otherwise, we will assume α to be zero.\n⊲ Ranked pairs: Given an election E, we pick a pair (ci, cj) ∈ C× C such that DE(ci, cj) is maximum. We fix the ordering between ci and cj to be ci ≻ cj unless it contradicts previously fixed orders. We continue this process until all pairwise elections are con-\nsidered. At this point, we have a complete order ≻ over the candidates. Now the top candidate of ≻ is chosen as the winner.\n⊲ Plurality with runoff: The top two candidates according to plurality score are selected\nfirst. The pairwise winner of these two candidates is selected as the winner of the election. This rule is often called the runoff voting rule.\n⊲ Single transferable vote: In single transferable vote (STV), a candidate with least\nplurality score is dropped out of the election and its votes are transferred to the next preferred candidate. If two or more candidates receive least plurality score, then some predetermined tie breaking rule is used. The candidate that remains after (m − 1) rounds is the winner. The single transferable vote is also called the instant runoff vote.\nWe use the parallel-universes tie breaking [CRX09, BF12] to define the winning candidate for the ranked pairs voting rule. In this setting, a candidate c is a winner if and only if there exists a way to break ties in all of the steps such that c is the winner."
    }, {
      "heading" : "2.2 Computational Complexity",
      "text" : ""
    }, {
      "heading" : "2.2.1 Parameterized Complexity",
      "text" : "Preprocessing, as a strategy for coping with hard problems, is universally applied in practice. The main goal here is instance compression - the objective is to output a smaller instance\n18\nwhile maintaining equivalence. In the classical setting, NP-hard problems are unlikely to have efficient compression algorithms (since repeated application would lead to an efficient solution for the entire problem, which is unexpected). However, the breakthrough notion of kernelization in parameterized complexity provides a mathematical framework for analyzing the quality of preprocessing strategies. In parameterized complexity, each problem instance comes with a parameter k, and the central notion is fixed parameter tractability (FPT) which means, for a given instance (x, k), solvability in time f(k) · p(|x|), where f is an arbitrary function of k and p is a polynomial in the input size |x|. We use the notation O∗(f(k)) to denote O(f(k)poly(|x|)). A parameterized problem Π is a subset of Γ∗ ×N, where Γ is a finite alphabet. An instance of a parameterized problem is a tuple (x, k), where k is the parameter. We refer the reader to the books [GN07, DF99, FG06] for a detailed introduction to this paradigm, and below we state only the definitions that are relevant to our work.\nA kernelization algorithm is a set of preprocessing rules that runs in polynomial time and reduces the instance size with a guarantee on the output instance size. This notion is formalized below.\nDefinition 2.1. [Kernelization] [Nie02, FG06] A kernelization algorithm for a parameterized problem Π ⊆ Γ∗ × N is an algorithm that, given (x, k) ∈ Γ∗ × N, outputs, in time polynomial in |x| + k, a pair (x ′, k ′) ∈ Γ∗ × N such that (a) (x, k) ∈ Π if and only if (x ′, k ′) ∈ Π and (b) |x ′|, k ′ 6 g(k), where g is some computable function. The output instance x ′ is called the kernel, and the function g is referred to as the size of the kernel. If g(k) = kO(1), then we say that Π admits a polynomial kernel.\nFor many parameterized problems, it is well established that the existence of a polynomial kernel would imply the collapse of the polynomial hierarchy to the third level (or more precisely, CoNP ⊆ NP/Poly). Therefore, it is considered unlikely that these problems would admit polynomial-sized kernels. For showing kernel lower bounds, we simply establish reductions from these problems.\nDefinition 2.2. [Polynomial Parameter Transformation] [BTY09] Let Γ1 and Γ2 be parameterized problems. We say that Γ1 is polynomial time and parameter reducible to Γ2, written Γ1 6Ptp Γ2, if there exists a polynomial time computable function f : Σ ∗ × N → Σ∗ × N, and a polynomial p : N → N, and for all x ∈ Σ∗ and k ∈ N, if f ((x, k)) = (x ′, k ′), then (x, k) ∈ Γ1 if and only if (x ′, k ′) ∈ Γ2, and k ′ 6 p (k). We call f a polynomial parameter transformation (or a PPT) from Γ1 to Γ2.\n19\nThis notion of a reduction is useful in showing kernel lower bounds because of the follow-\ning theorem.\nTheorem 2.1. [BTY09, Theorem 3] Let P and Q be parameterized problems whose derived classical problems are Pc,Qc, respectively. Let Pc be NP-complete, and Qc ∈ NP. Suppose there exists a PPT from P to Q. Then, if Q has a polynomial kernel, then P also has a polynomial kernel."
    }, {
      "heading" : "2.2.2 Approximation Factor of an Algorithm",
      "text" : "For a minimization problem P, we say an algorithm A archives an approximation factor of α if A(I) 6 αOPT(I) for every problem instance I of P. In the above, OPT(I) denotes the value of the optimal solution of the problem instance I."
    }, {
      "heading" : "2.2.3 Communication Complexity",
      "text" : "Communication complexity of a function measures the number of bits that need to be exchanged between two players to compute a function whose input is split among those two players [Yao79]. In a more restrictive one-way communication model, Alice, the first player, sends only one message to Bob, the second player, and Bob outputs the result. A protocol is a method that the players follow to compute certain functions of their input. Also the protocols can be randomized; in that case, the protocol needs to output correctly with probability at least 1−δ, for δ ∈ (0, 1) (the probability is taken over the random coin tosses of the protocol). The randomized one-way communication complexity of a function f with error probability δ is denoted by R 1-way δ (f). [KN97] is a standard reference for communication complexity."
    }, {
      "heading" : "2.3 Probability Theory",
      "text" : ""
    }, {
      "heading" : "2.3.1 Statistical Distance Measures",
      "text" : "Given a finite set X, a distribution µ on X is defined as a function µ : X −→ [0, 1], such that ∑ x∈X µ(x) = 1. The finite set X is called the base set of the distribution µ. We use the following distance measures among distributions in our work.\n20\nDefinition 2.3. The KL divergence [KL51] and the Jensen-Shannon divergence [Lin91] between two distributions µ1 and µ2 on X are defined as follows.\nDKL(µ1||µ2) = ∑\nx∈X µ1(x) ln\nµ1(x) µ2(x)\nJS(µ1,µ2) = 1\n2\n(\nDKL\n(\nµ1|| µ1 + µ2\n2\n)\n+DKL\n(\nµ2|| µ1 + µ2\n2\n))\nThe Jensen-Shannon divergence has subsequently been generalized to measure the mu-\ntual distance among more than two distributions as follows.\nDefinition 2.4. Given n distributions µ1, . . . ,µn over the same base set, the generalized JensenShannon divergence1 among them is:\nJS(µ1, . . . ,µn) = 1\nn\nn∑\ni=1\nDKL\n(\nµi|| 1\nn\nn∑\nj=1\nµj\n)"
    }, {
      "heading" : "2.3.2 Concentration Inequalities of Probability",
      "text" : "Suppose we have n events namelyA1,A2, . . . ,An in a probability space. Then the probability of occurring any of these n events can be bounded by what is known as union bound.\nTheorem 2.2. (Union Bound) The probability of happening any of a given n events A1,A2, . . . ,An can be upper bounded as follows.\nPr[∪ni=1Ai] 6 n∑\ni=1\nPr[Ai]\nGiven a probability space (Ω,F,µ), a random variable X : Ω −→ R is a function from the sample space Ω to the set of real numbers R such that {ω ∈ Ω : X(ω) 6 c} ∈ F for every c ∈ R. We refer to [Dur10] for elementary notions of probability theory.\nGiven a positive random variable X with finite mean E[X], the classical Markov inequality\nbounds the probability by which the random variable X deviates from its mean.\nTheorem 2.3. (Markov Inequality) For a positive random variable X with mean E[X], the probability that the random variable X\n1The generalized Jensen-Shannon divergence is often formulated with weights on each of the n distribu-\ntions. The definition here puts equal weight on each distribution and is sufficient for our purposes.\n21\ntakes value more than cE[X] can be upper bounded as follows.\nPr[X > cE[X]] 6 1\nc\nIf the variance of a random variable X is known, the Chebyshev inequality provides a\nsharper concentration bound for a random variable.\nTheorem 2.4. (Chebyshev Inequality) For a random variable with mean E[X] and variance σ2, the probability that the random variable X takes value more than cσ away from its mean E[X] is upper bounded as follows.\nPr[|X− E[X]| > cσ] 6 1\nc2\nFor a sum of independent bounded random variables X, the Chernoff bound gives a much\ntighter concentration around the mean of X.\nTheorem 2.5. (Chernoff Bound)\nLet X1, . . . ,Xℓ be a sequence of ℓ independent random variables in [0, 1] (not necessarily identi-\ncal). Let S = ∑\ni Xi and let µ = E [S]. Then, for any 0 6 δ 6 1:\nPr[|S− µ| > δµ] < 2 exp(−δ2µ/3)"
    }, {
      "heading" : "2.4 Information Theory",
      "text" : "For a discrete random variable X with possible values {x1, x2, . . . , xn}, the Shannon entropy of X is defined as H(X) = − ∑n\ni=1 Pr(X = xi) log2 Pr(X = xi). Let Hb(p) = −p log2 p − (1 −\np) log2(1− p) denote the binary entropy function when p ∈ (0, 1). For two random variables X and Y with possible values {x1, x2, . . . , xn} and {y1,y2, . . . ,ym}, respectively, the conditional entropy of X given Y is defined as H(X | Y) = ∑\ni,j Pr(X = xi, Y = yj) log2 Pr(Y=yj)\nPr(X=xi,Y=yj) . Let\nI(X; Y) = H(X) − H(X | Y) = H(Y) − H(Y | X) denote the mutual information between two random variables X, Y. Let I(X; Y | Z) denote the mutual information between two random variables X, Y conditioned on Z, i.e., I(X; Y | Z) = H(X | Z) − H(X | Y,Z). The following summarizes several basic properties of entropy and mutual information.\nProposition 2.1. Let X, Y,Z,W be random variables.\n22\n1. If X takes value in {1, 2, . . . ,m}, then H(X) ∈ [0, logm].\n2. H(X) > H(X | Y) and I(X; Y) = H(X) −H(X | Y) > 0.\n3. If X and Z are independent, then we have I(X; Y | Z) > I(X; Y). Similarly, if X,Z are\nindependent given W, then I(X; Y | Z,W) > I(X; Y | W).\n4. (Chain rule of mutual information) I(X, Y;Z) = I(X;Z) + I(Y;Z | X). More generally, for\nany random variables X1,X2, . . . ,Xn, Y, I(X1, . . . ,Xn; Y) = ∑n i=1 I(Xi; Y | X1, . . . ,Xi−1). Thus, I(X, Y;Z | W) > I(X;Z | W).\n5. (Fano’s inequality) Let X be a random variable chosen from domain X according to distri-\nbution µX, and Y be a random variable chosen from domain Y according to distribution µY. For any reconstruction function g : Y → X with error δg,\nHb(δg) + δg log(|X|− 1) > H(X | Y).\nWe refer readers to [CT12] for a nice introduction to information theory."
    }, {
      "heading" : "2.5 Relevant Terminology for Trees",
      "text" : "A tree T = (V,E) is a set of vertices V along with a set of edges E ⊂ P2(V) such that for every two vertices x,y ∈ V, there exists exactly one path between x and y in T. The following definitions pertaining to the structural aspects of trees will be useful.\n⊲ The pathwidth of T is the minimum width of a path decomposition of T [Hei93].\n⊲ A set of disjoint paths Q = {Q1 = (X1,E1), . . . ,Qk = (Xk,Ek)} is said to cover a tree\nT = (X,E) if X = ∪i∈[k]Xi,Ei ⊆ E,Xi ∩Xj = ∅,Ei ∩ Ej = ∅ for every i, j ∈ [k] with i 6= j. The path cover number of T is the cardinality of the smallest set Q of disjoint paths that cover T.\n⊲ The distance of a tree T from a path is the smallest number of nodes whose removal\nmakes the tree a path.\n⊲ The diameter of a tree T is the number of edges in the longest path in T.\n23\nWe also list some definitions of subclasses of trees (which are special types of trees, see\nalso Figure 2.2).\n⊲ A tree is a star if there is a center vertex and every other vertex is a neighbor of this\nvertex.\n⊲ A tree is a subdivision of a star if it can be constructed by replacing each edge of a star\nby a path.\n⊲ A subdivision of a star is called balanced if there exists an integer ℓ such that the distance\nof every leaf node from the center is ℓ.\n⊲ A tree is a caterpillar if there is a central path and every other vertex is at a distance of\none it.\n⊲ A tree is a complete binary tree rooted at r if every nonleaf node has exactly two children\nand there exists an integer h, called the height of the tree, such that every leaf node is at a distance of either h or h − 1 from the root node r.\n24"
    }, {
      "heading" : "2.6 Universal Family of Hash Functions",
      "text" : "In this section, we discuss universal family of hash functions. Intuitively, a family of hash functions is universal if a hash function picked uniformly at random from the family behaves like a completely random function. The formal definition of universal family of hash functions is as follows. We use this notion crucially in Chapter 6.\nDefinition 2.5. (Universal family of hash functions) A family of functions H = {h|h : A → B} is called a universal family of hash functions if for all a 6= b ∈ A, Pr{h(a) = h(b)} = 1/|B|, where h is picked uniformly at random from H.\nPr h picked uniformly at random from H {h(a) = h(b)} = 1/|B|\nWe know that there exists a universal family of hash functions H from [k] to [ℓ] for every\npositive integer ℓ and every prime k [Cor09]. Moreover, |H|, the size of H, is O(k2).\n25\nPart I\nPreference Elicitation\nThis part of the thesis consists of the following two chapters.\n⊲ In Chapter 3 – Preference Elicitation for Single Peaked Profiles on Trees – we present ef-\nficient algorithms for preference elicitation when the preferences belong to the domain of single peaked preferences on a tree. We show interesting connections between query complexity for preference elicitation and various common parameters of trees.\n⊲ In Chapter 4 – Preference Elicitation for Single Crossing Profiles – we show optimal\nalgorithms (except for one scenario) for eliciting single crossing preference profiles. Our results show that the query complexity for preference elicitation for single crossing preference profiles crucially depends on the way the votes are allowed to access.\n27\nChapter 3\nPreference Elicitation for Single Peaked\nProfiles on Trees\nIn multiagent systems, we often have a set of agents each of which has a preference ordering over a set of items. One would often like to know these preference orderings for various tasks like data analysis, preference aggregation, voting etc. However, we often have a large number of items which makes it impractical to directly ask the agents for their complete preference ordering. In such scenarios, one convenient way to know these agents’ preferences is to ask the agents to compare (hopefully a small number of) pairs of items.\nPrior work on preference elicitation focuses on unrestricted domain and the domain\nof single peaked preferences. They show that the preferences in single peaked domain can\nbe elicited with much less number of queries compared to unrestricted domain. We extend\nthis line of research and study preference elicitation for single peaked preferences on trees\nwhich is a strict superset of the domain of single peaked preferences. We show that the\nquery complexity for preference elicitation crucially depends on the number of leaves, the\npath cover number, and the minimum number of nodes that should be removed to turn the\nsingle peaked tree into a path. We also observe that the other natural parameters of the\nsingle peaked tree like maximum and minimum degrees of a node, diameter, pathwidth do\nnot play any direct role in determining the query complexity of preference elicitation. We\nA preliminary version of the work in this chapter was published as [DM16a]: Palash Dey and Neeldhara\nMisra. Elicitation for preferences single peaked on trees. In Proc. Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 215-221, 2016.\n29\nthen investigate the query complexity for finding a weak Condorcet winner (we know that\nat least one weak Condorcet winner always exists for profiles which are single peaked on\na tree) for preferences single peaked on a tree and show that this task has much less query\ncomplexity than preference elicitation. Here again we observe that the number of leaves\nin the underlying single peaked tree and the path cover number of the tree influence the\nquery complexity of the problem."
    }, {
      "heading" : "3.1 Introduction",
      "text" : "In multiagent systems, we often have scenarios where a set of agents have to arrive at a consensus although they possess different opinions over the alternatives available. Typically, the agents have preferences over a set of items, and the problem of aggregating these preferences in a suitable manner is one of the most well-studied problems in social choice theory [BCE+15]. There are many ways of expressing preferences over a set of alternatives. One of the most comprehensive ways is to specify a complete ranking over the set of alternatives. However, one of the downsides of this model is the fact that it can be expensive to solicit the preferences when a large number of alternatives and agents are involved.\nSince asking agents to provide their complete rankings is impractical, a popular notion is one of elicitation, where we ask agents simple comparison queries, such as if they prefer an alternative x over another alternative y. This naturally gives rise to the problem of preference elicitation, where we hope to recover the complete ranking (or possibly the most relevant part of the ranking) based on a small number of comparison queries.\nIn the context of a fixed voting rule, we may also want to query the voters up to the point of determining the winner (or the aggregate ranking, as the case may be). Yet another refinement in this setting is when we have prior information about how agents are likely to vote, and we may want to determine which voters to query first, to be able to quickly rule out a large number of alternatives, as explored by [CS02].\nWhen our goal is to elicit preferences that have no prior structure, one can demonstrate scenarios where it is imperative to ask each agent (almost) as many queries as would be required to determine an arbitrary ranking. However, in recent times, there has been considerable interest in voting profiles that are endowed with additional structure. The motivation for this is two-fold. The first is that in several application scenarios commonly considered, it is rare that votes are ad-hoc, demonstrating no patterns whatsoever. For example, the notion\n30\nof single-peaked preferences, which we will soon discuss at length, forms the basis of several studies in the analytical political sciences [HM97]. In his work on eliciting preferences that demonstrate the “single-peaked” structure, Conitzer argues that the notion of singlepeakedness is also a reasonable restriction in applications outside of the domain of political elections [Con09].\nThe second motivation for studying restricted preferences is somewhat more technical, but is just as compelling. To understand why structured preferences have received considerable attention from social choice theorists, we must first take a brief detour into some of the foundational ideas that have shaped the landscape of voting theory as we understand them today. As it turns out, the axiomatic approach of social choice involves defining certain “properties” that formally capture the quality of a voting rule. For example, we would not want a voting rule to be, informally speaking, a dictatorship, which would essentially mean that it discards all but one voter’s input. Unfortunately, a series of cornerstone results establish that it is impossible to devise voting rules which respect some of the simplest desirable properties. Indeed, the classic work of Arrow [Arr50] and Gibbard-Satterthwaite [Gib73, Sat75] show that there is no straight forward way to simultaneously deal with properties like voting paradoxes, strategy-proofness, nondictatorship, unanimity etc. We refer to [Mou91] for a more elaborate discussion on this. Making the matter worse, many classical voting rules, for example the Kemeny voting rule [Kem59, BNM+58, BITT89, HSV05], the Dodgson voting rule [Dod76, HHR97], the Young voting rule [You77] etc., turn out to be computationally intractable.\nThis brings us to the second reason for why structured preferences are an important consideration. The notion of single-peakedness that we mentioned earlier is an excellent illustration (we refer the reader to Section 3.2 for the formal definition). Introduced in [Bla48], it not only captures the essence of structure in political elections, but also turns out to be extremely conducive to many natural theoretical considerations. To begin with, one can devise voting rules that are “nice” with respect to several properties, when preferences are singlepeaked. Further, they are structurally elegant from the point of view of winner determination, since they always admit a weak Condorcet winner — a candidate which is not defeated by any other candidate in pairwise election — thus working around the Condorcet paradox which is otherwise a prominent concern in the general scenario. In a landmark contribution, Brandt et al. [BBHH15] show that several computational problems in social choice theory that are\n31\nintractable in the general setting become polynomial time solvable when we consider singlepeaked preferences.\nA natural question at this point is if the problem of elicitation becomes any easier — that is, if we can get away with fewer queries — by taking advantage of the structure provided by single-peakedness. It turns out that the answer to this is in the affirmative, as shown in a detailed study by Conitzer [Con09]. The definition of single-peakedness involves an ordering over the candidates (called the harmonious ordering by some authors). The work of Conitzer [Con09] shows that O(mn) queries suffice, assuming either that the harmonious ordering is given, or one of the votes is known, where m and n are the number of candidates and voters respectively.\nWe now return to the theme of structural restrictions on preferences. As it turns out, the single peaked preference domain has subsequently been generalized to single peakedness on trees (roughly speaking, these are profiles that are single peaked on every path on a given tree) [Dem82, Tri89]. This is a class that continues to exhibit many desirable properties of single peaked domain. For example, there always exists a weak Condorcet winner and further, many voting rules that are intractable in an unrestricted domain are polynomial time computable if the underlying single peaked tree is “nice” [YCE13, PE16]. We note the class of profiles that are single peaked on trees are substantially more general than the class of single peaked preferences. Note that the latter is a special case since a path is, in particular, a tree. Our work here addresses the issue of elicitation on profiles that are single-peaked on trees, and can be seen as a significant generalization of the results in [Con09]. We now detail the specifics of our contributions."
    }, {
      "heading" : "3.1.1 Our Contribution",
      "text" : "We study the query complexity for preference elicitation when the preference profile is single peaked on a tree. We provide tight connections between various parameters of the underlying single peaked tree and the query complexity for preference elicitation. Our broad goal is to provide a suitable generalization of preference elicitation for single peaked profiles to profiles that are single peaked on trees. Therefore, we consider various ways of quantifying the “closeness” of a tree to a path, and reflect on how these measures might factor into the query complexity of an algorithm that is actively exploiting the underlying tree structure.\nWe summarize our results for preference elicitation in Table 3.1, where the readers will note that most of the parameters (except diameter) chosen are small constants (typically\n32\nzero, one or two) when the tree under consideration is a path. Observe that in some cases — such as the number of leaves, or the path cover number — the dependence on the parameter is transparent (and we recover the results of [Con09] as a special case), while in other cases, it is clear that the perspective provides no additional mileage (the non-trivial results here are the matching lower bounds).\nIn terms of technique, our strategy is to “scoop out the paths from the tree” and use the algorithm form [Con09] to efficiently elicit the preference on the parts of the trees that are paths. We then efficiently merge this information across the board, and that aspect of the algorithm varies depending on the parameter under consideration. The lower bounds typically come from trees that provide large “degrees of freedom” in reordering candidates, typically these are trees that don’t have too many long paths (such as stars). The arguments are often subtle but intuitive.\nWe then study the query complexity for finding a weak Condorcet winner of a preference profile which is single peaked on a tree. Here, we are able to show that a weak Condorcet winner can be found with far fewer queries than the corresponding elicitation problem. In particular, we establish that a weak Condorcet winner can be found using O(mn) many queries for profiles that are single peaked on trees [Theorem 3.7], and we also show that this bound is the best that we can hope for [Theorem 3.11]. We also consider the problem for the special\n33\ncase of single peaked profiles. While Conitzer in [Con09] showed that Ω(mn) queries are necessary to determine the aggregate ranking, we show that only O(n logm) queries suffice if we are just interested in (one of the) weak Condorcet winners. Moreover, we show this bound is tight under the condition that the algorithm does not interleave queries to different voters [Theorem 3.12] (our algorithm indeed satisfies this condition).\nFinally, for expressing the query complexity for determining a weak Condorcet winner in terms of a measure of closeness to a path, we show an algorithm with query complexity O(nk logm) where k is the path cover number of T [Theorem 3.10] or the number of leaves in T [Corollary 3.6]. We now elaborate further on our specific contributions for preference elicitation.\n⊲ We design novel algorithms for preference elicitation for profiles which are single\npeaked on a tree with ℓ leaves with query complexity O(mn log ℓ) [Corollary 3.1]. Moreover, we prove that there exists a tree T with ℓ leaves such that any preference elicitation algorithm for profiles which are single peaked on tree T has query complexity Ω(mn log ℓ) [Theorem 3.4]. We show similar results for the parameter path cover number of the underlying tree [Theorem 3.2 and Corollary 3.2].\n⊲ We provide a preference elicitation algorithm with query complexity O(mn + nd logd)\nfor single peaked profiles on trees which can be made into a path by deleting at most d nodes [Theorem 3.3]. We show that our query complexity upper bound is tight up to constant factors [Theorem 3.6]. These results show that the query complexity for preference elicitation tightly depends on the number of leaves, the path cover number, and the distance from path of the underlying tree.\n⊲ We then show that there exists a tree T with pathwidth one or logm [Corollary 3.5] or\nmaximum degree is 3 or m − 1 [Corollary 3.3] or diameter is 2 or m/2 [Corollary 3.4] such that any preference elicitation algorithm for single peaked profiles on the tree T has query complexity Ω(mn logm). These results show that the query complexity for preference elicitation does not directly depend on the parameters above.\nWe next investigate the query complexity for finding a weak Condorcet winner for profiles\nwhich are single peaked on trees and we have the following results.\n⊲ We show that a weak Condorcet winner can be found using O(mn) queries for profiles\nthat are single peaked on trees [Theorem 3.7] which is better than the query complexity\n34\nfor preference elicitation. Moreover, we prove that this bound is tight in the sense that any algorithm for finding a weak Condorcet winner for profiles that are single peaked on stars has query complexity Ω(mn) [Theorem 3.11].\n⊲ On the other hand, we can find a weak Condorcet winner using only O(n logm) queries\nfor single peaked profiles [Theorem 3.8]. Moreover, we show that this bound is tight under the condition that the algorithm does not interleave queries to different voters [Theorem 3.12] (our algorithm indeed satisfies this condition). For any arbitrary underlying single peaked tree T, we provide an algorithm for finding a weak Condorcet winner with query complexity O(nk logm) where k is the path cover number of T [Theorem 3.10] or the number of leaves in T [Corollary 3.6].\nTo summarize, we remark that our results non-trivially generalize earlier works on query complexity for preference elicitation in [Con09]. We believe revisiting the preference elicitation problem in the context of profiles that are single peaked on trees is timely, and that this work also provides fresh algorithmic and structural insights on the domain of preferences that are single peaked on trees."
    }, {
      "heading" : "3.1.2 Related Work",
      "text" : "We have already mentioned the work in [Con09] addressing the question of eliciting preferences in single-peaked profiles, which is the closest predecessor to our work. Before this, Conitzer and Sandholm addressed the computational hardness for querying minimally for winner determination [CS02]. They also prove that one would need to make Ω(mn logm) queries even to decide the winner for many commonly used voting rules [CS05] which matches with the trivial O(mn logm) upper bound for preference elicitation in unrestricted domain (due to sorting lower bound). Ding and Lin study preference elicitation under partial information setting and show interesting properties of what they call a deciding set of queries [DL13a]. Lu and Boutilier provide empirical study of preference elicitation under probabilistic preference model [LB11b] and devise several novel heuristics which often work well in practice [LB11a].\n35"
    }, {
      "heading" : "3.2 Domain of Single Peaked Profiles on Trees",
      "text" : "A preference ≻∈ L(C) over a set of candidates C is called single peaked with respect to an order ≻′∈ L(C) if, for every candidates x,y ∈ C, we have x ≻ y whenever we have either c ≻′ x ≻′ y or y ≻′ x ≻′ c, where c ∈ C is the candidate at the first position of ≻. A profile P = (≻i)i∈[n] is called single peaked with respect to an order ≻′∈ L(C) if ≻i is single peaked with respect to ≻′ for every i ∈ [n]. Notice that if a profile P is single peaked with respect to an order ≻′∈ L(C), then P is also single peaked with respect to the order ←−≻ ′. Example 3.1 exhibits an example of a single peaked preference profile.\nExample 3.1. (Example of single peaked preference profile) Consider a set C of m candidates, corresponding m distinct points on the Real line, a set V of n voters, also corresponding n points on the Real line, and the preference of every voter are based on their distance to the candidates – given any two candidates, every voter prefers the candidate nearer to her (breaking the tie arbitrarily). Then the set of n preferences is single peaked with respect to the ordering of the candidates according to the ascending ordering of their positions on the Real line.\nGiven a path Q = (x1, x2, . . . , xℓ) from a vertex x1 to another vertex xℓ in a tree T, we define the order induced by the path Q to be x1 ≻ x2 ≻ · · · ≻ xℓ. Given a tree T = (C,E) with the set of nodes as the set of candidates C, a profile P is called single peaked on the tree T if P is single peaked on the order induced by every path of the tree T; that is for every two candidates x,y ∈ C, the profile P(X) is single peaked with respect to the order ≻ on the set of candidates X induced by the unique path from x to y in T. We call the tree T the underlying single peaked tree of the preference profile P. It is known (c.f. [Dem82]) that there always exists a weakly Condorcet winner for a profile P which is single peaked on a tree T."
    }, {
      "heading" : "3.3 Problem Definitions and Known Results",
      "text" : "Suppose we have a profile P with n voters and m candidates. For any pair of distinct candidates x and y, and a voter ℓ ∈ [n], we introduce the boolean-valued function QUERY(x ≻ℓ y) as follows. The output of this function is TRUE if the voter ℓ prefers the candidate x over the candidate y and FALSE otherwise. We now formally state the two problems that we consider in this work.\n36\nDefinition 3.1. PREFERENCE ELICITATION\nGiven a tree T = (C,E) and an oracle access to the function QUERY (·) for a profile P which is single peaked on T, find P.\nSuppose we have a set of candidates C = {c1, . . . , cm}. We say that an algorithm A makes q queries if there are exactly q distinct tuples (ℓ, ci, cj) ∈ [n] × C × C with i < j such that A calls QUERY (ci ≻ℓ cj) or QUERY (cj ≻ℓ ci). We call the maximum number of queries made by an algorithm A for any input its query complexity.\nWe state some known results that we will appeal to later. The first observation employs a sorting algorithm like merge sort to elicit every vote with O(m logm) queries, while the second follows from the linear-time merge subroutine of merge sort ([Cor09]).\nObservation 3.1. There is a PREFERENCE ELICITATION algorithm with query complexity O(mn logm).\nObservation 3.2. Suppose C1,C2 ⊆ C form a partition of C and ≻ is a ranking of the candidates in C. Then there is a linear time algorithm that finds ≻ given ≻ (C1) and ≻ (C2) with query complexity O(|C|).\nTheorem 3.1. [Con09] There is a PREFERENCE ELICITATION algorithm with query complexity O(mn) for single peaked profiles.\nWe now state the WEAK CONDORCET WINNER problem, which asks for eliciting only up to the point of determining a weak Condercet winner (recall that at least one such winner is guaranteed to exist on profiles that are single-peaked on trees).\nDefinition 3.2. WEAK CONDORCET WINNER Given a tree T = (C,E) and an oracle access to the function QUERY (·) for a profile P which is single peaked on T, find a weak Condorcet winner of P."
    }, {
      "heading" : "3.4 Results for Preference Elicitation",
      "text" : "In this section, we present our results for PREFERENCE ELICITATION for profiles that are single peaked on trees. Recall that we would like to generalize Theorem 3.1 in a way to profiles that are single peaked on trees. Since the usual single peaked profiles can be viewed as profiles single peaked with respect to a path, we propose the following measures of how much a tree resembles a path.\n37\n⊲ Leaves. Recall any tree has at least two leaves, and paths are the trees that have exactly\ntwo leaves. We consider the class of trees that have ℓ leaves, and show an algorithm with query complexity of O(mn log ℓ).\n⊲ Path Cover. Consider the notion of a path cover number of a tree, which is the smallest\nnumber of disjoint paths that the tree can be partitioned into. Clearly, the path cover number of a path is one; and for trees that can be covered with k paths, we show an algorithm with query complexity O(mn log k).\n⊲ Distance from Paths. Let d be the size of the smallest set of vertices whose removal\nmakes the tree a path. Again, if the tree is a path, then the said set is simply the empty set. For trees that are at a distance d from being a path (in the sense of vertex deletion), we provide an algorithm with query complexity O(mn logd).\n⊲ Pathwidth and Maximum Degree. Finally, we note that paths are also trees that have\npathwidth one, and maximum degree two. These perspectives turn out to be less useful: in particular, there are trees where these parameters are constant, for which we show that elicitation is as hard as it would be on an arbitrary profile, and therefore the easy algorithm from Observation 3.1 is actually the best that we can hope for.\nFor the first three perspectives that we employ, that seemingly capture an appropriate aspect of paths and carry it forward to trees, the query complexities that we obtain are tight — we have matching lower bounds in all cases. Also, while considering structural parameters, it is natural to wonder if there is a class of trees that are incomparable with paths but effective for elicitation. Our attempt in this direction is to consider trees of bounded diameter. However, again, we find that this is not useful, as we have examples to show that there exist trees of diameter two that are as hard to elicit as general profiles.\nWe remark at this point that all these parameters are polynomially computable for trees, making the algorithmic results viable. For example, the distance from path can be found by finding the longest path in the single peaked tree which can be computed in polynomial amount of time [Cor09]. Also, for the parameters of pathwidth, maximum degree and diameter, we show lower bounds on trees where these parameters are large (such as trees with pathwidthO(logm), maximum degreem−1, and diameterm/2), which — roughly speaking — also rules out the possibility of getting a good inverse dependence. As a concrete example, motivated by the O(mn) algorithm for paths, which have diameter m, one might wonder if\n38\nthere is an algorithm with query complexity O(mn logm logω ) for single peaked profiles on a tree with diameter ω. This possibility, in particular, is ruled out. We are now ready to discuss the results in Table 3.1.\nWe begin with showing a structural result about trees: any tree with ℓ leaves can be partitioned into ℓ paths. The idea is to fix some non-leaf node as root and iteratively find a path from low depth nodes (depth of a node is its distance from root) to some leaf node which is disjoint from all the paths chosen so far. We formalize this idea below.\nLemma 3.1. Let T = (X,E) be a tree with ℓ leaves. Then there is a polynomial time algorithm which partitions T into ℓ disjoint paths Qi = (Xi,Ei), i ∈ [ℓ]; that is we have Xi∩Xj = ∅,Ei∩Ej = ∅ for every i, j ∈ [ℓ] with i 6= j, X = ∪i∈[ℓ]Xi, E = ∪i∈[ℓ]Ei, and Qi is a path in T for every i ∈ [ℓ].\nProof. We first make the tree T rooted at any arbitrary nonleaf node r. We now partition the tree T into paths iteratively as follows. Initially every node of the tree T is unmarked and the set of paths Q we have is empty. Let Q1 = (X1,E1) be the path in T from the root node to any leaf node. We put Q1 into Q and mark all the nodes in Q1. More generally, in the i th iteration we pick an unmarked node u that is closest to the root r, breaking ties arbitrarily, and add any path Qi in T from u to any leaf node in the subtree Tu rooted at u, and mark all the nodes in Qi = (Xi,Ei). Since u is unmarked, we claim that every node in Tu is unmarked. Indeed, otherwise suppose there is a nodew in Tu which is already marked. Then there exists two paths from r to w one including u and another avoiding u since u is currently unmarked and w is already marked. This contradicts the fact that T is a tree. Hence every node in Tu is unmarked. We continue until all the leaf nodes are marked and return Q. Since T has ℓ leaves and in every iteration at least one leaf node is marked (since every Qi contains a leaf node), the algorithm runs for at most ℓ iterations. Notice that, since the algorithm always picks a path consisting of unmarked vertices only, the set of paths in Q are pairwise disjoint. We claim that Q forms a partition of T. Indeed, otherwise there must be a node x in T that remains unmarked at the end. From the claim above, we have all the nodes in the subtree Tx rooted at x unmarked which includes at least one leaf node of T. This contradicts the fact that the algorithm terminates when all the leaf nodes are marked.\nUsing Lemma 3.1, and the fact that any path can account for at most two leaves, we have that the path cover number of a tree is the same as the number of leaves up to a factor of two.\n39\nLemma 3.2. Suppose the path cover number of a tree T with ℓ leaves is k. Then we have ℓ/2 6 k 6 ℓ.\nProof. The inequality ℓ/2 6 k follows from the fact that any path in T can involve at most two leaves in T and there exists k paths covering all the leaf nodes. The inequality k 6 ℓ follows from the fact from Lemma 3.1 that any tree T with ℓ leaves can be partitioned into ℓ paths."
    }, {
      "heading" : "3.4.1 Algorithmic Results for PREFERENCE ELICITATION",
      "text" : "We now present our main algorithmic results. We begin with generalizing the result of Theorem 3.1 to any single peaked profiles on trees whose path cover number is at most k. The idea is to partition the tree into k disjoint paths, use the algorithm from Theorem 3.1 on each paths to obtain an order of the candidates on each path of the partition, and finally merge these suborders intelligently. We now formalize this idea as follows.\nTheorem 3.2. There is a PREFERENCE ELICITATION algorithm with query complexity O(mn log k) for profiles that are single peaked on trees with path cover number at most k.\nProof. Since the path cover number is at most k, we can partition the tree T = (C,E) into t disjoint paths Pi = (Ci,Ei), i ∈ [t], where t is at most k. We now show that we can elicit any preference ≻ which is single peaked on the tree T by making O(m log t) queries which in turn proves the statement. We first find the preference ordering restricted to Ci using Theorem 3.1 by makingO(|Ci|) queries for every i ∈ [t]. This step needs ∑ i∈[t] O(|Ci|) = O(m) queries since Ci, i ∈ [t] forms a partition of C. We next merge the t orders ≻ (Ci), i ∈ [t], to obtain the complete preference ≻ by using a standard divide and conquer approach for t-way merging as follows which makes O(m log t) queries [HUA83].\nInitially we have t orders to merge. We arbitrarily pair the t orders into ⌈t/2⌉ pairs with at most one of them being singleton (when t is odd). By renaming, suppose the pairings are as follows: (≻ (C2i−1),≻ (C2i)), i ∈ [⌊t/2⌋]. Let us define C′i = C2i−1 ∪ C2i for every i ∈ [⌊t/2⌋] and C′⌈t/2⌉ = Ct if t is an odd integer. We merge ≻ (C2i−1) and ≻ (C2i) to get ≻ (C′i) for every i ∈ [⌊t/2⌋] using Observation 3.2. The number queries the algorithm makes in this iteration is ∑\ni∈[⌊t/2⌋]O(|C2i−1|+ |C2i|) = O(m) since (Ci)i∈[t] forms a partition of C. At the end of the first iteration, we have ⌈t/2⌉ orders ≻ (C′i), i ∈ [⌈t/2⌉] to merge to get ≻. The algorithm repeats the step above O(log t) times to obtain ≻ and the query complexity of each iteration is O(m). Thus the query complexity of the algorithms is O(m +m log t) = O(m logk).\n40\nUsing the fact from Lemma 3.2 that a tree with ℓ leaves can be partitioned into at most ℓ paths, we can use the algorithm from Theorem 3.2 to obtain the following bound on query complexity for preference elicitation in terms of leaves.\nCorollary 3.1. There is a PREFERENCE ELICITATION algorithm with query complexity O(mn log ℓ) for profiles that are single peaked on trees with at most ℓ leaves.\nFinally, if we are given a subset of candidates whose removal makes the single peaked tree a path, then we have an elicitation algorithm that makes O(mn + nd logd) queries. As before, we first determine the ordering among the candidates on the path (after removing those d candidates) with O(m − d) queries. We then determine the ordering among the rest in O(d logd) queries using Observation 3.1 and merge using Observation 3.2 these two orderings. This leads us to the following result.\nTheorem 3.3. There is a PREFERENCE ELICITATION algorithm with query complexity O(mn + nd logd) for profiles that are single peaked on trees with distance d from path.\nProof. Let X be the smallest set of nodes of a tree T = (C,E) such that T \\ X, the subgraph of T after removal of the nodes in X, is a path. We have |X| 6 d. For any preference ≻, we make O(d logd) queries to find ≻ (X) using Observation 3.1, make O(|C \\ X|) = O(m − d) queries to find ≻ (C \\ X) using Theorem 3.1, and finally make O(m) queries to find ≻ by merging ≻ (X) and ≻ (C \\ X) using Observation 3.2. This gives an overall query complexity of O(mn + nd logd)."
    }, {
      "heading" : "3.4.2 Lower Bounds for PREFERENCE ELICITATION",
      "text" : "We now turn to query complexity lower bounds for preference elicitation. Our first result is based on a counting argument, showing that the query complexity for preference elicitation in terms of the number of leaves, given by Corollary 3.1, is tight up to constant factors. Indeed, let us consider a subdivision of a star T with ℓ leaves and let t denote the distance from the center to the leaves, so that we have a total of tℓ + 1 candidates. One can show that if the candidates are written out in level order, the candidates that are distance i from the star center can be ordered arbitrarily within this ordering. This tells us that the number of possible preferences ≻ that are single peaked on the tree T is at least (ℓ!)t. We obtain the lower bound by using a decision tree argument, wherein we are able to show that it is always possible for an oracle to answer the comparison queries asked by the algorithm in such a way\n41\nthat the total number of possibilities for the preference of the current voter decreases by at most a factor of two. Since the decision tree of the algorithm must entertain at least (ℓ!)t leaves to account for all possibilities, we obtain the claimed lower bound.\nTheorem 3.4. Let T = (C,E) be a balanced subdivision of a star on m nodes with ℓ leaves. Then any deterministic PREFERENCE ELICITATION algorithm for single peaked profiles on T has query complexity Ω(mn log ℓ).\nProof. Suppose the number of candidates m be (tℓ + 1) for some integer t. Let c be the center of T. We denote the shortest path distance between any two nodes x,y ∈ C in T by d(x,y). We consider the partition (C0, . . . ,Ct) of the set of candidates C where Ci = {x ∈ C : d(x, c) = i}. We claim that the preference ≻= π0 ≻ π1 ≻ · · · ≻ πt of the set of candidates C is single peaked on the tree T where πi is any arbitrary order of the candidates in Ci for every 0 6 i 6 t. Indeed; consider any path Q = (X,E′) in the tree T. Let y be the candidate closest to c among the candidates in X; that is y = argminx∈X d(x, c). Then clearly ≻ (X) is single peaked with respect to the path Q having peak at y. We have |Ci| = ℓ for every i ∈ [t] and thus the number of possible preferences ≻ that are single peaked on the tree T is at least (ℓ!)t.\nLet A be any PREFERENCE ELICITATION algorithm for single peaked profiles on the tree T. We now describe our oracle to answer the queries that the algorithm A makes. For every voter v, the oracle maintains the set Rv of possible preferences of the voter v which is consistent with the answers to all the queries that the algorithm A have already made for the voter v. At the beginning, we have |Rv| > (ℓ!) t for every voter v as argued above. Whenever the oracle receives a query on v for any two candidates x and y, it computes the numbers n1 and n2 of orders in Rv which prefers x over y and y over x respectively; the oracle can compute the integers n1 and n2 since the oracle has infinite computational power. The oracle answers that the voter v prefers the candidate x over y if and only if n1 > n2 and updates the set Rv accordingly. Hence, whenever the oracle is queried for a voter v, the size of the set Rv decreases by a factor of at most two. On the other hand, we must have, from the correctness of the algorithm, Rv to be a singleton set when the algorithm terminates for every voter v – otherwise there exists a voter v (whose corresponding Rv is not singleton) for which there exist two possible preferences which are single peaked on the tree T and are consistent with all the answers the oracle has given and thus the algorithm A fails to output the preference of the voter v correctly. Hence every voter must be queried at least Ω(log((ℓ!)t)) = Ω(tℓ log ℓ) = Ω(m log ℓ) times.\n42\nSince the path cover number of a subdivided star on ℓ leaves is at least ℓ/2, we also obtain\nthe following.\nCorollary 3.2. There exists a tree T with path cover number k such that any deterministic PREFERENCE ELICITATION algorithm for single peaked profiles on T has query complexity Ω(mn log k).\nMimicking the level order argument above on a generic tree with ℓ leaves, and using the connection between path cover and leaves, we obtain lower bounds that are functions of (n, ℓ) and (n, k), as given below. This will be useful for our subsequent results.\nTheorem 3.5. Let T = (C,E) be any arbitrary tree with ℓ leaves and path cover number k. Then any deterministic PREFERENCE ELICITATION algorithm for single peaked profiles on T has query complexity Ω(nℓ log ℓ) and Ω(nk log k).\nProof. Let X be the set of leaves in T. We choose any arbitrary nonleaf node r as the root of T. We denote the shortest path distance between two candidates x,y ∈ C in the tree T by d(x,y). Let t be the maximum distance of a node from r in T; that is t = maxy∈C\\X d(r, x). We partition the candidates in C \\X as (C0,C1, . . . ,Ct) where Ci = {y ∈ C \\X : d(r,y) = i} for 0 6 i 6 t. We claim that the preference ≻= π0 ≻ π1 ≻ · · · ≻ πt ≻ π of the set of candidates C is single peaked on the tree T where πi is any arbitrary order of the candidates in Ci for every 0 6 i 6 t and π is an arbitrary order of the candidates in C \\ X. Indeed, otherwise consider any path Q = (Y,E′) in the tree T. Let y be the candidate closest to r among the candidates in Y; that is y = argminx∈Y d(x, r). Then clearly ≻ (Y) is single peaked with respect to the path Q having peak at y. We have the number of possible preferences ≻ that are single peaked on the tree T is at least |X|! = ℓ!. Again using the oracle same as in the proof of Theorem 3.4, we deduce that any PREFERENCE ELICITATION algorithm A for profiles that are single peaked on the tree T needs to make Ω(nℓ log ℓ) queries. The bound with respect to the path cover number now follows from Lemma 3.2.\nThe following results can be obtained simply by applying Theorem 3.5 on particular graphs. For instance, we use the fact that stars have (m − 1) leaves and have pathwidth one to obtain the first part of Corollary 3.5, while appealing to complete binary trees that have O(m) leaves and pathwidth O(logm) for the second part. These examples also work in the context of maximum degree, while for diameter we use stars and caterpillars with a central path of length m/2 in Corollary 3.4.\n43\nCorollary 3.3. There exist two trees T and T′ with maximum degree ∆ = 3 andm−1 respectively such that any deterministic PREFERENCE ELICITATION algorithm for single peaked profiles on T and T′ respectively has query complexity Ω(mn logm).\nProof. Using Theorem 3.5, we know that any PREFERENCE ELICITATION algorithm for profiles which are single peaked on a complete binary tree has query complexity Ω(mn logm) since a complete binary tree has Ω(m) leaves. The result now follows from the fact that the maximum degree ∆ of a node is three for any binary tree. The case of ∆ = m − 1 follows immediately from Theorem 3.5 applied on stars.\nCorollary 3.4. There exists two trees T and T′ with diameters ω = 2 and ω = m/2 respectively such that any deterministic PREFERENCE ELICITATION algorithm for profiles which are single peaked on T and T′ respectively has query complexity Ω(mn logm).\nProof. The ω = 2 andω = m/2 cases follow from Theorem 3.5 applied on star and caterpillar graphs with a central path of length m/2 respectively.\nWe next consider the parameter pathwidth of the underlying single peaked tree. We immediately get the following result for PREFERENCE ELICITATION on trees with pathwidths one or logm from Theorem 3.5 and the fact that the pathwidths of a star and a complete binary tree are one and logm respectively.\nCorollary 3.5. There exist two trees T and T′ with pathwidths one and logm respectively such that any deterministic PREFERENCE ELICITATION algorithm for single peaked profiles on T and T′ respectively has query complexity Ω(mn logm).\nOur final result, which again follows from Theorem 3.5 applied of caterpillar graphs with\na central path of length m− d, shows that the bound in Theorem 3.3 is tight.\nTheorem 3.6. For any integers m and d with 1 6 d 6 m/4, there exists a tree T with distance d from path such that any deterministic PREFERENCE ELICITATION algorithm for profiles which are single peaked on T has query complexity Ω(mn + nd logd).\nProof. Consider the caterpillar graph where the length of the central path Q is m − d; there exists such a caterpillar graph since d 6 m/4. Consider the order ≻= π ≻ σ of the set of candidates C where π is an order of the candidates in Q which is single peaked on Q and σ is any order of the candidates in C \\ Q. Clearly, ≻ is single peaked on the tree T. Any elicitation algorithm A needs to make Ω(m − d) queries involving only the candidates in Q to elicit π\n44\ndue to [Con09] and Ω(d logd) queries to elicit σ due to sorting lower bound for every voter. This proves the statement."
    }, {
      "heading" : "3.5 Results for Weak Condorcet Winner",
      "text" : "In this section, we present our results for the query complexity for finding a weak Condorcet winner in profiles that are single peaked on trees."
    }, {
      "heading" : "3.5.1 Algorithmic Results for Weak Condorcet Winner",
      "text" : "We now show that we can find a weak Condorcet winner of profiles that are single peaked on trees using fewer queries than the number of queries needed to find the profile itself. We note that if a Condorcet winner is guaranteed to exist for a profile, then it can be found using O(mn) queries — we pit an arbitrary pair of candidates x,y and use O(n) queries to determine if x defeats y. We push the winning candidate forward and repeat the procedure, clearly requiring at most m rounds. Now, if a profile is single peaked with respect to a tree, and there are an odd number of voters, then we have a Condorcet winner and the procedure that we just described would work. Otherwise, we simply find a Condorcet winner among the first (n − 1) voters. It can be easily shown that such a winner is one of the weak Condorcet winners for the overall profile, and we therefore have the following results. We begin with the following general observation.\nObservation 3.3. Let P be a profile where a Condorcet winner is guaranteed to exist. Then we can find the Condorcet winner of P by making O(mn) queries.\nProof. For any two candidates x,y ∈ C we find whether x defeats y or not by simply asking all the voters to compare x and y; this takes O(n) queries. The algorithms maintains a set S of candidates which are potential Condorcet winners. We initialize S to C. In each iteration we pick any two candidates x,y ∈ S from S, remove x from S if x does not defeat y and vice versa using O(n) query complexity until S is singleton. After at most m− 1 iterations, the set S will be singleton and contain only Condorcet winner since we find a candidate which is not a Condorcet winner in every iteration and thus the size of the set S decreases by at least one in every iteration. This gives a query complexity bound of O(mn).\nUsing Observation 3.3 we now develop a WEAK CONDORCET WINNER algorithm with\nquery complexity O(mn) for profiles that are single peaked on trees.\n45\nTheorem 3.7. There is a WEAK CONDORCET WINNER algorithm with query complexity O(mn) for single peaked profiles on trees.\nProof. Let P = (≻i)i∈[n] be a profile which is single peaked on a tree T. If n is an odd integer, then we know that there exists a Condorcet winner in P since no two candidates can tie and there always exists at least one weak Condorcet winner in every single peaked profile on trees. Hence, if n is an odd integer, then we use Observation 3.3 to find a weak Condorcet winner which is the Condorcet winner too. Hence let us now assume that n is an even integer. Notice that P−1 = (≻2, . . . ,≻n) is also single peaked on T and has an odd number of voters and thus has a Condorcet winner. We use Observation 3.3 to find the Condorcet winner c of P−1 and output c as a weak Condorcet winner of P. We claim that c is a weak Condorcet winner of P. Indeed otherwise there exists a candidate x other than c who defeats c in P. Since n is an even integer, x must defeat c by a margin of at least two (since all pairwise margins are even integers) in P. But then x also defeats c by a margin of at least one in P−1. This contradicts the fact that c is the Condorcet winner of P−1.\nFor the special case of single peaked profiles, we can do even better. Here we take advantage of the fact that a “median candidate” [MCWG95] is guaranteed to be a weak Condorcet winner. We make O(logm) queries per vote to find the candidates placed at the first position of all the votes using the algorithm in [Con09] and find a median candidate to have an algorithm for finding a weak Condorcet winner. If a profile is single peaked (on a path), then there is a WEAK CONDORCET WINNER algorithm with query complexity O(n logm) as shown below. Let us define the frequency f(x) of a candidate x ∈ C to be the number of votes where x is placed at the first position. Then we know that a median candidate according to the single peaked ordering of the candidates along with their frequencies as defined above is a weak Condorcet winner for single peaked profiles [MCWG95].\nTheorem 3.8. There is a WEAK CONDORCET WINNER algorithm with query complexity O(n logm) for single peaked profiles (on a path).\nProof. Let P be a profile that is single peaked with respect to an ordering ≻∈ L(C) of candidates. Then we find, for every voter v, the candidate the voter v places at the first position using O(logm) queries using the algorithm in [Con09] and return a median candidate.\nThe next result uses Theorem 3.8 on paths in a path cover of the single peaked tree eliminating the case of even number of voters by the idea of setting aside one voter that was used in Theorem 3.7.\n46\nTheorem 3.9. Let T be a tree with path cover number at most k. Then there is an algorithm for WEAK CONDORCET WINNER for profiles which are single peaked on T with query complexity O(nk logm).\nRecalling that the number of leaves bounds the path cover number, we have the following\nconsequence.\nCorollary 3.6. Let T be a tree with ℓ leaves. Then there is an algorithm for WEAK CONDORCET WINNER for profiles which are single peaked on T with query complexity O(nℓ logm).\nFrom Theorem 3.7 and 3.8 we have the following result for any arbitrary tree.\nTheorem 3.10. Let T be a tree with path cover number at most k. Then there is an algorithm for WEAK CONDORCET WINNER for profiles which are single peaked on T with query complexity O(nk logm).\nProof. Let P be the input profile and Qi = (Xi,Ei) i ∈ [t] be t(6 k) disjoint paths that cover the tree T. Here again, if the number of voters is even, then we remove any arbitrary voter and the algorithm outputs the Condorcet winner of the rest of the votes. The correctness of this step follows from the proof of Theorem 3.7. Hence we assume, without loss of generality, that we have an odd number of voters. The algorithm proceeds in two stages. In the first stage, we find the Condorcet winner wi of the profile P(Xi) for every i ∈ [t] using Theorem 3.8. The query complexity of this stage is O(n ∑ i∈[t] log |Xi|) = O(nt log( m/t)). In the second stage, we find the Condorcet winner w of the profile P({wi : i ∈ [t]}) using Theorem 3.7 and output w. The query complexity of the second stage is O(nt log t). Hence the overall query complexity of the algorithm is O(nt log(m/t)) + O(nt log t) = O(nk logm)."
    }, {
      "heading" : "3.5.2 Lower Bounds for Weak Condorcet Winner",
      "text" : "We now state the lower bounds pertaining to WEAK CONDORCET WINNER. First, we show that any algorithm for WEAK CONDORCET WINNER for single peaked profiles on stars has query complexity Ω(mn), showing that the bound of Theorem 3.7 is tight.\nTheorem 3.11. Any deterministic WEAK CONDORCET WINNER algorithm for single peaked profiles on stars has query complexity Ω(mn).\nProof. Let T be a star with center vertex c. We now design an oracle that will “force” any WEAK CONDORCET WINNER algorithm A for single peaked profiles on T to make Ω(mn)\n47\nqueries. For every voter v, the oracle maintains a set of “marked” candidates which can not be placed at the first position of the preference of v. Suppose the oracle receives a query to compare two candidates x and y for a voter ℓ. If the order between x and y for the voter ℓ follows from the answers the oracle has already provided to all the queries for the voter ℓ, then the oracle answers accordingly. Otherwise it answers x ≻ℓ y if y is unmarked and marks y; otherwise the oracle answers y ≻ℓ x and marks x. Notice that the oracle marks at most one unmarked candidate every time it is queried. We now claim that there must be at least n/10 votes which have been queried at least m/4 times. If not, then there exists n−n/10 = 9n/10 votes each of which has at least m − m/4 = 3m/4 candidates unmarked. In such a scenario, there exists a constant N0 such that for every m,n > N0, we have at least two candidates x and y who are unmarked in at least (⌊n/2⌋ + 1) votes each. Now if the algorithm outputs x, then we put y at the first position in at least (⌊n/2⌋ + 1) votes and at the second position in the rest of the votes and this makes y the (unique) Condorcet winner. If the algorithm does not output x, then we put x at the first position in at least (⌊n/2⌋+ 1) votes and at the second position in the rest of the votes and this makes x the (unique) Condorcet winner. Hence the algorithm fails to output correctly in both the cases contradicting the correctness of the algorithm. Also the resulting profile is single peaked on T with center at y in the first case and at x in the second case. Therefore the algorithm A must have query complexity Ω(mn).\nOur next result uses an adversary argument, and shows that the query complexity for WEAK CONDORCET WINNER for single peaked profiles in Theorem 3.8 is essentially optimal, provided that the queries to different voters are not interleaved, as is the case with our algorithm.\nTheorem 3.12. Any deterministic WEAK CONDORCET WINNER algorithm for single peaked profiles which does not interleave the queries to different voters has query complexity Ω(n logm).\nProof. Let a profile P be single peaked with respect to the ordering of the candidates ≻= c1 ≻ c2 ≻ · · · ≻ cm. The oracle maintains two indices ℓ and r for every voter such that any candidate from {cℓ, cℓ+1, . . . , cr} can be placed at the first position of the preference of the voter v and still be consistent with all the answers provided by the oracle for v till now and single peaked with respect to ≻. The algorithm initializes ℓ to one and r to m for every voter. The oracle answers any query in such a way that maximizes the new value of r − ℓ. More specifically, suppose the oracle receives a query to compare candidates ci and cj with i < j for a voter v. If the ordering between ci and cj follows, by applying transitivity, from the answers\n48\nto the queries that have already been made so far for this voter, then the oracle answers accordingly. Otherwise the oracle answers as follows. If i < ℓ, then the oracle answers that cj is preferred over ci; else if j > r, then the oracle answers that ci is preferred over cj. Otherwise (that is when ℓ 6 i < j 6 r), if j − ℓ > r − i, then the oracle answers that ci is preferred over cj and changes r to j; if j−ℓ 6 r−i, then the oracle answers that cj is preferred over ci and changes ℓ to i. Hence whenever the oracle answers a query for a voter v, the value of r−ℓ for that voter v decreases by a factor of at most two. Suppose the election instance has an odd number of voters. Let V be the set of voters. Now we claim that the first ⌊n/5⌋ voters must be queried (logm − 1) times each. Suppose not, then consider the first voter v′ that is queried less than (logm− 1) times. Then there exist at least two candidates ct and ct+1 each of which can be placed at the first position of the vote v′. The oracle fixes the candidates at the first positions of all the votes that have not been queried till v′ is queried (and there are at least ⌈4n/5⌉ such votes) in such a way that ⌊n/2⌋ voters in V \\ {v′} places some candidate in the left of ct at the first positions and ⌊n/2⌋ voters in V \\ {v′} places some candidate in the right of ct+1. If the algorithm outputs ct as the Condorcet winner, then the oracle makes ct+1 the (unique) Condorcet winner by placing ct+1 at the top position of v ′, because ct+1 is the unique median in this case. If the algorithm does not output ct as the Condorcet winner, then the oracle makes ct the (unique) Condorcet winner by placing ct at the top position of v ′, because ct is the unique median in this case. Hence the algorithm fails to output correctly in both the cases thereby contradicting the correctness of the algorithm."
    }, {
      "heading" : "3.6 Conclusion",
      "text" : "In this work, we present algorithms for eliciting preferences of a set of voters when the preferences are single peaked on a tree thereby significantly extending the work of [Con09]. Moreover, we show non-trivial lower bounds on the number of comparison queries any preference elicitation algorithm would need to ask in a domain of single peaked profiles on a tree. From this, we conclude that our algorithms asks optimal number of comparison queries up to constant factors. Our main finding in this work is the interesting dependencies between the number of comparison queries any preference elicitation algorithm would ask and various parameters of the single peaked tree. For example, our results show that the query complexity for preference elicitation is a monotonically increasing function on the number of leaf nodes in the single peaked tree. On the other hand, the query complexity for preference elicitation\n49\ndoes not directly depend on other tree parameters like, maximum degree, minimum degree, path width, diameter etc.\nWe then move on to study query complexity for finding a weak Condorcet winner of a set of votes which is single peaked on a tree. Here, our results show that a weak Condorcet winner can be found with much less number of queries compared to preference elicitation.\nIn the next chapter, we explore the preference elicitation problem again, but for another\nwell-studied domain known as single crossing profiles.\n50\nChapter 4\nPreference Elicitation for Single Crossing\nProfiles\nIn this chapter, we consider the domain of single crossing preference profiles and\nstudy the query complexity of preference elicitation under various situations. We consider\ntwo distinct scenarios: when an ordering of the voters with respect to which the profile\nis single crossing is known a priori versus when it is unknown. We also consider two\ndifferent access models: when the votes can be accessed at random, as opposed to when\nthey are coming in a predefined sequence. In the sequential access model, we distinguish\ntwo cases when the ordering is known: the first is that the sequence in which the votes\nappear is also a single-crossing order, versus when it is not. The main contribution of our\nwork is to provide polynomial time algorithms with low query complexity for preference\nelicitation in all the above six cases. Further, we show that the query complexities of our\nalgorithms are optimal up to constant factors for all but one of the above six cases."
    }, {
      "heading" : "4.1 Introduction",
      "text" : "Eliciting the preferences of a set of agents is a nontrivial task since we often have a large number of candidates (ranking restaurants for example) and it will be infeasible for the\nA preliminary version of the work in this chapter was published as [DM16b]: Palash Dey and Neeldhara Misra. Preference elicitation for single crossing domain. In Proc. Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 222-228, 2016.\n51\nagents to rank all of them. Hence it becomes important to elicit the preferences of the agents by asking them (hopefully a small number of) comparison queries only - ask an agent i to compare two candidates x and y.\nUnfortunately, it turns out that one would need to ask every voter Ω(m logm) queries to know her preference (due to sorting lower bound). However, if the preferences are not completely arbitrary, and admit additional structure, then possibly we can do better. Indeed, an affirmation of this thought comes from the work of [Con09, DM16a], who showed that we can elicit preferences using only O(m) queries per voter if the preferences are single peaked. The domain of single peaked preferences has been found to be very useful in modeling preferences in political elections. We, in this work, study the problem of eliciting preferences of a set of agents for another popular domain namely the domain of single crossing profiles. A profile is called single crossing if the voters can be arranged in a complete order ≻ such that for every two candidates x and y, all the voters who prefer x over y appear consecutively in ≻ [Mir71, Rob77]. Single crossing profiles have been extensively used to model income of a set of agents [Rob77, MR81]. The domain is single crossing profiles are also popular among computational social scientists since many computationally hard voting rules become tractable if the underlying preference profile is single crossing [MBC+16]."
    }, {
      "heading" : "4.1.1 Related Work",
      "text" : "Conitzer and Sandholm show that determining whether we have enough information at any point of the elicitation process for finding a winner under some common voting rules is computationally intractable [CS02]. They also prove in their classic paper [CS05] that one would need to make Ω(mn logm) queries even to decide the winner for many commonly used voting rules which matches with the trivial O(mn logm) upper bound (based on sorting) for preference elicitation in unrestricted domain.\nA natural question at this point is if these restricted domains allow for better elicitation algorithms as well. The answer to this is in the affirmative, and one can indeed elicit the preferences of the voters using only O(mn) many queries for the domain of single peaked preference profiles [Con09]. Our work belongs to this kind of research– we investigate the number of queries one has to ask for preference elicitation in single crossing domains. When some partial information is available about the preferences, Ding and Lin prove interesting properties of what they call a deciding set of queries [DL13a]. Lu and Boutilier empirically show that several heuristics often work well [LB11b, LB11a].\n52"
    }, {
      "heading" : "4.1.2 Single Crossing Domain",
      "text" : "A preference profile P = (≻1, . . . ,≻n) of n agents or voters over a set C of candidates is called a single crossing profile if there exists a permutation σ ∈ Sn of [n] such that, for every two distinct candidates x,y ∈ C, whenever we have x ≻σ(i) y and x ≻σ(j) y for two integers i and j with 1 6 i < j 6 n, we have x ≻σ(k) y for every i 6 k 6 j. Example 4.1 exhibits an example of a single crossing preference profile.\nExample 4.1. (Example of single crossing preference profile) Consider a set C of m candidates, corresponding m distinct points on the Real line, a set V of n voters, also corresponding n points on the Real line, and the preference of every voter are based on their distance to the candidates – given any two candidates, every voter prefers the candidate nearer to her (breaking the tie arbitrarily). Then the set of n preferences is single crossing with respect to the ordering of the voters according to the ascending ordering of their positions on the Real line.\nThe following observation is immediate from the definition of single crossing profiles.\nObservation 4.1. Suppose a profile P is single crossing with respect to an ordering σ ∈ Sn of votes. Then P is single crossing with respect to the ordering ←−σ too."
    }, {
      "heading" : "4.1.3 Single Crossing Width",
      "text" : "A preference P = (≻1, . . . ,≻n) of n agents or voters over a set C of candidates is called a single crossing profile with width w if the set of candidates C can be partitioned into (Ci)i∈[k] such that |Ci| 6 w for every i and for every two candidates x ∈ Cℓ and y ∈ Ct with ℓ 6= t from two different subsets of the partition, whenever we have x ≻σ(i) y and x ≻σ(j) y for two integers i and j with 1 6 i < j 6 n, we have x ≻σ(k) y for every i 6 k 6 j."
    }, {
      "heading" : "4.1.4 Our Contribution",
      "text" : "In this work we present novel algorithms for preference elicitation for the domain of single crossing profiles in various settings. We consider two distinct situations: when an ordering of the voters with respect to which the profile is single crossing is known versus when it is unknown. We also consider different access models: when the votes can be accessed at random, as opposed to when they are coming in a pre-defined sequence. In the sequential access model, we distinguish two cases when the ordering is known: the first is that sequence\n53\nin which the votes appear is also a single-crossing order, versus when it is not. We also prove lower bounds on the query complexity of preference elicitation for the domain of single crossing profiles; these bounds match the upper bounds up to constant factors (for a large number of voters) for all the six scenarios above except the case when we know a single crossing ordering of the voters and we have a random access to the voters; in this case, the upper and lower bounds match up to a factor of O(m). We summarize our results in Table 6.1.\nWe then extend our results to domains which are single crossing of width w in Section 4.3.3. We also prove that a weak Condorcet winner and the Condorcet winner (if it exists) of a single crossing preference profile can be found out with much less number of queries in Section 4.4."
    }, {
      "heading" : "4.2 Problem Formulation",
      "text" : "The query and cost model is the same as Definition 3.1 in Chapter 3. We recall it below for ease of access.\nSuppose we have a profile P with n voters and m candidates. Let us define a function QUERY(x ≻ℓ y) for a voter ℓ and two different candidates x and y to be TRUE if the voter ℓ prefers the candidate x over the candidate y and FALSE otherwise. We now formally define the problem.\n54\nDefinition 4.1. PREFERENCE ELICITATION Given an oracle access to QUERY (·) for a single crossing profile P, find P.\nFor two distinct candidates x,y ∈ C and a voter ℓ, we say a PREFERENCE ELICITATION algorithmAcompares candidates x and y for voter ℓ, ifAmakes a call to either QUERY(x ≻ℓ y) or QUERY(y ≻ℓ x). We define the number of queries made by the algorithm A, called the query complexity of A, to be the number of distinct tuples (ℓ, x,y) ∈ V × C × C with x 6= y such that the algorithm A compares the candidates x and y for voter ℓ. Notice that, even if the algorithm A makes multiple calls to QUERY (·) with same tuple (ℓ, x,y), we count it only once in the query complexity of A. This is without loss of generality since we can always implement a wrapper around the oracle which memorizes all the calls made to the oracle so far and whenever it receives a duplicate call, it replies from its memory without “actually” making a call to the oracle. We say two query complexities q(m,n) and q′(m,n) are tight up to a factor of ℓ for a large number of voters if 1/ℓ 6 limn→∞ q(m,n)/q′(m,n) 6 ℓ.\nNote that by using a standard sorting routine like merge sort, we can fully elicit an unknown preference using O(m logm) queries. We state this explicitly below, as it will be useful in our subsequent discussions.\nObservation 4.2. There is a PREFERENCE ELICITATION algorithm with query complexity O(m logm) for eliciting one vote from a single crossing preference profile."
    }, {
      "heading" : "4.2.1 Model of Input",
      "text" : "We study two models of input for PREFERENCE ELICITATION for single crossing profiles.\n⊲ Random access to voters: In this model, we have a set of voters and we are allowed\nto ask any voter to compare any two candidates at any point of time. Moreover, we are also allowed to interleave the queries to different voters. Random access to voters is the model of input for elections within an organization where every voter belongs to the organization and can be queried any time.\n⊲ Sequential access to voters: In this model, voters are arriving in a sequential manner\none after another to the system. Once a voter ℓ arrives, we can query voter ℓ as many times as we like and then we “release” the voter ℓ from the system to access the next voter in the queue. Once voter ℓ is released, it can never be queried again. Sequential\n55\naccess to voters is indeed the model of input in many practical elections scenarios such as political elections, restaurant ranking etc."
    }, {
      "heading" : "4.3 Results for Preference Elicitation",
      "text" : "In this section, we present our technical results. We first consider the (simpler) situation when one single crossing order is known, and then turn to the case when no single crossing order is a priori known. In both cases, we explore all the relevant access models."
    }, {
      "heading" : "4.3.1 Known Single Crossing Order",
      "text" : "We begin with a simple PREFERENCE ELICITATION algorithm when we are given a random access to the voters and one single crossing ordering is known.\nLemma 4.1. Suppose a profile P is single crossing with respect to a known permutation of the voters. Given a random access to voters, there is a PREFERENCE ELICITATION algorithm with query complexity O(m2 logn).\nProof. By renaming, we assume that the profile is single crossing with respect to the identity permutation of the votes. Now, for every (\nm 2\n)\npair of candidates {x,y} ⊂ C, we perform a binary search over the votes to find the index i({x,y}) where the ordering of x and y changes. We now know how any voter j orders any two candidates x and y from i({x,y}) and thus we have found P.\nInterestingly, the simple algorithm in Lemma 4.1 turns out to be optimal up to a multiplicative factor of O(m) as we prove next. The idea is to “pair up” the candidates and design an oracle which “hides” the vote where the ordering of the two candidates in any pair (x,y) changes unless it receives at least (logm − 1) queries involving only these two candidates x and y. We formalize this idea below. Observe that, when a single crossing ordering of the voters in known, we can assume without loss of generality, by renaming the voters, that the preference profile is single crossing with respect to the identity permutation of the voters.\nTheorem 4.1. Suppose a profile P is single crossing with respect to the identity permutation of votes. Given random access to voters, any deterministic PREFERENCE ELICITATION algorithm has query complexity Ω(m logm +m logn).\n56\nProof. The Ω(m logm) bound follows from the query complexity lower bound of sorting and the fact that any profile consisting of only one preference ≻∈ L(C) is single crossing. Let C = {c1, . . . , cm} be the set of m candidates where m is an even integer. Consider the ordering Q = c1 ≻ c2 ≻ · · · ≻ cm ∈ L(C) and the following pairing of the candidates: {c1, c2}, {c3, c4}, . . . , {cm−1, cm}. Our oracle answers QUERY (·) as follows. The oracle fixes the preferences of the voters one and n to be Q and ←− Q respectively. For every odd integer i ∈ [m], the oracle maintains θi (respectively βi) which corresponds to the largest (respectively smallest) index of the voter for whom (ci, ci+1) has already been queried and the oracle answered that the voter prefers ci over ci+1 (ci+1 over ci respectively). The oracle initially sets θi = 1 and βi = n for every odd integer i ∈ [m]. Suppose oracle receives a query to compare candidates ci and cj for i, j ∈ [m] with i < j for a voter ℓ. If i is an even integer or j− i > 2 (that is, ci and cj belong to different pairs), then the oracle answers that the voter ℓ prefers ci over cj. Otherwise we have j = i+1 and i is an odd integer. The oracle answers the query to be ci ≻ ci+1 and updates θi to ℓ keeping βi fixed if |ℓ − θi| 6 |ℓ − βi| and otherwise answers ci+1 ≻ ci and updates βi to ℓ keeping θi fixed (that is, the oracle answers according to the vote which is closer to the voter ℓ between θi and βi and updates θi or βi accordingly). If the pair (ci, ci+1) is queried less than (logn − 2) times, then we have βi − θi > 2 at the end of the algorithm since every query for the pair (ci, ci+1) decreases βi − θi by at most a factor of two and we started with βi − θi = n − 1. Consider a voter κ with θi < κ < βi. If the elicitation algorithm outputs that the voter κ prefers ci over ci+1 (respectively ci+1 over ci), then the oracle sets all the voters κ ′ with θi < κ ′ < βi to prefer ci+1 over ci (respectively ci over ci+1). Clearly, the algorithm does not elicit the preference of the voter κ correctly. Also, the profile is single crossing with respect to the identity permutation of the voters and consistent with the answers of all the queries made by the algorithm. Hence, for every odd integer i ∈ [m], the algorithm must make at least (logn − 1) queries for the pair (ci, ci+1) thereby making Ω(m logn) queries in total.\nWe now present our PREFERENCE ELICITATION algorithm when we have a sequential access to the voters according to a single crossing order. We elicit the preference of the first voter using Observation 4.2. From second vote onwards, we simply use the idea of insertion sort relative to the previously elicited vote [Cor09]. Since we are using insertion sort, any particular voter may be queried O(m2) times. However, we are able to bound the query complexity of our algorithm due to two fundamental reasons: (i) consecutive preferences will often be almost similar in a single crossing ordering, (ii) our algorithm takes only O(m)\n57\nqueries to elicit the preference of the current voter if its preference is indeed the same as the preference of the voter preceding it. In other words, every time we have to “pay” for shifting a candidate further back in the current vote, the relative ordering of that candidate with all the candidates that it jumped over is now fixed, because for these pairs, the one permitted crossing is now used up. We begin with presenting an important subroutine called Elicit(·) which finds the preference of a voter ℓ given another preference R by performing an insertion sort using R as the order of insertion.\nAlgorithm 1 Elicit(C, R, ℓ) Input: A set of candidates C = {ci : i ∈ [m]}, an ordering R = c1 ≻ · · · ≻ cm of C, a voter ℓ Output: Preference ordering ≻ℓ of voter ℓ on C 1: Q ← c1 ⊲ Q will be the preference of the voter ℓ 2: for i ← 2 to m do ⊲ ci is inserted in the ith iteration 3: Scan Q linearly from index i − 1 to 1 to find the index j where ci should be inserted\naccording to the preference of voter ℓ and insert ci in Q at j\n4: end for 5: return Q\nFor the sake of the analysis of our algorithm, let us introduce a few terminologies. Given two preferences ≻1 and ≻2, we call a pair of candidates (x,y) ∈ C × C, x 6= y, good if both ≻1 and ≻2 order them in a same way; a pair of candidates is called bad if it is not good. We divide the number of queries made by our algorithm into two parts: goodCost(·) and badCost(·) which are the number of queries made between good and respectively bad pair of candidates. In what follows, we show that goodCost(·) for Elicit(·) is small and the total badCost(·) across all the runs of Elicit(·) is small.\nLemma 4.2. The goodCost(Elicit(C,R, ℓ)) of Elicit(C,R, ℓ) is O(m) (good is with respect to the preferences R and ≻ℓ).\nProof. Follows immediately from the observation that in any iteration of the for loop at line 2 in Algorithm 1, only one good pair of candidates are compared.\nWe now use Algorithm 1 iteratively to find the profile. We present the pseudocode in Algorithm 2 which works for the more general setting where a single crossing ordering is known but the voters are arriving in any arbitrary order π. We next compute the query complexity of Algorithm 2 when voters are arriving in a single crossing order.\n58\nAlgorithm 2 PreferenceElicit(π) Input: π ∈ Sn Output: Profile of all the voters 1: Q[π(1)] ← Elicit ≻π(1) using Observation 4.2 ⊲ Q stores the profile 2: X ← {π(1)} ⊲ Set of voters’ whose preferences have already been elicited 3: for i ← 2 to n do ⊲ Elicit the preference of voter π(i) in iteration i 4: k ← minj∈X |j − i| ⊲ Find the closest known preference 5: R ← Q[k],X ← X ∪ {π(i)} 6: Q[π(i)] ← Elicit(C,R,π(i)) 7: end for 8: return Q\nTheorem 4.2. Assume that the voters are arriving sequentially according to an order with respect to which a profile P is single crossing. Then there is a PREFERENCE ELICITATION algorithm with query complexity O(mn +m2).\nProof. By renaming, let us assume, without loss of generality, that the voters are arriving according to the identity permutation idn of the voters and the profile P is single crossing with respect to idn. Let the profile P be (P1, P2, . . . , Pn) ∈ L(C)n. For two candidates x,y ∈ C and a voter i ∈ {2, . . . ,n}, let us define a variable b(x,y, i) to be one if x and y are compared for the voter i by Elicit(C,Pi−1, i) and (x,y) is a bad pair of candidates with respect to the preferences of voter i and i − 1; otherwise b(x,y, i) is defined to be zero. Then we have the following.\nCostPreferenceElicit(idn)\n= O(m logm) +\nn∑\ni=2\n(goodCost(QUERY(C, Pi−1, i)) + badCost(QUERY(C, Pi−1, i)))\n6 O(m logm+mn) +\nn∑\ni=2\nbadCost(QUERY(C, Pi−1, i))\n= O(m logm+mn) + ∑\n(x,y)∈C×C\n(\nn∑\ni=2\nb(x,y, i)\n)\n6 O(m logm+mn) + ∑\n(x,y)∈C×C 1\n= O(mn +m2)\n59\nThe first inequality follows from Lemma 4.2, the second equality follows from the definition of b(x,y, i), and the second inequality follows from the fact that ∑n\ni=2 b(x,y, i) 6 1 for every\npair of candidates (x,y) ∈ C since the profile P is single crossing.\nWe show next that, when the voters are arriving in a single crossing order, the query complexity upper bound in Theorem 4.3 is tight for a large number of voters up to constant factors. The idea is to pair up the candidates in a certain way and argue that the algorithm must compare the candidates in every pair for every voter thereby proving a Ω(mn) lower bound on query complexity.\nTheorem 4.3. Assume that the voters are arriving sequentially according to an order with respect to which a profile P is single crossing. Then any deterministic PREFERENCE ELICITATION algorithm has query complexity Ω(m logm+mn).\nProof. The Ω(m logm) bound follows from the fact that any profile consisting of only one preference P ∈ L(C) is single crossing. By renaming, let us assume without loss of generality that the profile P is single crossing with respect to the identity permutation of the voters. Suppose we have an even number of candidates and C = {c1, . . . , cm}. Consider the order Q = c1 ≻ c2 ≻ · · · ≻ cm and the pairing of the candidates {c1, c2}, {c3, c4}, . . . , {cm−1, cm}. The oracle answers all the query requests consistently according to the order Q till the first voter κ for which there exists at least one odd integer i ∈ [m] such that the pair (ci, ci+1) is not queried. If there does not exist any such κ, then the algorithm makes at least mn/2 queries thereby proving the statement. Otherwise, let κ be the first vote such that the algorithm does not compare ci and ci+1 for some odd integer i ∈ [m]. The oracle answers the queries for the rest of the voters {κ + 1, . . . ,n} according to the order Q′ = c1 ≻ c2 ≻ · · · ≻ ci−1 ≻ ci+1 ≻ ci ≻ ci+2 ≻ · · · ≻ cm. If the algorithm orders ci ≻κ ci+1 in the preference of the voter κ, then the oracle sets the preference of the voter κ to be Q′. On the other hand, if the algorithm orders ci+1 ≻κ ci in the preference of voter κ, then the oracle sets the preference of voter κ to be Q. Clearly, the elicitation algorithm fails to correctly elicit the preference of the voter κ. However, the profiles for both the cases are single crossing with respect to the identity permutation of the voters and are consistent with the answers given to all the queries made by the algorithm. Hence, the algorithm must make at least mn/2 queries.\nWe next move on to the case when we know a single crossing order of the voters; however, the voters arrive in an arbitrary order π ∈ Sn. The idea is to call the function Elicit(C,R, i) where the current voter is the voter i and R is the preference of the voter which is closest to\n60\ni according to a single crossing ordering and whose preference has already been elicited by the algorithm.\nTheorem 4.4. Assume that a profile P is known to be single crossing with respect to a known ordering of voters σ ∈ Sn. However, the voters are arriving sequentially according to an arbitrary order π ∈ Sn which may be different from σ. Then there is a PREFERENCE ELICITATION algorithm with query complexity O(mn +m2 logn).\nProof. By renaming, let us assume, without loss of generality, that the profile P is single peaked with respect to the identity permutation of the voters. Let the profile P be (P1, P2, . . . , Pn) ∈ L(C)n. Let f : [n] −→ [n] be the function such that f(i) is the k corresponding to the i at line 4 in Algorithm 2. For candidates x,y ∈ C and voter ℓ, we define b(x,y, ℓ) analogously as in the proof of Theorem 4.2. We claim that B(x,y) = ∑n i=2 b(x,y, i) 6 logn. To see this, we consider any arbitrary pair (x,y) ∈ C× C. Let the set of indices of the voters that have arrived immediately after the first time (x,y) contributes to B(x,y) be {i1, i2, . . . , it}. Without loss of generality, let us assume i1 < i2 < · · · < it. Again, without loss of generality, let us assume that voters i1, i2, . . . , ij prefer x over y and voters ij+1, . . . , it prefer y over x. Let us define ∆ to be the difference between smallest index of the voter who prefers y over x and the largest index of the voter who prefers x over y. Hence, we currently have ∆ = ij+1 − ij. A crucial observation is that if a new voter ℓ contributes to B(x,y) then we must necessarily have ij < ℓ < ij+1. Another crucial observation is that whenever a new voter contributes to B(x,y), the value of ∆ gets reduced at least by a factor of two by the choice of k at line 4 in Algorithm 2. Hence, the pair (x,y) can contribute at most (1 + log∆) = O(logn) to B(x,y) since we have ∆ 6 n to begin with. Then we have the following.\n61\nCostPreferenceElicit(π)\n= O(m logm) +\nn∑\ni=2\ngoodCost(QUERY(C, Pf(i), i)) + badCost(QUERY(C, Pf(i), i))\n6 O(m logm+mn) +\nn∑\ni=2\nbadCost(QUERY(C, Pf(i), i))\n= O(m logm+mn) + ∑\n(x,y)∈C×C\nn∑\ni=2\nb(x,y, i)\n6 O(m logm+mn) + ∑\n(x,y)∈C×C logn\n= O(mn +m2 logn)\nThe first inequality follows from Lemma 4.2, the second equality follows from the definition of b(x,y, i), and the second inequality follows from the fact that ∑n\ni=2 b(x,y, i) 6 logn."
    }, {
      "heading" : "4.3.2 Unknown Single Crossing Order",
      "text" : "We now turn our attention to PREFERENCE ELICITATION for single crossing profiles when no single crossing ordering is known. Before we present our PREFERENCE ELICITATION algorithm for this setting, let us first prove a few structural results about single crossing profiles which we will use crucially later. We begin with showing an upper bound on the number of distinct preferences in any single crossing profile.\nLemma 4.3. Let P be a profile on a set C of candidates which is single crossing. Then the number of distinct preferences in P is at most (\nm 2\n)\n+ 1.\nProof. By renaming, let us assume, without loss of generality, that the profile P is single crossing with respect to the identity permutation of the voters. We now observe that whenever the ith vote is different from the (i + 1)th vote for some i ∈ [n − 1], there must exist a pair of candidates (x,y) ∈ C× C whom the ith vote and the (i + 1)th vote order differently. Now the statement follows from the fact that, for every pair of candidates (a,b) ∈ C × C, there can exist at most one i ∈ [n − 1] such that the ith vote and the (i + 1)th vote order a and b differently.\n62\nWe show next that in every single crossing preference profile P where all the preferences are distinct, there exists a pair of candidates (x,y) ∈ C× C such that nearly half of the voters in P prefer x over y and the other voters prefer y over x.\nLemma 4.4. Let P be a preference profile of n voters such that all the preferences are distinct. Then there exists a pair of candidates (x,y) ∈ C such that x is preferred over y in at least ⌊n/2⌋ preferences and y is preferred over x in at least ⌊n/2⌋ preferences in P.\nProof. Without loss of generality, by renaming, let us assume that the profile P is single crossing with respect to the identity permutation of the voters. Since all the preferences in P are distinct, there exists a pair of candidates (x,y) ∈ C × C such that the voter ⌊n/2⌋ and the voter ⌊n/2⌋ + 1 order x and y differently. Let us assume, without loss of generality, that the voter ⌊n/2⌋ prefers x over y. Now, since the profile P is single crossing, every voter in [⌊n/2⌋] prefer x over y and every voter in {⌊n/2⌋+ 1, . . . ,n} prefer y over x.\nUsing Lemma 4.3 and 4.4 we now design a PREFERENCE ELICITATION algorithm when no single crossing ordering of the voters is known. The overview of the algorithm is as follows. At any point of time in the elicitation process, we have the set Q of all the distinct preferences that we have already elicited completely and we have to elicit the preference of a voter ℓ. We first search the set of votes Q for a preference which is possibly same as the preference ≻ℓ of the voter ℓ. It turns out that we can find a possible match ≻∈ Q using O(log |Q|) queries due to Lemma 4.4 which is O(logm) due to Lemma 4.3. We then check whether the preference of the voter ℓ is indeed the same as ≻ or not using O(m) queries. If ≻ is the same as ≻ℓ, then we have elicited ≻ℓ using O(m) queries. Otherwise, we elicit ≻ℓ using O(m logm) queries using Observation 4.2. Fortunately, Lemma 4.3 tells us that we would use the algorithm in Observation 4.2 at most O(m2) times. We present the pseudocode of our PREFERENCE ELICITATION algorithm in this setting in Algorithm 4. It uses Algorithm 3 as a subroutine which returns TRUE if the preference of any input voter is same as any given preference.\nTheorem 4.5. Assume that a profile P is known to be single crossing. However, no ordering of the voters with respect to which P is single crossing is known a priori. The voters are arriving sequentially according to an arbitrary order π ∈ Sn. Then there is a PREFERENCE ELICITATION algorithm with query complexity O(mn +m3 logm).\nProof. We present the pesudocode in Algorithm 4. We maintain two arrays in the algorithm. The array R is of length n and the jth entry stores the preference of voter j. The other array\n63\nAlgorithm 3 Same(R, ℓ) Input: R = c1 ≻ c2 ≻ · · · ≻ cm ∈ L(C), ℓ ∈ [n] Output: TRUE if the preference of the ℓth voter is R; FALSE otherwise 1: for i ← 1 to m− 1 do 2: if QUERY (ci ≻ℓ ci+1) = FALSE then 3: return FALSE ⊲ We have found a mismatch.\n4: end if 5: end for 6: return TRUE\nQ stores all the votes seen so far after removing duplicate votes; more specifically, if some specific preference ≻ has been seen ℓ many times for any ℓ > 0, Q stores only one copy of ≻. Upon arrival of voter i, we first check whether there is a preference in Qwhich is “potentially” same as the preference of voter i. At the beginning of the search, our search space Q′ = Q for a potential match in Q is of size |Q|. We next iteratively keep halving the search space as follows. We find a pair of candidates (x,y) ∈ C× C such that at least ⌊|Q′|/2⌋ preferences in Q′ prefer x over y and at least ⌊|Q′|/2⌋ preferences prefer y over x. The existence of such a pair of candidates is guaranteed by Lemma 4.4 and can be found in O(m2) time by simply going over all possible pairs of candidates. By querying how voter i orders x and y, we reduce the search space Q′ for a potential match in Q to a set of size at most ⌊|Q′|/2⌋ + 1. Hence, in O(logm) queries, the search space reduces to only one preference since we have |Q| 6 m2 by Lemma 4.3. Once we find a potential match w in Q (line 12 in Algorithm 4), we check whether the preference of voter i is the same asw or not usingO(m) queries. If the preference of voter i is indeed same as w, then we output w as the preference of voter i. Otherwise, we use Observation 4.2 to elicit the preference of voter i using O(m logm) queries and put the preference of voter i in Q. Since the number of times we need to use the algorithm in Observation 4.2 is at most the number of distinct votes in P which is known to be at most m2 by Lemma 4.3, we get the statement.\nTheorem 4.5 immediately gives us the following corollary in the random access to voters\nmodel when no single crossing ordering is known.\nCorollary 4.1. Assume that a profile P is known to be single crossing. However, no ordering of the voters with respect to which P is single crossing is known. Given a random access to voters, there is a PREFERENCE ELICITATION algorithm with query complexity O(mn +m3 logm).\n64\nAlgorithm 4 PreferenceElicitUnknownSingleCrossingOrdering(π) Input: π ∈ Sn Output: Profile of all the voters\n1: R,Q ← ∅ ⊲ Q stores all the votes seen so far without duplicate. R stores the profile. 2: for i ← 1 to n do ⊲ Elicit preference of the ith voter in ith iteration of this for loop. 3: Q′ ← Q 4: while |Q′| > 1 do ⊲ Search Q to find a vote potentially same as the preference of π(i) 5: Let x,y ∈ C be two candidates such that at least ⌊|Q′|/2⌋ votes in Q′ prefer x over y\nand at least ⌊|Q′|/2⌋ votes in Q′ prefer y over x. 6: if QUERY (x ≻π(i) y) = TRUE then 7: Q′ ← {v ∈ Q′ : v prefers x over y} 8: else 9: Q′ ← {v ∈ Q′ : v prefers y oer x}\n10: end if 11: end while 12: Let w be the only vote in Q′ ⊲ w is potentially same as the preference of π(i) 13: if Same(w,π(i)) = TRUE then ⊲ Check whether the vote π(i) is potentially same as w 14: R[π(i)] ← w 15: else 16: R[π(i)] ← Elicit using Observation 4.2 17: Q ← Q ∪ {R[π(i)]} 18: end if 19: end for 20: return R\nProof. Algorithm 4 works for this setting also and exact same bound on the query complexity holds.\nWe now show that the query complexity upper bound of Corollary 4.1 is tight up to con-\nstant factors for large number of voters.\nTheorem 4.6. Given a random access to voters, any deterministic PREFERENCE ELICITATION algorithm which do not know any ordering of the voters with respect to which the input profile is single crossing has query complexity Ω(m logm +mn).\nProof. The Ω(m logm) bound follows from sorting lower bound and the fact that any profile consisting of only one preference P ∈ L(C) is single crossing. Suppose we have an even number of candidates and C = {c1, . . . , cm}. Consider the ordering Q = c1 ≻ c2 ≻ · · · ≻ cm and the pairing of the candidates {c1, c2}, {c3, c4}, . . . , {cm−1, cm}. The oracle answers all the query requests consistently according to the ordering Q. We claim that any PREFERENCE\n65\nELICITATION algorithm A must compare ci and ci+1 for every voter and for every odd integer i ∈ [m]. Indeed, otherwise, there exist a voter κ and an odd integer i ∈ [m] such that the algorithm A does not compare ci and ci+1. Suppose the algorithm outputs a profile P′. If the voter κ prefers ci over ci+1 in P ′, then the oracle fixes the preference ≻κ to be c1 ≻ c2 ≻ · · · ≻ ci−1 ≻ ci+1 ≻ ci ≻ ci+2 ≻ · · · ≻ cm; otherwise the oracle fixes ≻κ to be Q. The algorithm fails to correctly output the preference of the voter κ in both the cases. Also the final profile with the oracle is single crossing with respect to any ordering of the voters that places the voter κ at the end. Hence, A must compare ci and ci+1 for every voter and for every odd integer i ∈ [m] and thus has query complexity Ω(mn)."
    }, {
      "heading" : "4.3.3 Single Crossing Width",
      "text" : "We now consider preference elicitation for profiles which are nearly single crossing. We begin with profiles with bounded single crossing width.\nProposition 4.1. Suppose a profile P is single crossing with width w. Given a PREFERENCE ELICITATION algorithm A with query complexity q(m,n) for random (or sequential) access to the voters when a single crossing order is known (or unknown), there exists a PREFERENCE ELICITATION algorithm A′ for the single crossing profiles with width w which has query complexity O(q(m/w,n) +mn logw) under same setting as A.\nProof. Let the partition of the set of candidates C with respect to which the profile P is single crossing be Ci, i ∈ [⌈m/w⌉]. Hence, C = ∪i∈[⌈m/w⌉]Ci and Ci ∩ Cj = ∅ for every i, j ∈ [⌈m/w⌉] with i 6= j. Let C′ be a subset of candidates containing exactly one candidate from Ci for each i ∈ [⌈m/w⌉]. We first find P(C′) using A. The query complexity of this step is O(q(m/w,n)). Next we find P(Ci) using Observation 4.2 for every i ∈ [m/w] thereby finding P. The overall query complexity is O(q(m/w,n) + (m/w)nw logw) = O(q(m/w,n) +mn logw).\nFrom Proposition 4.1, Lemma 4.1, Theorem 4.2, 4.4 and 4.5, and Corollary 4.1 we get the\nfollowing.\nCorollary 4.2. Let a profile P be single crossing with width w. Then there exists a PREFERENCE ELICITATION algorithm with query complexity O((m2/w) log(n/w) +mn logw) for known single crossing order and random access to votes, O(m2/w2 + mn logw) for sequential access to votes according to a single crossing order, O((m2/w2) log(n/w) + mn logw) for known single crossing order but arbitrary sequential access to votes, O(m3/w3 log(m/w)+mn logw) for unknown single crossing order and arbitrary sequential access to votes or random access to votes.\n66"
    }, {
      "heading" : "4.4 Results for Condorcet Winner",
      "text" : "Lemma 4.5. Let an n voter profile P is single crossing with respect to the ordering v1, v2, . . . , vn. Then the candidate which is placed at the top position of the vote v⌈n/2⌉ is a weak Condorcet winner.\nProof. Let c be the candidate which is placed at the top position of the vote v⌈n/2⌉ and w be any other candidate. Now the result follows from the fact that either all the votes in {vi : 1 6 i 6 ⌈n/2⌉} prefer c to w or all the votes in {vi : ⌈n/2⌉ 6 i 6 n} prefer c to w.\nLemma 4.5 immediately gives the following.\nCorollary 4.3. Given either sequential or random access to votes, there exists a WEAK CONDORCET WINNER algorithm with query complexity O(m) when a single crossing ordering is known.\nProof. Since we know a single crossing ordering v1, v2, . . . , vn, we can find the candidate at the first position of the vote v⌈n/2⌉ (which is a weak Condorcet winner by Lemma 4.5) by making O(m) queries.\nWe now move on to finding the Condorcet winner. We begin with the following observa-\ntion regarding existence of the Condorcet winner of a single crossing preference profile.\nLemma 4.6. Let an n voter profile P is single crossing with respect to the ordering v1, v2, . . . , vn. If n is odd, then the Condorcet winner is the candidate placed at the first position of the vote v⌈n/2⌉. If n is even, then there exists a Condorcet winner if and only there exists a candidate which is placed at the first position of both the votes vn/2 and vn/2+1.\nProof. If n is odd, the result follows from Lemma 4.5 and the fact that a candidate c is a Condorcet winner if and only if c is a weak Condorcet winner.\nNow suppose n is even. Let a candidate c is placed at the first position of the votes vn/2 and vn/2+1. Then for every candidate x ∈ C \\ {c}, either all the votes in {vi : 1 6 i 6 n/2 + 1} or all the votes in {vi : n/2 6 i 6 n} prefer c over x. Hence, c is a Condorcet winner. Now suppose a candidate w is a Condorcet winner. Then we prove that w must be placed at the first position of both the votes vn/2 and vn/2+1. If not, then both the two candidates placed at the first positions of the votes vn/2 and vn/2+1 are weak Condorcet winners according to Lemma 4.5 applied to single crossing orders v1, v2, . . . , vn and vn, vn−1, . . . , v1.\n67\nLemma 4.6 immediately gives us the following.\nCorollary 4.4. Given either sequential or random access to votes, there exists a WEAK CONDORCET WINNER algorithm with query complexity O(m) when a single crossing ordering is known.\nProof. Since we know a single crossing ordering v1, v2, . . . , vn, we can find the candidates which are placed at the first positions of the votes vn/2 and vn/2+1 by making O(m) queries. Now the result follows from Lemma 4.6."
    }, {
      "heading" : "4.5 Conclusion",
      "text" : "In this work, we have presented PREFERENCE ELICITATION algorithms with low query complexity for single crossing profiles under various settings. Moreover, we have proved that the query complexity of our algorithms are tight for a large number of voters up to constant factors for all but one setting namely when the voters can be accessed randomly but we do not know any ordering with respect to which the voters are single crossing. We then move on to show that a weak Condorcet winner and the Condorcet winner (if one exists) can be found from a single crossing preference profile using much less number of queries.\nWith this, we conclude the first part of the thesis. In the next part of the thesis, we study\nthe problem of finding a winner of an election under various real world scenarios.\n68\nPart II\nWinner Determination\nIn the second part of the thesis, we present our work on determining the winner of an\nelection under various circumstances. This part consists of the following chapters.\n⊲ In Chapter 5 – Winner Prediction andMargin of Victory Estimation – we present efficient\nalgorithms based on sampling to predict the winner of an election and its robustness. We prove that both the problems of winner prediction and robustness estimation can be solved simultaneously by sampling only a few votes uniformly at random.\n⊲ In Chapter 6 – Streaming Algorithms for Winner Determination – we develop (often)\noptimal algorithms for determining the winner of an election when the votes are arriving in a streaming fashion. Our results show that an approximate winner can be determined fast with a small amount of space.\n⊲ In Chapter 7 – Kernelization for Possible Winner and Coalitional Manipulation – we\npresent interesting results on kernelization for determining possible winners from a set of incomplete votes. Our results prove that the problem of determining possible winners with incomplete votes does not have any efficient preprocessing strategies under plausible complexity theoretic assumptions even when the number of candidates is relatively small. We also present efficient kernelization algorithms for the problem of manipulating an election.\n69\nChapter 5\nWinner Prediction and Margin of Victory\nEstimation\nPredicting the winner of an election and estimating the margin of victory of that election are favorite problems both for news media pundits and computational social choice theorists. Since it is often infeasible to elicit the preferences of all the voters in a typical prediction scenario, a common algorithm used for predicting the winner and estimating the margin of victory is to run the election on a small sample of randomly chosen votes and predict accordingly. We analyze the performance of this algorithm for many commonly used voting rules.\nMore formally, for predicting the winner of an election, we introduce the (ε, δ)WINNER DETERMINATION problem, where given an election E on n voters and m candidates in which the margin of victory is at least εn votes, the goal is to determine the winner with probability at least 1−δ where ε and δ are parameters with 0 < ε, δ < 1. The margin of victory of an election is the smallest number of votes that need to be modified in order to change the election winner. We show interesting lower and upper bounds on the number of samples needed to solve the (ε, δ)-WINNER DETERMINATION problem for many\nA preliminary version of the work on winner prediction in this chapter was published as [DB15]: Palash\nDey and Arnab Bhattacharyya. Sample complexity for winner prediction in elections. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2015, Istanbul, Turkey, May 4-8, 2015, pages 1421-1430, 2015.\nA preliminary version of the work on the estimation of margin of victory in this chapter was published as [DN15b]: Palash Dey and Y. Narahari. Estimating the margin of victory of an election using sampling. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 1120-1126, 2015.\n71\ncommon voting rules, including all scoring rules, approval, maximin, Copeland, Bucklin, plurality with runoff, and single transferable vote. Moreover, the lower and upper bounds match for many common voting rules up to constant factors.\nFor estimating the margin of victory of an election, we introduce the (c, ε, δ)–MARGIN\nOF VICTORY problem, where given an election E on n voters, the goal is to estimate the margin of victory M(E) of E within an additive error of cM(E)+εn with probability of er-\nror at most δ where ε, δ, and c are the parameters with 0 < ε, δ < 1 and c > 0. We exhibit interesting bounds on the sample complexity of the (c, ε, δ)–MARGIN OF VICTORY problem\nfor many commonly used voting rules including all scoring rules, approval, Bucklin, maximin, and Copelandα. We observe that even for the voting rules for which computing the\nmargin of victory is NP-hard, there may exist efficient sampling based algorithms for estimating the margin of victory, as observed in the cases of maximin and Copelandα voting\nrules."
    }, {
      "heading" : "5.1 Introduction",
      "text" : "In many situations, one wants to predict the winner without holding the election for the entire population of voters. The most immediate such example is an election poll. Here, the pollster wants to quickly gauge public opinion in order to predict the outcome of a full-scale election. For political elections, exit polls (polls conducted on voters after they have voted) are widely used by news media to predict the winner before official results are announced. In surveys, a full-scale election is never conducted, and the goal is to determine the winner, based on only a few sampled votes, for a hypothetical election on all the voters. For instance, it is not possible to force all the residents of a city to fill out an online survey to rank the local Chinese restaurants, and so only those voters who do participate have their preferences aggregated.\nIf the result of the poll or the survey has to reflect the true election outcome, it is obviously necessary that the number of sampled votes not be too small. Here, we investigate this fundamental question:\nWhat is the minimum number of votes that need to be sampled so that the winner of the election on the sampled votes is the same as the winner of the election on all the votes?\n72\nThis question can be posed for any voting rule. The most immediate rule to study is the plurality voting rule, where each voter votes for a single candidate and the candidate with most votes wins. Although the plurality rule is the most common voting rule used in political elections, it is important to extend the analysis to other popular voting rules. For example, the single transferable vote is used in political elections in Australia, India and Ireland, and it was the subject of a nationwide referendum in the UK in 2011. The Borda voting rule is used in the Icelandic parliamentary elections. Outside politics, in private companies and competitions, a wide variety of voting rules are used. For example, the approval voting rule has been used by the Mathematical Association of America, the American Statistical Institute, and the Institute of Electrical and Electronics Engineers, and Condorcet consistent voting rules are used by many free software organizations.\nA voting rule is called anonymous if the winner does not change after any renaming of the voters. All popular voting rules including the ones mentioned before are anonymous. For any anonymous voting rule, the question of finding the minimum number of vote samples required becomes trivial if a single voter in the election can change the winning candidate. In this case, all the votes need to be counted, because otherwise that single crucial vote may not be sampled. We get around this problem by assuming that in the elections we consider, the winning candidate wins by a considerable margin of victory. Formally, the margin of victory of an election is defined as the minimum number of votes that must be changed in order to change the election winner. Note that the margin of victory depends not only on the votes cast but also on the voting rule used in the election.\nOther than predicting the winner of an election, one may also like to know how robust the election outcome is with respect to the changes in votes [SYE13, CPS14, RGMT06]. One way to capture robustness of an election outcome is the margin of victory of that election. An election outcome is considered to be robust if the margin of victory of that election is large.\nIn addition to formalizing the notion of robustness of an election outcome, the margin of victory of an election plays a crucial role in many practical applications. One such example is post election audits — methods to observe a certain number of votes (which is often selected randomly) after an election to detect an incorrect outcome. There can be a variety of reasons for an incorrect election outcome, for example, software or hardware bugs in voting machine [NL07], machine output errors, use of various clip-on devices that can tamper with the memory of the voting machine [WWH+10], human errors in counting votes. Post election audits have nowadays become common practice to detect problems in electronic\n73\nvoting machines in many countries, for example, USA. As a matter of fact, at least thirty states in the USA have reported such problems by 2007 [NL07]. Most often, the auditing process involves manually observing some sampled votes. Researchers have subsequently proposed various risk limiting auditing methodologies that not only minimize the cost of manual checking, but also limit the risk of making a human error by sampling as few votes as possible [Sta08a, Sta08b, Sta09, SCS11]. The sample size in a risk limiting audit critically depends on the margin of victory of the election.\nAnother important application where the margin of victory plays an important role is polling. One of the most fundamental questions in polling is: how many votes should be sampled to be able to correctly predict the outcome of an election? It turns out that the sample complexity in polling too crucially depends on the margin of victory of the election from which the pollster is sampling [CEG95, DB15]. Hence, computing (or at least approximating sufficiently accurately) the margin of victory of an election is often a necessary task in many practical applications. However, in many applications including the ones discussed above, one cannot observe all the votes. For example, in a survey or polling, one cannot first observe all the votes to compute the margin of victory and then sample the required number of votes based on the margin of victory computed. Hence, one often needs a “good enough” estimate of the margin of victory by observing a small number votes. We precisely address this problem: estimate the margin of victory of an election by sampling as few votes as possible."
    }, {
      "heading" : "5.1.1 Our Contribution",
      "text" : "In this work, we show nontrivial bounds for the sample complexity of predicting the winner of an election and estimating the margin of victory of an election for many commonly used voting rules."
    }, {
      "heading" : "5.1.1.1 Winner Prediction",
      "text" : "Let n be the number of voters, m the number of candidates, and r a voting rule. We introduce and study the following problem in the context of winner prediction:\nDefinition 5.1. ((ε, δ)-WINNER DETERMINATION) Given a r-election E whose margin of victory is at least εn, determine the winner of the election with probability at least 1 − δ. (The probability is taken over the internal coin tosses of the algorithm.)\n74\nWe remind the reader that there is no assumption about the distribution of votes in this problem. Our goal is to solve the (ε, δ)-WINNER DETERMINATION problem by a randomized algorithm that is allowed to query the votes of arbitrary voters. Each query reveals the full vote of the voter. The minimum number of votes queried by any algorithm that solves the (ε, δ)-WINNER DETERMINATION problem is called the sample complexity of this problem. The sample complexity can of course depend on ε, δ, n, m, and the voting rule in use.\nA standard result in [CEG95] implies that solving the above problem for the majority rule\non 2 candidates requires at least (1/4ε2) ln( 1 8e √ πδ ) samples (Theorem 5.1). Also, a straightforward argument (Theorem 5.3) using Chernoff bounds shows that for any homogeneous voting rule, the sample complexity is at most (9m!2/2ε2) ln(2m!/δ). So, when m is a constant, the sample complexity is of the order Θ((1/ε2) ln(1/δ)) for any homogeneous voting rule that reduces to the majority voting rule on 2 candidates and this bound is tight up to constant factors. We note that all the commonly used voting rules including the ones we study here, are homogeneous and they reduce to the majority voting rule when we have only 2 candidates. Note that this bound is independent of n if ε and δ are independent of n.\nOur main technical contribution is in understanding the dependence of the sample complexity on the number of candidates m. Note that the upper bound cited above has very bad dependence onm and is clearly unsatisfactory in situations whenm is large (such as in online surveys about restaurants).\n⊲ We show that the sample complexity of the (ε, δ)-WINNER DETERMINATION problem\nis at most (9/2ε2) ln(2k/δ) for the k-approval voting rule (Theorem 5.6) and at most (27/ε2) ln(4/δ) for the plurality with runoff voting rule (Theorem 5.11). In particular, for the plurality rule, the sample complexity is independent of m as well as n.\n⊲ We show that the sample complexity of the (ε, δ)-WINNER DETERMINATION problem is\nat most (9/2ε2) ln(2m/δ) andΩ ((1− δ)(1/ε2) lnm) for the Borda (Theorem 5.2), approval (Theorem 5.4), maximin (Theorem 6.6), and Bucklin (Theorem 5.10) voting rules. Note that when δ is a constant, the upper and lower bounds match up to constant factors.\n⊲ We show a sample complexity upper bound of (25/2ε2) ln3(m/δ) for the (ε, δ)-\nWINNER DETERMINATION problem for the Copelandα voting rule (Theorem 5.9) and (3m2/ε2)(m ln 2+ ln(2m/δ)) for the STV voting rule (Theorem 5.12).\nWe summarize these results in Table 5.1.\n75\n76"
    }, {
      "heading" : "5.1.1.2 Estimating Margin of Victory",
      "text" : "The margin of victory of an election is defined as follows.\nDefinition 5.2. Margin of Victory (MOV) Given an election E, the margin of victory of E is defined as the smallest number of votes that must be changed to change the winner of the election E.\nWe abbreviate margin of victory as MOV. We denote the MOV of an election E by M(E). We introduce and study the following computational problem for estimating the margin of victory of an election:\nDefinition 5.3. ((c, ε, δ)–MARGIN OF VICTORY(MOV ))\nGiven a r-election E, determine the margin of victory Mr(E) of E, within an additive error of at most cMr(E)+ εn with probability at least 1− δ. The probability is taken over the internal coin tosses of the algorithm.\nOur goal here is to solve the (c, ε, δ)–MARGIN OF VICTORY problem by observing as few sample votes as possible. Our main technical contribution is to come up with efficient sampling based polynomial time randomized algorithms to solve the (c, ε, δ)–MARGIN OF VICTORY problem for common voting rules. Each sample reveals the entire preference order of the sampled vote. We summarize the results on the (c, ε, δ)–MARGIN OF VICTORY problem in Table 5.2.\nTable 5.2 shows a practically appealing positive result — the sample complexity of all the algorithms presented here is independent of the number of voters. Our lower bounds on the sample complexity of the (c, ε, δ)–MARGIN OF VICTORY problem for all the voting rules studied here match with the upper bounds up to constant factors when we have a constant number of candidates. Moreover, the lower and upper bounds on the sample complexity for the kapproval voting rule match up to constant factors irrespective of number of candidates, when k is a constant.\n⊲ We show a sample complexity lower bound of ((1−c)2/36ε2) ln (1/8e√πδ) for the (c, ε, δ)–\nMARGIN OF VICTORY problem for all the commonly used voting rules, where c ∈ [0, 1) (Theorem 5.13 and Corollary 5.3).\n⊲ We show a sample complexity upper bound of (12/ε2) ln(2m/δ) for the (1/3, ε, δ)–MARGIN\nOF VICTORY problem for arbitrary scoring rules (Theorem 5.14). However, for a special\n77\nclass of scoring rules, namely, the k-approval voting rules, we prove a sample complexity upper bound of (12/ε2) ln(2k/δ) for the (0, ε, δ)–MARGIN OF VICTORY problem (Theorem 5.15).\nOne key finding of our work is that, there may exist efficient sampling based polynomial time algorithms for estimating the margin of victory, even if computing the margin of victory is NP-hard for a voting rule [Xia12], as observed in the cases of maximin and Copelandα voting rules."
    }, {
      "heading" : "5.1.2 Related Work",
      "text" : "The subject of voting is at the heart of (computational) social choice theory, and there is a vast amount of literature in this area. Elections take place not only in human societies but also in man made social networks [BBCV09, RSW+07] and, generally, in many multiagent systems [ER91, PHG00]. The winner determination problem is the task of finding the winner in an election, given the voting rule in use and the set of all votes cast. It is known that there are natural voting rules, e.g., Kemeny’s rule and Dodgson’s method, for which the winner determination problem is NP-hard [BITT89, HSV05, HHR97].\nThe basic model of election has been generalized in several other ways to capture real world situations. One important consideration is that the votes may be incomplete\n78\nrankings of the candidates and not a complete ranking. There can also be uncertainty over which voters and/or candidates will eventually turn up. The uncertainty may additionally come up from the voting rule that will be used eventually to select the winner. In these incomplete information settings, several winner models have been proposed, for example, robust winner [BLOP14, LB11a, SYE13], multi winner [LB13], stable winner [FMT12], approximate winner [DLC14], probabilistic winner [BBF10], possible winner [MBC+16, DMN15b, DMN16c]. Hazon et al. [HAKW08] proposed useful methods to evaluate the outcome of an election under various uncertainties. We do not study the role of uncertainty in this work.\nThe general question of whether the outcome of an election can be determined by less than the full set of votes is the subject of preference elicitation, a central category of problems in AI. The (ε, δ)-WINNER DETERMINATION problem also falls in this area when the elections are restricted to those having margin of victory at least εn. For general elections, the preference elicitation problem was studied by Conitzer and Sandholm [CS02], who defined an elicitation policy as an adaptive sequence of questions posed to voters. They proved that finding an efficient elicitation policy is NP-hard for many common voting rules. Nevertheless, several elicitation policies have been developed in later work [Con09, LB11a, LB11b, DL13b, OFB13, DM16a, DM16b] that work well in practice and have formal guarantees under various assumptions on the vote distribution. Another related work is that of Dhamal and Narahari [DN13] who show that if the voters are members of a social network where neighbors in the network have similar candidate votes, then it is possible to elicit the votes of only a few voters to determine the outcome of the full election.\nIn contrast, in our work on winner prediction, we posit no assumption on the vote distribution other than that the votes create a substantial margin of victory for the winner. Under this assumption, we show that even for voting rules in which winner determination is NPhard in the worst case, it is possible to sample a small number of votes to determine the winner. Our work falls inside the larger framework of property testing [Ron01], a class of problems studied in theoretical computer science, where the inputs are promised to either satisfy some property or have a “gap” from instances satisfying the property. In our case, the instances are elections which either have some candidate w as the winner or are “far” from having w being the winner (in the sense that many votes need to be changed).\nThere have been quite a few work on computing the margin of victory of an election. Most prominent among them is the work of Xia [Xia12]. Xia presents polynomial time algorithms\n79\nfor computing the margin of victory of an election for various voting rules, for example the scoring rules, and proved intractability results for several other voting rules, for example the maximin and Copelandα voting rules. Magrino et al. [MRSW11] present approximation algorithms to compute the margin of victory for the instant runoff voting (IRV) rule. Cary [Car11] provides algorithms to estimate the margin of victory of an IRV election. Endriss et al. [EL14] compute the complexity of exact variants of the margin of victory problem for Schulze, Cup, and Copeland voting rules. However, all the existing algorithms to either compute or estimate the margin of victory need to observe all the votes, which defeats the purpose in many applications including the ones we discussed. We, in this work, show that we can estimate the margin of victory of an election for many commonly used voting rules quite accurately by sampling a few votes only. Moreover, the accuracy of our estimation algorithm is good enough for many practical scenarios. For example, Table 5.2 shows that it is enough to select only 3600 many votes uniformly at random to estimate MOV n of a plurality election within an additive error of 0.1 with probability at least 0.99, where n is the number of votes. We note that in all the sampling based applications we discussed, the sample size is inversely proportional to MOV n [CEG95] and thus it is enough to estimate MOV n accurately (see Table 5.1).\nThe problem of finding the margin of victory in an election is the same as the optimization version of the destructive bribery problem introduced by Faliszewski et al. [FHH06, FHH09]. However, to the best of our knowledge, there is no prior work on estimating the cost of bribery by sampling votes."
    }, {
      "heading" : "5.2 Results for Winner Prediction",
      "text" : "In this section, we present our results for the (ε, δ)-WINNER DETERMINATION problem."
    }, {
      "heading" : "5.2.1 Results on Lower Bounds",
      "text" : "We begin with presenting our lower bounds for the (ε, δ)-WINNER DETERMINATION problem for various voting rules. Our lower bounds for the sample complexity of the (ε, δ)-WINNER DETERMINATION problem are derived from information-theoretic lower bounds for distinguishing distributions.\nWe start with the following basic observation. Let X be a random variable taking value 1\nwith probability 1 2 − ε and 0 with probability 1 2 + ε; Y be a random variable taking value 1\n80\nwith probability 1 2 + ε and 0 with probability 1 2 − ε. Then, it is known that every algorithm needs at least 1 4ε2 ln 1 8e √ πδ many samples to distinguish between X and Y with probability of making an error being at most δ [CEG95, BYKS01]. We immediately have the following:\nTheorem 5.1. The sample complexity of the (ε, δ)-WINNER DETERMINATION problem for the plurality voting rule is at least 1 4ε2 ln 1 8e √ πδ even when the number of candidates is 2.\nProof. Consider an election with two candidates a and b. Consider two vote distributions X and Y. In X, exactly 1 2 + ε fraction of voters prefer a to b and thus a is the plurality winner of the election. In Y, exactly 1 2 + ε fraction of voters prefer b to a and thus b is the plurality winner of the election. Also, the margin of victory of both the elections corresponding to the vote distributions X and Y is εn, since each vote change can change the plurality score of any candidate by at most one. Observe that any (ε, δ)-WINNER DETERMINATION algorithm for plurality will give us a distinguisher between the distributions X and Y with probability of error at most δ. and hence will need 1 4ε2 ln 1 8e √ πδ samples.\nTheorem 5.1 immediately gives us the following sample complexity lower bounds for the\n(ε, δ)-WINNER DETERMINATION problem for other voting rules.\nCorollary 5.1. Every (ε, δ)-WINNER DETERMINATION algorithm needs at least 1 4ε2 ln 1 8e √ πδ samples for any voting rule which reduces to the plurality rule for two candidates. In particular, the lower bound holds for approval, scoring rules, maximin, Copeland, Bucklin, plurality with runoff, and STV voting rules.\nProof. All the voting rules mentioned in the statement except the approval voting rule is same as the plurality voting rule for elections with two candidates. Hence, the result follows immediately from Theorem 5.1 for the above voting rules except the approval voting rule. The result for the approval voting rule follows from the fact that any arbitrary plurality election is also a valid approval election where every voter approves exactly one candidate.\nWe derive stronger lower bounds in terms of m by explicitly viewing the (ε, δ)-WINNER DETERMINATION problem as a statistical classification problem. In this problem, we are given a black box that contains a distribution µwhich is guaranteed to be one of ℓ known distributions µ1, . . . ,µℓ. A classifier is a randomized oracle which has to determine the identity of µ, where each oracle call produces a sample from µ. At the end of its execution, the classifier announces a guess for the identity of µ, which has to be correct with probability at least 1 − δ. Using information-theoretic methods, Bar-Yossef [BY03] showed the following:\n81\nLemma 5.1. The worst case sample complexity q of a classifier C for ℓ probability distributions µ1, . . . ,µℓ which does not make error with probability more than δ satisfies following.\nq > Ω\n(\nln ℓ\nJS (µ1, . . . ,µℓ) . (1− δ)\n)\nThe connection with our problem is the following. A set V of n votes on a candidate set C generates a probability distribution µV on L(C), where µV(≻) is proportional to the number of voters who vote ≻. Querying a random vote from V is then equivalent to sampling from the distribution µV . The margin of victory is proportional to the minimum statistical distance between µV and µW, over all the voting profiles W having a different winner than the winner of V.\nNow suppose we have m voting profiles V1, . . . ,Vm having different winners such that each Vi has margin of victory at least εn. Any (ε, δ)-WINNER DETERMINATION algorithm must also be a statistical classifier for µV1, . . . ,µVm in the above sense. It then remains to construct such voting profiles for various voting rules which we do in the proof of the following theorem:\nTheorem 5.2. Every (ε, δ)-WINNER DETERMINATION algorithm needs Ω ( lnm ε2 . (1− δ) ) samples for approval, Borda, Bucklin, and any Condorcet consistent voting rules, and Ω (\nlnk ε2 . (1− δ) )\nsamples for the k-approval voting rule for k 6 cm for any constant c ∈ (0, 1).\nProof. For each voting rule mentioned in the theorem, we will show d (d = k + 1 for the k-approval voting rule and d = m for the rest of the voting rules) distributions µ1, . . . ,µd on the votes with the following properties. Let Vi be an election where each vote v ∈ L(C) occurs exactly µi(v) · n many times. Let µ = 1d ∑d i=1 µi.\n1. For every i 6= j, the winner in Vi is different from the winner in Vj.\n2. For every i, the margin of victory of Vi is Ω(εn).\n3. DKL(µi||µ) = O(ε 2)\nThe result then follows from Lemma 5.1. The distributions for different voting rules are as follows. Let the candidate set be C = {c1, . . . , cm}.\n⊲ k-approval voting rule for k 6 cm for any constant c ∈ (0, 1): Fix any arbitrary M := k + 1 many candidates c1, . . . , cM. For i ∈ [M], we define a distribution µi on all\n82\nk sized subsets of C (for the k-approval voting rule, each vote is a k-sized subset of C) as follows. Each k sized subset corresponds to top k candidates in a vote.\nµi(x) =\n  \n \n(ε/(M−1k−1 )) + (1−ε/( M k )) if ci ∈ x and x ⊆ {c1, . . . , cM} (1−ε)/(Mk ) ci /∈ x and x ⊆ {c1, . . . , cM} 0 else\nThe score of ci in Vi is n ( ε+ (1−ε)( M−1 k−1 )/(Mk ) ) , the score of any other candidate cj ∈ {c1, . . . , cM} \\ {ci} is n(1−ε)( M−1 k−1 )/(Mk ), and the score of the rest of the candidates is zero. Hence, the margin of victory is Ω(εn), since each vote change can reduce the score of ci by at most one and increase the score of any other candidate by at most one and k 6 cm for constant c ∈ (0, 1). This proves the result for the k-approval voting rule. Now we show that DKL(µi||µ) to be O(ε 2).\nDKL(µi||µ) =\n(\nε+ (1 − ε) k\nM\n)\nln\n(\n1− ε+ ε M\nk\n)\n+ (1− ε)\n(\n1− k\nM\n)\nln (1− ε)\n6\n(\nε+ (1 − ε) k\nM\n)(\nε M\nk − ε\n)\n− (1 − ε)\n(\n1 − k\nM\n)\nε\n= ε2 ( M\nk − 1\n)\n6 2ε2\n⊲ Approval voting rule: The result follows from the fact that every m 2 -approval election\nis also a valid approval election and Lemma 5.2.\n⊲ Borda and any Condorcet consistent voting rule: The score vector for the Borda\nvoting rule which we use in this proof is (m,m − 1, . . . , 1). For i ∈ [m], we define a distribution µi on all possible linear orders over C as follows.\nµi(x) =\n \n\n2ε/m! + (1−ε)/m! if ci is within top m 2 positions in x. (1−ε)/m! else\nThe score of ci in Vi is at least n(3εm/5+ (1−ε)m/2) = n(m/2+ εm/10) whereas the score of any other candidate cj 6= ci is mn/2. Hence, the margin of victory is at least εn/8, since each vote change can reduce the score of ci by at most m and increase the score of any\n83\nother candidate by at most m. Also, in the weighted majority graph for the election Vi, w(ci, cj) > εn/10. Hence, the margin of victory is at least εn/4, since each vote change can change the weight of any edge in the weighted majority graph by at most two. Now we show that DKL(µi||µ) to be O(ε 2).\nDKL(µi||µ) = 1 + ε\n2 ln (1 + ε) +\n1− ε\n2 ln (1− ε)\n6 1 + ε\n2 ε−\n1 − ε\n2 ε\n= ε2\n⊲ Bucklin: For i ∈ [m], we define a distribution µi on all m/4 sized subsets of C as follows. Each m/4 sized subset corresponds to the top m\n4 candidates in a vote.\nµi(x) =\n \n\n(ε/(m−1m 4 −1)) + ((1−ε)/( m m 4 )) if ci ∈ x (1−ε)/(mm 4 ) else\nThe candidate ci occurs within the top m( 1 2 − ε 10 ) positions at least n(1 2 − ε 10 + ε) = n(1 2 + 9ε 10 ) times. On the other hand any candidate cj 6= ci occurs within the topm(12− ε10) positions at most n(1 2 − ε 10 ) times. Hence the margin of victory is at least εn 30 = Ω(εn). Now we show that DKL(µi||µ) to be O(ε 2).\nDKL(µi||µ) =\n(\nε+ 1− ε\n4\n)\nln (1+ 3ε) + 3\n4 (1− ε) ln (1 − ε)\n= 1\n4 (3ε (1 + 3ε) − 3ε (1 − ε))\n= 3ε2"
    }, {
      "heading" : "5.2.2 Results on Upper Bounds",
      "text" : "In this section, we present the upper bounds on the sample complexity of the (ε, δ)-WINNER DETERMINATION problem for various voting rules. The general framework for proving the upper bounds is as follows. For each voting rule, we first prove a useful structural property\n84\nabout the election when the margin of victory is known to be at least εn. Then, we sample a few votes uniformly at random to estimate either the score of the candidates for score based voting rules or weights of the edges in the weighted majority graph for the voting rules which are defined using weighted majority graph (maximin and Copeland for example). Finally, appealing to the structural property that has been established, we argue that, the winner of the election on the sampled votes will be the same as the winner of the election, if we are able to estimate either the scores of the candidates or the weights of the edges in the weighted majority graph to a certain level of accuracy.\nBefore getting into specific voting rules, we prove a straightforward bound on the sample\ncomplexity for the (ε, δ)-winner determination problem for any homogeneous voting rule.\nTheorem 5.3. There is a (ε, δ)-WINNER DETERMINATION algorithm for every homogeneous voting rule with sample complexity (9m!2/2ε2) ln(2m!/δ).\nProof. We sample ℓ votes uniformly at random from the set of votes with replacement. For x ∈ L(C), let Xxi be an indicator random variable that is 1 exactly when x is the i’th sample, and let g(x) be the total number of voters whose vote is x. Define ĝ(x) = n\nℓ\n∑ℓ i=1 X x i . Using\nthe Chernoff bound (Theorem 2.5), we have the following:\nPr [ |ĝ(x) − g(x)| > εn\n3m!\n] 6 2 · exp ( − 2ε2ℓ\n9m!2\n)\nBy using the union bound, we have the following,\nPr [ ∃x ∈ L(C), |ĝ(x) − g(x)| > εn 3m! ]\n6 2m! · exp ( − 2ε2ℓ\n9m!2\n)\nSince the margin of victory is εn and the voting rule is anonymous, the winner of the ℓ sample votes will be same as the winner of the election if |ĝ(x)−g(x)| 6 εn/3m! for every linear order x ∈ L(C). Hence, it is enough to take ℓ = (9m!2/2ε2) ln(2m!/δ)."
    }, {
      "heading" : "5.2.2.1 Approval Voting Rule",
      "text" : "We derive the upper bound on the sample complexity for the (ε, δ)-WINNER DETERMINATION problem for the approval voting rule.\nLemma 5.2. If MOV > εn and w be the winner of an approval election, then, s(w)−s(x) > εn, for every candidate x 6= w, where s(y) is the number of approvals that a candidate y receives.\n85\nProof. Suppose there is a candidate x 6= w such that s(w) − s(x) < εn. Then there must exist εn − 1 votes which does not approve the candidate x. We modify these votes to make it approve x. This makes w not the unique winner in the modified election. This contradicts the fact that the MOV is at least εn.\nTheorem 5.4. There is a (ε, δ)-WINNER DETERMINATION algorithm for the approval voting rule with sample complexity at most 9 ln(2m/δ)/2ε2.\nProof. Suppose w is the winner. We sample ℓ votes uniformly at random from the set of votes with replacement. For a candidate x, let Xxi be a random variable indicating whether the i’th vote sampled approved x. Define ŝ(x) = n ℓ ∑ℓ i=1 X x i . Then, by an argument analogous to the proof of Theorem 5.3, Pr[∃x ∈ C, |ŝ(x) − s(x)| > εn/3] 6 2m · exp ( −2ε2ℓ/9 ) . Thus since MOV> εn and by Lemma 5.2, if we take ℓ = 9 ln(2m/δ)/2ε2, ŝ(w) is greater than ŝ(x) for all x 6= w."
    }, {
      "heading" : "5.2.2.2 Scoring Rules",
      "text" : "Now we move on to the scoring rules. Again, we first establish a structural consequence of having large MOV.\nLemma 5.3. Let α = (α1, . . . ,αm) be any normalized score vector (hence, αm = 0). If w and z are the candidates that receive highest and second highest score respectively in a α–scoring rule election instance E = (V,C) and Mα(E) is the margin of victory of E, then,\nα1(Mα(E) − 1) 6 s(w) − s(z) 6 2α1Mα(E)\nProof. We claim that there must be at leastMα(E)−1 many votes v ∈ V wherew is preferred over z. Indeed, otherwise, we swap w and z in all the votes where w is preferred over z. This makes z win the election. However, we have changed at most Mα(E) − 1 votes only. This contradicts the definition of margin of victory (see Definition 5.2). Let v ∈ V be a vote where w is preferred over z. Let αi and αj(6 αi) be the scores received by the candidates w and z respectively from the vote v. We replace the vote v by v′ = z ≻ · · · ≻ c. This vote change reduces the value of s(w) − s(z) by α1 + αi − αj which is at least α1. Hence, α1(Mα(E) − 1) 6 s(w) − s(z). Each vote change reduces the value of s(w) − s(z) by at most 2α1 since αm = 0. Hence, s(w) − s(z) 6 2α1Mα(E).\n86\nUsing Lemma 5.3, we prove the following sample complexity upper bound for the (ε, δ)-\nWINNER DETERMINATION problem for the scoring rules.\nTheorem 5.5. Suppose α = (α1, . . . ,αm) be a normalized score vector. There is a (ε, δ)-WINNER DETERMINATION algorithm for the α-scoring rule with sample complexity at most 9 ln(2m/δ)/2ε2.\nProof. We sample ℓ votes uniformly at random from the set of votes with replacement. For a candidate x, define Xi = αi/α1 if x gets a score of αi from the ith sample vote, and let ŝ(x) = nα1 ℓ ∑ℓ i=1 Xi. Now using Chernoff bound (Theorem 2.5), we have:\nPr [|ŝ(x) − s(x)| > α1εn/3] 6 2 exp\n(\n− 2ε2ℓ\n9\n)\nThe rest of the proof follows from an argument analogous to the proof of Theorem 5.4 using Lemma 5.3.\nFrom Theorem 5.5, we have a (ε, δ)-winner determination algorithm for the k-approval voting rule which needs 9 ln(2m/δ)/2ε2 many samples for any k. We now improve this bound to 9 ln(2k/δ)/2ε2 for the k-approval voting rule. Before embarking on the proof of the above fact, we prove the following lemma which we will use crucially in Theorem 5.6.\nLemma 5.4. Let f : R −→ R be a function defined by f(x) = e−λ/x. Then,\nf(x) + f(y) 6 f(x+ y), for x,y > 0, λ\nx + y > 2, x < y\nProof. For the function f(x), we have following.\nf(x) = e− λ/x\n⇒ f′(x) = λ x2 e− λ/x\n⇒ f′′(x) = λ 2\nx4 e−\nλ/x − 2λ\nx3 e− λ/x\n87\nHence, for x,y > 0, λ x+y > 2, x < y we have f′′(x), f′′(y), f′′(x+y) > 0. This implies following for x < y and an infinitesimal positive δ.\nf′(x) 6 f′(y)\n⇒ f(x− δ) − f(x) δ > f(y) − f(y− δ)\nδ ⇒ f(x) + f(y) 6 f(x− δ) + f(y+ δ) ⇒ f(x) + f(y) 6 f(x+ y)\nWe now present our (ε, δ)-WINNER DETERMINATION algorithm for the k-approval voting\nrule.\nTheorem 5.6. There is a (ε, δ)-WINNER DETERMINATION algorithm for the k-approval voting rule with sample complexity at most 9 ln(2k/δ)/2ε2.\nProof. We sample ℓ votes uniformly at random from the set of votes with replacement. For a candidate x, let Xxi be a random variable indicating whether x is among the top k candidates for the ith vote sample. Define ŝ(x) = n ℓ ∑ℓ i=1 X x i , and let s(x) be the actual score of x. Then by the multiplicative Chernoff bound (Theorem 2.5), we have:\nPr [|ŝ(x) − s(x)| > εn/3] 6 2 exp\n(\n− 2ε2ℓn\n9s(x)\n)\nBy union bound, we have the following,\nPr[∃x ∈ C, |ŝ(x) − s(x)| > εn/3] 6 ∑\nx∈C 2 exp\n( −2ε2ℓn/9s(x) )\n6 2k exp ( −2ε2ℓ/9 )\nLet the candidate w be the winner of the election. The second inequality in the above derivation follows from the fact that, the function ∑\nx∈C 2exp ( −2ε2ℓn/9s(x) ) is maximized in the do-\nmain, defined by the constraint: for every candidate x ∈ C, s(x) ∈ [0,n] and ∑x∈C s(x) = kn, by setting s(x) = n for every x ∈ C′ and s(y) = 0 for every y ∈ C \\ C′, for any arbitrary subset C′ ⊂ C of cardinality k (due to Lemma 5.4). The rest of the proof follows by an argument analogous to the proof of Theorem 5.3 using Lemma 5.3.\n88\nNotice that, the sample complexity upper bound in Theorem 5.6 is independent of m for the plurality voting rule. Theorem 5.6 in turn implies the following Corollary which we consider to be of independent interest.\nCorollary 5.2. There is an algorithm to estimate the ℓ∞ norm ℓ∞(µ) of a distribution µ within an additive factor of ε by querying only 9 ln(2/δ)/2ε2 many samples, if we are allowed to get i.i.d. samples from the distribution µ.\nSuch a statement seems to be folklore in the statistics community [DKW56]. Recently in an independent and nearly simultaneous work, Waggoner [Wag15] obtained a sharp bound of 4 ε2 ln( 1 δ ) for the sample complexity in Corollary 5.2.\nWe now turn our attention to the k-veto voting rule. For the k-veto voting rule, we have\nthe following result for the (ε, δ)-WINNER DETERMINATION problem.\nTheorem 5.7. There is a (ε, δ)-WINNER DETERMINATION algorithm for the k-veto voting rule with sample complexity at most 9 ln(2k/δ)/2ε2.\nProof. We first observe that, since the margin of victory of the input election is at least εn, every candidate other than the winner must receive at least εn vetoes. Hence we have the following.\nεn(m − 1) > kn i.e. m− 1 6 k/ε\nLet us sample ℓ votes uniformly at random from the set of votes with replacement. For a candidate x, let Xxi be a random variable indicating whether x is among the bottom k candidates for the ith vote sample. Define ŝ(x) = n ℓ ∑ℓ i=1 X x i , and let s(x) be the actual score of x. Then by the multiplicative Chernoff bound (Theorem 2.5), we have:\nPr [|ŝ(x) − s(x)| > εn/3] 6 2 exp\n(\n− 2ε2ℓn\n9s(x)\n)\nBy union bound, we have the following,\nPr[∃x ∈ C, |ŝ(x) − s(x)| > εn/3] 6 ∑\nx∈C 2 exp\n( −2ε2ℓn/9s(x) )\n6 2k exp ( −2ε2ℓ/9 )\nLet the candidate w be the winner of the election. The second inequality in the above derivation follows from the fact that, the function ∑\nx∈C 2exp ( −2ε2ℓn/9s(x) ) is maximized in the do-\n89\nmain, defined by the constraint: for every candidate x ∈ C, s(x) ∈ [0,n] and ∑x∈C s(x) = kn, by setting s(x) = n for every x ∈ C′ and s(y) = 0 for every y ∈ C \\ C′, for any arbitrary subset C′ ⊂ C of cardinality k (due to Lemma 5.4). The rest of the proof follows by an argument analogous to the proof of Theorem 5.3 using Lemma 5.3."
    }, {
      "heading" : "5.2.2.3 Maximin Voting Rule",
      "text" : "We now turn our attention to the maximin voting rule. The idea here is to sample enough number of votes such that we are able to estimate the weights of the edges in the weighted majority graph with certain level of accuracy which in turn leads us to predict winner.\nLemma 5.5. Let E = (V,C) be any instance of a maximin election. Ifw and z are the candidates that receive highest and second highest maximin score respectively in E and Mmaximin(E) is the margin of victory of E, then,\n2Mmaximin(E) 6 s(w) − s(z) 6 4Mmaximin(E)\nProof. Each vote change can increase the value of s(z) by at most two and decrease the value of s(w) by at most two. Hence, we have s(w) − s(z) 6 4Mmaximin(E). Let x be the candidate that minimizes DE(w, x), that is, x ∈ argminx∈C\\{w}{DE(w, x)}. Let v ∈ V be a vote where w is preferred over x. We replace the vote v by the vote v′ = z ≻ x ≻ · · · ≻ w. This vote change reduces the score of w by two and does not reduce the score of z. Hence, s(w) − s(z) > 2Mmaximin(E).\nWe now present our (ε, δ)-WINNER DETERMINATION algorithm for the maximin voting\nrule.\nTheorem 5.8. There is a (ε, δ)-WINNER DETERMINATION algorithm for the maximin voting rule with sample complexity (9/2ε2) ln(2m/δ).\nProof. Let x and y be any two arbitrary candidates. We sample ℓ votes uniformly at random from the set of votes with replacement. Let Xi be a random variable defined as follows.\nXi =\n \n 1, if x ≻ y in the ith sample −1, else\n90\nDefine D̂(x,y) = n ℓ ∑ℓ i=1 Xi. We estimate D̂(x,y)within the closed ball of radius εn/2 around D(x,y) for every candidates x,y ∈ C and the rest of the proof follows from by an argument analogous to the proof of Theorem 5.4 using Lemma 5.5."
    }, {
      "heading" : "5.2.2.4 Copeland Voting Rule",
      "text" : "Now we move on to the Copelandα voting rule. The approach for the Copelandα voting rule is similar to the maximin voting rule. However, it turns out that we need to estimate the edge weights of the weighted majority graph more accurately for the Copelandα voting rule. Xia introduced the quantity called the relative margin of victory (see Section 5.1 in [Xia12]) which we will use crucially for showing sample complexity upper bound for the Copelandα voting rule. Given an election, a candidate x ∈ C, and an integer (may be negative also) t, s′t(V, x) is defined as follows.\ns′t(V, x) = |{y ∈ C : y 6= x,D(y, x) < 2t}|+ α|{y ∈ C : y 6= x,D(y, x) = 2t}|\nFor every two distinct candidates x and y, the relative margin of victory, denoted by RM(x,y), between x and y is defined as the minimum integer t such that, s′−t(V, x) 6 s′t(V,y). Let w be the winner of the election E. We define a quantity Γ(E) to be minx∈C\\{w}{RM(w, x)}. Notice that, given an election E, Γ(E) can be computed in polynomial amount of time. Now we have the following lemma.\nLemma 5.6. Suppose MOV > εn and w be the winner of a Copelandα election. Then, RM(w, x) > εn/(2(⌈lnm⌉+1)), for every candidate x 6= w.\nProof. Follows from Theorem 11 in [Xia12].\nTheorem 5.9. There is a (ε, δ)-WINNER DETERMINATION algorithm for the Copelandα voting rule with sample complexity (25/2ε2) ln 3 (2m/δ).\nProof. Let x and y be any two arbitrary candidates and w the Copelandα winner of the election. We estimate D(x,y) within the closed ball of radius εn/(5(⌈lnm⌉+1)) around D(x,y) for every candidates x,y ∈ C in a way analogous to the proof of Theorem 6.6. This needs (25/2ε2) ln 3 (2m/δ)many samples. The rest of the proof follows from Lemma 5.6 by an argument analogous to the proof of Theorem 5.3.\n91"
    }, {
      "heading" : "5.2.2.5 Bucklin Voting Rule",
      "text" : "For the Bucklin voting rule, we will estimate how many times each candidate occurs within the first k position for every k ∈ [m]. This eventually leads us to predict the winner of the election due to the following lemma.\nLemma 5.7. Suppose MOV of a Bucklin election be at least εn. Let w be the winner of the election and x be any arbitrary candidate other than w. Suppose\nbw = min i {i : w is within top i places in at least n/2 + εn/3 votes}\nbx = min i {i : x is within top i places in at least n/2 − εn/3 votes}\nThen, bw < bx.\nProof. We prove it by contradiction. So, assume bw > bx. Now by changing εn/3 votes, we can make the Bucklin score of w to be at least bw. By changing another εn/3 votes, we can make the Bucklin score of x to be at most bx. Hence, by changing 2εn/3 votes, it is possible not to make w the unique winner which contradicts the fact that the MOV is at least εn.\nOur (ε, δ)-WINNER DETERMINATION algorithm for the Bucklin voting rule is as follows.\nTheorem 5.10. There is a (ε, δ)-WINNER DETERMINATION algorithm for the Bucklin voting rule with sample complexity (9/2ε2) ln(2m/δ).\nProof. Let x be any arbitrary candidate and 1 6 k 6 m. We sample ℓ votes uniformly at random from the set of votes with replacement. Let Xi be a random variable defined as follows.\nXi =\n \n\n1, if x is within top k places in ith sample\n0, else\nLet ŝk(x) be the estimate of the number of times the candidate x has been placed within top k positions. That is, ŝk(x) = n ℓ ∑ℓ i=1 Xi. Let sk(x) be the number of times the candidate x been placed in top k positions. Clearly, E[ŝk(x)] = n ℓ ∑ℓ i=1 E[Xi] = sk(x). We estimate ŝk(x) within the closed ball of radius εn/3 around sk(x) for every candidate x ∈ C and every integer k ∈ [m], and the rest of the proof follows from by an argument analogous to the proof of Theorem 5.4 using Lemma 5.7.\n92"
    }, {
      "heading" : "5.2.2.6 Plurality with Runoff Voting Rule",
      "text" : "Now we move on to the plurality with runoff voting rule. In this case, we first estimate the plurality score of each of the candidates. In the next round, we estimate the pairwise margin of victory of the two candidates that qualifies to the second round.\nLemma 5.8. Suppose MOV > εn, and w and r be the winner and runner up of a plurality with runoff election respectively, and x be any arbitrary candidate other than and r. Then, following holds. Let s(.) denote plurality score of candidates. Then following holds.\n1. D(w, r) > 2εn.\n2. For every candidate x ∈ C \\ {w, r}, 2s(w) > s(x) + s(r) + εn.\n3. If s(x) > s(r) − εn/2, then D(w, x) > εn/2.\nProof. If the first property does not hold, then by changing εn votes, we can make r winner. If the second property does not hold, then by changing εn votes, we can make both x and r qualify to the second round. If the third property does not hold, then by changing εn/2 votes, the candidate x can be sent to the second round of the runoff election. By changing another εn/2 votes, x can be made to win the election. This contradicts the MOV assumption.\nNow we present our (ε, δ)-WINNER DETERMINATION algorithm for the plurality with runoff\nvoting rule.\nTheorem 5.11. There is a (ε, δ)-WINNER DETERMINATION algorithm for the plurality with runoff voting rule with sample complexity (27/ε2) ln(4/δ).\nProof. Let x be any arbitrary candidate. We sample ℓ votes uniformly at random from the set of votes with replacement. Let, Xi be a random variable defined as follows.\nXi =\n \n\n1, if x is at first position in the ith sample\n0, else\nThe estimate of the plurality score of x be ŝ(x). Then ŝ(x) = n ℓ ∑ℓ i=1 Xi. Let s(x) be the actual plurality score of x. Then we have following,\nE[Xi] = s(x)\nn ,E[ŝ(x)] =\nn ℓ\nℓ∑\ni=1\nE[Xi] = s(x)\n93\nBy Chernoff bound, we have the following,\nPr[|ŝ(x) − s(x)| > εn] 6 2\nexp{ε2ℓn/3s(x)}\nBy union bound, we have the following,\nPr[∃x ∈ C, |ŝ(x) − s(x)| > εn] 6 ∑\nx∈C\n2\nexp{ε2ln/3s(x)}\n6 2\nexp{ε2ℓ/3}\nThe last line follows from Lemma 5.4. Notice that, we do not need the random variables ŝ(x) and ŝ(y) to be independent for any two candidates x and y. Hence, we can use the same ℓ sample votes to estimate ŝ(x) for every candidate x.\nNow let y and z be the two candidates that go to the second round.\nYi =\n \n 1, if y ≻ z in the ith sample −1, else\nThe estimate of D(y, z) be D̂(y, z). Then D̂(y, z) = n ℓ ∑ℓ i=1 Yi. Then we have following,\nE[Yi] = D(y, z)\nn ,E[D̂(y, z)] =\nn ℓ\nℓ∑\ni=1\nE[Yi] = D(y, z)\nBy Chernoff bound, we have the following,\nPr[|D̂(y, z) −D(y, z)| > εn] 6 2\nexp{ε2ℓ/3}\nLet A be the event that ∀x ∈ C, |ŝ(x) − s(x)| 6 εn and |D̂(y, z) −D(y, z)| 6 εn. Now we have,\nPr[A] > 1 − ( 2\nexp{ε2ℓ/3} +\n2\nexp{ε2ℓ/3} )\nSince we do not need independence among the random variables ŝ(a), ŝ(b), D̂(w, x), D̂(y, z) for any candidates a,b,w, x,y, and z, we can use the same ℓ sampled votes. Now from Lemma 5.8, if |ŝ(x)−s(x)| 6 εn/3 for every candidate x and |D̂(y, z)−D(y, z)| 6 εn/3 for every\n94\ncandidates y and z, then the plurality with runoff winner of the sampled votes coincides with the actual runoff winner. The above event happens with probability at least 1−δ by choosing an appropriate ℓ = (27/ε2) ln(4/δ)."
    }, {
      "heading" : "5.2.2.7 STV Voting Rule",
      "text" : "Now we move on the STV voting rule. The following lemma provides an upper bound on the number of votes that need to be changed to make some arbitrary candidate win the election. More specifically, given a sequence of m candidates {xi} m i=1 with xm not being the winner, Lemma 5.9 below proves an upper bound on the number of number of votes that need to be modified such that the candidate xi gets eliminated at the i th round in the STV voting rule.\nLemma 5.9. Suppose V be a set of votes and w be the winner of a STV election. Consider the following chain with candidates x1 6= x2 6= . . . 6= xm and xm 6= w.\nC ⊃ C \\ {x1} ⊃ C \\ {x1, x2} ⊃ . . . ⊃ {xm}\nLet sV(A, x) be the plurality score of a candidate x when all the votes in V are restricted to the set of candidates A ⊂ C. Let us define C−i = C \\ {x1, . . . , xi} and s∗V(A) := minx∈A{sV(A, x)}. Then, we have the following.\nm−1∑\ni=0\n(sV (C−i, xi+1) − s ∗ V (C−i)) > MOV\nProof. We will show that by changing ∑m−1\ni=0 (sV (C−i, xi+1) − s ∗ V (C−i)) votes, we can make\nthe candidate xm winner. If x1 minimizes sV(C, x) over x ∈ C, then we do not change anything and define V1 = V. Otherwise, there exist sV(C, x1) − s ∗ V(C) many votes of following type.\nx1 ≻ a1 ≻ a2 ≻ . . . ≻ am−1,ai ∈ C, ∀1 6 i 6 m− 1\nWe replace sV(C, x1) − s ∗ V(C) many votes of the above type by the votes as follows.\na1 ≻ x1 ≻ a2 ≻ . . . ≻ am−1\nLet us call the new set of votes by V1. We claim that, sV(C \\ x1, x) = sV1(C \\ x1, x) for every candidate x ∈ C \\ {x1}. Fix any arbitrary candidate x ∈ C \\ {x1}. The votes in V1 that are same\n95\nas in V contributes same quantity to both side of the equality. Let v be a vote that has been changed as described above. If x = a1 then, the vote v contributes one to both sides of the equality. If x 6= a1, then the vote contributes zero to both sides of the equality. Hence, we have the claim. We repeat this process for (m − 1) times. Let Vi be the set of votes after the candidate xi gets eliminated. Now in the above argument, by replacing V by Vi−1, V1 by Vi, the candidate set C by C \\ {x1, . . . , xi−1}, and the candidate x1 by the candidate xi, we have the following.\nsVi−1(C−i, x) = sVi(C−i, x)∀x ∈ C \\ {x1, . . . , xi}\nHence, we have the following.\nsV(C−i, x) = sVi(C−i, x)∀x ∈ C \\ {x1, . . . , xi}\nIn the above process, the total number of votes that are changed is ∑m−1\ni=0 (sV (C−i, xi+1) − s ∗ V (C−i)).\nWe now use Lemma 5.9 to prove the following sample complexity upper bound for the\n(ε, δ)-WINNER DETERMINATION problem for the STV voting rule.\nTheorem 5.12. There is a (ε, δ)-WINNER DETERMINATION algorithm for the STV voting rule with sample complexity (3m2/ε2)(m ln 2 + ln(2m/δ)).\nProof. We sample ℓ votes uniformly at random from the set of votes with replacement and output the STV winner of those ℓ votes say w′ as the winner of the election. Let, w be the winner of the election. We will show that for ℓ = (3m2/ε2)(m ln 2+ ln(2m/δ)) for which w = w′ with probability at least 1 − δ. Let A be an arbitrary subset of candidates and x be any candidate in A. Let us define a random variables Xi, 1 6 i 6 ℓ as follows.\nXi =\n \n\n1, if x is at top ith sample when restricted to A\n0, else\nDefine another random variable ŝV(A, x) := ∑ℓ i=1 Xi. Then we have, E[ŝV(A, x)] = sV(A, x). Now using Chernoff bound, we have the following,\nPr[|ŝV(A, x) − sV(A, x)| > εn\nm ] 6\n2\nexp{ε2ℓ/3m2}\n96\nLet E be the event that ∃A ⊂ C and ∃x ∈ A, |ŝV(A, x) − sV(A, x)| > εnm . By union bound, we have,\nPr[Ē] > 1− m2m+1\nexp{ε2ℓ/3m2}\nThe rest of the proof follows by an argument analogous to the proof of Theorem 5.3 using Lemma 5.9."
    }, {
      "heading" : "5.3 Results for Estimating Margin of Victory",
      "text" : "In this section we present our results for the (c, ε, δ)–MARGIN OF VICTORY problem."
    }, {
      "heading" : "5.3.1 Results on Lower Bounds",
      "text" : "Our lower bounds for the sample complexity of the (c, ε, δ)–MARGIN OF VICTORY problem are derived from the information-theoretic lower bound for distinguishing two distributions.\nTheorem 5.13. The sample complexity of the (c, ε, δ)–MARGIN OF VICTORY problem for the plurality voting rule is at least ((1−c)2/36ε2) ln (1/8e√πδ) for any c ∈ [0, 1).\nProof. Consider two vote distributions X and Y, each over the candidate set {a,b}. In X, exactly 1 2 + 6ε+2c/n 1−c fraction of voters prefer a to b and thus the margin of victory is 3ε+c/n 1−c n. In Y, exactly 1 2 fraction of voters prefer b to a and thus the margin of victory is one. Any (c, ε, δ)–MARGIN OF VICTORY algorithm A for the plurality voting rule gives us a distinguisher between X and Y with probability of error at most 2δ. This is so because, if the input to A is X then, the output of A is less than c + 2εn with probability at most δ, whereas, if the input to A is Y then, the output of A is more than c + εn with probability at most δ. Now since n can be arbitrarily large, we get the result.\nTheorem 5.1 immediately gives the following corollary.\nCorollary 5.3. For any c ∈ [0, 1), every (c, ε, δ)–MARGIN OF VICTORY algorithm needs at least ((1−c)2/36ε2) ln (1/8e√πδ) many samples for all voting rules which reduce to the plurality rule for two candidates. In particular, the lower bound holds for scoring rules, approval, Bucklin, maximin, and Copelandα voting rules.\n97\nWe note that the lower bound results in Theorem 5.1 and Corollary 5.1 do not assume\nanything about the sampling strategy or the computational complexity of the estimator."
    }, {
      "heading" : "5.3.2 Results on Upper Bounds",
      "text" : "A natural approach for estimating the margin of victory of an election efficiently is to compute the margin of victory of a suitably small number of sampled votes. Certainly, it is not immediate that the samples chosen uniformly at random preserve the value of the margin of victory of the original election within some desired factor. Although it may be possible to formulate clever sampling strategies that tie into the margin of victory structure of the election, we will show that uniformly chosen samples are good enough to design algorithms for estimating the margin of victory for many common voting rules. Our proposal has the advantage that the sampling component of our algorithms are always easy to implement, and further, there is no compromise on the bounds in the sense that they are optimal for any constant number of candidates.\nOur algorithms involve computing a quantity (which depends on the voting rule under consideration) based on the sampled votes, which we argue to be a suitable estimate of the margin of victory of the original election. This quantity is not necessarily the margin of victory of the sampled votes. For scoring rules, for instance, we will use the sampled votes to estimate candidate scores, and we use the difference between the scores of the top two candidates (suitably scaled) as an estimate for the margin of victory. We also establish a relationship between scores and the margin of victory to achieve the desired bounds on the estimate. The overall strategy is in a similar spirit for other voting rules as well, although the exact estimates may be different. We now turn to a more detailed description."
    }, {
      "heading" : "5.3.2.1 Scoring Rules and Approval Voting Rule",
      "text" : "We begin with showing that the margin of victory of any scoring rule based election can be estimated quite accurately by sampling only 12 ε2 ln 2m δ many votes. An important thing to note is that, the sample complexity upper bound is independent of the score vector.\nTheorem 5.14. There is a polynomial time (1/3, ε, δ)–MOV algorithm for the scoring rules with sample complexity at most (12/ε2) ln(2m/δ).\nProof. Let α = (α1, . . . ,αm) be any arbitrary normalized score vector and E = (V,C) an election instance. We sample ℓ (the value of ℓ will be chosen later) votes uniformly at ran-\n98\ndom from the set of votes with replacement. For a candidate x, define a random variable Xi(x) = αi/α1 if x gets a score of αi from the ith sample vote. Define s̄(x) = nα1 ℓ ∑ℓ i=1 Xi(x) the estimate of s(x), the score of x. Also define ε′ = ε/2. Now using Chernoff bound (Theorem 2.5), we have the following.\nPr [|s̄(x) − s(x)| > α1ε ′n] 6 2 exp\n( − ε′2ℓ\n3\n)\nWe now use the union bound to get the following.\nPr[∃x ∈ C, |s̄(x) − s(x)| > α1ε′n] 6 2m exp ( − ε′2ℓ\n3\n)\n(5.1)\nDefine M̄ def == (s̄(w̄)−s̄(z̄))/1.5α1 the estimate of the margin of victory of the election E (and thus the output of the algorithm), where w̄ ∈ argmaxx∈C{s̄(x)} and z̄ ∈ argmaxx∈C\\{w̄}{s̄(x)}. We claim that, if ∀x ∈ C, |s̄(x) − s(x)| 6 ε′n, then |M̄ −Mα(E)| 6 Mα(E)/3 + εn. This can be shown as follows.\nM̄−Mα(E) = s̄(w̄) − s̄(z̄)\n1.5α1 −Mα(E)\n6 s(w) − s(z)\n1.5α1 +\n2ε′n\n1.5 −Mα(E)\n6 1\n3 Mα(E) + εn\nThe second inequality follows from the fact that, s̄(w̄) 6 s(w̄) + ε′n 6 s(w) + ε′n and s̄(z̄) > s̄(z) > s(z) − ε′n. The third inequality follows from Lemma 5.3. Similarly, we bound Mα(E) − M̄ as follows.\nMα(E) − M̄ = Mα(E) − s̄(w) − s̄(z)\n1.5α1\n6 Mα(E) − s(w) − s(z)\n1.5α1 +\n2ε′n\n1.5\n6 1\n3 Mα(E) + εn\n99\nThis proves the claim. Now we bound the success probability of the algorithm as follows.\nLet A be the event that ∀x ∈ C, |s̄(x) − s(x)| 6 ε′n.\nPr\n[\n|M̄ −Mα(E)| 6 1\n3 Mα(E) + εn\n]\n> Pr\n[\n|M̄ −Mα(E)| 6 1\n3 Mα(E) + εn\n∣ ∣ ∣ ∣ A ] Pr[A]\n= Pr[A] > 1− 2m exp ( −ε′2ℓ/3 )\nThe third equality follows from Lemma 5.3 and the fourth inequality follows from inequality 5.1. Now by choosing ℓ = (12/ε2) ln(2m/δ), we get a (1/3, ε, δ)–MOV algorithm for the scoring rules.\nNow we show an algorithm for the (0, ε, δ)–MOV problem for the k-approval voting rule which not only provides more accurate estimate of the margin of victory, but also has a lower sample complexity. The following structural result will be used subsequently.\nLemma 5.10. Let E = (V,C) be an arbitrary instance of a k-approval election. Ifw and z are the candidates that receive highest and second highest score respectively in E and Mk−approval(E) is the margin of victory of E, then,\n2(Mk−approval(E) − 1) < s(w) − s(z) 6 2Mk−approval(E)\nProof. We call a vote v ∈ V favorable if w appears within the top k positions and z does not appear within top the k positions in v. We claim that the number of favorable votes must be at least Mk−approval(E). Indeed, otherwise, we swap the positions of w and z in all the favorable votes while keeping the other candidates fixed. This makes the score of z at least as much as the score of w which contradicts the fact that the margin of victory is Mk−approval(E). Now notice that the score of z must remain less than the score of w even if we swap the positions of w and z in Mk−approval(E) − 1 many favorable votes, since the margin of victory is Mk−approval(E). Each such vote change increases the score of z by one and reduces the score of w by one. Hence, 2(Mk−approval(E) − 1) < s(w) − s(z). Again, since the margin of victory is Mk−approval(E), there exists a candidate x other than w and Mk−approval(E) many votes in V which can be modified such that x becomes a winner of\n100\nthe modified election. Now each vote change can reduce the score of w by at most one and increase the score of x by at most one. Hence, s(w) − s(x) 6 2Mk−approval(E) and thus s(w) − s(z) 6 2Mk−approval(E) since s(z) > s(x).\nWith Lemma 5.4 and 5.10 at hand, we now describe our margin of victory estimation\nalgorithm for the k-approval voting rule.\nTheorem 5.15. There is a polynomial time (0, ε, δ)–MOV algorithm for the k-approval voting rule with sample complexity at most (12/ε2) ln(2k/δ).\nProof. Let E = (V,C) be an arbitrary k-approval election. We sample ℓ votes uniformly at random from V with replacement. For a candidate x, define a random variable Xi(x) which takes value 1 if x appears among the top k candidates in the ith sample vote, and 0 otherwise. Define s̄(x) def == n\nℓ ∑ℓ i=1 Xi(x) the estimate of the score of the candidate x, and let s(x) be the\nactual score of x. Also define ε′ = ε 2 . Then by the Chernoff bound (Theorem 2.5), we have:\nPr [|s̄(x) − s(x)| > ε′n] 6 2 exp\n( − ε′2ℓn\n3s(x)\n)\nNow we apply the union bound to get the following.\nPr[∃x ∈ C, |s̄(x) − s(x)| > ε′n] 6 ∑\nx∈C 2 exp\n( − ε′2ℓn\n3s(x)\n)\n6 2k exp ( −ε′2ℓ/3 )\n(5.2)\nThe second inequality follows from Lemma 5.4 : The expression ∑ x∈C 2 exp ( − ε ′2ℓn\n3s(x)\n)\nis\nmaximized subject to the constraints that 0 6 s(x) 6 n, ∀x ∈ C and ∑x∈C s(x) = kn, when s(x) = n∀x ∈ C′ for any subset of candidates C′ ⊆ C with |C′| = k and s(x) = 0∀x ∈ C \\ C′.\nNow to estimate the margin of victory of the given election E, let w̄ and z̄ be candidates with maximum and second maximum estimated score respectively. That is, w̄ ∈ argmaxx∈C{s̄(x)} and z̄ ∈ argmaxx∈C\\{w̄}{s̄(x)}. We define M̄ def == (s̄(w̄)−s̄(z̄))/2 the estimate of the margin of victory of the election E (and thus the output of the algorithm). Let A be the event that ∀x ∈ C, |s̄(x) − s(x)| 6 ε′n. We bound the success probability of the algorithm as follows.\n101\nPr [ |M̄−Mk−approval(E)| 6 εn ]\n> Pr [ |M̄−Mk−approval(E)| 6 εn ∣ ∣A ] Pr[A]\n= Pr[A] > 1 − 2k exp ( −ε′2ℓ/3 )\nThe second equality follows from Lemma 5.10 and an argument analogous to the proof of Theorem 5.14. The third inequality follows from inequality 5.2. Now by choosing ℓ = (12/ε2) ln(2k/δ), we get a (0, ε, δ)–MOV algorithm.\nNote that, the sample complexity upper bound matches with the lower bound proved in Corollary 5.3 for the k-approval voting rule when k is a constant, irrespective of the number of candidates. Next, we estimate the margin of victory of an approval election.\nTheorem 5.16. There is a polynomial time (0, ε, δ)–MOV algorithm for the approval rule with sample complexity at most (12/ε2) ln(2m/δ).\nProof. We estimate the approval score of every candidate within an additive factor of εn/2 by sampling (12/ε2) ln(2m/δ) many votes uniformly at random with replacement and the result follows from an argument analogous to the proofs of Lemma 5.10 and Theorem 5.15."
    }, {
      "heading" : "5.3.2.2 Bucklin Voting Rule",
      "text" : "Now we consider the Bucklin voting rule. Given an election E = (V,C), a candidate x ∈ C, and an integer ℓ ∈ [m], we denote the number of votes in V in which x appears within the top ℓ positions by nℓ(x). We prove useful bounds on the margin of victory of any Bucklin election in Lemma 5.11.\nLemma 5.11. Let E = (V,C) be an arbitrary instance of a Bucklin election, w the winner of E, and MBucklin(E) the margin of victory of E. Let us define a quantity ∆(E) as follows.\n∆(E) def == min\nℓ∈[m−1]:nℓ(w)>n/2, x∈C\\{w}:nℓ(x)6n/2\n{nℓ(w) − nℓ(x) + 1}\nThen, ∆(E)\n2 6 MBucklin(E) 6 ∆(E)\n102\nProof. Pick any ℓ ∈ [m − 1] and x ∈ C \\ {w} such that, nℓ(w) > n/2 and nℓ(x) 6 n/2. Now by changing nℓ(w) − ⌊n/2⌋ many votes, we can ensure that w is not placed within the top ℓ positions in more than n/2 votes: choose nℓ(w) − ⌊n/2⌋ many votes where w appears within top ℓ positions and swap w with candidates placed at the last position in those votes. Similarly, by changing ⌊n/2⌋ + 1 − nℓ(x) many votes, we can ensure that x is placed within top ℓ positions in more than n/2 votes. Hence, by changing at most nℓ(w)− ⌊n/2⌋+ ⌊n/2⌋+ 1 − nℓ(x) = nℓ(w) − nℓ(x) + 1 many votes, we can make w not win the election. Hence, MBucklin(E) 6 nℓ(w) − nℓ(x) + 1. Now since we have picked an arbitrary ℓ and an arbitrary candidate x, we have MBucklin(E) 6 ∆(E).\nFor the other inequality, since the margin of victory is MBucklin(E), there exists an ℓ ′ ∈ [m− 1], a candidate x ∈ C \\ {w}, and MBucklin(E) many votes in V such that, we can change those votes in such a way that in the modified election, w is not placed within top ℓ′ positions in more than n/2 votes and x is placed within top ℓ′ positions in more than n/2 votes. Hence, we have the following.\nMBucklin(E) > n ′ ℓ(w) − ⌊ n 2 ⌋ ,MBucklin(E) > ⌊ n 2 ⌋ + 1 − n′ℓ(x)\n⇒ MBucklin(E) > max{nℓ′(w) − ⌊n\n2\n⌋ , ⌊n\n2\n⌋\n+ 1 − nℓ′(x)}\n⇒ MBucklin(E) > nℓ′(w) −\n⌊\nn 2\n⌋ + ⌊ n 2 ⌋ + 1 − nℓ′(x)\n2\n> ∆(E)\n2\nNotice that, given an election E, ∆(E) can be computed in a polynomial amount of time.\nLemma 5.7 leads us to the following result for the Bucklin voting rule.\nTheorem 5.17. There is a polynomial time (1/3, ε, δ)–MOV algorithm for the Bucklin rule with sample complexity (12/ε2) ln(2m/δ).\nProof. Similar to the proof of Theorem 5.15, we estimate, for every candidate x ∈ C and for every integer ℓ ∈ [m], the number of votes where x appears within top ℓ positions within an approximation factor of (0, ε/2). Next, we compute an estimate of ∆̄(E) from the sampled votes and output the estimate for the margin of victory as ∆̄(E)/1.5. Using Lemma 5.11, we can argue the rest of the proof in a way that is analogous to the proofs of Theorem 5.5 and 5.6.\n103"
    }, {
      "heading" : "5.3.2.3 Maximin Voting Rule",
      "text" : "Next, we present our (1/3, ε, δ)–MOV algorithm for the maximin voting rule.\nTheorem 5.18. There is a polynomial time (1/3, ε, δ)–MOV algorithm for the maximin rule with sample complexity (24/ε2) ln(2m/δ).\nProof. Let E = (V,C) be an instance of maximin election. Let x and y be any two candidates. We sample ℓ votes uniformly at random from the set of all votes with replacement. Let Xi(x,y) be a random variable defined as follows.\nXi(x,y) =\n \n 1, if x ≻ y in the ith sample vote −1, else\nDefine D̄E(x,y) = n ℓ ∑ℓ i=1 Xi(x,y). By using the Chernoff bound and union bound, we have the following.\nPr [ ∃x,y ∈ C, |D̄E(x,y) −DE(x,y)| > εn ] 6 2m2 exp\n(\n− ε2ℓ\n3\n)\nWe define M̄ def == (s̄(w̄)−s̄(z̄))/3, the estimate of the margin of victory of E, where w̄ ∈ argmaxx∈C{s̄(x)} and z̄ ∈ argmaxx∈C\\{w̄}{s̄(x)}. Now using Lemma 5.5, we can complete the rest of the proof in a way that is analogous to the proof of Theorem 5.14."
    }, {
      "heading" : "5.3.2.4 Copelandα Voting Rule",
      "text" : "Now we present our result for the Copelandα voting rule. The following lemma is immediate from Theorem 11 in [Xia12].\nLemma 5.12. Γ(E) 6 MCopelandα(E) 6 2(⌈lnm⌉ + 1)Γ(E).\nProof. Follows from Theorem 11 in [Xia12].\nTheorem 5.19. For the Copelandα voting rule, there is a polynomial time (1 −O (1/lnm) , ε, δ)– MOV algorithm whose sample complexity is (96/ε2) ln(2m/δ).\nProof. Let E = (V,C) be an instance of a Copelandα election. For every x,y ∈ C, we compute D̄E(x,y), which is an estimate of DE(x,y), within an approximation factor of (0, ε ′), where ε′ = ε/4. This can be achieved with an error probability at most δ by sampling\n104\n(96/ε2) ln(2m/δ) many votes uniformly at random with replacement (the argument is same as the proof of Theorem 5.5). We define s̄′t(V, x) = |{y ∈ C : y 6= x,DE(y, x) < 2t}| + α|{y ∈ C : y 6= x,DE(y, x) = 2t}|. We also define RM(x,y) between x and y to be the minimum integer t such that, s̄′−t(V, x) 6 s ′ t(V,y). Let w̄ be the winner of the sampled election, z̄ = argminx∈C\\{w̄}{RM(w, x)}, w the winner of E, and z = argminx∈C\\{w}{RM(w, x)}. Since, D̄E(x,y) is an approximation of DE(x,y) within a factor of (0, ε ′), we have the following for every candidate x,y ∈ C.\ns′t(V, x) − ε ′n 6 s̄′t(V, x) 6 s ′ t(V, x) + ε ′n\nRM(x,y) − 2ε′n 6 RM(x,y) 6 RM(x,y) + 2ε′n (5.3)\nDefine Γ̄(E) = RM(w̄, z̄) to be the estimate of Γ(E). We show the following claim.\nClaim 5.1. With the above definitions of w, z, w̄, and z̄, we have the following.\nΓ(E) − 4ε′n 6 Γ̄(E) 6 Γ(E) + 4ε′n\nProof. Below, we show the upper bound for Γ̄(E).\nΓ̄(E) = RM(w̄, z̄) 6 RM(w, z̄) + 2ε′n\n6 RM(w, z) + 2ε′n 6 RM(w, z) + 4ε′n = Γ(E) + 4ε′n\nThe second inequality follows from the fact that D̄E(x,y) is an approximation ofDE(x,y) by a factor of (0, ε′). The third inequality follows from the definition of z̄, and the fourth inequality uses inequality 5.3. Now we show the lower bound for Γ̄(E).\nΓ̄(E) = RM(w̄, z̄) > RM(w, z̄) − 2ε′n\n> RM(w, z̄) − 4ε′n > RM(w, z) − 4ε′n = Γ(E) − 4ε′n\n105\nThe third inequality follows from inequality 5.3 and the fourth inequality follows from the definition of z.\nWe define M̄, the estimate ofMCopelandα(E), to be 4(lnm+1)\n2 lnm+3 Γ̄(E). The following argument\nshows that M̄ is a ( 1 −O ( 1 lnm ) , ε, δ ) –estimate of MCopelandα(E).\nM̄−MCopelandα(E)\n= 4(lnm + 1)\n2 lnm+ 3 Γ̄(E) −MCopelandα(E)\n6 4(lnm + 1)\n2 lnm+ 3 Γ(E) −MCopelandα(E) +\n16(lnm + 1)\n2 lnm+ 3 ε′n\n6 4(lnm + 1)\n2 lnm+ 3 MCopelandα(E) −MCopelandα(E) + εn\n6 2 lnm + 1\n2 lnm + 3 MCopelandα(E) + εn\n6\n(\n1 −O\n(\n1\nlnm\n))\nMCopelandα(E) + εn\nThe second inequality follows from Claim 5.1 and the third inequality follows from\nLemma 5.12. Analogously, we have:\nMCopelandα(E) − M̄\n= MCopelandα(E) − 4(lnm+ 1)\n2 lnm + 3 Γ̄(E)\n6 MCopelandα(E) − 4(lnm+ 1)\n2 lnm + 3 Γ(E) +\n16(lnm + 1)\n2 lnm+ 3 ε′n\n6 MCopelandα(E) − 2(lnm+ 1)\n2 lnm + 3 MCopelandα(E) + εn\n6 2 lnm + 1\n2 lnm + 3 MCopelandα(E) + εn\n6\n(\n1 −O\n(\n1\nlnm\n))\nMCopelandα(E) + εn\nThe second line follows Claim 5.1 and the third line follows from Lemma 5.12.\nThe approximation factor in Theorem 5.19 is weak when we have a large number of candidates. The main difficulty for showing a better approximation factor for the Copelandα voting\n106\nrule is to find a polynomial time computable quantity (for example, Γ(E) in Lemma 5.12) that exhibits tight bounds with margin of victory. We remark that, existence of such a quantity will not only imply a better estimation algorithm, but also, a better approximation algorithm (the best known approximation factor for finding the margin of victory for the Copelandα voting rule is O(lnm) and it uses the quantity Γ(E)). However, we remark that Theorem 5.19 will be useful in applications, for example, post election audit and polling, where the number of candidates is often small."
    }, {
      "heading" : "5.4 Conclusion",
      "text" : "In this work, we introduced the (ε, δ)-WINNER DETERMINATION problem and showed (often tight) bounds for the sample complexity for many common voting rules. We have also introduced the (c, ε, δ)–MARGIN OF VICTORY problem and presented efficient sampling based algorithms for solving it for many commonly used voting rules which are also often observes an optimal number of sample votes. We observe that predicting the winner of an elections needs least number of queries, whereas more involved voting rules like Borda and maximin need significantly more queries.\nIn the next chapter, we study the problem of finding a winner of an election when votes\nare arriving one by one in a sequential manner.\n107\nChapter 6\nStreaming Algorithms for Winner\nDetermination\nWe give the first optimal bounds for returning the ℓ1-heavy hitters in a data stream of insertions, together with their approximate frequencies, closing a long line of work on\nthis problem. For a stream of m items in {1, 2, . . . ,n} and parameters 0 < ε < φ 6 1, let\nfi denote the frequency of item i, i.e., the number of times item i occurs in the stream. With arbitrarily large constant probability, our algorithm returns all items i for which\nfi > φm, returns no items j for which fj 6 (φ − ε)m, and returns approximations f̃i with |f̃i − fi| 6 εm for each item i that it returns. Our algorithm uses O(ε −1 logφ−1 + φ−1 logn+log logm) bits of space, processes each stream update inO(1)worst-case time,\nand can report its output in time linear in the output size. We also prove a lower bound,\nwhich implies that our algorithm is optimal up to a constant factor in its space complexity.\nA modification of our algorithm can be used to estimate the maximum frequency up to\nan additive εm error in the above amount of space, resolving Question 3 in the IITK\n2006 Workshop on Algorithms for Data Streams for the case of ℓ1-heavy hitters. We also introduce several variants of the heavy hitters and maximum frequency problems, inspired\nby rank aggregation and voting schemes, and show how our techniques can be applied in\nsuch settings. Unlike the traditional heavy hitters problem, some of these variants look at\nA preliminary version of the work in this chapter was published as [BDW16]: Arnab Bhattacharyya, Palash Dey, and David P. Woodruff. An optimal algorithm for l1-heavy hitters in insertion streams and related problems. In Proc. 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 16, pages 385- 400, New York, NY, USA, 2016. ACM.\n108\ncomparisons between items rather than numerical values to determine the frequency of\nan item."
    }, {
      "heading" : "6.1 Introduction",
      "text" : "The data stream model has emerged as a standard model for processing massive data sets. Because of the sheer size of the data, traditional algorithms are no longer feasible, e.g., it may be hard or impossible to store the entire input, and algorithms need to run in linear or even sublinear time. Such algorithms typically need to be both randomized and approximate. Moreover, the data may not physically reside on any device, e.g., if it is internet traffic, and so if the data is not stored by the algorithm, it may be impossible to recover it. Hence, many algorithms must work given only a single pass over the data. Applications of data streams include data warehousing [HSST05, BR99, FSG+98, HPDW01], network measurements [ABW03, CKMS08, DLOM02, EV03], sensor networks [BGS01, SBAS04], and compressed sensing [GSTV07, CRT06]. We refer the reader to recent surveys on the data stream model [Mut05, Nel12, Cor12].\nOne of the oldest and most fundamental problems in the area of data streams is the problem of finding the ℓ1-heavy hitters (or simply, “heavy hitters”), also known as the topk, most popular items, frequent items, elephants, or iceberg queries. Such algorithms can be used as subroutines in network flow identification at IP routers [EV03], association rules and frequent itemsets [AS94, SON95, Toi96, Hid99, HPY00], iceberg queries and iceberg datacubes [FSG+98, BR99, HPDW01]. The survey [CH08] presents an overview of the stateof-the-art for this problem, from both theoretical and practical standpoints.\nWe now formally define the heavy hitters problem that we focus on in this work:\nDefinition 6.1. ((ε,φ)-Heavy Hitters Problem) In the (ε,φ)-Heavy Hitters Problem, we are given parameters 0 < ε < φ 6 1 and a stream a1, . . . ,am of items aj ∈ {1, 2, . . . ,n}. Let fi denote the number of occurrences of item i, i.e., its frequency. The algorithm should make one pass over the stream and at the end of the stream output a set S ⊆ {1, 2, . . . ,n} for which if fi > φm, then i ∈ S, while if fi 6 (φ − ε)m, then i /∈ S. Further, for each item i ∈ S, the algorithm should output an estimate f̃i of the frequency fi which satisfies |fi − f̃i| 6 εm.\nNote that other natural definitions of heavy hitters are possible and sometimes used. For\nexample, ℓ2-heavy hitters are those items i for which f 2 i > φ\n2 ∑n\nj=1 f 2 j , and more generally,\n109\nℓp-heavy hitters are those items i for which f p i > φ\np ∑n\nj=1 f p j . It is in this sense that Definition\n6.1 corresponds to ℓ1-heavy hitters. While ℓp-heavy hitters for p > 1 relax ℓ1-heavy hitters and algorithms for them have many interesting applications, we focus on the most direct and common formulation of the heavy hitters notion.\nWe are interested in algorithms which use as little space in bits as possible to solve the (ε,φ)-Heavy Hitters Problem. Further, we are also interested in minimizing the update time and reporting time of such algorithms. Here, the update time is defined to be the time the algorithm needs to update its data structure when processing a stream insertion. The reporting time is the time the algorithm needs to report the answer after having processed the stream. We allow the algorithm to be randomized and to succeed with probability at least 1 − δ for 0 < δ < 1. We do not make any assumption on the ordering of the stream a1, . . . ,am. This is desirable as often in applications one cannot assume a best-case or even a random order. We are also interested in the case when the length m of the stream is not known in advance, and give algorithms in this more general setting.\nThe first algorithm for the (ε,φ)-Heavy Hitters Problem was given by Misra and Gries [MG82], who achieved O(ε−1(logn+ logm)) bits of space for any φ > ε. This algorithm was rediscovered by Demaine et al. [DLOM02], and again by Karp et al. [KSP03]. Other than these algorithms, which are deterministic, there are also a number of randomized algorithms, such as the CountSketch [CCFC04], Count-Min sketch [CM05], sticky sampling [MM02], lossy counting [MM02], space-saving [MAE05], sample and hold [EV03], multi-stage bloom filters [CFM09], and sketch-guided sampling [KX06]. Berinde et al. [BICS10] show that using O(kε−1 log(mn)) bits of space, one can achieve the stronger guarantee of reporting, for each item i ∈ S, f̃i with |f̃i − fi| 6 ε/kFres(k)1 , where F res(k) 1 < m denotes the sum of frequencies of items in {1, 2, . . . ,n} excluding the frequencies of the k most frequent items.\nWe emphasize that prior to our work the best known algorithms for the (ε,φ)-Heavy Hitters Problem used O(ε−1(logn + logm)) bits of space. Two previous lower bounds were known. The first is a lower bound of log( (\nn 1/φ\n)\n) = Ω(φ−1 log(φn)) bits, which comes from\nthe fact that the output set S can contain φ−1 items and it takes this many bits to encode them. The second lower bound is Ω(ε−1) which follows from a folklore reduction from the randomized communication complexity of the Index problem. In this problem, there are two players, Alice and Bob. Alice has a bit string x of length (2ε)−1, while Bob has an index i. Alice creates a stream of length (2ε)−1 consisting of one copy of each j for which xj = 1 and copies of a dummy item to fill the rest of the stream. She runs the heavy hitters streaming\n110\nalgorithm on her stream and sends the state of the algorithm to Bob. Bob appends (2ε)−1 copies of the item i to the stream and continues the execution of the algorithm. For φ = 1/2, it holds that i ∈ S. Moreover, fi differs by an additive εm factor depending on whether xi = 1 or xi = 0. Therefore by the randomized communication complexity of the Index problem [KNR99], the (ε, 1/2)-heavy hitters problem requires Ω(ε−1) bits of space. Although this proof was for φ = 1/2, no better lower bound is known for any φ > ε.\nThus, while the upper bound for the (ε,φ)-Heavy Hitters Problem is O(ε−1(logn + logm)) bits, the best known lower bound is only Ω(φ−1 logn + ε−1) bits. For constant φ, and logn ≈ ε−1, this represents a nearly quadratic gap in upper and lower bounds. Given the limited resources of devices which typically run heavy hitters algorithms, such as internet routers, this quadratic gap can be critical in applications.\nA problem related to the (ε,φ)-Heavy Hitters Problem is estimating the maximum frequency in a data stream, also known as the ℓ∞-norm. In the IITK 2006 Workshop on Algorithms for Data Streams, Open Question 3 asks for an algorithm to estimate the maximum frequency of any item up to an additive εm error using as little space as possible. The best known space bound is still O(ε−1 logn) bits, as stated in the original formulation of the question (note that the “m” in the question there corresponds to the “n” here). Note that, if one can find an item whose frequency is the largest, up to an additive εm error, then one can solve this problem. The latter problem is independently interesting and corresponds to finding approximate plurality election winners in voting streams [DB15]. We refer to this problem as the ε-Maximum problem.\nFinally, we note that there are many other variants of the (ε,φ)-Heavy Hitters Problem that one can consider. One simple variant of the above is to output an item of frequency within εm of the minimum frequency of any item in the universe. We refer to this as the ε-Minimum problem. This only makes sense for small universes, as otherwise outputting a random item typically works. This is useful when one wants to count the “number of dislikes”, or in anomaly detection; see more motivation below. In other settings, one may not have numerical scores associated with the items, but rather, each stream update consists of a “ranking” or “total ordering” of all stream items. This may be the case in ranking aggregation on the web (see, e.g., [MBG04, MYCC07]) or in voting streams (see, e.g., [CS05, CP11, DB15, Xia12]). One may consider a variety of aggregation measures, such as the Borda score of an item i, which asks for the sum, over rankings, of the number of items j 6= i for which i is ranked ahead of j in the ranking. Alternatively, one may consider the Maximin score of an item i,\n111\nwhich asks for the minimum, over items j 6= i, of the number of rankings for which i is ranked ahead of j. For these aggregation measures, one may be interested in finding an item whose score is an approximate maximum. This is the analogue of the ε-Maximum problem above. Or, one may be interested in listing all items whose score is above a threshold, which is the analogue of the (ε,φ)-Heavy Hitters Problem.\nWe give more motivation of these variants of heavy hitters in this section below, and more\nprecise definitions in Section 6.2."
    }, {
      "heading" : "6.1.1 Our Contribution",
      "text" : "Our results are summarized in Table 6.1. We note that independently of this work and nearly parallelly, there have been improvements to the space complexity of the ℓ2-heavy hitters problem in insertion streams [BCIW16] and to the time complexity of the ℓ1-heavy hitters problem in turnstile1 streams [LNNT16]. These works use very different techniques.\nOur first contribution is an optimal algorithm and lower bound for the (ε,φ)-Heavy Hitters Problem. Namely, we show that there is a randomized algorithm with constant proba-\n1In a turnstile stream, updates modify an underlying n-dimensional vector x initialized at the zero vector; each update is of the form x ← x + ei or x ← x − ei where ei is the i’th standard unit vector. In an insertion stream, only updates of the form x ← x+ ei are allowed.\n112\nbility of success which solves this problem using\nO(ε−1 logφ−1 + φ−1 logn+ log logm)\nbits of space, and we prove a lower bound matching up to constant factors. In the unit-cost RAM model with O(logn) bit words, our algorithm has O(1) update time and reporting time linear in the output size, under the standard assumptions that the length of the stream and universe size are at least poly(ε−1 log(1/φ)). Furthermore, we can achieve nearly the optimal space complexity even when the length m of the stream is not known in advance. Although the results of [BICS10] achieve stronger error bounds in terms of the tail, which are useful for skewed streams, here we focus on the original formulation of the problem.\nNext, we turn to the problem of estimating the maximum frequency in a data stream up\nto an additive εm. We give an algorithm using\nO(ε−1 log ε−1 + logn + log logm)\nbits of space, improving the previous best algorithms which required space at least Ω(ε−1 logn) bits, and show that our bound is tight. As an example setting of parameters, if ε−1 = Θ(logn) and log logm = O(logn), our space complexity is O(logn log logn) bits, improving the previousΩ(log2 n) bits of space algorithm. We also prove a lower bound showing our algorithm is optimal up to constant factors. This resolves Open Question 3 from the IITK 2006 Workshop on Algorithms for Data Streams in the case of insertion streams, for the case of “ℓ1-heavy hitters”. Our algorithm also returns the identity of the item with the approximate maximum frequency, solving the ε-Maximum problem.\nWe then focus on a number of variants of these problems. We first give nearly tight bounds for finding an item whose frequency is within εm of the minimum possible frequency. While this can be solved using our new algorithm for the (ε, ε)-Heavy Hitters Problem, this would incur Ω(ε−1 log ε−1 + log logm) bits of space, whereas we give an algorithm using only O(ε−1 log log(ε−1) + log logm) bits of space. We also show a nearly matching Ω(ε−1 + log logm) bits of space lower bound. We note that for this problem, a dependence on n is not necessary since if the number of possible items is sufficiently large, then outputting the identity of a random item among the first say, 10ε−1 items, is a correct solution with large constant probability.\n113\nFinally, we study variants of heavy hitter problems that are ranking-based. In this setting, each stream update consists of a total ordering of the n universe items. For the ε-Borda problem, we give an algorithm using O(n(log ε−1 + log logn) + log logm) bits of space to report the Borda score of every item up to an additive εmn. We also show this is nearly optimal by proving an Ω(n log ε−1 + log logm) bit lower bound for the problem, even in the case when one is only interested in outputting an item maximum Borda score up to an additive εmn. For the ε-Maximin problem, we give an algorithm using O(nε−2 log2 n + log logm) bits of space to report the maximin score of every item up to an additive εm, and prove an Ω(nε−2 + log logm) bits of space lower bound even in the case when one is only interested in outputting the maximum maximin score up to an additive εm. This shows that finding heavy hitters with respect to the maximin score is significantly more expensive than with respect to the Borda score."
    }, {
      "heading" : "6.1.2 Motivation for Variants of Heavy Hitters",
      "text" : "While the (ε,φ)-Heavy Hitters and ε-Maximum problem are very well-studied in the data stream literature, the other variants introduced are not. We provide additional motivation for them here.\nFor the ε-Minimum problem, in our formulation, an item with frequency zero, i.e., one that does not occur in the stream, is a valid solution to the problem. In certain scenarios, this might not make sense, e.g., if a stream containing only a small fraction of IP addresses. However, in other scenarios we argue this is a natural problem. For instance, consider an online portal where users register complaints about products. Here, minimum frequency items correspond to the “best” items. That is, such frequencies arise in the context of voting or more generally making a choice: in cases for which one does not have a strong preference for an item, but definitely does not like certain items, this problem applies, since the frequencies correspond to “number of dislikes”.\nThe ε-Minimum problem may also be useful for anomaly detection. Suppose one has a known set of sensors broadcasting information and one observes the “From:” field in the broadcasted packets. Sensors which send a small number of packets may be down or defective, and an algorithm for the ε-Minimum problem could find such sensors.\n114\nFinding items with maximum and minimum frequencies in a stream correspond to finding winners under plurality and veto voting rules respectively in the context of voting1 [BCE+15]. The streaming aspect of voting could be crucial in applications like online polling [KTW11], recommender systems [RV97, HKTR04, AT05] where the voters are providing their votes in a streaming fashion and at every point in time, we would like to know the popular items. While in some elections, such as for political positions, the scale of the election may not be large enough to require a streaming algorithm, one key aspect of these latter votingbased problems is that they are rank-based which is useful when numerical scores are not available. Orderings naturally arise in several applications - for instance, if a website has multiple parts, the order in which a user visits the parts given by its clickstream defines a voting, and for data mining and recommendation purposes the website owner may be interested in aggregating the orderings across users. Motivated by this connection, we define similar problems for two other important voting rules, namely Borda andmaximin. The Borda scoring method finds its applications in a wide range of areas of artificial intelligence, for example, machine learning [HHS94, CP06, VZ14, PPR15], image processing [LN06, MG09], information retrieval [Li14, AM01, NC06], etc. The Maximin score is often used when the spread between the best and worst outcome is very large (see, e.g., p. 373 of [MR91]). The maximin scoring method also has been used frequently in machine learning [WL04, JMP+14], human computation [MPC12, MPC13], etc."
    }, {
      "heading" : "6.2 Problem Definitions",
      "text" : "We now formally define the problems we study here. Suppose we have 0 < ε < ϕ < 1.\nDefinition 6.2. (ε,ϕ)-LIST HEAVY HITTERS Given an insertion-only stream of length m over a universe U of size n, find all items in U with frequency more than ϕm, along with their frequencies up to an additive error of εm, and report no items with frequency less than (ϕ− ε)m.\nDefinition 6.3. ε-MAXIMUM Given an insertion-only stream of length m over a universe U of size n, find the maximum frequency up to an additive error of εm.\n1In fact, the first work [Moo81] to formally pose the heavy hitters problem couched it in the context of voting.\n115\nNext we define the minimum problem for 0 < ε < 1.\nDefinition 6.4. ε-MINIMUM Given an insertion-only stream of length m over a universe U of size n, find the minimum frequency up to an additive error of εm.\nNext we define related heavy hitters problems in the context of rank aggregation. The input is a stream of rankings (permutations) over an item set U for the problems below. The Borda score of an item i is the sum, over all rankings, of the number of items j 6= i for which i is ranked ahead of j in the ranking.\nDefinition 6.5. (ε,ϕ)-LIST BORDA Given an insertion-only stream over a universe L(U) where |U| = n, find all items with Borda score more than ϕmn, along with their Borda score up to an additive error of εmn, and report no items with Borda score less than (ϕ− ε)mn.\nDefinition 6.6. ε-BORDA Given an insertion-only stream over a universe L(U) where |U| = n, find the maximum Borda score up to an additive error of εmn.\nThe maximin score of an item i is the minimum, over all items j 6= i, of the number of rankings for which i is ranked ahead of j.\nDefinition 6.7. (ε,ϕ)-LIST MAXIMIN Given an insertion-only stream over a universe L(U) where |U| = n, find all items with maximin score more than ϕm along with their maximin score up to an additive error of εm, and report no items with maximin score less than (ϕ− ε)m.\nDefinition 6.8. ε-MAXIMIN Given an insertion-only stream over a universe L(U) where |U| = n, find the maximum maximin score up to an additive error of εm.\nNotice that the maximum possible Borda score of an item is m(n − 1) = Θ(mn) and the maximum possible maximin score of an item is m. This justifies the approximation factors in Definition 6.5 to 6.8. We note that finding an item with maximum Borda score within additive εmn or maximum maximin score within additive εm corresponds to finding an approximate winner of an election (more precisely, what is known as an ε-winner) [DB15].\n116"
    }, {
      "heading" : "6.3 Our Algorithms",
      "text" : "In this section, we present our upper bound results. All omitted proofs are in Appendix B. Before describing specific algorithms, we record some claims for later use. We begin with the following space efficient algorithm for picking an item uniformly at random from a universe of size n below.\nLemma 6.1. Suppose m is a power of two1. Then there is an algorithm A for choosing an item with probability 1/m that has space complexity of O(log logm) bits and time complexity of O(1) in the unit-cost RAM model.\nProof. We generate a (log2 m)-bit integer C uniformly at random and record the sum of the digits in C. Choose the item only if the sum of the digits is 0, i.e. if C = 0.\nWe remark that the algorithm in Lemma 6.1 has optimal space complexity as shown in\nProposition 6.1 in Appendix B.\nWe remark that the algorithm in Lemma 6.1 has optimal space complexity as shown in Proposition 6.1 below which may be of independent interest. We also note that every algorithm needs to toss a fair coin at least Ω(logm) times to perform any task with probability at least 1/m.\nProposition 6.1. Any algorithm that chooses an item from a set of size n with probability p for 0 < p 6 1 n , in unit cost RAM model must use Ω(log logm) bits of memory.\nProof. The algorithm generates t bits uniformly at random (the number of bits it generates uniformly at random may also depend on the outcome of the previous random bits) and finally picks an item from the say x. Consider a run R of the algorithm where it chooses the item x with smallest number of random bits getting generated; say it generates t random bits in this run R. This means that in any other run of the algorithm where the item x is chosen, the algorithm must generate at least t many random bits. Let the random bits generated in R be r1, · · · , rt. Let si be the memory content of the algorithm immediately after it generates ith random bit, for i ∈ [t], in the run R. First notice that if t < log2 n, then the probability with which the item x is chosen is more than 1\nn , which would be a contradiction. Hence,\nt > log2 n. Now we claim that all the si’s must be different. Indeed otherwise, let us assume\n1In all our algorithms, whenever we pick an item with probability p > 0, we can assume, without loss of generality, that 1/p is a power of two. If not, then we replace p with p ′ where 1/p′ is the largest power of two less than 1/p. This does not affect correctness and performance of our algorithms.\n117\nsi = sj for some i < j. Then the algorithm chooses the item x after generating t − (j − i) many random bits (which is strictly less than t) when the random bits being generated are r1, · · · , ri, rj+1, · · · , rt. This contradicts the assumption that the runRwe started with chooses the item x with smallest number of random bits generated.\nOur second claim is a standard result for universal families of hash functions.\nLemma 6.2. For S ⊆ A, δ ∈ (0, 1), and universal family of hash functions H = {h|h : A → [⌈|S|2/δ⌉]}:\nPr h∈UH\n[∃i 6= j ∈ S,h(i) = h(j)] 6 δ\nProof. For every i 6= j ∈ S, since H is a universal family of hash functions, we have Prh∈UH[h(i) = h(j)] 6 1 ⌈|S|2/δ⌉ . Now we apply the union bound to get Prh∈UH[∃i 6= j ∈ S,h(i) = h(j)] 6 |S|2\n⌈|S|2/δ⌉ 6 δ\nOur third claim is folklore and also follows from the celebrated DKW inequality [DKW56].\nWe provide a simple proof here that works for constant δ.\nLemma 6.3. Let fi and f̂i be the frequencies of an item i in a stream S and in a random sample T of size r from S, respectively. Then for r > 2ε−2 log(2δ−1), with probability 1 − δ, for every universe item i simultaneously, ∣\n∣ ∣ ∣ ∣ f̂i r − fi m\n∣ ∣ ∣ ∣ ∣ 6 ε.\nProof for constant δ. This follows by Chebyshev’s inequality and a union bound. Indeed, consider a given i ∈ [n] with frequency fi and suppose we sample each of its occurrences pairwise-independently with probability r/m, for a parameter r. Then the expected number E[f̂i] of sampled occurrences is fi · r/m and the variance Var[f̂i] is fi · r/m(1− r/m) 6 fir/m. Applying Chebyshev’s inequality,\nPr [ ∣ ∣\n∣ f̂i − E[f̂i]\n∣ ∣ ∣ > rε\n2\n] 6 Var[f̂i]\n(rε/2)2 6\n4fir\nmr2ε2 .\nSetting r = C ε2 for a constant C > 0 makes this probability at most 4fi Cm . By the union bound, if we sample each element in the stream independently with probability r m , then the probability there exists an i for which |f̂i − E[f̂i]| > rε 2 is at most ∑n i=1 4fi Cm 6 4 C , which for C > 400 is at most 1 100 , as desired.\n118\nFor now, assume that the length of the stream is known in advance; we show in sec-\ntion 6.3.5 how to remove this assumption."
    }, {
      "heading" : "6.3.1 List Heavy Hitters Problem",
      "text" : "For the LIST HEAVY HITTERS problem, we present two algorithms. The first is slightly suboptimal, but simple conceptually and already constitutes a very large improvement in the space complexity over known algorithms. We expect that this algorithm could be useful in practice as well. The second algorithm is more complicated, building on ideas from the first algorithm, and achieves the optimal space complexity upto constant factors.\nWe note that both algorithms proceed by sampling O(ε−2 ln(1/δ)) stream items and updating a data structure as the stream progresses. In both cases, the time to update the data structure is bounded by O(1/ε), and so, under the standard assumption that the length of the stream is at least poly(ln(1/δ)ε), the time to perform this update can be spread out across the next O(1/ε) stream updates, since with large probability there will be no items sampled among these next O(1/ε) stream updates. Therefore, we achieve worst-case1 update time of O(1)."
    }, {
      "heading" : "6.3.1.1 A Simpler, Near-Optimal Algorithm",
      "text" : "Theorem 6.1. Assume the stream length is known beforehand. Then there is a randomized onepass algorithm A for the (ε,ϕ)-LIST HEAVY HITTERS problem which succeeds with probability at least 1−δ using O ( ε−1(log ε−1 + log log δ−1) +ϕ−1 logn+ log logm ) bits of space. Moreover, A has an update time of O(1) and reporting time linear in its output size.\nOverview The overall idea is as follows. We sample ℓ = O(ε−2) many items from the stream uniformly at random as well as hash the id’s (the word “id” is short for identifier) of the sampled elements into a space of size O(ε−4). Now, both the stream length as well as the universe size are poly(ε−1). From Lemma 6.3, it suffices to solve the heavy hitters problem on the sampled stream. From Lemma 6.2, because the hash function is chosen from a universal family, the sampled elements have distinct hashed id’s. We can then feed these elements into a standard Misra-Gries data structure with ε−1 counters, incurring space O(ε−1 log ε−1).\n1We emphasize that this is stronger than an amortized guarantee, as on every insertion, the cost will be O(1).\n119\nBecause we want to return the unhashed element id’s for the heavy hitters, we also use logn space for recording the φ−1 top items according to the Misra-Gries data structure and output these when asked to report.\nAlgorithm 5 for (ε,ϕ)-LIST HEAVY HITTERS Input: A stream S of length m over U = [n]; let f(x) be the frequency of x ∈ U in S Output: A set X ⊆ U and a function f̂ : X → N such that if f(x) > ϕm, then x ∈ X and\nf(x) − εm 6 f̂(x) 6 f(x) + εm and if f(y) 6 (φ− ε)m, then y /∈ X for every x,y ∈ U 1: Initialize:\n2: ℓ ← 6 log(6/δ)/ε2 3: Hash function h uniformly at random from a universal family\nH ⊆ {h : [n] → ⌈4ℓ2/δ⌉}. 4: An empty table T1 of (key, value) pairs of length ε −1. Each key entry of\nT1 can store an integer in [0, ⌈400ℓ2/δ⌉] and each value entry can store an integer in [0, 11ℓ]. ⊲ The table T1 will be in sorted order by value throughout.\n5: An empty table T2 of length 1/ϕ. Each entry of T2 can store an integer\nin [0,n]. ⊲ The entries of T2 will correspond to ids of the keys in T1 of\nthe highest 1/ϕ values 6: 7: procedure INSERT(x) 8: With probability p = 6ℓ/m, continue. Otherwise, 9: return .\n10: Perform Misra-Gries update using h(x) maintaining T1 sorted by values. 11: if The value of h(x) is among the highest 1/ϕ valued items in T1 then 12: if xi is not in T2 then 13: if T2 currently contains 1/ϕ many items then 14: For y in T2 such that h(y) is not among the highest 1/ϕ valued items in T1,\nreplace y with x.\n15: else 16: We put x in T2. 17: end if 18: end if 19: Ensure that elements in T2 are ordered according to corresponding values in T1. 20: end if 21: end procedure 22: 23: procedure REPORT( ) 24: 25: return items in T2 along with their corresponding values in T1 26: end procedure\n120\nProof of Theorem 6.1. The pseudocode of our (ε,ϕ)-LIST HEAVY HITTERS algorithm is in Algorithm 5. By Lemma 6.3, if we select a subset S of size at least ℓ = 6ε−2log(6δ−1) uniformly at random from the stream, then Pr[∀i ∈ U, |(f̂i/|S|)− (fi/n)| 6 ε] > 1− δ/3, where fi and f̂i are the frequencies of item i in the input stream and S respectively. First we show that with the choice of p in line 11 in Algorithm 5, the number of items sampled is at least ℓ and at most 11ℓ with probability at least (1 − δ/3). Let Xi be the indicator random variable of the event that the item xi is sampled for i ∈ [m]. Then the total number of items sampled X = ∑m i=1 Xi. We have E[X] = 6ℓ since p = 6ℓ/m. Now we have the following.\nPr[X 6 ℓ or X > 11ℓ] 6 Pr[|X− E[X]| > 5ℓ] 6 δ/3\nThe inequality follows from the Chernoff bound and the value of ℓ. From here onwards we\nassume that the number of items sampled is in [ℓ, 11ℓ].\nWe use (a modified version of) the Misra-Gries algorithm [MG82] to estimate the frequencies of items in S. The length of the table in the Misra-Gries algorithm is ε−1. We pick a hash function h uniformly at random from a universal family H = {h|h : [n] → ⌈4ℓ2/δ⌉} of hash functions of size |H| = O(n2). Note that picking a hash function h uniformly at random from H can be done using O(logn) bits of space. Lemma 6.2 shows that there are no collisions in S under this hash function h with probability at least 1−δ/3. From here onwards we assume that there is no collision among the ids of the sampled items under the hash function h.\nWe modify the Misra-Gries algorithm as follows. Instead of storing the id of any item x in the Misra-Gries table (table T1 in line 5 in Algorithm 5) we only store the hash h(x) of the id x. We also store the ids (not the hash of the id) of the items with highest 1/ϕ values in T1 in another table T2. Moreover, we always maintain the table T2 consistent with the table T1 in the sense that the ith highest valued key in T1 is the hash of the i th id in T2.\nUpon picking an item x with probability p, we create an entry corresponding to h(x) in T1 and make its value one if there is space available in T1; decrement the value of every item in T1 by one if the table is already full; increment the entry in the table corresponding to h(x) if h(x) is already present in the table. When we decrement the value of every item in T1, the table T2 remains consistent and we do not need to do anything else. Otherwise there are three cases to consider. Case 1: h(x) is not among the 1/ϕ highest valued items in T1. In this case, we do not need to do anything else. Case 2: h(x) was not among the 1/ϕ highest valued items in T1 but now it is among the 1/ϕ highest valued items in T1. In this case the last item y in T2 is no longer among the 1/ϕ highest valued items in T1. We replace y with x in T2. Case\n121\n3: h(x) was among the 1/ϕ highest valued items in T1. When the stream finishes, we output the ids of all the items in table T2 along with the values corresponding to them in table T1. Correctness follows from the correctness of the Misra-Gries algorithm and the fact that there is no collision among the ids of the sampled items."
    }, {
      "heading" : "6.3.1.2 An Optimal Algorithm",
      "text" : "Theorem 6.2. Assume the stream length is known beforehand. Then there is a randomized one-pass algorithm A for the (ε,ϕ)-LIST HEAVY HITTERS problem which succeeds with constant probability using O ( ε−1 logφ−1 + φ−1 logn+ log logm ) bits of space. Moreover, A has an update time of O(1) and reporting time linear in its output size.\nNote that in this section, for the sake of simplicity, we ignore floors and ceilings and state\nthe results for a constant error probability, omitting the explicit dependence on δ.\nOverview As in the simpler algorithm, we sample ℓ = O(ε−2) many stream elements and solve the (ε/2,φ)-LIST HEAVY HITTERS problem on this sampled stream. Also, the MisraGries algorithm for (φ/2,φ)-LIST HEAVY HITTERS returns a candidate set of O(φ−1) items containing all items of frequency at least φℓ. It remains to count the frequencies of these O(φ−1) items with upto εℓ/2 = O(ε−1) additive error, so that we can remove those whose frequency is less than (φ− ε/2)ℓ.\nFix some item i ∈ [n], and let fi be i’s count in the sampled stream. A natural approach to count fi approximately is to increment a counter probabilistically, instead of deterministically, at every occurrence of i. Suppose that we increment a counter with probability 0 6 pi 6 1 whenever item i arrives in the stream. Let the value of the counter be ĉi, and let f̂i = ĉi/pi. We see that E [ f̂i ] = fi and Var[f̂i] 6 fi/pi. It follows that if pi = Θ(ε 2fi), then Var[f̂i] = O(ε −2), and hence, f̂i is an unbiased estimator of fi with additive error O(ε −1) with constant probability. We call such a counter an accelerated counter as the probability of incrementing accelerates with increasing counts. For each i, we can maintain O(logφ−1) accelerated counters independently and take their median to drive the probability of deviating by more than O(ε−1) down to O(φ). So, with constant probability, the frequency for each of the O(φ−1) items in the Misra-Gries data structure is estimated within O(ε−1) error, as desired.\nHowever, there are two immediate issues with this approach. The first problem is that we may need to keep counts for Ω(ℓ) = Ω(ε−2) distinct items, which is too costly for our\n122\nAlgorithm 6 for (ε,ϕ)-LIST HEAVY HITTERS Input: A stream S of length m over universe U = [n]; let f(x) be the frequency of x ∈ U in S Output: A set X ⊆ U and a function f̂ : X → N such that if f(x) > ϕm, then x ∈ X and\nf(x) − εm 6 f̂(x) 6 f(x) + εm and if f(y) 6 (φ− ε)m, then y /∈ X for every x,y ∈ U 1: Initialize:\n2: ℓ ← 105ε−2 3: s ← 0 4: Hash functions h1, . . . ,h200 log(12φ−1) uniformly at random from a\nuniversal family H ⊆ {h : [n] → [100/ε]}. 5: An empty table T1 of (key, value) pairs of length 2φ −1. Each key entry\nof T1 can store an element of [n] and each value entry can store an integer in [0, 10ℓ]. 6: An empty table T2 with 100ε −1 rows and 200 log(12φ−1) columns.\nEach entry of T2 can store an integer in [0, 100εℓ]. 7: An empty 3-dimensional table T3 of size at most\n100ε−1 × 200 log(12φ−1)× 4 log(ε−1). Each entry of T3 can store an integer in [0, 10ℓ]. ⊲ These are upper bounds; not all the allowed cells will actually be used.\n8:\n9: procedure INSERT(x) 10: With probability ℓ/m, increment s and continue. Else, 11: return 12: Perform Misra-Gries update on T1 with x. 13: for j ← 1 to 200 log(12φ−1) do 14: i ← hj(x) 15: With probability ε, increment T2[i, j] 16: t ← ⌊log(10−6T2[i, j]2)⌋ and p ← min(ε · 2t, 1) 17: if t > 0 then 18: With probability p, increment T3[i, j, t] 19: end if 20: end for 21: end procedure 22: 23: procedure REPORT( ) 24: X ← ∅ 25: for each key x with nonzero value in T1 do 26: for j ← 1 to 200 log(12φ−1) do 27: f̂j(x) ← ∑4 log(ε−1) t=0 T3[h(x), j, t]/min(ε2\nt, 1) 28: end for 29: f̂(x) ← median(f̂1, . . . , f̂10 logφ−1) 30: if f̂(x) > (φ− ε/2)s then 31: X ← X ∪ {x} 32: end if 33: end for 34: 35: return X, f̂ 36: end procedure 123\npurposes. To get around this, we use a hash function from a universal family to hash the universe to a space of size u = Θ(ε−1), and we work throughout with the hashed id’s. We can then show that the space complexity for each iteration is O(ε−1). Also, the accelerated counters now estimate frequencies of hashed id’s instead of actual items, but because of universality, the expected frequency of any hashed id is ℓ/u = O(ε−1), our desired error bound.\nThe second issue is that we need a constant factor approximation of fi, so that we can set pi to Θ(ε 2fi). But because the algorithm needs to be one-pass, we cannot first compute pi in one pass and then run the accelerated counter in another. So, we divide the stream into epochs in which fi stays within a factor of 2, and use a different pi for each epoch. In particular, set pti = ε · 2t for 0 6 t 6 log(pi/ε). We want to keep a running estimate of i’s count to within a factor of 2 to know if the current epoch should be incremented. For this, we subsample each element of the stream with probability ε independently and maintain exact counts for the observed hashed id’s. It is easy to see that this requires only O(ε−1) bits in expectation. Consider any i ∈ [u] and the prefix of the stream upto b 6 ℓ, and let fi(b) be i’s frequency in the prefix, let c̄i(b) be i’s frequency among the samples in the prefix, and f̄i(b) = c̄i(b)\nε . We see that E\n[ f̄i(b) ] = fi(b), and Var[f̄i(b)] 6 fi(b)\nε . By Chebyshev, for any\nfixed b, Pr[|f̄i(b) − fi(b)| > fi(b)/ √ 2] 6 2\nfi(b)ε , and hence, can show that f̄i(b) is a\n√ 2-factor\napproximation of fi(b) with probability 1 −O((fi(b)ε) −1). Now, let pi(b) = Θ(ε 2fi(b)), and for any epoch t, set bi,t = min{b : pi(b) > p t−1 i }. The last makes sense because pi(b) is non-decreasing with b. Also, note that fi(bi,t) = Ω(2 t/2/ε). So, by the union bound, the probability that there exists t for which f̄i(bi,t) is not a √ 2-factor approximation of fi(bi,t) is at most ∑\nt 1 Ω(fi(bi,t)ε) = ∑ t 1 Ω(2t/2) , a small constant. In fact, it follows then that with\nconstant probability, for all b ∈ [ℓ], f̄i(b) is a 2-factor approximation of fi(b). Moreover, we show that for any b ∈ [ℓ], f̄i(b) is a 4-factor approximation of fi(b) with constant probability. By repeating O(logφ−1) times independently and taking the median, the error probability can be driven down to O(φ).\nNow, for every hashed id i ∈ [u], we need not one accelerated counter but O(log(εfi)) many, one corresponding to each epoch t. When an element with hash id i arrives at position b, we decide, based on f̄i(b), the epoch t it belongs to and then increment the t’th accelerated counter with probability pti . The storage cost over all i is still O(1/ε). Also, we iterate the whole set of accelerated counters O(logφ−1) times, making the total storage cost O(ε−1 logφ−1).\n124\nLet ĉi,t be the count in the accelerated counter for hash id i and epoch t. Then, let\nf̂i = ∑ t ĉi,t/p t i. Clearly, E [ f̂i ] = fi. The variance is O(ε −2) in each epoch, and so, Var[f̂i] = O(ε−2 log ε−1), not O(ε−2) which we wanted. This issue is fixed by a change in how the sampling probabilities are defined. We now go on to the formal proof.\nProof of Theorem 6.2. Pseudocode appears in algorithm 6. Note that the numerical constants are chosen for convenience of analysis and have not been optimized. Also, for the sake of simplicity, the pseudocode does not have the optimal reporting time, but it can be modified to achieve this; see the end of this proof for details.\nBy standard Chernoff bounds, with probability at least 99/100, the length of the sampled stream ℓ/10 6 s 6 10ℓ. For x ∈ [n], let fsamp(x) be the frequency of x in the sampled stream. By Lemma 6.3, with probability at least 9/10, for all x ∈ [n]:\n∣ ∣ ∣ ∣ fsamp(x)\ns −\nf(x)\nm\n∣ ∣ ∣ ∣ 6 ε\n4\nNow, fix j ∈ [10 logφ−1] and x ∈ [n]. Let i = hj(x) and fi = ∑\ny:hj(y)=hj(x) fsamp(y). Then, for\na random hj ∈ H, the expected value of fis − fsamp(x) s is ε 100 , since H is a universal mapping to a space of size 100ε−1. Hence, using Markov’s inequality and the above:\nPr\n[ ∣\n∣ ∣ ∣\nf(x)\nm −\nfi s\n∣ ∣ ∣ ∣ > ε\n2\n]\n6 Pr\n[ ∣\n∣ ∣ ∣\nf(x)\nm −\nfsamp\ns\n∣ ∣ ∣ ∣ > ε\n4\n]\n+ Pr\n[ ∣\n∣ ∣ ∣\nfsamp(x)\nm −\nfi s\n∣ ∣ ∣ ∣ > ε\n4\n]\n< 1\n10 +\n1\n25 <\n3\n20 (6.1)\nIn Lemma 6.4 below, we show that for each j ∈ [200 log(12φ−1)], with error probability at most 3/10, f̂j(x) (in line 27) estimates fi with additive error at most 5000ε −1, hence estimating fi s with additive error at most ε 2 . Taking the median over 200 log(12φ−1) repetitions (line 29) makes the error probability go down to φ 6 using standard Chernoff bounds. Hence, by the union bound, with probability at least 2/3, for each of the 2/φ keys x with nonzero values in T1, we have an estimate of f(x)\nm within additive error ε, thus showing correctness.\nLemma 6.4. Fix x ∈ [n] and j ∈ [200 log 12φ−1], and let i = hj(x). Then, Pr[|f̂j(x) − fi| > 5000ε−1] 6 3/10, where f̂j is the quantity computed in line 27.\nProof. Index the sampled stream elements 1, 2, . . . , s, and for b ∈ [s], let fi(b) be the frequency of items with hash id i restricted to the first b elements of the sampled stream. Let f̄i(b) denote the value of T2[i, j] · ε−1 after the procedure INSERT has been called for the first b items of the sampled stream.\n125\nClaim 6.1. With probability at least 9/10, for all b ∈ [s] such that fi(b) > 100ε−1, f̄i(b) is within a factor of 4 of fi(b).\nProof. Fix b ∈ [s]. Note that E [ f̄i(b) ] = fi(b) as T2 is incremented with rate ε. Var[f̄i(b)] 6 fi/ε, and so by Chebyshev’s inequality:\nPr[|f̄i(b) − fi(b)| > fi(b)/2] < 4\nfi(b)ε\nWe now break the stream into chunks, apply this inequality to each chunk and then take a union bound to conclude. Namely, for any integer t > 0, define bt to be the first b such that 100ε−12t 6 fi(b) < 100ε −12t+1 if such a b exists. Then:\nPr[∃t > 0 : |f̄i(bt) − fi(bt)| > fi(bt)/2] < ∑\nt\n4\n100 · 2t−1\n< 1\n10\nSo, with probability at least 9/10, every f̄i(bt) and fi(bt) are within a factor of 2 of each other. Since for every b > b0, fi(b) is within a factor of 2 from some fi(bt), the claim follows.\nAssume the event in Claim 6.1 henceforth. Now, we are ready to analyze T3 and in particular, f̂j(x). First of all, observe that if t < 0 in line 16, at some position b in the stream, then T2[i, j] at that time must be at most 1000, and so by standard Markov and Chernoff bounds, with probability at least 0.85,\nfi(b)\n \n\n< 4000ε−1, if t < 0 > 100ε−1, if t > 0 (6.2)\nAssume this event. Then, fi − 4000ε −1 6 E\n[ f̂j(x) ] 6 fi.\nClaim 6.2.\nVar(f̂j(x)) 6 20000ε −2\nProof. If the stream element at position b causes an increment in T3 with probability ε2 t (in line 18), then 1000 · 2t/2 6 T2[i, j] 6 1000 · 2(t+1)/2, and so, f̄i(b) 6 1000ε−12(t+1)/2. This must be the case for the highest b = b̄t at which the count for i in T3 increments at the t’th\n126\nslot. The number of such occurrences of i is at most fi(b̄t) 6 4f̄i(b̄t) 6 4000ε −12(t+1)/2 by Claim 6.1 (which can be applied since fi(b) > 100ε −1 by Equation 6.2). So:\nVar[f̂j(x)] 6 ∑\nt>0\nfi(b̄t)\nε2t 6\n∑\nt>0\n4000\nε2 2−t/3 6 20000ε−2\nElements inserted with probability 1 obviously do not contribute to the variance.\nSo, conditioning on the events mentioned, the probability that f̂j(x) deviates from fi by\nmore than 5000ε−1 is at most 1/50. Removing all the conditioning yields what we wanted:\nPr[|f̂j(x) − fi| > 5000ε −1] 6\n1\n50 +\n3\n20 +\n1\n10 6 0.3\nWe next bound the space complexity.\nClaim 6.3. With probability at least 2/3, algorithm 6 uses O(ε−1 logφ−1 + φ−1 logn + log logm) bits of storage, if n = ω(ε−1).\nProof. The expected length of the sampled stream is ℓ = O(ε−2). So, the number of bits stored in T1 is O(φ −1 logn). For T2, note that in lines 13-15, for any given j, T2 is storing a total of εℓ = O(ε−1) elements in expectation. So, for k > 0, there can be at most O((ε2k)−1) hashed id’s with counts between 2k and 2k+1. Summing over all k’s and accounting for the empty cells gives O(ε−1) bits of storage, and so the total space requirement of T2 is O(ε −1 logφ−1). .\nThe probability that a hashed id i gets counted in table T3 is at most 10 −6ε3f̄2i(s) from line 16 and our definition of f̄i above. Moreover, from Claim 6.1, we have that this is at most 16 · 10−6ε3f2i(s) if fi > 100ε−1. Therefore, if fi = 2k · 100ε−1 with k > 0, then the expected value of a cell in T3 with first coordinate i is at most 1600 · 22kε = 2O(k). Taking into account that there are at mostO((ε2k)−1)many such id’s i and that the number of epochs t associated with such an i is at most log(16 · 10−6ε2f2i) = O(log(εfi)) = O(k) (from line 16), we get that\n127\nthe total space required for T3 is:\nO(logφ−1)∑\nj=1\n(\nO(ε−1) +\n∞∑\nk=0\nO((ε2k)−1) ·O(k) ·O(k) )\n= O(ε−1 logφ−1)\nwhere the first O(ε−1) term inside the summation is for the i’s with fi < 100ε −1. Since we have an expected space bound, we obtain a worst-case space bound with error probability 1/3 by a Markov bound.\nThe space required for sampling is an additional O(log logm), using Lemma 6.1.\nWe note that the space bound can be made worst case by aborting the algorithm if it tries\nto use more space.\nThe only remaining aspect of Theorem 6.2 is the time complexity. As observed in section 6.3.1, the update time can be made O(1) per insertion under the standard assumption of the stream being sufficiently long. The reporting time can also be made linear in the output by changing the bookkeeping a bit. Instead of computing f̂j and f̂ at reporting time, we can maintain them after every insertion. Although this apparently makes INSERT costlier, this is not true in fact because we can spread the cost over future stream insertions. The space complexity grows by a constant factor."
    }, {
      "heading" : "6.3.2 ε-Maximum Problem",
      "text" : "By tweaking Algorithm 5 slightly, we get the following result for the ε-MAXIMUM problem.\nTheorem 6.3. Assume the length of the stream is known beforehand. Then there is a randomized one-pass algorithm A for the ε-MAXIMUM problem which succeeds with probability at least 1 − δ using O (min{1/ε,n}(log 1/ε + log log 1/δ) + logn+ log logm) bits of space. Moreover, the algorithm A has an update time of O(1).\nProof. Instead of maintaining the table T2 in Algorithm 5, we just store the actual id of the item with maximum frequency in the sampled items.\n128"
    }, {
      "heading" : "6.3.3 ε-Minimum Problem",
      "text" : "Theorem 6.4. Assume the length of the stream is known beforehand. Then there is a randomized one-pass algorithm A for the ε-MINIMUM problem which succeeds with probability at least 1− δ using O ((1/ε) log log(1/εδ) + log logm) bits of space. Moreover, the algorithm A has an update time of O(1).\nOverview Pseudocode is provided in algorithm 7. The idea behind our ε-Minimum problem is as follows. It is most easily explained by looking at the REPORT(x) procedure starting in line 13. In lines 14-15 we ask, is the universe size |U| significantly larger than 1/ε? Note that if it is, then outputting a random item from |U| is likely to be a solution. Otherwise |U| is O(1/ε).\nThe next point is that if the number of distinct elements in the stream were smaller than 1/(ε log(1/ε)), then we could just store all the items together with their frequencies with O(1/ε) bits of space. Indeed, we can first sample O(1/ε2) stream elements so that all relative frequencies are preserved up to additive ε, thereby ensuring each frequency can be stored with O(log(1/ε) bits. Also, since the universe size is O(1/ε), the item identifiers can also be stored with O(log(1/ε) bits. So if this part of the algorithm starts taking up too much space, we stop, and we know the number of distinct elements is at least 1/(ε log(1/ε)), which means that the minimum frequency is at most O(mε log(1/ε)). This is what is being implemented in steps 9-10 and 18-19 in the algorithm.\nWe can also ensure the minimum frequency is at least Ω(mε/ log(1/ε)). Indeed, by randomly sampling O((log(1/ε)/ε) stream elements, and maintaining a bit vector for whether or not each item in the universe occurs - which we can with O(1/ε) bits of space since |U| = O(1/ε) - any item with frequency at least Ω(εm/ log(1/ε)) will be sampled and so if there is an entry in the bit vector which is empty, then we can just output that as our solution. This is what is being implemented in steps 8 and 16-17 of the algorithm.\nFinally, we now know that the minimum frequency is at least Ω(mε/ log(1/ε)) and at most O(mε log(1/ε)). At this point if we randomly sample O((log6 1/ε)/ε) stream elements, then by Chernoff bounds all item frequencies are preserved up to a relative error factor of (1 ± 1/ log2(1/ε)), and in particular the relative minimum frequency is guaranteed to be preserved up to an additive ε. At this point we just maintain the exact counts in the sampled stream but truncate them once they exceed poly(log(1/ε))) bits, since we know such counts\n129\ndo not correspond to the minimum. Thus we only needO(log log(1/ε)) bits to represent their counts. This is implemented in step 11 and step 20 of the algorithm.\nAlgorithm 7 for ε-MINIMUM Input: A stream S = (xi)i∈[m] ∈ Um of length m over U; let f(x) be the frequency of x ∈ U in S Output: An item x ∈ U such that f(x) 6 f(y) + εm for every y ∈ U 1: Initialize:\n2: ℓ1 ← log(6/εδ)/ε, ℓ2 ← log(6/δ)/ε2, ℓ3 ← log6(6/δε)/ε 3: p1 ← 6ℓ1/m, p2 ← 6ℓ2/m, p3 ← 6ℓ3/m 4: S1, S2, S3 ← ∅ 5: B1 ← the bit vector for S1\n6: 7: procedure INSERT(x) 8: Put x in S1 with probability p1 by updating the bit vector B1 9: if the number of distinct items in the stream so far is at most 1/(ε log(1/ε)) then\n10: Pick x with probability p2 and put the id of x in S2 and initialize the corresponding counter to 1 if x /∈ S2 and increment the counter corresponding to x by 1. 11: end if 12: Pick xwith probability p3, put the id of x in S3 and initialize the corresponding counter\nto 1 if xi /∈ S3 and increment the counter corresponding to xi by 1. Truncate counters of S3 at 2 log\n7(2/εδ). 13: end procedure 14: 15: procedure REPORT( ) 16: if |U| > 1/((1−δ)ε) then 17: 18: return an item x from the first 1/((1−δ)ε) items in U (ordered arbitrarily) uniformly at random 19: end if 20: if S1 6= U then 21: 22: return any item from U \\ S1 23: end if 24: if the number of distinct items in the stream is at most 1/(ε log(1/ε)) then 25: 26: return an item in S2 with minimum counter value in S2 27: end if 28: 29: return the item with minimum frequency in S3 30: end procedure\n130\nProof of Theorem 6.4. The pseudocode of our ε-MINIMUM algorithm is in Algorithm 7. If the size of the universe |U| is at least 1/((1−δ)ε), then we return an item x chosen from U uniformly at random. Note that there can be at most 1/ε many items with frequency at least εm. Hence every item x among other remaining δ/((1−δ)ε) many items has frequency less than εm and thus is a correct output of the instance. Thus the probability that we answer correctly is at least (1 − δ). From here on, let us assume |U| < 1/((1−δ)ε).\nNow, by the value of pj, it follows from the proof of Theorem 6.1 that we can assume ℓj < |Sj| < 11ℓj for j = 1, 2, 3 which happens with probability at least (1 − (δ/3)). We first show that every item in U with frequency at least εm is sampled in S1 with probability at least (1 − (δ/6)). For that, let Xji be the indicator random variable for the event that the j th sample in S1 is item i where i ∈ U is an item with frequency at least εm. Let H ⊂ U be the set of items with frequencies at least εm. Then we have the following.\nPr[Xji = 0] = 1− ε ⇒ Pr[Xji = 0 ∀j ∈ S1] 6 (1 − ε)ℓ1 6 exp{−εℓ1} = εδ/6\nNow applying union bound we get the following.\nPr[∃i ∈ H,Xji = 0 ∀j ∈ S1] 6 (1/ε)εδ/6 6 δ/6\nHence with probability at least (1 − (δ/3) − (δ/6)) > (1 − δ), the output at line 22 is correct. Now we show below that if the frequency of any item x ∈ U is at most ε ln(6/δ)/ln(6/εδ), then x ∈ S1 with probability at least (1 − (δ/6)).\nPr[x /∈ S1] = (1 − ε ln(6/δ)/ln(6/εδ))ln( 6/εδ)/ε 6 δ/6\nHence from here onwards we assume that the frequency of every item in U is at least εm ln(6/δ)/ln(6/εδ).\nIf the number of distinct elements is at most 1/(ε ln(1/ε)), then line 26 outputs the minimum frequency item up to an additive factor of εm due to Chernoff bound. Note that we need only O(ln(1/((1−δ)ε))) bits of space for storing ids. Hence S2 can be stored in space O((1/ε ln(1/ε)) ln(1/((1−δ)ε) ln ln(1/δ)) = O(1/ε ln ln(1/δ)).\nNow we can assume that the number of distinct elements is at least 1/(ε ln(1/ε)). Hence if f(t) is the frequency of the item t with minimum frequency, then we have mε/ln(1/ε) 6 f(t) 6 mε ln(1/ε).\n131\nLet fi be the frequency of item i ∈ U, ei be the counter value of i in S3, and f̂i = eim/ℓ3. Now again by applying Chernoff bound we have the following for any fixed i ∈ U.\nPr[|fi − f̂i| > fi/ln2(1/ε)] 6 2 exp{−ℓ3fi/(m ln4(1/ε))}\n6 2 exp{−fi ln 2(6/εδ)/(εm)}\n6 εδ/6.\nNow applying union bound we get the following using the fact that |U| 6 1/ε(1−δ).\nPr[∀i ∈ U, |fi − f̂i| 6 fi/ln2(1/ε)] > 1− δ/6\nAgain by applying Chernoff bound and union bound we get the following.\nPr[∀i ∈ U with fi > 2mε ln(1/ε), |fi − f̂i| 6 fi/2] > 1− δ/6\nHence the items with frequency more than 2mε ln(1/ε) are approximated up to a multiplicative factor of 1/2 from below in S3. The counters of these items may be truncated. The other items with frequency at most 2mε ln(1/ε) are be approximated up to (1 ± 1/ln2(1/ε)) relative error and thus up to an additive error of εm/3. The counters of these items would not get truncated. Hence the item with minimum counter value in S3 is the item with minimum frequency up to an additive εm.\nWe need O(ln(1/εδ)) bits of space for the bit vector B1 for the set S1. We need O(ln 2(1/εδ)) bits of space for the set S2 and O((1/ε) ln ln(1/εδ)) bits of space for the set S3 (by the choice of truncation threshold). We need an additional O (ln lnm) bits of space for sampling using Lemma 6.1. Moreover, using the data structure of Section 3.3 of [DLOM02] Algorithm 7 can be performed in O(1) time. Alternatively, we may also use the strategy described in section 6.3.1 of spreading update operations over several insertions to make the cost per insertion be O(1)."
    }, {
      "heading" : "6.3.4 Problems for the Borda and Maximin Voting Rules",
      "text" : "Theorem 6.5. Assume the length of the stream is known beforehand. Then there is a randomized one-pass algorithm A for (ε,ϕ)-LIST BORDA problem which succeeds with probability at least 1 − δ using O ( n (\nlogn + log 1 ε + log log 1 δ\n) + log logm ) bits of space.\n132\nProof. Let ℓ = 6ε−2 log(6nδ−1) and p = 6ℓ/m. On each insertion of a vote v, select v with probability p and store for every i ∈ [n], the number of candidates that candidate i beats in the vote v. Keep these exact counts in a counter of length n.\nThen it follows from the proof of Theorem 6.1 that ℓ 6 |S| 6 11ℓ with probability at least (1− δ/3). Moreover, from a straightforward application of the Chernoff bound (see [DB15]), it follows that if ŝ(i) denotes the Borda score of candidate i restricted to the sampled votes, then:\nPr\n[ ∀i ∈ [n], ∣ ∣ ∣\n∣\nm |S| ŝ(i) − s(i)\n∣ ∣ ∣ ∣ < εmn ] > 1 − δ\nThe space complexity for exactly storing the counts isO(n log(nℓ)) = O(n(logn+log ε−1+\nlog log δ−1)) and the space for sampling the votes is O(log logm) by Lemma 6.1.\nTheorem 6.6. Assume the length of the stream is known beforehand. Then there is a randomized one-pass algorithm A for (ε,ϕ)-LIST MAXIMIN problem which succeeds with probability at least 1 − δ using O ( nε−2 log 2 n + nε−2 logn log δ−1 + log logm ) bits of space.\nProof. Let ℓ = (8/ε2) ln(6n/δ) and p = 6ℓ/m. We put the current vote in a set S with probability p. Then it follows from the proof of Theorem 6.1 that ℓ 6 |S| 6 11ℓ with probability at least (1 − δ/3). Suppose |S| = ℓ1; let S = {vi : i ∈ [ℓ1]} be the set of votes sampled. Let DE(x,y) be the total number of votes in which x beats y and DS(x,y)) be the number of such votes in S. Then by the choice of ℓ and the Chernoff bound (see [DB15]), it follows that |DS(x,y)m/ℓ1 − DE(x,y)| 6 εm/2 for every pair of candidates x,y ∈ U. Note that each vote can be stored in O(n logn) bits of space. Hence simply finding DS(x,y) for every x,y ∈ U by storing S and returning all the items with maximin score at least (φ− ε/2)ℓ1 in S requires O ( nε−2 logn(logn+ log δ−1) + log logm ) bits of memory, with the additive O(log logm) due to Lemma 6.1."
    }, {
      "heading" : "6.3.5 Algorithms with Unknown Stream Length",
      "text" : "Now we consider the case when the length of the stream is not known beforehand. We present below an algorithm for (ε,ϕ)-LIST HEAVY HITTERS and ε-MAXIMUM problems in the setting where the length of the stream is not known beforehand.\nTheorem 6.7. There is a randomized one-pass algorithm for (ε,ϕ)-LIST HEAVY HITTERS and ε-MAXIMUM problems with space complexity O ( ε−1 log ε−1 +ϕ−1 logn + log logm ) bits and update time O(1) even when the length of the stream is not known beforehand.\n133\nProof. We describe below a randomized one-pass algorithm for the (8ε,ϕ)-LIST HEAVY HITTERS problem. We may assume that the length of the stream is at least 1/ε2; otherwise, we use the algorithm in Theorem 6.1 and get the result. Now we guess the length of the stream to be 1/ε2, but run an instance I1 of Algorithm 5 with ℓ = log( 6/δ)/ε3 at line 2. By the choice of the size of the sample (which is Θ(log(1/δ)/ε3)), I1 outputs correctly with probability at least (1 − δ), if the length of the stream is in [1/ε2, 1/ε3]. If the length of the stream exceeds 1/ε2, we run another instance I2 of Algorithm 5 with ℓ = log( 6/δ)/ε3 at line 2. Again by the choice of the size of the sample, I2 outputs correctly with probability at least (1 − δ), if the length of the stream is in [1/ε3, 1/ε4]. If the stream length exceeds 1/ε3, we discard I1, free the space it uses, and run an instance I3 of Algorithm 5 with ℓ = log( 6/δ)/ε3 at line 2 and so on. At any point of time, we have at most two instances of Algorithm 5 running. When the stream ends, we return the output of the older of the instances we are currently running. We use the approximate counting method of Morris [Mor78] to approximately count the length of the stream. We know that the Morris counter outputs correctly with probability (1 − 2−k/2) using O(log logm + k) bits of space at any point in time [Fla85]. Also, since the Morris counter increases only when an item is read, it outputs correctly up to a factor of four at every position if it outputs correctly at positions 1, 2, 4, . . . , 2⌊log2m⌋; call this event E. Then we have Pr(E) > 1 − δ by choosing k = 2 log2(log2 m/δ) and applying union bound over the positions 1, 2, 4, . . . , 2⌊log2 m⌋. The correctness of the algorithm follows from the correctness of Algorithm 5 and the fact that we are discarding at most εm many items in the stream (by discarding a run of an instance of Algorithm 5). The space complexity and the O(1) update time of the algorithm follow from Theorem 6.1, the choice of k above, and the fact that we have at most two instances of Algorithm 5 currently running at any point of time.\nThe algorithm for the ε-MAXIMUM problem is same as the algorithm above except we use\nthe algorithm in Theorem 6.3 instead of Algorithm 5.\nNote that this proof technique does not seem to apply to our optimal algorithm 6. Similarly to Theorem 6.7, we get the following result for the ε-MINIMUM, (ε,φ)-BORDA, and (ε,φ)-MAXIMIN problems.\nTheorem 6.8. There are randomized one-pass algorithms for ε-MINIMUM, (ε,φ)-BORDA, and (ε,φ)-MAXIMIN problems with space complexity O ((1/ε) log log(1/εδ) + log logm), O ( n (\nlogn + log 1 ε + log log 1 δ\n) + log logm ) , and\nO ( nε−2 log 2 n+ nε−2 logn log(1/δ) + log logm ) bits respectively even when the length of the stream is not known beforehand. Moreover, the update time for ε-MINIMUM is O(1).\n134"
    }, {
      "heading" : "6.4 Results on Space Complexity Lower Bounds",
      "text" : "In this section, we prove space complexity lower bounds for the ε-HEAVY HITTERS, εMINIMUM, ε-BORDA, and ε-MAXIMIN problems. We present reductions from certain communication problems for proving space complexity lower bounds. Let us first introduce those communication problems with necessary results."
    }, {
      "heading" : "6.4.1 Communication Complexity",
      "text" : "Definition 6.9. (INDEXINGm,t) Let t and m be positive integers. Alice is given a string x = (x1, · · · , xt) ∈ [m]t. Bob is given an index i ∈ [t]. Bob has to output xi.\nThe following is a well known result [KN97].\nLemma 6.5. R 1-way δ (INDEXINGm,t) = Ω(t logm) for constant δ ∈ (0, 1).\nDefinition 6.10. (AUGMENTED-INDEXINGm,t) Let t and m be positive integers. Alice is given a string x = (x1, · · · , xt) ∈ [m]t. Bob is given an integer i ∈ [t] and (x1, · · · , xi−1). Bob has to output xi.\nThe following communication complexity lower bound result is due to [EJS10] by a simple\nextension of the arguments of Bar-Yossef et al [BYJKS02].\nLemma 6.6. R 1-way δ (AUGMENTED-INDEXINGm,t) = Ω((1− δ)t logm) for any δ < 1 − 3 2m .\n[SW15] defines a communication problem called PERM, which we generalize to ε-PERM\nas follows.\nDefinition 6.11. (ε-PERM) Alice is given a permutation σ over [n] which is partitioned into 1/ε many contiguous blocks. Bob is given an index i ∈ [n] and has to output the block in σ where i belongs.\nOur lower bound for ε-PERM matches the lower bound for PERM in Lemma 1 in [SW15] when ε = 1/n. For the proof, the reader may find useful some information theory facts described in Appendix A.\nLemma 6.7. R 1-way δ (ε− PERM) = Ω(n log( 1/ε)), for any constant δ < 1/10.\n135\nProof. Let us assume σ, the permutation Alice has, is uniformly distributed over the set of all permutations. Let τj denotes the block the item j is in for j ∈ [n], τ = (τ1, . . . , τn), and τ<j = (τ1, . . . , τj−1). Let M(τ) be Alice’s message to Bob, which is a random variable depending on the randomness of σ and the private coin tosses of Alice. Then we have R1−way(ε− PERM) > H(M(τ)) > I(M(τ); τ). Hence it is enough to lower bound I(M(τ); τ). Then we have the following by chain rule.\nI(M(τ); τ) =\nn∑\nj=1\nI(M(τ); τj|τ<j)\n=\nn∑\nj=1\nH(τj|τ<j) −H(τj|M(τ), τ<j)\n>\nn∑\nj=1\nH(τj|τ<j) −\nn∑\nj=1\nH(τj|M(τ))\n= H(τ) −\nn∑\nj=1\nH(τj|M(τ))\nThe number of ways to partition n items into 1/ε blocks is n!/((εn)!)(1/ε) which isΩ((n/e)n/(εn/e)n). Hence we have H(τ) = n log(1/ε). Now we consider H(τj|M(τ)). By the correctness of the algorithm, Fano’s inequality, we haveH(τj|M(τ)) 6 H(δ)+(1/10) log2((1/ε)−1) 6 (1/2) log(1/ε). Hence we have the following.\nI(M(τ); τ) > (n/2) log(1/ε)\nFinally, we consider the GREATER-THAN problem.\nDefinition 6.12. (GREATER-THANn) Alice is given an integer x ∈ [n] and Bob is given an integer y ∈ [n],y 6= x. Bob has to output 1 if x > y and 0 otherwise.\nThe following result is due to [Smi88, MNSW98]. We provide a simple proof of it that\nseems to be missing1 in the literature.\nLemma 6.8. R 1-way δ (GREATER-THANn) = Ω(logn), for every δ < 1/4.\n1A similar proof appears in [KNR99] but theirs gives a weaker lower bound.\n136\nProof. We reduce the AUGMENTED-INDEXING2,⌈log n⌉+1 problem to the GREATER-THANn problem thereby proving the result. Alice runs the GREATER-THANn protocol with its input number whose representation in binary is a = (x1x2 · · ·x⌈logn⌉1)2. Bob participates in the GREATER-THANn protocol with its input number whose representation in binary is b = (x1x2 · · ·xi−11 0 · · ·0︸ ︷︷ ︸ (⌈logn⌉−i+1) 0 ′s )2. Now xi = 1 if and only if a > b."
    }, {
      "heading" : "6.4.2 Reductions from Problems in Communication Complexity",
      "text" : "We observe that a trivial Ω((1/ϕ) logn) bits lower bound for (ε,ϕ)-LIST HEAVY HITTERS, (ε,ϕ)-LIST BORDA, (ε,ϕ)-LIST MAXIMIN follows from the fact that any algorithm may need to output 1/φ many items from the universe. Also, there is a trivial Ω(n logn) lower bound for (ε,ϕ)-LIST BORDA and (ε,ϕ)-LIST MAXIMIN because each stream item is a permutation on [n], hence requiring Ω(n logn) bits to read.\nWe show now a space complexity lower bound ofΩ( 1 ε log 1 φ ) bits for the ε-HEAVY HITTERS\nproblem.\nTheorem 6.9. Suppose the size of universe n is at least 1/(εφµ) for any constant µ > 0 and that φ > 2ε. Any randomized one pass (ε,φ)-HEAVY HITTERS algorithm with success probability at least (1− δ) must use Ω((1/ε) log 1/φ) bits of space, for constant δ ∈ (0, 1).\nProof. Let µ > 0 be any constant. Without loss of generality, we can assume µ 6 1. We will show that, when n > 1/(εφµ), any ε-HEAVY HITTERS algorithm must use Ω((1/ε) log 1/φ) bits of memory, thereby proving the result. Consider the INDEXING1/φµ,1/ε problem where Alice is given a string x = (x1, x2, · · · , x1/ε) ∈ [1/φµ]1/ε and Bob is given an index i ∈ [1/ε]. The stream we generate is over [1/φµ] × [1/ε] ⊆ U (this is possible since |U| > 1/(εφµ)). Alice generates a stream of length m/2 in such a way that the frequency of every item in {(xj, j) : j ∈ [1/ε]} is at least ⌊εm/2⌋ and the frequency of any other item is 0. Alice now sends the memory content of the algorithm to Bob. Bob resumes the run of the algorithm by generating another stream of length m/2 in such a way that the frequency of every item in {(j, i) : j ∈ [1/φµ]} is at least ⌊φµm/2⌋ and the frequency of any other item is 0. The frequency of the item (xi, i) is at least ⌊εm/2 + φµm/2⌋ whereas the frequency of every other item is at most ⌊φµm/2⌋. Hence from the output of the (ε/5,φ/2)-HEAVY HITTERS algorithm Bob knows i with probability at least (1− δ). Now the result follows from Lemma 6.5.\n137\nWe now use the same idea as in the proof of Theorem 6.9 to prove an Ω( 1 ε log 1 ε ) space\ncomplexity lower bound for the ε-Maximum problem.\nTheorem 6.10. Suppose the size of universe n is at least 1 ε1+µ for any constant µ > 0. Any randomized one pass ε-MAXIMUM algorithm with success probability at least (1 − δ) must use Ω( 1 ε log 1 ε ) bits of space, for constant δ ∈ (0, 1).\nProof. Let µ > 0 be any constant. Without loss of generality, we can assume µ 6 1. We will show that, when n > 1 ε1+µ , any ε-MAXIMUM algorithm must use Ω( 1 ε log 1 ε ) bits of memory, thereby proving the result. Consider the INDEXING1/εµ,1/ε problem where Alice is given a string x = (x1, x2, · · · , x1/ε) ∈ [1/εµ]1/ε and Bob is given an index i ∈ [1/ε]. The stream we generate is over [1/εµ] × [1/ε] ⊆ U (this is possible since |U| > 1\nε1+µ ). Alice generates a stream of length\nm/2 in such a way that the frequency of every item in {(xj, j) : j ∈ [1/ε]} is at least ⌊εm/2⌋ and the frequency of any other item is 0. Alice now sends the memory content of the algorithm to Bob. Bob resumes the run of the algorithm by generating another stream of length m/2 in such a way that the frequency of every item in {(j, i) : j ∈ [1/εµ]} is at least ⌊εµm/2⌋ and the frequency of any other item is 0. The frequency of the item (xi, i) is at least ⌊εm/2 + εµm/2⌋ where as the frequency of every other item is at most ⌊εµm/2⌋. Hence the ε/5-MAXIMUM algorithm must output (xi, i) with probability at least (1 − δ). Now the result follows from Lemma 6.5.\nFor ε-MINIMUM, we prove a space complexity lower bound of Ω(1/ε) bits.\nTheorem 6.11. Suppose the universe size n is at least 1/ε. Then any randomized one pass ε-MINIMUM algorithm must use Ω(1/ε) bits of space.\nProof. We reduce from INDEXING2,5/ε to ε-MINIMUM thereby proving the result. Let the inputs to Alice and Bob in INDEXING2,5/ε be (x1, . . . , x5/ε) ∈ {0, 1}5/ε and an index i ∈ [5/ε] respectively. Alice and Bob generate a stream S over the universe [(5/ε) + 1]. Alice puts two copies of item j in S for every j ∈ U with xj = 1 and runs the ε-MINIMUM algorithm. Alice now sends the memory content of the algorithm to Bob. Bob resumes the run of the algorithm by putting two copies of every item in U\\{i, (5/ε)+1} in the stream S. Bob also puts one copy of (5/ε)+1 in S. Suppose the size of the support of (x1, . . . , x5/ε) be ℓ. Since 1/(2ℓ+(2/ε)−1) > ε/5, we have the following. If xi = 0, then the ε-MINIMUM algorithm must output i with probability at least (1− δ). If xi = 1, then the ε-MINIMUM algorithm must output (5/ε) + 1 with probability at least (1− δ). Now the result follows from Lemma 6.5.\n138\nWe show next a Ω(n log(1/ε)) bits space complexity lower bound for ε-BORDA.\nTheorem 6.12. Any one pass algorithm for ε-BORDA must use Ω(n log(1/ε)) bits of space.\nProof. We reduce ε-PERM to ε-BORDA. Suppose Alice has a permutation σ over [n] and Bob has an index i ∈ [n]. The item set of our reduced election is U = [n] ⊔ D, where D = {d1,d2, . . . ,d2n}. Alice generates a vote v over the item set U from σ as follows. The vote v is B1 ≻ B2 ≻ · · · ≻ B1/ε where Bj for j = 1, . . . , 1/ε is defined as follows.\nBj = d(j−1)2εn+1 ≻ d(j−1)2εn+2 ≻ · · · ≻ d(2j−1)εn ≻ σjεn+1 ≻ · · · ≻ σ(j+1)εn ≻ d(2j−1)ε+1 ≻ · · · ≻ d2jεn\nAlice runs the ε-BORDA algorithm with the vote v and sends the memory content to Bob.\nLet D−i = D \\ {i}, −−→ D−i be an arbitrary but fixed ordering of the items in D−i, and ←−− D−i be the reverse ordering of −−→ D−i. Bob resumes the algorithm by generating two votes each of the form i ≻ −−→D−i and i ≻ ←−− D−i. Let us call the resulting election E. The number of votes m in E is 5. The Borda score of the item i is at least 12n. The Borda score of every item x ∈ U is at most 9n. Hence for ε < 1/15, the ε-BORDA algorithm must output the item i. Moreover, it follows from the construction of v that an εmn additive approximation of the Borda score of the item i reveals the block where i belongs in the ε-PERM instance.\nWe next give a nearly-tight lower bound for the ε-MAXIMIN problem.\nTheorem 6.13. Any one-pass algorithm for ε-MAXIMIN requires Ω(n/ε2) memory bits of storage.\nProof. We reduce from INDEXING. Let γ = 1/ε2. Suppose Alice has a string y of length (n−γ) ·γ, partitioned into n−γ blocks of length γ each. Bob has an index ℓ = i+(j−γ−1) ·γ where i ∈ [γ], j ∈ {γ + 1, . . . ,n}. The INDEXING problem is to return yℓ for which there is a Ω(|y|) = Ω(n/ε2) lower bound (Lemma 6.5).\nThe initial part of the reduction follows the construction in the proof of Theorem 6 in\n[VWWZ15], which we encapsulate in the following lemma.\nLemma 6.9 (Theorem 6 in [VWWZ15]). Given y, Alice can construct a matrix P ∈ {0, 1}n×γ using public randomness, such that if Pi and Pj are the i’th and j’th rows of P respectively, then with probability at least 2/3, ∆(Pi, Pj) > γ 2 + √ γ if yℓ = 1 and ∆(a,b) 6 γ 2 − √ γ if yℓ = 0.\n139\nLet Alice construct P according to Lemma 6.9 and then adjoin the bitwise complement of the matrix P below P to form the matrix P ′ ∈ {0, 1}2n×γ; note that each column of P ′ has exactly n 1’s and n 0’s. Now, we interpret each row of P as a candidate and each column of P as a vote in the following way: for each v ∈ [γ], vote v has the candidates in {c : P ′c,v = 1} in ascending order in the top n positions and the rest of the candidates in ascending order in the bottom n positions. Alice inserts these γ votes into the stream and sends the state of the ε-MAXIMIN algorithm to Bob as well as the Hamming weight of each row in P ′. Bob inserts γ more votes, in each of which candidate i comes first, candidate j comes second, and the rest of the 2n − 2 candidates are in arbitrary order.\nNote that because of Bob’s votes, the maximin score of j is the number of votes among the ones casted by Alice in which j defeats i. Since i < j, in those columns v where Pi,v = Pj,v, candidate i beats candidate j. Thus, the set of votes in which j defeats i is {v | Pi,v = 0, Pj,v = 1}. The size of this set is 1 2 ( ∆(Pi, Pj) + |Pj|− |Pi| ) . Therefore, if Bob can estimate the maximin score of j upto √ γ/4 additive error, he can find ∆(Pi, Pj) upto √ γ/2 additive error as Bob knows |Pi| and |Pj|. This is enough, by Lemma 6.9, to solve the INDEXING problem with probability at least 2/3.\nFinally, we show a space complexity lower bound that depends on the length of the stream\nm.\nTheorem 6.14. Any one pass algorithm for ε-HEAVY HITTERS, ε-MINIMUM, ε-BORDA, and εMAXIMIN must use Ω(log logm) memory bits, even if the stream is over a universe of size 2, for every ε < 1 4 .\nProof. It is enough to prove the result only for ε-HEAVY HITTERS since the other three problems reduce to ε-HEAVY HITTERS for a universe of size 2. Suppose we have a randomized one pass ε-HEAVY HITTERS algorithm which uses s(m) bits of space. Using this algorithm, we will show a communication protocol for the GREATER-THANm problem whose communication complexity is s(2m) thereby proving the statement. The universal set is U = {0, 1}. Alice generates a stream of 2x many copies of the item 1. Alice now sends the memory content of the algorithm. Bob resumes the run of the algorithm by generating a stream of 2y many copies of the item 0. If x > y, then the item 1 is the only ε-winner; whereas if x < y, then the item 0 is the only ε-winner.\n140"
    }, {
      "heading" : "6.5 Conclusion",
      "text" : "In this work, we not only resolve a long standing fundamental open problem in the data streaming literature namely heavy hitters but also provide an optimal algorithm for a substantial generalization of heavy hitters by introducing another parameter φ. We also initiate a promising direction of research on finding a winner of a stream of votes.\nIn the next chapter, we study the scenario when voters are allowed to have incomplete\npreferences in the form of a partial order.\n141\nChapter 7\nKernelization for Possible Winner and\nCoalitional Manipulation\nIn the POSSIBLE WINNER problem in computational social choice theory, we are given a set of partial preferences and the question is whether a distinguished candidate could be made winner by extending the partial preferences to linear preferences. Previous work has provided, for many common voting rules, fixed parameter tractable algorithms for the POSSIBLE WINNER problem, with number of candidates as the parameter. However, the corresponding kernelization question is still open and in fact, has been mentioned as a key research challenge [BCF+14a]. In this work, we settle this open question for many common voting rules.\nWe show that the POSSIBLE WINNER problem for maximin, Copeland, Bucklin, ranked\npairs, and a class of scoring rules that includes the Borda voting rule do not admit a poly-\nnomial kernel with the number of candidates as the parameter. We show however that\nthe COALITIONAL MANIPULATION problem which is an important special case of the POS-\nSIBLE WINNER problem does admit a polynomial kernel for maximin, Copeland, ranked\npairs, and a class of scoring rules that includes the Borda voting rule, when the number of\nA preliminary version of the work in this chapter was published as [DMN15b]: Palash Dey, Neeldhara Misra, and Y. Narahari. Kernelization complexity of possible winner and coalitional manipulation problems in voting. In Proc. 2015 International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2015, Istanbul, Turkey, May 4-8, 2015, pages 87-96, 2015. The full version of the work in this chapter was published as [DMN16c]: Palash Dey, Neeldhara Misra, and Y. Narahari. Kernelization complexity of possible winner and coalitional manipulation problems in voting. Theor. Comput. Sci., 616:111- 125, 2016.\n142\nmanipulators is polynomial in the number of candidates. A significant conclusion of this\nwork is that the POSSIBLE WINNER problem is harder than the COALITIONAL MANIPULA-\nTION problem since the COALITIONAL MANIPULATION problem admits a polynomial kernel\nwhereas the POSSIBLE WINNER problem does not admit a polynomial kernel."
    }, {
      "heading" : "7.1 Introduction",
      "text" : "Usually, in a voting setting, it is assumed that the votes are complete orders over the candidates. However, due to many reasons, for example, lack of knowledge of voters about some candidates, a voter may be indifferent between some pairs of candidates. Hence, it is both natural and important to consider scenarios where votes are partial orders over the candidates. When votes are only partial orders over the candidates, the winner cannot be determined with certainty since it depends on how these partial orders are extended to linear orders. This leads to a natural computational problem called the POSSIBLE WINNER [KL05] problem: given a set of partial votes P and a distinguished candidate c, is there a way to extend the partial votes to linear ones to make c win? The POSSIBLE WINNER problem has been studied extensively in the literature [LPR+07, PRVW07, Wal07, XC11, BHN09, BD09, CLMM10, BBN10, BRR11, LPR+12, FRRS14] following its definition in [KL05]. Betzler et al. [BD09] and Baumeister et al. [BR12] show that the POSSIBLE WINNER winner problem is NP-complete for all scoring rules except for the plurality and veto voting rules; the POSSIBLE WINNER winner problem is in P for the plurality and veto voting rules. Moreover the POSSIBLE WINNER problem is known to be NP-complete for many common voting rules, for example, scoring rules, maximin, Copeland, Bucklin etc. even when the maximum number of undetermined pairs of candidates in every vote is bounded above by small constants [XC11]. Walsh showed that the POSSIBLE WINNER problem can be solved in polynomial time for all the voting rules mentioned above when we have a constant number of candidates [Wal07]. An important special case of the POSSIBLE WINNER problem is the COALITIONAL MANIPULATION problem [BTT89] where only two kinds of partial votes are allowed - complete preference and empty preference. The set of empty votes is called the manipulators’ vote and is denoted by M. The COALITIONAL MANIPULATION problem is NP-complete for maximin, Copeland, and ranked pairs voting rules even when |M| > 2 [FHS08, FHS10, XZP+09]. The COALITIONAL MANIPULATION problem is in P for the Bucklin voting rule [XZP+09]. We refer to [XC11, Wal07, XZP+09] for detailed overviews.\n143"
    }, {
      "heading" : "7.1.1 Our Contribution",
      "text" : "Discovering kernelization algorithms is currently an active and interesting area of research in computational social choice theory [BBN10, Bet10, BFG+09, BCH+14, FVBNS13, BCF+14b, BBCN12, DS12]. Betzler et al. [BHN09] showed that the POSSIBLE WINNER problem admits fixed parameter tractable algorithm when parameterized by the total number of candidates for scoring rules, maximin, Copeland, Bucklin, and ranked pairs voting rules. Yang et al. [YG13, Yan14] provides efficient fixed parameter tractable algorithms for the COALITIONAL MANIPULATION problem for the Borda, maximin, and Copeland voting rules. A natural and practical follow-up question is whether the POSSIBLE WINNER and COALITIONAL MANIPULATION problems admit a polynomial kernel when parameterized by the number of candidates. This question has been open ever since the work of Betzler et al. and in fact, has been mentioned as a key research challenge in parameterized algorithms for computational social choice theory [BCF+14a]. Betzler et al. showed non-existence of polynomial kernel for the POSSIBLE WINNER problem for the k-approval voting rule when parameterized by (t, k), where t is the number of partial votes [Bet10]. The NP-complete reductions for the POSSIBLE WINNER problem for scoring rules, maximin, Copeland, Bucklin, and ranked pairs voting rules given by Xia et al. [XZP+09] are from the EXACT 3 SET COVER problem. Their results do not throw any light on the existence of a polynomial kernel since EXACT 3 SET COVER has a trivial O(m3) kernel where m is the size of the universe. In this work, we show that there is no polynomial kernel (unless CoNP ⊆ NP/Poly) for the POSSIBLE WINNER problem, when parameterized by the total number of candidates, with respect to maximin, Copeland, Bucklin, and ranked pairs voting rules, and a class of scoring rules that includes the Borda voting rule. These hardness results are shown by a parameter-preserving many-to-one reduction from the SMALL UNIVERSE SET COVER problem for which there does not exist any polynomial kernel parameterized by universe size unless CoNP ⊆ NP/Poly [DLS09]. On the other hand, we show that the COALITIONAL MANIPULATION problem admits a polynomial kernel for maximin, Copeland, and ranked pairs voting rules, and a class of scoring rules that includes the Borda voting rule when we have poly(m) number of manipulators – specifically, we exhibit an O(m2|M|) kernel for maximin and Copeland voting rules, and an O(m4|M|) kernel for the ranked pairs voting rule, where m is the number of candidates and M is the set of manipulators. The COALITIONAL MANIPULATION problem for the Bucklin voting rule is in P [XZP+09] and thus the kernelization question does not arise.\n144\nA significant conclusion of our work is that, although the POSSIBLE WINNER and COALITIONAL MANIPULATION problems are both NP-complete for many voting rules, the POSSIBLE WINNER problem is harder than the COALITIONAL MANIPULATION problem since the COALITIONAL MANIPULATION problem admits a polynomial kernel whereas the POSSIBLE WINNER problem does not admit a polynomial kernel."
    }, {
      "heading" : "7.2 Problem Definitions",
      "text" : "The POSSIBLE WINNER problem with respect to r is the following:\nPOSSIBLE WINNER [r] Parameter: m Input: A set C of candidates and votes V, where each vote is a partial order over C, and a candidate c ∈ C. Question: Is there a linear extension of V for which c is the unique winner with respect to r?\nAn important special case of the POSSIBLE WINNER problem is the COALITIONAL MANIPULATION problem where every partial vote is either an empty order or a complete order. We call the complete votes the votes of the non-manipulators and the empty votes the votes of the manipulators. Formally the COALITIONAL MANIPULATION problem is defined as follows.\nCOALITIONAL MANIPULATION [r] Parameter: m Input: A set C of candidates, a set V of complete votes, an integer t corresponding to the number of manipulators, and a candidate c ∈ C. Question: Does there exist a set of votes V′ of size t such that c is the unique winner with respect to r for the voting profile (V,V′)?\nWhen the voting rule r is clear from the context, we often refer to the problems as POSSIBLE WINNER and COALITIONAL MANIPULATION without any further qualification. We note that one might also consider the variant of the problem where the designated candidate c is only required to be a co-winner, instead of being the unique winner. All the results in this work can be easily adapted to this variant as well.\nWe now briefly describe the framework in which we analyze the computational complexity\nof POSSIBLE WINNER and COALITIONAL MANIPULATION problems.\n145"
    }, {
      "heading" : "7.3 Kernelization Hardness of the Possible Winner Problem",
      "text" : "In this section, we show non-existence of polynomial kernels for the POSSIBLE WINNER problem for the maximin, Copeland, Bucklin, and ranked pairs voting rules, and a class of scoring rules that includes the Borda voting rule. We do this by demonstrating polynomial parameter transformations from the SMALL UNIVERSE SET COVER problem, which is the classic SET COVER problem, but now parameterized by the size of the universe and the budget.\nSMALL UNIVERSE SET COVER Parameter: m+ k Input: A set U = {u1, . . . ,um} and a family F = {S1, . . . , St}. Question: Is there a subfamily H ⊆ F of size at most k such that every element of the universe belongs to at least one H ∈ H?\nIt is well-known [DLS09] that RED-BLUE DOMINATING SET parameterized by k and the number of non-terminals does not admit a polynomial kernel unless CoNP ⊆ NP/Poly. It follows, by the duality between dominating set and set cover, that SET COVER when parameterized by the solution size and the size of the universe (in other words, the SMALL UNIVERSE SET COVER problem defined above) does not admit a polynomial kernel unless CoNP ⊆ NP/Poly. We now consider the POSSIBLE WINNER problem parameterized by the number of candidates for the maximin, Copeland, Bucklin, and ranked pairs voting rules, and a class of scoring rules that includes the Borda rule, and establish that they do not admit a polynomial kernel unless CoNP ⊆ NP/Poly, by polynomial parameter transformations from SMALL UNIVERSE SET COVER."
    }, {
      "heading" : "7.3.1 Results for the Scoring Rules",
      "text" : "We begin with proving the hardness of kernelization for the POSSIBLE WINNER problem for a class of scoring rules that includes the Borda voting rule. For that, we use the following lemma which has been used before [BRR11].\nLemma 7.1. Let C = {c1, . . . , cm} ⊎ D, (|D| > 0) be a set of candidates, and ~α a normalized score vector of length |C|. Then, for any given X = (X1, . . . ,Xm) ∈ Zm, there exists λ ∈ R and a voting profile such that the ~α-score of ci is λ+ Xi for all 1 6 i 6 m, and the score of candidates d ∈ D is less than λ. Moreover, the number of votes is O(poly(|C| ·∑mi=1 |Xi|)).\nWith the above lemma at hand, we now show hardness of polynomial kernel for the class\nof strict scoring rules.\n146\nTheorem 7.1. The POSSIBLE WINNER problem for any strict scoring rule, when parameterized by the number of candidates, does not admit a polynomial kernel unless CoNP ⊆ NP/Poly.\nProof. Let (U,F, k) be an instance of SMALL UNIVERSE SET COVER, where U = {u1, . . . ,um} and F = {S1, . . . , St}. We use Ti to denote U \\ Si for i ∈ [t]. We let ~α = (α1,α2, . . . ,α2m+3) denote the score vector of length t, and let δi denote the difference (αi−αi+1) for i ∈ [2m+2]. Note that for a strict scoring rule, all the δi’s will be strictly positive. We now construct an instance (C,V, c) of POSSIBLE WINNER as follows.\nCandidates C = U ⊎ V ⊎ {w, c,d}, where V := {v1, . . . , vm}.\nPartial Votes, P The first part of the voting profile comprises t partial votes, and will be denoted by P. Let Vj denote the set {v1, . . . , vj}. For each i ∈ [t], we first consider a profile built on a total order ηi:\nηi := d ≻ Si ≻ Vj ≻ w ≻ others, where j = m − |Si|.\nNow we obtain a partial order λi based on ηi for every i ∈ [t] as follows:\nλi := ηi \\ ({w}× ({d} ⊎ Si ⊎ Vj))\nThat is, λi is the partial vote where the order between the candidates x and y for x 6= w or y /∈ {d} ⊎ Si ⊎ Vj, is same as the order between x and y in ηi. Whereas, if x = w and y ∈ {d} ⊎ Si ⊎ Vj, then the order between x and y is unspecified. Let P′ be the set of votes {ηi | i ∈ [t]} and P be the set of votes {λi | i ∈ [t]}.\nComplete Votes, Q We now add complete votes, which we denote by Q, such that s(c) = s(ui), s(d)−s(c) = (k−1)δ1, s(c)−s(w) = k(δ2+δ3+ · · ·+δm+1)+δ1, s(c) > s(vi)+1 for all i ∈ [t], where s(a) is the score of candidate a from the combined voting profile P′ ⊎Q. From the proof of Lemma 7.1, we can see that such votes can always be constructed. In particular, also note that the voting profile Q consists of complete votes. Note that the number of candidates is 2m+ 3, which is polynomial in the size of the universe, as desired.\nWe now claim that the candidate c is a possible winner for the voting profile P ⊎Q with respect to the strict scoring rule ~α if and only if (U,F) is a YES instance of Set Cover.\n147\nIn the forward direction, suppose, without loss of generality, that S1, . . . , Sk form a set\ncover. Then we propose the following extension for the partial votes λ1, . . . , λk:\nw > d > Si > Vj > others,\nand the following extension for the partial votes λk+1, . . . , λt:\nd > Si > Vj > w > others\nFor i ∈ [k], the position of d in the extension of λi proposed above is one lower than its original position in ηi. Therefore, the score of d decreases by kδ1 making the final score of d less than the score of c. Similarly, since S1, . . . , Sk form a set cover, the score of ui decreases by at least minmi=2{δi} for every i ∈ [m], which is strictly positive as the scoring rule is strict. Finally, the score of w increase by at most k(δ2 + δ3 + · · ·+ δm+1), since there are at most k votes where the position of w in the extension of λi improved from it’s original position in ηi for i ∈ [t]. Therefore, the score of c is greater than any other candidate, implying that c is a possible winner.\nFor the reverse direction, notice that there must be at least k extensions where d is in the second position, since the score of d is (k−1)δ1 more than the score of c. In these extensions, observe that w will be at the first position. On the other hand, placing w in the first position causes its score to increase by (δ2 + δ3 + · · ·+ δm+1), therefore, if w is in the first position in ℓ extensions, its score increases by ℓ(δ2 + δ3 + · · ·+ δm+1). Since the score difference between w and c is only k(δ2 + δ3 + · · · + δm+1) + 1, we can afford to have w in the first position in at most k votes. Therefore, apart from the extensions where d is in the second position, in all remaining extensions, w appears after Vj, and therefore the candidates from Si continue to be in their original positions. Moreover, there must be exactly k votes where d is at the second position. We now claim that the sets corresponding to the k votes where d is at the second position form a set cover. Indeed, if not, suppose the element ui is not covered. It is easily checked that the score of such a ui remains unchanged in this extension, and therefore its score is equal to c, contradicting our assumption that we started with an extension for which c was a winner.\nThe proof of Theorem 7.1 can be generalized to a wider class of scoring rules as stated in\nthe following corollary.\n148\nCorollary 7.1. Let r be a positional scoring rule such that there exists a polynomial function f : N → N, such that for every m ∈ N, there exists an index l in the f(m) length score vector ~α satisfying following,\nαi − αi+1 > 0 ∀l 6 i 6 l +m\nThen the POSSIBLE WINNER problem for r, when parameterized by the number of candidates, does not admit a polynomial kernel unless CoNP ⊆ NP/Poly."
    }, {
      "heading" : "7.3.2 Results for the Maximin Voting Rule",
      "text" : "We will need the following lemma in subsequent proofs. The lemma has been used before [McG53, XC11].\nLemma 7.2. Let f : C× C −→ Z be a function such that\n1. ∀a,b ∈ C, f(a,b) = −f(b,a).\n2. ∀a,b, c,d ∈ C, f(a,b) + f(c,d) is even.\nThen we can construct in time O ( |C| ∑ {a,b}∈C×C |f(a,b)| ) an election E with n votes over the candidate set C such that for all a,b ∈ C, DE(a,b) = f(a,b).\nWe now describe the reduction for the POSSIBLE WINNER problem for the maximin voting\nrule parameterized by the number of candidates.\nTheorem 7.2. The POSSIBLE WINNER problem for the maximin voting rule, when parameterized by the number of candidates, does not admit a polynomial kernel unless CoNP ⊆ NP/Poly.\nProof. Let (U,F, k) be an instance of SMALL UNIVERSE SET COVER, where U = {u1, . . . ,um} and F = {S1, . . . , St}. We use Ti to denote U \\ Si. We now construct an instance (C,V, c) of the POSSIBLE WINNER as follows.\nCandidates C := U ⊎W ⊎ {c,d, x} ⊎ L, where W := {w1,w2, . . . ,wm,wx}, L := {l1, l2, l3}.\nPartial Votes, P The first part of the voting profile comprises t partial votes, and will be denoted by P. For each i ∈ [t], we first consider a profile built on a total order ηi. We denote the order w1 ≻ · · · ≻ wm ≻ wx by ~W. From this point onwards, whenever we place a set of candidates in some position of a partial order, we mean that the candidates in the set can\n149\nbe ordered arbitrarily. For example, the candidates in Si can be ordered arbitrarily among themselves in the total order ηi below for every i ∈ [t].\nηi := L ≻ ~W ≻ x ≻ Si ≻ d ≻ c ≻ Ti\nNow we obtain a partial order λi based on ηi for every i ∈ [t] as follows:\nλi := ηi \\ (W × ({c,d, x} ⊎ U))\nThe profile P consists of {λi | 1 ∈ [t]}.\nComplete Votes, Q We now describe the remaining votes in the profile, which are linear orders designed to achieve specific pairwise difference scores among the candidates. This profile, denoted by Q, is defined according to Lemma 7.2 to contain votes such that the pairwise score differences of P ∪Q satisfy the following.\n⊲ D(c,w1) = −2k.\n⊲ D(c, l1) = −t.\n⊲ D(d,w1) = −2k − 2.\n⊲ D(x,wx) = −2k − 2.\n⊲ D(wi,ui) = −2t ∀ i ∈ [m].\n⊲ D(ai, l1) = D(wx, l1) = −4t.\n⊲ D(l1, l2) = D(l2, l3) = D(l3, l1) = −4t.\n⊲ D(l, r) 6 1 for all other pairs (l, r) ∈ C× C.\nWe note that the for all c, c ′ ∈ C, the difference |D(c, c ′) −DP(c, c ′)| is always even, as long as t is even and the number of sets in F that contain any element u ∈ U is always even. Note that the latter can always be ensured without loss of generality: indeed, if u ∈ U occurs in an odd number of sets, then we can always add the set {u} if it is missing and remove it if it is present, flipping the parity in the process. In case {u} is the only set containing the element u, then we remove the set from both F and U and decrease k by one. The number of sets t can\n150\nbe assumed to be even by adding a dummy element and a dummy pair of sets that contains the said element. It is easy to see that these modifications always preserve the instance. Thus, the constructed instance of POSSIBLE WINNER is (C,V, c), where V := P ∪Q. We now turn to the proof of correctness.\nIn the forward direction, let H ⊆ F be a set cover of size at most k. Without loss of generality, let |H| = k (since a smaller set cover can always be extended artificially) and let H = {S1, . . . , Sk} (by renaming).\nIf i 6 k, let:\nλ∗i := L ≻ x ≻ Si ≻ d ≻ c ≻ ~W ≻ Ti\nIf k < i 6 t, let:\nλ∗i := L ≻ ~W ≻ x ≻ Si ≻ d ≻ c ≻ Ti\nClearly λ∗i extends λi for every i ∈ [t]. Let V∗ denote the extended profile consisting of the votes {λ∗i | i ∈ [t]}∪Q. We now claim that c is the unique winner with respect to the maximin voting rule in V∗.\nSince there are k votes in V∗ where c is preferred over w1 and (t − k) votes where w1 is\npreferred to c, we have:\nDV∗(c,w1) = DV(c,w1) + k − (t− k)\n= −2k + k− (t− k) = −t\nIt is easy to check that maximin score of c is −t. Also, it is straightforward to verify the following table.\nTherefore, c is the unique winner for the profile V∗.\nWe now turn to the reverse direction. Let P∗ be an extension of P such that V∗ := P∗ ∪Q admits c as a unique winner with respect to the maximin voting rule. We first argue that P∗ must admit a certain structure, which will lead us to an almost self-evident set cover for U.\nLet us denote by P∗C the set of votes in P ∗ which are consistent with c ≻ w1, and let P∗W be the set of votes in P∗ which are consistent with w1 ≻ c. We first argue that P∗C has at most k votes.\nClaim 7.1. Let P∗C be as defined above. Then |P ∗ C| 6 k.\n151\nCandidate maximin score\nwi, ∀i ∈ {1, 2, . . . ,m} < −t\nui, ∀i ∈ {1, 2, . . . ,m} 6 −4t\nwx 6 −4t\nl1, l2, l3 6 −4t\nx 6 −t− 2\nd 6 −t− 2\nProof. Suppose, for the sake of contradiction, that more than k extensions are consistent with c ≻ w1. Then we have:\nDV∗(c,w1) > DV(c,w1) + k + 1 − (t− k − 1)\n= −2k + 2k − t+ 2 = −t+ 2\nSince DV∗(c, l1) = −t, the maximin score of c is −t. On the other hand, we also have that the maximin score of d is given by DV∗(d,w1), which is now at least (−t):\nDV∗(d,w1) > DV(d,w1) + k + 1 − (t− k− 1)\n= −2k − 2+ 2k − t+ 2 = −t\nTherefore, c is no longer the unique winner in V∗ with respect to the maximin voting rule, which is the desired contradiction.\nWe next propose that a vote that is consistent withw1 ≻ cmust be consistent withwx ≻ x.\nClaim 7.2. Let P∗W be as defined above. Then any vote in P ∗ W must respect wx ≻ x.\n152\nProof. Suppose there are r votes in P∗C, and suppose that in at least one vote in P ∗ W where x ≻ wx. Notice that any vote in P∗C is consistent with x ≻ wx. Now we have:\nDV∗(c,w1) = DV(c,w1) + r− (t− r)\n= −2k + 2r − t\n= −t− 2(k− r)\nAnd further:\nDV∗(x,wx) > DV(x,wx) + (r+ 1) − (t− r− 1)\n= −2k − 2 + 2r − t+ 2\n= −t− 2(k − r)\nIt is easy to check that the maximin score of c in V∗ is at most −t − 2(k − r), witnessed by DV∗(c,w1), and the maximin score of x is at least −t − 2(k − r), witnessed by DV∗(x,wx). Therefore, c is no longer the unique winner in V∗ with respect to the maximin voting rule, and we have a contradiction.\nWe are now ready to describe a set cover of size at most k for U based on V∗. Define J ⊆ [t] as being the set of all indices i for which the extension of λi in V ∗ belongs to P∗C. Consider:\nH := {Si | i ∈ J}.\nThe set H is our proposed set cover. Clearly, |H| 6 k. It remains to show that H is a set cover.\nWe assume, for the sake of contradiction, that there is an element ui ∈ U that is not covered by H. This means that we have ui ∈ Ti for all i ∈ J, and thus wi ≻ ui in the corresponding extensions of λi in V\n∗. Further, for all i /∈ J, we have that the extension of λi in V∗ is consistent with:\nw1 ≻ · · · ≻ wi ≻ · · · ≻ wx ≻ x ≻ Si ≻ c ≻ Ti,\nimplying again that wi ≻ ui in these votes. Therefore, we have:\nDV∗(wi,ui) = DV(wi,ui) + k + (t− k) = −2t+ t = −t.\n153\nWe know that the maximin score of c is less than or equal to −t, since DV∗(c, l1) = −t, and we now have that the maximin score of wi is −t. This overrules c as the unique winner in V∗, contradicting our assumption to that effect. This completes the proof."
    }, {
      "heading" : "7.3.3 Results for the Copeland Voting Rule",
      "text" : "We now describe the result for the POSSIBLE WINNER problem for the Copeland voting rule parameterized by the number of candidates.\nTheorem 7.3. The POSSIBLE WINNER problem for the Copeland voting rule, when parameterized by the number of candidates, does not admit a polynomial kernel unless CoNP ⊆ NP/Poly.\nProof. Let (U,F, k) be an instance of SMALL UNIVERSE SET COVER, where U = {u1, . . . ,um} and F = {S1, . . . , St}. For the purpose of this proof, we assume (without loss of generality) that m > 6. We now construct an instance (C,V, c) of POSSIBLE WINNER as follows.\nCandidates C := U ⊎ {z, c,d,w}.\nPartial Votes, P The first part of the voting profile comprises of m partial votes, and will be denoted by P. For each i ∈ [t], we first consider a profile built on a total order:\nηi := U \\ Si ≻ z ≻ c ≻ d ≻ Si ≻ w\nNow we obtain a partial order λi based on ηi as follows for each i ∈ [t]:\nλi := ηi \\ ({z, c}× (Si ⊎ {d,w}))\nThe profile P consists of {λi | i ∈ [t]}.\nComplete Votes, Q We now describe the remaining votes in the profile, which are linear orders designed to achieve specific pairwise difference scores among the candidates. This profile, denoted by Q, is defined according to Lemma 7.2 to contain votes such that the pairwise score differences of P ∪Q satisfy the following.\n⊲ D(c,d) = t− 2k + 1\n⊲ D(z,w) = t− 2k − 1\n154\n⊲ D(c,ui) = t− 1\n⊲ D(c, z) = t+ 1\n⊲ D(c,w) = −t− 1\n⊲ D(ui,d) = D(z,ui) = t+ 1 ∀ i ∈ [m]\n⊲ D(z,d) = t+ 1\n⊲ D(ui,uj) = t+ 1 ∀ j ∈ [i+ 1 (mod ∗)m, i+ ⌊m/2⌋ (mod ∗)m]\nWe note that the difference |D(c, c ′) − DP(c, c ′)| is always even for all c, c ′ ∈ C, as long as t is odd and the number of sets in F that contain any element a ∈ U is always odd. Note that the latter can always be ensured without loss of generality: indeed, if a ∈ U occurs in an even number of sets, then we can always add the set {a} if it is missing and remove it if it is present, flipping the parity in the process. In case {a} is the only set containing the element a, then we remove the set from both F and U and decrease k by one. The number of sets t can be assumed to be odd by adding a dummy element in U, adding a dummy set that contains the said element in F, and incrementng k by one. It is easy to see that these modifications always preserve the instance.\nThus the constructed instance of POSSIBLE WINNER is (C,V, c), where V := P∪Q. We now turn to the proof of correctness.\nIn the forward direction, let H ⊆ F be a set cover of size at most k. Without loss of generality, let |H| = k (since a smaller set cover can always be extended artificially) and let H = {S1, . . . , Sk} (by renaming).\nIf i 6 k, let:\nλ∗i := U \\ Si ≻ z ≻ c ≻ d ≻ Si ≻ w\nIf k < i 6 t, let:\nλ∗i := U \\ Si ≻ d ≻ Si ≻ w ≻ z ≻ c\nClearly λ∗i extends λi for every i ∈ [t]. Let V∗ denote the extended profile consisting of the votes {λ∗i | i ∈ [t]}∪Q. We now claim that c is the unique winner with respect to the Copeland voting rule in V∗.\nFirst, consider the candidate z. For every i ∈ [m], between z and ui, even if z loses to ui in λ ∗ j , for every j ∈ [t], because D(z,ui) = t + 1, z wins the pairwise election between z\n155\nand ui. The same argument holds between z and d. Therefore, the Copeland score of z, no matter how the partial votes were extended, is at least (m+ 1).\nFurther, note that all other candidates (apart from c) have a Copeland score of less than (m + 1), because they are guaranteed to lose to at least three candidates (assuming m > 6). In particular, observe that ui loses to at least ⌊m/2⌋ candidates, and d loses to ui (merely by its position in the extended votes), and w loses to ui (because of way the scores were designed) for every i ∈ [m]. Therefore, the Copeland score of all candidates in C \\ {z, c} is strictly less than the Copeland score of z, and therefore they cannot be possible (co-)winners.\nNow we restrict our attention to the contest between z and c. First note that c beats ui for every i ∈ [m]: since the sets of H form a set cover, ui is placed in a position after c in some λ∗j for j ∈ [k]. Since the difference of score between c and ui was (t − 1), even if c suffered defeat in every other extension, we have the pairwise score of c and ui being at least t − 1 − (t − 1) + 1 = 1, which implies that c defeats every ui in their pairwise election. Note that c also defeats d by getting ahead of d in k votes, making its final score t− 2k+ 1+ k− (t− k) = 1. Finally, c is defeated by w, simply by the preset difference score. Therefore, the Copeland score of c is (m+ 2).\nNow all that remains to be done is to rule z out of the running. Note that z is defeated by w in their pairwise election: this is because z defeats w in k of the extended votes, and is defeated by w in the remaining. This implies that its final pairwise score with respect to w is at most t − 2k − 1 + k − (t − k) = −1. Also note that z loses to c because of its predefined difference score. Thus, the Copeland score of z in the extended vote is exactly (m + 1), and thus c is the unique winner of the extended vote.\nWe now turn to the reverse direction. Let P∗ be an extension of P such that V∗ := P∗ ∪Q admits c as a unique winner with respect to the Copeland voting rule. As with the argument for the maximin voting rule, we first argue that P∗ must admit a certain structure, which will lead us to an almost self-evident set cover for U.\nLet us denote by P∗C the set of votes in P ∗ which are consistent with c ≻ d, and let P∗W be the set of votes in P∗ which are consistent with w ≻ z. Note that the votes in P∗C necessarily have the form:\nλ∗i := U \\ Si ≻ z ≻ c ≻ d ≻ Si ≻ w\nand those in P∗W have the form:\nλ∗i := U \\ Si ≻ d ≻ Si ≻ w ≻ z ≻ c\n156\nIt is easy to check that this structure is directly imposed by the relative orderings that are fixed by the partial orders.\nBefore we argue the details of the scores, let us recall that in any extension of P, z loses to c and z wins over d and all candidates in U. Thus the Copeland score of z is at least (m+ 1). On the other hand, in any extension of P, c loses to w, and therefore the Copeland score of c is at most (m + 2). (These facts follow from the analysis in the forward direction.)\nThus, we have the following situation. If z wins over w, then c cannot be the unique winner in the extended vote, because the score of z goes up to (m + 2). Similarly, c cannot afford to lose to any ofU∪{d}, because that will cause its score to drop below (m+2), resulting in either a tie with z, or defeat. These facts will successively lead us to the correctness of the reverse direction.\nNow let us return to the sets P∗C and P ∗ W. If P ∗ C has more than k votes, then z wins over w: the final score of z is at least t − 2k − 1 + (k + 1) − (t − k − 1) = 1, and we have a contradiction. If P∗C has fewer than k votes, then c loses to d, with a score of at most t− 2k + 1 + (k − 1) − (t− k + 1) = −1, and we have a contradiction. Hence, P∗C must have exactly k votes.\nFinally, suppose the sets corresponding to the votes of P∗C do not form a set cover. Consider an element ui ∈ U not covered by the union of these sets. Observe that c now loses the pairwise election between itself and ui and is no longer in the running for being the unique winner in the extended vote. Therefore, the sets corresponding to the votes of P∗C form a set cover of size exactly k, as desired."
    }, {
      "heading" : "7.3.4 Results for the Bucklin Voting Rule",
      "text" : "We now describe the result for the POSSIBLE WINNER problem for the Bucklin voting rule parameterized by the number of candidates.\nTheorem 7.4. The POSSIBLE WINNER problem for the Bucklin voting rule, when parameterized by the number of candidates, does not admit a polynomial kernel unless CoNP ⊆ NP/Poly.\nProof. Let (U,F, k) be an instance of SMALL UNIVERSE SET COVER, where U = {u1, . . . ,um} and F = {S1, . . . , St}. Without loss of generality, we assume that t > k+ 1, and that every set in F has at least two elements. We now construct an instance (C,V, c) of POSSIBLE WINNER as follows.\n157\nCandidates C := U⊎ {z, c,a}⊎W⊎D1⊎D2⊎D3, where D1, D2, andD3 are sets of “dummy candidates” such that |D1| = m, |D2| = 2m, and |D3| = 2m. W := {w1,w2, . . . ,w2m}.\nPartial Votes, P The first part of the voting profile comprises of t partial votes, and will be denoted by P. For each i ∈ [t], we first consider a profile built on a total order:\nηi := U \\ Si ≻ Si ≻ wi (mod ∗)m ≻ wi+1 (mod ∗)m ≻ z ≻ c ≻ D3 ≻ others\nNow we obtain a partial order λi based on ηi for every i ∈ [t] as follows:\nλi := ηi \\ (( {wi (mod ∗)m,wi+1 (mod ∗)m, z, c} ⊎D3 ) × Si )\nThe profile P consists of {λi | i ∈ [t]}.\nComplete Votes, Q\nt− k − 1 : D1 ≻ z ≻ c ≻ others 1 : D1 ≻ c ≻ a ≻ z ≻ others\nk − 1 : D2 ≻ others\nWe now show that (U,F, k) is a YES instance if and only if (C,V, c) is a YES instance.\nSuppose {Sj : j ∈ J} forms a set cover. Then consider the following extension of P:\n(U \\ Sj) ≻ wj (mod ∗)m ≻ wj+1 (mod ∗)m ≻ z ≻ c ≻ D3 ≻ Sj ≻ others, for j ∈ J\n(U \\ Sj) ≻ Sj ≻ wj (mod ∗)m ≻ wj+1 (mod ∗)m ≻ z ≻ c ≻ D3 ≻ others, for j /∈ J\nWe claim that in this extension, c is the unique winner with Bucklin score (m + 2). First, let us establish the score of c. The candidate c is already within the top (m + 1) choices in (t−k) of the complete votes. In all the sets that form the set cover, c is ranked within the first (m + 2) votes in the proposed extension of the corresponding vote (recall that every set has at least two elements). Therefore, there are a total of t votes where c is ranked within the top (m+ 2) preferences. Further, consider a candidate v ∈ U. Such a candidate is not within the top (m+ 2) choices of any of the complete votes. Let Si be the set that covers the element v. Note that in the extension of the vote λi, v is not ranked among the top (m + 2) spots, since\n158\nthere are at least m candidates from D3 getting in the way. Therefore, v has strictly fewer than t votes where it is ranked among the top (m + 2) spots, and thus has a Bucklin score more than c.\nNow the candidate z is within the top (m+2) ranks of at most (t−k−1) votes among the complete votes. In the votes corresponding to the sets not in the set cover, z is placed beyond the first (m + 2) spots. Therefore, the number of votes where z is among the top (m + 2) candidates is at most (t− 1), which makes its Bucklin score strictly more than (m+ 2).\nThe candidates from W are within the top (m + 2) positions only in a constant number of votes. The candidates D1 ∪ {a} have (t − k) votes (among the complete ones) in which they are ranked among the top (m + 2) preferences, but in all extensions, these candidates have ranks below (m + 2). Finally, the candidates in D3 do not feature in the top (m + 2) positions of any of the complete votes, and similarly, the candidates in D2 do not feature in the top (m + 2) positions of any of the extended votes. Therefore, the Bucklin scores of all these candidates is easily seen to be strictly more than (m + 2), concluding the argument in the forward direction.\nNow consider the reverse direction. Suppose (C,V, c) is a YES instance. For the same reasons described in the forward direction, observe that only the following candidates can win depending upon how the partial preferences get extended - either one of the candidates in U, or one of z or c. Note that the Bucklin score of z in any extension is at most (m + 3). Therefore, the Bucklin score of c has to be (m + 2) or less. Among the complete votes Q, there are (t − k) votes where the candidate c appears in the top (m + 2) positions. To get majority within top (m + 2) positions, c should be within top (m + 2) positions for at least k of the extended votes in P. Let us call these set of votes P′. Now notice that whenever c comes within top (m + 2) positions in a valid extension of P, the candidate z also comes within top (m + 2) positions in the same vote. However, the candidate z is already ranked among the top (m + 2) candidates in (t − k − 1) complete votes. Therefore, z can appear within top (m+ 2) positions in at most k extensions (since c is the unique winner), implying that |P′| = k. Further, note that the Bucklin score of c cannot be strictly smaller than (m+ 2) in any extension. Indeed, candidate c features in only one of the complete votes within the top (m + 1) positions, and it would have to be within the top (m + 1) positions in at least (t− 1) extensions. However, as discussed earlier, this would give z exactly the same mileage, and therefore its Bucklin score would be (m − 1) or even less; contradicting our assumption that c is the unique winner.\n159\nNow we claim that the Si’s corresponding to the votes in P ′ form a set cover for U. If not, there is an element x ∈ U that is uncovered. Observe that x appears within top m positions in all the extensions of the votes in P′, by assumption. Further, in all the remaining extensions, since z is not present among the top (m + 2) positions, we only have room for two candidates from W. The remaining positions must be filled by all the candidates corresponding to elements of U. Therefore, x appears within the top (m + 2) positions of all the extended votes. Since these constitute half the total number of votes, we have that x ties with c in this situation, a contradiction."
    }, {
      "heading" : "7.3.5 Results for the Ranked Pairs Voting Rule",
      "text" : "We now describe the reduction for POSSIBLE WINNER parameterized by the number of candidates, for the ranked pairs voting rule.\nTheorem 7.5. The POSSIBLE WINNER problem for the ranked pairs voting rule, when parameterized by the number of candidates, does not admit a polynomial kernel unless CoNP ⊆ NP/Poly.\nProof. Let (U,F, k) be an instance of SMALL UNIVERSE SET COVER, where U = {u1, . . . ,um} and F = {S1, . . . , St}. Without loss of generality, we assume that t is even. We now construct an instance (C,V, c) of POSSIBLE WINNER as follows.\nCandidates C := U ⊎ {a,b, c,w}.\nPartial Votes, P The first part of the voting profile comprises of t partial votes, and will be denoted by P. For each i ∈ [t], we first consider a profile built on a total order:\nηi := U \\ Si ≻ Si ≻ b ≻ a ≻ c ≻ others\nNow we obtain a partial order λi based on ηi for every i ∈ [t] as follows:\nλi := ηi \\ ({a, c}× (Si ⊎ {b}))\nThe profile P consists of {λi | i ∈ [t]}.\nComplete Votes, Q We add complete votes such that along with the already determined pairs from the partial votes P, we have the following.\n160\n⊲ D(ui, c) = 2 ∀ i ∈ [m]\n⊲ D(c,b) = 4t\n⊲ D(c,w) = t+ 2\n⊲ D(b,a) = 2k + 4\n⊲ D(w,a) = 4t\n⊲ D(a, c) = t+ 2\n⊲ D(w,ui) = 4t ∀ i ∈ [m]\nWe now show that (U,F, k) is a YES instance if and only if (C,V, c) is a YES instance.\nSuppose {Sj : j ∈ J} forms a set cover. Then consider the following extension of P :"
    }, {
      "heading" : "U \\ Sj ≻ a ≻ c ≻ Sj ≻ b ≻ others ∀j ∈ J",
      "text" : ""
    }, {
      "heading" : "U \\ Sj ≻ Sj ≻ b ≻ a ≻ c ≻ others ∀j /∈ J",
      "text" : "We claim that the candidate c is the unique winner in this extension. Note that the pairs (w ≻ a) and (w ≻ ui) for every i ∈ [t] get locked first (since these differences are clearly the highest and unchanged). The pair (c,b) gets locked next, with a difference score of (3t+2k). Now since the votes in which c ≻ b are based on a set cover of size at most k, the pairwise difference between b and a becomes at least 2k + 4 − k + (t − k) = t + 4. Therefore, the next pair to get locked is b ≻ a. Finally, for every element ui ∈ U, the difference D(ui, c) is at most 2 + (t− 1) = t+ 1, since there is at least one vote where c ≻ ui (given that we used a set cover in the extension). It is now easy to see that the next highest pairwise difference is between c and w, so the ordering c ≻ w gets locked, and at this point, by transitivity, c is superior to w,b,a and all ui. It follows that c wins the election irrespective the sequence in which pairs are considered subsequently.\nNow suppose (C,V, c) is a YES instance. Notice that, irrespective of the extension of the votes in P, c ≻ b,w ≻ a,w ≻ ui ∀i ∈ [m] are locked first. Now if b ≻ c in all the extended votes, then it is easy to check that b ≻ a gets locked next, with a difference score of 2k+4+t; leaving us with D(ui, c) = t + 2 = D(c,w), where ui ≻ c could be a potential lock-in. This implies the possibility of a ui being a winner in some choice of tie-breaking, a contradiction to the assumption that c is the unique winner. Therefore, there are at least some votes in the\n161\nextended profile where c ≻ b. We now claim that there are at most k such votes. Indeed, if there are more, thenD(b,a) = 2k+4−(k+1)+(t−k−1) = t+2. Therefore, after the forced lock-ins above, we have D(b,a) = D(c,w) = D(a, c) = t + 2. Here, again, it is possible for a ≻ c to be locked in before the other choices, and we again have a contradiction. Finally, we have that c ≻ b in at most k many extensions in P. Call the set of indices of these extensions J. We claim that {Sj : j ∈ J} forms a set cover. If not, then suppose an element ui ∈ U is not covered by {Sj : j ∈ J}. Then the candidate ui comes before c in all the extensions which makes D(ui, c) become (t+ 2), which in turn ties with D(c,w). This again contradicts the fact that c is the unique winner. Therefore, if there is an extension that makes c the unique winner, then we have the desired set cover."
    }, {
      "heading" : "7.4 Polynomial Kernels for the Coalitional Manipulation",
      "text" : "Problem\nWe now describe a kernelization algorithm for every scoring rule which satisfies certain properties mentioned in Theorem 7.6 below. Note that the Borda voting rule satisfies these properties.\nTheorem 7.6. For m ∈ N, let (α1, . . . ,αm) and (α′1, . . . ,α′m+1) be the normalized score vectors for a scoring rule r for an election withm and (m+1) candidates respectively. Let α′1 = poly(m) and αi = α ′ i+1 for every i ∈ [m]. Then the COALITIONAL MANIPULATION problem for r admits a polynomial kernel when the number of manipulators is poly(m).\nProof. Let c be the candidate whom the manipulators aim to make winner. Let M be the set of manipulators and C the set of candidates. Let sNM(x) be the score of candidate x from the votes of the non-manipulators. Without loss of generality, we assume that, all the manipulators place c at top position in their votes. Hence, the final score of c is sNM(c) + |M|α1, which we denote by s(c). Now if sNM(x) > s(c) for any x 6= c, then c cannot win and we output no. Hence, we assume that sNM(x) < s(c) for all x 6= c. Now let us define s∗NM(x) as follows.\ns∗NM(x) := max{sNM(x), sNM(c)}\nAlso define s∗NM(c) as follows.\ns∗NM(c) := sNM(c) − |M|(α ′ 1 − α1)\n162\nWe define a COALITIONAL MANIPULATION instance with (m+1) candidates as (C′,NM,M, c), where C′ = C⊎ {d} is the set of candidates, M is the set of manipulators, c is the distinguished candidate, and NM is the non-manipulators’ vote such that it generates score of x ∈ C to be K + (s∗NM(x) − sNM(c)), where K ∈ N is same for x ∈ C, and the score of d is less than K − α′1|M|. The existence of such a voting profile NM of size poly(m) is due to Lemma 7.1 and the fact that α′1 = poly(m). Hence, once we show the equivalence of these two instances, we have a kernel whose size is polynomial in m. The equivalence of the two instances is due to the following facts: (1) The new instance has (m+ 1) candidates and c is always placed at the top position without loss of generality. The candidate c recieves |M|(α′1 − α1) score more than the initial instance and this is compensated in s∗NM(c). (2) The relative score difference from the final score of c to the current score of every x ∈ C \\ {c} is same in both the instances. (3) In the new instance, we can assume without loss of generality that the candidate d will be placed in the second position in all the manipulators’ votes.\nWe now move on to the voting rules that are based on the weighted majority graph. The reduction rules modify the weighted majority graph maintaining the property that there exists a set of votes that can realize the modified weighted majority graph. In particular, the final weighted majority graph is realizable with a set of votes.\nTheorem 7.7. The COALITIONAL MANIPULATION problem for the maximin voting rule admits a polynomial kernel when the number of manipulators is poly(m).\nProof. Let c be the distinguished candidate of the manipulators. Let M be the set of all manipulators. We can assume that |M| > 2 since for |M| = 1, the problem is in P [BTT89]. Define s to be minx∈C\\{c}D(V\\M)(c, x). So, s is the maximin score of the candidate c from the votes except from M. Since the maximin voting rule is monotone, we can assume that the voters in M put the candidate c at top position of their preferences. Hence, c’s final maximin score will be s+ |M|. This provides the following reduction rule.\nReduction rule 7.1. If s+ |M| > 0, then output YES.\nIn the following, we will assume s + |M| is negative. Now we propose the following\nreduction rules on the weighted majority graph.\nReduction rule 7.2. If D(V\\M)(ci, cj) < 0 and D(V\\M)(ci, cj) > 2|M| + s, then make D(V\\M)(ci, cj) either 2|M| + s+ 1 or 2|M| + s+ 2 whichever keeps the parity of D(V\\M)(ci, cj) unchanged.\n163\nIf D(V\\M)(ci, cj) > 2|M| + s, then DV(ci, cj) > |M| + s irrespective of the way the manipulators vote. Hence, given any votes of the manipulators, whether or not the maximin score of ci and cj will exceed the maximin score of c does not gets affected by this reduction rule. Hence, Reduction rule 7.1 is sound.\nReduction rule 7.3. If D(V\\M)(ci, cj) < s, then make D(V\\M)(ci, cj) either s − 1 or s − 2 whichever keeps the parity of D(V\\M)(ci, cj) unchanged.\nThe argument for the correctness of Reduction rule 7.3 is similar to the argument for Reduction rule 7.1. Here onward, we may assume that whenever D(V\\M)(ci, cj) < 0, s− 2 6 D(V\\M)(ci, cj) 6 2|M| + s+ 2\nReduction rule 7.4. If s < −4|M| then subtract s + 5|M| from D(V\\M)(x,y) for every x,y ∈ C, x 6= y.\nThe correctness of Reduction rule 7.4 follows from the fact that it adds linear fixed offsets to all the edges of the weighted majority graph. Hence, if there a voting profile of the voters in M that makes the candidate c win in the original instance, the same voting profile will make c win the election in the reduced instance and vice versa.\nNow we have a weighted majority graph with O(|M|) weights for every edge. Also, all the\nweights have uniform parity and thus the result follows from Lemma 7.2.\nWe next present a polynomial kernel for the COALITIONAL MANIPULATION problem for the\nCopeland voting rule.\nTheorem 7.8. The COALITIONAL MANIPULATION problem for the Copeland voting rule admits a polynomial kernel when the number of manipulators is poly(m).\nProof. We apply the following reduction rule.\nReduction rule 7.1. If D(V\\M)(x,y) > |M| for x,y ∈ C, then make D(V\\M)(x,y) either |M|+ 1 or |M| + 2 whichever keeps the parity of D(V\\M)(x,y) unchanged.\nGiven any votes of M, we have DV(x,y) > 0 in the original instance if and only if DV(x,y) > 0 in the reduced instance for every x,y ∈ C. Hence, each candidate has the same Copeland score and thus the reduction rule is correct.\nNow we have a weighted majority graph with O(|M|) weights for every edges. Also, all the weights have uniform parity. From Lemma 7.2, we can realize the weighted majority graph using O(m2|M|) votes.\n164\nNow we move on to the ranked pairs voting rule.\nTheorem 7.9. The COALITIONAL MANIPULATION problem for the ranked pairs voting rule admits a polynomial kernel when the number of manipulators is poly(m).\nProof. Consider all non-negative D(V\\M)(ci, cj) and arrange them in non-decreasing order. Let the ordering be x1, x2, . . . , xl where l = ( m 2 ) . Now keep applying following reduction rule till possible. Define x0 = 0.\nReduction rule 7.1. If there exist any i such that, xi − xi−1 > |M| + 2, subtract an even offset to all xi, xi+1, . . . , xl such that xi becomes either (xi−1 + |M|+ 1) or (xi−1 + |M| + 2).\nThe reduction rule is correct since for any set of votes by M, for any four candidates a,b, x,y ∈ C, D(a,b) > D(x,y) in the original instance if and only if D(a,b) > D(x,y) in the reduced instance. Now we have a weighted majority graph with O(m2|M|) weights for every edges. Also, all the weights have uniform parity and hence can be realized with O(m4|M|) votes by Lemma 7.2."
    }, {
      "heading" : "7.5 Conclusion",
      "text" : "Here we showed that the POSSIBLE WINNER problem does not admit a polynomial kernel for many common voting rules under the complexity theoretic assumption that CoNP ⊆ NP/Poly is not true. We also showed the existence of polynomial kernels for the COALITIONAL MANIPULATION problem for many common voting rules. This shows that the POSSIBLE WINNER problem is a significantly harder problem than the COALITIONAL MANIPULATION problem, although both the problems are NP-complete.\nWith this, we conclude the winner determination part of the thesis. We now move on to the last part of the thesis which study computational complexity of various form of election control.\n165\nPart III\nElection Control\nIn this part of the thesis, we present our results on the computational complexity of various\nproblems in the context of election control. We have the following chapters in this part.\n⊲ In Chapter 8 – Manipulation with Partial Votes – we show that manipulating an election\nbecomes a much harder problem when the manipulators only have a partial knowledge about the votes of the other voters. Hence, manipulating an election, although feasible in theory, may often be harder in practice.\n⊲ In Chapter 9 – Manipulation Detection – we initiate the work on detecting possible\ninstances of manipulation behavior in elections. Our work shows that detecting possible instances of manipulation may often be a much easier problem than manipulating the election itself.\n⊲ In Chapter 10 – Frugal Bribery – we show that the computational problem of bribery in\nan election remains an intractable problem even with a much weaker notion of bribery which we call frugal bribery. Hence, our results strengthen the intractability results from the literature on bribery.\n166\nChapter 8\nManipulation with Partial Votes\nThe Coalitional Manipulation problem has been studied extensively in the literature for many voting rules. However, most studies have focused on the complete information setting, wherein the manipulators know the votes of the non-manipulators. While this assumption is reasonable for purposes of showing intractability, it is unrealistic for algorithmic considerations. In most real-world scenarios, it is impractical for the manipulators to have accurate knowledge of all the other votes. In this work, we investigate manipulation with incomplete information. In our framework, the manipulators know a partial order for each voter that is consistent with the true preference of that voter. In this setting, we formulate three natural computational notions of manipulation, namely weak, opportunistic, and strong manipulation. We say that an extension of a partial order is viable if there exists a manipulative vote for that extension. We propose the following notions of manipulation when manipulators have incomplete information about the votes of other voters.\n1. WEAK MANIPULATION: the manipulators seek to vote in a way that makes their\npreferred candidate win in at least one extension of the partial votes of the nonmanipulators.\n2. OPPORTUNISTIC MANIPULATION: the manipulators seek to vote in a way that makes\ntheir preferred candidate win in every viable extension of the partial votes of the non-manipulators.\nA preliminary version of the work in this chapter was published as [DMN16a]: Palash Dey, Neeldhara Misra, and Y. Narahari. Complexity of manipulation with partial information in voting. In Proc. Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 229-235, 2016.\n168\n3. STRONG MANIPULATION: the manipulators seek to vote in a way that makes their pre-\nferred candidate win in every extension of the partial votes of the non-manipulators.\nWe consider several scenarios for which the traditional manipulation problems are\neasy (for instance, Borda with a single manipulator). For many of them, the correspond-\ning manipulative questions that we propose turn out to be computationally intractable.\nOur hardness results often hold even when very little information is missing, or in other\nwords, even when the instances are very close to the complete information setting. Our\nresults show that the impact of paucity of information on the computational complexity of\nmanipulation crucially depends on the notion of manipulation under consideration. Our\noverall conclusion is that computational hardness continues to be a valid obstruction to\nmanipulation, in the context of a more realistic model."
    }, {
      "heading" : "8.1 Introduction",
      "text" : "A central issue in voting is the possibility of manipulation. For many voting rules, it turns out that even a single vote, if cast differently, can alter the outcome. In particular, a voter manipulates an election if, by misrepresenting her preference, she obtains an outcome that she prefers over the “honest” outcome. In a cornerstone impossibility result, Gibbard and Satterthwaite [Gib73, Sat75] show that every unanimous and non-dictatorial voting rule with three candidates or more is manipulable. We refer to [BCE+15] for an excellent introduction to various strategic issues in computational social choice theory.\nConsidering that voting rules are indeed susceptible to manipulation, it is natural to seek ways by which elections can be protected from manipulations. The works of Bartholdi et al. [BTT89, BO91] approach the problem from the perspective of computational intractability. They exploit the possibility that voting rules, despite being vulnerable to manipulation in theory, may be hard to manipulate in practice. Indeed, a manipulator is faced with the following decision problem: given a collection of votes P and a distinguished candidate c, does there exist a vote v that, when tallied with P, makes c win for a (fixed) voting rule r? The manipulation problem has subsequently been generalized to the problem of COALITIONAL MANIPULATION by Conitzer et al. [CSL07], where one or more manipulators collude together and try to make a distinguished candidate win the election. The manipulation problem, fortunately, turns out to be NP-hard in several settings. This established the success of the approach of demonstrating a computational barrier to manipulation.\n169\nHowever, despite having set out to demonstrate the hardness of manipulation, the initial results in [BTT89] were to the contrary, indicating that many voting rules are in fact easy to manipulate. Moreover, even with multiple manipulators involved, popular voting rules like plurality, veto, k-approval, Bucklin, and Fallback continue to be easy to manipulate [XZP+09]. While we know that the computational intractability may not provide a strong barrier [PR06, PR07, FKN08, XC08a, XC08b, FHH10, Wal10, Wal11b, IKM12, Dey15, DMN15b, DMN16c, DMN15a, DN14, DN15a] even for rules for which the coalitional manipulation problem turns out to be NP-hard, in all other cases the possibility of manipulation is a much more serious concern."
    }, {
      "heading" : "8.1.1 Motivation and Problem Formulation",
      "text" : "In our work, we propose to extend the argument of computational intractability to address the cases where the approach appears to fail. We note that most incarnations of the manipulation problem studied so far are in the complete information setting, where the manipulators have complete knowledge of the preferences of the truthful voters. While these assumptions are indeed the best possible for the computationally negative results, we note that they are not reflective of typical real-world scenarios. Indeed, concerns regarding privacy of information, and in other cases, the sheer volume of information, would be significant hurdles for manipulators to obtain complete information. Motivated by this, we consider the manipulation problem in a natural partial information setting. In particular, we model the partial information of the manipulators about the votes of the non-manipulators as partial orders over the set of candidates. A partial order over the set of candidates will be called a partial vote. Our results show that several of the voting rules that are easy to manipulate in the complete information setting become intractable when the manipulators know only partial votes. Indeed, for many voting rules, we show that even if the ordering of a small number of pairs of candidates is missing from the profile, manipulation becomes an intractable problem. Our results therefore strengthen the view that manipulation may not be practical if we limit the information the manipulators have at their disposal about the votes of other voters [CWX11].\nWe introduce three new computational problems that, in a natural way, extend the question of manipulation to the partial information setting. In these problems, the input is a set of partial votes P corresponding to the votes of the non-manipulators, a non-empty set of manipulators M, and a preferred candidate c. The task in the WEAK MANIPULATION (WM) problem is to determine if there is a way to cast the manipulators’ votes such that c wins\n170\nthe election for at least one extension of the partial votes in P. On the other hand, in the STRONG MANIPULATION (SM) problem, we would like to know if there is a way of casting the manipulators’ votes such that c wins the election in every extension of the partial votes in P.\nWe also introduce the problem of OPPORTUNISTIC MANIPULATION (OM), which is an “intermediate” notion of manipulation. Let us call an extension of a partial profile viable if it is possible for the manipulators to vote in such a way that the manipulators’ desired candidate wins in that extension. In other words, a viable extension is a YES-instance of the standard COALITIONAL MANIPULATION problem. We have an opportunistic manipulation when it is possible for the manipulators to cast a vote which makes c win the election in all viable extensions. Note that any YES-instance of STRONG MANIPULATION is also a YES-instance of OPPORTUNISTIC MANIPULATION, but this may not be true in the reverse direction. As a particularly extreme example, consider a partial profile where there are no viable extensions: this would be a NO-instance for STRONG MANIPULATION, but a (vacuous) YES-instance of OPPORTUNISTIC MANIPULATION. The OPPORTUNISTIC MANIPULATION problem allows us to explore a more relaxed notion of manipulation: one where the manipulators are obliged to be successful only in extensions where it is possible to be successful. Note that the goal with STRONG MANIPULATION is to be successful in all extensions, and therefore the only interesting instances are the ones where all extensions are viable.\nIt is easy to see that YES instance of STRONG MANIPULATION is also a YES instance of OPPORTUNISTIC MANIPULATION and WEAK MANIPULATION. Beyond this, we remark that all the three problems are questions with different goals, and neither of them render the other redundant. We refer the reader to Figure 8.1 for a simple example distinguishing these scenarios.\nAll the problems above generalize COALITIONAL MANIPULATION, and hence any computational intractability result for COALITIONAL MANIPULATION immediately yields a corresponding intractability result for WEAK MANIPULATION, STRONG MANIPULATION, and OPPORTUNISTIC MANIPULATION under the same setting. For example, it is known that the COALITIONAL MANIPULATION problem is intractable for the maximin voting rule when we have at least two manipulators [XZP+09]. Hence, the WEAK MANIPULATION, STRONG MANIPULATION, and OPPORTUNISTIC MANIPULATION problems are intractable for the maximin voting rule when we have at least two manipulators.\n171"
    }, {
      "heading" : "8.1.2 Related Work",
      "text" : "A notion of manipulation under partial information has been considered by Conitzer et al. [CWX11]. They focus on whether or not there exists a dominating manipulation and show that this problem is NP-hard for many common voting rules. Given some partial votes, a dominating manipulation is a non-truthful vote that the manipulator can cast which makes the winner at least as preferable (and sometimes more preferable) as the winner when the manipulator votes truthfully. The dominating manipulation problem and the WEAK MANIPULATION, OPPORTUNISTIC MANIPULATION, and STRONG MANIPULATION problems do not seem to have any apparent complexity-theoretic connection. For example, the dominating manipulation problem is NP-hard for all the common voting rules except plurality and veto, whereas, the STRONG MANIPULATION problem is easy for most of the cases (see Table 8.1). However, the results in [CWX11] establish the fact that it is indeed possible to make manipulation intractable by restricting the amount of information the manipulators possess about the votes of the other voters. Elkind and Erdélyi [EE12] study manipulation under voting rule uncertainty. However, in our work, the voting rule is fixed and known to the manipulators.\nTwo closely related problems that have been extensively studied in the context of incomplete votes are POSSIBLE WINNER and NECESSARY WINNER [KL05]. In the POSSIBLE WINNER problem, we are given a set of partial votes P and a candidate c, and the question is whether\n172\nthere exists an extension of P where c wins, while in the NECESSARY WINNER problem, the question is whether c is a winner in every extension of P. Following the work in [KL05], a number of special cases and variants of the POSSIBLE WINNER problem have been studied in the literature [CLMM10, BBF10, BRR11, BRR+12, GNNW14, XC11, DL13a, NW14, BFLR12, ML15]. The flavor of the WEAK MANIPULATION problem is clearly similar to POSSIBLE WINNER. However, we emphasize that there are subtle distinctions between the two problems. A more elaborate comparison is made in the next section."
    }, {
      "heading" : "8.1.3 Our Contribution",
      "text" : "Our primary contribution in this work is to propose and study three natural and realistic generalizations of the computational problem of manipulation in the incomplete information setting. We summarize the complexity results in this work in Table 8.1. Our results provides the following interesting insights on the impact of lack of information on the computational difficulty of manipulation. We note that the number of undetermined pairs of candidates per vote are small constants in all our hardness results.\n⊲ We observe that the computational problem of manipulation for the plurality and veto\nvoting rules remains polynomial time solvable even with lack of information, irrespective of the notion of manipulation under consideration [Proposition 8.1, Theorem 8.11 and 8.15, and observation 8.4]. We note that the plurality and veto voting rule also remain vulnerable under the notion of dominating manipulation [CWX11].\n⊲ The impact of absence of information on the computational complexity of manipulation\nis more dynamic for the k-approval, k-veto, Bucklin, Borda, and maximin voting rules. Only the WEAK MANIPULATION and OPPORTUNISTIC MANIPULATION problems are computationally intractable for the k-approval [Theorem 8.1 and 8.5], k-veto [Theorem 8.2 and 8.6], Bucklin [Theorem 8.3 and 8.10], Borda [Observation 8.3 and Theorem 8.7], and maximin [Observation 8.3 and Theorem 8.8] voting rules, whereas the STRONG MANIPULATION problem remains computationally tractable [Theorem 8.11 to 8.14].\n⊲ Table 8.1 shows an interesting behavior of the fallback voting rule. The Fallback vot-\ning rule is the only voting rule among the voting rules we study here for which the WEAK MANIPULATION problem is NP-hard [Theorem 8.3] but both the OPPORTUNISTIC MANIPULATION and STRONG MANIPULATION problems are polynomial time solvable\n173\n[Theorem 8.13 and observation 8.4]. This is because the OPPORTUNISTIC MANIPULATION problem can be solved for the fallback voting rule by simply making manipulators vote for their desired candidate.\n⊲ Our results show that absence of information makes all the three notions of manipula-\ntions intractable for the Copelandα voting rule for every rational α ∈ [0, 1] \\ {0.5} for the WEAK MANIPULATION problem [Observation 8.3] and for every α ∈ [0, 1] for the OPPORTUNISTIC MANIPULATION and STRONG MANIPULATION problems [Theorem 8.4 and 8.9].\nOur results (see Table 8.1) show that whether lack of information makes the manipulation problems harder, crucially depends on the notion of manipulation applicable to the situation under consideration. All the three notions of manipulations are, in our view, natural extension of manipulation to the incomplete information setting and tries to capture different behaviors of manipulators. For example, the WEAK MANIPULATION problem may be applicable to an optimistic manipulator whereas for an pessimistic manipulator, the STRONG MANIPULATION problem may make more sense."
    }, {
      "heading" : "8.1.4 Problem Definitions",
      "text" : "We now formally define the three problems that we consider in this work, namely WEAK MANIPULATION, OPPORTUNISTIC MANIPULATION, and STRONG MANIPULATION. Let r be a fixed voting rule. We first introduce the WEAK MANIPULATION problem.\nDefinition 8.1. r-WEAK MANIPULATION Given a set of partial votes P over a set of candidates C, a positive integer ℓ (> 0) denoting the number of manipulators, and a candidate c, do there exist votes ≻1, . . . ,≻ℓ ∈ L(C) such that there exists an extension ≻∈ L(C)|P| of P with r(≻,≻1, . . . ,≻ℓ) = c?\nTo define the OPPORTUNISTIC MANIPULATION problem, we first introduce the notion of an\n(r, c)-opportunistic voting profile, where r is a voting rule and c is any particular candidate.\nDefinition 8.2. (r, c)-Opportunistic Voting Profile Let ℓ be the number of manipulators and P a set of partial votes. An ℓ-voter profile (≻i)i∈[ℓ] ∈ L(C)ℓ is called an (r, c)-opportunistic voting profile if for each extension P of P for which there exists an ℓ-vote profile (≻′i)i∈[ℓ] ∈ L(C)ℓ with r ( P ∪ (≻′i)i∈[ℓ] ) = c, we have r ( P ∪ (≻i)i∈[ℓ] ) = c.\n174\n175\nIn other words, an ℓ-vote profile is (r, c)-opportunistic with respect to a partial profile if, when put together with the truthful votes of any extension, c wins if the extension is viable to begin with. We are now ready to define the OPPORTUNISTIC MANIPULATION problem.\nDefinition 8.3. r-OPPORTUNISTIC MANIPULATION Given a set of partial votes P over a set of candidates C, a positive integer ℓ (> 0) denoting the number of manipulators, and a candidate c, does there exist an (r, c)-opportunistic ℓ-vote profile?\nWe finally define the STRONG MANIPULATION problem.\nDefinition 8.4. r-STRONG MANIPULATION Given a set of partial votes P over a set of candidates C, a positive integer ℓ (> 0) denoting the number of manipulators, and a candidate c, do there exist votes (≻i)i∈ℓ ∈ L(C)ℓ such that for every extension ≻∈ L(C)|P| of P, we have r(≻, (≻i)i∈[ℓ]) = c?\nWe use (P, ℓ, c) to denote instances of WEAK MANIPULATION, OPPORTUNISTIC MANIPULATION, and STRONG MANIPULATION, where P denotes a profile of partial votes, ℓ denotes the number of manipulators, and c denotes the desired winner.\nFor the sake of completeness, we provide the definitions of the COALITIONAL MANIPULA-\nTION and POSSIBLE WINNER problems below.\nDefinition 8.5. r-COALITIONAL MANIPULATION Given a set of complete votes ≻ over a set of candidates C, a positive integer ℓ (> 0) denoting the number of manipulators, and a candidate c, do there exist votes (≻i)i∈ℓ ∈ L(C)ℓ such that r ( ≻, (≻i)i∈[ℓ] ) = c?\nDefinition 8.6. r-POSSIBLE WINNER Given a set of partial votes P and a candidate c, does there exist an extension ≻ of the partial votes in P to linear votes such that r(≻) = c?"
    }, {
      "heading" : "8.1.5 Comparison with Possible Winner and Coalitional Manipulation",
      "text" : "Problems\nFor any fixed voting rule, the WEAK MANIPULATION problem with ℓ manipulators reduces to the POSSIBLE WINNER problem. This is achieved by simply using the same set as truthful votes and introducing ℓ empty votes. We summarize this in the observation below.\n176\nObservation 8.1. The WEAK MANIPULATION problem many-to-one reduces to the POSSIBLE WINNER problem for every voting rule.\nProof. Let (P, ℓ, c) be an instance of WEAK MANIPULATION. Let Q be the set consisting of ℓ many copies of partial votes {∅}. Clearly the WEAK MANIPULATION instance (P, ℓ, c) is equivalent to the POSSIBLE WINNER instance (P ∪ Q).\nHowever, whether the POSSIBLE WINNER problem reduces to the WEAK MANIPULATION problem or not is not clear since in any WEAK MANIPULATION problem instance, there must exist at least one manipulator and a POSSIBLE WINNER instance may have no empty vote. From a technical point of view, the difference between the WEAK MANIPULATION and POSSIBLE WINNER problems may look marginal; however we believe that the WEAK MANIPULATION problem is a very natural generalization of the COALITIONAL MANIPULATION problem in the partial information setting and thus worth studying. Similarly, it is easy to show, that the COALITIONAL MANIPULATION problem with ℓ manipulators reduces to WEAK MANIPULATION, OPPORTUNISTIC MANIPULATION, and STRONG MANIPULATION problems with ℓ manipulators, since the former is a special case of the latter ones.\nObservation 8.2. The COALITIONAL MANIPULATION problem with ℓ manipulators many-to-one reduces to WEAK MANIPULATION, OPPORTUNISTIC MANIPULATION, and STRONG MANIPULATION problems with ℓ manipulators for all voting rules and for all positive integers ℓ.\nProof. Follows from the fact that every instance of the COALITIONAL MANIPULATION problem is also an equivalent instance of the WEAK MANIPULATION, OPPORTUNISTIC MANIPULATION, and STRONG MANIPULATION problems.\nFinally, we note that the COALITIONAL MANIPULATION problem with ℓmanipulators can be reduced to the WEAK MANIPULATION problem with just one manipulator, by introducing ℓ−1 empty votes. These votes can be used to witness a good extension in the forward direction. In the reverse direction, given an extension where the manipulator is successful, the extension can be used as the manipulator’s votes. This argument leads to the following observation.\nObservation 8.3. The COALITIONAL MANIPULATION problem with ℓ manipulators many-to-one reduces to the WEAK MANIPULATION problem with one manipulator for every voting rule and for every positive integer ℓ.\n177\nProof. Let (P, ℓ, c) be an instance of COALITIONAL MANIPULATION. Let Q be the set of consisting of ℓ − 1 many copies of partial vote {c ≻ others}. Clearly the WEAK MANIPULATION instance (P ∪ Q, 1, c) is equivalent to the COALITIONAL MANIPULATION instance (P, ℓ, 1).\nThis observation can be used to derive the hardness of WEAK MANIPULATION even for one manipulator whenever the hardness for COALITIONAL MANIPULATION is known for any fixed number of manipulators (for instance, this is the case for the voting rules such as Borda, maximin and Copeland). However, determining the complexity of WEAK MANIPULATION with one manipulator requires further work for voting rules where COALITIONAL MANIPULATION is polynomially solvable for any number of manipulators (such as k-approval, Plurality, Bucklin, and so on)."
    }, {
      "heading" : "8.2 Hardness Results for WEAK MANIPULATION, OPPOR-",
      "text" : "TUNISTIC MANIPULATION, and STRONG MANIPULATION\nProblems\nIn this section, we present our hardness results. While some of our reductions are from the POSSIBLE WINNER problem, the other reductions in this section are from the EXACT COVER BY 3-SETS problem, also referred to as X3C. This is a well-known NP-complete [GJ79] problem, and is defined as follows.\nDefinition 8.7 (Exact Cover by 3-Sets (X3C)). Given a set U and a collection S = {S1, S2, . . . , St} of t subsets of U with |Si| = 3 ∀i = 1, . . . , t, does there exist a T ⊂ S with |T| = |U|\n3 such that ∪X∈TX = U?\nWe use X3C to refer to the complement of X3C, which is to say that an instance of X3C is a YES instance if and only if it is a NO instance of X3C. The rest of this section is organized according to the problems being addressed."
    }, {
      "heading" : "8.2.1 Weak Manipulation Problem",
      "text" : "To begin with, recall that the COALITIONAL MANIPULATION problem is NP-complete for the Borda [DKNW11, BNW11], maximin [XZP+09], and Copelandα [FHS08, FHHR09, FHS10] voting rules for every rational α ∈ [0, 1]\\{0.5}, when we have two manipulators. Therefore, it\n178\nfollows from Observation 8.3 that the WEAK MANIPULATION problem is NP-complete for the Borda, maximin, and Copelandα voting rules for every rational α ∈ [0, 1] \\ {0.5}, even with one manipulator.\nFor the k-approval and k-veto voting rules, we reduce from the corresponding POSSIBLE WINNER problems. While it is natural to start from the same voting profile, the main challenge is in undoing the advantage that the favorite candidate receives from the manipulator’s vote, in the reverse direction.\n8.2.1.1 Result for the k-Approval Voting Rule\nWe begin with proving that the WEAK MANIPULATION problem is NP-complete for the kapproval voting rule even with one manipulator and at most 4 undetermined pairs per vote.\nTheorem 8.1. The WEAK MANIPULATION problem is NP-complete for the k-approval voting rule even with one manipulator for any constant k > 1, even when the number of undetermined pairs in each vote is no more than 4.\nProof. For simplicity of presentation, we prove the theorem for 2-approval. We reduce from the POSSIBLE WINNER problem for 2-approval which is NP-complete [XC11], even when the number of undetermined pairs in each vote is no more than 4. Let P be the set of partial votes in a POSSIBLE WINNER instance, and let C = {c1, . . . , cm, c} be the set of candidates, where the goal is to check if there is an extension of P that makes c win. For developing the instance of WEAK MANIPULATION, we need to “reverse” any advantage that the candidate c obtains from the vote of the manipulator. Notice that the most that the manipulator can do is to increase the score of c by one. Therefore, in our construction, we “artificially” increase the score of all the other candidates by one, so that despite of the manipulator’s vote, c will win the new election if and only if it was a possible winner in the POSSIBLE WINNER instance. To this end, we introduce (m + 1) many dummy candidates d1, . . . ,dm+1 and the complete votes:\nwi = ci ≻ di ≻ others, for every i ∈ {1, . . . ,m}\nFurther, we extend the given partial votes of the POSSIBLE WINNER instance to force the dummy candidates to be preferred least over the rest - by defining, for every vi ∈ P, the corresponding partial vote v′i as follows.\nv′i = vi ∪ {C ≻ {d1, . . . ,dm+1}}.\n179\nThis ensures that all the dummy candidates do not receive any score from the modified partial votes corresponding to the partial votes of the POSSIBLE WINNER instance. Notice that since the number of undetermined pairs in vi is no more than 4, the number of undetermined pairs in v′i is also no more than 4. Let (C ′,Q, c) denote this constructed WEAK MANIPULATION instance. We claim that the two instances are equivalent.\nIn the forward direction, suppose c is a possible winner with respect to P, and let P be an extension where c wins. Then it is easy to see that the manipulator can make c win in some extension by placing c and dm+1 in the first two positions of her vote (note that the partial score of dm+1 is zero in Q). Indeed, consider the extension of Q obtained by mimicking the extension P on the “common” partial votes, {v′i | vi ∈ P}. Notice that this is well-defined since vi and v ′ i have exactly the same set of incomparable pairs. In this extension, the score of c is strictly greater than the scores of all the other candidates, since the scores of all candidates in C is exactly one more than their scores in P, and all the dummy candidates have a score of at most one.\nIn the reverse direction, notice that the manipulator puts the candidates c and dm+1 in the top two positions without loss of generality. Now suppose the manipulator’s vote c ≻ dm+1 ≻ others makes c win the election for an extension Q of Q. Then consider the extension P obtained by restricting Q to C. Notice that the score of each candidate in C in this extension is one less than their scores in Q. Therefore, the candidate c wins this election as well, concluding the proof.\nThe above proof can be imitated for any other constant values of k by reducing it from the POSSIBLE WINNER problem for k-approval and introducing (m+1)(k−1) dummy candidates.\n8.2.1.2 Result for the k-Veto Voting Rule\nTheorem 8.2. TheWEAK MANIPULATION problem for the k-veto voting rule is NP-complete even with one manipulator for any constant k > 1.\nProof. We reduce from the POSSIBLE WINNER problem for the k-veto voting rule which is known to be NP-complete [BD09]. Let P be the set of partial votes in a POSSIBLE WINNER problem instance, and let C = {c1, . . . , cm, c} be the set of candidates, where the goal is to check if there is an extension that makes cwin with respect to k-veto. We assume without loss of generality that c’s position is fixed in all the partial votes (if not, then we fix the position of c as high as possible in every vote).\n180\nWe introduce k + 1 many dummy candidates d1, . . . ,dk,d. The role of the first k dummy candidates is to ensure that the manipulator is forced to place them at the “bottom k” positions of her vote, so that all the original candidates get the same score from the additional vote of the manipulator. The most natural way of achieving this is to ensure that the dummy candidates have the same score as c in any extension (note that we know the score of c since c’s position is fixed in all the partial votes). This would force the manipulator to place these k candidates in the last k positions. Indeed, doing anything else will cause these candidates to tie with c, even when there is an extension of P that makes c win.\nTo this end, we begin by placing the dummy candidates in the top k positions in all the\npartial votes. Formally, we modify every partial vote as follows:\nw = di ≻ others, for every i ∈ {1, . . . , k}\nAt this point, we know the scores of c and di, for every i ∈ {1, . . . , k}. Using Lemma 7.1, we add complete votes such that the final score of c is the same with the score of every di and the score of c is strictly more than the score of d. The relative score of every other candidate remains the same. This completes the description of the construction. We denote the augmented set of partial votes by P.\nWe now argue the correctness. In the forward direction, if there is an extension of the votes that makes c win, then we repeat this extension, and the vote of the manipulator puts the candidate di at the positionm+i+2; and all the other candidates in an arbitrary fashion. Formally, we let the manipulator’s vote be:\nv = c ≻ c1 ≻ · · · ≻ cm ≻ d ≻ d1 ≻ · · · ≻ dk.\nBy construction cwins the election in this particular setup. In the reverse direction, consider a vote of the manipulator and an extension Q of P in which c wins. Note that the manipulator’s vote necessarily places the candidates di in the bottom k positions — indeed, if not, then c cannot win the election by construction. We extend a partial vote w ∈ P by mimicking the extension of the corresponding partial vote w′ ∈ P, that is, we simply project the extension of w′ on the original set of candidates C. Let Q denote this proposed extension of P. We claim that c wins the election given by Q. Indeed, suppose not. Let ci be a candidate whose score is at least the score of c in the extension Q. Note that the scores of ci and c in the extension Q are exactly the same as their scores in Q, except for a constant offset — importantly, their\n181\nscores are offset by the same amount. This implies that the score of ci is at least the score of c in Q as well, which is a contradiction. Hence, the two instances are equivalent."
    }, {
      "heading" : "8.2.1.3 Result for the Bucklin Voting Rule",
      "text" : "We next prove, by a reduction from X3C, that the WEAK MANIPULATION problem for the Bucklin and simplified Bucklin voting rules is NP-complete even with one manipulator and at most 16 undetermined pairs per vote.\nTheorem 8.3. The WEAK MANIPULATION problem is NP-complete for Bucklin, simplified Bucklin, Fallback, and simplified Fallback voting rules, even when we have only one manipulator and the number of undetermined pairs in each vote is no more than 16.\nProof. We reduce the X3C problem to WEAK MANIPULATION for simplified Bucklin. Let (U = {u1, . . . ,um}, S := {S1, S2, . . . , St}) be an instance of X3C, where each Si is a subset of U of size three. We construct a WEAK MANIPULATION instance based on (U, S) as follows.\nCandidate set: C = W ∪ X ∪D ∪ U ∪ {c,w,a,b}, where |W| = m− 3, |X| = 4, |D| = m + 1\nWe first introduce the following partial votes P in correspondence with the sets in the family as follows.\nW ≻ X ≻ Si ≻ c ≻ (U \\ Si) ≻ D \\ ({X× ({c} ∪ Si)}) , ∀i 6 t\nNotice that the number of undetermined pairs in every vote in P is 16. We introduce the following additional complete votes Q:\n⊲ t copies of U ≻ c ≻ others\n⊲ m/3 − 1 copies of U ≻ a ≻ c ≻ others\n⊲ m/3 + 1 copies of D ≻ b ≻ others\nThe total number of voters, including the manipulator, is 2t+ 2m/3 + 1. Now we show equivalence of the two instances.\nIn the forward direction, suppose we have an exact set cover T ⊂ S. Let the vote of the manipulator v be c ≻ D ≻ others. We consider the following extension P of P.\nW ≻ Si ≻ c ≻ X ≻ (U \\ Si) ≻ D\n182\nOn the other hand, if Si ∈ S \\ T, then we have:\nW ≻ X ≻ Si ≻ c ≻ (U \\ Si) ≻ D\nWe claim that c is the unique simplified Bucklin winner in the profile (P,W, v). Notice that the simplified Bucklin score of c is m + 1 in this extension, since it appears in the top m + 1 positions in the m/3 votes corresponding to the set cover, t votes from the complete profile Q and one vote v of the manipulator. For any other candidate ui ∈ U, ui appears in the top m+ 1 positions once in P and t+ m\n3 − 1 times in Q. Thus, ui does not get majority in\ntop m+ 1 positions making its simplified Bucklin score at least m+ 2. Hence, c is the unique simplified Bucklin winner in the profile (P,W, v). Similarly, the candidate w1 appears only t times in the top m + 1 positions. The same can be argued for the remaining candidates in D,W, and w.\nIn the reverse direction, suppose the WEAK MANIPULATION is a YES instance. We may assume without loss of generality that the manipulator’s vote v is c ≻ D ≻ others, since the simplified Bucklin score of the candidates in D is at least 2m. Let P be the extension of P such that c is the unique winner in the profile (P,Q, v). As w1 is ranked within top m + 2 positions in t+ m 3 +1 votes in Q, for c to win, c ≻ wm−2 must hold in at least m3 votes in P. In those votes, all the candidates in Si are also within top m+2 positions. Now if any candidate in U is within top m+ 1 positions in P more than once, then c will not be the unique winner. Hence, the Si’s corresponding to the votes where c ≻ wm−2 in P form an exact set cover. The reduction above also works for the Bucklin voting rule. Specifically, the argument for the forward direction is exactly the same as the simplified Bucklin above and the argument for the reverse direction is as follows. The candidate w1 is ranked within top m+ 2 positions in t + m 3 + 1 votes in Q and c is never placed within top m + 2 positions in any vote in Q. Hence, for c to win, c ≻ wm−2 must hold in at least m3 votes in P. In those votes, all the candidates in Si are also within top m positions. Notice that c never gets placed within top m positions in any vote in (P,Q). Now if any candidate x ∈ U is within top m positions in P more than once, then x gets majority within top m positions and thus c cannot win.\nThe result for the Fallback and simplified Fallback voting rules follow from the corresponding results for the Bucklin and simplified Bucklin voting rules respectively since every Bucklin and simplified Bucklin election is also a Fallback and simplified Fallback election respectively.\n183"
    }, {
      "heading" : "8.2.2 Strong Manipulation Problem",
      "text" : "We know that the COALITIONAL MANIPULATION problem is NP-complete for the Borda, maximin, and Copelandα voting rules for every rational α ∈ [0, 1] \\ {0.5}, when we have two manipulators. Thus, it follows from Observation 8.2 that STRONG MANIPULATION is NP-hard for Borda, maximin, and Copelandα voting rules for every rational α ∈ [0, 1] \\ {0.5} for at least two manipulators.\nFor the case of one manipulator, STRONG MANIPULATION turns out to be polynomial-time solvable for most other voting rules. For Copelandα, however, we show that the problem is coNP-hard for every α ∈ [0, 1] for a single manipulator, even when the number of undetermined pairs in each vote is bounded by a constant. This is achieved by a careful reduction from X3C.\nWe have following intractability result for the STRONG MANIPULATION problem for the\nCopelandα rule with one manipulator and at most 10 undetermined pairs per vote.\nTheorem 8.4. STRONG MANIPULATION is co-NP-hard for Copelandα voting rule for every α ∈ [0, 1] even when we have only one manipulator and the number of undetermined pairs in each vote is no more than 10.\nProof. We reduce X3C to STRONG MANIPULATION for Copelandα rule. Let (U = {u1, . . . ,um}, S = {S1, S2, . . . , St}) is an X3C instance. We assume, without loss of generality, t to be an even integer (if not, replicate any set from S). We construct a corresponding WEAK MANIPULATION instance for Copelandα as follows.\nCandidate set C = U ∪ {c,w, z,d}\nPartial votes P:\n∀i 6 t, (U \\ Si) ≻ c ≻ z ≻ d ≻ Si ≻ w \\ {{z, c}× (Si ∪ {d,w})}\nNotice that the number of undetermined pairs in every vote in P is 10. Now we add a set Q of complete votes with |Q| even and |Q| = poly(m, t) using Lemma 7.2 to achieve the following margin of victories in pairwise elections.\n⊲ DQ(d, z) = DQ(z, c) = DQ(c,d) = DQ(w, z) = 4t\n⊲ DQ(ui,d) = DQ(c,ui) = 4t ∀ui ∈ U\n⊲ DQ(z,ui) = t ∀ui ∈ U\n184\n⊲ DQ(c,w) = t− 2q 3 − 2\n⊲ DQ(ui,ui+1 (mod ∗)q) = 4t ∀ui ∈ U\n⊲ DQ(a,b) = 0 for every a,b ∈ C,a 6= b, not mentioned above\nWe have only one manipulator who tries to make c winner. Notice that the number of votes in the STRONG MANIPULATION instance (P∪Q, 1, c) including the manipulator’s vote is odd (since |P| and |Q| are even integers). Therefore, DP∗∪Q∪{v∗}(a,b) is never zero for every a,b ∈ C,a 6= b in every extension P∗ of P and manipulators vote v∗ and consequently the particular value of α does not play any role in this reduction. Hence, we assume, without loss of generality, α to be zero from here on and simply use the term Copeland instead of Copelandα.\nNow we show that the X3C instance (U, S) is a YES instance if and only if the STRONG MANIPULATION instance (P ∪ Q, 1, c) is a NO instance (a STRONG MANIPULATION instance is a NO instance if there does not exist a vote of the manipulator which makes c the unique winner in every extension of the partial votes). We can assume without loss of generality that manipulator puts c at first position and z at last position in her vote v.\nAssume that the X3C instance is a YES instance. Suppose (by renaming) that S1, . . . , Sm3 forms an exact set cover. We claim that the following extension P of P makes both z and c Copeland co-winners.\nExtension P of P:\ni 6 m\n3 , (U \\ Si) ≻ c ≻ z ≻ d ≻ Si ≻ w\ni > m\n3 + 1, (U \\ Si) ≻ d ≻ Si ≻ w ≻ c ≻ z\nWe have summarize the pairwise margins between z and c and the rest of the candidates from the profile (P ∪ Q ∪ v) in Table 8.2. The candidates z and c are the co-winners with Copeland score (m+ 1).\n185\nFor the other direction, notice that Copeland score of c is at least m + 1 since c defeats d and every candidate in U in every extension of P. Also notice that the Copeland score of z can be at most m+ 1 since z loses to w and d in every extension of P. Hence the only way c cannot be the unique winner is that z defeats all candidates in U and w defeats c.\nThis requires w ≻ c in at least t − m 3 extensions of P. We claim that the sets Si in the remaining of the extensions where c ≻ w forms an exact set cover for (U, S). Indeed, otherwise some candidate ui ∈ U is not covered. Then, notice that ui ≻ z in all t votes, making D(z,ui) = −1."
    }, {
      "heading" : "8.2.3 Opportunistic Manipulation Problem",
      "text" : "All our reductions for the co-NP-hardness for OPPORTUNISTIC MANIPULATION start from X3C. We note that all our hardness results hold even when there is only one manipulator. Our overall approach is the following. We engineer a set of partial votes in such a way that the manipulator is forced to vote in a limited number of ways to have any hope of making her favorite candidate win. For each such vote, we demonstrate a viable extension where the vote fails to make the candidate a winner, leading to a NO instance of OPPORTUNISTIC MANIPULATION. These extensions rely on the existence of an exact cover. On the other hand, we show that if there is no exact set cover, then there is no viable extension, thereby leading to an instance that is vacuously a YES instance of OPPORTUNISTIC MANIPULATION.\n8.2.3.1 Result for the k-Approval Voting Rule\nOur first result on OPPORTUNISTIC MANIPULATION shows that the OPPORTUNISTIC MANIPULATION problem is co-NP-hard for the k-approval voting rule for constant k > 3 even when the number of manipulators is one and the number of undetermined pairs in each vote is no more than 15.\nTheorem 8.5. The OPPORTUNISTIC MANIPULATION problem is co-NP-hard for the k-approval voting rule for constant k > 3 even when the number of manipulators is one and the number of undetermined pairs in each vote is no more than 15.\nProof. We reduce X3C to OPPORTUNISTIC MANIPULATION for k-approval rule. Let (U = {u1, . . . ,um}, S = {S1, S2, . . . , St}) is an X3C instance. We construct a corresponding OPPORTUNISTIC MANIPULATION instance for k-approval voting rule as follows. We begin by introducing a candidate for every element of the universe, along with k − 3 dummy candidates\n186\n(denoted by W), and special candidates {c, z1, z2,d, x,y}. Formally, we have:\nCandidate set C = U ∪ {c, z1, z2,d, x,y} ∪W.\nNow, for every set Si in the universe, we define the following total order on the candidate set, which we denote by P′i:\nW ≻ Si ≻ y ≻ z1 ≻ z2 ≻ x ≻ (U \\ Si) ≻ c ≻ d\nUsing P′i, we define the partial vote Pi as follows:\nPi = P ′ i \\ ({{y, x, z1, z2}× Si} ∪ {(z1, z2), (x, z1), (x, z2)}).\nWe denote the set of partial votes {Pi : i ∈ [t]} by P and {P′i : i ∈ [t]} by P′. We remark that the number of undetermined pairs in each partial vote Pi is 15.\nWe now invoke Lemma 1 from [DMN16c], which allows to achieve any pre-defined scores on the candidates using only polynomially many additional votes. Using this, we add a set Q of complete votes with |Q| = poly(m, t) to ensure the following scores, where we denote the k-approval score of a candidate from a set of votes V by sV(·): sQ(z1) = sQ(z2) = sQ(y) = sQ(c)−m/3; sQ(d), sQ(w) 6 sQ(c)−2t ∀w ∈ W; sQ(x) = sQ(c)−1; sP′∪Q(uj) = sQ(c)+1 ∀j ∈ [m].\nOur reduced instance is (P ∪ Q, 1, c). The reasoning for this score configuration will be apparent as we argue the equivalence. We first argue that if we had a YES instance of X3C (in other words, there is no exact cover), then we have a YES instance of OPPORTUNISTIC MANIPULATION. It turns out that this will follow from the fact that there are no viable extensions, because, as we will show next, a viable extension implies the existence of an exact set cover.\nTo this end, first observe that the partial votes are constructed in such a way that c gets no additional score from any extension. Assuming that the manipulator approves c (without loss of generality), the final score of c in any extension is going to be sQ(c)+1. Now, in any viable extension, every candidate uj has to be “pushed out” of the top k positions at least once. Observe that whenever this happens, y is forced into the top k positions. Since y is behind the score of c by onlym/3 votes, Si’s can be pushed out of place in onlym/3 votes. For every uj to lose one point, these votes must correspond to an exact cover. Therefore, if there is no exact cover, then there is no viable extension, showing one direction of the reduction.\n187\nOn the other hand, suppose we have a NO instance of X3C – that is, there is an exact cover. We will now use the exact cover to come up with two viable extensions, both of which require the manipulator to vote in different ways to make c win. Therefore, there is no single manipulative vote that accounts for both extensions, leading us to a NO instance of OPPORTUNISTIC MANIPULATION.\nFirst, consider this completion of the partial votes:\ni = 1,W ≻ y ≻ x ≻ z1 ≻ z2 ≻ Si ≻ (U \\ Si) ≻ c ≻ d\n2 6 i 6 m/3,W ≻ y ≻ z1 ≻ z2 ≻ x ≻ Si ≻ (U \\ Si) ≻ c ≻ d\nm/3 + 1 6 i 6 t,W ≻ Si ≻ y ≻ z1 ≻ z2 ≻ x ≻ (U \\ Si) ≻ c ≻ d\nNotice that in this completion, once accounted for along with the votes in Q, the score of c is tied with the scores of all uj’s, z1, x and y, while the score of z2 is one less than the score of c. Therefore, the only k candidates that the manipulator can afford to approve are W, the candidates c,d and z2. However, consider the extension that is identical to the above except with the first vote changed to:\nW ≻ y ≻ x ≻ z2 ≻ z1 ≻ Si ≻ (U \\ Si) ≻ c ≻ d\nHere, on the other hand, the only way for c to be an unique winner is if the manipulator approves W, c,d and z1. Therefore, it is clear that there is no way for the manipulator to provide a consolidated vote for both these profiles. Therefore, we have a NO instance of OPPORTUNISTIC MANIPULATION.\n8.2.3.2 Result for the k-Veto Voting Rule\nWe next move on to the k-veto voting rule and show that the OPPORTUNISTIC MANIPULATION problem for the k-veto is co-NP-hard for every constant k > 4 even when the number of manipulators is one and the number of undetermined pairs in each vote is no more than 9.\nTheorem 8.6. The OPPORTUNISTIC MANIPULATION problem is co-NP-hard for the k-veto voting rule for every constant k > 4 even when the number of manipulators is one and the number of undetermined pairs in each vote is no more than 9.\n188\nProof. We reduce X3C to OPPORTUNISTIC MANIPULATION for k-veto rule. Let (U = {u1, . . . ,um}, S = {S1, S2, . . . , St}) is an X3C instance. We construct a corresponding OPPORTUNISTIC MANIPULATION instance for k-veto voting rule as follows.\nCandidate set C = U ∪ {c, z1, z2,d, x,y} ∪A ∪W, where A = {a1,a2,a3}, |W| = k − 4\nFor every i ∈ [t], we define P′i as follows:\n∀i 6 t, c ≻ A ≻ (U \\ Si) ≻ d ≻ Si ≻ y ≻ x ≻ z1 ≻ z2 ≻ W\nUsing P′i, we define partial vote Pi = P ′ i \\ ({{y, x, z1, z2} × Si} ∪ {(z1, z2), (x, z1), (x, z2)}) for every i ∈ [t]. We denote the set of partial votes {Pi : i ∈ [t]} by P and {P′i : i ∈ [t]} by P′. We note that the number of undetermined pairs in each partial vote Pi is 9. Using Lemma 7.1, we add a set Q of complete votes with |Q| = poly(m, t) to ensure the following. We denote the k-veto score of a candidate from a set of votes W by sW(·).\n⊲ sP′∪Q(z1) = sP′∪Q(z2) = sP′∪Q(c) − m/3\n⊲ sP′∪Q(ai) = sP′∪Q(uj) = sP′∪Q(w) = sP′∪Q(c) ∀ai ∈ A,uj ∈ U,w ∈ W\n⊲ sP′∪Q(y) = sP′∪Q(c) − m/3 − 1\n⊲ sP′∪Q(x) = sP′∪Q(c) − 2\nWe have only one manipulator who tries to make c winner. Now we show that the X3C instance (U, S) is a YES instance if and only if the OPPORTUNISTIC MANIPULATION instance (P ∪ Q, 1, c) is a NO instance. In the forward direction, let us now assume that the X3C instance is a YES instance. Suppose (by renaming) that S1, . . . , Sm/3 forms an exact set cover. Let us assume that the manipulator’s vote v disapproves every candidate in W ∪ A since otherwise c can never win uniquely. We now show that if v does not disapprove z1 then, v is not a c-optimal vote. Suppose v does not disapprove z1. Then we consider the following extension P of P.\ni = 1, c ≻ A ≻ (U \\ Si) ≻ d ≻ y ≻ z1 ≻ x ≻ z2 ≻ Si ≻ W\n2 6 i 6 m/3, c ≻ A ≻ (U \\ Si) ≻ d ≻ y ≻ z1 ≻ z2 ≻ x ≻ Si ≻ W\nm/3 + 1 6 i 6 t, c ≻ A ≻ (U \\ Si) ≻ d ≻ Si ≻ y ≻ x ≻ z1 ≻ z2 ≻ W\n189\nWe have the following scores sP∪Q(c) = sP∪Q(z1) = sP∪Q(z2)+1 = sP∪Q(x)+1 = sP∪Q(uj)+ 1 ∀uj ∈ U. Hence, both c and z1 win for the votes P ∪ Q ∪ {v}. However, the vote v′ which disapproves a1,a2,a3, z1 makes c a unique winner for the votes P ∪ Q ∪ {v′}. Hence, v is not a c-optimal vote. Similarly, we can show that if the manipulator’s vote does not disapprove z2 then, the vote is not c-optimal. Hence, there does not exist any c-optimal vote and the OPPORTUNISTIC MANIPULATION instance is a NO instance.\nIn the reverse direction, we show that if the X3C instance is a NO instance, then there does not exist a vote v of the manipulator and an extension P of P such that c is the unique winner for the votes P ∪ Q ∪ {v′} thereby proving that the OPPORTUNISTIC MANIPULATION instance is vacuously YES (and thus every vote is c-optimal). Notice that, there must be at least m/3 votes P1 in P where the corresponding Si gets pushed in bottom k positions since sP′∪Q(uj) = sP′∪Q(c) ∀ai ∈ A,uj ∈ U. However, in each vote in P1, y is placed within top m− k many position and thus we have |P1| is exactly m/3 since sP′∪Q(y) = sP′∪Q(c) − m/3 − 1. Now notice that there must be at least one candidate u ∈ U which is not covered by the sets Sis corresponding to the votes P1 because the X3C instance is a NO instance. Hence, c cannot win the election uniquely irrespective of the manipulator’s vote. Thus every vote is c-optimal and the OPPORTUNISTIC MANIPULATION instance is a YES instance."
    }, {
      "heading" : "8.2.3.3 Result for the Borda Voting Rule",
      "text" : "We show next similar intractability result for the Borda voting rule too with only at most 7 undetermined pairs per vote.\nTheorem 8.7. The OPPORTUNISTIC MANIPULATION problem is co-NP-hard for the Borda voting rule even when the number of manipulators is one and the number of undetermined pairs in every vote is no more than 7.\nProof. We reduce X3C to OPPORTUNISTIC MANIPULATION for the Borda rule. Let (U = {u1, . . . ,um}, S = {S1, S2, . . . , St}) is an X3C instance. Without loss of generality we assume that m is not divisible by 6 (if not, then we add three new elements b1,b2,b3 to U and a set {b1,b2,b3} to S). We construct a corresponding OPPORTUNISTIC MANIPULATION instance for the Borda voting rule as follows.\nCandidate set C = U ∪ {c, z1, z2,d,y}\nFor every i ∈ [t], we define P′i as follows:\n190\n∀i 6 t,y ≻ Si ≻ z1 ≻ z2 ≻ (U \\ Si) ≻ d ≻ c\nUsing P′i, we define partial vote Pi = P ′ i \\ ({({y}∪Si)× {z1, z2}}∪ {(z1, z2)}) for every i ∈ [t]. We denote the set of partial votes {Pi : i ∈ [t]} by P and {P′i : i ∈ [t]} by P′. We note that the number of undetermined pairs in each partial vote Pi is 7. Using Lemma 7.1, we add a set Q of complete votes with |Q| = poly(m, t) to ensure the following. We denote the Borda score of a candidate from a set of votes W by sW(·).\n⊲ sP′∪Q(y) = sP′∪Q(c) +m + m/3 + 3\n⊲ sP′∪Q(z1) = sP′∪Q(c) − 3⌊m/6⌋− 2\n⊲ sP′∪Q(z2) = sP′∪Q(c) − 5⌊m/6⌋− 3\n⊲ sP′∪Q(ui) = sP′∪Q(c) +m + 5 − i ∀i ∈ [m]\n⊲ sP′∪Q(d) 6 sP′∪Q(c) − 5m\nWe have only one manipulator who tries to make c winner. Now we show that the X3C instance (U, S) is a YES instance if and only if the OPPORTUNISTIC MANIPULATION instance (P ∪ Q, 1, c) is a NO instance. Notice that we can assume without loss of generality that the manipulator places c at the first position, d at the second position, the candidate ui at (m + 5 − i)th position for every i ∈ [m], and y at the last position, since otherwise c can never win uniquely irrespective of the extension of P (that it, the manipulator’s vote looks like c ≻ d ≻ {z1, z2} ≻ um ≻ um−1 ≻ · · · ≻ u1 ≻ y). In the forward direction, let us now assume that the X3C instance is a YES instance. Suppose (by renaming) that S1, . . . , Sm/3 forms an exact set cover. Let the manipulator’s vote v be c ≻ d ≻ z1 ≻ z2 ≻ um ≻ · · · ≻ u1 ≻ y. We now argue that v is not a c-optimal vote. The other case where the manipulator’s vote v′ be c ≻ d ≻ z2 ≻ z1 ≻ um ≻ · · · ≻ u1 ≻ y can be argued similarly. We consider the following extension P of P.\n1 6 i 6 ⌊m/6⌋, z2 ≻ y ≻ Si ≻ z1 ≻ (U \\ Si) ≻ d ≻ c\n⌈m/6⌉ 6 i 6 m/3, z1 ≻ y ≻ Si ≻ z2 ≻ (U \\ Si) ≻ d ≻ c\nm/3 + 1 6 i 6 t,y ≻ Si ≻ z1 ≻ z2 ≻ (U \\ Si) ≻ d ≻ c\n191\nWe have the following Borda scores sP∪Q∪{v}(c) = sP∪Q∪{v}(y) + 1 = sP∪Q∪{v}(z2) + 6 = sP∪Q∪{v}(z1) = sP∪Q∪{v}(ui) + 1 ∀i ∈ [m]. Hence, c does not win uniquely for the votes P ∪ Q ∪ {v}. However, c is the unique winner for the votes P ∪ Q ∪ {v′}. Hence, there does not exist any c-optimal vote and the OPPORTUNISTIC MANIPULATION instance is a NO instance.\nIn the reverse direction, we show that if the X3C instance is a NO instance, then there does not exist a vote v of the manipulator and an extension P of P such that c is the unique winner for the votes P ∪ Q ∪ {v′} thereby proving that the OPPORTUNISTIC MANIPULATION instance is vacuously YES (and thus every vote is c-optimal). Notice that the score of y must decrease by at least m/3 for c to win uniquely. However, in every vote v where the score of y decreases by at least one in any extension P of P, at least one of z1 or z2 must be placed at top position of the vote v. However, the candidates z1 and z2 can be placed at top positions of the votes in P at most m/3 many times while ensuring c does not lose the election. Also, even after manipulator places the candidate ui at (m + 5 − i) th position for every i ∈ [m], for c to win uniquely, the score of every ui must decrease by at least one. Hence, altogether, there will be exactly m/3 votes (denoted by the set P1) in any extension of P where y is placed at the second position. However, since the X3C instance is a NO instance, the Sis corresponding to the votes in P1 does not form a set cover. Let u ∈ U be an element not covered by the Sis corresponding to the votes in P1. Notice that the score of u does not decrease in the extension P and thus c does not win uniquely irrespective of the manipulator’s vote. Thus every vote is c-optimal and thus the OPPORTUNISTIC MANIPULATION instance is a YES instance. Thus every vote is c-optimal and the OPPORTUNISTIC MANIPULATION instance is a YES instance."
    }, {
      "heading" : "8.2.3.4 Result for the Maximin Voting Rule",
      "text" : "For the maximin voting rule, we show intractability of OPPORTUNISTIC MANIPULATION with one manipulator even when the number of undetermined pairs in every vote is at most 8.\nTheorem 8.8. The OPPORTUNISTIC MANIPULATION problem is co-NP-hard for the maximin voting rule even when the number of manipulators is one and the number of undetermined pairs in every vote is no more than 8.\nProof. We reduce X3C to OPPORTUNISTIC MANIPULATION for the maximin rule. Let (U = {u1, . . . ,um}, S = {S1, S2, . . . , St}) is an X3C instance. We construct a corresponding OPPORTUNISTIC MANIPULATION instance for the maximin voting rule as follows.\nCandidate set C = U ∪ {c, z1, z2, z3,d,y}\n192\nFor every i ∈ [t], we define P′i as follows:\n∀i 6 t, Si ≻ x ≻ d ≻ y ≻ (U \\ Si) ≻ z1 ≻ z2 ≻ z3\nUsing P′i, we define partial vote Pi = P ′ i \\ ({({x}∪Si)× {d,y}}) for every i ∈ [t]. We denote the set of partial votes {Pi : i ∈ [t]} by P and {P′i : i ∈ [t]} by P′. We note that the number of undetermined pairs in each partial vote Pi is 8. We define another partial vote p as follows.\np = (z1 ≻ z2 ≻ z3 ≻ others ) \\ {(z1, z2), (z2, z3), (z1, z3)}\nUsing Lemma 7.2, we add a set Q of complete votes with |Q| = poly(m, t) to ensure the following pairwise margins (notice that the pairwise margins among z1, z2, and z3 does not include the partial vote p).\n⊲ DP′∪Q∪{p}(d, c) = 4t+ 1\n⊲ DP′∪Q∪{p}(x,d) = 4t+ 2m/3 + 1\n⊲ DP′∪Q∪{p}(y, x) = 4t− 2m/3 + 1\n⊲ DP′∪Q∪{p}(d,uj) = 4t− 1 ∀uj ∈ U\n⊲ DP′∪Q(z1, z2) = DP′∪Q(z2, z3) = DP′∪Q(z3, z1) = 4t+ 2\n⊲ |DP′∪Q∪{p}(a,b)| 6 1 for every a,b ∈ C not defined above.\nWe have only one manipulator who tries to make c winner. Now we show that the X3C instance (U, S) is a YES instance if and only if the OPPORTUNISTIC MANIPULATION instance (P ∪ Q ∪ {p}, 1, c) is a NO instance. Notice that we can assume without loss of generality that the manipulator’s vote prefers c to every other candidate, y to x, x to d, and d to uj for every uj ∈ U. In the forward direction, let us now assume that the X3C instance is a YES instance. Suppose (by renaming) that S1, . . . , Sm/3 forms an exact set cover. Notice that the manipulator’s vote must prefer either z2 to z1 or z1 to z3 or z3 to z2. We show that if the manipulator’s vote v prefers z2 to z1, then v is not a c-optimal vote. The other two cases are symmetrical. Consider the following extension P of P and p of p.\n1 6 i 6 m/3,d ≻ y ≻ Si ≻ x ≻ (U \\ Si) ≻ z1 ≻ z2 ≻ z3\n193\nm/3 + 1 6 i 6 t, Si ≻ x ≻ d ≻ y ≻ (U \\ Si) ≻ z1 ≻ z2 ≻ z3\np = z2 ≻ z3 ≻ z1 ≻ others\nFrom the votes in P ∪ Q ∪ {v, p}, the maximin score of c is −4t, of d, x,uj ∀uj ∈ U are −4t − 2, of z1, z3 are at most than −4t − 2, and of z2 is −4t. Hence, c is not the unique maximn winner. However, the manipulator’s vote c ≻ z1 ≻ z2 ≻ z3 ≻ other makes c the unique maximin winner. Hence, v is not a c-optimal vote.\nFor the reverse direction, we show that if the X3C instance is a NO instance, then there does not exist a vote v of the manipulator and an extension P of P such that c is the unique winner for the votes P ∪ Q ∪ {v′} thereby proving that the OPPORTUNISTIC MANIPULATION instance is vacuously YES (and thus every vote is c-optimal). Consider any extension P of P. Notice that, for c to win uniquely, y ≻ x must be at least m/3 of the votes in P; call these set of votes P1. However, d ≻ x in every vote in P1 and d ≻ x can be in at most m/3 votes in P for c to win uniquely. Hence, we have |P1| = m/3. Also for c to win, each d ≻ uj must be at least one vote of P and d ≻ uj is possible only in the votes in P1. However, the sets Sis corresponding to the votes in P1 does not form a set cover since the X3C instance is a NO instance. Hence, there must exist a uj ∈ U for which uj ≻ d in every vote in P and thus c cannot win uniquely irrespective of the vote of the manipulator. Thus every vote is c-optimal and the OPPORTUNISTIC MANIPULATION instance is a YES instance."
    }, {
      "heading" : "8.2.3.5 Result for the Copelandα Voting Rule",
      "text" : "Our next result proves that the OPPORTUNISTIC MANIPULATION problem is co-NP-hard for the Copelandα voting rule too for every α ∈ [0, 1] even with one manipulator and at most 8 undetermined pairs per vote.\nTheorem 8.9. The OPPORTUNISTIC MANIPULATION problem is co-NP-hard for the Copelandα voting rule for every α ∈ [0, 1] even when the number of manipulators is one and the number of undetermined pairs in each vote is no more than 8.\nProof. We reduce X3C to OPPORTUNISTIC MANIPULATION for the Copelandα voting rule. Let (U = {u1, . . . ,um}, S = {S1, S2, . . . , St}) is an X3C instance. We construct a corresponding OPPORTUNISTIC MANIPULATION instance for the Copelandα voting rule as follows.\nCandidate set C = U ∪ {c, z1, z2, z3,d1,d2,d3, x,y}\n194\nFor every i ∈ [t], we define P′i as follows:\n∀i 6 t, Si ≻ x ≻ y ≻ c ≻ others\nUsing P′i, we define partial vote Pi = P ′ i \\ ({({x}∪ Si)× {c,y}}) for every i ∈ [t]. We denote the set of partial votes {Pi : i ∈ [t]} by P and {P′i : i ∈ [t]} by P′. We note that the number of undetermined pairs in each partial vote Pi is 8. We define another partial vote p as follows.\np = (z1 ≻ z2 ≻ z3 ≻ others ) \\ {(z1, z2), (z2, z3), (z1, z3)}\nUsing Lemma 7.2, we add a set Q of complete votes with |Q| = poly(m, t) to ensure the following pairwise margins (notice that the pairwise margins among z1, z2, and z3 does not include the partial vote p).\n⊲ DP′∪Q∪{p}(uj, c) = 2 ∀uj ∈ U\n⊲ DP′∪Q∪{p}(x,y) = 2m/3\n⊲ DP′∪Q∪{p}(c,y) = DP′∪Q∪{p}(x, c) = DP′∪Q∪{p}(di, c) = DP′∪Q∪{p}(zk, c) =\nDP′∪Q∪{p}(uj, x) = DP′∪Q∪{p}(x, zk) = DP′∪Q∪{p}(di, x) = DP′∪Q∪{p}(y,uj) = DP′∪Q∪{p}(di,y) = DP′∪Q∪{p}(y, zk) = DP′∪Q∪{p}(zk,uj) = DP′∪Q∪{p}(uj,di) = DP′∪Q∪{p}(zk,d1) = DP′∪Q∪{p}(zk,d2) = DP′∪Q∪{p}(d3, zk) = 4t ∀i, k ∈ [3], j ∈ [m]\n⊲ DP′∪Q∪{p}(uj,uℓ) = −4t for at least m/3 many uℓ ∈ U\n⊲ DP′∪Q(z1, z2) = DP′∪Q(z2, z3) = DP′∪Q(z3, z1) = 1\n⊲ |DP′∪Q∪{p}(a,b)| 6 1 for every a,b ∈ C not defined above.\nWe have only one manipulator who tries to make c winner. Now we show that the X3C instance (U, S) is a YES instance if and only if the OPPORTUNISTIC MANIPULATION instance (P∪Q∪ {p}, 1, c) is a NO instance. Since the number of voters is odd, α does not play any role in the reduction and thus from here on we simply omit α. Notice that we can assume without loss of generality that the manipulator’s vote prefers c to every other candidate and x to y.\nIn the forward direction, let us now assume that the X3C instance is a YES instance. Suppose (by renaming) that S1, . . . , Sm/3 forms an exact set cover. Suppose the manipulator’s vote v order z1, z2, and z3 as z1 ≻ z2 ≻ z3. We will show that v is not a c-optimal vote.\n195\nSymmetrically, we can show that the manipulator’s vote ordering z1, z2, and z3 in any other order is not c-optimal. Consider the following extension P of P and p of p.\n1 6 i 6 m/3,y ≻ c ≻ Si ≻ x ≻ others\nm/3 + 1 6 i 6 t, Si ≻ x ≻ y ≻ c ≻ others\np = z1 ≻ z2 ≻ z3 ≻ others\nFrom the votes in P ∪ Q ∪ {v, p}, the Copeland score of c is m+ 4 (defeating y, zk,uj ∀k ∈ [3], j ∈ [m]), of y ism+3 (defeating zk,uj ∀k ∈ [3], j ∈ [m]), of uj is at most 2m/3+4 (defeating x,di ∀i ∈ [3] and at most 2m/3 many uℓ ∈ U), of x is 5 (defeating c,y, zk ∀l ∈ [3]), of d1,d2 is 2 (defeating y and c), of d3 is 5 (defeating y, c, zk ∀k ∈ [3]). of z3 is m + 3 (defeating di,uj∀i ∈ [3], j ∈ [m]) for every k ∈ [3], of z3 is m + 2 (defeating d1,d2,uji ∈ [3], j ∈ [m]), z2 is m + 3 (defeating d1,d2, z3,uji ∈ [3], j ∈ [m]), z1 is m + 4 (defeating d1,d2, z2, z3,uji ∈ [3], j ∈ [m]). Hence, c co-wins with z1 with Copeland scorem+4. However, the manipulator’s vote c ≻ z3 ≻ z2 ≻ z1 makes c win uniquely. Hence, v is not a c-optimal vote and thus the OPPORTUNISTIC MANIPULATION instance is a NO instance.\nFor the reverse direction, we show that if the X3C instance is a NO instance, then there does not exist a vote v of the manipulator and an extension P of P such that c is the unique winner for the votes P ∪ Q ∪ {v′} thereby proving that the OPPORTUNISTIC MANIPULATION instance is vacuously YES (and thus every vote is c-optimal). Consider any extension P of P. Notice that, for c to win uniquely, c must defeat each uj ∈ U and thus c is preferred over uj in at least one vote in P; we call these votes P1. However, in every vote in P1, y is preferred over x and thus |P1| 6 m/3 because x must defeat y for c to win uniquely. Since the X3C instance is a NO instance, there must be a candidate u ∈ U which is not covered by the sets corresponding to the votes in P1 and thus u is preferred over c in every vote in P. Hence, c cannot win uniquely irrespective of the vote of the manipulator. Thus every vote is c-optimal and the OPPORTUNISTIC MANIPULATION instance is a YES instance."
    }, {
      "heading" : "8.2.3.6 Result for the Bucklin Voting Rule",
      "text" : "For the Bucklin and simplified Bucklin voting rules, we show intractability of the OPPORTUNISTIC MANIPULATION problem with at most 15 undetermined pairs per vote and only one manipulator.\n196\nTheorem 8.10. The OPPORTUNISTIC MANIPULATION problem is co-NP-hard for the Bucklin and simplified Bucklin voting rules even when the number of manipulators is one and the number of undetermined pairs in each vote is no more than 15.\nProof. We reduce X3C to OPPORTUNISTIC MANIPULATION for the Bucklin and simplified Bucklin voting rules. Let (U = {u1, . . . ,um}, S = {S1, S2, . . . , St}) is an X3C instance. We assume without loss of generality that m is not divisible by 6 (if not, we introduce three elements in U and a set containing them in S) and t is an even integer (if not, we duplicate any set in S). We construct a corresponding OPPORTUNISTIC MANIPULATION instance for the Bucklin and simplified Bucklin voting rules as follows.\nCandidate set C = U ∪ {c, z1, z2, x1, x2,d} ∪W, where |W| = m− 3\nFor every i ∈ [t], we define P′i as follows:\n∀i 6 t, (U \\ Si) ≻ Si ≻ d ≻ x1 ≻ x2 ≻ z1 ≻ z2 ≻ others\nUsing P′i, we define partial vote Pi = P ′ i \\ ({({d}∪Si)× {x1, x2, z1, z2}}∪ {(z1, z2)}) for every i ∈ [t]. We denote the set of partial votes {Pi : i ∈ [t]} by P and {P′i : i ∈ [t]} by P′. We note that the number of undetermined pairs in each partial vote Pi is 15. We introduce the following additional complete votes Q:\n⊲ t/2 − ⌊m/6⌋− 1 copies of W ≻ z1 ≻ z2 ≻ x1 ≻ c ≻ others\n⊲ t/2 − ⌊m/6⌋− 1 copies of W ≻ z1 ≻ z2 ≻ x2 ≻ c ≻ others\n⊲ 2⌈m/6⌉ copies of W ≻ z1 ≻ z2 ≻ d ≻ c ≻ others\n⊲ ⌊m/6⌋ copies of W ≻ z1 ≻ d ≻ x1 ≻ c ≻ others\n⊲ ⌊m/6⌋ copies of W ≻ z1 ≻ d ≻ x2 ≻ c ≻ others\n⊲ 2⌈m/6⌉− 1 copies of U ≻ x1 ≻ others\n⊲ One U ≻ c ≻ others\nWe have only one manipulator who tries to make c winner. Now we show that the X3C instance (U, S) is a YES instance if and only if the OPPORTUNISTIC MANIPULATION instance (P∪Q, 1, c) is a NO instance. The total number of voters in the OPPORTUNISTIC MANIPULATION\n197\ninstance is 2t + 2m/3 + 1. We notice that within top m + 1 positions of the votes in P′ ∪ Q, c appears t + m/3 times, z1 and z2 appear t + ⌊m/6⌋ times, x1 appears t/2 + m/3 − 1 times, x2 appears t/2 − 1 times, every candidate in W appears t + m/3 − 1 times, every candidate in U appears t+ m/3 + 1 times. Also every candidate in U appears t+ m/3 + 1 times within top m positions of the votes in P ∪ Q. Hence, for both Bucklin and simplified Bucklin voting rules, we can assume without loss of generality that the manipulator puts c, every candidate in W, x1, x2, and exactly one of z1 and z2.\nIn the forward direction, let us now assume that the X3C instance is a YES instance. Suppose (by renaming) that S1, . . . , Sm/3 forms an exact set cover. Suppose the manipulator’s vote v puts c, every candidate in W, x1, x2, and z1 within top m + 1 positions. We will show that v is not c-optimal. The other case where the manipulator’s vote v′ puts c, every candidate in W, x1, x2, and z2 within top m + 1 positions is symmetrical. Consider the following extension P of P:\n1 6 i 6 ⌊m/6⌋, (U \\ Si)d ≻ x1 ≻ x2 ≻ z2 ≻ Si ≻≻ z1 ≻ others\n⌈m/6⌉ 6 i 6 m/3, (U \\ Si)d ≻ x1 ≻ x2 ≻ z1 ≻ Si ≻≻ z2 ≻ others\nm/3 + 1 6 i 6 t, (U \\ Si) ≻ Si ≻ d ≻ x1 ≻ x2 ≻ z1 ≻ z2 ≻ others\nFor both Bucklin and simplified Bucklin voting rules, c co-wins with z1 for the votes in P ∪ Q ∪ {v}. However, c wins uniquely for the votes in P ∪ Q ∪ {v′}. Hence, v is not a c-optimal vote and thus the OPPORTUNISTIC MANIPULATION instance is a NO instance.\nFor the reverse direction, we show that if the X3C instance is a NO instance, then there does not exist a vote v of the manipulator and an extension P of P such that c is the unique winner for the votes P ∪ Q ∪ {v′} thereby proving that the OPPORTUNISTIC MANIPULATION instance is vacuously YES (and thus every vote is c-optimal). Consider any extension P of P. Notice that, for c to win uniquely, every candidate must be pushed out of top m+ 1 positions in at least one vote in P; we call these set of votes P1. Notice that, |P1| > m/3. However, in every vote in P1, at least one of z1 and z2 appears within top m + 1 many positions. Since, the manipulator has to put at least one of z1 and z2 within its top m + 1 positions and z1 and z2 appear t + ⌊m/6⌋ times in the votes in P′ ∪ Q, we must have |P1| 6 m/3 and thus |P1| = m/3, for c to win uniquely. However, there exists a candidate u ∈ U not covered by the Sis corresponding to the votes in P1. Notice that u gets majority within top m positions of the votes and c can never get majority within top m + 1 positions of the votes. Hence, c\n198\ncannot win uniquely irrespective of the vote of the manipulator. Thus every vote is c-optimal and the OPPORTUNISTIC MANIPULATION instance is a YES instance."
    }, {
      "heading" : "8.3 Polynomial Time Algorithms for WEAK MANIPULATION,",
      "text" : "STRONG MANIPULATION, and OPPORTUNISTIC MANIPULA-\nTION Problems\nWe now turn to the polynomial time cases depicted in Table 8.1. This section is organized in three parts, one for each problem considered."
    }, {
      "heading" : "8.3.1 Weak Manipulation Problem",
      "text" : "Since the POSSIBLE WINNER problem is in P for the plurality and the veto voting rules [BD09], it follows from Observation 8.1 that the WEAK MANIPULATION problem is in P for the plurality and veto voting rules for any number of manipulators.\nProposition 8.1. The WEAK MANIPULATION problem is in P for the plurality and veto voting rules for any number of manipulators.\nProof. The POSSIBLE WINNER problem is in P for the plurality and the veto voting rules [BD09]. Hence, the result follows from Observation 8.1."
    }, {
      "heading" : "8.3.2 Strong Manipulation Problem",
      "text" : "We now discuss our algorithms for the STRONG MANIPULATION problem. The common flavor in all our algorithms is the following: we try to devise an extension that is as adversarial as possible for the favorite candidate c, and if we can make c win in such an extension, then roughly speaking, such a strategy should work for other extensions as well (where the situation only improves for c). However, it is challenging to come up with an extension that is globally dominant over all the others in the sense that we just described. So what we do instead is we consider every potential nemesis w who might win instead of c, and we build profiles that are “as good as possible” for w and “as bad as possible” for c. Each such profile leads us to constraints on how much the manipulators can afford to favor w (in terms of which positions among the manipulative votes are safe for w). We then typically show that\n199\nwe can determine whether there exists a set of votes that respects these constraints, either by using a greedy strategy or by an appropriate reduction to a flow problem. We note that the overall spirit here is similar to the approaches commonly used for solving the NECESSARY WINNER problem, but as we will see, there are non-trivial differences in the details. We begin with the k-approval and k-veto voting rules.\nTheorem 8.11. The STRONG MANIPULATION problem is in P for the k-approval and k-veto voting rules, for any k and any number of manipulators.\nProof. For the time being, we just concentrate on non-manipulators’ votes. For each candidate c′ ∈ C \\ {c}, calculate the maximum possible value of smaxNM (c, c′) = sNM(c′) − sNM(c) from non-manipulators’ votes, where sNM(a) is the score that candidate a receives from the votes of the non-manipulators. This can be done by checking all possible O(m2) pairs of positions for c and c′ in each vote v and choosing the one which maximizes sv(c ′) − sv(c) from that vote. We now fix the position of c at the top position for the manipulators’ votes and we check if it is possible to place other candidates in the manipulators’ votes such that the final value of smaxNM (c, c ′) + sM(c ′) − sM(c) is negative which can be solved easily by reducing it to the max flow problem which is polynomial time solvable.\nWe now prove that the STRONG MANIPULATION problem for scoring rules is in P for one\nmanipulator.\nTheorem 8.12. The STRONG MANIPULATION problem is in P for any scoring rule when we have only one manipulator.\nProof. For each candidate c′ ∈ C\\ {c}, calculate smaxNM (c, c′) using same technique described in the proof of Theorem 8.11. We now put c at the top position of the manipulator’s vote. For each candidate c′ ∈ C \\ {c}, c′ can be placed at positions i ∈ {2, . . . ,m} in the manipulator’s vote which makes smaxNM (c, c ′) + αi − α1 negative. Using this, construct a bipartite graph with C \\ {c} on left and {2, . . . ,m} on right and there is an edge between c′ and i iff the candidate c′ can be placed at i in the manipulator’s vote according to the above criteria. Now solve the problem by finding existence of perfect matching in this graph.\nOur next result proves that the STRONG MANIPULATION problem for the Bucklin, simplified\nBucklin, fallback, and simplified fallback voting rules are in P.\nTheorem 8.13. The STRONG MANIPULATION problem is in P for the Bucklin, simplified Bucklin, fallback, and simplified fallback voting rules, for any number of manipulators.\n200\nProof. Let (C,P,M, c) be an instance of STRONG MANIPULATION for simplified Bucklin, and let m denote the total number of candidates in this instance. Recall that the manipulators have to cast their votes so as to ensure that the candidate c wins in every possible extension of P. We use Q to denote the set of manipulating votes that we will construct. To begin with, without loss of generality, the manipulators place c in the top position of all their votes. We now have to organize the positioning of the remaining candidates across the votes of the manipulators to ensure that c is a necessary winner of the profile (P,Q).\nTo this end, we would like to develop a system of constraints indicating the overall number of times that we are free to place a candidate w ∈ C \\ {c} among the top ℓ positions in the profile Q. In particular, let us fix w ∈ C\\{c} and 2 6 ℓ 6 m. Let ηw,ℓ be the maximum number of votes of Q in which w can appear in the top ℓ positions. Our first step is to compute necessary conditions for ηw,ℓ.\nWe use Pw,ℓ to denote a set of complete votes that we will construct based on the given partial votes. Intuitively, these votes will represent the “worst” possible extensions from the point of view of c when pitted against w. These votes are engineered to ensure that the manipulators can make c win the elections Pw,ℓ for all w ∈ C \\ {c} and ℓ ∈ {2, . . . ,m}, if, and only if, they can strongly manipulate in favor of c. More formally, there exists a voting profile Q of the manipulators so that c wins the election Pw,ℓ∪Q, for allw ∈ C\\{c} and ℓ ∈ {2, . . . ,m} if and only if c wins in every extension of the profile P ∪ Q.\nWe now describe the profile Pw,ℓ. The construction is based on the following case analysis, where our goal is to ensure that, to the extent possible, we position c out of the top ℓ − 1 positions, and incorporate w among the top ℓ positions.\n⊲ Let v ∈ P be such that either c and w are incomparable or w ≻ c. We add the complete vote v′ to Pw,ℓ, where v ′ is obtained from v by placing w at the highest possible position\nand c at the lowest possible position, and extending the remaining vote arbitrarily.\n⊲ Let v ∈ P be such that c ≻ w, but there are at least ℓ candidates that are preferred over w in v. We add the complete vote v′ to Pw,ℓ, where v ′ is obtained from v by placing c at\nthe lowest possible position, and extending the remaining vote arbitrarily.\n⊲ Let v ∈ P be such that c is forced to be within the top ℓ − 1 positions, then we add the complete vote v′ to Pw,ℓ, where v ′ is obtained from v by first placing w at the highest\npossible position followed by placing c at the lowest possible position, and extending the remaining vote arbitrarily.\n201\n⊲ In the remaining votes, notice that whenever w is in the top ℓ positions, c is also in the\ntop ℓ− 1 positions. Let P∗w,ℓ denote this set of votes, and let t be the number of votes in P∗w,ℓ.\nWe now consider two cases. Let dℓ(c) be the number of times c is placed in the top ℓ − 1 positions in the profile Pw,ℓ ∪ Q, and let dℓ(w) be the number of times w is placed in the top ℓ positions in the profile Pw,ℓ. Let us now formulate the requirement that in Pw,ℓ ∪ Q, the candidate c does not have a majority in the top ℓ − 1 positions and w does have a majority in the top ℓ positions. Note that if this requirement holds for any w and ℓ, then strong manipulation is not possible. Therefore, to strongly manipulate in favor of c, we must ensure that for every choice of w and ℓ, we are able to negate the conditions that we derive.\nThe first condition from above simply translates to dℓ(c) 6 n/2. The second condition amounts to requiring first, that there are at least n/2 votes where w appears in the top ℓ positions, that is, dℓ(w)+ηw,ℓ+ t > n/2. Further, note that the gap between dℓ(w)+ηw,ℓ and majority will be filled by using votes from P∗w,ℓ to “push” w forward. However, these votes contribute equally to w and c being in the top ℓ and ℓ − 1 positions, respectively. Therefore, the difference between dℓ(w) + ηw,ℓ and n/2 must be less than the difference between dℓ(c) and n/2. Summarizing, the following conditions, which we collectively denote by (⋆), are sufficient to defeat c in some extension: dℓ(c) 6 n/2,dℓ(w)+ηw,ℓ+t > n/2, n/2−dℓ(w)+ηw,ℓ < n/2 − dℓ(c).\nFrom the manipulator’s point of view, the above provides a set of constraints to be satisfied as they place the remaining candidates across their votes. Whenever dℓ(c) > n/2, the manipulators place any of the other candidates among the top ℓ positions freely, because c already has majority. On the other hand, if dℓ(c) 6 n/2, then the manipulators must respect at least one of the following constraints: ηw,ℓ 6 n/2 − t− dℓ(w) and ηw,ℓ 6 dℓ(c) − dℓ(w).\nExtending the votes of the manipulator while respecting these constraints (or concluding that this is impossible to do) can be achieved by a natural greedy strategy — construct the manipulators’ votes by moving positionally from left to right. For each position, consider each manipulator and populate her vote for that position with any available candidate. We output the profile if the process terminates by completing all the votes, otherwise, we say NO.\nWe now argue the proof of correctness. Suppose the algorithm returns NO. This implies that there exists a choice of w ∈ C \\ {c} and ℓ ∈ {2, . . . ,m} such that for any voting profile Q of the manipulators, the conditions in (⋆) are satisfied. (Indeed, if there exists a voting profile that violated at least one of these conditions, then the greedy algorithm would have\n202\ndiscovered it.) Therefore, no matter how the manipulators cast their vote, there exists an extension where c is defeated. In particular, for the votes in P \\ P∗w,ℓ, this extension is given by Pw,ℓ. Further, we choose n/2 − ηw,ℓ − dℓ(w) votes among the votes in P ∗ w,ℓ and extend them by placing w in the top ℓ positions (and extending the rest of the profile arbitrary). We extend the remaining votes in P∗w,ℓ by positioning w outside the top ℓ positions. Clearly, in this extension, c fails to achieve majority in the top ℓ − 1 positions while w does achieve majority in the top ℓ positions.\nOn the other hand, if the algorithm returns YES, then consider the voting profile of the manipulators. We claim that c wins in every extension of P∪Q. Suppose, to the contrary, that there exists an extension R and a candidate w such that the simplified Bucklin score of c is no more than the simplified Bucklin score of w in R. In this extension, therefore, there exists ℓ ∈ {2, . . . ,m} for which w attains majority in the top ℓ positions and c fails to attain majority in the top ℓ − 1 positions. However, note that this is already impossible in any extension of the profile Pw,l ∪ P∗w,ℓ, because of the design of the constraints. By construction, the number of votes in which c appears in the top ℓ− 1 positions in R is only greater than the number of times c appears in the top ℓ − 1 positions in any extension of Pw,l ∪ P∗w,ℓ (and similarly for w). This leads us to the desired contradiction.\nFor the Bucklin voting rule, we do the following modifications to the algorithm. If dℓ(c) > dℓ(w) for some w ∈ C \\ {c} and ℓ < m, then we make ηw,ℓ = ∞. The proof of correctness for the Bucklin voting rule is similar to the proof of correctness for the simplified Bucklin voting rule above.\nFor Fallback and simplified Fallback voting rules, we consider the number of candidates each voter approves while computing ηw,ℓ. We output YES if and only if ηw,ℓ > 0 for every w ∈ C \\ {c} and every ℓ 6 m, since we can assume, without loss of generality, that the manipulator approves the candidate c only. Again the proof of correctness is along similar lines to the proof of correctness for the simplified Bucklin voting rule.\nWe next show that the STRONG MANIPULATION problem for the maximin voting rule is\npolynomial-time solvable when we have only one manipulator.\nTheorem 8.14. The STRONG MANIPULATION problem for the maximin voting rules are in P, when we have only one manipulator.\nProof. For the time being, just concentrate on non-manipulators’ votes. Using the algorithm for NW for maximin in [XC11], we compute for all pairs w,w′ ∈ C, N(w,w′)(w,d) and\n203\nN(w,w′)(c,w ′) for all d ∈ C \\ {c}. This can be computed in polynomial time. Now we place c at the top position in the manipulator’s vote and increase all N(w,w′)(c,w ′) by one. Now we place a candidate w at the second position if for all w′ ∈ C, N′(w,w′)(w,d) < N(w,w′)(c,w′) for all d ∈ C \\ {c}, where N′(w,w′)(w,d) = N(w,w′)(w,d) of the candidate d has already been assigned some position in the manipulator’s vote, andN′(w,w′)(w,d) = N(w,w′)(w,d)+1 else. The correctness argument is in the similar lines of the classical greedy manipulation algorithm of [BTT89]."
    }, {
      "heading" : "8.3.3 Opportunistic Manipulation Problem",
      "text" : "For the plurality, fallback, and simplified fallback voting rules, it turns out that the voting profile where all the manipulators approve only c is a c-opportunistic voting profile, and therefore it is easy to devise a manipulative vote.\nObservation 8.4. The OPPORTUNISTIC MANIPULATION problem is in P for the plurality and fallback voting rules for a any number of manipulators.\nFor the veto voting rule, however, a more intricate argument is needed, that requires building a system of constraints and a reduction to a suitable instance of the maximum flow problem in a network, to show polynomial time tractability of OPPORTUNISTIC MANIPULATION.\nTheorem 8.15. The OPPORTUNISTIC MANIPULATION problem is in P for the veto voting rule for a constant number of manipulators.\nProof. Let (P, ℓ, c) be an input instance of OPPORTUNISTIC MANIPULATION. We may assume without loss of generality that the manipulators approve c. We view the voting profile of the manipulators as a tuple (na)a∈C\\{c} ∈ (N∪ {0})m−1 with ∑ a∈C\\{c} na = ℓ, where the na many manipulators disapprove a. We denote the set of such tuples as T and we have T = O((2m)ℓ) which is polynomial inm since ℓ is a constant. A tuple (na)a∈C\\{c} ∈ T is not c-optimal if there exists another tuple (n′a)a∈C\\{c} ∈ T and an extension P of P with the following properties. We denote the veto score of a candidate from P by s(·). For every candidate a ∈ C \\ {c}, we define two quantities w(a) and d(a) as follows.\n⊲ s(c) > s(a) for every a ∈ C\\{c}with na = n′a = 0 and we definew(a) = s(c)−1,d(a) = 0\n204\n⊲ s(c) > s(a) − n′a for every a ∈ C \\ {c} with na > n′a and we define w(a) = s(c) − n′a − 1,d(a) = 0\n⊲ s(a) − na > s(c) > s(a) − n ′ a for every a ∈ C \\ {c} with na < n′a and we define\nw(a) = s(c) − n′a,d(a) = s(a) − na\nWe guess the value of s(c). Given a value of s(c), we check the above two conditions by reducing this to a max flow problem instance as follows. We have a source vertex s and a sink t. We have a vertex for every a ∈ C (call this set of vertices Y) and a vertex for every vote v ∈ P (call this set of vertices X). We add an edge from s to each in X of capacity one. We add an edge of capacity one from a vertex x ∈ X to a vertex y ∈ Y if the candidate corresponding to the vertex y can be placed at the last position in an extension of the partial vote corresponding to the vertex x. We add an edge from a vertex y to t of capacity w(a), where a is the voter corresponding to the vertex y. We also set the demand of every vertex y d(a) (that is the total amount of flow coming into vertex y must be at least d(a)), where a is the voter corresponding to the vertex y. Clearly, the above three conditions are met if and only if there is a feasible |P| amount of flow in the above flow graph. Since s(c) can have only |P|+ 1 possible values (from 0 to P) and |T| = O((2m)ℓ), we can iterate over all possible pairs of tuples in T and all possible values of s(c) and find a c-optimal voting profile if there exists a one."
    }, {
      "heading" : "8.4 Conclusion",
      "text" : "We revisited many settings where the complexity barrier for manipulation was non-existent, and studied the problem under an incomplete information setting. Our results present a fresh perspective on the use of computational complexity as a barrier to manipulation, particularly in cases that were thought to be dead-ends (because the traditional manipulation problem was polynomially solvable). To resurrect the argument of computational hardness, we have to relax the model of complete information, but we propose that the incomplete information setting is more realistic, and many of our hardness results work even with very limited amount of incompleteness in information.\nIn the next chapter, we will see how possible instances of manipulation can be detected\nfor various voting rules.\n205\nChapter 9\nManipulation Detection\nManipulation is a problem of fundamental importance in voting theory in which the voters exercise their votes strategically instead of voting honestly to make an alternative that is more preferred to her, win the election. The classical Gibbard-Satterthwaite theorem shows that there is no strategy-proof voting rule that simultaneously satisfies certain combination of desirable properties. Researchers have attempted to get around the impossibility result in several ways such as domain restriction and computational hardness of manipulation. However, these approaches are known to have fundamental limitations. Since prevention of manipulation seems to be elusive even after substantial research effort, an interesting research direction therefore is detection of manipulation. Motivated by this, we initiate the study of detecting possible instances of manipulation in elections.\nWe formulate two pertinent computational problems in the context of manipulation\ndetection - Coalitional Possible Manipulators (CPM) and Coalitional Possible Manipulators\ngiven Winner (CPMW), where a suspect group of voters is provided as input and we have\nto find whether they can be a potential coalition of manipulators. In the absence of\nany suspect group, we formulate two more computational problems namely Coalitional\nPossible Manipulators Search (CPMS) and Coalitional Possible Manipulators Search given\nWinner (CPMSW). We provide polynomial time algorithms for these problems, for several\npopular voting rules. For a few other voting rules, we show that these problems are\nA preliminary version of the work in this chapter was published as [DMN15a]: Palash Dey, Neeldhara Misra, and Y. Narahari. Detecting possible manipulators in elections. In Proc. 2015 International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2015, Istanbul, Turkey, May 4-8, 2015, pages 1441-1450, 2015.\n206\nNP-complete. We observe that detecting possible instances of manipulation may be easy\neven when the actual manipulation problem is computationally intractable, as seen for\nexample, in the case of the Borda voting rule."
    }, {
      "heading" : "9.1 Introduction",
      "text" : "A basic problem with voting rules is that the voters may vote strategically instead of voting honestly, leading to the selection of a candidate which is not the actual winner. We call a candidate actual winner if it wins the election when every voter votes truthfully. This phenomenon of strategic voting is called manipulation in the context of voting. The GibbardSatterthwaite (G-S) theorem [Gib73, Sat75] proves that manipulation is unavoidable for any unanimous and non-dictatorial voting rule if we have at least three candidates. A voting rule is called unanimous if whenever any candidate is most preferred by all the voters, such a candidate is the winner. A voting rule is called non-dictatorial if there does not exist any voter whose most preferred candidate is always the winner irrespective of the votes of other voters. The problem of manipulation is particularly relevant for multiagent systems since agents have computational power to determine strategic votes. There have been several attempts to bypass the impossibility result of the G-S theorem.\nEconomists have proposed domain restriction as a way out of the impossibility implications of the G-S theorem. The G-S theorem assumes all possible preference profiles as the domain of voting rules. In a restricted domain, it has been shown that we can have voting rules that are not vulnerable to manipulation. A prominent restricted domain is the domain of single peaked preferences, in which the median voting rule provides a satisfactory solution [MCWG95]. To know more about other domain restrictions, we refer to [MCWG95, Gae01]. This approach of restricting the domain, however, suffers from the requirement that the social planner needs to know the domain of preference profiles of the voters, which is often impractical."
    }, {
      "heading" : "9.1.1 Related Work",
      "text" : "Researchers in computational social choice theory have proposed invoking computational intractability of manipulation as a possible work around for the G-S theorem. Bartholdi et al. [BO91, BTT89] first proposed the idea of using computational hardness as a barrier\n207\nagainst manipulation. Bartholdi et al. defined and studied the computational problem called manipulation where a set of manipulators have to compute their votes that make their preferred candidate win the election. The manipulators know the votes of the truthful voters and the voting rule that will be used to compute the winner. Following this, a large body of research [NWX11, DKNW11, XZP+09, XCP10, CSL07, OEH11, EL05, FHH13, NW13, GKNW13, OE12, FHH10, ZLR11, DN14, FRRS14, EE12] shows that the manipulation problem is NPcomplete for many voting rules. However, Procaccia et al. [PR06, PR07] showed average case easiness of manipulation assuming junta distribution over the voting profiles. Friedgut et al. [FKN08] showed that any neutral voting rule which is sufficiently far from being dictatorial is manipulable with non-negligible probability at any uniformly random preference profile by a uniformly random preference. The above result holds for elections with three candidates only. A voting rule is called neutral if the names of the candidates are immaterial. Isaksson et al. [IKM12] generalize the above result to any number of candidates which has been further generalized to all voting rules which may not be neutral by Mossel and Racz in [MR15]. Walsh [Wal10] empirically shows ease of manipulating an STV (single transferable vote) election – one of the very few voting rules where manipulation, even by one voter, is NP-complete [BO91]. In addition to the results mentioned above, there exist many other results in the literature that emphasize the weakness of considering computational complexity as a barrier against manipulation [CS06, XC08b, XC08a, FP10, Wal11a]. Hence, the barrier of computational hardness is ineffective against manipulation in many settings."
    }, {
      "heading" : "9.1.2 Motivation",
      "text" : "In a situation where multiple attempts for prevention of manipulation fail to provide a fully satisfactory solution, detection of manipulation is a natural next step of research. There have been scenarios where establishing the occurrence of manipulation is straightforward, by observation or hindsight. For example, in sport, there have been occasions where the very structure of the rules of the game have encouraged teams to deliberately lose their matches. Observing such occurrences in, for example, football (the 1982 FIFA World Cup football match played between West Germany and Austria) and badminton (the quarter-final match between South Korea and China in the London 2012 Olympics), the relevant authorities have subsequently either changed the rules of the game (as with football) or disqualified the teams in question (as with the badminton example). The importance of detecting manipulation lies in the potential for implementing corrective measures in the future. For reasons that will be\n208\nevident soon, it is not easy to formally define the notion of manipulation detection. Assume that we have the votes from an election that has already happened. A voter is potentially a manipulator if there exists a preference ≻, different from the voter’s reported preference, which is such that the voter had an “incentive to deviate” from the ≻. Specifically, suppose the candidate who wins with respect to this voter’s reported preference is preferred (in ≻) over the candidate who wins with respect to ≻. In such a situation, ≻ could potentially be the voter’s truthful preference, and the voter could be refraining from being truthful because an untruthful vote leads to a more favorable outcome with respect to ≻. Note that we do not (and indeed, cannot) conclusively suggest that a particular voter has manipulated an election. This is because the said voter can always claim that she voted truthfully; since her actual preference is only known to her, there is no way to prove or disprove such a claim. Therefore, we are inevitably limited to asking only whether or not a voter has possibly manipulated an election.\nDespite this technical caveat, it is clear that efficient detection of manipulation, even if it is only possible manipulation, is potentially of interest in practice. We believe that, the information whether a certain group of voters have possibly manipulated an election or not would be useful to social planners. For example, the organizers of an event, say London 2012 Olympics, may be interested to have this information. Also, in settings where data from many past elections (roughly over a fixed set of voters) is readily available, it is conceivable that possible manipulation could serve as suggestive evidence of real manipulation. Aggregate data about possible manipulations, although formally inconclusive, could serve as an important evidence of real manipulation, especially in situations where the instances of possible manipulation turn out to be statistically significant. Thus, efficient detection of possible manipulation would provide an useful input to a social planner for future elections. We remark that having a rich history is typically not a problem, particularly for AI related applications, since the data generated from an election is normally kept for future requirements (for instance, for data mining or learning). For example, several past affirmatives for possible manipulation is one possible way of formalizing the notion of erratic past behavior. Also, applications where benefit of doubt may be important, for example, elections in judiciary systems, possible manipulation detection may be useful. Thus the computational problem of detecting possible manipulation is of definite interest in many settings.\n209"
    }, {
      "heading" : "9.1.3 Our Contribution",
      "text" : "The novelty of this work is in initiating research on detection of possible manipulators in elections. We formulate four pertinent computational problems in this context:\n⊲ CPM: In the coalitional possible manipulators problem, we are interested in whether or\nnot a given subset of voters is a possible coalition of manipulators [Definition 9.4].\n⊲ CPMW: The coalitional possible manipulators given winner is the CPM problem with the\nadditional information about who the winner would have been if the possible manipulators had all voted truthfully [Definition 9.2].\n⊲ CPMS, CPMSW: In CPMS (Coalitional Possible Manipulators Search), we want to know,\nwhether there exists any coalition of possible manipulators of a size at most k [Definition 9.6]. Similarly, we define CPMSW (Coalitional Possible Manipulators Search given Winner) [Definition 9.5].\nOur specific findings are as follows.\n⊲ We show that all the four problems above, for scoring rules and the maximin voting\nrule, are in P when the coalition size is one [Theorem 9.1 and 9.4].\n⊲ We prove that all the four problems, for any coalition size, are in P for a wide class\nof scoring rules which include the Borda voting rule [Theorem 9.2, Theorem 9.3 and Corollary 9.1].\n⊲ We show that, for the Bucklin voting rule [Theorem 9.6], both the CPM and CPMW\nproblems are in P, for any coalition size. The CPMS and CPMSW problems for the Bucklin voting rule are also in P, when we have maximum possible coalition size k = O(1).\n⊲ We show that both the CPM and the CPMW problems are NP-complete for the STV\nvoting rule [Theorem 9.7 and Corollary 9.2], even for a coalition of size one. We also prove that the CPMW problem is NP-complete for maximin voting rule [Theorem 9.5], for a coalition of size two.\nWe observe that all the four problems are computationally easy for many voting rules that we study in this work. This can be taken as a positive result. The results for the CPM and the CPMW problems are summarized in Table 9.1.\n210"
    }, {
      "heading" : "9.2 Problem Formulation",
      "text" : "Consider an election that has already happened in which all the votes are known and thus the winner x ∈ C is also known. We call the candidate x the current winner of the election. The authority may suspect that the voters belonging to a subset M ⊂ V of the set of voters have formed a coalition among themselves and manipulated the election by voting non-truthfully. The authority believes that other voters who do not belong to M, have voted truthfully. We denote |M|, the size of the coalition, by k. Suppose the authority has auxiliary information, may be from some other sources, which says that the actual winner should have been some candidate y ∈ C other than x. We call a candidate actual winner if it wins the election where all the voters vote truthfully. This means that the authority thinks that, had the voters in M voted truthfully, the candidate y would have been the winner. We remark that there are practical situations, for example, 1982 FIFA World cup or 2012 London Olympics, where the authority knows the actual winner. This situation is formalized below.\nDefinition 9.1. Let r be a voting rule, and (≻i)i∈V be a voting profile of a set V of n voters. Let x be the winning candidate with respect to r for this profile. For a candidate y 6= x, M ⊂ V is called a coalition of possible manipulators against y with respect to r if there exists a |M|-voters’ profile (≻′j)j∈M ∈ L(C)|M| such that x ≻′j y, ∀j ∈ M, and further, r((≻j)j∈V\\M, (≻′i)i∈M) = y.\nUsing the notion of coalition of possible manipulators, we formulate a computational problem called Coalitional Possible Manipulators given Winner (CPMW) as follows. Let r be any voting rule.\nDefinition 9.2. (r–CPMW Problem) Given a preference profile (≻i)i∈V of a set of voters V over a set of candidates C, a subset of voters\n211"
    }, {
      "heading" : "M ⊂ V, and a candidate y, determine if M is a coalition of possible manipulators against y with",
      "text" : "respect to r.\nIn the CPMW problem, the actual winner is given in the input. However, it may very well happen that the authority does not have any other information to guess the actual winner – the candidate who would have won the election had the voters in M voted truthfully. In this situation, the authority is interested in knowing whether there is a |M|-voter profile which along with the votes in V \\ M makes some candidate y ∈ C the winner who is different from the current winner x ∈ C and all the preferences in the |M|-voters’ profile prefer x to y. If such a |M|-voter profile exists for the subset of voters M, then we call M a coalition of possible manipulators and the corresponding computational problem is called Coalitional Possible Manipulators (CPM). These notions are formalized below.\nDefinition 9.3. Let r be a voting rule, and (≻i)i∈V be a voting profile of a set V of n voters. A subset of voters M ⊂ V is called a coalition of possible manipulators with respect to r if M is a coalition of possible manipulators against some candidate y with respect to r.\nDefinition 9.4. (r–CPM Problem) Given a preference profile (≻i)i∈V of a set of voters V over a set of candidates C, and a subset of voters M ⊂ V, determine if M is a coalition of possible manipulators with respect to r.\nIn both the CPMW and CPM problems, a subset of voters which the authority suspect to be a coalition of manipulators, is given in the input. However, there can be situations where there is no specific subset of voters to suspect. In those scenarios, it may still be useful to know, what are the possible coalition of manipulators of size less than some number k. Towards that end, we extend the CPMW and CPM problems to search for a coalition of potential possible manipulators and call them Coalitional Possible Manipulators Search given Winner (CPMSW) and Coalitional Possible Manipulators Search (CPMS) respective.\nDefinition 9.5. (r–CPMSW Problem) Given a preference profile (≻i)i∈V of a set of voters V over a set of candidates C, a candidate y, and an integer k, determine whether there exists any M ⊂ V with |M| 6 k such that M is a coalition of possible manipulators against y.\nDefinition 9.6. (r–CPMS Problem) Given a preference profile (≻i)i∈V of a set of voters V over a set of candidates C, and an integer k, determine whether there exists any M ⊂ V with |M| 6 k such that M is a coalition of possible manipulators.\n212"
    }, {
      "heading" : "9.2.1 Discussion",
      "text" : "The CPMW problem may look very similar to the manipulation problem [BTT89, CSL07] – in both the problems a set of voters try to make a candidate winner. However, in the CPMW problem, the actual winner must be less preferred to the current winner in every manipulator’s vote. Although it may look like a subtle difference, it changes the nature and complexity theoretic behavior of the problem completely. For example, we show that all the four problems have an efficient algorithm for a large class of voting rules that includes the Borda voting rule, for any coalition size. However, the manipulation problem for the Borda voting rule is NP-complete, even when we have at least two manipulators [DKNW11, BNW11]. Another important difference is that the manipulation problem, in contrast to the problems studied in this work, does not take care of manipulators’ preferences. We believe that there does not exist any formal reduction between the CPMW problem and the manipulation problem.\nOn the other hand, the CPMS problem is similar to the margin of victory problem defined by Xia [Xia12], where also we are looking for changing the current winner by changing at most some k number of votes, which in turn identical to the destructive bribery problem [FHH09]. Whereas, in the CPMS problem, the vote changes can occur in a restricted fashion. An important difference between the two problems is that the margin of victory problem has the hereditary property which the CPMS problem does not possess (there is no coalition of possible manipulators of size n in any election for all the common voting rules). These two problems do not seem to have any obvious complexity theoretic implications.\nNow we explore the connection among the four problems that we study here. Notice that, a polynomial time algorithm for the CPM and the CPMW problems gives us a polynomial time algorithm for the CPMS and the CPMSW problems for any maximum possible coalition size k = O(1). Also, observe that, a polynomial time algorithm for the CPMW (respectively CPMSW) problem implies a polynomial time algorithm for the CPM (respectively CPMS) problem. Hence, we have the following observations.\nObservation 9.1. For every voting rule, if the maximum possible coalition size k = O(1), then,\nCPMW ∈ P ⇒ CPM,CPMSW,CPMS ∈ P\nObservation 9.2. For every voting rule,\nCPMSW ∈ P ⇒ CPMS ∈ P\n213"
    }, {
      "heading" : "9.3 Results for the CPMW, CPM, CPMSW, and CPMS Prob-",
      "text" : "lems\nIn this section, we present our algorithmic results for the CPMW, CPM, CPMSW, and CPMS problems for various voting rules."
    }, {
      "heading" : "9.3.1 Scoring Rules",
      "text" : "Below we have certain lemmas which form a crucial ingredient of our algorithms. To begin with, we define the notion of a manipulated preference. Let r be a scoring rule and ≻:= (≻i ,≻−i) be a voting profile of n voters. Let ≻′i be a preference such that\nr(≻) >′i r(≻′i,≻−i)\nThen we say that ≻′i is a (≻, i)-manipulated preference with respect to r. We omit the reference to r if it is clear from the context.\nLemma 9.1. Let r be a scoring rule and ≻:= (≻i,≻−i) be a voting profile of n voters. Let a and b be two candidates such that score≻−i(a) > score≻−i(b), and let ≻′i be (≻, i)-manipulated preference where a precedes b:\n≻′i:= · · · > a > · · · > b > · · ·"
    }, {
      "heading" : "If a and b are not winners with respect to either (≻′i,≻−i) or ≻, then the preference ≻′′i obtained",
      "text" : "from ≻′i by interchanging a and b is also (≻, i)-manipulated.\nProof. Let x := r(≻′i,≻−i). If suffices to show that x continues to win in the proposed profile (≻′′i ,≻−i). To this end, it is enough to argue the scores of a and b with respect to x. First, consider the score of b in the new profile:\nscore(≻′′i ,≻−i)(b) = score≻′′i (b) + score≻−i(b)\n< score≻′i(a) + score≻−i(a) = score(≻′i,≻−i)(a) 6 score(≻′i,≻−i)(x) = score(≻′′i ,≻−i)(x)\n214\nThe second line uses the fact that score≻′′i (b) = score≻′i(a) and score≻−i(b) < score≻−i(a). The fourth line comes from the fact that x is the winner and the last line follows from the fact that the position of x is same in both profiles. Similarly, we have the following argument for the score of a in the new profile (the second line below simply follows from the definition of scoring rules).\nscore(≻′′i ,≻−i)(a) = score≻′′i (a) + score≻−i(a)\n6 score≻′i(a) + score≻−i(a) = score(≻′i,≻−i)(a) 6 score(≻′i,≻−i)(x) = score(≻′′i ,≻−i)(x)\nSince the tie breaking rule is according to some predefined fixed order ≻t∈ L(C) and the candidates tied with winner in (≻′′i ,≻−i) also tied with winner in (≻′i,≻−i), we have the following,\nr(≻) >′′i r(≻′′i ,≻−i)\nWe now show that, if there is some (≻, i)-manipulated preference with respect to a scoring rule r, then there exists a (≻, i)-manipulated preference with a specific structure.\nLemma 9.2. Let r be a scoring rule and ≻:= (≻i,≻−i) be a voting profile of n voters. If there is some (≻, i)-manipulated preference with respect to r, then there also exists a (≻, i)-manipulated preference ≻′i where the actual winner y immediately follows the current winner x:\n≻′i:= · · · > x > y > · · ·\nand the remaining candidates are in nondecreasing ordered of their scores from ≻−i.\nProof. Let ≻′′ be a (≻, i)-manipulated preference with respect to r. Let x := r(≻),y := r(≻′′ ,≻−i). From Lemma 9.1, without loss of generality, we may assume that, all candidates except x,y are in nondecreasing order of score≻−i(.) in the preference ≻′′. If ≻′′i := · · · ≻ x ≻ · · · ≻ y ≻ · · · ≻ · · · , we define ≻′i:= · · · ≻ x ≻ y ≻ · · · ≻ · · · from ≻′′i where y is moved to the position following x and the position of the candidates in between x and y in ≻′′i is\n215\ndeteriorated by one position each. The position of the rest of the candidates remain same in both ≻′′i and ≻′i. Now we have following,\nscore(≻′i,≻−i)(y) = score≻′i(y) + score≻−i(y)\n> score≻′′i (y) + score≻−i(y) = score(≻′′i ,≻−i)(y)\nWe also have,\nscore(≻′i,≻−i)(a) 6 score(≻′′i ,≻−i)(a), ∀a ∈ C \\ {y}\nSince the tie breaking rule is according to some predefined order ≻t∈ L(C), we have the following,\nr(≻) >′i r(≻′,≻−i)\nUsing Lemma 9.1 and 9.2, we now present our results for the scoring rules.\nTheorem 9.1. The CPMW, CPM, CPMSW, and CPMS problems for scoring rules are in P for a coalition of size 1 (that is, the coalition size k = 1).\nProof. From Observation 9.1, it is enough to give a polynomial time algorithm for the CPMW problem. So consider the CPMW problem. We are given the actual winner y and we compute the current winner x with respect to r. Let ≻[j] be a preference where x and y are in positions j and (j + 1) respectively, and the rest of the candidates are in nondecreasing order of the score that they receive from ≻−i. For j ∈ {1, 2, . . . ,m− 1}, we check if y wins with the profile (≻−i,≻[j]). If we are successful with at least one j we report YES, otherwise we say NO. The correctness follows from Lemma 9.2. Thus we have a polynomial time algorithm for CPMW when k = 1.\nNow we present our results for the CPMW and the CPM problems when k > 1. If m = O(1), then both the CPMW and the CPM problems for any anonymous and efficient voting rule r can be solved in polynomial time by iterating over all possible (\nm!+k−1 m!\n)\nways\nthe manipulators can have actual preferences. A voting rule is called efficient if winner determination under it is in P.\n216\nTheorem 9.2. For scoring rules with α1−α2 6 αi−αi+1, ∀i, the CPMW and the CPM problems are in P, for any coalition size.\nProof. We provide a polynomial time algorithm for the CPMW problem in this setting. Let x be the current winner and y be the given actual winner. Let M be the given subset of voters. Let ((≻i)i∈M, (≻j)j∈V\\M) be the reported preference profile. Without loss of generality, we assume that x is the most preferred candidate in every ≻i, i ∈ M. Let us define ≻′i, i ∈ M, by moving y to the second position in the preference ≻i. In the profile ((≻′i)i∈M, (≻j)j∈V\\M), the winner is either x or y since only y’s score has increased. We claim that M is a coalition of possible manipulators with respect to y if and only if y is the winner in preference profile ((≻′i)i∈M, (≻j)j∈V\\M). This can be seen as follows. Suppose there exist preferences ≻′′i , with x ≻′′i y, i ∈ M, for which y wins in the profile ((≻′′i )i∈M, (≻j)j∈V\\M). Now without loss of generality, we can assume that y immediately follows x in all ≻′′i , i ∈ M, and α1 − α2 6 αi − αi+1, ∀i implies that we can also assume that x and y are in the first and second positions respectively in all ≻′′i , i ∈ M. Now in both the profiles, ((≻′i)i∈M, (≻j)j∈V\\M) and ((≻′′i )i∈M, (≻j)j∈V\\M), the score of x and y are same. But in the first profile x wins and in the second profile y wins, which is a contradiction.\nWe now prove a similar result for the CPMSW and CPMS problems.\nTheorem 9.3. For scoring rules with α1 − α2 6 αi − αi+1, ∀i, the CPMSW and the CPMS problems are in P, for any coalition size.\nProof. From Observation 9.2, it is enough to prove that CPMSW ∈ P. Let x be the current winner, y be the given actual winner and s(x) and s(y) be their current respective scores. For each vote v ∈ V, we compute a number ∆(v) = α2 − αj − α1 + αi, where x and y are receiving scores αi and αj respectively from the vote v. Now, we output yes iff there are k votes vi, 1 6 i 6 k such that, ∑k i=1 ∆(vi) > s(x)−s(y), which can be checked easily by sorting the ∆(v)’s in nonincreasing order and checking the condition for the first k ∆(v)’s, where k is the maximum possible coalition size specified in the input. The proof of correctness follows by exactly in the same line of argument as the proof of Theorem 9.2.\nFor the k-approval voting rule, we can solve all the problems easily using max flow. Hence,\nfrom Theorem 9.2 and Theorem 9.3, we have the following result.\nCorollary 9.1. The CPMW, CPM, CPMSW, and CPMS problems for the Borda and k-approval voting rules are in P, for any coalition size.\n217"
    }, {
      "heading" : "9.3.2 Maximin Voting Rule",
      "text" : "For the maximin voting rule, we show that all the four problems are in P, when we have a coalition of size one.\nTheorem 9.4. The CPMW, CPM, CPMSW, and CPMS problems for maximin voting rule are in P for any coalition size k = 1 (for CPMW and CPM) or maximum possible coalition size k = 1 (for CMPWS and CPMS).\nProof. Given a n-voters’ profile ≻∈ L(C)n and a voter vi, let the current winner be x := r(≻) and the given actual winner be y. We will construct ≻′= (≻′i,≻−i), if it exists, such that r(≻) >′i r(≻′) = y, thus deciding whether vi is a possible manipulator or not. Now, the maximin score of x and y in the profile≻′ can take one of values from the set {score≻−i(x)±1} and {score≻−i(y)± 1}. The algorithm is as follows. We first guess the maximin score of x and y in the profile ≻′. There are only four possible guesses. Suppose, we guessed that x’s score will decrease by one and y’s score will decrease by one assuming that this guess makes y win. Now notice that, without loss of generality, we can assume that y immediately follows x in the preference≻′i since y is the winner in the profile≻′. This implies that there are onlyO(m) many possible positions for x and y in ≻′i. We guess the position of x and thus the position of y in ≻′i. Let B(x) and B(y) be the sets of candidates with whom x and respectively y performs worst. Now since, x’s score will decrease and y’s score will decrease, we have the following constraint on ≻′i. There must be a candidate each from B(y) and B(x) that will precede x. We do not know a-priori if there is one candidate that will serve as a witness for both B(x) and B(y), or if there separate witnesses. In the latter situation, we also do not know what order they appear in. Therefore we guess if there is a common candidate, and if not, we guess the relative ordering of the distinct candidates from B(x) and B(y). Now we place any candidate at the top position of ≻′i if this action does not make y lose the election. If there are many choices, we prioritize in favor of candidates from B(x) and B(y) — in particular, we focus on the candidates common to B(x) and B(y) if we expect to have a common witness, otherwise, we favor a candidate from one of the sets according to the guess we start with. If still there are multiple choices, we pick arbitrarily. After that we move on to the next position, and do the same thing (except we stop prioritizing explicitly for B(x) and B(y) once we have at least one witness from each set). The other situations can be handled similarly with minor modifications. In this way, if it is able to get a complete preference, then it checks whether vi is a possible manipulator or not using this preference. If yes, then it returns YES. Otherwise, it\n218\ntries other positions for x and y and other possible scores of x and y. After trying all possible guesses, if it cannot find the desired preference, then it outputs NO. Since there are only polynomial many possible guesses, this algorithm runs in a polynomial amount of time. The proof of correctness follows from the proof of Theorem 1 in [BTT89].\nWe now show that the CPMW problem for the maximin voting rule is NP-complete when we have k > 1. Towards that, we use the fact that the unweighted coalitional manipulation (UCM) problem for the maximin voting rule is NP-complete [XZP+09], when we have k > 1. The UCM problem is as follows. Let r be any voting rule.\nDefinition 9.7. (r–UCM Problem) Given a set of manipulators M ⊂ V, a profile of non-manipulators’ vote (≻i)i∈V\\M, and a candidate z ∈ C, we are asked whether there exists a profile of manipulators’ votes (≻′j)j∈M such that r((≻i)i∈V\\M, (≻′j)j∈M) = z. Assume that ties are broken in favor of z.\nWe define a restricted version of the UCM problem called R-UCM as follows.\nDefinition 9.8. (r–R-UCM Problem) This problem is the same as the UCM problem with a given guarantee - let k := |M|. The candidate z loses pairwise election with every other candidate by 4k votes. For any two candidates a,b ∈ C, either a and b ties or one wins pairwise election against the other one by margin of either 2k + 2 or of 4k or of 8k. We denote the margin by which a candidate a defeats b, by d(a,b).\nThe R-UCM problem for the maximin voting rule is NP-complete [XZP+09], when we have\nk > 1.\nTheorem 9.5. The CPMW problem for the maximin voting rule is NP-complete, for a coalition of size at least 2.\nProof. Clearly the CPMW problem for maximin voting rule is NP. We provide a many-one reduction from the R-UCM problem for the maximin voting rule to it. Given a R-UCM problem instance, we define a CPMW problem instance Γ = (C′, (≻′i)i∈V′ ,M′) as follows.\nC′ := C ∪ {w,d1,d2,d3}\nWe define V′ such that d(a,b) is the same as the R-UCM instance, for all a,b ∈ C and d(d1,w) = 2k + 2,d(d1,d2) = 8k,d(d2,d3) = 8k,d(d3,d1) = 8k. The existence of such\n219\na V′ is guaranteed from Lemma 7.2. Moreover, Lemma 7.2 also ensures that |V′| is O(mc). The votes of the voters in M is w ≻ . . . . Thus the current winner is w. The actual winner is defined to be z. The tie breaking rule is ≻t= w ≻ z ≻ . . . , where z is the candidate whom the manipulators in M want to make winner in the R-UCM problem instance. Clearly this reduction takes polynomial amount of time. Now we show that, M is a coalition of possible manipulators iff z can be made a winner.\nThe if part is as follows. Let ≻i, i ∈ M be the votes that make z win. We can assume that z is the most preferred candidate in all the preferences ≻i, i ∈ M. Now consider the preferences for the voters in M is follows.\n≻′i:= d1 ≻ d2 ≻ d3 ≻ w ≻i, i ∈ M\nThe score of every candidate in C is not more than z. The score of z is −3k. The score of w is −3k−2 and the scores of d1,d2, and d3 are less than −3k. Hence,M is a coalition of possible manipulators with the actual preferences ≻′i:= d1 ≻ d2 ≻ d3 ≻ w ≻i, i ∈ M. The only if part is as follows. Suppose M is a coalition of possible manipulators with actual preferences ≻′i, i ∈ M. Consider the preferences ≻′i, i ∈ M, but restricted to the set C only. Call them ≻i, i ∈ M. We claim that ≻i, i ∈ M with the votes from V makes z win the election. If not then, there exists a candidate, say a ∈ C, whose score is strictly more than the score of z - this is so because the tie breaking rule is in favor of z. But this contradicts the fact that z wins the election when the voters in M vote ≻′i, i ∈ M along with the votes from V′."
    }, {
      "heading" : "9.3.3 Bucklin Voting Rule",
      "text" : "In this subsection, we design polynomial time algorithms for both the CPMW and the CPM problems for the Bucklin voting rule. Again, we begin by showing that if there are profiles witnessing manipulation, then there exist profiles that do so with some additional structure, which we will exploit subsequently in our algorithm.\nLemma 9.3. Consider a preference profile (≻i)i∈V, where x is the winner with respect to the Bucklin voting rule. Suppose a subset of votersM ⊂ V forms a coalition of possible manipulators. Let y be the actual winner. Then there exist preferences (≻′i)i∈M such that y is a Bucklin winner in ((≻i)i∈V\\M, (≻′i)i∈M), and further:\n220\n1. y immediately follows x in each ≻′i.\n2. The rank of x in each ≻′i is in one of the following - first, b(y) − 1, b(y), b(y) + 1, where b(y) be the Bucklin score of y in ((≻i)i∈V\\M, (≻′i)i∈M).\nProof. From Definition 9.3, y’s rank must be worse than x’s rank in each ≻′i. We now exchange the position of y with the candidate which immediately follows x in ≻′i. This process does not decrease Bucklin score of any candidate except possibly y’s, and x’s score does not increase. Hence y will continue to win and thus ≻′i satisfies the first condition. Now to begin with, we assume that ≻′i satisfies the first condition. If the position of x in ≻′i is b(y)−1 or b(y), we do not change it. If x is above b(y)−1 in ≻′i, then move x and y at the first and second positions respectively. Similarly if x is below b(y) + 1 in ≻′i, then move x and y at the b(y)+1 and b(y)+2 positions respectively. This process does not decrease score of any candidate except y because the Bucklin score of x is at least b(y). The transformation cannot increase the score y since its position has only been improved. Hence y continues to win and thus ≻′i satisfies the second condition.\nLemma 9.3 leads us to the following theorem.\nTheorem 9.6. The CPMW problem and the CPM problems for Bucklin voting rule are in P for any coalition of size. Therefore, by Observation 9.1, the CPMSW and the CPMS problems are in P when the maximum coalition size k = O(1).\nProof. Observation 9.1 says that it is enough to prove that the CPMW problem is in P. Let x be the current winner and y be the given actual winner. For any final Bucklin score b(y) of y, there are polynomially many possibilities for the positions of x and y in the profile of ≻i, i ∈ M, since Bucklin voting rule is anonymous. Once the positions of x and y is fixed, we try to fill the top b(y) positions of each ≻′i - place a candidate in an empty position above b(y) in any ≻′i if doing so does not make y lose the election. If we are able to successfully fill the top b(y) positions of all ≻′i for all i ∈ M, then M is a coalition of possible manipulators. If the above process fails for all possible above mentioned positions of x and y and all possible guesses of b(y), then M is not a coalition of possible manipulators. Clearly the above algorithm runs in poly(m,n) time.\nThe proof of correctness is as follows. If the algorithm outputs that M is a coalition of possible manipulators, then it actually has constructed ≻′i for all i ∈ M with respect to which they form a coalition of possible manipulators. On the other hand, if they form a coalition of\n221\npossible manipulators, then Lemma 9.3 ensures that our algorithm explores all the sufficient positions of x and y in ≻′i for all i ∈ M. Now if M is a possible coalition of manipulators, then the corresponding positions for x and y have also been searched. Our greedy algorithm must find it since permuting the candidates except x and z which are ranked above b(y) in ≻′i cannot stop y to win the election since the Bucklin score of other candidates except y is at least b(y)."
    }, {
      "heading" : "9.3.4 STV Voting Rule",
      "text" : "Next we prove that the CPMW and the CPM problems for STV rule is NP-complete. To this end, we reduce from the Exact Cover by 3-Sets Problem (X3C), which is known to be NP-complete [GJ79]. The X3C problem is as follows.\nDefinition 9.9. (X3C Problem) Given a set S of cardinality n and m subsets S1, S2, . . . , Sm ⊂ S with |Si| = 3, ∀i = 1, . . . ,m, does there exist an index set I ⊆ {1, . . . ,m} with |I| = |S|\n3 such that ∪i∈ISi = S.\nTheorem 9.7. The CPM problem for STV rule is NP-complete even for a coalition of size 1.\nProof. Clearly the problem is NP. To show NP hardness, we show a many-one reduction from the X3C problem to it. The reduction is analogous to the reduction given in [BO91]. Given an X3C instance, we construct an election as follows. The unspecified positions can be filled in any arbitrary way. The candidate set is as follows.\nC = {x,y} ∪ {a1, . . . ,am} ∪ {a1, . . . ,am} ∪ {b1, . . . ,bm} ∪ {b1, . . . ,bm} ∪ {d0, . . . ,dn} ∪ {g1, . . . , gm}\nThe votes are as follows.\n⊲ 12m votes for y ≻ x ≻ . . .\n⊲ 12m − 1 votes for x ≻ y ≻ . . .\n⊲ 10m + 2n 3 votes for d0 ≻ x ≻ y ≻ . . .\n⊲ 12m − 2 votes for di ≻ x ≻ y ≻ . . . , ∀i ∈ [n]\n⊲ 12m votes for gi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n222\n⊲ 6m + 4i − 5 votes for bi ≻ bi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n⊲ 2 votes for bi ≻ dj ≻ x ≻ y ≻ . . . , ∀i ∈ [m], ∀j ∈ Si\n⊲ 6m + 4i − 1 votes for bi ≻ bi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n⊲ 2 votes for bi ≻ d0 ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n⊲ 6m + 4i − 3 votes for ai ≻ gi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n⊲ 1 vote for ai ≻ bi ≻ gi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n⊲ 2 votes for ai ≻ ai ≻ gi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n⊲ 6m + 4i − 3 votes for ai ≻ gi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n⊲ 1 vote for ai ≻ bi ≻ gi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\n⊲ 2 votes for ai ≻ ai ≻ gi ≻ x ≻ y ≻ . . . , ∀i ∈ [m]\nThe tie breaking rule is ≻t= · · · ≻ x. The vote of v is x ≻ · · · . We claim that v is a possible manipulator iff the X3C is a yes instance. Notice that, of the first 3m candidates to be eliminated, 2m of them are a1, . . . ,am and a1, . . . ,am. Also exactly one of bi and bi will be eliminated among the first 3m candidates to be eliminated because if one of bi, bi then the other’s score exceeds 12m. We show that the winner is either x or y irrespective of the vote of one more candidate. Let J := {j : bj is eliminated before bj}. If J is an index of set cover then the winner is y. This can be seen as follows. Consider the situation after the first 3m eliminations. Let i ∈ Sj for some j ∈ J. Then bj has been eliminated and thus the score of di is at least 12m. Since J is an index of a set cover, every di’s score is at least 12m. Notice that bj has been eliminated for all j /∈ J. Thus the revised score of d0 is at least 12m. After the first 3m eliminations, the remaining candidates are x,y, {di : i ∈ [n]}, {gi : i ∈ [m]}, {bj : j /∈ J}, {bj : j ∈ J}. All the remaining candidates except x has score at least 12m and x’s score is 12m−1. Hence x will be eliminated next which makes y’s score at least 24m−1. Next di’s will get eliminated which will in turn make y’s score (12n+36)m−1. At this point gi’s score is at most 32m. Also all the remaining bi and bi’s score is at most 32m. Since each of the remaining candidate’s scores gets transferred to y once they are eliminated, y is the winner.\nNow we show that, if J is not an index of set cover then the winner is x. This can be seen as follows. If |J| > n 3 , then the number of bj that gets eliminated in the first 3m iterations\n223\nis less than m − n 3 . This makes the score of d0 at most 12m − 2. Hence d0 gets eliminated before x and all its scores gets transferred to x. This makes the elimination of x impossible before y and makes x the winner of the election.\nIf |J| 6 n 3 and there exists an i ∈ S that is not covered by the corresponding set cover, then di gets eliminated before x with a score of 12m − 2 and its score gets transferred to x. This makes x win the election.\nHence y can win iff X3C is a yes instance. Also notice that if y can win the election, then\nit can do so with the voter v voting a preference like · · · ≻ x ≻ y ≻ · · · .\nFrom the proof of the above theorem, we have the following corollary by specifying y as\nthe actual winner for the CPMW problem.\nCorollary 9.2. The CPMW problem for STV rule is NP-complete even for a coalition of size 1."
    }, {
      "heading" : "9.4 Conclusion",
      "text" : "In this work, we have initiated a promising research direction for detecting possible instances of manipulation in elections. We have proposed the notion of possible manipulation and explored several concrete computational problems, which we believe to be important in the context of voting theory. These problems involve identifying if a given set of voters are possible manipulators (with or without a specified candidate winner). We have also studied the search versions of these problems, where the goal is to simply detect the presence of possible manipulation with the maximum coalition size. We believe there is theoretical as well as practical interest in studying the proposed problems. We have provided algorithms and hardness results for these problems for many common voting rules. It is our conviction that both the problems that we have studied here have initiated an interesting research direction with significant promise and potential for future work.\nIn the next chapter of the thesis, we study another interesting form of election control\ncalled bribery.\n224\nChapter 10\nFrugal Bribery\nBribery in elections is an important problem in computational social choice theory. We introduce and study two important special cases of the classical $BRIBERY problem, namely, FRUGAL-BRIBERY and FRUGAL-$BRIBERY where the briber is frugal in nature. By this, we mean that the briber is only able to influence voters who benefit from the suggestion of the briber. More formally, a voter is vulnerable if the outcome of the election improves according to her own preference when she accepts the suggestion of the briber. In the FRUGAL-BRIBERY problem, the goal is to make a certain candidate win the election by changing only the vulnerable votes. In the FRUGAL-$BRIBERY problem, the vulnerable votes have prices and the goal is to make a certain candidate win the election by changing only the vulnerable votes, subject to a budget constraint. We further formulate two natural variants of the FRUGAL-$BRIBERY problem namely UNIFORM-FRUGAL-$BRIBERY and NONUNIFORM-FRUGAL-$BRIBERY where the prices of the vulnerable votes are, respectively, all the same or different.\nWe observe that, even if we have only a small number of candidates, the problems are\nintractable for all voting rules studied here for weighted elections, with the sole exception\nof the FRUGAL-BRIBERY problem for the plurality voting rule. In contrast, we have poly-\nnomial time algorithms for the FRUGAL-BRIBERY problem for plurality, veto, k-approval,\nk-veto, and plurality with runoff voting rules for unweighted elections. However, the\nFRUGAL-$BRIBERY problem is intractable for all the voting rules studied here barring the\nA preliminary version of the work in this chapter was published as [DMN16b]: Palash Dey, Neeldhara\nMisra, and Y. Narahari. Frugal bribery in voting. In Proc. Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 24662472, 2016.\n226\nplurality and the veto voting rules for unweighted elections. These intractability results\ndemonstrate that bribery is a hard computational problem, in the sense that several spe-\ncial cases of this problem continue to be computationally intractable. This strengthens the\nview that bribery, although a possible attack on an election in principle, may be infeasible\nin practice."
    }, {
      "heading" : "10.1 Introduction",
      "text" : "Activities that try to influence voter opinions, in favor of specific candidates, are very common during the time that an election is in progress. For example, in a political election, candidates often conduct elaborate campaigns to promote themselves among a general or targeted audience. Similarly, it is not uncommon for people to protest against, or rally for, a national committee or court that is in the process of approving a particular policy. An extreme illustration of this phenomenon is bribery — here, the candidates may create financial incentives to sway the voters. Of course, the process of influencing voters may involve costs even without the bribery aspect; for instance, a typical political campaign or rally entails considerable expenditure.\nAll situations involving a systematic attempt to influence voters usually have the following aspects: an external agent, a candidate that the agent would like to be the winner, a budget constraint, a cost model for a change of vote, and knowledge of the existing election. The formal computational problem that arises from these inputs is the following: is it possible to make a distinguished candidate win the election in question by incurring a cost that is within the budget? This question, with origins in [FHH06, FHH09, FHHR09], has been subsequently studied intensely in computational social choice literature. In particular, bribery has been studied under various cost models, for example, uniform price per vote which is known as $BRIBERY [FHH06], nonuniform price per vote [Fal08], nonuniform price per shift of the distinguished candidate per vote which is called SHIFT BRIBERY, nonuniform price per swap of candidates per vote which is called SWAP BRIBERY [EFS09]. A closely related problem known as campaigning has been studied for various vote models, for example, truncated ballots [BFLR12], soft constraints [PRV13], CP-nets [DK16], combinatorial domains [MPVR12] and probabilistic lobbying [BEF+14]. The bribery problem has also been studied under voting rule uncertainty [EHH14]. Faliszewski et al. [FRRS14] study the complexity of bribery in Bucklin and Fallback voting rules. Xia [Xia12] studies destructive bribery, where the goal\n227\nof the briber is to change the winner by changing minimum number of votes. Dorn et al. [DS12] studies the parameterized complexity of the SWAP BRIBERY problem and Bredereck et al. [BCF+14b] explores the parameterized complexity of the SHIFT BRIBERY problem for a wide range of parameters. We recall again that the costs and the budgets involved in all the bribery problems above need not necessarily correspond to actual money traded between voters and candidates. They may correspond to any cost in general, for example, the amount of effort or time that the briber needs to spend for each voter."
    }, {
      "heading" : "10.1.1 Motivation",
      "text" : "In this work, we propose an effective cost model for the bribery problem. Even the most general cost models that have been studied in the literature fix absolute costs per voter-candidate combination, with no specific consideration to the voters’ opinions about the current winner and the distinguished candidate whom the briber wants to be the winner. In our proposed model, a change of vote is relatively easier to effect if the change causes an outcome that the voter would find desirable. Indeed, if the currently winning candidate is, say, a, and a voter is (truthfully) promised that by changing her vote from c ≻ d ≻ b ≻ a to d ≻ b ≻ c ≻ a, the winner of the election would change from a to d, then this is a change that the voter is likely to be happy to make. While the change does not make her most favorite candidate win the election, it does improve the result from her point of view. Thus, given the circumstances (namely that of her least favorite candidate winning the election), the altered vote serves the voter better than the original one.\nWe believe this perspective of voter influence is an important one to study. The cost of a change of vote is proportional to the nature of the outcome that the change promises — the cost is low or nil if the change results in a better outcome with respect to the voter’s original ranking, and high or infinity otherwise. A frugal agent only approaches voters of the former category, thus being able to effectively bribe with minimal or no cost. Indeed the behavior of agents in real life is often frugal. For example, consider campaigners in favor of a relatively smaller party in a political election. They may actually target only vulnerable voters due to lack of human and other resources they have at their disposal.\nMore formally, let c be the winner of an election and p (other than c) the candidate whom the briber wishes to make the winner of the election. Now the voters who prefer c to p will be reluctant to change their votes, and we call these votes non-vulnerable with respect to p — we do not allow these votes to be changed by the briber, which justifies the frugal nature of\n228\nthe briber. On the other hand, if a voter prefers p to c, then it may be very easy to convince her to change her vote if doing so makes p win the election. We name these votes vulnerable with respect to p. When the candidate p is clear from the context, we simply call these votes non-vulnerable and vulnerable, respectively.\nThe computational problem is to determine whether there is a way to make a candidate p win the election by changing only those votes that are vulnerable with respect to p. We call this problem FRUGAL-BRIBERY. Note that there is no cost involved in the FRUGAL-BRIBERY problem — the briber does not incur any cost to change the votes of the vulnerable votes. We also extend this basic model to a more general setting where each vulnerable vote has a certain nonnegative integer price which may correspond to the effort involved in approaching these voters and convincing them to change their votes. We also allow for the specification of a budget constraint, which can be used to enforce auxiliary constraints. This leads us to define the FRUGAL-$BRIBERY problem, where we are required to find a subset of vulnerable votes with a total cost that is within a given budget, such that these votes can be changed in some way to make the candidate p win the election. Note that the FRUGAL-$BRIBERY problem can be either uniform or nonuniform depending on whether the prices of the vulnerable votes are all identical or different. If not mentioned otherwise, the prices of the vulnerable votes will be assumed to be nonuniform. We remind that the briber is not allowed to change the non-vulnerable votes in both the FRUGAL-BRIBERY and the FRUGAL-$BRIBERY problems."
    }, {
      "heading" : "10.1.2 Our Contribution",
      "text" : "Our primary contribution in this work is to formulate and study two important and natural models of bribery which turn out to be special cases of the well studied $BRIBERY problem in elections. Our results show that both the FRUGAL-BRIBERY and the FRUGAL-$BRIBERY problems are intractable for many commonly used voting rules for weighted as well as unweighted elections, barring a few exceptions. These intractability results can be interpreted as an evidence that the bribery in elections is a hard computational problem in the sense that even many of its important and natural special cases continue to be intractable. Thus bribery, although a possible attack on elections in principle, may be practically not viable. From a more theoretical perspective, our intractability results strengthen the existing hardness results for the $BRIBERY problem. On the other hand, our polynomial time algorithms exhibit interesting tractable special cases of the $BRIBERY problem.\n229\nOur Results for Unweighted Elections\nWe have the following results for unweighted elections.\n⊲ The FRUGAL-BRIBERY problem is in P for the k-approval, Bucklin, and plurality with\nrunoff voting rules. Also, the FRUGAL-$BRIBERY problem is in P for the plurality and veto voting rules. In contrast, the FRUGAL-$BRIBERY problem is NP-complete for the Borda, maximin, Copeland, and STV voting rules [Observation 10.3].\n⊲ The FRUGAL-BRIBERY problem is NP-complete for the Borda voting rule [Theorem 10.1].\nThe FRUGAL-$BRIBERY is NP-complete for the k-approval for any constant k > 5 [Theorem 10.2], k-veto for any constant k > 3 [Theorem 10.3], and a wide class of scoring rules [Theorem 10.5] even if the price of every vulnerable vote is either 1 or ∞. Moreover, the UNIFORM-FRUGAL-$BRIBERY is NP-complete for the Borda voting rule even if all the vulnerable votes have a uniform price of 1 and the budget is 2 [Theorem 10.6].\n⊲ The FRUGAL-$BRIBERY problem is in P for the k-approval, Bucklin, and plurality with\nrunoff voting rules when the budget is a constant [Theorem 10.4].\nOur Results for Weighted Elections\nWe have the following results for weighted elections.\n⊲ The FRUGAL-BRIBERY problem is in P for the maximin and Copeland voting rules when\nwe have only three candidates [Observation 10.4], and for the plurality voting rule for any number of candidates [Theorem 10.7].\n⊲ The FRUGAL-BRIBERY problem is NP-complete for the STV [Theorem 10.10], plurality\nwith runoff [Corollary 10.1], and every scoring rule except the plurality voting rule [Observation 10.5] for three candidates. The FRUGAL-$BRIBERY problem is NP-complete for the plurality voting rule for three candidates [Theorem 10.8].\n⊲ When we have only four candidates, the FRUGAL-BRIBERY problem is NP-complete\nfor the maximin [Theorem 10.9], Bucklin [Theorem 10.12], and Copeland [Theorem 10.13] rules.\nWe summarize the results in the Table 10.1.\n230\n231"
    }, {
      "heading" : "10.1.3 Related Work",
      "text" : "The pioneering work of [FHH06] defined and studied the $BRIBERY problem wherein, the input is a set of votes with prices for each vote and the goal is to make some distinguished candidate win the election, subject to a budget constraint of the briber. The FRUGAL-$BRIBERY problem is the $BRIBERY problem with the restriction that the price of every non-vulnerable vote is infinite. Also, the FRUGAL-BRIBERY problem is a special case of the FRUGAL-$BRIBERY problem. Hence, whenever the $BRIBERY problem is computationally easy in a setting, both the FRUGAL-BRIBERY and the FRUGAL-$BRIBERY problems are also computationally easy (see Proposition 10.1 for a more formal proof). However, the $BRIBERY problem is computationally intractable in most of the settings. This makes the study of important special cases such as FRUGAL-BRIBERY and FRUGAL-$BRIBERY, interesting. We note that a notion similar to vulnerable votes has been studied in the context of dominating manipulation by [CWX11]. Hazon et al. [HLK13] introduced and studied PERSUASION and k-PERSUASION problems where an external agent suggests votes to vulnerable voters which are beneficial for the vulnerable voters as well as the external agent. It turns out that the PERSUASION and the k-PERSUASION problems Turing reduce to the FRUGAL-BRIBERY and the FRUGAL-$BRIBERY problems respectively (see Proposition 10.3). Therefore, the polynomial time algorithms we propose in this work imply polynomial time algorithms for the persuasion analog. On the other hand, since the reduction in Proposition 10.3 from PERSUASION to FRUGAL-BRIBERY is a Turing reduction, the existing NP-completeness results for the persuasion problems do not imply NP-completeness results for the corresponding frugal bribery variants. We refer to [RR67] for Turing reductions."
    }, {
      "heading" : "10.2 Problem Definition",
      "text" : "In all the definitions below, r is a fixed voting rule. We define the notion of vulnerable votes as follows. Intuitively, the vulnerable votes are those votes whose voters can easily be persuaded to change their votes since doing so will result in an outcome that those voters prefer over the current one.\nDefinition 10.1. (Vulnerable votes) Given a voting rule r, a set of candidates C, a profile of votes ≻= (≻1, . . . ,≻n), and a distinguished candidate p, we say a vote ≻i is p-vulnerable if p ≻i r(≻).\n232\nRecall that, whenever the distinguished candidate is clear from the context, we drop it from the notation. With the above definition of vulnerable votes, we formally define the FRUGAL-BRIBERY problem as follows. Intuitively, the problem is to determine whether a particular candidate can be made winner by changing only the vulnerable votes.\nDefinition 10.2. (r-FRUGAL-BRIBERY) Given a preference profile ≻= (≻1, . . . ,≻n) over a candidate set C, and a candidate p, determine if there is a way to make p win the election by changing only the vulnerable votes.\nNext we generalize the FRUGAL-BRIBERY problem to the FRUGAL-$BRIBERY problem which involves prices for the vulnerable votes and a budget for the briber. Intuitively, the price of a vulnerable vote v is the cost the briber incurs to change the vote v.\nDefinition 10.3. (r-FRUGAL-$BRIBERY) Let ≻= (≻1, . . . ,≻n) be a preference profile over a candidate set C. We are given a candidate p, a finite budget b ∈ N, and a price function c : [n] −→ N ∪ {∞} such that c(i) = ∞ if ≻i is not a p-vulnerable vote. The goal is to determine if there exist p vulnerable votes ≻i1, . . . ,≻iℓ∈≻ and votes ≻′i1 , . . . ,≻′iℓ∈ L(C) such that:\n(a) the total cost of the chosen votes is within the budget, that is, ∑ℓ\nj=1 c(ij) 6 b, and\n(b) the new votes make the desired candidate win, that is, r(≻[n]\\{i1,...,iℓ},≻′i1, . . . ,≻′iℓ) = p.\nThe special case of the problem when the prices of all the vulnerable votes are the same is called UNIFORM-FRUGAL-$BRIBERY. We refer to the general version as NONUNIFORM-FRUGAL$BRIBERY. If not specified, FRUGAL-$BRIBERY refers to the nonuniform version.\nThe above problems are important special cases of the well studied $BRIBERY problem. Also, the COALITIONAL-MANIPULATION problem [BTT89, CSL07], one of the classic problems in computational social choice theory, turns out to be a special case of the FRUGAL-$BRIBERY problem [see Proposition 10.1].\nFor the sake of completeness, we include the definitions of these problems here.\nDefinition 10.4. (r-$BRIBERY) [FHH09] Given a preference profile ≻= (≻1, . . . ,≻n) over a set of candidates C, a distinguished candidate p, a price function c : [n] −→ N ∪ {∞}, and a budget b ∈ N, determine if there a way to make p win the election.\n233\nDefinition 10.5. (COALITIONAL-MANIPULATION) [BTT89, CSL07] Given a preference profile ≻t= (≻1, . . . ,≻n) of truthful voters over a set of candidates C, an integer ℓ, and a distinguished candidate p, determine if there exists a ℓ voter preference profile ≻ℓ such that the candidate p wins uniquely (does not tie with any other candidate) in the profile (≻t,≻ℓ).\nThe following proposition shows the relationship among the above problems. Proposi-\ntion 10.1 to 10.3 below hold for both weighted and unweighted elections.\nProposition 10.1. For every voting rule, FRUGAL-BRIBERY 6P UNIFORM-FRUGAL-$BRIBERY 6P NONUNIFORM-FRUGAL-$BRIBERY 6P $BRIBERY. Also, COALITIONAL-MANIPULATION 6P NONUNIFORM-FRUGAL-$BRIBERY.\nProof. In the reductions below, let us assume that the election to start with is a weighted election. Since we do not change the weights of any vote in the reduction and since there is a natural one to one correspondence between the votes of the original instance and the reduced instance, the proof also works for unweighted elections.\nGiven a FRUGAL-BRIBERY instance, we construct a UNIFORM-FRUGAL-$BRIBERY instance by defining the price of every vulnerable vote to be zero and the budget to be zero. Clearly, the two instances are equivalent. Hence, FRUGAL-BRIBERY 6P UNIFORM-FRUGAL-$BRIBERY.\nUNIFORM-FRUGAL-$BRIBERY 6P NONUNIFORM-FRUGAL-$BRIBERY 6P $BRIBERY follows from the fact that UNIFORM-FRUGAL-$BRIBERY is a special case of NONUNIFORM-FRUGAL$BRIBERY which in turn is a special case of $BRIBERY.\nGiven a COALITIONAL-MANIPULATION instance, we construct a NONUNIFORM-FRUGAL$BRIBERY instance as follows. Let p be the distinguished candidate of the manipulators and ≻f= p ≻ others be any arbitrary but fixed ordering of the candidates given in the COALITIONAL-MANIPULATION instance. Without loss of generality, we can assume that p does not win if all the manipulators vote ≻f (Since, this is a polynomially checkable case of COALITIONAL-MANIPULATION). We define the vote of the manipulators to be ≻f, the distinguished candidate of the campaigner to be p, the budget of the campaigner to be zero, the price of the manipulators to be zero (notice that all the manipulators’ votes are p-vulnerable), and the price of the rest of the vulnerable votes to be one. Clearly, the two instances are equivalent. Hence, COALITIONAL-MANIPULATION 6P NONUNIFORM-FRUGAL-$BRIBERY.\nAlso, the FRUGAL-BRIBERY problem reduces to the COALITIONAL-MANIPULATION problem\nby simply making all vulnerable votes to be manipulators.\n234\nProposition 10.2. For every voting rule, FRUGAL-BRIBERY 6P COALITIONAL-MANIPULATION.\nWe can also establish the following relationship between the PERSUASION (respectively k-PERSUASION) problem and the FRUGAL-BRIBERY (respectively FRUGAL-$BRIBERY) problem. The persuasions differ from the corresponding frugal bribery variants in that the briber has her own preference order, and desires to improve the outcome of the election with respect to her preference order. The following proposition is immediate from the definitions of the problems.\nProposition 10.3. For every voting rule, there is a Turing reduction from PERSUASION (respectively k-PERSUASION) to FRUGAL-BRIBERY (respectively FRUGAL-$BRIBERY).\nProof. Given an algorithm for the FRUGAL-BRIBERY problem, we iterate over all possible distinguished candidates to have an algorithm for the persuasion problem.\nGiven an algorithm for the FRUGAL-$BRIBERY problem, we iterate over all possible distinguished candidates and fix the price of the corresponding vulnerables to be one to have an algorithm for the k-persuasion problem."
    }, {
      "heading" : "10.3 Results for Unweighted Elections",
      "text" : "Now we present the results for unweighted elections. We begin with some easy observations that follow from known results.\nObservation 10.1. The FRUGAL-BRIBERY problem is in P for the k-approval voting rule for any k, Bucklin, and plurality with runoff voting rules.\nProof. The COALITIONAL-MANIPULATION problem is in P for these voting rules [XZP+09]. Hence, the result follows from Proposition 10.2.\nObservation 10.2. The FRUGAL-$BRIBERY problem is in P for the plurality and veto voting rules.\nProof. The $BRIBERY problem is in P for the plurality [FHH06] and veto [Fal08] voting rules. Hence, the result follows from Proposition 10.1.\nObservation 10.3. The FRUGAL-$BRIBERY problem is NP-complete for Borda, maximin, Copeland, and STV voting rules.\n235\nProof. The COALITIONAL-MANIPULATION problem is NP-complete for the above voting rules. Hence, the result follows from Proposition 10.1.\nWe now present our main results. We begin with showing that the FRUGAL-BRIBERY problem for the Borda voting rule and the FRUGAL-$BRIBERY problem for various scoring rules are NP-complete. To this end, we reduce from the PERMUTATION SUM problem, which is known to be NP-complete [YHL04]. The PERMUTATION SUM problem is defined as follows.\nPERMUTATION SUM Input: n integers Xi, i ∈ [n] with 1 6 Xi 6 2n for every i ∈ [n] and ∑n i=1 Xi = n(n+ 1). Question: Do there exist two permutations π and σ of [n] such that π(i)+σ(i) = Xi for every i ∈ [n]?\nWe now prove that the FRUGAL-BRIBERY problem is NP-complete for the Borda voting rule, by a reduction from PERMUTATION SUM. Our reduction is inspired by the reduction used by Davies et al. [DKNW11] and Betzler et al. [BNW11] to prove NP-completeness of the COALITIONAL-MANIPULATION problem for the Borda voting rule.\nTheorem 10.1. The FRUGAL-BRIBERY problem is NP-complete for the Borda voting rule.\nProof. The problem is clearly in NP. To show NP-hardness, we reduce an arbitrary instance of the PERMUTATION SUM problem to the FRUGAL-BRIBERY problem for the Borda voting rule. Let (X1, . . . ,Xn) be an instance of the PERMUTATION SUM problem. Without loss of generality, let us assume that n is an odd integer – if n is an even integer, then we consider the instance (X1, . . . ,Xn,Xn+1 = 2(n+ 1)) which is clearly equivalent to the instance (X1, . . . ,Xn).\nWe define a FRUGAL-BRIBERY instance (C,P,p) as follows. The candidate set is:\nC = X ⊎D ⊎ {p, c}, where X = {xi : i ∈ [n]} and |D| = 3n − 1\nNote that the total number of candidates is 4n + 1, and therefore the Borda score of a\ncandidate placed at the top position is 4n.\nBefore describing the votes, we give an informal overview of how the reduction will proceed. The election that we define will consist of exactly two vulnerable votes. Note that when placed at the top position in these two votes, the distinguished candidate p gets a score of 8n (4n from each vulnerable vote). We will then add non-vulnerable votes, which will be\n236\ndesigned to ensure that, among them, the score of xi is 8n−Xi more than the score of the candidate p. Using the “dummy candidates”, we will also be able to ensure that the candidates xi receive (without loss of generality) scores between 1 and n from the modified vulnerable votes.\nNow suppose these two vulnerable votes can be modified to make p win the election. Let s1 and s2 be the scores that xi obtains from these altered vulnerable votes. It is clear that for p to emerge as a winner, s1 + s2 must be at most Xi. Since the Borda scores for the candidates in X range from 1 to n in the altered vulnerable votes, the total Borda score that all the candidates in X can accumulate from two altered vulnerable votes is n(n+ 1). On the other hand, since the sum of the Xi’s is also n(n+ 1), it turns out that s1 + s2 must in fact be equal to Xi for the candidate p to win. From this point, it is straightforward to see how the permutations σ and π can be inferred from the modified vulnerable votes: σ(i) is given by the score of the candidate xi from the first vote, while π(i) is the score of the candidate xi from the second vote. These functions turn out to be permutations because these n candidates receive n distinct scores from these votes.\nWe are now ready to describe the construction formally. We remark that instead of 8n−Xi, as described above, we will maintain a score difference of either 8n − Xi or 8n − Xi − 1 depending on whether Xi is even or odd respectively — this is a minor technicality that comes from the manner in which the votes are constructed and does not affect the overall spirit of the reduction.\nLet us fix any arbitrary order ≻f among the candidates in X⊎D. For any subsetA ⊂ X⊎D, let −→ A be the ordering among the candidates in A as defined in ≻f and ←− A the reverse order of −→ A . For each i ∈ [n], we add two votes vji and vj ′\ni as follows for every j ∈ [4]. Let ℓ denote |D| = 3n− 1. Also, for d ∈ D, let Di,Dℓ/2 ⊂ D \\ {d} be such that:\n|Di| = ℓ/2 + n+ 1− ⌈Xi/2⌉ and |Dℓ/2| = ℓ/2.\nvji :\n \n\nc ≻ p ≻ d ≻ −−−−−−−−−−−−−−−→C \\ ({d, c,p, xi} ⊎Di) ≻ xi ≻ −→ Di for 1 6 j 6 2 xi ≻ ←− Di ≻ ←−−−−−−−−−−−−−−− C \\ ({d, c,p, xi} ⊎Di) ≻ c ≻ p ≻ d for 3 6 j 6 4\nvj ′\ni :\n \n\nc ≻ p ≻ d ≻ −−−−−−−−−−−−−−−−→C \\ ({d, c,p, xi} ⊎Dℓ/2) ≻ xi ≻ −−→ Dℓ/2 for 1 6 j ′ 6 2 xi ≻ ←−− Dℓ/2 ≻ ←−−−−−−−−−−−−−−−− C \\ ({d, c,p, xi} ⊎Dℓ/2) ≻ c ≻ p ≻ d for 3 6 j′ 6 4\n237\nIt is convenient to view the votes corresponding to j = 3, 4 as a near-reversal of the votes\nin j = 1, 2 (except for candidates c,d and xi). Let P1 = {v j i, v\nj′\ni : i ∈ [n], j ∈ [4]}. Since there are 8n votes in all, and c always appears immediately before p, it follows that the score of c is exactly 8n more than the score of the candidate p in P1.\nWe also observe that the score of the candidate xi is exactly 2(ℓ + n + 1) − Xi = 8n − Xi more than the score of the candidate p in P1 for every i ∈ [n] such that Xi is an even integer. On the other hand, the score of the candidate xi is exactly 2(ℓ+n+1)−Xi −1 = 8n−Xi −1 more than the score of the candidate p in P1 for every i ∈ [n] such that Xi is an odd integer. Note that for i′ ∈ [n] \\ {i}, p and xi receive the same Borda score from the votes vji′ and vj ′ i′ (where j, j′ ∈ [4]). We now add the following two votes µ1 and µ2.\nµ1 : p ≻ c ≻ others\nµ2 : p ≻ c ≻ others\nLet P = P1 ⊎ {µ1,µ2},Xo = {xi : i ∈ [n],Xi is odd}, and Xe = X \\ Xo. We recall that the distinguished candidate is p. The tie-breaking rule is according to the order Xo ≻ p ≻ others. We claim that the FRUGAL-BRIBERY instance (C,P,p) is equivalent to the PERMUTATION SUM instance (X1, . . . ,Xn).\nIn the forward direction, suppose there exist two permutations π and σ of [n] such that π(i) + σ(i) = Xi for every i ∈ [n]. We replace the votes µ1 and µ2 with respectively µ′1 and µ′2 as follows.\nµ′1 : p ≻ D ≻ xπ−1(n) ≻ xπ−1(n−1) ≻ · · · ≻ xπ−1(1) ≻ c µ′2 : p ≻ D ≻ xσ−1(n) ≻ xσ−1(n−1) ≻ · · · ≻ xσ−1(1) ≻ c\nWe observe that, the candidates c and every x ∈ Xe receive same score as p, every candidate x′ ∈ Xo receives 1 score less than p, and every candidate in D receives less score than p in P1 ⊎ {µ′1,µ′2}. Hence p wins in P1 ⊎ {µ′1,µ′2} due to the tie-breaking rule. Thus (C,P,p) is a YES instance of FRUGAL-BRIBERY.\nTo prove the other direction, suppose the FRUGAL-BRIBERY instance is a YES instance.\nNotice that the only vulnerable votes are µ1 and µ2. Let µ ′ 1 and µ ′ 2 be two votes such that the candidate p wins in the profile P1 ⊎ {µ′1,µ′2}. We assume, without loss of generality, that\n238\ncandidate p is placed at the first position in both µ′1 and µ ′ 2. Since c receives 8n scores more than p in P1, c must be placed at the last position in both µ ′ 1 and µ ′ 2 since otherwise p cannot win in P1 ⊎ {µ′1,µ′2}. We also assume, without loss of generality, that every candidate in D is preferred over every candidate in X since otherwise, if x ≻ d in either µ′1 or µ′2 for some x ∈ X and d ∈ D, then we can exchange the positions of x and d and p continues to win since no candidate in D receives more score than p in P1. Hence, every x ∈ X receives some score between 1 and n in both the µ′1 and µ ′ 2. Let us define two permutations π and σ of [n] as follows. For every i ∈ [n], we define π(i) and σ(i) to be the scores the candidate xi receives in µ′1 and µ ′ 2 respectively. The fact that π and σ, as defined above, is indeed a permutation of [n] follows from the structure of the votes µ′1,µ ′ 2 and the Borda score vector. Since p wins in P1 ⊎ {µ′1,µ′2}, we have π(i) + σ(i) 6 Xi. We now have the following.\nn(n+ 1) =\nn∑\ni=1\n(π(i) + σ(i)) 6\nn∑\ni=1\nXi = n(n + 1)\nHence, we have π(i) + σ(i) = Xi for every i ∈ [n] and thus (X1, . . . ,Xn) is a YES instance of PERMUTATION SUM.\nWe will use Lemma 7.1 in subsequent proofs.\nTheorem 10.2. The FRUGAL-$BRIBERY problem is NP-complete for the k-approval voting rule for any constant k > 5, even if the price of every vulnerable vote is either 1 or ∞.\nProof. The problem is clearly in NP. To show NP-hardness, we reduce an arbitrary instance of X3C to FRUGAL-$BRIBERY. Let (U, {S1, . . . , St}) be an instance of X3C. We define a FRUGAL$BRIBERY instance as follows. The candidate set is C = U⊎D ⊎ {p,q}, where |D| = k− 1. For each Si, 1 6 i 6 t, we add a vote vi as follows.\nvi : p ≻ q ≻ Si︸ ︷︷ ︸ 5 candidates ≻ D ≻ others\nBy Lemma 7.1, we can add poly(|U|) many additional votes to ensure the following scores (denoted by s(·)). s(q) = s(p) + |U|/3, s(x) = s(p) + 1, ∀x ∈ U,\ns(d) < s(p) − |U|/3, ∀d ∈ D\n239\nThe tie-breaking rule is “p ≻ others”. The winner is q. The distinguished candidate is p and thus all the votes in {vi : 1 6 i 6 t} are vulnerable. The price of every vi is 1 and the price of every other vulnerable vote is ∞. The budget is |U|/3. We claim that the two instances are equivalent. Suppose there exists an index set I ⊆ [t] with |I| = |U|/3 such that ⊎i∈ISi = U. We replace the votes vi with v ′ i, i ∈ I, which are defined as follows.\nv′i : p ≻ D︸ ︷︷ ︸ k candidates ≻ others\nThis makes the score of p not less than the score of any other candidate and thus p wins.\nTo prove the result in the other direction, suppose the FRUGAL-$BRIBERY instance is a YES instance. Then notice that there will be |U|/3 votes in {vi : 1 6 i 6 t} where the candidate q should not be placed within the top k positions since s(p) = s(q) − |U|/3 and the budget is |U|/3. We claim that the Si’s corresponding to the vi’s that have been changed must form an exact set cover. Indeed, otherwise, there will be a candidate x ∈ U, whose score never decreases which contradicts the fact that p wins the election since s(p) = s(x) − 1.\nWe next present a similar result for the k-veto voting rule.\nTheorem 10.3. The FRUGAL-$BRIBERY problem is NP-complete for the k-veto voting rule for any constant k > 3, even if the price of every vulnerable vote is either 1 or ∞.\nProof. The problem is clearly in NP. To show NP-hardness, we reduce an arbitrary instance of X3C to FRUGAL-$BRIBERY. Let (U, {S1, S2, . . . , St}) be any instance of X3C. We define a FRUGAL-$BRIBERY instance as follows. The candidate set is C = U ⊎ Q ⊎ {p,a1,a2,a3,d}, where |Q| = k − 3. For each Si, 1 6 i 6 t, we add a vote vi as follows.\nvi : p ≻ others ≻ Si ≻ Q︸ ︷︷ ︸ k candidates\nBy Lemma 7.1, we can add poly(|U|) many additional votes to ensure following scores (denoted by s(·)).\ns(p) > s(d), s(p) = s(x) + 2, ∀x ∈ U, s(p) = s(q) + 1, ∀q ∈ Q,\ns(p) = s(ai) − |U|/3 + 1, ∀i = 1, 2, 3\n240\nThe tie-breaking rule is “a1 ≻ · · · ≻ p”. The winner is a1. The distinguished candidate is p and thus all the votes in {vi : 1 6 i 6 t} are vulnerable. The price of every vi is one and the price of any other vote is ∞. The budget is |U|/3.\nWe claim that the two instances are equivalent. In the forward direction, suppose there exists an index set I ⊆ {1, . . . , t} with |I| = |U|/3 such that ⊎i∈ISi = U. We replace the votes vi with v′i, i ∈ I, which are defined as follows.\nv′i : others ≻ a1 ≻ a2 ≻ a3 ≻ Q︸ ︷︷ ︸ k candidates\nThe score of each ai decreases by |U|/3 and their final scores are s(p) − 1, since the score of p is not affected by this change. Also the final score of each x ∈ U is s(p) − 1 since I forms an exact set cover. This makes p win the election.\nTo prove the result in the other direction, suppose the FRUGAL-$BRIBERY instance is a YES instance. Then, notice that there will be exactly |U|/3 votes in vi, 1 6 i 6 t, where every aj, j = 1, 2, 3, should come in the last k positions since s(p) = s(aj)− |U|/3+1 and the budget is |U|/3. Notice that candidates in Q must not be placed within top m − k positions since s(p) = s(q) + 1, for every q ∈ Q. Hence, in the votes that have been changed, a1,a2,a3 and all the candidates in Qmust occupy the last k positions. We claim that the Si’s corresponding to the vi’s that have been changed must form an exact set cover. If not, then, there must exist a candidate x ∈ U and two votes vi and vj such that, both vi and vj have been replaced by v′i 6= vi and v′j 6= vj and the candidate x was present within the last k positions in both vi and vj. This makes the score of x at least the score of p which contradicts the fact that p wins.\nHowever, we show the existence of a polynomial time algorithm for the FRUGAL-$BRIBERY problem for the k-approval, Bucklin, and plurality with runoff voting rules, when the budget is a constant. The result below follows from the existence of a polynomial time algorithm for the COALITIONAL-MANIPULATION problem for these voting rules for a constant number of manipulators [XZP+09].\nTheorem 10.4. The FRUGAL-$BRIBERY problem is in P for the k-approval, Bucklin, and plurality with runoff voting rules, if the budget is a constant.\nProof. Let the budget b be a constant. Then, at most b many vulnerable votes whose price is not zero can be changed since the prices are assumed to be in N. Notice that we may assume, without loss of generality, that all the vulnerable votes whose price is zero will be\n241\nchanged. We iterate over all the O(nb) many possible vulnerable vote changes and we can solve each one in polynomial time since the COALITIONAL-MANIPULATION problem is in P for these voting rules [XZP+09].\nWe show that the FRUGAL-$BRIBERY problem is NP-complete for a wide class of scoring rules as characterized in the following result. Our next result shows that, the FRUGAL$BRIBERY problem is NP-complete for a wide class of scoring rules that includes the Borda voting rule. Theorem 10.5 can be proved by a reduction from the X3C problem.\nTheorem 10.5. For any positional scoring rule r with score vectors {−→si : i ∈ N}, if there exists a polynomial function f : N −→ N such that, for every m ∈ N, f(m) > 2m and in the score vector (α1, . . . ,αf(m)), there exists a 1 6 ℓ 6 f(m) − 5 satisfying the following condition:\nαi − αi+1 = αi+1 − αi+2 > 0, ∀ℓ 6 i 6 ℓ+ 3\nthen the FRUGAL-$BRIBERY problem is NP-complete for r even if the price of every vulnerable vote is either 1 or ∞.\nProof. Since the scoring rules remain unchanged if we multiply every αi by any constant λ > 0 and/or add any constant µ, we can assume the following without loss of generality.\nαi − αi+1 = αi+1 − αi+2 = 1, ∀ℓ 6 i 6 ℓ+ 3\nThe problem is clearly in NP. To show NP-hardness, we reduce an arbitrary instance of X3C to FRUGAL-$BRIBERY. Let (U, {S1, . . . , St}) be an instance of X3C. We define a FRUGAL-$BRIBERY instance as follows. The candidate set is C = U⊎Q⊎ {p,a,d}, where |Q| = f(|U|) − ℓ− 4. For each Si = {x,y, z}, 1 6 i 6 t, we add a vote vi as follows.\nvi : p ≻ d ≻ others ≻ a ≻ x ≻ y ≻ z ≻ Q︸ ︷︷ ︸ l candidates\nBy Lemma 7.1, we can add poly(|U|) many additional votes to ensure the following scores (denoted by s(·)). Note that the proof of Lemma 7.1 in [BRR11] also works for the normalization of α defined in the beginning of the proof.\ns(d) < s(p), s(x) = s(p) − 2, ∀x ∈ U,\n242\ns(a) = s(p) + |U|/3 − 1, s(q) = s(p) + 1\nThe tie-breaking rule is “· · · ≻ p”. The distinguished candidate is p. The price of every vi is 1 and the price of every other vulnerable vote is ∞. The budget is |U|/3.\nWe claim that the two instances are equivalent. In the forward direction, there exists an index set I ⊆ [t], |I| = |U|/3, such that ⊎i∈ISi = U. We replace the votes vi with v′i, i ∈ I, which are defined as follows.\nv′i : p ≻ d ≻ others ≻ x ≻ y ≻ z ≻ a ≻ Q\nThis makes the score of p at least one more than the score of every other candidate and thus p wins.\nTo prove the result in the other direction, suppose there is a way to make p win the election. Notice that the candidates in Q cannot change their positions in the changed votes and must occupy the last positions due to their score difference with p. Now we claim that there will be exactly |U|/3 votes where the candidate a must be placed at the (l+4)th position since s(p) = s(a)− |U|/3+1 and the budget is |U|/3. We claim that the Si’s corresponding to the changed votes must form an exact set cover. If not, then there must exist a candidate x ∈ U whose score has increased by at least two contradicting the fact that p wins the election.\nFor the sake of concreteness, an example of a function f, stated in Theorem 10.5, that works for the Borda voting rule is f(m) = 2m. Theorem 10.5 shows that the FRUGAL$BRIBERY problem is intractable for the Borda voting rule. However, the following theorem shows the intractability of the UNIFORM-FRUGAL-$BRIBERY problem for the Borda voting rule, even in a very restricted setting. Theorem 10.6 can be proved by a reduction from the COALITION MANIPULATION problem for the Borda voting rule for two manipulators which is known to be NP-complete [BNW11, DKNW11].\nTheorem 10.6. The UNIFORM-FRUGAL-$BRIBERY problem is NP-complete for the Borda voting rule, even when every vulnerable vote has a price of 1 and the budget is 2.\nProof. The problem is clearly in NP. To show NP-hardness, we reduce an arbitrary instance of the COALITIONAL-MANIPULATION problem for the Borda voting rule with two manipulators to an instance of the UNIFORM-FRUGAL-$BRIBERY problem for the Borda voting rule. Let (C,≻t , 2,p) be an arbitrary instance of the COALITIONAL-MANIPULATION problem for the Borda voting rule. The corresponding FRUGAL-$BRIBERY instance is as follows. The candidate set is\n243\nC′ = C ⊎ {d,q}. For each vote vi ∈≻t, we add a vote v′i : vi ≻ d ≻ q. Corresponding to the two manipulators’, we add two more votes ν1,ν2 : −−−−→ C \\ {p} ≻ d ≻ p ≻ q, where −−−−→C \\ {p} is an arbitrary but fixed order of the candidates in C \\ {p}. We add more votes to ensure following score differences (s(·) and s′(·) are the score functions for the COALITIONAL-MANIPULATION and the UNIFORM-FRUGAL-$BRIBERY instances respectively).\ns′(p) = λ+ s(p) − 2, s′(x) = λ+ s(x) for every x ∈ C,\ns′(q) = s′(p) − 2m + 1, s′(p) > s′(d) + 2m for some λ ∈ Z\nThis will be achieved as follows. For any two arbitrary candidates a and b, the following two votes increase the score of a by one more than the rest of the candidates except b whose score increases by one less. This construction has been used before [XCP10, DKNW11].\na ≻ b ≻ −−−−−−→C \\ {a,b}\n←−−−−−− C \\ {a,b} ≻ a ≻ b\nAlso, we can ensure that candidate p is always in (m−1/2,m+1/2) positions and the candidate q never immediately follows p in these new votes. The tie-breaking rule is “others ≻ p”. The distinguished candidate is p. The price of every vulnerable vote is one and the budget is two. We claim that the two instances are equivalent.\nIn the forward direction, suppose the COALITIONAL-MANIPULATION instance is a YES instance. Let u1,u2 be the manipulators’ votes that make p win. In the FRUGAL-$BRIBERY instance, we replace νi by ν ′ i : p ≻ d ≻ ui ≻ q for i = 1, 2. This makes p win the election. In the reverse direction, recall that in all the vulnerable votes except ν1 and ν2, the candidate q never immediately follows candidate p. Therefore, changing any of these votes can never make p win the election since s′(q) = s′(p) − 2m + 1 and the budget is two. Hence, the only way p can win the election, if at all possible, is by changing the votes ν1 and ν2. Let a vote ν′i replaces νi for i = 1, 2. We can assume, without loss of generality, that p and d are at the first and the second positions respectively in both ν′1 and ν ′ 2. Let ui be the order ν ′ i restricted only to the candidates in C. This makes p winner of the COALITIONAL-MANIPULATION instance since, s′(p) = λ + s(p) − 2, s′(x) = λ+ s(x) for every x ∈ C.\n244"
    }, {
      "heading" : "10.4 Results for Weighted Elections",
      "text" : "Now we turn our attention to weighted elections. As before, we begin with some easy observations that follow from known results.\nObservation 10.4. The FRUGAL-BRIBERY problem is in P for the maximin and the Copeland voting rules for three candidates.\nProof. The MANIPULATION problem is in P for the maximin, Copeland voting rules for three candidates [CSL07]. Hence, the result follows from Proposition 10.2.\nObservation 10.5. The FRUGAL-BRIBERY problem is NP-complete for any scoring rule except plurality for three candidates.\nProof. The same proof for Theorem 6 of [CSL07] would work here.\nTheorem 10.7. The FRUGAL-BRIBERY problem is in P for the plurality voting rule.\nProof. Let p be the distinguished candidate of the campaigner. The greedy strategy of just replacing every vulnerable vote by p ≻ others solves the problem due to the monotonicity property of the plurality voting rule.\nOur hardness results in this section are based on the PARTITION problem, which is known\nto be NP-complete [GJ79], and is defined as follows.\nDefinition 10.6. (PARTITION Problem) Given a finite multi-set W of positive integers with ∑\nw∈W w = 2K, does there exist a subset\nW′ ⊂ W such that ∑w∈W′ w = K? An arbitrary instance of PARTITION is denoted by (W, 2K).\nWe define another problem which we call 1 4 -PARTITION as below. We prove that 1 4 -\nPARTITION is also NP-complete. We will use this fact in the proof of Theorem 10.10.\nDefinition 10.7. (The 1 4 -PARTITION Problem) Given a finite multi-set W of positive integers with ∑\nw∈W w = 4K, does there exist a subset\nW′ ⊂ W such that∑w∈W′ w = K? An arbitrary instance of 14 -PARTITION is denoted by (W, 4K).\nLemma 10.1. 1 4 -PARTITION problem is NP-complete.\n245\nProof. The problem is clearly in NP. To show NP-hardness, we reduce the PARTITION problem to it. Let (W, 2K) be an arbitrary instance of the PARTITION problem. We can assume, without loss of generality, that 2K /∈ W, since otherwise the instance is trivially a no instance. The corresponding 1\n4 -PARTITION problem instance is defined by (W1, 4K), where W1 = W ∪ {2K}.\nWe claim that the two instances are equivalent. Suppose the PARTITION instance is a YES instance and thus there exists a set W′ ⊂ W such that ∑w∈W′ w = K. This W′ gives a solution to the 1\n4 -PARTITION instance. To prove the result in the other direction, suppose\nthere is a set W′ ⊂ W1 such that ∑ w∈W′ w = K. This W ′ gives a solution to the PARTITION problem instance since 2K /∈ W′.\nIn the rest of this section, we present the hardness results in weighted elections for the following voting rules: plurality, maximin, STV, Copelandα, and Bucklin. For plurality, recall that the FRUGAL-BRIBERY problem is in P, and we will show that FRUGAL-$BRIBERY is NPcomplete. For all the other rules, we will establish that even FRUGAL-BRIBERY is NP-complete.\nTheorem 10.8. The FRUGAL-$BRIBERY problem is NP-complete for the plurality voting rule for three candidates.\nProof. The problem is clearly in NP. We reduce an arbitrary instance of PARTITION to an instance of FRUGAL-$BRIBERY for the plurality voting rule. Let (W, 2K), with W = {w1, . . . ,wn} and ∑n\ni=1 wi = 2K, be an arbitrary instance of the PARTITION problem. The candidates are\np,a, and b. We will add votes in such a way that makes b win the election. The distinguished candidate is p. For every i ∈ [n], there is a vote a ≻ p ≻ b of both weight and price wi. There are two votes b ≻ p ≻ a of weight 3K (we do not need to define the price of this vote since it is non-vulnerable) and p ≻ a ≻ b of both weight and price 2K+ 1. The tie-breaking rule is “a ≻ b ≻ p”. We define the budget to be K.\nWe claim that the two instances are equivalent. Suppose there exists a W′ ⊂ W such that ∑\nw∈W′ w = K. We change the votes corresponding to the weights in W ′ to p ≻ a ≻ b. This makes p win the election with a score of 3K + 1. To prove the other direction, for p to win, its score must increase by at least K. Also, the prices ensure that p’s score can increase by at most K. Hence, p’s score must increase by exactly by K and the only way to achieve this is to increase its score by changing the votes corresponding to the weights in W. Thus, p can win only if there exists a W′ ⊂ W such that ∑w∈W′ w = K.\nNext we show the hardness result for the maximin voting rule.\n246\nTheorem 10.9. The FRUGAL-BRIBERY problem is NP-complete for the maximin voting rule for four candidates.\nProof. The problem is clearly in NP. We reduce an arbitrary instance of PARTITION to an instance of FRUGAL-BRIBERY for the maximin voting rule. Let (W, 2K), withW = {w1, . . . ,wn} and ∑n\ni=1 wi = 2K, be an arbitrary instance of the PARTITION problem. The candidates are\np,a,b, and c. For every i ∈ [n], there is a vote p ≻ a ≻ b ≻ c of weight wi. There is one vote c ≻ a ≻ b ≻ p, one b ≻ c ≻ a ≻ p, and one a ≻ c ≻ b ≻ p each of weight K. The tie-breaking rule is “p ≻ a ≻ b ≻ c”. The distinguished candidate is p. Let T denotes the set of votes corresponding to the weights in W and the rest of the votes S. Notice that only the votes in T are vulnerable. We claim that the two instances are equivalent.\nSuppose there exists a W′ ⊂ W such that ∑w∈W′ w = K. We change the votes corresponding to the weights in W′ to p ≻ a ≻ b ≻ c. We change the rest of the votes in T to p ≻ b ≻ c ≻ a. The maximin score of every candidate is −K and thus due to the tie-breaking rule, p wins the election.\nOn the other hand, suppose there is a way to change the vulnerable votes, that is the votes in T , that makes p win the election. Without loss of generality, we can assume that all the votes in T place p at top position. First notice that the only way p could win is that the vertices a,b, and c must form a cycle in the weighted majority graph. Otherwise, one of a,b, and c will be a Condorcet winner and thus the winner of the election. Now we show that the candidate b must defeat the candidate c. If not, then c must defeat b by a margin of K since the maximin score of p is fixed at −K. Also, a must defeat c by a margin of K, otherwise the maximin score of c will be more than −K. This implies that all the votes in T must be p ≻ a ≻ c ≻ b which makes a defeat b. This is a contradiction since the vertices a,b, and c must form a cycle in the weighted majority graph. Hence b must defeat c by a margin of K. This forces every vote in T to prefer b over c. Hence, without loss of generality, we assume that all the votes in T are either p ≻ a ≻ b ≻ c or p ≻ b ≻ c ≻ a, since whenever c is right after a, we can swap a and c and this will only reduce the score of a without affecting the score of any other candidates. If the total weight of the votes p ≻ a ≻ b ≻ c in T is more than K, then DE(c,a) < K, thereby making the maximin score of a more than the maximin score of p. If the total weight of the votes p ≻ a ≻ b ≻ c in T is less than K, then DE(a,b) < K, thereby making the maximin score of b more than the maximin score of p. Thus the total weight of the votes p ≻ a ≻ b ≻ c in T should be exactly K which corresponds to a partition of W.\n247\nWe now prove the hardness result for the STV voting rule.\nTheorem 10.10. The FRUGAL-BRIBERY problem is NP-complete for the STV voting rule for three candidates.\nProof. The problem is clearly in NP. We reduce an arbitrary instance of 1 4 -PARTITION to an instance of FRUGAL-BRIBERY for the STV voting rule. Let (W, 4K), withW = {w1, . . . ,wn} and ∑n\ni=1 wi = 4K, be an arbitrary instance of the 1 4 -PARTITION problem. The candidates are p,a, and b. For every i ∈ [n], there is a vote p ≻ a ≻ b of weight wi. There is a vote a ≻ p ≻ b of weight 3K − 1 and a vote b ≻ a ≻ p of weight 2K. The tie-breaking rule is “a ≻ b ≻ p”. The distinguished candidate is p. Let T denotes the set of votes corresponding to the weights in W and the rest of the votes be S. Notice that only the votes in T are vulnerable. We claim that the two instances are equivalent.\nSuppose there exists a W′ ⊂ W such that ∑w∈W′ w = K. We change the votes corresponding to the weights in W′ to b ≻ p ≻ a. We do not change the rest of the votes in T . This makes p win the election.\nFor the other direction, suppose there is a way to change the votes in T that makes p win the election. First Notice that p can win only if b qualifies for the second round. Hence, the total weight of the votes in T that put b at the first position must be at least K. On the other hand, if the total weight of the votes in T that put b at the first position is strictly more than K, then p does not qualify for the second round and thus cannot win the election. Hence the total weight of the votes in T that put b at the first position must be exactly equal to K which constitutes a 1 4 -partition of W.\nFor three candidates, the STV voting rule is the same as the plurality with runoff voting\nrule. Hence, we have the following corollary.\nCorollary 10.1. The FRUGAL-BRIBERY problem is NP-complete for the plurality with runoff voting rule for three candidates.\nWe turn our attention to the Copelandα voting rule next.\nTheorem 10.11. The FRUGAL-BRIBERY problem is NP-complete for the Copelandα voting rule for four candidates, whenever α ∈ [0, 1).\nProof. The problem is clearly in NP. We reduce an arbitrary instance of PARTITION to an instance of FRUGAL-BRIBERY for the Copelandα voting rule. Let (W, 2K), withW = {w1, . . . ,wn}\n248\nand ∑n\ni=1 wi = 2K, be an arbitrary instance of the PARTITION problem. The candidates are\np,a,b, and c. For every i ∈ [n], there is a vote p ≻ a ≻ b ≻ c of weight wi. There are two votes a ≻ p ≻ b ≻ c and c ≻ b ≻ a ≻ p each of weight K + 1. The tie-breaking rule is “a ≻ b ≻ c ≻ p”. The distinguished candidate is p. Let T denotes the set of votes corresponding to the weights in W and the rest of the votes be S. Notice that only the votes in T are vulnerable. We claim that the two instances are equivalent.\nSuppose there exists a W′ ⊂ W such that ∑w∈W′ w = K. We change the votes corresponding to the weights in W′ to p ≻ c ≻ b ≻ a. We change the rest of the votes in T to p ≻ b ≻ c ≻ a. This makes p win the election with a Copelandα score of two.\nOn the other hand, suppose there is a way to change the votes in T that makes p win the election. Without loss of generality, we can assume that all the votes in T place p at top position. We will show that one of the three pairwise elections among a,b, and c must be a tie. Suppose not, then a must lose to both b and c, otherwise a wins the election due to the tie-breaking rule. Now consider the pairwise election between b and c. If b defeats c, then b wins the election due to the tie-breaking rule. If c defeats b, then c wins the election again due to the tie-breaking rule. Hence, one of the pairwise elections among a,b, and c must be a tie. Without loss of generality suppose a and b ties. Then the total weight of the votes that prefer a to b in T must be K which constitutes a partition of W.\nFinally, we show that the FRUGAL-BRIBERY problem for the Bucklin voting rule is NP-\ncomplete.\nTheorem 10.12. The FRUGAL-BRIBERY problem is NP-complete for the Bucklin voting rule for four candidates.\nProof. The problem is clearly in NP. We reduce an arbitrary instance of PARTITION to an instance of FRUGAL-BRIBERY for the Bucklin voting rule. Let (W, 2K), with W = {w1, . . . ,wn} and ∑n\ni=1 wi = 2K, be an arbitrary instance of the PARTITION problem. The candidates are\np,a,b, and c. For every i ∈ [n], there is a vote p ≻ a ≻ b ≻ c of weight wi. There are two votes a ≻ b ≻ p ≻ c and c ≻ b ≻ a ≻ p each of weight K. The tie-breaking rule is “p ≻ a ≻ b ≻ c”. The distinguished candidate is p. Let T denote the set of votes corresponding to the weights in W and the rest of the votes be S. Notice that only the votes in T are vulnerable. We claim that the two instances are equivalent.\n249\nSuppose there exists a W′ ⊂ W such that ∑w∈W′ w = K. We change the votes corresponding to the weights in W′ to p ≻ c ≻ b ≻ a. This makes p win the election with a Bucklin score of three.\nTo prove the result in the other direction, suppose there is a way to change the votes in T that makes p win the election. Without loss of generality, we can assume that all the votes in T place p at the first position. First Notice that the Bucklin score of p is already fixed at three. In the votes in T , the candidate b can never be placed at the second position since that will make the Bucklin score of b to be two. Also the total weight of the votes in T that place a in their second position can be at most K. The same holds for c. Hence, the total weight of the votes that place a in their second position will be exactly equal to K which constitutes a partition of W.\nWe also have the following results for the Copelandα and Bucklin voting rules by reducing\nfrom PARTITION.\nTheorem 10.13. The FRUGAL-BRIBERY problem is NP-complete for the Copelandα and Bucklin voting rules for four candidates, whenever α ∈ [0, 1).\nFrom Proposition 10.1, Observation 10.5, Theorem 10.8 to 10.10, 10.12 and 10.13, and\nCorollary 10.1, we get the following corollary.\nCorollary 10.2. The UNIFORM-FRUGAL-$BRIBERY and the NONUNIFORM-FRUGAL-$BRIBERY problems are NP-complete for the scoring rules except plurality, STV, and the plurality with runoff voting rules for three candidates and for the maximin, Copeland, and Bucklin voting rules for four candidates."
    }, {
      "heading" : "10.5 Conclusion",
      "text" : "We proposed and studied two important special cases of the $BRIBERY problem where the briber is frugal. Our results show that even for these special cases, the bribery problem continues to be intractable, thus subsuming known hardness results in the literature. Our results reinforce that bribery is a rather hard computational problem, because of the hardness of several important special cases. This also strengthens the view that bribery, although a possible attack on an election in principle, may be infeasible in practice.\n250\nChapter 11\nSummary and Future Directions\nIn this chapter, we summarize our work in this thesis and provide interesting future directions of research.\nIn this thesis, we studied computational complexity of three fundamental aspects of voting. We began with studying efficient strategies for eliciting preferences of voters in the first part of the thesis. We then moved on to study the problem of finding a winner under various interesting circumstances in the second part of the thesis. Finally, we showed interesting complexity theoretic results for the computational problem of controlling an election system in various forms. We now summarize our work in this thesis."
    }, {
      "heading" : "11.1 Summary of Contributions",
      "text" : ""
    }, {
      "heading" : "11.1.1 Part I: Preference Elicitation",
      "text" : "Chapter 3: Preference Elicitation for Single Peaked Profiles on Trees\nIn this work, we presented algorithms for eliciting the preferences of a set of voters when the preferences are single peaked on a tree. Moreover, our algorithms ask minimum number of comparison queries up to constant factors. We also presented algorithms for finding a weak Condorcet winner from a set of preferences which are single peaked on a tree by asking minimum number of comparison queries up to constant factors. We observed that, the query complexity of finding a weak Condorcet is much less than the query complexity for preference elicitation.\n251\nChapter 4: Preference Elicitation for Single Crossing Profiles\nIn this work, we presented preference elicitation algorithm for single crossing preference profiles. We studied this problem when an ordering of the voters with respect to which the profile is single crossing is known and when it is unknown. We also considered different access models: when the votes can be accessed at random, as opposed to when they are coming in a predefined sequence. In the sequential access model, we distinguished two cases when the ordering is known: the first is that sequence in which the votes appear is also a single-crossing order, versus when it is not. Our algorithms ask for minimum number of comparison queries up to constant factors for all the above situations except one when we have a large number of voters."
    }, {
      "heading" : "11.1.2 Part II: Winner Determination",
      "text" : "Chapter 5: Winner Prediction and Margin of Victory Estimation\nWe presented efficient algorithms for predicting the winner of an election as well as estimating the margin of victory of an election. We also showed interesting lower bounds for sample complexity of these problems which establish that our algorithms are often optimal up to constant factors.\nChapter 6: Streaming Algorithms for Winner Determination\nIn this work, we studied the space complexity for determining approximate winners in the setting where votes are inserted continually into a data stream. We showed that allowing randomization and approximation indeed allows for much more space-efficient algorithms. Moreover, our bounds are tight in certain parameter ranges."
    }, {
      "heading" : "11.1.3 Part III: Election Control",
      "text" : "Chapter 7: Kernelization for Possible Winner and Coalitional Manipulation\nIn this work, we proved that the possible winner problem does not admit any efficient preprocessing rules, more formally any kernelization algorithms for many common voting rules including scoring rules, maximin, Copeland, ranked pairs, and Bucklin when parameterized\n252\nby the number of candidates. However, we showed that the coalitional manipulation problem which is an important special case of the possible winner problem does admit polynomial time kernelization algorithms parameterized by the number of candidates.\nChapter 8: Manipulation with Partial Votes\nIn this chapter, we pursued a comprehensive study of manipulationwith incomplete votes. We proposed three natural extension of manipulation in the usual complete information setting to the incomplete information setting namely, weak manipulation, opportunistic manipulation, and strong manipulation. We completely resolved computational complexity of all the three problems for many common voting rules including plurality, veto, k-approval, k-veto, Borda, maximin, Copeland, Bucklin, and Fallback voting rules.\nChapter 9: Manipulation Detection\nIn this work, we initiated a promising direction of research namely detecting instances of manipulation in elections. We showed that detecting possible instances of manipulation can often be a much easier computational problem than the corresponding problem of manipulating election itself as seen for the case of the Borda voting rule.\nChapter 10: Frugal Bribery\nIn this work, we studied the classical problem of bribery under a weak notion of briber namely when the briber if frugal in nature. We proved that the bribery problems remain intractable even with this weak briber thereby strengthening the intractability results from the literature on bribery. Hence, although theoretically possible, bribery may not be easy to do in practice."
    }, {
      "heading" : "11.2 Future Directions of Research",
      "text" : "We next discuss some of the interesting directions of research from this thesis.\nChapter 3: Preference Elicitation for Single Peaked Profiles on Trees\n⊲ One can generalize the notion of single peaked profiles on trees for preferences that are\nnot necessarily a complete order. Indeed, voters often are indifferent between two or\n253\nmore candidates and there are interesting domains in this setting [EL15]. Eliciting such incomplete preference is an interesting direction of research to pursue.\n⊲ The domain of single peaked profiles on trees can further be generalized to single\npeaked profiles on forests. It would be interesting to study this both as a domain itself and from the point of view of existence of efficient preference elicitation strategies.\n⊲ Can we reduce the query complexity for preference elicitation further by assuming more\non their preferences, for example, a social network structure on the voters?\n⊲ In this work, we assume complete knowledge of single peaked tree. How the query\ncomplexity would change if we only assume the knowledge of the structure of the single peaked tree without the identity of the candidate associated with each node of the tree?\nChapter 4: Preference Elicitation for Single Crossing Profiles\n⊲ An immediate direction of research is to close the gap between upper and lower bound\non query complexity for preference elicitation when we know a single crossing order and voters are allowed to be queried randomly.\n⊲ Another interesting direction is to study preference elicitation assuming a social network\nstructure among the voters where neighbors tend to have similar preferences.\nChapter 5: Winner Prediction and Margin of Victory Estimation\n⊲ Is there an axiomatic characterization of the voting rules for which the sample com-\nplexity is independent of m and n? We note that a similar problem in graph property testing was the subject of intense study [AFNS06, BCL+06].\n⊲ Specifically for scoring rules, is the sample complexity determined by some natural\nproperty of the score vector, such as its sparsity?\n⊲ Is it worthwhile for the algorithm to elicit only part of the vote from each sampled voter\ninstead of the full vote? As mentioned in the Introduction, vote elicitation is a welltrodden area, but as far as we know, it has not been studied how assuming a margin of victory can change the number of queries.\n254\n⊲ How can knowledge of a social network on the voters be used to minimize the number\nof samples made? Some initial progress in this direction has been made by Dhamal and Narahari [DN13] and by Agrawal and Devanur (private communication).\nChapter 6: Streaming Algorithms for Winner Determination\n⊲ An immediate future direction of research is to find optimal algorithms for heavy hitters\nvariants for other voting rule.\n⊲ It may be interesting to implement these streaming algorithms for use in practice (say,\nfor participatory democracy experiments or for online social networks) and investigate how they perform.\n⊲ Finally, instead of having the algorithms which passive observes a few random votes,\ncould we improve performance of the algorithm by actively querying voters as they appear in the stream?\nChapter 7: Kernelization for Possible Winner and Coalitional Manipula-\ntion\n⊲ There are other interesting parameterizations of these problems for which fixed param-\neter tractable algorithms are known but the corresponding kernelization questions are still open. One such parameter is the total number of pairs s in all the votes for which an ordering has not been specified. With this parameter, a simple O(2s. poly(m,n)) algorithm is known [BHN09]. However, the corresponding kernelization question is still open.\n⊲ Another interesting problem in the context of incomplete votes is the necessary winner\nproblem which asks for a candidate which wins in every extension of the partial votes. Necessary winner is known to be intractable for Copeland, ranked pairs, voting trees, and STV voting rules [XC11]. Studying parameterized complexity of the necessary winner problem is also another interesting direction of research to pursue in future.\n255\nChapter 8: Manipulation with Partial Votes\n⊲ we leave open the problem of completely establishing the complexity of strong, oppor-\ntunistic, and weak manipulations for all the scoring rules. It would be interesting to resolve it.\n⊲ Other fundamental forms of manipulation and control do exist in voting, such as de-\nstructive manipulation and control by adding candidates. It would be interesting to investigate the complexity of these problems in a partial information setting.\n⊲ Another exciting direction is the study of average case complexity, as opposed to the\nworst case results that we have pursued. These studies have already been carried out in the setting of complete information [PR06, FP10, Wal10]. Studying the problems that we propose in the average-case model would reveal further insights on the robustness of the incomplete information setting as captured by our model involving partial orders.\nChapter 9: Manipulation Detection\n⊲ In this work, we considered elections with unweighted voters only. An immediate future\nresearch direction is to study the complexity of these problems in weighted elections.\n⊲ Verifying the number of false manipulators that this model catches in a real or synthetic\ndata set, where, we already have some knowledge about the manipulators, would be interesting.\nChapter 10: Frugal Bribery\n⊲ A potential and natural direction for future work is to study these problems under\nvarious other settings. One obvious setting is to restrict the campaigner’s knowledge about the votes and/or the candidates who will actually turn up. The uncertainty can also arise from the voting rule that will eventually be used among a set of voting rules.\n⊲ Studying these bribery problems when the pricing model for vulnerable votes is similar\nto swap bribery would be another interesting future direction.\n256"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In many real world situations, especially involving multiagent systems and artificial intelligence, participating agents often need to agree upon a common alternative even if they have differing preferences over the available alternatives. Voting is one of the tools of choice in these situations. Common and classic applications of voting in modern applications include collaborative filtering and recommender systems, metasearch engines, coordination and planning among multiple automated agents etc. Agents in these applications usually have computational power at their disposal. This makes the study of computational aspects of voting crucial. This thesis is devoted to a study of computational complexity of several fundamental algorithmic and complexity-theoretic problems arising in the context of voting theory. The typical setting for our work is an “election”; an election consists of a set of voters or agents, a set of alternatives, and a voting rule. The vote of any agent can be thought of as a ranking (more precisely, a complete order) of the set of alternatives. A voting profile comprises a collection of votes of all the agents. Finally, a voting rule is a mapping that takes as input a voting profile and outputs an alternative, which is called the “winner” or “outcome” of the election. Our contributions in this thesis can be categorized into three parts and are described below. Part I: Preference Elicitation. In the first part of the thesis, we study the problem of eliciting the preferences of a set of voters by asking a small number of comparison queries (such as who a voter prefers between two given alternatives) for various interesting domains of preferences. We commence with considering the domain of single peaked preferences on trees in Chapter 3. This domain is a significant generalization of the classical well studied domain of single peaked preferences. The domain of single peaked preferences and its generalizations are",
    "creator" : "LaTeX with hyperref package"
  }
}