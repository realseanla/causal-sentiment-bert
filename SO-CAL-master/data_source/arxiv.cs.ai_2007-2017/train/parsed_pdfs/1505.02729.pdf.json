{
  "name" : "1505.02729.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sample Complexity of Learning Mahalanobis Distance Metrics",
    "authors" : [ "Nakul Verma", "Kristin Branson" ],
    "emails" : [ "verman@janelia.hhmi.org;", "bransonk@janelia.hhmi.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In many machine learning tasks, data is represented in a high-dimensional Euclidean space where each dimension corresponds to some interesting measurement of the observation. Often, practitioners include a variety of measurements in hopes that some combination of these features will capture the relevant information. While it is natural to represent such data in a Real space of measurements, there is no reason to expect that using Euclidean (L2) distances to compare the observations will be necessarily useful for the task at hand. Indeed, the presence of uninformative or mutually correlated measurements simply inflates the L2-distances between pairs of observations, rendering distance-based comparisons ineffective.\nMetric learning has emerged as a powerful technique to learn a good notion of distance or a metric in the representation space that can emphasize the feature combinations that help in the predication task while suppressing the contribution of spurious measurements. The past decade has seen a variety of successful metric learning algorithms that leverage various attributes of the problem domain. A few notable examples include exploiting class labels to find a Mahalanobis distance metric that maximizes the distance between dissimilar observations while minimizing distances between similar ones to improve classification quality (Weinberger & Saul, 2009; Davis et al., 2007), ∗email: verman@janelia.hhmi.org; corresponding author. †email: bransonk@janelia.hhmi.org\nar X\niv :1\n50 5.\n02 72\n9v 1\n[ cs\n.L G\n] 1\n1 M\nand explicitly optimizing for a downstream prediction task such as information retrieval (McFee & Lanckriet, 2010).\nDespite the popularity of metric learning methods, few studies have focused on studying how the problem complexity scales with key attributes of a given dataset. For instance, how do we expect the generalization error to scale—both theoretically and practically—as one varies the number of informative and uninformative measurements, or changes the noise levels?\nHere we study supervised metric learning more formally and gain a better understanding of how different modalities in data affect the metric learning problem. We develop two general frameworks for PAC-style analysis of supervised metric learning. We can categorize the popular metric learning algorithms into an empirical error minimization problem in one of the two frameworks. The first generic framework, the distance-based metric learning framework, uses class label information to derive distance constraints. The key objective is to learn a metric that on average yields smaller distances between examples from the same class than those from different classes. Some popular algorithms that optimize for such distance-based objectives include Mahalanobis Metric for Clustering (MMC) by Xing et al. (2002) and Information Theoretic Metric Learning (ITML) by Davis et al. (2007). Instead of using distance comparisons as a proxy, however, one can also optimize for a specific prediction task directly. The second generic framework, the classifier-based metric learning framework, explicitly incorporates the hypothesis associated with the prediction task of interest to learn effective distance metrics. A few interesting examples in this regime include the work by McFee & Lanckriet (2010) that finds metrics that improve ranking quality in information retrieval tasks, and the work by Shaw et al. (2011) that learns metrics that help predict connectivity structure in networked data.\nOur analysis shows that in both frameworks, the sample complexity scales with the representation dimension for a given dataset (Lemmas 1 and 3), and this dependence is necessary in the absence of any specific assumptions on the underlying data distribution (Lemmas 2 and 4). By considering any Lipschitz loss, our results generalize previous sample complexity results (see our discussion in Section 6) and, for the first time in the literature, provide matching lower bounds.\nIn light of the observation made earlier that data measurements often include uninformative or weakly informative features, we expect a metric that yields good generalization performance to deemphasize such features and accentuate the relevant ones. We can thus formalize the metric learning complexity of a given dataset in terms of the intrinsic complexity d of the metric that reweights the features in a way that yields the best generalization performance. (For Mahalanobis distance metrics, we can characterize the intrinsic complexity by the norm of the matrix representation of the metric.) We refine our sample complexity result and show a dataset-dependent bound for both frameworks that scales with dataset’s intrinsic metric learning complexity d (Corollary 7).\nTaking guidance from our dataset-dependent result, we propose a simple variation on the empirical risk minimizing (ERM) algorithm that, when given an i.i.d. sample, returns a metric (of complexity d̂) that jointly minimizes the observed sample bias and the expected intra-class variance for metrics of fixed complexity d̂. This bias-variance balancing algorithm can be viewed as a structural risk minimizing algorithm that provides better generalization performance than an ERM algorithm and justifies norm-regularization of weighting metrics in the optimization criteria for metric learning.\nFinally, we evaluate the practical efficacy of our proposed norm-regularization criteria with some popular metric learning algorithms on benchmark datasets (Section 5). Our experiments highlight that the norm-regularization indeed helps in learning weighting metrics that better adapt to the signal in data in high-noise regimes."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Given a representation space X = RD of D real-valued measurements of observations of interest, the goal of metric learning is to learn a metric M (that is, a D × D real-valued weighting matrix on X; to remove arbitrary scaling we shall assume that the maximum singular value of M , that is, σmax(M) = 1)1 that minimizes some notion of error on data drawn from an unknown underlying distribution D on X × {0, 1}. Specifically, we want to find the metric\nM∗ := argminM∈M err(M,D),\nfrom the class of metricsM under consideration, that is,M := {M |M ∈ RD×D, σmax(M) = 1}. For supervised metric learning, this error is typically label-based and can be defined in multiple reasonable ways. As discussed earlier, we explore two intuitive regimes for defining error.\nDistance-based error. A popular criterion for quantifying error in metric learning is by comparing distances amongst points drawn from the underlying data distribution. Ideally, we want a weighting metric M that brings data from the same class closer together than those from opposite classes. In a distance-based framework, a natural way to accomplish this is to find a weighting M that yields shorter distances between pairs of observations from the same class than those from different classes. By penalizing how often and by how much the distances violate these constraints gives rise to the particular form of the error.\nLet the variable z = (x, y) denote a random draw from D with x ∈ X as the observation and y ∈ {0, 1} its associated label, and let λ denote how severely one wants to penalize the distance violations, then a natural definition of distance-based error becomes:\nerrλdist(M,D) := Ez1,z2∼D [ φλ ( ρ M (x1, x2), Y )] ,\nfor a generic distance-based loss function φλ(ρ M , Y ), that computes the degree of violation between weighted distance ρ M\n(x1, x2) := ‖M(x1 − x2)‖2 and the label agreement Y := 1[y1 = y2] among a pair z1 = (x1, y1) and z2 = (x2, y2) drawn from D.\nAn example instantiation of φ popular in literature encourages metrics that yield distances that are no more than some upper limit U between observations from the same class, and distances that are no less than some lower limit L between those from different classes (for some U < L). Thus\nφλL,U (ρM , Y ) :=\n{ min{1, λ[ρ\nM −U ] + } if Y = 1\nmin{1, λ[L− ρM ]+} otherwise , (1)\nwhere [A] + := max{0, A}. Xing et al. (2002) optimize an efficiently computable variant of this criterion, in which they look for a metric that keeps the total pairwise distance amongst the observations from the same class less than a constant while maximizing the total pairwise distance amongst the observations from opposite classes. The variant proposed by Davis et al. (2007) explicitly includes the upper and lower limits with an added regularization on the learned M to be close to a pre-specified metric of interest M0.\nWhile we discuss loss-functions φ that handle distances between a pair of observations, it is easy to extend to distances among triplets. Rather than having hard upper and lower limits which every\n1Note that we are looking at the linear form of the metric M ; usually the corresponding quadratic form MTM is discussed in the literature, which is necessarily positive semi-definite.\npair of the same and the opposite classes must obey, a triplet-based comparison typically focuses on relative distances between three observations at a time. A natural instantiation in this case becomes:\nφλtriple(ρM(x1, x2), ρM(x1, x3), (y1, y2, y3)) := { min{1, λ[ρM(x1, x2)− ρM(x1, x3)]+} if y1 = y2 6= y3 0 otherwise ,\nfor a triplet (x1, y1), (x2, y2), (x3, y3) drawn from D. Weinberger & Saul (2009) discuss an interesting variant of this, in which instead of looking at all triplets in a given training sample, they focus on triplets of observations in local neighborhoods and learn a metric that maintains a gap or a margin among distances between observations from the same class and those from the opposite class. Improving the quality of distance comparisons in local neighborhoods directly affects the nearest neighbor performance, making this a popular technique.\nClassifier-based Error. Distance comparisons typically act as a surrogate for a specific downstream prediction task. If we want a metric that directly optimizes for a task, we need to explicitly incorporate the hypothesis class being used for that task while finding a good weighting metric.\nThis simple but effective insight has been used recently by McFee & Lanckriet (2010) for improving ranking results in information retrieval problems by explicitly incorporating ranking losses while learning an effective weighting metric. Shaw et al. (2011) also follow this principle and explicitly include network topology constraints to learn a weighting metric that can better predict the connectivity structure in social and web networks.\nWe can formalize the classifier-based metric learning framework by considering a fixed hypothesis classH of interest on the measurement domain. To keep the discussion general, we shall assume that the hypotheses are real-valued and can be regarded as a measure of confidence in classification, that is, each h ∈ H is of the form h : X → [0, 1]. (One can obtain the binary predictions from h by a simple thesholding at 1/2.) Then, the error induced by a particular weighting metric M on the measurement space X can be defined as the best possible error that can be obtained by hypotheses inH, that is\nerrhypoth(M,D) := inf h∈H\nE(x,y)∼D [ 1 [ |h(Mx)− y| ≥ 1/2 ]] .\nWe shall study how this error scales with various key parameters of the metric learning problem."
    }, {
      "heading" : "3 Learning a Metric from Samples",
      "text" : "In any practical setting, we estimate the ideal weighting metric M∗ by minimizing the empirical version of the error criterion from a finite size sample from D.\nLet Sm denote a sample of size m, and err(M,Sm) denote the empirical error on the sample Sm (the exact definitions of Sm and the form of err(M,Sm) are discussed later). We can then define the empirical risk minimizing metric based on m samples as M∗m := argminM err(M,Sm). Most practical algorithms, of course, return some approximation of M∗m, and thus it is important to compare the generalization ability of M∗m to that of theoretically optimal M ∗. That is, how\nerr(M∗m,D)− err(M∗,D) (2)\nconverges as the sample size m grows."
    }, {
      "heading" : "3.1 Distance-Based Error Analysis",
      "text" : "Given an i.i.d. sequence of observations z1, z2, . . . from D, we can pair the observations together to form a paired sample Sm = {(z1, z2), (z3, z4), . . . , (z2m−1, z2m)} = {(z1,i, z2,i)}mi=1 of size m, and define the sample based distance error errλdist(M,Sm) induced by a metric M as\nerrλdist(M,Sm) := 1\nm m∑ i=1 φλ ( ρM(x1,i, x2,i),1[y1,i = y2,i] ) .\nThen for any bounded support distribution D (that is, each (x, y) ∼ D, ‖x‖ ≤ B < ∞), we have the following convergence result.2\nLemma 1 Fix any sample size m, and let Sm be an i.i.d. paired sample of size m from an unknown bounded distribution D (with bound B). For any distance-based loss function φλ that is λ-Lipschitz in the first argument, with probability at least 1− δ over the draw of Sm,\nsup M∈M\n[ errλdist(M,D)− errλdist(M,Sm) ] ≤ O ( λB2 √ D ln(1/δ)\nm\n) .\nUsing this lemma we can get the desired convergence rate (Eq. 2). Fix M∗ ∈ M, then for any 0 < δ < 1 and m ≥ 1, with probability at least 1− δ, we have\nerrλdist(M ∗ m,D)− errλdist(M∗,D)\n= errλdist(M ∗ m,D)− errλdist(M∗m, Sm) + errλdist(M∗m, Sm)− errλdist(M∗, Sm)\n+ errλdist(M ∗, Sm)− errλdist(M∗,D)\n≤ O ( λB2 √ D ln(1/δ)\nm\n) + √ ln(2/δ)\n2m\n= O ( λB2 √ D ln(1/δ)\nm\n) ,\nby noting (i) errλdist(M ∗ m, Sm) ≤ errλdist(M∗, Sm), since M∗m is empirical error minimizing on Sm, and (ii) by using Hoeffding’s inequality on the fixed M∗ to conclude that with probability at least\n1− δ/2, errλdist(M∗, Sm)− errλdist(M∗,D) ≤ √ ln(2/δ) 2m .\nThus to achieve a specific estimation error rate , the number of samplesm = Ω (( λB2 )2 D ln( 1δ ) ) are sufficient to conclude, with confidence at least 1− δ, the empirical risk minimizing metric M∗m will have estimation error of at most . This shows that one never needs more than a number proportional to the representation dimension D examples to achieve the desired level of accuracy.\nSince typical applications have a large representation dimension, it is instructive to study if such a strong dependency on D necessary. It turns out that even for simple distance-based loss functions like φλL,U (cf. Eq. 1), there are data distributions for which one cannot get away with fewer than linear in D samples and ensure good estimation errors. In particular we have the following.\nLemma 2 Let A be any algorithm that, given an i.i.d. sample Sm (of size m) from a fixed unknown bounded support distribution D, returns a weighting metric fromM that minimizes the empirical\n2We only present the results for paired distance comparisons; the results are easily extended to triplet-based comparisons.\nerror with respect to distance-based loss function φλL,U . There exist λ ≥ 0, 0 ≤ U < L, such that for all 0 < , δ < 1/64, there exists a bounded support distribution D, such that if m ≤ D+1512 2 then\nPSm [ errλdist(A(Sm),D)− errλdist(M∗,D) > ] > δ.\nWhile this may seem discouraging for large-scale applications of metric learning, note that here we made no assumptions about the underlying structure of the data distribution D, making this a worst-case analysis. As the individual features in real-world datasets contain varying amounts of information for good classification performance, one hopes for a more relaxed dependence on D for metric learning in these settings. This is explored in Section 4."
    }, {
      "heading" : "3.2 Classifier-Based Error Analysis",
      "text" : "In this setting, we can use an i.i.d. sequence of observations z1, z2, . . . from D to obtain the sample Sm = {zi}mi=1 of size m directly. To analyze the generalization ability of the weighting metrics optimized with respect to an underlying hypothesis class H, we need to effectively analyze the classification complexity of H. The scale sensitive version of VC-dimension, also known as the “fat-shattering dimension”, of a real-valued hypothesis class (denoted by Fatγ(H)) encodes the right notion of classification complexity and provides an intuitive way to relate the generalization error to the empirical error at a margin γ (see for instance the work of Anthony & Bartlett (1999) for an excellent discussion).\nIn the context of metric learning with respect to a fixed hypothesis class, define the empirical error at a margin γ as\nerrγhypoth(M,Sm) := inf h∈H\n1\nm ∑ (xi,yi)∈Sm 1[Margin(h(Mxi), yi) < γ],\nwhere Margin(ŷ, y) := { ŷ − 1/2 if y = 1\n1/2− ŷ otherwise .\nThen for any bounded support distribution D (that is, each (x, y) ∼ D, ‖x‖ ≤ B < ∞), we have the following convergence result that relates the estimation error rate of the weighting metrics with that of the fat-shattering dimension of the underlying base hypothesis class.\nLemma 3 Let H be a λ-Lipschitz base hypothesis class. Pick any 0 < γ < 1/2, and let m ≥ Fatγ/16(H) ≥ 1. Then with probability at least 1 − δ over an i.i.d. draw of sample Sm (of size m) from a bounded unknown distribution D (with bound B) on X × {0, 1},\nsup M∈M\n[ errhypoth(M,D)− errγhypoth(M,Sm) ] ≤ O\n(√ 1\nm ln\n1 δ + D2 m ln D 0 + Fatγ/16(H) m ln (m γ )) .\nwhere 0 := min{γ2 , 1 2λB }, and Fatγ/16(H) is the fat-shattering dimension of the base hypothesis classH at margin γ/16.\nUsing a similar line of argument as before, we can bound the key quantity of interest (Eq. 2) and conclude for any 0 < γ < 1/2 and any m ≥ 1, with probability ≥ 1− δ\nerrhypoth(M∗m,D)− err γ hypoth(M ∗,D) = O\n(√ D2 ln(D/ 0)\nm + Fatγ/16(H) ln(m/δγ) m\n) .\nHere 0 = min{γ2 , 1 2λB } for a λ-Lipschitz hypothesis classH. Thus to achieve a specific estimation error rate , the number of samples m = Ω ( D2 ln(λDB/γ)+Fatγ/16(H) ln(1/δγ)\n2\n) suffices to say, with\nconfidence at least 1 − δ, the empirical risk minimizing metric M∗m will have estimation error at most .\nIt is interesting to note that the task of finding an optimal metric only additively increases the sample complexity over the complexity of finding the optimal hypothesis from the underlying hypothesis class.\nIn contrast to the sample complexity of distance-based framework (c.f. Lemma 1), here we get a quadratic dependence on the representation dimension. The following lemma shows that a strong dependence on the representation dimension is necessary in absence of any specific assumptions on the underlying data distribution and the base hypothesis class.\nLemma 4 Pick any 0 < γ < 1/8. Let H be a base hypothesis class of λ-Lipschitz functions mapping from X = RD into the interval [1/2 − 4γ, 1/2 + 4γ] that is closed under addition of constants. That is\nh ∈ H =⇒ h′ ∈ H, where h′ : x→ h(x) + c for all c.\nThen for any classification algorithm A, and for any B ≥ 1, there exists λ ≥ 0, for all 0 < , δ < 1/64, there exists a bounded support distribution D (with bound B) such that if m ln2m < O (\nD2+d 2 ln(1/γ2) ) PSm∼D[errhypoth(h ∗,D) > errγhypoth(A(Sm),D) + ] > δ,\nwhere d := Fat768γ(H) is the fat-shattering dimension ofH at margin 768γ."
    }, {
      "heading" : "4 Data with Uninformative and Weakly Informative Features",
      "text" : "Different measurements have varying degrees of “information content” for the particular supervised classification task of interest. Any algorithm or analysis that studies the design of effective comparisons between observations must account for this variability.\nTo get a solid footing for our study, we introduce the concept of metric learning complexity of a given dataset. Our key observation is that a metric that yields good generalization performance should emphasize relevant features while suppressing the contribution of spurious features. Thus, a good metric reflects the quality of individual feature measurements of data and their relative value for the learning task. We can leverage this and define the metric learning complexity of a given dataset as the intrinsic complexity d of the weighting metric that yields the best generalization performance for that dataset (if multiple metrics yield best performance, we select the one with minimum d). A natural way to characterize the intrinsic complexity of a weighting metric M is via the norm of the matrix representation of M . Using metric learning complexity as our gauge for the richness of the feature set in a given dataset, we can refine our analysis in both our canonical metric learning frameworks."
    }, {
      "heading" : "4.1 Distance-Based Refinement",
      "text" : "We start with the following refinement of the distance-based metric learning sample complexity for a class of Frobenius norm-bounded weighting metrics.\nLemma 5 LetM be any class of weighting metrics on the feature space X = RD. Fix any sample size m, and let Sm be an i.i.d. paired sample of size m from an unknown bounded distribution D on X × {0, 1} (with bound B). For any distance-based loss function φλ that is λ-Lipschitz in the first argument, with probability at least 1− δ over the draw of Sm,\nsup M∈M\n[ errλdist(M,D)− errλdist(M,Sm) ] ≤ O ( λB2 √ d ln(1/δ)\nm\n) ,\nwhere d is a uniform upperbound on the Frobenius norm of the quadratic form of weighting metrics inM, that is, supM∈M ‖MTM‖2F ≤ d.\nObserve that if our dataset has a low metric learning complexity (say, d D), then considering an appropriate class of norm-bounded weighting metrics can help sharpen the sample complexity result, yielding a dataset-dependent bound. We discuss how to automatically adapt to the right complexity class in Section 4.3 below."
    }, {
      "heading" : "4.2 Classifier-Based Refinement",
      "text" : "Effective data-dependent analysis of classifier-based metric learning requires accounting for potentially complex interactions between an arbitrary base hypothesis class and the distortion induced by a weighting metric to the unknown underlying data distribution. To make the analysis tractable while still keeping our base hypothesis class H general, we shall assume that H is a class of two layer feed-forward neural networks. Recall that for any smooth target function f∗, a two layer feed-forward neural network (with appropriate number of hidden units and connection weights) can approximate f∗ arbitrarily well (Hornik et al., 1989), so this class is flexible enough to incorporate most reasonable target hypotheses.\nMore formally, define the base hypothesis class of two layer feed-forward neural network with K hidden units as\nH2-netσγ := { x 7→ K∑ i=1 wi σ γ(vi · x) ∣∣∣ ‖w‖1 ≤ 1, ‖vi‖1 ≤ 1}, where σγ : R → [−1, 1] is a smooth, strictly monotonic, γ-Lipschitz activation function with σγ(0) = 0. Then for the generalization error of a weighting metric M defined with respect to any classifier-based λ-Lipschitz loss function φλ\nerrλhypoth(M,D) := inf h∈H2-net\nσγ\nE(x,y)∼D [ φλ ( h(Mx), y )] ,\nwe have the following.3\nLemma 6 LetM be any class of weighting metrics on the feature space X = RD. For any γ > 0, let H2-netσγ be a two layer feed-forward neural network base hypothesis class (as defined above) and φλ be a classifier-based loss function that λ-Lipschitz in its first argument. Fix any sample size m, and let Sm be an i.i.d. sample of size m from an unknown bounded distribution D on X × {0, 1} (with bound B). Then with probability at least 1− δ,\nsup M∈M\n[ errλhypoth(M,D)− errλhypoth(M,Sm) ] ≤ O ( Bλγ √ d ln(D/δ)\nm\n) ,\n3Since we know the functional form of the base hypothesis class H (i.e., a two layer feed-forward neural net), we can provide a more precise bound than leaving it as Fat(H).\nwhere d is a uniform upperbound on the Frobenius norm of the quadratic form of weighting metrics inM, that is, supM∈M ‖MTM‖2F ≤ d."
    }, {
      "heading" : "4.3 Automatically Adapting to Intrinsic Complexity",
      "text" : "Note that while Lemmas 5 and 6 provide a sample complexity bound that is tuned to the metric learning complexity of a given dataset, these results are not useful directly since one cannot select the correct norm bounded classM a priori (as the underlying distribution D is unknown).\nFortunately, by considering an appropriate sequence of norm-bounded classes of weighting metrics, we can provide a uniform bound that automatically adapts to the intrinsic complexity of the unknown underlying data distribution D. In particular, we have the following.\nCorollary 7 Fix anym, and let Sm be an i.i.d. sample of sizem from an unknown bounded distribution D (with bound B). DefineMd := {M | ‖MTM‖2\nF ≤ d}, and consider the nested sequence of\nweighting metric classM1 ⊂M2 ⊂ · · · . Let µd be any non-negative measure across the sequence Md such that ∑ d µd = 1 (for d = 1, 2, · · · ). Then for any λ ≥ 0, with probability at least 1 − δ, for all d = 1, 2, · · · , and all Md ∈Md,\n[ errλ(Md,D)− errλ(Md, Sm) ] ≤ O ( C ·Bλ √ d ln(1/δµd)\nm\n) , (3)\nwhere C := B for distance-based error, or C := γ √\nlnD for classifier-based error (with base hypothesis classH2-netσγ ).\nIn particular, for a data distribution D that has metric learning complexity at most d ∈ N, if there are m ≥ Ω ( d(CBλ)2 ln(1/δµd)\n2 ) samples, then with probability at least 1− δ[\nerrλ(M regm ,D)− errλ(M∗,D) ] ≤ O( ),\nforM regm :=argminM∈M [ errλ(M,Sm) + ΛMdM ] , where Λ M := CBλ √ ln (\n1 δµ dM\n) /m and d\nM :=⌈\n‖MTM‖2 F\n⌉ .\nObserve that the measure (µd) above encodes our prior belief on the complexity classMd from which a target metric is selected by a metric learning algorithm given the training sample Sm. In absence of any prior beliefs, µd can be simply set to 1/D (for d = 1, . . . , D) for unit spectral-norm weighting metrics.\nThus, for an unknown underlying data distribution D with metric learning complexity d, with number of samples just proportional to d, we can find a good weighting metric.\nThis result also highlights that the generalization error of any weighting metric returned by an algorithm is proportional to the (smallest) norm-bounded class to which it belongs (cf. Eq. 3). If two metrics M1 and M2 have similar empirical errors on a given sample, but have different intrinsic complexities, then the expected risk of the two metrics can be considerably different. We expect the metric with lower intrinsic complexity to yield better generalization error. This partly explains the observed empirical success of various types of norm-regularized optimization criteria for finding the optimal weighting metric (Lim et al., 2013; Law et al., 2014).\nUsing this as a guiding principle, we can design an improved optimization criteria for metric learning problems that jointly minimizes the sample error and a Frobenius norm regularization penalty. In particular,\nmin M∈M\nerr(M,Sm) + Λ ‖MTM‖2F (4)\nfor any error criteria ‘err’ used in a downstream prediction task of interest and a regularization hyper-parameter Λ proportional to m−1/2. We explore the practical efficacy of this augmented optimization on some representative applications below."
    }, {
      "heading" : "5 Empirical Evaluation",
      "text" : "Our analysis shows that the generalization error of metric learning can scale with the representation dimension, and regularization can help mitigate this by adapting to the intrinsic metric learning complexity of the given dataset. We want to explore to what degree these effects manifest in practice.\nWe select two popular metric learning algorithms, LMNN by Weinberger & Saul (2009) and ITML by Davis et al. (2007), that are designed to find metrics that improve nearest-neighbor classification quality. These algorithms have varying degrees of regularization built into their optimization criteria: LMNN implicitly regularizes the metric via its “large margin” criterion, while ITML allows for explicit regularization by letting the practitioners specify a “prior” weighting metric. We modified the LMNN optimization criteria as per Eq. (4) to also allow for an explicit norm-regularization controlled by the trade-off parameter Λ.\nWe can evaluate how the unregularized criteria (i.e., unmodified LMNN, or ITML with the prior set to the identity matrix) compares to the regularized criteria (i.e., modified LMNN with best Λ, or ITML with the prior set to a low-rank matrix).\nDatasets. We use the UCI benchmark datasets for our experiments: IRIS (4 dim., 150 samples), WINE (13 dim., 178 samples) and IONOSPHERE (34 dim., 351 samples) datasets (Bache & Lichman, 2013). Each dataset has a fixed (unknown) intrinsic dimension; we can vary the representation dimension by augmenting each dataset with synthetic correlated noise of varying dimensions, simulating regimes where datasets contain large numbers of uninformative features.\nEach UCI dataset is augmented with synthetic D-dimensional correlated noise as follows. We first sample a covariance matrix ΣD from unit-scale Wishart distribution (that is, let A be a D ×D Gaussian random matrix with entry Aij ∼ N(0, 1) drawn i.i.d., and set ΣD := ATA). Then each sample xi from the dataset is appended independently by drawing noise vector xσ ∼ N(0,ΣD).\nExperimental setup. We varied the ambient noise dimension D between 0 and 500 dimensions and\nadded it to the UCI datasets, creating the noise-augmented datasets. Each noise-augmented dataset was randomly split between 70% training, 10% validation, and 20% test samples.\nWe used the default settings for each algorithm. For regularized LMNN, we picked the best performing trade-off parameter Λ from {0, 0.1, 0.2, ..., 1} on the validation set. For regularized ITML, we seeded with the rank-one discriminating metric, i.e., we set the prior as the matrix with all zeros, except the diagonal entry corresponding to the most discriminating coordinate set to one.\nAll the reported results were averaged over 20 runs.\nResults. Figure 1 shows the nearest-neighbor performance (with k = 3) of LMNN and ITML on noise-augmented UCI datasets. Notice that the unregularized versions of both algorithms (dashed red lines) scale poorly when noisy features are introduced. As the number of uninformative features grows, the performance of both algorithms quickly degrades to that of classification performance in the original unweighted space with no metric learning (solid gray line), showing poor adaptability to the signal in the data.\nInterestingly, neither of the unregularized algorithms performs consistently better than the other on datasets with high noise: ITML yields better results on WINE, whereas LMNN seems better for IONOSPHERE, and both algorithms yield similar performance on IRIS.\nThe regularized versions of both algorithms (solid blue lines) significantly improve the classification performance. Remarkably, regularized ITML shows almost no degradation in classification performance, even in very high noise regimes, demonstrating a strong robustness to noise.\nThese results underscore the value of regularization in metric learning, showing that regularization encourages adaptability to the intrinsic complexity and improved robustness to noise."
    }, {
      "heading" : "6 Discussion and Conclusion",
      "text" : "Previous theoretical work on metric learning has focused almost exclusively on analyzing the generalization error of variants of the optimization criteria for the distance-based metric learning framework.\nJin et al. (2009), for instance, analyzed the generalization ability of regularized, convex-loss optimization criteria for pairwise distances via an algorithmic stability analysis. They derive an interesting sample complexity result that is sublinear in √ D for datasets of representation dimension D. They discuss that the sample complexity can potentially be independent of D, but do not characterize specific instances or classes of problems where this may be possible.\nLikewise, recent work by Bellet & Habrard (2012) uses algorithmic robustness to analyze the generalization ability for pairwise- and triplet-based distance metric learning. Their analysis relies on the existence of a partition of the input space, such that in each cell of the partition, the training loss and test loss does not deviate much (robustness criteria). Note that their sample complexity bound scales with the partition size, which in general can be exponential in the representation dimension.\nPerhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al. (2013). Bian & Tao (2011) analyze the consistency of the ERM criterion for metric learning. They show a O(m−1/2) rate of convergence for the ERM with m samples to the expected risk for thresholds on bounded convex losses for distance-based metric learning. Our upper-bound in Lemma 1 generalizes this result by considering arbitrary (possibly non-convex) distance-based Lipschitz losses and explicitly shows the dependence on the representation dimensionD. Cao et al. (2013) provide an alternate analysis based on norm regularization of the weighting metric for distance-based metric learning. Their result parallels our norm-regularized criterion in\nLemma 5. While they focus on analyzing a specific optimization criterion – thresholds on the hinge loss with norm-regularization, our result holds for general Lipschitz losses.\nIt is worth emphasizing that none of these related works discuss the importance of or leverage the intrinsic structure in data for the metric learning problem. Our results in Section 4 formalize an intuitive notion of dataset’s intrinsic complexity for metric learning and show sample complexity rates that are finely tuned to this metric learning complexity.\nThe classifier-based framework we discuss has parallels with the kernel learning literature. The typical focus in kernel learning is to analyze the generalization ability of the hypothesis class of linear separators in general Hilbert spaces (Ying & Campbell, 2009; Cortes et al., 2010). Our work provides a complementary analysis for learning explicit linear transformations of the given representation space for arbitrary hypotheses classes.\nOur theoretical analysis partly justifies the empirical success of norm-based regularization as well. Our empirical results show that such regularization not only helps in designing new metric learning algorithms (Lim et al., 2013; Law et al., 2014), but can even benefit existing metric learning algorithms in high-noise regimes."
    }, {
      "heading" : "A Appendix: Various Proofs",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Lemma 1",
      "text" : "Let P be the probability measure induced by the random variable (X, Y ), where X := (x, x′), Y := 1[y = y′], st. ((x, y), (x′, y′)) ∼ (D×D).\nDefine function class\nF := { fM : X 7→ ‖M(x− x′)‖2 ∣∣∣∣∣ M ∈MX = (x, x′) ∈ (X ×X) } ,\nand consider any loss function φλ(ρ, Y ) that is λ-Lipschitz in the first argument. Then, we are interested in bounding the quantity\nsup fM∈F\nE(X,Y )∼P [φλ(fM (X), Y )]− 1\nm m∑ i=1 φλ(fM (Xi), Yi),\nwhere Xi := (x1,i, x2,i), Yi := 1[y1,i = y2,i] from the paired sample Sm = {((x1,i, y1,i), (x2,i, y2,i))}mi=1. Define x̄i := x1,i − x2,i for each Xi = (x1,i, x2,i). Then, the Rademacher complexity4 of our function class F (with respect to the distribution P) is bounded, since (let σ1, . . . , σm denote independent uniform {±1}-valued random variables)\nRm(F ,P) := EXi,σi i∈[m] [ sup fM∈F 1 m m∑ i=1 σifM (Xi) ]\n= 1\nm EXi,σi i∈[m] sup M∈M [ m∑ i=1 σix̄ T iM TMx̄i ] = 1\nm EXi,σi i∈[m] sup M∈M, s.t.\n[ajk]jk:=M TM\n[∑ j,k ajk m∑ i=1 σix̄ j i x̄ k i ]\n≤ 1 m EXi,σi i∈[m] sup M∈M\n[ ‖MTM‖F (∑ j,k ( m∑ i=1 σix̄ j i x̄ k i )2)1/2]\n≤ √ D\nm EXi,i∈[m]\n( Eσi,i∈[m] ∑ j,k ( m∑ i=1 σix̄ j i x̄ k i )2)1/2\n=\n√ D\nm EXi,i∈[m] (∑ j,k m∑ i=1 ( x̄ji )2( x̄ki )2)1/2\n=\n√ D\nm EXi,i∈[m] ( m∑ i=1 ‖x̄i‖4 )1/2\n=\n√ D\nm E(xi,x′i)∼(D |X×D |X),\ni∈[m]\n( m∑ i=1 ‖xi − x′i‖4 )1/2\n4See the definition of Rademacher complexity in the statement of Lemma 8.\n≤ √ D\nm\n( E(x,x′)∼(D |X×D |X)‖x− x ′‖4 )1/2\n≤ 4B2 √ D\nm ,\nwhere the second inequality is by noting that supM∈M ‖MTM‖F ≤ √ D for the class of weighting metricsM := { M |M ∈ RD×D, σmax(M) = 1 } .\nRecall that D has bounded support (with bound B). Thus, by noting that φλ is 8B2 bounded function that is λ-Lipschitz in the first argument, we can apply Lemma 8 and get the desired uniform deviation bound.\nLemma 8 [Rademacher complexity of bounded Lipschitz loss functions Bartlett & Mendelson (2002)] Let D be a fixed unknown distribution over X ×{−1, 1}, and let Sm be an i.i.d. sample of size m from D. Given a hypothesis class H ⊂ RX and a loss function ` : R×{−1, 1} → R, such that ` is c-bounded, and is λ-Lipschitz in the first argument, that is, sup(y′,y)∈R×{−1,1} |`(y′, y)| ≤ c, and |`(y′, y)− `(y′′, y)| ≤ λ|y′ − y′′|, we have the following:\nfor any 0 < δ < 1, with probability at least 1− δ, every h ∈ H satisfies\nerr(` ◦ h,D) ≤ err(` ◦ h, Sm) + 2λRm(H,D) + c √ 2 ln(1/δ)\nm ,\nwhere\n• err(` ◦ h,D) := Ex,y∼D[`(h(x), y)], • err(h, Sm) := 1m ∑ (xi,yi)∈Sm `(h(xi), yi),\n• Rm(H,D) is the Rademacher complexity of the function classH with respect to the distribution D given m i.i.d. samples, and is defined as:\nRm(H,D) := E xi∼D |X , σi∼unif{±1},\ni∈[m]\n[ sup h∈H 1 m m∑ i=1 σih(xi) ] ,\nwhere σi are independent uniform {±1}-valued random variables."
    }, {
      "heading" : "A.2 Proof of Lemma 2",
      "text" : "We shall exhibit a finite class of bounded support distributions D, such that ifD is chosen uniformly at random from D, the expectation (over the random choice ofD) of the probability of failure (that is, generalization error of the metric returned by A compared to that of the optimal metric exceeds the specified tolerance level ) is at least δ. This implies that for some distribution in D the probability of failure is at least δ as well.\nLet ∆D := {x0, . . . , xD} be a set of D + 1 points that from the vertices of a regular unitsimplex from the underlying space X = RD as per Definition 1 (see below). For a fixed parameter 0 < α < 1 (exact value determined later), define D as the class of all distributions D on X ×{0, 1} such that:\n• D assigns zero probability to all sets not intersecting ∆D × {0, 1}.\n• for each i = 0, . . . , D, either\n– P[(xi, 1)] = (1 + √ α)/2 and P[(xi, 0)] = (1− √ α)/2, or – P[(xi, 1)] = (1− √ α)/2 and P[(xi, 0)] = (1 + √ α)/2.\nFor concreteness, we shall use a specific instantiation of φλL,U in err λ dist with U = 0, L = 4/D\nand λ = D/4.\nProof overview. We first show, by the construction of the distributions under consideration in D, the sample error and the generalization error minimizing metrics over any D ∈ D belong to a restricted class of weighting matrices (Eq. 5). We then make a second simplification by noting that finding these (sample- and generalization-) error minimizing metrics (in the restricted class) is equivalent to solving a binary classification problem (Eq. 6). This reduction to binary classification enables us to use VC-style lower bounding techniques to give a lower bound on the sample complexity. We now fill in the details.\nConsider a subset of weighting metrics M0-1 that map points in ∆D to exactly one of two possible points that are (squared) distance at least 4/D apart, that is,\nM0-1 := {M |M ∈M,∃z0, z1 ∈ RD,∀x ∈ ∆D, Mx ∈ {z0, z1} and ‖z0 − z1‖2 ≥ 4/D}.\nNow pick any D ∈ D, let Sm be an i.i.d. paired sample from D. Observe that both the samplebased and the distribution-based error minimizing weighting metric fromM on D also belongs to M0-1. That is, (c.f. Lemma 10)\nargminM∈M errdist(M,D) = argminM∈M0-1 errdist(M,D) argminM∈M errdist(M,Sm) = argminM∈M0-1 errdist(M,Sm). (5)\nA reduction to binary classification on product space. For each M ∈ M0-1, we associate a classifier fM : (∆D × ∆D) → {0, 1} defined as (xi, xj) 7→ 1[Mxi = Mxj ]. Now, consider the probability measureP induced by the random variable (X, Y ), where X := (x, x′), Y := 1[y = y′], s.t. ((x, y), (x′, y′)) ∼ ( D |(∆D×{0,1}) ×D |(∆D×{0,1}) ) . It is easy to check that for all M ∈M0-1\nerrλdist(M,D) = E(X,Y )∼P [ 1[fM (X) 6= Y ] ] errλdist(M,Sm) = 1\nm ∑ ((x,y),(x′,y′))∈Sm 1 [ fM ((x, x ′)) 6= 1[y = y′] ] . (6)\nDefine\nη(X) := PY∼P|Y |X [Y = 1|X] = P(y,y′)∼(D×D)|(y,y′)|(x,x′) [y = y ′|x, x′]\n=\n{ 1 2 + α 2 if P(y|x) = P(y\n′|x′) 1 2 − α 2 if P(y|x) 6= P(y ′|x′) . (7)\nObserve that η(X) is the Bayes error rate at X for distribution P . Since, by construction ofM0-1, the class {fM}M∈M0-1 contains a classifier that achieves the Bayes error rate, the optimal classifier\nf∗ := argminfM E(X,Y )∼P 1[fM (X) 6= Y ] necessarily has f ∗(X) = 1[η(X) > 12 ] (for all X). Then, for any fM ,\nE(X,Y )∼P [ 1[fM (X) 6= Y ] ] − E(X,Y )∼P [ 1[f∗(X) 6= Y ] ] = EX∼P|X [ η(X) ( 1[f∗(X) = 1]− 1[fM (X) = 1]\n) + (1− η(X)) ( 1[f∗(X) = 0]− 1[fM (X) = 0]\n)] = EX∼P|X [ (2η(X)− 1) ( 1[f∗(X) = 1]− 1[fM (X) = 1]\n)] = EX∼P|X [ 2|η(X)− 1/2| · 1[fM (X) 6= f∗(X)]\n] = 2α\n(D + 1)2 ∑ i>j [ 1[fM ((xi, xj)) 6= f∗((xi, xj))] ] , (8)\nwhere (i) the second to last equality is by noting that f∗(X) 6= 1 ⇐⇒ η(X) ≤ 1/2, and (ii) the last equality is by noting Eq. (7), fM ((xi, xi)) = f∗((xi, xi)) = 1 for all i and f((xi, xj)) = f((xj , xi)) for all f . For notational simplicity, we shall define Xi,j := (xi, xj).\nNow, for a given paired sample Sm, let N(Sm) := (Ni)i (for all 0 ≤ i ≤ D), where Ni is the number of occurrences of the point xi in Sm. Then for any fM ,\nESm\n[ 1\n(D + 1)2 ∑ i>j 1[fM (Xi,j) 6= f∗(Xi,j)]\n]\n= 1\n(D + 1)2 ∑ i>j PSm [fM (Xi,j) 6= f∗(Xi,j)]\n= 1\n(D + 1)2 ∑ i>j ∑ N∈ND+1 PSm [fM (Xi,j) 6= f∗(Xi,j)|N(Sm) = N ] ·P[N(Sm) = N ]\n= 1\n(D + 1)2 ∑ N∈ND+1 P[N(Sm) = N ] · ∑ i>j PSm [fM (Xi,j) 6= f∗(Xi,j)|Ni, Nj ]\n≥ 1 (D + 1)2 ∑ N∈ND+1 P[N(Sm) = N ] · ∑ i>j 1 4\n( 1− √√√√1− exp(−(max{Ni, Nj}+ 1)α2 1− α2 ))\n≥ 1 4 D D + 1\n( 1− √√√√1− exp(−((2m/(D + 1)) + 1)α2 1− α2 ))\n≥ 1 8\n( 1− √√√√1− exp(−((2m/(D + 1)) + 1)α2 1− α2 )) ,\nwhere (i) the first inequality is by applying Lemma 11, (ii) the second inequality is by assuming WLOG Ni ≥ Nj , and noting that the expression above is convex in Ni so one can apply Jensen’s inequality and by observing that E[Ni] = 2m/(D+ 1) and that there are total D(D+ 1) summands for i > j, and (iii) the last inequality is by noting that D ≥ 1. Now, let B denote the r.h.s. quantity above. Then by recalling that for any [0, 1]-valued random variable Z, P(Z > γ) > EZ− γ (for all 0 < γ < 1), we have\nPSm [ 1 (D + 1)2 ∑ i>j 1[fM ((xi, xj)) 6= f∗((xi, xj))] > γB ] > (1− γ)B.\nOr equivalently, by combining Eqs. (5), (6) and (8), we have ED∼unif(D)PSm∼D [ errdist(A(Sm),D)− errdist(M∗D,D) > 2αγB ] > (1− γ)B,\nwhere M∗D := argminM∈M errdist(M,D) and A(Sm) is any metric returned by empirical error minimizing algorithm. Now, if (cond. 1) B ≥ δ/1 − γ and (cond. 2) ≤ 2γαB, it follows that for some D ∈ D\nPSm∼D [ errdist(A(Sm),D)− errdist(M∗D,D) > ] > δ. (9)\nNow, to satisfy cond. 1 & 2, we shall select γ = 1− 16δ. Then cond. 1 follows if\nm ≤ (D + 1) 2\n( 1− α2\nα2 ln(4/3)− 1\n) .\nChoosing parameter α = 8 /γ (and by noting B ≥ 1/16 by cond. 1 for choice of γ and m), cond. 2 is satisfied as well. Hence,\nm ≤ (D + 1) 2\n( (1− 16δ)2 − (8 )2\n64 2 ln(4/3)− 1\n)\nimplies Eq. (9). Moreover, if 0 < , δ < 1/64 then m ≤ (D+1)512 2 would suffice.\nDefinition 1 Define n+ 1 vectors ∆n = {v0, . . . , vn}, with each vi ∈ Rn as\nv0,j = −1√ n\nfor 1 ≤ j ≤ n\nvi,j =\n{ (n−1) √ n+1+1\nn √ n if i = j −( √ n+1−1) n √ n otherwise for 1 ≤ i, j ≤ n\nFact 9 [properties of vertices of a regular n-simplex] Let ∆n = {v0, . . . , vn} be a set of n + 1 vectors in Rn as per Definition 1. Then, ∆n defines vertices of a regular n-simplex circumscribed in a unit (n− 1)-sphere, with\n(i) ‖vi‖2 = 1 (for all i), and\n(ii) ‖vi − vj‖2 = 2(n+ 1)/n (for i 6= j).\nMoreover, for any non-empty bi-partition of ∆n into ∆ (1) n and ∆ (2) n with |∆(1)n | = k and |∆(2)n | = n + 1 − k, define a(1) and a(2) the means (centroids) of the points in ∆(1)n and ∆(2)n respectively. Then, we also have\n(i) (a(1) − a(2)) · (a(i) − vj) = 0 (for i ∈ {1, 2}, and vj ∈ ∆(i)n ).\n(ii) ‖a(1) − a(2)‖2 = (n+1) 2\nkn(n+1−k) ≥ 4 n , for 1 ≤ k ≤ n.\nLemma 10 Let ∆D be a set of D + 1 points {X0, . . . , XD} in RD as per Definition 1, and let D be an arbitrary distribution over ∆D × {0, 1}. Define Pi := 1[PD[(Xi, 1)] > 1/2]. Define Π := {π : ∆D → RD} be the collection of all functions that maps points in ∆D to arbitrary points in RD. Define\nf((x, y),(x′, y′);π) :=\n{ min{1, D4 ‖π(x)− π(x\n′)‖2} if y = y′ min{1, [1− D4 ‖π(x)− π(x ′)‖2]+} if y 6= y′ .\nLet E(π) := E(x,y),(x′,y′)∼D×D[f((x, y), (x′, y′);π)] and E∗ := infπ E(π). Then, for any π̄ ∈ Π such that\n(i) π̄(Xi) = π̄(Xj), if Pi = Pj\n(ii) ‖π̄(Xi)− π̄(Xj)‖2 ≥ 4D , if Pi 6= Pj , we have that E(π̄) = E∗. Moreover, define Ā as • Ā := A1−A0‖A1−A0‖ , where A0 := mean(Xi) such that Pi = 0, and A1 := mean(Xi) such that Pi = 1 (if exists at least one Pi = 0 and at least one Pi = 1).\n• Ā := 0, i.e. the zero vector in RD (otherwise). And let M be a D ×D matrix (with σmax(M) = 1) defined as\nM := ĀĀ T .\nThen the map πM : x 7→ Mx constitutes a map that satisfies conditions (i) and (ii) and thus E(πM ) = E∗.\nProof. The proof follows from the geometric properties of ∆D and Fact 9.\nLemma 11 Given two random variables α1 and α2, each uniformly distributed on {α−, α+} independently, where α− = 1/2 − /2 and α+ = 1/2 + /2 with 0 < < 1. Suppose that ξ11 , . . . , ξ1m and ξ21 , . . . , ξ 2 m are two i.i.d. sequences of {0, 1}-valued random variables with P(ξ1i = 1) = α1 and P(ξ2i = 1) = α2 for all i. Then, for any likelihood maximizing function f from {0, 1}m to {α−, α+} that estimates the bias α1 and α2 from the samples,\nP [( f(ξ11 , . . . , ξ 1 m) 6= α1 and f(ξ21 , . . . , ξ2m) = α2 ) ,\nor ( f(ξ11 , . . . , ξ 1 m) = α1 and f(ξ 2 1 , . . . , ξ 2 m) 6= α2 )] > 1\n4\n( 1− √ 1− exp (−2dm/2e 2 1− 2 )) .\nProof. Note that P [( f(ξ11 , . . . , ξ 1 m) 6= α1 and f(ξ21 , . . . , ξ2m) = α2 ) , or ( f(ξ11 , . . . , ξ 1 m) = α1 and f(ξ 2 1 , . . . , ξ 2 m) 6= α2 )] = P[f(ξ11 , . . . , ξ 1 m) 6= α1] ·P[f(ξ21 , . . . , ξ2m) = α2] + P[f(ξ11 , . . . , ξ1m) = α1] ·P[f(ξ21 , . . . , ξ2m) 6= α2]\n≥ 1 2 P [ f(ξ11 , . . . , ξ 1 m) 6= α1 ] + 1 2 P [ f(ξ21 , . . . , ξ 2 m) 6= α2 ] > 1\n4\n( 1− √ 1− exp (−2dm/2e 2 1− 2 )) ,\nwhere the first inequality is by noting that a likelihood maximizing f will select the correct bias better than random (which has probability 1/2), and the second inequality is by applying Lemma 12.\nLemma 12 [Lemma 5.1 of Anthony & Bartlett (1999)] Suppose that α is a random variable uniformly distributed on {α−, α+}, where α− = 1/2− /2 and α+ = 1/2 + /2, with 0 < < 1. Suppose that ξ1, . . . , ξm are i.i.d. {0, 1}-valued random variables with P(ξi = 1) = α for all i. Let f be a function from {0, 1}m to {α−, α+}. Then\nP [ f(ξ1, . . . , ξm) 6= α ] > 1\n4\n( 1− √ 1− exp (−2dm/2e 2 1− 2 )) ."
    }, {
      "heading" : "A.3 Proof of Lemma 3",
      "text" : "For any M ∈ M define real-valued hypothesis class on domain X as HM := {x 7→ h(Mx) : h ∈ H} and define\nF := {x 7→ h(Mx) : M ∈M, h ∈ H} = ⋃ M HM .\nObserve that a uniform convergence of errors induced by the functions inF implies convergence of the class of weighted matrices as well.\nNow for any domain X , real-valued hypothesis class G ⊂ [0, 1]X , margin γ > 0, and a sample S ⊂ X , define\ncovγ(G, S) := { C ⊂ G ∣∣∣ ∀g ∈ G,∃g′ ∈ C, maxs∈S |g(s)− g′(s)| ≤ γ } as the set of γ-covers of S by G. Let γ-covering number of G for any integer m > 0 be defined as\nN∞(γ,G,m) := max S⊂X:|S|=m min C∈covγ(G,S) |C|,\nwith the minimizing cover C called as the minimizing (γ,m)-cover of G\nNow, for the given γ, we will first estimate the γ-covering number of F , that is, N∞(γ,F ,m). For any M ∈ M, let HM be the minimizing (γ/2,m)-cover of HM . Note that |HM | = N∞(γ/2,HM ,m) ≤ N∞(γ/2,H,m) (because MX ⊂ X). Now letM be an -spectral cover ofM (that is, for every M ∈M, exists M ′ ∈M such that σmax(M −M ′) ≤ ), and define\nF̄ := {x 7→ h(Mx) : M ∈M , h ∈ HM}.\nNote that |F̄ | ≤ |M ||HI | ≤ N∞(γ/2,H,m)(1 + 2D/ )D 2\n(c.f. Lemma 13). Observe that F̄ is a (γ/2 + Bλ )-cover of F , since (i) for any f ∈ F (formed by combining, say, M0 ∈ M and h0 ∈ H), exists f̄ ∈ F̄ , namely the f̄ formed by M̄0 such that σmax(M0 − M̄0) ≤ , and (ii) h̄0 ∈ HM̄0 such that |h0(M̄0x)− h̄0(M̄0x)| ≤ γ/2 (for all x ∈ X). So, (for any x ∈ X)\n|f(x)− f̄(x)| = |h0(M0x)− h̄0(M̄0x)| ≤ |h0(M0x)− h0(M̄0x)|\n+|h0(M̄0x)− h̄0(M̄0x)| ≤ λ‖M0x− M̄0x‖+ γ/2 ≤ λσmax(M0 − M̄0)‖x‖+ γ/2 ≤ λ B + γ/2.\nSo, if we pick = min{ 12λB , γ 2 }, it follows that\nN∞(γ,F ,m) ≤ |F̄ | ≤ N∞(γ/2,H,m)(1 + 2D/ )D 2 .\nBy noting Lemmas 14 and 15, it follows that\nPSm∼D [ ∃f ∈ F : err(f) ≥ errγ(f, Sm) + α ] ≤ 4 ( 1 + 2D )D2(128m γ2\n)Fatγ/16(H) ln( 32emFatγ/16(H)γ )e−α2m/8. The lemma follows by bounding this failure probability with at most δ.\nLemma 13 [ -spectral coverings of D ×D matrices] LetM := {M |M ∈ RD×D, σmax(M) = 1} be the set of matrices with unit spectral norm. DefineM as the -cover ofM, that is, for every M ∈M, there exists M ′ ∈M such that σmax(M −M ′) ≤ . Then for all > 0, there existsM such that |M | ≤ ( 1 + 2D )D2 .\nProof. Fix any > 0 and letN /D be a minimal size ( /D)-cover of Euclidean unit ball BD in RD. That is, for any v ∈ BD, there exists v′ ∈ N /D such that ‖v − v′‖ ≤ /D. Using standard volume arguments (see e.g. proof of Lemma 5.2 of Vershynin (2010)), we know that |N /D| ≤ ( 1 + 2D )D . Define\nM := { M ′ ∣∣M ′ = [v′1 · · · v′D] ∈ RD×D, v′i ∈ N /D}. ThenM constitutes as an -cover ofM, since for any M = [v1 · · · vD] ∈ M there exists M ′ = [v′1 · · · v′D] ∈M , in particular M ′ such that ‖vi − v′i‖ ≤ /D (for all i). Then\nσmax(M −M ′) ≤ ‖M −M ′‖F = ∑ i ‖vi − v′i‖ ≤ .\nWithout loss of generality we can assume that each M ′ ∈ M , σmax(M ′) = 1. Moreover, by construction, |M | ≤ ( 1 + 2D )D2 .\nLemma 14 [extension of Theorem 12.8 of Anthony & Bartlett (1999)] Let H be a set of real functions from a domain X to the interval [0, 1]. Let γ > 0. Then for all m ≥ 1,\nN∞(γ,H,m) < c0(4m/γ2) Fatγ/4(H) ln 4emFatγ/4(H)γ .\nfor some universal constant c0.\nProof. Theorem 12.8 of Anthony & Bartlett (1999) asserts this for m ≥ Fatγ/4(H) ≥ 1 with c0 = 2. Now, if 1 ≤ m < Fatγ/4(H), for some universal constant c′, we have N∞(γ,H,m) ≤ (c′/γ)m ≤ (c′/γ)Fatγ/4(H).\nLemma 15 [Theorem 10.1 of Anthony & Bartlett (1999)] Suppose thatH is a set of real-valued functions defined on domainX . LetD be any probability distribution onZ = X×{0, 1}, 0 ≤ ≤ 1, real γ > 0 and integer m ≥ 1. Then,\nPSm∼D [ ∃h ∈ H : err(h) ≥ errγ(h, Sm) + ] ≤ 2N∞ (γ 2 ,H, 2m ) e− 2m/8,\nwhere Sm is an i.i.d. sample of size m from D."
    }, {
      "heading" : "A.4 Proof of Lemma 4",
      "text" : "For any fixed 0 < γ < 1/8 and the given bounded class of distributions with bound B ≥ 1, consider a (1/B)-bi-Lipschitz base hypothesis class H that maps hypothesis from the domain X to [1/2− 4γ, 1/2 + 4γ], and define\nF := {x 7→ h(Mx) : M ∈M, h ∈ H}.\nNote that finding M that minimizes errhypoth is equivalent to finding f that minimizes error on F . Using Lemma 19, we have for any 0 < γ < 1/2, the sample complexity of F is (for all 0 < , δ < 1/64)\nm ≥ Fat2γ(π4γ(F)) 320 2 , (10)\nwhere π4γ(F) is the (4γ)-squashed function class of F (see Definition 2 below). We lower bound Fat2γ(π4γ(F)) in terms of fat-shattering dimension ofH to yield the lemma.\nTo this end we shall first define the (γ,m)-covering and packing number of a generic real-valued hypothesis class G. For any domain X , real-valued hypothesis class G ⊂ [0, 1]X , margin γ > 0, and a sample S ⊂ X , define\ncovγ(G, S) := { C ⊂ G ∣∣∣ ∀g ∈ G,∃g′ ∈ C, maxs∈S |g(s)− g′(s)| ≤ γ } ,\npakγ(G, S) :=\n{ P ⊂ G ∣∣∣ ∀g 6= g′ ∈ P, maxs∈S |g(s)− g′(s)| ≥ γ }\nas the set of γ-covers (resp. γ-packings) of S by G. Let γ-covering number (resp. γ-packing number) of G for any integer m > 0 be defined as\nN∞(γ,G,m) := max S⊂X:|S|=m min C∈covγ(G,S) |C|,\nP∞(γ,G,m) := max S⊂X:|S|=m max P∈pakγ(G,S) |P |\nwith the minimizing cover C (resp. maximizing packing P ) called as the minimizing (γ,m)-cover (resp. maximizing (γ,m)-packing) of G.\nWith these definitions, we have the following (for some universal constant c0).\nc0 ( m 16γ2 )Fat2γ(π4γ(F)) ln(em/2γ) ≥ N∞(8γ, π4γ(F),m) [Lemma 14]\n≥ P∞(16γ, π4γ(F),m) [Lemma 17] ≥ ( 1\n32γ\n)D2 P∞(48γ, π4γ(H),m) [see (*) below]\n= ( 1\n32γ\n)D2 P∞(48γ,H,m) [by the choice ofH]\n≥ ( 1\n32γ\n)D2 N∞(48γ,H,m) [Lemma 17]\n≥ ( 1\n32γ\n)D2 eFat768γ(H)/8. [Lemma 18] (11)\n(*) We show that P∞(16γ, π4γ(F),m) ≥ (1/32γ)D 2P∞(48γ, π4γ(H),m), by exhibiting a set"
    }, {
      "heading" : "S ⊂ π4γ(F) of size (1/32γ)D",
      "text" : "2P∞(48γ, π4γ(H),m) that is a (16γ)-packing of π4γ(F).\nLet π4γ(H48γ) ⊂ π4γ(H) be a maximal (32γ)-packing of π4γ(H) (that is, a maximal set such that for all distinct (π4γ ◦ h), (π4γ ◦ h′) ∈ π4γ(H48γ), exists x ∈ X such that |π4γ(h(x)) − π4γ(h ′(x))| ≥ 48γ). Fix (exact value determined later), and define\nS := { x 7→ (π4γ ◦ h)(Mx) ∣∣∣ (π4γ ◦ h) ∈ π4γ(H48γ), M ∈M } ,\nwhereM is a -spectral net ofM, that is, for all M ∈ M, exists M ′ ∈ M such that σmax(M − M ′) ≤ , and for all distinct M ′,M ′′ ∈M , σmax(M ′ −M ′′) ≥ /2.\nThen for any two distinct f, f ′ ∈ S , such that f(x) = (π4γ ◦ h)(Mx) and f ′(x) = (π4γ ◦ h′)(M ′x), we have\n• (case 1) h and h′ are distinct. In this case, there exists x ∈ X , s.t.\n|f(x)− f ′(x)| =|π4γ(h(Mx))− π4γ(h′(M ′x))| ≥ |π4γ(h(Mx))− π4γ(h′(Mx))| − |π4γ(h′(Mx))− π4γ(h′(M ′x))| ≥ 48γ − (1/B)σmax(M −M ′)‖x‖ ≥ 48γ − (1/B) B = 48γ − .\n• (case 2) h, h′ same but M and M ′ distinct. In this case, there exists x (with ‖x‖ = 1) s.t.\n|f(x)− f ′(x)| = |π4γ(h(Mx))− π4γ(h(M ′x))| = |h(Mx)− h(M ′x)| ≥ B‖(M −M ′)x‖ ≥ B · min\nM 6=M ′∈M σmax(M −M ′)\n≥ B( /2).\nThus, by setting = 32γ, distinct classifiers f, f ′ ∈ S32γ are at least 16γ apart (since B ≥ 1). Hence S32γ forms a (16γ)-packing of π4γ(F). Therefore, the packing number\nP∞(16γ, π4γ(F),m) ≥ |S32γ | = |M32γ ||H48γ | ≥ (1/32γ)D 2 P∞(48γ, π4γ(H),m).\nThus, from Eq. (11), it follows that\nFat2γ(π4γ(F)) ≥ Ω (D2 ln(1/γ) + Fat768γ(H)\nln(m/γ2) ln(m/γ)\n) .\nCombining this with Eq. (10), the lemma follows.\nLemma 16 [ -spectral packings of D ×D matrices] LetM := {M | M ∈ RD×D, σmax(M) = 1} be the set of matrices with unit spectral norm. DefineM ⊂ M as the -packing ofM, that is, for every distinct M,M ′ ∈M , σmax(M −M ′) ≥ . Then for all > 0, there existsM such that |M | ≥ ( 1 2 )D2 .\nProof. Fix any > 0 and let P be a maximal size -packing of Euclidean unit ball BD in RD. That is, for all distinct v, v′ ∈ BD, ‖v − v′‖ ≥ . Using standard volume arguments (see e.g. proof of Lemma 5.2 of Vershynin (2010)), we know that |P | ≥ ( 1 2 )D . Define\nM := { M ′ ∣∣M ′ = [v′1 · · · v′D] ∈ RD×D, v′i ∈ P }. Then M constitutes as an -packing of M, since for any distinct M,M ′ ∈ M such that M = [v1 · · · vD] and M ′ = [v′1 · · · v′D], we have\nσmax(M −M ′) ≥ max i ‖vi − v′i‖ ≥ .\nWithout loss of generality we can assume that each M ∈ M , σmax(M) = 1. Moreover, by construction, |M | ≥ ( 1 2 )D2 .\nLemma 17 [follows from Theorem 12.1 of Anthony & Bartlett (1999)] For any real valued hypothesis classH into [0, 1], all m ≥ 1, and 0 < γ < 1/2,\nP∞(2γ,H,m) ≤ N∞(γ,H,m) ≤ P∞(γ,H,m).\nLemma 18 [Theorem 12.10 of Anthony & Bartlett (1999)] Let H be a set of real functions from a domain X to the interval [0, 1]. Let γ > 0. Then for m ≥ Fat16γ(H),\nN∞(γ,H,m) ≥ eFat16γ(H)/8.\nLemma 19 [Theorem 13.5 of Anthony & Bartlett (1999)] Suppose that H is a set of real-valued functions mapping into the interval [0, 1] that is closed under addition of constants, that is,\nh ∈ H =⇒ h′ ∈ H, where h′ : x→ h(x) + c for all c.\nPick any 0 < γ < 1/2. Then for any metric learning algorithm A for all 0 < , δ < 1/64, there exists a distribution D such that if m ≤ d320 2 , then\nPSm∼D[err(h ∗,D) > errγ(A(Sm),D) + ] > δ\nwhere d := Fat2γ(π4γ(H)) ≥ 1 is the fat-shattering dimension of π4γ(H)—the (4γ)-squashed function class ofH, see Definition 2 below—at margin 2γ.\nDefinition 2 [squashing function] For any 0 < γ < 1/2, define the squashing function πγ : R → [1/2− γ, 1/2 + γ] as\nπγ(α) = { 1/2 + γ if α ≥ 1/2 + γ 1/2− γ if α ≤ 1/2− γ α otherwise .\nMoreover, for a collection F of functions into R, define πγ(F ) := {πγ ◦ f | f ∈ F}."
    }, {
      "heading" : "A.5 Proof of Lemma 5",
      "text" : "Let P be the probability measure induced by the random variable (X, Y ), where X := (x, x′), Y := 1[y = y′], st. ((x, y), (x′, y′)) ∼ (D×D).\nDefine function class\nF := { fM : X 7→ ‖M(x− x′)‖2 ∣∣∣∣∣ M ∈MX = (x, x′) ∈ (X ×X) } ,\nFollowing the steps of proof of Lemma 1, we can conclude that the Rademacher complexity of F is bounded. In particular,\nRm(F) ≤ 4B2 √\nsupM∈M ‖MTM‖2F m .\nThe result follows by noting that φ is λ-Lipschitz in the first argument and by applying Lemma 8."
    }, {
      "heading" : "A.6 Proof of Lemma 6",
      "text" : "Consider the function class\nF := { fv,M : x 7→ v ·Mx ∣∣ ‖v‖1 ≤ 1,M ∈M}, and define the composition class\nFσ := { x 7→\nK∑ i=1 wiσ γ(fi(x)) ∣∣∣ ‖wi‖1 ≤ 1, f1, . . . , fK ∈ F } .\nThen, first note that the Gaussian complexity ofF (with respect to the distributionD) is bounded, since (let g1, . . . , gm denote independent standard Gaussian random variables)\nGm(F ,D) := Exi∼D |X gi,i∈[m]\n[ sup\nfv,M∈F\n1\nm m∑ i=1 gifv,M (xi)\n]\n= 1\nm Exi∼D |X gi,i∈[m] [ sup M∈M ‖v‖1≤1 v · m∑ i=1 gi(Mxi) ]\n= 1\nm Exi∼D |X gi,i∈[m]\n[ max j sup M∈M m∑ i=1 gi(Mxi)j ]\n≤ 1 m Exi∼D |X gi,i∈[m] max j∈[D] [ m∑ i=1 gi sup M∈M ∣∣(Mxi)j∣∣]\n≤ c ln 1 2 (D)\nm Exi∼DX max j,j′∈[D]\n( Egi [ m∑ i=1 gi ( sup M∈M ∣∣(Mxi)j∣∣− sup M ′∈M ∣∣(M ′xi)j′ ∣∣)]2) 1 2\n= c ln\n1 2 (D) m Exi∼DX max j,j′∈[D] ( m∑ i=1 [ sup M∈M ∣∣(Mxi)j∣∣− sup M ′∈M ∣∣(M ′xi)j′∣∣]2) 1 2\n≤ c′B √ d lnD\nm ,\nwhere (i) second to last inequality is by applying Lemma 20, (ii) c, c′ are absolute constants, (iii) d := supM∈M ‖MTM‖2F . Note that bounding the Gaussian complexity also bounds the Rademacher complexity by Lemma 21.\nFinally by noting thatFσ is a γ-Lipschitz composition class ofF and φλ is a classification based loss function that is λ-Lipschitz in the first argument, we can apply Lemma 8 yielding the desired result.\nLemma 20 [Lemma 20 of Bartlett & Mendelson (2002)] Let Z1, . . . , ZD be random variables such that each Zj = ∑m i=1 aijgi, where each gi is independent N(0, 1) random variables. Then there is an absolute constant c such that\nEgi max j Zj ≤ c ln\n1 2 (D) max\nj,j′\n√ Egi(Zj − Zj′)2.\nLemma 21 [Lemma 4 of Bartlett & Mendelson (2002)] There are absolute constants c and C such that for every class F and every integer m\ncRm(F ,D) ≤ Gm(F ,D) ≤ C ln(m)Rm(F ,D),\nwhereR and G are Rademacher and Gaussian complexities of a function class F with respect to the distribution D respectively."
    }, {
      "heading" : "A.7 Proof of Corollary 7",
      "text" : "The conclusion of Eq. (3) is immediate by dividing the given failure probability δ across the sequence M1,M2, · · · such that δµd failure probability is associated with class Md, then apply Lemma 5 (for distance based metric learning) or Lemma 6 (for classifier based metric learning) to each class Md individually, and finally combining the individual deviations together with a union bound.\nFor the second part, for any M ∈M define dM and ΛM as per the lemma statement. Then with probability at least 1− δ\nerrλ(M regm ,D)− errλ(M∗,D) ≤ errλ(M regm , Sm) + dM regm ΛM regm − err λ(M∗,D)\n≤ errλ(M∗, Sm) + dM∗ΛM∗ − err λ(M∗,D) ≤ O(d M∗ΛM∗ ) = O( ),\nwhere (i) the first inequality is by applying Eq. (3) on weighting metric M regm (with failure probability set to δ/2), (ii) the second inequality is by noting that M regm is the (regularized) sample error minimizer as per the lemma statement, (iii) the third inequality is by applying Eq. (3) on weighting metric M∗ (with failure probability set to δ/2), and (iv) the last equality by noting the definitions of ΛM∗ and our choice of m."
    } ],
    "references" : [ {
      "title" : "Neural network learning: Theoretical foundations",
      "author" : [ "M. Anthony", "P. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "Anthony and Bartlett,? \\Q1999\\E",
      "shortCiteRegEx" : "Anthony and Bartlett",
      "year" : 1999
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Bartlett and Mendelson,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson",
      "year" : 2002
    }, {
      "title" : "Robustness and generalization for metric learning",
      "author" : [ "A. Bellet", "A. Habrard" ],
      "venue" : "CoRR, abs/1209.1086,",
      "citeRegEx" : "Bellet and Habrard,? \\Q2012\\E",
      "shortCiteRegEx" : "Bellet and Habrard",
      "year" : 2012
    }, {
      "title" : "A survey on metric learning for feature vectors and structured data",
      "author" : [ "A. Bellet", "A. Habrard", "M. Sebban" ],
      "venue" : "CoRR, abs/1306.6709,",
      "citeRegEx" : "Bellet et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bellet et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning a distance metric by empirical loss minimization",
      "author" : [ "W. Bian", "D. Tao" ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Bian and Tao,? \\Q2011\\E",
      "shortCiteRegEx" : "Bian and Tao",
      "year" : 2011
    }, {
      "title" : "Generalization bounds for metric and similarity learning",
      "author" : [ "Q. Cao", "Z. Guo", "Y. Ying" ],
      "venue" : "CoRR, abs/1207.5437,",
      "citeRegEx" : "Cao et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2013
    }, {
      "title" : "New generalization bounds for learning kernels",
      "author" : [ "C. Cortes", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Cortes et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2010
    }, {
      "title" : "Information-theoretic metric learning",
      "author" : [ "J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Davis et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Davis et al\\.",
      "year" : 2007
    }, {
      "title" : "Generalization classification via regularized similarity learning",
      "author" : [ "Z. Guo", "Y. Ying" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Guo and Ying,? \\Q2014\\E",
      "shortCiteRegEx" : "Guo and Ying",
      "year" : 2014
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "K. Hornik", "M. Stinchcombe", "H. White" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Hornik et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Hornik et al\\.",
      "year" : 1989
    }, {
      "title" : "Regularized distance metric learning: Theory and algorithm",
      "author" : [ "R. Jin", "S. Wang", "Y. Zhou" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Jin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2009
    }, {
      "title" : "Fantope regularization in metric learning",
      "author" : [ "M.T. Law", "N. Thome", "M. Cord" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Law et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Law et al\\.",
      "year" : 2014
    }, {
      "title" : "Robust structural metric learning",
      "author" : [ "D.K.H. Lim", "B. McFee", "G.R.G. Lanckriet" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Lim et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lim et al\\.",
      "year" : 2013
    }, {
      "title" : "Metric learning to rank",
      "author" : [ "B. McFee", "G.R.G. Lanckriet" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "McFee and Lanckriet,? \\Q2010\\E",
      "shortCiteRegEx" : "McFee and Lanckriet",
      "year" : 2010
    }, {
      "title" : "Learning a distance metric from a network",
      "author" : [ "B. Shaw", "B. Huang", "T. Jebara" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Shaw et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shaw et al\\.",
      "year" : 2011
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "In Compressed Sensing, Theory and Applications",
      "citeRegEx" : "Vershynin,? \\Q2010\\E",
      "shortCiteRegEx" : "Vershynin",
      "year" : 2010
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K.Q. Weinberger", "L.K. Saul" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Weinberger and Saul,? \\Q2009\\E",
      "shortCiteRegEx" : "Weinberger and Saul",
      "year" : 2009
    }, {
      "title" : "Distance metric learning with application to clustering with side-information",
      "author" : [ "E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Xing et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2002
    }, {
      "title" : "Generalization bounds for learning the kernel",
      "author" : [ "Y. Ying", "C. Campbell" ],
      "venue" : "Conference on Computational Learning Theory (COLT),",
      "citeRegEx" : "Ying and Campbell,? \\Q2009\\E",
      "shortCiteRegEx" : "Ying and Campbell",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "A few notable examples include exploiting class labels to find a Mahalanobis distance metric that maximizes the distance between dissimilar observations while minimizing distances between similar ones to improve classification quality (Weinberger & Saul, 2009; Davis et al., 2007), ∗email: verman@janelia.",
      "startOffset" : 235,
      "endOffset" : 280
    }, {
      "referenceID" : 15,
      "context" : "Some popular algorithms that optimize for such distance-based objectives include Mahalanobis Metric for Clustering (MMC) by Xing et al. (2002) and Information Theoretic Metric Learning (ITML) by Davis et al.",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "(2002) and Information Theoretic Metric Learning (ITML) by Davis et al. (2007). Instead of using distance comparisons as a proxy, however, one can also optimize for a specific prediction task directly.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "(2002) and Information Theoretic Metric Learning (ITML) by Davis et al. (2007). Instead of using distance comparisons as a proxy, however, one can also optimize for a specific prediction task directly. The second generic framework, the classifier-based metric learning framework, explicitly incorporates the hypothesis associated with the prediction task of interest to learn effective distance metrics. A few interesting examples in this regime include the work by McFee & Lanckriet (2010) that finds metrics that improve ranking quality in information retrieval tasks, and the work by Shaw et al.",
      "startOffset" : 59,
      "endOffset" : 491
    }, {
      "referenceID" : 7,
      "context" : "(2002) and Information Theoretic Metric Learning (ITML) by Davis et al. (2007). Instead of using distance comparisons as a proxy, however, one can also optimize for a specific prediction task directly. The second generic framework, the classifier-based metric learning framework, explicitly incorporates the hypothesis associated with the prediction task of interest to learn effective distance metrics. A few interesting examples in this regime include the work by McFee & Lanckriet (2010) that finds metrics that improve ranking quality in information retrieval tasks, and the work by Shaw et al. (2011) that learns metrics that help predict connectivity structure in networked data.",
      "startOffset" : 59,
      "endOffset" : 606
    }, {
      "referenceID" : 16,
      "context" : "Xing et al. (2002) optimize an efficiently computable variant of this criterion, in which they look for a metric that keeps the total pairwise distance amongst the observations from the same class less than a constant while maximizing the total pairwise distance amongst the observations from opposite classes.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "The variant proposed by Davis et al. (2007) explicitly includes the upper and lower limits with an added regularization on the learned M to be close to a pre-specified metric of interest M0.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "Shaw et al. (2011) also follow this principle and explicitly include network topology constraints to learn a weighting metric that can better predict the connectivity structure in social and web networks.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "Recall that for any smooth target function f∗, a two layer feed-forward neural network (with appropriate number of hidden units and connection weights) can approximate f∗ arbitrarily well (Hornik et al., 1989), so this class is flexible enough to incorporate most reasonable target hypotheses.",
      "startOffset" : 188,
      "endOffset" : 209
    }, {
      "referenceID" : 12,
      "context" : "This partly explains the observed empirical success of various types of norm-regularized optimization criteria for finding the optimal weighting metric (Lim et al., 2013; Law et al., 2014).",
      "startOffset" : 152,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "This partly explains the observed empirical success of various types of norm-regularized optimization criteria for finding the optimal weighting metric (Lim et al., 2013; Law et al., 2014).",
      "startOffset" : 152,
      "endOffset" : 188
    }, {
      "referenceID" : 7,
      "context" : "We select two popular metric learning algorithms, LMNN by Weinberger & Saul (2009) and ITML by Davis et al. (2007), that are designed to find metrics that improve nearest-neighbor classification quality.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "Jin et al. (2009), for instance, analyzed the generalization ability of regularized, convex-loss optimization criteria for pairwise distances via an algorithmic stability analysis.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "Jin et al. (2009), for instance, analyzed the generalization ability of regularized, convex-loss optimization criteria for pairwise distances via an algorithmic stability analysis. They derive an interesting sample complexity result that is sublinear in √ D for datasets of representation dimension D. They discuss that the sample complexity can potentially be independent of D, but do not characterize specific instances or classes of problems where this may be possible. Likewise, recent work by Bellet & Habrard (2012) uses algorithmic robustness to analyze the generalization ability for pairwise- and triplet-based distance metric learning.",
      "startOffset" : 0,
      "endOffset" : 522
    }, {
      "referenceID" : 9,
      "context" : "Jin et al. (2009), for instance, analyzed the generalization ability of regularized, convex-loss optimization criteria for pairwise distances via an algorithmic stability analysis. They derive an interesting sample complexity result that is sublinear in √ D for datasets of representation dimension D. They discuss that the sample complexity can potentially be independent of D, but do not characterize specific instances or classes of problems where this may be possible. Likewise, recent work by Bellet & Habrard (2012) uses algorithmic robustness to analyze the generalization ability for pairwise- and triplet-based distance metric learning. Their analysis relies on the existence of a partition of the input space, such that in each cell of the partition, the training loss and test loss does not deviate much (robustness criteria). Note that their sample complexity bound scales with the partition size, which in general can be exponential in the representation dimension. Perhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al.",
      "startOffset" : 0,
      "endOffset" : 1082
    }, {
      "referenceID" : 5,
      "context" : "Perhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al. (2013). Bian & Tao (2011) analyze the consistency of the ERM criterion for metric learning.",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Perhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al. (2013). Bian & Tao (2011) analyze the consistency of the ERM criterion for metric learning.",
      "startOffset" : 107,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "Perhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al. (2013). Bian & Tao (2011) analyze the consistency of the ERM criterion for metric learning. They show a O(m−1/2) rate of convergence for the ERM with m samples to the expected risk for thresholds on bounded convex losses for distance-based metric learning. Our upper-bound in Lemma 1 generalizes this result by considering arbitrary (possibly non-convex) distance-based Lipschitz losses and explicitly shows the dependence on the representation dimensionD. Cao et al. (2013) provide an alternate analysis based on norm regularization of the weighting metric for distance-based metric learning.",
      "startOffset" : 107,
      "endOffset" : 593
    }, {
      "referenceID" : 6,
      "context" : "The typical focus in kernel learning is to analyze the generalization ability of the hypothesis class of linear separators in general Hilbert spaces (Ying & Campbell, 2009; Cortes et al., 2010).",
      "startOffset" : 149,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : "Our empirical results show that such regularization not only helps in designing new metric learning algorithms (Lim et al., 2013; Law et al., 2014), but can even benefit existing metric learning algorithms in high-noise regimes.",
      "startOffset" : 111,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "Our empirical results show that such regularization not only helps in designing new metric learning algorithms (Lim et al., 2013; Law et al., 2014), but can even benefit existing metric learning algorithms in high-noise regimes.",
      "startOffset" : 111,
      "endOffset" : 147
    }, {
      "referenceID" : 15,
      "context" : "2 of Vershynin (2010)), we know that |N /D| ≤ ( 1 + 2D )D .",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "2 of Vershynin (2010)), we know that |P | ≥ ( 1 2 )D .",
      "startOffset" : 5,
      "endOffset" : 22
    } ],
    "year" : 2015,
    "abstractText" : "Metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lowerand upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. However, by leveraging the structure of the data distribution, we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset. Our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset’s intrinsic complexity, yielding better generalization. Experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise.",
    "creator" : "LaTeX with hyperref package"
  }
}