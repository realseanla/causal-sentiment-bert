{
  "name" : "1508.05804.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "bgonc@lncc.br", "fporto@lncc.br" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 8.\n05 80\n4v 1\n[ cs\n.A I]\n2 4\nA ug\nIn this paper we provide a concise report on the complexity of the causal ordering problem, originally introduced by Simon to reason about causal relations implicit in mathematical models. We show that Simon’s classical algorithm to infer causal ordering is NP-Hard—an intractability previously guessed by Nayak but not yet proven. We present then a detailed account based on Nayak’s suggested algorithmic solution (the best available), which is dominated by computing transitive closure—bounded in time by O(|S|3), where S(E ,V) is the input system structure composed of a set E of equations over a set V of variables with density |S|. We also comment on the potential of causal ordering for emerging applications in large-scale hypothesis management and predictive analytics.\nKeywords: Causal reasoning, Structural equations, Hypothesis management."
    }, {
      "heading" : "1. Introduction",
      "text" : "The causal ordering problem has long been introduced by Simon as a technique to infer the causal orientations implicit in a deterministic mathematical model [1]. Simon was motivated by studies in econometrics, and not very concerned with the algorithmic aspects of his vision—which turned out to be quite influential in the artificial intelligence literature (cf. survey [3]).\nIn a recent study, Dash and Druzdzel revisit the problem and motivate it in light of modern applications [3]. They show that Simon’s causal ordering algorithm (COA) is correct in the sense that any valid causal ordering extracted from a self-contained (well-posed) system of equations must be\nEmail addresses: bgonc@lncc.br (Bernardo Gonçalves), fporto@lncc.br (Fabio Porto)\nPreprint submitted to Artificial Intelligence August 25, 2015\ncompatible with the causal ordering output by Simon’s COA [3]. Their aim has also been (sic.) to validate decades of research that has shown COA to be a powerful tool for operating on causal models. In addition to the result on the correctness of COA, their note also provides a convenient survey of related work that connects to Simon’s early vision on causal reasoning.\nThere is a need, though, to distinguish COA from the causal ordering problem (COP) itself. Simon’s COA still seems to be the main entry point to COP in the specialized literature. Yet there is no review on the computational properties of COA, which as we show in this note turns out to be NP-Hard. The interested reader who needs an efficient algorithmic approach to address COP in a real, large-scale application can only scarcely find some comments spread through Nayak [4, p. 287-91], and then Iwasaki and Simon [5, p. 149] and Pearl [2, p. 226] both pointing to the former. In fact Nayak only suggests in words (sic.) that it is a worst-case exponential time algorithm [12, p. 37].\nCOP is significant also in view of emerging applications in large-scale hypothesis management and predictive analytics [6]. The modeling of physical and socio-economical systems as a set of mathematical equations is a traditional approach in science and engineering and a very large bulk of models exist which are ever more available in machine-readable format. Simon’s early vision on the automatic extraction of the causal mechanisms implicit in (large-scale) models for the sake of informed intervention finds nowadays new applications in the context of open simulation laboratories [7], large-scale model management [8] and online, shared model repositories [9, 10, 11].\nThe contributions of this paper are (§2) to originally show that COA is in fact NP-Hard, confirming Nayak’s earlier intuition; and then (§3) to organize into a concise note his hints on a polynomial-time algorithm for COP.\n1.1. Informal Preliminaries\nGiven a system of mathematical equations involving a set of variables, to build a structural equation model (SEM) is, essentially, to establish a one-toone mapping between equations and variables [1]. That shall enable further detecting the hidden asymmetry between variables, i.e., their causal ordering. For instance, Einstein’s famous equation E = mc2 states the equivalence of mass and energy, summarizing (in its scalar version) a theory that can be imposed two different asymmetries for different applications. Say, given a fixed amount of mass m = m0 (and recalling that c is a constant), predict\nthe particle’s relativistic rest energy E; or given the particle’s rest energy, predict its mass or potential for nuclear fission.\nFor really large systems, having structures say in the order of one million equations [13], the causal ordering problem is critical to provide more specific accountability on the predictive accuracy of a large-scale system—which specific variables and subsystems account for possible inaccurate predictions (?). That is key for managing the uncertainty of alternative modeling variations systematically [7, 13].\n1.2. Related Work\nCOA. As mentioned, Dash and Druzdzel provide a formal description of how Simon’s COA gives a summary of the causal dependencies implicit in a given SEM. That is, in clustering the strongly coupled variables into a causal graph, COA provides a condensed representation of the causal model implicit in the given SEM. They show then that any valid total causal mapping produced for a given SEM must be consistent with COA’s partial causal mapping. They leave open, though, any observation regarding COA’s computational properties. In this note we supplement their work in that sense.\nCOP. Inspired by Serrano and Gossard’s work on constraint modeling and reasoning [14], Nayak describes in words an approach that is provably quite effective to process the causal ordering: extract a total causal mapping and then compute the transitive closure of the direct causal dependencies. We shall arrange his insights into a succinct description shortly."
    }, {
      "heading" : "2. The Complexity of Causal Ordering",
      "text" : "2.1. Notation and Basic Concepts\nDef. 1. A structure is a pair S(E ,V), where E is a set of equations over set V of variables, |E| ≤ |V|, such that:\n(a) In any subset of k equations of the structure, at least k different variables appear;\n(b) In any subset of k equations in which r variables appear, k ≤ r, if the values of any (r−k) variables are chosen arbitrarily, then the values of the remaining k variables can be determined uniquely — finding these unique values is a matter of solving the equations.\nDef. 2. Let S(E ,V) be a structure. We say that S is self-contained or complete if |E| = |V|.\nIn short, the COP is concerned with systems of equations that are ‘structural’ (Def. 1) and ‘complete’ (Def. 2), viz., that has as many equations as variables and no subset of equations has fewer variables than equations.1\nDef. 3. Let S be a structure. We say that S is minimal if it is complete and there is no complete proper substructure S ′⊂ S.\nExample 1. Consider structure S(E ,V), where E={ f1(x1), f2(x2), f3(x3), f4(x1, x2, x3, x4, x5), f5(x1, x3, x4, x5), f6(x4, x6), f7(x5, x7) }. Note that S is complete, as |E|= |V|=7, but not minimal. There are exactly three minimal substructures S1,S2,S3 ⊂ S, whose sets of equations are E1={f1(x1)}, E2= {f2(x2)}, E3={f3(x3)}.\nWe are now in a position to begin our study of COA and its complexity.\n2.2. Simon’s Causal Ordering Algorithm\nSimon’s COA is based on a data structure which is introduced in Def. 4.\nDef. 4. The structure matrix AS of a structure S(E ,V), with f1, f2, ..., fn ∈ E and x1, x2, ..., xm ∈ V, is a n×m matrix of 1’s and 0’s in which entry aij is non-zero if variable xj appears in equation fi, and zero otherwise.\nElementary row operations (e.g., row multiplication by a constant) on the structure matrix may hinder the structure’s causal ordering and then are not valid in general [1]. This also emphasizes that the problem of causal ordering is not about solving the system of mathematical equations of a structure, but identifying its hidden asymmetries.\nExample 1. (continued). Fig. 1a shows the matrix of the structure S given above in this example. By eliminating the variables identified with the minimal substructures S1,S2,S3 ⊂ S, a smaller structure T ⊂ S is derived to be input at the next recursive step (see Fig. 1b). This is the main insight of Simon to arrive at his recursive algorithm COA we introduce next.\nDef. 5. Let S(E ,V) be a complete structure.Then a total causal mapping over S is a bijection ϕ : E → V such that, for all f ∈ E , if ϕ(f) = x then x∈ V ars(f).\nThe algorithm Simon has informally described in [1] is given a complete structure S(E ,V) and computes a partial causal mapping ϕp from partitions on the set of equations to same-cardinality partitions on the set of variables. The causal mapping returned by Simon’s COA is not total when S has variables that are strongly coupled (because they can only be determined simultaneously), but any total mapping ϕ over S must be consistent with COA’s partial mapping ϕp [3]. The latter may be made partial by design (merge strongly coupled variables into partitions or clusters) in order to force its induced causal graph Gϕp to be acyclic. Algorithm 1, COA, is a variant of Simon’s COA that returns a total causal mapping ϕ instead of a partial causal mapping.2 We illustrate it through Example 1 and Fig. 1.\nExample 1. (continued). Let T ⊂ S be the structure returned by COA’s first\n1Also, for inferring causal ordering the systems of equations given as input is expected to be ‘independent’ in the sense of Linear Algebra (can only have non-redundant equations).\n2This difference, though, only takes place in lines 7–10 in Algorithm 1 and is irrelevant to COA intractability, which is due to line 3.\nAlgorithm 1 COA, a slight variant of Simon’s original algorithm.\n1: procedure COA(S : structure over E and V) Require: S given is complete, i.e., |E| = |V| Ensure: Returns total causal mapping ϕ : E → V 2: ϕ ← ∅, Sk ← ∅ 3: identify all minimal substructures S ′ ⊆ S 4: for all S ′ ⊆ S do 5: Sk ← Sk ∪ S ′ ⊲ stores each minimal substructure scanned 6: V ′ ← ∅ 7: for all f ∈ S ′(E) do 8: x ← any xa such that xa ∈ V ars(f) and xa /∈ V ′ 9: ϕ ← ϕ ∪ 〈f, x〉 ⊲ maps to f some variable x ∈ V ars(f) 10: V ′ ← V ′ ∪ {x} 11: T ← S \\ ⋃\nS′∈Sk S ′\n12: if T 6= ∅ then 13: return ϕ ∪ COAt(T ) 14: return ϕ\nrecursive step k = 0 for this example. Then a valid total causal mapping that can be returned at k = 1 is COA(T ) = {〈f4, x4〉, 〈f5, x5〉, 〈f6, x6〉, 〈f7, x7〉}. Since x4 and x5 are strongly coupled (see Fig.1b), COA maps them arbitrarily (e.g., it could be f4 7→ x5, f5 7→ x4 instead). Such total mapping ϕ renders a cycle in the directed causal graph Gϕ induced by ϕ (see Fig.1c), which might not be desirable for some applications.\n2.3. COA’s Hardness\nThe serious issue with COA is that finding the minimal structures in a given structure (line 3) is hard and could only be addressed heuristically as a specific problem of co-clustering (also called biclustering [15, 16]) in Boolean matrices.\nAny structure S(E ,V) satisfying Def. 1 can be represented as a bipartite graph G = (V1∪V2, E), where the set E of equations and the set V of variables are the disjoint vertex sets, i.e., V1 7→ E , V2 7→ V, and E 7→ S is the edge set connecting equations to the variables appearing in them. Fig. 2 shows the\nbipartite graph G corresponding to the structure given in Example 1 — for a comprehensive text on graph concepts and its related algorithmic problems, cf. Even [17].\nA biclique (or complete bipartite graph) is a bipartite graph G = (V1 ∪ V2, E) such that for every two vertices s ∈ V1, t ∈ V2, we have (s, t) ∈ E [17]. Note that for balanced bicliques, i.e., when |V1| = |V2| = K, the degree deg(u) of any vertex u ∈ V1 ∪ V2 must be deg(u) = |V1| = |V2| = K. The total number of edges in a balanced biclique is then |E| = K2.\nRecent approaches to co-clustering problems (e.g., [18]) have been working with the notion of pseudo-biclique (also called ‘quasi-biclique’), which is a relaxation (extension) of the biclique concept for less than complete connectivity, say when some edges are missing so that the ratio of the number of edges |E| in comparison to the biclique (|E| = K2) is no less than a threshold.\nNow recall that COA needs to find, at each recursive step, all minimal substructures S ′ ⊆ S. There two special features of COA that we have to capture in our notion of pseudo-biclique: (1) the context is important—we have to define it as a subgraph w.r.t. the bipartite graph given; and (2) we have to distinguish not only that there are two disjoint vertex sets, but also keep track of their labels specifically: for bipartite graph G = (V1 ∪ V2, E) we shall say that V1 is the primary vertex set. This is because COA operates asymmetrically w.r.t. equation vertices V1 7→ E and variable vertices V2 7→ V.\nWe shall then prove that finding the minimal substructures in a given complete structure at each COA’s recursive step is equivalent to find the\nminimal-size ‘pseudo-bicliques’ in its corresponding bipartite graph—for a specific notion of pseudo-biclique that suits our problem, cf. Def. 6.\nFig. 3 introduces another example of complete structure and its correspondence with a bipartite graph.\nDef. 6. Let G = (V1 ∪ V2, E) be a bipartite graph with primary vertex set V1. We say that G has a K-balanced pseudo-biclique G\n′ = (V ′1 ∪V ′2 , E ′), where G′ ⊆ G, if |V ′1 | = |V ′2 | = K and ∑\nu∈V1\ndeg(u) ≤ ∑\nv∈V ′ 2\ndeg(v).\nObserve that the vertex degrees from the primary vertex set V1 are computed w.r.t. graph G, not subgraph G′ (unless G = G′). Moreover, it is easy to see that the case when deg(u) = K for all u ∈ V1 ∪ V2, (K-balanced biclique) is a particular case of the K-balanced pseudo-biclique case.\nWe now originally state the balanced pseudo-biclique problem (BPBP) as a decision problem as follows.\n(BPBP). Given a bipartite graphG = (V1∪V2, E) with primary vertex set V1 and a positive integer K, does G have a K-balanced pseudo-biclique?\nLemma 1. The balanced pseudo-biclique problem (BPBP) is NP-Complete.\nProof 1. We show (by restriction [19, p. 63-5]) that the BPBP is a generalization of the balanced biclique problem (BBP), referred ‘balanced complete\nbipartite subgraph’ problem [19, GT24, p. 196], which is known to be NPComplete by means of a transformation from ‘clique’ [20, p. 446]. The restriction from BPBP to BBP (special case) is made by requiring deg(u) = K for all u ∈ V1 ∪ V2, which enforces any K-balanced pseudo-biclique G′ ⊆ G and G itself to be more specifically a K-balanced biclique.\nTheorem 1. Let S(E ,V) be a complete structure. The extraction of its causal ordering by Simon’s COA(S) is NP-Hard.\nProof 2. We show that, at each recursive step k of COA, to find all minimal substructures S ′ ⊆ S is an optimization problem associated with the decision problem BPBP, which we know by Lemma 1 to be NP-Complete. We elaborate on the proof in Appendix A.\nNonetheless, the causal ordering problem (COP) can be solved efficiently by means of a different approach due to Nayak [4], which we introduce and build upon next."
    }, {
      "heading" : "3. Nayak’s Efficient Algorithm to COP",
      "text" : "3.1. Total Causal Mappings\nThe problem of causal ordering can be solved in polynomial time by just (i) finding any total causal mapping ϕ : E → V over structure S given (cf. Def. 5); and then (ii) by computing the transitive closure C+ϕ of the set Cϕ of direct causal dependencies induced by ϕ, see Eq. 1.\nCϕ= { (xa, xb) | there exists f ∈ E such that ϕ(f) = xb and xa ∈ V ars(f) } (1)\nDef. 7. Let S(E ,V) be a structure with variables xa, xb ∈ V, and ϕ a total causal mapping over S inducing set of direct causal dependencies Cϕ and indirectly a transitive closure C+ϕ . We say that (xa, xb) is a direct causal dependency in S if (xa, xb) ∈ Cϕ, and that (xa, xb) is a causal dependency in S if (xa, xb) ∈ C+ϕ .\nIn other words, (xa, xb) is in Cϕ iff xb direct and causally depends on xa, given the causal asymmetries induced by ϕ. Such notions shall enable us to extract the causal ordering by computing the transitive closure C+ϕ of Cϕ—which is known to be bounded by O(|S|3) [22, p. 633], where |S| is the number of edges in the bipartite graph associated with the structure. For this to be effective, though, if we have to ensure some properties of total causal mappings.\nFor a given structure S, there may be multiple total causal mappings over S (recall Example 1). But the causal ordering of S must be unique (see Fig. 1c). Therefore, a question that arises is whether the transitive closure C+ϕ is the same for any total causal mapping ϕ over S. Proposition 1, originally from Nayak [4], ensures that is the case.\nProposition 1. Let S(E ,V) be a structure, and ϕ1 : E → V and ϕ2 : E → V be any two total causal mappings over S. Then C+ϕ1 = C+ϕ2.\nProof 3. The proof is based on an argument from Nayak [4], which we present in a bit more of detail (see Appendix B). Intuitively, it shows that if ϕ1 and ϕ2 differ on the variable an equation f is mapped to, then such variables, viz., ϕ1(f) = xa and ϕ2(f) = xb, must be causally dependent on each other (strongly coupled).\nAnother issue is concerned with the precise conditions under which total causal mappings exist (i.e., whether or not all variables in the equations can be causally determined). In fact, by Proposition 2, based on Nayak [4] apud. Hall [17, p. 135-7], we know that the existence condition holds iff the given structure is complete.\nLet us refer to Even [17] to briefly introduce the additional graph-theoretic concepts which are necessary here. A matching in a graph is a subset of edges such that no two edges in the matching share a common node. A matching is said maximum if no edge can be added to the matching (without hindering the matching property). Finally, a matching in a graph is said ‘perfect’ if every vertex is an end-point of some edge in the matching — in a bipartite graph, a perfect matching is said a complete matching.\nProposition 2. Let S(E ,V) be a structure. Then a total causal mapping ϕ : E → V over S exists iff S is complete.\nProof 4. We observe that a total causal mapping ϕ : E → V over S corresponds exactly to a complete matching M in a bipartite graph B = (V1 ∪ V2, E), where V1 7→ E , V2 7→ V, and E 7→ S. In fact, by Even apud. Hall’s theorem (cf. [17, 135-7]), we know that B has a complete matching iff (a) for every subset of vertices F ⊆ V1, we have |F | ≤ |E(F )|, where E(F ) is the set of all vertices connected to the vertices in F by edges in E; and (b) |V1| = |V2|. By Def. 1 (no subset of equations has fewer variables than equations), and Def. 2 (number of equations is the same as number of variables), it is easy to see that conditions (a) and (b) above hold iff S is a complete structure.\nThe problem of finding a maximum matching is a well-studied algorithmic problem. We adopt the Hopcroft-Karp algorithm [21], which is known to be polynomial-time bounded by O( √\n|V1|+ |V2| |E|).3 That is, we handle the problem of total causal mapping by (see Alg. 2) translating it to the problem of maximum matching in a bipartite graph (in linear time) and then applying the Hopcroft-Karp algorithm to get the matching and finally translate it back to the total causal mapping, as suggested by the proof of Proposition 2.\n3The Hopcroft-Karp algorithm solves maximum matching in a bipartite graph efficiently as a problem of finding maximum flow in a network (cf. [17, p. 135-7], or [22, p. 664-9]).\nAlgorithm 2 Find a total causal mapping for a given structure.\n1: procedure TCM(S : structure over E and V) Require: S given is a complete structure, i.e., |E| = |V| Ensure: Returns a total causal mapping ϕ 2: B(V1 ∪ V2, E) ← ∅ 3: ϕ ← ∅ 4: for all 〈f,X〉 ∈ S do ⊲ translates structure S to a bipartite graph B 5: V1 ← V1 ∪ {f} 6: for all x ∈ X do 7: V2 ← V2 ∪ {x} 8: E ← E ∪ {(f, x)} 9: M ← Hopcroft-Karp(B) ⊲ solves the maximum matching problem 10: for all (f, x) ∈ M do ⊲ translates the matching to a total causal\nmapping 11: ϕ ← ϕ ∪ {〈f, x〉} 12: return ϕ\nFig. 4 shows the complete matching found by the Hopcroft-Karp algorithm for the structure given in Example 1.\nCorollary 1 summarizes the results we have so far.\nCorollary 1. Let S(E ,V) be a complete structure. Then a total causal mapping ϕ : E → V over S can be found by (Alg. 2) TCM in time that is bounded by O( √ |E| |S|).\nProof 5. Let B = (V1 ∪ V2, E) be the bipartite graph corresponding to complete structure S given to TCM, where V1 7→ E , V2 7→ V, and E 7→ S. The translation of S into B is done by a scan over it. This scan is of length |S| = |E|. Note that number |E| of edges rendered is precisely the length |S| of structure, where the denser the structure, the greater |S| is. The retranslation of the matching computed by internal procedure Hopcroft-Karp, in turn, is done at expense of |E| = |V| ≤ |S|. Thus, it is easy to see that TCM is dominated by the maximum matching algorithm Hopcroft-Karp, which is known to be O( √ |V1|+ |V2| |E|), i.e., O( √\n|E|+ |V| |S|). Since S is assumed complete, we have |E| = |V| then √ |E|+ |V| = √ 2 √\n|E|. Therefore, TCM must have running time at most O( √ |E| |S|).\nRemark 1. Let S(E ,V) be a complete structure. Then we know (cf. Proposition 2) that a total causal mapping over S exists. Let it be defined ϕ , TCM(S), which can be done in O( √\n|E| |S|). Then the causal ordering implicit in S shall be correctly extracted (cf. Proposition 1) by computing the set C+ϕ causal dependencies induced by ϕ in time bounded by O(|S|3). That is, it is dominated by computing transitive closure (cf. [22, p. 663])."
    }, {
      "heading" : "4. Conclusions",
      "text" : "• By Theorem 1, we know (an original hardness result) that Simon’s approach to process the causal ordering of a structure is NP-Hard;\n• By building upon the work of Simon [1] and Nayak [4] (cf. Propositions 1 and 2), we have conveyed an approach to efficiently extract the basic information (a total causal mapping) for processing the causal ordering implicit in a given system of mathematical equations;\n• By Corollary 1, we know how to process the complete structure of a set of mathematical equations into a total causal mapping in time that is bounded by O( √ |E| |S|).\n• By Remark 1, we know how to extract the causal ordering of a complete structure in time bounded by O(|S|3). This time bound can still be\ndecreased by means of optimized implementation [22, p. 663], and a careful handling of cycles in the directed graph. The machinery of causal ordering is then suitable for processing very large structures."
    }, {
      "heading" : "Appendix A. Proof of Theorem 1",
      "text" : "“Let S(E ,V) be a complete structure. The extraction of its causal ordering by Simon’s COA(S) is NP-Hard.”\nProof 6. We show that, at each recursive step of COA, to find all minimal substructures S ′ ⊆ S translates into an optimization problem associated with the decision problem BPBP, which we know by Lemma 1 to be NP-Complete.\nFirst, we show that, for any bipartite graph G = (V1∪V2, E) with primary vertex set V1 and a positive integer K, BPBP corresponds to decide whether there is a complete substructure S ′ ⊆ S, for S ′(E ′,V ′) and |E ′| = |V ′| = K.\nLet us map vertex sets V1 7→ E and V2 7→ V (n.b., primary vertex set V1 is mapped to the equation vertex set). Now we show that any K-balanced pseudo-biclique G′ ⊆ G, where G′ = (V ′1 ∪ V ′2 , E ′) must correspond to a complete substructure S ′(E ′,V ′) where V ′1 7→ E ′ and V ′2 7→ V ′. In fact, by Def. 6 we have properties (i) |V ′1 | = |V ′2 | = K and (ii)∑\nu∈V1\ndeg(u) ≤ ∑\nv∈V ′ 2\ndeg(v). By property (i) and Def. 2 we know that S ′ ⊆ S is\ncomplete, if it is a valid structure. But that is ensured by property (ii) and Def. 1.\nFinally, we expose the optimization problem associated with BPBP and COA. In fact, it is not enough to decide if G has a K-balanced pseudo-biclique but we have to always find the minimal K for so and only identify additional balanced pseudo-bicliques in the next recursive step k + 1 after eliminating the minimal substructures found in step k. For instance, note in Fig. 3b that the bipartite graph G associated with the complete structure S(E ,V) given is itself a K-balanced pseudo-biclique with K = |E| = |V| = 4. But it is not minimal—note substructure S ′ ⊂ S highlighted in Fig. 3a, with E ′ = {f1(x1, x3), f2(x1, x2), f3(x2, x3)}. That is, the latter also satisfies the Kbalanced pseudo-biclique property but for a smaller K = 3."
    }, {
      "heading" : "Appendix B. Proof of Proposition 1",
      "text" : "“Let S(E ,V) be a structure, and ϕ1 : E → V and ϕ2 : E → V be any two total causal mappings over S. Then C+1 = C+2 .”\nProof 7. The proof is based on an argument from Nayak [4], which we present here in a bit more of detail. Intuitively, it shows that if ϕ1 and ϕ2 differ on the variable an equation f is mapped to, then such variables, viz., ϕ1(f) and ϕ2(f), must be causally dependent on each other (strongly coupled).\nTo show C+1 = C + 2 reduces to C + 1 ⊆ C+2 and C+2 ⊆ C+1 . We show the first containment, with the second being understood as following by symmetry.\nClosure operators are extensive, X ⊆ cl(X), and idempotent, cl(cl(X)) = cl(X). That is, if we have C1 ⊆ C+2 , then we shall have C+1 ⊆ (C+2 )+ and, by idempotence, C+1 ⊆ C+2 .\nThen it suffices to show that C1 ⊆ C+2 , i.e., for any (x′, x) ∈ C1, we must show that (x′, x) ∈ C+2 as well. Observe by Def. 5 that both ϕ1 and ϕ2 are bijections, then, invertible functions. If ϕ−11 (x) = ϕ −1\n2 (x), then we have (x′, x) ∈ C2 and thus, trivially, (x′, x) ∈ C+2 . Else, ϕ1 and ϕ2 disagree in which equations they map onto x. But we show next, in any case, that we shall have (x′, x) ∈ C+2 .\nTake all equations g ∈ E ′ ⊆ E such that ϕ1(g) 6= ϕ2(g), and let n ≤ |E| be the number of such ‘disagreed’ equations. Now, let f ∈ E ′ be such that its mapped variable is x = ϕ1(f). Construct a sequence of length 2n such that, s0 = ϕ1(f) = x and, for 1 ≤ i ≤ 2n, element si is defined si = ϕ2(ϕ −1\n1 (si−1)). That is, we are defining the sequence such that, for each equation g ∈ E ′, its disagreed mappings ϕ1(g) = xa and ϕ2(g) = xb are such that ϕ1(g) is immediately followed by ϕ2(g). As xa, xb ∈ V ars(g), we have (xa, xb) ∈ C2 and, symmetrically, (xb, xa) ∈ C1. The sequence is of form s = 〈x, xf\n︸ ︷︷ ︸\nf\n, . . . , xa, xb ︸ ︷︷ ︸\ng\n, . . . , x2n−1, x2n ︸ ︷︷ ︸\nh\n〉.\nSince x must be in the codomain of ϕ2, we must have a repetition of x at some point 2 ≤ k ≤ 2n in the sequence index, with sk = x and sk−1 = x′′ such that (x′′, x) ∈ C2. If x′′ = x′, then (x′, x) ∈ C2 and obviously (x′, x) ∈ C+2 . Else, note that xf must also be in the codomain of ϕ1, while x\n′′ in the codomain of ϕ2. Let ℓ be the point in the sequence, 3 ≤ ℓ ≤ 2n−1, at which sℓ = xf = xa and sℓ+1 = xb for some xb such that (xf , xb) ∈ C2. It is easy to see that, either we have xb = x\n′′ or xb 6= x′′ but (xb, x′′) ∈ C+2 . Thus, by transitivity on such a causal chain, we must have (xf , x\n′′) ∈ C+2 and eventually (xf , x) ∈ C+2 . Finally, since x′ ∈ V ars(f) and ϕ2(f) = xf , we have (x′, xf ) ∈ C2 and, by transitivity, (x′, x) ∈ C+2 ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In this paper we provide a concise report on the complexity of the causal<lb>ordering problem, originally introduced by Simon to reason about causal<lb>relations implicit in mathematical models. We show that Simon’s classical<lb>algorithm to infer causal ordering is NP-Hard—an intractability previously<lb>guessed by Nayak but not yet proven. We present then a detailed account<lb>based on Nayak’s suggested algorithmic solution (the best available), which<lb>is dominated by computing transitive closure—bounded in time by O(|S|3),<lb>where S(E ,V) is the input system structure composed of a set E of equations<lb>over a set V of variables with density |S|. We also comment on the po-<lb>tential of causal ordering for emerging applications in large-scale hypothesis<lb>management and predictive analytics.",
    "creator" : "LaTeX with hyperref package"
  }
}