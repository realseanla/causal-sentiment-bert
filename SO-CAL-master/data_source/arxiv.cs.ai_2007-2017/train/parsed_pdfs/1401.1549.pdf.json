{
  "name" : "1401.1549.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Demand Response Using Device Based Reinforcement Learning",
    "authors" : [ "Zheng Wen", "Daniel O’Neill", "Hamid Reza Maei" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS’s rescheduling problem as a reinforcement learning (RL) problem (referred to as the device based RL problem), and show that this RL problem decomposes over devices under reasonable assumptions. Compared with existent formulations, our new formulation (1) does not require explicitly modeling the user’s dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also propose several new performance metrics for RL algorithms applied to the device based RL problem, and demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.\nKeywords: Demand Response, Energy Management System, Building and Home Automation, Reinforcement Learning, Markov Decision Process"
    }, {
      "heading" : "1 Introduction",
      "text" : "Demand response (DR) systems [Borenstein et al., 2002, Braithwait and Eakin, 2002, Barbose et al., 2004] dynamically adjust electrical demand in response to changing electrical energy prices or other grid signals. DR offers several benefits. By suitably adjusting energy prices, load can be shifted from peak energy consumption periods to other times. This, in turn, can improve operational efficiency, reduce operating costs, improve capital efficiency, and reduce harmful emissions and risk of outages. The variability of renewables can create an additional need to shift energy consumption in order to better match energy demand with unforecasted changes in electrical energy generation. The benefit is a reduction in backup (ancillary) generation frequently used to hedge renewable sources.\nThere are several types of DR. In direct DR a utility or other entity directly modifies the energy consumption of users by adjusting the operation of user’s equipment. Interruptible tariffs allow a utility to interrupt the supply of power to a company under predefined conditions. Price driven DR uses pricing mechanisms to attempt to modulate energy demand. DR has been extensively investigated for larger energy users and has been implemented in many areas (e.g., [Roos and Lane, 1998, Piette et al., 2005]). Residential and small commercial building DR [Herter, 2007b,a, Faruqui and George, 2005, Koch and Piette, 2007] offers similar potential benefits. DR for residential and\n∗Z. Wen, D. O’Neill and H. Maei are with the Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA.\nar X\niv :1\n40 1.\n15 49\nv1 [\ncs .L\nG ]\n8 J\nan 2\n01 4\nsmall commercial buildings was estimated to account for as much as 65% of the total energy savings potential of DR. However, DR in the residential and small commercial building sector faces several challenges.\nTechnical challenges include deploying an infrastructure supplying real-time pricing information to energy consumers1 in a useful way, ensuring security, and implementing advanced metering and networking devices [Koch and Piette, 2007, LeMay et al., 2008]. In addition to all these technical challenges, another challenge vital to the success of DR in the residential and small commercial building sectors is that it requires a fully automated Energy Management System (EMS) [Koch and Piette, 2007, Piette et al., 2007]. This is because with price driven DR, consumers face a continuing sequence of decisions to either use a particular device now and consume energy at current (known) prices or to defer using the device until later at possibly unknown prices. Each decision requires the consumer to weigh the cost differential against his dissatisfaction due to rescheduling device usage. This is particularly burdensome when the consumer must also estimate future energy prices. Further many of these decisions have limited financial impact on the consumer [ONeill et al., 2010], and, as a result, many rational consumers in the residential and small commercial building sectors may not be sufficiently incentivized to make these decisions over the long run (known as “decision fatigue” in [ONeill et al., 2010]).\nTo be effective, an EMS system needs to automatically make energy consumption decisions that are consistent with the cost delay trade-offs of energy users, in this way acting as an energy agent. It is often difficult to cost-effectively model the behaviors of the idiosyncratic consumers and the temporal variations of the energy prices, a successful DR EMS needs to learn to make optimal decisions for consumers from interacting with the consumers and energy prices. Recently, [ONeill et al., 2010] proposed a fully-automated EMS algorithm based on reinforcement learning (RL), which learns how to make optimal decisions for consumers. To the best of our knowledge, this is the first paper to apply RL to DR in residential and small commercial building sector. The authors adopt a request inventory model for the system dynamics and use Q-learning, a classical RL algorithm, to learn how to make the optimal decisions for energy consumers. In this approach, users make energy requests to the EMS system (e.g. pushing a button on a device that a user wants to run) and the system schedules the time of operation by calculating the user’s trade-offs between delay and energy prices. It learns these trade-offs by observing energy consumer’s behaviors and observing the patterns of energy pricing. Over time the EMS learns to make the best decisions for energy users in the sense that it balances energy cost and the delay in energy usage in the same way that the customer optimally would, but without the consumer having to make the decision. The authors explicitly assume that (1) consumers’ dissatisfaction with delay can be modeled by known disutility functions, and (2) that consumers explicitly initiate all energy usage. This approach has several limitations:\n• Finding specific disutility functions for a particular residence or small business can be difficult and costly. In [ONeill et al., 2010], the authors assume these functions have particular mathematical properties, but do not address how these functions might be determined. These functions are likely to be idiosyncratic and are specific to energy price vs. time delay tradeoffs.\n• Many energy consuming activities occur without the consumer directly initiating them. HVAC in office buildings and pool heaters in residential settings are obvious examples. A useful EMS would self-initiate jobs for these and similar devices without explicit user requests or reservations. For example, if it is unexpectedly hot in a summer afternoon and the current energy\n1Throughout this paper, we use the terms “user” and “consumer” interchangeably.\nprice is expected to be cheaper than that in the evening, then the EMS should be allowed to turn on AC without an explicit request/reservation from the consumer.\n• The computational complexity of this approach grows exponentially as the number of devices. Known as the “curse-of-dimensionality” in dynamic programming (DP) and RL literature, this problem limits the approach to fairly small numbers of devices.\nIn this paper, we propose a novel EMS formulation that addresses the limitations of [ONeill et al., 2010] described above. Our proposed algorithm also uses RL, but adopts a device centric point of view, and, as we will discuss in detail in Section 3, under reasonable assumptions, the RL problem decomposes over devices and it is sufficient to apply an RL algorithm to each individual device. Specifically, the algorithm addresses these issues:\n1. Our EMS formulation does not require a pre-specified disutility function modeling the consumer’s dissatisfaction on job rescheduling. Instead, under this formulation, the EMS is able to learn the consumer’s dissatisfaction by sampling his evaluations on completed/canceled jobs. In other words, our new RL formulation has eliminated the impractical assumption in [ONeill et al., 2010] that consumers’ dissatisfactions with delay can be captured by known disutility functions.\n2. Our approach allows both user-initiated jobs and EMS-initiated jobs. The EMS-initiated jobs use a probing/feedback mechanism to find the best way to anticipate future energy usage.\n3. Our EMS algorithm also enables more flexible user-initiated jobs, specifically\n• A consumer request’s target time can be different from its request time, where the request time is the time when the EMS receives this consumer request, and the target time is the time when the consumer prefers this request to be satisfied.\n• Consumer requests/reservations can have different priorities, whereas in [ONeill et al., 2010], all the consumer requests have the same priority.\n• Energy requests can be canceled by the consumer, reflecting the behavior of real energy users.\n4. The computational complexity of our approach grows linearly with the number of devices, and thus many classical RL algorithms can be applied even when there are a large number of devices.\nIn this paper, we also propose several new performance metrics for any RL algorithm applied to this problem. In particular, we suggest methods of measuring performance relative to the user’s current pattern of behavior and relative to a prescient optimal pattern of behavior.\nThe remainder of this paper is organized as follows. In Section 2, we briefly describe how a practical fully-automated EMS should interact with the consumer and the grid signals. In Section 3, we pose the optimal demand response problem faced by an EMS as an infinite horizon discounted Markov decision process (MDP), and decompose this high-dimensional MDP into a collection of low-dimensional device based MDPs under appropriate assumptions. Then, in Section 4, we propose the device based RL problem and discuss several performance metrics for RL algorithms applied to this problem. In Section 5, we demonstrate the simulation results on a representative example when the classical Q-learning is applied. We conclude in Section 6."
    }, {
      "heading" : "2 Description of fully-automated EMS",
      "text" : "A fully-automated EMS (henceforth referred to as EMS) is a necessary prerequisite to DR in the residential and small commercial building sectors. Furthermore, an EMS needs to learn how to make the optimal decisions for the user while interacting with them and the real-time grid signals. In this section, we describe how a fully-automated EMS should interact with the user and the grid signals.\nGenerally speaking, a fully-automated EMS observes the grid signals, receives requests and evaluations from the user, and schedules the jobs over the devices (Figure 1). We detail the interaction mechanisms in the remainder of this subsection.\nGrid Signals\nThe EMS observes the grid signals through a communication network, where the “communication network” refers to the infrastructure that supplies grid signals to the EMS. Any exogenous information that is effectively delivered by the communication network and is useful for the EMS to make the scheduling decisions can be regarded as a grid signal. The most common grid signal is the real-time energy price; other grid signals might include the expected future energy prices, the real-time temperature and weather condition, and other useful exogenous information.\nIn this paper, we assume that (1) the only available grid signal is the real-time energy price, and (2) the energy price is exogenous in the sense that the EMS’s actions will not influence it (i.e. the EMS is a price-taker). Notice that the second assumption is reasonable since in the residential and small commercial building sectors, the market power of an individual EMS (or equivalently, of an individual consumer) is so small that the impact of its actions on energy prices is negligible. On the other hand, the first assumption is proposed mainly to simplify the exposition and does not incur loss of generality. Specifically, both the model and the algorithm proposed in this paper can be applied to the cases with more general grid signals (e.g. the case in which the grid signals are energy price and temperature), as long as the grid signals are exogenous.\nUser-Initiated Jobs and EMS-Initiated Jobs\nA fully-automated EMS should perform the following functions:\n• The EMS receives requests from the consumers, and then schedules when to fulfill the received requests. We henceforth refer to this case as a user-initiated job.\n• If a smart device managed by the EMS is idle (i.e. currently there is no request for that device), the EMS could speculatively power that device. We henceforth refer to this case as an EMSinitiated job. For instance, in a small commercial building, the EMS might speculatively turn on the building’s air conditioning in advance of the tenant’s arrival to capture early morning lower energy costs or to mask the latency of cooling the building. Notice that we should not allow the EMS to do speculative jobs on all the smart devices (such as dishwashers).\nWe assume time is discrete t = 0, 1, · · · and that there are N smart devices managed by the EMS and numbered n = 1, 2, · · · , N . To simplify exposition, we assume that all the jobs done by device n are standardized and hence they can be completed in one time step and consume a constant energy Cn, which only depends on the type of the smart device. This assumption can be readily relaxed to devices with different operating periods. We further assume that the EMS will ignore all the user requests to device n if device n currently has an uncompleted request, but the EMS allows a consumer to cancel an existing uncompleted request. Specifically, if the consumer wants to replace an existing request with a new request, he must cancel the existent request first, and then start the new request. Notice under this assumption, at each time a smart device has at most one uncompleted request.\nInteraction between user and EMS\nWe now describe how the EMS interacts with the consumer (user) in a user-initiated job and an EMS-initiated job. Specifically, as its name suggests, a user-initiated job starts with a user sending a request to the EMS. Specifically, each consumer request is represented by a four-tuple J = (n, τr, τg, g), where\n• n denotes the requested device;\n• τr is the request time and denotes when the EMS receives this request;\n• τg is the target time and denotes when the user prefers this requested job to be completed;\n• g denotes the priority of this request, with higher priority implying the “stronger preference” of the user that they want the requested job to be completed at a time close to the target time τg.\nNotice that the target time τg is not necessarily equal to the request time τr; instead, the user might request to use a device in a later time (i.e. τg ≥ τr). On the other hand, for a request J = (n, τr, τg, g), it is unreasonable to assume that τg − τr, the difference between the target time and the request time, can be arbitrarily large. Thus, in this paper, we assume that (1) for any request J = (n, τr, τg, g), its target time τg must satisfy τr ≤ τg ≤ τr + Wn, and (2) if the request J = (n, τr, τg, g) is not fulfilled by time τg +Wn, then it will be canceled by the user, where Wn is a known time window and only depends on the type of device.\nWe also assume that an unsatisfied consumer request can be canceled by the user. Furthermore, we assume that when a consumer request is completed or canceled, the user will send an evaluation on the completed/canceled request to the EMS. As we will see later, the EMS can use such evaluations to learn the user’s dissatisfaction on the rescheduling of the user-initiated jobs.\nOn the other hand, an EMS-initiated job is started by the EMS, without receiving a request from the user. The only interaction between the EMS and the user for such jobs is that once an\nEMS-initiated job is completed, the user will send an evaluation on it to the EMS. As is in the user-initiated jobs, the EMS also exploits such evaluations to learn user’s dissatisfaction with the EMS-initiated jobs.\nIn summary, the interaction between user and EMS is as follows: for user-initiated jobs, the possible interactions include (1) the user sends requests to the EMS, (2) the user can choose to cancel the unsatisfied requests and (3) the user evaluates the completed/canceled requests. On the other hand, for EMS-initiated jobs, the only interaction between user and EMS is that the user evaluates the completed EMS-initiated jobs.\nTo complete the description of the user-EMS interaction, we need to specify the “user-EMS interaction timeline” in a single time period (i.e. whether the EMS receives the user requests/cancellations/evaluations before or after it schedules jobs in that time period). Without loss of generality, in this paper, we consider the user-EMS interaction timeline illustrated in Figure 2."
    }, {
      "heading" : "3 Device Based DR Model",
      "text" : "This section proceeds as follows. We first motivate and define the dissatisfaction function and the instantaneous cost function for a rational consumer in Subsection 3.1 and 3.2, respectively. Explicit dissatisfaction functions are not required in practice, but in this section we assume they are known to facilitate easy exposition. Then, Subsection 3.3 formulates the optimal demand response problem as a collection of device-based MDPs under suitable assumptions."
    }, {
      "heading" : "3.1 Dissatisfaction Function and Rational Consumers",
      "text" : "To formalize the notion of optimal demand response, in this subsection, we define the dissatisfaction function for a consumer (user) that captures their preferences (dissatisfaction) over job rescheduling. We also motivate and propose a specific class of functional forms for the dissatisfaction functions of the rational consumers.\nLet us start by defining some useful notation. Specifically, for each time t, we define the sets of smart devices D(t), C(t) ⊆ {1, 2, · · · , N} as\nD(t) = {devices that do a job at time t} C(t) = {devices for which an unsatisfied request is canceled at time t} .\nNote that D(t) ⋂ C(t) = ∅, since if device n does a job in time period t, then there is no unsatisfied request for device n at the end of time period t (recall that at any time, device n has at most one unsatisfied request, and see Figure 2 for the timeline of time period t). Furthermore, we use J (t) to denote the set of unsatisfied requests right after the requests in time period t have been\nreceived. Since at any time device n has at most one unsatisfied request, thus, if J (t) contains one unsatisfied request for device n, then we use J(t, n) to denote this particular request; otherwise, we set J(t, n) = NULL. Notice that Ht = {(D(s), C(s),J (s)) , ∀0 ≤ s ≤ t} completely specifies the “history” of device operations, consumer requests/cancellations and the EMS decisions until the end of time period t.\nGenerally speaking, the user’s dissatisfaction on rescheduling at time t should be a function of Ht, and we denote this function as\nŪ (t) [Ht] = Ū (t) [(D(s), C(s),J (s)) , ∀0 ≤ s ≤ t] . (1)\nObviously, directly working with Ū (t) [Ht] defined in Eqn(1) will result in a computationally intractable problem. To overcome this challenge, in the remainder of this subsection, we make some assumptions on the functional form of Ū (t) [Ht]. All such assumptions are motivated by observations of the preferences and behaviors of the rational consumers. Generally speaking, we think a rational consumer’s preferences and behaviors should have the following characteristics:\n1. A rational consumer prefers low electricity price to high electricity price.\n2. For a consumer request sent to the EMS, a rational consumer prefers that job is completed at a time close to the target time, and the higher the request’s priority is, the “stronger” this preference is.\n3. For an EMS-initiated job, a rational consumer will be “happy” if the EMS does this job at a “right” time, and will be “unhappy” if it does this job at a “wrong” time.\n4. When a rational consumer decides to cancel an unsatisfied consumer request, he is usually “unhappy” if the target time has already passed.\n5. A rational consumer generally discounts future costs and benefits.\n6. A rational consumer’s preference over job rescheduling changes slowly over time and is reflected in his evaluations on completed/canceled jobs.\n7. A rational consumer’s preference over job rescheduling on smart devices is weak compared to his preferences in other aspects of life.\nWe now motivate and propose a specific class of functional forms for the dissatisfaction function Ū (t) [Ht] of the rational consumers. To formalize the idea, we define the “history” for device n at time t as\nH(n)t = {(1(n ∈ D(s)),1(n ∈ C(s)), J(s, n)) , ∀0 ≤ s ≤ t} ,\nwhere 1(·) is the indicator function and J(s, n) is defined above. First, we make the following simplifying assumption:2\n2Note that a rational consumer’s preference over job rescheduling is weak compared to his preferences in other aspects of life. Based on this, let us provide an intuitive motivation for Assumption 1 from the following perspective: let ūDR ∈ <N be a vector encoding the user’s dissatisfactions on job rescheduling over N devices, and ūother be a vector encoding the user’s dissatisfactions in other aspects of life. Assume the overall dissatisfaction (unhappiness) of the user is f(ūDR, ūother), where f is a general non-linear function. Now we consider f(ūDR + ∆ūDR, ūother), notice that the weak preference over job rescheduling implies that ∆ūDR is “small”, thus, f(ūDR + ∆ūDR, ūother) can be well approximated by f(ūDR, ūother) + ∇ūDRf(ūDR, ūother)∆ūDR. Notice that for our purpose, we only care about the “change” of the user’s overall unhappiness as a function of the “changes” in his dissatisfactions on job rescheduling, thus, the user’s dissatisfaction function is Ū(∆ūDR) = f(ūDR + ∆ūDR, ūother) − f(ūDR, ūother) ≈ ∇ūDRf(ūDR, ūother)∆ūDR, which is a weighted sum of ∆ūDR. In Assumption 1, we further simplify the dissatisfaction function by assuming that all the weights are equal to 1.\nAssumption 1 For any t ≥ 0, the dissatisfaction function Ū (t) is additive over the devices, that is\nŪ (t) (Ht) = N∑ n=1 Ū (t,n)(H(n)t ), (2)\nwhere Ū (t,n) captures the consumer’s dissatisfaction at time t for device n and H(n)t is the “history” for device n by time t.\nFurthermore, since a rational consumer’s preference over job rescheduling is weak compared to his preferences in other aspects of life, and changes slowly over time, we assume that\n• If there is no job completed/cancelled at time t on device n, we assume that Ū (t,n)(H(n)t ) = 0.\n• If there is a job completed/cancelled at time t on device n, we assume that Ū (t,n)(H(n)t ) only depends on the job completed/canceled. That is, Ū (t,n)(·) is time-invariant and does not depend on the previously completed/canceled jobs.\nFinally, based on the assumptions above and the characteristics (2)-(4) of a rational consumer’s preferences over job rescheduling, we assume the dissatisfaction function Ū (t,n)(H(n)t ) takes the following forms:\n• If device n satisfies consumer request J(t, n) = (n, τr, τg, g) at time t, we assume\nŪ (t,n)(H(n)t ) = Ũ (n)r (t− τg, g), (3)\nwhere the subscript “r” denotes that it is the dissatisfaction incurred when a request is satisfied, and g is the priority of the request. Note t − τg captures not only the distance between the current time t and the request’s target time τg, but also whether or not the target time has passed. For rational consumers, Ũ (n) r should be small when t− τg is close to 0 and increases as t − τg deviates from 0; furthermore, the higher the priority g, the higher this increase rate will be.\n• Similarly, if the request J(t, n) = (n, τr, τg, g) is cancelled by the consumer at time t, we assume\nŪ (t,n)(H(n)t ) = Ũ (n)c (t− τg, g), (4)\nwhere the subscript “c” denotes that it is the dissatisfaction incurred when a request is cancelled. In practice, Ũ (n) c should be very small if t < τg, and it will increase with t when t ≥ τg; furthermore, the higher the priority g, the higher this increase rate will be.\n• If EMS initiates a job on device n at time t, we assume that\nŪ (t,n)(H(n)t ) = Ũ (n)e (t− τp), (5)\nwhere the subscript “e” denotes that it is the dissatisfaction incurred when an EMS-initiated job is done, and τp− 1 is the time when the previous job on device n (either user-initiated or EMS-initiated) is completed or cancelled. In practice, Ũ (n) e should decrease as t−τp increases; the large Ũ (n) e for small t− τp prevents too frequent EMS-initiated jobs.\n• Otherwise, we assume that\nŪ (t,n)(H(n)t ) = 0. (6)\nNote that the dissatisfaction function class described in Equations (3-6) is still quite general,\nand it is very challenging to proceed to derive the specific functional forms of Ũ (n) r , Ũ (n) c and Ũ (n) e . As we will see in Section 4, there is no actual need to estimate Ũ (n) r , Ũ (n) c and Ũ (n) e . In practice, the EMS learns Ũ (n) r , Ũ (n) c and Ũ (n) e based on the consumer’s evaluations on completed/canceled jobs."
    }, {
      "heading" : "3.2 Instantaneous Cost Function",
      "text" : "We assume the instantaneous cost function of the consumer at time t has the following form: Pt ∑\nn∈D(t)\nCn + γŪ (t) (Ht) , (7)\nwhere Pt is the electricity price at time t and Ū is a dissatisfaction function capturing the consumer’s dissatisfaction on job rescheduling at time t. Specifically, notice that ∑ n∈D(t)Cn is the total\nelectricity energy consumed at time t, and hence Pt ∑\nn∈D(t)Cn is the electricity bill the consumer pays at time t. We assume the consumer’s dissatisfaction on rescheduling at time t depends on the “rescheduling history” Ht and γ > 0 represents the tradeoff between the electricity bill paid and the consumer’s dissatisfaction on rescheduling.\nNote that from the EMS’s perspective, both the electricity price and the consumer behavior are exogenous and stochastic; thus, in this paper, we assume that EMS aims to minimize the expected infinite-horizon discounted cost:\nE  ∞∑ t=0 αt Pt ∑ n∈D(t) Cn + γŪ (t)(Ht)  , where 0 < α < 1 is the discrete-time discount and captures the assumption that a rational consumer generally discounts future costs and benefits. Notice that in this problem formulation, the state at time t is (Pt,Ht−1,J (t)), and the action is D(t) (i.e. the EMS chooses which devices to power on).\nUnder Assumption 1, the dissatisfaction function is decomposable and the infinite-horizon discounted cost function can be written as\nN∑ n=1\n[ E { ∞∑ t=0 αt [ PtCn1(n ∈ D(t)) + γŪ (t,n) ( H(n)t )]}] . (8)"
    }, {
      "heading" : "3.3 Device Based MDP Model",
      "text" : "In this subsection, we first propose a dynamic model for the energy price Pt. Then, we motivate an assumption on user behavior (Assumption 3), and show that under the proposed assumptions, the EMS’s decision-making (job scheduling) problem is decomposed over devices. Finally, we formulate the job scheduling problem on each device as a Markov Decision Process (MDP), which we henceforth refer to as the device based MDP model.\nLet us start by proposing a dynamic model for the energy price Pt. As discussed in Section 2, Pt is exogenous in the sense that the EMS’s actions will not influence it. Furthermore, recent literature (e.g. [ONeill et al., 2010]) shows that Pt can be modeled as a Markov process. Thus, throughout this paper, we make the following assumption:\nAssumption 2 Energy price Pt follows an exogenous finite-state ergodic Markov chain.\nWe use P to denote the set of states of this Markov chain, and use |P| to denote the number of states.\nIn this paper, we assume that the user is rational and the user’s dissatisfaction function has the functional form specified by Eqn(3-6). Thus, to complete the description of the user behavior, we only need to model when the user sends requests/cancellations to the EMS. Similarly as Pt, we model the consumer requests/cancellations as stochastic processes and make the following assumption on them throughout this paper:\nAssumption 3 (A) The consumer request/cancellation processes are statistically independent of the energy price process Pt; and (B) consumer requests/cancellations to different devices are statistically independent.\nAssumption 3 is a key assumption in this paper. Thus, we briefly motivate it before proceeding: first, we claim that Assumption 3(A) is reasonable since\n• On one hand, as we have discussed in Section 2, in the residential and small building sector, the market power of an individual consumer is so small that the impact of their behavior on Pt is negligible.\n• On the other hand, as we have discussed in Section 1, due to “decision fatigue”, most rational consumers are not incentivized to reference current and expected future energy prices before sending requests/cancellations to the EMS. Thus, for such consumers, their requests/cancellations are independent of current and expected future energy prices.\nConsequently, it is reasonable to assume Assumption 3(A). We now briefly motivate Assumption 3(B). One observation about devices is that they can be classified into categories such that for most consumers, their requests to devices in different categories are weakly dependent. For instance, in a residential household, we can classify the airconditioner, electric vehicle, laundry machine and dryer into three categories: category 1 includes the air-conditioner; category 2 includes the laundry machine and dryer; and category 3 includes the electric vehicle. Under this classification, it is observed that for most residential consumers, the statistical dependence between their requests to devices in different categories are weak. Thus, it is reasonable to assume that consumer requests/cancellations to different device categories are statistically independent. To simplify, we assume each category has only one device and hence propose Assumption 3(B). Of course, a category might have multiple devices (e.g. laundry machine and dryer), and usually the consumer requests to devices in the same category are statistically dependent. We discuss how to extend the results of this paper to that scenario in Section 6.\nRecall that the EMS’s objective is to find an optimal scheduling policy to minimize (8), which decomposes over devices under Assumption 1. Assumption 3 states that the consumer requests/cancellations to different devices are statistically independent, thus, we have the following proposition:\nProposition 1 Under Assumptions 1 and 3, the EMS’s decision-making problem is decomposed over devices.\nIn the remainder of this paper, we focus on deriving the optimal scheduling policy for a single device and drop the subscript n to simplify the exposition. For example, we will use W instead of Wn to denote the time window and represent a request as J = (τr, τg, g). We also use the term “smart\ndevice” and “EMS” interchangably henceforth, since due to the decomposition of the problem, one can think each smart device has its own EMS.\nWe further assume that the timeline for a smart device can be divided into “episodes”. Specifically, we assume that whenever the smart device completes a job (either initiated by the user or by the EMS) or the current unsatisfied request is canceled by the user, the current episode terminates. In the next time step, the smart device “regenerates” its state according to a fixed distribution π0 and a new episode starts. The notion of episode is illustrated in Figure 3.\nWe now propose an MDP model for a single smart device (the device based MDP model). The state of the MDP at time t is xt = [Pt, st, gt]\nT ∈ S, where Pt is the exogenous electricity price at time t, st is the elapsed time at time t, gt is the priority of request at time t and S is the state space. Specifically, we define\nst = { t− τp if no request received in the current episode t− τg otherwise\nwhere τp is the start time of the current episode and τg is the target time of the received request. Furthermore, we use gt = 0 to denote that no request has yet been received in the current episode; once a request is received in the current episode, we assume its priority gt ∈ {1, 2, · · · , gmax}.\nSince the electricity price is exogenous, we can partition the state xt = [Pt, st, gt] T as the “price portion” Pt and “device portion” [st, gt] T . The “device portion” of the MDP state transition model is summarized in Figure 4; notice that there are (2W + 1)gmax + Ŵ + 1 “device portion” states. Recall that we use P to denote the set of states of the price Markov chain, so the price Markov chain has |P| states, and the cardinality of the state space for the device based MDP is |S| = |P| [ (2W + 1)gmax + Ŵ + 1 ] , which is affine in |P|, W , Ŵ and gmax.\nNote that the action space at each state x ∈ S is A = {off, on}, where action “on” means the smart device completes a job3 at the current time and “off” means the smart device does nothing at the current time. We use Φ (xt, at, xt+1) to denote the instantaneous cost at state-action-state triple (xt, at, xt+1).\nBased on the above discussion, the device based MDP model is detailed below:\n• Pt follows an exogenous Markov Chain with |P| states. The transition probability from Pt to Pt+1 is denoted as Pr(Pt+1|Pt).\n• If the smart device has not received a consumer request in the current episode, recall that we set gt = 0 and elapsed time st = t− τp. Notice that:\n1. If action “off” is selected, the current cost is Φ (xt, at, xt+1) = 0. Then the smart device receives a consumer request (t + 1, τg, g) at the next time step (time t + 1) with\n3More specifically, it completes a user-initiated job if a consumer request has been received in the current episode, and completes an EMS-initiated job otherwise.\n(0,0)   (1,0)   (Ŵ,0)   (Ŵ-‐1,0)  \n(-‐W,1)  \n(-‐W+1,1)  \n(W,1)  \n(-‐W,2)  \n(-‐W+1,2)  \n(W,2)  \n(-‐W,gmax)  \n(-‐W+1,gmax)  \n(W,gmax)  \nNo  request  received  \nPriority  1   Priority  2   Priority  gmax  \np(0,)   P(1,)   p(Ŵ-‐1,)   p(Ŵ,)  \nnot satisfied this request, recall we set st = t− τg, where τg is the target time of this request. Notice that:\n1. If action “off” is selected, then with probability 1− p̃st,gt , the user will cancel this request at the end of time period t. In this case, the current episode terminates and the smart device regenerates the “device portion” state based on distribution π0 at time t+1. The cost associated with this transition is Φ (xt, at, xt+1) = γŨc(st, gt), where Ũc is defined in (4). On the other hand, with probability p̃st,gt , the “device portion” state transits to\n[st+1 = st + 1, gt+1 = gt] T at time t + 1 and the cost associated with this transition is Φ (xt, at, xt+1) = 0. Notice that we assume p̃st,gt = 0 if st = W .\n2. If action “on” is selected, the current cost is Φ (xt, at, xt+1) = PtC + γŨr (st, gt), where Ũr (st, gt) is defined in (3) and C is the constant energy consumed by a standardized job. Then the current episode terminates and the smart device regenerates the “device portion” state based on distribution π0 at time t+ 1.\nNote that if the transition model of the device based MDP and the dissatisfaction function of the consumer are known, the device based MDP can be solved by dynamic programming (DP). Following ideas in classical DP, it is straightforward to derive the Bellman equation (see Appendix A) from which we can compute the optimal Q-function Q∗. Specifically, many DP algorithms, such as value iteration and policy iteration, can be used to compute Q∗ (see [Bertsekas, 2005]). We observe that for many DP algorithms, computing Q∗ is tractable since the cardinalities of the state space S and action space A in a device based MDP are usually small. Once Q∗ is available, one optimal policy µ∗ is µ∗(xt) ∈ argmina∈AQ∗(xt, a)."
    }, {
      "heading" : "4 Reinforcement Learning and Performance Metrics",
      "text" : ""
    }, {
      "heading" : "4.1 Reinforcement Learning Formulation",
      "text" : "However, as we discussed in Section 1, in most practical cases, the transition probabilities and the dissatisfaction function of the user are initially unknown. Thus, the EMS must learn how to make optimal decisions for the user based on the incrementally gathered data from the user and the realtime energy price. Reinforcement learning (RL) is a collection of techniques for a decision-maker to learn how to make optimal decisions while interacting with an unknown “environment” [Sutton and Barto, 1998]. RL tries to strike a balance between learning (exploration) and optimization (exploitation), and has been extensively used in many other fields, such as artificial intelligence and robotics [Coates et al., 2010]. As has been discussed in [ONeill et al., 2010], it is natural to use RL to solve EMS’s learning problem since EMS needs to learn to make decisions while interacting with the user and the real-time energy price. As is shown in Figure 5, under the RL formulation of the optimal DR problem, the agent is the EMS, and the environment includes both the energy price and consumer behavior. We refer to this RL problem as the device based RL problem.\nAs in the RL literature, we assume the EMS (decision-maker) initially knows the state space S and the action space A = {off, on}. Furthermore, we also assume it knows the discrete-time discount α, and the price-rescheduling trade-off parameter γ. Note that the second assumption is reasonable since before the user starts to use the smart device, they can manually set up an α reflecting their discount on future costs, as well as a γ indicating their tradeoff between the electricity bill and rescheduling. With experience, the user can also tune these parameters.\nHowever, as is discussed above, in theory the EMS (the decision-maker) needs to learn the transition model and the dissatisfaction function of the user based on its experience. Obviously,\nthe transition model can be learned by directly observing the user behavior and the real-time energy price. We now discuss how the EMS learns the user’s dissatisfaction function. Note that under our proposed device based MDP model, the user’s dissatisfaction function is non-zero only when a job is completed or canceled. Furthermore, notice that as has been discussed in Section 2, once a job is completed/canceled, the EMS will receive an evaluation from the user. In the remainder of this paper, we make the following assumption:\nAssumption 4 The user’s dissatisfaction on the rescheduling of a completed/canceled job is equal to their evaluation on this job.5\nWe now briefly motivate Assumption 4: for a rational user, their evaluation on a completed/canceled job mainly reflects their dissatisfaction on this job, and this dissatisfaction is either due to the (high) energy price or the (undesirable) rescheduling of the job. As we have discussed in Section 1, due to “decision fatigue”, most rational users will not reference the current energy price or any expected future energy prices before they send their evaluations to the EMS. Thus, their evaluations will mainly reflect their dissatisfactions on job rescheduling. Thus, it is reasonable to assume Assumption 4.\nIn summary, in our proposed device based RL problem, the EMS initially knows the state space S, the action spaceA, the discount α, the trade-off parameter γ and the per-job energy consumption C; but it does not know the state transition model or the user’s dissatisfaction function. It observes the state transitions and the user’s evaluations (which are equal to the user’s dissatisfactions on rescheduling under Assumption 4) as it interacts with the user and the real-time energy price. At each time step, it aims to make good decisions based on its initial information, past observations and current state."
    }, {
      "heading" : "4.2 Baseline and Demand Response Potential",
      "text" : "In order to justify a RL algorithm achieves satisfactory performance, we need to compare its experimental performance with respect to a reasonable baseline. In this subsection, we propose a reasonable baseline and define the notion of the demand response (DR) potential.\nLet’s start by defining some useful notation: we use µ : S ×A → [0, 1] to denote a (randomized and stationary) policy of a device based MDP. Specifically, under (randomized) policy µ, at any\n5Assumption 4 is an idealized assumption on the relationship between evaluation and dissatisfaction. In practice, it might be more reasonable to assume that the user’s evaluation on a job is equal to their dissatisfaction on the rescheduling of this job plus a (zero-mean, finite-variance, statistically independent) “behavioral noise”. However, the “behavioral noise” does not fundamentally change the assumption and we make Assumption 4 to simplify exposition.\nstate x = [P, s, g]T ∈ S, action a ∈ A is chosen with probability µ(x, a). As is classical in DP and RL, we use Qµ to denote the Q-function of the device based MDP under policy µ (see [Sutton and Barto, 1998]). Note that under Assumption 2, the energy price Pt follows an exogenous ergodic Markov chain; thus, we use πP to denote the unique stationary distribution of this price Markov chain. Moreover, recall that once an episode terminates, the “device portion” state is regenerated according to distribution π0. Recall that state x = [P, s, g]\nT , thus, we use x ∼ πP × π0 to denote P ∼ πP , [s, g]T ∼ π0, and P and [s, g]T are statistically independent. We define the performance of a policy µ as follows:\nVµ = Ex∼πP×π0 { Ea∼µ(x,·) [Qµ (x, a)|x] } . (9)\nSince this paper focuses on how demand response (DR) can potentially reduce a user’s (expected infinite-horizon discounted) cost, we choose the baseline policy as the one without job rescheduling and denote it as µbase. Specifically, under µbase, the EMS never self-initiates a job, and all the jobs initiated by the user are scheduled to be completed at their target times. We are interested in how much a proposed RL algorithm can reduce the user’s cost with respect to Vµbase .\nRecall that we use µ∗ to denote the optimal policy. We define the demand response (DR) potential as\nDRP = Vµbase − Vµ∗ . (10)\nBy definition, DR potential is the maximum expected cost reduction that can be achieved by DR. Obviously, in the case when the transition model and the dissatisfaction function are known, µ∗ can be derived beforehand and hence DRP is achieved. However, in the practical cases when the EMS needs to learn µ∗ through some RL algorithm, DRP is generally not achievable. As we will see, performance loss with respect to DRP is one performance metric for RL algorithms.\nWe define the relative DR potential (RDRP) as\nRDRP = DRP/Vµbase = (Vµbase − Vµ∗) /Vµbase . (11)\nOf course, DRP and RDRP depend on the problem instance. We are particularly interested in how DRP and RDRP vary with γ, since γ specifies the consumer’s tradeoff between job rescheduling and the electricity bill paid. To simplify the expositions of the result, we make the following assumption in the remainder of the paper:\nAssumption 5 (A) Energy price is always strictly positive; (B) the user’s dissatisfaction function satisfies Eqn(3-6) and is always non-negative; (C) if a user’s request is satisfied at its target time, the user’s dissatisfaction is 0; and (D) if the user cancels their request before the target time, their dissatisfaction is 0.\nNote that Assumption 5(C) and 5(D) are reasonable since for a rational user, it is proper to assume their dissatisfaction is minimal if their request is satisfied at the target time, or canceled by them before the target time. Then Assumption 5(B) can be achieved by shifting the dissatisfaction function by a constant. Assumption 5(A) follows from the daily observation. Under Assumption 5, we have the following result:\nTheorem 1 Under Assumption 5, we have that:\n1. Vµbase does not depend on γ.\n2. There exists a γ∗ > 0 s.t. µbase is an optimal policy when γ > γ ∗. Thus, as γ →∞, DRP→ 0.\n3. DRP and RDRP are non-increasing functions of γ.\n4. If γ = 0, then RDRP = 1.\nPlease refer to Appendix B for the proof of Theorem 1."
    }, {
      "heading" : "4.3 Performance Metrics",
      "text" : "In this subsection, we propose several performance metrics for a RL algorithm applied to our proposed device based RL problem. We start by defining some notation: for any t = 0, 1, · · · , use µ̃t to denote the policy under which the RL algorithm chooses the action at at the beginning of time period t. We define the performance of the RL algorithm as\nṼ = E [ ∞∑ t=0 αtΦ (xt, µ̃t (xt) , xt+1) ] , (12)\nnote that the expectation is not only taken with respect to the initial state (similarly as Eqn(9), we assume x0 = [P0, s0, g0]\nT ∼ πP × π0 in Eqn(12)), but also with respect to the subsequent stochastic transitions, noisy evaluations and possible randomizations in the RL algorithm. Note that by definition, we have Vµ∗ ≤ Ṽ .\nThe first performance metric of an RL algorithm is its relative improvement (RI) with respect to the baseline, which is defined as\nRI = (Vµbase − Ṽ )/Vµbase . (13)\nRI captures the normalized expected cost reduction the user will benefit from an RL algorithm. Note that since Vµ∗ ≤ Ṽ , we have RI ≤ RDRP. That is, the relative DR potential is an upper bound on the relative improvement.\nThe second performance metric of an RL algorithm is its relative realized DR potential (RRDRP), which is defined as\nRRDRP = RI/RDRP = (Vµbase − Ṽ )/(Vµbase − Vµ∗), (14)\nif RDRP > 0. Intuitively, RRDRP captures how much DR potential is realized by an RL algorithm. Since Vµ∗ ≤ Ṽ , we have RRDRP ≤ 1.\nWe briefly comment on RI and RRDRP before proceeding. First, notice that both RI and RRDRP can be negative, since there is no guarantee that Ṽ ≤ Vµbase . Specifically, under Assumption 5, it is likely that Ṽ > Vµbase when γ is sufficiently large. This is because Vµbase ↓ Vµ∗ as γ → ∞, and whatever γ is, an RL algorithm is likely to incur a non-trivial performance loss before it learns an optimal policy.\nSecond, we observe that RRDRP also serves as an indicator on whether or not it is worthy to explicitly model the user behavior/dissatisfaction and the energy price. Specifically, if DRP is large but RRDRP is small for many widely used RL algorithms, then it might be worthy to explicitly model the user behavior/dissatisfaction and the energy price, as long as the cost of such modeling is smaller than Ṽ − Vµ∗ .\nWe also note that it is important for our proposed RL algorithm to achieve satisfactory performance quickly, since otherwise the consumer will not be incentivized to use it. The third performance metric focuses on this aspect. Specifically, ∀j = 0, 1, · · · , we use tj to denote the end time\nof episode j.6 We define\nVµbase(j) = E  tj∑ t=0 αtΦ (xt, µbase (xt) , xt+1)  . Thus, Vµbase(j) is the user’s expected total discounted cost in the first j + 1 episodes (from episode 0 to episode j) under the baseline policy. Similarly, we define\nṼ (j) = E  tj∑ t=0 αtΦ (xt, µ̃t (xt) , xt+1)  , where µ̃t is the policy under which the RL algorithm chooses the action at at the beginning of time period t. Similarly, Ṽ (j) is the user’s expected total discounted cost in the first j+1 episodes under the RL algorithm.\nWe define the third performance metric, the baseline crossing time, as BCT = min { j∗ ≥ 0 : Ṽ (j) ≤ Vµbase(j), ∀j ≥ j ∗ } . (15)\nNote that BCT is measured in the number of episodes instead of the number of time periods. This is because measuring time in episodes is more meaningful in our problem: each episode corresponds to one usage of the device, and hence the number of episodes corresponds to how many times the consumer has used the device. Thus, measuring time in episodes avoids the problem that the consumer might sometimes use the device more frequently, while at other times use it less frequently."
    }, {
      "heading" : "5 Simulation",
      "text" : "In this section, we first briefly review the classical Q-learning algorithm in Subsection 5.1. Then we discuss a representative simulation example in Subsection 5.2. Finally, the simulation results are demonstrated in Subsection 5.3."
    }, {
      "heading" : "5.1 The Q-Learning Algorithm",
      "text" : "As we have discussed in Section 4, many RL algorithms can be applied to our proposed device based RL problem. In this subsection, we implement one of the most popular and classical RL algorithms, known as Q-learning [Watkins, 1989]. Q-learning is an off-policy learning algorithm; that is, it allows the learning agent to follow an exploratory policy while learning about an optimal policy. Another desirable features of Q-learning is that it is online, incremental and is easy to implement on real-time data.\nQ-learning works based on temporal-difference learning [Sutton and Barto, 1998]. At each time-step t the learning algorithm receives an input data in the form of (xt, at,Φt, xt+1), where Φt def = Φ (xt, at, xt+1) is the observed instantaneous cost after taking action at from state xt and arriving at state xt+1. Then the Q-learning algorithm updates the action-value function Qt(xt, at) according to\nQt+1(xt, at) := Qt(xt, at) + βt [ Φt + α min\na′∈A Qt(xt+1, a\n′)−Qt(xt, at) ] ,\n6Note that tj ’s are random variables (more specifically, stopping times). Furthermore, the distribution of tj ’s depends on the policy.\nand Qt+1(x, a) := Qt(x, a) if (x, a) 6= (xt, at). Note Qt ∈ <|S||A| is the state-action value function (Q-function) estimate in time period t, and βt > 0 denotes step-size in time period t. If the step-size sequence satisfies ∑+∞ t=0 βt = +∞ and ∑+∞ t=0 β 2 t < +∞, then Q-learning is guaranteed to converge to the optimal solution if all states are visited infinitely often (see [Sutton and Barto, 1998]). To complete the description of a Q-learning algorithm, we also need to specify a behavioral policy µb under which the algorithm chooses actions. The choice of µb will affect data and thus would help the algorithm to learn an optimal policy faster. One main ingredient of µb is that is it has to be exploratory policy during the learning process. One of standard, but crude, suggestions for how to select µb is as follows: for a small 0 < < 1, with probability (henceforth referred to as the “exploration probability”), the algorithm chooses action at from state xt according to a randomized policy, and with probability 1− , the algorithm chooses at ∈ argmina∈AQt (xt, a), where Qt refers to the state-action value function estimate at time t. Here, we consider a randomized policy in the form of Boltzmann exploration\nπ(at|xt) = exp [−Qt(xt, at)/η]∑ a′∈A exp [−Qt(xt, a′)/η] ,\nwhere η > 0 is a tuning parameter and is referred to as the “temperature” of the Boltzmann exploration. The Q-learning algorithm that we implement in this paper is illustrated in Algorithm 1.\nAlgorithm 1 The Q-Learning Algorithm\n1: Initialize Q0 arbitrarily 2: Repeat for each episode j: 3: Choose a small constant step-size βj > 0 for each episode 4: for each time period t in episode j do 5: Take action at at state xt according to the behavioral policy µb 6: Observe the instantaneous cost Φt and new state xt+1 7: Compute the TD error\nδt := Φt + α min a′∈A\nQt ( xt+1, a ′)−Qt (xt, at) 8: Update\nQt+1(x, a) := { Qt(x, a) + βjδt if (x, a) = (xt, at) Qt(x, a) otherwise\n9: end for"
    }, {
      "heading" : "5.2 Simulation Setup",
      "text" : "In this subsection, we propose a representative example to which we apply the Q-learning algorithm detailed in Algorithm 1. Specifically, in this example, we assume that the exogenous price Markov chain has state space P = {10, 12, 15, 20}, and the consumer requests have two different priorities, “high” and “normal”. We set the time windows W = 4 and Ŵ = 5, the discrete-time discount α = 0.9995 and the per job energy consumption C = 1. Thus, there are |S| = 96 states in this example. The dissatisfaction functions Ũr, Ũc and Ũe are illustrated in Figure 6(a), 6(b) and 6(c). Notice that these dissatisfaction functions satisfy the characteristics of the rational consumer preferences discussed in Subsection 3.1, as well as Assumption 5.\nAs to the transition model, we assume that if the smart device has not received a consumer request in the current episode, then under action “off”, it will receive a consumer request in the next time step with probability pst . Notice that pst is chosen to be an increasing function of the elapsed time st (see Figure 6(d)). We further notice that there are (W + 1)gmax = 10 types of consumer requests (with different target times and priorities), for simplicity, we assume these 10 types of requests are equally likely. Furthermore, if the smart device has received a consumer request in the current episode, then under action “off”, the unsatisfied request will be canceled with probability p̂st . In this example, we assume the “cancellation probability” p̂ only depends on the elapsed time st and is independent of the priority gt. We choose p̂st as an increasing function of st (see Figure 6(e)). Finally, we assume that when the smart device regenerates its state, with probability 1, the regenerated “device portion” state is [st+1 = 0, gt+1 = 0]\nT . We plot the RDRP of this example as a function of the trade-off parameter γ (see Figure 7). Since this example satisfies Assumption 5, thus, as Theorem 1 indicates, the RDRP is a non-increasing function of γ. Furthermore, when γ = 0, RDRP = 1, and RDRP→ 0 as γ →∞."
    }, {
      "heading" : "5.3 Performance",
      "text" : "In this subsection, we present the simulation results of the Q-learning algorithm on the representative example detailed in Subsection 5.2. We start by describing how we implement the Q-learning algorithm. We choose the “exploration probability” = 0.05 and the “temperature” of the softmin policy η = 0.1. For episode j, we choose the step-size βj = 10 20+j . We initialize the Q-learning algorithm by setting Q0 = 0.\nFor each trade-off parameter γ = 0, 0.1, 0.2, · · · , we run the Q-learning algorithm for ⌈\n2 1−α\n⌉ =\n4, 000 episodes, and repeat the simulation for 200 times. Then we approximate Ṽ and Ṽ (j),\n∀j = 0, 1, · · · by their corresponding sample means, i.e., by averaging the simulation results in these 200 simulations. Note that both Vµbase and Vµbase(j), ∀j = 0, 1, · · · , can be analytically derived based on the Bellman equation under policy µbase. Thus, RI, RRDRP and BCT can be (approximately) computed based on Vµbase , Vµbase(j) and the sample means of Ṽ and Ṽ (j). The simulation results are summarized in Figure 8-10, where we plot the RI, RRDRP and BCT as functions of the trade-off parameter γ. Note that we only plot the simulation results for 0 ≤ γ ≤ 4.3, since for γ ≥ 4.4, we have Ṽ > Vµbase . Consequently, for γ ≥ 4.4, RI and RRDRP are negative, and BCT =∞.\nWe conclude this section by briefly discussing about the simulation results. Notice that Figures 8-10 show that in this representative example, RI and RRDRP are decreasing functions of γ, while BCT is an increasing function of γ. This implies that with all the other parameters fixed, the more the user prefers “no rescheduling” to “low energy price”, the harder for the Q-learning algorithm to (1) achieve a significant improvement compared with the baseline, (2) realize a large fraction of the DR potential, and (3) quickly outperform the baseline policy.\nWe also observe that BCT = 0 for γ ≤ 2.1. This implies that for such γ’s, by initializing Q0 = 0, the Q-learning algorithm outperforms the baseline policy from episode 0. Furthermore, as γ ↑ 4.4, Ṽ ↑ Vµbase and hence BCT→∞."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have motivated and proposed a novel EMS formulation for the DR problem in the residential and small commercial building sectors, which we refer to as the device based RL problem. Specifically, we have shown that under appropriate assumptions, our proposed EMS formulation does not require a pre-specified disutility function modeling the consumer’s dissatisfaction on job rescheduling, and has a computational complexity that grows linearly with the number of devices. Our new EMS formulation also enables the EMS to self-initiate jobs and allows the users to the initiate more flexible requests.\nWe have also motivated and proposed several performance metrics for RL algorithms applied to the device based RL problem, and demonstrated the simulation results when the classical Qlearning algorithm is applied to a representative example. Simulation results suggest that for a broad range of trade-off parameter γ, the Q-learning algorithm outperforms the baseline policy without any job rescheduling.\nFinally, it is worth pointing out that many assumptions on the device based MDP model discussed in this paper are proposed to simplify the exposition, and we can easily relax some of these assumptions to obtain a more general device based MDP model. In Appendix C, we discuss two relaxations that are of particular interest: stacking request and dependent devices."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We acknowledge Prof. Benjamin Van Roy for his insightful comments on this paper."
    }, {
      "heading" : "Appendix A Bellman Equation",
      "text" : "It is worth pointing out that if the transition model of the device based MDP and the dissatisfaction function of the consumer are known, the device based MDP can be solved by dynamic programming (DP). Following ideas in classical DP, in this section, we derive the Bellman equation from which we can compute the optimal Q-function.\nRecall that xt = [Pt, st, gt] T and the action space at each state is A = {off, on}, we have\n• If the smart device has not received a consumer request in the current episode, we have Q∗ (xt, on) = PtC + γŨe (st) + αE {\nmin a∈A\nQ∗ ( [Pt+1, st+1, gt+1] T , a )} ,\nwhere the expectation is over Pt+1, st+1 and gt+1. Specifically, Pt+1 is drawn according to the transition probability of the price Markov chain, and [st+1, gt+1]\nT is drawn according to the “device portion” regeneration distribution π0.\nOn the other hand, we have\nQ∗ (xt, off) = αE\n[ 0∑\ns′=−W gmax∑ g=1 pst,s′,g min a∈A Q∗ ( [Pt+1, s ′, g] T , a ) + (1− pst) min a∈A Q∗ ( [Pt+1, s̃(t+ 1), 0] T , a )] ,\nwhere the expectation is over Pt+1. Specifically, pst = ∑0 s′=−W ∑gmax g=1 pst,s′,g is the probability that a consumer request will be received in the next time step, s̃(t+ 1) = st + 1 if st < Ŵ and s̃(t+ 1) = Ŵ if st = Ŵ , and Pt+1 is drawn according to the transition probability of the price Markov chain.\n• If the smart device has already received a consumer request in the current episode, we have Q∗ (xt, on) = PtC + γŨr (st, gt) + αE {\nmin a∈A\nQ∗ ( [Pt+1, st+1, gt+1] T , a )} ,\nwhere the expectation is over Pt+1, st+1 and gt+1. Similarly, Pt+1 is drawn according to the transition probability of the price Markov chain, and [st+1, gt+1]\nT is drawn according to the “device portion” regeneration distribution π0.\nOn the other hand, we have Q∗ (xt, off) = αp̃st,gtE [ min a∈A Q∗ ( [Pt+1, st + 1, gt] T , a )] + (1− p̃st,gt) [ γŨc (st, gt)\n+ αE {\nmin a∈A\nQ∗ ( [Pt+1, st+1, gt+1] T , a )}] .\nNote that p̃st,gt = 0 if st = W . The expectation in the first line is over Pt+1, where Pt+1 is drawn according to the transition probability of the price Markov chain. On the other hand, the expectation in the third line is over Pt+1, st+1 and gt+1, where Pt+1 is also drawn according to the transition probability of the price Markov chain, and [st+1, gt+1]\nT is drawn according to the “device portion” regeneration distribution π0.\nFrom the classical DP theory, the optimal Q-function Q∗ is the unique solution of the above-derived Bellman equation. Furthermore, many DP algorithms, such as value iteration and policy iteration, can be used to compute Q∗. We observe that for many DP algorithms, computing Q∗ is tractable since the cardinalities of the state space S and action space A in a device based MDP are usually small. Once Q∗ is available, one optimal policy µ∗ is\nµ∗(xt) ∈ argmin a∈A Q∗(xt, a)."
    }, {
      "heading" : "Appendix B Proof for Theorem 1",
      "text" : "Proof for Theorem 1: Note that ∀µ : S ×A → [0, 1], Vµ can be expressed as\nVµ = Aµ + γBµ,\nwhere Aµ is the expected infinite-horizon discounted electricity bill, and Bµ is the expected infinitehorizon discounted user dissatisfaction on rescheduling. Furthermore, neither Aµ nor Bµ depends on γ.\nFirst, we prove that Vµbase does not depend on γ. Note that under policy µbase, the EMS will never initiate a job; furthermore, any job initiated by the user is either completed at its target time or canceled by the user before its target time. Thus, under Assumption 5, we have Bµbase = 0. So we have Vµbase = Aµbase , which does not depend on γ.\nSecond, we prove that there exists a γ∗ > 0 s.t. µbase is an optimal policy when γ > γ ∗. Note that under Assumption 5, µbase, which is a deterministic policy, minimizes Bµ, since Bµ ≥ 0 for any µ and Bµbase = 0. Note that there are finite deterministic policies, and we use ∆B to denote the “second best” Bµ under deterministic policies\n∆B = min deterministic µ:Bµ>0 Bµ.\nNote that the maximum cost reduction from Aµ is 1 1−α (Pmax − Pmin)C, where Pmax = maxP∈P P is the highest energy price and Pmin = minP∈P P is the lowest energy price. Thus, if we choose\nγ∗ = (Pmax − Pmin)C\n(1− α)∆B ,\nthen, ∀γ > γ∗, µbase outperforms all the deterministic policies (i.e. Vµbase ≤ Vµ for any deterministic µ). Consequently, it is an optimal policy. Thus DRP = 0 for any γ > γ∗. As a result, we have DRP→ 0 as γ →∞.\nThird, we prove that DRP is a non-increasing function of γ. To formalize the result, we use Vµ(γ) to denote the performance of policy µ at tradeoff parameter γ. Recall Vµ(γ) = Aµ + γBµ, and Bµ ≥ 0 under Assumption 5, thus Vµ(γ) is a non-decreasing function of γ for any µ. Furthermore, we use µ∗(γ) to denote an optimal policy in the problem instance with tradeoff parameter γ. Thus, for any 0 ≤ γ1 ≤ γ2, we have that\nVµ∗(γ1)(γ1) ≤ Vµ∗(γ2)(γ1),\nsince µ∗(γ1) is an optimal policy in the problem instance with parameter γ1, and\nVµ∗(γ2)(γ1) ≤ Vµ∗(γ2)(γ2),\nsince µ∗(γ2) is a fixed policy and γ1 ≤ γ2. Thus, Vµ∗(γ1)(γ1) ≤ Vµ∗(γ2)(γ2), that is, Vµ∗ is a nondecreasing function of γ. Since DRP = Vµbase − Vµ∗ , and Vµbase does not depend on γ, and Vµ∗ is non-decreasing in γ, thus, DRP is non-increasing in γ. From the definition of RDRP, it is also non-increasing in γ.\nFinally, we prove that RDRP = 1 when γ = 0. Note when γ = 0, the EMS does not care about the user’s dis-satisfaction on job rescheduling. Consider a policy µ′ under which\n• The EMS will never initiate a job.\n• The EMS ignores all the jobs initiated by the user; it just waits for the user to cancel the requested job.\nObviously, when γ = 0, we have Vµ′ = 0. Thus we have Vµ∗ ≤ Vµ′ = 0. On the other hand, from Assumption 5, we have Vµ∗ ≥ 0 (since the energy price is always strictly positive and and the user’s dissatisfaction function is always non-negative). Thus we have Vµ∗ = 0 and\nRDRP = Vµbase − Vµ∗\nVµbase = 1.\nQ.E.D."
    }, {
      "heading" : "Appendix C Possible Extensions",
      "text" : "We discuss some possible extensions of the device based MDP model proposed in the paper. It is worth pointing out that many assumptions on this model are proposed to simplify the exposition. We can easily relax some of these assumptions to obtain a more general device based MDP model. We now discuss two relaxations that are of particular interest: stacking request and dependent devices.\nStacking request\nUnder the current device based MDP model, the smart device will ignore a new user request if it currently has an uncompleted request. If the user wants to replace the existent uncompleted request with a new request, they must cancel the existent request first, and then send the new request to the device.\nHowever, in some cases, the user might want to start a new request while keeping the existent uncompleted request. For instance, a user sent a request with target time 4PM to the AC at 1PM; at 2PM, this request was still uncompleted, and the user wanted to send a new request with target time 5PM to the AC while keeping the old request. Starting a new request without canceling the old request is referred to as initiating a stacking request.\nWe can easily extend our proposed device based MDP model to take care of the stacking request by allowing τg, a request target time, to be a vector. Specifically, in the extended MDP model, the user can request multiple jobs in a request J = (n, τr, τg, g), where τg is the target time vector and its components denote when the user prefers each of their requested jobs to be completed. Requested device n, request time τr and priority g are defined the same as in Section II. With the new definition of τg, a stacking request can be realized as follows: assume the target time vector of the current user request is τg, and the user wants to stack new jobs with target time vector τ ′ g on it, then they can cancel the existent request first, and then start a new request with target time vector [τg, τ ′ g] T .\nObviously, allowing τg to be a vector will make the device based MDP model more complicated; however, it does not fundamentally change the problem and all the existent results still apply after proper modifications. For computational tractability, one should also upper bound the length of the target time vector.\nDependent devices\nIn Assumption 3, we assume that consumer requests/cancellations to different devices are statistically independent. As we have discussed after Assumption 3, it should be relaxed for some devices: specifically, devices can be classified into categories, and requests to devices in the same category (such as laundry machine and dryer) are statistically dependent.\nOne way to overcome this problem is to combine all the devices in the same category as a “super device”. After such combinations, the requests/cancellations to different “super devices” can be assumed to be statistically independent. Obviously, allowing “super devices” will make the device based MDP model more complicated; however, it also does not fundamentally change the problem and all the existent results still apply after proper modifications."
    } ],
    "references" : [ {
      "title" : "A survey of utility experience with real time pricing",
      "author" : [ "G. Barbose", "C. Goldman", "B. Neenan" ],
      "venue" : "Lawrence Berkeley National Laboratory: Lawrence Berkeley National Laboratory,",
      "citeRegEx" : "Barbose et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Barbose et al\\.",
      "year" : 2004
    }, {
      "title" : "Dynamic Programming and Optimal Control",
      "author" : [ "D. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2005
    }, {
      "title" : "Dynamic pricing, advanced metering, and demand response in electricity markets",
      "author" : [ "S. Borenstein", "M. Jaske", "A. Rosenfeld" ],
      "venue" : "UC Berkeley: Center for the Study of Energy Markets,",
      "citeRegEx" : "Borenstein et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Borenstein et al\\.",
      "year" : 2002
    }, {
      "title" : "The role of demand response in electric power market design",
      "author" : [ "S. Braithwait", "K. Eakin" ],
      "venue" : "Edison Electric Institute,",
      "citeRegEx" : "Braithwait and Eakin.,? \\Q2002\\E",
      "shortCiteRegEx" : "Braithwait and Eakin.",
      "year" : 2002
    }, {
      "title" : "Autonomous helicopter flight using reinforcement learning",
      "author" : [ "Adam Coates", "Pieter Abbeel", "Andrew Y. Ng" ],
      "venue" : "In Encyclopedia of Machine Learning,",
      "citeRegEx" : "Coates et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2010
    }, {
      "title" : "Quantifying customer response to dynamic pricing",
      "author" : [ "Ahmad Faruqui", "Stephen George" ],
      "venue" : "The Electricity Journal,",
      "citeRegEx" : "Faruqui and George.,? \\Q2005\\E",
      "shortCiteRegEx" : "Faruqui and George.",
      "year" : 2005
    }, {
      "title" : "Residential implementation of critical-peak pricing of electricity",
      "author" : [ "Karen Herter" ],
      "venue" : "Energy Policy,",
      "citeRegEx" : "Herter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Herter.",
      "year" : 2007
    }, {
      "title" : "An exploratory analysis of California residential customer response to critical peak pricing of electricity",
      "author" : [ "Karen Herter" ],
      "venue" : "January 2007b. URL http://www.sciencedirect. com/science/article/B6V2S-4JG5F91-2/2/bb70d54%6082f9f5483829aabeef5279e",
      "citeRegEx" : "Herter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Herter.",
      "year" : 2007
    }, {
      "title" : "Architecture concepts and technical issues for an open, interoperable automated demand response infrastructure",
      "author" : [ "E. Koch", "M.A. Piette" ],
      "venue" : "In Grid Interop Forum,",
      "citeRegEx" : "Koch and Piette.,? \\Q2007\\E",
      "shortCiteRegEx" : "Koch and Piette.",
      "year" : 2007
    }, {
      "title" : "An integrated architecture for demand response communications and control",
      "author" : [ "M. LeMay", "R. Nelli", "G. Gross", "C.A. Gunter" ],
      "venue" : "in Proc. of the 41st Hawaii International Conference on System Sciences,",
      "citeRegEx" : "LeMay et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "LeMay et al\\.",
      "year" : 2008
    }, {
      "title" : "Residential demand response using reinforcement learning",
      "author" : [ "D. ONeill", "M. Levorato", "A.J. Goldsmith", "U. Mitra" ],
      "venue" : "In IEEE SmartGridComm,",
      "citeRegEx" : "ONeill et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "ONeill et al\\.",
      "year" : 2010
    }, {
      "title" : "Development and evaluation of fully automated demand response in large facilities",
      "author" : [ "M.A. Piette", "O. Sezgen", "D.Watson", "N. Motegi", "C. Shockman", "L. ten Hope" ],
      "venue" : "URL http://escholarship.org/uc/item/4r45b9zt",
      "citeRegEx" : "Piette et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Piette et al\\.",
      "year" : 2005
    }, {
      "title" : "Automated critical peak pricing field tests: 2006 pilot program description and results",
      "author" : [ "M.A. Piette", "D.Watson", "N. Motegi", "S. Kiliccote" ],
      "venue" : "In LBNL Report 62218,",
      "citeRegEx" : "Piette et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Piette et al\\.",
      "year" : 2007
    }, {
      "title" : "Industrial power demand response analysis for one-part real-time pricing",
      "author" : [ "J.G. Roos", "I.E. Lane" ],
      "venue" : "Power Systems, IEEE Transactions on,",
      "citeRegEx" : "Roos and Lane.,? \\Q1998\\E",
      "shortCiteRegEx" : "Roos and Lane.",
      "year" : 1998
    }, {
      "title" : "Learning from Delayed Rewards",
      "author" : [ "C. Watkins" ],
      "venue" : "PhD thesis, University of Cambridge,",
      "citeRegEx" : "Watkins.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins.",
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Further many of these decisions have limited financial impact on the consumer [ONeill et al., 2010], and, as a result, many rational consumers in the residential and small commercial building sectors may not be sufficiently incentivized to make these decisions over the long run (known as “decision fatigue” in [ONeill et al.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : ", 2010], and, as a result, many rational consumers in the residential and small commercial building sectors may not be sufficiently incentivized to make these decisions over the long run (known as “decision fatigue” in [ONeill et al., 2010]).",
      "startOffset" : 219,
      "endOffset" : 240
    }, {
      "referenceID" : 10,
      "context" : "Recently, [ONeill et al., 2010] proposed a fully-automated EMS algorithm based on reinforcement learning (RL), which learns how to make optimal decisions for consumers.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "In [ONeill et al., 2010], the authors assume these functions have particular mathematical properties, but do not address how these functions might be determined.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "In this paper, we propose a novel EMS formulation that addresses the limitations of [ONeill et al., 2010] described above.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "In other words, our new RL formulation has eliminated the impractical assumption in [ONeill et al., 2010] that consumers’ dissatisfactions with delay can be captured by known disutility functions.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "• Consumer requests/reservations can have different priorities, whereas in [ONeill et al., 2010], all the consumer requests have the same priority.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "[ONeill et al., 2010]) shows that Pt can be modeled as a Markov process.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "Specifically, many DP algorithms, such as value iteration and policy iteration, can be used to compute Q∗ (see [Bertsekas, 2005]).",
      "startOffset" : 111,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "RL tries to strike a balance between learning (exploration) and optimization (exploitation), and has been extensively used in many other fields, such as artificial intelligence and robotics [Coates et al., 2010].",
      "startOffset" : 190,
      "endOffset" : 211
    }, {
      "referenceID" : 10,
      "context" : "As has been discussed in [ONeill et al., 2010], it is natural to use RL to solve EMS’s learning problem since EMS needs to learn to make decisions while interacting with the user and the real-time energy price.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "In this subsection, we implement one of the most popular and classical RL algorithms, known as Q-learning [Watkins, 1989].",
      "startOffset" : 106,
      "endOffset" : 121
    } ],
    "year" : 2017,
    "abstractText" : "Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS’s rescheduling problem as a reinforcement learning (RL) problem (referred to as the device based RL problem), and show that this RL problem decomposes over devices under reasonable assumptions. Compared with existent formulations, our new formulation (1) does not require explicitly modeling the user’s dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also propose several new performance metrics for RL algorithms applied to the device based RL problem, and demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.",
    "creator" : "LaTeX with hyperref package"
  }
}