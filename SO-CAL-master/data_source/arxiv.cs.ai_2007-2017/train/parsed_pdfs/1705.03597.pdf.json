{
  "name" : "1705.03597.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Solving Multi-Objective MDP with Lexicographic Preference: An application to stochastic planning with multiple quantile objective",
    "authors" : [ "Yan Li", "Zhaohan Sun" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n03 59\n7v 1\nIn most common settings of Markov Decision Process (MDP), an agent evaluate a policy based on expectation of (discounted) sum of rewards. However in many applications this criterion might not be suitable from two perspective: first, in risk aversion situation expectation of accumulated rewards is not robust enough, this is the case when distribution of accumulated reward is heavily skewed; another issue is that many applications naturally take several objective into consideration when evaluating a policy, for instance in autonomous driving an agent needs to balance speed and safety when choosing appropriate decision. In this paper, we consider evaluating a policy based on a sequence of quantiles it induces on a set of target states, our idea is to reformulate the original problem into a multi-objective MDP problem with lexicographic preference naturally defined. For computation of finding an optimal policy, we proposed an algorithm FLMDP that could solve general multi-objective MDP with lexicographic reward preference."
    }, {
      "heading" : "1 Introduction",
      "text" : "The most classical MDP problem consider maximizing a scalar reward’s expectation [3], however in many situation a single scalar objective is not enough to represent an agent’s objective. For example, in self-autonomous driving one need to balance speed and safety [2]. A common approach is to use a weight vector and scalarization function to project the multi-objective function to single objective problem. However in practice it is hard to evaluate and analyze the projected problem since there might be many viable Pareto optimal solutions to the original problem [1]. On the other hand,\nin some cases, an agent might have explicit preference over the objectives, that is, an agent might expect to optimize the higher priority objective over the lower priority ones when finding optimal policy. For example, in autonomous driving an agent would consider safety the highest priority, placing speed in the second place.\nSeveral previous studies have considered such multi-objective problem with lexicographical order. Using a technique called Ordinal dynamic programming, Mitten [5] assumed a specific preference ordering over outcomes for a finite horizon MDP; Sobel [7] extended this model to infinite horizon MDPs. Ordinal dynamic programming has been explored under reinforcement learning. Wray et.al [2] also consider a more general setting when lexicographical order depends on initial state and slack for higher objective value is allowed for improvement over lower priority objective. In their paper they proposed an algorithm called LVI that tries to approximate optimal policy in infinite horizon setting, although work empirically well, the algorithm lacks theoretical guarantee, in fact, the performance could be arbitrarily worse if the MDP is adversarially designed.\nEven in the setting that an agent indeed has only one reward, the expectation of accumulated reward is not always suitable. This is the case when the agent is risk aversion, for instance in financial market an institutional fund would like to design an auto-trading system that maximize certain lower quantile. The essential idea of such strategy is to improve the worst case situation as much as possible. Based on this motivation, Hugo and Weng [8] proposed quantile based reinforcement learning algorithm which seeks to optimize certain lower/upper quantile on the random outcome. In their paper they define a set of end states in finite horizon setting, let Pπ(·) be the probability distribution induced by policy π on the end states, they seek to find the optimal policy in the sense that the τ -lower quantile of Pπ(·) is maximized. Note that their objective could be improved by following observation:\n1. Among all the policy that achieve the optimal τ -lower quantile, a refined class of policy could be chosen in the sense that following such policy, the probability of ending at a state the is less preferable than the optimal quantile state is minimized.\n2. Suppose τ1 < τ2, then after finding policy class that maximize τ1-quantile, one can further find policy that maximize τ2-quantile in this policy class. For situation when multiple τi-quantile are to be optimized, we can find optimal policy by repeating the same procedure iteratively.\nIn general, if τ1 < τ2 < . . . < τL are in consideration, we have a multi-quantileobjective MDP, in this paper, we showed a proper way to transfer this problem into a pure multi-objective MDP with lexicographic preference. To tackle computation of an optimal policy, we will introduce an algorithm called FLMDP that not only solve our multi-quantile-objective MDP, but also generalize multi-objective MDP with finite states, action, and horizon. Generalization to infinite states or actions to find ǫ-optimal policy could be done fairly easy with small modification in our algorithm."
    }, {
      "heading" : "2 Problem Definition",
      "text" : "We consider finite horizon problem here, a multi-objective Markov Decision Process is described by a tuple (S,A, P,R) where:\n• S is finite state space.\n• A is finite action space.\n• G is finite end state space.\n• T is finite horizon.\n• P is transition function given by: P (s, a, s′) = P(s′|s, a), i.e., the probability of transiting from s to s′ after performing action a.\n• R = [R1, R2, . . . , Rk] is reward vector, with each component Ri(s, a, s ′) defining\nreward of starting from state s, performing action a and transit to state s′.\nWithout loss of generality we may assume G = {g1, . . . , gn}. On G we may define our preference as g1 6 g2 6 . . . 6 gn where gi 6 gj denotes gj is preferred over gi. To enforce end state nature of set G, we further define transition probability and reward function have following properties:\nP (g, a, g) = 1, ∀g ∈ G, ∀a ∈ A\nR(g, a, g) = 0, ∀g ∈ G, ∀a ∈ A\nThat is, whenever the current state is in set G, we remains at state g until process ends at horizon T, in the meantime receiving no rewards at all. To enforce the process ends at one of the end state, we define a special end state g0 = t = T and declare g0 6 gi, ∀i > 1. Let π be any policy, we define the probability distribution P\nπ(·) induced by π induced on set G as Pπ(gi) = P\nπ(sT = gi). Then we can further define cumulative distribution function.\nF π(g) = ∑\ngi6g\nP π(gi)\nThe associated τ -lower quantile is given by:\nqπ τ = min{gi : F π(gi) > τ}"
    }, {
      "heading" : "Finding Optimal Policy",
      "text" : "Given τ1 < . . . < τL ∈ [0, 1], following our motivation in Introduction section, our procedure to find optimal policy is a series of optimization procedure, we will show later this could be reshaped into multi-objective MDP with lexicographic preference.\nAlgorithmic Scheme 1\n1. Denote Π0 = {all possible policy}.\n2. After finding Πi−1, construct Πi:\nq⋆ τi = max π∈Πi−1 {qπ τi }. Π̂i = {π ∈ Πi−1 : q π\nτi = q⋆ τi }\nThat is, Π̂i is the set of policy that maximize the τi quantile in Πi. Let pi be the ”biggest” state that is ”smaller” than q⋆\nτi . Here ”biggest” and ”smaller” should\nbe interpreted in terms of preference. Then to minimize the probability of ending at a state that is less preferable than q⋆\nτi , we should have Πi as follows:\nΠi = argmin π∈Π̂i\nF π(pi) (1)\n3. Proceed as step 2 until we have found ΠL. Then any policy π that is in ΠL will be our optimal policy.\nNote however it is unclear how to translate such algorithmic scheme into a tractable algorithm, the problem is that we do not know how to properly ”choose” an policy from a policy class. We’ll tackle this issue in the next section."
    }, {
      "heading" : "3 Multi-Quantile-Objective MDP",
      "text" : "In this section we first present a lemma that generalize the Lemma 1 of Hugo and Weng [8], this lemma fully characterize the q⋆\nτi\nLemma 1. For i = 1, . . . , L, let q⋆ τi and Πi be defined as before, then q ⋆ τi satisfies the following condition:\nq⋆ τi = min{g : F ⋆i (g) > τi}\nF ⋆i (g) = min π∈Πi−1 F π(g), ∀g ∈ G\nProof. We proof by induction: For i=1: observe that\nF ⋆1 (g) 6 F π(g), ∀π, ∀g\nThis follows directly from the definition of F ⋆1 (g). Hence the τ1-quantile of F ⋆ 1 (g)(denoted as gi1) is greater or equal than q π τ1 for all π. Now by the definition of gi1 , we have F ⋆ 1 (gi1) > τ1 and F ⋆ 1 (gi1−1) < τ1. Then by definition of F ⋆1 (), we have ∃π1, s.t.:\nF π1(gi1−1) = F ⋆ 1 (gi1−1) < τ1\nF π1(gi1) > F ⋆ 1 (gi1) > τ1\nThis means that gi1 is τ1-quantile of both F ⋆ 1 () and F π1(). Hence we have gi1 > q π τ1 , ∀π, and gi1 = q π1 τ1 . Thus q⋆ τ1 = gi1 by definition of q ⋆ τ1 . Assume the claim holds for i < k: For i = k: observe that\nF ⋆k (g) 6 F π(g), ∀π ∈ Πk−1, ∀g\nHence the τk-quantile of F ⋆ k (g)(denoted as gik) is greater or equal than q π τk for all π ∈ Πk−1.\nNow by the definition of gik , we have F ⋆ k (gik) > τk and F ⋆ k (gik−1) < τk. Then by definition of F ⋆k (), we have ∃πk ∈ Πk−1, s.t.:\nF πk(gik−1) = F ⋆ k (gik−1) < τk\nF πk(gik) > F ⋆ k (gik) > τk\nThis means that gik is τk-quantile of both F ⋆ k () and F πk(). Hence we have gik > q π τk , ∀π, and gik = q πk τk . Thus q⋆ τk = gik by definition of q ⋆ τk . By induction, proof complete.\nFollowing the proof of Lemma 1, we could construct Πi as follows:\nAlgorithmic Scheme 2\n1. Let Π0={all possible policy}.\n2. Suppose Πi−1 has been constructed, then we construct Πi as following:\nq⋆ τi = max π∈Πi−1 {qπ τi }.\nLet pi the same as before, i.e. pi be the ”biggest state” that is ”smaller” than q⋆ τi . Then we construct Πi as follows:\nΠi = argmin π∈Πi−1\nF π(pi) (2)\nNote that in equation (2) we construct Πi here directly from Πi−1 instead of from Π̂i in equation (1), the reason here is that by proof of Lemma 1, the policy π that minimize F π(pi) also has q\n⋆ τi as its τi-quantile.\nSolving the Algorithmic Scheme mentioned before is hard in general, but giving our work before we are now ready to formulate the previous Algorithmic Scheme into a MDP with Lexicographical objective preference. We may now restrict ourself in the setting that q⋆\nτ1 , q⋆ τ2 · · · q⋆ τL are known beforehand, and consider the more general case\nlater.\nTo do this, we define reward functions {Ri} L i=1 as follows:\nRi(st, at, st+1) =\n{\n1 if st 6∈ G and st+1 = gi, gi > q ⋆ τi 0 otherwise (3)\nThen it is easy to verify that Eπ[ ∑T t=0 Ri(st, at, st+1)] = 1−F π(pi). Hence minimizing F π(pi) is equivalent to maximizing expected reward of the MDP. Define V πi = E π[ ∑T\nt=1 Ri(st, at, st+1)] the expected total reward corresponding to reward function Ri, then equation (2) becomes as:\nΠi = argmax π∈Πi−1\nV πi (4)\nWe will show in the next subsection, if {q⋆ τi }Li=1 are known, the procedure described in Algorithmic Scheme 2 exactly corresponds to the procedure of solving a multi-objective MDP with lexicographic preference."
    }, {
      "heading" : "Multi-Objective MDP with Lexicographic Preference",
      "text" : "Definition 1. Recall that a point ū is lexicographical larger than 0 if ui = 0 for i =1,2 · · · j and uj > 0 for some 1 6 j 6 n, we write u = (u1, u2 · · · un) >l 0. We then define our lexicographical order index as j, which is the first index in the vector that strictly larger than zero. Thus say ū is lexicographical larger than v̄ if ū− v̄ >l 0\nA multi-objective MDP differs from standard MDP that it has reward vector R(s, a) = [R1(s, a), . . . , RL(s, a)] and associated value vector V(s) = [V1(s), . . . , VL(s)], and a preference is defined on the value function associated with different rewards, say V1(s) > V2(s) > . . . > VL(s). Classic multi-objective MDP seeks to find a policy that has Pareto optimal value vector. With lexicographic preference defined on value vectors, we say a policy π⋆ is lexicographic optimal if there is no policy π so that V π(s) >l V π⋆(s).\nIn pure algorithmic scheme, an multi-objective MDP is solved by iteratively finding the optimal policy class for lower priority value function in the optimal policy class for higher priority ones. That is, denote Π0={any policy}, Πi+1 is found by:\nΠi+1 = argmax π∈Πi\nV πi+1(s)\nIn our multi-quantile-objective MDP, (S,A, P,R) is defined as the same as in section 2, the reward functions Ri is defined as in equation (3). With value vector V\nπ = (V π1 , . . . , V π L ), we define lexicographical preference on V\nπ as defined in definition 1. Then with equation (4) replacing equation (2) in Algorithmic 2, it is easy to see that Algorithmic Scheme 2 now become a procedure of solving multi-objective lexicographic MDP with parameters (S,A, P,R,V)."
    }, {
      "heading" : "4 Solving Multi-Quantile-Objective MDP",
      "text" : "Solving Multi-Quantile-Objective MDP lies in general situation of solving multi-objective MDP with lexicographical preference. A natural one is to shape the original problem to a sequence of constrained case MDP and solve this sequence of constrained MDP iteratively. In the next subsection we proposed an algorithm that can solve general multi-objective MDP with lexicographic preference directly, thus solving multiquantile-objective MDP here is just a special case."
    }, {
      "heading" : "Constrained MDP formulation",
      "text" : "The following procedure reshape a multi-objective MDP with lexicographic preference to a sequence of constrained MDP problem.\n1. At step 1, Π0={all possible policy}. Optimize objective V π 1 , V ⋆ 1 = maxπ∈Π0 V π 1 . 2. At step i, Optimize objective V πi with constraints:\nminimize π\nV πi\nsubject to V πj > V ⋆ j , j = 1, . . . , i− 1.\n3. Proceed as in 2 until step L is finished.\nIt is easy to see that at step i the constraints in the optimization procedure naturally restrict the algorithm to search policy in the class that is identical to Πi−1, thus correctness of this reshape is guaranteed. Altman [6] has shown that an optimal randomized policy could be found in such constrained MDP, Chen and Feinberg [4] also showed how to find optimal deterministic policy. Note this type of algorithm indeed does unnecessary work by restarting from searching whole policy space in every step. In this next subsection, we design a dynamic programming flavor algorithm that finds an optimal deterministic policy for general lexicographic order MDP."
    }, {
      "heading" : "Lexicographic Markov Decision Process",
      "text" : "In this subsection we introduce an algorithm FLMDP that solves general lexicographic MDP in finite horizon, in particular it can be used to solve our previous formulated multi-quantile-objective MDP.\nLet V πi,t: L×S×T → R be the expected reward obtained by using policy π in decision epochs t, t+1, · · · T, here, for simplicity, we let reward of end state equals zero, thus V πi,t can be represented as\nV πi,t(s) = E π st=s[\nT ∑\nn=t\nRi(sn, an)]\nNote that although in our problem Ri() relates to out next state, we can solve this problem by simply define Ri(st, at) = E[Ri(st, at, st+1)] with expectation taken w.r.t st+1.\nWe first define state value function:\nQπi,t(s, a) = Ri(s, a) + ∑\ns′∈S\nPr(s′|s, a)V πi,t+1(s ′)\nThen following the definition in constrained MDP, ∀t = T, T−1 · · · 1, and ∀i = 1, 2 · · ·L, we define restricted bellman equation operator Bti as\nBtiV π i,t(s) = max\na∈Ati−1\n{Ri(s, a) + ∑\ns′∈S\nPr(s′|s, a)V πi,t+1(s ′)}\nwhere Ati+1(s) = {a ∈ A t i(s)| max\na′∈At i (s)\nQπi,t(s, a ′) = Qπi,t(s, a)}\nand At0(s) = A(s)\nAlgorithm 1 Finite-horizon Lexicographic MDP - FLMDP\nInput Ri(s, a), i = 1, 2 · · ·L Set V πi,T (s) = 0, ∀i = 1, 2 · · ·L, ∀s ∈ S for t = T − 1, T − 2 · · ·1 do for i = 1, 2 · · ·L do V πi,t(s) = B t iV π i,t(s)\nend for π⋆t ∈ A t L\nend for Output π⋆1 , π ⋆ 2, · · · , π ⋆ T\nTheorem 1. In our algorithm 1, ∀t = T −1, T −2, · · · 1, {π⋆t }t6T−1 are optimal policy for our Lexicographic MDP problem.\nProof. Before beginning our proof, we need some notations. Recall:\nV πi,t(s) = E π st=s[\nT ∑\nt\nRi(st, at)]\nV ⋆i,t(s) = E π⋆ st=s[\nT ∑\nt\nRi(st, at)]\nV π t (s) = [V π 1,t(s), . . . , V π L,t(s)] V ⋆ t (s) = [V ⋆ 1,t(s), . . . , V ⋆ L,t(s)]\nwhere {π⋆t } denotes the policy output by Algorithm 1. Then V π j,t(s) defines the value function associated with reward Ri, starting a tail problem with initial state s at time t following given policy π. Note that Vπt (s) is exactly the value vector for full horizon MDP with initial state s. By our specification of reward function Ri, we naturally have V\nπ T (s) = 0 and V ⋆ T (s) = 0.\nLet 6l, <l, >l,>l denotes lexicographical order relationship on value vector V π t (s). We use backward induction to show that for ∀π, and for ∀ t = 1, . . . , T −1, for ∀ s, we have V\nπ t (s) 6l V ⋆ t (s).\nFor t = T − 1, VπT−1(s) 6l V ⋆ T−1(s) is trivial by procedure of our algorithm. A simple induction on i suffice to give a formal proof, we omit the details here.\nSuppose the claim holds for t+ 1, . . . , T − 1, now we proceed to prove the claim holds for t: assume Vπt+1(s) <l V ⋆ t+1(s)\nV πi,t+1(s) = V ⋆ i,t+1(s), i = 1, . . . , it+1 − 1\nV πit+1,t+1(s) <l V ⋆ it+1,t+1(s)\nWe next show that Vπt (s) <l V ⋆ t (s) also holds:\n1. if V π1,t(s) < V ⋆ 1,t(s), then we are done. 2. if V π1,t(s) = V ⋆ 1,t(s), construction of π\n⋆ and value iteration for finite horizon MDP gives us:\nV π1,t(s) = R1(s, π(s)) + ∑\nj\nP (s, π(s), j)V π1,t+1(j)\nV ⋆1,t(s) = max a\nR1(s, a) + ∑\nj\nP (s, a, j)V ⋆1,t+1(j)\nBy induction hypothesis we have V π1,t+1(j) = V ⋆ 1,t+1(j), then we must have V π 1,t(s) 6 V ⋆1,t(s). Now since we have equality achieved, by our definition of A1(s) in our algorithm, we must have π(s) ∈ A1(s).\n3. We now use induction to show that for if i < it+1 − 1, and\nV πj,t(s) = V ⋆ j,t(s), j = 1, . . . , i\nthen we must have π(s) ∈ Ai(s) and V π i+1,t(s) ≤ V ⋆ i+1,t(s). The base case i=1 have been proved in step 2. Suppose the claim holds for i− 1, then for i: By induction hypothesis we have π(s) ∈ Ai−1(s). Construction of π\n⋆ and value iteration for finite horizon MDP gives us:\nV πi,t(s) = Ri(s, π(s)) + ∑\nj\nP (s, π(s), j)V πi,t+1(j) (5)\nV ⋆i,t(s) = max a∈Ai−1(s)\nRi(s, a) + ∑\nj\nP (s, a, j)V ⋆i,t+1(j) (6)\nBy induction hypothesis we have V πi,t+1(j) = V ⋆ i,t+1(j), then we must have V π i,t(s) 6 V ⋆i,t(s). Now since we have equality achieved, by our definition of Ai(s) in our algorithm, we must have π(s) ∈ Ai(s). Then replacing i equation (5) with i + 1 we have:\nV πi+1,t(s) = Ri+1(s, π(s)) + ∑\nj\nP (s, π(s), j)V πi+1,t+1(j)\nV ⋆i+1,t(s) = max a∈Ai(s)\nRi+1(s, a) + ∑\nj\nP (s, a, j)V πi+1,t+1(j)\nBy induction hypothesis we have V πi+1,t+1(j) = V ⋆ i+1,t+1(j), noticing that now π(s) ∈ Ai(s), then we have:\nV πi+1,t(s) 6 V ⋆ i+1,t(s)\nFinally, when i = it+1 − 1, if\nV πj,t(s) = V ⋆ j,t(s), j = 1, . . . , i\nThen following the argument as before, and utilizing that now V πi+1,t+1(j) < V ⋆i+1,t+1(j), we must have:\nV πi+1,t(s) < V ⋆ i+1,t(s)\nwhich gives us Vπt (s) <l V ⋆ t (s)\nNotice that our previous argument could also be used to prove Vπt+1(s) = V ⋆ t+1(s) ⇒ V π t (s) 6l V ⋆ t (s). Then combining all the ingredients we have, the following statement holds: V\nπ t+1(s) 6l V ⋆ t+1(s) ⇒ V π t (s) 6l V ⋆ t (s) (7)\nTo conclude our proof, notice we have VπT−1(s) 6l V ⋆ T−1(s), apply equation (7) iteratively, we have Vπ1 (s) 6l V ⋆ 1(s), the optimality of our output policy follows immediately.\nNow we return to the general case where optimal quantiles {q⋆ τi }Li=1 is not known before hand. Out idea is to proceed iteratively, at kth iteration, we used bisection to guess the location of the unknown q⋆\nτk , we then solve a lexicographic MDP with k reward\n[R1, . . . , Rk] and preference aligns with our preference for total L reward. Specifically, at k-th iteration, we maintain u and l such that F ⋆k (gl−1) < τk and F ⋆ k (gu−1) > τk. We successively reduce u− l by half until u− l = 1. Then q⋆ τk = gu−1. To proceed the k-th iteration, we need to define our reward function as follows:\nR qτi i (st, at, st+1) =\n{\n1 if st 6∈ G and st+1 = gi, gi > qτi 0 otherwise\nThe reward vector at k-th iteration is then given by:\nR = [R q⋆ τ1 1 , . . . , R q⋆ τk−1 k−1 , R q τk k ]\nwhere q τk is our guess for q⋆ τk .\nAlgorithm 2 Multi-Quantile-Objective(MQO) MDP\nSet V πi,T (s) = 0, ∀i = 1, 2 · · ·L, ∀s ∈ S for i = 1, 2 · · ·L do Guess a proper q\nτi , which should be larger than q⋆ τk , ∀k = i− 1 · · ·1\nSet l be the largest index of {gk} s.t. gk < qτi , Set u ← n repeat\nSolve Lexicographic MDP with q⋆ τ1 , q⋆ τ2 · · · q⋆ τj , j 6 i− 1 Output V πi,t(s) if V πi,t(s) 6 1− τi then l ← i\nelse\nu ← i end if i ← ⌈ l+u 2 ⌉\nuntil u− l = 1 q⋆ τi ← gu−1\nend for"
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper we consider a multi-quantile-objective MDP problem that combines previous work in quantile objective MDP and multi-objective MDP. Our contribution is\ntwo folds, first we formulate the problem into multi-objective MDP problem, the second is that our algorithm to solve this problem could also solve general multi-objective MDP problem with finite horizon, state space and action space. Extension to infinite state space or action space could be also done with slight modification.\nWe note our possible future work here: Pineda et.al [1] has showed that constrained MDP could be reshaped into a sequence of multi-objective MDP with lexicographic preference and additional slack variables, thus if one could solve lexicographic MDP with slack variable efficiently, then solution of constrained MDP follows. For finite horizon, we believe similar dynamic programming flavor algorithm could be invented for solving lexicographic MDP with slack variables, we leave it here as an open problem and our future work."
    } ],
    "references" : [ {
      "title" : "Revisiting Multi-Objective MDPs with relaxed Lexicographic Preferences",
      "author" : [ "Luis Pineda", "Kyle H. Wray", "Shlomo Zilberstein" ],
      "venue" : "AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Multi-Objective MDPs with Conditional Lexicographic Reward Preferences",
      "author" : [ "Kyle H. Wray", "Shlomo Zilberstein", "Abdel-Illah Mouaddib" ],
      "venue" : "In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Preference Order Dynamic Programming",
      "author" : [ "L.G. Mitten" ],
      "venue" : "Management Science. Volume 21,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1974
    }, {
      "title" : "Constrained Markov Decision Process",
      "author" : [ "Eitan Altman" ],
      "venue" : "Chapman and Hall/CRC,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1999
    }, {
      "title" : "Ordinal Dynamic Programming",
      "author" : [ "Matthew J. Sobel" ],
      "venue" : "Management Science",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1975
    }, {
      "title" : "Quantile Reinforcement Learning",
      "author" : [ "Hugo Gilbert", "Paul Weng" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Sutton", "Richard S", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Markov Decision Processes with Ordinal Rewards: Reference Point- Based Preferences",
      "author" : [ "Weng", "Paul" ],
      "venue" : "In ICAPS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "For example, in self-autonomous driving one need to balance speed and safety [2].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "However in practice it is hard to evaluate and analyze the projected problem since there might be many viable Pareto optimal solutions to the original problem [1].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 2,
      "context" : "Using a technique called Ordinal dynamic programming, Mitten [5] assumed a specific preference ordering over outcomes for a finite horizon MDP; Sobel [7] extended this model to infinite horizon MDPs.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "Using a technique called Ordinal dynamic programming, Mitten [5] assumed a specific preference ordering over outcomes for a finite horizon MDP; Sobel [7] extended this model to infinite horizon MDPs.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "al [2] also consider a more general setting when lexicographical order depends on initial state and slack for higher objective value is allowed for improvement over lower priority objective.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "Based on this motivation, Hugo and Weng [8] proposed quantile based reinforcement learning algorithm which seeks to optimize certain lower/upper quantile on the random outcome.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "< τL ∈ [0, 1], following our motivation in Introduction section, our procedure to find optimal policy is a series of optimization procedure, we will show later this could be reshaped into multi-objective MDP with lexicographic preference.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "In this section we first present a lemma that generalize the Lemma 1 of Hugo and Weng [8], this lemma fully characterize the q τi Lemma 1.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "Altman [6] has shown that an optimal randomized policy could be found in such constrained MDP, Chen and Feinberg [4] also showed how to find optimal deterministic policy.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "al [1] has showed that constrained MDP could be reshaped into a sequence of multi-objective MDP with lexicographic preference and additional slack variables, thus if one could solve lexicographic MDP with slack variable efficiently, then solution of constrained MDP follows.",
      "startOffset" : 3,
      "endOffset" : 6
    } ],
    "year" : 2017,
    "abstractText" : "In most common settings of Markov Decision Process (MDP), an agent evaluate a policy based on expectation of (discounted) sum of rewards. However in many applications this criterion might not be suitable from two perspective: first, in risk aversion situation expectation of accumulated rewards is not robust enough, this is the case when distribution of accumulated reward is heavily skewed; another issue is that many applications naturally take several objective into consideration when evaluating a policy, for instance in autonomous driving an agent needs to balance speed and safety when choosing appropriate decision. In this paper, we consider evaluating a policy based on a sequence of quantiles it induces on a set of target states, our idea is to reformulate the original problem into a multi-objective MDP problem with lexicographic preference naturally defined. For computation of finding an optimal policy, we proposed an algorithm FLMDP that could solve general multi-objective MDP with lexicographic reward preference.",
    "creator" : "LaTeX with hyperref package"
  }
}