{
  "name" : "1604.02126.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Stochastic Belief Revision and Update and their Combination",
    "authors" : [ "Gavin Rens" ],
    "emails" : [ "gavinrens@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Information acquired can be due to evolution of the world or revelation about the world. That is, one may notice via some ‘signal’ generated by the changing environment that the environment has changed, or, one may be informed by an independent agent in a static environment that some ‘fact’ holds.\nIn the present work, I deal with belief change of agents who handle uncertainty by maintaining a probability distribution over possible situations. The agents in this framework also have models for nondeterministic events, and noisy observations. Noisy observation models can model imperfect sensory equipment for receiving environmental signals, but they can also model untrustworthy informants in a static world.\nIn this paper, I provide the work of Boutilier (1998) as background, because it has several connections with and was the seed for the present work. However, I do not intend simply to give a probabilistic version of his Generalized Update Semantics. Whereas Boutilier (1998) presents a model for unifying qualitative belief revision and update, I build on his work to present a unified model of belief revision and update in a stochastic (probabilistic) setting. I also take a dynamical systems perspective, like him. Due to my quantitative approach, an agent can maintain a probability distribution\nCopyright c© 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nover the worlds it believes possible, using an expectation semantics of change. This is in contrast to Boutilier’s “generalized update” approach, which takes a most-plausible event and most-plausible world approach. Finally, my proposal requires a trade-off factor to mix the changes in probability distribution over possible worlds brought about due to the probabilistic belief revision process and, respectively, the probabilistic belief update process. Boutilier’s model has revision and update more tightly coupled. For this reason, his approach is better called “unified” while mine is called “hybrid”.\nThe belief change community does not study probabilistic belief update; it is studied almost exclusively in frameworks employing Bayesian conditioning – for modeling events and actions in dynamical domains (e.g., DBNs, MDPs, POMDPs) (Koller and Friedman, 2009; Poole and Mackworth, 2010, e.g.). The part of my approach responsible for updating stays within the Bayesian framework, but combines the essential elements of belief update with unobservable events and belief update as partially observable Markov decision process (POMDP) state estimation.\nOn the other hand, there is plenty of literature on probabilistic belief revision (Voorbraak, 1999; Grove and Halpern, 1998; Kern-Isberner, 2008; Yue and Liu, 2008, e.g.). The subject is both deep and broad. There is no one accepted approach and to argue which is the best is not the focus of this paper. I shall choose one reasonable method for probabilistic belief revision suitable to the task at hand.\nIn the first section, Boutilier’s ‘generalized update’ is reviewed. Then, in the next section, I introduce stochastic update and stochastic revision, culminating in the ‘hybrid stochastic belief change’ (HSBC) approach. The final section presents an example inspired by Boutilier’s article (1998) and analyses the results.\nSome proofs of propositions are omitted to save space; they are available on request."
    }, {
      "heading" : "Boutilier’s Generalized Update",
      "text" : "I use Boutilier’s notation and descriptions, except that I am more comfortable with α and β to represent sentences, instead of A and B. It is assumed that an agent has a deductively closed belief set K, a set of sentences drawn from some logical language reflecting the agent’s beliefs about the current state of the world. For ease of presentation, I assume\nar X\niv :1\n60 4.\n02 12\n6v 1\n[ cs\n.A I]\n7 A\npr 2\n01 6\na logically finite, classical propositional language, denoted L (LCPL in Boutilier (1998)), and consequence operation Cn . The belief set K will often be generated by some finite knowledge base KB (i.e., K = Cn(KB)). The identically true and false propositions are denoted > and ⊥, respectively. Given a set of possible worlds W (or valuations over L) and α ∈ L, the set of α-worlds, that is, the elements of W satisfying α, is denoted by ||α||. The worlds satisfying all sentences in a set K is denoted ||K||."
    }, {
      "heading" : "Update",
      "text" : "Given a belief set K, an agent will often observe a change in the world α, requiring the agent to change K. This is the update of K by α, denoted K α.\n“||KB || represents the set of possibilities we are prepared to accept as the actual state of affairs. Since observation α is the result of some change in the actual world, we ought to consider, for each possibilityw ∈ ||KB ||, the most plausible way (or ways) in which w might have changed in order to make α true. That is, we want to consider the most plausible evolution of world w into a world satisfying the observation α. To capture this intuition, Katsuno and Mendelzon (1991) propose a family of preorders {≤w| w ∈ W}, where each ≤w is a reflexive, transitive relation over W . We interpret each such relation as follows: if u ≤w v then u is at least as plausible a change relative to w as is v; that is, situation w would more readily evolve into u than it would into v.\nFinally, a faithfulness condition is imposed: for every world w, the preorder ≤w has w as a minimum element; that is, w <w v for all v 6= w. Naturally, the most plausible candidate changes in w that result in α are those worlds v satisfying α that are minimal in the relation ≤w. The set of such minimal α-worlds for each relation ≤w, and each w ∈ ||KB ||, intuitively capture the situations we ought to accept as possible when updating KB with α,” (Boutilier, 1998, p. 9). In other words,\n||KB α|| = ⋃\nw∈||KB||\n{Min(α,≤w)},\nwhere Min(α,≤w) specifies the minimal α-worlds with respect to the preorder ≤w. Then K α = Cn(KB α), where K is the belief set associated with KB ."
    }, {
      "heading" : "Revision",
      "text" : "Given a belief set K, an agent will often obtain information α in a static world, which must be incorporated into K. This is the revision of K by α, denoted K∗α.\nThe AGM theory of belief revision (Alchourrón, Gärdenfors, and Makinson, 1985) provides a set of guidelines, in the form of the postulates, governing the process. “Unfortunately, while the postulates constrain possible revisions, they do not dictate the precise beliefs that should be retracted when α is observed. An alternative model of revision, based on the notion of epistemic entrenchment (Gärdenfors, 1988), has a more constructive nature,” (Boutilier, 1998, p. 6).\n“Semantically, an entrenchment relation (hence a revision function) can be modeled using an ordering on possible worlds reflecting their relative plausibility (Grove, 1988;\nBoutilier, 1994). However, rather than use a qualitative ranking relation, we adopt the presentation of (Spohn, 1988; Goldszmidt and Pearl, 1992) and rank all possible worlds using a κ-ranking. Such a ranking κ : W → N assigns to each world a natural number reflecting its plausibility or degree of believability. If κ(w) < κ(v) then w is more plausible than v or more consistent with the agent’s beliefs. We insist that κ−1(0) 6= ∅, so that maximally plausible worlds are assigned rank 0. These maximally plausible worlds are exactly those consistent with the agent’s beliefs; that is, the epistemically possible worlds according to K are those deemed most plausible in κ (see Spohn (1988) for further details). We sometimes assume κ is a partial function, and loosely write κ(w) = ∞ to mean κ(w) is not defined (i.e., w is not in the domain of κ, or w is impossible),” (Boutilier, 1998, p. 6).\nA κ-ranking captures the entrenchment of the agent’s beliefs in its belief set K. This entrenchment determines how K will be revised when the agent receives new information / makes an observation α. κ induces a belief set as follows.\nK = {α ∈ L | κ−1(0) ⊆ ||α||}.\nDue to the ranking or entrenchment of knowledge provided by κ, κ is considered an epistemic state.\n“In other words, the set of most plausible worlds (those such that κ(w) = 0) determine the agent’s beliefs. The ranking κ also induces a revision function: to revise by α an agent adopts the most plausible α-worlds as epistemically possible,” (Boutilier, 1998, p. 6).\nLet Wi = {w ∈ W | κ(w) = i}. And let Min(α, κ) be the setWi with the least i such that for allwi ∈Wi,wi |= α. Then K∗α := {β ∈ L | Min(α, κ) ⊆ ||β||}. In words, the belief set revised by α contains all those sentences entailed by the set of worlds with the same rank, where that rank is the least such that they are all α-worlds."
    }, {
      "heading" : "Generalized Update",
      "text" : "As explained in the introduction, my intention with this paper is not to give a probabilistic version of the Generalized Update approach (Boutilier, 1998). For completeness, however, I sketch the approach here covering the approach in detail would take up unnecessary space without lending much insight into my Hybrid Stochastic Belief Change approach.\nBoutilier motivates the need for a generalized update method which includes revision, by claiming that KM update (Katsuno and Mendelzon, 1991) is insufficient. He provides the following example adopted from Moore (1990). Suppose you want to test whether the contents of a beaker are chemically acid or base. If it is acid, a piece of litmus paper will turn red, if base, the paper will turn blue. Suppose that the test has not yet been performed, but you believe that the contents in the beaker are acidic. When the litmus paper is dipped into and pulled out of the beaker, the paper turns blue, indicating a basic compound. “Unfortunately, the KM theory does not allow this to take place. [...] One is forced to accept that, if the contents were acidic (in which case it should turn red), some extraordinary change occurred (the\ntest failed, the contents of the beaker were switched, etc.). [...] Of course, the right thing to do is simply admit that the beaker did not, in fact, contain an acid—the agent should revise its beliefs about the contents of the beaker,” (Boutilier, 1998, p. 13).\nBoutilier adopts an event-based approach where a set of events E is assumed. These events are allowed to be nondeterministic, and each possible outcome of an event is ranked according to its plausibility via a κ-ranking. “As in the original event-based semantics, we will assume each world has an event ordering associated with it that describes the plausibility of various event occurrences at that world,” (Boutilier, 1998, p. 14).\nA generalized update model is then defined as 〈W,κ,E, µ〉, where W is a set of worlds (the agent’s epistemic state), κ is a ranking overW ,E is a set of events (mappings overW ), and µ is an event ordering (a set of mappings over E).\nAs with KM update, updates usually occur in response to some observation, with the assumption that something occurred to cause this observation. After observing α, an agent should adjust its beliefs by considering that only the most plausible transitions leading to α actually occurred. The set of possible α-transitions are those transitions leading to αworlds. The most plausible α-transitions are those possible α-transitions with the minimal κ-ranking. Given that α has actually been observed, an agent should assume that one of these transitions describes the actual course of events. The worlds judged to be epistemically possible are those that result from the most plausible of these transitions.\nBoutilier (1998) has a proposition that states that generalized belief update as described above is equivalent to “first determining the predicted updated ranking κ followed by standard (AGM) revision by α with respect to κ ,” (Boutilier, 1998, p. 16). κ is determined by taking the worlds in the current possible worlds ||K|| (induced from κ) and shifting them to all possible worlds given all possible transitions given all possible events (the actual event is unknown), taking into account the relevant plausibility rankings."
    }, {
      "heading" : "Stochastic Belief Change",
      "text" : "I now consider agents who deal with uncertainty by maintaining a probability distribution over possible situations (worlds) they could be in. Let a belief state b be defined as the set {(w, p) | w ∈W,p ∈ [0, 1]}, where ∑ (w,p)∈b p = 1. The probability of being in w is denoted b(w). That is, b is a probability distribution over all the worlds in W . In the hybrid stochastic belief change (HSBC) framework, an agent maintains a belief state, which changes as new information is received or observed.\nAn agent is assumed to have a model of how the world works.\nDefinition 1. The stochastic belief change modelM has the form 〈W, ε, T,E,O, os〉, where • W is a set of possible worlds, • ε is a set of events,\n• T : (W × ε ×W ) → [0, 1] is a transition function such that for every e ∈ ε and w ∈ W , ∑ w′∈W T (w, e, w\n′) = 1 (T (w, e, w′) models the probability of a transition to world w′, given the occurrence of event e in world w), • E is the event likelihood function (E(e, w) = P (e | w), the probability of the occurrence of event e in w), • O : (L×W )→ [0, 1] is an observation function such that for every world w, ∑ α∈ΩO(α,w) = 1 (O(α,w) models\nthe probability of observing α in w), where Ω ⊂ L is the set of possible observations, up to equivalence, and where if α ≡ β, then O(α,w) = O(β,w), for all worlds w.1 • os : (Ω × W ) → [0, 1] (os(α,w) is the agent’s ontic strength for α perceived in w.)\nDefinition 2. b(α) := ∑ w∈W,w|=α b(w).\nLet b◦α := b ◦ α so that we can write b◦α(w), where ◦ is any update or revision operator.\nOften, in the exposition of this paper, a world will be referred to by its truth vector. For instance, if the vocabulary is {q, r} and w3 |= ¬q ∧ r, then w3 may be referred to as 01.\nFor parsimony, let b = 〈p1, . . . , pn〉 be the probabilities that belief state b assigns to w1, . . . , wn where 〈w1, w2, w3, w4〉 = 〈11, 10, 01, 00〉, and 〈w1, w2, . . . , w8〉 = 〈111, 110, . . . , 000〉."
    }, {
      "heading" : "Update",
      "text" : "Transitions associated with the observation of α from a world w in the current belief state bcur to a world w′ could be caused by different events. According to Boutilier (1998), update can be defined as\nbeventnew := {\n(w′, p′) | w′ ∈W,p′ =∑ w∈W ∑ e∈ε T (w, e, w′)E(e, w)bcur (w) } .\nBecause the actual event is unobservable/hidden, p′ is the expected probability of reaching w′, given the event probabilities.\nIn partially observable Markov decision process (POMDP) theory (Aström, 1965; Monahan, 1982; Lovejoy, 1991), events are actions chosen by the agent (and thus observable) and observations are hidden. Then, given current belief state bcur , selected action a and observation o, the state estimation function is defined by\nbpomdpnew :=\n{ (w′, p′) | w′ ∈W,p′ =\nO(o, a, w′) ∑ w∈W T (w, a,w ′)bcur (w)\nP (o | a, bcur )\n} ,\nwhere Ω is a set of observation objects and O : (Ω × A × W ) → [0, 1] is an observation function, such that for every a and w′, ∑ o∈ΩO(o, a, w\n′) = 1. O(o, a, w′) models the probability of perceiving o in arrival world w′, given the execution of some action a ∈ A. Note that P (o | a, bcur ) is a normalizing constant.\n1≡ denotes logical equivalence.\nBut what is the probabilistic update, given new information/evidence α? I suggest that α is the (overt) ‘signal’ generated by the (covert) event. An important question is, When is α received – in the current/departure world (wc) or in the new/arrival world (wn)? Although it is not clear to me, in POMDP theory, observations are always assumed to be received in the arrival world – I shall assume the same.\nIn the present framework, actions are not selected by the agent, but by nature. In other words, actions are considered to be events occurring in the environment, uncontrollable by the agent. Further, at the present stage of research, I shall assume that the agent has a less detailed observation model, that is, an agent only knows O(α,wn), the probability of perceiving α in arrival world wn (defined in Def. 1). Hence, I propose to weight beventnew (w\n′) by O(α,wn) when receiving new information α and one knows that one’s belief state should be updated (due to an evolving world). Then we can define\nDefinition 3. b α := {\n(w′, p′) | w′ ∈W,p′ = 1 γ O(α,w′) ∑ w∈W ∑ e∈ε T (w, e, w′)E(e, w)b(w) } ,\nwhere γ is a normalizing factor.\nAs far as I know, no-one has proposed rationality postulates for probabilistic update. The reason is likely due to probabilistic update being defined in terms of standard probability theory. The axioms of probability theory have been argued to be rational for several decades (although it is not without its detractors).\nThe following basic postulates for my probabilistic belief update are proposed. (Unless stated otherwise, it is assumed that α is logically satisfiable, i.e., ` ¬α is false.)\n(P 1) b α is a belief state iff not ` ¬α (P 2) b α(α) = 1 (P 3) If α ≡ β, then b α = b β\nProposition 1. If b α(α) > 0, it is not necessary that b α(α) = 1.\nProof. Let the vocabulary be {q, r}. Let b = 〈0.4, 0, 0.1, 0.5〉. Let there be only one event e. Let the transition function be specified as T (11, e, 11) = 0.5, T (11, e, 10) = 0.5, T (10, e, 01) = 1, T (01, e, 00) = 1, T (00, e, 11) = 1. Let E(e, w) = 1 for all w ∈ W . Let the evidence be q. Let O(q, 11) = 0.2, O(q, 10) = 0, O(q, 01) = 0, O(q, 00) = 0.3. Then applying operation to b produces b q = 〈0.82, 0, 0, 0.18〉. Hence, b q(q) = 0.82 6= 1.\nAlthough the following proposition is mostly negative, the reader will soon see that constraining the stochastic belief change model to be ‘rational’, the negative postulates become positive.\nProposition 2. Postulate (P 3) holds, while (P 1) and (P 2) do not hold.\nDefinition 4. We say event e is event-rational when for all w ∈ W : there exists a w′ such that T (w, e, w′) > 0 iff E(e, w) > 0. Definition 5. We say α is an e-signal when for all w′ ∈W : there exists a w such that T (w, e, w′) > 0 iff O(α,w′) > 0. Definition 6. We say a model M is observation-rational iff for all α, whenever ` ¬α, O(α,w) = 0 for all w ∈W .\nThe proposition below says that if one is rational w.r.t. observations and w.r.t. some event, and α is a signal produced by that event, then updating on α is defined. Proposition 3. If M is observation-rational, there exists an event e ∈ ε which is event-rational and α is an e-signal, then b α is a belief state iff not ` ¬α (i.e., then (P 1) holds).\n(P 2) does not hold under the antecedents of Proposition 3. Another definition is required as qualification: Definition 7. We say evidence α is trustworthy iff for all w ∈W , if w 6|= α, then O(α,w) = 0.\nThe proposition below says that if α is trustworthy, one is rational w.r.t. some event, and α is a signal produced by that event, then one should accept α in the updated belief state. Proposition 4. If α is trustworthy, there exists an event e ∈ ε which is event-rational and α is an e-signal, then b α(α) = 1 (i.e., then (P 2) holds).\nProof. Not ` ¬α is assumed by default. Recall that b α(α) = ∑ w∈W,w|=α b α(w). Referring to the (⇐) part of\nthe proof of Proposition 3, b α(α) is a belief state and thus∑ w∈W b α(w) = 1. Hence, for b α(α) to be less than 1, there must exist a w′ ∈ W s.t. w′ 6|= α and b α(w′) > 0. But then O(α,w′) > 0. Therefore, for (P 2) not to hold, an agent needs to believe thatO(α,w′) > 0 for some world w′ where w′ 6|= α. But then α cannot be trustworthy (i.e., then (P 2) holds.\nDefinition 8 (Gärdenfors, 1988). A probabilistic belief change operation ◦ is said to be preservative iff for all belief states P and for all propositions α and β, if P (α) > 0 and P (β) = 1, then P ◦α(β) = 1. Proposition 5. Operation is not preservative. Definition 9. We say evidence α is β-trustworthy if for all w ∈W , if w 6|= β, then O(α,w) = 0. Proposition 6. If b α(β) is a belief state, b(β) = 1 and α is β-trustworthy, then b α(β) = 1.\nProof. ∑ w∈W b α(w) = 1. Hence, for b α(β) to be < 1, there must exist a w× ∈ W s.t. w× 6|= β and b α(w×) > 0. And because b(β) = 1, b(w×) = 0. So some probability mass must have been shifted from some β-world to the nonβ-world w×.\nBy definition, b α(w ×) = 1γ O(α,w ×)∑ w∈W ∑ e∈ε T (w, e, w ×)E(e, w)b(w). So for b α(w ×) to be > 0, O(α,w×) must be > 0. However, because α is β-trustworthy, O(α,w×) = 0. Hence, O(α,w×) 6> 0 and b α(β) 6< 1.\nProposition 7. b α∧β 6= (b α) β .\nProof. For instance, consider the example used in the proof of Proposition 1. Let α be q and let β be q ∧ r. Note that α∧β is then logically equivalent to q∧r. LetO(q∧r, 11) = O(q ∧ r, 10) = 0.5 and O(q ∧ r, 01) = O(q ∧ r, 00) = 0.\nWe know that b q = 〈0.82, 0, 0, 0.18〉. Then (b q) q∧r = 〈1, 0, 0, 0〉. On the other hand, b q∧r = 〈0.875, 0, 0, 0.125〉."
    }, {
      "heading" : "Revision",
      "text" : "Using Bayes’ Rule2 , P (wn | α) can be determined:\nP (w | α) := O(α,w)b(w)∑ w′∈W O(α,w ′)b(w′) .\nNote that if O(α,w) = 0, then P (w | α) = 0. It is not yet universally agreed what revision means in a probabilistic setting. In classical belief change, it is understood that if the new information α is consistent with the agent’s current beliefs KB , then revision is equivalent to belief expansion (denoted +), where expansion is the logical consequences of KB ∪ {α}. It is mostly agreed upon that Bayesian conditioning corresponds to classical belief expansion. This is evidenced by Bayesian conditioning (BC) being defined only when b(α) 6= 0. In other words, one could define revision to be\nb BC α := {(w, p) | w ∈W,p = P (w | α)},\nas long as P (α) 6= 0.3 To accommodate cases where b(α) 6= 0, that is, where α contradicts the agent’s current beliefs and its beliefs need to be revised in the stronger sense, we shall make use of imaging. Imaging was introduced by Lewis (1976) as a means of revising a probability function. It has also been discussed in the work of, for instance, Gärdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015). The following version of imaging must not be regarded as a fundamental part of the larger belief change framework presented here; it should be regarded as a placeholder or suggestion for the ‘revision-module’ of the framework. Informally, Lewis’s original solution for accommodating contradicting evidence α is to move the probability of each world to its closest, α-world. Lewis made the strong assumption that every world has a unique closest α-world. More general versions of imaging allow worlds to have several, equally proximate, closest worlds.\nGärdenfors (1988) calls one of his generalizations of Lewis’s imaging general imaging. Our method is also a generalization. We thus refer to his as Gärdenfors’s general imaging and to our method as generalized imaging to distinguish them. It should be noted that these imaging methods are general revision methods and can be used in place of Bayesian conditioning for expansion. “Thus imaging is a more general method of describing belief changes than conditionalization,” (Gärdenfors, 1988, p. 112).\n2Bayes’ Rule states (in the notation of this paper) that P (w | α) = P (α | w)P (w)/P (α) or P (w | α) = P (α | w)P (w)/ ∑ w′∈W P (α | w\n′)P (w′). 3Note that in my notation, b(α) is equivalent to P (α).\nLet Min(α,w, d) be the set of α-worlds closest to w measured with d. Formally,\nMin(α,w, d) :=\n{w′ ∈ ||α|| | ∀w′′ ∈ ||α||, d(w′, w) ≤ d(w′′, w)}, where d(·) is some acceptable measure of distance between worlds (e.g., Hamming or Dalal distance). It must also obey the faithfulness condition that for every world w, d(w,w) < d(v, w) for all v 6= w. Example 1. Let the vocabulary be {q, r, s}. Let α be (q ∧ r) ∨ (q ∧ ¬r ∧ s). Suppose d is Hamming distance. Then\nMin((q ∧ r) ∨ (q ∧ ¬r ∧ s), 111, d) = {111} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 110, d) = {110} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 101, d) = {101} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 100, d) = {110, 101} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 011, d) = {111} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 010, d) = {110} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 001, d) = {101} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 000, d) = {110, 101}\nThen generalized imaging (denoted GI) is defined as Definition 10.\nb GI α := { (w, p) | w ∈W,p = 0 if w 6∈ ||α||,\nelse p = ∑ w′∈W\nw∈Min(α,w′,d)\nb(w′)/|Min(α,w′, d)| } .\nExample 2. Continuing on Example 1: Let b = 〈0, 0.1, 0, 0.2, 0, 0.3, 0, 0.4〉.\n(q ∧ r) ∨ (q ∧ ¬r ∧ s) is abbreviated as α. bGIα (111) = ∑ w′∈W\n111∈Min(α,w′,d) b(w′)/|Min(α,w′, d)|\n= b(111)/|Min(α, 111, d)| + b(011)/|Min(α, 011, d)| = 0/1 + 0/1 = 0. bGIα (110) = ∑ w′∈W\n110∈Min(α,w′,d) b(w′)/|Min(α,w′, d)|\n= b(110)/|Min(α, 110, d)| + b(100)/|Min(α, 100, d)| + b(010)/|Min(α, 010, d)| + b(000)/|Min(α, 000, d)| = 0.1/1 + 0.2/2 + 0.3/1 + 0.4/2 = 0.7. bGIα (101) = ∑ w′∈W\n101∈Min(α,w′,d) b(w′)/|Min(α,w′, d)|\n= b(101)/|Min(α, 101, d)| + b(100)/|Min(α, 100, d)| + b(001)/|Min(α, 001, d)| + b(000)/|Min(α, 000, d)| = 0/1 + 0.2/2 + 0/1 + 0.4/2 = 0.3.\nAnd bGIα (100) = b GI α (011) = b GI α (010) = b GI α (001) = bGIα (000) = 0. Notice how the probability mass of non-α-worlds is shifted to their closest α-worlds. If a non-α-world w× with probability p has n closest α-worlds (equally distant), then each of these closest α-worlds gets p/n mass from w×.\nRecall that in the proposed framework, agents have access to an observation model (formalized via an observation function O(·, ·)). Given enough computational power and time, it would be irrational for an agent to ignore its observation model when revising its beliefs. Another proposed\ndefinition for a stochastic belief revision operation based on imaging (denoted OGI) is thus Definition 11.\nb OGI α := { (w, p) | w ∈W,p = O(α,w)b GI α (w)∑\nw′∈W O(α,w ′)bGIα (w ′)\n} ,\nwhere the denominator is a normalizing factor. b OGI α is not defined as{\n(w, p) | w ∈W,p = 0 if w 6∈ ||α||, else p = ∑ w′∈W\nw∈Min(α,w′,d)\nO(α,w′)b(w′)/|Min(α,w′, d)| } ,\nbecause α is assumed perceived in the new world w, not the old world w′.\nNote that if P (· | α) were used instead of O(α, ·), then OGI would be undefined whenever b(α) = 0. But this is exactly the problem we want to avoid by using imaging. Another justification to rather use O(α,w) is that its value is positively correlated with P (w | α): If O(α,w) = 0, then P (w | α) = 0. If P (w | α) = 1, then O(α,w) is maximal in b in the following sense: for all w′ ∈ W , if w′ 6= w, then either b(w′) = 0 or O(α,w′) = 0, whereas b(w) > 0 and O(α,w) > 0.\nNote that the denominator my be zero, making OGI undefined in that case. I shall deal with this issue a little bit later. Example 3. Recall from Example 2 that bGIα = 〈0, 0.7, 0.3, 0, 0, 0, 0, 0〉 and α is (q ∧ r) ∨ (q ∧ ¬r ∧ s). Let O(α,w) = 0.3, say, for all w ∈ W . Then bOGIα = bGIα . Obviously, if the observation model carries no information with respect to α, then it has no influence on the agent’s revised beliefs.\nNow let O(α,w) = 0.3 if w |= r, else O(α,w) = 0.2. Then bOGIα = 〈0.3 × 0/0.23, 0.2 × 0.7/0.23, 0.3 × 0.3/0.23, 0.2 × 0/0.23, 0.3 × 0/0.23, 0.2 × 0/0.23, 0.3 × 0/0.23, 0.2 × 0/0.23〉 = 〈0, 0.61, 0.39, 0, 0, 0, 0, 0〉. If the agent has an observation model telling it that α is more likely to be perceived in r-worlds than in ¬r-worlds, then when it receives α, the agent should be biased to believing that it is actually in an r-world. However, the agent was certain that it was in a ¬r-world when its belief state was b. GI thus pushes the agent to favour the α-worlds being ¬rworlds. Hence, in this example there is tension between being in a ¬r-world (due to previous beliefs) and being in an r-world (due to the observation model). Definition 12.\nb BCI α := { b BC α if b(α) > 0 b OGI α if b(α) = 0\nI denote the expansion of belief state b on α as b+α (resp., probability function P on α as P+α ) and delay its definition till later. P⊥ is conventionally defined to be the absurd probability function which is defined to be P⊥(δ) = 1 for all δ ∈ L.\nGärdenfors (1988) proposed six rationality postulates for probabilistic belief revision. (Unless stated otherwise, it is assumed that α is logically satisfiable, i.e., ` ¬α is false.)\n1. P ∗α is a probability function 2. P ∗α(α) = 1 3. If α ≡ β, then P ∗α = P ∗β 4. P ∗α 6= P⊥ iff not ` ¬α 5. If P (α) > 0, then P ∗α = P + α 6. If P ∗α(β) > 0, then P ∗ α∧β = (P ∗ α) + β .\nInstead of saying that the result of an operation is P⊥, I simply say that the result is undefined. And by noting that the result of an operation is not a belief state if it is undefined, one can merge postulates 1 and 4. The stochastic belief revision postulates in my notation are thus\n(P ∗1) b∗α is a belief state iff not ` ¬α (P ∗2) b∗α(α) = 1 (P ∗3) If α ≡ β, then b∗α = b∗β (P ∗4) If b(α) > 0, then b∗α = b + α (P ∗5) If b∗α(β) > 0, then b ∗ α∧β = (b ∗ α) + β .\nI now test OGI and BCI against each of the five postulates. Recall that if the denominator in the definition of OGI\nis zero, it is undefined. To guarantee that OGI is defined,∑ w′∈W O(α,w ′)bGIα (w ′) must be non-zero, that is, there must be at least one w′ ∈ W for which O(α,w′)bGIα (w′) > 0. We know that when w′ 6∈ ||α||, O(α,w′)bGIα (w′) = bGIα (w\n′) = 0. Definition 13. We say α is weakly observable iff there exists a w ∈ W such that w |= α and O(α,w) > 0. We say α is strongly observable iff for all w ∈ W for which w |= α, O(α,w) > 0. Proposition 8. When ∗ is OGI, postulate (P ∗1), in general, does not hold, but does hold if evidence α is strongly observable. Proof. Firstly, observe that b(w′) =∑ |Min(α,w′,d)| b(w ′)/|Min(α,w′, d)|. Therefore,\n1 = ∑ w′∈w b(w′)\n= ∑ w′∈w ∑ |Min(α,w′,d)| b(w′)/|Min(α,w′, d)|\n= ∑\nw′∈w,|Min(α,w′,d)|\nb(w′)/|Min(α,w′, d)|\n= ∑\nw′∈w,w∈W,w∈Min(α,w′,d)\nb(w′)/|Min(α,w′, d)|\n= ∑ w∈W ∑ w′∈w,w∈Min(α,w′,d) b(w′)/|Min(α,w′, d)|\n= ∑ w∈W bGIα (w).\nLet b = 〈0, 0.1, 0, 0.2, 0, 0.3, 0, 0.4〉 and α be (q ∧ r) ∨ (q ∧ ¬r ∧ s). Let O(α, 111) = 0.9 and O(α,w) = 0 for all w ∈ W , w 6= 111. (Notice that α is weakly observable.) From Example 2, we know that bGIα (111) = 0, implying that bOGIα (111) = 0, and one can deduce that b OGI α (w) = 0 for all w ∈W , due to the specification of the observation model.\nNow, let α be strongly observable: let O(α, 111) = O(α, 110) = O(α, 101) = 0.1, else O(α, ·) = 0. Then bOGIα = 〈0, 0.7, 0.3, 0, 0, 0, 0, 0〉. In general, let O(α,w) > 0 for all w ∈ W for which w |= α. By definition of GI, the probability mass of all non-α-worlds is shifted to their closest α-worlds; the total mass (of the α-worlds) thus remains 1. Hence, bGIα (α) = 1 and there exists a w\n′ |= α s.t. bGIα (w\n′) > 0. Now, by definition of strong observability, O(α,w′) > 0. Therefore, O(α,w′)bGIα (w\n′) > 0. And due to the normalizing effect of the denominator in the definition of OGI, bOGIα is a belief state.\nProposition 9. When ∗ is OGI, postulate (P ∗2), in general, does not hold and does hold when α is strongly observable.\nProof. This result follows directly from an understanding of the proof of Proposition 8.\nProposition 10. When ∗ is OGI, postulate (P ∗3) holds. Proposition 11. Let ∗ be OGI. If + is OGI, postulate (P ∗4) holds, otherwise it does not.\nAssuming (P ∗4) holds, I consider whether (P ∗5) holds only for two combinations of instantiations of ∗ and +. Proposition 12. When ∗ is OGI and + is OGI, postulate (P ∗5) does not hold.\nProof. An instance is provided where bOGIα (β) > 0 and bOGIα∧β 6= (bOGIα )OGIβ .\nContinuing with Example 3, where b = 〈0, 0.1, 0, 0.2, 0, 0.3, 0, 0.4〉, α is (q ∧ r) ∨ (q ∧ ¬r ∧ s) and bOGIα = b GI α = 〈0, 0.7, 0.3, 0, 0, 0, 0, 0〉. Let β be q ∧ r, then bOGIα (β) = 0.7 > 0. But bOGIα∧β = b OGI β = 〈0, 1, 0, 0, 0, 0, 0, 0〉 and (bOGIα )OGIβ = 〈0.3, 0.7, 0, 0, 0, 0, 0, 0〉.\nProposition 13. When ∗ is BCI, postulate (P ∗1) holds.\nProof. It is known that Bayesian conditioning results in a belief state when the conditional is non-contradictory.\nProposition 14. When ∗ is BCI, postulate (P ∗2) holds.\nProof. By definition of BC, all non-α-worlds get zero probability and the probabilities of the remaining α-worlds are magnified to sum to 1.\nProposition 15. When ∗ is BCI, postulate (P ∗3) holds. Proposition 16. Let ∗ be BCI. If + is BC or BCI, postulate (P ∗4) holds, otherwise it does not.\nFor the proof of the next proposition, a lemma is required.\nLemma 1. Let b(α) > 0. If bBCα (β) > 0, then b(α∧β) > 0.\nProof. Assume bBCα (β) > 0. Then there exists a w β ∈ W s.t. wβ |= β and bBCα (wβ) > 0. By definition, bBCα (w) = b(α,w) b(α) , implying b(α,wβ) b(α) > 0. Hence, b(w\nβ) > 0 and wβ |= α. But if wβ |= α, then wβ |= α∧ β, and due to b(wβ) > 0, b(α ∧ β) > 0.\nProposition 17. When ∗ is BCI and + is BC, postulate (P ∗5), in general, does not hold, but does hold when b(α) > 0.\nRecall that a probabilistic belief change operation ◦ is preservative iff for all belief states b and for all propositions α and β, if b(α) > 0 and b(β) = 1, then b◦α(β) = 1. Proposition 18. Operation OGI is not preservative, while BCI is preservative.\nProof. OGI: Let the vocabulary be {q, r} and b = 〈0, 0.5, 0.5, 0〉. Let α be q and β be q ↔ ¬r. Then b(α) > 0, b(β) = 1 and bGIα = 〈0.5, 0.5, 0, 0〉. Let O(q, w) = 1 for all w ∈W . Then bOGIα = 〈0.5, 0.5, 0, 0〉 and bOGIα (β) = 0.5.4\nBCI: bBCIα (β) = b BC α (β). By assuming that b(α) > 0 and b(β) = 1, one is implicitly assuming that if w |= α s.t. b(α) > 0, then w |= β. This in turn implies that whenever bBCα (w) > 0, that w |= β. The latter is due to conditionalization: {w ∈ W | bBCα (w) > 0} is a subset of {w ∈ W | w |= α, b(w) > 0}. By (P ∗2), bBCα (α) = 1. But due to the fact that for all w ∈ W , if bBCα (w) > 0, then w |= β, it must then be the case that bBCα (β) = 1."
    }, {
      "heading" : "Ontic and Epistemic Strength",
      "text" : "Suppose there is a range of degrees for information being ontic (the effect of a physical action or occurrence) or epistemic (purely informative). I shall assume that the higher the information’s degree of being ontic, the lower the epistemic status of that information. An agent has a certain sense of the degree to which a piece of received information is due to a physical action or event in the world. This sense may come about due to a combination of sensor readings and reasoning. If the agent performs an action and a change in the local environment matches the expected effect of the action, it can be quite certain that the effect is ontic information. If the agent receives the information from another agent (e.g., radio, through reading, a person speaking directly to the agent), then it should be clear to the agent that the information is epistemic and thus has a low degree of being ontic. If the agent’s sensors show activity, but the agent knows that it did not presently perform an action with an effect matching its sensor readings, and if the readings do not reveal an epistemic source for the information, then the agent will have to infer from the present world conditions and the information received, or access learnt knowledge matching the present world conditions and the information received, the degree to which the information should be regarded as ontic. For instance, a person might stop talking just after you ask him/her to be quiet. Under particular conditions the person may stop talking due to your request and in other conditions he/she may have stopped talking anyway. Depending on the present world conditions, you might assign a higher (but not definitely certainty) or lower (but not definitely zero) degree of likelihood that the information (i.e., that the person stopped talking) is ontic. Or suppose you have been wearing dark glasses for one hour. You put them on due to the sky being clear and (too) bright. When you take your glasses off, it is not as bright as you thought it would be. So, has the ambient\n4Here, d is Hamming distance.\nbrightness decreased due to changes in the weather, or does it only seem darker when you remove your glasses, due to some unknown physiological process? In this case, it would be convenient to consider the brightness/darkness information as being equally likely ontic and epistemic.\nRecall from Definition 1 that os(α,w) indicates an agent’s sense for the ontic strength of α received in w. We say that os(α,w) = 1 when α is certainly ontic in w. When α is certainly epistemic in w, then os(α,w) = 0. In fact, let the epistemic strength of α in w be defined as es(α,w) := 1− os(α,w)."
    }, {
      "heading" : "Combining Update and Revision",
      "text" : "I propose a way of trading off the probabilistic update and probabilistic revision defined earlier, using the notion of ontic strength.\nThe hybrid stochastic change of belief state b due to new information α with ontic strength (denoted bCα) is defined as\nDefinition 14. bC α := {\n(w, p) | w ∈W,p = 1\nγ\n[ (1− os(α,w))b∗α(w) + os(α,w)b α(w) ]} ,\nwhere γ is a normalizing factor so that ∑ w∈W b C α (w) = 1.\nDue to our assumption that α is observed in the arrival world, not the departure world, os(·) is applied to the arrival world.\nConsidering the rationality postulates presented so far for belief update and revision, one can naturally suggest the following postulates for their combination.\n(PC1) bCα is a belief state iff not ` ¬α (PC2) bCα (α) = 1 (PC3) If α ≡ β, then bCα = bCβ\nProposition 19. Postulate (PC1) does not hold. Proposition 20. Postulate (PC2) does not hold.\nProof. (PC2) does not hold because (P 2) does not hold.\nProposition 21. Postulate (PC3) holds.\nProof. (PC3) is holds because (P 3) and (P ∗3) hold.\nTheorem 1. If: the agent model M is observation-rational, α is trustworthy and strongly observable, there exists an event e ∈ ε which is event-rational and α is an e-signal, then (i) bCα is a belief state iff not ` ¬α (i.e., then (PC1) is true) and (ii) bCα (α) = 1 (i.e., then (P C2) is true).\nProof. Note that by Propositions 3 and 4, (P 1) and (P 2) hold. And recall that (P ∗1) and (P ∗2) are true when α is strongly observable (see Props. 8, 9, 13 and 14).\n(i)(PC1) Given the antecedents of this proposition, we know by Proposition 4 that b α is defined iff not ` ¬α. And by (P ∗4), b∗α is defined iff not ` ¬α.\n(⇒) Assume bCα is defined. So there exists a w ∈ W s.t. bCα (w) > 0, that is, 1 γ [ (1 − os(α,w))b∗α(w) +\nos(α,w)b α(w) ] > 0. Thus, either b∗α(w) > 0 (while 1 − os(α,w) > 0) or b α(w) > 0 (while os(α,w) > 0) (or both), which implies that b∗α resp. b α is defined. Therefore, not ` ¬α. (⇐) Assume not ` ¬α. Then b∗α and b α are defined. This implied that there exists a w ∈ W s.t. either b∗α(w) > 0 or b α(w) > 0 (or both). Hence, b C α (w) > 0 and due to normalization in the definition of C, bCα is defined. (ii)(PC2) bCα (α) = ∑ w∈W,w|=α b\nC α (w) =∑\nw∈W,w|=α 1 γ [ (1 − os(α,w))b∗α(w) + os(α,w)b α(w) ] ,\nwhere γ = ∑ w∈W [ (1 − os(α,w))b∗α(w) +\nos(α,w)b α(w) ] . But by (P ∗2) and (P 2), if w 6|= α, then b∗α(w) = 0 and b α(w) = 0. Hence, γ = ∑ w∈W,w|=α [ (1 − os(α,w))b∗α(w) + os(α,w)b α(w) ] . Therefore, bCα (α) =∑\nw∈W,w|=α (1−os(α,w))b∗α(w)+os(α,w)b α(w)∑\nw∈W,w|=α [ (1−os(α,w))b∗α(w)+os(α,w)b α(w) ] = 1.\nAlthough one cannot expect C to be preservative, due to probabilistic update not being preservative (Prop. 5), one can expect C to have preservative-like behaviour under particular conditions: Recall that α is defined to be β-trustworthy if for all w ∈W , if w 6|= β, then O(α,w) = 0. Proposition 22. If b α(β) is a belief state, bCα (β) is a belief state, b(β) = 1, α is β-trustworthy and ∗ is BCI, then bCα (β) = 1.\nProof. By Proposition 6, b α(β) = 1, when α is βtrustworthy. By Proposition 18, ∗ is preservative when defined as BCI. Then, for all w ∈ W , if w 6|= β, then b α(β) = b ∗ α(β) = 0. Hence, for all w ∈ W , if w 6|= β, bCα (w) = 0. Therefore, because b C α (β) is a belief state,\nbCα (β) = 1− bCα (¬β) = 1− ∑ w∈w,w|=¬β bCα (w)\n= 1− ∑\nw∈w,w 6|=β\nbCα (w)\n= 1− 0 = 1."
    }, {
      "heading" : "Examples and Analysis",
      "text" : "HSBC is now analyzed via examples. The example domain is adapted from one of the domains in the article of Boutilier (1998) – here though, worlds are associated with probabilities, not plausibility ranks. There are eight possible worlds, depending on whether a book B is inside the house (if it is not in the house, then it is assumed to be on the patio, adjacent to the lawn), whether the book is dry and whether the lawn-grass G is dry. There are three events: rain – it rains, sprnk – the sprinkler is on, and null – neither of these, the\nnull event.5 In Boutilier’s example, events are deterministic; however, events in this paper are modeled to be stochastic, to better illustrate the behaviour of the framework.\nTo simplify calculations and to aid the reader in understanding the results, in the following examples, the agent will associate equal epistemic/ontic strength to a particular piece of information for all worlds (per example case). I shall compute the agent’s new belief state for each of os(α,w) ∈ {0, 0.25, 0.5, 0.75, 1} (for all w ∈ W ), for the two cases where α is ¬Dry(G) and where α is ¬Dry(G) ∧ Dry(B).\nBoutilier models the agent’s current (initial) epistemic state with the most plausible situation (rank 0) being (¬Inside(B), Dry(B), Dry(G)) and the next plausible situation (rank 1) being (Inside(B), Dry(B), Dry(G)). I translate this as the agent having a belief state where b(¬Inside(B), Dry(B), Dry(G)) = 0.67 and b(Inside(B), Dry(B), Dry(G)) = 0.33. Observe that in these examples, revision as OGI is equivalent to revision as BCI, because b(¬Dry(G)) = b(¬Dry(G) ∧ Dry(B)) = 0.\nThe HSBC model M = 〈W, ε, T,E,O, os〉 is now specified.\nLet w1, . . . , w8 refer to worlds w1: (Inside(B), Dry(B), Dry(G)) w2: (Inside(B), Dry(B),¬Dry(G)) w3: (Inside(B),¬Dry(B), Dry(G)) w4: (Inside(B),¬Dry(B),¬Dry(G)) w5: (¬Inside(B), Dry(B), Dry(G)) w6: (¬Inside(B), Dry(B),¬Dry(G)) w7: (¬Inside(B),¬Dry(B), Dry(G)) w8: (¬Inside(B),¬Dry(B),¬Dry(G))\nThe events are ε = {rain, sprnk, null}. The following probabilities are debatable; they should not\nbe taken too seriously but serve to illustrate the framework.\nT (w1, null, w1) = 0.75 T (w5, null, w5) = 1 T (w1, null, w2) = 0.1 T (w5, null, w6) = 0 T (w1, null, w3) = 0.1 T (w5, null, w7) = 0 T (w1, null, w4) = 0.05 T (w5, null, w8) = 0\nT (w1, rain, w1) = 0 T (w5, rain, w5) = 0 T (w1, rain, w2) = 0.75 T (w5, rain, w6) = 0.05 T (w1, rain, w3) = 0 T (w5, rain, w7) = 0.05 T (w1, rain, w4) = 0.25 T (w5, rain, w8) = 0.9\nT (w1, sprnk, w1) = 0 T (w5, sprnk, w5) = 0 T (w1, sprnk, w2) = 0.8 T (w5, sprnk, w6) = 0.8 T (w1, sprnk, w3) = 0 T (w5, sprnk, w7) = 0.05 T (w1, sprnk, w4) = 0.2 T (w5, sprnk, w8) = 0.15\nE(null, w1) = 0.06 E(null, w5) = 0.15 E(rain, w1) = 0.31 E(rain, w5) = 0.7 E(sprnk, w1) = 0.63 E(sprnk, w5) = 0.15\n5I shall assume that the null event may include some unknown events (with unknown effects).\nO(¬Dry(G), w1) = 0.05 O(¬Dry(G) ∧ Dry(B), w1) = 0.5 O(¬Dry(G), w2) = 0.95 O(¬Dry(G) ∧ Dry(B), w2) = 0.8 O(¬Dry(G), w3) = 0.05 O(¬Dry(G) ∧ Dry(B), w3) = 0.1 O(¬Dry(G), w4) = 0.95 O(¬Dry(G) ∧ Dry(B), w4) = 0.05 O(¬Dry(G), w5) = 0.05 O(¬Dry(G) ∧ Dry(B), w5) = 0.6 O(¬Dry(G), w6) = 0.95 O(¬Dry(G) ∧ Dry(B), w6) = 0.98 O(¬Dry(G), w7) = 0.05 O(¬Dry(G) ∧ Dry(B), w7) = 0.2 O(¬Dry(G), w8) = 0.95 O(¬Dry(G) ∧ Dry(B), w8) = 0.15\nRecall that the current belief state is b = 〈0.33, 0, 0, 0, 0.67, 0, 0, 0〉. The following is a list of resulting belief states b′ = b C ¬Dry(G) for the specified ontic strengths.\nos(·) bC ¬Dry(G)\n0.00 〈0.00, 0.33, 0.00, 0.00, 0.00, 0.67, 0.00, 0.00〉 0.25 〈0.00, 0.32, 0.00, 0.02, 0.00, 0.53, 0.00, 0.13〉 0.50 〈0.00, 0.31, 0.00, 0.04, 0.00, 0.40, 0.00, 0.25〉 0.75 〈0.00, 0.30, 0.00, 0.06, 0.00, 0.26, 0.00, 0.38〉 1.00 〈0.00, 0.28, 0.00, 0.08, 0.01, 0.12, 0.00, 0.51〉\nSeveral behaviours can be noted: When the observation is completely epistemic, the probabilities of the two believed worlds are each shifted to their closest ¬Dry(G)-worlds. The more the agent considers the information to be ontic, the more its beliefs are spread out due to the nondeterminism of the events. Whether the observation is considered ontic or epistemic, the agent has a relatively strong belief (between 28% and 33%) that the book is inside and dry. However, in cases where the book is outside, there is a considerable shift in probability from the book being dry (w6) to it being wet (w8), as the agent moves towards an ontic mindset. One could perhaps argue that in an ontic mindset, the agent has access to event/transition information so as to reason about the causes of the book getting wet: it believes that there is a moderate to high likelihood that the book will get wet if it is on the patio, due to the sprinkler coming on or it starting to rain (explaining the wet-grass evidence).\nThe following is a list of resulting belief states b′ = b C ¬Dry(G) ∧ Dry(B) for the specified epistemic strengths.\nos(·) bC ¬Dry(G) ∧ Dry(B)\n0.00 〈0.00, 0.29, 0.00, 0.00, 0.00, 0.71, 0.00, 0.00〉 0.25 〈0.00, 0.33, 0.00, 0.00, 0.03, 0.59, 0.00, 0.04〉 0.50 〈0.01, 0.37, 0.00, 0.00, 0.07, 0.47, 0.01, 0.07〉 0.75 〈0.01, 0.41, 0.00, 0.01, 0.10, 0.35, 0.01, 0.11〉 1.00 〈0.02, 0.45, 0.00, 0.01, 0.14, 0.23, 0.01, 0.15〉\nWhen the agent considers the observation completely epistemically, its beliefs change very similarly to when it was only told that the grass is wet; the agent already believed that the book was dry. However, the extra information has a significant impact on how the agent’s beliefs change when the observation is considered ontically. The agent now believes much less that the book is outside and wet and the grass is wet, and with 78% (as opposed to 40% with the first observation) that the book is dry and the grass is wet (independent of where the book is located). The reason is that when the received information includes a dry book, transitions are focused on going to dry-book worlds."
    }, {
      "heading" : "Conclusion",
      "text" : "In this paper I suggested a method to arrive at a new (probabilistic) belief state when the agent has mixed feelings about whether to revise or update its beliefs, given a new piece of information. Much attention was given to the design and analysis of the separate update and revision operations. The postulates and finally Theorem 1 add weight to my argument that the hybrid stochastic belief change (HSBC) operation is rational when the agent has a rational frame of mind.\nLooking at the examples above, the way in which probabilities shift among the possible worlds, given the different ontic/epistemic strengths, seems justifiable. However, more analysis is required here, especially when considering more complicated specification patterns of the ontic/epistemic strengths.\nDetermining os(α,w) for every foreseen α in every possible world w will be challenging for a designer. Some deep questions are: Should the designer/agent provide the strengths (via stored values or programmed reasoning), or do these strengths come to the agent attached to the new information? What is the reasoning process we go through to determine whether information is epistemic or ontic, if at all? In general, how does an agent know when information is epistemic (requiring revision) or ontic (requiring update)?"
    } ],
    "references" : [ {
      "title" : "On the logic of theory change: Partial meet contraction and revision functions",
      "author" : [ "C.E. Alchourrón", "P. Gärdenfors", "D. Makinson" ],
      "venue" : "Journal of Symbolic Logic 50(2):510–530.",
      "citeRegEx" : "Alchourrón et al\\.,? 1985",
      "shortCiteRegEx" : "Alchourrón et al\\.",
      "year" : 1985
    }, {
      "title" : "Optimal control of Markov decision processes with incomplete state estimation",
      "author" : [ "K. Aström" ],
      "venue" : "Journal of Mathematical Analysis and Applications 10:174–205.",
      "citeRegEx" : "Aström,? 1965",
      "shortCiteRegEx" : "Aström",
      "year" : 1965
    }, {
      "title" : "Unifying default reasoning and belief revision in a modal framework",
      "author" : [ "C. Boutilier" ],
      "venue" : "Artificial Intelligence 68:33–85.",
      "citeRegEx" : "Boutilier,? 1994",
      "shortCiteRegEx" : "Boutilier",
      "year" : 1994
    }, {
      "title" : "A unified model of qualitative belief change: a dynamical systems perspective",
      "author" : [ "C. Boutilier" ],
      "venue" : "Artificial Intelligence 98(1–2):281–316.",
      "citeRegEx" : "Boutilier,? 1998",
      "shortCiteRegEx" : "Boutilier",
      "year" : 1998
    }, {
      "title" : "Proceedings of the thirteenth pacific rim international conference on artificial intelligence (pricai 2014)",
      "author" : [ "K. Chhogyal", "A. Nayak", "R. Schwitter", "A. Sattar" ],
      "venue" : "Pham, D., and Park, S., eds., Proc. of PRICAI 2014, volume 8862 of LNCS, 694–707. Springer-Verlag.",
      "citeRegEx" : "Chhogyal et al\\.,? 2014",
      "shortCiteRegEx" : "Chhogyal et al\\.",
      "year" : 2014
    }, {
      "title" : "Belief revision and updates in numerical formalisms: An overview, with new results for the possibilistic framework",
      "author" : [ "D. Dubois", "H. Prade" ],
      "venue" : "Proceedings of the 13th International Joint Conference on Artifical Intelligence, volume 1 of IJCAI’93, 620–625. San Francisco,",
      "citeRegEx" : "Dubois and Prade,? 1993",
      "shortCiteRegEx" : "Dubois and Prade",
      "year" : 1993
    }, {
      "title" : "Knowledge in Flux: Modeling the Dynamics of Epistemic States",
      "author" : [ "P. Gärdenfors" ],
      "venue" : "Massachusetts/England: MIT Press.",
      "citeRegEx" : "Gärdenfors,? 1988",
      "shortCiteRegEx" : "Gärdenfors",
      "year" : 1988
    }, {
      "title" : "Rank-based systems: A simple approach to belief revision, belief update",
      "author" : [ "M. Goldszmidt", "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Goldszmidt and Pearl,? \\Q1992\\E",
      "shortCiteRegEx" : "Goldszmidt and Pearl",
      "year" : 1992
    }, {
      "title" : "Updating sets of probabilities",
      "author" : [ "A. Grove", "J. Halpern" ],
      "venue" : "Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, UAI’98, 173–182. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Grove and Halpern,? 1998",
      "shortCiteRegEx" : "Grove and Halpern",
      "year" : 1998
    }, {
      "title" : "Two modellings for theory change",
      "author" : [ "A. Grove" ],
      "venue" : "Journal of Philosophical Logic 17:157–170.",
      "citeRegEx" : "Grove,? 1988",
      "shortCiteRegEx" : "Grove",
      "year" : 1988
    }, {
      "title" : "On the difference between updating a knowledge base and revising it",
      "author" : [ "H. Katsuno", "A. Mendelzon" ],
      "venue" : "Proceedings of the Second International Conference on Principles of Knowledge Representation and Reasoning, 387–394.",
      "citeRegEx" : "Katsuno and Mendelzon,? 1991",
      "shortCiteRegEx" : "Katsuno and Mendelzon",
      "year" : 1991
    }, {
      "title" : "Linking iterated belief change operations to nonmonotonic reasoning",
      "author" : [ "G. Kern-Isberner" ],
      "venue" : "Proceedings of the Eleventh International Conference on Principles of Knowledge Representation and Reasoning, 166–176. Menlo Park, CA: AAAI Press.",
      "citeRegEx" : "Kern.Isberner,? 2008",
      "shortCiteRegEx" : "Kern.Isberner",
      "year" : 2008
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "Cambridge, MA and London, England: The MIT Press.",
      "citeRegEx" : "Koller and Friedman,? 2009",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "Probabilities of conditionals and conditional probabilities",
      "author" : [ "D. Lewis" ],
      "venue" : "Philosophical Review 85(3):297–315.",
      "citeRegEx" : "Lewis,? 1976",
      "shortCiteRegEx" : "Lewis",
      "year" : 1976
    }, {
      "title" : "A survey of algorithmic methods for partially observed Markov decision processes",
      "author" : [ "W. Lovejoy" ],
      "venue" : "Annals of Operations Research 28:47–66.",
      "citeRegEx" : "Lovejoy,? 1991",
      "shortCiteRegEx" : "Lovejoy",
      "year" : 1991
    }, {
      "title" : "A survey of partially observable Markov decision processes: Theory, models, and algorithms",
      "author" : [ "G. Monahan" ],
      "venue" : "Management Science 28(1):1–16.",
      "citeRegEx" : "Monahan,? 1982",
      "shortCiteRegEx" : "Monahan",
      "year" : 1982
    }, {
      "title" : "A formal theory of knowledge and action",
      "author" : [ "R. Moore" ],
      "venue" : "Allen, J.; Hendler, J.; and Tate, A., eds., Readings in Planning. San Mateo: Morgan-Kaufmann. 480–519.",
      "citeRegEx" : "Moore,? 1990",
      "shortCiteRegEx" : "Moore",
      "year" : 1990
    }, {
      "title" : "Artificial Intelligence: Foundations of Computational Agents",
      "author" : [ "D. Poole", "A. Mackworth" ],
      "venue" : "New York, USA: Cambridge University Press.",
      "citeRegEx" : "Poole and Mackworth,? 2010",
      "shortCiteRegEx" : "Poole and Mackworth",
      "year" : 2010
    }, {
      "title" : "A new approach to probabilistic belief change",
      "author" : [ "G. Rens", "T. Meyer" ],
      "venue" : "Russell, I., and Eberle, W., eds., Proceedings of the International Florida AI Research Society Conference (FLAIRS), 582–587. AAAI Press.",
      "citeRegEx" : "Rens and Meyer,? 2015",
      "shortCiteRegEx" : "Rens and Meyer",
      "year" : 2015
    }, {
      "title" : "Ordinal conditional functions: A dynamic theory of epistemic states",
      "author" : [ "W. Spohn" ],
      "venue" : "Harper, W., and Skyrms, B., eds., Causation in Decision, Belief Change, and Statistics, volume 42 of The University of Western Ontario Series in Philosophy of Science. Springer Netherlands. 105–134.",
      "citeRegEx" : "Spohn,? 1988",
      "shortCiteRegEx" : "Spohn",
      "year" : 1988
    }, {
      "title" : "Partial Probability: Theory and Applications",
      "author" : [ "F. Voorbraak" ],
      "venue" : "Proceedings of the First International Symposium on Imprecise Probabilities and Their Applications, 360–368. url: decsai.ugr.es/ smc/isipta99/proc/073.html.",
      "citeRegEx" : "Voorbraak,? 1999",
      "shortCiteRegEx" : "Voorbraak",
      "year" : 1999
    }, {
      "title" : "Revising imprecise probabilistic beliefs in the framework of probabilistic logic programming",
      "author" : [ "A. Yue", "W. Liu" ],
      "venue" : "Proceedings of the Twenty-third AAAI Conf. on Artificial Intelligence (AAAI-08), 590–596.",
      "citeRegEx" : "Yue and Liu,? 2008",
      "shortCiteRegEx" : "Yue and Liu",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "In this paper, I provide the work of Boutilier (1998) as background, because it has several connections with and was the seed for the present work.",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "In this paper, I provide the work of Boutilier (1998) as background, because it has several connections with and was the seed for the present work. However, I do not intend simply to give a probabilistic version of his Generalized Update Semantics. Whereas Boutilier (1998) presents a model for unifying qualitative belief revision and update, I build on his work to present a unified model of belief revision and update in a stochastic (probabilistic) setting.",
      "startOffset" : 37,
      "endOffset" : 274
    }, {
      "referenceID" : 2,
      "context" : "This is in contrast to Boutilier’s “generalized update” approach, which takes a most-plausible event and most-plausible world approach. Finally, my proposal requires a trade-off factor to mix the changes in probability distribution over possible worlds brought about due to the probabilistic belief revision process and, respectively, the probabilistic belief update process. Boutilier’s model has revision and update more tightly coupled. For this reason, his approach is better called “unified” while mine is called “hybrid”. The belief change community does not study probabilistic belief update; it is studied almost exclusively in frameworks employing Bayesian conditioning – for modeling events and actions in dynamical domains (e.g., DBNs, MDPs, POMDPs) (Koller and Friedman, 2009; Poole and Mackworth, 2010, e.g.). The part of my approach responsible for updating stays within the Bayesian framework, but combines the essential elements of belief update with unobservable events and belief update as partially observable Markov decision process (POMDP) state estimation. On the other hand, there is plenty of literature on probabilistic belief revision (Voorbraak, 1999; Grove and Halpern, 1998; Kern-Isberner, 2008; Yue and Liu, 2008, e.g.). The subject is both deep and broad. There is no one accepted approach and to argue which is the best is not the focus of this paper. I shall choose one reasonable method for probabilistic belief revision suitable to the task at hand. In the first section, Boutilier’s ‘generalized update’ is reviewed. Then, in the next section, I introduce stochastic update and stochastic revision, culminating in the ‘hybrid stochastic belief change’ (HSBC) approach. The final section presents an example inspired by Boutilier’s article (1998) and analyses the results.",
      "startOffset" : 23,
      "endOffset" : 1782
    }, {
      "referenceID" : 2,
      "context" : "L (LCPL in Boutilier (1998)), and consequence operation Cn .",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "To capture this intuition, Katsuno and Mendelzon (1991) propose a family of preorders {≤w| w ∈ W}, where each ≤w is a reflexive, transitive relation over W .",
      "startOffset" : 27,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "An alternative model of revision, based on the notion of epistemic entrenchment (Gärdenfors, 1988), has a more constructive nature,” (Boutilier, 1998, p.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "“Semantically, an entrenchment relation (hence a revision function) can be modeled using an ordering on possible worlds reflecting their relative plausibility (Grove, 1988; Boutilier, 1994).",
      "startOffset" : 159,
      "endOffset" : 189
    }, {
      "referenceID" : 2,
      "context" : "“Semantically, an entrenchment relation (hence a revision function) can be modeled using an ordering on possible worlds reflecting their relative plausibility (Grove, 1988; Boutilier, 1994).",
      "startOffset" : 159,
      "endOffset" : 189
    }, {
      "referenceID" : 19,
      "context" : "However, rather than use a qualitative ranking relation, we adopt the presentation of (Spohn, 1988; Goldszmidt and Pearl, 1992) and rank all possible worlds using a κ-ranking.",
      "startOffset" : 86,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "However, rather than use a qualitative ranking relation, we adopt the presentation of (Spohn, 1988; Goldszmidt and Pearl, 1992) and rank all possible worlds using a κ-ranking.",
      "startOffset" : 86,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "An alternative model of revision, based on the notion of epistemic entrenchment (Gärdenfors, 1988), has a more constructive nature,” (Boutilier, 1998, p. 6). “Semantically, an entrenchment relation (hence a revision function) can be modeled using an ordering on possible worlds reflecting their relative plausibility (Grove, 1988; Boutilier, 1994). However, rather than use a qualitative ranking relation, we adopt the presentation of (Spohn, 1988; Goldszmidt and Pearl, 1992) and rank all possible worlds using a κ-ranking. Such a ranking κ : W → N assigns to each world a natural number reflecting its plausibility or degree of believability. If κ(w) < κ(v) then w is more plausible than v or more consistent with the agent’s beliefs. We insist that κ−1(0) 6= ∅, so that maximally plausible worlds are assigned rank 0. These maximally plausible worlds are exactly those consistent with the agent’s beliefs; that is, the epistemically possible worlds according to K are those deemed most plausible in κ (see Spohn (1988) for further details).",
      "startOffset" : 134,
      "endOffset" : 1022
    }, {
      "referenceID" : 3,
      "context" : "As explained in the introduction, my intention with this paper is not to give a probabilistic version of the Generalized Update approach (Boutilier, 1998).",
      "startOffset" : 137,
      "endOffset" : 154
    }, {
      "referenceID" : 10,
      "context" : "Boutilier motivates the need for a generalized update method which includes revision, by claiming that KM update (Katsuno and Mendelzon, 1991) is insufficient.",
      "startOffset" : 113,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "As explained in the introduction, my intention with this paper is not to give a probabilistic version of the Generalized Update approach (Boutilier, 1998). For completeness, however, I sketch the approach here covering the approach in detail would take up unnecessary space without lending much insight into my Hybrid Stochastic Belief Change approach. Boutilier motivates the need for a generalized update method which includes revision, by claiming that KM update (Katsuno and Mendelzon, 1991) is insufficient. He provides the following example adopted from Moore (1990). Suppose you want to test whether the contents of a beaker are chemically acid or base.",
      "startOffset" : 138,
      "endOffset" : 573
    }, {
      "referenceID" : 2,
      "context" : "According to Boutilier (1998), update can be defined as",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "In partially observable Markov decision process (POMDP) theory (Aström, 1965; Monahan, 1982; Lovejoy, 1991), events are actions chosen by the agent (and thus observable) and observations are hidden.",
      "startOffset" : 63,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "In partially observable Markov decision process (POMDP) theory (Aström, 1965; Monahan, 1982; Lovejoy, 1991), events are actions chosen by the agent (and thus observable) and observations are hidden.",
      "startOffset" : 63,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "In partially observable Markov decision process (POMDP) theory (Aström, 1965; Monahan, 1982; Lovejoy, 1991), events are actions chosen by the agent (and thus observable) and observations are hidden.",
      "startOffset" : 63,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "Definition 8 (Gärdenfors, 1988).",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "Imaging was introduced by Lewis (1976) as a means of revising a probability function.",
      "startOffset" : 26,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "It has also been discussed in the work of, for instance, Gärdenfors (1988); Dubois and Prade (1993); Chhogyal et al.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "It has also been discussed in the work of, for instance, Gärdenfors (1988); Dubois and Prade (1993); Chhogyal et al.",
      "startOffset" : 76,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "It has also been discussed in the work of, for instance, Gärdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015).",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "It has also been discussed in the work of, for instance, Gärdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015). The following version of imaging must not be regarded as a fundamental part of the larger belief change framework presented here; it should be regarded as a placeholder or suggestion for the ‘revision-module’ of the framework.",
      "startOffset" : 101,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "It has also been discussed in the work of, for instance, Gärdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015). The following version of imaging must not be regarded as a fundamental part of the larger belief change framework presented here; it should be regarded as a placeholder or suggestion for the ‘revision-module’ of the framework. Informally, Lewis’s original solution for accommodating contradicting evidence α is to move the probability of each world to its closest, α-world. Lewis made the strong assumption that every world has a unique closest α-world. More general versions of imaging allow worlds to have several, equally proximate, closest worlds. Gärdenfors (1988) calls one of his generalizations of Lewis’s imaging general imaging.",
      "startOffset" : 101,
      "endOffset" : 718
    }, {
      "referenceID" : 6,
      "context" : "Gärdenfors (1988) proposed six rationality postulates for probabilistic belief revision.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "The example domain is adapted from one of the domains in the article of Boutilier (1998) – here though, worlds are associated with probabilities, not plausibility ranks.",
      "startOffset" : 72,
      "endOffset" : 89
    } ],
    "year" : 2016,
    "abstractText" : "I propose a framework for an agent to change its probabilistic beliefs when a new piece of propositional information α is observed. Traditionally, belief change occurs by either a revision process or by an update process, depending on whether the agent is informed with α in a static world or, respectively, whether α is a ‘signal’ from the environment due to an event occurring. Boutilier suggested a unified model of qualitative belief change, which “combines aspects of revision and update, providing a more realistic characterization of belief change.” In this paper, I propose a unified model of quantitative belief change, where an agent’s beliefs are represented as a probability distribution over possible worlds. As does Boutilier, I take a dynamical systems perspective. The proposed approach is evaluated against several rationality postulated, and some properties of the approach are worked out. Information acquired can be due to evolution of the world or revelation about the world. That is, one may notice via some ‘signal’ generated by the changing environment that the environment has changed, or, one may be informed by an independent agent in a static environment that some ‘fact’ holds. In the present work, I deal with belief change of agents who handle uncertainty by maintaining a probability distribution over possible situations. The agents in this framework also have models for nondeterministic events, and noisy observations. Noisy observation models can model imperfect sensory equipment for receiving environmental signals, but they can also model untrustworthy informants in a static world. In this paper, I provide the work of Boutilier (1998) as background, because it has several connections with and was the seed for the present work. However, I do not intend simply to give a probabilistic version of his Generalized Update Semantics. Whereas Boutilier (1998) presents a model for unifying qualitative belief revision and update, I build on his work to present a unified model of belief revision and update in a stochastic (probabilistic) setting. I also take a dynamical systems perspective, like him. Due to my quantitative approach, an agent can maintain a probability distribution Copyright c © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. over the worlds it believes possible, using an expectation semantics of change. This is in contrast to Boutilier’s “generalized update” approach, which takes a most-plausible event and most-plausible world approach. Finally, my proposal requires a trade-off factor to mix the changes in probability distribution over possible worlds brought about due to the probabilistic belief revision process and, respectively, the probabilistic belief update process. Boutilier’s model has revision and update more tightly coupled. For this reason, his approach is better called “unified” while mine is called “hybrid”. The belief change community does not study probabilistic belief update; it is studied almost exclusively in frameworks employing Bayesian conditioning – for modeling events and actions in dynamical domains (e.g., DBNs, MDPs, POMDPs) (Koller and Friedman, 2009; Poole and Mackworth, 2010, e.g.). The part of my approach responsible for updating stays within the Bayesian framework, but combines the essential elements of belief update with unobservable events and belief update as partially observable Markov decision process (POMDP) state estimation. On the other hand, there is plenty of literature on probabilistic belief revision (Voorbraak, 1999; Grove and Halpern, 1998; Kern-Isberner, 2008; Yue and Liu, 2008, e.g.). The subject is both deep and broad. There is no one accepted approach and to argue which is the best is not the focus of this paper. I shall choose one reasonable method for probabilistic belief revision suitable to the task at hand. In the first section, Boutilier’s ‘generalized update’ is reviewed. Then, in the next section, I introduce stochastic update and stochastic revision, culminating in the ‘hybrid stochastic belief change’ (HSBC) approach. The final section presents an example inspired by Boutilier’s article (1998) and analyses the results. Some proofs of propositions are omitted to save space; they are available on request. Boutilier’s Generalized Update I use Boutilier’s notation and descriptions, except that I am more comfortable with α and β to represent sentences, instead of A and B. It is assumed that an agent has a deductively closed belief set K, a set of sentences drawn from some logical language reflecting the agent’s beliefs about the current state of the world. For ease of presentation, I assume ar X iv :1 60 4. 02 12 6v 1 [ cs .A I] 7 A pr 2 01 6 a logically finite, classical propositional language, denoted L (LCPL in Boutilier (1998)), and consequence operation Cn . The belief set K will often be generated by some finite knowledge base KB (i.e., K = Cn(KB)). The identically true and false propositions are denoted > and ⊥, respectively. Given a set of possible worlds W (or valuations over L) and α ∈ L, the set of α-worlds, that is, the elements of W satisfying α, is denoted by ||α||. The worlds satisfying all sentences in a set K is denoted ||K||.",
    "creator" : "LaTeX with hyperref package"
  }
}