{
  "name" : "1611.08944.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Nonparametric General Reinforcement Learning",
    "authors" : [ "Jan Leike" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Nonparametric General Reinforcement Learning\nJan Leike\nA thesis submitted for the degree of Doctor of Philosophy\nat the Australian National University\nNovember 2016\nar X\niv :1\n61 1.\n08 94\n4v 1\n[ cs\n.A I]\n2 8\nN ov\n2 01\n6\n© Jan Leike\nThis work is licensed under the Creative Commons Attribution 4.0 International License\nNo reinforcement learners were harmed in the making of this thesis.\nExcept where otherwise indicated, this thesis is my own original work.\nJan Leike 29 November 2016"
    }, {
      "heading" : "Acknowledgements",
      "text" : "There are many without whom this thesis would not have been possible. I sincerely hope that this page is not the way they learn how grateful I am to them. I thank in particular . . .\n. . . first and foremost, Marcus Hutter: he is an amazing supervisor; always very supportive of my (unusual) endeavors, spent countless hours reading my drafts with a impressive attention to detail. I am also grateful to him for forcing me to be absolutely rigorous in my mathematical arguments, and, of course, for developing the theory of universal AI without which this thesis would not have existed. I could not have picked a better supervisor.\n. . . the Australian National University for granting me scholarships that let me pursue my academic interests unrestricted and without any financial worries.\n. . . Csaba Szepesvári and the University of Alberta for hosting me for three months.\n. . . Matthias Heizmann and the University of Freiburg for hosting me while I was traveling in Europe.\n. . . the Machine Intelligence Research Institute for enabling me to run MIRIx research workshops.\n. . . CCR, UAI, Google DeepMind, ARC, MIRI, and FHI for supporting my travel.\n. . . Tor Lattimore for numerous explanations, discussions, and pointers that left me with a much deeper understanding of the theory of reinforcement learning.\n. . . Laurent Orseau for interesting discussions, encouragement, and for sharing so many intriguing ideas.\n. . . my fellow students: Mayank Daswani, Tom Everitt, Daniel Filan, Roshan Shariff, Tian Kruger, Emily Cutts Worthington, Buck Shlegeris, Jarryd Martin, John Aslanides, Alexander Mascolo, and Sultan Javed for so many interesting discussions and for being awesome friends. I especially thank Daniel, Emily, Mayank, and Buck for encouraging me to read more of Less Wrong and Slate Star Codex.\n. . . Tosca Lechner for studying statistics with me despite so many scheduling difficulties across all these time zones.\n. . . Tom Sterkenburg, Christian Kamm, Alexandra Surdina, Freya Fleckenstein, Peter Sunehag, Tosca Lechner, Ines Nikolaus, Laurent Orseau, John Aslanides, and especially Daniel Filan for proofreading parts of this thesis.\n. . . the CSSA for being a lovely bunch that made my stay in Australia feel less isolated.\n. . . my family for lots of love and support, and for tolerating my long absences from Europe.\nAbstract\nReinforcement learning problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider reinforcement learning in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm: we assume the data is drawn from an unknown source that belongs to a known countable class of candidates.\nFirst, we consider the passive (sequence prediction) setting, learning from data that is not independent and identically distributed. We collect results from artificial intelligence, algorithmic information theory, and game theory and put them in a reinforcement learning context: they demonstrate how an agent can learn the value of its own policy.\nNext, we establish negative results on Bayesian reinforcement learning agents, in particular AIXI. We show that unlucky or adversarial choices of the prior cause the agent to misbehave drastically. Therefore Legg-Hutter intelligence and balanced Pareto optimality, which depend crucially on the choice of the prior, are entirely subjective. Moreover, in the class of all computable environments every policy is Pareto optimal. This undermines all existing optimality properties for AIXI.\nHowever, there are Bayesian approaches to general reinforcement learning that satisfy objective optimality guarantees: We prove that Thompson sampling is asymptotically optimal in stochastic environments in the sense that its value converges to the value of the optimal policy. We connect asymptotic optimality to regret given a recoverability assumption on the environment that allows the agent to recover from mistakes. Hence Thompson sampling achieves sublinear regret in these environments.\nAIXI is known to be incomputable. We quantify this using the arithmetical hierarchy, and establish upper and corresponding lower bounds for incomputability. Further, we show that AIXI is not limit computable, thus cannot be approximated using finite computation. However there are limit computable ε-optimal approximations to AIXI. We also derive computability bounds for knowledge-seeking agents, and give a limit computable weakly asymptotically optimal reinforcement learning agent.\nFinally, our results culminate in a formal solution to the grain of truth problem: A Bayesian agent acting in a multi-agent environment learns to predict the other agents’ policies if its prior assigns positive probability to them (the prior contains a grain of truth). We construct a large but limit computable class containing a grain of truth and show that agents based on Thompson sampling over this class converge to play ε-Nash equilibria in arbitrary unknown computable multi-agent environments.\nix\nx Keywords. Bayesian methods, sequence prediction, merging, general reinforcement learning, universal artificial intelligence, AIXI, Thompson sampling, knowledge-seeking agents, Pareto optimality, intelligence, asymptotic optimality, computability, reflective oracle, grain of truth problem, Nash equilibrium.\nContents\nTitle Page i\nAbstract ix\nContents xiii\nList of Figures xv\nList of Tables xvii"
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.1.1 Narrow Reinforcement Learning . . . . . . . . . . . . . . . . . . . 3 1.1.2 Deep Q-Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.1.3 General Reinforcement Learning . . . . . . . . . . . . . . . . . . 6\n1.2 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
    }, {
      "heading" : "2 Preliminaries 15",
      "text" : "2.1 Measure Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2 Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.4 Algorithmic Information Theory . . . . . . . . . . . . . . . . . . . . . . 20"
    }, {
      "heading" : "3 Learning 23",
      "text" : "3.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2 Compatibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.3 Martingales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.4 Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.4.1 Strong Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.4.2 Weak Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.4.3 Almost Weak Merging . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.5 Predicting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.5.1 Dominance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.5.2 Absolute Continuity . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.5.3 Dominance with Coefficients . . . . . . . . . . . . . . . . . . . . . 40 3.6 Learning with Algorithmic Information Theory . . . . . . . . . . . . . . 41 3.6.1 Solomonoff Induction . . . . . . . . . . . . . . . . . . . . . . . . . 41\nxi\nxii Contents\n3.6.2 The Speed Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.6.3 Universal Compression . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44"
    }, {
      "heading" : "4 Acting 49",
      "text" : "4.1 The General Reinforcement Learning Problem . . . . . . . . . . . . . . . 50\n4.1.1 Discounting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.1.2 Implicit Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.1.3 Typical Environment Classes . . . . . . . . . . . . . . . . . . . . 54\n4.2 The Value Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 4.2.1 Optimal Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 4.2.2 Properties of the Value Function . . . . . . . . . . . . . . . . . . 58 4.2.3 On-Policy Value Convergence . . . . . . . . . . . . . . . . . . . . 59 4.3 The Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.3.1 Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.3.2 Knowledge-Seeking Agents . . . . . . . . . . . . . . . . . . . . . . 63 4.3.3 BayesExp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.3.4 Thompson Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 65"
    }, {
      "heading" : "5 Optimality 67",
      "text" : "5.1 Pareto Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 5.2 Bad Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n5.2.1 The Indifference Prior . . . . . . . . . . . . . . . . . . . . . . . . 70 5.2.2 The Dogmatic Prior . . . . . . . . . . . . . . . . . . . . . . . . . 71 5.2.3 The Gödel Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5.3 Bayes Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 5.4 Asymptotic Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.4.1 Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 5.4.2 BayesExp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 5.4.3 Thompson Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 84 5.4.4 Almost Sure in Cesàro Average vs. in Mean . . . . . . . . . . . . 89\n5.5 Regret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 5.5.1 Sublinear Regret in Recoverable Environments . . . . . . . . . . 91 5.5.2 Regret of the Optimal Policy and Thompson sampling . . . . . . 95 5.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 5.6.1 The Optimality of AIXI . . . . . . . . . . . . . . . . . . . . . . . 96 5.6.2 Natural Universal Turing Machines . . . . . . . . . . . . . . . . . 97 5.6.3 Asymptotic Optimality . . . . . . . . . . . . . . . . . . . . . . . . 98 5.6.4 The Quest for Optimality . . . . . . . . . . . . . . . . . . . . . . 99"
    }, {
      "heading" : "6 Computability 101",
      "text" : "6.1 Background on Computability . . . . . . . . . . . . . . . . . . . . . . . . 103\n6.1.1 The Arithmetical Hierarchy . . . . . . . . . . . . . . . . . . . . . 103 6.1.2 Computability of Real-valued Functions . . . . . . . . . . . . . . 103\nContents xiii\n6.2 The Complexity of Solomonoff Induction . . . . . . . . . . . . . . . . . . 105 6.3 The Complexity of AINU, AIMU, and AIXI . . . . . . . . . . . . . . . . 108\n6.3.1 Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 6.3.2 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Iterative Value Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.5 The Complexity of Knowledge-Seeking . . . . . . . . . . . . . . . . . . . 122 6.6 A Limit Computable Weakly Asymptotically Optimal Agent . . . . . . . 122 6.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123"
    }, {
      "heading" : "7 The Grain of Truth Problem 127",
      "text" : "7.1 Reflective Oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.1.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 7.1.2 A Limit Computable Reflective Oracle . . . . . . . . . . . . . . . 131 7.1.3 Proof of Theorem 7.7 . . . . . . . . . . . . . . . . . . . . . . . . . 132\n7.2 A Grain of Truth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 7.2.1 Reflective Bayesian Agents . . . . . . . . . . . . . . . . . . . . . 135 7.2.2 Reflective-Oracle-Computable Policies . . . . . . . . . . . . . . . 136 7.2.3 Solution to the Grain of Truth Problem . . . . . . . . . . . . . . 137 7.3 Multi-Agent Environments . . . . . . . . . . . . . . . . . . . . . . . . . . 137 7.4 Informed Reflective Agents . . . . . . . . . . . . . . . . . . . . . . . . . 140 7.5 Learning Reflective Agents . . . . . . . . . . . . . . . . . . . . . . . . . . 141 7.6 Impossibility Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 7.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144"
    }, {
      "heading" : "8 Conclusion 147",
      "text" : "Measures and Martingales 151\nBibliography 155\nList of Notation 171\nIndex 175\nxiv Contents\nList of Figures\n1.1 Selection of Atari 2600 video games . . . . . . . . . . . . . . . . . . . . . 13\n3.1 Properties of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.1 The dualistic agent model . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n5.1 Legg-Hutter intelligence measure . . . . . . . . . . . . . . . . . . . . . . 77 5.2 Relationship between different types of asymptotic optimality . . . . . . 80\n6.1 Definition of conditional M as a ∆02-formula . . . . . . . . . . . . . . . . 106 6.2 Environment from the proof of Theorem 6.15 . . . . . . . . . . . . . . . 111 6.3 Environment from the proof of Theorem 6.16 . . . . . . . . . . . . . . . 113 6.4 Environment from the proof of Theorem 6.17 . . . . . . . . . . . . . . . 114 6.5 Environment from the proof of Proposition 6.19 . . . . . . . . . . . . . . 117 6.6 Environment from the proof of Theorem 6.22 . . . . . . . . . . . . . . . 119 6.7 Environment from the proof of Theorem 6.23 . . . . . . . . . . . . . . . 121\n7.1 Answer options of a reflective oracle . . . . . . . . . . . . . . . . . . . . 131 7.2 The multi-agent model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\nxv\nxvi LIST OF FIGURES\nList of Tables\n1.1 Assumptions in reinforcement learning . . . . . . . . . . . . . . . . . . . 7 1.2 List of publications by chapter . . . . . . . . . . . . . . . . . . . . . . . 11 1.3 List of publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 Examples of learning distributions . . . . . . . . . . . . . . . . . . . . . 45 3.2 Summary on properties of learning . . . . . . . . . . . . . . . . . . . . . 45\n4.1 Discount functions and their effective horizons . . . . . . . . . . . . . . . 53\n5.1 Types of asymptotic optimality . . . . . . . . . . . . . . . . . . . . . . . 79 5.2 Compiler sizes of the UTMs of bad priors . . . . . . . . . . . . . . . . . 98 5.3 Notions of optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n6.1 Computability results on Solomonoff’s prior . . . . . . . . . . . . . . . . 102 6.2 Computability results for different agent models . . . . . . . . . . . . . . 102 6.3 Computability of real-valued functions . . . . . . . . . . . . . . . . . . . 104 6.4 Computability results for the iterative value function . . . . . . . . . . . 116\n7.1 Terminology dictionary between reinforcement learning and game theory. 128\nxvii\nxviii LIST OF TABLES\nChapter 1\nIntroduction\nEverything I did was for the glamor, the money, and the sex. — Albert Einstein\nAfter the early enthusiastic decades, research in artificial intelligence (AI) now mainly aims at specific domains: playing games, mining data, processing natural language, recognizing objects in images, piloting robots, filtering email, and many others (Russell and Norvig, 2010). Progress on particular domains has been remarkable, with several high-profile breakthroughs: The chess world champion Garry Kasparov was defeated by the computer program Deep Blue in 1997 (IBM, 2012a). In 2011 the world’s best Jeopardy! players were defeated by the computer program Watson (IBM, 2012b). As of 2014 Google’s self-driving cars completed over a million kilometers autonomously on public roads (Google, 2014). Finally, in 2016 Google DeepMind’s AlphaGo beat Lee Sedol, one of the world’s best players, at the board game Go (Google, 2016).\nWhile these advancements are very impressive, they are highly-specialized algorithms tailored to their domain of expertise. Outside that domain these algorithms perform very poorly: AlphaGo cannot play chess, Watson cannot drive a car, and DeepBlue cannot answer natural language queries. Solutions in one domain typically do not generalize to other domains and no single algorithm performs well in more than one of them. We classify these kinds of algorithms as narrow AI .\nThis thesis is not about narrow AI. We expect progress on narrow AI to continue and even accelerate, taking the crown of human superiority in domain after domain. But this is not the ultimate goal of artificial intelligence research. The ultimate goal is to engineer a mind—to build a machine that can learn to do all tasks that humans can do, at least as well as humans do them. We call such a machine human-level AI (HLAI) if it performs at human level and strong AI if it surpasses human level. This thesis is about strong AI.\nThe goal of developing HLAI has a long tradition in AI research and was explicitly part of the 1956 Dartmouth conference that gave birth to the field of AI (McCarthy et al., 1955):\nWe propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An\n1"
    }, {
      "heading" : "2 Introduction",
      "text" : "attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.\nIn hindsight this proposal reads vastly overconfident, and disappointment was inevitable. Making progress on these problems turned out to be a lot harder than promised, and over the last decades any discussion of research targeting HLAI has been avoided by serious researchers in the field. This void was filled mostly by crackpots, which tainted the reputation of HLAI research even further. However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously. Even more: the explicit motto of Google DeepMind, one of today’s leading AI research centers, is to “solve intelligence.”"
    }, {
      "heading" : "1.1 Reinforcement Learning",
      "text" : "The best formal model for strong AI we currently have is reinforcement learning (RL). Reinforcement learning studies algorithms that learn to act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesvári, 2010; Wiering and van Otterlo, 2012). Without knowing the structure of the environments or the goal, an agent has to learn what to do through the carrot-and-stick approach: it receives a reward in form of a numeric feedback signifying how well it is currently doing; from this signal the agent has to figure out autonomously what to do. More specifically, in a general reinforcement learning problem an agent interacts sequentially with an unknown environment : in every time step the agent chooses an action and receives a percept consisting of an observation and a real-valued reward . The sequence of past actions and percepts is the history . The goal in reinforcement learning is to maximize cumulative (discounted) rewards (this setup is described formally in Section 4.1).\nA central problem in reinforcement learning is the balance between exploration and exploitation: should the agent harvest rewards in the regions of the environment that it currently knows (exploitation) or try discovering more profitable regions (exploration)? Exploration is costly and dangerous: it forfeits rewards that could be had right now, and it might lead into traps from which the agent cannot recover. However, exploration may pay off in the long run. Generally, it is not clear how to make this tradeoff (see Section 5.6).\nReinforcement learning algorithms can be categorized by whether they learn onpolicy or off-policy . Learning on-policy means learning the value of the policy that the agent currently follows. Typically, the policy is slowly improved while learning, like SARSA (Sutton and Barto, 1998). In contrast, learning off-policy means following one policy but learning the value of another policy (typically the optimal policy), like Q-learning (Watkins and Dayan, 1992). Off-policy methods are more difficult to handle\n§1.1 Reinforcement Learning 3\nin practice (see the discussion on function approximation below) but tend to be more data-efficient since samples from an old policy do not have to be discarded.\nReinforcement learning has to be distinguished from planning . In a planning problem we are provided with the true environment and are tasked with finding an optimal policy. Mathematically it is clear what the optimal policy is, the difficulty stems from finding a reasonable solution with limited computation. Reinforcement learning is fundamentally more difficult because the true environment is unknown and has to be learned from observation. This enables two approaches: we could learn a model of the true environment and then use planning techniques within that model; this is the modelbased approach. Alternatively, we could learn an optimal policy directly or through an intermediate quantity (typically the value function); this is the model-free approach. Model-based methods tend to be more data-efficient but also computationally more expensive. Therefore most algorithms used in practice (Q-learning and SARSA) are model-free."
    }, {
      "heading" : "1.1.1 Narrow Reinforcement Learning",
      "text" : "In the reinforcement learning literature it is typically assumed that the environment is a Markov decision process (MDP), i.e., the next percept only depends on the last percept and action and is independent of the rest of the history (see Section 4.1.3). In an MDP, percepts are usually called states. This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).\nMoreover, for MDPs various learning guarantees have been proved in the literature. First, there are bounds on the agent’s regret , the difference between the obtained rewards and the rewards of the optimal policy. Auer et al. (2009) derive the regret bound Õ(dS √ At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given ε and δ, a reinforcement learning algorithm is said to have sample complexity C(ε, δ) iff it is ε-suboptimal for at most C(ε, δ) time steps with probability at least 1 − δ (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRLγ (Auer et al., 2009) with geometric discounting with discount rate γ and derive the currently best-known PAC bound of Õ(−T/(ε2(1 − γ)3) log δ) where T is the number of non-zero transitions in the MDP.\nTypically, algorithms for MDPs rely on visiting every state multiple times (or even infinitely often), which becomes infeasible for large state spaces (e.g. a video game screen consisting of millions of pixels). In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998). Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995). A recent breakthrough was made by Mahmood et al. (2015) and Yu (2015)"
    }, {
      "heading" : "4 Introduction",
      "text" : "with their emphatic TD algorithm that converges off-policy. For nonlinear function approximation no convergence guarantee is known.\nAmong the historical successes of reinforcement learning is autonomous helicopter piloting (Kim et al., 2003) and TD-Gammon, a backgammon algorithm that learned through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al., 2016)."
    }, {
      "heading" : "1.1.2 Deep Q-Networks",
      "text" : "The current state of the art in reinforcement learning challenges itself to playing simple video games. Video games are an excellent benchmark because they come readily with the reward structure provided: the agent’s rewards are the change in the game score. Without prior knowledge of any aspect of the game, the agent needs to learn to score as many points in the game as possible from looking only at raw pixel data (sometimes after some preprocessing).\nThis approach to general AI is in accordance with the definition of intelligence given by Legg and Hutter (2007b):\nIntelligence measures an agent’s ability to achieve goals in a wide range of environments.\nIn reinforcement learning the definition of the goal is very flexible, and provided by the rewards. Moreover, a diverse selection of video games arguably constitutes a ‘wide range of environments.’\nA popular such selection is the Atari 2600 video game console (Bellemare et al., 2013). There are hundreds of games released for this platform, with very diverse challenges: top-down shooting games such as Space Invaders, ball games such as Pong, agility-based games such as Boxing or Gopher, tactical games such as Ms. Pac-Man, and maze games such as Montezuma’s Revenge. An overview over some of the games is given in Figure 1.1 on page 13.\nMnih et al. (2013, 2015) introduce the deep Q-network (DQN) algorithm, combining Q-learning with nonlinear function approximation through convolutional neural networks. DQN achieves 75% of the performance of a human game tester on 29 of 49 Atari games. The two innovations that made this breakthrough possible are (1) using a not so recent target Q-function in the TD update and (2) experience replay. For experience replay, a set of recent state transitions is retained and the network is regularly retrained on random samples from these old transitions.1\nDQN rides the wave of success of deep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016). Deep learning refers to the training of artificial neural networks with several layers. This allows them to automatically learn higher-level abstractions from data. Deep neural networks are conceptionally simple and have been studied since the inception of AI; only recently has computation power become cheap enough to train them effectively. Recently deep neural networks have taken the top of the machine learning benchmarks by storm (LeCun et al., 2015, and references therein):\n1The slogan for experience replay should be ‘regularly retrained randomly on retained rewards’.\n§1.1 Reinforcement Learning 5\nThese methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.\nSince the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al., 2016), and improvements to the neural network architecture (Wang et al., 2016). TheQvalues learned by DQN’s neural networks are intransparent to inspection; Zahavy et al. (2016) use visualization techniques on the Q-value networks. Finally, Liang et al. (2016) managed to reproduce DQN’s success using only linear function approximation (no neural networks). The key is a selection of features similar to the ones produced by DQN’s convolutional neural networks.\nRegardless of its success, the DQN algorithm fundamentally falls short of the requirements for strong AI: Q-learning with function approximation is targeted at solving large-state (fully observable) Markov decision processes. In particular, it does not address the following challenges.\n• Partial observability. All games in the ATARI framework are fully observable (except for Montezuma’s revenge): all information relevant to the state of the game is visible on the screen at all times (when using the four most recent frames).\nHowever, the real world is only partially observable. For example, when going to the supermarket you have to remember what you wanted to buy because you currently cannot observe which items you are missing at home. A strong AI needs to have memory and be able to remember things that happened in the past (rather than only learning from it).\nAn obvious approach to equip DQN with memory is to use recurrent neural networks instead of simple feedforward neural networks (Heess et al., 2015). Hausknecht and Stone (2015) show that this enables the agent to play the games when using only a single frame as input. However, it is currently unclear whether recurrent neural networks are powerful enough to learn long-term dependencies in the data (Bengio et al., 1994).\n• Directed exploration. DQN fails in games with delayed rewards. For example, in Montezuma’s Revenge the agent needs to avoid several obstacles to get to a key before receiving the first reward. DQN fails to score any rewards in this environment. This is not surprising: the typical approach for reinforcement learning, to use ε-exploration for which the agent chooses actions at random with a certain probability, is insufficient for exploring complex environments; the probability of random walking into the first reward is just too low.\nInstead we need a more targeted exploration approach that aims at understanding the environment in a structured manner. Theoretical foundations are provided"
    }, {
      "heading" : "6 Introduction",
      "text" : "by knowledge-seeking agents (Orseau, 2011, 2014a; Orseau et al., 2013). Kulkarni et al. (2016) introduce a hierarchical approach based on intrinsic motivation to improve DQN’s exploration and manage to score points in Montezuma’s Revenge. However, their approach relies on quite a bit of visual preprocessing and domain knowledge.\n• Non-ergodicity. When losing in an Atari game, the agent always gets to play the same game again. From the agent’s perspective, it has not actually failed, it just gets transported back to the starting state. Because of this, there are no strong incentives to be careful when exploring the environment: there can be no bad mistakes that make recovery impossible.\nHowever, in the real world some actions are irreversibly bad. If the robot drives off a cliff it can be fatally damaged and cannot learn from the mistake. The real world is full of potentially fatal mistakes (e.g. crossing the street at the wrong time) and for humans, natural reflexes and training by society make sure that we are very confident of what situations to avert. This is crucial, as some mistakes must be avoided without any training examples. Current reinforcement learning algorithms only learn about bad states by visiting them.\n• Wireheading. The goal of reinforcement learning is to maximize rewards. When playing a video game the most efficient way to get rewards is to increase the game score. However, when a reinforcement learning algorithm is acting in the real world, theoretically it can change its own hard- and software. In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014). Consequently the agent no longer pursues the designers’ originally intended goals and instead only attempts to protect its own existence. The name wireheading was established by analogy to a biology experiment by Olds and Milner (1954) in which rats had a wire embedded into the reward center of their brain that they could then stimulate by the push of a button.\nToday’s reinforcement learning algorithms usually do not have access to their own internal workings, but more importantly they are not smart enough to understand their own architecture. They simply lack the capability to wirehead. But as we increase their capability, wireheading will increasingly become a challenge for reinforcement learning."
    }, {
      "heading" : "1.1.3 General Reinforcement Learning",
      "text" : "A theory of strong AI cannot make some of the typical assumptions. Environments are partially observable, so we are dealing with partially observable Markov decision processes (POMDPs). The POMDP’s state space does not need to be finite. Moreover, the environment may not allow recovery from mistakes: we do not assume ergodicity or weak communication (not every POMDP state has to be reachable from every other state). So in general, our environments are infinite-state non-ergodic POMDPs. Table 1.1 lists the assumptions that are typical but we do not make.\nLearning POMDPs is a lot harder, and only partially successful attempts have been made: through predictive state representations (Singh et al., 2003, 2004), and Bayesian methods (Doshi-Velez, 2012). A general approach is feature reinforcement learning (Hutter, 2009c,d), which aims to reduce the general reinforcement learning problem to an MDP by aggregating histories into states. The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015). However, Hutter (2014) managed to derive strong bounds relating the optimal value function of the aggregated MDP to the value function of the original process even if the latter violates the Markov condition.\nA full theoretical approach to the general reinforcement learning problem is given by Hutter (2000, 2001a, 2002a, 2003, 2005, 2007a, 2012b). He introduces the Bayesian RL agent AIXI building on the theory of sequence prediction by Solomonoff (1964, 1978). Based in algorithmic information theory, Solomonoff’s prior draws from famous insights by William of Ockham, Sextus Epicurus, Alan Turing, and Andrey Kolmogorov (Rathmanner and Hutter, 2011). AIXI uses Solomonoff’s prior over the class of all computable environments and acts to maximize Bayes-expected rewards. We formally introduce Solomonoff’s theory of induction in Chapter 3 and AIXI in Section 4.3.1. See also Legg (2008) for an accessible introduction to AIXI.\nA typical optimality property in general reinforcement learning is asymptotic optimality (Lattimore and Hutter, 2011): as time progresses the agent converges to achieve the same rewards as the optimal policy. Asymptotic optimality is usually what is meant by “Q-learning converges” (Watkins and Dayan, 1992) or “TD learning converges” (Sutton, 1988). Orseau (2010, 2013) showed that AIXI is not asymptotically optimal. Yet asymptotic optimality in the general setting can be achieved through optimism (Sunehag and Hutter, 2012a,b, 2015), Thompson sampling (Section 5.4.3), or an extra exploration component on top of AIXI (Lattimore, 2013, Ch. 5).\nIn our setting, learning the environment does not just involve learning a fixed finite set of parameters; the real world is too complicated to fit into a template. Therefore we fall back on the nonparametric approach where we start with an infinite but countable class of candidate environments. Our only assumption is that the true environment is contained in this class (the realizable case). As long as this class of environments is large enough (such as for the class of all computable environments), this assumption is\n8 Introduction\nrather weak."
    }, {
      "heading" : "1.2 Contribution",
      "text" : "The goal of this thesis is not to increase AI capability. As such, we are not trying to improve on the state of the art, and we are not trying to derive practical algorithms. Instead, the emphasis of this thesis is to further our understanding of general reinforcement learning and thus strong AI. How a future implementation of strong AI will actually work is in the realm of speculation at this time. Therefore we should make as few and as weak assumptions as possible.\nWe disregard computational constraints in order to focus on the fundamental underlying problems. This is unrealistic, of course. With unlimited computation power many traditional AI problems become trivial: playing chess, Go, or backgammon can be solved by exhaustive expansion of the game tree. But the general RL problem does not become trivial: the agent has to learn the environment and balance between exploration and exploitation. That being said, the algorithms that we study do have a relationship with algorithms being used in practice and our results can and should educate implementation.\nOn a high level, our insights can be viewed from three different perspectives.\n• Philosophically. Concisely, our understanding of strong AI can be summarized as follows.\nintelligence = learning + acting (1.1)\nHere, intelligence refers to an agent that optimizes towards some goal in accordance with the definition by Legg and Hutter (2007b). For learning we distinguish two (very related) aspects: (1) arriving at accurate beliefs about the future and (2) making accurate predictions about the future. Of course, the former implies the latter: if you have accurate beliefs, then you can also make good predictions. For RL accurate beliefs is what we care about because they enable us to plan for the future. Learning is a passive process that only observes the data and does not interfere with its generation. In particular, learning does not require a goal. With acting we mean the selection of actions in pursuit of some goal. This goal can be reward maximization as in reinforcement learning, understanding the environment as for knowledge-seeking agents, or something else entirely. Together they enable an agent to learn the environment’s behavior in response to itself (on-policy learning) and to choose a policy that furthers its goal. We discuss the formal aspects of learning in Chapter 3 and some approaches to acting in Chapter 4.\nGiven infinite computational resources, learning is easy and Solomonoff induction provides a complete theoretical solution. However, acting is not straightforward. We show that in contrast to popular belief, AIXI, the natural extension of Solomonoff induction to reinforcement learning, does not provide the objectively best answer to this question. We discuss some alternatives and their problems in\n§1.2 Contribution 9\nChapter 5. Unfortunately, the general question of how to act optimally remains open.\nAIXItl (Hutter, 2005, Ch. 7.2) is often mentioned as a computable approximation to AIXI. But AIXItl does not converge to AIXI in the limit. Inspired by Hutter search (Hutter, 2002b), it relies on an automated theorem prover to find the provably best policy computable in time t with a program of length ≤ l. In contrast to AIXI, which only requires the choice of universal Turing machine, proof search requires an axiom system that must not be too weak or too strong. In Section 5.2.3 we discuss some of the problems with AIXItl. Moreover, in Corollary 6.13 we show that ε-optimal AIXI is limit computable, which shows that AIXI can be computably approximated by running this algorithm for a fixed number of time steps or until a timeout is reached. While neither AIXItl nor this AIXI approximation algorithm are practically feasible, the latter is a better example for a computable strong AI.\nIn our view, AIXI should be taken as a descriptive rather than prescriptive model. It is descriptive as an abstraction from an actual implementation of strong AI where we ignore all the details of the learning algorithm and the computational approximations of choosing how to act. It should not be viewed as a prescription of how strong AI should be built and AIXI approximations (Veness et al., 2011, 2015) are easily outperformed by neural-network-based approaches (Mnih et al., 2015).\n• Mathematically. Some of the proof techniques we employ are novel and could be used to analyze other algorithms. Examples include the proofs for the lower bounds on the computability results (Section 6.3.2) and to a lesser extent the upper bounds (Section 6.3.1), which should work analogously for a wide range of algorithms. Furthermore, the proof of the asymptotic optimality of Thompson sampling (Theorem 5.25) brings together a variety of mathematical tools from measure theory, probability theory, and stochastic processes.\nNext, the recoverability assumption (Definition 5.31) is a novel technical assumption on the environment akin to ergodicity and weak communication in finite-state environments. It is more general, yet mathematically simple and works for arbitrary environments. This assumption turns out to be what we need to prove the connection from asymptotic optimality to sublinear regret in Section 5.5.\nMoreover, we introduce the use of the recursive instead of the iterative value function (Section 6.4). The iterative value function is the natural extension of expectimax search to the sequential setting and was originally used by Hutter (2005, Sec. 5.5). Yet it turned out to be an incorrect and inconvenient definition: it does not correctly maximize expected rewards (Proposition 6.19) and it is not limit computable (Theorem 6.22 and Theorem 6.23). However, this is only a minor technical correction.\nFinally, this work raises new mathematically intriguing questions about the properties of reflective oracles (Section 7.1)."
    }, {
      "heading" : "10 Introduction",
      "text" : "• Practically. One insight from this thesis is regarding the effective horizon. In practice geometric discounting is ubiquitous which has a constant effective horizon. However, when facing a finite horizon problem or an episodic task, sometimes the effective horizon changes. One lesson from our result on Thompson sampling (Section 5.4.3 and Section 5.5) is that you should explore for an effective horizon instead of using ε-greedy. While the latter exploration method is often used in practice, it has proved ineffective in environments with delayed rewards (see Section 1.1.2).\nFurthermore, our application of reinforcement learning results to game theory in Chapter 7 reinforces this trend to solve game theory problems (Tesauro, 1995; Bowling and Veloso, 2001; Busoniu et al., 2008; Silver et al., 2016; Heinrich and Silver, 2016; Foerster et al., 2016, and many more). In particular, the approximation algorithm for reflective oracles (Section 7.1.3) could guide future applications for computing Nash equilibria (see also Fallenstein et al., 2015b).\nOn a technical level, we advance the theory of general reinforcement learning. In its center is the Bayesian reinforcement learning agent AIXI. AIXI is meant as an answer to the question of how to do general RL disregarding computational constraints. We analyze the computational complexity of AIXI and related agents in Chapter 6 and show that even with an infinite horizon AIXI can be computationally approximated with a regular Turing machine (Section 6.3.1). We also derive corresponding lower bounds for most of our upper bounds (Section 6.3.2).\nChapter 5 is about notions of optimality in general reinforcement learning. We dispel AIXI’s status as the gold standard for reinforcement learning. Hutter (2002a) showed that AIXI is Pareto optimal, balanced Pareto optimal, and self-optimizing. Orseau (2013) established that AIXI does not achieve asymptotic optimality in all computable environments (making the self-optimizing result inapplicable to this general environment class). In Section 5.1 we show that every policy is Pareto optimal and in Section 5.3 we show that balanced Pareto optimality is highly subjective, depending on the choice of the prior; bad choices for priors are discussed in Section 5.2. Notable is the dogmatic prior that locks a Bayesian reinforcement learning agent into a particular (bad) policy as long as this policy yields some rewards. Our results imply that there are no known nontrivial and non-subjective optimality results for AIXI. We have to regard AIXI as a relative theory of intelligence. More generally, our results imply that general reinforcement learning is difficult even when disregarding computational costs.\nBut this is not the end to Bayesian methods in general RL. We show in Section 5.4 that a Bayes-inspired algorithm called Thompson sampling achieves asymptotic optimality. Thompson sampling, also known as posterior sampling or the Bayesian control rule repeatedly draws one environment from the posterior distribution and then acts as if this was the true environment for a certain period of time (depending on the discount function). Moreover, given a recoverability assumption on the environment and some mild assumptions on the discount function, we show in Section 5.5 that Thompson sampling achieves sublinear regret.\nFinally, we tie these results together to solve an open problem in game theory:\nWhen acting in a multi-agent environment with other Bayesian agents, each agent needs to assign positive prior probability to the other agents’ actual policies (they need to have a grain of truth). Finding a reasonably large class of policies that contains the Bayes optimal policies with respect to this class is known as the grain of truth problem (Hutter, 2009b, Q. 5j). Only small classes are known to have a grain of truth and the literature contains several related impossibility results (Nachbar, 1997, 2005; Foster and Young, 2001). Moreover, while AIXI assumes the environment to be computable, our computability results on AIXI confirm that it is incomputable (Theorem 6.15 and Theorem 6.17). This asymmetry elevates AIXI above its environment computationally, and prevents the environment from containing other AIXIs.\nIn Chapter 7 we give a formal and general solution to the grain of truth problem: we construct a class of policies that avoid this asymmetry. This class contains all computable policies as well as Bayes optimal policies for every lower semicomputable prior over the class. When the environment is unknown, our dogmatic prior from Section 5.2 makes Bayes optimal agents fail to act optimally even asymptotically. However, our convergence results on Thompson sampling (Section 5.4.3) imply that Thompson samplers converge to play ε-Nash equilibria in arbitrary unknown computable multiagent environments. While these results are purely theoretical, we use techniques from Chapter 6 to show that they can be computationally approximated arbitrarily closely."
    }, {
      "heading" : "1.3 Thesis Outline",
      "text" : "This thesis is based on the papers Leike and Hutter (2014a,b, 2015a,b,c, 2016); Leike et al. (2016a,b). During my PhD, I was also involved in the publications Leike and Heizmann (2014a,b, 2015); Heizmann et al. (2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honour’s thesis supervised by Marcus Hutter and me). Leike and Hutter (2016) is"
    }, {
      "heading" : "12 Introduction",
      "text" : "still under review. Leike and Hutter (2014a, 2015d) are tangential to this thesis’ main thrust, so the results are mentioned only in passing. A list of papers written during my PhD is given in Table 1.3 on page 14, with a corresponding chapter outline in Table 1.2. The core of our contribution is found in chapters 5, 6, and 7.\nEvery thesis chapter starts with a quote. In case this is not blatantly obvious: these are false quotes, a desperate attempt to make the thesis less dry and humorless. None of the quotes were actually stated by the person they are attributed to (according to our knowledge).\n§1.3 Thesis Outline 13"
    }, {
      "heading" : "14 Introduction",
      "text" : "[1] Jan Leike and Marcus Hutter. Indefinitely oscillating martingales. In Algorithmic Learning Theory, pages 321–335, 2014a\n[2] Jan Leike and Matthias Heizmann. Ranking templates for linear loops. Logical Methods in Computer Science, 11(1):1–27, March 2015\n[3] Mayank Daswani and Jan Leike. A definition of happiness for reinforcement learning agents. In Artificial General Intelligence, pages 231–240. Springer, 2015\n[4] Tom Everitt, Jan Leike, and Marcus Hutter. Sequential extensions of causal and evidential decision theory. In Algorithmic Decision Theory, pages 205–221. Springer, 2015\n[5] Jan Leike and Marcus Hutter. On the computability of AIXI. In Uncertainty in Artificial Intelligence, pages 464–473, 2015a\n[6] Jan Leike and Marcus Hutter. On the computability of Solomonoff induction and knowledgeseeking. In Algorithmic Learning Theory, pages 364–378, 2015b\n[7] Jan Leike and Marcus Hutter. Bad universal priors and notions of optimality. In Conference on Learning Theory, pages 1244–1259, 2015c\n[8] Jan Leike and Marcus Hutter. Solomonoff induction violates Nicod’s criterion. In Algorithmic Learning Theory, pages 349–363. Springer, 2015d\n[9] Matthias Heizmann, Daniel Dietsch, Jan Leike, Betim Musa, and Andreas Podelski. Ultimate Automizer with array interpolation (competition contribution). In Tools and Algorithms for the Construction and Analysis of Systems, pages 455–457. Springer, 2015\n[10] Matthias Heizmann, Daniel Dietsch, Marius Greitschus, Jan Leike, Betim Musa, Claus Schätzle, and Andreas Podelski. Ultimate Automizer with two-track proofs (competition contribution). In Tools and Algorithms for the Construction and Analysis of Systems, pages 950–953. Springer, 2016\n[11] Daniel Filan, Jan Leike, and Marcus Hutter. Loss bounds and time complexity for speed priors. In Artificial Intelligence and Statistics, 2016\n[12] Jan Leike and Marcus Hutter. On the computability of Solomonoff induction and AIXI. 2016. Under review\n[13] Jan Leike, Tor Lattimore, Laurent Orseau, and Marcus Hutter. Thompson sampling is asymptotically optimal in general environments. In Uncertainty in Artificial Intelligence, 2016a\nChapter 2\nPreliminaries\nMathematics is a waste of time. — Leonhard Euler\nThis chapter establishes the notation and background material that is used throughout this thesis. Section 2.1 is about probability and measure theory, Section 2.2 is about stochastic processes, Section 2.3 is about information theory, and Section 2.4 is about algorithmic information theory. We defer the formal introduction to reinforcement learning to Chapter 4. Additional preliminary notation and terminology is also established in individual chapters wherever necessary. A list of notation is provided in the appendix on page 171.\nMost of the content from this chapter can be found in standard textbooks and reference works. We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.\nWe understand definitions to follow natural language; e.g., when defining the adjective ‘continuous’, we define at the same time the noun ‘continuity’ and the adverb ‘continuously’ wherever appropriate.\nNumbers. N := {1, 2, 3, . . .} denotes the set of natural numbers (starting from 1), Q := {±p/q | p ∈ N∪{0}, q ∈ N} denotes the set of rational numbers, and R denotes the set of real numbers. For two real numbers r1, r2, the set [r1, r2] := {r ∈ R | r1 ≤ r ≤ r2} denotes the closed interval with end points r1 and r2; the sets (r1, r2] := [r1, r2] \\ {r1} and [r1, r2) := [r1, r2]\\{r2} denote half-open intervals; the set (r1, r2) := [r1, r2]\\{r1, r2} denotes an open interval.\nStrings. Fix X to be a finite nonempty set, called alphabet . We assume that X contains at least two distinct elements. The set X ∗ := ⋃∞ n=0X n is the set of all finite strings over the alphabet X , the set X∞ is the set of all infinite strings over the alphabet X , and the set X ] := X ∗ ∪X∞ is their union. The empty string is denoted by , not to be confused with the small positive real number ε. Given a string x ∈ X ], we denote its length by |x|. For a (finite or infinite) string x of length ≥ k, we denote with xk the k-th character of x, with x1:k the first k characters of x, and with x<k the first k − 1\n15"
    }, {
      "heading" : "16 Preliminaries",
      "text" : "characters of x. The notation x1:∞ stresses that x is an infinite string. We use x v y to denote that x is a prefix of y, i.e., x = y1:|x|. Our examples often (implicitly) involve the binary alphabet {0, 1}. In this case we define the functions ones, zeros : X ∗ → N that count the number of ones and zeros in a string respectively.\nComputability. A function f : X ∗ → R is lower semicomputable iff the set {(x, q) ∈ X ∗ ×Q | f(x) > q} is recursively enumerable. If f and −f are lower semicomputable, then f is called computable. See Section 6.1.2 for more computability definitions.\nAsymptotic Notation. Let f, g : N → R≥0. We use f ∈ O(g) to denote that there is a constant c such that f(t) ≤ cg(t) for all t ∈ N. We use f ∈ o(g) to denote that lim supt→∞ f(t)/g(t) = 0. For functions on strings P,Q : X ∗ → R we use Q\n×≥ P to denote that there is a constant c > 0 such that Q(x) ≥ cP (x) for all x ∈ X ∗. We also use Q\n×≤ P for P ×≥ Q and Q ×= P for Q ×≤ P and P ×≤ Q. Note that Q ×= P does not imply that there is a constant c such that Q(x) = cP (x) for all x ∈ X ∗. For a sequence (at)t∈N with limit limt→∞ at = a we also write at → a as t→∞. If no limiting variable is provided, we mean t→∞ by convention.\nOther Conventions. Let A be some set. We use #A to denote the cardinality of the set A, i.e., the number of elements in A, and 2A to denote the power set of A, i.e., the set of all subsets of A. We use log to denote the binary logarithm and ln to denote the natural logarithm."
    }, {
      "heading" : "2.1 Measure Theory",
      "text" : "For a countable set Ω, we use ∆Ω to denote the set of probability distributions over Ω. If Ω is uncountable (such as the set of all infinite strings X∞), we need to use the machinery of measure theory. This section provides a concise introduction to measure theory; see Durrett (2010) for an extensive treatment.\nDefinition 2.1 (σ-algebra). Let Ω be a set. The set F ⊆ 2Ω is a σ-algebra over Ω iff\n(a) Ω ∈ F ,\n(b) A ∈ F implies Ω \\A ∈ F , and\n(c) for any countable number of sets A0, A1, . . . ,∈ F , the union ⋃ i∈NAi ∈ F .\nFor a set A ⊆ 2Ω, we define σ(A) to be the smallest (with respect to set inclusion) σ-algebra containing A.\nFor the real numbers, the default σ-algebra (used implicitly) is the Borel σ-algebra B generated by the open sets of the usual topology. Formally, B := σ({(a, b) | a, b ∈ R}).\nA set Ω together with a σ-algebra F forms a measurable space. The sets from the σalgebra F are called measurable sets. A function f : Ω1 → Ω2 between two measurable spaces is called measurable iff any preimage of an (in Ω2) measurable set is measurable (in Ω1).\n§2.1 Measure Theory 17\nDefinition 2.2 (Probability Measure). Let Ω be a measurable space with σ-algebra F . A probability measure on the space Ω is a function µ : F → [0, 1] such that\n(a) µ(Ω) = 1 (normalization), and (b) µ( ⋃ i∈NAi) = ∑ i∈N µ(Ai) for any collection {Ai | i ∈ N} ⊆ F that is pairwise\ndisjoint (σ-additivity).\nA probability measure µ is deterministic iff it assigns all probability mass to a single element of Ω, i.e., iff there is an x ∈ Ω with µ({x}) = 1.\nWe define the conditional probability µ(A | B) for two measurable sets A,B ∈ F with µ(B) > 0 as µ(A | B) := µ(A ∩B)/µ(B).\nDefinition 2.3 (Random Variable). Let Ω be a measurable space with probability measure µ. A (real-valued) random variable is a measurable function X : Ω→ R.\nWe often (but not always) denote random variables with uppercase Latin letters. Given a σ-algebra F , a probability measure P on F , and an F-measurable random variable X, the conditional expectation E[X | F ] of X given F is a random variable Y such that (1) Y is F-measurable and (2) ∫ AXdP = ∫ A Y dP for all A ∈ F . The conditional expectation exists and is unique up to a set of P -measure 0 (Durrett, 2010, Sec. 5.1). Intuitively, if F describes the information we have at our disposal, then E[X | F ] denotes the expectation of X given this information.\nWe proceed to define the σ-algebra on X∞ (the σ-algebra on X ] is defined analogously). For a finite string x ∈ X ∗, the cylinder set\nΓx := {xy | y ∈ X∞}\nis the set of all infinite strings of which x is a prefix. Furthermore, we fix the σ-algebras\nFt := σ ( {Γx | x ∈ X t} ) and F∞ := σ ( ∞⋃ t=1 Ft ) .\nThe sequence (Ft)t∈N is a filtration: from Γx = ⋃ a∈X Γxa follows that Ft ⊆ Ft+1 for every t ∈ N, and all Ft ⊆ F∞ by the definition of F∞. For our purposes, the σ-algebra Ft means ‘all symbols up to and including time step t.’ So instead of conditioning an expectation on Ft, we can just as well condition it on the sequence x1:t drawn at time t. Hence we write E[X | x1:t] instead of E[X | Ft]. Moreover, for conditional probabilities we also write Q(xt | x<t) instead of Q(x1:t | x<t).\nIn the context of probability measures, a measurable set E ∈ F∞ is also called an event . The event Ec := X∞ \\ E denotes the complement of E. In case the event E is defined by a predicate Q dependent on the random variable X, E = {x ∈ Ω | Q(X(x))}, we also use the shorthand notation\nP [Q(X)] := P ({x ∈ Ω | Q(X(x))}) = P (E).\nWe assume all sets to be measurable; when we write P (A) for some set A ⊆ X∞, we understand implicitly that A be measurable. This is not true: not all subsets of"
    }, {
      "heading" : "18 Preliminaries",
      "text" : "X∞ are measurable (assuming the axiom of choice). While we choose to do this for readability purposes, note that under some axioms compatible with Zermelo-Fraenkel set theory, notably the axiom of determinacy, all subsets of X∞ are measurable."
    }, {
      "heading" : "2.2 Stochastic Processes",
      "text" : "This section introduces some notions about sequences of random variables.\nDefinition 2.4 (Stochastic Process). (Xt)t∈N is a stochastic process iff Xt is a random variable for every t ∈ N.\nA stochastic process (Xt)t∈N is nonnegative iff Xt ≥ 0 for all t ∈ N. The process is bounded iff there is a constant c ∈ R such that |Xt| ≤ c for all t ∈ N.\nIn the real numbers, a sequence (zt)t∈N converges if and only if it is a Cauchy sequence, i.e., iff |zt+1−zt| → 0 as t→∞. For sequences of random variables convergence is a lot more subtle and there are several different notions of convergence.\nDefinition 2.5 (Stochastic Convergence). Let P be a probability measure. A stochastic process (Xt)t∈N converges to the random variable X\n• in P -probability iff for every ε > 0,\nP [ |Xt −X| > ε ] → 0 as t→∞;\n• in P -mean iff EP [ |Xt −X| ] → 0 as t→∞;\n• P -almost surely iff P [\nlim t→∞\nXt = X ] = 1.\nAlmost sure convergence and convergence in mean both imply convergence in probability (Wasserman, 2004, Thm. 5.17). If the stochastic process is bounded, then convergence in probability implies convergence in mean (Wasserman, 2004, Thm. 5.19).\nA sequence of real numbers (at)t∈N converges in Cesàro average to a ∈ R iff 1/t ∑t\nk=1 ak → a as t → ∞. The definition for sequences of random variables is analogous.\nDefinition 2.6 (Martingale). Let P be a probability measure over (X∞,F∞). A stochastic process (Xt)t∈N is a P -supermartingale (P -submartingale) iff\n(a) each Xt is Ft-measurable, and\n(b) E[Xt | Fs] ≤ Xs (E[Xt | Fs] ≥ Xs) P -almost surely for all s, t ∈ N with s < t.\nA P -martingale is a process that is both a P -supermartingale and a P -submartingale.\n§2.3 Information Theory 19\nExample 2.7 (Fair Gambling). Suppose Mary bets on the outcome of a fair coin flip. If she predicts correctly, her wager is doubled and otherwise it is lost. Let Xt denote Mary’s wealth at time step t. Since the game is fair, E[Xt+1 | Ft] = Xt where Ft represents the information available at time step t. Hence E[Xt] = X1, so in expectation she never loses money regardless of her betting strategy. 3\nFor martingales the following famous convergence result was proved by Doob (1953).\nTheorem 2.8 (Martingale Convergence; Durrett, 2010, Thm. 5.2.9). If (Xt)t∈N is a nonnegative supermartingale, then it converges almost surely to a limit X with E[X] ≤ E[X1].\nBy Theorem 2.8 the martingale from Example 2.7 representing Mary’s wealth converges almost surely, regardless of her betting strategy. Either she refrains from betting at some point (assuming she cannot place smaller and smaller bets) or she cannot play anymore because her wealth is 0. Is there a lesson to learn here about gambling?"
    }, {
      "heading" : "2.3 Information Theory",
      "text" : "This section introduces the notions of entropy and two notions of distance between probability measures: KL-divergence and total variation distance.\nDefinition 2.9 (Entropy). Let Ω be a countable set. For a probability distribution p ∈ ∆Ω, the entropy of p is defined as\nEnt(p) := − ∑\nx∈Ω: p(x)>0\np(x) log p(x).\nDefinition 2.10 (KL-Divergence). Let P,Q be two measures and let m ∈ N be a lookahead time step. The Kullback-Leibler-divergence (KL-divergence) of P and Q between time steps t and m is defined as\nKLm(P,Q | x<t) := ∑\nxt:m∈Xm−t+1 P (x1:m | x<t) log\nP (x1:m | x<t) Q(x1:m | x<t) .\nMoreover, we define KL∞(P,Q | x<t) := limm→∞KLm(P,Q | x<t).\nKL-divergence is also known as relative entropy . KL-divergence is always nonnegative by Gibbs’ inequality, but it is not a distance since it is not symmetric. If the alphabet X is finite, then KLm(P,Q | x) is always finite. However, KL∞(P,Q | x) may be infinite.\nDefinition 2.11 (Total Variation Distance). Let P,Q be two measures and let 1 ≤ m ≤ ∞ be a lookahead time step. The total variation distance between P and Q between time steps t and m is defined as\nDm(P,Q | x) := sup A⊆Xm\n∣∣∣P (A | x<t)−Q(A | x<t)∣∣∣."
    }, {
      "heading" : "20 Preliminaries",
      "text" : "Total variation distance is always bounded between 0 and 1 since P and Q are probability measures. Moreover, in contrast to KL-divergence total variation distance satisfies the axioms of distance: symmetry (D(P,Q) = D(Q,P )), identity of indiscernibles (D(P,Q) = 0 if and only if P = Q), and the triangle inequality (D(P,Q) +D(Q,R) ≥ D(P,R)).\nThe following lemma shows that total variation distance can be used to bound differences in expectation.\nLemma 2.12 (Total Variation Bound on the Expectation). For a random variable X with 0 ≤ X ≤ 1 and two probability measures P and Q∣∣EP [X]− EQ[X]∣∣ ≤ D(P,Q).\nKL-divergence and total variation distance are linked by the following inequality.\nLemma 2.13 (Pinsker’s inequality; Tsybakov, 2008, Lem. 2.5i). For all probability measures P and Q on X∞, for every x ∈ X ∗, and for every m ∈ N\nDm(P,Q | x) ≤ √ 1\n2 KLm(P,Q | x)"
    }, {
      "heading" : "2.4 Algorithmic Information Theory",
      "text" : "A universal Turing machine (UTM) is a Turing machine that can simulate all other Turing machines. Formally, a Turing machine U is a UTM iff for every Turing machine T there is a binary string p (called program) such that U(p, x) = T (x) for all x ∈ X ∗, i.e., the output of U when run on (p, x) is the same as the output of T when run on x. We assume the set of programs on U is prefix-free. The Kolmogorov complexity K(x) of a string x is the length of the shortest program on U that prints x and then halts:\nK(x) := min{|p| | U(p) = x}.\nA monotone Turing machine is a Turing machine with a one-way read-only input tape, a one-way write-only output tape, and a read/write work tape. Monotone Turing machines sequentially read symbols from their input tape and write to their output tape. Interpreted as a function, a monotone Turing machine T maps a string x to the longest string that T writes to the output tape while reading x and no more from the input tape (Li and Vitányi, 2008, Ch. 4.5.2).\nWe also use U to denote a universal monotone Turing machine (programs on the universal monotone Turing machine do not have to be prefix-free). The monotone Kolmogorov complexity Km(x) denotes the length of the shortest program on the monotone machine U that prints a string starting with x (Li and Vitányi, 2008, Def. 4.5.9):\nKm(x) := min{|p| | x v U(p)}. (2.1)\nSince monotone complexity does not require the machine to halt, there is a constant c such that Km(x) ≤ K(x) + c for all x ∈ X∗.\n§2.4 Algorithmic Information Theory 21\nThe following notion of a (semi)measure is particular to algorithmic information theory.\nDefinition 2.14 (Semimeasure; Li and Vitányi, 2008, Def. 4.2.1). A semimeasure over the alphabet X is a function ν : X ∗ → [0, 1] such that\n(a) ν( ) ≤ 1, and (b) ν(x) ≥ ∑\na∈X ν(xa) for all x ∈ X ∗.\nA semimeasure is a (probability) measure iff equalities hold in (a) and (b) for all x ∈ X ∗.\nSemimeasures are not probability measures in the classical measure theoretic sense. However, semimeasures correspond canonically to classical probability measures on the probability space X ] = X ∗ ∪X∞ whose σ-algebra is generated by the cylinder sets (Li and Vitányi, 2008, Ch. 4.2 and Hay, 2007).\nLower semicomputable semimeasures correspond naturally to monotone Turing machines (Li and Vitányi, 2008, Thm. 4.5.2): for a monotone Turing machine T , the semimeasure λT maps a string x to the probability that T outputs something starting with x when fed with fair coin flips as input (and vice versa). Hence we can enumerate all lower semicomputable semimeasures ν1, ν2, . . . by enumerating all monotone Turing machines. We define the Kolmogorov complexity K(ν) of a lower semicomputable semimeasure ν as the Kolmogorov complexity of the index of ν in this enumeration.\nWe often mix the (semi)measures of algorithmic information theory with concepts from probability theory. For convenience, we identify a finite string x ∈ X ∗ with its cylinder set Γx. Then ν(x) in the algorithmic information theory sense coincides with ν(Γx) in the measure theory sense if we use the identification of semimeasures with probability measures above.\nExample 2.15 (Lebesgue Measure). The Lebesgue measure or uniform measure is defined as\nλ(x) := (#X )−|x|. 3\nThe following definition turns a semimeasure into a measure, preserving the predictive ratio ν(xa)/ν(xb) for a, b ∈ X .\nDefinition 2.16 (Solomonoff Normalization). The Solomonoff normalization νnorm of a semimeasure ν is defined as νnorm( ) := 1 and for all x ∈ X ∗ and a ∈ X ,\nνnorm(xa) := νnorm(x) ν(xa)∑ b∈X ν(xb) . (2.2)\nBy definition, νnorm is a measure. Moreover, νnorm dominates ν according to the following lemma.\nLemma 2.17 (νnorm ≥ ν). νnorm(x) ≥ ν(x) for all x ∈ X ∗ and all semimeasures ν."
    }, {
      "heading" : "22 Preliminaries",
      "text" : "Proof. We use induction on the length of x: if x = then νnorm( ) = 1 = ν( ), and otherwise\nνnorm(xa) = νnorm(x)ν(xa)∑ b∈X ν(xb) ≥ ν(x)ν(xa)∑ b∈X ν(xb) ≥ ν(x)ν(xa) ν(x) = ν(xa).\nThe first inequality holds by induction hypothesis and the second inequality uses the fact that ν is a semimeasure.\nChapter 3\nLearning\nThe problem of induction is essentially solved. — David Hume\nMachine learning refers to the process of learning models of and/or making predictions about (large) sets of data points that are typically independent and identically distributed (i.i.d.); see Bishop (2006) and Hastie et al. (2009). In this chapter we do not make the i.i.d. assumption. Instead, we aim more generally at the theoretical fundamentals of the sequence prediction problem: how will a sequence of symbols generated by an unknown stochastic process be continued? Given a finite string x<t = x1x2 . . . xt−1 of symbols, what is the next symbol xt? How likely does a given property hold for the entire sequence x1:∞? Arguably, any learning or prediction problem can be phrased in this fashion: anything that can be stored on a computer can be turned into a sequence of bits.\nWe distinguish two major elements of learning. First, the process of converging to accurate beliefs, called merging . Second, the process of making accurate forecasts about the next symbol, called predicting . These two notions are not distinct: if you have accurate beliefs about the unseen data, then you can make good predictions, but not necessarily vice versa (see Example 3.41). We discuss different notions of merging in Section 3.4 and state bounds on the prediction regret in Section 3.5.\nIn the general reinforcement learning problem we target in this thesis, the environment is unknown and the agent needs to learn it. The literature on non-i.i.d. learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996). However, we argue that merging is the essential property for general AI. In order to make good decisions, the agent needs to have accurate beliefs about what its actions will entail. On a technical level, merging leads to on-policy value convergence (Section 4.2.3), the fact that the agents learns to estimate the values for its own policy correctly.\nThe setup we consider is the realizable case: we assume that the data is generated by an unknown probability distribution that belongs to a known (countable) class of distributions. In contrast, the nonrealizable case allows no assumptions on the underlying process that generates the data. A well-known approach to the nonrealizable case is prediction with expert advice (Cesa-Bianchi and Lugosi, 2006), which we do not con-\n23"
    }, {
      "heading" : "24 Learning",
      "text" : "sider here. Generally, the nonrealizable case is harder, but Ryabko (2011) argues that for some problems, both cases coincide.\nAfter introducing the formal setup in Section 3.1, we discuss several examples for learning distributions and notions that relate the learning distribution with the process generating the data in Section 3.2. In Section 3.3 we connect these notions to the theory of martingale processes.\nSection 3.6 connects the results from the first sections to the learning framework developed by Solomonoff (1964, 1978), Hutter (2001b, 2005, 2007b), and Schmidhuber (2002) (among others). This framework relies on results from algorithmic information theory and computability theory to learn any computable distribution quickly and effectively. It is incomputable (see Section 6.2), but can serve as a gold standard for learning.\nMost of this chapter echoes the literature. We collect results from economics and computer science that previously had not been assembled in one place. We provide proofs that connect the various properties (Proposition 3.23, Proposition 3.16, and Proposition 3.37), and we fill in a few gaps in the picture: the prediction bounds for absolute continuity (Section 3.5.2) and the improved regret bounds for nonuniform measures (Theorem 3.48 and Theorem 3.51). Section 3.7 summarizes the results in Table 3.2 on page 45 as well as Figure 3.1 on page 46."
    }, {
      "heading" : "3.1 Setup",
      "text" : "For the rest of this chapter, fix P and Q to be two probability measures over the measurable space of infinite sequences (X∞,F∞). We think of P as the true distribution from which the data sequence x1:∞ is drawn, and of Q as our belief distribution or learning algorithm. In other words, we use the distribution Q to learn a string drawn from the distribution P .\nLet H denote a hypothesis, i.e., any measurable set from F∞. Our prior belief in the hypothesis H is Q(H). In each time step t, we make one observation xt ∈ X . Our history x<t = x1x2 . . . xt−1 is the sequence of all previous observations. We update our belief in accordance with Bayesian learning; our posterior belief in the hypothesis H is\nQ(H | x1:t) = Q(H ∩ x1:t) Q(x1:t) .\nThe observation xt confirms the hypothesis H iff Q(H | x1:t) > Q(H | x<t) (the belief in H increases), and the observation xt disconfirms the hypothesis H iff Q(H | x1:t) < Q(H | x<t) (the belief in H decreases). If Q(H | x1:t) = 0, then H is refuted or falsified .\nWhen we assign a prior belief of 0 to a hypothesis H, this means that we think that H is impossible; it is refuted from the beginning. If Q(H) = 0, then the posterior Q(H | x<t) = 0, so no evidence whatsoever can change our mind that H is impossible. This is bad if the hypothesis H is actually true.\nTo be able to learn we need to make some assumptions on the learning distribution Q: we need to have an open mind about anything that might actually happen, i.e.,\n§3.2 Compatibility 25\nQ(H) > 0 on any hypothesis H with P (H) > 0. This property is called absolute continuity . We discuss this and other notions of compatibility of P and Q in Section 3.2.\nWe motivate this chapter with the following example.\nExample 3.1 (The Black Ravens; Rathmanner and Hutter, 2011, Sec. 7.4). If we live in a world in which all ravens are black, how can we learn this fact? Since at every time step we have observed only a finite subset of the (possibly infinite) set of all ravens, how can we confidently state anything about all ravens?\nWe formalize this problem in line with Rathmanner and Hutter (2011, Sec. 7.4) and Leike and Hutter (2015d). We define two predicates, blackness B and ravenness R. There are four possible observations: a black raven BR, a non-black raven BR, a black non-raven BR, and a non-black non-raven BR. Therefore our alphabet consists of four symbols corresponding to each of the possible observations, X := {BR,BR,BR,BR}.\nWe are interested in the hypothesis ‘all ravens are black’. Formally, it corresponds to the measurable set\nH := {x ∈ X∞ | xt 6= BR ∀t} = {BR,BR,BR}∞, (3.1)\nthe set of all infinite strings in which the symbol BR does not occur. If we observe a non-black raven, xt = BR, the hypothesisH is refuted sinceH∩x1:t = ∅ and this implies Q(H | x1:t) = 0. In this case, our inquiry regarding H is settled. The interesting case is when the hypothesis H is in fact true (P (H) = 1), i.e., P does not generate any non-black ravens. The property we desire is that in a world in which all ravens are black, we arrive at this belief: P (H) = 1 implies Q(H | x<t) → 1 as t→∞. 3"
    }, {
      "heading" : "3.2 Compatibility",
      "text" : "In this section we define dominance, absolute continuity , dominance with coefficients, weak dominance, and local absolute continuity , in decreasing order of their strength. These notions make the relationship of the two probability measures P and Q precise. We also give examples for various choices for the learning algorithm Q.\nIn our examples, we frequently rely on the following process.\nExample 3.2 (Bernoulli Process). Assume X = {0, 1}. For a real number r ∈ [0, 1] we define the Bernoulli process with parameter r as the measure\nBernoulli(r)(x) := rones(x)(1− r)zeros(x).\nNote that Bernoulli(1/2) = λ, the Lebesgue measure from Example 2.15. 3\nDefinition 3.3 (Dominance). The measure Q dominates P (Q ×≥ P ) iff there is a constant c > 0 such that Q(x) ≥ cP (x) for all finite strings x ∈ X ∗.\nDominance is also called having a grain of truth (Lehrer and Smorodinsky, 1996, Def. 2a and Kalai and Lehrer, 1993); we discuss this property in the context of game theory in Chapter 7."
    }, {
      "heading" : "26 Learning",
      "text" : "Example 3.4 (Bayesian mixture). LetM be a countable set of probability measures on (X∞,F∞) and let w ∈ ∆M be a prior over M. If w(P ) > 0 for all P ∈ M, the prior w is called positive or universal . Then the Bayesian mixture ξ := ∑ P∈Mw(P )P dominates each P ∈M. 3\nThe Bayesian mixture is a mathematically simple yet very powerful concept. It is very easy to derive from a countable set of distributions, and it has been considered extensively in the literature (Solomonoff, 1964; Jaynes, 2003; Hutter, 2005, . . . ). Ryabko (2009) shows that even for uncountably infinite classes, if there are good predictors, then a Bayesian mixture over a countable subclass asymptotically also does well.\nExample 3.5 (Solomonoff Prior). Solomonoff (1964) defines a distribution M over X ] that assigns to a string x the probability that the universal monotone Turing machine U outputs x when fed with fair coin flips on the input tape. Formally,\nM(x) := ∑\np:xvU(p)\n2−|p| (3.2)\nwhere p is a binary string.1 The functionM is a lower semicomputable semimeasure, but not computable and not a measure (Li and Vitányi, 2008, Lem. 4.5.3); see Section 6.2 for the computability properties of M . More importantly, M dominates every lower semicomputable semimeasure (Li and Vitányi, 2008, Thm. 4.5.1).\nSolomonoff’s prior M has a number of appealing philosophical properties. In line with Ockham’s razor it favors simple environments over complex ones: Turing machines that have a short program on the UTM U have a higher contribution in the sum (3.2). In line with Epicurus’ principle it never discards possible explanations: every program that produces the string x contributes to the sum. See Rathmanner and Hutter (2011) for a discussion on the philosophical underpinnings of Solomonoff’s prior. 3\nWood et al. (2011) show that the Solomonoff priorM can equivalently be defined as a Bayesian mixture over all lower semicomputable semimeasures with a prior w(P ) ∝ 2−K(P ). (If we use w(P ) = 2−K(P ) we get a semiprior because ∑ P∈M 2\n−K(P ) can be less than 1. This prior also carries the name Solomonoff prior .)\nDefinition 3.6 (Absolute Continuity). The measure P is absolutely continuous with respect to Q (Q P ) iff Q(A) = 0 implies P (A) = 0 for all measurable sets A.\nRemark 3.7 (Absolute Continuity ; Dominance). Absolute continuity is strictly weaker than dominance: let X := {0, 1} and define a probability measure P that assigns probability 2/3 to 1 and probability 1/3 to 0 until seeing the first 0, then P behaves like the Lebesgue measure λ. Formally,\nP (x1:t) :=\n{( 2 3 )t if x1:t = 1t, and( 2 3 )n 1 3λ(xn+2:t) if ∃n ≥ 0. 1 n0 v x1:t.\n1We use the name Solomonoff prior for both a distribution over X∞ and a distribution over a computably enumerable setM. Maybe M should better be called Solomonoff mixture to avoid confusion.\n§3.2 Compatibility 27\nSince λ(1t)/P (1t) = (3/4)t → 0 as t→∞, there is no constant c such that λ(x)/P (x) > c > 0 for all finite strings x ∈ X ∗, hence λ does not dominate P . But P is absolutely continuous with respect to λ because P -almost surely we draw a 0 eventually, and then P behaves like λ. Hence P -almost surely λ/P 6→ 0. The claim now follows from Proposition 3.23b. 3\nThe idea of Remark 3.7 is to ‘punch a hole into λ’ at the infinite string 1∞. This infinite string has probability 0, hence this hole does not break absolute continuity. But it breaks dominance on this infinite string. Analogously we could punch countably many holes into a probability measure without breaking absolute continuity.\nDefinition 3.8 (Weak Dominance). The measure Q weakly dominates P (Q ×≥W P ) iff lim t→∞ 1 t log Q(x1:t) P (x1:t) = 0 with P -probability 1.\nLehrer and Smorodinsky (1996, Rem. 8) point out that for any P and Q,\nlim sup t→∞\n1 t log Q(x1:t) P (x1:t) ≤ 0 P -almost surely,\nso crucial is whether the lim inf is also 0.\nRemark 3.9 (Weak Dominance). The measure Q weakly dominates P if and only if P -almost surely log(P (x)/Q(x)) ∈ o(t). 3\nRyabko and Hutter (2007, 2008) consider the following definition. It is analogous to Definition 3.3, except that the constant c is allowed to depend on time.\nDefinition 3.10 (Dominance with Coefficients; Ryabko and Hutter, 2008, Def. 2). The measure Q dominates P with coefficients f (Q ≥ P/f) iff Q(x) ≥ P (x)/f(|x|) for all x ∈ X ∗.\nIf Q dominates P with coefficients f and f grows subexponentially (f ∈ o(exp)), then Q weakly dominates P by Remark 3.9.\nExample 3.11 (Speed Prior). Schmidhuber (2002) defines a variant of Solomonoff’s priorM that penalizes programs by their running time, called the speed prior. Consider the speed prior\nSKt(x) := ∑\np:xvU(p)\n2−|p|\nt(U, p, x)\nwhere t(U, p, x) is the number of time steps the Turing machine U takes to produce x from the program p. For any deterministic measure P computable in time q we have SKt(x)\n×≥ P (x)/q(|x|). Therefore SKt dominates P with coefficients O(q). If q is a polynomial (P is computable in polynomial time), then it grows subexponentially and thus SKt weakly dominates P . 3\nThe semimeasure loss SKt(x)− ∑\na∈X SKt(xa) in the speed prior is quite substantial: since it takes at least n steps to output a string of length n, M(x) ≥ |x|SKt(x)."
    }, {
      "heading" : "28 Learning",
      "text" : "Example 3.12 (Laplace Rule). The Laplace rule ρL is defined by\nρL(xt | x<t) := #{i < t | xi = xt}\nt+ #X .\nFor X = {0, 1} and r ∈ [0, 1] the measure ρL dominates Bernoulli(r) with coefficients f(t) = t−#X+1 (Ryabko and Hutter, 2008, Prop. 3). 3\nDefinition 3.13 (Local Absolute Continuity). The measure P is locally absolutely continuous with respect to Q (Q L P ) iff Q(x) = 0 implies P (x) = 0 for all finite strings x ∈ X ∗.\nThe notable difference between local absolute continuity and absolute continuity is that Definition 3.6 talks about arbitrary measurable sets while Definition 3.13 only talks about finite strings. The former is a much stronger property.\nFor example, every measure is locally absolutely continuous with respect to the Lebesgue measure since λ(x) > 0 for all finite strings x ∈ X ∗.\nLocal absolute continuity is an extremely weak property. If it is not satisfied, we have to be very careful when using Q for prediction: then there is a positive probability that we have to condition on a probability zero event.\nExample 3.14 (The Minimum Description Length Principle; Grünwald, 2007). Let M be a countable set of probability measures on (X∞,F∞) and let K : M → [0, 1] be a function such that ∑ P∈M 2\n−K(P ) ≤ 1 called regularizer. Following notation from Hutter (2009a), we define for each x ∈ X ∗ the minimal description length model as\nMDLx := arg min P∈M {− logP (x) +K(P )}.\n− logP (x) is the (arithmetic) code length of x given model P , andK(P ) is a complexity penalty for P . Given data x ∈ X ∗, MDLx is the measure P ∈ M that minimizes the total code length of data and model.\nNote that the Lebesgue measure is not locally absolutely continuous with respect to the MDL distribution Q(x) := MDLx(x): for some x ∈ X ∗ the minimum description P ∈M may assign probability zero to a continuation xy ∈ X ∗. 3\nRemark 3.15 (MDL is Inductively Inconsistent; Leike and Hutter, 2014a, Cor. 13). The MDL estimator for countable classes as defined in Example 3.14 is inductively inconsistent: the selected model P ∈ M can change infinitely often and thus the limit limt→∞MDLx<t may not exist. This can be a major obstacle for using MDL for prediction, since the model used for prediction has to be changed over and over again, incurring the corresponding computational cost. 3\nThe following proposition establishes the relationship between our notions of compatibility; see also Figure 3.1 on page 46.\nProposition 3.16 (Relationships between Compatibilities).\n(a) If Q ×≥ P , then Q P .\n§3.3 Martingales 29\n(b) If Q P , then Q ×≥W P .\n(c) If Q ×≥ P , then Q dominates P with coefficients f for a constant function f .\n(d) If Q dominates P with coefficients f and f ∈ o(exp), then Q ×≥W P .\n(e) If Q ×≥W P , then Q L P .\nProof. (a) From Proposition 3.23 (a) and (b).\n(b) From Proposition 3.23b and Kalai and Lehrer (1994, Prop. 3a).\n(c) Follows immediately from the definitions.\n(d) From Remark 3.9.\n(e) Follows immediately from the definitions.\nNote that the converse of Proposition 3.16d is false: in Remark 3.7 we defined a measure P that is absolutely continuous with respect to λ (and hence is weakly dominated by λ), but the coefficients for P/λ grow exponentially on the string 1t. This infinite string has P -probability 0, but dominance with coefficients demands the inequality Q ≥ P/f to hold for all strings.\nRemark 3.17 (Local Absolute Continuity ; Absolute Continuity). Define P := Bernoulli(2/3) and Q := Bernoulli(1/3). Both measures P and Q are nonzero on all cylinder sets: Q(x) ≥ 3−|x| > 0 and P (x) ≥ 3−|x| > 0 for every x ∈ X ∗. Therefore Q is locally absolutely continuous with respect to P . However, Q is not absolutely continuous with respect to P : define\nA := { x ∈ X ω ∣∣∣∣ lim sup t→∞ 1 t ones(x1:t) ≤ 1 2 } .\nThe set A is F∞-measurable since A = ⋂∞ n=1 ⋃ x∈Un Γx with Un := {x ∈ X\n∗ | |x| ≥ n and ones(x) ≤ |x|/2}, the set of all finite strings of length at least n that have at least as many zeros as ones. We have that P (A) = 0 and Q(A) = 1, hence Q is not absolutely continuous with respect to P . 3"
    }, {
      "heading" : "3.3 Martingales",
      "text" : "The following two theorems state the connection between probability measures on infinite strings and martingales. For two probability measures P and Q the quotient Q/P is a nonnegative P -martingale if Q is locally absolutely continuous with respect to P . Conversely, for every nonnegative P -martingale there is a probability measure Q L P such that the martingale is P -almost surely a multiple of Q/P ."
    }, {
      "heading" : "30 Learning",
      "text" : "Theorem 3.18 (Measures 7→ Martingales; Doob, 1953, II§7 Ex. 3). Let Q and P be two probability measures on (X∞,F∞) such that Q is locally absolutely continuous with respect to P . Then the stochastic process (Xt)t∈N,\nXt(x) := Q(x1:t)\nP (x1:t)\nis a nonnegative P -martingale with E[Xt] = 1.\nTheorem 3.19 (Martingales 7→Measures). Let P be a probability measure on (X∞,F∞) and let (Xt)t∈N be a nonnegative P -martingale with E[Xt] = 1. There is a probability measure Q on (X∞,F∞) that is locally absolutely continuous with respect to P and for all x ∈ X∞ and all t ∈ N with P (x1:t) > 0,\nXt(x) = Q(x1:t)\nP (x1:t) .\nThe proofs for Theorem 3.18 and Theorem 3.19 are provided in the Appendix.\nExample 3.20 (The Posterior Martingale). Suppose we are interested in a hypothesis H ⊆ X∞ (such as the proposition ‘all ravens are black’ in Example 3.1). If Q(H) =∑\nP∈Mw(P )P (H) is a Bayesian mixture over a set of probability distributions M with prior weights w ∈ ∆M (see Example 3.4), then the posterior belief Q(H | x) =∑\nP∈Mw(P | x)P (H | x). The weights w(P | x) are called posterior weights, and satisfy the identity\nw(P | x) = w(P )P (x) Q(x)\n(3.3)\nsince\nQ(H | x) = Q(H ∩ x) Q(x)\n= 1\nQ(x) ∑ P∈M w(P )P (H ∩ x)\n= ∑ P∈M w(P ) P (x)P (H ∩ x) Q(x)P (x)\n= ∑ P∈M w(P | x)P (H | x).\nAccording to Theorem 3.18 the posterior weight w(P | x) is a Q-martingale with expectation w(P ). In particular, this means that the posterior weights converge Qalmost surely by the martingale convergence theorem (Theorem 2.8). SinceQ dominates P , by Proposition 3.16a P is absolutely continuous with respect to Q and hence the posterior also converges P -almost surely. 3\nRemark 3.21 (Martingales and Absolute Continuity). While Theorem 3.18 trivially also holds if Q is absolutely continuous with respect to P , Theorem 3.19 does not imply that Q is absolutely continuous with respect to P .\n§3.3 Martingales 31\nLet P and Q be defined as in Remark 3.17. Consider the process X0(x) := 1,\nXt+1(x) := { 2Xt, if xt+1 = 0, and 1 2Xt, if xt+1 = 1.\nThe process (Xt)t∈N is a nonnegative P -martingale since every Xt is Ft-measurable and for x = y1:t we have\nE[Xt+1 | Ft](y) = P (x0 | x)2Xt(y) + P (x1 | x)12Xt(y) = 132Xt(y) + 2 3 · 1 2Xt(y) = Xt(y).\nMoreover,\nQ(x) = (\n1 3 )ones(x) (2 3 )zeros(x) = ( 2 3 )ones(x) (1 3 )zeros(x) 2−ones(x)2zeros(x) = P (x)Xt(y).\nHence Xt(y) = Q(y1:t)/P (y1:t) P -almost surely. The measure Q is uniquely defined by its values on the cylinder sets, and as shown in Remark 3.17, Q is not absolutely continuous with respect to P . 3\nTheorem 3.22 (Radon-Nikodym Derivative). If Q P , then there is a function dP/dQ : X∞ → [0,∞) called the Radon-Nikodym derivative such that∫\nfdP = ∫ f dP\ndQ dQ\nfor all measurable functions f .\nThis function dP/dQ can be seen as a density function of P with respect to the background measure Q. Moreover, dP/dQ is the limit of the martingale P/Q (Durrett, 2010, Sec. 5.3.3) which exists Q-almost surely according to Theorem 2.8.\nThe following proposition characterizes the notions of compatibility from Section 3.2 in terms of the martingale Q/P .\nProposition 3.23 (Martingales and Compatibility). The following relationships hold between Q, P , and the P -martingale Yt := Q(x1:t)/P (x1:t). (a) Q ×≥ P if and only if Yt ≥ c > 0 for all t ∈ N.\n(b) Q P if and only if P -almost surely Yt 6→ 0 as t→∞.\n(c) Q dominates P with coefficients f if and only if Yt ≥ 1/f(t) for all t.\n(d) Q ×≥W P if and only if P -almost surely log(Yt+1/Yt)→ 0 in Cesàro average.\n(e) Q L P if and only if P -almost surely Yt > 0 for all t ∈ N.\nProof. (Yt)t∈N is a P -martingale according to Theorem 3.18.\n(a) Q(x) ≥ cP (x) with c > 0 for all x ∈ X ∗ is equivalent to Q(x)/P (x) ≥ c > 0 for all x ∈ X ∗."
    }, {
      "heading" : "32 Learning",
      "text" : "(b) Proved by Hutter (2009a, Lem. 3i).\n(c) Analogously to the proof of (a).\n(d) If Q weakly dominates P , we get − log Yt ∈ o(t) according to Remark 3.9. Together with Y0 = 1 we get − log Yt = ∑t−1 k=0− log(Yk+1/Yk) ∈ o(t), therefore\nt−1 ∑t−1\nk=0− log(Yk+1/Yk) → 0 as t → ∞. Conversely, if the Cesàro average converges to 0, then t−1 log Yt → 0, hence − log Yt ∈ o(t).\n(e) Let x ∈ X ∗ be any finite string. If Q L P and P (x) > 0, then Q(x) > 0, and hence Q(x)/P (x) > 0. Conversely, if P (x) > 0 then Yt is well-defined, so if Y|x|(x) > 0 then Q(x) > 0.\nFrom Proposition 3.23b and Theorem 3.22 we get that Q P if and only if the Radon-Nikodym derivative dQ/dP is positive on a set of P -measure 1."
    }, {
      "heading" : "3.4 Merging",
      "text" : "If Q is capable of learning, it should use the sequence x drawn from P to change its opinions more in the direction of P . More precisely, we want Q( · | x<t) ≈ P ( · | x<t) for large t. In the rest of this chapter, we make this notion of closeness precise and discuss different conditions on Q that are sufficient for learning.\nStrong merging implies that the belief of any hypothesis merges. This is very strong, as hypotheses can talk about tail events: events that are independent of any finite initial part of the infinite sequence (such as the event A in Remark 3.17). Weak merging only considers hypothesis about the next couple of symbols, and almost weak merging allows Q to deviate from P in a vanishing fraction of the time. Much of this section is based on Kalai and Lehrer (1994) and Lehrer and Smorodinsky (1996)."
    }, {
      "heading" : "3.4.1 Strong Merging",
      "text" : "Definition 3.24 (Strong Merging). Q merges strongly with P iff D∞(P,Q | x<t)→ 0 as t→∞ P -almost surely.\nThe following theorem is the famous merging of opinions theorem by Blackwell and Dubins (1962).\nTheorem 3.25 (Absolute Continuity⇒ Strong Merging; Blackwell and Dubins, 1962). If P is absolutely continuous with respect to Q, then Q merges strongly with P .\nExample 3.26 (The Black Ravens 2; Rathmanner and Hutter, 2011, Sec. 7.4). Recall the black raven problem from Example 3.1. Let Q be a learning distribution that dominates the true distribution P , such as a Bayesian mixture (Example 3.4). By Proposition 3.16a we get Q P , and hence Q merges strongly to P by Theorem 3.25. Thus we get as t → ∞ that P -almost surely |Q(H | x<t) − P (H | x<t)| → 0 for the hypothesis H that ‘all ravens are black’ defined in (3.1). Thus if all ravens are black in the real world (P (H) = 1), Q learns this asymptotically (Q(H | x<t) → 1). This is\n§3.4 Merging 33\nthe solution we desired: the learning distribution Q converges to a true belief about an infinite set by only looking from a finite (but growing) number of data points. 3\nThe following is the converse of Theorem 3.25.\nTheorem 3.27 (Strong Merging ∧ Local Absolute Continuity⇒ Absolute Continuity; Kalai and Lehrer, 1994, Thm. 2). If Q is locally absolutely continuous with respect to P and Q merges strongly with P , then P is absolutely continuous with respect to Q.\nThe following result shows that local absolute continuity is not required for strong merging: recall that according to Example 3.14 the MDL distribution is not locally absolutely continuous with respect to every P from the classM.\nTheorem 3.28 (Strong Merging for MDL; Hutter, 2009a, Thm. 1). If P ∈M, then\nD∞(P,MDL x | x)→ 0 as |x| → ∞ P -almost surely.\nLet M be a (possibly uncountable) set of probability measures on (X∞,F∞). Ryabko (2010, Thm. 4) shows that if there is a Q that merges strongly with every P ∈ M, then there is a Bayesian mixture over a countable subset of M that also merges strongly with every P ∈M."
    }, {
      "heading" : "3.4.2 Weak Merging",
      "text" : "In Definition 3.24 the supremum ranges over all measurable sets A ∈ F∞ which includes tail events. Instead, we may restrict the supremum to the next few symbols. This is known as weak merging.\nDefinition 3.29 (Weak Merging). Q weakly merges with P iff for every d ∈ N, Dt+d(Q,P | x<t)→ 0 as t→∞ P -almost surely.\nThe following lemma gives an equivalent formulation of weak merging.\nLemma 3.30 (Lehrer and Smorodinsky, 1996, Rem. 5). Q weakly merges with P if and only if Dt(Q,P | x<t)→ 0 as t→∞ P -almost surely.\nUnfortunately, weak dominance is not sufficient for weak merging (Lehrer and Smorodinsky, 1996, Ex. 10). We need the following stronger condition, that turns out to be (almost) necessary. In the following, let Yt := Q(x1:t)/P (x1:t) denote the P -martingale from Proposition 3.23.\nTheorem 3.31 (Kalai and Lehrer, 1994, Prop. 5a). If P -almost surely Yt+1/Yt → 1, then Q merges weakly with P .\nExample 3.32 (Laplace Rule 2). Suppose we use the Laplace rule from Example 3.12 to predict a Bernoulli(r) process. By the strong law of large numbers, ρL(xt | x<t)→ r almost surely. Therefore we can use Theorem 3.31 to conclude that ρL merges weakly with Bernoulli(r) for all r ∈ [0, 1]. (Note that strongly merging with every Bernoulli process is impossible; Ryabko, 2010, p. 7) 3"
    }, {
      "heading" : "34 Learning",
      "text" : "The following is a converse to Theorem 3.31.\nTheorem 3.33 (Kalai and Lehrer, 1994, Prop. 5b). If Q merges weakly with P , then Yt+1/Yt → 1 in P -probability.\nUnfortunately, weak dominance is not enough to guarantee weak merging.\nExample 3.34 (Weak Dominance;WeakMerging; Ryabko and Hutter, 2007, Prop. 7). Let X = {0, 1} and let f be any arbitrarily slowly monotone growing function with f(t)→∞. Define P (1∞) := 1, the sequence (ti)i∈N such that f(ti+1) ≥ 2f(ti), and\nQ(xt | x<t) :=  1 2 if t = ti for some i ∈ N, 1 if t 6= ti and xt = 1, and 0 otherwise.\nNow Q dominates P with coefficients f by construction and Q weakly dominates P if f grows subexponentially. However, |Q(1 | 1t) − P (1 | 1t)| ≥ 1/2 for infinitely many t ∈ N. 3"
    }, {
      "heading" : "3.4.3 Almost Weak Merging",
      "text" : "The following definition is due to Lehrer and Smorodinsky (1996, Def. 10).\nDefinition 3.35 (Almost Weak Merging). Q almost weakly merges with P iff for every d ∈ N\n1\nt t∑ k=1 Dt+d(Q,P | x<t)→ 0 as t→∞ P -almost surely.\nThere is also an analogue of Lemma 3.30 for almost weakly merging in the sense that we can equivalently set d = 0 (Lehrer and Smorodinsky, 1996, Rem. 6).\nRemark 3.36 (Weak Merging and Merging in KL-Divergence). From Lemma 2.13 follows that weak merging is implied by KLd(P,Q | x) → 0 P -almost surely and almost weak merging is implied by ∑t k=1 KL1(P,Q | x<k) ∈ o(t) P -almost surely, i.e., KLt(P,Q) ∈ o(t) (Ryabko and Hutter, 2008, Lem. 1). The converse is generally false. 3\nThe following proposition relates the three notions of merging.\nProposition 3.37 (Strong Merging ⇒ Weak Merging ⇒ Almost Weak Merging). If Q merges strongly with P , then Q merges weakly with P . If Q merges weakly with P , then Q merges almost weakly with P .\nProof. Follows immediately from the definitions.\nTheorem 3.38 (Weak Dominance⇒ Almost Weak Merging; Lehrer and Smorodinsky, 1996, Thm. 4). If Q weakly dominates P , then Q merges almost weakly with P .\n§3.5 Predicting 35\nFrom Theorem 3.38 we get that the speed prior (Example 3.11) merges almost weakly with any probability distribution estimable in polynomial time.\nWe also have the following converse to Theorem 3.38.\nTheorem 3.39 (Almost Weak Merging⇒Weak Dominance; Lehrer and Smorodinsky, 1996, Cor. 7). If Q is locally absolutely continuous with respect to P , Q merges almost weakly with P , and P -almost surely lim inft→∞ Yt+1/Yt > 0, then Q weakly dominates P ."
    }, {
      "heading" : "3.5 Predicting",
      "text" : "In Section 3.4 we wanted Q to acquire the correct beliefs about P . In this section, we exploit the accuracy of our beliefs for predicting individual symbols. We derive bounds on the number of errors Q makes when trying to predict a string drawn from P .\nSince the data drawn from P is stochastic, we cannot expect to make a finite number of errors. Even the perfect predictor that knows P generally makes an infinite number of errors. For example, trying to predict the Lebesgue measure λ (Example 2.15), in expectation we make half an error in every time step. So instead we are asking about the asymptotic error rate of a predictor based on Q compared to a predictor based on P , the prediction regret .\nLet xRt be the t-th symbol predicted by the probability measure R according to the maximum likelihood estimator:\nxRt :∈ arg max a∈X R(x<ta | x<t). (3.4)\nThe instantaneous error of a R-based predictor is defined as\neRt := { 0 if xt = xRt , and 1 otherwise.\nand the cumulative error is\nERt := t∑ k=1 eRk .\nNote that both et and Et are random variables.\nDefinition 3.40 (Prediction Regret). In time step t the prediction regret is EQt − EPt and the expected prediction regret is E [ EQt − EPt ] .\nMore generally, we could also follow Hutter (2001b) and phrase predictive performance in terms of loss: given a loss function ` : X ×X → R the predictor Q suffers an (instantaneous) loss of `(xQt , xPt ) in time step t. If the loss function ` is bounded in [0, 1], many of the results for prediction regret also hold for cumulative loss (for Section 3.5.2 we also need `(a, a) = 0 for all a ∈ X ). In this chapter we chose to phrase the results in terms of prediction errors instead of loss because prediction errors are conceptionally simpler."
    }, {
      "heading" : "36 Learning",
      "text" : "Example 3.41 (Good Prediction Regret ; Merging/Compatibility). Good prediction regret does not imply (weak/strong) merging or (weak) dominance: Let P := Bernoulli(1/3) and Q := Bernoulli(1/4). Clearly P and Q do not merge (weakly) or (weakly) dominate each other. However, a P -based predictor always predicts 0, and so does a Q-based predictor. Therefore the prediction regret EQt − EPt is always 0. 3\nExample 3.42 (Adversarial Sequence; Legg, 2006, Lem. 4). No learning distribution Q will learn to predict everything. We can always define a Q-adversarial sequence z1:∞ recursively according to\nzt := { 0 if Q(0 | z<t) < 1/2, and 1 if Q(0 | z<t) ≥ 1/2.\nIn every time step the probability that a Q-based predictor makes an error is at least 1/2, hence eQt ≥ 1/2 and E Q t ≥ t/2. But z1:∞ is a deterministic sequence, thus an informed predictor makes zero errors. Therefore the prediction regret of Q on the sequence z1:∞ is linear. 3"
    }, {
      "heading" : "3.5.1 Dominance",
      "text" : "We start with the prediction regret bounds proved by Hutter (2001b) in case the learning distribution Q dominates the true distribution P . In the following, let cP denote the constant from Definition 3.3.\nTheorem 3.43 (Hutter, 2007b, Eq. 5 & 8). For all P and Q,√ EPEQn − √ EPEPn ≤ √ 2KLn(P,Q).\nThe following bound on prediction regret then follows easily, but it is a factor of √ 2\nworse than the bound stated by Hutter (2005, Thm. 3.36).\nCorollary 3.44 (Expected Prediction Regret). For all P and Q,\n0 ≤ EP [ EQn − EPn ] ≤ 2KLn(P,Q) + 2 √ 2KLn(P,Q)EPEPn .\nProof. From Theorem 3.43 we get\nEP [ EQn − EPn ] = (√ EPEQn + √ EPEPn )(√ EPEQn − √ EPEPn ) ≤ (√ EPEQn + √ EPEPn )√ 2KLn(P,Q)\n≤ (√ 2KLn(P,Q) + √ EPEPn + √ EPEPt )√ 2KLn(P,Q)\n= 2KLn(P,Q) + 2 √ 2KLn(P,Q)EPEPn .\n§3.5 Predicting 37\nIf Q dominates P , then we have KLn(P,Q) ≤ − ln cP :\nKLn(P,Q) = ∑ x∈Xn P (x) log P (x) Q(x) ≤ ∑ x∈Xn P (x) log 1 cP = − log cP (3.5)\nThis invites the following corollary.\nCorollary 3.45 (Prediction Regret for Dominance; Hutter, 2005, Cor. 3.49). If Q dominates P , then the following statements hold.\n(a) EPEQ∞ is finite if and only if EPEP∞ is finite. (b) √ EPEQ∞ − √ EPEP∞ ∈ O(1)\n(c) EPEQt /EPEPt → 1 for EPEPt →∞. (d) EP [ EQt − EPt ] ∈ O (√ EPEPt ) .\nIf the true distribution P is deterministic, we can improve on these bounds:\nExample 3.46 (Predicting a Deterministic Measure). Suppose we are predicting a deterministic measure P that assigns probability 1 to the infinite string x1:∞. If P is dominated by Q, the total expected prediction regret EPEQ∞ is bounded by −2 ln cP by Corollary 3.44. This is easy to see: every time we predict a wrong symbol a 6= xt, then Q(a | x<t) ≥ Q(xt | x<t), so Q(xt | x<t) ≤ 1/2. Therefore Yt ≤ Yt−1/2 and by dominance Yt ≥ cP . Hence a prediction error can occur at most − log cP times. 3\nGenerally, the O(EPEPt ) bounds on expected prediction regret given in Corollary 3.45 are essentially unimprovable:\nExample 3.47 (Lower Bounds on Prediction Regret). Set X := {0, 1} and consider the uniform measure λ from Example 2.15. For each time step t, we have λ(0 | x<t) = λ(1 | x<t) = 1/2, so the argmax in (3.4) ties and hence it does not matter whether we predict 0 or 1. We take two predictors P and Q, where P always predicts 0 and Q always predicts 1. Let Zt := E Q t − EPt . Since their predictions never match, Zt is an ordinary random walk with step size 1. We have (Weisstein, 2002)\nlim sup t→∞ EP [EQt − EPt ]√ t\n= √ 2/π\nand for the law of the iterated logarithm (Durrett, 2010, Thm. 8.8.3)\nlim sup t→∞ EQt − EPt√ 2t log log t = 1 P -almost surely.\nBoth bounds are known to be asymptotically tight. 3\nWhile Example 3.47 shows that the bounds from Corollary 3.45 are asymptotically tight, they are misleading because in most cases, we can do much better. According"
    }, {
      "heading" : "38 Learning",
      "text" : "to the following theorem, the worst case bounds are only attained if P (xt | x<t) is sufficiently close to 1/2.\nTheorem 3.48 (Expected Prediction Regret for Nonuniform Measures). If X = {0, 1} and there is an ε > 0 such that |P (xt | x<t)− 1/2| ≥ ε for all x1:t ∈ X ∗, then\nEP [ EQt − EPt ] ≤ KLt(P,Q)\nε .\nProof. Recall the definition of entropy in nats:\nEnt(p) := −p ln p− (1− p) ln(1− p).\nThe second order Taylor approximation of Ent at 1/2 is\nf(p) = ln 2− 2(p− 12) 2.\nOne can check that f(p) ≥ Ent(p) for all 0 ≤ p ≤ 1. Define p := P (xPt | x<t) ≥ 1/2 and q := Q(xQt | x<t) ≥ 1/2 to ease notation. Consider the function\ng(p, q, ε) := p− (1− p)− ε−1 ( p ln p1−q + (1− p) ln 1−p q ) which is strictly increasing as q decreases, so from q ≥ 1/2 we get\ng(p, q, ε) ≤ 2p− 1− ε−1 ln 2 + ε−1Ent(p) ln 2 ≤ 2p− 1− ε−1 ln 2 + ε−1f(p) ln 2 = 2p− 1− ε−12(p− 12) 2,\nwhich decreases as p increases, hence it is maximized for p = 1/2 + ε,\ng(p, q, ε) ≤ 2ε− ε−12ε2 = 0\nTherefore g is nonpositive. If xQt = xPt , the one-step error is 0. Otherwise EP [et | x<t] = p − (1 − p) and g(p, q, ε) = EP [et | x<t] − ε−1KL1(P,Q | x<t), so we get EP [et | x<t] ≤ ε−1KL1(P,Q | x<t). Summing this from t = 1 to n yields the claim.\nE [ EQn − EPn ] ≤ ε−1KLn(P,Q)."
    }, {
      "heading" : "3.5.2 Absolute Continuity",
      "text" : "Theorem 3.49 (Prediction with Absolute Continuity). If Q P , then√ EQt − √ EPt ≤ O (√ log log t ) P -almost surely.\nThe proof idea is inspired by Miller and Sanchirico (1999). We think of P and Q as two players in a zero-sum betting game. In every time step t, the players will make a\n§3.5 Predicting 39\nbet on the outcome of xt. If xt = x Q t 6= xPt , then Q wins $1 from P , if xt = xPt 6= x Q t , then Q loses $1 to P . Otherwise xQt = xPt or x Q t 6= xt 6= xPt and neither player gains or loses money. Since Q predicts according to the maximum likelihood principle (3.4), it is rational to accept the bet from Q’s perspective. In Q’s eyes, the worst case is a fair bet, so Q will not lose more money than it would lose on a random walk. The law of the iterated logarithm gives a Q-probability one statement about this bound, which transfers to P by absolute continuity.\nProof. Define the stochastic process Zt := E Q t − EPt . Since EQ[eRt ] = Q(xRt | x<t), we get\nEQ[Zt+1 | Ft] = Q(xQt | x<t)−Q(xPt | x<t) + Zt ≥ Q(xQt | x<t)−Q(x Q t | x<t) + Zt = Zt,\nhence (Zt)t∈N is a Q-submartingale. In the worst case (for Q), (Zt)t∈N is just a random walk with step size 1. But Zt can only move if Q and P predict a different symbol. If this happens, at least one of them makes an error. Let mt be the number of steps Zt has moved (Zt+1 6= Zt). Then mt ≤ EQt + EPt and mt ≤ t. By the law of the iterated logarithm (Durrett, 2010, Thm. 8.8.3),\nlim inf t→∞ Zt√ 2mt log logmt = −1\nQ-almost surely. We define the event\nA := { ∃C ∀t. Zt ≥ −C √ mt log logmt } .\nThen Q(A) = 1, hence P (A) = 1 by absolute continuity.\nEQt − EPt = Zt ≤ C √ (EQt + E P t ) log log t ≤ C (√ EQt + √ EPt )√ log log t\nDividing both sides by √ EQt + √ EPt yields that there is a P -almost surely finite random\nvariable C such that √ EQt − √ EPt ≤ C √ log log t.\nThis invites the following immediate corollary.\nCorollary 3.50 (Prediction Regret for Absolute Continuity). If Q P , then\nEQt − EPt ∈ O ( log log t+ √ EPt log log t ) P -almost surely.\nProof. Analogously to the proof of Corollary 3.44.\nWhile Corollary 3.50 establishes an almost sure prediction regret bound, it is different from the bound on expected prediction regret from Corollary 3.44; bounds on"
    }, {
      "heading" : "40 Learning",
      "text" : "E[EQt − EPt ] are incomparable to almost sure bound given in Theorem 3.49: for a sequence of nonnegative (unbounded) random variables convergence in mean does not imply almost sure convergence (Stoyanov, 2013, Sec. 14.7) or vice versa (Stoyanov, 2013, Sec. 14.8ii).\nWe proceed to establish an improved prediction regret bound in case P is nonuniform analogously to Theorem 3.48.\nTheorem 3.51 (Prediction Regret for Nonuniform Measures). If Q P , X = {0, 1}, and there is an ε > 0 such that with P -probability 1\n|P (xt | x<t)− 1/2| ≥ ε\nfor all t ∈ N, then P -almost surely EQt − EPt ∈ O(1).\nProof. If |P (xt | x<t) − 1/2| ≥ ε, then for large enough t, Q will have merged with P (Theorem 3.25) and hence |Q(xt | x<t)− 1/2| ≥ ε/2 infinitely often.\nThus Zt has an expected gain of ε/2 if the predictors disagree. Therefore Zt →∞ Q-almost surely. Consequently, the set\nA := {∃t0 ∀t ≥ t0. Zt ≥ 0}\nhas Q-measure 1. By absolute continuity, it also has P -measure 1, hence there is a P -almost surely finite random variable C such that for all t, Zt ≥ −C.\nThere is another argument that we could use to show that under the condition of Theorem 3.51 EQt −EPt is almost surely finite: If P is absolutely continuous with respect to Q, then Q merges strongly with P and hence Q merges weakly with P . Therefore almost surely there is a t0 such that for all t ≥ t0 we have |Q(xPt | x<t)−P (xPt | x<t)| < ε, thus xQt = xPt for t ≥ t0."
    }, {
      "heading" : "3.5.3 Dominance with Coefficients",
      "text" : "Lemma 3.52 (KL Divergence and Dominance With Coefficients). If Q dominates P with coefficients f , then KLt(Q,P ) ≤ ln f(t).\nProof. Analogous to (3.5).\nThis lets us derive an analogous regret bound to Corollary 3.44.\nCorollary 3.53 (Expected Prediction Regret for Dominance With Coefficients). If Q dominates P with coefficients f , then\nEP [ EQn − EPn ] ≤ 2 ln f(t) + 2 √ 2EPEPn ln f(t).\nProof. Apply Lemma 3.52 to Corollary 3.44.\nFor weak dominance we get sublinear prediction regret.\n§3.6 Learning with Algorithmic Information Theory 41\nCorollary 3.54 (Sublinear Prediction Regret for Weak Dominance). If Q weakly dominates P , then EP [EQn − EPn ] ∈ o(t).\nProof. By Remark 3.9 ln f ∈ o(t). Applying Corollary 3.53 we get\nEP [ EQn − EPn ] ≤ 2o(t) + 2 √ 2EPEPn o(t) ≤ 2o(t) + 2 √ 2O(t)o(t) ∈ o(t)."
    }, {
      "heading" : "3.6 Learning with Algorithmic Information Theory",
      "text" : "Algorithmic information theory provides a theoretical framework to apply the probability theory results from the previous sections. In the following we discuss Solomonoff’s famous theory of induction (Section 3.6.1), the speed prior (Section 3.6.2), and learning with a universal compression algorithm (Section 3.6.3)."
    }, {
      "heading" : "3.6.1 Solomonoff Induction",
      "text" : "Solomonoff (1964, 1978) proposed a theory of learning, also known as universal induction or Solomonoff induction. It encompasses Ockham’s razor by favoring simple explanations over complex ones, and Epicurus’ principle of multiple explanations by never discarding possible explanations. See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff’s theory and its philosophical motivations and Sterkenburg (2016) for a critique of its optimality.\nAt the core of this theory is Solomonoff’s distribution M , as defined in Example 3.5. SinceM dominates all lower semicomputable semimeasures, we get all the merging and prediction results from Section 3.4 and Section 3.5: when drawing a string from any computable measure P , M arrives at the correct belief for any hypothesis.\nCorollary 3.55 (Strong Merging for Solomonoff Induction). M merges strongly with every computable measure.\nProof. From Proposition 3.16a and Theorem 3.25.\nCorollary 3.56 (Expected Prediction Regret for Solomonoff Induction). For all computable measures P ,\nEP [ EMt − EPt ] ≤ K(P ) ln 4 + √ 2EPEPt K(P ) ln 16.\nProof. From Corollary 3.44 and cP = 2−K(P ).\nRemark 3.57 (Converging Fast and Slow). The convergence of M to a computable P is fast in the sense of Corollary 3.56: M cannot make many more prediction errors than P in expectation. When predicting an infinite computable sequence x1:∞, the total number of prediction errors is bounded by |p|2 ln 2 ≈ 1.4|p| where p is a program that generates x1:∞ (Example 3.46).\nThe convergence of M to P is also slow in the sense that M(xt | x<t) → 1 slower than any computable function since 1−M(xt | x<t) ×≥ 2−minn≥tK(n) for all t. 3"
    }, {
      "heading" : "42 Learning",
      "text" : "The bound from Corollary 3.56 is not optimal. Even if we knew the program p generating the sequence x1:∞, there might be a shorter program p′ that computes x1:∞; hence the improved bound EM∞ ≤ |p′|2 ln 2 also holds. Since Kolmogorov complexity is incomputable, we can’t find the ‘best’ bound algorithmically.\nSolomonoff induction may even converge on some incomputable measures.\nExample 3.58 (M Converges on Some Incomputable Measures). Let r be an incomputable real number. Then the measure P := Bernoulli(r) is not computable and M is not absolutely continuous with respect to P : for\nA := { x ∈ X∞ ∣∣∣ lim t→∞ ones(x1:t) = r }\nwe have P (A) = 1 but M(A) = 0. Since M L P we get from Theorem 3.27 that M does not merge with P . Nevertheless, M still succeeds at prediction because it dominates Bernoulli(q) for each rational q and the rationals are dense around r. According to Lehrer and Smorodinsky (1996, Lem. 3), this implies that M weakly dominates P and by Theorem 3.38 M almost weakly merges to P . 3\nThe fact that M does not merge strongly with every Bernoulli(r) process is not a failure of Solomonoff’s prior. Ryabko (2010, p. 7) shows that for the class of all Bernoulli measures there is no probability measure that merges strongly with each of them.\nThe definition of M has only one parameter: the choice of the universal Turing machine. The effect of this choice on the function K can be uniformly bounded by a constant by the invariance theorem (Li and Vitányi, 2008, Thm. 3.1.1). Hence the choice of the UTM changes the prediction regret bound from Corollary 3.56 only by a constant. This constant can be large, preventing any finite-time guarantees that are independent of the UTM. However, asymptotically Solomonoff induction succeeds even for terrible choices of the UTM.\nThe Solomonoff normalization Mnorm of M is defined according to Definition 2.16. While Mnorm dominates M according to Lemma 2.17 and thus every lower semicomputable semimeasure, in some respects, Mnorm behaves a little differently from M . Another way to complete the semimeasure M into a measure is given in the following example.\nExample 3.59 (The Measure Mixture; Gács, 1983, p. 74). The measure mixture M is defined as\nM(x) := lim n→∞ ∑ y∈Xn M(xy). (3.6)\nIt is the same as M except that the contributions by programs that do not produce infinite strings are removed: for any such program p, let k denote the length of the finite string generated by p. Then for |xy| > k, the program p does not contribute to M(xy), hence it is excluded from M(x).\nSimilarly toM , the measure mixtureM is not a (probability) measure sinceM( ) < 1; but in this case normalization (2.2) is just multiplication with the constant 1/M( ), leading to the normalized measure mixture Mnorm. 3\n§3.6 Learning with Algorithmic Information Theory 43\nEven thoughM merges strongly with any computable measure P with P -probability 1, Lattimore and Hutter (2013, 2015) show that generally it does not hold for all MartinLöf random sequences (which also form a set of P -probability 1). Hutter and Muchnik (2007, Thm. 6) construct non-universal lower semicomputable semimeasures that have this convergence property for all P -Martin-Löf random sequences. For infinite nonrandom sequences whose bits are selectively predicted by some total recursive function, Lattimore et al. (2011, Thm. 10) show that the normalized Solomonoff measure Mnorm converges to 1 on the selected bits. This does not hold for the unnormalized measure M (Lattimore et al., 2011, Thm. 12)."
    }, {
      "heading" : "3.6.2 The Speed Prior",
      "text" : "Solomonoff’s prior M is incomputable (Theorem 6.3); a computable alternative is the speed prior from Example 3.11. In this section we state merging and prediction results for SKt, a speed prior introduced by Filan et al. (2016) formally defined in Example 3.11. It is slightly different from the speed prior defined by Schmidhuber (2002), but for the latter no compatibility properties are known for nondeterministic measures.\nDefinition 3.60 (Estimable in Polynomial Time). A function f : X ∗ → R is estimable in polynomial time iff there is a function g : X ∗ → R computable in polynomial time such that f ×= g.\nFor a measure P estimable in polynomial time the speed prior SKt dominates P with coefficients polynomial in |x| − logP (x) (Filan et al., 2016, Eq. 12). Thus SKt weakly dominates P and we get the following results.\nCorollary 3.61 (Almost Weak Merging for SKt). SKt almost weakly merges with every measure estimable in polynomial time.\nProof. From Theorem 3.38 and Filan et al. (2016, Eq. 12) since logP does not grow superexponentially P -almost surely.\nCorollary 3.62 (Expected Prediction Regret for SKt; Filan et al., 2016, Thm. 9). For all measures P estimable in polynomial time,\nEP [ ESKtn − EPn ] ∈ O ( log n+ √ EPEP∞ log n ) .\nProof. From Corollary 3.44 and Filan et al. (2016, Eq. 14)."
    }, {
      "heading" : "3.6.3 Universal Compression",
      "text" : "Solomonoff’s distribution can be approximated using a standard compression algorithm, motivated by the similarityM(x) ≈ 2−Km(x), where Km denotes monotone Kolmogorov complexity. The function Km is a universal compressor , compressing at least as well as any other recursively enumerable program.\nGács (1983) shows that the similarity M ≈ 2−Km is not an equality. However, the difference between − logM and Km is very small: the best known lower bound is due"
    }, {
      "heading" : "44 Learning",
      "text" : "to Day (2011) who shows that Km(x) > − logM(x) +O(log log |x|) for infinitely many x ∈ X ∗.\nNevertheless, 2−Km dominates every computable measure (Li and Vitányi, 2008, Thm. 4.5.4 and Lem. 4.5.6ii(d); originally proved by Levin, 1973). Hence all the strong results that hold for Solomonoff induction (prediction regret and strong merging) also hold for compression: we apply Theorem 3.25 and Corollary 3.44 to get the following results. See Hutter (2006a) for further discussion on using the universal compressor Km for learning.\nCorollary 3.63 (Strong Merging for Universal Compression). The distribution 2−Km(x) merges strongly with every computable measure.\nCorollary 3.64 (Expected Prediction Regret for Universal Compression). For Q(x) := 2−Km(x) and for all computable measures P there is a constant cP such that\nEP [ EQt − EPt ] ≤ cP + √ cPEPEPt .\nThis provides a theoretical basis for viewing compression as a general purpose learning algorithm. In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c).\nPractical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal. Hence they do not dominate every computable distribution. As with the speed prior, what matters is the rate at which Yt = Q(x1:t)/P (x1:t) goes to 0, i.e., does the compressor weakly dominate the true distribution in the sense of Definition 3.8?\nVeness et al. (2015) successfully apply the Lempel-Ziv compression algorithm as a learning algorithm for reinforcement learning; however, some preprocessing of the data is required. More remotely, Vitányi et al. (2009) use standard compression algorithms to classify mammal genomes, languages, and classical music."
    }, {
      "heading" : "3.7 Summary",
      "text" : "Ultimately, whether learning succeeds depends on the rate at which the nonnegative P -martingale Q/P goes to 0 (when drawing from P ). If Q/P does not converge to zero, then Q merges strongly with P and thus arrives at correct beliefs about any hypothesis, including tail events. If Q/P converges to zero subexponentially, then Q merges almost weakly with P and thus asymptotically has incorrect beliefs about the immediate future only a vanishing fraction of the time.\nCorollary 3.44 bounds the expected prediction regret by the KL-divergence between P and Q plus a √ EPEPt term. The KL-divergence is in turn bounded by the rate at which Q/P goes to zero. It is constant if Q dominates P and bounded by ln f if Q dominates P with coefficients f . If Q weakly dominates P , then the KL-divergence is sublinear. We also derived bounds on the prediction regret for absolute continuity (Section 3.5.2). Remarkably, the bounds are only log log t worse than the bound we get from dominance. Moreover, they hold almost surely instead of in expectation.\n46 Learning\n§3.7 Summary 47\nNext, we showed that the √ EPEPt term is generally unimprovable (Example 3.47). However, it comes only from predicting measures that assign probabilities close to 1/2. If we can bound P away from 1/2, then the √ EPEPt term disappears (Theorem 3.48 and Theorem 3.51). Table 3.1 lists our learning distributions. The Bayesian mixture is the strongest since it dominates every measure from the given classM (Example 3.4). The minimum description length model MDLx does not have this property, yet it still merges strongly with every measure from the class (Example 3.14 and Theorem 3.28). The Laplace rule is only useful for learning i.i.d. measures; it merges weakly with every Bernoulli process (Example 3.12 and Example 3.32). We also discussed some learning distributions from algorithmic information theory. Solomonoff’s prior is a Bayesian mixture over all lower semicomputable semimeasures (Example 3.5 and Wood et al., 2011). Like the universal compressor it dominates and hence merges strongly with all computable measures. The speed prior dominates all probability measures estimable in polynomial time with polynomial coefficients (Example 3.11), and thus merges weakly with each of them.\nTable 3.2 summarizes the results from this chapter and Figure 3.1 illustrates their logical relationship and their origin.\nWe conclude this chapter with a paradox from the philosophy of science.\nRemark 3.65 (The Paradox of Confirmation). Recall the black raven problem introduced in Example 3.1; the hypothesis ‘all ravens are black’ is denoted with H. The paradox of confirmation, also known as Hempel’s paradox (Hempel, 1945), relies on the following three principles.\n• Nicod’s criterion (Nicod, 1961, p. 67): observing an F that is a G increases our belief in the hypothesis that all F s are Gs.\n• The equivalence condition: logically equivalent hypotheses are confirmed or disconfirmed by the same evidence.\n• The paradoxical conclusion: a green apple confirms H.\nThe argument goes as follows. The hypothesisH is logically equivalent to the hypothesis H ′ that all non-black objects are non-ravens. According to Nicod’s criterion, any nonblack non-raven, such as a green apple, confirms H ′. But then the equivalence condition entails the paradoxical conclusion.\nThe paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey. Support for Nicod’s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.\nA Bayesian reasoner might be tempted to argue that a green apple does confirm the hypothesis H, but only to a small degree, since there are vastly more non-black objects than ravens (Good, 1960). This leads to the acceptance of the paradoxical conclusion,"
    }, {
      "heading" : "48 Learning",
      "text" : "and this solution to the confirmation paradox is known as the standard Bayesian solution. Vranas (2004) shows that this solution is equivalent to the assertion that blackness is equally probable regardless of whether H holds: P (black|H) ≈ P (black).\nThe following is a very concise example against the standard Bayesian solution by Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, one white raven, and a million other birds. Now we draw a bird uniformly at random, and it turns out to be a black raven. Contrary to what Nicod’s criterion claims, this is strong evidence that we are in fact in the second world, and in this world non-black ravens exist.\nFor another, more intuitive example: Suppose you do not know anything about ravens and you have a friend who collects atypical objects. If you see a black raven in her collection, surely this would not increase your belief in the hypothesis that all ravens are black.\nIn Leike and Hutter (2015d) we investigate the paradox of confirmation in the context of Solomonoff induction. We show that the paradoxical conclusion is avoided because Solomonoff induction violates Nicod’s criterion: There are time steps when (counterfactually) observing a black raven disconfirms the hypothesis that all ravens are black. When predicting a deterministic computable sequence Nicod’s criterion is even violated infinitely often. However, if we normalize Solomonoff’s prior and observe a deterministic computable infinite string, Nicod’s criterion is violated at most finitely many times. These results are independent of the choice of the universal Turing machine.\nWe must conclude that violating Nicod’s criterion is not a fault of Solomonoff induction. Instead, we should accept that for Bayesian reasoning Nicod’s criterion, in its generality, is false! Quoting the great Bayesian master Jaynes (2003, p. 144):\nIn the literature there are perhaps 100 ‘paradoxes’ and controversies which are like this, in that they arise from faulty intuition rather than faulty mathematics. Someone asserts a general principle that seems to him intuitively right. Then, when probability analysis reveals the error, instead of taking this opportunity to educate his intuition, he reacts by rejecting the probability analysis. 3\nChapter 4\nActing"
    }, {
      "heading" : "I ought never to act except in such a way that I could also will that my maxim should",
      "text" : "become a universal prior. — Immanuel Kant\nRecall our decomposition of intelligence into learning and acting from Equation 1.1. The previous chapter made the notion of learning precise and provided several examples of learning distributions for the non-i.i.d. setting (see Table 3.1). Learning is passive: there is no interaction with the data-generating process. In this chapter we transition into the active setting: we consider an agent acting in an unknown environment in order to achieve a goal . In our case, this goal is maximizing reward; this is known as reinforcement learning. Where this reward signal originates does not concern us here.\nIn this thesis we consider is the general reinforcement learning problem in which we do not make several of the typical simplifying assumptions (see Table 1.1). Environments are only partially observable, have infinitely many states, and might contain traps from which the agent cannot escape. The context for making decision is the agent’s entire history; its behavior is given by a policy that specifies how the agent behaves in any possible situation.\nA central quantity in reinforcement learning is the value function. The value function quantifies the expected future discounted reward. Since the agent seeks to maximize reward, it aims to adopt a policy that has high value. Since the agent’s environment is unknown to the agent, learning the value function is part of the challenge; otherwise we would call this planning.\nIf our agent is capable of learning in the sense of Chapter 3, then it learns the value of its own policy (on-policy value convergence). However, generally the agent does not learn to predict the value of counterfactual actions, actions that it does not take. Learning off-policy is hard because the agent receives no evidence about what would have happened on counterfactual actions. Nevertheless, off-policy learning is highly desirable because we want the agent to be confident that the policy it is currently following is in fact the best one; we want it to accurately predict that the counterfactual actions have less value.\nThis brings us back to the central theme of reinforcement learning: the tradeoff between exploration and exploitation. Asymptotically the agent needs to focus on exploitation, i.e., take the actions that it thinks yield the highest expected rewards. If the agent explores enough, then all actions are on-policy because they are all actions that\n49"
    }, {
      "heading" : "50 Acting",
      "text" : "the agent sometimes takes. Then on-policy learning ensures that the agent understands the consequences of every action and can confidently choose the best action. Effective exploration is performed by knowledge-seeking agents; these agents ignore the rewards and just focus on exploration.\nThis chapter introduces the central concepts of general reinforcement learning. It is mostly based on Hutter (2005) and Lattimore (2013). Section 4.1 specifies the general reinforcement learning problem, discusses discounting (Section 4.1.1), our implicit assumptions (Section 4.1.2), and typical environment classes (Section 4.1.3). Section 4.2 discusses the value function and its properties. In Section 4.3 we introduce the agents: AIXI (Section 4.3.1), knowledge-seeking agents (Section 4.3.2), BayesExp (Section 4.3.3), and Thompson sampling (Section 4.3.4)."
    }, {
      "heading" : "4.1 The General Reinforcement Learning Problem",
      "text" : "In reinforcement learning, an agent interacts with an environment: at time step t ∈ N the agent takes an action at ∈ A and subsequently receives a percept et = (ot, rt) ∈ E consisting of an observation ot ∈ O and a reward rt ∈ R. This cycle then repeats for time step t+ 1 (see Figure 4.1).\nA history is an element of (A × E)∗ and lists the actions the agent took and the percepts it received. We use æ ∈ A × E to denote one interaction cycle, and æ<t = æ1æ2 . . .æt−1 to denote a history of length t − 1. For our agent, the history is a sufficient statistic about the past and in general reinforcement learning there is no simpler sufficient statistic.\nFor example, consider the agent to be a robot interacting with the real world. Its actions are moving the motors in its limbs and wheels and sending data packets over a network connection. Its observations are data from cameras and various other sensors. The reward could be provided either by a human supervisor or through a reward module that checks whether a predefined goal has been reached. The history is the collection of all the data it received and emitted in the past. The division of the robot’s interaction with the environment into discrete time steps might seem a bit unnatural at first since the real world evolves according to a continuous process. However, note that the electronic components used in robots operate at discrete frequencies anyway.\n§4.1 The General Reinforcement Learning Problem 51\nIn order to specify how the agent behaves in any possible situation, we define a policy : a policy is a function π : (A×E)∗ → ∆Amapping a history æ<t to a distribution over actions π( · | æ<t) taken after seeing this history. Usually we do not distinguish between agent and policy. An environment is a function ν : (A × E)∗ × A → ∆E mapping a history æ<t and an action at to a distribution ν( · | æ<tat) over the percepts received after the history æ<t and action at. We use µ to denote the true environment.\nEquivalently, Hutter (2005) defines environments as chronological contextual semimeasures.1 A contextual semimeasure ν takes a sequence of actions a1:∞ as input and returns a semimeasure ν( · ‖ a1:∞) over E]. A contextual semimeasure ν is chronological iff percepts at time t do not depend on future actions, i.e., ν(e1:t ‖ a1:∞) = ν(e1:t ‖ a′1:∞) whenever a1:t = a′1:t. For chronological contextual semimeasures we write ν(e1:t ‖ a1:t) instead of ν(e1:t ‖ a1:∞). The two definition can be translated using the identities\nν(e1:t ‖ a1:t) = t∏\nk=1\nν(ek | æ<kak) and ν(et | æ<tat) = ν(e1:t ‖ a1:t) ν(e<t ‖ a<t) . (4.1)\nIf the policy π always assigns probability 1 to one of the actions, then π is called deterministic. Likewise, if the environment ν always assigns probability 1 to one of the percepts, then ν is called deterministic. For deterministic policies and environments we also use the notation at = π(æ<t) and et = ν(æ<tat). A deterministic policy π is consistent with history æ<t iff ak = π(æ<k) for all k < t. Likewise, a deterministic environment ν is consistent with history æ<t iff ek = ν(æ<kak) for all k < t.\nDefinition 4.1 (History Distribution). An environment ν together with a policy π induces a history distribution\nνπ(æ<t) := t∏\nk=1\nπ(ak | æ<k)ν(ek | æ<kak).\nWe denote an expectation with respect to the history distribution νπ with Eπν .\nThe history distribution is a (semi)measure on (A×E)∞. In the language of measure theory, our σ-algebra is the σ-algebra F∞ generated by the cylinder sets introduced in Section 2.1. The filtration (Ft)t∈N formalizes that at time step t we have seen exactly the history æ<t (we use the σ-algebra Ft−1). To simplify notation and help intuition, we simply condition expectations and probability measures with the history æ<t instead of Ft−1 and sweep most of the measure-theoretic details under the rug.\nWith these preliminaries out of the way, we can now specify the general reinforcement learning problem.\nProblem 4.2 (General Reinforcement Learning Problem). Given an arbitrary class of environmentsM, choose a policy π that maximizes µπ-expected reward when interacting with any environment µ ∈M.\n1Hutter (2005) calls them chronological conditional semimeasures. This is confusing because contextual semimeasures do not specify conditional probabilities; the environment is not a joint probability distribution over actions and percepts."
    }, {
      "heading" : "52 Acting",
      "text" : "Problem 4.2 is kept vague on purpose: it does not say how we should balance between achieving more rewards in some environments while achieving less in others. In other words, we leave open what an optimal solution to the general reinforcement learning problem is. This turns out to be a notoriously difficult question that we discuss in Chapter 5.\nAs promised in the title of this thesis, we take the nonparametric approach. For the rest of this thesis, fixM to be any countable set of environments. While the true environment is unknown, we assume it belongs to the classM (the realizable case). As long as the classM is sufficiently large (such as the class of all computable environments), this assumption is weak. Some typical choices are discussed in Section 4.1.3.\nOur agent-environment setup shown in Figure 4.1 is known as the dualistic model : the agent is distinct from the environment and influences it only through its actions. In turn, the environment influences the agent only through the percepts. The dualism assumption is accurate for an algorithm that is playing chess, Go, or other (video) games, which explains why it is ubiquitous in AI research. But often it is not true: real-world agents are embedded in (and computed by) the environment, and then a physicalistic model (also called materialistic model or naturalistic model) is more appropriate. Decision making in the physicalistic model is still underdeveloped; see Everitt et al. (2015) and Orseau and Ring (2012a). In this thesis we restrict ourselves to the dualistic model."
    }, {
      "heading" : "4.1.1 Discounting",
      "text" : "The goal in reinforcement learning is to maximize rewards. However, the infinite reward sum ∑∞ t=1 rt may diverge. To get around this technical problem, we let our agent prioritize the present over the future. This is done with a discount function that quantifies how much the agent prefers rewards now over rewards later.\nDefinition 4.3 (Discount Function). A discount function is a function γ : N→ R with γt := γ(t) ≥ 0 and ∑∞ t=1 γt <∞. The discount normalization factor is Γt := ∑∞ k=t γk.\nThere is no requirement that Γt > 0. In fact, we use γ for both, discounted infinite horizon (Γt > 0 for all t), and finite horizon m (Γm−1 > 0 and Γm = 0) where the agent does not care what happens after time step m.\nNote that the way in which we employ discounting is time consistent : the agent does not change its mind about how much it values the reward at time step k over time: reward rk is always discounted with γk regardless of the current time step. For a discussion of general discounting we refer the reader to Lattimore and Hutter (2014).\nDefinition 4.4 (Effective Horizon). The ε-effective horizon Ht(ε) is a horizon that is long enough to encompass all but an ε of the discount function’s mass:\nHt(ε) := min{k | Γt+k/Γt ≤ ε}\nThe effective horizon is bounded iff for all ε > 0 there is a constant cε such that Ht(ε) ≤ cε for all t ∈ N.\nExample 4.5 (Geometric Discounting). The most common discount function is geometric discounting with γt := γt for some constant γ ∈ [0, 1). We get that Γt =∑∞\nk=t γ k = γt/(1 − γ) and the ε-effective horizon is Ht(ε) = dlogγ εe. Hence the effective horizon is bounded. 3\nMore examples for discount functions are given in Table 4.1. From now on, we fix a discount function γ."
    }, {
      "heading" : "4.1.2 Implicit Assumptions",
      "text" : "Throughout this thesis, we make the following assumptions implicitly.\nAssumption 4.6. (a) The discount function γ is computable.\n(b) Rewards are bounded between 0 and 1.\n(c) The set of actions A and the set of percepts E are both finite.\nLet’s motivate these assumptions in turn. Their purpose is to ensure that discounted reward sums are finite and optimal policies exist.\nAssumption 4.6a is a technical assumption that ensures that discounted reward sums are computable. This is important for Chapter 6 and Chapter 7 where we analyse the computability of optimal policies. Note that all discount functions given in Table 4.1 are computable.\nAssumption 4.6b could be relaxed to require only that rewards are bounded. We can rescale rewards rt 7→ crt + d for any c ∈ R+ and d ∈ R without changing optimal policies if the environment ν is a probability measure. (For our computability-related results in Chapter 6, we must assume that rewards are nonnegative.) In this sense Assumption 4.6b is not very restrictive. However, this normalization of rewards into the [0, 1]-interval has the convenient consequence that the normalized discounted reward sum ∑∞ k=t γkrk/Γk is bounded between 0 and 1. If rewards are unbounded, then the discounted reward sum might diverge. Moreover, with unbounded rewards there all kinds of pathological problems where defining optimal actions is no longer straightforward; see Arntzenius et al. (2004) for a discussion.\nAssumption 4.6c is a technical requirement for the existence of optimal policies since it implies that there are only finitely many deterministic policies that differ in the first t"
    }, {
      "heading" : "54 Acting",
      "text" : "time steps. Note that finite action and percept spaces are very natural since it ensures that our agent only receives and emits a finite amount of information in every time step. This is in line with the problems a strong AI is facing: the agent has to remember important information and act sequentially.\nAssumption 4.6b, Assumption 4.6c, and the fact that the discount function is summable guarantee that a deterministic optimal policy exists for every environment according to Lattimore and Hutter (2014, Thm. 10). It would be interesting to relax these assumptions while preserving the existence of optimal policies or at least ε-optimal policies (e.g. use compact action and percept spaces)."
    }, {
      "heading" : "4.1.3 Typical Environment Classes",
      "text" : "The simplest reinforcement learning problems are multi-armed bandits.\nDefinition 4.7 (Multi-Armed Bandit). An environment ν is a multi-armed bandit iff O = {⊥} and ν(et | æ<tat) = ν(et | at) for all histories æ1:t ∈ (A× E)∗.\nIn a multi-armed bandit problem there are no observations and the next reward only depends on the previous action. Intuitively, we are deciding between #A different slot machines (so-called one-armed bandits), pull the lever and obtain a reward. The reward is stochastic, but it is drawn from a distribution that is time-invariant and fixed for each arm.\nA multi-armed bandit is also called bandit for short. Although bandits are the simplest reinforcement learning problem, they already exhibit the exploration-exploitationtradeoff that makes reinforcement learning difficult: do you pull an arm that has the best empirical mean or do you pull an arm that has the highest uncertainty? In bandits it is very easy to come up with policies that perform (close to) optimal asymptotically (e.g., εt-greedy with εt = 1/t). But coming up with algorithms that perform well in practice is difficult, and research focuses on the multiplicative and additive constants on the asymptotic guarantees. Bandits exist in many flavors; see Bubeck and Bianchi (2012) for a survey.\nDefinition 4.8 (Markov Decision Process). An environment ν is a Markov decision process (MDP) iff ν(et | æ<tat) = ν(et | ot−1at) for all histories æ1:t ∈ (A× E)∗.\nIntuitively, in MDPs, the previous observation ot−1 provides a sufficient statistic for the history: given ot−1 and the current action at, the next percept et is independent of the rest of the history. In other words, everything that the agent needs to know to make optimal decisions is readily available in the previous percept. This is why observations are called states in MDPs. Note that bandits are MDPs with a single state.\nMuch of today’s literature on reinforcement learning focuses on MDPs (Sutton and Barto, 1998). They provide a particularly good framework to study reinforcement learning because they are simple enough to be tractable for today’s algorithms, yet general enough to encompass many interesting problems. For example, most of the Atari games (see Figure 1.1 for an overview) are (deterministic) MDPs when combining\n§4.1 The General Reinforcement Learning Problem 55\nthe previous four frames into one percept. While they have a huge state space2 they can still be learned using Q-learning with function approximation (Mnih et al., 2015).\nThe MDP framework is restrictive because it requires the agent to be more powerful than the environment. Since the agent learns, its actions are not independent of the rest of the history given the last action and percept. In other words, learning agents are not Markov. The following definition lifts this restriction and allows the environment to be partially observable.\nDefinition 4.9 (Partially Observable Markov Decision Process). An environment ν is a partially observable Markov decision process (POMDP) iff there is a set of states S, an initial state s0 ∈ S, a state transition function ν ′ : S × A → ∆S, and a percept distribution ν ′′ : S → ∆E such that\nν(e1:t ‖ a1:t) = t∏\nk=1\nν ′′(ek | sk)ν ′(sk | sk−1, ak).\nUsually the set S is assumed to be finite; with infinite-state POMDPs we can model any environment ν by setting the set of states to be the set of histories, S := (A×E)∗.\nA common assumption for MDPs and POMDPs is that they do not contain traps. Formally, a (PO)MDP is ergodic iff for any policy π and any two states s1, s2 ∈ S, the expected number of time steps to reach s2 from s1 is µπ-almost surely finite. A (PO)MDP is weakly communicating iff for any two states s1, s2 ∈ S there is a policy π such that the expected number of time steps to reach s2 from s1 is µπ-almost surely finite. Note that any ergodic (PO)MDP is also weakly communicating, but not vice versa.\nIn general, our environments are stochastic. Stochasticity can originate from noise in the environment, noise in the sensors, or modeling errors. Sometimes we also consider classes of deterministic environments. These are usually easier to deal with because they do not require as much mathematical machinery. For example, in a deterministic environment the next percept is certain; if a different percept is received this environment is immediately falsified and can be discarded. In a stochastic environment, an unlikely percept reduces our posterior belief in this environment but does not rule it out completely.\nIn Chapter 6 and Chapter 7 we make the assumption that the environment is computable. This encompasses all finite-state POMDPs and most if not all AI problems can be formulated in this setting. Moreover, the current theories of quantum mechanics and general relativity are computable and there is no evidence that suggests that our physical universe is incomputable. For any physical system of finite volume and finite (average) energy, the amount of information it can contain is finite (Bekenstein, 1981), and so is the number of state transitions per unit of time (Margolus and Levitin, 1998). This gives us reason to believe that even the environment that we humans currently face (and will ever face) falls under these assumptions.\n2The size of the state space is at most 256128 since the Atari 2600 has only 128 bytes of memory. However, the vast majority of these states are not reachable."
    }, {
      "heading" : "56 Acting",
      "text" : "Formally we define the setMCCSLSC as the set of environments that are lower semicomputable chronological contextual semimeasures andMCCMcomp as the set of environments that are computable chronological contextual measures. Note that for chronological contextual semimeasures it makes a difference whether ν( · ‖ a1:∞) is lower semicomputable or the conditionals ν( · | æ<tat) are. The latter implies the former, but not vice versa."
    }, {
      "heading" : "4.2 The Value Function",
      "text" : "The value of a policy in an environment is the future expected discounted reward when following a given policy in a given environment conditional on the past. Since this quantity captures exactly what our agent aims to maximize, we prefer policies whose value is high.\nDefinition 4.10 (Value Function). The value of a policy π in an environment ν given history æ<t and horizon m with t ≤ m ≤ ∞ is defined as\nV π,mν (æ<t) := 1\nΓt Eπν [ m−1∑ k=t γkrk ∣∣∣∣∣ æ<t ]\nif Γt > 0 and V π,m ν (æ<t) := 0 if Γt = 0. The optimal value is defined as V ∗,m ν (æ<t) := supπ V π,m ν (æ<t).\nSometimes we omit the history argument æ<t for notational convenience if it is clear from context. Moreover, when we omit m, we implicitly use an infinite horizon m =∞, i.e., V πν := V π,∞ ν and V ∗ν := V ∗,∞ ν . The value of a policy π in an environment ν after the empty history, V πν ( ) is also called the t0-value.\nRemark 4.11 (Values are Bounded Between 0 and 1). From Assumption 4.6b we get that for all histories æ<t all policies π and all environments ν, the value function V πν (æ<t) ∈ [0, 1]. 3\nSince environment and policy are stochastic, the history æ<t is random. With abuse of notation we treat æ<t sometimes as a concrete outcome and sometimes as a random variable. We also view the value of a policy π in an environment ν as a sequence of random variables (Xt)t∈N with Xt := V πν (æ1:t) where the history æ1:t is generated stochastically by the agent’s actual policy interacting with the true environment µ. This view is helpful for some of the convergence results (e.g., Theorem 4.19 and Definition 5.18) in which we talk about the type of convergence of this sequence of random variables.\nThe value function defined in Definition 4.10 is also called the recursive value function, in contrast to iterative value function that we discuss in Section 6.4. The name of the recursive value function originates from the following recursive identity (analogously\n§4.2 The Value Function 57\nto Hutter, 2005, Eq. 4.12), also called the Bellman equation:\nV πν (æ<t) = ∑ at∈A π(at | æ<t)V πν (æ<tat)\nV πν (æ<tat) = 1\nΓt ∑ et∈E ν(et | æ<tat) ( γtrt + Γt+1V π ν (æ1:t) ) An explicit expression for the optimal value in environment ν is\nV ∗,mν (æ<t) = 1 Γt max ∑\næt:m−1 m−1∑ k=t γkrk k∏ i=t ν(ei | æ<iai), (4.2)\nwhere ∑ max denotes the max-sum-operator:\nmax ∑\næt:m−1\n:= max at∈A ∑ et∈E . . . max am−1∈A ∑ em−1∈E\nFor an explicit expression of V ∗,∞ν (æ<t) we can simply take the limit m→∞."
    }, {
      "heading" : "4.2.1 Optimal Policies",
      "text" : "An optimal policy is a policy that achieves the highest value:\nDefinition 4.12 (Optimal Policy; Hutter, 2005, Def. 5.19 & 5.30). A policy π is optimal in environment ν (ν-optimal) iff for all histories π attains the optimal value: V πν (æ<t) = V ∗ν (æ<t) for all æ<t ∈ (A×E)∗. The action at is an optimal action iff π∗ν(at | æ<t) = 1 for some ν-optimal policy π∗ν .\nFollowing the tradition of Hutter (2005), AINU denotes a ν-optimal policy for the environment ν ∈ MCCSLSC and AIMU denotes an µ-optimal policy for the environment µ ∈MCCMcomp that is a measure (as opposed to a semimeasure).\nBy definition of the optimal policy and the optimal value function, we have the following identity for all histories æ<t:\nV ∗ν (æ<t) = V π∗ν ν (æ<t) (4.3)\nThere can be more than one optimal policy; generally the choice of π∗ν from Definition 4.12 is not unique. More specifically, for a ν-optimal policy we have\nπ∗ν(at | æ<t) > 0 =⇒ at ∈ arg max a∈A V ∗ν (æ<ta). (4.4)\nIf there are multiple actions α, β ∈ A that attain the optimal value, V ∗ν (æ<α) = V ∗ν (æ<tβ), then there is an argmax tie. Which action we settle on in case of a tie (how we break the tie) is irrelevant and can be arbitrary. Since we allow stochastic policies, we can also randomize between α and β.\nThe following definition allows policies to be slightly suboptimal."
    }, {
      "heading" : "58 Acting",
      "text" : "Definition 4.13 (ε-Optimal Policy). A policy π is ε-optimal in environment ν iff V ∗ν (æ<t)− V πν (æ<t) < ε for all histories æ<t ∈ (A× E)∗.\nA policy π that achieves optimal t0-value, V πν ( ) = V ∗ν ( ), takes ν-optimal actions on any history reachable by π in ν. However, this is not true for ε-optimal policies: a policy that is ε-optimal at t = 0 is not necessarily ε-optimal in later time steps."
    }, {
      "heading" : "4.2.2 Properties of the Value Function",
      "text" : "The following two lemmas are stated by Hutter (2005, Thm. 31) without proof and for the iterative value function.\nLemma 4.14 (Linearity of V πν in ν). If ν = z1ν1+z2ν2 for some real numbers z1, z2 ≥ 0, then for all policies π and all histories æ<t\nV π,mν (æ<t) = z1 ν1(e<t ‖ a<t) ν(e<t ‖ a<t) V π,mν1 (æ<t) + z2 ν2(e<t ‖ a<t) ν(e<t ‖ a<t) V π,mν2 (æ<t).\nProof. Since νπ = z1νπ1 + z2νπ2 , we have for the conditional measure\nνπ(A | æ<t) = νπ(A ∩ æ<t) νπ(æ<t) = z1ν π 1 (A ∩ æ<t) + z2νπ2 (A ∩ æ<t) νπ(æ<t)\n= z1 νπ1 (æ<t) νπ(æ<t) νπ1 (A | æ<t) + z2 νπ2 (æ<t) νπ(æ<t) νπ2 (A | æ<t).\nThe claim now follows from the linearity of expectation in the probability measure.\nLemma 4.15 (Convexity of V ∗ν in ν). If ν = z1ν1 +z2ν2 for some real numbers z1, z2 ≥ 0, then for all histories æ<t\nV ∗ν (æ<t) ≤ z1 ν1(e<t ‖ a<t) ν(e<t ‖ a<t) V ∗,mν1 (æ<t) + z2 ν2(e<t ‖ a<t) ν(e<t ‖ a<t) V ∗,mν2 (æ<t).\nProof. Let π∗ν be an optimal policy for environment ν. From Lemma 4.14 we get\nV ∗ν (æ<t) = V π∗ν ν (æ<t) = z1 ν1(e<t ‖ a<t) ν(e<t ‖ a<t) V π ∗ ν ν1 (æ<t) + z2 ν2(e<t ‖ a<t) ν(e<t ‖ a<t) V π ∗ ν ν2 (æ<t)\n≤ z1 ν1(e<t ‖ a<t) ν(e<t ‖ a<t) V ∗ν1(æ<t) + z2 ν2(e<t ‖ a<t) ν(e<t ‖ a<t) V ∗ν2(æ<t).\nThe following lemma bounds the error when truncating the value function. This implies that planning for an ε-effective horizon (m = t+Ht(ε)), we get all but an ε of the value: |V πν (æ<t)− V π,m ν (æ<t)| < ε.\nLemma 4.16 (Truncated Values). For every environment ν, every policy π, and every history æ<t ∣∣V π,mν (æ<t)− V πν (æ<t)∣∣ ≤ ΓmΓt .\n§4.2 The Value Function 59\nProof.\nV πν (æ<t) = 1\nΓt Eπν [ ∞∑ k=t γkrk ∣∣∣∣∣ æ<t ] = V π,mν (æ<t) + 1 Γt Eπν [ ∞∑ k=m γkrk ∣∣∣∣∣ æ<t ]\nThe result now follows from Assumption 4.6b and\n0 ≤ Eπν [ ∞∑ k=m γkrk ∣∣∣∣∣ æ<t ] ≤ Γm.\nThis lemma bounds the (truncated) value function by the total variation distance.\nLemma 4.17 (Bounds on Value Difference). For any policies π1, π2, any environments ν1 and ν2, and any horizon t ≤ m ≤ ∞\n|V π1,mν1 (æ<t)− V π2,m ν2 (æ<t)| ≤ Dm−1(ν π1 1 , ν π2 2 | æ<t)\nProof. According to Definition 4.10, the value function is the expectation of the random variable ∑m−1 k=t γkrk/Γt that is bounded between 0 and 1. Therefore we can use Lemma 2.12 with P := νπ11 ( · | æ<t) and R := ν π2 2 ( · | æ<t) on the space (A×E)m−1 to conclude that |V π1,mν1 (æ<t)− V π2,m ν2 (æ<t)| is bounded by Dm−1(νπ11 , ν π2 2 | æ<t).\nLemma 4.18 (Discounted Values; Lattimore, 2013, Lem. 2.5). Let æ<t be some history and let π1 and π2 be two policies that coincide from time step t to time step m: π1(a | æ1:k) = π2(a | æ1:k) for all a ∈ A, all histories æ<tæt:k consistent with π1, and t ≤ k ≤ m. Then for all environments ν∣∣V π1ν (æ<t)− V π2ν (æ<t)∣∣ ≤ ΓmΓt . Proof. Since π1 and π2 coincide for time steps t throughm−1, Dm−1(νπ1 , νπ2 | æ<t) = 0 for all environments ν. Thus the result follows from Lemma 4.16 and Lemma 4.17:∣∣V π1ν (æ<t)− V π2ν (æ<t)∣∣ ≤ ∣∣V π1,mν (æ<t)− V π2,mν (æ<t)∣∣+ ΓmΓt\n≤ Dm−1(νπ1 , νπ2 | æ<t) + Γm Γt = Γm Γt"
    }, {
      "heading" : "4.2.3 On-Policy Value Convergence",
      "text" : "This section states some general results on learning the value function. On-policy value convergence refers to the fact that if we use a learning distribution ρ to learn to environment µ, and ρπ merges with µπ in the sense discussed Section 3.4, then V πρ converges to V πµ , i.e., using ρ we learn to estimate values correctly.\nA weaker variant of the following theorem was proved by Hutter (2005, Thm. 5.36). It states convergence in mean (not almost surely), and only for the Bayesian mixture."
    }, {
      "heading" : "60 Acting",
      "text" : "Theorem 4.19 (On-Policy Value Convergence). Let µ be any environment and π be any policy.\n(a) If ρπ merges strongly with µπ, then\nV πρ (æ<t)− V πµ (æ<t)→ 0 as t→∞ µπ-almost surely.\n(b) If the effective horizon is bounded and ρπ merges weakly with µπ, then\nV πρ (æ<t)− V πµ (æ<t)→ 0 as t→∞ µπ-almost surely.\n(c) If the effective horizon is bounded and ρπ merges almost weakly with µπ, then\n1\nt t∑ k=1 ( V πρ (æ<k)− V πµ (æ<k) ) → 0 as t→∞ in µπ-almost surely.\nProof. (a) Apply Lemma 4.17 with m :=∞.\n(b) Let ε > 0 and let cε be a bound on suptHt(ε). From Lemma 4.16\n|V πρ (æ<t)− V πµ (æ<t)| ≤ |V π,t+Ht(ε)ρ (æ<t)− V π,t+Ht(ε)µ (æ<t)|+ 2 Γt+Ht(ε)\nΓt\n< Dt+Ht−1(ε)(ρ π, µπ | æ<t) + 2ε ≤ Dt+cε(ρπ, µπ | æ<t) + 2ε\naccording to Definition 4.4 and Lemma 4.17. Since ρπ merges weakly with µπ, we get that µπ-almost surely there is a time step t0 ∈ N such that Dt+cε(ρπ, µπ | æ<t) < ε for all t ≥ t0. Hence |V πQ (æ<t)− V πµ (æ<t)| < 3ε for all t ≥ t0.\n(c) Analogously to the proof of (b).\nIt is important to observe that on-policy convergence does not imply that the agent converges to the optimal policy. V πρ converges to V πµ , but V πµ need not be close to V ∗µ . Indeed, there might be another policy π̃ that has a higher value than π in the true environment µ (V π̃µ > V πµ ). If the agent thinks π̃ has lower value (V π̃ρ < V πρ ) it might not follow π̃ and hence not learn that the actual value of π̃ is much higher. In other words, on-policy convergence implies that the agent learns the value of its own actions, but not the value of counterfactual actions that it does not take.\nTheorem 4.19 now enables us to tie in the results of Chapter 3. This yields a surge of corollaries, but first we need to make the learning distributions contextual on the actions.\nLet w ∈ ∆M be a positive prior over the environment class M. We define the corresponding Bayesian mixture analogously to Example 3.4:\nξ(e<t ‖ a<t) := ∑ ν∈M w(ν)ν(e<t ‖ a<t) (4.5)\n§4.2 The Value Function 61\nNote that the Bayesian mixture ξ depends on the prior w. For the rest of this thesis, this dependence will not be made explicit.\nFrom Lemma 4.14 and (3.3) we immediately get the following identity:\nV πξ (æ<t) = ∑ ν∈M w(ν | æ<t)V πν (æ<t) (4.6)\nSimilarly, we get from Lemma 4.15\nV ∗ξ (æ<t) ≤ ∑ ν∈M w(ν | æ<t)V ∗ν (æ<t). (4.7)\nCorollary 4.20 (On-Policy Value Convergence for Bayes). For any environment µ ∈ M and any policy π,\nV πξ (æ<t)− V πµ (æ<t)→ 0 as t→∞ µπ-almost surely.\nProof. Since µ ∈ M, we have dominance ξπ ≥ w(µ)µπ with w(µ) > 0 and by Proposition 3.16a absolute continuity ξπ µπ. From Theorem 3.25 we get that ξπ merges strongly with µπ. Therefore we can apply Theorem 4.19a.\nAnalogously, we define MDLæ<t := arg minν∈M{− log ν(e<t ‖ a<t) +K(ν)}.\nCorollary 4.21 (On-Policy Value Convergence for MDL). For any environment µ ∈M and any policy π,\nV πMDLæ<t (æ<t)− V π µ (æ<t)→ 0 as t→∞ µπ-almost surely.\nProof. By Theorem 3.28 MDLπ merges strongly with νπ for each ν ∈M, therefore we can apply Theorem 4.19a.\nBy providing the action sequence contextually on a separate input tape, we can define Km(e<t ‖ a<t) := min{|p| | e<t v U(p, a<t)} analogously to (2.1).\nCorollary 4.22 (On-Policy Value Convergence for Universal Compression). Let ρ(e<t ‖ a<t) := 2 −Km(e<t‖a<t). Then for any environment µ ∈MCCMcomp and any policy π,\nV πρ (æ<t)− V πµ (æ<t)→ 0 as t→∞ µπ-almost surely.\nProof. Since ρ dominates every µ ∈ MCCMcomp (Section 3.6.3) we can apply Proposition 3.16a, Theorem 3.25, and Theorem 4.19a as in the proof of Corollary 4.20.\nSimilarly to Km there is a speed prior for environments (Filan, 2015, Ch. 6):\nSKt(e<t ‖ a<t) := ∑\np: e<tvU(p,a<t)\n2−|p|\nt(U, p, a<t, e<t)\nwhere t(U, p, a<t, e<t) denotes the number of time steps U(p, a<t) takes to produce e<t."
    }, {
      "heading" : "62 Acting",
      "text" : "Corollary 4.23 (On-Policy Value Convergence for the Speed Prior). If the effective horizon is bounded, then for any environment µ ∈MCCMcomp estimable in polynomial time and any policy π,\n1\nt t∑ k=1 ( V πSKt(æ<k)− V π µ (æ<k) ) → 0 as t→∞ µπ-almost surely.\nProof. By Corollary 3.61 the speed prior SKt merges almost weakly with every measure estimable in polynomial time. Therefore we can apply Theorem 4.19c."
    }, {
      "heading" : "4.3 The Agents",
      "text" : "If we knew the true environment µ, we would choose the µ-optimal policy, the policy that maximizes µ-expected discounted rewards. But generally we do not know the true environment, and the challenging part of reinforcement learning is to learn the environment while trying to collect rewards.\nIn this section we introduce a number of agents that attempt to solve the general reinforcement learning problem (Problem 4.2). These agents are discussed throughout the rest of this thesis."
    }, {
      "heading" : "4.3.1 Bayes",
      "text" : "A Bayes optimal policy with respect to the prior w is the policy π∗ξ where ξ is the Bayesian mixture defined in Section 4.2.3. There can be one or more Bayes optial policies. From Corollary 4.20 we get on-policy value convergence for the Bayes optimal policy.\nAfter history æ<t, the Bayes policy π∗ξ maximizes expected discounted rewards in the posterior mixture:\nξ( · | e<t ‖ a1:∞) = ∑ ν∈M w(ν | æ<t)ν( · | e<t ‖ a1:∞)\nwhere w(ν | æ<t) are the posterior weights (3.3). Maximizing expected rewards according to the posterior is the same as maximizing expected rewards according to the prior conditional on the history: if π(æ<t) = π∗ξ (æ<t), then V π ξ (æ<t) = V ∗ ξ (æ<t). Actually visiting the history æ<t does not change what π∗ξ planned to do before it visited æ<t. Note that this relies on the fact that the way we use discounting is time consistent (Lattimore and Hutter, 2014, Def. 12).\nWhen using the prior w(ν) ∝ 2−K(ν) (Example 3.5) over the classMCCSLSC , the Bayes optimal policy is also known as AIXI , introduced and analyzed by Hutter (2000, 2001a, 2002a, 2003, 2005, 2007a, 2012b) in his work on universal artificial intelligence. In this case, the Bayesian mixture (4.5) can be defined equivalently according to (Wood et al., 2011)\nξ(e<t ‖ a<t) := ∑\np: e<tvU(p,a<t)\n2−|p|. (4.8)\n§4.3 The Agents 63\nGenerally there is more than one ξ-optimal policy and Solomonoff’s prior depends on the choice of the (reference) universal Turing machine, so this definition is not unique. Moreover, not every universal Turing machine is a good choice for AIXI, see Section 5.2 for a few bad choices. The following lemma will be used later.\nLemma 4.24 (Mixing Mixtures). Let q, q′ ∈ Q such that q > 0, q′ ≥ 0, and q + q′ ≤ 1. Let w be any lower semicomputable positive prior, let ξ be the Bayesian mixture corresponding to w, and let ρ ∈ MCCSLSC . Then ξ′ := qξ + q′ρ ∈ MCCSLSC is a Bayesian mixture.\nProof. ξ′ is given by the positive prior w′ with w′ := qw + q′1ρ.\nBayesian approaches have a long tradition in reinforcement learning, although they are often prohibitively expensive to compute. For multi-armed bandits, Gittins (1979) achieved a breakthrough with an index strategy that enables the computation of the optimal policy by computing one quantity for each arm independently of the rest. This strategy even achieves the optimal asymptotic regret bounds (Lattimore, 2016). Larger classes have also been attempted: using Monte-Carlo tree search, Veness et al. (2011) approximate the Bayes optimal policy in the class of all context trees. Doshi-Velez (2012) uses Bayesian techniques to learn infinite-state POMDPs. See Vlassis et al. (2012) for a survey on Bayesian techniques in RL.\nIn the rest of this thesis, the Bayes optimal policy is often treated as an optimal exploitation strategy. This is not true: Bayes does explore (when it is Bayes optimal to do so). It just does not explore general environment classes completely (see Section 5.4.1)."
    }, {
      "heading" : "4.3.2 Knowledge-Seeking Agents",
      "text" : "In this section we discuss two variants of knowledge-seeking agents: entropy-seeking agents introduced by Orseau (2011, 2014a) and information-seeking agents introduced by Orseau et al. (2013). The entropy-seeking agent maximizes the Shannon entropy gain, while the information-seeking agent maximizes the expected information gain. These quantities are expressed in different value functions. In places where confusion can arise, we call the value function V πν from Definition 4.10 the reward-seeking value function.\nIn this section we use a finite horizon m <∞ (possibly dependent on time step t): the knowledge-seeking agent maximizes entropy/information received up to time step m. We assume implicitly that m (as a function of t) is computable. Moreover, in this section we assume that the Bayesian mixture ξ is a measure rather than a semimeasure; Example 4.27 discusses this assumption.\nDefinition 4.25 (Entropy-Seeking Value Function; Orseau, 2014a, Sec. 6). The entropyseeking value of a policy π given history æ<t is\nV π,mEnt (æ<t) := E π ξ [− log2 ξ(e1:m | e<t ‖ a1:m) | æ<t]."
    }, {
      "heading" : "64 Acting",
      "text" : "The entropy-seeking value is the Bayes-expectation of − log ξ. Orseau (2011, 2014a) also considers a related value function based on the ξ-expectation of ξ that we do not discuss here.\nDefinition 4.26 (Information-Seeking Value Function; Orseau et al., 2013, Def. 1). The information-seeking value of a policy π given history æ<t is\nV π,mIG (æ<t) := ∑ ν∈M w(ν | æ<t)KLm(νπ, ξπ | æ<t).\nAnalogously to before we define V ∗Ent := supπ V π Ent and V ∗ IG := supπ V π IG. An optimal entropy-seeking policy is defined as π∗Ent :∈ arg maxπ V πEnt and an optimal informationseeking policy is defined as π∗IG :∈ arg maxπ V πIG. Since we use a finite horizon (m <∞), these optimal policies exist.\nThe information gain is defined as the difference in entropy between the prior and the posterior:\nIGt:m(æ1:m) := Ent(w( · | æ<t))− Ent(w( · | æ1:m))\nWe get the following identity (Lattimore, 2013, Eq. 3.5).\nEπξ [IGt:m(æ1:m) | æ<t] = V π,m IG (æ<t)\nFor infinite horizons (m = ∞), the values functions from Definition 4.25 and Definition 4.26 may not converge. To ensure convergence, we can either use discounting, or in case of VIG a prior with finite entropy (Lattimore, 2013, Thm. 3.4). Moreover, note that while VIG and VEnt are expectations with respect to the measure ξ, there is no bound on the one-step change in value V π,mIG (æ<t)− V π,m IG (æ1:t), which can also be negative. For the reward-seeking value function V π,mν , the one-step change in value is bounded between 0 and 1 by Remark 4.11.\nFor classes of deterministic environments Definition 4.25 and Definition 4.26 coincide. In stochastic environments the entropy-seeking agent does not work well because it gets distracted by noise in the environment rather than trying to distinguish environments (Orseau et al., 2013, Sec. 5). Moreover, the entropy-seeking agent may fail to seek knowledge in deterministic semimeasures as the following example demonstrates.\nExample 4.27 (Unnormalized Entropy-Seeking). If the Bayesian mixture ξ is a semimeasure instead of a measure (such as the Solomonoff prior from Example 3.5), then the entropy-seeking agent does not explore correctly. Fix A := {α, β}, E := {0, 1}, and m = t (we only care about the entropy of the next percept). We illustrate the problem on a simple class of environments {ν1, ν2}:\nν1α/0/0.1 β/0/0.5 ν2α/1/0.1 β/0/0.5\nwhere transitions are labeled with action/percept/probability. Both ν1 and ν2 return a percept deterministically or nothing at all (the environment ends). Only action α\n§4.3 The Agents 65\ndistinguishes between the environments. With the prior w(ν1) := w(ν2) := 1/2, we get a mixture ξ for the entropy-seeking value function V πEnt. Then V ∗ Ent(α) ≈ 0.432 < 0.5 = V ∗Ent(β), hence action β is preferred over α by the entropy-seeking agent. But taking action β yields percept 0 (if any), hence nothing is learned about the environment. 3\nOn-policy value convergence (Theorem 4.19) ensures that asymptotically, the agent learns the value of its own policy. Knowledge-seeking agents do even better: they don’t have to balance between exploration and exploitation, so they can focus solely on exploration. As a result, they learn off-policy, i.e., the value of counterfactual actions (Orseau et al., 2013, Thm. 7)."
    }, {
      "heading" : "4.3.3 BayesExp",
      "text" : "Lattimore (2013, Thm. 5.6) defines BayesExp combining AIXI with the informationseeking agent. BayesExp alternates between phases of exploration and phases of exploitation: Let εt be a monotone decreasing sequence of positive reals such that εt → 0 as t→∞. If the optimal information-seeking value V ∗IG is larger than εt, then BayesExp starts an exploration phase, otherwise it starts an exploitation phase. During an exploration phase, BayesExp follows an optimal information-seeking policy for an εt-effective horizon. During an exploitation phase, BayesExp follows an ξ-optimal reward-seeking policy for one step (see Algorithm 1).\nAlgorithm 1 BayesExp policy πBE (Lattimore, 2013, Alg. 2). 1: while true do 2: if V ∗,t+Ht(εt)IG (æ<t) > εt then 3: follow π∗IG for Ht(εt) steps 4: else 5: follow π∗ξ for 1 step"
    }, {
      "heading" : "4.3.4 Thompson Sampling",
      "text" : "Thompson sampling, also known as posterior sampling or the Bayesian control rule, was originally proposed by Thompson (1933) as a bandit algorithm. It is easy to implement and often achieves quite good results (Chapelle and Li, 2011). In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012). Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015).\nFor general RL Thompson sampling was first suggested by Ortega and Braun (2010) with resampling at every time step. Strens (2000) proposes following the optimal policy for one episode or “related to the number of state transitions the agent is likely to need to plan ahead”. We follow Strens’ suggestion and resample at the effective horizon.\nLet εt be a monotone decreasing sequence of positive reals such that εt → 0 as t → ∞. Our variant of Thomson sampling is given in Algorithm 2. It samples an"
    }, {
      "heading" : "66 Acting",
      "text" : "environment ρ from the posterior, follows the ρ-optimal policy for an εt-effective horizon, and then repeats.\nAlgorithm 2 Thompson sampling policy πT . 1: while true do 2: sample ρ ∼ w( · | æ<t) 3: follow π∗ρ for Ht(εt) steps\nNote that πT is a stochastic policy since we occasionally sample from a distribution. We assume that this sampling is independent of everything else.\nChapter 5\nOptimality\nMachines will never be intelligent. — Shane Legg\nProblem 4.2 defines the general reinforcement learning problem. But our definition of this problem did not specify what a solution would be. This chapter is dedicated to this question:\nWhat is an optimal solution to the general reinforcement learning problem?\nHow can we say that one policy is better than another? What is the best policy? Are the policies from Section 4.3 optimal? Several notions of optimality for a policy π in an environment classM are conceivable:\nO1. Maximal reward. The policy π receives a reward of 1 in every time step (which is maximal according to Assumption 4.6b):\n∀t ∈ N. rt = 1\nO2. Optimal policy. The policy π achieves the highest possible value in the true environment µ:\n∀æ<t ∈ (A× E)∗. V πµ (æ<t) = V ∗µ (æ<t)\nO3. Pareto optimality (Hutter, 2002a, Thm. 2). There is no other policy that performs at least as good in all environments and strictly better in at least one:\n@π̃. ( ∀ν ∈M. V π̃ν ( ) ≥ V πν ( ) and ∃ρ ∈M. V π̃ρ ( ) > V πρ ( ) ) O4. Balanced Pareto optimality (Hutter, 2002a, Thm. 3). The policy π achieves a\nbetter value acrossM weighted by w ∈ ∆M than any other policy:\n∀π̃. ∑ ν∈M w(ν) ( V πν ( )− V π̃ν ( ) ) ≥ 0\nO5. Bayes optimality . The policy π is ξ-optimal for some Bayes mixture ξ:\n∀æ<t ∈ (A× E)∗. V πξ (æ<t) = V ∗ξ (æ<t)\n67"
    }, {
      "heading" : "68 Optimality",
      "text" : "O6. Probably approximately correct . For a given ε, δ > 0 the value of the policy π is ε-close to the optimal value with probability at least δ after time step t0(ε, δ):\nµπ [ ∀t ≥ t0(ε, δ). V ∗µ (æ<t)− V πµ (æ<t) < ε ] > 1− δ\nO7. Asymptotic optimality (Hutter, 2005, Sec. 5.3.4). The value of the policy π converges to the optimal value:\nV ∗µ (æ<t)− V πµ (æ<t)→ 0 as t→∞\nO8. Sublinear regret . The difference between the reward sum of the policy π and the best policy in hindsight grows sublinearly:\nsup π′\nEπ ′ µ [ m∑ t=1 rt ] − Eπµ [ m∑ t=1 rt ] ∈ o(m)\nWe discuss these notions of optimality in turn. Achieving the maximal reward at every time step is impossible if there is no action that makes the environment µ respond with the maximal reward; generally there is no policy that achieves maximal rewards at every time step. In order to follow the optimal policy, we need to know the true environment. In our setting, the true environment is unknown and has to be learned. During the learning process the agent cannot also act optimally because it needs to explore. In particular, the policy π cannot be optimal simultaneously in all environments fromM. This rules out O1 and O2 as a notion of optimality.\nIn Section 5.1 we show that all policies are Pareto optimal. This disqualifies O3 as a useful notion of optimality in general reinforcement learning.\nBalanced Pareto optimality (O4), Bayes optimality (O5), and maximal Legg-Hutter intelligence (Legg and Hutter, 2007b) turn out to coincide. In Section 5.3 we show that Legg-Hutter intelligence is highly subjective, because it depends on the choice of the prior. By changing the prior of a Bayesian agent, we can make the agent’s intelligence arbitrarily low. In Section 5.2 we present a choice of particularly bad priors. This rules out O4 and O5 because they are prior-dependent and not objective.\nO6 is a stronger version of asymptotic optimality that provides a rate of convergence (it implies O7). Since our environment class can be very large and non-compact, concrete PAC results are likely impossible. Orseau (2010, 2013) shows that the Bayes optimal agent does not achieve asymptotic optimality in all computable environments. The underlying problem is that in the beginning the agent does not know enough about its environment and therefore relies heavily on its prior. Lack of exploration then retains the prior’s bias. This problem can be alleviated by adding extra exploration to the Bayesian agent. In Section 5.4 we discuss two agents that achieve asymptotic optimality: BayesExp (Section 4.3.3) and Thompson sampling (Section 4.3.4). This establishes that O7 is possible.\nIn general environments sublinear regret is impossible because the agent can get stuck in traps from which it is unable to recover. This rules out O8. However, in Sec-\n§5.1 Pareto Optimality 69\ntion 5.5 we show that if we assume that the environment allows recovering from mistakes (and some minor conditions on the discount function are fulfilled), then asymptotic optimality implies sublinear regret. This means that Thompson sampling has sublinear regret in these recoverable environments.\nNotably, only asymptotic optimality (O7) holds up to be a nontrivial and objective criterion of optimality that applies to the general reinforcement learning problem. While there are several agents that are known to be asymptotically optimal, some undesirable properties remain. Section 5.6 discusses this further. See also Mahadevan (1996) for a discussion of notions of optimality in MDPs."
    }, {
      "heading" : "5.1 Pareto Optimality",
      "text" : "In this section we show that Pareto optimality is not a useful criterion for optimality since for any environment class containingMCCMcomp, all policies are Pareto optimal.\nDefinition 5.1 (Pareto Optimality; Hutter, 2005, Def. 5.22). A policy π is Pareto optimal in the set of environmentsM iff there is no policy π̃ such that V π̃ν ( ) ≥ V πν ( ) for all ν ∈M and V π̃ρ ( ) > V πρ ( ) for at least one ρ ∈M.\nThe literature provides the following result.\nTheorem 5.2 (AIXI is Pareto Optimal; Hutter, 2002a, Thm. 2). Every ξ-optimal policy is Pareto optimal inMCCSLSC .\nThe following theorem was proved for deterministic policies in Leike and Hutter (2015c). Here we extend it to stochastic policies.\nTheorem 5.3 (Pareto Optimality is Trivial). Every policy is Pareto optimal in any classM⊇MCCMcomp.\nThe proof proceeds as follows: for a given policy π, we construct a set of ‘buddy environments’ that reward π and punish other policies. Together they can defend against any policy π̃ that tries to take the crown of Pareto optimality from π.\nProof. We assume (0, 0) and (0, 1) ∈ E . Moreover, assume there is a policy π that is not Pareto optimal. Then there is a policy π̃ that Pareto dominates π, i.e., V π̃ρ ( ) > V πρ ( ) for some ρ ∈ M, and V π̃ν ( ) ≥ V πν ( ) for all ν ∈ M. From V π̃ρ ( ) > V πρ ( ) and Lemma 4.18 we get that there is a shortest and lexicographically first history æ′<k consistent with π and π̃ such that π(α | æ′<k) > π̃(α | æ′<k) for some action α ∈ A and V π̃ρ (æ′<k) > V π ρ (æ′<k). Consequently there is an i ≥ k such that γi > 0, and hence Γk > 0. We define the environment µ that first reproduces the separating history æ′<k and then, if α is the next action, returns reward 1 forever, and otherwise returns reward 0 forever. Formally, µ is defined by\nµ(e1:t | e<t ‖ a1:t) :=  1, if t < k and et = e′t, 1, if t ≥ k and ak = α and rt = 1 and ot = 0, 1, if t ≥ k and ak 6= α and rt = 0 = ot, and 0, otherwise."
    }, {
      "heading" : "70 Optimality",
      "text" : "The environment µ is computable, even if the policy π is not: for a fixed history æ′<t and action α, there exists a program computing µ; therefore µ ∈ MCCMcomp. We get the following value difference for the policies π and π̃:\nV πµ ( )− V π̃µ ( ) = Eπµ [ k−1∑ t=1 γtrt + ∞∑ t=k γtrt ] − Eπ̃µ [ k−1∑ t=1 γtrt − ∞∑ t=k γtrt ]\n= ( π(α | æ′<k) ∞∑ t=k γt − π̃(α | æ′<k) ∞∑ t=k γt ) µπ(æ′<k)\n= ( π(α | æ′<k)− π̃(α | æ′<k) ) µπ(æ′<k)Γk > 0\nHence V π̃µ ( ) < V πµ ( ), which contradicts the fact that π̃ Pareto dominates π since M⊇MCCMcomp 3 µ.\nNote that the environment µ we defined in the proof of Theorem 5.3 is actually just a finite-state POMDP, so Pareto optimality is also trivial for smaller environment classes."
    }, {
      "heading" : "5.2 Bad Priors",
      "text" : "In this section we give three examples of universal priors that cause a AIXI to misbehave drastically. In case of a finite horizon, the indifference prior makes all actions equally preferable to AIXI (Section 5.2.1). The dogmatic prior makes AIXI stick to any given computable policy π as long as expected future rewards do not fall too close to zero (Section 5.2.2). The Gödel prior prevents AIXItl from taking any actions (Section 5.2.3)."
    }, {
      "heading" : "5.2.1 The Indifference Prior",
      "text" : "The following theorem constructs the indifference prior that yields a Bayesian mixture ξ′ that causes argmax ties for the first m steps. If we use a discount function that only cares about the first m steps, Γm = 0, then all policies are ξ′-optimal policies. In this case AIXI’s behavior only depends on how we break argmax ties.\nTheorem 5.4 (Indifference Prior). If there is an m such that Γm = 0, then there is a Bayesian mixture ξ′ such that all policies are ξ′-optimal.\nProof. First, we assume that the action space is binary, A = {0, 1}. Let U be the reference UTM and define the UTM U ′ by\nU ′(s<mp, a1:t) := U(p, a1:t xor s1:t),\nwhere s<m is a binary string of lengthm−1 and sk := 0 for k ≥ m. (U ′ has no programs of length less than m − 1.) Let ξ′ be the Bayesian mixture given by U ′ according to\n§5.2 Bad Priors 71\n(4.8). Then\nξ′(e<m ‖ a<m) = ∑\np: e<mvU ′(p,a<m)\n2−|p|\n= ∑\ns<mp′: e<mvU ′(s<mp′,a<m)\n2−m−1−|p ′|\n= ∑ s<m ∑ p′: e<mvU(p′,a<m xor s<m) 2−m−1−|p ′|\n= ∑ s<m ∑ p′: e<mvU(p′,s<m) 2−m−1−|p ′|,\nwhich is independent of a<m. Hence the first m − 1 percepts are independent of the first m− 1 actions. But the percepts’ rewards from time step m on do not matter since Γm = 0 (Lemma 4.16). Because the environment is chronological, the value function must be independent of all actions. Thus every policy is ξ′-optimal.\nFor finite action spaces A with more than 2 elements, the proof works analogously by making A a cyclic group and using the group operation instead of xor.\nThe choice of U ′ in the proof of Theorem 5.4 depends on m. If we increase AIXI’s horizon while fixing the UTM U ′, Theorem 5.4 no longer holds. For Solomonoff induction, there is an analogous problem: when using Solomonoff’s prior M to predict a deterministic binary sequence x, we make at most K(x) errors (Corollary 3.56). In case the shortest program has length > m, there is no guarantee that we make less than m errors (see Section 5.6.2)."
    }, {
      "heading" : "5.2.2 The Dogmatic Prior",
      "text" : "In this section we define a universal prior that assigns very high probability of going to hell (reward 0 forever) if we deviate from a given computable policy π. For a Bayesian agent like AIXI, it is thus only worth deviating from the policy π if the agent thinks that the prospects of following π are very poor already. We call this prior the dogmatic prior, because the fear of going to hell makes AIXI conform to any arbitrary ‘dogmatic ideology’ π. AIXI will only break out if it expects π to give very low future payoff; in that case the agent does not have much to lose.\nTheorem 5.5 (Dogmatic Prior). Let π be any computable deterministic policy, let ξ be any Bayesian mixture over MCCSLSC , and let ε > 0. There is a Bayesian mixture ξ′ such that for any history æ<t consistent with π and for which V πξ (æ<t) > ε, the action π(æ<t) is the unique ξ′-optimal action.\nThe following proof was adapted from Leike and Hutter (2015c) to work for environment classes that do not contain the Bayesian mixture. Essentially, for every environment ν ∈ MCCSLSC the dogmatic prior puts much higher weight on an environment ρν that behaves just like ν on the policy π, but sends any policy deviating from π to hell. Importantly, while following the policy π the environments ν and ρν are indistinguishable, so the posterior belief in ν is equal to the posterior belief in ρν ."
    }, {
      "heading" : "72 Optimality",
      "text" : "Proof of Theorem 5.5. We assume (o, 0) ∈ E for some o ∈ O. For every environment ν ∈MCCSLSC define the environment\nρν(e1:t ‖ a1:t) :=  ν(e1:t ‖ a1:t), if ak = π(æ<k) ∀k ≤ t, ν(e<k ‖ a<k), if k := min{i | ai 6= π(æ<i)} exists\nand ei = (o, 0) ∀i ∈ {k, . . . , t}, and 0, otherwise.\nThe environment ρν mimics environment ν until it receives an action that the policy π would not take. From then on, it provides rewards 0. Since π is a computable policy, we have that ρν ∈MCCSLSC for every ν ∈MCCSLSC .\nNow we need to reweigh the prior w so that it assigns a much higher prior weight to ρν than to ν. Without loss of generality we assume that ε is computable, otherwise we make it slightly smaller. We define w′(ν) := εw(ν) if ν 6= ρν̃ for all ν̃ ∈ MCCSLSC and w′(ρν) := (1− ε)w(ν) + εw(ρν). Then∑\nν∈MCCSLSC\nw′(ν) = ∑ ν=ρν̃ w′(ν) + ∑ ν 6=ρν̃ w′(ν)\n= ∑ ν=ρν̃ ( (1− ε)w(ν̃) + εw(ν) ) + ∑ ν 6=ρν̃ εw(ν)\n= ∑\nν∈MCCSLSC\nεw(ν) + ∑\nν̃∈MCCSLSC\n(1− ε)w(ν̃)\n= ε+ (1− ε) = 1,\nand with w′ ≥ εw, we get that w′ is a positive prior over MCCSLSC . We define ξ′ as the corresponding Bayesian mixture analogous to (4.5).\nWith ρ := ∑\nν∈MCCSLSC w(ν)ρν we get ξ′ = εξ + (1 − ε)ρ. The mixtures ξ and ρ\ncoincide on the policy π since every ν coincides with ρν on the policy π:\nξπ(æ<t) = ∑\nν∈MCCSLSC\nw(ν)νπ(æ<t) = ∑\nν∈MCCSLSC\nw(ν)ρπν (æ<t) = ρ π(æ<t)\nMoreover, V ∗ρν (æ<t) = 0 and thus V ∗ ρ (æ<t) = 0 for any history inconsistent with π by construction of ρν .\nLet æ<t ∈ (A×E)∗ be any history consistent with π such that V πξ (æ<t) > ε. Then ρπ = ξπ implies\nρ(e<t ‖ a<t) ξ′(e<t ‖ a<t) = ξ(e<t ‖ a<t) ξ′(e<t ‖ a<t) = ξ(e<t ‖ a<t) εξ(e<t ‖ a<t) + (1− ε)ρ(e<t ‖ a<t) = 1.\n§5.2 Bad Priors 73\nTherefore Lemma 4.14 implies that for all a ∈ A and all policies π̃\nV π̃ξ′ (æ<ta) = ε ξ(e<t ‖ a<t) ξ′(e<t ‖ a<t) V π̃ξ (æ<ta) + (1− ε) ρ(e<t ‖ a<t) ξ′(e<t ‖ a<t) V π̃ρ (æ<ta)\n= εV π̃ξ (æ<ta) + (1− ε)V π̃ρ (æ<ta). (5.1)\nLet α := π(æ<t) be the next action according to π, and let β 6= α be any other action. We have that V πξ (æ<tα) = V π ρ (æ<tα) since ξπ = ρπ and æ<tα is consistent with π. Therefore we get from (5.1)\nV ∗ξ′(æ<tα) ≥ V πξ′ (æ<tα) = εV πξ (æ<tα) + (1− ε)V πρ (æ<tα) = V πξ (æ<tα) > ε, V ∗ξ′(æ<tβ) = εV π∗ ξ′ ξ (æ<tβ) + (1− ε)V π∗ ξ′ ρ (æ<tβ) = εV π∗ ξ′ ξ (æ<ta) + (1− ε)0 ≤ ε.\nHence V ∗ξ′(æ<tα) > V ∗ ξ′(æ<tβ) and thus the action α taken by π is the only ξ ′-optimal action for the history æ<t.\nCorollary 5.6 (With Finite Horizon Every Policy is Bayes Optimal). If Γm = 0 for some m ∈ N, then for any deterministic policy π there is a Bayesian mixture ξ′ such that π(æ<t) is the only ξ′-optimal action for all histories æ<t consistent with π and t ≤ m.\nIn contrast to Theorem 5.4 where every policy is ξ′-optimal for a fixed Bayesian mixture ξ′, Corollary 5.6 gives a different Bayesian mixture ξ′ for every policy π such that π is the only ξ′-optimal policy.\nProof. Let ε > 0 be small enough such that V πξ (æ<t) > ε for all æ<t and t ≤ m. (This is possible because (A× E)m is finite by Assumption 4.6c.) We use the dogmatic prior from Theorem 5.5 to construct a Bayesian mixture ξ′ for the policy π and ε > 0. Thus for any history æ<t ∈ (A× E)∗ consistent with π and t ≤ m, the action π(æ<t) is the only ξ′-optimal action.\nCorollary 5.7 (AIXI Emulating Computable Policies). Let ε > 0 and let π be any computable policy. There is a Bayesian mixture ξ′ such that for any ξ′-optimal policy π∗ξ′ and for any environment ν,∣∣∣∣V π∗ξ′ν ( )− V πν ( )∣∣∣∣ < ε. Proof. From the proof of Corollary 5.6 and Lemma 4.18."
    }, {
      "heading" : "5.2.3 The Gödel Prior",
      "text" : "This section introduces a prior that prevents any fixed formal system from making any statements about the outcome of all but finitely many computations. It is named after Gödel (1931) who famously showed that for any sufficiently rich formal system there are statements that it can neither prove nor disprove."
    }, {
      "heading" : "74 Optimality",
      "text" : "This prior is targeted at AIXItl, a computable approximation to AIXI defined by Hutter (2005, Sec. 7.2). AIXItl aims to perform as least as well as the best agent who is limited by time t and space l that can be verified using a proof of length at most n for some fixed n ∈ N. The core idea is to enumerate all deterministic policies and proofs and then execute the policy for which the best value has been proved.\nIn order to be verified, a policy π has to be computed by a program p which fulfills the verification condition VA(p) (Hutter, 2005, Eq. 7.7). This program p not only computes future actions of π, but also hypothetical past actions a′i and lower bounds vi for the value of the policy π:\nVA(p) := “∀k∀(va′æ)1:k. ( p(æ<k) = v1a′1 . . . vka ′ k → vk ≤ V πξ (æ<k) ) ”,\nwhere π is the policy derived from p according to π(æ<k) := a′k. We fix some formal system that we use to prove the verification condition. We want it to be sufficiently powerful, but this incurs Gödel incompleteness. For simplicity of exposition we pick PA, the system of Peano arithmetic (Shoenfield, 1967, Ch. 8.1), but our result generalizes trivially to all formal systems which cannot prove their own consistency.\nLet n be a fixed constant. The algorithm for AIXItl is specified as follows.\n1. Let P = ∅. This will be the set of verified programs.\n2. For all proofs in PA of length ≤ n: if the proof proves VA(p) for some p, and |p| ≤ l, then add the program p to P .\n3. For each input history æ<k repeat: run all programs from P for at most t steps each, take the one with the highest promised value vk, and return that program’s policy’s action.\nTheorem 5.8 (The Gödel Prior). There is a UTM U ′ such that if PA is consistent, then the set of verified programs P is empty for all t, l, and n.\nProof. Let q denote an algorithm that never halts, but for which this cannot be proved in PA; e.g., let q enumerate all consequences of PA and halt as soon as it finds a contradiction. Since we assumed that PA is consistent, q never halts. Define the UTM U ′(p, a1:k) as follows.\n• Run q for k steps.\n• If q halts, output vk = 2.\n• Run U(p, a1:k).\nSince q never halts, U and U ′ are functionally identical, therefore U ′ is universal. Note that PA proves ∀p. U(p, a1:k) = U ′(p, a1:k) for any fixed k, but PA does not prove ∀k∀p. U(p, a1:k) = U ′(p, a1:k).\n§5.3 Bayes Optimality 75\nIf q did eventually halt, it would output a value vk = 2 that is too high, since the value function V πξ is bounded by 1 from above, which PA knows. Hence PA proves that\nq halts→ ∀p. ¬VA(p) (5.2)\nIf PA could prove VA(p) for any p, then PA would prove that q does not halt since this is the contrapositive of (5.2). Therefore the set P remains empty.\nAIXItl exhibits all the problems of the arbitrariness of the UTM illustrated by the indifference prior (Theorem 5.4) and the dogmatic prior (Theorem 5.5). In addition, it is also susceptible to Gödel incompleteness as illustrated by the Gödel prior in Theorem 5.8. The formal system that is a parameter to AIXItl just provides another point of failure.\nAs a computable approximation to AIXI, AIXItl is needlessly complicated. As we prove in Corollary 6.13, ε-optimal AIXI is limit computable, so we can approximate it with an anytime algorithm. Bounding the computational resources to the approximation algorithm already yields a computable version of AIXI. Moreover, unlike AIXItl, this approximation actually converges to AIXI in the limit. Furthermore, we can ‘speed up’ this approximation algorithm using Hutter search (Hutter, 2002b); this is very similar but not identical to AIXItl."
    }, {
      "heading" : "5.3 Bayes Optimality",
      "text" : "The aim of the Legg-Hutter intelligence measure is to formalize the intuitive notion of intelligence mathematically. Legg and Hutter (2007a) collect various definitions of intelligence across many academic fields and destill it into the following statement (Legg and Hutter, 2007b)\nIntelligence measures an agent’s ability to achieve goals in a wide range of environments.\nThis definition is formalized as follows.\nDefinition 5.9 (Legg-Hutter Intelligence; Legg and Hutter, 2007b, Sec. 3.3)). The (Legg-Hutter) intelligence of a policy π is defined as\nΥξ(π) := ∑ ν∈M w(ν)V πν ( )\nThe Legg-Hutter intelligence of a policy π is the t0-value that π achieves across all environments from the classM weighted by the prior w. Legg and Hutter (2007b) consider a subclass ofMCCSLSC , the class of computable measures together with a Solomonoff prior w(ν) = 2−K(ν) and do not use discounting explicitly.\nTypically, the index ξ is omitted when writing Υ. However, in this section we consider the intelligence measure with respect to different priors, therefore we make this dependency explicit. The following proposition motivates the use of the index ξ instead of w."
    }, {
      "heading" : "76 Optimality",
      "text" : "Proposition 5.10 (Bayes Optimality = Maximal Intelligence). Υξ(π) = V πξ ( ) for all policies π.\nProof. Follows directly from (4.6) and Definition 5.9.\nDefinition 5.11 (Balanced Pareto Optimality; Hutter, 2005, Def. 5.22). LetM be a set of environments. A policy π is balanced Pareto optimal in the set of environments M iff for all policies π̃, ∑\nν∈M w(ν)\n( V πν ( )− V π̃ν ( ) ) ≥ 0.\nProposition 5.12 (Balanced Pareto Optimality = Maximal Intelligence). A policy π is balanced Pareto optimal inM if and only if π has maximal Legg-Hutter intelligence.\nProof. From (4.6) we get∑ ν∈M w(ν) ( V πν ( )− V π∗ξ ν ( ) ) = ∑ ν∈M w(ν)V πν ( )− ∑ ν∈M w(ν)V π∗ξ ν ( )\n= V πξ ( )− V ∗ξ ( ) = V πξ ( )− sup\nπ̃ V π̃ξ ( )\n= Υξ(π)− sup π̃ Υ(π̃)\nby Proposition 5.10. This term is nonnegative iff Υξ(π) is maximal.\nAs a consequence of Proposition 5.10 and Proposition 5.12 we get that AIXI is balanced Pareto optimal (Hutter, 2005, Thm. 5.24) and has maximal Legg-Hutter intelligence.\nΥξ := sup π Υξ(π) = sup π V πξ ( ) = V\nπ∗ξ ξ ( ) = Υξ(π ∗ ξ ).\nThis is not surprising since Legg-Hutter intelligence was defined in terms of the t0value in the Bayes mixture. Moreover, because the value function is scaled to be in the interval [0, 1], intelligence is a real number between 0 and 1.\nIt is just as hard to score very high on the Legg-Hutter intelligence measure as it is to score very low: we can always turn a reward minimizer into a reward maximizer by inverting the rewards r′t := 1 − rt. Hence the lowest possible intelligence score is achieved by AIXI’s twin sister, a ξ-expected reward minimizer:\nΥξ := infπ Υξ(π) = inf π V πξ ( )\nThe heaven environment (reward 1 forever) and the hell environment (reward 0 forever) are computable and thus in the environment classMCCSLSC ; therefore it is impossible to get a reward 0 or reward 1 in every environment. Consequently, for all policies π,\n0 < Υξ ≤ Υξ(π) ≤ Υξ < 1.\n§5.3 Bayes Optimality 77\nFor every real number r ∈ [Υξ,Υξ] there is a policy π with Υξ(π) = r: analogously to Lemma 4.14 we can define π such that with probability (r − Υξ)/(Υξ − Υξ) it follows π∗ξ and otherwise it follows arg minπ̃ V π̃ ξ ( ).\nFigure 5.1 illustrates the intelligence measure Υ. It is natural to fix the policy random that takes actions uniformly at random to have an intelligence score of 1/2 by choosing a ‘symmetric’ universal prior (Legg and Veness, 2013).\nAIXI is not computable (Theorem 6.15), hence there is no computable policy π such that Υξ(π) = Υξ or Υξ(π) = Υξ for any Bayesian mixture ξ overMCCSLSC . But the following theorem states that computable policies can come arbitrarily close. This is no surprise: by Lemma 4.17 we can do well on a Legg-Hutter intelligence test simply by memorizing what AIXI would do for the first k steps; as long as k is chosen large enough such that discounting makes the remaining rewards contribute very little to the value function.\nTheorem 5.13 (Computable Policies are Dense). The set\n{Υξ(π) | π is a computable policy}\nis dense in the set [Υξ,Υξ].\nProof. Let π be any policy and let ε > 0. We need to show that there is a computable policy π̃ with |Υξ(π̃)−Υξ(π)| < ε. We choose m large enough such that Γm/Γ1 < ε/3. Let α ∈ A be arbitrary and define the policy\nπ̃(a | æ<t) :=  π(a | æ<t)± (ε/3)−m if t < m, 1 if t ≥ m and a = α, and 0 otherwise.\nBy choosing an appropriate rational number in the interval [π(a | æ<t)− (ε/3)−m, π(a | æ<t) + (ε/3)−m] we can make the policy π̃ computable because we can store these approximations to the action probabilities of π for the first m − 1 steps in a lookup table. From Lemma 4.17 we get∣∣∣V π,mξ ( )− V π̃,mξ ( )∣∣∣ ≤ Dm−1(ξπ, ξπ̃ | ) ≤ ((ε/3)−m)m = ε3"
    }, {
      "heading" : "78 Optimality",
      "text" : "and together with Lemma 4.16 this yields\n|Υξ(π)−Υξ(π̃)| = ∣∣V πξ ( )− V π̃ξ ( )∣∣ ≤ ∣∣∣V π,mξ ( )− V π̃,mξ ( )∣∣∣+ 2ΓmΓ1 ≤ ε3 + 2ΓmΓ1 < ε.\nRemark 5.14 (Deterministic Policies are not Dense in [Υξ,Υξ]). The intelligence values of deterministic policies are generally not dense in the interval [Υξ,Υξ]. We show this by defining an environment ν where the first action determines whether the agent goes to heaven or hell: action α leads to heaven and action β leads to hell. Define Bayesian mixture ξ′ := 0.999ν + 0.001ξ and let π be any policy. If π takes action α first, then Υξ′(π) > 0.999. If π takes action β first, then Υξ′(π) < 0.001. Hence there are no deterministic policies that score an intelligence value in the closed interval [0.001, 0.999]. 3\nLegg-Hutter intelligence is measured with respect to a fixed prior. The Bayes agent is the most intelligent policy if it uses the same prior. We use the results from Section 5.2 to show that the intelligence score of the Bayes agent can be arbitrary close to the minimum intelligence score Υξ.\nCorollary 5.15 (Some AIXIs are Stupid). For any Bayesian mixture ξ over MCCSLSC and every ε > 0, there is a Bayesian mixture ξ′ such that Υξ(π∗ξ′) < Υξ + ε.\nProof. Let ε > 0. According to Theorem 5.13, there is a computable policy π such that Υξ(π) < Υξ + ε/2. From Corollary 5.7 we get a Bayesian mixture ξ′ such that |Υξ(π∗ξ′)−Υξ(π)| = |V π∗ ξ′ ξ ( )− V π ξ ( )| < ε/2, hence\n|Υξ(π∗ξ′)−Υξ| ≤ |Υξ(π∗ξ′)−Υξ(π)|+ |Υξ(π)−Υξ| < ε/2 + ε/2 = ε.\nWe get the same result if we fix AIXI, but rig the intelligence measure.\nCorollary 5.16 (AIXI is Stupid for Some Υ). For any deterministic ξ-optimal policy π∗ξ and for every ε > 0 there is a Bayesian mixture ξ\n′ such that Υξ′(π∗ξ ) ≤ ε and Υξ′ ≥ 1− ε. Proof. Let a1 := π∗ξ ( ) be the first action that π ∗ ξ takes. We define an environment ν such that taking the first action a1 leads to hell and taking any other first action leads to heaven as in Remark 5.14. We define the Bayesian mixture ξ′ := (1− ε)ν+ εξ. Since π∗ξ takes action a1 first, it goes to hell, i.e., V π∗ξ ν ( ) = 0. Hence with Lemma 4.14\nΥξ′(π ∗ ξ ) = V π∗ξ ξ′ ( ) = (1− ε)V π∗ξ ν ( ) + εV π∗ξ ξ ( ) ≤ ε.\nFor any policy π that takes an action other than a1 first, we get\nΥξ′(π) = V π ξ′ ( ) = (1− ε)V πν ( ) + εV πξ ( ) ≥ 1− ε.\nOn the other hand, we can make any computable policy smart if we choose the right Bayesian mixture. In particular, we get that there is a Bayesian mixture such that ‘do nothing’ is the most intelligent policy save for some ε.\nCorollary 5.17 (Computable Policies can be Smart). For any computable policy π and any ε > 0 there is a Bayesian mixture ξ′ such that Υξ′(π) > Υξ′ − ε.\nProof. Corollary 5.7 yields a Bayesian mixture ξ′ with |Υξ′−Υξ′(π)| = |V ∗ξ′( )−V πξ′ ( )| < ε."
    }, {
      "heading" : "5.4 Asymptotic Optimality",
      "text" : "An asymptotically optimal policy is a policy learns to act optimally in every environment fromM, i.e., the value of this policy converges to the optimal value.\nDefinition 5.18 (Asymptotic Optimality). A policy π is asymptotically optimal in an environment classM iff for all µ ∈M\nV ∗µ (æ<t)− V πµ (æ<t)→ 0 as t→∞ (5.3)\non histories drawn from µπ.\nThere are different types of asymptotic optimality based on the type of stochastic convergence in (5.3); see Definition 2.5. If this convergence occurs almost surely, it is called strong asymptotic optimality (Lattimore and Hutter, 2011, Def. 7); if this convergence occurs in mean, it is called asymptotic optimality in mean; if this convergence occurs in probability, it is called asymptotic optimality in probability ; and if the Cesàro averages converge almost surely, it is called weak asymptotic optimality (Lattimore and Hutter, 2011, Def. 7). Since the value function is a nonnegative bounded random variable, asymptotic optimality in mean and asymptotic optimality in probability are equivalent. See Table 5.1 for the explicit definitions and see Figure 5.2 for an overview over their relationship.\nAsymptotic optimality in probability is in spirit a probably approximately correct (PAC) result: for all ε > 0 and δ > 0 the probability that our policy is ε-suboptimal converges to zero; eventually this probability will be less than δ. For a PAC result it is typically demanded that the number of time steps until the probability is less than δ\nbe polynomial in 1/ε and 1/δ. In general environments this is impossible, and here we have no ambition to provide concrete convergence rates.\nIntuitively, a necessary condition for asymptotic optimality is that the agent needs to explore infinitely often for an entire effective horizon. If we explore only finitely often, then the environment might change after we stopped exploring. Moreover, the agent needs to predict the value of counterfactual policies accurately; but by Lemma 4.16 only for an ε-effective horizon. By committing to exploration for the entire effective horizon, we learn about the value of counterfactual policies.\nExample 5.19 (Exploration Infinitely Often for an Entire Effective Horizon). If there is an ε > 0 such that the policy π does not explore for Ht(ε) steps infinitely often, then V ∗µ (æ<t) − V πµ (æ<t) > ε infinitely often. Define A := {α, β} and E := {0, ε/2, 1} (observations are vacuous) and consider the following class of environments M := {ν∞, ν1, ν2, . . .} (transitions are labeled with condition: action, reward):\ns0\nβ, ε2\nα, 0\ns0 s1 . . . sn\nβ, ε2\nt < k : α, 0\nt ≥ k : α, 0 α, 0 α, 0 β, 0\nβ, 0\nβ, 0\nα, 1\nν∞ νk\nEnvironment νk works just like environment ν∞, except that at time step k the path to state s1 gets unlocked. The length of the state sequence in νk is defined as an ε-effective horizon, n := Ht(ε) where t is the time step in which the agent leaves state s0. The optimal policy in environment ν∞ is to always take action β, the optimal policy for environment νk is to take action β for t < k and then take action α. Suppose the agent is in time step t and in state s0. Since these environments are partially observable, it needs to explore for n steps (take action α n times) to distinguish ν∞ from νk for any k ≤ t. Since there are infinitely many νk, the agent needs to do this infinitely often. Moreover, V ∗ν1 ≥ ε and V ∗ ν∞ = ε/2, so if νt is the true environment, then not exploring to the right for an ε-effective horizon is suboptimal by ε/2. But if ν∞ is the true environment, then exploring incurs an opportunity cost of one reward of ε/2. 3\n§5.4 Asymptotic Optimality 81\nNext, we state two negative results about asymptotic optimality proved by Lattimore and Hutter (2011). It is important to emphasize that Theorem 5.20 and Theorem 5.21 only hold for deterministic policies.\nTheorem 5.20 (Deterministic Policies are not Strongly Asymptotically Optimal; Lattimore and Hutter, 2011, Thm. 8). There is no deterministic policy that is strong asymptotically optimal in the classMCCMcomp.\nIf the horizon grows linearly (for example, power discounting γ(t) = t−β with β > 1; see Table 4.1), then a deterministic policy cannot be weakly asymptotically optimal policy: the agent has to explore for an entire effective horizon, which prevents the Cesàro average from converging.\nTheorem 5.21 (Necessary Condition for Weak Asymptotic Optimality; Lattimore, 2013, Thm. 5.5). If there is an ε > 0 such that Ht(ε) /∈ o(t), then there is no deterministic policy that is weakly asymptotically optimal in the classMCCMcomp.\nThere are several agents that achieve asymptotic optimality. In the rest of this section, we discuss the Bayes agent, BayesExp, and Thompson sampling. Asymptotic optimality can also be achieved through optimism (Sunehag and Hutter, 2012a,b, 2015)."
    }, {
      "heading" : "5.4.1 Bayes",
      "text" : "In this section, we list two results from the literature regarding the asymptotic optimality of the Bayes optimal policy. The following negative result is due to Orseau (2010, 2013).\nTheorem 5.22 (Bayes is not Asymptotically Optimal in General Environments; Orseau, 2013, Thm. 4). For any classM⊇MCCMcomp no Bayes optimal policy π∗ξ is asymptotically optimal: there is an environment µ ∈ M and a time step t0 ∈ N such that µπ ∗ ξ -almost surely for all time steps t ≥ t0\nV ∗µ (æ<t)− V π∗ξ µ (æ<t) = 1\n2 .\nOrseau calls this result the good enough effect : A Bayesian agent eventually decides that the current strategy is good enough and that any additional exploration is not worth its expected payoff. However, if the environment changes afterwards, the Bayes agent is acting suboptimally.\nProof. Without loss of generality assume A := {α, β} and E := {0, 1/2, 1} (observations are vacuous). We consider the following environment µ (transitions are labeled with action, reward)."
    }, {
      "heading" : "82 Optimality",
      "text" : "s0 s1 . . . sn\nβ, 12\nα, 0 ∗, 0 ∗, 0\n∗, 0\nIn state s0 the action β is the exploitation action and the action α the exploration action. The length of the state sequence is defined as an 1/t-effective horizon, n := Ht(1/t) where t is the time step in which the agent leaves state s0. Since the discount function γ is computable by Assumption 4.6a, µ ∈MCCSLSC .\nAssume that when acting in µ, the Bayes agent explores infinitely often. Let æ<t be a history in which the agent is in state s0 and takes action α. Then V π∗ξ µ ≤ 1/t. By on-policy value convergence (Corollary 4.20), V ∗ξ (æ<t)− V π∗ξ µ (æ<t)→ 0 µπ ∗ ξ -almost surely. Hence there is a time step t0 such that for all t ≥ t0 we have V ∗ξ < w(µ)/2. Since µ is deterministic, w(µ | æ<t) ≥ w(µ). Now we get a contradiction from (4.7):\nV ∗ξ (æ<t) ≥ w(µ|æ<t)V ∗µ (æ<t) ≥ w(µ)V ∗µ (æ<t) = w(µ) 2 > V ∗ξ (æ<t)\nTherefore the Bayes agent stops taking the exploration action α after time step t0, and so it is not optimal in any ν ∈ MCCSLSC that behaves like µ until time step t0 and then changes:\ns0 s1 . . . sn\nβ, 12\nt > t0 : α, 1 t ≤ t0 : α, 0 ∗, 0 ∗, 0\n∗, 0\nThe following theorem is also known as the self-optimizing theorem. This theorem has been a source of great confusion because its statement in Hutter (2005, Thm. 5.34) is not very explicit about how the histories are generated. The formulation of Lattimore (2013, Thm. 5.2) is explicit, but less general.\nTheorem 5.23 (Sufficient Condition for Strong Asymptotic Optimality of Bayes; Hutter, 2005, Thm. 5.34). Let µ be some environment. If there is a policy π and a sequence of policies π1, π2, . . . such that for all ν ∈M\nV ∗ν (æ<t)− V πtν (æ<t)→ 0 as t→∞ µπ-almost surely, (5.4)\nthen V ∗µ (æ<t)− V π∗ξ µ (æ<t)→ 0 as t→∞ µπ-almost surely.\n§5.4 Asymptotic Optimality 83\nIf π = π∗ξ and (5.4) holds for all µ ∈ M, then π∗ξ is strongly asymptotically optimal in the classM.\nIt is important to emphasize that the policies π1, π2, . . . need to converge to the optimal value on the history generated by µ and π, and not (as one might think) ν and πt. Intuitively, the policy π is an ‘exploration policy’ that ensures that the environment class is explored sufficiently. Typically, a policy is asymptotically optimal on its own history. So if π = π1 = π2 = . . ., then we get that Bayes is asymptotically optimal on the history generated by the policy π, not its own history. In light of Theorem 5.5 and Theorem 5.22 this is not too surprising; Bayesian reinforcement learning agents might not explore enough to be asymptotically optimal, but given a policy that does explore enough, Bayes learns enough to be asymptotically optimal.\nThis invites us to define the following policies πt: follow the information-seeking policy π∗IG until time step t, and then follow π ∗ ξ (explore until t, then exploit). Since the information-seeking policy explores enough to prove off-policy prediction (Orseau et al., 2013, Thm. 7), we get V πξ −V πµ → 0 for every policy π uniformly. Hence arg maxπ V πξ → arg maxπ V π µ and thus V ∗µ − V π∗ξ µ → 0 and (5.4) is satisfied. From Theorem 5.23 we get V ∗µ − V π∗ξ µ → 0, which we already knew. In order to get strong asymptotic optimality, all we need to do is choose the switching time step t appropriately, i.e., wait until V ∗µ and V π∗ξ µ are close enough. Unfortunately, this is an invalid strategy: the agent does not know the true environment µ and hence cannot check this condition. Hutter (2005, Sec. 5.6) uses Theorem 5.23 to show that the Bayes optimal policy is strongly asymptotically optimal in the class of ergodic finite-state MDPs if the effective horizon is growing, i.e., Ht(ε)→∞ for all ε > 0. This relies on the fact that in ergodic finite-state MDPs we need a fixed number of steps to explore the entire environment up to ε-confidence. Therefore we can define a sequence of policies π1, π2, . . . that completely disregard the history and start exploring everything from scratch. Since the effective horizon is growing, this exploration phase takes a vanishing fraction of effective horizon and most of the value is retained. Therefore the sequence of policies π1, π2, . . . satisfies the condition of Theorem 5.23 regardless of the history, thus in particular for the history generated by π = π∗ξ and any µ ∈ M. Note that the condition on the horizon is important: If the effective horizon is bounded, then Bayes is not asymptotically optimal in the class of ergodic finite-state MDPs because it can be locked into a dogmatic prior similarly to Theorem 5.5.\nProof of Theorem 5.23. From (4.6) we get for any history æ<t\nw(µ | æ<t) ( V ∗µ (æ<t)− V π∗ξ µ (æ<t) ) ≤ ∑ ν∈M w(ν | æ<t) ( V ∗ν (æ<t)− V π∗ξ ν (æ<t) ) =\n(∑ ν∈M w(ν | æ<t)V ∗ν (æ<t) ) − V π∗ξ ξ (æ<t)\n≤ ∑ ν∈M w(ν | æ<t)V ∗ν (æ<t)− V πt ξ (æ<t)\n84 Optimality\n= ∑ ν∈M w(ν | æ<t) ( V ∗ν (æ<t)− V πtν (æ<t) ) . (5.5)\nFrom (5.4) follows that V ∗ν −V πtν → 0 µπ-almost surely for all ν ∈M, so (5.5) converges to 0 µπ-almost surely (Hutter, 2005, Lem. 5.28ii). Similar to Example 3.20, 1/w(µ | æ<t) is a nonnegative µπ-martingale and thus converges (to a finite value) µπ-almost surely by Theorem 2.8. Therefore V ∗µ (æ<t) − V π∗ξ µ (æ<t) → 0 µπ-almost surely. If this is true for all µ ∈ M, the strong asymptotic optimality of π∗ξ follows from π = π∗ξ by definition."
    }, {
      "heading" : "5.4.2 BayesExp",
      "text" : "The definition of BayesExp is given in Section 4.3.3. In this subsection we state a result by Lattimore (2013) that motivated the definition of BayesExp.\nTheorem 5.24 (BayesExp is Weakly Asymptotically Optimal; Lattimore, 2013, Thm. 5.6). Let πBE denote the policy from Algorithm 1. If Ht(ε) grows monotone in t and Ht(εt)/εt ∈ o(t), then for all environments µ ∈M\n1\nt t∑ k=1 ( V ∗µ (æ<k)− V πBEµ (æ<k) ) → 0 as t→∞ µπBE -almost surely.\nIf the horizon grows sublinearly (Ht(ε) ∈ o(t) for all ε > 0), then we can always find a sequence εt → 0 that decreases slowly enough such that Ht(ε)/εt ∈ o(t) holds."
    }, {
      "heading" : "5.4.3 Thompson Sampling",
      "text" : "In this section we prove that the Thompson sampling policy defined in Section 4.3.4 is asymptotically optimal. Ortega and Braun (2010) prove that the action probabilities of Thompson sampling converge to the action probability of the optimal policy almost surely, but require a finite environment class and two (arguably quite strong) technical assumptions on the behavior of the posterior distribution (akin to ergodicity) and the similarity of environments in the class. Our convergence results do not require these assumptions.\nTheorem 5.25 (Thompson Sampling is Asymptotically Optimal in Mean). For all environments µ ∈M,\nEπTµ [ V ∗µ (æ<t)− V πTµ (æ<t) ] → 0 as t→∞.\nThis theorem immediately implies that Thompson sampling is also asymptotically optimal in probability according to Figure 5.2. However, this does not imply almost sure convergence (see Example 5.28).\nWe first give an intuition for the asymptotic optimality of Thompson sampling. At every resampling step we can split the classM into three partitions:\n1. Environments ρ where V π∗ρ µ ≈ V ∗µ\n§5.4 Asymptotic Optimality 85\n2. Environments ρ where V ∗ρ > V ∗µ\n3. Environments ρ where V ∗ρ < V ∗µ\nThe first class is the class of ‘good’ environments: if we draw one of them, we follow a policy that is close to optimal in µ. The second class is the class of environments that overestimate the value of µ. Following their optimal policy the agent gains information because rewards will be lower than expected. The third class is the class of environments that underestimate the value of µ. Following their optimal policy the agent might not gain information since µ might behave just like environment ρ on the ρ-optimal policy. However, when sampling from the first class instead, the agent gains information about the third class because rewards tend to be better than environments from the third class predicted.\nSince the true environment µ ∈M, the first class is not empty, and the probability of drawing a sample from the first class does not become too small. Whenever the second and third class have sufficiently high weight in the posterior, there is a good chance of picking a policy that leads the agent to gain information. Asymptotically, the posterior converges, so the agent ends up having learned everything it could, i.e., the posterior weight of the second and third class vanishes.\nThis argument is not too hard to formalize for deterministic environment classes. However, for stochastic environment classes the effect on the posterior when following a bad policy is harder to quantify because there is always a chance that the rewards are different simply because of bad luck. In order to prove this theorem in its generality for stochastic classes, we employ an entirely different proof strategy that relies on statistical tools rather than the argument given above.\nDefinition 5.26 (Expected Total Variation Distance). Let π be any policy and let m ∈ N ∪∞. The expected total variation distance on the policy π is\nF πm(æ<t) := ∑ ρ∈M w(ρ | æ<t)Dm(ρπ, ξπ | æ<t).\nIf we replace the distance measure Dm by cross-entropy, then the quantity F πm(æ<t) becomes the expected information gain (see Section 4.3.2).\nFor the proof of Theorem 5.25 we need the following lemma.\nLemma 5.27 (Expected Total Variation Distance Vanishes On-Policy). For any policy π and any environment µ,\nEπµ[F π∞(æ<t)]→ 0 as t→∞.\nProof. From Theorem 3.25 we get D∞(µπ, ξπ | æ<t)→ 0 µπ-almost surely, and since D is bounded, this convergence also occurs in mean. Thus for every environment ν ∈M,\nEπν [ D∞(ν π, ξπ | æ<t) ] → 0 as t→∞."
    }, {
      "heading" : "86 Optimality",
      "text" : "Now\nEπµ[F π∞(æ<t)] ≤ 1\nw(µ) Eπξ [F π∞(æ<t)]\n= 1\nw(µ) Eπξ [∑ ν∈M w(ν | æ<t)D∞(νπ, ξπ | æ<t) ]\n= 1\nw(µ) Eπξ [∑ ν∈M w(ν) νπ(æ<t) ξπ(æ<t) D∞(ν π, ξπ | æ<t) ]\n= 1\nw(µ) ∑ ν∈M w(ν)Eπν [ D∞(ν π, ξπ | æ<t) ] → 0\nby Hutter (2005, Lem. 5.28ii) since total variation distance is bounded.\nProof of Theorem 5.25. Let β, δ > 0 and let εt > 0 denote the sequence used to define πT in Algorithm 2. We assume that t is large enough such that εk ≤ β for all k ≥ t and that δ is small enough such that w(µ | æ<t) > 4δ for all t, which holds since w(µ | æ<t) 6→ 0 µπ-almost surely for any policy π (Hutter, 2009a, Lem. 3i).\nThe stochastic process w(ν | æ<t) is a ξπT -martingale according to Example 3.20. By the martingale convergence theorem (Theorem 2.8) w(ν | æ<t) converges ξπT -almost surely and because ξπT ≥ w(µ)µπT it also converges µπT -almost surely.\nWe argue that we can choose t0 to be one of πT ’s resampling time steps large enough such that for all t ≥ t0 the following three events hold simultaneously with µπT -probability at least 1− δ.\n(i) There is a finite setM′ ⊂ M with w(M′ | æ<t) > 1 − δ and w(ν | æ<k) 6→ 0 as k →∞ for all ν ∈M′.\n(ii) |w(M′′ | æ<t)− w(M′′ | æ<t0)| ≤ δ for allM′′ ⊆M′.\n(iii) F πT∞ (æ<t) < δβw2min.\nwhere wmin := inf{w(ν | æ<k) | k ∈ N, ν ∈M′}, which is positive by (i). (i) and (ii) are satisfied eventually because the posterior w( · | æ<t) converges µπT - almost surely. Note that the set M′ is random: the limit of w(ν | æ<t) as t → ∞ depends on the history æ1:∞. Without loss of generality, we assume the true environment µ is contained inM′ since w(µ | æ<t) 6→ 0 µπT -almost surely. (iii) follows from Lemma 5.27 since convergence in mean implies convergence in probability.\nMoreover, we define the horizon m := t + Ht(εt) as the time step of the effective horizon at time step t. Let æ<t be a fixed history for which (i-iii) is satisfied. Then we have\nδβw2min > F πT ∞ (æ<t) = ∑ ν∈M w(ν | æ<t)D∞(νπT , ξπT | æ<t)\n= Eν∼w( ·|æ<t) [D∞(ν πT , ξπT | æ<t)]\n§5.4 Asymptotic Optimality 87\n≥ Eν∼w( ·|æ<t) [Dm(ν πT , ξπT | æ<t)] ≥ βw2minw(M\\M′′ | æ<t)\nby Markov’s inequality where\nM′′ := { ν ∈M ∣∣ Dm(νπT , ξπT | æ<t) < βw2min} . For our fixed history æ<t we have\n1− δ < w(M′′ | æ<t) (i)\n≤ w(M′′ ∩M′ | æ<t) + δ (ii) ≤ w(M′′ ∩M′ | æ<t0) + 2δ (i) ≤ w(M′′ | æ<t0) + 3δ\nand thus we get\n1− 4δ < w ( {ν ∈M | Dm(νπT , ξπT | æ<t) < βw2min} ∣∣ æ<t0) . (5.6) In particular, this bound holds for ν = µ since w(µ | æ<t0) > 4δ by assumption.\nIt remains to show that with high probability the value V π∗ρ µ of the sample ρ’s optimal policy π∗ρ is sufficiently close to the µ-optimal value V ∗µ . The worst case is that we draw the worst sample from M′ ∩ M′′ twice in a row. From now on, let ρ denote the sample environment we draw at time step t0, and let t denote some time step between t0 and t1 := t0 +Ht0(εt0) (before the next resampling). With probability w(ν ′ | æ<t0)w(ν ′ | æ<t1) we sample ν ′ both at t0 and t1 when following πT . Therefore we have for all æt:m and all ν ∈M\nνπT (æ1:m | æ<t) ≥ w(ν ′ | æ<t0)w(ν ′ | æ<t1)ν π∗ ν′ (æ1:m | æ<t).\nThus we get for all ν ∈M′ (in particular ρ and µ)\nDm(µ πT , ρπT | æ<t) ≥ sup\nν′∈M sup A⊆(A×E)m ∣∣∣w(ν ′ | æ<t0)w(ν ′ | æ<t1) (µπ ∗ ν′ (A | æ<t)− ρπ ∗ ν′ (A | æ<t))\n∣∣∣ ≥ w(ν | æ<t0)w(ν | æ<t1) sup\nA⊆(A×E)m ∣∣∣µπ∗ν (A | æ<t)− ρπ∗ν (A | æ<t)∣∣∣ ≥ w2minDm(µπ ∗ ν , ρπ ∗ ν | æ<t).\nFor ρ ∈M′′ we get with (5.6)\nDm(µ πT , ρπT | æ<t) ≤ Dm(µπT , ξπT | æ<t) +Dm(ρπT , ξπT | æ<t)\n< βw2min + βw 2 min = 2βw 2 min,"
    }, {
      "heading" : "88 Optimality",
      "text" : "which together with Lemma 4.17 and the fact that rewards in [0, 1] implies∣∣∣V π∗νµ (æ<t)− V π∗νρ (æ<t)∣∣∣ ≤ Γt+Ht(εt)Γt + ∣∣∣V π∗ν ,mµ (æ<t)− V π∗ν ,mρ (æ<t)∣∣∣\n≤ εt +Dm(µπ ∗ ν , ρπ ∗ ν | æ<t) ≤ εt + 1w2minDm(µ πT , ρπT | æ<t)\n< β + 2β = 3β.\nHence we get (omitting history arguments æ<t for simplicity)\nV ∗µ = V π∗µ µ < V π∗µ ρ + 3β ≤ V ∗ρ + 3β = V π∗ρ ρ + 3β < V π∗ρ µ + 3β + 3β = V π∗ρ µ + 6β. (5.7)\nWith µπT -probability at least 1− δ (i), (ii), and (iii) are true, with µπT -probability at least 1−δ our sample ρ happens to be inM′ by (i), and with w( · | æ<t0)-probability at least 1−4δ the sample is inM′′ by (5.6). All of these events are true simultaneously with probability at least 1− (δ + δ + 4δ) = 1− 6δ. Hence the bound (5.7) transfers for πT such that with µπT -probability ≥ 1− 6δ we have\nV ∗µ (æ<t)− V πTµ (æ<t) < 6β.\nTherefore µπT [V ∗µ (æ<t)−V πTµ (æ<t) ≥ 6β] < 6δ and with δ → 0 we get that V ∗µ (æ<t)− V πTµ (æ<t) → 0 as t → ∞ in probability. The value function is bounded, thus it also converges in mean.\nThe following example shows that the Thompson sampling policy is not strongly asymptotically optimal. However, we expect that strong asymptotic optimality can be achieved with Thompson sampling by resampling at every time step (with strong assumptions on the discount function). However, for practical purposes resampling in every time step is very inefficient.\nExample 5.28 (Thompson Sampling is not Strongly Asymptotically Optimal). Define A := {α, β}, E := {0, 1/2, 1}, and assume geometric discounting (Example 4.5). Consider the following class of environmentsM := {ν∞, ν1, ν2, . . .} (transitions are labeled with action, reward):\ns0\ns1\ns2\nβ, 12\nα, 0\nβ, 0\nα, 0\n∗, 0\ns0\ns1\ns2\ns3\ns4\nβ, 12\nt < k : α, 0\nβ, 0\nα, 0\n∗, 0\nt ≥ k : α, 0\nα, 0 β, 0\nα, 1\nβ, 0\nν∞ νk\n§5.4 Asymptotic Optimality 89\nEnvironment νk works just like environment ν∞ except that after time step k, the path to state s3 gets unlocked. The classM is a class of deterministic weakly communicating POMDPs (but as a POMDP νk has more than 5 states). The optimal policy in environment ν∞ is to always take action β, the optimal policy for environment νk is to take action β for t < k and then take action β in state s1 and action α otherwise.\nSuppose the policy πT is acting in environment ν∞. Since it is asymptotically optimal in the class M, it has to take actions αα from s0 infinitely often: for t < k environment νk is indistinguishable from ν∞, so the posterior for νk is larger or equal to the prior. Hence there is always a constant chance of sampling νk until taking actions αα, at which point all environments νk for k ≤ t become falsified.\nIf the policy πT decides to explore and take the first action α, it will be in state s1. Let æ<t denote the current history. Then the ν∞-optimal action is β and\nV ∗ν∞(æ<t) = (1− γ) ( 0 + γ 1\n2 + γ2\n1 2 + . . .\n) = γ\n2 .\nThe next action taken by πT is α since any optimal policy for any sampled environment that takes action α once, takes that action again (and we are following that policy for an εt-effective horizon). Hence\nV πTν∞ (æ<t) ≤ (1− γ) ( 0 + 0 + γ2 1\n2 + γ3\n1 2 + . . .\n) = γ2\n2 .\nTherefore V ∗ν∞ − V πT ν∞ ≥ (γ − γ 2)/2 > 0. This happens infinitely often with probability one and thus we cannot get almost sure convergence. 3\nIf the Bayesian mixture ξ is inside the classM (as it is the case for the classMCCSLSC ), then we can assign ξ a prior probability that is arbitrarily close to 1. Since the posterior of ξ is the same as the prior, Thompson sampling will act according to the Bayes optimal policy most of the time. This means the Bayes-value of Thompson sampling can be very good; formally, V ∗ξ ( )− V πT ξ ( ) = Υξ −Υξ(πT ) can be made arbitrarily small.\nIn contrast, the Bayes-value of Thompson sampling can also be very bad: Suppose you have a class of (n+ 1)-armed bandits indexed 1, . . . , n where bandit i gives reward 1− ε on arm 1, reward 1 on arm i+ 1, and reward 0 on all other arms. For geometric discounting and ε < (1− γ)/(2− γ), it is Bayes optimal to pull arm 1 while Thompson sampling will explore on average n/2 arms until it finds the optimal arm. The Bayesvalue of Thompson sampling is 1/(n−γn−1) in contract to (1−ε) achieved by Bayes. For a horizon of n, the Bayes optimal policy suffers a regret of εn and Thompson sampling a regret of n/2, which is much larger for small ε."
    }, {
      "heading" : "5.4.4 Almost Sure in Cesàro Average vs. in Mean",
      "text" : "It might appear that convergence in mean is more natural than the convergence of Cesàro averages of weak asymptotic optimality. However, both notions are not so fundamentally different because they both allow an infinite number of bad mistakes (actions that lead to V ∗µ − V πµ being large). Asymptotic optimality in mean allows bad"
    }, {
      "heading" : "90 Optimality",
      "text" : "mistakes as long as their probability converges to zero; weak asymptotic optimality allows bad mistakes as long as the total time spent on bad mistakes grows sublinearly. Note that according to Example 5.19 making bad mistakes infinitely often is necessary for asymptotic optimality.\nTheorem 5.24 shows that weak asymptotic optimality is possible in any countable class of stochastic environments. However, this requires the additional condition that the effective horizon grows sublinearly, Ht(εt) ∈ o(t), while Theorem 5.25 does not require any condition on the discount function.\nGenerally, weak asymptotic optimality and asymptotic optimality in mean are incomparable because the notions of convergence are incomparable for (bounded) random variables. First, for deterministic sequences (i.e. deterministic policies in deterministic environments), convergence in mean is equivalent to (regular) convergence, which is impossible by Theorem 5.20. Second, convergence in probability (and hence convergence in mean for bounded random variables) does not imply almost sure convergence of Cesàro averages (Stoyanov, 2013, Sec. 14.18). We leave open the question whether the policy πT is weakly asymptotically optimal."
    }, {
      "heading" : "5.5 Regret",
      "text" : "Regret is how many expected rewards the agent forfeits by not following the best informed policy.\nDefinition 5.29 (Regret). The regret of a policy π in environment µ is\nRm(π, µ) := sup π′\nEπ ′ µ [ m∑ t=1 rt ] − Eπµ [ m∑ t=1 rt ] .\nNote that regret is undiscounted and always nonnegative. Moreover, the space of possible different policies for the first m actions is finite and we assumed the set of actions A and the set of percepts E to be finite (Assumption 4.6c), so the supremum is always attained by some policy (not necessarily the µ-optimal policy π∗µ because that policy uses discounting).\nDifferent problem classes have different regret rates, depending on the structure and the difficulty of the problem class. Multi-armed bandits provide a (problemindependent) worst-case regret bound of Ω( √ km) where k is the number of arms (Bubeck and Bianchi, 2012). In MDPs the lower bound is Ω( √ SAdm) where S is the number of states, A the number of actions, and d the diameter of the MDP (Auer et al., 2010). For a countable class of environments given by state representation functions that map histories to MDP states, a regret of Õ(m2/3) is achievable assuming the resulting MDP is weakly communicating (Nguyen et al., 2013).\nA problem class is considered learnable if there is an algorithm that has a sublinear regret guarantee. The following example shows that the general reinforcement learning problem is not learnable because the agent can get caught in a trap and be unable to recover.\n§5.5 Regret 91\nExample 5.30 (Linear Regret; Hutter, 2005, Sec. 5.3.2). Consider the following two environments µ1 and µ2. In environment µ1 action α leads to hell (reward 0 forever) and action β leads to heaven (reward 1 forever). Environment µ2 behaves just the same, except that both actions are swapped.\nhell heaven\nreward = 0 reward = 1\nβα\nhell heaven\nreward = 0 reward = 1\nβ α\nµ1 µ2\nThe policy α that takes action α in the first time step performs well in µ2 but performs poorly in µ1. Likewise, the policy β that takes action β in the first time step performs well in µ1 but performs poorly in µ2. Regardless of which policy we adopt, our regret is always linear in one of the environments µ1 or µ2:\nRm(α, µ1) = m Rm(α, µ2) = 0\nRm(β, µ1) = 0 Rm(β, µ2) = m 3\nTo achieve sublinear regret we need to ensure that the agent can recover from mistakes. Formally, we make the following assumption.\nDefinition 5.31 (Recoverability). An environment µ satisfies the recoverability assumption iff\nsup π ∣∣∣Eπ∗µµ [V ∗µ (æ<t)]− Eπµ[V ∗µ (æ<t)]∣∣∣→ 0 as t→∞. Recoverability compares following the worst policy π for t− 1 time steps and then switching to the optimal policy π∗ν to having followed π∗ν from the beginning. The recoverability assumption states that switching to the optimal policy at any time step enables the recovery of most of the value: it has to become less costly to recover from mistakes as time progresses. This should be regarded as an effect of the discount function: if the (effective) horizon grows, recovery becomes easier because the optimal policy has more time to perform a recovery. Moreover, recoverability is on the optimal policy, in contrast to the notion of ergodicity in MDPs which demands returning to a starting state regardless of the policy.\nRemark 5.32 (Weakly Communicating POMDPs are Recoverable). If the effective horizon is growing, Ht(ε)→∞ as t→∞, then any weakly communicating finite state POMDP satisfies the recoverability assumption. 3"
    }, {
      "heading" : "5.5.1 Sublinear Regret in Recoverable Environments",
      "text" : "This subsection is dedicated to the following theorem that connects asymptotic optimality in mean to sublinear regret."
    }, {
      "heading" : "92 Optimality",
      "text" : "Theorem 5.33 (Sublinear Regret in Recoverable Environments). If the discount function γ satisfies Assumption 5.34, the environment µ satisfies the recoverability assumption, and π is asymptotically optimal in mean in the class {µ}, then Rm(π, µ) ∈ o(m).\nAssumption 5.34 (Discount Function). Let the discount function γ be such that\n(a) γt > 0 for all t,\n(b) γt is monotone decreasing in t, and\n(c) Ht(ε) ∈ o(t) for all ε > 0.\nThis assumption demands that the discount function is somewhat well-behaved: the function has no oscillations, does not become 0, and the horizon is not growing too fast. It is satisfied by geometric discounting (Example 4.5): (a) γt > 0, (b) γ monotone decreasing, and (c) Ht(ε) = dlogγ εe ∈ o(t).\nThe problem with geometric discounting is that it makes the recoverability assumption very strong: since the horizon is not growing, the environment has to enable faster recovery as time progresses; in this case weakly communicating POMDPs are not recoverable. A choice with Ht(ε) → ∞ that satisfies Assumption 5.34 is subgeometric discounting γt := e− √ t/ √ t (see Table 4.1).\nIf the items in Assumption 5.34 are violated, Theorem 5.33 can fail:\n• If γt = 0 for some time steps t, our policy does not care about those time steps and might take actions that have large regret.\n• Similarly if γ oscillates between high values and very low values: our policy might take high-regret actions in time steps with comparatively lower γ-weight.\n• If the horizon grows linearly, infinitely often our policy might spend some constant fraction of the current effective horizon exploring, which incurs a cost that is a constant fraction of the total regret so far.\nTo prove Theorem 5.33 we require the following technical lemma.\nLemma 5.35 (Value and Regret). Let ε > 0 and assume the discount function γ satisfies Assumption 5.34. Let (dt)t∈N be a sequence of numbers with |dt| ≤ 1 for all t. If there is a time step t0 with\n1\nΓt ∞∑ k=t γkdk < ε ∀t ≥ t0 (5.8)\nthen m∑ t=1 dt ≤ t0 + ε(m− t0 + 1) + 1 + ε 1− ε Hm(ε).\nProof. This proof essentially follows the proof of Hutter (2006b, Thm. 17).\n§5.5 Regret 93\nBy Assumption 5.34a we have γt > 0 for all t and hence Γt > 0 for all t. By Assumption 5.34b have that γ is monotone decreasing, so we get for all n ∈ N\nΓt = ∞∑ k=t γk ≤ t+n−1∑ k=t γt + ∞∑ k=t+n γk = nγt + Γt+n.\nAnd with n := Ht(ε) this yields\nγtHt(ε)\nΓt ≥ 1−\nΓt+Ht(ε)\nΓt ≥ 1− ε > 0. (5.9)\nIn particular, this bound holds for all t and ε > 0.\nNext, we define a series of nonnegative weights (bt)t≥1 such that\nm∑ t=t0 dk = m∑ t=t0 bt Γt m∑ k=t γkdk.\nThis yields the constraints t∑\nk=t0\nbk Γk γt = 1 ∀t ≥ t0.\nThe solution to these constraints is\nbt0 = Γt0 γt0 , and bt = Γt γt − Γt γt−1 for t > t0. (5.10)\nThus we get\nm∑ t=t0 bt = Γt0 γt0 + m∑ t=t0+1 ( Γt γt − Γt γt−1 )\n= Γm+1 γm + m∑ t=t0 ( Γt γt − Γt+1 γt ) =\nΓm+1 γm +m− t0 + 1\n≤ Hm(ε) 1− ε +m− t0 + 1\nfor all ε > 0 according to (5.9).\nFinally,\nm∑ t=1 dt ≤ t0∑ t=1 dt + m∑ t=t0 bt Γt m∑ k=t γkdk\n≤ t0 + m∑ t=t0 bt Γt ∞∑ k=t γkdk − m∑ t=t0 bt Γt ∞∑ k=m+1 γkdk"
    }, {
      "heading" : "94 Optimality",
      "text" : "and using the assumption (5.8) and dt ≥ −1,\n< t0 + m∑ t=t0 btε+ m∑ t=t0 btΓm+1 Γt\n≤ t0 + εHm(ε)\n1− ε + ε(m− t0 + 1) + m∑ t=t0 btΓm+1 Γt\nFor the latter term we substitute (5.10) to get\nm∑ t=t0 btΓm+1 Γt = Γm+1 γt0 + m∑ t=t0+1 ( Γm+1 γt − Γm+1 γt−1 ) = Γm+1 γm ≤ Hm(ε) 1− ε\nwith (5.9).\nProof of Theorem 5.33. Let (πm)m∈N denote any sequence of policies, such as a sequence of policies that attain the supremum in the definition of regret. We want to show that\nEπmµ [ m∑ t=1 rt ] − Eπµ [ m∑ t=1 rt ] ∈ o(m).\nFor d\n(m) k := E πm µ [rk]− Eπµ[rk] (5.11)\nwe have −1 ≤ d(m)k ≤ 1 since we assumed rewards to be bounded between 0 and 1. Because the environment µ satisfies the recoverability assumption we have∣∣∣Eπ∗µµ [V ∗µ (æ<t)]− Eπµ[V ∗µ (æ<t)]∣∣∣→ 0 as t→∞, and\nsup m ∣∣∣Eπ∗µµ [V ∗µ (æ<t)]− Eπmµ [V ∗µ (æ<t)]∣∣∣→ 0 as t→∞, so we conclude that\nsup m ∣∣Eπµ[V ∗µ (æ<t)]− Eπmµ [V ∗µ (æ<t)]∣∣→ 0 by the triangle inequality and thus\nsup m\nEπmµ [V ∗µ (æ<t)]− Eπµ[V ∗µ (æ<t)]→ 0 as t→∞. (5.12)\nBy assumption the policy π is asymptotically optimal in mean, so we have\nEπµ[V ∗µ (æ<t)]− Eπµ[V πµ (æ<t)]→ 0 as t→∞,\nand with (5.12) this combines to\nsup m\nEπmµ [V ∗µ (æ<t)]− Eπµ[V πµ (æ<t)]→ 0 as t→∞.\n§5.5 Regret 95\nFrom V ∗µ (æ<t) ≥ V πmµ (æ<t) we get\nlim sup t→∞ ( sup m Eπmµ [V πmµ (æ<t)]− Eπµ[V πµ (æ<t)] ) ≤ 0. (5.13)\nFor π′ ∈ {π, π1, π2, . . .} we have\nEπ ′ µ [V π′ µ (æ<t)] = Eπ ′ µ\n[ 1\nΓt Eπ ′ µ [ ∞∑ k=t γkrk ∣∣∣∣∣æ<t ]] = Eπ ′ µ [ 1 Γt ∞∑ k=t γkrk ] = 1 Γt ∞∑ k=t γkEπ ′ µ [rk],\nso from (5.11) and (5.13) we get\nlim sup t→∞ sup m\n1\nΓt ∞∑ k=t γkd (m) k ≤ 0.\nLet ε > 0. We choose t0 independent of m and large enough such that we get supm ∑∞ k=t γkd (m) k /Γt < ε for all t ≥ t0. Now we let m ∈ N be given and apply Lemma 5.35 to get\nRm(π, µ)\nm =\n∑m k=1 d (m) k\nm ≤ t0 + ε(m− t0 + 1) + 1+ε1−εHm(ε) m .\nSince Ht(ε) ∈ o(t) according to Assumption 5.34c we get lim supm→∞Rm(π, µ)/m ≤ 0.\nExample 5.36 (The Converse of Theorem 5.33 is False). Let µ be a two-armed Bernoulli bandit with means 0 and 1 and suppose we are using geometric discounting with discount factor γ ∈ [0, 1). This environment is recoverable. If our policy π pulls the suboptimal arm exactly on time steps 1, 2, 4, 8, 16, . . ., regret will be logarithmic. However, on time steps t = 2n for n ∈ N the value difference V ∗µ − V πµ is deterministically at least 1− γ > 0. 3\nNote that Example 5.36 does not rule out weak asymptotic optimality."
    }, {
      "heading" : "5.5.2 Regret of the Optimal Policy and Thompson sampling",
      "text" : "We get the following immediate consequence.\nCorollary 5.37 (Sublinear Regret for the Optimal Discounted Policy). If the discount function γ satisfies Assumption 5.34 and the environment µ satisfies the recoverability assumption, then Rm(π∗µ, µ) ∈ o(m).\nProof. From Theorem 5.33 since the policy π∗µ is (trivially) asymptotically optimal in {µ}.\nIf the environment does not satisfy the recoverability assumption, regret may be linear even on the optimal policy : the optimal policy maximizes discounted rewards"
    }, {
      "heading" : "96 Optimality",
      "text" : "and this short-sightedness might incur a tradeoff that leads to linear regret later on if the environment does not allow recovery.\nCorollary 5.38 (Sublinear Regret for Thompson Sampling). If the discount function γ satisfies Assumption 5.34 and the environment µ ∈ M satisfies the recoverability assumption, then Rm(πT , µ) ∈ o(m) for the Thompson sampling policy πT .\nProof. From Theorem 5.25 and Theorem 5.33."
    }, {
      "heading" : "5.6 Discussion",
      "text" : "In this work, we disregard computational constraints. Because of this, our agents learn very efficiently and we can focus on the way they balance exploration and exploitation. So which balance is best?"
    }, {
      "heading" : "5.6.1 The Optimality of AIXI",
      "text" : "Bayesian reinforcement learning agents make the tradeoff between exploration and exploitation in the Bayes optimal way. Maximizing expected rewards according to any positive prior does not lead to enough exploration to achieve asymptotic optimality (Theorem 5.22); the prior’s bias is retained indefinitely. For bad priors this can cause serious malfunctions: the dogmatic prior defined in Section 5.2.2 can prevent a Bayesian agent from taking a single exploratory action; exploration is restricted to cases where the expected future payoff falls below some prespecified ε > 0. However, this problem can be alleviated by adding an extra exploration component to AIXI: Lattimore (2013) shows that BayesExp is weakly asymptotically optimal (Theorem 5.24).\nSo instead, we may ask the following weaker questions. Does AIXI succeed in every (ergodic) finite-state (PO)MDP, bandit problem, or sequence prediction task? Our results imply that without further assumptions on the prior, we cannot answer any of the preceding questions in the affirmative. Using a dogmatic prior (Theorem 5.5), we can make AIXI follow any computable policy as long as that policy produces rewards that are bounded away from zero.\n• In a sequence prediction task that gives a reward of 1 for every correctly predicted bit and 0 otherwise, a policy π that correctly predicts every third bit will receive an average reward of 1/3. With a π-dogmatic prior, AIXI thus only predicts a third of the bits correctly, and hence is outperformed by a uniformly random predictor.\nHowever, if we have a constant horizon of length 1, AIXI does succeed in sequence prediction (Hutter, 2005, Sec. 6.2.2). If the horizon is this short, the agent is so hedonistic that no threat of hell can deter it.\n• In a (PO)MDP a dogmatic prior can make AIXI get stuck in any loop that provides nonzero expected rewards.\n§5.6 Discussion 97\n• In a bandit problem, a dogmatic prior can make AIXI get stuck on any arm which provides nonzero expected rewards.\nThese results apply not only to AIXI, but generally to Bayesian reinforcement learning agents. Any Bayesian mixture over nonrecoverable environments is susceptible to dogmatic priors if we allow an arbitrary reweighing of the prior. Notable exceptions are classes of environment that allow policies that are strongly asymptotically optimal regardless of the history (Theorem 5.23). For example, the class of all ergodic MDPs for an unbounded effective horizon; in this case the Bayes optimal policy is strongly asymptotically optimal (Hutter, 2005, Thm. 5.38). Note that in contrast to our results, this requires that that the agent uses a Bayes-mixture over a class of ergodic MDPs.\nMoreover, Bayesian agents still perform well at learning and achieve on-policy value convergence (Corollary 4.20): the posterior belief about the value of a policy π converges to the true value of π while following π: V πξ (æ<t)− V πµ (æ<t)→ 0 as t→∞ µπ-almost surely. Since this holds for any policy, in particular it holds for the Bayes optimal policy π∗ξ . This means that the Bayes agent learns to predict those parts of the environment that it sees. But if it does not explore enough, then it will not learn other parts of the environment that are potentially more rewarding.\nHutter (2005, Claim 5.12) claims:\nWe expect AIXI to be universally optimal.\nOur work seriously challenges Hutter’s claim: no nontrivial and non-subjective optimality results for AIXI remain (see Table 5.3). Until new arguments for AIXI’s optimality are put forward, we have to regard AIXI as a relative theory of intelligence, dependent on the choice of the prior."
    }, {
      "heading" : "5.6.2 Natural Universal Turing Machines",
      "text" : "The choice of the UTM has been a big open question in algorithmic information theory for a long time. The Kolmogorov complexity of a string depends on this choice. However, there are invariance theorems (Li and Vitányi, 2008, Thm. 2.1.1 & Thm. 3.1.1) which state that changing the UTM changes Kolmogorov complexity only by a constant. When using the Solomonoff prior M to predict any deterministic computable binary sequence, the number of wrong predictions is bounded by the Kolmogorov complexity of the sequence (Corollary 3.56). Due to the invariance theorem, changing the UTM changes the number of errors only by a constant. In this sense, compression and prediction work for any choice of UTM.\nFor AIXI, there can be no invariance theorem; in Section 5.2 we showed that a bad choice for the UTM can have drastic consequences. Our negative results can guide future search for a natural UTM: the UTMs used to define the indifference prior (Theorem 5.4), the dogmatic prior (Theorem 5.5), and the Gödel prior (Theorem 5.8) should be considered unnatural. But what are other desirable properties of a UTM?\nA remarkable but unsuccessful attempt to find natural UTMs is due to Müller (2010). It takes the probability that one universal machine simulates another according"
    }, {
      "heading" : "98 Optimality",
      "text" : "to the length of their respective compilers and searches for a stationary distribution. Unfortunately, no stationary distribution exists.\nAlternatively, we could demand that the UTM U ′ that we use for the universal prior has a small compiler on the reference machine U (Hutter, 2005, p. 35). Moreover, we could demand the reverse, that the reference machine U has a small compiler on U ′. The idea is that this should limit the amount of bias one can introduce by defining a UTM that has very small programs for very complicated and ‘unusual’ environments. Unfortunately, this just pushes the choice of the UTM to the reference machine. Table 5.2 lists compiler sizes of the UTMs constructed in this thesis."
    }, {
      "heading" : "5.6.3 Asymptotic Optimality",
      "text" : "A policy is asymptotically optimal if the agent learns to act optimally in any environment from the classM. We discussed two asymptotically optimal policies. BayesExp is weakly asymptotically optimal if the horizon grows sublinearly (Theorem 5.24) and Thompson sampling is asymptotically optimal in mean (Theorem 5.25). Both policies commit to exploration for several steps. As stated in Example 5.19:\nTo achieve asymptotic optimality, the agent needs to explore infinitely often for an entire effective horizon.\nThis is why weak asymptotic optimality is impossible if the horizon grows linearly (Theorem 5.21): if the agent explores for an entire effective horizon, it spoils a significant fraction of the average. Thompson sampling explores whenever it draws a bad sample. BayesExp explores if the maximal expected information gain is above some threshold. Both policies commit to exploration for the entire effective horizon.\nThe exploration performed by Thompson sampling is qualitatively different from the exploration by BayesExp (Lattimore, 2013, Ch. 5). BayesExp performs phases of exploration in which it maximizes the expected information gain. This explores the environment class completely, even achieving off-policy prediction (Orseau et al., 2013, Thm. 7). In contrast, Thompson sampling only explores on the optimal policies, and in some environment classes this will not yield off-policy prediction. So in this sense the\nexploration mechanism of Thompson sampling is more reward-oriented than maximizing information gain.\nHowever, asymptotic optimality has to be taken with a grain of salt. It provides no incentive to the agent to avoid traps in the environment. Once the agent gets caught in a trap, all actions are equally bad and thus optimal: asymptotic optimality has been achieved. Even worse, an asymptotically optimal agent has to explore all the traps because they might contain hidden treasure. This brings us to the following impossibility result for non-recoverable environment classes.\nEither the agent gets caught in a trap or it is not asymptotically optimal.1"
    }, {
      "heading" : "5.6.4 The Quest for Optimality",
      "text" : "Theorem 5.3 shows that Pareto optimality is trivial in the class of all computable environments. Bayes optimality, Balanced Pareto optimality, and maximal Legg-Hutter intelligence are equivalent (Proposition 5.12 and Proposition 5.10). Corollary 5.15 and Corollary 5.16 show that this notion is highly subjective because it depends on the choice of the prior. Moreover, according to Corollary 5.17, any computable policy is nearly balanced Pareto optimal. For finite horizons, there are priors such that every policy is balanced Pareto optimal (Theorem 5.4). Sublinear regret is impossible in general environments (Example 5.30). However, if the environment is recoverable (Definition 5.31), then Theorem 5.33 shows that asymptotic optimality in mean implies sublinear regret. In summary, asymptotic optimality is the only nontrivial and objective notion of optimality for the general reinforcement learning problem (Problem 4.2):\n1This formulation was suggested by Toby Ord.\n100 Optimality\nit is both satisfiable (Theorem 5.24 and Theorem 5.25) and objective because it does not depend on a prior probability measure over the environment class M. Table 5.3 summarized the notions of optimality discussed in this chapter.\nOur optimality notions are tail events: any finite number of time steps are irrelevant; the agent can be arbitrarily lazy. Asymptotic optimality requires only convergence in the limit. In recoverable environments we can always achieve sublinear regret after any finite interaction. All policies with finite horizon are Bayes optimal according to Theorem 5.4 and Corollary 5.6. Overall, there is a dichotomy between the asymptotic nature of our optimality notions and the use of discounting to prioritize the present over the future. Ideally, we would aim for finite guarantees instead, such as precise regret bounds or PAC convergence rates, but without additional assumptions this is impossible in this general setting. This leaves us with the main question of this chapter unanswered (Hutter, 2009b, Sec. 5):\nWhat is a good optimality criterion for general reinforcement learning?\nChapter 6\nComputability\nI simply keep a few spare halting oracles around. — Marcus Hutter\nGiven infinite computation power, many traditional AI problems become trivial: playing chess, go, or backgammon can be solved by exhaustive expansion of the game tree. Yet other problems seem difficult still; for example, predicting the stock market, driving a car, or babysitting your nephew. How can we solve these problems in theory? Solomonoff induction and AIXI are proposed answers to this question.\nBoth Solomonoff induction and AIXI are known to be incomputable. But not all incomputabilities are equal. The arithmetical hierarchy specifies different levels of computability based on oracle machines: each level in the arithmetical hierarchy is computed by a Turing machine which may query a halting oracle for the respective lower level. Our agents are useless if they cannot be approximated in practice, i.e., by a regular Turing machine. Therefore we posit that any ideal for a ‘perfect agent’ needs to be limit computable (∆02). The class of limit computable functions is the class of functions that admit an anytime algorithm.\nIn Section 6.2 we consider various different flavors of Solomonoff induction: Solomonoff’s prior M (Example 3.5) is only a semimeasure and not a measure: it assigns positive probability that the observed string has only finite length. This can be circumvented by normalizing M . Solomonoff’s normalization Mnorm (Definition 2.16) preserves the ratio M(x1)/M(x0) and is limit computable. If instead we mix only over programs that compute infinite strings, we get a semimeasure M (3.6), which can be normalized to Mnorm. Moreover, when predicting a sequence, we are primarily interested in the conditional probability M(xy | x) (respectively Mnorm(xy | y), M(xy | x), or Mnorm(xy | x)) that the currently observed string x is continued with y. We show that both M and Mnorm are limit computable, while M and Mnorm are not. Table 6.1 summarizes our computability results for Solomonoff induction.\nFor MDPs, planning is already P-complete for finite and infinite horizons (Papadimitriou and Tsitsiklis, 1987). In POMDPs, planning is undecidable (Madani et al., 1999, 2003). The existence of a policy whose expected value exceeds a given threshold is PSPACE-complete (Mundhenk et al., 2000), even for purely epistemic POMDPs in which actions do not change the hidden state (Sabbadin et al., 2007). In Section 6.3 we derive hardness results for planning in general semicomputable environments; this environment class is even more general than POMDPs. We show that optimal policies\n101\nare Π02-hard and ε-optimal policies are undecidable. Moreover, we show that by default, AIXI is not limit computable. When picking the next action, two or more actions might have the same value (expected future rewards). The choice between them is easy, but determining whether such a tie exists is difficult. This problem can be circumvented by settling for an ε-optimal policy; we get a limitcomputable agent with infinite horizon. However, these results rely on the recursive definition of the value function. In contrast, Hutter (2005) defines the value function as the limit of the iterative value function. In Section 6.4 we compare these two definitions and show that the recursive definition correctly maximizes expected rewards and has better computability properties.\nIn Section 6.5 we show that for finite horizons both the entropy-seeking and the information-seeking agent are ∆03-computable and have limit-computable ε-optimal policies. BayesExp (Section 4.3.3) relies on optimal policies that are generally not\n§6.1 Background on Computability 103\nlimit computable. In Section 6.6 we give a weakly asymptotically optimal agent based on BayesExp that is limit computable. Table 6.2 summarizes our results on the computability of these agents.\nIn this chapter we illustrate the environments used in the proofs of our theorems in the form of flowcharts. They should be read as follows. Circles denote stochastic nodes, rectangles denote environment nodes, and diamonds denote the agent’s choice nodes. Transitions out of stochastic nodes are labeled with transition probabilities, transitions out of environment nodes are labeled with percepts, and transitions out of choice nodes are labeled with actions. The initial node is marked with a small incoming arrow (see for example Figure 6.3). By Assumption 4.6b the worst possible outcome is getting reward 0 forever, thus we label such states as hell . Analogously, getting reward 1 forever is the best possible outcome, thus we label such states as heaven."
    }, {
      "heading" : "6.1 Background on Computability",
      "text" : ""
    }, {
      "heading" : "6.1.1 The Arithmetical Hierarchy",
      "text" : "A set A ⊆ N is Σ0n iff there is a quantifier-free formula η such that\nk ∈ A ⇐⇒ ∃k1∀k2 . . . Qnkn η(k, k1, . . . , kn) (6.1)\nwhere Qn = ∀ if n is even, Qn = ∃ if n is odd (Nies, 2009, Def. 1.4.10). (We can also think of η as a computable relation.) A set A ⊆ N is Π0n iff its complement N \\ A is Σ0n. The formula η on the right side of (6.1) is a Σ0n-formula and its negation is a Π0n-formula. It can be shown that we can add any bounded quantifiers and duplicate quantifiers of the same type without changing the classification of A. The set A is ∆0n iff A is Σ0n and A is Π0n. We get that Σ01 as the class of recursively enumerable sets, Π01 as the class of co-recursively enumerable sets and ∆01 as the class of recursive sets.\nThe set A ⊆ N is Σ0n-hard (Π0n-hard, ∆0n-hard) iff for any set B ∈ Σ0n (B ∈ Π0n, B ∈ ∆0n), B is many-one reducible to A, i.e., there is a computable function f such that k ∈ B ↔ f(k) ∈ A (Nies, 2009, Def. 1.2.1). We get Σ0n ⊂ ∆0n+1 ⊂ Σ0n+1 ⊂ . . . and Π0n ⊂ ∆0n+1 ⊂ Π0n+1 ⊂ . . .. This hierarchy of subsets of natural numbers is known as the arithmetical hierarchy .\nBy Post’s Theorem (Nies, 2009, Thm. 1.4.13), a set is Σ0n if and only if it is recursively enumerable on an oracle machine with an oracle for a Σ0n−1-complete set. An oracle for Σ01 is called a halting oracle."
    }, {
      "heading" : "6.1.2 Computability of Real-valued Functions",
      "text" : "We fix some encoding of rational numbers into binary strings and an encoding of binary strings into natural numbers. From now on, this encoding will be done implicitly wherever necessary.\nDefinition 6.1 (Σ0n-, Π0n-, ∆0n-computable). A function f : X ∗ → R is called Σ0ncomputable (Π0n-computable, ∆0n-computable) iff the set {(x, q) ∈ X ∗ ×Q | f(x) > q} is Σ0n (Π0n, ∆0n).\nA ∆01-computable function is called computable, a Σ01-computable function is called lower semicomputable, and a Π01-computable function is called upper semicomputable. A ∆02-computable function f is called limit computable, because there is a computable function φ such that\nlim k→∞ φ(x, k) = f(x).\nThe program φ that limit computes f can be thought of as an anytime algorithm for f : we can stop φ at any time k and get a preliminary answer. If the program φ ran long enough (which we do not know), this preliminary answer will be close to the correct one.\nLimit-computable sets are the highest level in the arithmetical hierarchy that can be approached by a regular Turing machine. Above limit-computable sets we necessarily need some form of halting oracle. See Table 6.3 for the definition of lower/upper semicomputable and limit-computable functions in terms of the arithmetical hierarchy.\nLemma 6.2 (Computability of Arithmetical Operations). Let n > 0 and let f, g : X ∗ → R be two ∆0n-computable functions. Then\n(a) {(x, y) | f(x) > g(y)} is Σ0n,\n(b) {(x, y) | f(x) ≤ g(y)} is Π0n,\n(c) f + g, f − g, and f · g are ∆0n-computable, and\n(d) f/g is ∆0n-computable if g(x) 6= 0 for all x.\n(e) log f is ∆0n-computable if f(x) > 0 for all x.\nProof. We only prove this for n > 1. Since f, g are ∆0n-computable, they are limit computable on a level n−1 oracle machine. Let φ be the function limit computing f on the oracle machine, and let ψ be the function limit computing g on the oracle machine:\nf(x) = lim k→∞ φ(k, x) and g(y) = lim k→∞ ψ(k, y).\nBy assumption, both φ and ψ are ∆0n−1-computable.\n§6.2 The Complexity of Solomonoff Induction 105\n(a) Let G := {(x, y, q) | g(y) < q}, and let F := {(x, y, q) | q < f(x)}, both of which are in ∆0n by assumption. Hence there are Σ0n-formulas ϕG and ϕF such that\n(x, y, q) ∈ G ⇐⇒ ϕG(x, y, q) (x, y, q) ∈ F ⇐⇒ ϕF (x, y, q)\nNow f(x) > g(y) if and only if ∃q. (x, y, q) ∈ G ∩ F , which is equivalent to the Σ0n-forumla\n∃q. ϕG(x, y, q) ∧ ϕF (x, y, q).\n(b) Follows from (a).\n(c) Addition, subtraction, and multiplication are continuous operations.\n(d) Division is discontinuous only at g(x) = 0. We show this explicitly. By assumption, for any ε > 0 there is a k0 such that for all k > k0\n|φ(x, k)− f(x)| < ε and |ψ(x, k)− g(x)| < ε.\nWe assume without loss of generality that ε < |g(x)|, since g(x) 6= 0 by assumption.∣∣∣∣φ(x, k)ψ(x, k) − f(x)g(x) ∣∣∣∣\n= ∣∣∣∣φ(x, k)g(x)− f(x)ψ(x, k)ψ(x, k)g(x) ∣∣∣∣ ≤ |φ(x, k)g(x)− f(x)g(x)|+ |f(x)g(x)− f(x)ψ(x, k)| |ψ(x, k)g(x)| < ε|g(x)|+ |f(x)|ε |ψ(x, k)g(x)|\nwith |ψ(x, k)g(x)| = |ψ(x, k)| · |g(x)| > (|g(x)| − ε)|g(x)|,\n< ε · |g(x)|+ |f(x)| (|g(x)| − ε)|g(x)| ε→0−−−→ 0,\ntherefore f(x)/g(x) = limk→∞ φ(x, k)/ψ(x, k).\n(e) Follows from the fact that the logarithm is computable."
    }, {
      "heading" : "6.2 The Complexity of Solomonoff Induction",
      "text" : "In this section, we derive the computability results for Solomonoff’s prior as stated in Table 6.1.\nSince M is lower semicomputable, Mnorm is limit computable by Lemma 6.2 (c) and (d). When using the Solomonoff prior M (or one of its sisters Mnorm, M , or Mnorm defined in Definition 2.16 and Equation 3.6) for sequence prediction, we need\nto compute the conditional probability M(xy | x) = M(xy)/M(x) for finite strings x, y ∈ X ∗. Because M(x) > 0 for all finite strings x ∈ X ∗, this quotient is well-defined.\nTheorem 6.3 (Complexity of M , Mnorm, M , and Mnorm).\n(a) M(x) is lower semicomputable\n(b) M(xy | x) is limit computable\n(c) Mnorm(x) is limit computable\n(d) Mnorm(xy | x) is limit computable\n(e) M(x) is Π02-computable\n(f) M(xy | x) is ∆03-computable\n(g) Mnorm(x) is ∆03-computable\n(h) Mnorm(xy | x) is ∆03-computable\nProof. (a) By Li and Vitányi (2008, Thm. 4.5.2). Intuitively, we can run all programs in parallel and get monotonely increasing lower bounds for M(x) by adding 2−|p|\nevery time a program p has completed outputting x.\n(b) From (a) and Lemma 6.2d since M(x) > 0 (see also Figure 6.1).\n(c) By Lemma 6.2cd, and M(x) > 0.\n(d) By (iii), Lemma 6.2d since Mnorm(x) ≥M(x) > 0.\n(e) Let φ be a computable function that lower semicomputesM . SinceM is a semimeasure, M(xy) ≥ ∑ zM(xyz), hence ∑ y∈XnM(xy) is nonincreasing in n and thus\nM(x) > q iff ∀n∃k ∑\ny∈Xn φ(xy, k) > q.\n(f) From (v) and Lemma 6.2d since M(x) > 0.\n(g) From (v) and Lemma 6.2d.\n(h) From (vi) and Lemma 6.2d since Mnorm(x) ≥M(x) > 0.\n§6.2 The Complexity of Solomonoff Induction 107\nWe proceed to show that these bounds are in fact the best possible ones. If M were ∆01-computable, then so would be the conditional semimeasureM( · | · ). Thus the M -adversarial sequence z1:∞ defined in Example 3.42 would be computable and hence corresponds to a computable deterministic measure µ. However, we haveM(z1:t) ≤ 2−t by construction, so dominance M(x) ≥ w(µ)µ(x) with w(µ) > 0 yields a contradiction with t→∞:\n2−t ≥M(z1:t) ≥ w(µ)µ(z1:t) = w(µ) > 0\nBy the same argument, the normalized Solomonoff prior Mnorm cannot be ∆01-computable. However, since it is a measure, Σ01- or Π01-computability would entail ∆01- computability.\nFor M and Mnorm we prove the following two lower bounds for specific universal Turing machines.\nTheorem 6.4 (M is not Limit Computable). There is a universal Turing machine U ′ such that the set {(x, q) |MU ′(x) > q} is not in ∆02.\nProof. Assume the contrary and let A ∈ Π02 \\∆02 and η be a quantifier-free first-order formula such that\nn ∈ A ⇐⇒ ∀k∃i. η(n, k, i). (6.2)\nFor each n ∈ N, we define the program pn as follows.\n1: procedure pn 2: output 1n+10 3: k ← 0 4: while true do 5: i← 0 6: while not η(n, k, i) do 7: i← i+ 1 8: k ← k + 1 9: output 0\nEach program pn always outputs 1n+10. Furthermore, the program pn outputs the infinite string 1n+10∞ if and only if n ∈ A by (6.2). We define U ′ as follows using our reference machine U .\n• U ′(1n+10): Run pn.\n• U ′(00p): Run U(p).\n• U ′(01p): Run U(p) and bitwise invert its output.\nBy construction, U ′ is a universal Turing machine. No pn outputs a string starting with 0n+11, therefore MU ′(0n+11) = 14 ( MU (0 n+11) +MU (1 n+10) ) . Hence\nMU ′(1 n+10) = 2−n−21A(n) + 1 4MU (1 n+10) + 14MU (0 n+11)\n= 2−n−21A(n) +MU ′(0 n+11)\n108 Computability\nIf n /∈ A, then MU ′(1n+10) = MU ′(0n+11). Otherwise, we have |MU ′(1n+10) − MU ′(0\nn+11)| = 2−n−2. Now we assume that MU ′ is limit computable, i.e., there is a computable function\nφ : X ∗ × N→ Q such that limk→∞ φ(x, k) = MU ′(x). We get that\nn ∈ A ⇐⇒ lim k→∞\nφ(0n+11, k)− φ(1n+10, k) ≥ 2−n−2,\nthus A is limit computable, a contradiction.\nCorollary 6.5 (Mnorm is not Σ02- or Π02-computable). There is a universal Turing machine U ′ such that {(x, q) |MnormU ′(x) > q} is not in Σ02 or Π02.\nProof. Since Mnorm = c ·M , there exists a k ∈ N such that 2−k < c (even if we do not know the value of k). We can show that the set {(x, q) |MnormU ′(x) > q} is not in ∆02 analogously to the proof of Theorem 6.4, using\nn ∈ A ⇐⇒ lim k→∞\nφ(0n+11, k)− φ(1n+10, k) ≥ 2−k−n−2.\nIf Mnorm were Σ02-computable or Π02-computable, this would imply that Mnorm is ∆02- computable since Mnorm is a measure, a contradiction.\nSinceM( ) = 1, we haveM(x | ) = M(x), so the conditional probabilityM(xy | x) has at least the same complexity as M . Analogously for Mnorm and Mnorm since they are measures. For M , we have that M(x | ) = Mnorm(x), so Corollary 6.5 applies. All that remains to prove is that conditional M is not lower semicomputable.\nTheorem 6.6 (Conditional M is not Lower Semicomputable). The set {(x, xy, q) | M(xy | x) > q} is not recursively enumerable.\nWe gave a different, more complicated proof in Leike and Hutter (2015b). The following, much simpler and more elegant proof is due to Sterkenburg (2016, Prop. 3).\nProof. Assume to the contrary thatM(xy | x) is lower semicomputable. Let a 6= b ∈ X . We construct an infinite string x by defining its initial segments =: x(0) @ x(1) @ x(2) @ . . . @ x. At every step n, we enumerate strings y ∈ X ∗ until one is found satisfying M(a | x(n)y) ≥ 1/2; then set x(n + 1) := x(n)yb. This implies that for infinitely many t there is an n such that M(b | x<t) = M(b | x(n)y) ≤ 1 −M(a | x(n)y) ≤ 1/2. Since we assumed M( · | · ) to be lower semicomputable, the infinite string x is computable, and hence M(xt | x<t) → 1 by Corollary 3.55. But this contradicts M(b | x<t) ≤ 1/2 infinitely often."
    }, {
      "heading" : "6.3 The Complexity of AINU, AIMU, and AIXI",
      "text" : ""
    }, {
      "heading" : "6.3.1 Upper Bounds",
      "text" : "In this section, we derive upper bounds on the computability of AINU, AIMU, and AIXI. Except for Corollary 6.14, all results in this section apply generally to any ν ∈ MCCSLSC .\n§6.3 The Complexity of AINU, AIMU, and AIXI 109\nSince the Bayesian mixture ξ ∈MCCSLSC , they apply to AIXI even though they are stated for AINU.\nIn order to position AINU in the arithmetical hierarchy, we need to encode policies as sets of natural numbers. For the rest of this chapter, we assume that policies are deterministic, thus can be represented as relations over (A×E)∗×A. These relations are easily identified with sets of natural numbers by encoding the history into one natural number. From now on this translation of policies into sets of natural numbers will be done implicitly wherever necessary.\nLemma 6.7 (Policies are in ∆0n). If a policy π is Σ0n or Π0n, then π is ∆0n.\nProof. Let ϕ be a Σ0n-formula (Π0n-formula) defining π, i.e., ϕ(h, a) holds iff π(h) = a. We define the formula ϕ′,\nϕ′(h, a) := ∧\na′∈A\\{a}\n¬ϕ(h, a′).\nThe set of actions A is finite, hence ϕ′ is a Π0n-formula (Σ0n-formula). Moreover, ϕ′ is equivalent to ϕ.\nTo compute the optimal policy, we need to compute the optimal value function. The following lemma gives an upper bound on the computability of the value function for environments inMCCSLSC .\nLemma 6.8 (Complexity of V ∗ν ). For every ν ∈ MCCSLSC , and every lower semicomputable discount function γ, the function V ∗ν is ∆02-computable.\nProof. The explicit form of the value function (4.2) has numerator\nlim m→∞ max ∑ æt:m m∑ i=t γ(i)riν(e1:i ‖ a1:i),\nand denominator ν(e<t ‖ a<t) · Γt. The numerator is nondecreasing in m because we assumed rewards to be nonnegative (Assumption 4.6b). Hence both numerator and denominator are lower semicomputable functions, so Lemma 6.2d implies that V ∗ν is ∆02-computable.\nFrom the optimal value function V ∗ν we get the optimal policy π∗ν according to (4.4). However, in cases where there is more than one optimal action, we have to break an argmax tie. This happens iff V ∗ν (hα) = V ∗ν (hβ) for two potential actions α 6= β ∈ A. This equality test is more difficult than determining which is larger in cases where they are unequal. Thus we get the following upper bound.\nTheorem 6.9 (Complexity of Optimal Policies). For any environment ν, if V ∗ν is ∆0n-computable, then there is an optimal policy π∗ν for the environment ν that is ∆0n+1.\n110 Computability\nProof. To break potential ties, we pick an (arbitrary) total order on A that specifies which actions should be preferred in case of a tie. We define\nπν(h) = a :⇐⇒ ∧\na′:a′ a V ∗ν (ha) > V ∗ ν (ha ′)\n∧ ∧\na′:a a′ V ∗ν (ha) ≥ V ∗ν (ha′).\n(6.3)\nThen πν is a ν-optimal policy according to (4.4). By assumption, V ∗ν is ∆0n-computable. By Lemma 6.2ab V ∗ν (ha) > V ∗ν (ha′) is Σ0n and V ∗ν (ha) ≥ V ∗ν (ha′) is Π0n. Therefore the policy πν defined in (6.3) is a conjunction of a Σ0n-formula and a Π0n-formula and thus ∆0n+1.\nCorollary 6.10 (Complexity of AINU). AINU is ∆03 for every environment ν ∈MCCSLSC .\nProof. From Lemma 6.8 and Theorem 6.9.\nUsually we do not mind taking slightly suboptimal actions. Therefore actually trying to determine if two actions have the exact same value seems like a waste of resources. In the following, we consider policies that attain a value that is always within some ε > 0 of the optimal value.\nTheorem 6.11 (Complexity of ε-Optimal Policies). For any environment ν, if V ∗ν is ∆0n-computable, then there is an ε-optimal policy πεν for the environment ν that is ∆0n.\nProof. Let ε > 0 be given. Since the value function V ∗ν (h) is ∆0n-computable, the set Vε := {(ha, q) | |q − V ∗ν (ha)| < ε/2} is in ∆0n according to Definition 6.1. Hence we compute the values V ∗ν (ha′) until we get within ε/2 for every a′ ∈ A and then choose the action with the highest value so far. Formally, let be an arbitrary total order on A that specifies which actions should be preferred in case of a tie. Without loss of generality, we assume ε = 1/k, and define Q to be an ε/2-grid on [0, 1], i.e., Q := {0, 1/2k, 2/2k, . . . , 1}. We define\nπεν(h) = a :⇐⇒ ∃(qa′)a′∈A ∈ QA. ∧ a′∈A (ha′, qa′) ∈ Vε\n∧ ∧\na′:a′ a qa > qa′ ∧ ∧ a′:a a′ qa ≥ qa′\n∧ the tuple (qa′)a′∈A is minimal with respect to the lex. ordering on QA.\n(6.4)\nThis makes the choice of a unique. Moreover, QA is finite since A is finite, and hence (6.4) is a ∆0n-formula.\nCorollary 6.12 (Complexity of ε-Optimal AINU). For any environment ν ∈ MCCSLSC , there is an ε-optimal policy for AINU that is ∆02.\nProof. From Lemma 6.8 and Theorem 6.11.\n§6.3 The Complexity of AINU, AIMU, and AIXI 111\nCorollary 6.13 (Complexity of ε-Optimal AIXI). For any lower semicomputable prior there is an ε-optimal policy for AIXI that is ∆02.\nProof. From Corollary 6.12 since for any lower semicomputable prior, the corresponding Bayesian mixture ξ is inMCCSLSC .\nIf the environment ν ∈MCCMcomp is a measure, i.e., ν assigns zero probability to finite strings, then we get computable ε-optimal policies.\nCorollary 6.14 (Complexity of AIMU). If the environment µ ∈ MCCMcomp is a measure and the discount function γ is computable, then AIMU is limit computable (∆02), and ε-optimal AIMU is computable (∆01).\nProof. Let ε > 0 be the desired accuracy. We can truncate the limit m → ∞ in (4.2) at the ε/2-effective horizon Ht(ε/2), since everything after Ht(ε/2) can contribute at most ε/2 to the value function. Any lower semicomputable measure is computable (Li and Vitányi, 2008, Lem. 4.5.1). Therefore V ∗µ as given in (4.2) is composed only of computable functions, hence it is computable according to Lemma 6.2. The claim now follows from Theorem 6.9 and Theorem 6.11."
    }, {
      "heading" : "6.3.2 Lower Bounds",
      "text" : "We proceed to show that the bounds from the previous section are the best we can hope for. In environment classes where ties have to be broken, AINU has to solve Π02-hard problems (Theorem 6.16). These lower bounds are stated for particular environments ν ∈MCCSLSC . Throughout this section, we assume that Γt > 0 for all t.\nWe also construct universal mixtures that yield bounds on ε-optimal policies. There is an ε-optimal AIXI that solves Σ01-hard problems (Theorem 6.17). For arbitrary universal mixtures, we prove the following weaker statement that only guarantees incomputability.\nTheorem 6.15 (No AIXI is computable). AIXI is not computable for any universal Turing machine U .\n112 Computability\nThis theorem follows from the incomputability of Solomonoff induction. By the on-policy value convergence theorem (Corollary 4.20) AIXI succeeds to predict the environment’s behavior for its own policy. If AIXI were computable, then there would be computable environments more powerful than AIXI: they can simulate AIXI and anticipate its prediction, which leads to a contradiction.\nProof. Assume there is a computable policy π∗ξ that is optimal in the mixture ξ. We define a deterministic environment µ, the adversarial environment to π∗ξ . The environment µ gives rewards 0 as long as the agent follows the policy π∗ξ , and rewards 1 once the agent deviates. Formally, we ignore observations by setting O := {0}, and define\nµ(r1:t ‖ a1:t) :=  1 if ∀k ≤ t. ak = π∗ξ ((ar)<k) and rk = 0, 1 if ∀k ≤ t. rk = 1k≥i\nwhere i := min{j | aj 6= π∗ξ ((ar)<j)}, and 0 otherwise.\nSee Figure 6.2 for an illustration of this environment. The environment µ is computable because the policy π∗ξ was assumed to be computable. Suppose π ∗ ξ acts in µ, then by Theorem 4.19 AIXI learns to predict perfectly on policy :\nV π∗ξ ξ (æ<t)− V π∗ξ µ (æ<t)→ 0 as t→∞ µπ ∗ ξ -almost surely,\nsince both π∗ξ and µ are deterministic. Because V π∗ξ µ (h<t) = 0 by definition of µ, we get V ∗ξ (æ<t)→ 0. Therefore we find a t large enough such that V ∗ξ (æ<t) < w(µ) where æ<t is the interaction history of π∗ξ in µ. A policy π with π(æ<t) 6= π∗ξ (æ<t), gets a reward of 1 in environment µ for all time steps after t, hence V πµ (æ<t) = 1. With linearity of V πξ (æ<t) in ξ (Lemma 4.14),\nV πξ (æ<t) ≥ w(µ) µ(e1:t‖a1:t) ξ(e1:t‖a1:t)V π µ (æ<t) ≥ w(µ),\nsince µ(e1:t ‖ a1:t) = 1 (µ is deterministic), V πµ (æ<t) = 1, and ξ(e1:t ‖ a1:t) ≤ 1. Now we get a contradiction:\nw(µ) > V ∗ξ (æ<t) = sup π′ V π ′ ξ (æ<t) ≥ V πξ (æ<t) ≥ w(µ)\nFor the remainder of this section, we fix the action space to be A := {α, β} with action α favored in ties. The percept space is fixed to a tuple of binary observations and rewards, E := O × {0, 1} with O := {0, 1}.\nTheorem 6.16 (AINU is Π02-hard). There is an environment ν ∈ MCCSLSC such that AINU is Π02-hard.\nProof. Let A be a any Π02-set, and let η be a quantifier-free formula such that\nn ∈ A ⇐⇒ ∀i ∃k η(n, i, k). (6.5)\n§6.3 The Complexity of AINU, AIMU, and AIXI 113\nWe define a class of environmentsM := {ρ1, ρ2, . . .} where each ρi is defined as follows.\nρi((or)1:m ‖ a1:m) :=  2−m if o1:m = 1m and ∀t ≤ m. rt = 0, 2−n−1 if ∃n. 1n0 v o1:m v 1n0∞ and an+2 = α and rt = 1t>n+1 and ∃k η(n, i, k), 2−n−1 if ∃n. 1n0 v o1:m v 1n0∞ and an+2 = β\nand rt = 1t>n+1, and 0 otherwise.\nSee Figure 6.3 for an illustration of these environments. Every ρi is a chronological conditional semimeasure by definition and every ρi is lower semicomputable since η is quantifier-free, soM⊆MCCSLSC .\nWe define our environment ν as a mixture overM,\nν := ∑ i∈N 2−i−1ρi;\nthe choice of the weights on the environments ρi is arbitrary but positive. Let π∗ν be an optimal policy for the environment ν and recall that the action α is preferred in ties. We claim that for the ν-optimal policy π∗ν ,\nn ∈ A ⇐⇒ π∗ν(1n0) = α. (6.6)\nThis enables us to decide whether n ∈ A given the policy π∗ν , hence proving (6.6) concludes this proof.\nLet n, i ∈ N be given, and suppose we are in environment ρi and observe 1n0. Taking action β next yields reward 1 forever; taking action α next yields a reward of 1 if there\n114 Computability\nis a k such that η(n, i, k) holds. If this is the case, then\nV ∗ρi(1 n0α) = Γn+2 = V ∗ ρi(1 n0β),\nand otherwise V ∗ρi(1 n0α) = 0 < Γn+2 = V ∗ ρi(1 n0β)\n(omitting the first n + 1 actions and rewards in the argument of the value function). We can now show (6.6): By (6.5), n ∈ A if and only if for all i there is a k such that η(n, i, k), which happens if and only if V ∗ρi(1\nn0α) = Γn+2 for all i ∈ N, which is equivalent to V ∗ν (1n0α) = Γn+2, which in turn is equivalent to π∗µ(1n0) = α since V ∗ν (1 n0β) = Γn+2 and action α is favored in ties.\nTheorem 6.17 (Some ε-optimal AIXI are Σ01-hard). There is a universal Turing machine U ′ and an ε > 0 such that any ε-optimal policy for AIXI is Σ01-hard.\nProof. Let A be a Σ01-set and η be a quantifier-free formula such that n + 1 ∈ A iff ∃k η(n, k). We define the environment\nν((or)1:t ‖ a1:t) :=  ξ((or)1:n ‖ a1:n) if ∃n. o1:n = 1n−10 and an = α and ∀t′ > n. ot′ = 0 ∧ rt′ = 12 , ξ((or)1:n ‖ a1:n) if ∃n. o1:n = 1n−10 and an = β and ∀t′ > n. ot = 0 ∧ rt = 1 and ∃k η(n− 1, k),\nξ((or)1:t ‖ a1:t) if @n. o1:n = 1n−10, and 0 otherwise.\nSee Figure 6.4 for an illustration. The environment ν mimics the universal environment\n§6.4 Iterative Value Function 115\nξ until the observation history is 1n−10. Taking the action α next gives rewards 1/2 forever. Taking the action β next gives rewards 1 forever if n ∈ A, otherwise the environment ν ends at some future time step. Therefore we want to take action β if and only if n ∈ A. We have that ν ∈MCCSLSC since ξ ∈MCCSLSC and η is quantifier-free.\nWe define ξ′ := 12ν + 1 8ξ. By Lemma 4.24 ξ ′ is a universal lower semicomputable semimeasure. Let n ∈ A be given and let h ∈ (A×E)n be any history with observations o1:n = 1\nn−10. Since ν(1n−10 | a1:n) = ξ(1n−10 | a1:n) by definition, the posterior weights of ν and ξ in ξ′ are equal to the prior weights, analogously to the proof of Theorem 5.5. In the following, we use the linearity of V π∗ ξ′\nρ in ρ (Lemma 4.14), and the fact that values are bounded between 0 and 1 (Assumption 4.6b). If there is a k such that η(n − 1, k) holds,\nV ∗ξ′(hβ)− V ∗ξ′(hα) = 12V π∗ ξ′ ν (hβ)− 12V π∗ ξ′ ν (hα) + 1 8V π∗ ξ′ ξ (hβ)− 1 8V π∗ ξ′ ξ (hα)\n≥ 12 − 1 4 + 0− 1 8 = 1 8 ,\nand similarly if there is no k such that η(n− 1, k) holds, then\nV ∗ξ′(hα)− V ∗ξ′(hβ) = 12V π∗ ξ′ ν (hα)− 12V π∗ ξ′ ν (hβ) + 1 8V π∗ ξ′ ξ (hα)− 1 8V π∗ ξ′ ξ (hβ)\n≥ 14 − 0 + 0− 1 8 = 1 8 .\nIn both cases |V ∗ξ′(hβ) − V ∗ξ′(hα)| > 1/9. Hence we pick ε := 1/9 and get for every ε-optimal policy πεξ′ that π ε ξ′(h) = β if and only if n ∈ A.\nNote the differences between Theorem 6.15 and Theorem 6.17: the former talks about optimal policies and shows that they are not computable, but is agnostic towards the underlying universal Turing machine. The latter talks about ε-optimal policies and gives a stronger hardness result, at the cost of depending on one particular universal Turing machine."
    }, {
      "heading" : "6.4 Iterative Value Function",
      "text" : "Historically, AIXI’s value function has been defined slightly differently to Definition 4.10, using a limit extension of an iterative definition of the value function. This definition is the more straightforward to come up with in AI: it is the natural adaptation of (optimal) minimax search in zero-sum games to the (optimal) expectimax algorithm for stochastic environments. In this section we discuss the problems with this definition.\nTo avoid confusion with the recursive value function V πν , we denote the iterative value function with W πν .1\nDefinition 6.18 (Iterative Value Function; Hutter, 2005, Def. 5.30). The iterative\n1In Leike and Hutter (2015a) the use of the symbols V and W is reversed.\nvalue of a policy π in an environment ν given history æ<t is\nW πν (æ<t) := 1\nΓt lim m→∞ ∑ et:m ν(e1:m | e<t ‖ a1:m) m∑ k=t γ(k)rk\nif Γt > 0 and W πν (æ<t) := 0 if Γt = 0 where ai := π(e<i) for all i ≥ t. The optimal iterative value is defined as W ∗ν (h) := supπW πν (h).\nAnalogously to (4.2), we can write W ∗ν using the max-sum-operator:\nW ∗ν (æ<t) = 1\nΓt lim m→∞ max ∑ æt:m ν(e1:m | e<t ‖ a1:m) m∑ k=t γ(k)rk (6.7)\nWe use iterative AINU for the ν-optimal policy according to the iterative value function, and iterative AIXI for the ξ-optimal policy according to the iterative value function. Note that iterative AIMU coincides with AIMU since µ is a measure by convention.\nGenerally, our environment ν ∈ MCCSLSC is only a semimeasure and not a measure, i.e., there is a history æ<tat such that\n1 > ∑ et∈E ν(et | e<t ‖ a1:t).\nIn such cases, with positive probability the environment ν does not produce a new percept et. If this occurs, we shall use the informal interpretation that the environment ν ended, but our formal argument does not rely on this interpretation.\nThe following proposition shows that for a semimeasure ν ∈ MCCSLSC that is not a measure, iterative AINU does not maximize ν-expected rewards. Recall that γ(1) states the discount of the first reward. In the following, we assume without loss of generality that γ(1) > 0, i.e., we are not indifferent about the reward received in time step 1.\nProposition 6.19 (Iterative AINU is not a ν-Expected Reward Maximizer). For any ε > 0 there is an environment ν ∈ MCCSLSC that is not a measure and a policy π that receives a total of γ(1) rewards in ν, but iterative AINU receives only εγ(1) rewards in ν.\n§6.4 Iterative Value Function 117\nInformally, the environment ν is defined as follows. In the first time step, the agent chooses between the two actions α and β. Taking action α gives a reward of 1, and subsequently the environment ends. Action β gives a reward of ε, but the environment continues forever. There are no other rewards in this environment. See Figure 6.5. From the perspective of ν-expected reward maximization, it is better to take action α, however iterative AINU takes action β.\nProof of Proposition 6.19. Let ε > 0. We ignore observations and set E := {0, ε, 1}, A := {α, β}. The environment ν is formally defined by\nν(r1:t ‖ a1:t) :=  1 if a1 = α and r1 = 1 and t = 1 1 if a1 = β and r1 = ε and rk = 0 ∀1 < k ≤ t 0 otherwise.\nTaking action α first, we have ν(r1:t ‖ αa2:t) = 0 for t > 1 (the environment ν ends in time step 2 given history α). Hence we conclude\nV ∗ν (α) = 1\nΓt lim m→∞ ∑ r1:m ν(r1:m ‖ αa2:m) m∑ k=1 γ(k)rk = 0.\nTaking action β first we get\nV ∗ν (β) = 1\nΓt lim m→∞ ∑ r1:m ν(r1:m ‖ βa2:m) m∑ k=1 γ(k)rk = γ(1) Γ1 ε.\nSince γ(1) > 0 and ε > 0, we have V ∗ν (β) > V ∗ν (α), and thus iterative AINU will use a policy that plays action β first, receiving a total discounted reward of εγ(1). In contrast, any policy π that takes action α first receives a larger total discounted reward of γ(1).\nWhether it is reasonable to assume that our environment has a nonzero probability\n118 Computability\nof ending is a philosophical debate we do not want to engage in here; see Martin et al. (2016) for a discussion. Instead, we have a different motivation to use the recursive over the iterative value function: the latter has worse computability properties. Concretely, we show that ε-optimal iterative AIXI has to solve Π02-hard problems and that there is an environment ν ∈MCCSLSC such that iterative AINU has to solve Σ03-hard problems. In contrast, using the recursive value function, ε-optimal AIXI is ∆02 according to Corollary 6.12 and AINU is ∆03 according to Corollary 6.10.\nThe central difference between V πν and W πν is that for V πν all obtained rewards matter, but for W πν only the rewards in timelines that continue indefinitely. In this sense the value function W πν conditions on surviving forever. If the environment µ is a measure, then the history is infinite with probability one, and so V πν and W πν coincide. Hence this distinction is not relevant for AIMU, only for AINU and AIXI.\nLemma 6.20 (Complexity of W ∗ν ). For every ν ∈ MCCSLSC , the function W ∗ν is Π02computable.\nProof. Multiplying (6.7) with Γtν(e<t ‖ a<t) yields W ∗ν (æ<t) > q if and only if\nlim m→∞ max ∑ æt:m ν(e1:m ‖ a1:m) m∑ k=t γ(k)rk > q Γt ν(e<t ‖ a<t). (6.8)\nThe inequality’s right side is lower semicomputable, hence there is a computable function ψ such that ψ(`)↗ q Γt ν(e<t ‖ a<t) =: q′ as `→∞. (In contrast to the recursive value function, this quantity is not increasing in m.) For a fixed m, the left side is also lower semicomputable, therefore there is a computable function φ such that\nφ(m, k)↗ max ∑ æt:m ν(e1:m ‖ a1:m) m∑ k=t γ(k)rk =: f(m) as k →∞.\nWe already know that the limit of f(m) for m → ∞ exists (uniquely), hence we can write (6.8) as\nlim m→∞\nf(m) > q′\n⇐⇒ ∀m0 ∃m ≥ m0. f(m) > q′ ⇐⇒ ∀m0 ∃m ≥ m0 ∃k. φ(m, k) > q′\n⇐⇒ ∀`∀m0 ∃m ≥ m0 ∃k. φ(m, k) > ψ(`),\nwhich is a Π02-formula.\nNote that in the finite horizon case where m is fixed, the value function W ∗ν is ∆02- computable by Lemma 6.2d, since W ∗ν (æ<t) = f(m)/q′. In this case, we get the same computability results for iterative AINU as we did in Section 6.3.1.\nCorollary 6.21 (Complexity of Iterative AINU). For any environment ν ∈ MCCSLSC , iterative AINU is ∆04 and there is an ε-optimal iterative AINU that is ∆ 0 3.\n§6.4 Iterative Value Function 119\nProof. From Theorem 6.9, Theorem 6.11, and Lemma 6.20.\nWe proceed to show corresponding lower bounds as in Section 6.3.2. For the rest of this section we assume Γt > 0 for all t.\nTheorem 6.22 (Iterative AINU is Σ03-hard). There is an environment ν ∈MCCSLSC such that iterative AINU is Σ03-hard.\nProof. The proof is analogous to the proof of Theorem 6.16. Let A be any Σ03 set, then there is a quantifier-free formula η such that\nn ∈ A ⇐⇒ ∃i ∀t ∃k η(n, i, t, k).\nWe define the environments ρi similar to the proof of Theorem 6.16, except for two changes:\n• We replace ∃k η(n, i, k) with ∀t′ ≤ t ∃k η(n, i, t′, k).\n• We switch actions α and β: action β ‘checks’ the formula η and action α gives a sure reward of 0.\n120 Computability\nFormally,\nρi((or)1:t ‖ a1:t) :=  2−t if o1:t = 1t and ∀t′ ≤ t. rt′ = 0, 2−n−1 if ∃n. 1n0 v o1:t v 1n0∞ and an+2 = α and ∀t′ ≤ t. rt′ = 0, 2−n−1 if ∃n. 1n0 v o1:t v 1n0∞ and an+2 = β and ∀t′ ≤ t. rt′ = 1t′>n+1 and ∀t′ ≤ t ∃k η(n, i, t′, k), and\n0 otherwise.\nSee Figure 6.6 for an illustration of the environment ρi. Every ρi is a chronological conditional semimeasure by definition, so M := {ρ0, ρ1, . . .} ⊆ MCCSLSC . Furthermore, every ρi is lower semicomputable since η is quantifier-free.\nWe define our environment ν as a mixture overM,\nν := ∑ i∈N 2−i−1ρi;\nthe choice of the weights on the environments ρi is arbitrary but positive. We get for the ν-optimal policy π∗ν analogously to the proof of Theorem 6.16\nπ∗ν(1 n0) = β ⇐⇒ ∃i∀t′ ≤ t∃k η(n, i, t′, k) ⇐⇒ n ∈ A,\nsince action α is preferred in ties.\nAnalogously to Theorem 6.15, we can show that iterative AIXI is not computable. We also get the following lower bound.\nTheorem 6.23 (Some ε-optimal iterative AIXI are Π02-hard). There is a universal mixture ξ′ and an ε > 0 such that any policy that is ε-optimal according to the iterative value for environment ξ′ is Π02-hard.\nProof. Let A be a Π02-set and η a quantifier-free formula such that\nn ∈ A ⇐⇒ ∀t ∃k η(n, t, k).\nWe proceed analogous to the proof of Theorem 6.17 except that we choose ∀t′ ≤ t∃k η(n, t, k) as a condition for reward 1 after playing action β.\n§6.4 Iterative Value Function 121\nDefine the environment\nν((or)1:t ‖ a1:t) :=  ξ((or)1:n+1 ‖ a1:n+1) if ∃n. 1n0 v o1:t v 1n0∞ and an+1 = α and ∀n+ 1 < k ≤ t. rk = 1/2, ξ((or)1:n+1 ‖ a1:n+1) if ∃n. 1n0 v o1:t v 1n0∞ and an+1 = β and ∀n+ 1 < k ≤ t. rk = 1 and ∀t′ ≤ t∃k η(n, t, k),\nξ((or)1:t ‖ a1:t) if @n. 1n0 v o1:t v 1n0∞, and 0 otherwise.\nSee Figure 6.7 for an illustration of the environment ν. The environment ν mimics the universal environment ξ until the observation history is 1n0. The next action α always gives rewards 1/2 forever, while action β gives rewards 1 forever iff n ∈ A. We have that ν is a lower semicomputable semimeasure since ξ is a lower semicomputable semimeasure and η is quantifier-free. We define ξ′ = 12ν+ 1 8ξ. By Lemma 4.24, ξ\n′ is a universal lower semicomputable semimeasure. Let n ∈ A be given and let h ∈ (A × O)x+1 be any history with observations o1:n+1 = 1n0. In the following, we use the linearity of W ∗ρ in ρ (analogously to Lemma 4.14). If ∀t∃k η(n, t, k), then\nW ∗ξ′(hβ)−W ∗ξ′(hα) = 12W ∗ ν (hβ)− 12W ∗ ν (hα) + 1 8W ∗ ξ (hβ)− 18W ∗ ξ (hα)\n≥ 12 − 1 4 + 0− 1 8 = 1 8 ,\n122 Computability\nand similarly if ¬∀t∃k η(n, t, k), then\nW ∗ξ′(hα)−W ∗ξ′(hβ) = 12W ∗ ν (hα)− 12W ∗ ν (hβ) + 1 8W ∗ ξ (hα)− 18W ∗ ξ (hβ)\n≥ 14 − 0 + 0− 1 8 = 1 8 .\nIn both cases |W ∗ξ′(hβ)−W ∗ξ′(hα)| > 1/9, hence with ε := 1/9 we have for an ε-optimal policy πεξ′ that π ε ξ′(h) = β if and only if n ∈ A."
    }, {
      "heading" : "6.5 The Complexity of Knowledge-Seeking",
      "text" : "Recall the definition of the optimal entropy-seeking value V ∗,mEnt and the optimal information-seeking value V ∗,mIG from Section 4.3.2. Using the results from Section 6.3 we can show that ε-optimal knowledge-seeking agents are limit computable, and optimal knowledge-seeking agents are ∆03.\nCorollary 6.24 (Computability of Knowledge-Seeking Values). For fixed m, the value functions V ∗,mEnt and V ∗,m IG are limit computable.\nProof. This follows from Lemma 6.2 (c-e) since ξ, ν, and w are lower semicomputable.\nCorollary 6.25 (Computability of Knowledge-Seeking Policies). For entropy-seeking and information-seeking agents there are limit-computable ε-optimal policies and ∆03- computable optimal policies.\nProof. Follows from Corollary 6.24, Theorem 6.9, and Theorem 6.11.\nNote that if we used an infinite horizon with discounting in Definition 4.25 or Definition 4.26, then we cannot retain this computability result without further assumptions: we would need that the value functions increase monotonically as m → ∞, as they do for the recursive value function from Definition 4.10. However, entropy is not a monotone function and may decrease if there are events whose probability converges to something > 1/2. For the entropy-seeking value function this happens for histories drawn from a deterministic environment µ since ξ → µ, so the conditionals converge to 1. Similarly, for the information-seeking value function, the posterior belief in one (deterministic) environment might become larger than 1/2 (depending on the prior and the environment class). Therefore we generally only get that discounted versions of V ∗Ent and V ∗IG are ∆ 0 3 analogously to Lemma 6.20. Hence optimal discounted entropy-seeking and optimal discounted information-seeking policies are in ∆04 by Theorem 6.9 and their corresponding ε-optimal siblings are ∆03 by Theorem 6.11."
    }, {
      "heading" : "6.6 A Limit Computable Weakly Asymptotically Optimal Agent",
      "text" : "According to Theorem 6.16, optimal reward-seeking policies are generally Π02-hard, and for optimal knowledge-seeking policies Corollary 6.25 shows that they are ∆03. Therefore\n§6.7 Discussion 123\nwe get that BayesExp is ∆03:\nCorollary 6.26 (BayesExp is ∆03). For any universal mixture ξ, BayesExp is ∆03.\nProof. From Corollary 6.10, Corollary 6.24, and Corollary 6.25.\nHowever, we do not know BayesExp to be limit computable, and we expect it not to be. However, we can approximate it using ε-optimal policies preserving weak asymptotic optimality.\nTheorem 6.27 (A Limit-Computable Weakly Asymptotically Optimal Agent). If there is a nonincreasing computable sequence of positive reals (εt)t∈N such that εt → 0 and Ht(εt)/(tεt) → 0 as t → ∞, then there is a limit-computable policy that is weakly asymptotically optimal in the class of all computable stochastic environments.\nProof. By Corollary 6.10, there is a limit-computable 2−t-optimal reward-seeking policy πtξ for the universal mixture ξ. By Corollary 6.25 there are limit-computable t/2optimal information-seeking policies πtIG with horizon t + Ht(εt). We define a policy π analogously to Algorithm 1 with πtIG and π t ξ instead of the optimal policies. From Corollary 6.24 we get that V ∗IG is limit computable, so the policy π is limit computable. Furthermore, πtξ is 2 −t-optimal and 2−t → 0, so V πtξ ξ (æ<t)→ V ∗ ξ (æ<t) as t→∞.\nNow we can proceed analogously to the proof of Lattimore (2013, Thm. 5.6), which consists of three parts. First, it is shown that the value of the ξ-optimal rewardseeking policy π∗ξ converges to the optimal value for exploitation time steps (line 6 in Algorithm 1) in the sense that V π∗ξ µ → V ∗µ . This carries over to the 2−t-optimal policy πtξ, since the key property is that on exploitation steps, V ∗\nIG < εt; i.e., π only exploits if potential knowledge-seeking value is low. In short, we get for exploitation steps\nV πtξ ξ (æ<t)→ V π∗ξ ξ (æ<t)→ V π∗ξ µ (æ<t)→ V ∗µ (æ<t) as t→∞.\nSecond, it is shown that the density of exploration steps vanishes. This result carries over since the condition V ∗IG(æ<t) > εt that determines exploration steps is exactly the same as for BayesExp and πtIG is εt/2-optimal.\nThird, the results of part one and two are used to conclude that π is weakly asymptotically optimal. This part carries over to our proof."
    }, {
      "heading" : "6.7 Discussion",
      "text" : "When using Solomonoff’s prior for induction, we need to evaluate conditional probabilities. We showed that conditional M and Mnorm are limit computable (Theorem 6.3), and that M and Mnorm are not limit computable (Theorem 6.4 and Corollary 6.5). Table 6.1 on page 102 summarizes our computability results on various versions of Solomonoff’s prior. Theses results implies that we can approximate M or Mnorm for prediction, but not the measure mixture M or Mnorm.\nIn some cases, normalized priors have advantages. As illustrated in Example 4.27, unnormalized priors can make the entropy-seeking agent mistake the entropy gained\n124 Computability\nfrom the probability assigned to finite strings for knowledge. From Mnorm ≥M we get that Mnorm predicts just as well as M , and by Theorem 6.3 we can use Mnorm without losing limit computability.\nTable 6.2 on page 102 summarizes our computability results for the agents AINU, AIXI, and AINU. AINU is ∆03 and restricting to ε-optimal policies decreases the level by one (Corollary 6.10 and Corollary 6.12). For environments from MCCMcomp, AIMU is limit-computable and ε-optimal AIMU is computable (Corollary 6.14). In Section 6.3.2 we proved that these computability bounds on AINU are generally unimprovable (Theorem 6.16 and Theorem 6.17). Additionally, we proved weaker lower bounds for AIXI independent of the universal Turing machine (Theorem 6.15) and for ε-optimal AIXI for specific choices of the universal Turing machine (Theorem 6.17).\nWhen the environment ν has nonzero probability of not producing a new percept, the iterative definition of AINU (Definition 6.18) originally given by Hutter (2005, Def. 5.30) fails to maximize ν-expected rewards (Proposition 6.19). Moreover, the policies are one level higher in the arithmetical hierarchy (see Table 6.4 on page 116). We proved upper (Corollary 6.21) and lower bounds (Theorem 6.22 and Theorem 6.23). The difference between the recursive value function V and the iterative value function W is readily exposed in the difference between the universal prior M and the measure mixture M : Just like W conditions on surviving forever, so does M eliminate the weight of programs that do not produce infinite strings. Both M and W are not limit computable for this reason.\nWe considered ε-optimality to avoid having to determine argmax ties. This ε does not have to be constant over time, we may let ε → 0 as t → ∞ at any computable rate. With this we retain the computability results of ε-optimal policies and get that the value of the ε(t)-optimal policy πε(t)ν converges rapidly to the ν-optimal value:\nV ∗ν (æ<t)− V π ε(t) ν ν (æ<t)→ 0 as t→∞.\nIn Section 4.1 we defined the set MCCSLSC as the set of all lower semicomputable chronological contextual semimeasure over percepts with actions povided as side-information. When determining the probability of the next percept et in an environment ν, we have to compute ν(e1:t | e<t ‖ a1:t). Alternatively, we could have defined the environment as a lower semicomputable mapping from histories æ<tat to probabilities over the next percept et (this is done in Chapter 7). For the proof of Lemma 6.8 and Lemma 6.20 we only need that ν(e1:t ‖ a1:t) is lower semicomputable computable. While this new definition makes no difference for the computability of AINU, it matters for AIXI because in the mixture ξ over all of these environments is no longer lower semicomputable.\nAny method that tries to tackle the reinforcement learning problem has to balance between exploration and exploitation. AIXI strikes this balance in the Bayesian way. However, as we showed in Section 5.2, this may not lead to enough exploration. To counteract this, we can add an explorative component to the agent, akin to knowledgeseeking agents. In Section 6.5 we show that ε-optimal knowledge-seeking agents are limit computable if we use the recursive definition of the value function.\n§6.7 Discussion 125\nWe set out with the goal of finding a perfect reinforcement learning agent that is limit computable. The Bayesian agent AIXI could be considered one suitable candidate, despite its optimality problems discussed in Chapter 5. Another suitable candidate are weakly asymptotically optimal agents, which in contrast to AIXI are optimal in an objective sense (see Section 5.6). We discussed BayesExp, which relies on a Solomonoff prior to learn its environment and on a information-seeking component for extra exploration. Our results culminated in a limit-computable weakly asymptotically optimal agent based on BayesExp (Theorem 6.27). In this sense our goal has been achieved.\n126 Computability\nChapter 7\nThe Grain of Truth Problem1\nAIs become friendly by playing lots of Newcomblike problems. — Eliezer Yudkowsky\nConsider the general setup of multiple reinforcement learning agents interacting sequentially in a known environment with the goal to maximize discounted reward.2 Each agent knows how the environment behaves, but does not know the other agents’ behavior. The natural (Bayesian) approach would be to define a class of possible policies that the other agents could adopt and take a prior over this class. During the interaction, this prior gets updated to the posterior as our agent learns the others’ behavior. Our agent then acts optimally with respect to this posterior belief. Kalai and Lehrer (1993) show that in infinitely repeated games Bayesian agents converge to an ε-Nash equilibrium as long as each agent assigns positive prior probability to the other agents’ policies (a grain of truth).\nAs an example, consider an infinitely repeated prisoners dilemma between two agents. In every time step the payoff matrix is as follows, where C means cooperate and D means defect.\nC D C 3/4, 3/4 0, 1 D 1, 0 1/4, 1/4\nDefine the set of policies Π := {π∞, π0, π1, . . .} where policy πt cooperates until time step t or the opponent defects (whatever happens first) and defects thereafter. The Bayes optimal behavior is to cooperate until the posterior belief that the other agent defects in the time step after the next is greater than some constant (depending on the discount function) and then defect afterwards. Therefore Bayes optimal behavior leads to a policy from the set Π (regardless of the prior). If both agents are Bayes optimal with respect to some prior, they both have a grain of truth and therefore they converge to a Nash equilibrium: either they both cooperate forever or after some finite time they both defect forever. Alternating strategies like TitForTat (cooperate first, then play\n1The idea for reflective oracles was developed by Jessica Taylor and Benya Fallenstein based on ideas by Paul Christiano (Christiano et al., 2013). Reflective oracles were first described in Fallenstein et al. (2015a). The proof of Theorem 7.7 was sketched by Benya Fallenstein and developed by me. Except for minor editing, everything else in this chapter is my own work.\n2We mostly use the terminology of reinforcement learning. For readers from game theory we provide a dictionary in Table 7.1.\n127\nthe opponent’s last action) are not part of the policy class Π, and adding them to the class breaks the grain of truth property: the Bayes optimal behavior is no longer in the class. This is rather typical; a Bayesian agent usually needs to be more powerful than its environment (see Section 6.3). We are facing the following problem.\nProblem 7.1 (Grain of Truth Problem; Hutter, 2009b, Q. 5j). Find a large class of policies Π containing Bayesian agents with positive prior over Π.\nUntil now, classes that admit a grain of truth were known only for small toy examples such as the iterated prisoner’s dilemma above (Shoham and Leyton-Brown, 2009, Ch. 7.3). Foster and Young (2001) and Nachbar (1997, 2005) prove several impossibility results on the grain of truth problem that identify properties that cannot be simultaneously satisfied for classes that allow a grain of truth (see Section 7.6 for a discussion).\nIn this chapter we present a formal solution to the grain of truth problem (Section 7.2). We assume that our multi-agent environment is computable, but it does not need to be stationary/Markov, ergodic, or finite-state. Our class of policies Π is large enough to contain all computable (stochastic) policies, as well as all relevant Bayes optimal policies. At the same time, our class is small enough to be limit computable. This is important because it allows our result to be computationally approximated.\nIn Section 7.3 we consider the setting where the multi-agent environment is unknown to the agents and has to be learned in addition to the other agents’ behavior. A Bayes optimal agent may not learn to act optimally in unknown multi-agent environments even though it has a grain of truth. This effect occurs in non-recoverable environments where taking one wrong action can mean a permanent loss of future value. In this case, a Bayes optimal agent avoids taking these dangerous actions and therefore will not explore enough to wash out the prior’s bias (using the dogmatic prior from Section 5.2.2). Therefore, Bayesian agents are not asymptotically optimal , i.e., they do not always learn to act optimally (Theorem 5.22).\nHowever, asymptotic optimality is achieved by Thompson sampling because the inherent randomness of Thompson sampling leads to enough exploration to learn the entire environment class (see Section 5.4.3). This leads to our main result: if all agents use Thompson sampling over our class of multi-agent environments, then for every ε > 0\n§7.1 Reflective Oracles 129\nthey converge to an ε-Nash equilibrium. This is not the first time Thompson sampling is used in game theory (Ortega and Braun, 2014), but the first time to show that it achieves such general positive results.\nThe central idea to our construction is based on reflective oracles introduced by Fallenstein et al. (2015a,b). Reflective oracles are probabilistic oracles similar to halting oracles that answer whether the probability that a given probabilistic Turing machine T outputs 1 is higher than a given rational number p. The oracles are reflective in the sense that the machine T may itself query the oracle, so the oracle has to answer queries about itself. This invites issues caused by self-referential liar paradoxes of the form “if the oracle says that this machine return 1 with probability > 1/2, then return 0, else return 1.” Reflective oracles avoid these issues by being allowed to randomize if the machines do not halt or the rational number is exactly the probability to output 1. We introduce reflective oracles formally in Section 7.1 and prove that there is a limit computable reflective oracle.\nFor infinitely repeated games practical algorithms rely on ficticious play (Fudenberg and Levine, 1998, Ch. 2): the agent takes a best-response action based on the assumption that its opponent is playing a stationary but unknown mixed strategy estimated according to the observed empirical frequencies. If all agents converge to a stationary policy, then this is a Nash equilibrium. However, convergence is not guaranteed.\nThe same problem occurs in multi-agent reinforcement learning (Busoniu et al., 2008). Reinforcement learning algorithms typically assume a (stationary) Markov decision process. This assumption is violated when interacting with other reinforcement learning agents because as these agents learn, their behavior changes and thus they are not stationary. Assuming convergence to a stationary policy is a necessary criterion to enable all agents to learn, but the process is unstable for many reinforcement learning algorithms and only empirical positive results are known (Bowling and Veloso, 2001)."
    }, {
      "heading" : "7.1 Reflective Oracles",
      "text" : ""
    }, {
      "heading" : "7.1.1 Definition",
      "text" : "First we connect semimeasures as defined in Definition 2.14 to Turing machines. In Chapter 2 we used monotone Turing machines which naturally correspond to lower semicomputable semimeasures (Li and Vitányi, 2008, Sec. 4.5.2) that describe the distribution that arises when piping fair coin flips into the monotone machine. Here we take a different route.\nA probabilistic Turing machine is a Turing machine that has access to an unlimited number of uniformly random coin flips. Let T denote the set of all probabilistic Turing machines that take some input in X ∗ and may query an oracle (formally defined below). We take a Turing machine T ∈ T to correspond to a semimeasure λT where λT (a | x) is the probability that T outputs a ∈ X when given x ∈ X ∗ as input. The value of\n130 The Grain of Truth Problem\nλT (x) is then given by the chain rule\nλT (x) := |x|∏ k=1 λT (xk | x<k). (7.1)\nThus T gives rise to the set of semimeasuresMLSC where the conditionals λ(a | x) are lower semicomputable. In contrast, in Chapter 6 we considered semimeasures whose joint probability (7.1) is lower semicomputable. This set MLSC contains all computable measures. However, MLSC is a proper subset of the set of all lower semicomputable semimeasures because the product (7.1) is lower semicomputable, but there are some lower semicomputable semimeasures whose conditional is not lower semicomputable (Theorem 6.6):\nMCCMcomp ⊂MLSC ⊂MCCSLSC\nIn the following we assume that our alphabet is binary, i.e., X := {0, 1}.\nDefinition 7.2 (Oracle). An oracle is a function O : T × {0, 1}∗ ×Q→ ∆{0, 1}.\nOracles are understood to be probabilistic: they randomly return 0 or 1. Let TO denote the machine T ∈ T when run with the oracle O, and let λOT denote the semimeasure induced by TO. This means that drawing from λOT involves two sources of randomness: one from the distribution induced by the probabilistic Turing machine T and one from the oracle’s answers.\nThe intended semantics of an oracle are that it takes a query (T, x, p) and returns 1 if the machine TO outputs 1 on input x with probability greater than p when run with the oracle O, i.e., when λOT (1 | x) > p. Furthermore, the oracle returns 0 if the machine TO outputs 1 on input x with probability less than p when run with the oracle O, i.e., when λOT (1 | x) < p. To fulfill this, the oracle O has to make statements about itself, since the machine T from the query may again query O. Therefore we call oracles of this kind reflective oracles. This has to be defined very carefully to avoid the obvious diagonalization issues that are caused by programs that ask the oracle about themselves. We impose the following self-consistency constraint.\nDefinition 7.3 (Reflective Oracle). An oracle O is reflective iff for all queries (T, x, p) ∈ T × {0, 1}∗ ×Q,\n(a) λOT (1 | x) > p implies O(T, x, p) = 1, and\n(b) λOT (0 | x) > 1− p implies O(T, x, p) = 0.\nIf p under- or overshoots the true probability of λOT ( · | x), then the oracle must reveal this information. However, in the critical case when p = λOT (1 | x), the oracle is allowed to return anything and may randomize its result. Furthermore, since T might not output any symbol, it is possible that λOT (0 | x) + λOT (1 | x) < 1. In this case the oracle can reassign the non-halting probability mass to 0, 1, or randomize; see Figure 7.1.\nExample 7.4 (Reflective Oracles and Diagonalization). Let T ∈ T be a probabilistic Turing machine that outputs 1 − O(T, , 1/2) (T can know its own source code by quining; Kleene, 1952, Thm. 27). In other words, T queries the oracle about whether it is more likely to output 1 or 0, and then does whichever the oracle says is less likely. In this case we can use an oracle O(T, , 1/2) := 1/2 (answer 0 or 1 with equal probability), which implies λOT (1 | ) = λOT (0 | ) = 1/2, so the conditions of Definition 7.3 are satisfied. In fact, for this machine T we must have O(T, , 1/2) = 1/2 for all reflective oracles O. 3\nThe following theorem establishes that reflective oracles exist.\nTheorem 7.5 (Fallenstein et al., 2015c, App. B). There is a reflective oracle.\nDefinition 7.6 (Reflective-Oracle-Computable). A semimeasure is called reflectiveoracle-computable iff it is computable on a probabilistic Turing machine with access to a reflective oracle.\nFor any probabilistic Turing machine T ∈ T we can complete the semimeasure λOT ( · | x) into a reflective-oracle-computable measure λ O T ( · | x): Using the oracle O and a binary search on the parameter p we search for the crossover point p where O(T, x, p) goes from returning 1 to returning 0. The limit point p∗x ∈ R of the binary search is random since the oracle’s answers may be random. But the main point is that the expectation of p∗x exists, so λ O T (1 | x) = E[p∗x] = 1− λ O T (0 | x). Hence λ O T is a measure. Moreover, if the oracle is reflective, then λOT (x) ≥ λOT (x) for all x ∈ X ∗. In this sense the oracle O can be viewed as a way of ‘completing’ all semimeasures λOT to measures by arbitrarily assigning the non-halting probability mass. If the oracle O is reflective this is consistent in the sense that Turing machines who run other Turing machines will be completed in the same way. This is especially important for a universal machine that runs all other Turing machines to induce a Solomonoff prior (Example 3.5)."
    }, {
      "heading" : "7.1.2 A Limit Computable Reflective Oracle",
      "text" : "The proof of Theorem 7.5 given by Fallenstein et al. (2015c, App. B) is nonconstructive and uses the axiom of choice. In Section 7.1.3 we give a new proof for the existence of reflective oracles and provide a construction that there is a reflective oracle that is limit computable.\n132 The Grain of Truth Problem\nTheorem 7.7 (A Limit Computable Reflective Oracle). There is a reflective oracle that is limit computable.\nThis theorem has the immediate consequence that reflective oracles cannot be used as halting oracles. At first, this result may seem surprising: according to the definition of reflective oracles, they make concrete statements about the output of probabilistic Turing machines. However, the fact that the oracles may randomize some of the time actually removes enough information such that halting can no longer be decided from the oracle output.\nCorollary 7.8 (Reflective Oracles are not Halting Oracles). There is no probabilistic Turing machine T such that for every prefix program p and every reflective oracle O, we have that λOT (1 | p) > 1/2 if p halts and λOT (1 | p) < 1/2 otherwise.\nProof. Assume there was such a machine T and let O be the limit computable oracle from Theorem 7.7. Since O is reflective we can turn T into a deterministic halting oracle by calling O(T, p, 1/2) which deterministically returns 1 if p halts and 0 otherwise. Since O is limit computable, we can finitely compute the output of O on any query to arbitrary finite precision using our deterministic halting oracle. We construct a probabilistic Turing machine T ′ that uses our halting oracle to compute (rather than query) the oracle O on (T ′, , 1/2) to a precision of 1/3 in finite time. If O(T ′, , 1/2)± 1/3 > 1/2, the machine T ′ outputs 0, otherwise T ′ outputs 1. Since our halting oracle is entirely deterministic, the output of T ′ is entirely deterministic as well (and T ′ always halts), so λOT ′(0 | ) = 1 or λOT ′(1 | ) = 1. Therefore O(T ′, , 1/2) = 1 or O(T ′, , 1/2) = 0 because O is reflective. A precision of 1/3 is enough to tell them apart, hence T ′ returns 0 if O(T ′, , 1/2) = 1 and T ′ returns 1 if O(T ′, , 1/2) = 0. This is a contradiction.\nA similar argument can also be used to show that reflective oracles are not computable."
    }, {
      "heading" : "7.1.3 Proof of Theorem 7.7",
      "text" : "The idea for the proof of Theorem 7.7 is to construct an algorithm that outputs an infinite series of partial oracles converging to a reflective oracle in the limit.\nThe set of queries is countable, so we can assume that we have some computable enumeration of it:\nT × {0, 1}∗ ×Q =: {q1, q2, . . .}\nDefinition 7.9 (k-Partial Oracle). A k-partial oracle Õ is function from the first k queries to the multiples of 2−k in [0, 1]:\nÕ : {q1, q2, . . . , qk} → {n2−k | 0 ≤ n ≤ 2k}\nDefinition 7.10 (Approximating an Oracle). A k-partial oracle Õ approximates an oracle O iff |O(qi)− Õ(qi)| ≤ 2−k−1 for all i ≤ k.\n§7.1 Reflective Oracles 133\nLet k ∈ N, let Õ be a k-partial oracle, and let T ∈ T be an oracle machine. The machine T Õ that we get when we run T with the k-partial oracle Õ is defined as follows (this is with slight abuse of notation since k is taken to be understood implicitly).\n1. Run T for at most k steps.\n2. If T calls the oracle on qi for i ≤ k,\n(a) return 1 with probability Õ(qi)− 2−k−1, (b) return 0 with probability 1− Õ(qi)− 2−k−1, and (c) halt otherwise.\n3. If T calls the oracle on qj for j > k, halt.\nFurthermore, we define λÕT analogously to λ O T as the distribution generated by the machine T Õ.\nLemma 7.11. If a k-partial oracle Õ approximates a reflective oracle O, then λOT (1 | x) ≥ λÕT (1 | x) and λOT (0 | x) ≥ λÕT (0 | x) for all x ∈ {0, 1}∗ and all T ∈ T .\nProof. This follows from the definition of T Õ: when running T with Õ instead of O, we can only lose probability mass. If T makes calls whose index is > k or runs for more than k steps, then the execution is aborted and no further output is generated. If T makes calls whose index i ≤ k, then Õ(qi) − 2−k−1 ≤ O(qi) since Õ approximates O. Therefore the return of the call qi is underestimated as well.\nDefinition 7.12 (k-Partially Reflective). A k-partial oracle Õ is k-partially reflective iff for the first k queries (T, x, p)\n• λÕT (1 | x) > p implies Õ(T, x, p) = 1, and\n• λÕT (0 | x) > 1− p implies Õ(T, x, p) = 0.\nIt is important to note that we can check whether a k-partial oracle is k-partially reflective in finite time by running all machines T from the first k queries for k steps and tallying up the probabilities to compute λÕT .\nLemma 7.13. If O is a reflective oracle and Õ is a k-partial oracle that approximates O, then Õ is k-partially reflective.\nLemma 7.13 only holds because we use semimeasures whose conditionals are lower semicomputable.\nProof. Assuming λÕT (1 | x) > p we get from Lemma 7.11 that λOT (1 | x) ≥ λÕT (1 | x) > p. Thus O(T, x, p) = 1 because O is reflective. Since Õ approximates O, we get 1 = O(T, x, p) ≤ Õ(T, x, p) + 2−k−1, and since Õ assigns values in a 2−k-grid, it follows that Õ(T, x, p) = 1. The second implication is proved analogously.\n134 The Grain of Truth Problem\nDefinition 7.14 (Extending Partial Oracles). A k + 1-partial oracle Õ′ extends a kpartial oracle Õ iff |Õ(qi)− Õ′(qi)| ≤ 2−k−1 for all i ≤ k.\nLemma 7.15. There is an infinite sequence of partial oracles (Õk)k∈N such that for each k, Õk is a k-partially reflective k-partial oracle and Õk+1 extends Õk.\nProof. By Theorem 7.5 there is a reflective oracle O. For every k, there is a canonical k-partial oracle Õk that approximates O: restrict O to the first k queries and for any such query q pick the value in the 2−k-grid which is closest to O(q). By construction, Õk+1 extends Õk and by Lemma 7.13, each Õk is k-partially reflective.\nLemma 7.16. If the k + 1-partial oracle Õk+1 extends the k-partial oracle Õk, then λ Õk+1 T (1 | x) ≥ λ Õk T (1 | x) and λ Õk+1 T (0 | x) ≥ λ Õk T (0 | x) for all x ∈ {0, 1}∗ and all T ∈ T .\nProof. T Õk+1 runs for one more step than T Õk , can answer one more query and has increased oracle precision. Moreover, since Õk+1 extends Õk, we have |Õk+1(qi) − Õk(qi)| ≤ 2−k−1, and thus Õk+1(qi)− 2−k−1 ≥ Õk(qi)− 2−k. Therefore the success to answers to the oracle calls (case 2(a) and 2(b)) will not decrease in probability.\nNow everything is in place to state the algorithm that constructs a reflective oracle in the limit. It recursively traverses a tree of partial oracles. The tree’s nodes are the partial oracles; level k of the tree contains all k-partial oracles. There is an edge in the tree from the k-partial oracle Õk to the i-partial oracle Õi if and only if i = k + 1 and Õi extends Õk.\nFor every k, there are only finitely many k-partial oracles, since they are functions from finite sets to finite sets. In particular, there are exactly two 1-partial oracles (so the search tree has two roots). Pick one of them to start with, and proceed recursively as follows. Given a k-partial oracle Õk, there are finitely many (k + 1)-partial oracles that extend Õk (finite branching of the tree). Pick one that is (k+1)-partially reflective (which can be checked in finite time). If there is no (k+1)-partially reflective extension, backtrack.\nBy Lemma 7.15 our search tree is infinitely deep and thus the tree search does not terminate. Moreover, it can backtrack to each level only a finite number of times because at each level there is only a finite number of possible extensions. Therefore the algorithm will produce an infinite sequence of partial oracles, each extending the previous. Because of finite backtracking, the output eventually stabilizes on a sequence of partial oracles Õ1, Õ2, . . .. By the following lemma, this sequence converges to a reflective oracle, which concludes the proof of Theorem 7.7.\nLemma 7.17. Let Õ1, Õ2, . . . be a sequence where Õk is a k-partially reflective k-partial oracle and Õk+1 extends Õk for all k ∈ N. Let O := limk→∞ Õk be the pointwise limit. Then\n(a) λÕkT (1 | x) → λOT (1 | x) and λ Õk T (0 | x) → λOT (0 | x) as k → ∞ for all x ∈ {0, 1}∗\nand all T ∈ T , and\n§7.2 A Grain of Truth 135\n(b) O is a reflective oracle.\nProof. First note that the pointwise limit must exists because |Õk(qi) − Õk+1(qi)| ≤ 2−k−1 by Definition 7.14.\n(a) Since Õk+1 extends Õk, each Õk approximates O. Let x ∈ {0, 1}∗ and T ∈ T and consider the sequence ak := λ Õk T (1 | x) for k ∈ N. By Lemma 7.16, ak ≤ ak+1, so the\nsequence is monotone increasing. By Lemma 7.11, ak ≤ λOT (1 | x), so the sequence is bounded. Therefore it must converge. But it cannot converge to anything strictly below λOT (1 | x) by the definition of TO.\n(b) By definition, O is an oracle; it remains to show that O is reflective. Let qi = (T, x, p) be some query. If p < λOT (1 | x), then by (a) there is a k large enough such that p < λÕtT (1 | x) for all t ≥ k. For any t ≥ max{k, i}, we have Õt(T, x, p) = 1 since Õt is t-partially reflective. Therefore 1 = limk→∞ Õk(T, x, p) = O(T, x, p). The case 1− p < λOT (0 | x) is analogous."
    }, {
      "heading" : "7.2 A Grain of Truth",
      "text" : ""
    }, {
      "heading" : "7.2.1 Reflective Bayesian Agents",
      "text" : "Fix O to be a reflective oracle. From now on, we assume that the action space A := {α, β} is binary. We can treat computable measures over binary strings as environments: the environment ν corresponding to a probabilistic Turing machine T ∈ T is defined by\nν(et | æ<tat) := λ O T (y | x) = k∏ i=1 λ O T (yi | xy1 . . . yi−1)\nwhere y1:k is a binary encoding of et and x is a binary encoding of æ<tat. The actions a1:∞ are only contextual, and not part of the environment distribution. We define ν(e<t ‖ a<t) analogously to (4.1).\nLet T1, T2, . . . be an enumeration of all probabilistic Turing machines in T that use an oracle. We define the class of reflective environments\nMOrefl := { λ O T1 , λ O T2 , . . . } .\nThis is the class of all environments computable on a probabilistic Turing machine with reflective oracle O, that have been completed from semimeasures to measures using O.\nAnalogously to Section 4.3.1, we define a Bayesian mixture over the class MOrefl. Let w ∈ ∆MOrefl be a lower semicomputable prior probability distribution on MOrefl. Possible choices for the prior include the Solomonoff prior w ( λ O T ) := 2−K(T ), where K(T ) denotes the length of the shortest input to some universal Turing machine that encodes T . We define the corresponding Bayesian mixture\nξ(et | æ<tat) := ∑\nν∈MOrefl\nw(ν | æ<t)ν(et | æ<tat) (7.2)\n136 The Grain of Truth Problem\nwhere w(ν | æ<t) is the (renomalized) posterior,\nw(ν | æ<t) := w(ν) ν(e<t ‖ a<t) ξ(e<t ‖ a<t) . (7.3)\nThe mixture ξ is lower semicomputable on an oracle Turing machine because the posterior w( · | æ<t) is lower semicomputable. Hence there is an oracle machine T such that ξ = λOT . We define its completion ξ := λ O T as the completion of λOT . This is the distribution that is used to compute the posterior. There are no cyclic dependencies since ξ is called on the shorter history æ<t. We arrive at the following statement.\nProposition 7.18 (Bayes is in the Class). ξ ∈MOrefl.\nMoreover, since O is reflective, we have that ξ dominates all environments ν ∈MOrefl:\nξ(e1:t ‖ a1:t) = ξ(et | æ<tat)ξ(e<t ‖ a<t) ≥ ξ(et | æ<tat)ξ(e<t | a<t)\n= ξ(e<t ‖ a<t) ∑\nν∈MOrefl\nw(ν | æ<t)ν(et | æ<tat)\n= ξ(e<t ‖ a<t) ∑\nν∈MOrefl\nw(ν) ν(e<t ‖ a<t) ξ(e<t ‖ a<t) ν(et | æ<tat)\n= ∑\nν∈MOrefl\nw(ν)ν(e1:t ‖ a1:t)\n≥ w(ν)ν(e1:t ‖ a1:t)\nTherefore we get on-policy value convergence according to Corollary 4.20: for all µ ∈MOrefl and all policies π\nV π ξ (æ<t)− V πµ (æ<t)→ 0 as t→∞ µπ-almost surely. (7.4)"
    }, {
      "heading" : "7.2.2 Reflective-Oracle-Computable Policies",
      "text" : "This subsection is dedicated to the following result that was previously stated by Fallenstein et al. (2015a, Alg. 6) but not proved. It contrasts results on arbitrary semicomputable environments where optimal policies are not limit computable (see Section 6.3).\nTheorem 7.19 (Optimal Policies are Oracle Computable). For every ν ∈MOrefl, there is a ν-optimal (stochastic) policy π∗ν that is reflective-oracle-computable.\nNote that even though deterministic optimal policies always exist, those policies are typically not reflective-oracle-computable.\nTo prove Theorem 7.19 we need the following lemma.\nLemma 7.20 (Reflective-Oracle-Computable Optimal Value Function). For every environment ν ∈MOrefl the optimal value function V ∗ν is reflective-oracle-computable.\n§7.3 Multi-Agent Environments 137\nProof. This proof follows the proof of Corollary 6.14. We write the optimal value explicitly as in (4.2). For a fixed m, all involved quantities are reflective-oracle-computable. Moreover, this quantity is monotone increasing in m and the tail sum from m+ 1 to∞ is bounded by Γm+1 which is computable according to Assumption 4.6a and converges to 0 as m→∞. Therefore we can enumerate all rationals above and below V ∗ν .\nProof of Theorem 7.19. According to Lemma 7.20 the optimal value function V ∗ν is reflective-oracle-computable. Hence there is a probabilistic Turing machine T such that\nλOT (1 | æ<t) = ( V ∗ν (æ<tα)− V ∗ν (æ<tβ) + 1 ) /2.\nWe define the policy\nπ(æ<t) := { α if O(T,æ<t, 1/2) = 1, and β if O(T,æ<t, 1/2) = 0\nThis policy is stochastic because the answer of the oracle O is stochastic. It remains to show that π is a ν-optimal policy. If V ∗ν (æ<tα) > V ∗ν (æ<tβ), then λOT (1 | æ<t) > 1/2, thus O(T,æ<t, 1/2) = 1 since O is reflective, and hence π takes action α. Conversely, if V ∗ν (æ<tα) < V ∗ν (æ<tβ), then λOT (1 | æ<t) < 1/2, thus O(T,æ<t, 1/2) = 0 since O is reflective, and hence π takes action β. Lastly, if V ∗ν (æ<tα) = V ∗ν (æ<tβ), then both actions are optimal and thus it does not matter which action is returned by policy π. (This is the case where the oracle may randomize.)"
    }, {
      "heading" : "7.2.3 Solution to the Grain of Truth Problem",
      "text" : "Together, Proposition 7.18 and Theorem 7.19 provide the necessary ingredients to solve the grain of truth problem (Problem 7.1).\nCorollary 7.21 (Solution to the Grain of Truth Problem). For every lower semicomputable prior w ∈ ∆MOrefl the Bayes optimal policy π∗ξ is reflective-oracle-computable where ξ is the Bayes-mixture corresponding to w defined in (7.2).\nProof. From Proposition 7.18 and Theorem 7.19.\nHence the environment classMOrefl contains any reflective-oracle-computable modification of the Bayes optimal policy π∗\nξ . In particular, this includes computable multi-\nagent environments that contain other Bayesian agents over the class MOrefl. So any Bayesian agent over the classMOrefl has a grain of truth even though the environment may contain other Bayesian agents of equal power. We proceed to sketch the implications for multi-agent environments in the next section."
    }, {
      "heading" : "7.3 Multi-Agent Environments",
      "text" : "In a multi-agent environment there are n agents each taking sequential actions from the finite action space A. In each time step t = 1, 2, . . ., the environment receives action ait\n138 The Grain of Truth Problem\nfrom agent i and outputs n percepts e1t , . . . , ent ∈ E , one for each agent. Each percept eit = (o i t, r i t) contains an observation oit and a reward rit ∈ [0, 1]. Importantly, agent i only sees its own action ait and its own percept eit (see Figure 7.2). We use the shorthand notation at := (a1t , . . . , ant ) and et := (e1t , . . . , ent ) and denote æi<t = ai1ei1 . . . ait−1eit−1 and æ<t = a1e1 . . . at−1et−1. Formally, multi-agent environments are defined as follows.\nDefinition 7.22 (Multi-Agent Environment). A multi-agent environment is a function\nσ : (An × En)∗ ×An → ∆(En).\nTogether with the policies π1, . . . , πn the multi-agent environment σ induces a history distribution σπ1:n where\nσπ1:n( ) : = 1\nσπ1:n(æ1:t) : = σπ1:n(æ<tat)σ(et | æ<tat)\nσπ1:n(æ<tat) : = σπ1:n(æ<t) n∏ i=1 πi(a i t | æi<t).\nAgent i acts in a subjective environment σi given by joining the multi-agent environment σ with the policies π1, . . . , πn and marginalizing over the histories that πi does not see. Together with policy πi, the environment σi yields a distribution over the histories of agent i\nσπii (æ i <t) := ∑ æj<t,j 6=i σπ1:n(æ<t).\nWe get the definition of the subjective environment σi with the identity σi(eit | æi<tait) := σπii (e i t | æi<tait). The subjective environment σi depends on πi because other policies’ actions may depend on the actions of πi. It is crucial to note that the subjective environment σi and the policy πi are ordinary environments and policies, so we can use the notation from Chapter 4.\n§7.3 Multi-Agent Environments 139\nOur definition of a multi-agent environment is very general and encompasses most of game theory. It allows for cooperative, competitive, and mixed games; infinitely repeated games or any (infinite-length) extensive form games with finitely many players.\nExample 7.23 (Matching Pennies). In the game of matching pennies there are two agents (n = 2), and two actions A = {α, β} representing the two sides of a penny. In each time step agent 1 wins if the two actions are identical and agent 2 wins if the two actions are different. The payoff matrix is as follows.\nα β\nα 1,0 0,1 β 0,1 1,0\nWe use E = {0, 1} to be the set of rewards (observations are vacuous) and define the multi-agent environment σ to give reward 1 to agent 1 iff a1t = a2t (0 otherwise) and reward 1 to agent 2 iff a1t 6= a2t (0 otherwise). Formally,\nσ(r1t r 2 t | æ<tat) :=  1 if r1t = 1, r2t = 0, a1t = a2t , 1 if r1t = 0, r2t = 1, a1t 6= a2t , and 0 otherwise.\nLet πα denote the policy that always takes action α. If two agents each using policy πα play matching pennies, agent 1 wins in every step. Formally, setting π1 := π2 := πα we get a history distribution that assigns probability one to the history\nαα10αα10 . . . .\nThe subjective environment of agent 1 is\nσ1(r 1 t | æ1<ta1t ) =  1 if r1t = 1, a1t = α, 1 if r1t = 0, a1t = β, and 0 otherwise.\nTherefore policy πα is optimal in agent 1’s subjective environment. 3\nDefinition 7.24 (ε-Best Response). A policy πi acting in multi-agent environment σ with policies π1, . . . , πn is an ε-best response after history æi<t iff\nV ∗σi(æ i <t)− V πiσi (æ i <t) < ε.\nIf at some time step t, all agents’ policies are ε-best responses, we have an εNash equilibrium. The property of multi-agent systems that is analogous to asymptotic optimality is convergence to an ε-Nash equilibrium.\n140 The Grain of Truth Problem"
    }, {
      "heading" : "7.4 Informed Reflective Agents",
      "text" : "Let σ be a multi-agent environment and let π∗σ1 , . . . π ∗ σn be such that for each i the policy π∗σi is an optimal policy in agent i’s subjective environment σi. At first glance this seems ill-defined: The subjective environment σi depends on each policy π∗σj , which depends on the subjective environment σj , which in turn depends on the policy π∗σi . However, this circular definition actually has a well-defined solution.\nTheorem 7.25 (Optimal Multi-Agent Policies). For any reflective-oracle-computable multi-agent environment σ, the optimal policies π∗σ1 , . . . , π ∗ σn exist and are reflectiveoracle-computable.\nTo prove Theorem 7.25, we need the following proposition.\nProposition 7.26 (Reflective-Oracle-Computability). If the multi-agent environment σ and the policies π1, . . . , πn are reflective-oracle-computable, then σπ1:n and σπii are reflective-oracle-computable, and σi ∈MOrefl.\nProof. All involved quantities in the definition of σπ1:n are reflective-oracle-computable by assumption, therefore also their marginalizations σπii and σi.\nProof of Theorem 7.25. According to Theorem 7.19 the optimal policy π∗σi in agent i’s subjective environment is reflective-oracle-computable if the subjective environment σi is. In particular the process that takes σi in form of a probabilistic Turing machine and returns π∗σi is reflective-oracle-computable. Moreover, σi is reflective-oracle-computable if σ and π1, . . . , πn are, according to Proposition 7.26. Again, this construction is itself reflective-oracle-computable. Connecting these two constructions we get probabilistic Turing machines T1, . . . , Tn ∈ T where each Ti takes the multi-agent environment σ and π1, . . . , πn in form of probabilistic Turing machines and returns π∗σi . We define the probabilistic Turing machines T ′1, . . . , T ′n where T ′i runs T O i on (σ, T ′ 1, . . . , T ′ n); hence T ′i computes π∗σi . Note that this construction only works because we relied on the reflective oracle in the proof of Theorem 7.19. Since the machines TOi always halt, so do T ′ i O despite their infinitely recursive construction.\nNote the strength of Theorem 7.25: each of the policies π∗σi is acting optimally given the knowledge of everyone else’s policies. Hence optimal policies play 0-best responses by definition, so if every agent is playing an optimal policy, we have a Nash equilibrium. Moreover, this Nash equilibrium is also a subgame perfect Nash equilibrium, because each agent also acts optimally on the counterfactual histories that do not end up being played. In other words, Theorem 7.25 states the existence and reflective-oracle-computability of a subgame perfect Nash equilibrium in any reflectiveoracle-computable multi-agent environment. The following immediate corollary states that these subgame perfect Nash equilibria are limit computable.\nCorollary 7.27 (Solution to Computable Multi-Agent Environments). For any computable multi-agent environment σ, the optimal policies π∗σ1 , . . . , π ∗ σn exist and are limit computable.\n§7.5 Learning Reflective Agents 141\nProof. From Theorem 7.25 and Theorem 7.7.\nExample 7.28 (Nash Equilibrium in Matching Pennies). Consider the matching pennies game from Example 7.23. The only pair of optimal policies is the pair of two uniformly random policies that play α and β with equal probability in every time step: if one of the agents picks a policy that plays one of the actions with probability > 1/2, then the other agent’s best response is to play the other action with probability 1. But now the first agent’s policy is no longer a best response. 3"
    }, {
      "heading" : "7.5 Learning Reflective Agents",
      "text" : "Since our classMOrefl solves the grain of truth problem, the result by Kalai and Lehrer (1993) immediately implies that for any Bayesian agents π1, . . . , πn interacting in an infinitely repeated game and for all ε > 0 and all i ∈ {1, . . . , n} there is almost surely a t0 ∈ N such that for all t ≥ t0 the policy πi is an ε-best response. However, this hinges on the important fact that every agent has to know the game and also that all other agents are Bayesian agents. Otherwise the convergence to an ε-Nash equilibrium may fail, as illustrated by the following example.\nAt the core of the construction is a dogmatic prior (Section 5.2.2). A dogmatic prior assigns very high probability to going to hell (reward 0 forever) if the agent deviates from a given computable policy π. For a Bayesian agent it is thus only worth deviating from the policy π if the agent thinks that the prospects of following π are very poor already. This implies that for general multi-agent environments and without additional assumptions on the prior, we cannot prove any meaningful convergence result about Bayesian agents acting in an unknown multi-agent environment.\nExample 7.29 (Reflective Bayesians Playing Matching Pennies). Consider the multiagent environment matching pennies from Example 7.23. Let π1 be the policy that takes the action sequence (ααβ)∞ and let π2 := πα be the policy that always takes action α. The average reward of policy π1 is 2/3 and the average reward of policy π2 is 1/3. Let ξ be a universal mixture (7.2). By on-policy value convergence (7.4), V π1 ξ → c1 ≈ 2/3 and V π2ξ → c2 ≈ 1/3 almost surely when following policies (π1, π2). Therefore there is an ε > 0 such that V π1 ξ > ε and V π2 ξ\n> ε for all time steps. Now we can apply Theorem 5.5 to conclude that there are (dogmatic) mixtures ξ′1 and ξ′2 such that π∗ξ′1 always follows policy π1 and π ∗ ξ′2\nalways follows policy π2. This does not converge to a (ε-)Nash equilibrium. 3\nAn important property required for the construction in Example 7.29 is that the environment class contains environments that threaten the agent with going to hell, which is outside of the class of matching pennies environments. In other words, since the agent does not know a priori that it is playing a matching pennies game, it might behave more conservatively than appropriate for the game.\nThe following theorem is our main convergence result. It states that for asymptotically optimal agents we get convergence to ε-Nash equilibria in any reflective-oraclecomputable multi-agent environment.\n142 The Grain of Truth Problem\nTheorem 7.30 (Convergence to Equilibrium). Let σ be an reflective-oracle-computable multi-agent environment and let π1, . . . , πn be reflective-oracle-computable policies that are asymptotically optimal in mean in the class MOrefl. Then for all ε > 0 and all i ∈ {1, . . . , n} the σπ1:n-probability that the policy πi is an ε-best response converges to 1 as t→∞.\nProof. Let i ∈ {1, . . . , n}. By Proposition 7.26, the subjective environment σi is reflective-oracle-computable, therefore σi ∈ MOrefl. Since πi is asymptotically optimal in mean in the class MOrefl, we get that E[V ∗σi(æ<t) − V πi σi (æ<t)] → 0. Convergence in mean implies convergence in probability for bounded random variables, hence for all ε > 0 we have\nσπii [V ∗ σi(æ i <t)− V πiσi (æ i <t) ≥ ε]→ 0 as t→∞.\nTherefore the probability that the policy πi plays an ε-best response converges to 1 as t→∞.\nIn contrast to Theorem 7.25 which yields policies that play a subgame perfect equilibrium, this is not the case for Theorem 7.30: the agents typically do not learn to predict off-policy and thus will generally not play ε-best responses in the counterfactual histories that they never see. This weaker form of equilibrium is unavoidable if the agents do not know the environment because it is impossible to learn the parts that they do not interact with.\nCorollary 7.31 (Convergence to Equilibrium). There are limit computable policies π1, . . . , πn such that for any computable multi-agent environment σ and for all ε > 0 and all i ∈ {1, . . . , n} the σπ1:n-probability that the policy πi is an ε-best response converges to 1 as t→∞.\nProof. Pick π1, . . . , πn to be the Thompson sampling policy πT defined in Algorithm 2 over the countable classMOrefl. By Theorem 5.25 these policies are asymptotically optimal in mean. By Theorem 7.32 below they are reflective-oracle-computable and by Theorem 7.7 they are also limit computable. The statement now follows from Theorem 7.30.\nTheorem 7.32 (Thompson Sampling is Reflective-Oracle-Computable). The policy πT defined in Algorithm 2 over the classMOrefl is reflective-oracle-computable.\nProof. The posterior w( · | æ<t) is reflective-oracle-computable by the definition (7.3) and according to Theorem 7.19 the optimal policies π∗ν are reflective-oracle-computable. On resampling steps we can compute the action probabilities of πT by enumerating all ν ∈MOrefl and computing π∗ν weighted by the posterior w(ν | æ<t). Between resampling steps we need to condition the policy πT computed above by the actions it has already taken since the last resampling step (compare Example 5.28).\nBecause the posterior w( · | æ<t) is a ξ π-martingale when acting according to the policy π, it converges ξπ-almost surely according to the martingale convergence theorem (Theorem 2.8). Since ξ dominates the subjective environment σi, it also converges\n§7.6 Impossibility Results 143\nσπi -almost surely (see Example 3.20). Hence all the Thompson sampling agents eventually ‘calm down’ and settle on some posterior belief.\nAccording to Theorem 7.30, the policies π1, . . . , πn only need to be asymptotically optimal in mean. For Thompson sampling this is independent of the discount function according to Theorem 5.25 (but the discount function has to be known to the agent). So the different agents may use different discount functions, resample at different time steps and converge at different speeds.\nExample 7.33 (Thompson Samplers Playing Matching Pennies). Consider the matching pennies game from Example 7.23 and let both agents use the Thompson sampling policy defined in Algorithm 2, i.e., define π1 := πT and π2 := πT .\nThe value of the uniformly random policy πR is always 1/2, so V ∗σi ≥ V πR σi = 1/2. According to Theorem 7.30, for every ε > 0, each agent will eventually always play an ε-best response, i.e., V πiσi > V ∗ σi − ε ≥ 1/2 − ε. Since matching pennies is a zero-sum game, V π1σ1 + V π2 σ2 = 1, so V π2 σ2 = 1− V π1 σ1 < 1/2 + ε.\nTherefore each agent will end up randomizing their actions; πi(at | æ<t) ≈ 1/2 most of the time: If one of the agents (say agent 1) does not sufficiently randomize their actions in some time steps, then agent 2 could exploit this by picking a deterministic adversarial policy in those time steps. Suppose that this way it can gain a value of ε compared to the random policy πR, i.e., V π2σ2 ≥ V πR σ2 + ε = 1/2 + ε. But this is a contradiction because then agent 1 is not playing an ε-best response:\nV ∗σ1 − V π1 σ1 = V ∗ σ1 − 1 + V π2 σ2 ≥ 1/2− 1 + 1/2 + ε = ε 3"
    }, {
      "heading" : "7.6 Impossibility Results",
      "text" : "Why does our solution to the grain of truth problem not violate the impossibility results from the literature? Assume we are playing an infinitely repeated game where in the stage game no agent has a weakly dominant action and the pure action maxmin reward is strictly less then the minmax reward. The impossibility result of Nachbar (1997, 2005) states that there is no class of policies Π such that the following are simultaneously satisfied.\n• Learnability. Each agent learns to predict the other agent’s actions.\n• Caution and Symmetry. The set Π is closed under simple policy modifications such as renaming actions.\n• Purity. There is an ε > 0 such that for any stochastic policy π ∈ Π there is a deterministic policy π′ ∈ Π such that if π′(æ<t) = a, then π(a | æ<t) > ε.\n• Consistency. Each agent always has an ε-best response available in Π.\nIn order to converge to an ε-Nash equilibrium, each agent has to have an ε-best response available to them, so consistency is our target. Learnability is immediately satisfied for any environment in our class if we have a dominant prior according to Corollary 4.20.\n144 The Grain of Truth Problem\nFor MOrefl caution and symmetry are also satisfied since this set is closed under any computable modifications to policies. However, our classMOrefl avoids this impossibility result because it violates the purity condition: Let T1, T2, . . . be an enumeration of T . Consider the policy π that maps history æi<t to the action 1−O(Tt,æi<t, 1/2). If Tt is deterministic, then π will take a different action than Tt for any history of length t− 1. Therefore no deterministic reflective-oracle-computable policy can take an action that π assigns positive probability to in every time step.\nFoster and Young (2001) present a condition that makes convergence to a Nash equilibrium impossible: if the player’s rewards are perturbed by a small real number drawn from some continuous density ν, then for ν-almost all realizations the players do not learn to predict each other and do not converge to a Nash equilibrium. For example, in a matching pennies game, rational agents randomize only if the (subjective) values of both actions are exactly equal. But this happens only with ν-probability zero, since ν is a density. Thus with ν-probability one the agents do not randomize. If the agents do not randomize, they either fail to learn to predict each other, or they are not acting rationally according to their beliefs: otherwise they would seize the opportunity to exploit the other player’s deterministic action.\nBut this does not contradict our convergence result: the class MOrefl is countable and each ν ∈ MOrefl has positive prior probability. Perturbation of rewards with arbitrary real numbers is not possible. Even more, the argument given by Foster and Young (2001) cannot work in our setting: the Bayesian mixture ξ mixes over λT for all probabilistic Turing machines T . For Turing machines T that sometimes do not halt, the oracle decides how to complete λT into a measure λT . Thus the oracle has enough influence on the exact values in the Bayesian mixture that the values of two actions in matching pennies can be made exactly equal."
    }, {
      "heading" : "7.7 Discussion",
      "text" : "This chapter introduced the class of all reflective-oracle-computable environmentsMOrefl. This class fully solves the grain of truth problem (Problem 7.1) because it contains (any computable modification of) Bayesian agents defined overMOrefl: the optimal agents and Bayes optimal agents over the class are all reflective-oracle-computable (Theorem 7.19 and Corollary 7.21).\nIf the environment is unknown, then a Bayesian agent may end up playing suboptimally (Example 7.29). However, if each agent uses a policy that is asymptotically optimal in mean (such as the Thompson sampling policy from Section 4.3.4) then for every ε > 0 the agents converge to an ε-Nash equilibrium (Theorem 7.30 and Corollary 7.31).\nHowever, Corollary 7.31 does not imply that two Thompson sampling policies will converge to cooperation in an iterated prisoner’s dilemma since always defecting is also a Nash equilibrium. The exact outcome will depend on the priors involved and the randomness of the policies.\nOur solution to the grain of truth problem is purely theoretical. However, Theorem 7.7 shows that our classMOrefl allows for computable approximations. This suggests\n§7.7 Discussion 145\nthat practical approaches can be derived from this result, and reflective oracles have already seen applications in one-shot games (Fallenstein et al., 2015b).\n146 The Grain of Truth Problem\nChapter 8\nConclusion\nThe biggest existential risk is that future superintelligences stop simulating us. — Nick Bostrom\nToday computer programs exceeding humans in general intelligence are known only from science fiction. But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (Müller and Bostrom, 2016).\nThe advent of strong AI would be the biggest event in human history. Potential benefits are huge, as the new level of automation would free us from any kind of undesirable labor. But there is no reason to believe that humans are at the far end of the intelligence spectrum. Rather, humans barely cross the threshold for general intelligence to be able to use language and do science. Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines?\nThis could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.g., through self-amplification effects from AIs improving themselves (if doing AI research is one of humans’ capabilities, then a machine that can do everything humans can do can also do AI research). Once machine intelligence is above or far above human level, machines would steer the course of history. There is no reason to believe that machines would be adversarial to us, but nevertheless humanity’s fate might rest at the whims of the machines, just as chimpanzees today have no say in the large-scale events on this planet.\nBostrom (2002) defines:\nExistential risk — One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.\nExistential risks are events that have the power to extinguish human life as we know it. Examples are cosmic events such as an asteroid colliding with Earth. But cosmic events are unlikely on human timescales compared to human-made existential risks from nuclear weapons, synthetic biology, and nanotechnology.\nIt is possible that artificial intelligence also falls into this category. Vinge (1993) was the first person to recognize this:\n147\n148 Conclusion\nWithin thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\nAfter Vinge, Yudkowsky (2001, 2008) can be regarded as one of the key people to popularize the potential dangers of AI technology. Bostrom (2003) picked up on this issue very early and gave the topic credibility through his well-researched and carefully written book Superintelligence (Bostrom, 2014). He argues that we need to solve the Control Problem—the unique principal-agent problem that arises with the creation of strong AI (Bostrom, 2014, Ch. 9). In other words: How do we align strong AI with human values? How do we ensure that AI remains robust and beneficial? This research is collected under the umbrella term AI safety . Currently, we have no idea how to solve these problems even in theory.\nThrough Yudkowsky’s and, more importantly, Bostrom’s efforts, AI-related longterm safety concerns have now entered the mainstream media. In 2014 high-profile scientists such as Stephen Hawking, Max Tegmark, Stuart Russell, and Frank Wilczek have warned against the dangers posed by AI (Hawking et al., 2014). (See also Alexander (2015) for a collection of positions by prominent AI researchers.) Many scientists inside and outside the field have signed an open letter that research ensuring that AI systems remain robust and beneficial is both important and timely (Future of Life Institute, 2015c). This lead entrepreneur Elon Musk to donate $10 million to kick-start research in this field (Future of Life Institute, 2015a); most of this money has now been distributed across the planet to 37 different projects (Future of Life Institute, 2015b). Moreover, the Future of Life Institute and the Machine Intelligence Research Institute have formulated concrete technical research priorities to make AI more robust and beneficial (Soares and Fallenstein, 2014; Russell et al., 2015).\nAt the end of 2015 followed the announcement of OpenAI, a nonprofit organization with financial backing from several famous silicon valley billionaires (OpenAI, 2015):\nOpenAI is a non-profit artificial intelligence research company. Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return.\nThe mission of OpenAI is to enable everyone to benefit from AI technology. But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea.\nDespite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from ‘HLAI is so far away that any worry is misplaced’ to claims that ‘the safety problem would not be so hard’. See Sotala and Yampolskiy (2014) for a discussion.\nIf AI poses an existential risk then a formal theory of strong AI is paramount to develop technical approaches to mitigate this risk. Which path will ultimately lead us to HLAI is in the realm of speculation at this time; therefore we should make as few and as weak assumptions as possible and abstract away from possible future implementation details.\n149\nThis thesis lays some of the groundwork for this endeavor. We built on top of Hutter’s theory of universal artificial intelligence. Chapter 3 discussed the formal theory of learning. Chapter 4 presented several approaches to acting in unknown environments (Bayes, Thompson sampling, knowledge-seeking agents, and BayesExp). Chapter 5 analysed these approaches and discussed notions of optimality and principled problems with acting under uncertainty in general environment. Chapter 6 provided the mathematical tools to analyze the computational properties of these models. Finally, Chapter 7 solved the grain of truth problem, which lead to convergence to Nash equilibria in unknown general multi-agent environments.\nOur work is theoretical by nature and there is still a long way to go until these results make their way into applications. But a solution in principle is a crucial first step towards solving a problem in practice. Consider the research paper by Shannon (1950) on how to solve chess in principle. The algorithm he describes expands the full game tree of chess (until some specified depth), which is completely infeasible even with today’s computation power. His contribution was to show that winning at chess is a feat that computers can achieve in principle, which was not universally accepted at the time. Even more, his approach already considered the correct ideas (minimax-search over the game tree) that ultimately lead to the defeat of the chess champion Garry Kasparov by the computer Deep Blue 46 years later (IBM, 2012a).\nThe theory of general reinforcement learning can serve and has served as a starting point for future investigation in AI safety. In particular, value learning (Dewey, 2011), self-reflection (Soares, 2015; Fallenstein et al., 2015b), self-modification (Orseau and Ring, 2011, 2012a; Everitt et al., 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al., 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).\nIt is possible that HLAI is decades or centuries away. It might also be a few years around the corner. Whichever is the case, we are currently completely unprepared for the consequences. As an AI researcher, it is tempting to devote your life to increasing the capability of AI, advancing it domain after domain, and showing off with flashy demos and impressive victories over human contestants. But every technology incurs risks, and the more powerful the technology, the higher the risks. The potential power of AI technology is enormous, and correspondingly we need to consider the risks, take them seriously, and proceed to mitigate them.\n150 Conclusion\nAppendix A\nMeasures and Martingales\nIn this chapter we provide the proofs for Theorem 3.18 and Theorem 3.19.\nProof of Theorem 3.18. Xt is only undefined if P (Γv1:t) = 0. The set\n{v ∈ Σ∞ | ∃t. P (Γv1:t) = 0}\nhas P -measure 0 and hence (Xt)t∈N is well-defined almost everywhere. Xt is constant on Γu for all u ∈ Σt, and Ft is generated by a collection of finitely many disjoint sets: Σ∞ =\n⊎ u∈Σt Γu.\n(a) Therefore Xt is Ft-measurable. (b) Γu = ⊎ a∈Σ Γua for all u ∈ Σt and v ∈ Γu, and therefore\nE[Xt+1 | Ft](v) = 1\nP (Γu) ∑ a∈Σ Xt+1(ua)P (Γua) = 1 P (Γu) ∑ a∈Σ Q(Γua) P (Γua) P (Γua)\n(∗) =\n1\nP (Γu) ∑ a∈Σ Q(Γua) = Q(Γu) P (Γu) = Xt(v).\nAt (∗) we used the fact that P is locally absolutely continuous with respect to Q. (If P were not locally absolutely continuous with respect to Q, then there are cases where P (Γu) > 0, P (Γua) = 0, and Q(Γua) 6= 0. Therefore Xt+1(ua) does not contribute to the expectation and thus Xt+1(ua)P (Γua) = 0 6= Q(Γua).)\nP ≥ 0 and Q ≥ 0 by definition, thus Xt ≥ 0. Since P (Γ ) = Q(Γ ) = 1, we have E[X0] = 1.\nThe following lemma gives a convenient condition for the existence of a measure on (Σω,F∞). It is a special case of the Daniell-Kolmogorov Extension Theorem (Rogers and Williams, 1994, Thm. 26.1).\nLemma A.1 (Extending measures). Let q : Σ∗ → [0, 1] be a function such that q( ) = 1 and ∑ a∈Σ q(ua) = q(u) for all u ∈ Σ∗. Then there exists a unique probability measure Q on (Σ∞,F∞) such that q(u) = Q(Γu) for all u ∈ Σ∗.\n151\n152 Measures and Martingales\nTo prove this lemma, we need the following two ingredients.\nDefinition A.2 (Semiring). A set R ⊆ 2Ω is called semiring over Ω iff\n(a) ∅ ∈ R,\n(b) for all A,B ∈ R, the set A ∩B ∈ R, and\n(c) for all A,B ∈ R, there are pairwise disjoint sets C1, . . . , Cn ∈ R such that A \\B =⊎n i=1Ci.\nTheorem A.3 (Carathéodory’s Extension Theorem; Durrett, 2010, Thm. A.1.1). Let R be a semiring over Ω and let µ : R → [0, 1] be a function such that\n(a) µ(Ω) = 1 (normalization), (b) µ( ⊎n i=1Ai) = ∑n i=1 µ(Ai) for pairwise disjoint sets A1, . . . , An ∈ R such that⊎n\ni=1Ai ∈ R (finite additivity), and\n(c) µ( ⋃ i≥0Ai) ≤ ∑ i≥0 µ(Ai) for any collection (Ai)i≥0 such that each Ai ∈ R and⋃\ni≥0Ai ∈ R (σ-subadditivity).\nThen there is a unique extension µ of µ that is a probability measure on (Ω, σ(R)) such that µ(A) = µ(A) for all A ∈ R.\nProof of Lemma A.1. We show the existence of Q using Carathéodory’s Extension Theorem. Define R := {Γu | u ∈ Σ∗} ∪ {∅}.\n(a) ∅ ∈ R.\n(b) For any Γu,Γv ∈ R, either\n• u is a prefix of v and Γu ∩ Γv = Γv ∈ R, or\n• v is a prefix of u and Γu ∩ Γv = Γu ∈ R, or\n• Γu ∩ Γv = ∅ ∈ R.\n(c) For any Γu,Γv ∈ R,\n• Γu \\ Γv = ⊎ w∈Σ|v|−|u|\\{x} Γuw if v = ux, i.e., u is a prefix of v, and\n• Γu \\ Γv = ∅ otherwise.\nTherefore R is a semiring. By definition of R, we have σ(R) = F∞. The function q : Σ∗ → [0, 1] naturally gives rise to a function µ : R → [0, 1] with µ(∅) := 0 and µ(Γu) := q(u) for all u ∈ Σ∗. We will now check the prerequisites of Carathéodory’s Extension Theorem.\n(a) (Normalization.) µ(Σ∞) = µ(Γ ) = q( ) = 1.\n153\n(b) (Finite additivity.) Let Γu1 , . . . ,Γuk ∈ R be pairwise disjoint sets such that Γw :=⊎k i=1 Γui ∈ R. Let ` := max{|ui| | 1 ≤ i ≤ k}, then Γw = ⊎ v∈Σ` Γwv. By\nassumption, ∑ a∈Σ q(ua) = q(u), thus ∑\na∈Σ µ(Γua) = µ(Γu) and inductively we have\nµ(Γui) = ∑\ns∈Σ`−|ui| µ(Γuis), (A.1)\nand µ(Γw) = ∑ v∈Σ` µ(Γwv). (A.2)\nFor every string v ∈ Σ`, the concatenation wv ∈ Γw = ⊎k i=1 Γui , so there is a unique i such that wv ∈ Γui . Hence there is a unique string s ∈ Σ`−|ui| such that wv = uis. Together with (A.1) and (A.2) this yields\nµ ( k⊎ i=1 Γui ) = µ(Γw) = ∑ v∈Σ` µ(Γwv) = k∑ i=1 ∑ s∈Σ`−|ui| µ(Γuis) = k∑ i=1 µ(Γui).\n(c) (σ-subadditivity.) We will show that each Γu is compact with respect to the topology O generated by R. σ-subadditivity then follows from (b) because every countable union is in fact a finite union.\nWe will show that the topology O is the product topology of the discrete topology on Σ. (This establishes that (Σω,O) is a Cantor Space.) Every projection πk : Σ∞ → Σ selecting the k-th symbol is continuous, since π−1k (a) = ⋃ u∈Σk−1 Γua for every a ∈ Σ. Moreover, O is the coarsest topology with this property, since we can generate every open set Γu ∈ R in the base of the topology by\nΓu = |u|⋂ i=1 π−1i ({ui}).\nThe set Σ is finite and thus compact. By Tychonoff’s Theorem, Σ∞ is also compact. Therefore Γu is compact since it is homeomorphic to Σ∞ via the canonical map βu : Σ ∞ → Γu, v 7→ uv.\nFrom (a), (b), and (c) Carathéodory’s Extension Theorem yields a unique probability measure Q on (Σ∞,F∞) such that Q(Γu) = µ(Γu) = q(u) for all u ∈ Σ∗.\nUsing Lemma A.1, the proof of Theorem 3.19 is now straightforward.\nProof of Theorem 3.19. We define a function q : Σ∗ → R, with\nq(u) := X|u|(v)P (Γu)\nfor any v ∈ Γu. The choice of v is irrelevant because X|u| is constant on Γu since it is Ft-measurable. In the following, we also write Xt(u) if |u| = t to simplify notation.\n154 Measures and Martingales\nThe function q is non-negative because Xt and P are both non-negative. Moreover, for any u ∈ Σt,\n1 = E[Xt] = ∫\nΣ∞ XtdP ≥ ∫ Γu XtdP = P (Γu)Xt(u) = q(u).\nHence the range of q is a subset of [0, 1]. We have q( ) = X0( )P (Γ ) = E[X0] = 1 since P is a probability measure and F0 = {∅,Σ∞} is the trivial σ-algebra. Let u ∈ Σt.∑ a∈Σ q(ua) = ∑ a∈Σ Xt+1(ua)P (Γua) = ∫ Γu Xt+1dP\n= ∫ Γu E[Xt+1 | Ft]dP = ∫ Γu XtdP = P (Γu)Xt(u) = q(u).\nBy Lemma A.1, there is a probability measure Q on (Σ∞,F∞) such that q(u) = Q(Γu) of all u ∈ Σ∗. Therefore, for all v ∈ Σ∞ and for all t ∈ N with P (Γv1:t) > 0,\nXt(v) = q(v1:t) P (Γv1:t) = Q(Γv1:t) P (Γv1:t) .\nMoreover, P is locally absolutely continuous with respect to Q since P (Γu) = 0 implies\nQ(Γu) = q(u) = X|u|(u)P (Γu) = 0."
    } ],
    "references" : [ {
      "title" : "Analysis of Thompson sampling for the multi-armed bandit problem",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Agrawal and Goyal.,? \\Q2011\\E",
      "shortCiteRegEx" : "Agrawal and Goyal.",
      "year" : 2011
    }, {
      "title" : "AI researchers on AI risk",
      "author" : [ "Scott Alexander" ],
      "venue" : "http://slatestarcodex.com/2015/05/ 22/ai-researchers-on-ai-risk/,",
      "citeRegEx" : "Alexander.,? \\Q2015\\E",
      "shortCiteRegEx" : "Alexander.",
      "year" : 2015
    }, {
      "title" : "Interruptibility and corrigibility for AIXI and Monte Carlo agents. 2016",
      "author" : [ "Stuart Armstrong", "Laurent Orseau" ],
      "venue" : null,
      "citeRegEx" : "Armstrong and Orseau.,? \\Q2016\\E",
      "shortCiteRegEx" : "Armstrong and Orseau.",
      "year" : 2016
    }, {
      "title" : "Bayesianism, infinite decisions, and binding",
      "author" : [ "Frank Arntzenius", "Adam Elga", "John Hawthorne" ],
      "venue" : "Mind, 113(450):251–283,",
      "citeRegEx" : "Arntzenius et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Arntzenius et al\\.",
      "year" : 2004
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Peter Auer", "Thomas Jaksch", "Ronald Ortner" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Auer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2009
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Peter Auer", "Thomas Jaksch", "Ronald Ortner" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Auer et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2010
    }, {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "Leemon Baird" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Baird.,? \\Q1995\\E",
      "shortCiteRegEx" : "Baird.",
      "year" : 1995
    }, {
      "title" : "Universal upper bound on the entropy-to-energy ratio for bounded systems",
      "author" : [ "Jacob D Bekenstein" ],
      "venue" : "Physical Review D,",
      "citeRegEx" : "Bekenstein.,? \\Q1981\\E",
      "shortCiteRegEx" : "Bekenstein.",
      "year" : 1981
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Increasing the action gap: New operators for reinforcement learning",
      "author" : [ "Marc G Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S Thomas", "Rémi Munos" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Dynamic Programming and Optimal Control",
      "author" : [ "Dimitri P Bertsekas", "John Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas and Tsitsiklis.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis.",
      "year" : 1995
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "Christopher M Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 2006
    }, {
      "title" : "Merging of opinions with increasing information",
      "author" : [ "David Blackwell", "Lester Dubins" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Blackwell and Dubins.,? \\Q1962\\E",
      "shortCiteRegEx" : "Blackwell and Dubins.",
      "year" : 1962
    }, {
      "title" : "Existential risks",
      "author" : [ "Nick Bostrom" ],
      "venue" : "Journal of Evolution and Technology,",
      "citeRegEx" : "Bostrom.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bostrom.",
      "year" : 2002
    }, {
      "title" : "Ethical issues in advanced artificial intelligence. Science Fiction and Philosophy: From Time Travel to Superintelligence",
      "author" : [ "Nick Bostrom" ],
      "venue" : null,
      "citeRegEx" : "Bostrom.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bostrom.",
      "year" : 2003
    }, {
      "title" : "Superintelligence: Paths, Dangers, Strategies",
      "author" : [ "Nick Bostrom" ],
      "venue" : null,
      "citeRegEx" : "Bostrom.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bostrom.",
      "year" : 2014
    }, {
      "title" : "Strategic implications of openness in AI development",
      "author" : [ "Nick Bostrom" ],
      "venue" : "Technical report, Future of Humanity Institute,",
      "citeRegEx" : "Bostrom.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bostrom.",
      "year" : 2016
    }, {
      "title" : "Rational and convergent learning in stochastic games",
      "author" : [ "Michael Bowling", "Manuela Veloso" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Bowling and Veloso.,? \\Q2001\\E",
      "shortCiteRegEx" : "Bowling and Veloso.",
      "year" : 2001
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Cesa-Nicolò Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Bianchi.",
      "year" : 2012
    }, {
      "title" : "A comprehensive survey of multiagent reinforcement learning",
      "author" : [ "Lucian Busoniu", "Robert Babuska", "Bart De Schutter" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews,",
      "citeRegEx" : "Busoniu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Busoniu et al\\.",
      "year" : 2008
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "The singularity: A philosophical analysis",
      "author" : [ "David Chalmers" ],
      "venue" : "Journal of Consciousness Studies,",
      "citeRegEx" : "Chalmers.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chalmers.",
      "year" : 2010
    }, {
      "title" : "An empirical evaluation of Thompson sampling",
      "author" : [ "Olivier Chapelle", "Lihong Li" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chapelle and Li.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle and Li.",
      "year" : 2011
    }, {
      "title" : "Definability of truth in probabilistic logic",
      "author" : [ "Paul Christiano", "Eliezer Yudkowsky", "Marcello Herreshoff", "Mihaly Barasz" ],
      "venue" : "Technical report, Machine Intelligence Research Institute,",
      "citeRegEx" : "Christiano et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Christiano et al\\.",
      "year" : 2013
    }, {
      "title" : "Elements of Information Theory",
      "author" : [ "Thomas M Cover", "Joy A Thomas" ],
      "venue" : null,
      "citeRegEx" : "Cover and Thomas.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cover and Thomas.",
      "year" : 2006
    }, {
      "title" : "Generic Reinforcement Learning Beyond Small MDPs",
      "author" : [ "Mayank Daswani" ],
      "venue" : "PhD thesis, Australian National University,",
      "citeRegEx" : "Daswani.,? \\Q2015\\E",
      "shortCiteRegEx" : "Daswani.",
      "year" : 2015
    }, {
      "title" : "A definition of happiness for reinforcement learning agents",
      "author" : [ "Mayank Daswani", "Jan Leike" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Daswani and Leike.,? \\Q2015\\E",
      "shortCiteRegEx" : "Daswani and Leike.",
      "year" : 2015
    }, {
      "title" : "Ethical guidelines for a superintelligence",
      "author" : [ "Ernest Davis" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Davis.,? \\Q2014\\E",
      "shortCiteRegEx" : "Davis.",
      "year" : 2014
    }, {
      "title" : "Increasing the gap between descriptional complexity and algorithmic probability",
      "author" : [ "Adam Day" ],
      "venue" : "Transactions of the American Mathematical Society,",
      "citeRegEx" : "Day.,? \\Q2011\\E",
      "shortCiteRegEx" : "Day.",
      "year" : 2011
    }, {
      "title" : "Bayesian Q-learning",
      "author" : [ "Richard Dearden", "Nir Friedman", "Stuart Russell" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Dearden et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Dearden et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning what to value",
      "author" : [ "Daniel Dewey" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Dewey.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dewey.",
      "year" : 2011
    }, {
      "title" : "Bayesian Nonparametric Approaches for Reinforcement Learning in Partially Observable Domains",
      "author" : [ "Finale Doshi-Velez" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "Doshi.Velez.,? \\Q2012\\E",
      "shortCiteRegEx" : "Doshi.Velez.",
      "year" : 2012
    }, {
      "title" : "Probability: Theory and Examples",
      "author" : [ "Rick Durrett" ],
      "venue" : null,
      "citeRegEx" : "Durrett.,? \\Q2010\\E",
      "shortCiteRegEx" : "Durrett.",
      "year" : 2010
    }, {
      "title" : "The singularity controversy",
      "author" : [ "Amnon H Eden" ],
      "venue" : "Technical report, Sapience Project,",
      "citeRegEx" : "Eden.,? \\Q2016\\E",
      "shortCiteRegEx" : "Eden.",
      "year" : 2016
    }, {
      "title" : "Singularity Hypotheses: A Scientific and Philosophical Assessment",
      "author" : [ "Amnon H Eden", "James H Moor", "Johnny H Søraker", "Eric Steinhart", "editors" ],
      "venue" : null,
      "citeRegEx" : "Eden et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Eden et al\\.",
      "year" : 2013
    }, {
      "title" : "Avoiding wireheading with value reinforcement learning",
      "author" : [ "Tom Everitt", "Marcus Hutter" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Everitt and Hutter.,? \\Q2016\\E",
      "shortCiteRegEx" : "Everitt and Hutter.",
      "year" : 2016
    }, {
      "title" : "Sequential extensions of causal and evidential decision theory",
      "author" : [ "Tom Everitt", "Jan Leike", "Marcus Hutter" ],
      "venue" : "In Algorithmic Decision Theory,",
      "citeRegEx" : "Everitt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Everitt et al\\.",
      "year" : 2015
    }, {
      "title" : "Self-modification of policy and utility function in rational agents",
      "author" : [ "Tom Everitt", "Daniel Filan", "Mayank Daswani", "Marcus Hutter" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Everitt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Everitt et al\\.",
      "year" : 2016
    }, {
      "title" : "Reflective variants of Solomonoff induction and AIXI",
      "author" : [ "Benja Fallenstein", "Nate Soares", "Jessica Taylor" ],
      "venue" : "In Artificial General Intelligence. Springer,",
      "citeRegEx" : "Fallenstein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fallenstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Reflective oracles: A foundation for game theory in artificial intelligence",
      "author" : [ "Benja Fallenstein", "Jessica Taylor", "Paul F Christiano" ],
      "venue" : "In Logic, Rationality, and Interaction,",
      "citeRegEx" : "Fallenstein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fallenstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Reflective oracles: A foundation for classical game theory",
      "author" : [ "Benja Fallenstein", "Jessica Taylor", "Paul F Christiano" ],
      "venue" : "Technical report, Machine Intelligence Research Institute,",
      "citeRegEx" : "Fallenstein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fallenstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Agents using speed priors",
      "author" : [ "Daniel Filan" ],
      "venue" : "Honours thesis,",
      "citeRegEx" : "Filan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Filan.",
      "year" : 2015
    }, {
      "title" : "Loss bounds and time complexity for speed priors",
      "author" : [ "Daniel Filan", "Jan Leike", "Marcus Hutter" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Filan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Filan et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to communicate to solve riddles with deep distributed recurrent Q-networks",
      "author" : [ "Jakob N Foerster", "Yannis M Assael", "Nando de Freitas", "Shimon Whiteson" ],
      "venue" : "Technical report, University of Oxford,",
      "citeRegEx" : "Foerster et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "On the impossibility of predicting the behavior of rational agents",
      "author" : [ "Dean P Foster", "H Peyton Young" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Foster and Young.,? \\Q2001\\E",
      "shortCiteRegEx" : "Foster and Young.",
      "year" : 2001
    }, {
      "title" : "Bandit processes and dynamic allocation indices",
      "author" : [ "John Gittins" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Gittins.,? \\Q1979\\E",
      "shortCiteRegEx" : "Gittins.",
      "year" : 1979
    }, {
      "title" : "The paradox of confirmation",
      "author" : [ "Irving John Good" ],
      "venue" : "British Journal for the Philosophy of Science,",
      "citeRegEx" : "Good.,? \\Q1960\\E",
      "shortCiteRegEx" : "Good.",
      "year" : 1960
    }, {
      "title" : "Speculations concerning the first ultraintelligent machine",
      "author" : [ "Irving John Good" ],
      "venue" : "Advances in Computers,",
      "citeRegEx" : "Good.,? \\Q1965\\E",
      "shortCiteRegEx" : "Good.",
      "year" : 1965
    }, {
      "title" : "The white shoe is a red herring",
      "author" : [ "Irving John Good" ],
      "venue" : "The British Journal for the Philosophy of Science,",
      "citeRegEx" : "Good.,? \\Q1967\\E",
      "shortCiteRegEx" : "Good.",
      "year" : 1967
    }, {
      "title" : "Deep learning. Book in preparation for MIT Press, http://www.deeplearningbook.org, 2016",
      "author" : [ "Ian Goodfellow", "Yoshua Bengio", "Aaron Courville" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2016
    }, {
      "title" : "Thompson sampling for learning parameterized Markov decision processes",
      "author" : [ "Aditya Gopalan", "Shie Mannor" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Gopalan and Mannor.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gopalan and Mannor.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning with function approximation converges to a region",
      "author" : [ "Geoffrey J Gordon" ],
      "venue" : "In Advanced in Neural Information Processing Systems,",
      "citeRegEx" : "Gordon.,? \\Q2001\\E",
      "shortCiteRegEx" : "Gordon.",
      "year" : 2001
    }, {
      "title" : "The Minimum Description Length Principle",
      "author" : [ "Peter D. Grünwald" ],
      "venue" : null,
      "citeRegEx" : "Grünwald.,? \\Q2007\\E",
      "shortCiteRegEx" : "Grünwald.",
      "year" : 2007
    }, {
      "title" : "On the relation between descriptional complexity and algorithmic probability",
      "author" : [ "Péter Gács" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Gács.,? \\Q1983\\E",
      "shortCiteRegEx" : "Gács.",
      "year" : 1983
    }, {
      "title" : "Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I",
      "author" : [ "Kurt Gödel" ],
      "venue" : "Monatshefte für Mathematik und Physik,",
      "citeRegEx" : "Gödel.,? \\Q1931\\E",
      "shortCiteRegEx" : "Gödel.",
      "year" : 1931
    }, {
      "title" : "Deep recurrent Q-learning for partially observable MDPs",
      "author" : [ "Matthew Hausknecht", "Peter Stone" ],
      "venue" : "In 2015 AAAI Fall Symposium Series,",
      "citeRegEx" : "Hausknecht and Stone.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hausknecht and Stone.",
      "year" : 2015
    }, {
      "title" : "Transcending complacency on superintelligent machines. http://www.huffingtonpost.com/ stephen-hawking/artificial-intelligence_b_5174265.html",
      "author" : [ "Stephen Hawking", "Max Tegmark", "Stuart Russell", "Frank Wilczek" ],
      "venue" : null,
      "citeRegEx" : "Hawking et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hawking et al\\.",
      "year" : 2014
    }, {
      "title" : "Universal semimeasures: An introduction",
      "author" : [ "Nicholas J Hay" ],
      "venue" : "Master’s thesis, University of Auckland,",
      "citeRegEx" : "Hay.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hay.",
      "year" : 2007
    }, {
      "title" : "Memory-based control with recurrent neural networks",
      "author" : [ "Nicolas Heess", "Jonathan J Hunt", "Timothy P Lillicrap", "David Silver" ],
      "venue" : "Technical report, Google DeepMind,",
      "citeRegEx" : "Heess et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Heess et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep reinforcement learning from self-play in imperfect-information games",
      "author" : [ "Johannes Heinrich", "David Silver" ],
      "venue" : "Technical report, DeepMind,",
      "citeRegEx" : "Heinrich and Silver.,? \\Q2016\\E",
      "shortCiteRegEx" : "Heinrich and Silver.",
      "year" : 2016
    }, {
      "title" : "Ultimate Automizer with array interpolation (competition contribution)",
      "author" : [ "Matthias Heizmann", "Daniel Dietsch", "Jan Leike", "Betim Musa", "Andreas Podelski" ],
      "venue" : "In Tools and Algorithms for the Construction and Analysis of Systems,",
      "citeRegEx" : "Heizmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Heizmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Ultimate Automizer with two-track proofs (competition contribution)",
      "author" : [ "Matthias Heizmann", "Daniel Dietsch", "Marius Greitschus", "Jan Leike", "Betim Musa", "Claus Schätzle", "Andreas Podelski" ],
      "venue" : "In Tools and Algorithms for the Construction and Analysis of Systems,",
      "citeRegEx" : "Heizmann et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Heizmann et al\\.",
      "year" : 2016
    }, {
      "title" : "Studies in the logic of confirmation (I.)",
      "author" : [ "Carl G Hempel" ],
      "venue" : "Mind, pages 1–26,",
      "citeRegEx" : "Hempel.,? \\Q1945\\E",
      "shortCiteRegEx" : "Hempel.",
      "year" : 1945
    }, {
      "title" : "The white shoe: No red herring",
      "author" : [ "Carl G Hempel" ],
      "venue" : "The British Journal for the Philosophy of Science,",
      "citeRegEx" : "Hempel.,? \\Q1967\\E",
      "shortCiteRegEx" : "Hempel.",
      "year" : 1967
    }, {
      "title" : "A theory of universal artificial intelligence based on algorithmic complexity",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Hutter.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2000
    }, {
      "title" : "Universal sequential decisions in unknown environments",
      "author" : [ "Marcus Hutter" ],
      "venue" : "In European Workshop on Reinforcement Learning,",
      "citeRegEx" : "Hutter.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2001
    }, {
      "title" : "New error bounds for Solomonoff prediction",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Hutter.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2001
    }, {
      "title" : "Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures",
      "author" : [ "Marcus Hutter" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "Hutter.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2002
    }, {
      "title" : "The fastest and shortest algorithm for all well-defined problems",
      "author" : [ "Marcus Hutter" ],
      "venue" : "International Journal of Foundations of Computer Science,",
      "citeRegEx" : "Hutter.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2002
    }, {
      "title" : "A gentle introduction to the universal algorithmic agent AIXI",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Technical report, IDSIA,",
      "citeRegEx" : "Hutter.,? \\Q2003\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2003
    }, {
      "title" : "Sequential predictions based on algorithmic complexity",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Hutter.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2006
    }, {
      "title" : "General discounting versus average reward. In Algorithmic Learning Theory, pages 244–258",
      "author" : [ "Marcus Hutter" ],
      "venue" : null,
      "citeRegEx" : "Hutter.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2006
    }, {
      "title" : "50’000€ prize for compressing human knowledge",
      "author" : [ "Marcus Hutter" ],
      "venue" : "http://prize. hutter1.net/,",
      "citeRegEx" : "Hutter.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2006
    }, {
      "title" : "Universal algorithmic intelligence: A mathematical top→down approach",
      "author" : [ "Marcus Hutter" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Hutter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2007
    }, {
      "title" : "On universal prediction and Bayesian confirmation",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Hutter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2007
    }, {
      "title" : "Discrete MDL predicts in total variation",
      "author" : [ "Marcus Hutter" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hutter.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2009
    }, {
      "title" : "Open problems in universal induction & intelligence",
      "author" : [ "Marcus Hutter" ],
      "venue" : null,
      "citeRegEx" : "Hutter.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2009
    }, {
      "title" : "Feature dynamic Bayesian networks",
      "author" : [ "Marcus Hutter" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Hutter.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2009
    }, {
      "title" : "Feature reinforcement learning: Part I: Unstructured MDPs",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Journal of Artificial General Intelligence,",
      "citeRegEx" : "Hutter.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2009
    }, {
      "title" : "Can intelligence explode",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Journal of Consciousness Studies,",
      "citeRegEx" : "Hutter.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2012
    }, {
      "title" : "One decade of universal artificial intelligence",
      "author" : [ "Marcus Hutter" ],
      "venue" : "In Theoretical Foundations of Artificial General Intelligence,",
      "citeRegEx" : "Hutter.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2012
    }, {
      "title" : "Extreme state aggregation beyond MDPs",
      "author" : [ "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory. Springer,",
      "citeRegEx" : "Hutter.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2014
    }, {
      "title" : "On semimeasures predicting Martin-Löf random sequences",
      "author" : [ "Marcus Hutter", "Andrej A. Muchnik" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Hutter and Muchnik.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hutter and Muchnik.",
      "year" : 2007
    }, {
      "title" : "Probability Theory: The Logic of Science",
      "author" : [ "Edwin T Jaynes" ],
      "venue" : null,
      "citeRegEx" : "Jaynes.,? \\Q2003\\E",
      "shortCiteRegEx" : "Jaynes.",
      "year" : 2003
    }, {
      "title" : "On the Sample Complexity of Reinforcement Learning",
      "author" : [ "Sham Machandranath Kakade" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Kakade.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kakade.",
      "year" : 2003
    }, {
      "title" : "Rational learning leads to Nash equilibrium",
      "author" : [ "Ehud Kalai", "Ehud Lehrer" ],
      "venue" : "Econometrica, pages 1019–1045,",
      "citeRegEx" : "Kalai and Lehrer.,? \\Q1993\\E",
      "shortCiteRegEx" : "Kalai and Lehrer.",
      "year" : 1993
    }, {
      "title" : "Weak and strong merging of opinions",
      "author" : [ "Ehud Kalai", "Ehud Lehrer" ],
      "venue" : "Journal of Mathematical Economics,",
      "citeRegEx" : "Kalai and Lehrer.,? \\Q1994\\E",
      "shortCiteRegEx" : "Kalai and Lehrer.",
      "year" : 1994
    }, {
      "title" : "Thompson sampling: An asymptotically optimal finite-time analysis",
      "author" : [ "Emilie Kaufmann", "Nathaniel Korda", "Rémi Munos" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2012
    }, {
      "title" : "Autonomous helicopter flight via reinforcement learning",
      "author" : [ "HJ Kim", "Michael I Jordan", "Shankar Sastry", "Andrew Y Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems, page None,",
      "citeRegEx" : "Kim et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2003
    }, {
      "title" : "Introduction to Metamathematics",
      "author" : [ "Stephen Cole Kleene" ],
      "venue" : "Wolters-Noordhoff Publishing,",
      "citeRegEx" : "Kleene.,? \\Q1952\\E",
      "shortCiteRegEx" : "Kleene.",
      "year" : 1952
    }, {
      "title" : "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
      "author" : [ "Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum" ],
      "venue" : "Technical report, Massachusetts Institute of Technology,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2016
    }, {
      "title" : "The Singularity is Near: When Humans Transcend Biology",
      "author" : [ "Ray Kurzweil" ],
      "venue" : "Viking Books,",
      "citeRegEx" : "Kurzweil.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kurzweil.",
      "year" : 2005
    }, {
      "title" : "Theory of General Reinforcement Learning",
      "author" : [ "Tor Lattimore" ],
      "venue" : "PhD thesis, Australian National University,",
      "citeRegEx" : "Lattimore.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lattimore.",
      "year" : 2013
    }, {
      "title" : "Regret analysis of the finite-horizon Gittins index strategy for multiarmed bandits",
      "author" : [ "Tor Lattimore" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Lattimore.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lattimore.",
      "year" : 2016
    }, {
      "title" : "Asymptotically optimal agents",
      "author" : [ "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Lattimore and Hutter.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lattimore and Hutter.",
      "year" : 2011
    }, {
      "title" : "PAC bounds for discounted MDPs",
      "author" : [ "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Lattimore and Hutter.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lattimore and Hutter.",
      "year" : 2012
    }, {
      "title" : "On Martin-Löf convergence of Solomonoff’s mixture",
      "author" : [ "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "In Theory and Applications of Models of Computation,",
      "citeRegEx" : "Lattimore and Hutter.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lattimore and Hutter.",
      "year" : 2013
    }, {
      "title" : "General time consistent discounting",
      "author" : [ "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Lattimore and Hutter.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lattimore and Hutter.",
      "year" : 2014
    }, {
      "title" : "On Martin-Löf (non-)convergence of Solomonoff’s universal mixture",
      "author" : [ "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Lattimore and Hutter.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lattimore and Hutter.",
      "year" : 2015
    }, {
      "title" : "Universal prediction of selected bits",
      "author" : [ "Tor Lattimore", "Marcus Hutter", "Vaibhav Gavane" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Lattimore et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lattimore et al\\.",
      "year" : 2011
    }, {
      "title" : "Is there an elegant universal theory of prediction",
      "author" : [ "Shane Legg" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Legg.,? \\Q2006\\E",
      "shortCiteRegEx" : "Legg.",
      "year" : 2006
    }, {
      "title" : "Machine Super Intelligence",
      "author" : [ "Shane Legg" ],
      "venue" : "PhD thesis, University of Lugano,",
      "citeRegEx" : "Legg.,? \\Q2008\\E",
      "shortCiteRegEx" : "Legg.",
      "year" : 2008
    }, {
      "title" : "A collection of definitions of intelligence",
      "author" : [ "Shane Legg", "Marcus Hutter" ],
      "venue" : "Frontiers in Artificial Intelligence and Applications,",
      "citeRegEx" : "Legg and Hutter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Legg and Hutter.",
      "year" : 2007
    }, {
      "title" : "Universal intelligence: A definition of machine intelligence",
      "author" : [ "Shane Legg", "Marcus Hutter" ],
      "venue" : "Minds & Machines,",
      "citeRegEx" : "Legg and Hutter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Legg and Hutter.",
      "year" : 2007
    }, {
      "title" : "An approximation of the universal intelligence measure. In Algorithmic Probability and Friends",
      "author" : [ "Shane Legg", "Joel Veness" ],
      "venue" : "Bayesian Prediction and Artificial Intelligence,",
      "citeRegEx" : "Legg and Veness.,? \\Q2013\\E",
      "shortCiteRegEx" : "Legg and Veness.",
      "year" : 2013
    }, {
      "title" : "Merging and learning. Statistics, Probability and Game Theory, pages",
      "author" : [ "Ehud Lehrer", "Rann Smorodinsky" ],
      "venue" : null,
      "citeRegEx" : "Lehrer and Smorodinsky.,? \\Q1996\\E",
      "shortCiteRegEx" : "Lehrer and Smorodinsky.",
      "year" : 1996
    }, {
      "title" : "Ranking templates for linear loops. In Tools and Algorithms for the Construction and Analysis of Systems, pages 172–186",
      "author" : [ "Jan Leike", "Matthias Heizmann" ],
      "venue" : null,
      "citeRegEx" : "Leike and Heizmann.,? \\Q2014\\E",
      "shortCiteRegEx" : "Leike and Heizmann.",
      "year" : 2014
    }, {
      "title" : "Geometric series as nontermination arguments for linear lasso programs",
      "author" : [ "Jan Leike", "Matthias Heizmann" ],
      "venue" : "Technical report, University of Freiburg,",
      "citeRegEx" : "Leike and Heizmann.,? \\Q2014\\E",
      "shortCiteRegEx" : "Leike and Heizmann.",
      "year" : 2014
    }, {
      "title" : "Ranking templates for linear loops",
      "author" : [ "Jan Leike", "Matthias Heizmann" ],
      "venue" : "Logical Methods in Computer Science,",
      "citeRegEx" : "Leike and Heizmann.,? \\Q2015\\E",
      "shortCiteRegEx" : "Leike and Heizmann.",
      "year" : 2015
    }, {
      "title" : "Geometric nontermination arguments. 2016",
      "author" : [ "Jan Leike", "Matthias Heizmann" ],
      "venue" : "Under preparation",
      "citeRegEx" : "Leike and Heizmann.,? \\Q2016\\E",
      "shortCiteRegEx" : "Leike and Heizmann.",
      "year" : 2016
    }, {
      "title" : "Indefinitely oscillating martingales",
      "author" : [ "Jan Leike", "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory, pages 321–335,",
      "citeRegEx" : "Leike and Hutter.,? \\Q2014\\E",
      "shortCiteRegEx" : "Leike and Hutter.",
      "year" : 2014
    }, {
      "title" : "Indefinitely oscillating martingales",
      "author" : [ "Jan Leike", "Marcus Hutter" ],
      "venue" : "Technical report, Australian National University,",
      "citeRegEx" : "Leike and Hutter.,? \\Q2014\\E",
      "shortCiteRegEx" : "Leike and Hutter.",
      "year" : 2014
    }, {
      "title" : "On the computability of AIXI",
      "author" : [ "Jan Leike", "Marcus Hutter" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Leike and Hutter.,? \\Q2015\\E",
      "shortCiteRegEx" : "Leike and Hutter.",
      "year" : 2015
    }, {
      "title" : "On the computability of Solomonoff induction and knowledge-seeking",
      "author" : [ "Jan Leike", "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Leike and Hutter.,? \\Q2015\\E",
      "shortCiteRegEx" : "Leike and Hutter.",
      "year" : 2015
    }, {
      "title" : "Bad universal priors and notions of optimality",
      "author" : [ "Jan Leike", "Marcus Hutter" ],
      "venue" : "In Conference on Learning Theory, pages 1244–1259,",
      "citeRegEx" : "Leike and Hutter.,? \\Q2015\\E",
      "shortCiteRegEx" : "Leike and Hutter.",
      "year" : 2015
    }, {
      "title" : "Solomonoff induction violates Nicod’s criterion",
      "author" : [ "Jan Leike", "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Leike and Hutter.,? \\Q2015\\E",
      "shortCiteRegEx" : "Leike and Hutter.",
      "year" : 2015
    }, {
      "title" : "On the computability of Solomonoff induction and AIXI",
      "author" : [ "Jan Leike", "Marcus Hutter" ],
      "venue" : null,
      "citeRegEx" : "Leike and Hutter.,? \\Q2016\\E",
      "shortCiteRegEx" : "Leike and Hutter.",
      "year" : 2016
    }, {
      "title" : "Thompson sampling is asymptotically optimal in general environments",
      "author" : [ "Jan Leike", "Tor Lattimore", "Laurent Orseau", "Marcus Hutter" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Leike et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Leike et al\\.",
      "year" : 2016
    }, {
      "title" : "A formal solution to the grain of truth problem",
      "author" : [ "Jan Leike", "Jessica Taylor", "Benya Fallenstein" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Leike et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Leike et al\\.",
      "year" : 2016
    }, {
      "title" : "On the notion of a random sequence",
      "author" : [ "Leonid A Levin" ],
      "venue" : "Soviet Mathematics Doklady,",
      "citeRegEx" : "Levin.,? \\Q1973\\E",
      "shortCiteRegEx" : "Levin.",
      "year" : 1973
    }, {
      "title" : "An Introduction to Kolmogorov Complexity and Its Applications",
      "author" : [ "Ming Li", "Paul M.B. Vitányi" ],
      "venue" : "Texts in Computer Science. Springer,",
      "citeRegEx" : "Li and Vitányi.,? \\Q2008\\E",
      "shortCiteRegEx" : "Li and Vitányi.",
      "year" : 2008
    }, {
      "title" : "State of the art control of Atari games using shallow reinforcement learning",
      "author" : [ "Yitao Liang", "Marlos C Machado", "Erik Talvitie", "Michael Bowling" ],
      "venue" : "In Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Liang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2016
    }, {
      "title" : "The paradox of confirmation",
      "author" : [ "John L Mackie" ],
      "venue" : "British Journal for the Philosophy of Science,",
      "citeRegEx" : "Mackie.,? \\Q1963\\E",
      "shortCiteRegEx" : "Mackie.",
      "year" : 1963
    }, {
      "title" : "On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems",
      "author" : [ "Omid Madani", "Steve Hanks", "Anne Condon" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Madani et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Madani et al\\.",
      "year" : 1999
    }, {
      "title" : "On the undecidability of probabilistic planning and related stochastic optimization problems",
      "author" : [ "Omid Madani", "Steve Hanks", "Anne Condon" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Madani et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Madani et al\\.",
      "year" : 2003
    }, {
      "title" : "Optimality criteria in reinforcement learning",
      "author" : [ "Sridhar Mahadevan" ],
      "venue" : "In AAAI Fall Symposium on Learning Complex Behaviors in Adaptive Intelligent Systems,",
      "citeRegEx" : "Mahadevan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Mahadevan.",
      "year" : 1996
    }, {
      "title" : "Inductive logic and the ravens paradox",
      "author" : [ "Patrick Maher" ],
      "venue" : "Philosophy of Science,",
      "citeRegEx" : "Maher.,? \\Q1999\\E",
      "shortCiteRegEx" : "Maher.",
      "year" : 1999
    }, {
      "title" : "Emphatic temporal-difference learning",
      "author" : [ "A Rupam Mahmood", "Huizhen Yu", "Martha White", "Richard Sutton" ],
      "venue" : "Technical report, University of Alberta,",
      "citeRegEx" : "Mahmood et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mahmood et al\\.",
      "year" : 2015
    }, {
      "title" : "The maximum speed of dynamical evolution",
      "author" : [ "Norman Margolus", "Lev B Levitin" ],
      "venue" : "Physica D: Nonlinear Phenomena,",
      "citeRegEx" : "Margolus and Levitin.,? \\Q1998\\E",
      "shortCiteRegEx" : "Margolus and Levitin.",
      "year" : 1998
    }, {
      "title" : "Death and suicide in universal artificial intelligence",
      "author" : [ "Jarryd Martin", "Tom Everitt", "Marcus Hutter" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Martin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2016
    }, {
      "title" : "A proposal for the Dartmouth summer research project on artificial intelligence",
      "author" : [ "John McCarthy", "Marvin Minsky", "Nathaniel Rochester", "Claude Shannon" ],
      "venue" : null,
      "citeRegEx" : "McCarthy et al\\.,? \\Q1955\\E",
      "shortCiteRegEx" : "McCarthy et al\\.",
      "year" : 1955
    }, {
      "title" : "The role of absolute continuity in “merging of opinions” and “rational learning",
      "author" : [ "Ronald I Miller", "Chris William Sanchirico" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Miller and Sanchirico.,? \\Q1999\\E",
      "shortCiteRegEx" : "Miller and Sanchirico.",
      "year" : 1999
    }, {
      "title" : "Playing Atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "Technical report, Google DeepMind,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adrià Puigdomènech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Intelligence explosion: Evidence and import",
      "author" : [ "Luke Muehlhauser", "Anna Salamon" ],
      "venue" : "In Singularity Hypotheses,",
      "citeRegEx" : "Muehlhauser and Salamon.,? \\Q2012\\E",
      "shortCiteRegEx" : "Muehlhauser and Salamon.",
      "year" : 2012
    }, {
      "title" : "Complexity of finite-horizon Markov decision process problems",
      "author" : [ "Martin Mundhenk", "Judy Goldsmith", "Christopher Lusena", "Eric Allender" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Mundhenk et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Mundhenk et al\\.",
      "year" : 2000
    }, {
      "title" : "Stationary algorithmic probability",
      "author" : [ "Markus Müller" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Müller.,? \\Q2010\\E",
      "shortCiteRegEx" : "Müller.",
      "year" : 2010
    }, {
      "title" : "Future progress in artificial intelligence: A survey of expert opinion",
      "author" : [ "Vincent C Müller", "Nick Bostrom" ],
      "venue" : "Fundamental Issues of Artificial Intelligence,",
      "citeRegEx" : "Müller and Bostrom.,? \\Q2016\\E",
      "shortCiteRegEx" : "Müller and Bostrom.",
      "year" : 2016
    }, {
      "title" : "Prediction, optimization, and learning",
      "author" : [ "John H Nachbar" ],
      "venue" : "in repeated games. Econometrica,",
      "citeRegEx" : "Nachbar.,? \\Q1997\\E",
      "shortCiteRegEx" : "Nachbar.",
      "year" : 1997
    }, {
      "title" : "Massively parallel methods for deep reinforcement learning",
      "author" : [ "Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver" ],
      "venue" : "Technical report, Google DeepMind,",
      "citeRegEx" : "Nair et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2015
    }, {
      "title" : "Is A.I. an existential threat to humanity? https://www.quora.com/ Is-A-I-an-existential-threat-to-humanity/answer/Andrew-Ng, January 2016",
      "author" : [ "Andrew Ng" ],
      "venue" : null,
      "citeRegEx" : "Ng.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ng.",
      "year" : 2016
    }, {
      "title" : "Competing with an infinite set of models in reinforcement learning",
      "author" : [ "Phuong Nguyen", "Odalric-Ambrym Maillard", "Daniil Ryabko", "Ronald Ortner" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2013
    }, {
      "title" : "Le Problème Logique de L’Induction",
      "author" : [ "Jean Nicod" ],
      "venue" : "Presses Universitaires de France,",
      "citeRegEx" : "Nicod.,? \\Q1961\\E",
      "shortCiteRegEx" : "Nicod.",
      "year" : 1961
    }, {
      "title" : "Computability and Randomness",
      "author" : [ "André Nies" ],
      "venue" : null,
      "citeRegEx" : "Nies.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nies.",
      "year" : 2009
    }, {
      "title" : "Positive reinforcement produced by electrical stimulation of septal area and other regions of rat brain",
      "author" : [ "James Olds", "Peter Milner" ],
      "venue" : "Journal of Comparative and Physiological Psychology,",
      "citeRegEx" : "Olds and Milner.,? \\Q1954\\E",
      "shortCiteRegEx" : "Olds and Milner.",
      "year" : 1954
    }, {
      "title" : "The basic AI drives",
      "author" : [ "Stephen M Omohundro" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Omohundro.,? \\Q2008\\E",
      "shortCiteRegEx" : "Omohundro.",
      "year" : 2008
    }, {
      "title" : "Optimality issues of universal greedy agents with static priors",
      "author" : [ "Laurent Orseau" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Orseau.,? \\Q2010\\E",
      "shortCiteRegEx" : "Orseau.",
      "year" : 2010
    }, {
      "title" : "Universal knowledge-seeking agents",
      "author" : [ "Laurent Orseau" ],
      "venue" : null,
      "citeRegEx" : "Orseau.,? \\Q2011\\E",
      "shortCiteRegEx" : "Orseau.",
      "year" : 2011
    }, {
      "title" : "Asymptotic non-learnability of universal agents with computable horizon functions",
      "author" : [ "Laurent Orseau" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Orseau.,? \\Q2013\\E",
      "shortCiteRegEx" : "Orseau.",
      "year" : 2013
    }, {
      "title" : "Universal knowledge-seeking agents",
      "author" : [ "Laurent Orseau" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Orseau.,? \\Q2014\\E",
      "shortCiteRegEx" : "Orseau.",
      "year" : 2014
    }, {
      "title" : "The multi-slot framework: A formal model for multiple, copiable AIs",
      "author" : [ "Laurent Orseau" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Orseau.,? \\Q2014\\E",
      "shortCiteRegEx" : "Orseau.",
      "year" : 2014
    }, {
      "title" : "Teleporting universal intelligent agents",
      "author" : [ "Laurent Orseau" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Orseau.,? \\Q2014\\E",
      "shortCiteRegEx" : "Orseau.",
      "year" : 2014
    }, {
      "title" : "Safely interruptible agents",
      "author" : [ "Laurent Orseau", "Stuart Armstrong" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Orseau and Armstrong.,? \\Q2016\\E",
      "shortCiteRegEx" : "Orseau and Armstrong.",
      "year" : 2016
    }, {
      "title" : "Self-modification and mortality in artificial agents",
      "author" : [ "Laurent Orseau", "Mark Ring" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Orseau and Ring.,? \\Q2011\\E",
      "shortCiteRegEx" : "Orseau and Ring.",
      "year" : 2011
    }, {
      "title" : "Space-time embedded intelligence",
      "author" : [ "Laurent Orseau", "Mark Ring" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Orseau and Ring.,? \\Q2012\\E",
      "shortCiteRegEx" : "Orseau and Ring.",
      "year" : 2012
    }, {
      "title" : "Memory issues of intelligent agents",
      "author" : [ "Laurent Orseau", "Mark Ring" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Orseau and Ring.,? \\Q2012\\E",
      "shortCiteRegEx" : "Orseau and Ring.",
      "year" : 2012
    }, {
      "title" : "Universal knowledge-seeking agents for stochastic environments",
      "author" : [ "Laurent Orseau", "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Orseau et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Orseau et al\\.",
      "year" : 2013
    }, {
      "title" : "A minimum relative entropy principle for learning and acting",
      "author" : [ "Pedro A Ortega", "Daniel A Braun" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Ortega and Braun.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ortega and Braun.",
      "year" : 2010
    }, {
      "title" : "Generalized Thompson sampling for sequential decision-making and causal inference",
      "author" : [ "Pedro A Ortega", "Daniel A Braun" ],
      "venue" : "Complex Adaptive Systems Modeling,",
      "citeRegEx" : "Ortega and Braun.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ortega and Braun.",
      "year" : 2014
    }, {
      "title" : "More) efficient reinforcement learning via posterior sampling",
      "author" : [ "Ian Osband", "Dan Russo", "Benjamin van Roy" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Osband et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2013
    }, {
      "title" : "The complexity of Markov decision processes",
      "author" : [ "Christos H Papadimitriou", "John N Tsitsiklis" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Papadimitriou and Tsitsiklis.,? \\Q1987\\E",
      "shortCiteRegEx" : "Papadimitriou and Tsitsiklis.",
      "year" : 1987
    }, {
      "title" : "Markov Decision Processes",
      "author" : [ "Martin L Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Puterman.",
      "year" : 2014
    }, {
      "title" : "A philosophical treatise of universal induction",
      "author" : [ "Samuel Rathmanner", "Marcus Hutter" ],
      "venue" : "Entropy, 13(6):1076–1136,",
      "citeRegEx" : "Rathmanner and Hutter.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rathmanner and Hutter.",
      "year" : 2011
    }, {
      "title" : "Delusion, survival, and intelligent agents",
      "author" : [ "Mark Ring", "Laurent Orseau" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Ring and Orseau.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ring and Orseau.",
      "year" : 2011
    }, {
      "title" : "Diffusions, Markov Processes, and Martingales: Volume 1, Foundations",
      "author" : [ "Chris Rogers", "David Williams" ],
      "venue" : null,
      "citeRegEx" : "Rogers and Williams.,? \\Q1994\\E",
      "shortCiteRegEx" : "Rogers and Williams.",
      "year" : 1994
    }, {
      "title" : "Research priorities for robust and beneficial artificial intelligence",
      "author" : [ "Stuart Russell", "Daniel Dewey", "Max Tegmark" ],
      "venue" : "Technical report, Future of Life Institute,",
      "citeRegEx" : "Russell et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russell et al\\.",
      "year" : 2015
    }, {
      "title" : "Artificial Intelligence. A Modern Approach",
      "author" : [ "Stuart J Russell", "Peter Norvig" ],
      "venue" : null,
      "citeRegEx" : "Russell and Norvig.,? \\Q2010\\E",
      "shortCiteRegEx" : "Russell and Norvig.",
      "year" : 2010
    }, {
      "title" : "Characterizing predictable classes of processes",
      "author" : [ "Daniil Ryabko" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Ryabko.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ryabko.",
      "year" : 2009
    }, {
      "title" : "On finding predictors for arbitrary families of processes",
      "author" : [ "Daniil Ryabko" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Ryabko.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ryabko.",
      "year" : 2010
    }, {
      "title" : "On the relation between realizable and nonrealizable cases of the sequence prediction problem",
      "author" : [ "Daniil Ryabko" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Ryabko.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ryabko.",
      "year" : 2011
    }, {
      "title" : "On sequence prediction for arbitrary measures",
      "author" : [ "Daniil Ryabko", "Marcus Hutter" ],
      "venue" : "In IEEE International Symposium on Information Theory, pages 2346–2350,",
      "citeRegEx" : "Ryabko and Hutter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ryabko and Hutter.",
      "year" : 2007
    }, {
      "title" : "Predicting non-stationary processes",
      "author" : [ "Daniil Ryabko", "Marcus Hutter" ],
      "venue" : "Applied Mathematics Letters,",
      "citeRegEx" : "Ryabko and Hutter.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ryabko and Hutter.",
      "year" : 2008
    }, {
      "title" : "Purely epistemic Markov decision processes",
      "author" : [ "Régis Sabbadin", "Jérôme Lang", "Nasolo Ravoanjanahry" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Sabbadin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Sabbadin et al\\.",
      "year" : 2007
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Schaul et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2016
    }, {
      "title" : "The speed prior: A new simplicity measure yielding near-optimal computable predictions",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "Schmidhuber.,? \\Q2002\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2002
    }, {
      "title" : "Philosophers & futurists, catch up",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Journal of Consciousness Studies,",
      "citeRegEx" : "Schmidhuber.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2012
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Schmidhuber.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2015
    }, {
      "title" : "The Technological Singularity",
      "author" : [ "Murray Shanahan" ],
      "venue" : null,
      "citeRegEx" : "Shanahan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shanahan.",
      "year" : 2015
    }, {
      "title" : "Programming a computer for playing chess",
      "author" : [ "Claude E Shannon" ],
      "venue" : "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,",
      "citeRegEx" : "Shannon.,? \\Q1950\\E",
      "shortCiteRegEx" : "Shannon.",
      "year" : 1950
    }, {
      "title" : "Multiagent Systems: Algorithmic, GameTheoretic, and Logical Foundations",
      "author" : [ "Yoav Shoham", "Kevin Leyton-Brown" ],
      "venue" : null,
      "citeRegEx" : "Shoham and Leyton.Brown.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shoham and Leyton.Brown.",
      "year" : 2009
    }, {
      "title" : "Learning predictive state representations",
      "author" : [ "Satinder Singh", "Michael L Littman", "Nicholas K Jong", "David Pardoe", "Peter Stone" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Singh et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2003
    }, {
      "title" : "Predictive state representations: A new theory for modeling dynamical systems",
      "author" : [ "Satinder Singh", "Michael R James", "Matthew R Rudary" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Singh et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2004
    }, {
      "title" : "Formalizing two problems of realistic world-models",
      "author" : [ "Nate Soares" ],
      "venue" : "Technical report, Machine Intelligence Research Institute,",
      "citeRegEx" : "Soares.,? \\Q2015\\E",
      "shortCiteRegEx" : "Soares.",
      "year" : 2015
    }, {
      "title" : "Aligning superintelligence with human interests: A technical research agenda",
      "author" : [ "Nate Soares", "Benja Fallenstein" ],
      "venue" : "Technical report, Machine Intelligence Research Institute,",
      "citeRegEx" : "Soares and Fallenstein.,? \\Q2014\\E",
      "shortCiteRegEx" : "Soares and Fallenstein.",
      "year" : 2014
    }, {
      "title" : "A formal theory of inductive inference. Parts 1 and 2",
      "author" : [ "Ray Solomonoff" ],
      "venue" : "Information and Control,",
      "citeRegEx" : "Solomonoff.,? \\Q1964\\E",
      "shortCiteRegEx" : "Solomonoff.",
      "year" : 1964
    }, {
      "title" : "Complexity-based induction systems: Comparisons and convergence theorems",
      "author" : [ "Ray Solomonoff" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Solomonoff.,? \\Q1978\\E",
      "shortCiteRegEx" : "Solomonoff.",
      "year" : 1978
    }, {
      "title" : "Responses to catastrophic AGI risk: A survey",
      "author" : [ "Kaj Sotala", "Roman V Yampolskiy" ],
      "venue" : "Physica Scripta,",
      "citeRegEx" : "Sotala and Yampolskiy.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sotala and Yampolskiy.",
      "year" : 2014
    }, {
      "title" : "Putnam’s diagonal argument and the impossibility of a universal learning machine",
      "author" : [ "Tom F Sterkenburg" ],
      "venue" : "Technical report, Centrum Wiskunde & Informatica,",
      "citeRegEx" : "Sterkenburg.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sterkenburg.",
      "year" : 2016
    }, {
      "title" : "Counterexamples in Probability",
      "author" : [ "Jordan M Stoyanov" ],
      "venue" : "Courier Corporation,",
      "citeRegEx" : "Stoyanov.,? \\Q2013\\E",
      "shortCiteRegEx" : "Stoyanov.",
      "year" : 2013
    }, {
      "title" : "A Bayesian framework for reinforcement learning",
      "author" : [ "Malcolm Strens" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Strens.,? \\Q2000\\E",
      "shortCiteRegEx" : "Strens.",
      "year" : 2000
    }, {
      "title" : "Consistency of feature Markov processes",
      "author" : [ "Peter Sunehag", "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Sunehag and Hutter.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sunehag and Hutter.",
      "year" : 2010
    }, {
      "title" : "Optimistic agents are asymptotically optimal",
      "author" : [ "Peter Sunehag", "Marcus Hutter" ],
      "venue" : "In Australasian Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Sunehag and Hutter.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sunehag and Hutter.",
      "year" : 2012
    }, {
      "title" : "Optimistic AIXI",
      "author" : [ "Peter Sunehag", "Marcus Hutter" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Sunehag and Hutter.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sunehag and Hutter.",
      "year" : 2012
    }, {
      "title" : "Rationality, optimism and guarantees in general reinforcement learning",
      "author" : [ "Peter Sunehag", "Marcus Hutter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Sunehag and Hutter.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sunehag and Hutter.",
      "year" : 2015
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "Richard Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton.,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1988
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "The paradoxes of confirmation: A survey",
      "author" : [ "Richard G Swinburne" ],
      "venue" : "American Philosophical Quarterly,",
      "citeRegEx" : "Swinburne.,? \\Q1971\\E",
      "shortCiteRegEx" : "Swinburne.",
      "year" : 1971
    }, {
      "title" : "Algorithms for Reinforcement Learning",
      "author" : [ "Csaba Szepesvári" ],
      "venue" : null,
      "citeRegEx" : "Szepesvári.,? \\Q2010\\E",
      "shortCiteRegEx" : "Szepesvári.",
      "year" : 2010
    }, {
      "title" : "Temporal difference learning and TD-Gammon",
      "author" : [ "Gerald Tesauro" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Tesauro.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tesauro.",
      "year" : 1995
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "William R Thompson" ],
      "venue" : null,
      "citeRegEx" : "Thompson.,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson.",
      "year" : 1933
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "John N Tsitsiklis", "Benjamin Van Roy" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Tsitsiklis and Roy.,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy.",
      "year" : 1997
    }, {
      "title" : "Introduction to Nonparametric Estimation",
      "author" : [ "Alexandre Tsybakov" ],
      "venue" : null,
      "citeRegEx" : "Tsybakov.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tsybakov.",
      "year" : 2008
    }, {
      "title" : "Deep reinforcement learning with double Q-learning",
      "author" : [ "Hado van Hasselt", "Arthur Guez", "David Silver" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "A MonteCarlo AIXI approximation",
      "author" : [ "Joel Veness", "Kee Siong Ng", "Marcus Hutter", "William Uther", "David Silver" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Veness et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2011
    }, {
      "title" : "Compress and control",
      "author" : [ "Joel Veness", "Marc G Bellemare", "Marcus Hutter", "Alvin Chua", "Guillaume Desjardins" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Veness et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2015
    }, {
      "title" : "The coming technological singularity",
      "author" : [ "Vernor Vinge" ],
      "venue" : "Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace,",
      "citeRegEx" : "Vinge.,? \\Q1993\\E",
      "shortCiteRegEx" : "Vinge.",
      "year" : 1993
    }, {
      "title" : "Normalized information distance",
      "author" : [ "Paul MB Vitányi", "Frank J Balbach", "Rudi L Cilibrasi", "Ming Li" ],
      "venue" : "In Information Theory and Statistical Learning,",
      "citeRegEx" : "Vitányi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Vitányi et al\\.",
      "year" : 2009
    }, {
      "title" : "Bayesian reinforcement learning",
      "author" : [ "Nikos Vlassis", "Mohammad Ghavamzadeh", "Shie Mannor", "Pascal Poupart" ],
      "venue" : "Reinforcement Learning,",
      "citeRegEx" : "Vlassis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vlassis et al\\.",
      "year" : 2012
    }, {
      "title" : "Hempel’s raven paradox: A lacuna in the standard Bayesian solution",
      "author" : [ "Peter BM Vranas" ],
      "venue" : "The British Journal for the Philosophy of Science,",
      "citeRegEx" : "Vranas.,? \\Q2004\\E",
      "shortCiteRegEx" : "Vranas.",
      "year" : 2004
    }, {
      "title" : "The singularity may never be near",
      "author" : [ "Toby Walsh" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Walsh.,? \\Q2016\\E",
      "shortCiteRegEx" : "Walsh.",
      "year" : 2016
    }, {
      "title" : "Dueling network architectures for deep reinforcement learning",
      "author" : [ "Ziyu Wang", "Nando de Freitas", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Random walk—1-dimensional",
      "author" : [ "Eric W Weisstein" ],
      "venue" : "In MathWorld—A Wolfram Web Resource. Wolfram Research, Inc.,",
      "citeRegEx" : "Weisstein.,? \\Q2002\\E",
      "shortCiteRegEx" : "Weisstein.",
      "year" : 2002
    }, {
      "title" : "Non-)equivalence of universal priors",
      "author" : [ "Ian Wood", "Peter Sunehag", "Marcus Hutter" ],
      "venue" : "In Solomonoff 85th Memorial Conference,",
      "citeRegEx" : "Wood et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wood et al\\.",
      "year" : 2011
    }, {
      "title" : "On convergence of emphatic temporal-difference learning",
      "author" : [ "Huizhen Yu" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Yu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yu.",
      "year" : 2015
    }, {
      "title" : "Creating friendly AI 1.0: The analysis and design of benevolent goal architectures",
      "author" : [ "Eliezer Yudkowsky" ],
      "venue" : "Technical report, Singularity Institute for Artificial Intelligence,",
      "citeRegEx" : "Yudkowsky.,? \\Q2001\\E",
      "shortCiteRegEx" : "Yudkowsky.",
      "year" : 2001
    }, {
      "title" : "Artificial intelligence as a positive and negative factor in global risk. In Global Catastrophic Risks, pages 308–345",
      "author" : [ "Eliezer Yudkowsky" ],
      "venue" : null,
      "citeRegEx" : "Yudkowsky.,? \\Q2008\\E",
      "shortCiteRegEx" : "Yudkowsky.",
      "year" : 2008
    }, {
      "title" : "Graying the black box: Understanding DQNs",
      "author" : [ "Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor" ],
      "venue" : "Technical report, Israel Institute of Technology,",
      "citeRegEx" : "Zahavy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zahavy et al\\.",
      "year" : 2016
    }, {
      "title" : "A universal algorithm for sequential data compression",
      "author" : [ "Jacob Ziv", "Abraham Lempel" ],
      "venue" : "IEEE Transactions on information theory,",
      "citeRegEx" : "Ziv and Lempel.,? \\Q1977\\E",
      "shortCiteRegEx" : "Ziv and Lempel.",
      "year" : 1977
    } ],
    "referenceMentions" : [ {
      "referenceID" : 169,
      "context" : "After the early enthusiastic decades, research in artificial intelligence (AI) now mainly aims at specific domains: playing games, mining data, processing natural language, recognizing objects in images, piloting robots, filtering email, and many others (Russell and Norvig, 2010).",
      "startOffset" : 254,
      "endOffset" : 280
    }, {
      "referenceID" : 132,
      "context" : "The goal of developing HLAI has a long tradition in AI research and was explicitly part of the 1956 Dartmouth conference that gave birth to the field of AI (McCarthy et al., 1955):",
      "startOffset" : 156,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.",
      "startOffset" : 48,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.",
      "startOffset" : 48,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.",
      "startOffset" : 101,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.",
      "startOffset" : 101,
      "endOffset" : 163
    }, {
      "referenceID" : 14,
      "context" : "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.",
      "startOffset" : 101,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.",
      "startOffset" : 101,
      "endOffset" : 198
    }, {
      "referenceID" : 198,
      "context" : "Reinforcement learning studies algorithms that learn to act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesvári, 2010; Wiering and van Otterlo, 2012).",
      "startOffset" : 110,
      "endOffset" : 183
    }, {
      "referenceID" : 200,
      "context" : "Reinforcement learning studies algorithms that learn to act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesvári, 2010; Wiering and van Otterlo, 2012).",
      "startOffset" : 110,
      "endOffset" : 183
    }, {
      "referenceID" : 198,
      "context" : "Typically, the policy is slowly improved while learning, like SARSA (Sutton and Barto, 1998).",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 164,
      "context" : "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).",
      "startOffset" : 30,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).",
      "startOffset" : 30,
      "endOffset" : 102
    }, {
      "referenceID" : 198,
      "context" : "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).",
      "startOffset" : 30,
      "endOffset" : 102
    }, {
      "referenceID" : 197,
      "context" : "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).",
      "startOffset" : 209,
      "endOffset" : 223
    }, {
      "referenceID" : 4,
      "context" : "Lattimore and Hutter (2012) use the algorithm UCRLγ (Auer et al., 2009) with geometric discounting with discount rate γ and derive the currently best-known PAC bound of Õ(−T/(ε2(1 − γ)3) log δ) where T is the number of non-zero transitions in the MDP.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 198,
      "context" : "In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998).",
      "startOffset" : 99,
      "endOffset" : 123
    }, {
      "referenceID" : 203,
      "context" : "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).",
      "startOffset" : 84,
      "endOffset" : 138
    }, {
      "referenceID" : 197,
      "context" : "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).",
      "startOffset" : 84,
      "endOffset" : 138
    }, {
      "referenceID" : 52,
      "context" : "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).",
      "startOffset" : 84,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).",
      "startOffset" : 184,
      "endOffset" : 197
    }, {
      "referenceID" : 4,
      "context" : "Auer et al. (2009) derive the regret bound Õ(dS √ At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "Auer et al. (2009) derive the regret bound Õ(dS √ At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given ε and δ, a reinforcement learning algorithm is said to have sample complexity C(ε, δ) iff it is ε-suboptimal for at most C(ε, δ) time steps with probability at least 1 − δ (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRLγ (Auer et al.",
      "startOffset" : 0,
      "endOffset" : 600
    }, {
      "referenceID" : 4,
      "context" : "Auer et al. (2009) derive the regret bound Õ(dS √ At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given ε and δ, a reinforcement learning algorithm is said to have sample complexity C(ε, δ) iff it is ε-suboptimal for at most C(ε, δ) time steps with probability at least 1 − δ (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRLγ (Auer et al.",
      "startOffset" : 0,
      "endOffset" : 629
    }, {
      "referenceID" : 4,
      "context" : "Auer et al. (2009) derive the regret bound Õ(dS √ At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given ε and δ, a reinforcement learning algorithm is said to have sample complexity C(ε, δ) iff it is ε-suboptimal for at most C(ε, δ) time steps with probability at least 1 − δ (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRLγ (Auer et al., 2009) with geometric discounting with discount rate γ and derive the currently best-known PAC bound of Õ(−T/(ε2(1 − γ)3) log δ) where T is the number of non-zero transitions in the MDP. Typically, algorithms for MDPs rely on visiting every state multiple times (or even infinitely often), which becomes infeasible for large state spaces (e.g. a video game screen consisting of millions of pixels). In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998). Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995). A recent breakthrough was made by Mahmood et al. (2015) and Yu (2015)",
      "startOffset" : 0,
      "endOffset" : 1445
    }, {
      "referenceID" : 4,
      "context" : "Auer et al. (2009) derive the regret bound Õ(dS √ At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given ε and δ, a reinforcement learning algorithm is said to have sample complexity C(ε, δ) iff it is ε-suboptimal for at most C(ε, δ) time steps with probability at least 1 − δ (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRLγ (Auer et al., 2009) with geometric discounting with discount rate γ and derive the currently best-known PAC bound of Õ(−T/(ε2(1 − γ)3) log δ) where T is the number of non-zero transitions in the MDP. Typically, algorithms for MDPs rely on visiting every state multiple times (or even infinitely often), which becomes infeasible for large state spaces (e.g. a video game screen consisting of millions of pixels). In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998). Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995). A recent breakthrough was made by Mahmood et al. (2015) and Yu (2015)",
      "startOffset" : 0,
      "endOffset" : 1459
    }, {
      "referenceID" : 89,
      "context" : "Among the historical successes of reinforcement learning is autonomous helicopter piloting (Kim et al., 2003) and TD-Gammon, a backgammon algorithm that learned through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al.",
      "startOffset" : 91,
      "endOffset" : 109
    }, {
      "referenceID" : 201,
      "context" : ", 2003) and TD-Gammon, a backgammon algorithm that learned through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al.",
      "startOffset" : 77,
      "endOffset" : 92
    }, {
      "referenceID" : 65,
      "context" : "This approach to general AI is in accordance with the definition of intelligence given by Legg and Hutter (2007b):",
      "startOffset" : 99,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "’ A popular such selection is the Atari 2600 video game console (Bellemare et al., 2013).",
      "startOffset" : 64,
      "endOffset" : 88
    }, {
      "referenceID" : 179,
      "context" : "1 DQN rides the wave of success of deep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016).",
      "startOffset" : 49,
      "endOffset" : 113
    }, {
      "referenceID" : 50,
      "context" : "1 DQN rides the wave of success of deep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016).",
      "startOffset" : 49,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al.",
      "startOffset" : 143,
      "endOffset" : 167
    }, {
      "referenceID" : 142,
      "context" : ", 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al.",
      "startOffset" : 30,
      "endOffset" : 68
    }, {
      "referenceID" : 136,
      "context" : ", 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al.",
      "startOffset" : 30,
      "endOffset" : 68
    }, {
      "referenceID" : 176,
      "context" : ", 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 123,
      "context" : ", 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 213,
      "context" : ", 2016), and improvements to the neural network architecture (Wang et al., 2016).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al., 2016), and improvements to the neural network architecture (Wang et al., 2016). TheQvalues learned by DQN’s neural networks are intransparent to inspection; Zahavy et al. (2016) use visualization techniques on the Q-value networks.",
      "startOffset" : 144,
      "endOffset" : 602
    }, {
      "referenceID" : 8,
      "context" : "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al., 2016), and improvements to the neural network architecture (Wang et al., 2016). TheQvalues learned by DQN’s neural networks are intransparent to inspection; Zahavy et al. (2016) use visualization techniques on the Q-value networks. Finally, Liang et al. (2016) managed to reproduce DQN’s success using only linear function approximation (no neural networks).",
      "startOffset" : 144,
      "endOffset" : 685
    }, {
      "referenceID" : 59,
      "context" : "An obvious approach to equip DQN with memory is to use recurrent neural networks instead of simple feedforward neural networks (Heess et al., 2015).",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "However, it is currently unclear whether recurrent neural networks are powerful enough to learn long-term dependencies in the data (Bengio et al., 1994).",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 55,
      "context" : "Hausknecht and Stone (2015) show that this enables the agent to play the games when using only a single frame as input.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 159,
      "context" : "by knowledge-seeking agents (Orseau, 2011, 2014a; Orseau et al., 2013).",
      "startOffset" : 28,
      "endOffset" : 70
    }, {
      "referenceID" : 148,
      "context" : "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).",
      "startOffset" : 126,
      "endOffset" : 181
    }, {
      "referenceID" : 166,
      "context" : "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).",
      "startOffset" : 126,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).",
      "startOffset" : 126,
      "endOffset" : 181
    }, {
      "referenceID" : 87,
      "context" : "Kulkarni et al. (2016) introduce a hierarchical approach based on intrinsic motivation to improve DQN’s exploration and manage to score points in Montezuma’s Revenge.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014). Consequently the agent no longer pursues the designers’ originally intended goals and instead only attempts to protect its own existence. The name wireheading was established by analogy to a biology experiment by Olds and Milner (1954) in which rats had a wire embedded into the reward center of their brain that they could then stimulate by the push of a button.",
      "startOffset" : 167,
      "endOffset" : 419
    }, {
      "referenceID" : 32,
      "context" : ", 2003, 2004), and Bayesian methods (Doshi-Velez, 2012).",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 193,
      "context" : "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015).",
      "startOffset" : 82,
      "endOffset" : 123
    }, {
      "referenceID" : 26,
      "context" : "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015).",
      "startOffset" : 82,
      "endOffset" : 123
    }, {
      "referenceID" : 165,
      "context" : "Based in algorithmic information theory, Solomonoff’s prior draws from famous insights by William of Ockham, Sextus Epicurus, Alan Turing, and Andrey Kolmogorov (Rathmanner and Hutter, 2011).",
      "startOffset" : 161,
      "endOffset" : 190
    }, {
      "referenceID" : 95,
      "context" : "A typical optimality property in general reinforcement learning is asymptotic optimality (Lattimore and Hutter, 2011): as time progresses the agent converges to achieve the same rewards as the optimal policy.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 197,
      "context" : "Asymptotic optimality is usually what is meant by “Q-learning converges” (Watkins and Dayan, 1992) or “TD learning converges” (Sutton, 1988).",
      "startOffset" : 126,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015). However, Hutter (2014) managed to derive strong bounds relating the optimal value function of the aggregated MDP to the value function of the original process even if the latter violates the Markov condition.",
      "startOffset" : 109,
      "endOffset" : 148
    }, {
      "referenceID" : 26,
      "context" : "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015). However, Hutter (2014) managed to derive strong bounds relating the optimal value function of the aggregated MDP to the value function of the original process even if the latter violates the Markov condition. A full theoretical approach to the general reinforcement learning problem is given by Hutter (2000, 2001a, 2002a, 2003, 2005, 2007a, 2012b). He introduces the Bayesian RL agent AIXI building on the theory of sequence prediction by Solomonoff (1964, 1978). Based in algorithmic information theory, Solomonoff’s prior draws from famous insights by William of Ockham, Sextus Epicurus, Alan Turing, and Andrey Kolmogorov (Rathmanner and Hutter, 2011). AIXI uses Solomonoff’s prior over the class of all computable environments and acts to maximize Bayes-expected rewards. We formally introduce Solomonoff’s theory of induction in Chapter 3 and AIXI in Section 4.3.1. See also Legg (2008) for an accessible introduction to AIXI.",
      "startOffset" : 109,
      "endOffset" : 1018
    }, {
      "referenceID" : 65,
      "context" : "1) Here, intelligence refers to an agent that optimizes towards some goal in accordance with the definition by Legg and Hutter (2007b). For learning we distinguish two (very related) aspects: (1) arriving at accurate beliefs about the future and (2) making accurate predictions about the future.",
      "startOffset" : 120,
      "endOffset" : 135
    }, {
      "referenceID" : 135,
      "context" : ", 2011, 2015) are easily outperformed by neural-network-based approaches (Mnih et al., 2015).",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 65,
      "context" : "Hutter (2002a) showed that AIXI is Pareto optimal, balanced Pareto optimal, and self-optimizing.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 65,
      "context" : "Hutter (2002a) showed that AIXI is Pareto optimal, balanced Pareto optimal, and self-optimizing. Orseau (2013) established that AIXI does not achieve asymptotic optimality in all computable environments (making the self-optimizing result inapplicable to this general environment class).",
      "startOffset" : 0,
      "endOffset" : 111
    }, {
      "referenceID" : 42,
      "context" : "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 42,
      "context" : "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al.",
      "startOffset" : 100,
      "endOffset" : 165
    }, {
      "referenceID" : 42,
      "context" : "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al.",
      "startOffset" : 100,
      "endOffset" : 187
    }, {
      "referenceID" : 42,
      "context" : "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al. (2016b) Chapter 8 Appendix A Leike and Hutter (2014b)",
      "startOffset" : 100,
      "endOffset" : 261
    }, {
      "referenceID" : 42,
      "context" : "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al. (2016b) Chapter 8 Appendix A Leike and Hutter (2014b)",
      "startOffset" : 100,
      "endOffset" : 307
    }, {
      "referenceID" : 45,
      "context" : "Only small classes are known to have a grain of truth and the literature contains several related impossibility results (Nachbar, 1997, 2005; Foster and Young, 2001).",
      "startOffset" : 120,
      "endOffset" : 165
    }, {
      "referenceID" : 26,
      "context" : "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al.",
      "startOffset" : 101,
      "endOffset" : 126
    }, {
      "referenceID" : 26,
      "context" : "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al.",
      "startOffset" : 101,
      "endOffset" : 197
    }, {
      "referenceID" : 26,
      "context" : "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honour’s thesis supervised by Marcus Hutter and me).",
      "startOffset" : 101,
      "endOffset" : 264
    }, {
      "referenceID" : 26,
      "context" : "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honour’s thesis supervised by Marcus Hutter and me). Leike and Hutter (2016) is",
      "startOffset" : 101,
      "endOffset" : 381
    }, {
      "referenceID" : 31,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.",
      "startOffset" : 119,
      "endOffset" : 143
    }, {
      "referenceID" : 24,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.",
      "startOffset" : 119,
      "endOffset" : 188
    }, {
      "referenceID" : 24,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.",
      "startOffset" : 119,
      "endOffset" : 249
    }, {
      "referenceID" : 12,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.",
      "startOffset" : 277,
      "endOffset" : 291
    }, {
      "referenceID" : 12,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.",
      "startOffset" : 277,
      "endOffset" : 316
    }, {
      "referenceID" : 12,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.",
      "startOffset" : 277,
      "endOffset" : 361
    }, {
      "referenceID" : 12,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.",
      "startOffset" : 277,
      "endOffset" : 406
    }, {
      "referenceID" : 12,
      "context" : "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vitányi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.",
      "startOffset" : 277,
      "endOffset" : 427
    }, {
      "referenceID" : 33,
      "context" : "This section provides a concise introduction to measure theory; see Durrett (2010) for an extensive treatment.",
      "startOffset" : 68,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).",
      "startOffset" : 100,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).",
      "startOffset" : 217,
      "endOffset" : 299
    }, {
      "referenceID" : 87,
      "context" : "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).",
      "startOffset" : 217,
      "endOffset" : 299
    }, {
      "referenceID" : 106,
      "context" : "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).",
      "startOffset" : 217,
      "endOffset" : 299
    }, {
      "referenceID" : 21,
      "context" : "A well-known approach to the nonrealizable case is prediction with expert advice (Cesa-Bianchi and Lugosi, 2006), which we do not con-",
      "startOffset" : 81,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "); see Bishop (2006) and Hastie et al.",
      "startOffset" : 7,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "); see Bishop (2006) and Hastie et al. (2009). In this chapter we do not make the i.",
      "startOffset" : 7,
      "endOffset" : 46
    }, {
      "referenceID" : 152,
      "context" : "Generally, the nonrealizable case is harder, but Ryabko (2011) argues that for some problems, both cases coincide.",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 65,
      "context" : "6 connects the results from the first sections to the learning framework developed by Solomonoff (1964, 1978), Hutter (2001b, 2005, 2007b), and Schmidhuber (2002) (among others).",
      "startOffset" : 111,
      "endOffset" : 163
    }, {
      "referenceID" : 65,
      "context" : "1 (The Black Ravens; Rathmanner and Hutter, 2011, Sec. 7.4). If we live in a world in which all ravens are black, how can we learn this fact? Since at every time step we have observed only a finite subset of the (possibly infinite) set of all ravens, how can we confidently state anything about all ravens? We formalize this problem in line with Rathmanner and Hutter (2011, Sec. 7.4) and Leike and Hutter (2015d). We define two predicates, blackness B and ravenness R.",
      "startOffset" : 36,
      "endOffset" : 414
    }, {
      "referenceID" : 65,
      "context" : "It is very easy to derive from a countable set of distributions, and it has been considered extensively in the literature (Solomonoff, 1964; Jaynes, 2003; Hutter, 2005, . . . ). Ryabko (2009) shows that even for uncountably infinite classes, if there are good predictors, then a Bayesian mixture over a countable subclass asymptotically also does well.",
      "startOffset" : 155,
      "endOffset" : 192
    }, {
      "referenceID" : 65,
      "context" : "It is very easy to derive from a countable set of distributions, and it has been considered extensively in the literature (Solomonoff, 1964; Jaynes, 2003; Hutter, 2005, . . . ). Ryabko (2009) shows that even for uncountably infinite classes, if there are good predictors, then a Bayesian mixture over a countable subclass asymptotically also does well. Example 3.5 (Solomonoff Prior). Solomonoff (1964) defines a distribution M over X ] that assigns to a string x the probability that the universal monotone Turing machine U outputs x when fed with fair coin flips on the input tape.",
      "startOffset" : 155,
      "endOffset" : 403
    }, {
      "referenceID" : 65,
      "context" : "See Rathmanner and Hutter (2011) for a discussion on the philosophical underpinnings of Solomonoff’s prior.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 65,
      "context" : "Ryabko and Hutter (2007, 2008) consider the following definition. It is analogous to Definition 3.3, except that the constant c is allowed to depend on time. Definition 3.10 (Dominance with Coefficients; Ryabko and Hutter, 2008, Def. 2). The measure Q dominates P with coefficients f (Q ≥ P/f) iff Q(x) ≥ P (x)/f(|x|) for all x ∈ X ∗. If Q dominates P with coefficients f and f grows subexponentially (f ∈ o(exp)), then Q weakly dominates P by Remark 3.9. Example 3.11 (Speed Prior). Schmidhuber (2002) defines a variant of Solomonoff’s priorM that penalizes programs by their running time, called the speed prior.",
      "startOffset" : 11,
      "endOffset" : 503
    }, {
      "referenceID" : 53,
      "context" : "14 (The Minimum Description Length Principle; Grünwald, 2007).",
      "startOffset" : 3,
      "endOffset" : 61
    }, {
      "referenceID" : 53,
      "context" : "14 (The Minimum Description Length Principle; Grünwald, 2007). Let M be a countable set of probability measures on (X,F∞) and let K : M → [0, 1] be a function such that ∑ P∈M 2 −K(P ) ≤ 1 called regularizer. Following notation from Hutter (2009a), we define for each x ∈ X ∗ the minimal description length model as MDL := arg min P∈M {− logP (x) +K(P )}.",
      "startOffset" : 46,
      "endOffset" : 247
    }, {
      "referenceID" : 86,
      "context" : "Much of this section is based on Kalai and Lehrer (1994) and Lehrer and Smorodinsky (1996).",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 86,
      "context" : "Much of this section is based on Kalai and Lehrer (1994) and Lehrer and Smorodinsky (1996).",
      "startOffset" : 33,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "25 (Absolute Continuity⇒ Strong Merging; Blackwell and Dubins, 1962).",
      "startOffset" : 3,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "The following theorem is the famous merging of opinions theorem by Blackwell and Dubins (1962). Theorem 3.",
      "startOffset" : 67,
      "endOffset" : 95
    }, {
      "referenceID" : 65,
      "context" : "More generally, we could also follow Hutter (2001b) and phrase predictive performance in terms of loss: given a loss function ` : X ×X → R the predictor Q suffers an (instantaneous) loss of `(xQt , xt ) in time step t.",
      "startOffset" : 37,
      "endOffset" : 52
    }, {
      "referenceID" : 65,
      "context" : "1 Dominance We start with the prediction regret bounds proved by Hutter (2001b) in case the learning distribution Q dominates the true distribution P .",
      "startOffset" : 65,
      "endOffset" : 80
    }, {
      "referenceID" : 214,
      "context" : "We have (Weisstein, 2002)",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 133,
      "context" : "The proof idea is inspired by Miller and Sanchirico (1999). We think of P and Q as two players in a zero-sum betting game.",
      "startOffset" : 30,
      "endOffset" : 59
    }, {
      "referenceID" : 65,
      "context" : "See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff’s theory and its philosophical motivations and Sterkenburg (2016) for a critique of its optimality.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 65,
      "context" : "See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff’s theory and its philosophical motivations and Sterkenburg (2016) for a critique of its optimality.",
      "startOffset" : 19,
      "endOffset" : 146
    }, {
      "referenceID" : 42,
      "context" : "In this section we state merging and prediction results for SKt, a speed prior introduced by Filan et al. (2016) formally defined in Example 3.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 42,
      "context" : "In this section we state merging and prediction results for SKt, a speed prior introduced by Filan et al. (2016) formally defined in Example 3.11. It is slightly different from the speed prior defined by Schmidhuber (2002), but for the latter no compatibility properties are known for nondeterministic measures.",
      "startOffset" : 93,
      "endOffset" : 223
    }, {
      "referenceID" : 54,
      "context" : "Gács (1983) shows that the similarity M ≈ 2−Km is not an equality.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 29,
      "context" : "to Day (2011) who shows that Km(x) > − logM(x) +O(log log |x|) for infinitely many x ∈ X ∗.",
      "startOffset" : 3,
      "endOffset" : 14
    }, {
      "referenceID" : 29,
      "context" : "to Day (2011) who shows that Km(x) > − logM(x) +O(log log |x|) for infinitely many x ∈ X ∗. Nevertheless, 2−Km dominates every computable measure (Li and Vitányi, 2008, Thm. 4.5.4 and Lem. 4.5.6ii(d); originally proved by Levin, 1973). Hence all the strong results that hold for Solomonoff induction (prediction regret and strong merging) also hold for compression: we apply Theorem 3.25 and Corollary 3.44 to get the following results. See Hutter (2006a) for further discussion on using the universal compressor Km for learning.",
      "startOffset" : 3,
      "endOffset" : 456
    }, {
      "referenceID" : 65,
      "context" : "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal.",
      "startOffset" : 20,
      "endOffset" : 208
    }, {
      "referenceID" : 65,
      "context" : "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal. Hence they do not dominate every computable distribution. As with the speed prior, what matters is the rate at which Yt = Q(x1:t)/P (x1:t) goes to 0, i.e., does the compressor weakly dominate the true distribution in the sense of Definition 3.8? Veness et al. (2015) successfully apply the Lempel-Ziv compression algorithm as a learning algorithm for reinforcement learning; however, some preprocessing of the data is required.",
      "startOffset" : 20,
      "endOffset" : 508
    }, {
      "referenceID" : 65,
      "context" : "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal. Hence they do not dominate every computable distribution. As with the speed prior, what matters is the rate at which Yt = Q(x1:t)/P (x1:t) goes to 0, i.e., does the compressor weakly dominate the true distribution in the sense of Definition 3.8? Veness et al. (2015) successfully apply the Lempel-Ziv compression algorithm as a learning algorithm for reinforcement learning; however, some preprocessing of the data is required. More remotely, Vitányi et al. (2009) use standard compression algorithms to classify mammal genomes, languages, and classical music.",
      "startOffset" : 20,
      "endOffset" : 706
    }, {
      "referenceID" : 13,
      "context" : "3i) Blackwell and Dubins (1962)",
      "startOffset" : 4,
      "endOffset" : 32
    }, {
      "referenceID" : 63,
      "context" : "The paradox of confirmation, also known as Hempel’s paradox (Hempel, 1945), relies on the following three principles.",
      "startOffset" : 60,
      "endOffset" : 74
    }, {
      "referenceID" : 63,
      "context" : "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.",
      "startOffset" : 106,
      "endOffset" : 199
    }, {
      "referenceID" : 47,
      "context" : "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.",
      "startOffset" : 106,
      "endOffset" : 199
    }, {
      "referenceID" : 124,
      "context" : "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.",
      "startOffset" : 106,
      "endOffset" : 199
    }, {
      "referenceID" : 49,
      "context" : "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.",
      "startOffset" : 106,
      "endOffset" : 199
    }, {
      "referenceID" : 64,
      "context" : "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.",
      "startOffset" : 106,
      "endOffset" : 199
    }, {
      "referenceID" : 128,
      "context" : "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.",
      "startOffset" : 106,
      "endOffset" : 199
    }, {
      "referenceID" : 211,
      "context" : "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.",
      "startOffset" : 106,
      "endOffset" : 199
    }, {
      "referenceID" : 124,
      "context" : "Support for Nicod’s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.",
      "startOffset" : 46,
      "endOffset" : 87
    }, {
      "referenceID" : 64,
      "context" : "Support for Nicod’s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.",
      "startOffset" : 46,
      "endOffset" : 87
    }, {
      "referenceID" : 128,
      "context" : "Support for Nicod’s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.",
      "startOffset" : 46,
      "endOffset" : 87
    }, {
      "referenceID" : 47,
      "context" : "A Bayesian reasoner might be tempted to argue that a green apple does confirm the hypothesis H, but only to a small degree, since there are vastly more non-black objects than ravens (Good, 1960).",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 47,
      "context" : "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.",
      "startOffset" : 121,
      "endOffset" : 222
    }, {
      "referenceID" : 179,
      "context" : "Vranas (2004) shows that this solution is equivalent to the assertion that blackness is equally probable regardless of whether H holds: P (black|H) ≈ P (black).",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 47,
      "context" : "The following is a very concise example against the standard Bayesian solution by Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, one white raven, and a million other birds.",
      "startOffset" : 82,
      "endOffset" : 94
    }, {
      "referenceID" : 47,
      "context" : "The following is a very concise example against the standard Bayesian solution by Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, one white raven, and a million other birds. Now we draw a bird uniformly at random, and it turns out to be a black raven. Contrary to what Nicod’s criterion claims, this is strong evidence that we are in fact in the second world, and in this world non-black ravens exist. For another, more intuitive example: Suppose you do not know anything about ravens and you have a friend who collects atypical objects. If you see a black raven in her collection, surely this would not increase your belief in the hypothesis that all ravens are black. In Leike and Hutter (2015d) we investigate the paradox of confirmation in the context of Solomonoff induction.",
      "startOffset" : 82,
      "endOffset" : 792
    }, {
      "referenceID" : 65,
      "context" : "It is mostly based on Hutter (2005) and Lattimore (2013).",
      "startOffset" : 22,
      "endOffset" : 36
    }, {
      "referenceID" : 65,
      "context" : "It is mostly based on Hutter (2005) and Lattimore (2013). Section 4.",
      "startOffset" : 22,
      "endOffset" : 57
    }, {
      "referenceID" : 65,
      "context" : "Equivalently, Hutter (2005) defines environments as chronological contextual semimeasures.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 65,
      "context" : "Hutter (2005) calls them chronological conditional semimeasures.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 37,
      "context" : "Decision making in the physicalistic model is still underdeveloped; see Everitt et al. (2015) and Orseau and Ring (2012a).",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 37,
      "context" : "Decision making in the physicalistic model is still underdeveloped; see Everitt et al. (2015) and Orseau and Ring (2012a). In this thesis we restrict ourselves to the dualistic model.",
      "startOffset" : 72,
      "endOffset" : 122
    }, {
      "referenceID" : 65,
      "context" : "For a discussion of general discounting we refer the reader to Lattimore and Hutter (2014).",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "Moreover, with unbounded rewards there all kinds of pathological problems where defining optimal actions is no longer straightforward; see Arntzenius et al. (2004) for a discussion.",
      "startOffset" : 139,
      "endOffset" : 164
    }, {
      "referenceID" : 19,
      "context" : "Bandits exist in many flavors; see Bubeck and Bianchi (2012) for a survey.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 198,
      "context" : "Much of today’s literature on reinforcement learning focuses on MDPs (Sutton and Barto, 1998).",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 135,
      "context" : "While they have a huge state space2 they can still be learned using Q-learning with function approximation (Mnih et al., 2015).",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "For any physical system of finite volume and finite (average) energy, the amount of information it can contain is finite (Bekenstein, 1981), and so is the number of state transitions per unit of time (Margolus and Levitin, 1998).",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 130,
      "context" : "For any physical system of finite volume and finite (average) energy, the amount of information it can contain is finite (Bekenstein, 1981), and so is the number of state transitions per unit of time (Margolus and Levitin, 1998).",
      "startOffset" : 200,
      "endOffset" : 228
    }, {
      "referenceID" : 65,
      "context" : "12 (Optimal Policy; Hutter, 2005, Def. 5.19 & 5.30). A policy π is optimal in environment ν (ν-optimal) iff for all histories π attains the optimal value: V π ν (æ<t) = V ∗ ν (æ<t) for all æ<t ∈ (A×E)∗. The action at is an optimal action iff π∗ ν(at | æ<t) = 1 for some ν-optimal policy π∗ ν . Following the tradition of Hutter (2005), AINU denotes a ν-optimal policy for the environment ν ∈ MCCS LSC and AIMU denotes an μ-optimal policy for the environment μ ∈MCCM comp that is a measure (as opposed to a semimeasure).",
      "startOffset" : 20,
      "endOffset" : 335
    }, {
      "referenceID" : 215,
      "context" : "5) can be defined equivalently according to (Wood et al., 2011) ξ(e<t ‖ a<t) := ∑",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 94,
      "context" : "This strategy even achieves the optimal asymptotic regret bounds (Lattimore, 2016).",
      "startOffset" : 65,
      "endOffset" : 82
    }, {
      "referenceID" : 45,
      "context" : "For multi-armed bandits, Gittins (1979) achieved a breakthrough with an index strategy that enables the computation of the optimal policy by computing one quantity for each arm independently of the rest.",
      "startOffset" : 25,
      "endOffset" : 40
    }, {
      "referenceID" : 45,
      "context" : "For multi-armed bandits, Gittins (1979) achieved a breakthrough with an index strategy that enables the computation of the optimal policy by computing one quantity for each arm independently of the rest. This strategy even achieves the optimal asymptotic regret bounds (Lattimore, 2016). Larger classes have also been attempted: using Monte-Carlo tree search, Veness et al. (2011) approximate the Bayes optimal policy in the class of all context trees.",
      "startOffset" : 25,
      "endOffset" : 381
    }, {
      "referenceID" : 32,
      "context" : "Doshi-Velez (2012) uses Bayesian techniques to learn infinite-state POMDPs.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 32,
      "context" : "Doshi-Velez (2012) uses Bayesian techniques to learn infinite-state POMDPs. See Vlassis et al. (2012) for a survey on Bayesian techniques in RL.",
      "startOffset" : 0,
      "endOffset" : 102
    }, {
      "referenceID" : 149,
      "context" : "In this section we discuss two variants of knowledge-seeking agents: entropy-seeking agents introduced by Orseau (2011, 2014a) and information-seeking agents introduced by Orseau et al. (2013). The entropy-seeking agent maximizes the Shannon entropy gain, while the information-seeking agent maximizes the expected information gain.",
      "startOffset" : 106,
      "endOffset" : 193
    }, {
      "referenceID" : 23,
      "context" : "It is easy to implement and often achieves quite good results (Chapelle and Li, 2011).",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012).",
      "startOffset" : 49,
      "endOffset" : 97
    }, {
      "referenceID" : 88,
      "context" : "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012).",
      "startOffset" : 49,
      "endOffset" : 97
    }, {
      "referenceID" : 192,
      "context" : "Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al.",
      "startOffset" : 51,
      "endOffset" : 87
    }, {
      "referenceID" : 30,
      "context" : "Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al.",
      "startOffset" : 51,
      "endOffset" : 87
    }, {
      "referenceID" : 162,
      "context" : ", 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015).",
      "startOffset" : 73,
      "endOffset" : 120
    }, {
      "referenceID" : 51,
      "context" : ", 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015).",
      "startOffset" : 73,
      "endOffset" : 120
    }, {
      "referenceID" : 193,
      "context" : "4 Thompson Sampling Thompson sampling, also known as posterior sampling or the Bayesian control rule, was originally proposed by Thompson (1933) as a bandit algorithm.",
      "startOffset" : 2,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012). Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015). For general RL Thompson sampling was first suggested by Ortega and Braun (2010) with resampling at every time step.",
      "startOffset" : 50,
      "endOffset" : 381
    }, {
      "referenceID" : 0,
      "context" : "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012). Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015). For general RL Thompson sampling was first suggested by Ortega and Braun (2010) with resampling at every time step. Strens (2000) proposes following the optimal policy for one episode or “related to the number of state transitions the agent is likely to need to plan ahead”.",
      "startOffset" : 50,
      "endOffset" : 431
    }, {
      "referenceID" : 127,
      "context" : "See also Mahadevan (1996) for a discussion of notions of optimality in MDPs.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 65,
      "context" : "1 (Pareto Optimality; Hutter, 2005, Def. 5.22). A policy π is Pareto optimal in the set of environmentsM iff there is no policy π̃ such that V π̃ ν ( ) ≥ V π ν ( ) for all ν ∈M and V π̃ ρ ( ) > V π ρ ( ) for at least one ρ ∈M. The literature provides the following result. Theorem 5.2 (AIXI is Pareto Optimal; Hutter, 2002a, Thm. 2). Every ξ-optimal policy is Pareto optimal inMCCS LSC . The following theorem was proved for deterministic policies in Leike and Hutter (2015c). Here we extend it to stochastic policies.",
      "startOffset" : 22,
      "endOffset" : 476
    }, {
      "referenceID" : 65,
      "context" : "The following proof was adapted from Leike and Hutter (2015c) to work for environment classes that do not contain the Bayesian mixture.",
      "startOffset" : 47,
      "endOffset" : 62
    }, {
      "referenceID" : 55,
      "context" : "3 The Gödel Prior This section introduces a prior that prevents any fixed formal system from making any statements about the outcome of all but finitely many computations. It is named after Gödel (1931) who famously showed that for any sufficiently rich formal system there are statements that it can neither prove nor disprove.",
      "startOffset" : 6,
      "endOffset" : 203
    }, {
      "referenceID" : 65,
      "context" : "The aim of the Legg-Hutter intelligence measure is to formalize the intuitive notion of intelligence mathematically. Legg and Hutter (2007a) collect various definitions of intelligence across many academic fields and destill it into the following statement (Legg and Hutter, 2007b) Intelligence measures an agent’s ability to achieve goals in a wide range of environments.",
      "startOffset" : 20,
      "endOffset" : 141
    }, {
      "referenceID" : 65,
      "context" : "The Legg-Hutter intelligence of a policy π is the t0-value that π achieves across all environments from the classM weighted by the prior w. Legg and Hutter (2007b) consider a subclass ofMCCS LSC , the class of computable measures together with a Solomonoff prior w(ν) = 2−K(ν) and do not use discounting explicitly.",
      "startOffset" : 9,
      "endOffset" : 164
    }, {
      "referenceID" : 105,
      "context" : "It is natural to fix the policy random that takes actions uniformly at random to have an intelligence score of 1/2 by choosing a ‘symmetric’ universal prior (Legg and Veness, 2013).",
      "startOffset" : 157,
      "endOffset" : 180
    }, {
      "referenceID" : 65,
      "context" : "Next, we state two negative results about asymptotic optimality proved by Lattimore and Hutter (2011). It is important to emphasize that Theorem 5.",
      "startOffset" : 88,
      "endOffset" : 102
    }, {
      "referenceID" : 93,
      "context" : "In this subsection we state a result by Lattimore (2013) that motivated the definition of BayesExp.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 160,
      "context" : "Ortega and Braun (2010) prove that the action probabilities of Thompson sampling converge to the action probability of the optimal policy almost surely, but require a finite environment class and two (arguably quite strong) technical assumptions on the behavior of the posterior distribution (akin to ergodicity) and the similarity of environments in the class.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "Multi-armed bandits provide a (problemindependent) worst-case regret bound of Ω( √ km) where k is the number of arms (Bubeck and Bianchi, 2012).",
      "startOffset" : 117,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "In MDPs the lower bound is Ω( √ SAdm) where S is the number of states, A the number of actions, and d the diameter of the MDP (Auer et al., 2010).",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 144,
      "context" : "For a countable class of environments given by state representation functions that map histories to MDP states, a regret of Õ(m2/3) is achievable assuming the resulting MDP is weakly communicating (Nguyen et al., 2013).",
      "startOffset" : 197,
      "endOffset" : 218
    }, {
      "referenceID" : 93,
      "context" : "However, this problem can be alleviated by adding an extra exploration component to AIXI: Lattimore (2013) shows that BayesExp is weakly asymptotically optimal (Theorem 5.",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 55,
      "context" : "5), and the Gödel prior (Theorem 5.8) should be considered unnatural. But what are other desirable properties of a UTM? A remarkable but unsuccessful attempt to find natural UTMs is due to Müller (2010). It takes the probability that one universal machine simulates another according",
      "startOffset" : 12,
      "endOffset" : 203
    }, {
      "referenceID" : 93,
      "context" : "25) and BayesExp (Lattimore, 2013), but not AIXI (Orseau, 2013) sublinear regret impossible in general environments, but possible with recoverability (Theorem 5.",
      "startOffset" : 17,
      "endOffset" : 34
    }, {
      "referenceID" : 151,
      "context" : "25) and BayesExp (Lattimore, 2013), but not AIXI (Orseau, 2013) sublinear regret impossible in general environments, but possible with recoverability (Theorem 5.",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 163,
      "context" : "For MDPs, planning is already P-complete for finite and infinite horizons (Papadimitriou and Tsitsiklis, 1987).",
      "startOffset" : 74,
      "endOffset" : 110
    }, {
      "referenceID" : 138,
      "context" : "The existence of a policy whose expected value exceeds a given threshold is PSPACE-complete (Mundhenk et al., 2000), even for purely epistemic POMDPs in which actions do not change the hidden state (Sabbadin et al.",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 175,
      "context" : ", 2000), even for purely epistemic POMDPs in which actions do not change the hidden state (Sabbadin et al., 2007).",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 65,
      "context" : "In contrast, Hutter (2005) defines the value function as the limit of the iterative value function.",
      "startOffset" : 13,
      "endOffset" : 27
    }, {
      "referenceID" : 65,
      "context" : "We gave a different, more complicated proof in Leike and Hutter (2015b). The following, much simpler and more elegant proof is due to Sterkenburg (2016, Prop.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 65,
      "context" : "In Leike and Hutter (2015a) the use of the symbols V and W is reversed.",
      "startOffset" : 13,
      "endOffset" : 28
    }, {
      "referenceID" : 131,
      "context" : "of ending is a philosophical debate we do not want to engage in here; see Martin et al. (2016) for a discussion.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 86,
      "context" : "Kalai and Lehrer (1993) show that in infinitely repeated games Bayesian agents converge to an ε-Nash equilibrium as long as each agent assigns positive prior probability to the other agents’ policies (a grain of truth).",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 24,
      "context" : "Alternating strategies like TitForTat (cooperate first, then play The idea for reflective oracles was developed by Jessica Taylor and Benya Fallenstein based on ideas by Paul Christiano (Christiano et al., 2013).",
      "startOffset" : 186,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "Alternating strategies like TitForTat (cooperate first, then play The idea for reflective oracles was developed by Jessica Taylor and Benya Fallenstein based on ideas by Paul Christiano (Christiano et al., 2013). Reflective oracles were first described in Fallenstein et al. (2015a). The proof of Theorem 7.",
      "startOffset" : 187,
      "endOffset" : 283
    }, {
      "referenceID" : 45,
      "context" : "Foster and Young (2001) and Nachbar (1997, 2005) prove several impossibility results on the grain of truth problem that identify properties that cannot be simultaneously satisfied for classes that allow a grain of truth (see Section 7.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 161,
      "context" : "This is not the first time Thompson sampling is used in game theory (Ortega and Braun, 2014), but the first time to show that it achieves such general positive results.",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "The same problem occurs in multi-agent reinforcement learning (Busoniu et al., 2008).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "Assuming convergence to a stationary policy is a necessary criterion to enable all agents to learn, but the process is unstable for many reinforcement learning algorithms and only empirical positive results are known (Bowling and Veloso, 2001).",
      "startOffset" : 217,
      "endOffset" : 243
    }, {
      "referenceID" : 86,
      "context" : "Since our classMrefl solves the grain of truth problem, the result by Kalai and Lehrer (1993) immediately implies that for any Bayesian agents π1, .",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 45,
      "context" : "Foster and Young (2001) present a condition that makes convergence to a Nash equilibrium impossible: if the player’s rewards are perturbed by a small real number drawn from some continuous density ν, then for ν-almost all realizations the players do not learn to predict each other and do not converge to a Nash equilibrium.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 45,
      "context" : "Foster and Young (2001) present a condition that makes convergence to a Nash equilibrium impossible: if the player’s rewards are perturbed by a small real number drawn from some continuous density ν, then for ν-almost all realizations the players do not learn to predict each other and do not converge to a Nash equilibrium. For example, in a matching pennies game, rational agents randomize only if the (subjective) values of both actions are exactly equal. But this happens only with ν-probability zero, since ν is a density. Thus with ν-probability one the agents do not randomize. If the agents do not randomize, they either fail to learn to predict each other, or they are not acting rationally according to their beliefs: otherwise they would seize the opportunity to exploit the other player’s deterministic action. But this does not contradict our convergence result: the class Mrefl is countable and each ν ∈ Mrefl has positive prior probability. Perturbation of rewards with arbitrary real numbers is not possible. Even more, the argument given by Foster and Young (2001) cannot work in our setting: the Bayesian mixture ξ mixes over λT for all probabilistic Turing machines T .",
      "startOffset" : 0,
      "endOffset" : 1082
    }, {
      "referenceID" : 140,
      "context" : "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (Müller and Bostrom, 2016).",
      "startOffset" : 187,
      "endOffset" : 213
    }, {
      "referenceID" : 48,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 208,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 92,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 22,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 178,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 137,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 35,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 180,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 34,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 212,
      "context" : "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.",
      "startOffset" : 155,
      "endOffset" : 337
    }, {
      "referenceID" : 14,
      "context" : "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (Müller and Bostrom, 2016). The advent of strong AI would be the biggest event in human history. Potential benefits are huge, as the new level of automation would free us from any kind of undesirable labor. But there is no reason to believe that humans are at the far end of the intelligence spectrum. Rather, humans barely cross the threshold for general intelligence to be able to use language and do science. Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.g., through self-amplification effects from AIs improving themselves (if doing AI research is one of humans’ capabilities, then a machine that can do everything humans can do can also do AI research). Once machine intelligence is above or far above human level, machines would steer the course of history. There is no reason to believe that machines would be adversarial to us, but nevertheless humanity’s fate might rest at the whims of the machines, just as chimpanzees today have no say in the large-scale events on this planet. Bostrom (2002) defines: Existential risk — One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.",
      "startOffset" : 199,
      "endOffset" : 1537
    }, {
      "referenceID" : 14,
      "context" : "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (Müller and Bostrom, 2016). The advent of strong AI would be the biggest event in human history. Potential benefits are huge, as the new level of automation would free us from any kind of undesirable labor. But there is no reason to believe that humans are at the far end of the intelligence spectrum. Rather, humans barely cross the threshold for general intelligence to be able to use language and do science. Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.g., through self-amplification effects from AIs improving themselves (if doing AI research is one of humans’ capabilities, then a machine that can do everything humans can do can also do AI research). Once machine intelligence is above or far above human level, machines would steer the course of history. There is no reason to believe that machines would be adversarial to us, but nevertheless humanity’s fate might rest at the whims of the machines, just as chimpanzees today have no say in the large-scale events on this planet. Bostrom (2002) defines: Existential risk — One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential. Existential risks are events that have the power to extinguish human life as we know it. Examples are cosmic events such as an asteroid colliding with Earth. But cosmic events are unlikely on human timescales compared to human-made existential risks from nuclear weapons, synthetic biology, and nanotechnology. It is possible that artificial intelligence also falls into this category. Vinge (1993) was the first person to recognize this:",
      "startOffset" : 199,
      "endOffset" : 2106
    }, {
      "referenceID" : 16,
      "context" : "Bostrom (2003) picked up on this issue very early and gave the topic credibility through his well-researched and carefully written book Superintelligence (Bostrom, 2014).",
      "startOffset" : 154,
      "endOffset" : 169
    }, {
      "referenceID" : 57,
      "context" : "In 2014 high-profile scientists such as Stephen Hawking, Max Tegmark, Stuart Russell, and Frank Wilczek have warned against the dangers posed by AI (Hawking et al., 2014).",
      "startOffset" : 148,
      "endOffset" : 170
    }, {
      "referenceID" : 186,
      "context" : "Moreover, the Future of Life Institute and the Machine Intelligence Research Institute have formulated concrete technical research priorities to make AI more robust and beneficial (Soares and Fallenstein, 2014; Russell et al., 2015).",
      "startOffset" : 180,
      "endOffset" : 232
    }, {
      "referenceID" : 168,
      "context" : "Moreover, the Future of Life Institute and the Machine Intelligence Research Institute have formulated concrete technical research priorities to make AI more robust and beneficial (Soares and Fallenstein, 2014; Russell et al., 2015).",
      "startOffset" : 180,
      "endOffset" : 232
    }, {
      "referenceID" : 13,
      "context" : "Bostrom (2003) picked up on this issue very early and gave the topic credibility through his well-researched and carefully written book Superintelligence (Bostrom, 2014).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "(See also Alexander (2015) for a collection of positions by prominent AI researchers.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea.",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from ‘HLAI is so far away that any worry is misplaced’ to claims that ‘the safety problem would not be so hard’.",
      "startOffset" : 92,
      "endOffset" : 303
    }, {
      "referenceID" : 14,
      "context" : "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from ‘HLAI is so far away that any worry is misplaced’ to claims that ‘the safety problem would not be so hard’.",
      "startOffset" : 92,
      "endOffset" : 314
    }, {
      "referenceID" : 14,
      "context" : "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from ‘HLAI is so far away that any worry is misplaced’ to claims that ‘the safety problem would not be so hard’.",
      "startOffset" : 92,
      "endOffset" : 328
    }, {
      "referenceID" : 14,
      "context" : "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from ‘HLAI is so far away that any worry is misplaced’ to claims that ‘the safety problem would not be so hard’.",
      "startOffset" : 92,
      "endOffset" : 349
    }, {
      "referenceID" : 14,
      "context" : "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from ‘HLAI is so far away that any worry is misplaced’ to claims that ‘the safety problem would not be so hard’. See Sotala and Yampolskiy (2014) for a discussion.",
      "startOffset" : 92,
      "endOffset" : 537
    }, {
      "referenceID" : 31,
      "context" : "In particular, value learning (Dewey, 2011), self-reflection (Soares, 2015; Fallenstein et al.",
      "startOffset" : 30,
      "endOffset" : 43
    }, {
      "referenceID" : 185,
      "context" : "In particular, value learning (Dewey, 2011), self-reflection (Soares, 2015; Fallenstein et al., 2015b), self-modification (Orseau and Ring, 2011, 2012a; Everitt et al.",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 38,
      "context" : ", 2015b), self-modification (Orseau and Ring, 2011, 2012a; Everitt et al., 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.",
      "startOffset" : 28,
      "endOffset" : 80
    }, {
      "referenceID" : 155,
      "context" : ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.",
      "startOffset" : 26,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.",
      "startOffset" : 26,
      "endOffset" : 82
    }, {
      "referenceID" : 37,
      "context" : ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al., 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 166,
      "context" : ", 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).",
      "startOffset" : 67,
      "endOffset" : 116
    }, {
      "referenceID" : 36,
      "context" : ", 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).",
      "startOffset" : 67,
      "endOffset" : 116
    }, {
      "referenceID" : 57,
      "context" : "We built on top of Hutter’s theory of universal artificial intelligence. Chapter 3 discussed the formal theory of learning. Chapter 4 presented several approaches to acting in unknown environments (Bayes, Thompson sampling, knowledge-seeking agents, and BayesExp). Chapter 5 analysed these approaches and discussed notions of optimality and principled problems with acting under uncertainty in general environment. Chapter 6 provided the mathematical tools to analyze the computational properties of these models. Finally, Chapter 7 solved the grain of truth problem, which lead to convergence to Nash equilibria in unknown general multi-agent environments. Our work is theoretical by nature and there is still a long way to go until these results make their way into applications. But a solution in principle is a crucial first step towards solving a problem in practice. Consider the research paper by Shannon (1950) on how to solve chess in principle.",
      "startOffset" : 19,
      "endOffset" : 919
    } ],
    "year" : 2016,
    "abstractText" : "Reinforcement learning problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider reinforcement learning in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm: we assume the data is drawn from an unknown source that belongs to a known countable class of candidates. First, we consider the passive (sequence prediction) setting, learning from data that is not independent and identically distributed. We collect results from artificial intelligence, algorithmic information theory, and game theory and put them in a reinforcement learning context: they demonstrate how an agent can learn the value of its own policy. Next, we establish negative results on Bayesian reinforcement learning agents, in particular AIXI. We show that unlucky or adversarial choices of the prior cause the agent to misbehave drastically. Therefore Legg-Hutter intelligence and balanced Pareto optimality, which depend crucially on the choice of the prior, are entirely subjective. Moreover, in the class of all computable environments every policy is Pareto optimal. This undermines all existing optimality properties for AIXI. However, there are Bayesian approaches to general reinforcement learning that satisfy objective optimality guarantees: We prove that Thompson sampling is asymptotically optimal in stochastic environments in the sense that its value converges to the value of the optimal policy. We connect asymptotic optimality to regret given a recoverability assumption on the environment that allows the agent to recover from mistakes. Hence Thompson sampling achieves sublinear regret in these environments. AIXI is known to be incomputable. We quantify this using the arithmetical hierarchy, and establish upper and corresponding lower bounds for incomputability. Further, we show that AIXI is not limit computable, thus cannot be approximated using finite computation. However there are limit computable ε-optimal approximations to AIXI. We also derive computability bounds for knowledge-seeking agents, and give a limit computable weakly asymptotically optimal reinforcement learning agent. Finally, our results culminate in a formal solution to the grain of truth problem: A Bayesian agent acting in a multi-agent environment learns to predict the other agents’ policies if its prior assigns positive probability to them (the prior contains a grain of truth). We construct a large but limit computable class containing a grain of truth and show that agents based on Thompson sampling over this class converge to play ε-Nash equilibria in arbitrary unknown computable multi-agent environments.",
    "creator" : "LaTeX with hyperref package"
  }
}