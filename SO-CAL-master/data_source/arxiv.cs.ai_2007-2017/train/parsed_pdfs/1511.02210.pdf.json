{
  "name" : "1511.02210.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Optimized Or’s of And’s",
    "authors" : [ "Tong Wang", "Cynthia Rudin" ],
    "emails" : [ "tongwang@mit.edu", "rudin@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Or’s of And’s (OA) models are comprised of a small number of disjunctions of conjunctions, also called disjunctive normal form. An example of an OA model is as follows: If (x1 = ‘blue’ AND x2 = ‘middle’) OR (x1 = ‘yellow’), then predict Y = 1, else predict Y = 0. Or’s of And’s models have the advantage of being interpretable to human experts, since they are a set of conditions that concisely capture the characteristics of a specific subset of data. We present two optimization-based machine learning frameworks for constructing OA models, Optimized OA (OOA) and its faster version, Optimized OA with Approximations (OOAx). We prove theoretical bounds on the properties of patterns in an OA model. We build OA models as a diagnostic screening tool for obstructive sleep apnea, that achieves high accuracy with a substantial gain in interpretability over other methods."
    }, {
      "heading" : "1 Introduction",
      "text" : "We present mathematical programming formulations for producing Or’s of And’s, which are sparse disjunctive normal form (DNF) expressions. An OA model might say, for instance, consumers who are female AND single, AND younger than 35 years old, OR married AND earn more than $100K per year, are likely to purchase a product. In creating predictive models for healthcare, marketing, sociology, and in other domains, two aspects have long since been of interest: logical forms, and sparsity (see, e.g. Dawes, 1979; Johnson-Laird, Khemlani, and Goodwin, 2015; Miller, 1956). For example, physicians use sparse, easily checkable sets of conditions (symptoms, observations) to classify patients as to whether they have a dis-\nease. DNF formulae are particularly useful as screening models, where patients who do not meet the Or of And’s criteria are not considered for further testing. In marketing, DNF is also called “disjunctions of conjunctions\" or “non-compensatory decision rules.\" Marketing researchers strongly hypothesize that consumers use simple rules to screen products, and they would consider purchasing only the products in this consideration set to reduce the cognitive load of considering all products. The consideration set may be precisely an Or’s of And’s classifier (Hauser et al., 2010; Gilbride and Allenby, 2004).\nDespite the efforts that theoretical communities have placed on learning DNF (e.g., Klivans and Servedio, 2001; Littlestone, 1988; Ehrenfeucht et al., 1989), the algorithms designed for inductive logic programming that essentially produce DNF (Muggleton and De Raedt, 1994; Lavrac and Dzeroski, 1994), the associative classification algorithms (Ma, Liu, and Hsu, 1998; Han, Pei, and Yin, 2000; Li, Han, and Pei, 2001; Yin and Han, 2003; Chen et al., 2006; Cheng et al., 2007), and rule induction methods (e.g., Cohen, 1995), there has been little in the way of algorithms designed for applications where cognitive simplicity is first and foremost (with some exceptions, like Hauser et al., 2010, which we discuss later). For instance, Yin and Han (2003) reported that the average number of rules used by CMAR was 305 and CPAR had 244 rules on average, on 26 datasets from UCI ML Repository (Lichman, 2013); these are not cognitively simple models. Besides, all of the algorithms discussed above use greedy approximations, which hurts accuracy and sparsity. For instance, RIPPER employs local greedy splitting, meaning that a mistake at the beginning is difficult to undo. Inductive logic programming starts with a collection of rules and locally combines them. The associative classification methods also follow separateand-conquer or covering strategies. Unlike these methods, our methods aim to produce cognitive simple models, and do not use greedy approximations or similar heuristics.\nThe closest work to ours are that of Hauser et al. (2010) in the marketing literature, and Wang et al. (2015) on Bayesian modeling of DNF formulae. Hauser et al. (2010) pre-mines the rules and then uses an integer program equivalent to a set-covering problem. They try to minimize the\nar X\niv :1\n51 1.\n02 21\n0v 1\n[ cs\n.A I]\n6 N\nov 2\n01 5\nerror of coverage while favoring patterns that fit the largest subset of data. The work of Wang et al. (2015) has the advantage of a Bayesian interpretation of the prior parameters, but a disadvantage in that the analytical bounds on the maximum a posteriori solution of the Bayesian method are weaker than those we present in this paper for the minimizer of the optimization problem.\nOur goal is cognitive simplicity, as well as predictive accuracy. We choose mathematical programming (mixedinteger and integer linear programming – MIP and ILP) and rule mining to form our models. Using these tools have several benefits, namely flexibility on the user’s side on the objective and constraints, fast solvers that have been improving exponentially over recent years, and a guarantee on the optimality of the solution. We improve computation also using statistical approximations. In one of our algorithms, OOAx, we first mine rules and design an ILP to choose the subset of rules to form the OA model. This is a statistical assumption that dramatically speeds up computation, but we can show (in Theorem 2) that as long as we mine all rules with sufficiently high support, the optimal solution will be attained anyway. We also present various bounding conditions on the optimal solution."
    }, {
      "heading" : "2 Optimized Or’s of And’s",
      "text" : "Let us discuss the first framework for learning Or’s of And’s, called Optimized Or’s of And’s (OOA). We work with a data set S = {(Xn, Yn)}Nn , consisting of N examples with J attributes of mixed type. Yn ∈ {1,−1} represents the labels. Numerical attributes are indexed by index set Jn and categorical attributes are indexed by Jc. The j-th attribute of the n-th example is denoted as Xnj .\nAn OA classifier consists of a set of patterns that characterize a single class, here, the positive class. Each pattern is a conjunction of conditions (literals), and the number of conditions is called the length of a pattern. For example. the length of pattern “age ≥ 30 AND has hypertension AND is female” is 3. Let z denote a pattern, and 1z(X) indicate if X satisfies pattern z. A represents a set of patterns. An OA classifier built on A is denoted as fA:\nfA(X) = { 1 ∃z ∈ A, s.t.1z(X) = 1 0 otherwise.\n(1)"
    }, {
      "heading" : "2.1 MIP Formulation",
      "text" : "We formulate a mixed integer program to generate a pattern set containing numerical and categorical attributes. The MIP uses the following objective L(A) to minimize the training error while maintaining sparseness of the model.\nL(A) = #error(A) N + C1#literals(A) + C2#patterns(A). (2)\nThe first term in the objective is the loss function, which counts the number of classification errors. The regulariza-\ntion terms include 1) the total number of literals in A, denoted as #literals(A), which is the sum of the length of each pattern in A, and 2) the total number of patterns in A, denoted as #patterns(A). The two terms are scaled by parameters C1 and C2 to penalize the complexity of the model. C1 represents the percentage of training errors the user is willing to trade in order to reduce a pattern by one literal. Similarly, C2 represents the percentage of training errors a user needs to trade to reduce one pattern. A user can tune C1 and C2 to influence the shape of the output.\nNow we explain how the constraints work. A challenging part is to deal with both numerical and categorical attributes in the MIP. For numerical attributes, the MIP needs to select the upper and lower boundary to form a range; for categorical attributes, it needs to select a category for a literal. Simultaneously, the MIP needs to decide for each example if it satisfies the literals and patterns. All of the constraints are linear in the decision variable, to ensure a duality gap or proof of optimality on the solution we obtain."
    }, {
      "heading" : "2.1.1 Literals for Numerical Attributes",
      "text" : "For a numerical attribute, a literal j (for simplicity, when we refer to literal j, we mean a literal containing attribute j) has the form “lkj ≤ X·j ≤ ukj”, where ukj and lkj represent the upper and lower boundary of the range in literal j. k is a pattern index. For each example Xn, let ûnkj ∈ {0, 1} indicate if Xnj satisfies the upper bound ukj and l̂nkj ∈ {0, 1} indicate if Xnj satisfies the lower bound lkj . That is, ûnkj = 1 if Xnj ≤ ukj , and l̂nkj = 1 if Xnj ≥ lkj . Using a big M formulation, we obtain the following constraints, that for ∀n, k, ∀j ∈ Jn,\nukj −Xnj ≤Mûnkj , (3) ukj −Xnj ≥M(ûnkj − 1) + , (4)\nXnj − lkj ≤Ml̂nkj , (5)\nXnj − lkj ≥M(l̂nkj − 1) + , (6) ukj ≤ Uj (7) lkj ≥ Lj (8)\nA small number is used to force ûnkj = 1 when Xnj = ukj , and force l̂nkj = 1 when Xnj = lkj . Uj and Lj denote the maximum and minimum value of attribute j. Constraints (7) and (8) bound on ukj and lkj to ensure a bounded M for computation efficiency.\nIt is possible that the upper and lower bounds apply to all examples, when both constraints (7) and (8) are binding. In that case, the literal does not have any classification power. We need numerical literals that are meaningful, or what we call substantive, that their ranges only apply to a subset of training examples. This means at most one of constraints (7) and (8) can be binding. Let a binary variable δkj indicates if literal j in pattern k is substantive. We use a\nbig M formulation to construct the constraints. Therefore ∀k, ∀j ∈ Jn,\nMδkj ≥ Uj − ukj , (9) Mδkj ≥ lkj − Lj , (10) M(1− δkj) ≥ (ukj − lkj)− (Uj − Lj) + , (11)\nConstraint (9) means δkj = 1 if ukj < Uj . Constraint (10) means δkj = 1 if lkj > Lj . (11) forces δkj = 0 when ukj = Uj and lkj = Lj , i.e., literal j is non-substantive."
    }, {
      "heading" : "2.1.2 Literals for Categorical Attributes",
      "text" : "For a categorical attribute, a literal j has the form “X·j = the v-th category”, where v ∈ {1, ...Vj} is an index for categories of attribute j and Vj is the total number of categories. We use okjv ∈ {0, 1} to indicate whether the vth category of attribute j is present in literal j of pattern k. To determine if Xn satisfies the condition in literal j, let ônkj ∈ {0, 1} indicate if Xnj equals the category contained in this literal. We binary code Xnj into Xnjv such that Xnjv = 1 if Xnj takes the v-th category of attribute j. Therefore ônkj = 1 if and only if there exists v ∈ {1, ...Vj} such that okjv = 1 and Xnjv = 1. We formulate it as the following. For ∀n, k, ∀j ∈ Jc,\nônkj ≤ ∑ v Xnjvokjv, (12)\nVj ônkj ≥ ∑ v\nXnjvokjv, (13)∑ v∈{1,...,Vj} okjv ≤ 1. (14)\n(14) ensures each categorical literal contains at most one value. This constraint forces a pattern to have a fixed form. If we remove the constraint and allow a literal to take multiple values, a pattern could have the following form: “5 ≤ X1 ≤ 20 AND X2 = red or blue.” It depends on the application and users’ preference as to whether leave this constraint in the MIP. The model will work the same without changing the rest of the formulation.\nWe define that a categorical literal j is substantive, if there exists some v ∈ {1, ..., Vj} such that okjv = 1, indicated by δkj ∈ {0, 1}. For ∀k,∀j ∈ Jc,\nδkj ≤ ∑ v∈Vj okjv, (15)\nVjδkj ≥ ∑ v∈Vj okjv. (16)"
    }, {
      "heading" : "2.1.3 Counting Classification Errors",
      "text" : "Given the literals,Xn satisfies pattern k if and only if it satisfies every literal in the pattern. For categorical attributes, we consider both cases where a literal is substantive, i.e.,\nδkj = 1, and we need ônkj = 1; or non-substantive, i.e. δkj = 0, and ônkj is always 0 for all v. For numerical attributes, when the literal is substantive, the MIP needs to check if a data point satisfies both upper and lower bounds of the range, indicated by ûnkj and l̂nkj ; when the literal is non-substantive, i.e., ukj = Uj and lkj = Lj , then ûnkj = 1 and l̂nkj = 1 for all Xn. Using ωnk ∈ {0, 1} to indicate if Xn satisfies pattern k, the above conditions can be formulated below. ∀n, k,\nζk + ∑ j∈Jn ( ûnkj + l̂nkj ) + ∑ j∈Jc (ônkj + 1− δkj)−\n(2|Jn|+ |Jc|) ≤ ωnk, (17) (2|Jn|+ |Jc|+ 1)ωnk ≤ ζk + ∑ j∈Jn ( ûnkj + l̂nkj ) +\n∑ j∈Jc (ônkj + 1− δkj) . (18)\nLet ξn ∈ {0, 1} indicate if a classification error is made, which means either a positive data point does not satisfy any pattern, or a negative data point satisfies at least one pattern. In both cases ξn = 1. These two situations are captured by constraints (19) and (20).\nξn + K∑ k ωnk ≥ 1,∀n ∈ I+, (19)\nKξn ≥ K∑ k ωnk,∀n ∈ I−, (20)\nwhere I+ denotes the set of indices for positive examples and I− denotes the set of indices for negative examples. K is the upper bound on the number of patterns that we allow the solution to have. MIP creates this K “boxes” that will be filled up as it searches in the solution space.\nWhen the MIP is formulated, we do not know how many out of the K “boxes” the MIP will use. Therefore we introduce binary variables ζk ∈ {0, 1} to indicate if pattern k is non-empty in the final pattern set, which means it contains at least one substantive literal. For ∀k,\nJζk ≥ ∑ j δkj . (21)\nSince we are minimizing the total number of patterns in the objective, the constraint will always be binding."
    }, {
      "heading" : "2.1.4 The Objective",
      "text" : "Now we reprsent the objective using decision variables introduced before. The MIP minimizes\n1\nN N∑ n=1 ξn + C1 K∑ k J∑ j δkj + C2 K∑ k=1 ζk\nover variables ωnk,ukj , lkj , ûnkj , l̂nkj , okjv , ônkj , δkj , δj , ξn, and ζk, such that they satisfy constraints (3) to (21).\nThe complexity of the MIP comes from three aspects, 1) choosing the upper and lower boundaries for ranges in numerical literals, and picking categories for categorical literals, 2) deciding for each example if it satisfies every literal and every pattern, and 3) deciding how many patterns are constructed from the K “boxes.” There are in total O(NKJ) constraints and O(NKJ) decision variables for this MIP, though the full matrix of variables corresponding to the mixed integer programming formulation is sparse since most literals operate only on a small subset of the data. This formulation can be solved efficiently for small to medium sized datasets. As the size of the dataset grows, the computation gets complicated. We might need a faster method that operates in an approximate way on a much larger scale, presented below."
    }, {
      "heading" : "3 Optimized Or’s of And’s with Approximations",
      "text" : "To speed up the learning process, we propose Optimized Or’s of And’s with Approximations (OOAx), that separates from the optimization process, the first two previously mentioned aspects of complexity. OOAx uses a pre-mining then selecting approach. It takes advantage of mature pattern mining techniques to generate a set of patterns. Then a secondary criteria is applied to further screen the rules to form a candidate pattern set. Finally, an integer linear program (ILP) searches within these patterns set for an optimal set. This method consists of following three steps, pattern mining, pattern screening and pattern selecting."
    }, {
      "heading" : "3.1 Pattern Mining",
      "text" : "There are many frequently used pattern mining methods such as FP-growth (Han, Pei, and Yin, 2000), Apriori (Agrawal, Srikant, and others, 1994), Eclat (Zaki et al., 1997), etc. In our implementation, we use FP-growth in python (Borgelt, 2005) that takes the binary coded data, and user specified minimum support and maximum length, to generate patterns that satisfy the two requirements. The algorithm runs sufficiently fast (usually less than a second for thousands of observations). Since the FP-growth algorithm handles binarized data, we discretize the numerical attributes by manually selecting thresholds for bins. For instance, X = 3.5 can be transformed into 2 ≤ X ≤ 4, etc. Note that there are other pattern mining techniques that handle real-valued variables."
    }, {
      "heading" : "3.2 Pattern Screening",
      "text" : "In the pattern mining step, the number of generated patterns is usually overwhelming for even a medium size data set. For instance, for the sleep apnea data set (which we will\ndiscuss in detail in the experiment sections) of size 1192 patients and 112 binary coded attributes, if the maximum length is 3 and the minimum support is 5%, millions of patterns are generated. Ideally, we would like the candidate pattern set to contain thousands of patterns for computational convenience. Therefore, we use a secondary criteria to further screen the patterns.\nScore(z) = InfoGain(S|z)− γlz. (22)\nThis criteria considers the classification power of a pattern, measured by information gain InfoGain(S|z), and the sparsity, measured by the length of the pattern lz . Information gain of pattern z on data S is InfoGain(S|z) = H(S) − H(S|z), where H(S) is the entropy of S, written as H(S) = − ∑ i Pi logPi. H(S|z) is the conditional entropy of S. Using this criteria, we select a set of candidate patterns P of size KP .\nTo represent the sparseness of each pattern, we create a binary matrix P of size KP × J , where each row represents which attribute is present in a pattern. For instance, Pkj = 1 indicates that literal j is substantive in pattern k, and Pkj = 0 otherwise. We also need to determine for each example, which of the KP patterns it satisfies. For a data set with N examples, we create a matrix W of size N × KP , where the k-th element in the n-th row, ωnk, indicates if the n-th observation satisfies pattern k. Both matrices are pre-computed before the final step."
    }, {
      "heading" : "3.3 Pattern Selecting",
      "text" : "The previous two steps greatly reduce the computational load by feeding the final step with a set of high quality candidate patterns. Now our goal is only to select an optimal set A from the candidate set P . We formulate an ILP using the same objective (2), and present it below.\nmin ξn,ζk\n1\nN N∑ n=1 ξn + C1 KP∑ k ζklk + C2 KP∑ k ζk\nsuch that\nξn + KP∑ k=1 ωnkζk ≥ 1,∀n ∈ I+ (23)\nKξn ≥ KP∑ k=1 ωnkζk,∀n ∈ I− (24)\nξn, ζk ∈ {0, 1}. (25)\nThe length of pattern k, lk, can be pre-computed by lk =∑J j=1 Pkj . Constraint (23) means that an error occurs for a positive example if it does not satisfy any patterns that are selected. Constraint (24) means that an error occurs for a negative example if it satisfies at least one pattern that is selected. This ILP only involves O(N) constraints and\nO(N) +O(KP) variables, which is much simpler than the MIP in an OOA framework.\nThe difference between OOA and OOAx method is that the latter avoids forming patterns in the optimization process, by handling it to other efficient off-the-shelf algorithms. Separating the mining step from the optimization problem renders more control to users over the quality and size of desired patterns. Users can manually modify the premining and screening process by applying domain-specific minimum support, maximum length and secondary selection criteria."
    }, {
      "heading" : "4 Analysis on Patterns and OA Models",
      "text" : "In this section, we discuss the quality of patterns in an OA classifier. Certain properties of the patterns improve computation complexity. We also show the VC dimension of OA models and compare OA classifiers with other discrete classifiers (decision trees and random forests). Due to the page limit, some proofs are provided in the supplementary material."
    }, {
      "heading" : "4.1 Bounds on Patterns",
      "text" : "Define the support set of pattern z over data set S as\nIS(z) = {X|1z(X) = 1, X ∈ S}, (26)\nand the support of pattern z over S as\nsuppS(z) = |IS(z)|. (27)\nsuppS +\n(z) is called the positive support of z, which is the number of positive examples in IS(z), and suppS−(z) is called the negative support of z, which is the number of negative examples in IS(z).\nAn OA classifier is essentially an ensemble of weaker classifiers, patterns. Including patterns with a low quality is expensive, and as we will prove, unnecessary. First we show in Theorem 1 that the optimal solution never includes a pattern with a high negative support.\nTheorem 1 Take an OA model with regularization parameters C1 and C2. The OA model is trained on a data set S, consisting of N examples, N+ of which are positive examples. If A∗ ∈ arg minA L(A), then for any z ∈ A∗, suppS − (z) ≤ N+ −N (C1 + C2).\nThis means after pattern mining, we can safely reduce the pattern space by disregarding patterns with a negative support above N+ − N (C1 + C2). Similarly, we can also prove that if a pattern has a low positive support, removing it always achieves a better objective. Let A\\z denote the pattern set with pattern z removed from A.\nTheorem 2 Take an OA model with regularization parameters C1 and C2. The OA model is trained on a data set S,\nconsisting N examples. If suppS +\n(z) ≤ (C1 +C2)N , then L(A\\z) ≤ L(A).\nIt means we need not bother mining rules of low positive support. Theorem 2 is a stronger statement than Theorem 1 since it provides a lower bound on positive support for patterns in all pattern sets and saying that removing a low supported pattern always improves the performance; while Theorem 2 only applies to optimal solutions.\nWith the above theoretical guarantees, we know it is safe to reduce the pattern space, by setting the minimum positive support to be (C1 + C2)N when we pre-mine the patterns and throwing away patterns with negative support higher than N+ −N (C1 + C2) in the screening stage. This does not benefit an OOA framework as it directly forms patterns. But it provides strong computational motivation for premining patterns in an OOAx framework.\nThe sparseness of a model is also associated with the number of patterns in an OA model. We prove in Theorem 3 that the number of patterns in an optimal pattern set is upper bounded.\nTheorem 3 Take an OA model with regularization parameters C1 and C2. The OA model is trained on a set of N examples, N+ of which are positive examples. If A∗ ∈ arg minA L(A), then |A∗| ≤ N +/N C1+C2 .\nThis theorem is meaningful not only for showing the simplicity of the output, but also gives us a suggestion for K when we use the MIP in an OOA framework. Knowing that the optimal set can never be larger than N\n+/N C1+C2 , we can\nsafely set K to be N +/N\nC1+C2 . The smaller K can be set, the\nbetter it is computationally for the MIP."
    }, {
      "heading" : "4.2 VC Dimension of an OA classifier",
      "text" : "Let us consider the VC dimension of hypothesis classes representing pattern sets selected from a pre-mined set P . There are some results for k-DNF (Ehrenfeucht et al., 1989) and monotone functions (that is, Boolean functions that can be represented without negated literals) (Procaccia and Rosenschein, 2006). Littlestone (1988) has shown that the class of k-term monotone l-DNF formulas (i.e., with monomials containing at most l variables) has VC dimension at least lkblog( nm )c, where l ≤ m ≤ n, and k ≤ ( m l ) . However, his theorem does not have the constraint that the patterns come from a fixed pattern set.\nLet S = RJ represent the complete set of all possible data that could be constructed from J attributes. To compute the VC dimension, we introduce the following definition.\nDefinition 1 An Efficient Set of P is a set of patterns where the support set of each pattern is not a subset of the rest of the efficient set, i.e.,\nPE = {z|z ∈ P, IS(z) 6⊂ IS(PE\\z)}.\nThis means for any pattern z in PE , there exists data points that satisfy only z and none of the rest of the patterns in PE . We call the efficient set with the maximum number of patterns the Maximum Efficient Set of P , denoted as PEmax. We claim that the VC dimension of OOAx learned fromP depends on the size ofPEmax, stated as the following.\nTheorem 4 The VC dimension of an OA classifier f built from P equals the size of the maximum efficient set of P:\nVCdim(f) := |PEmax|.\nProof 1 First we prove that VCdim(f) ≥ |PEmax|, which means there exists a set of |PEmax| examples X1, ...X|PEmax| that any labels Y1, ..., Y|PEmax| can be realized by a classifier f built from P . To construct this example set, we use the maximum efficient set PEmax. For any pattern zi in PEmax, since IS(zi) 6⊂ IS(PEmax\\zi), there always exists a data point Xi ∈ S that satisfies only zi, i.e.,\nXi ∈ IS(PEmax\\z)− IS(z),\nfor i ∈ {1, ..., |PEmax|}. Each Xi is covered by exactly one pattern in PEmax. These points can always be shattered since for any labels, we can from a pattern set A = {zi|zi ∈ PEmax, s.t. 1zi(Xi) = 1, Yi = 1}. Therefore, all possible labels of Y1, ..., YN can be realized, which means that VCdim(f) ≥ |PEmax|.\nThen we show VCdim(f) ≤ |PEmax|. We prove this by contradiction. Let PEmax be the maximum efficient set of P . Assume there exists a set of h examples X1, ...Xh where h > |PEmax|, and their labels Y1, ...Yh can always be realized. Let 0\\i denote an all-zero vector of size h except a one at the i-th position. For 0\\i to be a realizable set of labels, there must exist a pattern zi that satisfies 1zi(Xi) = 1 and 1zi(Xj) = 0 for j 6= i. This should be true for all i ∈ {1, ..., h}. Therefore, there must exist h such patterns that each of them covers a data point that only satisfies this pattern. According to definition 1, this is equivalent to declaring that these h patterns is an efficient set, and the size of the set is h, which is greater than |PEmax|. This contradicts the assumption that PEmax is the maximum efficient set and should contain the largest number of patterns. Therefore, VCdim(f) ≤ |PEmax|.\nThus, we conclude that the VC-dimension of a classifier f built from P is |PEmax|. (Learning an efficient set will be another topic that we do not discuss in this paper.)"
    }, {
      "heading" : "4.3 Comparing with Other Discrete Classifiers",
      "text" : "Like OA classifiers, decision trees and random forests also discretize the input space and assign each subspace with a label. We prove that for these models, there always exist equivalent OA classifiers. These theorems are simple, but may not be obvious to those who have not thought about it.\nWe present the definition of two classifiers being equivalent below.\nDefinition 2 Two classifiers f1, f2 are equivalent if for any input X , f1(X) = f2(X).\nIn a decision tree, the leaves divide up the input space into areas with different labels, which will be the predicted outcome for any data that ends up in that area. A path from the root to a leaf is a conjunction of literals, i.e., a pattern. See Figure 1 as an example. The decision tree ends up with\n4 leaves, and therefore 4 patterns. To convert the tree into an equivalent OA classifier, we simply collect the patterns that are associated with positive leaves, in this case, leaf 4 and 6, shown in grey boxes.\nTheorem 5 For any decision tree f1, there exists an equivalent OA classifier f2, where the number of patterns in f2 equals to the number of positive Y labels in f1.\nSimilar inductions hold for random forests. Random forest is an ensemble method based on decision trees. If an input data point falls into a positive leaf in at least half of all the trees, then it is labeled as positive. Therefore, the equivalent OA classifier consists of patterns that are conjunctions of positive patterns from at least half of all the trees. We summarize the above statements into Theorem 6.\nTheorem 6 For any random forest f1, there exists an equivalent OA classifier f2. If f1 consists of Krf decision trees, and the k-th tree has nk positive leaves for k ∈ {1, ...,Krf}, then the size of the pattern set in f2 is upper bounded by ∑ π∈Π ∏ k∈π nk, where Π is a collection of all possible combinations of bKrf2 c+ 1 elements selected from {1, ...,K}.\nThe size is upper bounded by instead of exactly equal to∑ π∈Π ∏ k∈π nk because some patterns could be equivalent, or contained in others. Note that we only need conjunction of bKrf2 c+1 patterns because conjunctions of more than that are contained in conjunctions of exactly bKrf2 c+ 1 positive patterns.\nThe above theorems provide theoretical guarantees that OA classifiers can be as good as decision trees and random forests, in terms of predictive performance, although it might not be desired to create complex OA models, since the whole purpose of designing an OA model is to favor its interpretability over other models."
    }, {
      "heading" : "5 Experiments",
      "text" : "Our experiments include applying OA models to diagnose obstructive sleep apnea (OSA) and experimenting on 9 public datasets from UCI Machine Learning repository (Lichman, 2013).\nTo construct simple OA models for interpretability purposes, we set the maximum number of patterns to be 5 in all experiments. (In an OOA framework, we set K = 5; in a OOAx framework, we add a constraint that the sum of ζk’s is less than or equal to 5.) Since we placed strong restrictions on the characteristics of OA models, we expect to lose predictive accuracy over unrestricted baseline methods. In many of the experiments we did, we found that OA models do not lose in performance, and most of the time are the top performing models, while achieving a substantial gain in interpretability."
    }, {
      "heading" : "5.1 Diagnosing Obstructive Sleep Apnea",
      "text" : "The main experimental result is an application of OA models to build a diagnostic screening tool based on routinely available medical information. We analyzed polysomnography and self-reported clinical information from 1922 patients tested in the clinical sleep laboratory of Massachusetts General Hospital, first analyzed in 2015 (Ustun and Rudin, 2015; Ustun et al., 2015). The goal is to classify which patients who enter into the MGH Sleep Lab have OSA, based on a survey filled out upon admission. We produce predictive models for OSA screening using attributes from self-reported symptoms and self-reported medical information. The attributes include detailed information such as age, sex, BMI, sleepiness, if the patient snores, if the patient wakes up during sleep, if the patient falls back to sleep easily, the level of tiredness, etc. The data set was binary coded into 112 attributes.\nDue to the size of this dataset, we chose OOAx for faster computation. We mined patterns with minimum support of 5% and maximum length of 3. We tuned parameters C1 and C2 using nested cross-validation to obtain the best performance, under the constraint that the pattern size cannot exceed 5. We measured out-of-sample performance using accuracy from 5-fold cross validation for OA models and 5 other methods that adhere to a certain level of interpretability, BOA (Wang et al., 2015), Lasso, C4.5, CART and RIPPER. For all baseline methods, we tuned the hyperparameters with grid search in nested cross validation. The results are displayed in Table 1.\nTo compare interpretability, we reported the complexity of each model averaged across 5 folds. For OA and BOA models, we reported the total number of patterns, average length of patterns and the total number of literals. OA models achieve the same performance as BOA models but with higher interpretability. This is due to a more flexible control over the size and shape of the pattern set compared to BOA models. RIPPER models are decision lists, having a different form than Or’s of And’s. We reported the total number of rules, average length of rules and total number of literals. For decision trees C4.5 and CART, we reported the depth of a tree, and the total number of nodes in a tree. For lasso, we reported the number of non-negative coefficients. Since baseline models have different logical forms than OA models, we compare one universal metric, the number of literals/nodes used in each model, marked in bold in Table 1. We find that OA models used substantially fewer literals than all other models while achieving a competitive accuracy to all models.\nAn example of an OA model is shown below. if a patient satisfies (age ≥ 30 AND patient checked snoring as a potential symptom in the questionnaire),\nOR (age ≥ 30 AND patient checked snoring as a reason for \"why are you here\" in the questionnaire),\nOR (age ≥ 30 AND has hypertension), OR (BMI ≥ 25) then\npredict the patient has sleep apnea, else\npredict the patient does not have sleep apnea. end if\nThe model lists four patterns to characterize patients that has sleep apnea. It is a sparse model with only a few attributes and a simple structure, and can potentially be used\nby people without a machine learning background."
    }, {
      "heading" : "5.2 Performance on UCI Datasets",
      "text" : "We applied OOA and OOAx to several UCI datasets and compared with 5 previously mentioned interpretable models and 2 black box models, random forest and SVM. In the experimental set up, we set a time limit for the MIP in the OOA framework to ensure that it returns a solution in a reasonable amount of time. Table 2 displays the mean and standard deviation of out-of-sample accuracy across 5 folds.\nWe observed that even with the severe restrictions, OA classifiers achieve very competitive performance. For the four categorical datasets in Table 2, OA classifiers always do better than other models. Especially for tic-tac-toe and monks, where there are correct models that correctly classify all examples, OA models are able to discover the correct patterns and achieve 100% accuracy. For numerical and mixed datasets, OA models’ performance levels are on par with those of other methods, sometimes slightly dominated by uninterpretable machine learning models.\nWe show an example of an OA classifier learned from dataset “votes” using OOA framework. This data set includes votes for each of the U.S. House of Representatives Congressmen on 16 key votes on water project cost sharing, duty free exports, immigration, education spending, anti-satellite test ban and etc. The objective is to predict if the voter is democratic or republican.\nif a voter (votes for eduction spending AND for physician fee freeze AND against water project cost sharing),\nOR (votes for export administration act of South Africa AND for physician fee freeze AND agains synfuels corporation cutback),\nOR (votes against aid to Nicaraguan Contrast AND against adoption of the budget resolution AND against handicapped infants and toddlers act AND against superfund right to sue),\nOR (votes for adoption of the budget resolution AND\nfor physician fee freeze AND agains synfuels corporation cutback),\nOR (votes against adoption of the budget resolution AND for El Salvador aid AND for physician fee freeze),\nOR (votes for aid to Nicaraguan Contras AND against adoption of the budget resolution AND against duty free exports AND against synfuels corporation cutback), then\npredict the voter is republican, else\npredict the voter is democratic. end if"
    }, {
      "heading" : "6 Conclusion",
      "text" : "OA models have a long history. They are particularly useful as either (i) interpretable screening mechanisms, where they reduce much of the data from consideration from a further round of modeling, and (ii) consideration sets from marketing, which are rules that humans create to reduce cognitive load in order to make a decision.\nWe presented two optimization-based frameworks for learning Or’s of And’s. The first framework, OOA, uses a MIP to directly form patterns from data. It can deal with both categorical and numerical data without preprocessing. The second framework OOAx reduces computation through pre-mining patterns. We provided bounds on the support of patterns that guarantee that the pattern space can be safely reduced. Both methods can produce high quality OA classifiers, as demonstrated through experiments. They achieve competitive performance compared to other classifiers, with a substantial gain in sparsity and interpretability.\nOne of the main benefits not discussed extensively earlier is the benefit of customizability. Because we use MIP/ILP technology, constraints of almost any kind are very easy to include, and we do not need to derive a new algorithm; this benefit does not come with any other technology that we know of. Customizability is an important component of interpretability."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Or’s of And’s (OA) models are comprised of a small number of disjunctions of conjunctions, also called disjunctive normal form. An example of an OA model is as follows: If (x1 = ‘blue’ AND x2 = ‘middle’) OR (x1 = ‘yellow’), then predict Y = 1, else predict Y = 0. Or’s of And’s models have the advantage of being interpretable to human experts, since they are a set of conditions that concisely capture the characteristics of a specific subset of data. We present two optimization-based machine learning frameworks for constructing OA models, Optimized OA (OOA) and its faster version, Optimized OA with Approximations (OOAx). We prove theoretical bounds on the properties of patterns in an OA model. We build OA models as a diagnostic screening tool for obstructive sleep apnea, that achieves high accuracy with a substantial gain in interpretability over other methods.",
    "creator" : "LaTeX with hyperref package"
  }
}