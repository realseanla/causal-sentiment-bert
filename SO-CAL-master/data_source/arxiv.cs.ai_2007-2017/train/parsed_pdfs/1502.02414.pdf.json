{
  "name" : "1502.02414.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tractability and Decompositions of Global Cost Functions",
    "authors" : [ "David Allouche", "Christian Bessiere", "Patrice Boizumault", "Simon de Givry", "Patricia Gutierrez", "Jimmy H.M. Lee", "Kam Lun Leung", "Samir Loudni", "Jean-Philippe Métivier", "Thomas Schiex", "Yi Wu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n02 41\n4v 1\n[ cs\n.A I]\n9 F\nEnforcing local consistencies in cost function networks is performed by applying so-called Equivalent Preserving Transformations (EPTs) to the cost functions. As EPTs transform the cost functions, they may break the property that was making local consistency enforcement tractable on a global cost function. A global cost function is called tractable projection-safe when applying an EPT to it is tractable and does not break the tractability property. In this paper1, we prove that depending on the size r of the smallest scopes used for performing EPTs, the tractability of global cost functions can be preserved (r = 0) or destroyed (r > 1). When r = 1, the answer is indefinite. We show that on a large family of cost functions, EPTs can be computed via dynamic programming-based algorithms, leading to tractable projection-safety. We also show that when a global cost function can be decomposed into a Berge acyclic network of bounded arity cost functions, soft local consistencies such as soft Directed or Virtual Arc Consistency can directly emulate dynamic programming. These different approaches to decomposable cost functions are then embedded in a solver for extensive experiments that confirm the feasibility and efficiency of our proposal."
    }, {
      "heading" : "1. Introduction",
      "text" : "Cost Function Networks (CFNs) offer a simple and general framework for modeling and solving over-constrained and optimization problems. Besides being equipped with an efficient branch and bound procedure augmented with powerful local consistency techniques, a practical CFN solver should have a good library of global cost functions to model the often complex scenarios in real-life applications.\nEnforcing local consistencies on a global cost function requires to apply Equivalence Preserving Transformations (EPTs) such as cost projection and extension [15]. It is\n1This paper is an extended version of [44] and [1]. ∗Corresponding author\nPreprint submitted to Artificial Intelligence February 10, 2015\nnecessary to compute minima of the cost function to extend and/or project costs among functions and create pruning opportunities. Global cost functions have unbounded arity, but may have a specific semantics that enables dedicated polynomial time algorithms for minimization. However, when local consistencies apply EPTs, they modify a cost function and may break the properties that makes it polynomial time minimizable. The notion of tractable projection-safety captures precisely those functions that remain tractable even after EPTs.\nIn this paper, we prove that any tractable global cost function remains tractable after EPTs to/from the zero-arity cost function (W∅), and cannot remain tractable if arbitrary EPTs to/from r-ary cost functions for r ≥ 2 are allowed. When r = 1, we show that the answer is indefinite. We describe a simple tractable global cost function and show how it becomes intractable after projections/extensions to/from unary cost functions. Simultaneously, it is known that flow-based projection-safe cost functions [40] are positive examples of tractable projection-safe cost functions.\nFor r = 1, we introduce polynomially DAG-decomposable global cost functions, which can be decomposed into a Directed Acyclic Graph with a polynomial number of simpler cost functions for (minimum) cost calculation. Computing minima of such cost functions, which can be based on a polynomial time dynamic programming algorithm, is tractable and remains tractable after projections/extensions. Thus, polynomially DAGdecomposable cost functions are tractable projection-safe. Adding to the existing repertoire of global cost functions, cost function variants of existing global constraints such as Among, Regular, Grammar, and Max/Min, are proved to be polynomially DAGdecomposable.\nTo avoid the need to implement dedicated dynamic programming algorithms, we also consider the possibility of directly using decompositions of global cost functions into polynomial size networks of cost functions with bounded arities, usually ternary cost functions. We show how such network-decompositions can be derived from known global constraint decompositions and how Berge-acyclicity allows soft local consistencies to emulate dynamic programming in this case. We prove that Berge-acyclic networkdecompositions can also be used to directly build polynomial DAG-decompositions.\nTo demonstrate the feasibility of these approaches, we implement and embed various global cost functions in toulbar2 using DAG-decompositions and network-decompositions. We conduct experiments using different benchmarks to evaluate and to compare the performance of the DAG-based and network-based decomposition approaches."
    }, {
      "heading" : "2. Background",
      "text" : "We give preliminaries on cost function networks and global cost functions.\n2.1. Cost Function Networks\nA cost function network (CFN) is a special case of valued constraint satisfaction problem [57] with a specific cost structure ([0, . . . ,⊤],⊕,≤). We give the formal definitions of the cost structure and CFN as follows.\nDefinition 1. [57] The cost structure ([0, . . . ,⊤],⊕,≤) is a tuple defined as:\n• [0, . . . ,⊤] is the interval of integers from 0 to ⊤ ordered by the standard ordering ≤, where ⊤ is either a positive integer or +∞.\n• ⊕ is the addition operation defined as a ⊕ b = min(⊤, a+ b). We also define the subtraction ⊖ operator for any a and b, where a ≥ b, as:\na⊖ b =\n{\na− b, if a 6= ⊤; ⊤, otherwise\nNote that more general additive cost structures have also been used. Specifically, VAC and OSAC [14] local consistencies are defined using a structure using non negative rational instead of non negative integer numbers.\nDefinition 2. [56] A Cost Function Network (CFN) is a tuple (X ,W ,⊤), where:\n• X is a an ordered set of discrete domains variables {x1, x2, . . . , xn}, the domain of xi ∈ X being denoted as D(xi);\n• W is a set of cost functions WS each with a scope S = {xs1 , . . . , xsk} ⊆ X that maps a tuple ℓ ∈ LS, where LS = D(xs1)× · · · ×D(xsk), to [0, . . . ,⊤].\nThe hypergraph (X ,S) of a CFN has one vertex per variable xi ∈ X and one hyperedge for every scope S such that ∃WS ∈ W . The incidence graph of an hypergraph is a bipartite graph (X ∪ S, E) where an edge exists between a variable xi ∈ X and a scope S of a cost function cS ∈ W if and only if xi ∈ S. The incidence graph is also called the factor graph of the CFN. We consider only problems with connected hypergraphs.\nWhen the context is clear, we abuse notations by denoting an assignment of a set of variables S ⊆ X as a tuple ℓ = (vs1 , . . . , vsk) ∈ L\nS . The notation ℓ[xsi ] denotes the value vsi assigned to xsi ∈ S, and ℓ[S\n′] denotes the tuple formed by projecting ℓ onto S′ ⊆ S. If ∀ℓ ∈ LS , fS(ℓ) ≤ gS(ℓ), we say that the cost function fS is a relaxation of gS, and write fS ≤ gS . Notice that a usual hard constraint is just a cost function using only costs in {0,⊤}.\nWithout loss of generality, we assume W = {W∅} ∪ {Wi | xi ∈ X} ∪ W +. W∅ is a constant zero-arity cost function. Wi is a unary cost function associated with each xi ∈ X . W\n+ is a set of cost functions WS with scope S and |S| ≥ 2. If W∅ and {Wi} are not defined, we assume Wi(v) = 0 for all v ∈ D(xi) and W∅ = 0. To simplify notations, we also denote by Ws1,s2,...,sn the cost function on variables {xs1 , xs2 , . . . , xsn} if the context is clear.\nProperty 1. A Constraint Satisfaction problem (CSP) is a CFN where all cost functions WS ∈ W are such that ∀ℓ ∈ L S ,WS(ℓ) ∈ {0,⊤}.\nSuch cost functions define constraints and we preferably denote them using CS instead of WS . Definition 3. Given a CFN (X ,W ,⊤), the cost of a tuple ℓ ∈ LX is defined as cost(ℓ) = ⊕\nWS∈W WS(ℓ[S]). A tuple ℓ ∈ L\nX is feasible if cost(ℓ) < ⊤, and it is an optimal solution\nof the CFN if cost(ℓ) is minimum among all tuples in LX .\nThis optimization problem has an associated NP-complete decision problem and restrictions to Boolean variables and binary constraints are known to be APX-hard [47]. CFNs are typically solved with depth-first branch-and-bound search using W∅ as a lower bound. Search efficiency is enhanced by maintaining local consistencies that increase the lower bound by redistributing cost among WS , pushing costs into W∅ and Wi, and pruning values while preserving the equivalence of the problem (i.e., the cost of each tuple ℓ ∈ LX is unchanged).\n2.2. Soft local consistencies and EPTs\nDifferent consistency notions have been defined. Examples include NC* [35], (G)AC* [56, 35, 15, 40, 43], FD(G)AC* [35, 33, 40, 43], and (weak) ED(G)AC* [25, 42, 43]. Enforcing such local consistencies requires to apply equivalence preserving transformations (EPTs) that shift costs between different scopes. The main EPT [15] is defined below and described as Algorithm 1, a compact definition of the projection and extension from Cooper 2005.\nDefinition 4. [17] Given two cost functions WS1 and WS2 , S2 ⊂ S1, the EPT Project (S1, S2, ℓ, α) shifts an amount of cost α between a tuple ℓ ∈ L\nS2 of WS2 and the cost function WS1 . The direction of the shift is given by the sign of α. The precondition guarantees that costs remain non negative after the EPT as been applied.\nDenoting by r = |S2|, the EPT is called an r-projection when α ≥ 0 and an r-extension when α < 0.\nPrecondition: −WS2(ℓ) ≤ α ≤ minℓ′∈ LS1 ,ℓ′[S2]=ℓ WS1(ℓ\n′);\nProcedure Project(S1, S2, ℓ, α) WS2(ℓ) ← WS2(ℓ)⊕ α; foreach (ℓ′ ∈ LS1 such that ℓ′[S2] = ℓ) do\nWS1(ℓ ′) ← WS1(ℓ ′)⊖ α;\nAlgorithm 1: A cost shifting EPT used to enforce soft local consistencies. The ⊕,⊖ operations are extended here to handle possibly negative costs as follows: for non negative costs α, β, we have α⊖ (−β) = α⊕ β and for β ≤ α, α⊕ (−β) = α⊖ β.\nIt is now possible to introduce local consistency enforcing algorithms.\nDefinition 5. [35] A variable xi is node consistent (NC*) if each value v ∈ D(xi) satisfies Wi(v) ⊕W∅ < ⊤ and there exists a value v ′ ∈ D(xi) such that Wi(v ′) = 0. A CFN is NC* iff all variables are NC*.\nProcedure enforceNC*() in Algorithm 2 enforces NC*, where unaryProject() applies EPTs that move unary costs towards W∅ while keeping the solution unchanged, and pruneVar(xi) removes infeasible values (the returned Boolean value is not used at this stage but will be later).\nDefinition 6. [15, 40, 43] Given a CFN P = (X ,W ,⊤), a cost function WS ∈ W + and a variable xi ∈ S.\n• A tuple ℓ ∈ LS is a simple support for v ∈ D(xi) with respect to WS with xi ∈ S iff ℓ[xi] = v and WS(ℓ) = 0.\n• A variable xi ∈ S is star generalized arc consistent (GAC*) with respect to WS iff\n– xi is NC*; – each value vi ∈ D(xi) has a simple support ℓ with respect to WS .\n• A CFN is GAC* iff all variables are GAC* with respect to all related non-unary cost functions.\nProcedure enforceNC*() foreach xi ∈ X do unaryProject (xi);1 foreach xi ∈ X do pruneVar (xi);2\nProcedure unaryProject(xi) α := min{Wi(v) | v ∈ D(xi)};3 Project({xi},∅, (), α);4\nFunction pruneVar(xi) flag := false;5 foreach v ∈ D(xi) s.t. Wi(v)⊕W∅ = ⊤ do6 D(xi) := D(xi) \\ {v};7 flag := true;8\nreturn flag ;9\nAlgorithm 2: Enforce NC*\nNote that the definition and the GAC* algorithm we use is slightly different from the one given by Cooper and Schiex 2004, which also requires for every tuple ℓ ∈ LS , WS(ℓ) = ⊤ if W∅ ⊕ ⊕\nxi∈S Wi(ℓ[xi])⊕WS(ℓ) = ⊤.\nThe procedure enforceGAC*() in Algorithm 3, is a simplified version that enforces GAC* for a CFN (X ,W ,⊤). The propagation queue Q stores a set of variables xj . If xj ∈ Q, all variables involved in the same cost functions as xj are potentially not GAC*. Initially, all variables are in Q. A variable xj is pushed into Q only after values are removed from D(xj). At each iteration, an arbitrary variable xj is removed from the queue by the function pop() at line 3. For all cost functions involving it, the function findSupport() at line 3 enforces GAC* with respect to WS on all other variables by finding the simple supports. Infeasible values due to increased unary or zero-ary costs are removed by the function pruneVar(). If a value is removed from D(xi), the simple supports of other related variables may be destroyed. Thus, xi is pushed back to Q. If enforceGAC*() terminates, all values in each variable domain must have a simple support. The CFN is now GAC*.\nDirected Arc Consistency. Because GAC* can only use projection to move costs from one non unary cost function to a unary cost function, it is limited in its capacity to increase the lower bound. Directed Arc Consistency (DAC*) has been originally introduced on binary cost functions using the notion of full support [16].\nDefinition 7. [34] Given a CFN P = (X ,W ,⊤), with all cost functions WS ∈ W + satisfying |S| = 2 and a total order ≺ over variables:\n• For a cost function WS ∈ W +, a tuple ℓ ∈ LS is a full support for a value a ∈ D(xi)\nof xi ∈ S iff WS(ℓ) ⊕\nxj∈S,j 6=i Wj(ℓ[xj ]) = 0.\n• A variable xi ∈ S is star directed arc consistent (DAC*) with respect to WS iff\n– xi is NC*;\nProcedure enforceGAC*() Q := X ;1 while Q 6= ∅ do2 xj := pop (Q);3 foreach WS ∈ W\n+ s.t. xj ∈ S do4 foreach xi ∈ S \\ {xj} do findSupport (WS , xi);5\nforeach xi ∈ X s.t. pruneVar (xi) do Q := Q ∪ {xi};\nProcedure findSupport(WS, xi) foreach v ∈ D(xi) do6 α := min{WS(ℓ) | ℓ ∈ L S ∧ ℓ[xi] = v};7 Project(S, {xi}, (v), α)8\nunaryProject (xi);9\nAlgorithm 3: Enforcing GAC* for a CFN\n– each value vi ∈ D(xi) has a full support ℓ for WS .\n• A CFN is DAC* w.r.t. the order ≺ iff for all cost functions WS ∈ W +, the\nmaximum variable in S according to ≺ has a full support on WS.\nDAC has been extended to non-binary cost functions in [54] and [41] with different definitions that coincide on binary cost functions. In this paper, we are specifically interested in the simplest extension called T-DAC (for terminal DAC) which just relaxes the requirement of having a binary scope in the definition above.\nDefinition 8. Given a total order ≺ on variables, a CFN is said to be Terminal Directional Arc Consistent (T-DAC) w.r.t. ≺ iff for any cost function WS ∈ W\n+, any value (xi, vi) of the maximum variable xi ∈ S according to ≺ has a full support on WS.\nTo enforce T-DAC on a cost function WS , it suffices to first shift the cost of every unary cost function Wi, i ∈ S inside WS by applying Project(S, {xi}, (a),−Wi(a)) for every value a ∈ Di. Let xj be the maximum variable in S according to ≺, one can then apply Project(S, {xj}, (b), α) for every value b ∈ D(xj) with α = minℓ∈ LS ,ℓ[xj]=b WS(ℓ). Let ℓ be a tuple where this minimum is reached. The tuple ℓ is then a full support for b ∈ D(xj): WS(ℓ) ⊕\nxi∈S,i6=j Wi(ℓ[xi]) = 0. This support can only be broken if for some\nunary cost functions Wi, i ∈ S, i 6= j, Wi(a) increases for some value a ∈ D(xi). To enforce T-DAC on a complete CFN (X ,W ,⊤), one can simply sort W according to ≺ and apply the previous process on each cost function, successively. When a cost function WS is processed, all the cost functions whose maximum variable appears before the maximum variable of S have already been processed which guarantees that none of the established full supports will be broken in the future. Enforcing T-DAC is therefore in O(edr) in time, where e = |W| and r = maxWS∈W |S|.\nBased on NC*, (G)AC* and DAC, stronger FD(G)AC* and (weak) ED(G)AC* have been developed for binary [33, 25], ternary [55], and n-ary [40, 42, 43] cost functions. It is important to note that soft arc consistencies are equivalent to generalized arc consistency\nwhen applied on a CFN which is also a CSP (with the exception of DAC* which is equivalent to Directional Arc Consistency [23]).\nLocal consistency enforcement involves two types of operations: (1) finding the minimum cost returned by the cost functions among all (or part of) tuples; (2) applying EPTs that shift costs to and from smaller-arity cost functions.\nMinimum cost computation corresponds to line 2 in Algorithm 2, and line 3 in Algorithm 3. For simplicity, we write min{WS(ℓ) | ℓ ∈ L\nS} as min{WS}. In practice, projections and extensions can be performed in constant time using the ∆ data-structure introduced in Cooper and Schiex 2004. For example, when we perform 1-projections or 1-extensions, instead of modifying the costs of all tuples, we store the projected and extended costs in ∆−xi,v and ∆ + xi,v\nrespectively. Whenever we compute the value of the cost function WS for a tuple ℓ with ℓ[xi] = v, we returnWS(ℓ)⊖∆ − xi,v\n⊕∆+xi,v. The time complexity of enforcing one of the previous consistencies is thus entirely defined by the time complexity of computing the minimum of a cost function during the enforcing. For example, we can state the time complexity of findSupport() by simply abstracting the worst case time complexity of computing minimum as fmin in the following lemma.\nProposition 1. The procedure findSupport() in Algorithm 3 requires O(d·fmin), where d is the maximum domain size.\nProof. We analyze the time complexity of lines 3 to 3 in the procedure findSupport(). Line 3 requires fmin. We can replace the domain of xi by {v}, and run the minimum computation to get the minimum cost. Projection at line 3 can be performed in constant time. Thus, each iteration requires O(fmin). Since the procedure iterates d times, and the procedure unaryProject requires O(d), the overall complexity is O(d · fmin + d) = O(d · fmin).\nIn the general case, fmin is inO(d r) where r is the size of the scope and d the maximum domain size. However, a global cost function may have specialized algorithms which make the operation of finding minimum, and thus consistency enforcement, tractable.\nVirtual Arc Consistency (VAC) is a more recent local consistency property that establishes a link between CFN and classical CSP.\nDefinition 9. Let P = (X,D,W,⊤) be a CFN. We define the constraint network Bool(P ) as the CSP with the same set X,D of domain variables and which contains, for each cost function WS ∈ W, |S| > 0, a constraint cS with the same scope which exactly forbids all tuples ℓ ∈ LS such that WS(ℓ) 6= 0. A CFN P is said to be Virtual Arc Consistent (VAC) iff the arc consistent closure of the constraint network Bool(P ) is non empty [14].\nThe most efficient algorithms for enforcing VAC [14] enforces an approximation of VAC called VACε with a time complexity in O( ekdr\nε ) and a space complexity in\nO(edr). Alternatively, optimal soft arc consistency [18] can be used to enforce VAC in O(e6.5d(3r+3.5) logM) time (where M is the maximum finite cost in the network).\n2.3. Global Constraints, Soft Global Constraints and Global Cost Functions\nDefinition 10. A global constraint [5, 53], denoted by GC(S,A1, . . . , Ak), is a family of hard constraints parameterized by a scope S, and extra parameters A1, . . . , Ak.\nNote that A1, . . . , Ak may be empty. Examples of global constraints are AllDifferent [36], GCC [52], Same [8], Among [6], Regular [49], Grammar [29], and Maximum/Minimum constraints [3]. Because of their unbounded scope, global constraints cannot be efficiently propagated by generic local consistency algorithms, which are exponential in the arity of the constraint. Specific propagation algorithms are designed to achieve polynomial time complexity in the size of the input, i.e., the scope, the domains and extra parameters.\nTo capture the idea of costs assigned to constraint violations, the notion of soft global constraint has been introduced [51]. This is a traditional global constraint with one extra variable representing the cost (or violation measure) of the assignment w.r.t. to an existing global constraint.\nDefinition 11. A soft global constraint [51], denoted by Soft GCµ(S∪{z}, A1, . . . , Ak), is defined by a global constraint GC(S,A1, . . . , Ak) and a function µ : L\nS → N that defines the violation measure of tuples in LS w.r.t. GCµ(S ∪ {z}, A1, . . . , Ak) . The violation measure µ returns 0 if the tuple satisfies GC. The variable z is the cost variable that stores the degree of violation. A tuple ℓ satisfies Soft GCµ iff µ(ℓ[S]) = ℓ[z].\nFrom this definition, one can observe that one global constraint can give rise to different soft global constraints using different violation measures. For instance, the AllDifferent [36] global constraint has two usual soft variants: Soft AllDifferentvar defines the violation as the minimum number of variable assignments that need to be changed so that the tuple contains only distinct values [51, 27]; Soft AllDifferentdec defines the violations as the number of pairs of variables having the same assigned value [51, 27]. Other examples include Soft Regularvar/Soft Regularedit [4, 27], Soft GCCvar/Soft GCCval [27], Soft Amongvar [58], and Soft Grammarvar [30].\nThe notion of soft global constraint allows to remain in the CSP framework and is therefore compatible with a variety of existing constraint programming solvers. It however requires the introduction of extra cost variables and does not allow to benefit from the stronger propagation offered by soft local consistencies. One can instead directly use global cost functions.\nDefinition 12. A global cost function [60, 43], denoted as W GCF(S,A1, . . . , Ak), is a family of cost functions parameterized by a scope S and extra parameters A1, . . . , Ak.\nFor example, if S is a set of variables with non negative integer domains, it is easy to define the Global Cost Function W Sum(S) ≡ ⊕\nxi∈S min(⊤, xi).\nIt is possible to derive a global cost function from an existing soft global constraint Soft GCµ(S ∪{z}, A1, . . . , Ak) when µ takes integer values. In this case, we denote the corresponding global cost function as W GCFµ. Its value for a tuple ℓ ∈ LS is equal to min(⊤, µ(ℓ)).\nFor example, global cost functions W AllDifferentvar/W AllDifferentdec [40, 43] can be derived from two different violation measures of AllDifferent, namely variable-based and decomposition-based [51, 27], respectively. Other examples include W GCCvar and W GCCval [40, 43], W Samevar [40, 43], W SlidingSumvar [45], W Regularvar and W Regularedit [1, 40, 43], W EGCCvar [45], W Disjunctiveval and W Cumulativeval [45, 39]."
    }, {
      "heading" : "3. Tractable Projection-Safety",
      "text" : "All soft local consistencies are based on the use of EPTs, shifting costs between two scopes. The size of the smaller scope used in EPTs is directly related to the level of local consistency enforced: node consistency uses EPTs onto the empty scope, arc consistencies use unary scopes while higher-order consistencies [17] use larger scopes. In this section, we show that the order of the local consistency directly impacts the tractability of global cost function minimization.\nDefinition 13. A global cost function W GCF(S,A1, . . . , Ak) is said to be tractable iff the problem instance IsBetterThan(W GCF(S,A1, . . . , Ak),m) is in P:\nIsBetterThan(W GCF(S,A1, . . . , Ak),m)\nInstance. A global cost function W GCF, a scope S with domains for the variables in S, values for the parameters A1, . . . , Ak and a fixed integer m. Question. Does there exist a tuple ℓ ∈ LS such that W GCF(S,A1, . . . , Ak)(ℓ) ≤ m?\nNote that the problem instance IsBetterThan() is the decision problem of the optimization problem min{W GCF (S,A1, . . . , Ak)}. For a tractable global cost function WS =W GCF(S,A1, . . . , Ak), the time complexity of computing min{WS} is bounded by a polynomial function in the size of the input, including the scope, the corresponding domains, the other parameters of the global cost function and log(m).\nIn the following, we introduce tractable r-projection-safety global cost functions, which remain tractable after applying r-EPTs.\nDefinition 14. We say that a global cost function W GCF(S,A1, . . . , Ak) is tractable r-projection-safe iff:\n• it is tractable and;\n• any global cost functions that can be derived from W GCF(S,A1, . . . , Ak) by a series of r-EPTs is also tractable.\nThe tractability after r-EPTs depends on r. We divide the discussion of tractable r-projection-safety into three cases: r = 0, r ≥ 2 and r = 1.\n3.1. Tractability and Order 0 EPTs\nWhen r = 0, EPTs are performed to/from W∅. This kind of EPTs is used when enforcing Node Consistency (NC*) [35] but also in ∅-inverse consistency [60], and strong ∅-inverse consistency [40, 43].\nWe show that if a global cost function is tractable, it remains tractable after applying such EPTs.\nTheorem 1. Every tractable global cost function is tractable 0-projection-safe.\nProof. Given a tractable global cost function WS = W GCF (S,A1, . . . , Ak), we denote by ∇0(WS) the result of the application of an arbitrary finite sequence of 0-EPTs on it.\nClearly, WS and ∇0(WS) only differ by a constant, i.e. there exists α − and α+, where α−, α+ ∈ {0, . . . ,⊤}, such that:\n∇0(WS)(ℓ) = WS(ℓ)⊕ α + ⊖ α− ,for every ℓ ∈ LS\nIf WS(ℓ) = min{WS} for some ℓ ∈ L S , ∇0(WS)(ℓ) = min{∇0(WS)}. If WS is tractable, so is ∇0(WS).\n3.2. Tractability and Higher Order EPTs\nWhen r ≥ 2, EPTs are performed to/from r-arity cost functions. This is required for enforcing higher order consistencies and is used in practice in ternary cost functions processing [55] and complete k-consistency [17].\nIf arbitrary sequences of r-EPTs are allowed, we show that tractable global cost function with finite costs always become intractable after some sequence of r-EPT applications, where r ≥ 2.\nTheorem 2. Every tractable global cost functions W GCF(S,A1, . . . , Ak) : L S 7→ [0,∞) returning finite costs is not tractable r-projection-safe for r ≥ 2.\nProof. For r ≥ 2, we show by contradiction that we can solve any instance of the NPcomplete fixed arity Constraint Satisfaction Problem r-FixArityCSP in polynomial time if there exists a r-projection-safe global cost function W GC.\nr-FixArityCSP(X ,Wh)\nInstance. A CSP instance (X ,Wh), where each constraint ChS ∈ W h always restricts r variables, i.e. |S| = r, and r ≥ 2\nQuestion. Is the CSP (X ,Wh) satisfiable?\nIt can be easily shown that FixArityCSP is NP-hard for r ≥ 2, as graph coloring and r-SAT can be solved through modeling into FixArityCSP.\nWe reduce the problem r-FixArityCSP(X ,Wh) to IsBetterThan(∇r(WX ),⊤), where WX = W GC(X , A1, . . . , Ak) is an arbitrary global cost function using only finite costs. We first construct a WCSP (X ,W ∪ {WX },⊤) from the original CSP (X ,W\nh). The upper bound ⊤ is a sufficiently large integer such that ⊤ > WX (ℓ) for every ℓ ∈ L S which is always possible given that WX remains finite. The cost functions WS ∈ W \\ {WX } are defined as follows:\nWS(ℓ) =\n{\n0, if ℓ is accepted by ChS ; ⊤, otherwise\nFrom the WCSP, ∇r can be defined as follows: for each forbidden tuple ℓ[S] in each ChS ∈ W\nh, we add an extension of ⊤ from WS to WX with respect to ℓ[S] into ∇r. Under this construction, ∇r(WX )(ℓ) can be represented as:\n∇r(WX )(ℓ) = WX (ℓ)⊕ ⊕\nWS∈W\nWS(ℓ[S])\nA tuple ℓ ∈ LX satisfies ∇r(WX )(ℓ) = ⊤ iff it is forbidden by C h S ∈ W h. Therefore solving IsBetterThan(∇r(WX ),⊤) solves r-FixArityCSP(X ,W\nh). Since rFixArityCSP(X ,Wh) is NP-hard, ∇r(WX ) is not tractable, unless P = NP .\n3.3. Tractability and unary EPTs\nWhen r = 1, 1-EPTs cover 1-projections and 1-extensions, which are the backbone of the consistency algorithms of (G)AC* [35, 40, 43], FD(G)AC* [33, 40, 43], (weak) ED(G)AC* [25, 42, 43], VAC, and OSAC [14]. In these cases, tractable cost functions are tractable 1-projection-safe only under special conditions. For example, Lee and Leung, Lee and Leung 2009a, 2012 define flow-based projection-safety based on a flow-based global cost function.\nDefinition 15. [40, 43] A global cost function W GCF (S,A1, . . . , Ak) is flow-based iff it can be represented by a flow network G(S,A1, . . . , Ak) such that the minimum cost flow on G(S,A1, . . . , Ak) corresponds to min{W GCF (S,A1, . . . , Ak)}.\nDefinition 16. [40, 43] A global cost function W GCF (S,A1, . . . , Ak) is flow-based projection-safe iff it is is flow-based, and is still flow-based following any sequence of 1-projections and 1-extensions.\nLee and Leung, Lee and Leung 2009a, 2012 further propose sufficient conditions for tractable cost functions to be flow-based projection-safe. Flow-based projection-safety implies tractable 1-projection-safety. We state the result in the following theorem.\nTheorem 3. Any flow-based projection-safe global cost function is tractable 1-projectionsafe.\nProof. Follow directly from the tractability of the minimum cost flow algorithm.\nHowever, tractable cost functions are not necessarily tractable 1-projection-safe. One example is W 2SAT, which is a global cost function derived from an instance of the polynomial 2SAT problem.\nDefinition 17. Given a set of Boolean variables S, a set of binary clauses F , and a positive integer c. The global cost function W 2SAT(S, F, c) is defined as:\nW 2SAT (S, F, c)(ℓ) =\n{\n0, if ℓ satisfies F c, otherwise\nW 2SAT is tractable, because the 2SAT problem is tractable [31]. However, it is not tractable 1-projection-safe.\nTheorem 4. W 2SAT is not tractable 1-projection-safe.\nProof. In the following, we show that if W 2SAT(X , F, k) is tractable 1-projection-safe, the NP-Hard problem WSAT(2-CNF) is tractable [24, page 69].\nWSAT(2-CNF)\nInstance. A 2-CNF formula F (a set of binary clauses) and a fixed integer k.\nQuestion. Is there an assignment that satisfies all clauses in F with at most k variables set to true ?\nWe construct a particular sequence of 1-projections and/or 1-extensions ∇1 such that WSAT(2-CNF) can be solved using WX = W 2SAT (X , F, k+1) from the Boolean CFN P = (X ,W ∪{WX }, k+1). W only contains unary cost functions Wi, which are defined as follows:\nWi(v) =\n{\n1, if v = true; 0, otherwise\nBased on P , we construct ∇1 as follows: for each variable xi ∈ X , we add an extension of 1 from Wi to WX with respect to the value true into ∇1. As a result, a tuple ℓ with ∇1(WX )(ℓ) = k\n′ ≤ k contains exactly k′ variables set to true (since every xi = true will incur a cost of 1) and also satisfies F (or it would have cost k + 1 = ⊤). Thus, we reduce WSAT(2-CNF) with threshold k to IsBetterThan(∇1(WX ), k). However, WSAT(2-CNF) is NP-hard [24], so ∇1(WX ) is not tractable, unless P = NP .\nWhen the context is clear, we use tractable projection-safety, projection and extension to refer to tractable 1-projection-safety, 1-projection and 1-extension respectively hereafter."
    }, {
      "heading" : "4. Polynomial DAG-Decomposability",
      "text" : "Beyond flow-based global cost functions [40, 43], we introduce now an additional class of tractable projection-safe cost functions based on dynamic programming algorithms. As mentioned by Dasgupta et al. 2007, every dynamic programming has an underlying DAG structure.\nDefinition 18. A directed acylic graph (DAG) T = (V,E), where V is a set of vertices and E ⊆ V ×V is a set of directed edges, is a directed graph with no directed cycles, and:\n• An edge (u, v) ∈ E points from u to v, where u is the parent of v, and v is the child of u;\n• A root of a DAG is a vertex with zero in-degree;\n• A leaf of a DAG is a vertex with zero out-degree;\n• An internal vertex of a DAG is neither a root nor a leaf;\nWe now introduce the DAG-decomposition of a cost function based on a DAG.\nDefinition 19. A global cost function W GCF (S,A1, . . . , Ak) is DAG-decomposable iff any instance WS of it is DAG-decomposable. A cost function WS is DAG-decomposable into a connected DAG T = (V,E) iff:\n• V = {ωSi} ∪ {WS} is a set of vertices, each of which represents a global cost function, and WS is the only root of T ;\n• Each internal vertex ωSi ∈ V is associated with an aggregation function fi, which is based on an associative and commutative binary operator;\n• For every ωSi ∈ V , we have that:\n– the scope of ωSi is composed from its children’s scopes, i.e.:\nSi = ⋃\n(ωSi ,ωSj )∈E\nSj\n– ωSi is the aggregation of its children, i.e.:\nωSi(ℓ) = fi({ωSj(ℓ[Sj ]) | (ωSi , ωSj) ∈ E});\n– min is distributive over fi, i.e.:\nmin{ωSi} = fi({min{ωSj} | (ωSi , ωSj ) ∈ E}).\nIn other words, a DAG-decomposable function can be (recursively) divided into global cost functions with smaller scopes until it reaches the ones at the leaves of a DAG, which are usually trivial to solve. The (minimum) costs can then be aggregated using the fi functions at each internal vertex to get the resultant (minimum) cost. This facilitates the use of dynamic programming in minimum cost computations, relying on the existence of optimal sub-structures. However, further properties on DAG-decompositions are required to allow for projections and extensions to operate inside the DAG structure.\nDefinition 20. A global cost function W GCF (S,A1, . . . , Ak) is safely DAG-decomposable into a connected DAG T = (V,E) iff:\n• it is DAG-decomposable into T , with aggregation functions fi associated with each internal vertex ωSi ∈ V ; • projection and extension are distributive over fi, i.e. for a variable x ∈ S, a cost α and a tuple ℓ ∈ LS,\n– ωSi(ℓ)⊕ νx,Si(α) = fi({ωSk(ℓ[Sk])⊕ νx,Sk(α) | (ωSi , ωSk) ∈ E}), and; – ωSi(ℓ)⊖ νx,Si(α) = fi({ωSk(ℓ[Sk])⊖ νx,Sk(α) | (ωSi , ωSk) ∈ E}),\nwhere the function ν is defined as:\nνx,Sj (α) =\n{\nα, if x ∈ Sj, 0, otherwise.\nThe requirement of a distributive fi with respect to projection and extension at each vertex in T implies that the structure of the DAG is unchanged after projections and extensions. Both operations can be distributed down to the leaves. We formally state this as the following theorem. For a given variable x, with a value a ∈ D(x), we denote as δx,v,α(WS) the cost function obtained by the application of Project(S, {x}, (v), α) on WS if x ∈ S or WS otherwise.\nTheorem 5. If a global cost function = W GCF (S,A1, . . . , Ak) can be safely DAGdecomposed with the corresponding DAG T = (V,E), δx,v,α(W GCF (S,A1, . . . , Ak)) can also be safely DAG-decomposed into T ′ = (V ′, E′), where\n• T is isomorphic to T ′ and obtained by the vertex-wise application of δx,v,α, i.e.:\n– if ωSi ∈ V , δx,v,α(ωSi) ∈ V ′; – if (ωSi , ωSk) ∈ E, (δx,v,α(ωSi), δx,v,α(ωSk)) ∈ E ′;\n• if ωSi ∈ V is associated with the aggregation function fi, so is δx,v,α(ωSi) ∈ V ′.\nProof. Follows directly from Definition 20.\nTwo common choices for fi are ⊕ and min, with which distributivity depends on how the scopes intersect. In the following, we show that the global cost function is safely DAG-decomposable if the internal vertices that are associated with ⊕ have children with non-overlapping scopes, and those associated with min have children which identical scopes.\nTheorem 6. Given a DAG-decomposable global cost function W GCF (S,A1, . . . , Ak) with the corresponding DAG T = (V,E) such that\n• each ωSi ∈ V is associated with the aggregation function fi = ⊕ ; • for any distinct ωSj , ωSk ∈ V , children of ωSi , Sj ∩ Sk = ∅.\nThen W GCF (S,A1, . . . , Ak) is safely DAG-decomposable.\nProof. We need to show that min, projection and extension are distributive over ⊕. Since the scopes of the cost functions do not overlap, min is distributive over ⊕. We further show the distributivity with respect to projection (⊖), while extension (⊕) is similar. We consider an internal node ωSi ∈ V . Given a variable x ∈ Si, a cost α, and a tuple ℓ ∈ LS , since the scopes of the cost functions {ωSk | (ωSi , ωSk) ∈ E} are disjoint, there must exist exactly one cost function ωSj such that x ∈ Sj , i.e.:\nωSi(ℓ)⊖ α = (ωSj (ℓ[Sj ])⊖ α)⊕ ⊕\nk 6=j∧(ωSi ,ωSk )∈E\nωSk(ℓ[Sk])\n= ⊕\n(ωSi ,ωSk)∈E\n(ωSk(ℓ[Sk])⊖ νx,Sk(α))\nThe result follows.\nTheorem 7. Given a DAG-decomposable global cost function W GC(S,A1, . . . , Ak) with the corresponding DAG T = (V,E) such that:\n• each ωSi ∈ V is associated with the aggregation function fi = min; • ∀ωSj , ωSk ∈ V , children of ωSi, Sj = Sk = Si.\nThen W GCF (S,A1, . . . , Ak) is safely DAG-decomposable.\nProof. Since the scopes are completely overlapping,\nmin{ωSi} = min ℓ∈ LSi { min (ωSi ,ωSk )∈E {ωSk(ℓ)}}\n= min (ωSi ,ωSk )∈E { min ℓ∈ LSk {ωSk(ℓ)}}\n= fi({min{ωSk} | (ωSi , ωSk) ∈ E})\nAnd it is trivial to see that projection and extension are distributive over fi. The result follows.\nWe further refine safe DAG-decomposability to form polynomial DAG-decomposability.\nDefinition 21. A global cost function W GCF (S,A1, . . . , Ak) is polynomially DAGdecomposable into a connected DAG T = (V,E) iff:\n1. W GCF (S,A1, . . . , Ak) is safely DAG-decomposable into T ;\n2. |V | is polynomial in the size of the input parameters of W GCF (S,A1, . . . , Ak);\n3. each leaf in V is a unary cost function, and\n4. each aggregation function fi associated with each internal node is tractable.\nIn other words, a polynomially DAG-decomposable cost function can make use of dynamic programming to compute the minimum in a tractable fashion. Projections and extensions to/from such cost functions can also be distributed to the leaves in T . Polynomially DAG-decomposable cost functions are tractable and also tractable projection-safe as stated below.\nTheorem 8. Every global cost function WS = W GCF (S,A1, . . . , Ak) which is polynomially DAG-decomposable is tractable.\nProof. Suppose WS is polynomially DAG-decomposable into the corresponding DAG T = (V,E). Algorithm 4 can be applied to compute min{WS}. The algorithm uses a bottom-up memoization approach. Algorithm 4 first topologically sort V at line 4. After sorting, all the leaves will be grouped at the end of the sorted sequence, which is then processed in the reversed order at line 4. If the corresponding cost function of the processing node is a leaf, the minimum is computed and stored in the table Min at line 4. Otherwise, its minimum is computed by aggregating {Min[ωSk ] | (ωSi , ωSk) ∈ E}, which have been computed already, by the function fi at line 4. Line 4 returns the minimum of the root.\nThe computation is tractable. Since the leaves are unary cost functions, line 4 computes in O(d), where d is the maximum domain size. For the non-leaf node, the complexity depends on the complexity of fi, which is tractable by definition. Result follows.\nFunction Minimum (WS) Form the corresponding DAG T = (V,E);1 Topologically sort V ;2 foreach ωSi ∈ V in reverse topological order do3 if ωSi is the leaf of T then4 Min[ωSi ] := min{ωSi} ;5\nelse6 Min[ωSi ] := fi({Min[ωSk ] | (ωSi , ωSk) ∈ E}) ;7\nreturn Min[WS ];8\nAlgorithm 4: Computing min{WS}\nNote that Algorithm 4 always starts from scratch to compute the minimum. In practice, querying the minimum of the cost function WS when xi is assigned to v can be done in a lower time complexity, provided that pre-processing is performed beforehand: we compute the table Min+, in which Min+[ωSj , xi, v] stores min{ωSj(ℓ) | xi ∈ Sj ∧ ℓ[xi] = v}. Whenever we have to compute the minimum when xi is assigned to v, we return Min+[WS , xi, v]. The structure of the table Min\n+ varies from one cost function to another.\nIncrementality can also be maintained. If the domain of the variable xi ∈ S is changed, or the unary cost function Wi is modified, we only update the related entries Min[ωSh], or Min\n+[ωSh , x, v], where Sh ⊆ S and x ∈ Sh, without re-computing the whole table.\nWe further show that a polynomially DAG-decomposable cost function is tractable projection-safe. The following lemma is useful in proving our final result.\nLemma 1. If a global cost function W GCF (S,A1, . . . , Ak) is polynomially DAG-decomposable, then δx,v,α(W GCF (S,A1, . . . , Ak)) is polynomially DAG-decomposable.\nProof. We only prove the part on projection, while the proof on extension is similar. Suppose W GCF (S,A1, . . . , Ak) is polynomially DAG-decomposable into the corresponding DAG T = (V,E). We consider two cases when performing projection on a cost function ωSi ∈ V .\nCase 1: ωSi is a leaf of T . The sub-cost function ωSi is a tractable unary cost function. After applying projection on value v of variable x on ωSi , the value of the resultant cost function is either unchanged (if ℓ[x] 6= v) or otherwise δx,v,α(ωSi)(ℓ) = ωSi(ℓ)⊖νx,Si(α), which remains a unary and therefore tractable cost function;\nCase 2: ωSi is not a leaf of T . The sub-cost function ωSi is safely DAG-decomposed into the sub-DAG rooted in ωSi with aggregation function fi. By Theorem 5, the resultant function δxi,v,α(ωS) after projection can be safely DAG-decomposed into T ′. Since is isomorphic to T , it has polynomial size, it also uses tractable fi and has unary cost functions as leaves so δxi,v,α(ωS) can be polynomially decomposed into T ′.\nThe result follows.\nTheorem 9. A polynomially DAG-decomposable global cost function W GCF (S,A1, . . . , Ak) is tractable projection-safe.\nProof. Follows directly from Theorem 8 and Lemma 1.\nWe have presented a new useful class of tractable projection-safe global cost functions. Algorithm 4 also gives an efficient algorithm to compute the minimum cost. In the following, we give a number of global cost functions that belong to this class."
    }, {
      "heading" : "5. An Example of Polynomially DAG-Decomposable Global Cost Function",
      "text" : "In the following, we show that W Grammarvar is polynomially DAG-decomposable based on Theorems 6 and 7. Other examples of polynomially DAG-decomposable global cost functions can be found in [38]. For simplicity, we assume the scope of each global cost function is S = {x1, . . . , xn}.\nW Grammarvar is the cost function variant of the softened version of the hard global constraint Grammar [29] defined based on a context-free language.\nDefinition 22. A context-free language L(G) is represented by a context-free grammar G = (Σ, N, P,A0), where:\n• Σ is a set of terminals;\n• N is a set of non-terminals;\n• P is a set of production rules from N to (Σ∪N)∗, where ∗ is the Kleene star, and;\n• A0 ∈ N is a starting symbol.\nA string τ belongs to L(G), written as τ ∈ L(G) iff τ can be derived from G.\nThe hard constraint grammar(S,G) authorizes a tuple ℓ ∈ L(S) if τℓ ∈ L(G) [29]. Using the violation measure var by Katsirelos et al. 2011, the W Grammarvar cost function is defined as follows.\nDefinition 23. [30] Given a context-free grammar G = (Σ, N, P,A0). W Grammarvar(S,G) returns min{H(τℓ, τi) | τi ∈ L(G)} for each tuple ℓ ∈ L(S), where H(τ1, τ2) returns the Hamming distance between τ1 and τ2.\nTheorem 10. W Grammarvar(S,G) is a polynomially DAG-decomposable and thus tractable projection-safe global cost function.\nProof. We adopt the dynamic programming approach similar to the modified CYK parser [30]. Without loss of generality, we assume G is in Chomsky normal form, i.e. each production always has the form A → α or A → BC, where A ∈ N , B,C ∈ N \\{A0} and α ∈ Σ.\nDefine ωASi,j = W Grammar var(Si,j , GA), where i ≤ j, Si,j = {xi . . . xj} ⊆ S, and\nGA = (Σ, N, P,A) for A ∈ N . By definition,\nW Grammarvar(S,G)(ℓ) = ωA0S1,n(ℓ)\nThe base cases ωASi,i is defined as follows. Define ΣA = {α | A → α} to be the set of terminals that can be yielded from A.\nωASi,i(ℓ) =\n{\nmin{U {α} i (ℓ[xi]) | (A → α) ∈ P} if ΣA 6= ∅ ⊤ otherwise\nOther sub-cost functions ωASi,j , where i > j, are defined as follows. Define NA = {(B,C) | A → BC} to be the set of pairs of non-terminals that are yielded from A.\nωASi,j (ℓ) =\n{\nmin k=i,...j−1\n{ωBSi,k(ℓ[Si,k])⊕ ω C Sk+1,j (ℓ[Sk+1,j ]) | (A → BC) ∈ P} if NA 6= ∅\n⊤ otherwise\nBy Theorems 6 and 7, the cost function is safely DAG-decomposable. Moreover, the corresponding DAG (V,E) has size |V | = O(|N ∪Σ| · |S|2), in which the leaves are unary functions {U {α} i }. The DAG-decomposition is thus polynomial in the size of the entry, and by Theorem 9, the result follows.\nNote that Theorem 10 also gives a proof that W Amongvar and W Regularvar are tractable projection-safe as they can easily be reduced to W Grammarvar. A Deterministic Finite Automaton (DFA) can be transformed into a grammar with the number of non-terminals and production rules polynomial in the number of states in the DFA.\nFunction GrammarMin(S,G) for i := 1 to n do1 for c ∈ Σ do2 u[i, c] := min{U {c} i };3\nfor i := 1 to n do4 foreach A ∈ N do f [i, i, A] := ⊤;5 foreach (A, a) such that (A 7→ a) ∈ P do6 f [i, i, A] = min{f [i, i, A], u[i, a]} ;7\nfor len := 2 to n do8 for i := 1 to n− len+ 1 do9\nj := i+ len− 1 ;10 foreach A ∈ N do f [i, j, A] := ⊤;112 foreach (A,A1, A2) such that (A 7→ A1A2) ∈ P do13 for k := i to j − 1 do14 f [i, j, A] := min{f [i, j, A], f [i, k, A1]⊕ f [k + 1, j, A2]} ;15\nreturn f [1, n, A0];16\nAlgorithm 5: Finding the minimum of W Grammarvar\nFunction GrammarMin in Algorithm 5 computes the minimum of W Grammarvar(S,G). We first store the minimum of the unary cost function in the table u[i, c] at lines 5 to 5. Then the table f of size n × n × |N | is filled up according to the formulation of the W Grammarvar cost function in Theorem 10 by two separate for-loops at lines 5 to 5, and returns the result at line 5. The time complexity is stated as follows.\nTheorem 11. The function GrammarMin in Algorithm 5 computes the minimum of W Grammarvar(S,G), and it requires O(nd2 + n3 · |P |)), where n = |S| and d is the maximum domain size.\nProof. Lines 5 to 5 takes O(nd · |Σ|) = O(nd2), assuming |Σ| is bounded by d. The first for-loop at lines 5 to 5 requires O(n · |P |), while the second one at lines 5 to 5 requires O(n3 ·|P |). The overall time complexity is O(nd2+n·|P |+n3 ·|P |) = O(nd2+n3 ·|P |).\nThe time complexity of enforcing GAC* can be stated as follows.\nCorollary 1. Given WS = W Grammar var(S,G). Function findSupport() in Algorithm 3 requires O(nd(d2 + n2 · |P |)), where n = |S| and d is the maximum domain size.\nProof. Follows directly from Property 1 and Theorem 11.\nIn this section, we have seen how a global cost function can be decomposed into a DAG structure that allows for efficient minimization and therefore usual soft local consistency enforcement. However, each newly implemented cost function requires to buid a corresponding DAG structure with a dedicated dynamic programming algorithm.\nIn the next section, we show that, in some cases, it is also possible to avoid this by directly decomposing a global cost functions into a CFN in such a way that local consistency enforcement will emulate dynamic programming, avoiding the need for programming dedicated enforcement algorithms."
    }, {
      "heading" : "6. Decomposing Global Cost Functions into CFNs",
      "text" : "In classical CSP, some global constraints can be efficiently represented by a logically equivalent subnetwork of constraints of bounded arities [12, 9], and are said to be decomposable. Similarly, we will show that some global cost functions can be encoded as a sum of bounded arity cost functions. The definition below applies to any cost function, including constraints (cost functions using only costs in {0,⊤}), extending the definition in [12] and [9].\nDefinition 24. For a given integer p, a p-network-decomposition of a global cost function W GCF (S,A1, . . . , Ak) is a polynomial transformation δp that returns a CFN δp(S,A1, . . . , Ak) = (S ∪ E,F ,⊤), where S ∩ E = ∅, such that ∀WT ∈ F , |T | ≤ p and ∀ℓ ∈ LS ,W GCF (S,A1, . . . , Ak)(ℓ) = minℓ′∈ LS∪E ,ℓ′[S]=ℓ ⊕ WSi∈F WSi(ℓ ′[Si]).\nDefinition 24 above allows for the use of extra variables E, which do not appear in the original cost function scope and are eliminated by minimization. We assume, without loss of generality, that every extra variable x ∈ E is involved in at least two cost functions in the decomposition.2 Clearly, if W GCF (S,A1, . . . , Ak) appears in a CFN P = (X ,W ,⊤) and decomposes into (S ∪E,F ,⊤), the optimal solutions of P can directly be obtained by projecting the optimal solutions of the CFN P ′ = (X ∪ E,W \\ {W GCF (S,A1, . . . , Ak)} ∪ F ,⊤) on X .\n6.1. Building Network-Decomposable Global Cost Functions\nA global cost function can be shown to be network-decomposable by exhibiting a bounded arity network decomposition of the global cost function. There is a simple way of deriving network-decomposable cost functions from known decomposable global constraints. The process goes directly from a known decomposable global constraint to a network-decomposable global cost function and does not require to use an intermediate soft global constraint with an associated violation measure µ. Instead, the global cost function will use any relaxation of the decomposed global constraint.\nAs the previousAllDifferent example showed, from a network-decomposable global constraint, it is possible to define an associated network-decomposable global cost function by relaxing every constraint in the decomposition. In this definition, for two cost functions (or constraints) WT and W ′ T of the same scope, we say that WT ≤ W ′ T iff ∀ℓ ∈ LT ,WT (ℓ) ≤ W ′ T (ℓ) (WT is a relaxation W ′ T ).\nTheorem 12. Let GC(S,A1, . . . , Ak) be a global constraint that p-network decomposes into a classical constraint network (S ∪ E,F ,⊤) and fθ be a function parameterized by\n2Otherwise, such a variable can be removed by variable elimination: remove x from E and replace the WT involving x by the cost function minx WT on T \\ {x}. This preserves the Berge-acyclicity of the network if it exists.\nθ that maps every CT ∈ F to a cost function WT such that WT ≤ CT . The global cost function\nW GCF (S,A1, ..., Ak, fθ)(ℓ) = min ℓ′∈ LS∪E ℓ′[S]=ℓ\n⊕\nCT∈F\nfθ(CT )(ℓ ′[T ])\nis a relaxation of GC(S,A1, . . . , An) which is p-network-decomposable by definition.\nProof. Since (S ∪ E,F) is a network- decomposition of GC(S,A1, ..., Ak), for any tuple ℓ ∈ LS , GC(S,A1, ..., Ak)(ℓ) = 0 if and only if minℓ′∈ LS∪E ,ℓ′[S]=ℓ ⊕ CT∈F CT (ℓ ′[T ]) = 0. Let ℓ′ ∈ LS∪E be the tuple where this minimum is reached. This implies that ∀CT ∈ F , CT (ℓ ′[T ]) = 0. Since fθ(CT ) ≤ CT , fθ(CT )(ℓ ′[T ]) = 0. Therefore ⊕\nCT∈F fθ(CT )(ℓ ′[T ]) = 0 andW GCF (S,A1, . . . , Ak, fθ)(ℓ) = 0. Moreover, the global cost function is p-networkdecomposable by construction.\nBy definition, the global cost function W GCF (S,A1, ..., Ak, fθ) is p-network decomposable into (S ∪ E,W,⊤), where W is obtained by applying fθ on every element of C. Since fθ preserves scopes, the hypergraph of the decomposition is also preserved.\nTheorem 12 allows to immediately derive a long list of network decomposable global cost functions from existing network decompositions of global constraints such as AllDifferent, Regular [50], Among and Stretch [10]. The parameterization through fθ also allows a lot of flexibility.\nExample 1. Consider the softened variant W AllDifferentdec(S) of the global constraint AllDifferent(S) constraint using the decomposition violation measure [51] where the cost of an assignment is the number of pairs of variables taking the same value. It is well known that AllDifferent decomposes into a set of n.(n−1)2 binary difference constraints. Similarly, the W AllDifferentdec(S) cost function can be decomposed into a set of n.(n−1)2 soft difference cost functions. A soft difference cost function takes cost 1 iff the two involved variables have the same value and 0 otherwise. In these cases, no extra variable is required. Notice that the two decompositions have the same hypergraph structure.\nAnother relaxation can be considered. Take an arbitrary graph G = (V,E) over V , and consider the relaxation function fG (parameterized by G only) that preserves difference constraints xi 6= xj when (xi, xj) ∈ E but otherwise relaxes them to a constant cost function that is always equal to zero. This gives rise to a global cost function W AllDifferent(V, fG) that clearly captures the graph coloring problem on G, an NPhard problem. Enforcing any soft arc consistency on that single global cost function will be intractable as well since it requires to compute the minimum of the cost function. Instead, enforcing soft arc consistencies such as DAC or VAC on the network-decomposition into binary cost functions will obviously be polynomial but will hinder the level of filtering achieved.\nExample 2. Consider the Regular ({x1, . . . , xn},M) global constraint, defined by a (non necessarily deterministic) finite automaton M = (Q,Σ, δ, q0, F ), where Q is a set of states, Σ the emission alphabet, δ a transition function from Σ×Q → 2Q, q0 the initial state and F the set of final states. .As shown in [11], this constraint decomposes into a constraint network ({x1, . . . , xn} ∪ {Q0, . . . , Qn}, C) where the extra variables Qi have Q as their domain. The set of constraints C in the network decomposition contains two\nunary constraints restricting Q0 to {q0} and Qn to F and a sequence of identical ternary constraints c{Qi,xi+1,Qi+1} each of which authorizes a triple (q, s, q\n′) iff q′ ∈ δ(q, s), thus capturing δ. A relaxation of this decomposition may relax each of these constraints. The unary constraints on Q0 and Qn would be replaced by unary cost functions λQ0 and ρQn stating the cost for using every state as either an initial or final state while the ternary constraints would be relaxed to ternary cost functions σ{Qi,xi+1,Qi+1} stating the cost for using any (q, s, q′) transition.\nThis relaxation precisely corresponds to the use of a weighted automaton MW = (Q,Σ, λ, σ, ρ) where every transition, starting and finishing state has an associated, possibly intolerable, cost defined by the cost functions λ, σ and ρ [20]. The cost of an assignment in the decomposition is equal, by definition, to the cost of an optimal parse of the assignment by the weighted automaton. This defines a W Regular(S,MW ) global cost function which is not parameterized by a violation measure µ and a classical automata, but by a weighted automaton. As shown in [30], a weighted automaton can encode the Hamming and Edit distances to the language of a classical automaton. Contrary to the AllDifferent example, we will see that the W Regular network-decomposition can be handled efficiently and effectively by soft local consistencies."
    }, {
      "heading" : "7. Local Consistency and Network-decompositions",
      "text" : "As we have seen with the W AllDifferent(V, fG) global cost function, the use of network-decompositions instead of a monolithic variant has both advantages and drawbacks. Thanks to local reasoning, a decomposition may be filtered more efficiently, but this may hinder the level of filtering achieved. In classical CSP, it is known that if the decomposition is Berge-acyclic, then enforcing GAC on the decomposition enforces GAC on the global constraint itself [2].\nDefinition 25. The incidence graph of an hypergraph (X,E) is a bipartite graph G = (X ∪ E,EH) where {xi, ej} ∈ EH iff xi ∈ X, ej ∈ E and xi belongs to the hyperedge ej. An hypergraph (X,E) is Berge acyclic iff its incidence graph is acyclic.\nWe now show that a similar result can be obtained for cost functions using either a Terminal Directional Arc Consistency (T-DAC) or Virtual Arc Consistency (VAC).\n7.1. Berge acyclicity and directional arc consistency\nIn this section, we will show that enforcing T-DAC on a Berge-acyclic networkdecomposition of a cost function or on the original global cost function yields the same cost distribution on the last variable and therefore the same lower bound (obtained by node consistency [34]) provided a correct DAC variable ordering is used.\nTheorem 13. If a global cost function W GCF(S,A1, . . . , Ak) decomposes into a Bergeacyclic CFN N = (S ∪E,F), there exists an ordering on S ∪E such that the unary cost function Wxin on the last variable xin produced by enforcing T-DAC on the sub-network (S, {W GCF (S,A1, . . . , Ak)} ∪ {Wxi}xi∈S) is identical to the unary cost function W ′ xin produced by enforcing T-DAC on the decomposition N = (S ∪ E,F ∪ {Wxi}xi∈S).\nProof. Consider the decomposed network N and IN = (S ∪ E ∪ F , EI) its incidence graph. We know that IN is a tree whose vertices are the variables and the cost functions of N . We root IN in a variable of S. The neighbors (parent and children, if any) of a cost functions WT are the variables in T . The neighbors of a variable xi are the cost functions involving xi. Consider any topological ordering of the vertices of IN . This ordering induces a variable ordering (xi1 , . . . , xin), xin ∈ S which is used to enforce TDAC on N . Notice that for any cost function WT ∈ F , the parent variable of WT in IN appears after all the other variables of T .\nConsider a value a ∈ D(xin) of the root. If Wxin (a) = ⊤, then any complete assignment extending this value has cost Wxin (a). Otherwise, Wxin (a) < ⊤. Let WT be any child of xin and ℓ a full support of value a on WT . We have Wxin (a) = WT (ℓ) ⊕\nxi∈T Wxi(ℓ[xi]), which proves that WT (ℓ) = 0 and ∀xi ∈ T, i 6= in,Wxi(ℓ[xi]) =\n0. IN being a tree, we can inductively apply the same argument on all the descendants of xin until leaves are reached, proving that the assignment (xin = a) can be extended to a complete assignment with cost Wxin (a) in N . In either cases, Wxin (a) is the cost of an optimal extension of (xin = a) in N .\nSuppose now that we enforce T-DAC using the previous variable ordering on the undecomposed sub-network (S, {W GCF (S,A1, . . . , Ak)} ∪ {Wxi}xi∈S). Let ℓ be a full support of value a ∈ D(xin) on W GCF(S,A1, . . . , Ak). By definition, Wxin (a) = W GCF (S,A1, . . . , Ak)(ℓ) ⊕\nxi∈S Wxi(ℓ[xi]) which proves that Wxin (a) is the cost of\nan optimal extension of (xin = a) on (S, {W GCF (S,A1, . . . , Ak)} ∪ {Wxi}xii∈S). By definition of decomposition, and since xin 6∈ E, this is equal to the cost of an optimal extension of (xin = a) in N .\nT-DAC has therefore enough power to handle Berge-acyclic network-decompositions without losing any filtering strength, provided a correct order is used for applying EPTs. In this case, T-DAC emulates a simple form of dynamic programming on the networkdecomposition.\nIt should be pointed out that T-DAC is closely related to mini-buckets [22] and Theorem 13 can easily be adapted to this scheme. Mini-buckets perform a weakened form of variable elimination: when a variable x is eliminated, the cost functions linking x to the remaining variables are partitioned into sets containing at most i variables in their scopes and at most m functions (with arity > 1). If we compute mini-buckets using the same variable ordering, with m = 1 and unbounded i, we will obtain the same marginal cost function as T-DAC on the root variable r, with the same time and space complexity. Mini-buckets can be used along two main recipes: precomputed (static) mini-buckets do not require update during search but restrict search to one static variable ordering; dynamic mini-buckets allow for dynamic variable ordering (DVO) but suffer from a lack of incrementality. Soft local consistencies, being based on EPTs, always yield equivalent problems, providing incrementality during search and are compatible with DVO.\n7.2. Berge acyclicity and virtual arc consistency\nVirtual Arc Consistency offers a simple and direct link between CSPs and CFNs which allows to directly lift classical CSP properties to CFNs, under simple conditions.\nTheorem 14. If a global cost function W GCF(S,A1, . . . , Ak) decomposes into a Bergeacyclic CFN N = (S ∪E,F ,⊤) then enforcing VAC on either (S ∪E,F ∪{Wxi}xi∈S ,⊤) or on (S, {W GCF (S,A1, . . . , Ak)} ∪ {Wxi}i∈S ,⊤) yields the same lower bound W∅.\nProof. Enforcing VAC on the CFN P = (S ∪E,F ∪ {Wxi}xi∈S ,⊤) does not modify the set of scopes and yields an equivalent problem P ′ such that Bool(P ′) is Berge-acyclic, a situation where arc consistency is a decision procedure. We can directly make use of Proposition 10.5 of [14], which states that if a CFN P is VAC and Bool(P ) is in a class of CSPs for which arc consistency is a decision procedure, P has an optimal solution of cost w∅.\nSimilarly, the network Q = (S, {W GCF (S,A1, . . . , Ak)} ∪ {Wxi}xi∈T ,⊤) contains just one cost function with arity strictly above 1 and Bool(Q) will be decided by arc consistency. Enforcing VAC will therefore provide a CFN which also has an optimal solution of cost W∅. Finally, the networks P and Q have the same optimal cost by definition of a decomposition."
    }, {
      "heading" : "8. Comparison with tractable DAG-decomposability",
      "text" : "In this section, we show that Berge-acyclic network-decomposable global cost functions are also polynomially DAG-decomposable.\nTheorem 15. Let W GCF (S,A1, . . . , Ak) be a network-decomposable global cost function that decomposes into a CFN (S ∪ E,F ,⊤) with a Berge-acyclic hypergraph. Then W GCF(S,A1, . . . , Ak) is polynomially DAG-decomposable.\nProof. We consider the incidence graph of the Berge-acyclic hypergraph of the CFN (S ∪ E,F ,⊤) and choose a root for it in the original variables S, defining a rooted tree denoted as I. This root orients the tree I with leaves being variables in S and E. In the rest of the proof, we denote by I(xi) the subtree of I rooted in xi ∈ S ∪ E. Abusively, when the context is clear, I(xi) will also be used to denote the set of all variables in the subtree.\nThe proof is constructive. We will transform I in a DAG (actually a tree) of nodes that computes the correct cost min\nℓ′∈ LS∪E),ℓ′[S]=ℓ ⊕ WT∈F WT (ℓ ′[T ]) and satisfies all the\nrequired properties of safe polynomial DAG-decomposability. To achieve this, we need to guarantee that the aggregation function fi = ⊕ is always used on cost functions of disjoint scopes, that fi = min is always applied on identically scoped functions and that sizes remain polynomial.\nWe will be using three types of DAG nodes. A first type of node will be associated with every cost function WT ∈ F in the network-decomposition. Each cost function appears in I with a parent variable xi and a set of children variables among which some may be leaf variables. By the assumption that extra variables belong to at least two cost functions (see paragraph below Definition 24), leaf variables necessarily belong to S. We denote by leaf(T ) the set of leaf variables in the scope T . The first type of node aims at computing the value of the cost function WT combined with the unary cost functions on each leaf variables. This computation will be exploded in a family of nodes U ℓT , where ℓ ∈ LT−leaf(T ) is an assignment of non leaf variables. Therefore, for a given cost function WT and a given assignment ℓ of non leaf variables, we define a DAG node with scope leaf(T ):\nU ℓT (ℓ ′) = WT (ℓ ∪ ℓ\n′) ⊕\nxj∈leaf(T )\nWxj (ℓ ′[xj ])\nThese nodes will be leaf nodes of the DAG-decomposition. Given that all cost functions in I have bounded arity, these nodes have an overall polynomial size and can be computed in polynomial time in the size of the input global cost function.\nNodes of the second and third types are associated to every non leaf variable xi in I. For every value a ∈ D(xi), we will have a node ω a i with scope I(xi) ∩ S. xi may have different children cost functions in I and we denote by Wi the set of all the children cost functions of xi in I. For each WT ∈ Wi, we will also have a DAG node ω i,a T with scope S′i = (I(WT )∪{xi})∩S. Notice that even if these scopes may be large (ultimately equal to S for ωai if xi is the root of I), these nodes are not leaf nodes of the DAG-decomposition and do not rely on an extensional definition and will have reasonable size.\nThe aim of all these nodes is to compute the cost of an optimal extension of the assignment ℓ to the subtree I(WT ) (for ω i,a T ) or I(xi) (for ω a i ). We therefore define:\nωai (ℓ) = ⊕\nWT∈Wi\nωi,aT (ℓ[S ′ i])\nIndeed, if ωi,aT computes the cost of an optimal extension to the subtree rooted in WT , an optimal extension to I(xi) is just the ⊕ of each optimal extension on each children, since the scope S′i do not intersect (I is a tree). The DAG node therefore uses the ⊕ aggregation operator on non intersecting scopes.\nThe definition of the DAG nodes ωi,aT is a bit more involved. It essentially requires to get rid of extra variables in the scope T and to combine the cost of WT , unary cost functions on leaf variables in T (this is achieved by UT nodes) and costs of optimal extensions subtrees rooted in other non leaf variables (this is achieved by ωai nodes). For xi ∈ E or ℓ[xi] = a, this leads to the following definition of ω i,a T (ℓ):\nmin ℓ′∈ LT∩E\n(xi∈S∨ℓ ′[xi]=a)\n(U (ℓ∪ℓ′)[T−leaf(T )] T (ℓ[leaf(T )])\n⊕\nxj∈(T−leaf(T )−{xi})\nω ℓ[xj] j (ℓ[Sj ])) (1)\nOtherwise, ωi,aT (ℓ) = ⊤. Note that the definition of ω i,a T (ℓ) depends on whether xi ∈ E or not. First, if xi is not in E, then we minimize over all extra variables in T and else exclude xi from the minimization. Second, if xi ∈ S and ℓ[xi] 6= a, we return ⊤ as there is no optimal extension of ℓ that extends (xi, a).\nIf we consider the root variable xi ∈ S of I, the ω a i nodes provide the cost of a best extension of any assignment ℓ (if ℓ[xi] = a) or ⊤ otherwise. An ultimate root DAG node using the aggregation operator min over all these ωai will therefore return the optimal extension of ℓ ∈ LS to all variables in I(xi), including extra variables.\nFrom equation 1, one can see that nodes ωi,aT use the aggregation operator min on intermediary nodes. These intermediary nodes combine the node UT and ωj with ⊕ which have non intersecting scopes.\nOverall all those nodes form a DAG (actually a tree). In this tree, every node with the aggregation operation ⊕ is applied to operands with non intersecting scopes. Similarly, every node with the min aggregation operation is applied to functions whose scope is always identical. Note that the definitions of the ωai and ω i,a T are linear respectively in the number of children of WT or xi respectively. So, we have an overall safe and tractable DAG-decomposition.\nFor a global cost function which is Berge acyclic network-decomposable, and therefore also polynomially DAG-decomposable, a natural question is which approach should be preferred. From a pure theoretical point of view, the strongest possible lower bound that could be obtained in this settings would be using VAC on the network-decomposition. Indeed VAC is stronger than EDAC. In practice, however, this requires the availability of an efficient VAC enforcing implementation, able to cope with the maximum arity p encountered in the CFN-decomposition (usually 3).\nT-DAC is extremely efficient, easy to implement, and any consistency above T-DAC using a topological order of the Berge acyclic decomposition will provide good lower bounds and incrementality for little effort. However, when several global cost functions co-exist in a problem, there may not always exist a variable order that is a topological sort of all these global cost functions. It then becomes increasingly attractive to use one of the dedicated dynamic programming algorithms provided by polynomial DAGdecompositions to process these cost functions. Moreover, when using local consistencies above T-DAC, such as (weak) EDGAC*, dedicated dynamic programming algorithms will effectively provide eg. (weak) EDGAC* on the global cost function while enforcing EDAC* on a Berge-acyclic decomposition only theoretically guarantees to provide more than T-DAC.\nIn the end, the only truly informative answer will be provided by experimental results, as proposed in Section 9."
    }, {
      "heading" : "9. Experiments",
      "text" : "In this section, we put theory into practice, by implementing the cost functions described in the previous sections in toulbar2 v0.9.73 to demonstrate the practicality of our algorithmic framework in solving over-constrained and optimization problems. For each cost function, we implemented weak EDGAC* using a decidated DAG-structure and the corresponding dynamic programming algorithm (called DAG-based approach in the sequel). When possible, we also implemented an automatic Berge-acyclic decomposition into a cost function network to be propagated using EDAC* (called network-based approach).\nIn the experiments, we used default options for toulbar2 to perform a depth-first branch-and-bound search. The default variable ordering strategy is based on the Weighted Degree heuristics [13] with Last Conflict [37], while the default value ordering consists in choosing for each variable its fully supported value as defined by (weak) ED(G)AC*. The tests are conducted on a single core of an Intel Xeon E5430 (2.66GHz) machine with 64GB RAM. Each benchmark has a 5-minute timeout. We randomly generate 30 instances for each parameter setting of each benchmark. We first compare the number of solved instances, i.e., finding the optimum and proving its optimality (no initial upper bound). Among the instances solved by the considered technique, we report their average run-time in seconds and number of backtracks. The best results are marked in bold (taking first into account the number of solved instances in less than 5 minutes and secondly CPU time).\n3https://mulcyber.toulouse.inra.fr/projects/toulbar2.\n9.1. The Car Sequencing Problem The problem (prob001 in CSPLib) [48] requires sequencing of n cars of different type specified by a set of different options. For any subsequence of ci consecutive cars on the assembly line, the option oi can be installed on at most mi of them. The problem is to find a production sequence on the assembly line such that each car can be installed with all the required options without violating the capacity constraint. We use n variables with domain 1 to n to model this problem. The variable xi denotes the type of the ith car in the sequence. One GCC (global cardinality [46]) constraint ensures all cars are scheduled on the assembly line. We post n − ci + 1 Among constraints [7] for each option oi to ensure the capacity constraint is not violated. We randomly generate 30 over-constrained instances, each of which has 5 possible options, and the block size of at most 7, i.e. 1 ≤ mi < ci ≤ 7. Each car in each instance is randomly assigned to one type, and each type is randomly assigned to a set of options in such a way that each option has 1/2 chance to be included in each type.\nThe problem is softened in three different ways. The first softened model is obtained by replacing each Among constraint by the W Amongvar cost function and the hard GCC constraints by the W GCCvar cost function [43] that returns ⊤ on violation. This model is called Flow&DP-based approach in Table 1. The second model, called DAGbased approach in Table 1, uses a set of W Amongvar cost functions to encode GCC, i.e., replacing a single global cost function exploiting a flow network [43] by a set of DAGbased global cost functions [38]. In the third model, called network-based approach in Table 1, each W Amongvar is decomposed into a set of ternary cost functions with extra variables as described in Section 6. In all three models we randomly assign unary costs (between 0 to 9) to each variable.\nTable 1 gives the experimental results. Column n′ indicates the number of extra variables added in the network-based approach. Clearly this approach is inefficient as n grows. The pure DAG-based approach is up to one-order of magnitude faster than the mixed flow&DAG-based approach and surprisingly develops the least number of backtracks."
    }, {
      "heading" : "11 30 382.5 6.46 30 187.7 1.01 293 30 21,712.0 22.13",
      "text" : ""
    }, {
      "heading" : "12 30 838.1 21.02 30 324.1 2.48 344 28 105,093.0 122.10",
      "text" : ""
    }, {
      "heading" : "13 30 1,960.8 57.53 30 1,022.3 8.76 396 7 94,810.4 115.07",
      "text" : ""
    }, {
      "heading" : "14 21 1,729.7 70.23 30 2,401.1 26.91 451 2 12,341.5 16.89",
      "text" : ""
    }, {
      "heading" : "15 14 1,823.8 69.14 26 3,936.1 49.34 507 2 85,177.0 120.55",
      "text" : "9.2. The Nonogram Problem The problem (prob012 in CSPLIB) [28] is a typical board puzzle on a board of size p × p. Each row and column has a specified sequence of shaded blocks. For example, a\nrow specified (2, 3) implies it contains two sequences of shaded blocks, one with length 2 and another with length 3. The problem is to find out which blocks need to be shaded such that each row and each column contain the specific sequences of shaded blocks. We model the problem by n = p2 variables, in which xij denotes whether the block at the ith row and jth column needs to be shaded. In the experiments, we generate random instances from a set of sequences for each row and column, some of which are over-constrained. We model and soften the restrictions on each row and column by W Regularvar , resulting in three models: flow-based, DAG-based (See the associated technical report [38]), and network-based. The flow-based model use the W Regularvar implementation based on minimum cost flows described in [43], the DAG-based uses the DAG-decomposed version (described in detail in [38]) and the network-based version uses the decomposition presented in Section 6.\nTable 2 shows the results of the experiments. For this problem, the network-based approach develops the least number of backtracks and it is at least one-order of magnitude faster than the DAG-based approach and two-order of magnitude faster than the flowbased approach."
    }, {
      "heading" : "36 30 14.4 0.15 30 14.5 0.01 96 30 4.4 0.00",
      "text" : ""
    }, {
      "heading" : "49 30 49.0 0.74 30 56.2 0.06 133 30 20.9 0.00",
      "text" : ""
    }, {
      "heading" : "64 30 196.9 6.09 30 482.4 0.85 176 30 87.7 0.02",
      "text" : ""
    }, {
      "heading" : "81 30 396.9 22.00 30 625.3 1.95 225 30 386.1 0.07",
      "text" : ""
    }, {
      "heading" : "100 15 727.4 57.97 29 11,328.0 41.82 280 30 4,421.3 0.88",
      "text" : ""
    }, {
      "heading" : "121 10 740.4 90.91 19 12,278.3 49.96 341 29 17,056.5 3.50",
      "text" : ""
    }, {
      "heading" : "144 2 546.5 101.83 5 15,718.4 117.05 408 24 132,105.0 30.24",
      "text" : "9.3. Well-formed Parentheses\nGiven a set of 2p even length intervals within [1, . . . , 2p], the well-formed parentheses problem is to find a string of parentheses with length 2p such that substrings in each of the intervals are well-formed parentheses. We model this problem by a set of n = 2p variables. Domains of size 6 are composed of three different parenthesis types: ()[]{}. We post a W Grammarvar cost function for each interval to represent the requirement of well-formed parentheses. We generate 2p− 1 even length intervals by randomly picking their end points in [1, . . . , 2p], and add an interval covering the whole range to ensure that all variables are constrained. We also randomly assign unary costs (between 0 to 10) to each variable.\nWe compare two models. The first model, the DAG-based approach, is obtained by replacing each W Grammarvar cost function by its DAG-decomposition. In the second model, the network-based approach, we decompose each W Grammarvar cost function involving m variables using m(m − 1)/2 extra variables P [i, j] (j ≤ m, i ≤ m − j + 1) whose value corresponds to a pair of a symbol value S and a string length k (k < j)\nassociated to the substring (i, i+j−1), starting from i of length j. Ternary cost functions link every triplet P [i, j], P [i, k], P [i + k, j − k] such that there exists a compatible rule S->AB in order to get the substring (i, i + j − 1) from the two substrings (i, i + k − 1) and (i+ k, i+ j − 1) when P [i, j] = (S, k), P [i, k] = (A, u), P [i+ k, j − k] = (B, v) with u < k, v < j − k. Binary cost functions are used to encode the terminal rules between P [i, 1] (i ∈ [1,m]) and the original variables."
    }, {
      "heading" : "10 30 7.6 1.72 250 30 54,117.4 17.31",
      "text" : ""
    }, {
      "heading" : "12 30 10.5 5.60 392 5 334,507.0 130.23",
      "text" : ""
    }, {
      "heading" : "14 30 25.9 24.45 580 1 408,251.0 259.71",
      "text" : ""
    }, {
      "heading" : "16 24 46.1 47.27 841 0 N/A N/A",
      "text" : ""
    }, {
      "heading" : "18 22 44.1 49.96 1,146 0 N/A N/A",
      "text" : "Results are shown in Table 3. The network-based approach is clearly inefficient. It has n′ = 1146 extra variables on average for p = 9. The number of backtracks explodes rapidly. The DAG-based approach clearly dominates here.\nAs a second experiment on well-formed parentheses, we generate new instances using only one hard global grammar constraint and a set of p(2p − 1) binary cost functions. For each pair of positions, if a parentheses pair ((), [], or {}) is placed at these specific positions, then it incurs a randomly-generated cost (between 0 to 10). A single W Grammarvar cost function is placed on all the n = 2p variables, which returns ⊤ on violation, ensuring that the whole string has well-formed parentheses. As in the experiments of Table 3, the two models are characterized by the way the W Grammarvar cost function is decomposed: a DAG-decomposition for the DAG-based approach, a networkdecomposition for the network-based approach.\nResults are shown in Table 4. Although it develops more nodes on average, having only one global constraint implies less extra variables for the network-based approach (n′ = 189 for p = 9), resulting in better time efficiency compared to the DAG-based approach .\n9.4. Beyond network-decomposable cost functions\nIn some cases, problems may contain global cost functions which are not networkdecomposable because the bounded arity cost function decomposition is not polynomial in size. However, if the network is Berge-acyclic, Theorem 13 still applies. With exponential size networks, filtering will take exponential time, but may yield strong lower bounds. The linear equation global constraint\n∑n i=1 aixi = b (a and b being small integer\ncoefficients) can be easily decomposed by introducing n − 3 intermediate sum variables qi and ternary sum constraints of the form qi−1 + aixi = qi with i ∈ [3, n − 2] and a1x1 + a2x2 = q2, qn−2 + an−1xn−1 + anxn = b. The extra variables qi have b values. And b which is exponential in the size of its representation. We consider the Market Split"
    }, {
      "heading" : "10 30 148.8 1.08 65 30 116.6 0.18",
      "text" : ""
    }, {
      "heading" : "12 30 509.8 7.76 90 30 926.7 0.66",
      "text" : ""
    }, {
      "heading" : "14 30 1,574.3 46.20 119 30 8,358.2 3.46",
      "text" : ""
    }, {
      "heading" : "16 20 3,909.7 200.92 152 30 59,019.5 22.28",
      "text" : ""
    }, {
      "heading" : "18 0 N/A N/A 189 28 419,900.0 169.93",
      "text" : "problem defined in [19, 59]. The goal is to minimize ∑n i=1 oixi such that ∑n\ni=1 ai,jxi = bj for each j ∈ [1,m] and xi are Boolean variables in {0, 1} (o, a and b being positive integer coefficients). We compared the Berge-acyclic decomposition in toulbar2 (version 0.9.5) with a direct application of the Integer Linear Programming solver cplex (version 12.2.0.0). We generated random instances with random integer coefficients in [0, 99] for o and a, and bj = ⌊ 1 2 ∑n i=1 ai,j⌋. We used a sample of 50 problems with m = 4, n = 30 leading to max bj = 918. The mean number of nodes developed in cplex is 50% higher than in toulbar2. However, cplex was on average 6 times faster than toulbar2 on these problems. The 0/1 knapsack problem probably represent a worst case situation for toulbar2, given that cplex embeds much of what is known about 0/1 knapsacks (and only part of these extend to more complicated domains). Possible avenues to improve toulbar2 results in this unfavorable situation would be to use a combination of the m knapsack constraints into one as suggested in [59]."
    }, {
      "heading" : "10. Conclusion",
      "text" : "Our contributions are four-fold. First, we define the tractability of a global cost function, and study its behavior with respect to projections/extensions with different arities of cost functions. We show that tractable r-projection-safety is always possible for projections/extension to/from the nullary cost function, while it is alway impossible for projections/extensions to/from r-ary cost functions for r ≥ 2. When r = 1, we show that a tractable cost function may or may not be tractable 1-projection-safe. Second, we define polynomially decomposable cost functions and show them to be tractable 1-projection-safe. We give also a polytime dynamic programming based algorithm to compute the minimum of this class of global cost functions. We also show that the cost function W Grammarvaris polynomially decomposable and tractable 1-projection-safe. The same results applies to W Amongvar, W Regularvar , and W Max/W Min as shown in the associated technical report [38]. Third, we show that dynamic programming can be emulated by soft consistencies such as DAC and VAC if a suitable network decomposition of the global cost function into a Berge-acyclic network of bounded arity cost functions exists. In this case, local consistency on the decomposed network is essentially as strong as on the global cost function. This approach is shown to be a specific case of the previous approach in the sense that any Berge-acyclic network-decomposable\ncost function is also DAG-decomposable. Finally, we perform experiments and compare the DAG-based and network-based approaches, in terms of run-time and search space. The DAG-based approach dominates when there are several overlapping global cost functions. On the contrary, the network-based approach performs better if there are few global cost functions resulting in a reasonable number of extra variables. Moreover, enforcing EDGAC* on the DAG-based approach usually propagates better than EDAC* on the network-based approach. And conversely, additional techniques such as boosting search by variable elimination [32], Weighted Degree heuristics [13], and Dead-End Elimination [26] work better with low-arity cost functions of the network-based approach. We also compare against the flow-based approach [43] and show that our approaches are usually more competitive.\nAn immediate possible future work is to investigate other sufficient conditions for polynomial decomposability and also tractable 1-projection-safety. Our results only provide a partial answer. Whether there exists necessary conditions for polynomial decomposability is unknown. Besides polynomial decomposability, we would like to investigate other form of tractable 1-projection-safety and techniques for enforcing typical consistency notions efficiently."
    }, {
      "heading" : "1 Introduction",
      "text" : "This companion manuscript should be read in conjunction with the paper of Allouche, Bessiere, Boizumault, de Givry, Guttierez, Lee, Leung, Loudni, Metivier, Schiex, and Wu (2015), which contains the necessary definitions, notations and theorems."
    }, {
      "heading" : "2 Polynomial DAG-Decomposable Global Cost Functions",
      "text" : "In the following, we show that W Amongvar,W Regularvar , W Max, and W Min are Polynomially DAG-Decomposable and thus Tractable Projection-Safe. For simplicity, we assume the scope of each global cost function to be S = {x1, . . . , xn}.\n2.1 The W Amongvar Cost Function\nW Amongvar is the cost function variant of the softened version ofAmong using the corresponding variablebased violation measure (Solnon, Cung, Nguyen, & Artigues, 2008).\nDefinition 1. (Solnon et al., 2008) Given a set of values V , a lower bound lb and an upper bound ub such that 0 ≤ lb ≤ ub ≤ |S|. W Amongvar(S, lb, ub, V ) returns max{0, lb − t(ℓ, V ), t(ℓ, V ) − ub}, where t(ℓ, V ) = |{i | ℓ[xi] ∈ V }| for each tuple ℓ ∈ D(S).\nExample 1. Consider S = {x1, x2, x3}, where D(x1) = D(x2) = D(x3) = {a, b, c, d}. The cost returned by W Amongvar(S, 1, 2, {a, b})(ℓ) is:\n• 0 if ℓ = (a, b, c, d);\n• 1 if ℓ = (c, d, c, d);\n• 2 if ℓ = (a, b, a, b);\nTheorem 1. W Amongvar(S, lb, ub, V ) is polynomially DAG-decomposable and thus tractable projectionsafe.\nProof. We first define two base cases UVi and U V i . The function U V i is the cost function on xi defined as:\nUVi (v) =\n{\n0, if v ∈ V ; 1, otherwise\nand U V\ni (v) = 1− U V i is its negation.\nWe construct W Amongvar based on UVi and U V i . Define ω j Si = W Amongvar(Si, j, j, V ), where Si = {x1, . . . , xi} ⊆ S. By definition, Si = Si−1 ∪{xi} and S0 = ∅. W Among\nvar(S, lb, ub, V ) can be represented by the sub-cost functions ωjSi as:\nW Among var(S, lb, ub, V )(ℓ) = min lb≤j≤ub {ωjSn(ℓ)}\nand each ωjSi can be represented as:\nωjS0(ℓ) = j ω0Si(ℓ) = ω 0 Si−1 (ℓ[Si−1])⊕ U V i (ℓ[xi]) for i > 0\nωjSi(ℓ) = min\n{\nωj−1Si−1(ℓ[Si−1])⊕ U V i (ℓ[xi]) ωjSi−1(ℓ[Si−1])⊕ U V i (ℓ[xi]) for j > 0 and i > 0\nFunction AmongMin in Algorithm 1 computes the minimum of the W Amongvar(S, lb, ub, V ) cost function according to Theorem 1. Lines 1 to 3 compute the minimum costs returned by each additional unary cost functions and store at the arrays u and u. Lines 4 to 8 builds up the results by filling the table f of size n × ub according to the formulation stated in the proof of Theorem 1, and return the result at line 9. The complexity is stated in Theorem 2 as follows.\nTheorem 2. Function AmongMin in Algorithm 1 computes the minimum of W Amongvar(S, lb, ub, V ) and requires O(n(n+ d)), where n = |S| and d is the maximum domain size.\nProof. Lines 1 to 3 in Algorithm 1 take O(nd). Lines 4 to 9 requires O(n ·ub). Since ub is bounded by n, the result follows.\nCorollary 1. Given WS = W Among var(S, lb, ub, V ). Function findSupport() in Algorithm 3 from Allouche et al. (2015) requires O(nd(n+ d)).\nProof. Follow directly from Property 2 in Allouche et al. (2015) and Theorem 2 .\nFunction AmongMin(S, lb, ub, V ) for i = 1 to n do1\nu[i] := min{U V\ni };2 u[i] := min{UVi };3\nfor j = 0 to ub do f [0, j] := j ;4 for i = 1 to n do5 f [i, 0] := f [i− 1, 0]⊕ u[i] ;6 for j = 1 to ub do7 f [i, j] := min{f [i− 1, j − 1]⊕ u[i], f [i− 1, j]⊕ u[i]} ;8\nreturn minlb≤j≤ub{f [n, j]} ;9\nAlgorithm 1: Finding the minimum of W Amongvar\n2.2 The W Regularvar Cost Function\nW Regularvar is the cost function variant of the softened version of the hard constraint Regular (Pesant, 2004) based on a regular language.\nDefinition 2. A regular language L(M) is represented by a deterministic finite state automaton (DFA) M = (Q,Σ, δ, q0, F ), where:\n• Q is a set of states;\n• Σ is a set of characters;\n• The transition function δ is defined as: δ : Q × Σ 7→ Q;\n• q0 ∈ Q is the initial state, and;\n• F ⊆ Q is the set of final states.\nA string τ lies in L(M), written as τ ∈ L(M), iff τ can lead the transitions from q0 to qf ∈ F in M\nThe hard constraint Regular(S,M) authorizes a tuple ℓ ∈ D(S) if τℓ ∈ L(M), where τℓ is the string formed from ℓ (Pesant, 2004). A W Regularvar cost function is defined as follows, derived from the variable-based violation measure given by Beldiceanu, Carlsson, and Petit (2004) and van Hoeve, Pesant, and Rousseau (2006).\nDefinition 3. (Beldiceanu et al., 2004; van Hoeve et al., 2006) Given a DFA M = (Q,Σ, δ, q0, F ). The cost function W Regularvar(S,M) returns min{H(τℓ, τi) | τi ∈ L(M)} for each tuple ℓ ∈ D(S), where H(τ1, τ2) returns the Hamming distance between τ1 and τ2.\nExample 2. Consider S = {x1, x2, x3}, where D(x1) = {a} and D(x2) = D(x3) = {a, b}. Given the DFA M shown in Figure 2. The cost returned by W Regularvar(S,M)(ℓ) is 1 if ℓ = (a, b, a). The assignment of x3 need changed in the tuple (a, b, a) so that L(M) accepts the corresponding string aba.\nTheorem 3. W Regularvar(S,M) is polynomially DAG-decomposable and thus tractable projection-safe.\nProof. W Regularvar can be represented as a DAG (Beldiceanu et al., 2004; van Hoeve et al., 2006; Lee & Leung, 2012), which directly gives a polynomial DAG-decomposition. In the following, we reuse the symbols Si and U V i (v), which are defined in the proof of Theorem 1.\nDefine ωjSi to be the cost function W Regular var(Si,Mj), where Mj is the DFA (Q,Σ, δ, q0, {qj})).\nW Regularvar(S,M) can be represented as:\nW Regular var(S,M)(ℓ) = min qj∈F {ωjSn(ℓ)}\nThe base cases ωjS0 are defined as:\nωjS0(ℓ) =\n{\n0, if j = 0 ⊤, otherwise\nOther sub-cost functions ωjSi , where i > 0, are defined as follows. Define δqj = {(qi, v) | δ(qi, v) = qj}. If δqj = ∅, no transition can lead to qj .\nωjSi(ℓ) =\n{\nmin δ(qk,v)=qj\n{ωkSi−1(ℓ[Si−1])⊕ U {v} i (ℓ[xi])}, if δqj 6= ∅\n⊤, otherwise\nThe corresponding DAG is shown in Figure 3, based on Example 2. Again, the same notation as in Figure 1 is used. The DAG has a number of vertices |V | = O(|S| · |Q|), and its leaves are unary functions. The DAG-decomposition is thus polynomial, and, by Theorem 9 by Allouche et al. (2015), the result follows.\nFunction RegularMin in Algorithm 2 computes the minimum of a W Regularvar(S,M) cost function. The algorithm first initializes the table u by assigning min{U ci } to u[i, c] at lines 1 and 2. Lines 3 to 8 fills up the table f of the size n×|Q|. Each entry f [i, j] in f holds the value min{ωjSi}, which is computed according to the formulation stated in Theorem 3, and returns the result at line 9.\nThe time complexity of RegularMin in Algorithm 2 can be stated as follows.\nTheorem 4. Function RegularMin in Algorithm 2 computes the minimum of W Regularvar(S,M), and it requires O(nd · |Q|), where n and d are defined in Theorem 2.\nProof. Lines 1 and 2 in Algorithm 2 requires O(n · |Σ|). Because |Σ| is bounded by d, the time complexity is O(n · d). Lines 3 to 8 require O(nd · |Q|), according to the table size. Line 9 requires O(|Q|). The overall time complexity is O(nd+ nd · |Q|+ |Q|) = O(nd · |Q|).\nFunction RegularMin(S,M) for i := 1 to n do1 for c ∈ Σ do u[i, c] := min{U {c} i } ;2\nf [0, 0] := 0 ;3 for qj ∈ Q \\ {q0} do f [0, j] := ⊤ ;4 for i := 1 to n do5 f [i, j] := ⊤;6 foreach (qk, qj , c) such that δ(qk, c) = qj do7 f [i, j] = min{f [i, j], f [i, k]⊕ u[i, c]};8\nreturn minqj∈F {f [n, j]} ;9\nAlgorithm 2: Finding the minimum of W Regularvar\nWe state the time complexity of enforcing GAC* with respect to W Regularvar as follows.\nCorollary 2. Given WS = W Regular var(S,M). Function findSupport() in Algorithm 3 from Allouche et al. (2015) requires O(nd2 · |Q|), where n and d is defined in Theorem 2.\nProof. Follow directly from Property 2 in Allouche et al. (2015) and Theorem 4.\nThe time complexity is polynomial in the size of the input (S,M) which includes the scope of size n (with associated domains of size at most d) and the finite automata M , including Q.\n2.3 The W Max/W Min Cost Functions\nDefinition 4. Given a function f(xi, v) that maps every variable-value pair (xi, v), where v ∈ D(xi), to a cost in {0 . . .⊤}.\n• The W Max(S, f)(ℓ), where ℓ ∈ D(S), returns max{f(xi, ℓ[xi]) | xi ∈ S};\n• The W Min(S, f)(ℓ), where ℓ ∈ D(S), returns min{f(xi, ℓ[xi]) | xi ∈ S}.\nNote that theW Max andW Min cost functions are not a direct generalization of any global constraints. Therefore, their name does not follow the traditional format. However, they can be used to model the Maximum and Minimum hard constraints (Beldiceanu, 2001). For examples, the Maximum(xmax, S) can be represented as xmax = W Max(S, f), where f(xi, v) = v.\nExample 3. Consider S = {x1, x2, x3}, where D(x1) = {1, 3}, D(x2) = {2, 4}, and D(x3) = {2, 3}. Given f(xi, v) = 3× v, the cost of the tuple (1, 2, 3) given by W Max(S, f) is 9, while that of (3, 4, 2) is 12.\nTheorem 5. W Max(S, f) and W Min(S, f) are polynomially DAG-decomposable, and thus tractable projection-safe.\nProof. DecomposingW Max(S, f) and W Min(S, f) directly using Definition 4 does not lead to a tractablesafe DAG-decomposition. We give a polynomial DAG-decomposition of W Max(S, f) only, since that of W Min(S, f) is similar. For the ease of explanation, we arrange all possible outputs of f in a non-decreasing sequence A = [α0, α1, . . . , αk], where αi ≤ αj iff i ≤ j. The value α0 = 0 is added into the sequence as a base case.\nWe define three familys of unary cost functions {Hui }, {G α j } and {F α j }. Cost functions {H u i | xi ∈ S∧u ∈\nD(xi)} are unary functions on xi ∈ S defined as\nHui (v) =\n{\nf(xi, v), if v = u ⊤, if v 6= u\nThe unary cost functions F αk−1 j are unary cost functions on xj ∈ S defined as:\nFαkj (u) =\n{\n0, if αk = f(xj , u) ⊤, otherwise\nCost functions {Gαkj | xj ∈ S ∧ αk ∈ A} are unary functions on xj ∈ S, defined recursively as:\nThe corresponding DAG (V,E) of the decomposition as shown in Figure 4 based on Example 3. The notation is the same as Figure 1. The DAG contains |V | vertices, where |V | = O(nd · n2d) = O(n3d2)."
    }, {
      "heading" : "By Theorem 6 and 7 by Allouche et al. (2015), the decomposition is safely DAG-decomposition. Moreover, the leaves are unary cost functions. The DAG-decomposition is polynomial and, by Theorem 9 by Allouche et al. (2015), the result follows.",
      "text" : "Example 4. Following Example 3, the sequence A is defined as:\nA = [α0, α1, α2, α3, α4, α5, α6]\n= [0, f(x1, 1), f(x2, 2), f(x3, 2), f(x1, 3), f(x3, 3), f(x2, 4)]\n= [0, 3, 6, 6, 9, 9, 12]\nThe DAG-decomposition for W Max can be represented as:\nW Max(S, c)(ℓ) = min\n\n      \n      \nH42 (ℓ[x2])⊕G α6 1 (ℓ[x1])⊕G α6 3 (ℓ[x3]), H33 (ℓ[x3])⊕G α5 1 (ℓ[x1])⊕G α5 2 (ℓ[x2]), H31 (ℓ[x1])⊕G α4 2 (ℓ[x2])⊕G α4 3 (ℓ[x3]), H23 (ℓ[x3])⊕G α3 1 (ℓ[x1])⊕G α3 2 (ℓ[x2]), H22 (ℓ[x2])⊕G α2 1 (ℓ[x1])⊕G α2 3 (ℓ[x3]), H11 (ℓ[x1])⊕G α1 2 (ℓ[x2])⊕G α1 3 (ℓ[x3]),\n\n      \n      \nAssume ℓ = (1, 2, 3). We first compute the values of {Hui } and {G α j }, incrementally starting from α0. The results are shown in Table 1.\nTable 1: Computing the values of {Hui } and {G α j }\nαj H u i G αj 1 G αj 2 G αj 3\nα0 − ⊤ ⊤ ⊤ α1 = f(x1, 1) = 3 3 0 ⊤ ⊤ α2 = f(x2, 2) = 6 6 0 0 ⊤ α3 = f(x3, 2) = 6 ⊤ 0 0 0 α4 = f(x1, 3) = 9 ⊤ 0 0 0 α5 = f(x3, 3) = 9 9 0 0 0 α6 = f(x2, 4) = 12 ⊤ 0 0 0"
    }, {
      "heading" : "W Max(S,c)(ℓ) can be computed using Table 1, which gives the cost 9.",
      "text" : "W Max(S, c)(ℓ) = min\n\n     \n      ⊤⊕ 0⊕ 0, ⊤⊕ 0⊕ 0, 9⊕ 0⊕ 0, ⊤⊕ 0⊕ 0, 6⊕ 0⊕⊤, 3⊕⊤⊕⊤,\n\n     \n     \n= 9\nFunction WMaxMin in Algorithm 3 computes the minimum of a W Max(S, c) cost function, based on Equation 1. The one for W Min(S, c) is similar. The for-loop at line 5 tried every possible variable-value pair (xi, a) in the non-decreasing order of f(xi, a). At each iteration, it first computes the minimum among all tuple ℓ which ℓ[xi] = v and it is the maximum weighted component in the tuple in line 7, and update the global minimum in line 8. The variables {g[xi]} is then updated in line 9. They store the current minimum of {Gαi }, which is used for compute the minimum among tuples with ℓ[xi] is not the maximum weighted component.\nThe time complexity is given by the theorem below.\nTheorem 6. Function WMaxMin in Algorithm 3 computes the minimum of W Max(S, f), and it requires O(nd · log(nd)), where n and d are defined in Theorem 2.\nProof. Line 3 takes O(nd · log(nd)) to sort. The for-loop at line 5 iterates nd times. All operations in the iteration requires O(1) except line 7. As it is, line 7 requires O(n). By using special data structure like segment trees (Bentley, 1977), the time complexity can be reduced to O(log(n)). The overall complexity becomes O(nd · log(nd) + nd · log(n)) = O(nd · log(nd)).\nCorollary 3. Given WS = W Max(S, f). The function findSupport() in Algorithm 3 from Allouche et al. (2015) requires O(nd2 · log(nd)), where n and d is defined in Theorem 2.\nProof. Follow directly from Property 2 in Allouche et al. (2015) and Theorem 6.\nFunction WMaxMin(S, f) for i := 1 to n do g[xi] := ⊤ ;1 curMin := ⊤ ;2 A := {(xi, v) | xi ∈ S ∧ v ∈ D(xi)};3 sort A in the nondecreasing order of f(xi, v);4 foreach (xi, v) according to the sorted list A do5 α := f(xi, v);6 curCost := Hvi (v)⊕ ⊕ j=1...n,j 6=i g[xj ];7 curMin := min{curMin, curCost};8 g[xi] := min{g[xi], G α i (v)};9\nreturn curMin ;10\nAlgorithm 3: Finding the minimum of W Max"
    }, {
      "heading" : "3 Conclusion",
      "text" : "In this manuscript, we have shown that W Amongvar, W Regularvar ,W Max and W Min, are polynomially DAG-decomposable. We also give the respective polytime dynamic programming based algorithms to compute the minimum of this class of global cost functions."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Enforcing local consistencies in cost function networks is performed by applying so-called Equivalent Preserving Transformations (EPTs) to the cost functions. As EPTs transform the cost functions, they may break the property that was making local consistency enforcement tractable on a global cost function. A global cost function is called tractable projection-safe when applying an EPT to it is tractable and does not break the tractability property. In this paper, we prove that depending on the size r of the smallest scopes used for performing EPTs, the tractability of global cost functions can be preserved (r = 0) or destroyed (r > 1). When r = 1, the answer is indefinite. We show that on a large family of cost functions, EPTs can be computed via dynamic programming-based algorithms, leading to tractable projection-safety. We also show that when a global cost function can be decomposed into a Berge acyclic network of bounded arity cost functions, soft local consistencies such as soft Directed or Virtual Arc Consistency can directly emulate dynamic programming. These different approaches to decomposable cost functions are then embedded in a solver for extensive experiments that confirm the feasibility and efficiency of our proposal.",
    "creator" : "LaTeX with hyperref package"
  }
}