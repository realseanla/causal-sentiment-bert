{
  "name" : "1411.7806.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Two Gaussian Approaches to Black-Box Optomization",
    "authors" : [ "Lukáš Bajer", "Martin Holeňa" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n78 06\nv1 [\ncs .N\nE ]\n2 8\nN ov\n2 01\n4\nTwo Gaussian Approaches to Black-Box Optomization\nLukáš Bajer Martin Holeňa"
    }, {
      "heading" : "1 CMA Evolution Strategy",
      "text" : "CMA-ES [7, 8] is the state-of-the-art evolutionary optimization method, at least in the area of continuous black-box optimization. Basically, it consists in generating new search points by sampling from a multidimensional normal distribtion, the mean and variance of which are updated from generation to generation. In particular, the population x (g+1) 1 , . . . , x (g+1) λ ∈ R\nd of the g + 1-st generation, g ≥ 1, follows the normal distribution with mean m(g) ∈ Rd and variance (σ(g))2C(g) ∈ Rd,d resulting from the update in the g-th generation,\nx (g+1) i ∼ N ( m(g), (σ(g))2C(g) ) . (1)\nHere, σ(g) is the step size in the g-th generation. C(g) ∈ Rd,d and σ(g) > 0 are updated separately, C(g) being in the most simple case obtained as the unbiased empirical estimate based on the g-th generation:\nC(g)emp = 1\nλ− 1\nλ ∑\ni=1\n(\nx (g) i −\n1\nλ\nλ ∑\nj=1\nx (g) j\n)(\nx (g) i −\n1\nλ\nλ ∑\nj=1\nx (g) j\n)⊤\n. (2)\nThe positive semidefinite matrix C(g) can be diagonalised in the basis formed by its eigenvectors b\n(g) 1 , . . . , b (g) d ,\nC(g) = B(g)D(g)B(g)⊤, (3)\nwhere B(g) = (b (g) 1 · · · b (g) d ) andD (g) is a diagonal matrix such that its k-th diagonal element is the eigenvalue corresponding to the eigenvector b (g) k . Consequently, the coordinates of points generated according to (1) in that basis, {x (g+1) i } B(g) , are uncorrelated:\n{x (g+1) i } B(g) = B(g)⊤x (g+1) i ∼ N ( B(g)⊤m(g), (σ(g))2B(g)⊤C(g)B(g) ) . (4)"
    }, {
      "heading" : "2 Optimization Based on Gaussian Processes",
      "text" : "A Gaussian process (GP) on a d-dimensional Euclidean space X is a collection of random variables, GPX= (f(x))x∈X, such that the joint distribution of any finite number of them is a multidimensional normal distribtion. Following [4], we differentiate two ways of using GPs in black-box optimization:\n(i) As a surrogate model to be optimized instead of the black box objective function. On the found optimum or optima, the original black-box objective function is then evaluated. This use of GPs has been introduced in the popular Efficient Global Optimization (EGO) algorithm [14]. As the optimization method, also evolutionary optimization can be used, in particular CMA-ES [4], but this is not the only possibility: For example, traditional low-degree polynomial models, aka response surface models [20], are much more efficiently optimized using traditional smooth optimization methods. (ii) For the evolution control of the evolutionary optimization of the original objective function, i.e., for controlling the composition of its population [5, 6, 9, 23].\nIrrespective of the way in which a GP is used, its construction is always based on a sequence (x1, y1), . . . , (xn, yn) ∈ X × R of training pairs and is subsequently employed to compute the random variable f(x) for x 6∈ {x1, . . . , xn}. Since the distribution of (f(x), f(x1), . . . , f(xn)) is multidimensional normal, also the conditional distribution of f(x) conditioned on f(x1), . . . , f(xn) is normal,\nf(x)|f(x1), . . . , f(xn) ∼ N(µ(x;X, Y ),Σ(x;X, Y )), (5)\nwhere X = (x1, . . . , xn), Y = (y1, . . . , yn), µ(·;X, Y ) : X → X,Σ(·;X, Y ) : X → X 2. There are two commonly encountered possibilities how define the functions µ(·;X, Y ), describing the conditional GP mean, and Σ(·;X, Y ), describing the conditional GP variance:\n1. A GP is the superposition of a deterministic function f̄ : X → R and a GP with zero mean. For the latter, it can be shown [21] that the function describing its mean fulfils\n(∀x ∈ X) µ0(x;X, Y ) = K(x,X)(K(X,X) + σ 2 noiseIn) −1Y ⊤, (6)\nwhereas the function describing its variance fulfills\n(∀x ∈ X) Σ0(x;X, Y ) = K(x, x)−K(x,X)(K(X,X) + σ 2 noiseIn) −1K(X, x). (7)\nHere, an i.i.d. Gaussian noise is assumed, In is the n-dimensional identity matrix, K : X× X → R is a symmetric function, and\nK(x,X) = (K(x, x1), . . . , K(x, xn)), K(X, x) = K(x,X) ⊤,\nK(X,X) = (K(x1, X) ⊤, . . . , K(xn, X) ⊤)⊤. (8)\nThe resulting superposition then fulfils\n(∀x ∈ X) µ(x;X, Y ) = f̄(x) +K(x,X)(K(X,X) + σ2noiseIn) −1(Y − (f̄(x1), . . . , f̄(xn))) ⊤,\n(9)\nwhereas Σ = Σ0.\n2. A GP is the superposition of a Bayesian mean assuming the multiplicative form wf̄ and a GP with zero mean, where w is a random variable with w ∼ N(1, σ2w), σw > 0,\nand f̄ has the same meaning as above. In that case, it can be shown [21] that for x ∈ X,\nµ(x;X, Y ) = K(x,X)(K(X,X) + σ2noiseIn) −1Y ⊤+\n+ (f̄(x)− (f̄(x1), . . . , f̄(xn))(K(X,X) + σ 2 noiseIn) −1K(X, x))ŵ, (10)\nΣ(x;X, Y ) = K(x, x)−K(x,X)(K(X,X) + σ2noiseIn) −1K(X, x)+\n+ σ2w(K(X,X) + σ 2 noiseIn) −1Y ⊤ + κ̂2\n1 + σ2w(f̄(x1), . . . , f̄(xn))(K(X,X) + σ 2 noiseIn) −1(f̄(x1), . . . , f̄(xn))⊤ , (11)\nwhere\nŵ = 1 + σ2w(f̄(x1), . . . , f̄(xn))(K(X,X) + σ 2 noiseIn) −1(f̄(x1), . . . , f̄(xn)) ⊤\n1 + σ2w(f̄(x1), . . . , f̄(xn))(K(X,X) + σ 2 noiseIn)\n−1Y ⊤ , (12)\nκ̂ = (f̄(x)− (f̄(x1), . . . , f̄(xn))(K(X,X) + σ 2 noiseIn) −1K(X, x)) (13)\nSimple, but frequently used examples of the function K occurring in (6)–(7) and (10)– (11) include:\n• Squared exponential,\n(∀x, x′ ∈ X) K(x, x′) = e− ‖x−x′‖2 2ℓ2 , ℓ > 0. (14)\nHere, ℓ is a parameter called characteristic length-scale. It is a parameter of the function KSE, not of the Gaussian process, the process is nonparametric. Therefore ℓ is referred to as a hyperparameter.\n• γ-Exponential,\n(∀x, x′ ∈ X) K(x, x′) = e −\n( ‖x−x′‖ ℓ\n)γ\n, ℓ > 0, 0 < γ ≤ 2, (15)\nwhich has 2 hyperparameters, λ and γ.\n• Dot product,\n(∀x, x′ ∈ X) K(x, x′) = (σ2 + x⊤x′)p, σ ≥ 0, p ∈ N, (16)\nwhich has 2 hyperparameters, σ and p, or in its generalized version,\n(∀x, x′ ∈ X) K(x, x′) = (σ2 + x⊤Σx′)p,Σ ∈ Rd,d positive definite, (17)\nwhich has, in addition, the matrix of hyperparameters Σ, defining a dot product in general coordinates."
    }, {
      "heading" : "2.1 GP-based criteria to choose the points for evaluation",
      "text" : "Whereas traditional response surface and surrogate models employ basically only one criterion for the choice of points in which the black box objective function should be evaluated, namely the global or at least local minimum of the model (if the optimization objective is minimization), GPs offer several additional criteria:\n(i) Minimum of a prescribed quantile (Qα) of the distribution of f(x)|f(x1), . . . , f(xn), α ∈ (0, 1).\nx∗α = argmin x∈X qα(N(µ(x;X, Y ),Σ(x;X, Y ))). (18)\nUsually, (18) is expressed using quantiles of the standard normal distribution, uα = qα(N(0, 1)),\nx∗α = argmin x∈X\nµ(x;X, Y ) + √\nΣ(x;X, Y )uα = argmin x∈X\nµ(x;X, Y )− √\nΣ(x;X, Y )u1−α.\n(19)\nFor α = 0.5, (19) turns to the traditional global minimum criterion, applied to µ(·;X, Y ).\n(ii) Probability of improvement (PoI),\nx∗PI = argmax x∈X P (f(x) < fmin|f(x1), . . . , f(xn)) = φ\n(\nfmin − µ(x;X, Y ) √\nΣ(x;X, Y )\n)\n, (20)\nwhere φ denotes the distribution function of N(0, 1), fmin is the minimum value found so far. More generally, probability of improvement with respect to a given T ≤ fmin,\nx∗PI—T = argmax x∈X P (f(x) < T |f(x1), . . . , f(xn)) = φ\n(\nT − µ(x;X, Y ) √\nΣ(x;X, Y )\n)\n. (21)\n(iii) Expected improvement (EI),\nx∗EI = argmax x∈X E((fmin − f(x))I(f(x) < fmin)|f(x1), . . . , f(xn)),\nwhere I(f(x) < fmin) =\n{\n1 f(x) < fmin, 0 f(x) ≥ fmin. (22)\nIntroducing the normalized mean improvement ν : X → R,\n(∀x ∈ X) ν(x) = fmin − µ(x;X, Y ) √\nΣ(x;X, Y ) , (23)\nand the notation ϕ for the density of N(0, 1), (22) can be expressed as [13]\nx∗EI = argmax x∈X\n√\nΣ(x;X, Y )(ν(x)φ(ν(x)) + ϕ(ν(x)))). (24)"
    }, {
      "heading" : "3 Possible Synergy",
      "text" : "Directly connecting both considered Gaussian approaches is not possible because the normal distribution in CMA-ES is a distribution on the input space of the objective function (fitness), whereas the normal distribution in GPs is on the space of its function values. Nevertheless, it is still possible to achieve some synergy through using information from CMA-ES for the GP, and/or using information from the GP for CMA-ES. According to whoat was recalled at the beginning of Section 2, the latter possibility corresponds to using GP for the evolution control of CMA-ES."
    }, {
      "heading" : "3.1 Using Information from CMA-ES for the GP",
      "text" : "We see 2 straightforward possibilities where some information from CMA-ES can be used in the GP.\n1. The function f̄ occurring in (9), (10) and (11) can be constructed using the fit-\nness values f(x (g) 1 ), . . . . . . , f(x (g) λ ) of individuals from some particular generation or several generations of CMA-ES. Its construction can be as simple as setting f̄ to a constant aggregating the considered fitness values, e.g., their mean or weighted mean, but it can also consist in training, with those values, a response surface model, principally of any kind.\n2. Kruisselbrink et al. [16], who combine CMA-ES with a GP using the γ-exponential function K, propose to employ in (15) the Mahalanobis distance of vectors x and x′ given by the covariance matrix obtained in the g-th generation of CMA-ES, (σ(g))2C(g), instead of their Euclidean distance. This is actually a specific consequence of another possibility of using information from CMA-ES for the GP – replacing the original space of d-dimensional vectors, Rd, by the space of their principal components with respect to C(g). In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15]. In the approach presented there, the space of the principal components with respect to C(g) is used, together with an estimate of density, to locally weight the model predictions with respect to the considered input. In this way, a connection to the other kind of synergy is established, i.e., to the evolution control of CMA-ES, giving us the possibility to use both kinds in combination."
    }, {
      "heading" : "3.2 GP-Based Evolution Control of CMA-ES",
      "text" : "We intend to test the following approaches to using GP for the evolution control of CMAES.\n(i) Basic approach. In the (g+1)-th generation, λ′ points x̃1, . . . , x̃λ′ ∈ R d are sampled\nfrom the distribution N(m(g), (σ(g))2C(g)), where λ′ is several to many times larger then λ. For all of them, a selected criterion from among those introduced in 2.1\nis computed. Based on the value of that criterion, the λ points x (g+1) 1 , . . . , x (g+1) λ for the evaluation by the original black-box fitness are chosen. This can be done according to various strategies, we intend to use the one described in Algorithm 1, with which we have a good experience from using radial basis function networks for the evolution control in the evolutionary optimization of catalytic materials [10]. The linear ordering ≺c stands in the cases c =PoI and c =EI for ≤, in the case c = Qα for ≥. The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22]. Most similar is the approach by Ulmer et al. [23], the difference being that they don’t use clustering.\n(ii) GP on low-dimensional projections attempts to improve the basic approach in view of the experience reported in the literature [15] and obtained also in our earlier experiments [1] that GPs are actually advantageous only in low dimensional spaces. Instead of the sampled points x̃1, . . . , x̃λ′ , the GP is trained only with their first ℓ < d principal components with respect to C(g), x̃\n[ℓ] 1 , . . . , x̃ [ℓ] λ′ , i.e., with the projec-\ntions of x̃1, . . . , x̃λ′ to the ℓ-dimensional space Xℓ = span(b (g) 1 , . . . , b (g) ℓ ), provided the orthonormal eigenvectors b (g) i are enumerated according to decreasing eigenvalues.\n(iii) GP on low-dimensional projections within restricted distance attempts to decreaase the deterioration of the GP on low-dimensional projections with respect to the GP on the original sampled points x̃1, . . . , x̃λ′ by using only points x̃i within a prescribed small distance ǫ from their respective projections x̃\n[ℓ] i . To this end, points from\nthe distribution N(m(g), (σ(g))2C(g)) are resampled until λ′ points x̃i are obtained fulfilling\n‖x̃i − x̃ [ℓ] 1 ‖ < ǫ, or equivalently, ‖x̃i‖ 2 − ‖x̃ [ℓ] 1 ‖ 2 < ǫ2. (25)\n(iv) Two-stage sampling. Trainig the GP for the (g + 1)-th generation can easily suffer from the lack of training data in a part of the search space where a substantial proportion of the points x̃1, . . . , x̃λ′ will be sampled. To alleviate it, the points x (g+1) 1 , . . . , x (g+1) λ to be evaluated by the original fitness can be sampled in two stages:\n1. First, points x (g+1) 1 , . . . , x (g+1) λ′′ , where λ\n′′ < λ, are sampled from the distribution N(m(g), (σ(g))2C(g)), evaluated by the original fitness, and included into training the GP.\n2. Then, λ′ points x̃1, . . . , x̃λ′ are sampled from the same distribution, and for them, a selected criterion from among those introduced in 2.1 is computed, based on which the points x\n(g+1) λ′′+1 , . . . , x (g+1) λ for the evaluation by the original\nblack-box fitness are chosen. To this end, again Algorithm 1 can be used, with the following changes:\n• In the input, the numbers λ′ and k have now to fulfill\nλ− λ′′ < card{x̃1, . . . , x̃λ′}, k ∈ {1, . . . , λ− λ ′′}. (26)\n• In Step 3, x (g+1) j = max≺c S \\ {x (g+1) i : i = 1, j − 1} is chosen only for\nj = k + 1, . . . , λ− λ′′.\n• The output contains only the points x (g+1) λ′′+1 , . . . , x (g+1) λ .\nNeedless to say, this approach has to be combined with some of the approaches (i)–(iii) and can be combined with any of them.\n(v) Generation-based evolution control. Whereas the previous four approaches represent, in terms of [11, 12], individual-based evolution control, we want to test also one approach that is generation-based, in the sense that the desired number λ of points is evaluated by the original black-box fitness function only in selected generations. Similarly to the evolution strategy in [18, 19], our approach selects those generations adaptively, according to the agreement between the ranking of considered points by the surrogate model and by the black-box fitness. Differently to that strategy, however, we want to base the estimation if that agreement not on the generations in which λ points have been evaluated by the black-box fitness, but on evaluating a small and evolvable number λ′′′ of additional points in each generation. In this context, it is important that in situations when the fitness is evaluated empirically, using some measurement or testing, the evaluation hardware causes the evaluation costs to increase step-wise, and to remain subsequently constant for some λhw evaluated points (e.g. in the optimization of catalyst preformance described in [10], λhw is the number of channels in the chemical reactor in which the catalysts are tested). In such a situation, the costs of evaluation are the lowest if λ is a multiple of λhw, and if the evaluation of the λ\n′′′ additional points in each generation is cumulated for nhw generations such that\nnhw = max n∈N\nnλ′′′ ≤ λhw. (27)\nSetting nhw = 1 covers the case when such a situation does not occur and the evaluation costs increase linearly with the number of points. If glast is the last generation in which λ points have been evaluated by the black-box fitness, then for g = glast, . . . , glast +nhw − 1, points x̃ g+1 1 , . . . , x̃ g+1 λ′ ∈ R d are sampled from the distribution N(m(g), (σ(g))2C(g)), from which the points x (g+1) 1 , . . . , x (g+1) λ′′′ are selected by Algorithm 1, in which the input λ is replaced with λ′′′. Subsequently, all the points x (glast+1) 1 , . . . , x (glast+nhw) λ′′′ are evaluated by the black-box fitness, and for each generation g = glast + 1, . . . , glast + nhw, the agreement between the ranking of x g 1, . . . , x g λ′′′ by the current Gaussian process, GPcurrent, and the black-box fitness is estimated. If the agreement is sufficient in all considered generations, then the procedure is repeated using glast + nhw instead of glast and unchanged λ ′′′. Otherwise, additional\npoints xgλ′′′+1, . . . , x g λ′′′+λ are selected from x̃ g 1, . . . , x̃ g\nλ′ for the first generation g in which the agreement was not sufficient. Then a new Gaussian process, GPnew, is trained using the training set\nTnew =\ng ⋃\ng′=1\n{xg ′ 1 , . . . , x g′ λ′′′} ∪ {x g λ′′′+1, . . . , x g λ′′′+λ}, (28)\noptionally including also some or all points from the set Tcurrent used for training GPcurrent. At that occasion, aslo the value of λ\n′′′ can be changed. Provided the conditions 1 ≤ λ′′′ < λ and λ′′′ + λ ≤ λ′ are fulfilled, the value of λ′′′ can be arbitrary. Needless to say, the smaller λ′′′, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19]. It is also worth pointing out that our generation-based evolution control was explained here as a counterpart to the basic individual-based approach (i), but counterparts to the approaches (ii) and (iii) are possible as well."
    } ],
    "references" : [ {
      "title" : "Model guided sampling optimization with Gaussian processes for expensive black-box optimization",
      "author" : [ "L. Bajer", "V. Charypar", "M. Holeňa" ],
      "venue" : "GECCO 2013, pages 1715–1716",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Investigating the local-meta-model cmaes for large population sizes",
      "author" : [ "Z. Bouzarkouna", "A. Auger", "D.Y. Ding" ],
      "venue" : "EvoApplications 2010, pages 402–411",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Local-meta-model cma-es for partially separable functions",
      "author" : [ "Z. Bouzarkouna", "A. Auger", "D.Y. Ding" ],
      "venue" : "GECCO 2011, pages 869–876",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Accelerating evolutionary algorithms with Gaussian process fitness function models",
      "author" : [ "D. Büche", "N.N. Schraudolph", "P. Koumoutsakos" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 35:183–194",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Evolutionary optimization for computationally expensive problems using Gaussian processes",
      "author" : [ "M.A. El-Beltagy", "A.J. Keane" ],
      "venue" : "International Conference on Artificial Intelligence, pages 708–714",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Metamodelassisted evolution strategies",
      "author" : [ "M. Emmerich", "A. Giotis", "M. Özdemir", "T. Bäck", "K. Giannakoglou" ],
      "venue" : "PPSN VII, pages 361–370",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The CMA evolution strategy: A comparing review",
      "author" : [ "N. Hansen" ],
      "venue" : "Towards a New Evolutionary Computation, pages 75–102. Springer Verlag, Berlin",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Completely derandomized self-adaptation in evolution strategies",
      "author" : [ "N. Hansen", "A. Ostermaier" ],
      "venue" : "Evolutionary Computation, 9:159–195",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Controlled model assisted evolution strategy with adaptive preselection",
      "author" : [ "E. Hoffman", "S. Holemann" ],
      "venue" : "International IEEE Symposium on Evolving Fuzzy Systems, pages 182–187",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Surrogate modeling in the evolutionary optimization of catalytic materials",
      "author" : [ "M. Holeňa", "D. Linke", "L. Bajer" ],
      "venue" : "GECCO 2012, pages 1095–1102",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A comprehensive survery of fitness approximation in evolutionary computation",
      "author" : [ "Y. Jin" ],
      "venue" : "Soft Computing, 9:3–12",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Managing approximate models in evolutionary aerodynamic design optimization",
      "author" : [ "Y. Jin", "M. Olhofer", "B. Sendhoff" ],
      "venue" : "CEC 2001, pages 592–599",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A taxonomy of global optimization methods based on response surfaces",
      "author" : [ "D.R. Jones" ],
      "venue" : "Journal of Global Optimization, 21:345–383",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Efficient global optimization of expensive black-box functions",
      "author" : [ "D.R. Jones", "M. Schonlau", "W.J. Welch" ],
      "venue" : "Journal of Global Optimization, 13:455–492",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Local metamodels for optimization using evolution strategies",
      "author" : [ "S. Kern", "N. Hansen", "P. Koumoutsakos" ],
      "venue" : "PPSN IX, pages 939–948",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A robust optimization approach using kriging metamodels for robustness approximation in the CMA-ES",
      "author" : [ "J.W. Kruisselbrink", "M.T.M. Emmerich", "A.H. Deutz", "T. Bäck" ],
      "venue" : "CEC 2010, pages 1–8",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Comparison-based optimizers need comparison-based surrogates",
      "author" : [ "I. Loshchilov", "M. Schoenauer", "M. Seba" ],
      "venue" : "PPSN XI, pages 364–373",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Self-adaptive surrogate-assisted covariance matrix adaptation evolution strategy",
      "author" : [ "I. Loshchilov", "M. Schoenauer", "M. Seba" ],
      "venue" : "GECCO 2012, pages 321–328",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Intensive surrogate model exploitation in self-adaptive surrogate-assisted CMA-ES (saACM-ES)",
      "author" : [ "I. Loshchilov", "M. Schoenauer", "M. Seba" ],
      "venue" : "GECCO 2013, pages 439–446",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Response Surface Methodology: Proces and Product Optimization Using Designed Experiments",
      "author" : [ "R.H. Myers", "D.C. Montgomery", "C.M. Anderson-Cook" ],
      "venue" : "John Wiley and Sons, Hoboken",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Gaussian Process for Machine Learning",
      "author" : [ "E. Rasmussen", "C. Williams" ],
      "venue" : "MIT Press, Cambridge",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Ordinal regression in evolutionary computation",
      "author" : [ "T.P. Runarsson" ],
      "venue" : "PPSN IX, pages 1048–1057",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Evolution strategies assisted by Gaussian processes with improved pre-selection criterion",
      "author" : [ "H. Ulmer", "F. Streichert", "A. Zell" ],
      "venue" : "IEEE Congress on Evolutionary Computation, pages 692–699",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "CMA-ES [7, 8] is the state-of-the-art evolutionary optimization method, at least in the area of continuous black-box optimization.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 7,
      "context" : "CMA-ES [7, 8] is the state-of-the-art evolutionary optimization method, at least in the area of continuous black-box optimization.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "Following [4], we differentiate two ways of using GPs in black-box optimization:",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "This use of GPs has been introduced in the popular Efficient Global Optimization (EGO) algorithm [14].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "As the optimization method, also evolutionary optimization can be used, in particular CMA-ES [4], but this is not the only possibility: For example, traditional low-degree polynomial models, aka response surface models [20], are much more efficiently optimized using traditional smooth optimization methods.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "As the optimization method, also evolutionary optimization can be used, in particular CMA-ES [4], but this is not the only possibility: For example, traditional low-degree polynomial models, aka response surface models [20], are much more efficiently optimized using traditional smooth optimization methods.",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 4,
      "context" : ", for controlling the composition of its population [5, 6, 9, 23].",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : ", for controlling the composition of its population [5, 6, 9, 23].",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : ", for controlling the composition of its population [5, 6, 9, 23].",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : ", for controlling the composition of its population [5, 6, 9, 23].",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "For the latter, it can be shown [21] that the function describing its mean fulfils (∀x ∈ X) μ0(x;X, Y ) = K(x,X)(K(X,X) + σ 2 noiseIn) Y , (6)",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "In that case, it can be shown [21] that for x ∈ X,",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "and the notation φ for the density of N(0, 1), (22) can be expressed as [13] x∗EI = argmax x∈X √",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "[16], who combine CMA-ES with a GP using the γ-exponential function K, propose to employ in (15) the Mahalanobis distance of vectors x and x given by the covariance matrix obtained in the g-th generation of CMA-ES, (σ)C, instead of their Euclidean distance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].",
      "startOffset" : 174,
      "endOffset" : 184
    }, {
      "referenceID" : 2,
      "context" : "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].",
      "startOffset" : 174,
      "endOffset" : 184
    }, {
      "referenceID" : 14,
      "context" : "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].",
      "startOffset" : 174,
      "endOffset" : 184
    }, {
      "referenceID" : 9,
      "context" : "This can be done according to various strategies, we intend to use the one described in Algorithm 1, with which we have a good experience from using radial basis function networks for the evolution control in the evolutionary optimization of catalytic materials [10].",
      "startOffset" : 262,
      "endOffset" : 266
    }, {
      "referenceID" : 1,
      "context" : "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].",
      "startOffset" : 186,
      "endOffset" : 202
    }, {
      "referenceID" : 17,
      "context" : "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].",
      "startOffset" : 186,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].",
      "startOffset" : 186,
      "endOffset" : 202
    }, {
      "referenceID" : 21,
      "context" : "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].",
      "startOffset" : 186,
      "endOffset" : 202
    }, {
      "referenceID" : 22,
      "context" : "[23], the difference being that they don’t use clustering.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "(ii) GP on low-dimensional projections attempts to improve the basic approach in view of the experience reported in the literature [15] and obtained also in our earlier experiments [1] that GPs are actually advantageous only in low dimensional spaces.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "(ii) GP on low-dimensional projections attempts to improve the basic approach in view of the experience reported in the literature [15] and obtained also in our earlier experiments [1] that GPs are actually advantageous only in low dimensional spaces.",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : "Whereas the previous four approaches represent, in terms of [11, 12], individual-based evolution control, we want to test also one approach that is generation-based, in the sense that the desired number λ of points is evaluated by the original black-box fitness function only in selected generations.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "Whereas the previous four approaches represent, in terms of [11, 12], individual-based evolution control, we want to test also one approach that is generation-based, in the sense that the desired number λ of points is evaluated by the original black-box fitness function only in selected generations.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Similarly to the evolution strategy in [18, 19], our approach selects those generations adaptively, according to the agreement between the ranking of considered points by the surrogate model and by the black-box fitness.",
      "startOffset" : 39,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "Similarly to the evolution strategy in [18, 19], our approach selects those generations adaptively, according to the agreement between the ranking of considered points by the surrogate model and by the black-box fitness.",
      "startOffset" : 39,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "in the optimization of catalyst preformance described in [10], λhw is the number of channels in the chemical reactor in which the catalysts are tested).",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "Needless to say, the smaller λ, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19].",
      "startOffset" : 344,
      "endOffset" : 360
    }, {
      "referenceID" : 11,
      "context" : "Needless to say, the smaller λ, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19].",
      "startOffset" : 344,
      "endOffset" : 360
    }, {
      "referenceID" : 17,
      "context" : "Needless to say, the smaller λ, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19].",
      "startOffset" : 344,
      "endOffset" : 360
    }, {
      "referenceID" : 18,
      "context" : "Needless to say, the smaller λ, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19].",
      "startOffset" : 344,
      "endOffset" : 360
    } ],
    "year" : 2014,
    "abstractText" : "CMA-ES [7, 8] is the state-of-the-art evolutionary optimization method, at least in the area of continuous black-box optimization. Basically, it consists in generating new search points by sampling from a multidimensional normal distribtion, the mean and variance of which are updated from generation to generation. In particular, the population x (g+1) 1 , . . . , x (g+1) λ ∈ R d of the g + 1-st generation, g ≥ 1, follows the normal distribution with mean m ∈ R and variance (σ)C ∈ R resulting from the update in the g-th generation,",
    "creator" : "LaTeX with hyperref package"
  }
}