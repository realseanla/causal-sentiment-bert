{
  "name" : "1301.2293.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Aggregating Learned Probabilistic Beliefs",
    "authors" : [ "Pedrito Maynard-Reid" ],
    "emails" : [ "urszula@cs.stanford.edu" ],
    "sections" : null,
    "references" : [ {
      "title" : "Theory refinement on bayesian net足",
      "author" : [ "W. Buntine" ],
      "venue" : null,
      "citeRegEx" : "Buntine.,? \\Q1991\\E",
      "shortCiteRegEx" : "Buntine.",
      "year" : 1991
    }, {
      "title" : "A normative examination of ensemble learning algorithms",
      "author" : [ "D.M. Pennock", "P.E. Horvitz" ],
      "venue" : "In Proc. ICML'OO,",
      "citeRegEx" : "Pennock and Horvitz.,? \\Q2000\\E",
      "shortCiteRegEx" : "Pennock and Horvitz.",
      "year" : 2000
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "We consider the task of aggregating beliefs of sev足 eral experts. We assume that these beliefs are rep足 resented as probability distributions. We argue that the evaluation of any aggregation technique depends on the semantic context of this task. We propose a framework, in which we assume that nature generates samples from a 'true' distribution and different experts form their beliefs based on the subsets of the data they have a chance to observe. Naturally, the optimal ag足 gregate distribution would be the one learned from the combined sample sets. Such a formulation leads to a natural way to measure the accuracy of the aggregation mechanism. We show that the well-known aggregation operator LinOP is ideally suited for that task. We propose a LinOP-based learning algorithm, inspired by the techniques developed for Bayesian learning, which aggregates the experts' distributions represented as Bayesian networks. We show experimentally that this algorithm performs well in practice.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}