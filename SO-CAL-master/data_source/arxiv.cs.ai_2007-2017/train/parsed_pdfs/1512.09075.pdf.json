{
  "name" : "1512.09075.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Notation for Markov Decision Processes",
    "authors" : [ "Philip S. Thomas", "Billy Okal" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "A Notation for Markov Decision Processes\nPhilip S. Thomas1 and Billy Okal2\n1Carnegie Mellon University, 2Albert-Ludwigs-Universität Freiburg"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many reinforcement learning (RL) research papers contain paragraphs that define Markov decision processes (MDPs). These paragraphs take up space that could otherwise be used to present more useful content. In this paper we specify a notation for MDPs that can be used by other papers. Declaring the use this notation using a single sentence can replace several paragraphs of notational specifications in other papers. Importantly, the notation that we define is a common foundation that appears in many RL papers, and is not meant to be a complete notation for an entire paper.\nWe refer to our notation as the Markov Decision Process Notation, version 1 or MDPNv1. It can be invoked in research papers with the sentence:\n“We use the notational standard MDPNv1.”\nThis sentence denotes that the notation specified in this document should be inserted at the current location. One challenge with this system is that any reasonably complete notation will define a large subset of the commonly used mathematical symbols, some of which an author may wish to use with a meaning other than that specified in MDPNv1. To overcome this problem, definitions that occur after the sentence invoking MDPNv1 can modify or overwrite the definitions in MDPNv1.\nFor example, an author may write “We assume that the state and action sets are finite,” which overrules MDPNv1’s more general definition of the state and action sets, or “Let A denote the set of all possible advantage functions,” which overwrites the definition of A in MDPNv1 (where it is the set of possible actions). In general, MDPNv1 should serve as a notational foundation, which the author is free to build upon or remove from to best suit the needs of the paper.\nThis paper is not an introduction to RL. It assumes that the reader is already familiar with the basic concepts of RL, as covered by Sutton and Barto (1998). Also, we try to minimize the number of assumptions that we make. This means that authors using our notation will have to specify their own assumptions, rather than specify which of our assumptions must be removed.\nBilly Okal has provided a style file for MDPNv1 at https://github.com/makokal/MDPN. Not only does this style file allow you to easily switch between the different notational variants defined below, but using it allows you to change the notation used in your paper by modifying the style file rather than by editing every equation individually."
    }, {
      "heading" : "2 Discrete and Continuous Random Variables",
      "text" : "In general, the state, action, and reward at time t can be discrete or continuous random variables, or even a mixture of both. A discrete random variable, X, that takes values in a set, X , has a probability mass function (PMF), f : X → [0, 1], such that f(x) = Pr(X = x) for all x ∈ X . However, continuous random variables (and random variables that are a mixture of discrete and continuous) are not characterized by a PMF. Although measure theoretic probability offers a unified notation for discussing arbitrary random variables, its use is not commonplace in reinforcement learning literature, and so it may dilute the message of a paper and shrink a paper’s audience.\nWe therefore introduce an abuse of notation into MDPNv1: notationally, we treat the state, action, and reward as though they are discrete random variables, even though they may not be. That is, our expressions are written using PMFs for distributions over states, actions, and rewards, even if they should technically be written using probability measures. The author of a paper using MDPNv1 should ensure that all claims carry over to states, actions, and rewards that are arbitrary random variables, or should explicitly restrict the states, actions, and rewards to be discrete random variables or continuous random variables that have density functions.\nar X\niv :1\n51 2.\n09 07\n5v 2\n[ cs\n.A I]\n8 S\nep 2\n01 6\nLet a Markov decision process (MDP) be a tuple, (S,A,R, P,R, d0, γ), where\n1. We use t ∈ N≥0 to denote the time step, where N≥0 denotes the natural numbers including zero.\n2. S is the set of possible states that the agent can be in, and is called the state set. The state of the environment at time t is a random variable that we denote by St. We will typically use s to denote an element of the state set.\n3. A is the set of possible actions that the agent can select between, and is called the action set. The action chosen by the agent at time t is a random variable that we denote by At. We will typically use a to denote a specific element of the action set.\n4. R ⊆ R ∪ {−∞,∞} is the set of possible rewards that the agent can receive, and is called the reward set. The reward provided to the agent at time t is a random variable that we denote by Rt. We will typically use r to denote an element of the reward set. Let rmin and rmax be the infimum and supremum of R, respectively.\n5. P : S ×A×S → [0, 1] is called the transition function. For all (s, a, s′, t) ∈ S×A×S×N≥0, let P (s, a, s′) := Pr(St+1 = s′|St = s,At = a).1 That is, P characterizes the distribution over states at time t + 1 given the state and action at time t. We introduce a Markov assumption: the distribution over St+1 is independent of all prior events given St and At. That is, the distribution over states at time t + 1 is fully determined by the state and action at time t, and this distribution is characterized by P .\nWe allow three alternate notations for P . First, let P (s′|s, a) := P (s, a, s′). This form takes approximately the same amount of space, but makes it more clear that P is a conditional distribution over the next state given the current state and action. Second, let P as (s\n′) := P (s, a, s′). This notation moves terms into subscripts and superscripts in order to save some space. Third, let P as,s′ := P (s, a, s\n′). This final form is particularly useful when space is limited. Although the author is allowed to select between the four notations for P , the use of P should be consistent within each paper.\n6. R is called the reward function. For all (s, a, s′, t, r) ∈ S ×A×S ×N≥0 ×R, let R(s, a, s′, r) := Pr(Rt = r|St = s,At = a, St+1 = s\n′). That is, R characterizes the distribution over rewards at time t given St, At, and St+1. We introduce another Markov assumption: the distribution of Rt is independent of all prior events given St, At, and St+1. Also notice that the reward function, R, has no subscripts or superscripts, unlike the visually similar reward at time t, Rt. As with P , we allow for several alternate notations for R that the author is free to select from. Let R(r|s, a, s′) := Ras,s′(r) := R a,r s,s′ := R(s, a, s ′, r).\n7. We call d0 : S → [0, 1] the initial state distribution, since d0(s) := Pr(S0 = s) for all s ∈ S.\n8. Let γ ∈ [0, 1] be the reward discount parameter, which may be used to discount rewards based on how far in the future they occur.\nLet π : S×A → [0, 1] be called a policy. A policy specifies the distribution over At given St, i.e., π(s, a) := Pr(At = a|St = s) for all (s, a, t) ∈ S × A × N≥0. All policies are assumed to be Markovian—the distribution of At is independent of prior events given St. Let Π be the set of all possible policies. If there exists a state, s ∈ S, and two unique actions, (a1, a2) ∈ A2, where a1 6= a2, and both a1 and a2 have non-zero probability in s, i.e., π(s, a1) > 0 and π(s, a2) > 0, then we refer to π as a stochastic policy, and we refer to it as a deterministic policy otherwise. Let µ : S → A be an alternate definition of a deterministic policy. We allow several additional shorthands: π(a|s) := πs(a) := πas := π(s, a) and µs := µ(s).\nWe abuse notation and give π a second definition. It should be clear from context which definition is intended. Let π : S ×A×Rnθ → [0, 1], where nθ ∈ N≥0. Let θ ∈ Rnθ denote a nθ-dimensional vector called the policy parameters, and let π(s, a,θ) := Pr(At = a|St = s,θ) for all (s, a,θ, t) ∈ S ×A× Rnθ × N≥0. We call this definition of π a parameterized policy. We allow several shorthands: π(a|s,θ) := πθ(s, a) := πθ(a|s) := πsθ(a) := π s,a θ := π(s, a,θ). Similarly, µ : S × Rnθ → A is a parameterized deterministic policy, and µθ(s) := µ s θ := µ(s,θ).\nAn episode is one sequence of states, actions, and rewards, starting from t = 0 and continuing indefinitely. An MDP may have a state, ∞ s ∈ S, called the terminal absorbing state. In the state ∞s only one action can be taken. Taking this action causes a transition back to ∞ s and results in a reward of zero. Once the agent reaches ∞ s the system has effectively terminated since there are no more decisions to be made or rewards to collect. If a state, s ∈ S always causes a transition to ∞s with a reward of zero, then we call s a terminal state. Let L ∈ N≥0 ∪ {∞} be the horizon of the MDP, i.e., the smallest time step such that for all π ∈ Π,Pr(SL = ∞ s |π) = 1, and ∞ if no such time step exists.\n1Notice that we use := to denote “is defined to be”.\nIn this section we discuss some of the decisions that we made regarding notation. In general, we use calligraphic capital letters for sets, e.g., X . Elements of sets are lowercase letters that are typically similar to the set they belong to, e.g., x ∈ X . Random variables are denoted by capital letters, e.g., X, and their instantiations by lowercase letters, e.g., x. Vectors are bold lowercase letters, like x.\nAlthough we would have liked to use lowercase letters for real-valued functions, we use P and R to denote real-valued functions. This is for two different reasons. First, we use P rather than p because p is a commonly used symbol that we would like to avoid defining (notice that we have not defined x, y, i, j, f, g, p, or q, all of which are commonly used symbols). Second, we use R because r is already used to denote an element of R, and to preserve alliteration we do not want to use a different letter. Although R is visually similar to Rt, it is typically clear from context which is intended, even if the reader does not notice the subscript or lack thereof.\nSometimes the set of actions that can be selected by the agent changes depending on the state of the environment. We do not include this in our notation because it is rarely used in the literature. If the author wishes to include this additional structure in an MDP, then we recommend using A (s) to denote the set of actions that can be chosen in the state s. However, this is not part of MDPNv1, and must be specified by the author.\nOften MDPs are defined without explicitly defining the set of possible rewards, R. We include R so that the author can write ∑ r∈R f(r) for some f : R → R. This is useful because the two obvious choices for implicit definitions of R both have\nproblems: ∑ r∈R, while technically valid, may be confusing since the reals are typically integrated over, and ∑\nr∈Z does not allow for rewards that are not integers.\nAlthough there are many other terms that we could include in MDPNv1, we have decided to only define the terms necessary to define an MDP. This both makes it easier for the reader to remember which terms are defined by MDPNv1 and avoids including controversial definitions. Furthermore, it avoids limiting the setting to only the discounted or average-reward setting (we could define symbols for both settings, but this would be unnecessarily complex).\n5 LATEX Style File Usage\nIn this section we demonstrate how to use the style file accompanying this text.\n1. The package can be included using any of three options: alpha, beta, kappa.\n1 % ... 2 \\usepackage[alpha]{mdpn} % Most verbose 3 %\\usepackage[beta]{mdpn} % Compressed 4 %\\usepackage[kappa ]{mdpn} % Most compressed 5 % ...\n2. You can use any of the defined commands in text as:\n1 % ... 2 Some text $\\ command$, for example $\\sset$ for state set 3 % ...\nSome of the commands require a specific number of arguments that should be provided in the order indicated. For example \\T requires three arguments: the current state s, current action a and next state s′. So, \\T{s}{a}{s’} will produce P (s′ | s, a).\n3. Most of the commands allow usual modifications such as subscripts and superscripts. For example, \\pp (which denotes a parametrised policy) can be modified to \\pp {sub} to yield π(a | s,θ)sub."
    } ],
    "references" : [ {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It assumes that the reader is already familiar with the basic concepts of RL, as covered by Sutton and Barto (1998). Also, we try to minimize the number of assumptions that we make.",
      "startOffset" : 92,
      "endOffset" : 116
    } ],
    "year" : 2016,
    "abstractText" : "Many reinforcement learning (RL) research papers contain paragraphs that define Markov decision processes (MDPs). These paragraphs take up space that could otherwise be used to present more useful content. In this paper we specify a notation for MDPs that can be used by other papers. Declaring the use this notation using a single sentence can replace several paragraphs of notational specifications in other papers. Importantly, the notation that we define is a common foundation that appears in many RL papers, and is not meant to be a complete notation for an entire paper. We refer to our notation as the Markov Decision Process Notation, version 1 or MDPNv1. It can be invoked in research papers with the sentence:",
    "creator" : "LaTeX with hyperref package"
  }
}