{
  "name" : "1312.6214.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Volumetric Spanners and their Applications to Machine Learning",
    "authors" : [ "Elad Hazan", "Zohar Karnin", "Raghu Meka" ],
    "emails" : [ "ehazan@ie.technion.ac.il", "zkarnin@ymail.com", "meka@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases.\nWe show how efficient volumetric spanners give rise to the first efficient and optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set."
    }, {
      "heading" : "1 Introduction",
      "text" : "A fundamental difficulty in machine learning is environment exploration. A prominent example is the famed multi-armed bandit (MAB) problem, in which a decision maker iteratively chooses an action from a set of available actions and receives a payoff, without observing the payoff of all other actions she could have taken. The MAB problem displays an exploration-exploitation tradeoff, in which the decision maker trades exploring the action space vs. exploiting the knowledge already obtained to pick the best arm.\nAnother example in which environment exploration is crucial, or perhaps the main point, is active learning and experiment design. In these fields it is important to correctly identify the most informative queries so as to efficiently construct a solution.\nExploration is hardly summarised by picking an action uniformly at random. Indeed, sophisticated techniques from various areas of optimization, statistics and convex geometry have been applied to designing ever better exploration algorithms. To mention a few: Awerbuch and Kleinberg [3] devise the notion of barycentric spanners, and use this construction to give the first low-regret algorithms for complex decision problems such as online routing. Abernethy, Hazan and Rakhlin [1] use self-concordant barriers to build an efficient exploration strategy for convex sets in Euclidean space. Bubeck, Cesa-Bianchi and Kakade [7] apply tools from convex geometry, namely John’s ellipsoid to construct optimal regret algorithms for bandit linear optimization (albeit not always efficiently).\nIn this paper we consider a generic approach to exploration, and quantify what efficient exploration with low variance requires in general. We define a novel construction called volumetric spanners and give efficient algorithms to construct them. We further investigate the convex geometry implications of our construction, and define the notion of volumetric ellipsoid of a convex body. We give structural theorems on the existence and properties of these ellipsoids, as well as constructive algorithms to compute them in several cases.\nWe complement our findings with an application to machine learning, in which we resolve a well-studied open problem that has exploration as its core difficulty: an efficient and optimalregret algorithm for bandit linear optimization. As evidenced by this application, we expect that volumetric spanners and volumetric ellipsoids can be useful elsewhere in experiment design and active learning.\nBandit Linear Optimization Bandit linear optimization is a fundamental problem in decision making under uncertainty that efficiently captures structured action sets. The canonical example\nar X\niv :1\n31 2.\n62 14\nv1 [\ncs .L\nG ]\n2 1\nD ec\n2 01\n3\nis that of online routing in graphs: a decision maker iteratively chooses a path in a given graph from source to destination, the adversary chooses lengths of the edges of the graph, and the decision maker receives as feedback the length of the path she chose but no other information (see [3]). Her goal over many iterations is to attain an average travel time as short as that of the best fixed shortest path in the graph.\nThis decision problem is readily modeled in the “experts” framework, albeit with efficiency issues: the number of possible paths is potentially exponential in the graph representation. The BLO framework gives an efficient model for capturing such structured decision problems: iteratively a decision maker chooses a point in a convex set and receives as a payoff an adversarially chosen linear cost function. In the particular case of online routing, the decision set is taken to be the s-t-flow polytope, which captures the convex hull of all source-destination shortest paths in a given graph, and has a succinct representation with polynomially many constraints and low dimensionality. The linear cost function corresponds to a weight function on the graphs edges, where the length of a path is defined as the sum of weights of its edges.\nThe BLO framework captures many other structured problems efficiently, i.e. learning permutations, rankings and other examples (see [1]). As such, it has been the focus of much research in the past few years. The reader is referred to the recent survey of Bubeck and Cesa-Bianchi [6] for more details on algorithmic results for BLO.\nIn this paper we contribute to the large literature on the BLO model by giving the first efficient and optimal regret algorithm for BLO over general decision sets. Previously efficient algorithms (with non-optimal regret) were known over convex sets that admit an efficient selfconcordant barrier [1], and optimal regret algorithms were known over general sets [7] but using computationally in-efficient machinery. We next describe how to use novel convex geometric concepts to attain the best of both worlds."
    }, {
      "heading" : "1.1 Introduction to Exploration: Barycentric Spanners and John’s Ellipsoid",
      "text" : "In this section we review two geometric constructions that have been used in previous work in machine learning for designing an exploration basis for a body K ∈ Rd. The first is the ellipsoid corresponding to the barycentric spanner of K which is defined as the ellipsoid of maximum volume, supported by exactly d points from K. The second is the minimum volume enclosing ellipsoid (MVEE) also known as John’s ellipsoid.\nAs we show later on, our definition of a volumetric spanner enjoys properties of both objects. Similar to barycentric spanners, it is supported by a small (quasi-linear) set of points of K. Simultaneously and unlike the barycentric counterpart, the volumetric ellipsoid contains the body K, a property shared with John’s ellipsoid."
    }, {
      "heading" : "1.2 Barycentric Spanners",
      "text" : "The notion of Barycentric spanners was introduced in the work of [3], in order to define an exploration basis for an online shortest path problem. Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.\nDefinition 1.1. A barycentric spanner of K ⊆ Rd is a set of d points S = {u1, . . . , ud} ⊆ K such that any point in K may be expressed as a linear combination of the elements of S using coefficients in [−1, 1]. For C > 1, S is a C-approximate barycentric spanner of K if any point in K may be expressed as a linear combination of the elements of S using coefficients in [−C,C]\nIn [3] it is shown that any compact set has a barycentric spanner. Moreover, they show that given an oracle with the ability to solve linear optimization problems over K, an approximate barycentric spanner can be efficiently obtained. In the following sections we will use this constructive result.\nTheorem 1.1 (Proposition 2.5 in [3]). Let K be a compact set in Rd that is not contained in any proper linear subspace. Given an oracle for optimizing linear functions over K, for any C > 1, it is possible to compute a C-approximate barycentric spanner for K, using O(d2 logC(d)) calls to the optimization oracle."
    }, {
      "heading" : "1.3 The Fritz John Ellipsoid",
      "text" : "The John ellipsoid is the unique ellipsoid of smallest volume containing a given convex body in Euclidean space. Its properties have been the subject of study in convex geometry since John’s work [14] (see [4] and [13] for historic information).\nSuppose that we have linearly transformed K such that its minimum volume enclosing ellipsoid is the unit ball (in convex geometric terms, K is in John’s position). Then Johns theorem asserts the surprising fact that\n1 d Bn ⊆ K ⊆ Bn,\nwhere Bn denotes the unit ball in Rn. Furthermore, for symmetric convex bodies (which are far more abundant in machine learning), the factor 1d above can be replaced by 1√ d .\nJohn’s ellipsoid and in particular its contact points with the convex body it encapsulates makes for an appealing exploration basis, and indeed [7] have used exactly this machinery to attain an optimal-regret bandit linear optimization algorithm. Unfortunately we know of no efficient algorithm to compute, or even approximate up to a constant, the John ellipsoid for a general convex set, thus the latter result does not give a polynomial time algorithm for BLO.\nThe following theorem gives a characterization of the minimum enclosing ellipsoid, and was originally proved by John, restated here from [4] and [11]. Henceforth, let Id denote the d × d identity matrix.\nTheorem 1.2. [4] Let K ∈ Rd be a symmetric convex set and assume that the unit sphere is its minimal enclosing ellipsoid. Then there exist m ≤ d(d + 1)/2 − 1 contact points of K and the sphere u1, . . . , um and a vector c ∈ Rm such that c ≥ 0, ∑ ci = d and ∑ ciuiu T i = Id.\nThe computation of the linear transformation that makes the unit sphere to be the minimum volume enclosing ellipsoid (MVEE) is not known to be efficient in general, nor are the contact points known to be efficiently computable. For our construction of volumetric spanners and the volumetric ellipsoid that we define later on, it suffices to compute the MVEE of a discrete symmetric set, which is known to be efficiently computable. We make use of the following (folklore) algorithmic result:\nTheorem 1.3 (folklore, see e.g. [16, 8]). Let K ⊆ Rd be a set of n points. It is possible to compute an ε-approximate MVEE for K (an enclosing ellipsoid of volume at most (1 + ε) that of the MVEE) in time O(n3.5 log 1ε ).\nThe latter is attainable via the ellipsoid method or path-following interior point methods (see references in theorem statement). An approximation algorithm rather than an exact one is necessary in a real-valued computation model, and the logarithmic dependence on the approximation guarantee is as good as one can hope for in general.\nNote that this theorem addresses the case of a discrete set of points and/or their convex hull, rather than general convex sets. For our purposes, henceforth it suffices to consider this case: we use a different methodology to build an exploration basis for general convex sets.\nThus, the above theorem allows us to efficiently compute a linear transformation such that the MVEE of K is essentially the unit sphere. We can then use linear programming to compute an approximate representation as follows:\nTheorem 1.4. Let {x1, . . . , xn} = K ⊆ Rd be a set of n points and assume that:\n1. K is symmetric, i.e. if x ∈ K then also −x ∈ K.\n2. The John Ellipsoid of K is the unit ball.\nThen it is possible, in O(( √ n+ d)n3) time, to compute a vector c ∈ Rn such that:\n1. c ≥ 0 2. ∑ ci ≤ d\n3. ∑n i=1 cixix > i = Id\nProof. Denote the MVEE of K by E and let V be its corresponding d × d matrix, meaning V is such that ‖y‖2E = y>V −1y ≤ 1 for all y ∈ K. By our assumptions Id = V .\nAs K is symmetric and its MVEE is the unit ball, according to Theorem 1.2, there exist m ≤ d(d + 1)/2 − 1 contact points u1, . . . , um of K with the unit ball and a vector c′ ∈ Rm such that c′ ≥ 0, ∑ c′i = d and ∑ c′iuiu T i = Id. It follows that the following LP has a feasible\nsolution: Find c ∈ Rn such that c ≥ 0, ∑ ci ≤ d and ∑ ciuiu T i = Id. The described LP has\nO(n + d2) constraints and n variables. It can thus be solved in time O(d + √ n)n3) via interior point methods."
    }, {
      "heading" : "1.4 Structure of the paper",
      "text" : "In the next section we dive into convex geometry and define the key notions behind our constructions. Following that, we list preliminaries and known results from measure concentration, convex geometry and online learning in section 3. In section 4 and 5 we give the construction of an efficient volumetric spanner for, respectively, continuous and discrete sets. We then proceed to describe an application to bandit linear optimization in section 6."
    }, {
      "heading" : "2 Volumetric Ellipsoids and Spanners",
      "text" : "This section gives the main convex geometry constructions that we apply to machine learning. Consider a set K ⊆ Rd in Euclidean space. We mainly consider the case in which K = conv{v1, ..., vn} is the convex hull of n points. One of the most well studied objects in convex geometry is the John ellipsoid, defined as the enclosing ellipsoid of smallest volume for K. The Fritz John theorem characterises this ellipsoid and its properties, and we shall make used of it in this section to construct exploration basis for certain decision sets.\nBefore doing so, we define yet another shape, of a more discrete nature, that fundamentally characterises a convex set. Given a set of vectors S = {v1, ..., vt}, we denote by E(S) the ellipsoid defined by them, i.e.\nE(S) = { x ∈ Rd such that x =\n∑ i∈S αivi , ∑ i α2i ≤ 1\n}\nthat is, the ellipsoid given by all vectors spanned by the set S with Euclidean norm at most one. To see that this is indeed an ellipsoid, consider a point x such that x ∈ E(S). Then x = V α, where V is the matrix whose columns are all vectors of S, and α ∈ Bt. Thus, α = V −1x, where V −1 is the Moore-Penrose pseudo-inverse of V . By the bound on the norm of α, we have\n1 ≥ ‖α‖2 = ‖V −1x‖2 = x>(V V >)−1x\nwhich is exactly the definition of an ellipsoid with defining norm (V V >)−1 = ( ∑ i∈S viv > i ) −1 (note that V V > is a full rank matrix since S spans the space). When discussing this norm we shall\nalso use the notation ‖x‖E(S) ∆ = √ x>(V V >)−1x; indeed the described norm is that defined by the ellipsoid E(S).\nDefinition 2.1. Let K ⊆ Rd be a set in Euclidean space. For S ⊆ K, we say that E(S) is a volumetric ellipsoid for K if it contains K. We say that EK = E(S) is a a minimal volumetric ellipsoid if it is a containing ellipsoid defined by a set of minimal cardinality\nEK ∈ min |S| {E(S) such that S ⊆ K ⊆ E(S)} .\nWe say that |S| is the order of the minimal volumetric ellipsoid or of the convex set1 K denoted order(K).\nWe first make a few observations about the above notion of order:\n• The definition of order is linear invariant: for any invertible linear transformation T : Rd → Rd and K ⊆ Rd, order(K) = order(TK). We defer the simple proof to preliminaries.\n• The minimum volumetric ellipsoid is not unique in general; see example in figure 2. Further, it is in general different from John’s ellipsoid.\n• For non-degenerate convex sets K, their order is naturally lower bounded by d, and there are examples in which it is strictly more than d (e.g., figure 2).\nWe next state our main structural result in convex geometry giving a universal bound on the order of sets. The result is proved in the next section.\nTheorem 2.1 (Main). Any compact set K ⊆ Rd admits a volumetric ellipsoid of order O(d log d). A convex set K admits a volumetric ellipsoid of order O(d).\nFurther, if K = {v1, . . . , vn} is a discrete set, then a volumetric ellipsoid for K of order O(d log d) can be constructed in time poly(n, d).\nWe also give a different algorithmic construction for the discrete case in Section 5, which while being sub-optimal by logarithmic factors (gives an ellipsoid of order O(d(log d)(log n)) has the advantage of being much simpler and more efficient (no need for convex programming).\nThe above definition of volumetric ellipsoids is closely related to the kind of bases which allow for efficient exploration in our applications. To make this concrete and to simplify some terminology later on, we introduce the closely related notion of volumetric spanners. Informally, these correspond to sets S that spans all points in a given set with coefficients having Euclidean norm at most one, or formally:\nDefinition 2.2. Let K ⊆ Rd be a compact set and let S ⊆ K. We say that S is a volumetric spanner for K if E(S) ⊇ conv(K)\nIt is immediate from the definition of order and the above definition that a set K has a volumetric spanner of cardinality at most t if and only if order(K) ≤ t."
    }, {
      "heading" : "2.1 Existence and Construction of Volumetric Ellipsoids",
      "text" : "In this section we prove our main structural result, Theorem 2.1. Let K ⊆ Rd be a compact set in Euclidean space. We are particularly interested in the discrete case in which K = {v1, ..., vn} (or equivalently K = conv{v1, . . . , vn} is the convex hull of n points), but the discussion below is general. For this section assume that K is symmetric and thus contains the origin.\n1We note that our definition allows for multi-sets, meaning that S may contain the same vector twice\nRecall from preceding sections that the John ellipsoid for set K is the minimum volume ellipsoid that contains K. Henceforth, assume that we have linearly transformed K such that this largest contained ellipsoid is the unit ball. Then Johns theorem says that\n1√ d Bd ⊆ K ⊆ Bd\nFurther, there are m = O(d2) points S = {ui} on the enclosing unit ball that intersect K, and satisfy: ∑\ni∈S ciuiu\n> i = Id\nThis implies by taking trace that ∑ i ci = d.\nLemma 2.1. Let T = {w1, ..., wq} ⊆ S be a multi-set obtained by sampling q i.i.d elements of S according to the induced distribution given by\nwi = ui w.p. ci d .\nIf q > Cd log(d) for some sufficiently large constant C, then with probability at least 2/3, T is a volumetric spanner for K.\nProof. Notice that the wi’s are i.i.d, and E[dwiw > i ] = Id and ‖ √ dwi‖2 ≤ d. It thus follows from Theorem 3.3 that for q ≥ Cd log(d) for sufficiently large constant C, it holds with probability at least 2/3 that\nd q · ∑ wj∈T wjw > j 1 2 Id\nLet U be the matrix whose columns are vectors of T , thus UUT = ∑ j∈T wjw > j q 2dId. Since for positive definite matrices A B implies B−1 A−1 we get that for arbitrary x ∈ K,\n‖x‖2E(T ) = x >(UU>)−1x ≤ ‖x‖2 · 2d\nq < 1\nThe last inequality holds since q > 2d and x ∈ K is contained in the unit sphere and hence ‖x‖ ≤ 1.\nNote: For discrete sets K the above existence proof can be made constructive using Theorem 1.4.\nAs corollaries of the above lemma and Theorem 1.4 we get the following.\nCorollary 2.1. Any compact set K has a volumetric spanner of size O(d log d). In particular, for any compact set K, order(K) = O(d log d).\nCorollary 2.2. For a discrete set K ⊆ Rd of size n, a volumetric spanner of size O(d log d), and hence a volumetric ellipsoid of order O(d log d), can be constructed in time O(n3.5 + dn3).\nThe above corollary is the one of most interest for our applications. For the case where K is convex and symmetric we can reduce the size of the spanner to O(d). The latter proof, however is not efficient and does not give a polynomial-time algorithm.\nTheorem 2.2. For convex and symmetric K there exist a volumetric spanner of size O(d).\nProof. We use the following non-constructive result stating that K can be approximated by a different symmetric body H having at most O(d) contact points with its MVEE.\nLemma 2.2 ([19]). For any convex body K ∈ Rd there exist a body H ⊆ K ⊆ 3H such that H has at most O(d) contact points with its minimal volume enclosing ellipsoid (MVEE). 2.\n2For our machine learning applications we can always assume w.l.o.g that K is symmetric, in which case the constant 3 can be reduced to 1 + ε for every constant ε > 0 independent of d.\nLet u1, . . . , um be the contact points of 3H with its MVEE, where m = O(d). Assume w.l.o.g that the ellipsoid is the unit sphere. By the above property of John’s ellipsoid there exist c1, . . . , cm where ci ≥ 0, ∑ ci = d and ∑ i ciuiu T i = Id. Consider the multi-set T obtained by taking each\nvector ui/3 an amount of d3cie times. For this set, ∑ v∈T vv\n> Id. It follows that for any x ∈ K, as ‖x‖ ≤ 1,\n‖x‖2E(T ) = x >( ∑ v∈T vv>)−1x ≤ ‖x‖2 ≤ 1\nAlso, since H ⊆ K and ui/3 ∈ H for all i, T ⊂ K. It follows that T is a volumetric spanner for K. As for its size, it is an easy task to verify that |T | ≤ 3d+m = O(d)."
    }, {
      "heading" : "2.2 Approximate Volumetric Spanners",
      "text" : "Above we show a construction of volumetric spanners based on a minimal volume ellipsoid. In the case of general convex bodies, it is not known how to obtain such an ellipsoid, even approximately. For such difficult cases, we show that a softer version of the notion is sufficiently useful. In this section we present two different types of approximations for a volumetric spanner. In both types we require a small support for the spanner of roughly linear size. In the first case we allow the ellipsoid to contain the body only after being expanded by some product. In the second approximation we allow a small fraction of the points of the body to be outside the ellipsoid.\nDefinition 2.3. A ρ-ratio-volumetric spanner S of K is a subset S ⊆ K such that for all x ∈ K,\n‖x‖E(S) ≤ ρ\nOne example for such an approximate spanner with ρ = √ d is a barycentric spanner (Definition 1.1). In fact, it is easy to see that a C-approximate barycentric spanner is a C √ d-ratiovolumetric spanner . The following is immediate from Theorem 1.1.\nCorollary 2.3. Let K be a compact set in Rd that is not contained in any proper linear subspace. Given an oracle for optimizing linear functions over K, for any C > 1, it is possible to compute a C √ d-ratio-volumetric spanner S of K of cardinality |S| = d, using O(d2 logC(d)) calls to the optimization oracle.\nFor the second definition we describe a spanner that covers all but an ε fraction of the points in K and moreover, the measure of the points decays exponentially fast w.r.t their E(S)-norm. Since we are discussing a measure over the points of a body it makes sense not only to consider a uniform distribution over the body but an arbitrary one. As it turns out, the approximation can be efficiently obtained for any log-concave distribution.\nDefinition 2.4. Let K be a body in Rd and p a distribution over it. Let ε > 0. A (p, ε)-expvolumetric spanner of K is a set S ⊆ K where for any θ > 1\nPr x∼p\n[‖x‖E(S) ≥ θ] ≤ ε−θ\nBelow we prove that such spanners can be efficiently obtained. Specifically we prove\nTheorem 2.3. Let K be a convex body in Rd and p a log-concave distribution over it. By sampling O(d+log2(1/ε)) i.i.d. points from p one obtains, w.p. at least 1−exp(− √ d), a (p, ε)-exp-volumetric spanner for K. In particular, for general log-concave distribution p over convex K it is possible to compute a (p, ε)-exp-volumetric spanner in time Õ(d5 + d3/δ4) with success probability of at least 1− exp(− √ d)− δ."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "For a convex set K, let X ∼ K denote a uniformly random vector from K. We denote by Id the d× d identity matrix.\nDefinition 3.1. A distribution over Rd is log-concave when for its probability distribution function (pdf) p it holds that for any x, y ∈ Rn, λ ∈ [0, 1],\np(λx+ (1− λ)y) ≥ p(x)λp(y)1−λ\nTwo cases of interest of log-concave distributions are (1) the uniform distribution over a convex body and (2) a distribution over a convex body where p(x) ∝ exp(L>x), where L is some vector in Rd. In the following sections we use a result showing that a log-concave distribution can be efficiently sampled.\nLemma 3.1 ([17], Theorems 2.1 and 2.2). Let p be a log-concave distribution over Rd and let δ > 0. An algorithm for approximate sampling p exist with the following properties\n1. The distance, in terms of total deviation between the produced distribution and actual distribution is no more than δ. That is, the difference between the probabilities of any event in the produced and actual distribution is bounded by δ.\n2. The algorithm requires a pre-processing time of Õ(d5).\n3. A single sample can be produced in time Õ(d4/δ4), or amortized time of Õ(d3/δ4) if more than d samples are needed.\nDefinition 3.2 (Isotropic position). A random variable x is said to be in isotropic position (or isotropic) if\nE[x] = 0, E[xx>] = Id.\nA set K ⊆ Rd is said to be in isotropic position if x ∼ K is isotropic. Similarly, a distribution p is in isotropic position if x ∼ p is isotropic.\nHenceforth we use several results regarding the concentration of log-concave isotropic random vectors. We use slight modification where the center of the distribution is not necessarily in the origin. For completeness we present the proof of the modified theorems in Appendix A\nTheorem 3.1 (Theorem 4.1 in [2]). Let p be a log-concave distribution over Rd in isotropic position. There is a constant C such that for all t, δ > 0, the following holds for n = Ct\n4d log2(t/δ) δ2 .\nFor independent random vectors x1, . . . , xn ∼ p, with probability at least 1− exp(−t √ d),∥∥∥∥∥ 1n n∑ i=1 xix > i − Id ∥∥∥∥∥ ≤ δ. Corollary 3.1. Let p be a log-concave distribution over Rd and x ∼ p. Assume that x is such that E[xxT ] = Id. Then, there is a constant C such that for all t ≥ 1, δ > 0, the following holds for n = Ct\n4d log2(t/δ) δ2 . For independent random vectors x1, . . . , xn ∼ p, with probability at least\n1− exp(−t √ d), ∥∥∥∥∥ 1n n∑ i=1 xix > i − Id ∥∥∥∥∥ ≤ δ. Theorem 3.2 (Theorem 1.1 in [12]). There exist constants c, C such that the following holds. Let p be a log-concave distribution over Rd in isotropic position and let x ∼ p. Then, for all θ ≥ 0,\nPr [∣∣∣‖x‖ − √d∣∣∣ > θ√d] ≤ C exp(−c√d ·min(θ3, θ)).\nCorollary 3.2. Let p be a log-concave distribution over Rn and let x ∼ p. Assume that E[xxT ] = Id. Then for some universal C, c it holds for any θ ≥ 3 that\nPr [ ‖x‖ > θ √ d ] ≤ C exp ( −cθ √ d )\nThe following theorem provides a concentration bound for random vectors originating from an arbitrary distribution.\nTheorem 3.3 ([18]). Let X be a vector-valued random variable over Rd with E[XX>] = Σ and ‖Σ−1/2X‖2 ≤ R. Then, for independent samples X1, . . . , XM from X, and M ≥ CR log(R/ε)/ 2 the following holds with probability at least 1/2:∥∥∥∥∥ 1M M∑ i=1 Xi − Σ\n∥∥∥∥∥ ≤ ‖Σ‖. The following simple observation shows that the notion of order defined in Definition 2.1 is a\nlinearly invariant notion.\nObservation 3.1. Let T : Rd → Rd be an invertible linear transformation and let K ⊆ Rd be a non-degenerate convex body. For any S ⊂ K, K ⊆ E(S) iff TK ⊆ E(TS). In particular, order(TK) = order(K).\nProof. Let S ⊆ K be such that K ⊆ E(S). Then, clearly TK ⊆ E(TS). Thus, order(TK) ≤ order(K). The same argument applied to T−1 and TK shows that order(K) ≤ order(TK)."
    }, {
      "heading" : "4 Algorithmic Construction for Convex Sets",
      "text" : "In this section we provide a construction for (p, ε)-exp-volumetric spanner (as in Definition 2.4). In what follows we prove Theorem 2.3. We start by providing a more technical definition of a spanner. Note that unlike previous definitions, the following is not impervious to linear operators and will only be used to aid our construction.\nDefinition 4.1. A β-relative-spanner is a discrete subset S ⊆ K such that for all x ∈ K, ‖x‖2E(S) ≤ β‖x‖2.\nA first step is a spectral characterization of relative spanners:\nLemma 4.1. Let S = {v1, ..., vT } ⊆ K span K and be such that\nW = T∑ i=1 viv > i\n1 β Id\nThen S is a β-relative-spanner.\nProof. Let V ∈ Rd×T be a matrix whose columns are the vectors of S. As V V > = W 1β Id we have that\nβId (V V >)−1\nIt follows that ‖x‖E(S) = x>(V V >)−1x ≤ β‖x‖2\nas required.\nAlgorithm 1\n1: Input: An oracle to x ∼ p, where p is a distribution over K, T ∈ N. 2: for t = 1 to T do 3: Choose ut ∼ p 4: end for 5: return S = {u1, ..., uT }.\nproof of Theorem 2.3. We analyze Algorithm 1, previously defined within Theorem 2.3 assuming the vectors are sampled exactly according to the log-concave distribution. The result involving an approximate sample, which is necessary for implementing the algorithm in the general case, is an immediate application of Lemma 3.1.\nOur analysis of the algorithm is for T = C(d+log2(1/ε)) samples, where C is some sufficiently\nlarge constant. Assume first that Ex∼p[xx >] = Id. Let W = ∑T i=1 uiu > i . Then, for C > 0 large\nenough, by Corollary 3.1, ‖ 1TW − Id‖ ≤ 1/2 w.p. at least 1 − exp(− √ d). Therefore, S spans Rd and 1\nT W Id −\n1 2 Id = 1 2 Id\nThus according to Lemma 4.1, S is a (2/T )-relative spanner. Consider the case where Σ = Ex∼p[xx\n>] is not necessarily the identity. By the above analysis we get that Σ−1/2S = {Σ−1/2u1, . . . ,Σ−1/2uT } form a (2/T )-relative spanner for Σ−1/2K. This is since the r.v defined as Σ−1/2x where x ∼ p is log-concave. The latter along with corollary 3.2 implies that for any θ ≥ 1,\nPr x∼p\n[ ‖Σ−1/2x‖ ≥ 3θ √ d ] ≤ c1 exp ( −c2θ √ d )\n(1)\nfor some universal constants c1, c2 > 0. It follows that for our set S and any θ ≥ 1,\nPr x∼p\n[ ‖x‖E(S) > θ ] = Prx∼p [ ‖Σ−1/2x‖E(Σ−1/2S) > θ ] ‖x‖E(S) = ‖Σ−1/2x‖E(Σ−1/2S)\n≤ Prx∼p [ ‖Σ−1/2x‖ > θ √ T/2 ] Σ−1/2S is a 2/T -relative-spanner\n= Prx∼p\n[ ‖Σ−1/2x‖ > 3θ √ d √\nC 18 ·\n√ 1 + log\n2(1/ε) d\n] T = C(d+ log2(1/ε))\n≤ c1 exp ( −c2θ √ d √ C 18 · √ 1 + log 2(1/ε) d ) Equation (1), C ≥ 18\n≤ exp ( −θ √ d+ log2(1/ε) ) C sufficiently large\n≤ ε−θ"
    }, {
      "heading" : "5 Algorithmic Construction for Discrete Sets",
      "text" : "In this section we describe an algorithm that constructs volumetric spanners for discrete sets. The order of the spanners we construct is suboptimal, and not as good as mentioned attainable in polynomial time in previous sections. However, the algorithm is particularly simple and efficient to implement without appealing to convex programming techniques, and it is suboptimal only by logarithmic factors (a factor of log n to be precise).\nAlgorithm 2\n1: Input K = {x1, ..., xn} ⊆ Rd. 2: if n < Cd log d then 3: return S ← K 4: end if 5: Compute Σ = ∑ i xix > i and let ui = Σ\n−1/2xi. 6: For i ∈ [n], let pi = 1/2n+‖ui‖2/2d. Let S be a random set obtained by drawing M = Cd log d\nsamples with replacement from [n] according to the distribution p1, . . . , pn. 7: Verify that for at least n/2 vectors from {x1, . . . , xn}, it holds that ‖xi‖E(S) ≤ 1. If that is\nnot the case discard S and repeat the above step. 8: Apply the algorithm recursively on the data points for which ‖xi‖E(S) > 1.\nTheorem 5.1. Given a set of vectors K = {x1, . . . , xn} ∈ Rd, Algorithm 2 outputs a volumetric spanner of size O((d log d)(log n)) and has an expected running time of O(nd2).\nProof. Consider a single iteration of the algorithm with input v1, . . . , vn ∈ Rd. We claim that the random set S obtained in step 6 satisfies the following condition with constant probability:\nPr x∈K\n[ ‖x‖E(S) ≤ 1 ] ≥ 1/2 (2)\nSuppose the above statement is true. Then, the lemma follows easily as it implies that for the next iteration there are fewer than n/2 vectors. Hence, after (log n) recursive calls we will have a volumetric spanner. The total size of the set will be O((d log d)(log n)). To see the time complexity, consider a single run of the algorithm. The most computationally intensive steps are computing Σ and Σ−1/2 which take time O(nd2) and O(d3) respectively. We also need to compute ( ∑ v∈S vv\n>)−1 (to compute the E(S) norm) which takes time O(d3 log d), and compute the E(S) norm of all the vectors which requires O(nd2). As n = Ω(d log(d)), it follows that a single iteration runs of a total expected time of O(nd2). Since the size of n is split in half between iterations, the claim follows.\nWe now prove that Equation 2 holds with constant probability\nx>j (∑ v∈S vv> )−1 xj = u > j (∑ v∈S′ vv> )−1 uj . (3)\nwhere S′ = {Σ−1/2v|v ∈ S} is the (linearly) shifted version of S. Therefore, it suffices to show that with sufficiently high probability, the right hand side of the above equation is bounded by 1 for at least n/2 indices j ∈ [n].\nNote that pi = 1/2n+‖ui‖2/2d form a probability distribution: ∑ i pi = 1/2+( ∑ i ‖ui‖2)/2d =\n1. Let X ∈ Rd be a random variable with X = ui/ √ pi with probability pi for i ∈ [n]. Then, E[XX>] = Id. Further, for any i ∈ [n]\n‖ui‖2/pi ≤ 2d.\nTherefore, by Theorem 3.3, if we take M = Cd(log d) samples X1, . . . , XM for C sufficiently large, then with probability of at least 1/2, it holds that\nM∑ i=1 XiX > i (M/2)Id.\nLet T ⊆ [n] be the multiset corresponding to the indices of the sampled vectors X1, . . . , XM . The above inequality implies that ∑\ni∈T\n1 pi uiu > i (M/2)Id.\nNow, ∑ v∈S′ vv> (min i pi) ∑ v∈S′ 1 pi vv> (min i pi)(M/2)Id (M/4n)Id.\nTherefore,\nn∑ i=1 u>i (∑ v∈S′ vv> )−1 ui = n∑ i=1 Tr (∑ v∈S′ vv> )−1 ( uiu > i ) = Tr\n(∑ v∈S′ vv> )−1( n∑ i=1 uiu > i ) = Tr\n(∑ v∈S′ vv> )−1 ≤ 4nd\nM ≤ 4n C log d ≤ n 2 log d ,\nfor C sufficiently large. Therefore, by Markov’s inequality and Equation 3, it follows that Equation 2 holds with high probability. The theorem now follows."
    }, {
      "heading" : "6 Bandit Linear Optimization",
      "text" : "Recall the problem of Bandit Linear Optimization (BLO): iteratively at each time sequence t, the environment chooses a loss vector Lt that is not revealed to the player. The player chooses a vector xt ∈ K where K ⊆ Rd is convex, and once she commits to her choice, the loss `t = x>t Lt is revealed. The objective is to minimize the loss and specifically, the regret, defined as the strategy’s loss minus the loss of the best fixed strategy of choosing some x∗ ∈ K for all t. We henceforth assume, and this is w.l.o.g. via standard scaling techniques, that the loss vectors Lt’s are chosen from the polar of K, meaning from {L : |L>x| ≤ 1 ∀x ∈ K}.\nThe problem of BLO is a natural generalization of the classical Multi-Armed Bandit problem and extremely useful for efficiently modelling decision making under partial feedback for structured problems. As such the research literature is rich with algorithms and insights into this fundamental problem. For a brief historical survey please refer to earlier sections of this manuscript. In this section we focus on the first efficient and optimal-regret algorithm, and thus immediately jump to Algorithm 3. We make the following assumptions over the decision set K:\n1. The set K is equipped with a membership oracle. This implies via [17] (Lemaa 3.1) that there exists an efficient algorithm for sampling from a given log-concave distribution over K. Via the discussion in previous sections, this also implies that we can construct approximate (both types of approximations, see Definitions 2.3 and 2.4) volumetric spanners efficiently over K.\n2. The losses are bounded in absolute values by 1. That is, the loss functions are always chosen (by an oblivious adversary) from a convex set Z such that K is contained in its polar, i.e. ∀L ∈ Z, x ∈ K, |L>x| ≤ 1. This implies that the set K admits for any ε > 0 an ε-net, w.r.t the norm defined by Z, whose size we denote by |K|ε ≤ (ε/2)−d.\nFor Algorithm 3 we prove the following optimal regret bound:\nAlgorithm 3 EXP2 with Volumetric Spanners Exploration\n1: K, parameters γ, η, horizon T . 2: p1(x) uniform distribution over K. 3: for t = 1 to T do 4: Let S′t be a (pt, exp(−(4 √ d+ log(2T ))))-exp-volumetric spanner of K.\n5: Let S′′t be a 2 √ d-ratio-volumetric spanner of K 6: Set St as the union of S ′ t, S ′′ t . 7: p̂t(x) = (1− γ)p(x) + γ|St|1x∈St 8: sample xt according to p̂t 9: observe loss `t ∆ = L>t xt\n10: Let Ct ∆ = Ex∼p̂t [xx >] 11: L̂t ∆ = `tC −1 t xt 12: pt+1(x) ∝ pt(x)e−ηL̂ > t x 13: end for\nTheorem 6.1. Under the assumptions stated above, and let s = maxt |St|, η = √ log |K|1/T dT and\nlet γ = s\n√ log(|K|1/T )\ndT . Algorithm 3 given parameters γ, η suffers a regret bounded by\nO ( (s+ d) √ T log |K|1/T\nd\n)\nWe note that while the size log(|K|1/T ) can be bounded by d log(T ), in certain scenarios such as s-t paths in graphs it is possible to obtain sharper upper bounds that immediately imply better regret via Theorem 6.1.\nCorollary 6.1. There exist an efficient algorithm for BLO for any convex set K with regret of O (√ dT log |K|1/T ) = O ( d √ T log(T ) ) Proof. The spanner in step 4 of the algorithm does not have to be explicitly constructed. According to Theorem 2.3, to obtain such as spanner it suffices to sample sufficiently many points from the distribution pt, hence this portion of the exploration strategy is identical to the exploitation strategy.\nAccording to Corollary 2.3, a 2 √ d-ratio-volumetric spanner of size d can be efficiently con-\nstructed. Hence, it follows that for the purpose of the analysis, s = d and the bound follows.\nTo prove the theorem we follow the general methodology used in analyzing the performance of the geometric hedge algorithm. The major deviation from standard technique is the following sub-exponential tail bound, which we use to replace the the standard absolute bound for |L̂tx|. After giving its proof and a few auxiliary lemmas, we give the proof of the main theorem.\nLemma 6.1. Let x ∼ pt, xt ∼ p̂t and let L̂t be defined according to xt. It holds, for any θ > 1 that\nPr [ |L̂>t x| > θs\nγ\n] ≤ exp(−2θ)/T\nProof.\nPr [ |L̂>t x| > θs\nγ\n] ≤ Pr [ ‖x‖E(St) · ‖xt‖E(St) ≥ θ ] Lemma 6.2\n≤ Pr [ ‖x‖E(St) ≥ √ θ ∨ ‖xt‖E(St) ≥ √ θ ]\n≤ Pr [ ‖x‖E(St) ≥ √ θ ] + Pr [ ‖xt‖E(St) ≥ √ θ ]\n≤ 2 Pr [ ‖x‖E(St) ≥ √ θ ]\nTo justify the last inequality notice that x ∼ pt and xt ∼ p̂t where p̂t is a convex sum of pt and a distribution qt for which Pry∼qt [ ‖y‖E(St) ≥ √ θ > 1 ] = 0. Before we continue recall that we can assume that √ θ ≤ 2 √ d, since S′′t is a 2 √ d-ratio-volumetric spanner .\nPr [ |L̂>t x| > θs\nγ\n] ≤ 2 Pr [ ‖x‖E(St) ≥ √ θ ]\n≤ 2 exp(− √ θ(4 √ d+ log 2T )) property of exp-volumetric spanner\n≤ 1T exp(−2 √ θ · 4d)\n≤ 1T exp(−2θ) since θ ≤ 4d\nLemma 6.2. For all x ∈ K it holds that |L̂>t x| ≤ |St|‖x‖E(St)‖xt‖E(St) γ .\nProof. Let x ∈ K. Denote by Vt the matrix whose columns are the elements of St and recall that ‖y‖2E(St) = y >(VtV > t ) −1y. Since Ct ∆ = Ex∼p̂t [xx >], it holds that\nCt γ |St| ∑ v∈St vv> = γ |St| VtV > t\nsince both matrices are full rank, it holds that\nC−1t |St| γ (VtV > t ) −1\nNotice that due to the Cauchy-Schwartz inequality,\n|x>L̂t| = |`t| · |x>C−1t xt| ≤ |`t| · ‖x>C −1/2 t ‖ · ‖C −1/2 t xt‖\nThe matrix C −1/2 t is defined as Ct is positive definite. Now,\n‖x>C−1/2t ‖2 = x>C−1t x ≤ x> |St| γ (VtV > t ) −1x = |St| γ ‖x‖2E(St)\nSince the analog can be said for ‖C−1/2t xt‖ (as xt ∈ K), it follows that\n|x>L̂t| ≤ |`t| |St|‖x‖E(St)‖xt‖E(St) γ ≤ |St|‖x‖E(St)‖xt‖E(St) γ\nThe last inequality is since we assume the rewards are in [−1, 1]."
    }, {
      "heading" : "6.1 Proof of Theorem 6.1",
      "text" : "We continue the analysis of the Geometric Hedge algorithm similarly to [10, 7], under certain assumptions over the exploration strategy. For convenience we will assume that the set of possible arms K is finite. This assumption holds w.l.o.g since if K is infinite, a √ 1/T -net of it can be considered as described earlier (this will have no effect on the computational complexity of our algorithm, but a mere technical convenience in the proof below).\nBefore proving the theorem we will require three technical lemmas. In the first we show that L̂t is an unbiased estimator of Lt. In the second, we bound its variance. In the third, we bound the expected value of its exponent.\nLemma 6.3. In each t, L̂t is an unbiased estimator of Lt\nProof. L̂t = `tC −1 t xt = (L > t xt)C −1 t xt = C −1 t (xtx > t )Lt\nHence, E\nxt∼pt [L̂t] = C\n−1 t E\nxt∼pt [xtx\n> t ]Lt = C −1 t CtLt = Lt\nLemma 6.4. Let t ∈ [T ], x ∼ pt and xt ∼ p̂t. It holds that E[(L̂>t x)2] ≤ d/(1− γ) ≤ 2d\nProof. For convenience, denote by qt the uniform distribution over St - the exploration strategy at round t. First notice that for any x ∈ K,\nE xt∼p̂t\n[(L̂>t x) 2] = x>Ext∼p̂t [L̂tL̂ > t ]x = x >Ext∼p̂t [` 2 tC −1 t xtx > t C −1 t ]x\n= `2tx >C−1t Ext∼p̂t [xtx > t ]C −1 t x = ` 2 tx >C−1t x\n≤ x>C−1t x (4)\nNext,\nE x∼p̂t [x>C−1t x] = E x∼p̂t [C−1t • xx>] = C−1t • E x∼p̂t [xx>] = C−1t • Ct = Tr(Id) = d\nWhere we used linearity of expectation and denote A • B = Tr(AB). Since C−1t is positive semi definite,\n(1− γ) E x∼pt [x>C−1t x] ≤ (1− γ) E x∼pt [x>C−1t x] + γ E x∼qt [x>C−1t x] = E x∼p̂t [x>C−1t x] = d (5)\nThe lemma follows from combining Equations 4 and 5.\nLemma 6.5. Let t ∈ [T ], xt ∼ p̂t and x ∼ pt. For L̂t defined by xt it holds that\nE [ exp(−ηL̂>t x)1−ηL̂>t x>1 ] ≤ 2 T\nProof. Let f, F be the pdf and cdf of the random variable Y = −ηL̂>t x correspondingly. From Lemma 6.1 and the fact that 1/η = s/γ we have that for any θ ≥ 1,\n1− F (θ) ≤ 1 T e−2θ\nand we’d like to prove that under this condition,\nE[eY 1Y >1] = ∫ ∞ θ=1 eθf(θ)dθ ≤ 2 T\nwhich follows from the definition of the cdf and pdf: E[eY 1Y >1] = ∫∞ θ=1 eθf(θ)dθ\n= ∑∞ k=1 ∫ k+1 θ=k eθf(θ)dθ\n≤ ∑∞ k=1 e k+1 ∫ k+1 θ=k f(θ)dθ\n≤ ∑∞ k=1 e\nk+1(F (k + 1)− F (k)) ≤ ∑∞ k=1 e\nk+1(1− F (k)) ≤ ∑∞ k=1 e k+1 · 1T e −2k Lemma 6.1\n= eT ∑∞ k=1 e −k = eT · e−1 1−e−1 ≤ 2 T\nProof of Theorem 6.1. For convenience we define within this proof for x ∈ K, ˆ̀1:t−1(x) ∆ = ∑t−1 i=1 L̂ > i x and let ˆ̀t(x) ∆ = L̂>t x. Let Wt = ∑ x∈K exp(−η ˆ̀1:t−1(x)). For all t ∈ [T ]:\nE [ Wt+1 Wt ] = E [∑ x∈K exp(−η ˆ̀1:t−1(x)) exp(−η ˆ̀t(x)) Wt ] = Ext∼p̂t [∑ x∈K pt(x) exp(−η ˆ̀t(x))\n] = Ext∼p̂t,x∼pt [exp(−η ˆ̀t(x))] ≤\n≤ 1− ηE[L̂>t x] + η2 E[(L̂>t x)2] + E [ exp(−ηL̂>t x)1−ηL̂>t x>1 ] using the inequality exp(y) ≤ 1 + y + y2 + exp(y) · 1y>1\n≤ 1− ηE[L̂>t x] + η2 E[(L̂>t x)2] + 2T Lemma 6.5\nSince L̂t is an unbiased estimator of Lt (Lemma 6.3) and according to Lemma 6.4, E[(L̂ > t x) 2] ≤ 2d, we get:\nE [ Wt+1 Wt ] ≤ 1− ηL>t E x∼pt [x] + 2η2d+ 2 T (6)\nWe now use Jensen’s inequality:\nE[log(WT )]−E[log(W1)] = E[log(WT /W1)] = ∑T−1 t=1 E[log(Wt+1/Wt)]\n≤ ∑T−1 t=1 log(E[Wt+1/Wt]) Jensen\n≤ ∑T−1 t=1 log ( 1− ηL>t Ex∼pt [x] + 2η2d+ 2T ) (6)\n≤ ∑T−1 t=1 −ηL>t Ex∼pt [x] + 2η2d+ 2 T Due to ln(1 + y) ≤ y for all y > −1\n≤ 2 + 2η2Td− η ∑ t Ex∼pt [L > t x]\nNow, since log(W1) = log(|K|) and WT ≥ exp(−η ˆ̀1:T (x∗)) for any x∗ ∈ K, by shifting sides of the above it holds for any x∗ ∈ K that∑\nt\nE x∼pt [L>t x]− ∑ t L>t x ∗ ≤ ∑ t E x∼pt [L>t x] + E[logWT ] ≤ log(|K|) + 2 η + 2ηTd\nFinally, by noticing that ∑ t E x∼p̂t [L>t x]− ∑ t E x∼pt [L>t x] ≤ γT\nwe obtain a bound of\nE[Regret] = E[ ∑ t L>t xt]− ∑ t L>t x ∗ = ∑ t E x∼p̂t [L>t x]− Loss(x∗) ≤ log(|K|) + 2 η + 2ηTd+ γT\non the expected regret. By plugging in the values of η, γ we get the bound of\nO ( (s+ d) √ T log(|K|)\nd ) as required.\nDiscussion: Notice that to obtain a (p, ε)-exp-volumetric spanner for a log-concave distribution p over a body K we simply choose sufficiently many i.i.d samples from p. Since in the above algorithm pt is always log-concave, it follows that S ′ t consists of i.i.d samples from pt, meaning that if we would not have required S′′t , the exploration and exploration strategies would be the same! Since we still require the set S′′t , there exists a need for a separate exploration strategy. Interestingly, the 2 √ d-ratio-volumetric spanner is obtained by taking a barycentric spanner, which is the exploration strategy of [10]."
    }, {
      "heading" : "A Concentration bounds for non centered isotropic log con-",
      "text" : "cave distributions\nWe begin by proving an auxiliary lemma used in the proof of Corollary 3.1.\nLemma A.1. Let δ > 0, t ≥ 1, let d be a positive integer and let n = Ct 4d log2(t/δ)\nδ2 for some sufficiently large universal constant C. Let y1, . . . , yn be i.i.d d-dimensional vectors from an isotropic log-concave distribution. Then\nPr [∥∥∥∥ 1n∑ yi ∥∥∥∥ > δ] ≤ exp(−t√d)\nProof. For convenience let Sn = 1√ n ∑n i=1 yi. Since the y’s are independent, Sn is also log-concave distributed. Notice that E[Sn] = 0 and E[SnS T n ] = 1 n ∑ E[yiy T i ] = Id hence Sn is isotropic. Now,\nPr [∥∥∥∥ 1n∑ yi ∥∥∥∥ > δ] = Pr [‖Sn‖ > √nδ] = Pr [‖Sn‖ > √d ·√Ct4 log2(t/δ)] ≤ Pr [‖Sn‖ > √d+√d · 12 t√C ] .\nThe last inequality holds for t ≥ 1 and C ≥ 4. It now follows from Theorem 3.2 that\nPr [∥∥∥∥ 1n∑ yi ∥∥∥∥ > δ] ≤ c1 exp(−c2t√Cd)\nwhere c1, c2 are some universal constants. Since t √ d ≥ 1, setting C ≥ ( 1+log(c1)c2 )\n2 proves the claim.\nProof of Corollary 3.1. Let a = E[x] and let ã = 1n ∑ xi. Notice that\nE[(x− a)(x− a)T ] = E[xxT ]−E[x]aT − aE[x] + aaT = Id − aaT\nis a PSD matrix hence ‖a‖ ≤ 1. Consider the following equality.\n1\nn n∑ i=1 (xi − a)(xi − a)> = 1 n n∑ i=1 xix > i − aã> − ãa> + aa>\nAccording to Lemma A.1, w.p. at least 1− exp(−t √ d),\n‖ã− a‖ ≤ δ\nin which case, since ‖a‖ ≤ 1 and according to the triangle inequality,∥∥∥∥∥ 1n n∑ i=1 xix > i − Id ∥∥∥∥∥ ≤ ∥∥∥∥∥ 1n n∑ i=1 (xi − a)(xi − a)> − (Id − aa>) ∥∥∥∥∥+ 2δ According to Theorem 3.1, w.p. at least 1− exp(−t\n√ d)∥∥∥∥∥ 1n n∑ i=1 (xi − a)(xi − a)> − (Id − aa>) ∥∥∥∥∥ ≤ δ and the corollary follows.\nProof of Corollary 3.2. Let E[x] = a. Consider the r.v y = x − a. It holds that E[y] = 0 and E[yyT ] = Id − aaT . Notice that we can derive that\n‖a‖ ≤ 1 (7)\nAs E[yyT ] is a PSD matrix. Also, it is easy to verify that y is log-concave distributed. We now consider the r.v3 z = (Id − aat)−1/2y. It is easy to verify that the distribution of z is also log-concave and isotropic. It follows, from Theorem 3.2 that for any θ ≥ 2\nPr [ ‖z‖ > θ √ d ] ≤ Pr [ ‖z‖ − √ d > 1\n2 θ √ d\n] ≤ C ′ exp(−cθ √ d)\nBy using equation 7 we get that for θ > 3 Pr [ ‖x‖ > θ √ d ] ≤ Pr [ ‖y‖ > θ √ d− 1 ] ≤ Pr [ ‖z‖ > (θ − 1/ √ d) √ d ] ≤ C ′ exp(c′ − c′θ √ d).\nThe last inequality holds since θ − 1/ √ d ≥ 2.\n3if Id−aaT is not of full rank then y is in fact supported in an affine subspace of rank d−1 and we can continue the analysis there."
    } ],
    "references" : [ {
      "title" : "Interior-point methods for full-information and bandit online learning",
      "author" : [ "J.D. Abernethy", "E. Hazan", "A. Rakhlin" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles",
      "author" : [ "Radoslaw Adamczak", "Alexander E. Litvak", "Alain Pajor", "Nicole Tomczak-Jaegermann" ],
      "venue" : "Journal of American Mathematical Society,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Online linear optimization and adaptive routing",
      "author" : [ "Baruch Awerbuch", "Robert Kleinberg" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "An elementary introduction to modern convex geometry. In Flavors of Geometry, pages 1–58",
      "author" : [ "Keith Ball" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "High-probability regret bounds for bandit online linear optimization",
      "author" : [ "Peter L. Bartlett", "Varsha Dani", "Thomas P. Hayes", "Sham Kakade", "Alexander Rakhlin", "Ambuj Tewari" ],
      "venue" : "In COLT,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems, volume 5 of Foundations and Trends in Machine Learning",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Towards minimax policies for online linear optimization with bandit feedback",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi", "Sham M. Kakade" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Linear convergence of a modified frankwolfe algorithm for computing minimum-volume enclosing ellipsoids",
      "author" : [ "S. Damla Ahipasaoglu", "Peng Sun", "Michael J. Todd" ],
      "venue" : "Optimization Methods and Software,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "Varsha Dani", "Thomas P Hayes", "Sham M Kakade" ],
      "venue" : "In COLT,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "Varsha Dani", "Sham M Kakade", "Thomas P Hayes" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "An arithmetic proof of john’s ellipsoid theorem",
      "author" : [ "Peter M. Gruber", "Franz E. Schuster" ],
      "venue" : "In arXiv:1207.7246,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Interpolating thin-shell and sharp large-deviation estimates for lsotropic log-concave measures",
      "author" : [ "Olivier Gudon", "Emanuel Milman" ],
      "venue" : "Geometric and Functional Analysis,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Löwner-John ellipsoids",
      "author" : [ "Martin Henk" ],
      "venue" : "Documenta Mathematica,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Extremum Problems with Inequalities as Subsidiary Conditions",
      "author" : [ "F. John" ],
      "venue" : "Studies and Essays: Courant Anniversary Volume,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1948
    }, {
      "title" : "Playing games with approximation algorithms",
      "author" : [ "Sham M Kakade", "Adam Tauman Kalai", "Katrina Ligett" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Rounding of polytopes in the real number model of computation",
      "author" : [ "Leonid G Khachiyan" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1996
    }, {
      "title" : "The geometry of logconcave functions and sampling algorithms",
      "author" : [ "László Lovász", "Santosh Vempala" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Random vectors in the isotropic position",
      "author" : [ "M. Rudelson" ],
      "venue" : "Journal of Functional Analysis,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "To mention a few: Awerbuch and Kleinberg [3] devise the notion of barycentric spanners, and use this construction to give the first low-regret algorithms for complex decision problems such as online routing.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "Abernethy, Hazan and Rakhlin [1] use self-concordant barriers to build an efficient exploration strategy for convex sets in Euclidean space.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "Bubeck, Cesa-Bianchi and Kakade [7] apply tools from convex geometry, namely John’s ellipsoid to construct optimal regret algorithms for bandit linear optimization (albeit not always efficiently).",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "is that of online routing in graphs: a decision maker iteratively chooses a path in a given graph from source to destination, the adversary chooses lengths of the edges of the graph, and the decision maker receives as feedback the length of the path she chose but no other information (see [3]).",
      "startOffset" : 290,
      "endOffset" : 293
    }, {
      "referenceID" : 0,
      "context" : "learning permutations, rankings and other examples (see [1]).",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "The reader is referred to the recent survey of Bubeck and Cesa-Bianchi [6] for more details on algorithmic results for BLO.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "Previously efficient algorithms (with non-optimal regret) were known over convex sets that admit an efficient selfconcordant barrier [1], and optimal regret algorithms were known over general sets [7] but using computationally in-efficient machinery.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "Previously efficient algorithms (with non-optimal regret) were known over convex sets that admit an efficient selfconcordant barrier [1], and optimal regret algorithms were known over general sets [7] but using computationally in-efficient machinery.",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 2,
      "context" : "2 Barycentric Spanners The notion of Barycentric spanners was introduced in the work of [3], in order to define an exploration basis for an online shortest path problem.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 8,
      "context" : "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.",
      "startOffset" : 285,
      "endOffset" : 288
    }, {
      "referenceID" : 2,
      "context" : "In [3] it is shown that any compact set has a barycentric spanner.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "5 in [3]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 13,
      "context" : "Its properties have been the subject of study in convex geometry since John’s work [14] (see [4] and [13] for historic information).",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "Its properties have been the subject of study in convex geometry since John’s work [14] (see [4] and [13] for historic information).",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "Its properties have been the subject of study in convex geometry since John’s work [14] (see [4] and [13] for historic information).",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "John’s ellipsoid and in particular its contact points with the convex body it encapsulates makes for an appealing exploration basis, and indeed [7] have used exactly this machinery to attain an optimal-regret bandit linear optimization algorithm.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "The following theorem gives a characterization of the minimum enclosing ellipsoid, and was originally proved by John, restated here from [4] and [11].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "The following theorem gives a characterization of the minimum enclosing ellipsoid, and was originally proved by John, restated here from [4] and [11].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "[4] Let K ∈ R be a symmetric convex set and assume that the unit sphere is its minimal enclosing ellipsoid.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "[16, 8]).",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "[16, 8]).",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "If the vertices are [0, 1], [− √ 3 2 ,− 1 2 ], [ √ 3 2 ,− 1 2 ], then the eigenpoles of the ellipsoid of the bottom two vertices are [0.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "23 ], [2, 0].",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "A distribution over R is log-concave when for its probability distribution function (pdf) p it holds that for any x, y ∈ R, λ ∈ [0, 1], p(λx+ (1− λ)y) ≥ p(x)λp(y)1−λ",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "1 ([17], Theorems 2.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "1 in [2]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 11,
      "context" : "1 in [12]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 17,
      "context" : "3 ([18]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "This implies via [17] (Lemaa 3.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "1 We continue the analysis of the Geometric Hedge algorithm similarly to [10, 7], under certain assumptions over the exploration strategy.",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "1 We continue the analysis of the Geometric Hedge algorithm similarly to [10, 7], under certain assumptions over the exploration strategy.",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "Interestingly, the 2 √ d-ratio-volumetric spanner is obtained by taking a barycentric spanner, which is the exploration strategy of [10].",
      "startOffset" : 132,
      "endOffset" : 136
    } ],
    "year" : 2017,
    "abstractText" : "Numerous machine learning problems require an exploration basis a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases. We show how efficient volumetric spanners give rise to the first efficient and optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.",
    "creator" : "LaTeX with hyperref package"
  }
}