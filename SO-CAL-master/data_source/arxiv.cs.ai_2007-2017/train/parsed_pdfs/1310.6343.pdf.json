{
  "name" : "1310.6343.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Provable Bounds for Learning Some Deep Representations",
    "authors" : [ "Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma" ],
    "emails" : [ "arora@cs.princeton.edu.", "bhaskara@cs.princeton.edu.", "rongge@microsoft.com.", "tengyu@cs.princeton.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights. 1"
    }, {
      "heading" : "1 Introduction",
      "text" : "Can we provide theoretical explanation for the practical success of deep nets? Like many other ML tasks, learning deep neural nets is NP-hard, and in fact seems “badly\n∗Princeton University, Computer Science Department and Center for Computational Intractability. Email: arora@cs.princeton.edu. This work is supported by the NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, and Simons Investigator Grant. †Google Research NYC. Email: bhaskara@cs.princeton.edu. The work was done while the author was a Postdoc at EPFL, Switzerland. ‡Microsoft Research, New England. Email: rongge@microsoft.com. Part of this work was done while the author was a graduate student at Princeton University and was supported in part by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, and Simons Investigator Grant. §Princeton University, Computer Science Department and Center for Computational Intractability. Email: tengyu@cs.princeton.edu. This work is supported by the NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, and Simons Investigator Grant.\n1The first 18 pages of this document serve as an extended abstract of the paper, and a long technical appendix follows.\nar X\niv :1\n31 0.\n63 43\nv1 [\ncs .L\nG ]\n2 3\nNP-hard”because of many layers of hidden variables connected by nonlinear operations. Usually one imagines that NP-hardness is not a barrier to provable algorithms in ML because the inputs to the learner are drawn from some simple distribution and are not worst-case. This hope was recently borne out in case of generative models such as HMMs, Gaussian Mixtures, LDA etc., for which learning algorithms with provable guarantees were given [HKZ12, MV10, HK13, AGM12, AFH+12]. However, supervised learning of neural nets even on random inputs still seems as hard as cracking cryptographic schemes: this holds for depth-5 neural nets [JKS02] and even ANDs of thresholds (a simple depth two network) [KS09].\nHowever, modern deep nets are not “just”neural nets (see the survey [Ben09]). The underlying assumption is that the net (or some modification) can be run in reverse to get a generative model for a distribution that is a close fit to the empirical input distribution. Hinton promoted this viewpoint, and suggested modeling each level as a Restricted Boltzmann Machine (RBM), which is “reversible”in this sense. Vincent et al. [VLBM08] suggested using many layers of a denoising autoencoder, a generalization of the RBM that consists of a pair of encoder-decoder functions (see Definition 1). These viewpoints allow a different learning methodology than classical backpropagation: layerwise learning of the net, and in fact unsupervised learning. The bottom (observed) layer is learnt in unsupervised fashion using the provided data. This gives values for the next layer of hidden variables, which are used as the data to learn the next higher layer, and so on. The final net thus learnt is also a good generative model for the distribution of the bottom layer. In practice the unsupervised phase is followed by supervised training2.\nThis viewpoint of reversible deep nets is more promising for theoretical work because it involves a generative model, and also seems to get around cryptographic hardness. But many barriers still remain. There is no known mathematical condition that describes neural nets that are or are not denoising autoencoders. Furthermore, learning even a a single layer sparse denoising autoencoder seems at least as hard as learning sparse-used overcomplete dictionaries (i.e., a single hidden layer with linear operations), for which there were no provable bounds at all until the very recent manuscript [AGM13]3.\nThe current paper presents both an interesting family of denoising autoencoders as well as new algorithms to provably learn almost all models in this family. Our ground truth generative model is a simple multilayer neural net with edge weights in [−1, 1] and simple threshold (i.e., > 0) computation at the nodes. A k-sparse 0/1 assignment is provided at the top hidden layer, which is computed upon by successive hidden\n2Recent work suggests that classical backpropagation-based learning of neural nets together with a few modern ideas like convolution and dropout training also performs very well [KSH12], though the authors suggest that unsupervised pretraining should help further.\n3The parameter choices in that manuscript make it less interesting in context of deep learning, since the hidden layer is required to have no more than √ n nonzeros where n is the size of the observed layer —in other words, the observed vector must be highly compressible.\nlayers in the obvious way until the “observed vector”appears at the bottommost layer. If one makes no further assumptions, then the problem of learning the network given samples from the bottom layer is still harder than breaking some cryptographic schemes. (To rephrase this in autoencoder terminology: our model comes equipped with a decoder function at each layer. But this is not enough to guarantee an efficient encoder function—this may be tantamount to breaking cryptographic schemes.)\nSo we make the following additional assumptions about the unknown “ground truth deep net”(see Section 2): (i) Each feature/node activates/inhibits at most nγ features at the layer below, and is itself activated/inhibited by at most nγ features in the layer above, where γ is some small constant; in other words the ground truth net is not a complete graph. (ii) The graph of these edges is chosen at random and the weights on these edges are random numbers in [−1, 1].\nOur algorithm learns almost all networks in this class very efficiently and with low sample complexity; see Theorem 1. The algorithm outputs a network whose generative behavior is statistically indistinguishable from the ground truth net. (If the weights are discrete, say in {−1, 1} then it exactly learns the ground truth net.)\nAlong the way we exhibit interesting properties of such randomly-generated neural nets. (a) Each pair of adjacent layers constitutes a denoising autoencoder in the sense of Vincent et al.; see Lemma 2. Since the model definition already includes a decoder, this involves showing the existence of an encoder that completes it into an autoencoder. (b) The encoder is actually the same neural network run in reverse by appropriately changing the thresholds at the computation nodes. (c) The reverse computation is stable to dropouts and noise. (d) The distribution generated by a two-layer net cannot be represented by any single layer neural net (see Section 8), which in turn suggests that a random t-layer network cannot be represented by any t/2-level neural net4.\nNote that properties (a) to (d) are assumed in modern deep net work: for example (b) is a heuristic trick called “weight tying”. The fact that they provably hold for our random generative model can be seen as some theoretical validation of those assumptions. Context. Recent papers have given theoretical analyses of models with multiple levels of hidden features, including SVMs [CS09, LSSS13]. However, none of these solves the task of recovering a ground-truth neural network given its output distribution.\nThough real-life neural nets are not random, our consideration of random deep networks makes some sense for theory. Sparse denoising autoencoders are reminiscent of other objects such as error-correcting codes, compressed sensing, etc. which were all first analysed in the random case. As mentioned, provable reconstruction of the hidden layer (i.e., input encoding) in a known autoencoder already seems a nonlinear generalization of compressed sensing, whereas even the usual (linear) version\n4Formally proving this for t > 3 is difficult however since showing limitations of even 2-layer neural nets is a major open problem in computational complexity theory. Some deep learning papers mistakenly cite an old paper for such a result, but the result that actually exists is far weaker.\nof compressed sensing seems possible only if the adjacency matrix has “random-like” properties (low coherence or restricted isoperimetry or lossless expansion). In fact our result that a single layer of our generative model is a sparse denoising autoencoder can be seen as an analog of the fact that random matrices are good for compressed sensing/sparse reconstruction (see Donoho [Don06] for general matrices and Berinde et al. [BGI+08] for sparse matrices). Of course, in compressed sensing the matrix of edge weights is known whereas here it has to be learnt, which is the main contribution of our work. Furthermore, we show that our algorithm for learning a single layer of weights can be extended to do layerwise learning of the entire network.\nDoes our algorithm yield new approaches in practice? We discuss this possibility after sketching our algorithm in the next section."
    }, {
      "heading" : "2 Definitions and Results",
      "text" : "Our generative neural net model (“ground truth”) has ` hidden layers of vectors of binary variables h(`), h(`−1), .., h(1) (where h(`) is the top layer) and an observed layer y at bottom. The number of vertices at layer i is denoted by ni, and the set of edges between layers i and i + 1 by Ei. In this abstract we assume all ni = n; in appendix we allow them to differ.5 (The long technical appendix serves partially as a full version of the paper with exact parameters and complete proofs). The weighted graph between layers h(i) and h(i+1) has degree at most d = nγ and all edge weights are in [−1, 1]. The generative model works like a neural net where the threshold at every node6 is 0. The top layer h(`) is initialized with a 0/1 assignment where the set of nodes that are 1 is picked uniformly7 among all sets of size ρ`n. For i = ` down to 2, each node in layer i − 1 computes a weighted sum of its neighbors in layer i, and becomes 1 iff that sum strictly exceeds 0. We will use sgn(x) to denote the threshold function that is 1 if x > 0 and 0 else. (Applying sgn() to a vector involves applying it componentwise.) Thus the network computes as follows: h(i−1) = sgn(Gi−1h(i)) for all i > 0 and h(0) = G0h (1) (i.e., no threshold at the observed layer)8. Here Gi stands\n5When the layer sizes differ the sparsity of the layers are related by ρi+1di+1ni+1/2 = ρini. Nothing much else changes.\n6It is possible to allow these thresholds to be higher and to vary between the nodes, but the calculations are harder and the algorithm is much less efficient.\n7It is possible to prove the result when the top layer has not a random sparse vector and has some bounded correlations among them. This makes the algorithm more complicated.\n8 It is possible to stay with a generative deep model in which the last layer also has 0/1 values. Then our calculations require the fraction of 1’s in the lowermost (observed) layer to be at most 1/ log n. This could be an OK model if one assumes that some handcoded net (or a nonrandom layer like convolutional net) has been used on the real data to produce a sparse encoding, which is the bottom layer of our generative model.\nHowever, if one desires a generative model in which the observed layer is not sparse, then we can do this by allowing real-valued assignments at the observed layer, and remove the threshold gates there. This is the model described here.\nfor both the weighted bipartite graph at a level and its weight matrix.\nRandom deep net assumption: We assume that in this ground truth the edges between layers are chosen randomly subject to expected degree d being9 nγ, where γ < 1/(` + 1), and each edge e ∈ Ei carries a weight that is chosen randomly in [−1, 1]. This is our model R(`, ρl, {Gi}). We also consider —because it leads to a simpler and more efficient learner—a model where edge weights are random in {±1} instead of [−1, 1]; this is called D(`, ρ`, {Gi}). Recall that ρ` > 0 is such that the 0/1 vector input at the top layer has 1’s in a random subset of ρ`n nodes.\nIt can be seen that since the network is random of degree d, applying a ρ`n-sparse vector at the top layer is likely to produce the following density of 1’s (approximately) at the successive layers: ρ`d/2, ρ`(d/2)\n2, etc.. We assume the density of last layer ρ`d\n`/2` = O(1). This way the density at the last-but-one layer is o(1), and the last layer is real-valued and dense.\nNow we state our main result. Note that 1/ρ` is at most n.\nTheorem 1 When degree d = nγ for 0 < γ ≤ 0.2, density ρ`(d/2)l = C for some large constant C10, the network model D(`, ρ`, {Gi}) can be learnt using O(log n/ρ2`) samples and O(n2`) time. The network model R(`, ρ`, {Gi}) can be learnt in polynomial time and using O(n3`2 log n/η2) samples, where η is the statistical distance between the true distribution and that generated by the learnt model.\nAlgorithmic ideas. We are unable to analyse existing algorithms. Instead, we give new learning algorithms that exploit the very same structure that makes these random networks interesting in the first place i.e., each layer is a denoising autoencoder. The crux of the algorithm is a new twist on the old Hebbian rule [Heb49] that “Things that fire together wire together.” In the setting of layerwise learning, this is adapted as follows: “Nodes in the same layer that fire together a lot are likely to be connected\n9In the appendix we allow degrees to be different for different layers. 10In this case the output is dense\n(with positive weight) to the same node at the higher layer.” The algorithm consists of looking for such pairwise (or 3-wise) correlations and putting together this information globally. The global procedure boils down to the graph-theoretic problem of reconstructing a bipartite graph given pairs of nodes that are at distance 2 in it (see Section 6). This is a variant of the GRAPH SQUARE ROOT problem which is NP-complete on worst-case instances but solvable for sparse random (or random-like) graphs.\nNote that current algorithms (to the extent that they are Hebbian, roughly speaking) can also be seen as leveraging correlations. But putting together this information is done via the language of nonlinear optimization (i.e., an objective function with suitable penalty terms). Our ground truth network is indeed a particular local optimum in any reasonable formulation. It would be interesting to show that existing algorithms provably find the ground truth in polynomial time but currently this seems difficult.\nCan our new ideas be useful in practice? We think that using a global reconstruction procedure to leverage local correlations seems promising, especially if it avoids the usual nonlinear optimization. Our proof currently needs that the hidden layers are sparse, and the edge structure of the ground truth network is “random like”(in the sense that two distinct features at a level tend to affect fairly disjoint-ish sets of features at the next level). Finally, we note that random neural nets do seem useful in so-called reservoir computing, so perhaps they do provide useful representational power on real data. Such empirical study is left for future work.\nThroughout, we need well-known properties of random graphs with expected degree d, such as the fact that they are expanders; these properties appear in the appendix. The most important one, unique neighbors property, appears in the next Section."
    }, {
      "heading" : "3 Each layer is a Denoising Auto-encoder",
      "text" : "As mentioned earlier, modern deep nets research often assumes that the net (or at least some layers in it) should approximately preserve information, and even allows easy going back/forth between representations in two adjacent layers (what we earlier called “reversibility”). Below, y denotes the lower layer and h the higher (hidden) layer. Popular choices of s include logistic function, soft max, etc.; we use simple threshold function in our model.\nDefinition 1 (Denoising autoencoder) An autoencoder consists of a decoding function D(h) = s(Wh+b) and an encoding function E(y) = s(W ′y+b′) where W,W ′ are linear transformations, b, b′ are fixed vectors and s is a nonlinear function that acts identically on each coordinate. The autoencoder is denoising if E(D(h) + η) = h with high probability where h is drawn from the distribution of the hidden layer, η is a\nnoise vector drawn from the noise distribution, and D(h)+η is a shorthand for “D(h) corrupted with noise η.” The autoencoder is said to use weight tying if W ′ = W T .\nIn empirical work the denoising autoencoder property is only implicitly imposed on the deep net by minimizing the reconstruction error ||y−D(E(y+η))||, where η is the noise vector. Our definition is slightly different but is actually stronger since y is exactly D(h) according to the generative model. Our definition implies the existence of an encoder E that makes the penalty term exactly zero. We show that in our ground truth net (whether from model D(`, ρ`, {Gi}) or R(`, ρ`, {Gi})) every pair of successive levels whp satisfies this definition, and with weight-tying.\nWe show a single-layer random network is a denoising autoencoder if the input layer is a random ρn sparse vector, and the output layer has density ρd/2 < 1/20.\nLemma 2 If ρd < 0.1 (i.e., the assignment to the observed layer is also fairly sparse) then the single-layer network above is a denoising autoencoder with high probability (over the choice of the random graph and weights), where the noise distribution is allowed to flip every output bit independently with probability 0.1. It uses weight tying.\nThe proof of this lemma highly relies on a property of random graph, called the strong unique-neighbor property.\nFor any node u ∈ U and any subset S ⊂ U , let UF (u, S) be the sets of unique neighbors of u with respect to S,\nUF (u, S) , {v ∈ V : v ∈ F (u), v 6∈ F (S \\ {u})}\nProperty 1 In a bipartite graph G(U, V,E,w), a node u ∈ U has (1 − )-unique neighbor property with respect to S if\n∑\nv∈UF (u,S) |w(u, v)| ≥ (1− )\n∑\nv∈F (u) |w(u, v)| (1)\nThe set S has (1 − )-strong unique neighbor property if for every u ∈ U , u has (1− )-unique neighbor property with respect to S.\nWhen we just assume ρd n, this property does not hold for all sets of size ρn. However, for any fixed set S of size ρn, this property holds with high probability over the randomness of the graph.\nNow we sketch the proof for Lemma 2 (details are in Appendix).For convenience assume the edge weights are in {−1, 1}.\nFirst, the decoder definition is implicit in our generative model: y = sgn(Wh). (That is, b = ~0 in the autoencoder definition.) Let the encoder be E(y) = sgn(W Ty+\nb′) for b′ = 0.2d×~1.In other words, the same bipartite graph and different thresholds can transform an assignment on the lower level to the one at the higher level.\nTo prove this consider the strong unique-neighbor property of the network. For the set of nodes that are 1 at the higher level, a majority of their neighbors at the lower level are adjacent only to them and to no other nodes that are 1. The unique neighbors with a positive edge will always be 1 because there are no −1 edges that can cancel the +1 edge (similarly the unique neighbors with negative edge will always be 0). Thus by looking at the set of nodes that are 1 at the lower level, one can easily infer the correct 0/1 assignment to the higher level by doing a simple threshold of say 0.2d at each node in the higher layer."
    }, {
      "heading" : "4 Learning a single layer network",
      "text" : "Our algorithm, outlined below (Algorithm 1), learns the network layer by layer starting from the bottom. Thus the key step is that of learning a single layer network, which we now focus on.11 This step, as we noted, amounts to learning nonlinear dictionaries with random dictionary elements. The algorithm illustrates how we leverage the sparsity and the randomness of the support graph, and use pairwise or 3-wise correlations combined with our graph recovery procedure of Section 6. We first give a simple algorithm and then outline one that works with better parameters.\nAlgorithm 1. High Level Algorithm\nInput: samples y’s generated by a deep network described in Section 2 Output: the network/encoder and decoder functions\n1: for i = 1 TO l do 2: Construct correlation graph using samples of h(i−1) 3: Call RecoverGraph to learn the positive edges E+i 4: Use PartialEncoder to encode all h(i−1) to h(i) 5: Use LearnGraph/LearnDecoder to learn the graph/decoder between layer i and i− 1. 6: end for\nFor simplicity we describe the algorithm when edge weights are {−1, 1}, and sketch the differences for real-valued weights at the end of this section.\nThe hidden layer and observed layer each have n nodes, and the generative model assumes the assignment to the hidden layer is a random 0/1 assignment with ρn nonzeros.\nSay two nodes in the observed layer are related if they have a common neighbor in the hidden layer to which they are attached via a +1 edge.\n11Learning the bottom-most (real valued) layer is mildly different and is done in Section 7.\nStep 1: Construct correlation graph: This step is a new twist on the classical Hebbian rule (“things that fire together wire together”).\nAlgorithm 2. PairwiseGraph\nInput: N = O(log n/ρ) samples of y = sgn(Gh), Output: Ĝ on vertices V , u, v connected if related\nfor each u, v in the output layer do if ≥ ρN/3 samples have yu = yv = 1 then\nconnect u and v in Ĝ end if\nend for\nClaim In a random sample of the output layer, related pairs u, v are both 1 with probability at least 0.9ρ, while unrelated pairs are both 1 with probability at most (ρd)2. (Proof Sketch): First consider a related pair u, v, and let z be a vertex with +1 edges to u, v. Let S be the set of neighbors of u, v excluding z. The size of S cannot be much larger than 2d. Under the choice of parameters, we know ρd 1, so the event hS = ~0 conditioned on hz = 1 has probability at least 0.9. Hence the probability of u and v being both 1 is at least 0.9ρ. Conversely, if u, v are unrelated then for both u, v to be 1 there must be two different causes, namely, nodes y and z that are 1, and additionally, are connected to u and v respectively via +1 edges. The chance of such y, z existing in a random sparse assignment is at most (ρd)2 by union bound.\nThus, if ρ satisfies (ρd)2 < 0.1ρ, i.e., ρ < 0.1/d2, then using O(log n/ρ2) samples we can recover all related pairs whp, finishing the step. Step 2: Use graph recover procedure to find all edges that have weight +1. (See Section 6 for details.) Step 3: Using the +1 edges to encode all the samples y.\nAlgorithm 3. PartialEncoder Input: positive edges E+, y = sgn(Gh), threshold θ Output: the hidden variable h\nLet M be the indicator matrix of E+ (Mi,j = 1 iff (i, j) ∈ E+) return h = sgn(MTy − θ~1)\nAlthough we have only recovered the positive edges, we can use PartialEncoder algorithm to get h given y!\nLemma 3 If support of h satisfies 11/12-strong unique neighbor property, and y = sgn(Gh), then Algorithm 3 outputs h with θ = 0.3d.\nThis uses the unique neighbor property: for every z with hz = 1, it has at least 0.4d unique neighbors that are connected with +1 edges. All these neighbors must be 1 so [(E+)Ty]z ≥ 0.4d. On the other hand, for any z with hz = 0, the unique neighbor property (applied to supp(h) ∪ {z}) implies that z can have at most 0.2d positive edges to the +1’s in y. Hence h = sgn((E+)Ty − 0.3d~1). Step 4: Recover all weight −1 edges.\nAlgorithm 4. Learning Graph Input: positive edges E+, samples of (h, y) Output: E−\n1: R← (U × V ) \\ E+. 2: for each of the samples (h, y), and each v do 3: Let S be the support of h 4: if yv = 1 and S ∩B+(v) = {u} for some u then 5: for s ∈ S do 6: remove (s, v) from R. 7: end for 8: end if 9: end for\n10: return R\nNow consider many pairs of (h, y), where h is found using Step 3. Suppose in some sample, yu = 1 for some u, and exactly one neighbor of u in the +1 edge graph (which we know entirely) is in supp(h). Then we can conclude that for any z with hz = 1, there cannot be a −1 edge (z, u), as this would cancel out the unique +1 contribution.\nLemma 4 Given O(log n/(ρ2d)) samples of pairs (h, y), with high probability (over the random graph and the samples) Algorithm 4 outputs the correct set E−.\nTo prove this lemma, we just need to bound the probability of the following event for any non-edge (x, u): hx = 1, |supp(h) ∩B+(u)| = 1, supp(h)∩B−(u) = ∅ (B+, B− are positive and negative parents). These three events are almost independent, the first has probability ρ, second has probability ≈ ρd and the third has probability almost 1.\nLeveraging 3-wise correlation: The above sketch used pairwise correlations to recover the +1 weights when ρ < 1/d2, roughly. It turns out that using 3-wise correlations allow us to find correlations under a weaker requirement ρ < 1/d3/2. Now call three observed nodes u, v, s related if they are connected to a common node at the hidden layer via +1 edges. Then we can prove a claim analogous to the one above, which says that for a related triple, the probability that u, v, s are all 1 is at\nleast 0.9ρ, while the probability for unrelated triples is roughly at most (ρd)3. Thus as long as ρ < 0.1/d3/2, it is possible to find related triples correctly. The graph recover algorithm can be modified to run on 3-uniform hypergraph consisting of these related triples to recover the +1 edges.\nThe end result is the following theorem. This is the learner used to get the bounds stated in our main theorem.\nTheorem 5 Suppose our generative neural net model with weights {−1, 1} has a single layer and the assignment of the hidden layer is a random ρn-sparse vector, with ρ 1/d3/2. Then there is an algorithm that runs in O(n(d3 + n)) time and uses O(log n/ρ2) samples to recover the ground truth with high probability over the randomness of the graph and the samples.\nWhen weights are real numbers. We only sketch this and leave the details to the appendix. Surprisingly, steps 1, 2 and 3 still work. In the proofs, we have only used the sign of the edge weights – the magnitude of the edge weights can be arbitrary. This is because the proofs in these steps relies on the unique neighbor property, if some node is on (has value 1), then its unique positive neighbors at the next level will always be on, no matter how small the positive weights might be. Also notice in PartialEncoder we are only using the support of E+, but not the weights.\nAfter Step 3 we have turned the problem of unsupervised learning of the hidden graph to a supervised one in which the outputs are just linear classifiers over the inputs! Thus the weights on the edges can be learnt to any desired accuracy."
    }, {
      "heading" : "5 Correlations in a Multilayer Network",
      "text" : "We now consider multi-layer networks, and show how they can be learnt layerwise using a slight modification of our one-layer algorithm at each layer. At a technical level, the difficulty in the analysis is the following: in single-layer learning, we assumed that the higher layer’s assignment is a random ρn-sparse binary vector. In the multilayer network, the assignments in intermediate layers (except for the top layer) do not satisfy this, but we will show that the correlations among them are low enough that we can carry forth the argument. Again for simplicity we describe the algorithm for the model D(`, ρl, {Gi}), in which the edge weights are ±1. Also to keep notation simple, we describe how to bound the correlations in bottom-most layer (h(1)). It holds almost verbatim for the higher layers. We define ρi to be the “expected” number of 1s in the layer h(i). Because of the unique neighbor property, we expect roughly ρl(d/2) fraction of h\n(`−1) to be 1. So also, for subsequent layers, we obtain ρi = ρ` · (d/2)`−i. (We can also think of the above expression as defining ρi).\nLemma 6 Consider a network from D(`, ρl, {Gi}). With high probability (over the random graphs between layers) for any two nodes u, v in layer h(1),\nPr[h(1)u = h (1) v = 1] { ≥ ρ2/2 if u, v related ≤ ρ2/4 otherwise\nProof:(outline) The first step is to show that for a vertex u in level i, Pr[h(i)(u) = 1] is at least 3ρi/4 and at most 5ρi/4. This is shown by an inductive argument (details in the full version). (This is the step where we crucially use the randomness of the underlying graph.)\nNow suppose u, v have a common neighbor z with +1 edges to both of them. Consider the event that z is 1 and none of the neighbors of u, v with −1 weight edges are 1 in layer h(2). These conditions ensure that h(1)(u) = h(1)(v) = 1; further, they turn out to occur together with probability at least ρ2/2, because of the bound from the first step, along with the fact that u, v combined have only 2d neighbors (and 2dρ2n n), so there is good probability of not picking neighbors with −1 edges.\nIf u, v are not related, it turns out that the probability of interest is at most 2ρ21 plus a term which depends on whether u, v have a common parent in layer h(3) in the graph restricted to +1 edges. Intuitively, picking one of these common parents could result in u, v both being 1. By our choice of parameters, we will have ρ21 < ρ2/20, and also the additional term will be < ρ2/10, which implies the desired conclusion. 2\nThen as before, we can use graph recovery to find all the +1 edges in the graph at the bottom most layer. This can then be used (as in Step 3) in the single layer algorithm to encode h(1) and obtain values for h(2). Now as before, we have many pairs (h(2), h(1)), and thus using precisely the reasoning of Step 4 earlier, we can obtain the full graph at the bottom layer.\nThis argument can be repeated after ‘peeling off’ the bottom layer, thus allowing us to learn layer by layer."
    }, {
      "heading" : "6 Graph Recovery",
      "text" : "Graph reconstruction consists of recovering a graph given information about its subgraphs [BH77]. A prototypical problem is the Graph Square Root problem, which calls for recovering a graph given all pairs of nodes whose distance is at most 2. This is NP-hard.\nDefinition 2 (Graph Recovery) Let G1(U, V,E1) be an unknown random bipartite graph between |U | = n and |V | = n vertices where each edge is picked with probability d/n independently. Given: Graph G(V,E) where (v1, v2) ∈ E iff v1 and v2 share a common parent in G1 (i.e. ∃u ∈ U where (u, v1) ∈ E1 and (u, v2) ∈ E1). Goal: Find the bipartite graph G1.\nSome of our algorithms (using 3-wise correlations) need to solve analogous problem where we are given triples of nodes which are mutually at distance 2 from each other, which we will not detail for lack of space.\nWe let F (S) (resp. B(S)) denote the set of neighbors of S ⊆ U (resp. ⊆ V ) in G1. Also Γ(·) gives the set of neighbors in G. Now for the recovery algorithm to work, we need the following properties (all satisfied whp by random graph when d3/n 1):\n1. For any v1, v2 ∈ V , |(Γ(v1) ∩ Γ(v2))\\(F (B(v1) ∩B(v2)))| < d/20.\n2. For any u1, u2 ∈ U , |F (u1) ∪ F (u2)| > 1.5d.\n3. For any u ∈ U , v ∈ V and v 6∈ F (u), |Γ(v) ∩ F (u)| < d/20.\n4. For any u ∈ U , at least 0.1 fraction of pairs v1, v2 ∈ F (u) does not have a common neighbor other than u.\nThe first property says “most correlations are generated by common cause”: all but possibly d/20 of the common neighbors of v1 and v2 in G, are in fact neighbors of a common neighbor of v1 and v2 in G1.\nThe second property basically says the sets F (u)’s should be almost disjoint, this is clear because the sets are chosen at random.\nThe third property says if a vertex v is not related to the cause u, then it cannot have correlation with all many neighbors of u.\nThe fourth property says every cause introduces a significant number of correlations that is unique to that cause.\nIn fact, Properties 2-4 are closely related from the unique neighbor property.\nLemma 7 When graph G1 satisfies Properties 1-4, Algorithm 5 successfully recovers the graph G1 in expected time O(n 2).\nProof: We first show that when (v1, v2) has more than one unique common cause, then the condition in the if statement must be false. This follows from Property 2. We know the set S contains F (B(v1) ∩B(v2)). If |B(v1) ∩B(v2)| ≥ 2 then Property 2 says |S| ≥ 1.5d, which implies the condition in the if statement is false.\nThen we show if (v1, v2) has a unique common cause u, then S ′ will be equal to F (u). By Property 1, we know S = F (u) ∪ T where |T | ≤ d/20. For any vertex v in F (u), it is connected to every other vertex in F (u). Therefore |Γ(v) ∩ S| ≥ |Γ(v) ∩ F (u)| ≥ 0.8d− 1, and v must be in S ′. For any vertex v′ outside F (u), by Property 3 it can only be connected to d/20 vertices in F (u). Therefore |Γ(v) ∩ S| ≤ |Γ(v) ∩ F (u)|+ |T | ≤ d/10. Hence v′ is not in S ′.\nFollowing these arguments, S ′ must be equal to F (u), and the algorithm successfully learns the edges related to u.\nThe algorithm will successfully find all vertices u ∈ U because of Property 4: for every u there are enough number of edges in G that is only caused by u. When one of them is sampled, the algorithm successfully learns the vertex u.\nFinally we bound the running time. By Property 4 we know that the algorithm identifies a new vertex u ∈ U in at most 10 iterations in expectation. Each iteration takes at most O(n) time. Therefore the algorithm takes at most O(n2) time in expectation. 2\nAlgorithm 5. RecoverGraph\nInput: G given as in Definition 2 Output: Find the graph G1 as in Definition 2.\nrepeat Pick a random edge (v1, v2) ∈ E. Let S = {v : (v, v1), (v, v2) ∈ E}. if |S| < 1.3d then S ′ = {v ∈ S : |Γ(v) ∩ S| ≥ 0.8d− 1} {S ′ should be a clique in G} In G1, create a vertex u and connect u to every v ∈ S ′. Mark all the edges (v1, v2) for v1, v2 ∈ S ′.\nend if until all edges are marked"
    }, {
      "heading" : "7 Learning the lowermost (real-valued) layer",
      "text" : "Note that in our model, the lowest (observed) layer is real-valued and does not have threshold gates. Thus our earlier learning algorithm cannot be applied as is. However, we see that the same paradigm – identifying correlations and using Graph recover – can be used.\nThe first step is to show that for a random weighted graph G, the linear decoder D(h) = Gh and the encoder E(y) = sgn(GTy+ b) form a denoising autoencoder with real-valued outputs, as in Bengio et al. [BCV13].\nLemma 8 If G is a random weighted graph, the encoder E(y) = sgn(GTy − 0.4d~1) and linear decoder D(h) = Gh form a denoising autoencoder, for noise vectors γ which have independent components, each having variance at most O(d/ log2 n).\nThe next step is to show a bound on correlations as before. For simplicity we state it assuming the layer h(1) has a random 0/1 assignment of sparsity ρ1. In the full version we state it keeping in mind the higher layers, as we did in the previous sections.\nTheorem 9 When ρ1d = O(1), d = Ω(log\n2 n), with high probability over the choice of the weights and the choice of the graph, for any three nodes u, v, s the assignment y to the bottom layer satisfies:\n1. If u, v and s have no common neighbor, then |Eh[yuyvys]| ≤ ρ1/3\n2. If u, v and s have a unique common neighbor, then |Eh[yuyvys]| ≥ 2ρ1/3"
    }, {
      "heading" : "8 Two layers cannot be represented by one layer",
      "text" : "In this section we show that a two-layer network with ±1 weights is more expressive than one layer network with arbitrary weights. A two-layer network (G1, G2) consists of random graphs G1 and G2 with random ±1 weights on the edges. Viewed as a generative model, its input is h(3) and the output is h(1) = sgn(G1 sgn(G2h\n(3))). We will show that a single-layer network even with arbitrary weights and arbitrary threshold functions must generate a fairly different distribution.\nLemma 10 For almost all choices of (G1, G2), the following is true. For every one layer network with matrix A and vector b, if h(3) is chosen to be a random ρ3n-sparse vector with ρ3d2d1 1, the probability (over the choice of h(3)) is at least Ω(ρ23) that sgn(G1 sgn(G1h (3))) 6= sgn(Ah(3) + b).\nThe idea is that the cancellations possible in the two-layer network simply cannot all be accomodated in a single-layer network even using arbitrary weights. More precisely, even the bit at a single output node v cannot be well-represented by a simple threshold function.\nFirst, observe that the output at v is determined by values of d1d2 nodes at the top layer that are its ancestors. It is not hard to show in the one layer net (A, b), there should be no edge between v and any node u that is not its ancestor. Then consider structure in Figure 2. Assuming all other parents of v are 0 (which happen with probability at least 0.9), and focus on the values of (u1, u2, u3, u4). When these values are (1, 1, 0, 0) and (0, 0, 1, 1), v is off. When these values are (1, 0, 0, 1) and (0, 1, 1, 0), v is on. This is impossible for a one layer network because the first two ask for\n∑ Aui,v +2bv ≤ 0 and the second two ask for ∑ Aui,v +2bv < 0."
    }, {
      "heading" : "9 Conclusions",
      "text" : "Rigorous analysis of interesting subcases of any ML problem can be beneficial for triggering further improvements: see e.g., the role played in Bayes nets by the rigorous analysis of message-passing algorithms for trees and graphs of low tree-width. This is the spirit in which to view our consideration of a random neural net model (though\nnote that there is some empirical work in reservoir computing using randomly wired neural nets).\nThe concept of a denoising autoencoder (with weight tying) suggests to us a graph with random-like properties. We would be very interested in an empirical study of the randomness properties of actual deep nets learnt in real life. (For example, in [KSH12] some of the layers use convolution, which is decidedly nonrandom. But other layers do backpropagation starting with a complete graph and may end up more random-like.)\nNetwork randomness is not so crucial for single-layer learning. But for provable layerwise learning we rely on the support (i.e., nonzero edges) being random: this is crucial for controlling (i.e., upper bounding) correlations among features appearing in the same hidden layer (see Lemma 6). Provable layerwise learning under weaker assumptions would be very interesting."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Yann LeCun, Ankur Moitra, Sushant Sachdeva, Linpeng Tang for numerous helpful discussions throughout various stages of this work. This work was done when the first, third and fourth authors were visiting EPFL."
    }, {
      "heading" : "Provable Bounds for Learning Some Deep",
      "text" : "Representations: Long Technical Appendix∗\nSanjeev Arora Aditya Bhaskara Rong Ge Tengyu Ma"
    }, {
      "heading" : "A Preliminaries and Notations",
      "text" : "Here we describe the class of randomly chosen neural nets that are learned by our algorithm. A networkR(`, ρl, {Gi}) has ` hidden layers of binary variables h(`), h(`−1), .., h(1) from top to bottom and an observed layer x at bottom. The set of nodes at layer h(i) is denoted by Ni, and |Ni| = ni. For simplicity of analysis, let n = maxi ni, and assume each ni > n c for some positive constant c.\nThe edges between layers i and i − 1 are assumed to be chosen according to a random bipartite graph Gi(Ni+1, Ni, Ei, w) that includes every pair (u, v) ∈ Ni+1×Ni in Ei with probability pi. We denote this distribution by Gni+1,ni,pi . Each edge e ∈ Ei\n∗This appendix is self-contained in terms of technicality, though the readers are encouraged to read the extended abstract first, which contains abstract, introduction, reference, etc. Also note that the notations, numbering in this appendix are also independent with the extended abstract.\ncarries a weight w(e) in [−1, 1] that is randomly chosen.The set of positive edges are denoted by E+i = {(u, v) ∈ Ni+1 × Ni : w(u, v) > 0}. Define E− to be the negative edges similarly. Denote by G+ and G− the corresponding graphs defined by E+ and E−, respectively.\nThe generative model works like a neural net where the threshold at every node is 0. The top layer h(`) is initialized a 0/1 assignment where the set of nodes that are 1 is picked uniformly among all sets of size ρlnl. Each node in layer `− 1 computes a weighted sum of its neighbors in layer `, and becomes 1 iff that sum strictly exceeds 0. We will use sgn(·) to denote the threshold function:\nsgn(x) = 1 if x > 0 and 0 else. (1)\nApplying sgn() to a vector involves applying it componentwise. Thus the network computes as follows: h(i−1) = sgn(Gi−1h(i)) for all i > 0 and h(0) = G0h(1) (i.e., no threshold at the observed layer)1. Here (with slight abuse of notation) Gi stands for both the bipartite graph and the bipartite weight matrix of the graph at layer i.\nWe also consider a simpler case when the edge weights are in {±1} instead of [−1, 1]. We call such a network D(`, ρl, {Gi}).\nThroughout this paper, by saying “with high probability” we mean the probability is at least 1−n−C for some large constant C. Moreover, f g means f ≥ Cg , f g means f ≤ g/C for large enough constant C (the constant required is determined implicitly by the related proofs).\nMore network notations. The expected degree from Ni to Ni+1 is di, that is, di , pi|Ni+1| = pini+1, and the expected degree from Ni+1 to Ni is denoted by d′i , pi|Ni| = pini. The set of forward neighbors of u ∈ Ni+1 in graph Gi is denoted by Fi(u) = {v ∈ Ni : (u, v) ∈ Ei}, and the set of backward neighbors of v ∈ Ni in Gi is denoted by Bi(v) = {u ∈ Ni+1 : (u, v) ∈ Ei}. We use F+i (u) to denote the positive neighbors: F+i (u) , {v, : (u, v) ∈ E+i } (and similarly for B+i (v)). The expected density of the layers are defined as ρi−1 = ρidi−1/2 (ρ` is given as a parameter of the model).\nOur analysis works while allowing network layers of different sizes and different degrees. For simplicity, we recommend first-time readers to assume all the ni’s are equal, and di = d ′ i for all layers.\nBasic facts about random graphs We will assume that in our random graphs the expected degree d′, d log n so that most events of interest to us that happen in expectation actually happen with high probability (see Appendix J): e.g., all hidden nodes have backdegree d±√d log n. Of particular interest will be the fact (used often\n1We can also allow the observed layer to also use threshold but then our proof requires the output vector to be somewhat sparse. This could be meaningful in modeling practical settings where each datapoint has been represented as a somewhat sparse 0/1 vector via a sparse coding algorithm.\nin theoretical computer science) that random bipartite graphs have a unique neighbor property. This means that every set of nodes S on one layer has |S| (d′ ± o(d′)) neighbors on the neighboring layer provided |S| d′ n, which implies in particular that most of these neighboring nodes are adjacent to exactly one node in S: these are called unique neighbors. We will need a stronger version of unique neighbors property which doesn’t hold for all sets but holds for every set with probability at least 1 − exp(−d′) (over the choice of the graph). It says that every node that is not in S shares at most (say) 0.1d′ neighbors with any node in S. This is crucial for showing that each layer is a denoising autoencoder."
    }, {
      "heading" : "B Main Results",
      "text" : "In this paper, we give an algorithm that learns a random deep neural network.\nTheorem 1 For a network D(`, ρl, {Gi}), if all graphs Gi’s are chosen according to Gni+1,ni,pi , and the parameters satisfy:\n1. All di log2 n, d′i log2 n.\n2. For all but last layer (i ≥ 1), ρ3i ρi+1.\n3. For all layers, n3i (d ′ i−1) 8/n8i−1 1.\n4. For last layer, ρ1d0 = O(1), d 3/2 0 /d1d2 < O(log −3/2 n), √ d0/d1 < O(log\n−3/2 n), d51 < n, d0 log3 n.\nThen there is an algorithm usingO(log n/ρ2`) samples, running in timeO( ∑` i=1 ni((d ′ i)\n3+ ni−1)) that learns the network with high probability on both the graph and the samples.\nRemark 1 We include the last layer whose output is real instead of 0/1, in order to get fully dense outputs. We can also learn a network without this layer, in which case the last layer needs to have density at most 1/poly log(n), and condition 4 is no long needed.\nRemark 2 If a stronger version of condition 2, ρ2i ρi+1 holds, there is a faster and simpler algorithm that runs in time O(n2).\nAlthough we assume each layer of the network is a random graph, we are not using all the properties of the random graph. The properties of random graphs we need are listed in Section J.\nWe can also learn a network even if the weights are not discrete.\nTheorem 2 For a network R(`, ρl, {Gi}), if all graphs Gi’s are chosen according to Gni+1,ni,pi , and the parameters satisfy the same conditions as in Theorem 1, there is an algorithm using O(n2l nl−1l\n2 log n/η2) samples, running in time poly(n) that learns a network R′(`, ρl, {G′i}). The observed vectors of network R′ agrees with R(`, ρl, {Gi}) on (1− η) fraction of the hidden variable h(l)."
    }, {
      "heading" : "C Each layer is a Denoising Auto-encoder",
      "text" : "Experts feel that deep networks satisfy some intuitive properties. First, intermediate layers in a deep representation should approximately preserve the useful information in the input layer. Next, it should be possible to go back/forth easily between the representations in two successive layers, and in fact they should be able to use the neural net itself to do so. Finally, this process of translating between layers should be noise-stable to small amounts of random noise. All this was implicit in the early work on RBM and made explicit in the paper of Vincent et al. [VLBM08] on denoising autoencoders. For a theoretical justification of the notion of a denoising autoencoder based upon the ”manifold assumption” of machine learning see the survey of Bengio [Ben09].\nDefinition 1 (Denoising autoencoder) An autoencoder consists of an decoding function D(h) = s(Wh + b) and a encoding function E(y) = s(W ′y + b′) where W,W ′ are linear transformations, b, b′ are fixed vectors and s is a nonlinear function that acts identically on each coordinate. The autoencoder is denoising if E(D(h)+η) = h with high probability where h is drawn from the input distribution, η is a noise vector drawn from the noise distribution, and D(h) + η is a shorthand for “E(h) corrupted with noise η.” The autoencoder is said to use weight tying if W ′ = W T .\nThe popular choices of s includes logistic function, soft max, etc. In this work we choose s to be a simple threshold on each coordinate (i.e., the test > 0, this can be viewed as an extreme case of logistic function). Weight tying is a popular constraint and is implicit in RBMs. Our work also satisfies weight tying.\nIn empirical work the denoising autoencoder property is only implicitly imposed on the deep net by minimizing the reconstruction error ||y − D(E(ỹ))||, where ỹ is a corrupted version of y; our definition is very similar in spirit that it also enforces the noise-stability of the autoencoder in a stronger sense. It actually implies that the reconstruction error corresponds to the noise from ỹ, which is indeed small. We show that in our ground truth net (whether from model D(`, ρ`, {Gi}) or R(`, ρ`, {Gi})) every pair of successive levels whp satisfies this definition, and with weight-tying.\nWe will show that each layer of our network is a denoising autoencoder with very high probability. (Each layer can also be viewed as an RBM with an additional energy term to ensure sparsity of h.) Later we will of course give efficient algorithms to learn\nsuch networks without recoursing to local search. In this section we just prove they satisfy Definition 1.\nThe single layer has m hidden and n output (observed) nodes. The connection graph between them is picked randomly by selecting each edge independently with probability p and putting a random weight on it in [−1, 1]. Then the linear transformation W corresponds simply to this matrix of weights. In our autoencoder we set b = ~0 and b′ = 0.2d′ × ~1, where d′ = pn is the expected degree of the random graph on the hidden side. (By simple Chernoff bounds, every node has degree very close to d′.) The hidden layer h has the following prior: it is given a 0/1 assignment that is 1 on a random subset of hidden nodes of size ρm. This means the number of nodes in the output layer that are 1 is at most ρmd′ = ρnd, where d = pm is the expected degree on the observed side. We will see that since b = ~0 the number of nodes that are 1 in the output layer is close to ρmd′/2.\nLemma 3 If ρmd′ < 0.05n (i.e., the assignment to the observed layer is also fairly sparse) then the single-layer network above is a denoising autoencoder with high probability (over the choice of the random graph and weights), where the noise distribution is allowed to flip every output bit independently with probability 0.01.\nRemark: The parameters accord with the usual intuition that the information content must decrease when going from observed layer to hidden layer. Proof: By definition, D(h) = sgn(Wh). Let’s understand what D(h) looks like. If S is the subset of nodes in the hidden layer that are 1 in h, then the unique neighbor property (Corollary 30) implies that (i) With high probability each node u in S has at least 0.9d′ neighboring nodes in the observed layer that are neighbors to no other node in S. Furthermore, at least 0.44d′ of these are connected to u by a positive edge and 0.44d′ are connected by a negative edge. All 0.44d′ of the former nodes must therefore have a value 1 in D(h). Furthermore, it is also true that the total weight of these 0.44d′ positive edges is at least 0.21d′. (ii) Each v not in S has at most 0.1d′ neighbors that are also neighbors of any node in S. Now let’s understand the encoder, specifically, E(D(h)). It assigns 1 to a node in the hidden layer iff the weighted sum of all nodes adjacent to it is at least 0.2d′. By (i), every node in S must be set to 1 in E(D(h)) and no node in S is set to 1. Thus E(D(h)) = h for most h’s and we have shown that the autoencoder works correctly. Furthermore, there is enough margin that the decoding stays stable when we flip 0.01 fraction of bits in the observed layer. 2"
    }, {
      "heading" : "D Learning a single layer network",
      "text" : "We first consider the question of learning a single layer network, which as noted amounts to learning nonlinear dictionaries. It perfectly illustrates how we leverage the sparsity and the randomness of the support graph.\nThe overall algorithm is illustrated in Algorithm 1.\nAlgorithm 1. High Level Algorithm\nInput: samples y’s generated by a deep network described in Section A Output: Output the network/encoder and decoder functions\n1: for i = 1 TO l do 2: Call LastLayerGraph/PairwiseGraph/3-Wise Graph on h(i−1) to construct the correlation structure 3: Call RecoverGraphLast/RecoverGraph/RecoverGraph3Wise to learn the positive edges E+i 4: Use PartialEncoder to encode all h(i−1) to h(i) 5: Call LearnGraph/LearnDecoder to learn the graph/decoder between layer i and i− 1. 6: end for\nIn Section D.1.1 we start with the simplest subcase: all edge weights are 1 (nonedges may be seen as 0-weight edges). First we show how to use pairwise or 3-wise correlations of the observed variables to figure out which pairs/triples “wire together”(i.e., share a common neighbor in the hidden layer). Then the correlation structure is used by the Graph Recovery procedure (described later in Section F) to learn the support of the graph.\nIn Section D.1.2 we show how to generalize these ideas to learn single-layer networks with both positive and negative edge weights.\nIn Section D.2 we show it is possible to do encoding even when we only know the support of positive edges. The result there is general and works in the multi-layer setting.\nFinally we give a simple algorithm for learning the negative edges when the edge weights are in {±1}. This algorithm needs to be generalized and modified if we are working with multiple layers or real weights, see Section G for details.\nD.1 Hebbian rule: Correlation implies common cause"
    }, {
      "heading" : "D.1.1 Warm up: 0/1 weights",
      "text" : "In this part we work with the simplest setting: a single level network with m hidden nodes, n observed nodes, and a random (but unknown) bipartite graph G(U, V,E) connecting them where each observed node has expected backdegree degree d. All edge weights are 1, so learning G is equivalent to finding the edges. Recall that we denote the hidden variables by h (see Figure 2) and the observed variables by y, and the neural network implies y = sgn(Gh).\nAlso, recall that h is chosen uniformly at random among vectors with ρm 1’s. The vector is sparse enough so that ρd 1.\nAlgorithm 2. PairwiseGraph\nInput: N = O(log n/ρ) samples of y = sgn(Gh), where h is unknown and chosen from uniform ρm-sparse distribution Output: Graph Ĝ on vertices V , u, v are connected if u, v share a positive neighbor in G for each u, v in the output layer do\nif there are at least 2ρN/3 samples of y satisfying both u and v are fired then connect u and v in Ĝ\nend if end for\nThe learning algorithm requires the unknown graph to satisfy some properties that hold for random graphs with high probability. We summarize these properties as Psing and Psing+, see Section J. Theorem 4 Let G be a random graph satisfying properties Psing. Suppose ρ 1/d2, with high probability over the samples, Algorithm 2 construct a graph Ĝ, where u, v are connected in Ĝ iff they have a common neighbor in G.\nAs mentioned, the crux of the algorithm is to compute the correlations between observed variables. The following lemma shows pairs of variables with a common parent fire together (i.e., both get value 1) more frequently than a typical pair. Let ρy = ρd be the approximate expected density of output layer.\nLemma 5 Under the assumptions of Theorem 4, if two observed nodes u, v have a common neighbor in the hidden layer then\nPr h\n[yu = 1, yv = 1] ≥ ρ\notherwise, Pr h [yu = 1, yv = 1] ≤ 3ρ2y\nProof: When u and v has a common neighbor z in the input layer, as long as z is fired both u and v are fired. Thus Pr[yu = 1, yv = 1] ≥ Pr[hz = 1] = ρ.\nOn the other hand, suppose the neighbor of u (B(u)) and the neighbors of v (B(v)) are disjoint. Since yu = 1 only if the support of h intersect with the neighbors of u, we have Pr[yu = 1] = Pr[supp(h) ∩ B(u) 6= ∅]. Similarly, we know Pr[yu = 1, yv = 1] = Pr[supp(h) ∩B(u) 6= ∅, supp(h) ∩B(v) 6= ∅].\nNote that under assumptions Psing B(u) andB(v) have size at most 1.1d. Lemma 38 implies Pr[supp(h) ∩B(u) 6= ∅, supp(h) ∩B(u) 6= ∅] ≤ 2ρ2|B(u)| · |B(v)| ≤ 3ρ2y. 2\nThe lemma implies that when ρ2y ρ(which is equivalent to ρ 1/(d2)), we can find pairs of nodes with common neighbors by estimating the probability that they are both 1.\nIn order to prove Theorem 4 from Lemma 5, note that we just need to estimate the probability Pr[yu = yv = 1] up to accuracy ρ/4, which by Chernoff bounds can be done using by O(log n/ρ2) samples.\nAlgorithm 3. 3-WiseGraph\nInput: N = O(log n/ρ) samples of y = sgn(Gh), where h is unknown and chosen from uniform ρm-sparse distribution Output: Hypergraph Ĝ on vertices V . {u, v, s} is an edge if and only if they share a positive neighbor in G for each u, v, s in the observed layer of y do\nif there are at least 2ρN/3 samples of y satisfying all u, v and s are fired then add {u, v, s} as an hyperedge for Ĝ\nend if end for\nThe assumption that ρ 1/d2 may seem very strong, but it can be weakened using higher order correlations. In the following Lemma we show how 3-wise correlation works when ρ d−3/2. Lemma 6 For any u, v, s in the observed layer,\n1. Prh[yu = yv = ys = 1] ≥ ρ, if u, v, s have a common neighbor\n2. Prh[yu = yv = ys = 1] ≤ 3ρ3y + 50ρyρ otherwise.\nProof: The proof is very similar to the proof of Lemma 5. If u,v and s have a common neighbor z, then with probability ρ, z is fired and so are u, v and s. On the other hand, if they don’t share a common neighbor, then Prh[u, v, s are all fired] = Pr[supp(h) intersects with B(u), B(v), B(s)]. Since the graph has property Psing+, B(u), B(v), B(s) satisfy the condition of Lemma 40, and thus we have that Prh[u, v, s are all fired] ≤ 3ρ3y + 50ρyρ. 2\nD.1.2 General case: finding common positive neighbors\nIn this part we show that Algorithm 3 still works even if there are negative edges. The setting is similar to the previous parts, except that the edges now have a random weights. We will only be interested in the sign of the weights, so without loss of generality we assume the nonzero weights are chosen from {±1} uniformly at random. All results still hold when the weights are uniformly random in [−1, 1].\nA difference in notation here is ρy = ρd/2. This is because only half of the edges have positive weights. We expect the observed layer to have “positive” density ρy when the hidden layer has density ρ.\nThe idea is similar as before. The correlation Pr[yu = 1, yv = 1, ys = 1] will be higher for u, v, s with a common positive cause; this allows us to identify the +1 edges in G.\nRecall that we say z is a positive neighbor of u if (z, u) is an +1 edge, the set of positive neighbors are F+(z) and B+(u).\nWe have a counterpart of Lemma 6 for general weights.\nLemma 7 When the graph G satisfies properties Psing and Psing+ and when ρy 1, for any u, v, s in the observed layer,\n1. Prh[yu = yv = ys = 1] ≥ ρ/2, if u, v, s have a common positive neighbor\n2. Prh[yu = yv = ys = 1] ≤ 3ρ3y + 50ρyρ, otherwise.\nProof: The proof is similar to the proof of Lemma 6. First, when u, v, s have a common positive neighbor z, let U be the neighbors of u, v, s except z, that is, U = B(u) ∪ B(v) ∪ B(s) \\ {z}. By property Psing, we know the size of U is at most 3.3d, and with at least 1 − 3.3ρd ≥ 0.9 probability, none of them is fired. When this happens (supp(h) ∩ U = ∅), the remaining entries in h are still uniformly random ρm sparse. Hence Pr[hz = 1| supp(h) ∩ U = ∅] ≥ ρ. Observe that u, v, s must all be fired if supp(h) ∩ U = ∅ and hz = 1, therefore we know\nPr h\n[yu = yv = ys = 1] ≥ Pr[supp(h) ∩ U = ∅] Pr[hz = 1| supp(h) ∩ U = ∅] ≥ 0.9ρ.\nOn the other hand, if u, v and s don’t have a positive common neighbor, then we have Prh[u, v, s are all fired] ≤ Pr[supp(h) intersects with B+(u), B+(v), B+(s)]. Again by Lemma 40 and Property Pmul+, we have Prh[yu = yv = ys = 1] ≤ 3ρ3y+50ρyρ 2"
    }, {
      "heading" : "D.2 Paritial Encoder: Finding h given y",
      "text" : "Suppose we have a graph generated as described earlier, and that we have found all the positive edges (denoted E+). Then, given y = sgn(Gh), we show how to recover h\nas long as it possesses a “strong” unique neighbor property (definition to come). The recovery procedure is very similar to the encoding function E(·) of the autoencoder (see Section C) with graph E+.\nConsider a bipartite graph G(U, V,E). An S ⊆ U is said to have the (1− )-strong unique neighbor property if for each u ∈ S, (1− ) fraction of its neighbors are unique neighbors with respect to S. Further, if u 6∈ S, we require that |F+(u)∩F+(S)| < d′/4. Not all sets of size o(n/d′) in a random bipartite graph have this property. However, most sets of size o(n/d′) have this property. Indeed, if we sample polynomially many S, we will not, with high probability, see any sets which do not satisfy this property. See Property 1 in Appendix J for more on this.\nHow does this property help? If u ∈ S, since most of F (u) are unique neighbors, so are most of F+(u), thus they will all be fired. Further, if u 6∈ S, less than d′/4 of the positive neighbors will be fired w.h.p. Thus if d′/3 of the positive neighbors of u are on, we can be sure (with failure probability exp−Ω(d\n′) in case we chose a bad S), that u ∈ S. Formally, the algorithm is simply (with θ = 0.3d′):\nAlgorithm 4. PartialEncoder Input: positive edges E+, sample y = sgn(Gh), threshold θ Output: the hidden variable h\nreturn h = sgn((E+)Ty − θ~1)\nLemma 8 If the support of vector h has the 11/12-strong unique neighbor property in G, then Algorithm 4 returns h given input E+ and y = sgn(Gh).\nProof: As we saw above, if u ∈ S, at most d′/6 of its neighbors (in particular that many of its positive neighbors) can be shared with other vertices in S. Thus u has at least (0.3)d′ unique positive neighbors (since u has d(1 ± d−1/2) positive neighbors), and these are all “on”.\nNow if u 6∈ S, it can have an intersection at most d′/4 with F (S) (by the definition of strong unique neighbors), thus there cannot be (0.3)d′ of its neighbors that are 1. 2\nRemark 3 Notice that Lemma 8 only depends on the unique neighbor property, which holds for the support of any vector h with high probability over the randomness of the graph. Therefore this ParitialEncoder can be used even when we are learning the layers of deep network (and h is not a uniformly random sparse vector). Also the proof only depends on the sign of the edges, so the same encoder works when the weights are random in {±1} or [−1, 1]."
    }, {
      "heading" : "D.3 Learning the Graph: Finding −1 edges.",
      "text" : "Now that we can find h given y, the idea is to use many such pairs (h, y) and the partial graph E+ to determine all the non-edges (i.e., edges of 0 weight) of the graph. Since we know all the +1 edges, we can thus find all the −1 edges.\nConsider some sample (h, y), and suppose yv = 1, for some output v. Now suppose we knew that precisely one element of B+(v) is 1 in h (recall: B+ denotes the back edges with weight +1). Note that this is a condition we can verify, since we know both h and E+. In this case, it must be that there is no edge between v and S \\B+, since if there had been an edge, it must be with weight −1, in which case it would cancel out the contribution of +1 from the B+. Thus we ended up “discovering” that there is no edge between v and several vertices in the hidden layer.\nWe now claim that observing polynomially many samples (h, y) and using the above argument, we can discover every non-edge in the graph. Thus the complement is precisely the support of the graph, which in turn lets us find all the −1 edges.\nAlgorithm 5. Learning Graph Input: positive edges E+, samples of (h, y), where h is from uniform ρm-sparse distribution, and y = sgn(Gh) Output: E−\n1: R← (U × V ) \\ E+. 2: for each of the samples (h, y), and each v do 3: Let S be the support of h 4: if yv = 1 and S ∩B+(v) = {u} for some u then 5: for s ∈ S do 6: remove (s, v) from R. 7: end for 8: end if 9: end for\n10: return R\nNote that the algorithm R maintains a set of candidate E−, which it initializes to (U × V ) \\ E+, and then removes all the non-edges it finds (using the argument above). The main lemma is now the following.\nLemma 9 Suppose we have N = O(log n/(ρ2d)) samples (h, y) with uniform ρm-sparse h, and y = sgn(Gh). Then with high probability over choice of the samples, Algorithm 5 outputs the set E−.\nThe lemma follows from the following proposition, which says that the probability that a non-edge (z, u) is identified by one sample (h, y) is at least ρ2d/3. Thus the probability that it is not identified after O(log n/(ρ2d)) samples is < 1/nC . All nonedges must be found with high probability by union bound.\nProposition 10 Let (z, u) be a non-edge, then with probability at least ρ2d/3 over the choice of samples, all of the followings hold: 1. hz = 1, 2. |B+(u) ∩ supp(h)| = 1, 3. |B−(u) ∩ supp(h)| = 0.\nIf such (h, y) is one of the samples we consider, (z, u) will be removed from R by Algorithm 5.\nProof: The latter part of the proposition follows from the description of the algorithm. Hence we only need to bound the probability of the three events.\nEvent 1 (hz = 1) happens with probability ρ by the distribution on h. Conditioning on 1, the distribution of h is still ρm− 1 uniform sparse on m− 1 nodes. By Lemma 39, we have that Pr[Event 2 and 3 | Event 1] ≥ ρ|B+(u)|/2 ≥ ρd/3. Thus all three events happen with at least ρ2d/3 probability. 2"
    }, {
      "heading" : "E Correlations in a Multilayer Network",
      "text" : "We show in this section that Algorithm PairwiseGraph/3-WiseGraph also work in the multi-layer setting. Consider graph Gi in this case, the hidden layer h\n(i+1) is no longer uniformly random ρi+1 sparse unless i + 1 = `.\n2 In particular, the pairwise correlations can be as large as ρi+2, instead of ρ 2 i+1. The key idea here is that although the maximum correlation between two nodes in z, t in layer h(i+1) can be large, there are only a few pairs with such high correlation. Since the graph Gi is random and independent of the upper layers, we don’t expect to see a lot of such pairs in the neighbors of u, v in h(i).\nWe make this intuition formal in the following Theorem:\nTheorem 11 For any 1 ≤ i ≤ ` − 1, and if the network satisfies Property Pmul+ with parameters ρ3i+1 ρi, then given O(log n/ρi+1) samples, Algorithm 3 3-WiseGraph constructs a hypergraph Ĝ, where (u, v, s) is an edge if and only if they share a positive neighbor in Gi.\nLemma 12 Flor any i ≤ `− 1 and any u, v, s in the layer of h(i) , if they have a common positive neighbor(parent) in layer of h(i+1)\nPr[h(i)u = h (i) v = h (i) s = 1] ≥ ρi+1/3,\notherwise Pr[h(i)u = h (i) v = h (i) s = 1] ≤ 2ρ3i + 0.2ρi+1\n2Recall that ρi = ρi+1di/2 is the expected density of layer i.\nProof: Consider first the case when u, v and s have a common positive neighbor z in the layer of h(i+1). Similar to the proof of Lemma 7, when h (i+1) z = 1 and none of other neighbors of u, v and s in the layer of h(i+1) is fired, we know h (i) u = h (i) v = h (i) s = 1. However, since the distribution of h(i+1) is not uniformly sparse anymore, we cannot simply calculate the probability of this event.\nIn order to solve this problem, we go all the way back to the top layer. Let S = supp(h(`)), and let event E1 be the event that S ∩B(`)+ (u)∩B(`)+ (v)∩B(`)+ (s) 6= ∅, and S ∩ (B(`)(u)∪B(`)(v)∪B(`)(s)) = S ∩B(`)+ (u)∩B(`)+ (v)∩B(`)+ (s) (that is, S does not intersect at any other places except B\n(`) + (u)∩B(`)+ (v)∩B(`)+ (s)). By the argument\nabove we know E1 implies h (i) u = h (i) v = h (i) s = 1.\nNow we try to bound the probability of E1. Intuitively, B (`) + (u)∩B(`)+ (v)∩B(`)+ (s)\ncontains B (`) + (z), which is of size roughly di+1 . . . d`−1/2 `−i−1 = ρi+1/ρ`. On the other hand, B(`)(u) ∪ B(`)(v) ∪ B(`)(s) is of size roughly 3di . . . d`−1 ≈ 2`−iρi/ρ`. These numbers are still considerably smaller than 1/ρ` due to our assumption on the sparsity of layers (ρi 1). Thus by applying Lemma 39 with T1 = B(`)+ (z) and T2 = B(`)(u)∪ B(`)(v) ∪B(`)(s), we have\nPr[h(i)u = h (i) v = h (i) s = 1] ≥ Pr[S ∩ T1 6= ∅, S ∩ (T2 − T1) = ∅] ≥ ρ`|T1|/2 ≥ ρi+1/3,\nthe last inequality comes from Property Pmul. On the other hand, if u, v and s don’t have a common positive neighbor in layer of h(i+1), consider event E2: S intersects each of B (`) + (u), B (`) + (v), B (`) + (s). Clearly, the target probability can be upperbounded by the probability of E2. By the graph properties we know each of the sets B\n(`) + (u), B (`) + (v), B (`) + (s) has size at\nmost A = 1.2di . . . d`−1/2`−i = 1.2ρi/ρ`. Also, we can bound the size of their inter-\nsections by graph property Pmul and Pmul+: ∣∣∣B(`)+ (u) ∩B(`)+ (v) ∣∣∣ ≤ B = 10ρi+1/ρ`,∣∣∣B(`)+ (u) ∩B(`)+ (v) ∩B(`)+ (s) ∣∣∣ ≤ C = 0.1ρi+1/ρ`. Applying Lemma 40 with these bounds, we have\nPr[E2] ≤ ρ3`A3 + 3ρ2`AB + ρ`C ≤ 2ρ3i + 0.2ρi+1,\n2"
    }, {
      "heading" : "F Graph Recovery",
      "text" : "Graph reconstruction consists of recovering a graph given information about its subgraphs.A prototypical problem is the Graph Square Root problem, which calls for recovering a graph given all pairs of nodes whose distance is at most 2. This is NPhard. Our setting is a subcase of Graph Square root, whereby there is an unknown bipartite graph and we are told for all pairs of nodes on one side whether they have distance 2. This is also NP-hard in general but is solvable when the bipartite graph\nis random or “random-like”. Recall that we apply this algorithm to all positive edges between the hidden and observed layer.\nDefinition 2 (Graph Recovery Problem) There is an unknown random bipartite graph G1(U, V,E1) between |U | = m and |V | = n vertices. Every edge is chosen with probability d′/n. Given: Graph Ĝ(V,E) where (v1, v2) ∈ E iff v1 and v2 share a common parent in G1 (i.e. ∃u ∈ U where (u, v1) ∈ E1 and (u, v2) ∈ E1). Goal: Find the bipartite graph G1.\nSince U and V are just the last two layers in the deep network, we adapt the notations in previous sections. We use F (u) to denote the forward neighbors {v ∈ V : (u, v) ∈ E1} and B(v) to denote the backward neighbors {u ∈ U : (u, v) ∈ E1}. For sets of vertices, let F (S) = ∪u∈SF (u) and B(S) = ∪v∈SB(v). We use Γ(v) to denote the neighbors of V in Ĝ. In particular, the construction of graph Ĝ implies Γ(v) = F (B(v)).\nNotice that the graph Ĝ is the union of cliques on F (u)’s. This problem can be thought of as a “community finding” problem: vertices in U are communities and vertices in V are people; two people are connected if they are in the same community.\nWe shall give an algorithm that works when m2d′3/n3 1 and show how to improve the guarantee using three-wise correlations."
    }, {
      "heading" : "F.1 Graph Recovery from Pairwise Correlations",
      "text" : "Algorithm 6. RecoverGraph\nInput: Ĝ given as in Definition 2 Output: Find the graph G1 as in Definition 2.\nrepeat Pick a random edge (v1, v2) ∈ E. Let S = {v : (v, v1), (v, v2) ∈ E}. if |S| < 1.3d′ then S ′ = {v ∈ S : |Γ(v) ∩ S| ≥ 0.8d′ − 1} {S ′ should be a clique in Ĝ} In G1, create a vertex u and connect u to every v ∈ S ′. Mark all the edges (v1, v2) for v1, v2 ∈ S ′.\nend if until all edges are marked\nThe idea of the algorithm is simple: since the graph G1 is randomly generated, and the size of neighborhoods are small, the communities should have very small intersections. In particular, if we pick two random vertices within a community, the intersection of their neighborhoods will almost be equal to this community.\nThe algorithm uses this idea to find most vertices S of the community that both v1 and v2 belong to, and applies a further refinement to get S\n′. By properties of the graph S ′ should be exactly equal to the community.\nWe make these intuitions precise in the following theorem.\nTheorem 13 Graph Recovery When the graph is chosen randomly according to Definition 2, and when m2d′3/n3 1, with high probability (over the choice of the graph), Algorithm 6 solves Graph Recovery Problem. The expected running time (over the algorithm’s internal randomness) is O(mn).\nIn order to prove the theorem, we need the following properties from the random graph:\n1. For any v1, v2 ∈ V , |(Γ(v1) ∩ Γ(v2))\\(F (B(v1) ∩B(v2)))| < d′/20.\n2. For any u1, u2 ∈ U , |F (u1) ∪ F (u2)| > 1.5d′.\n3. For any u ∈ U , v ∈ V and v 6∈ F (u), |Γ(v) ∩ F (u)| < d′/20.\n4. For any u ∈ U , at least 0.1 fraction of pairs v1, v2 ∈ F (u) does not have a common neighbor other than u.\n5. For any u ∈ U , its degree is in [0.8d′, 1.2d′]\nThe first property says “most correlations are generated by common cause”: every vertex v in F (B(v1) ∩ B(v2)) is necessarily a neighbor of both v1 and v2 by the definition of graph Ĝ, because they have a common cause u ∈ U . The first property asserts except this case, the number of common neighbors of v1 and v2 is very small.\nThe second property basically says the sets F (u)’s should be almost disjoint, this is clear because the sets are chosen at random.\nThe third property says if a vertex v is not related to the cause u, then it cannot have correlation with all many neighbors of u.\nThe fourth property says every cause introduces a significant number of correlations that is unique to that cause.\nIn fact, Properties 2-4 all follow from the unique neighbor property in Lemma 29. The last property is very standard because d′ log n Lemma 14 When the graph is chosen randomly according to Definition 2, and when m2d′3/n3 1, with probability e−Ω(d ′) Properties 1-5 holds.\nProof: Property 1: Fix any v1 and v2, Consider the graph G1 to be sampled in the following order. First fix the degrees (the degrees are arbitrary between 0.8d′ and 1.2d′), then sample the edges related to v1 and v2, finally sample the rest of the graph.\nAt step 2, let S1 = B(v1)\\B(v2), S2 = B(v1)\\B(v2). By the construction of the graph Ĝ, every vertex in (Γ(v1)∩Γ(v2))\\(F (B(v1)∩B(v2))) must be in F (S1)∩F (S2). With high probability (e−Ω(d\n′)) we know |S1| ≤ |B(v1)| ≤ 2md/n (this is by Chernoff bound, because each vertex u is connected to v with probability du/n < 1.2d\n′/n). Similar things hold for S2.\nNow F (S1) and F (S2) are two random sets of size at most 3m(d ′)2/n, thus again by Chernoff bound we know their intersection has size smaller than 10(md′2/n)2/n, which is smaller than d′/20 by assumption.\nProperty 2: This is easy, for any two vertices u1 and u2, F (u1) and F (u2) are two random sets of size at most 1.2d′. Their expected intersection is less than 2(d′)2/n and the probability that their intersection is larger than 0.1d′ is at most n−Ω(0.1d\n′). Therefore |F (u1) ∪ F (u2)| = |F (u1)|+ |F (u2)| − |F (u1) ∩ F (u2)| ≥ 1.5d′.\nProperty 3: Consider we sample the edges related to u in the last step. Before sampling F (u) we know v 6∈ F (u), and Γ(v) is already a fixed set of size at most 3md′2/n. The probability that F (u) has more than d′/20 elements in Γ(v) is at most e−Ω(d\n′) by Chernoff bounds. Property 4: Again change the sampling process: first sample all the edges not related to u, then sample 1/2 of the edges connecting to u, and finally sample the second half.\nLet S1 be the first half of F (u). For a vertex outside S1, similar to property 3 we know every v 6∈ S1 has at most d′/20 neighbors in S1, therefore any new sample in the second half is going to introduce 0.8d′/2− d′/20 new correlations that are unique to u. The total number of new correlations is at least (0.8d′/2−d′/20)du/2 > 0.1 ( du 2 ) .\nProperty 5 follows from simple concentration bounds. 2 Now we show that when the graph satisfies all these properties, the algorithm works. Lemma 15 When graph G1 satisfies Properties 1-5, Algorithm 6 successfully recovers the graph G1 in expected time O(mn).\nProof: We first show that when (v1, v2) has more than one unique common cause, then the condition in the if statement must be false. This follows from Property 2. We know the set S contains F (B(v1) ∩B(v2)). If |B(v1) ∩B(v2)| ≥ 2 then Property 2 says |S| ≥ 1.5d′, which implies the condition in the if statement is false.\nThen we show if (v1, v2) has a unique common cause u, then S ′ will be equal to F (u). By Property 1, we know S = F (u) ∪ T where |T | ≤ d′/20. For any vertex v in F (u), it is connected to every other vertex in F (u). Therefore |Γ(v) ∩ S| ≥ |Γ(v) ∩ F (u)| ≥ 0.8d′ − 1, and v must be in S ′. For any vertex v′ outside F (u), by Property 3 it can only be connected to d′/20 vertices in F (u). Therefore |Γ(v) ∩ S| ≤ |Γ(v) ∩ F (u)|+ |T | ≤ d′/10. Hence v′ is not in S ′.\nFollowing these arguments, S ′ must be equal to F (u), and the algorithm successfully learns the edges related to u.\nThe algorithm will successfully find all vertices u ∈ U because of Property 4: for every u there are enough number of edges in Ĝ that is only caused by u. When one of them is sampled, the algorithm successfully learns the vertex u.\nFinally we bound the running time. By Property 4 we know that the algorithm identifies a new vertex u ∈ U in at most 10 iterations in expectation. Each iteration takes at most O(n) time. Therefore the algorithm takes at most O(mn) time in expectation. 2"
    }, {
      "heading" : "F.2 Graph Recovery with Higher-Order Correlations",
      "text" : "When d becomes larger, it becomes harder to recover the graph from only pairwise correlations because more and more pairs of nodes on the other side are at distance 2. In particular if d′2 > n then almost all pairs are at distance 2 and the graph Ĝ given to us is simply a complete graph and thus reveals no information about G1.\nWe show how to use higher-order correlations in order to improve the dependency on d. For simplicity we only use 3-wise correlation, but this can be extended to higher order correlations.\nDefinition 3 (Graph Recovery with 3-wise Correlation) There is an unknown random bipartite graph G1(U, V,E1) between |U | = m and |V | = n vertices. Every edge is chosen randomly with probability d′/n. Given: Hypergraph Ĝ(V,E) where (v1, v2, v3) ∈ E iff there exists u ∈ U where (u, v1), (u, v2) and (u, v3) are all in E1. Goal: Find the bipartite graph G1.\nAlgorithm 7. RecoverGraph3Wise\nInput: Hypergraph Ĝ in Definition 3 Output: Graph G1 in Defintion 3\nrepeat Pick a random hyperedge (v1, v2, v3) in E Let S = {v : (v, v1, v2), (v, v1, v3), (v, v2, v3) ∈ E} if |S| < 1.3d′ then\nLet S ′ = {v ∈ S : v is correlated with at least (\n0.8d′−1 2\n) pairs in S}\nIn G1, create a vertex u and connect u to every v ∈ S ′. Mark all hyperedges (v1, v2, v3) for v1, v2, v3 ∈ S ′\nend if until all hyperedges are marked\nThe intuitions behind Algorithm 7 are very similar to Algorithm 6: since 3-wise correlations are rare, not many vertices should have 3-wise correlation with all three pairs (v1, v2), (v1, v3) and (v2, v3) unless they are all in the same community. The\nperformance of Algorithm 7 is better than the previous algorithm because 3-wise correlations are rarer.\nTheorem 16 When the graph is chosen according to Definition 3, and when m3d′8/n8 1, with high probability over the randomness of the graph, Algorithm 7 solves Graph Recovery with 3-wise Correlation. The expected running time is O(m(d′3 + n)) over the randomness of the algorithm.\nThe theorem uses very similar properties as Theorem 13, but the pairwise correlations are replaced by three-wise correlations:\n1’ For any (v1, v2, v3) ∈ E, if S is the set defined as in the algorithm, then |S\\F (B(v1) ∩B(v2) ∩B(v3))| < d′/20.\n2’ For any u1, u2 ∈ U , |F (u1) ∪ F (u2)| > 1.5d′.\n3’ For any u ∈ U , v ∈ V and v 6∈ F (u), v is correlated with at most d′2/40 pairs in F (u).\n4’ For any u ∈ U , at least 0.1 fraction of triples v1, v2, v3 ∈ F (u) does not have a common neighbor other than u.\n5’ For any u ∈ U , its degree is in [0.8d′, 1.2d′]\nWe can use concentration bounds to show all these properties hold for a random graph.\nLemma 17 When the graph is chosen randomly according to Definition 3, and when m3d′8/n8 1, with probability e−Ω(d ′), graph G1 satisfies property 1 ′ − 5′.\nNow the following lemma implies the theorem.\nLemma 18 When the graph G1 satisfies properties 1\n′−5′, Algorithm 7 finds G1 in expected time O(m(d′3 + n)).\nProof: The idea is again very similar to the pairwise case (Lemma 15). If (v1, v2, v3) has more than one common neighbor, then Property 2\n′ shows the condition in if statement must be false (as |S| ≥ |F (B(v1) ∩B(v2) ∩B(v3))| ≥ 1.5d′.\nWhen (v1, v2, v3) has only one common neighbor u, then Property 1 ′ shows S = F (u) ∪ T where |T | ≤ d′/20. Now consider S ′, for any v ∈ F (u), it is correlated with all other pairs in F (u).\nHence it must be correlated with at least (\n0.8d′−1 2\n) pairs in S, which implies v is in S.\nFor any v′ 6∈ F (u), by Property 3′ it can only be correlated with d′2/40 pairs in F (u). Therefore, the total number of correlated pairs it can have in S is bounded by |T | |F (u)|+ (|T | 2 ) + d′2/40 < ( 0.8d′−1 2 ) . This implies v′ is not in S.\nThe argument above shows S ′ = F (u), so the algorithm correctly learns the edges related to u.\nFinally, to bound the running time, notice that Property 4′ shows the algorithm finds a new vertex u in 10 iterations in expectation. Each iteration takes at most n+ d′3 time. Therefore the algorithm takes O(m(d′3 + n)) expected time. 2"
    }, {
      "heading" : "G Learning the graph",
      "text" : "In this section we show how to learn the graph Gt (t = 1, 2, ..., `). For simplicity, we focus on the layer above the bottommost layer (the last layer with discrete output). Recall the graph G(U, V,E,w) is chosen from the distribution Gm,n,p (m = n2 and n = n1). The vector h on the U side is the h\n(2) in the original network, and is generated according to the distribution D2. The vector y on V side is h(1) in the original network, and y = sgn(Gh). As usual, we denote the expected degree of nodes in U as d′ = pn, and the expected degree of nodes in V as d = pm. Let ρ be the sparsity on the U side, ρy be the sparsity on the V side. The V side is also relatively sparse in the sense that ρyd 1.\nAt this step, we have already applied PartialEncoder to y, and Lemma 8 ensures we get the correct vector h. Hence in this Section we assume we are given pairs (h, y).\nG.1 Learning the graph for D(`, ρl, {Gi}) If the edges have weights ±1, we have shown how to learn the negative edges E− in Section D. The only difference here is that h is chosen from D2 instead of random ρ2n2 sparse vectors. We prove the following Lemma that replaces Lemma 9.\nLemma 19 Suppose ρyd 1, using O(log n/(ρ2d)) samples, with high probability over the choice of the samples, Algorithm 5 returns E−.\nProof: Similar as in Lemma 9, it suffices to prove the following Proposition 20. Once this Proposition is true, any (s, v) 6∈ E will be removed from R with high probability. By union bound all non-edges are removed, and the remaining pairs are the edges E−.\nProposition 20 Suppose h is from D2 as defined in beginning of this Section, for any (x, u) 6∈ E, with probability Ω(ρ2d) over the choice of h, the following events happen simultaneously: 1. x ∈ supp(h), 2. |B+(u) ∩ supp(h)| = 1, 3. |B−(u) ∩ supp(h)| = 0.\nWhen these events happen, (x, u) is removed from R by Algorithm 5.\nProof: The idea is very similar to the case when h is chosen from uniformly ρ2n2 sparse vectors. Note that event 1 happens with roughly probability ρ the expected fractions of 1’s in h is ρ. More precisely, for any z in the second layer, Pr[hz = 1] ≥ ρ`|B(`)+ (z)| = ρ2 = ρ. Similarly, event 2 happens if for some nodes t ∈ B+(u),∣∣∣supp(h(`)) ∩B(`)+ (t) ∣∣∣ ≥ 1, and event 3 happens when B(`)+ (B−(u)) ∩ supp(h(`)) = ∅. Using Property Pmul and Lemma 39, one can show that actually h behaves very similarly to ρ2n2-sparse vector, where the probability that both the three events happen is Ω(ρ2d).\n2\n2\nG.2 Learning a Decoder for R(`, ρ, {Gi}) When the weights on the edges are continuous (in [−1, 1]), learning the decoder becomes harder. In particular, later in Section I we show it is hard to learn the weights exactly. Here we give an algorithm that achieves a slightly weaker guarantee: the decoder learned by this algorithm is correct with probability 1− η.\nThe key observation here is that every coordinate of y is a half-plane on the h vector, so learning the decoder actually reduces to the famous Support Vector Machines.\nThe hypothesis class for coordinate yv is simply all the halfplanes sgn((Gh)v). By VC-dimension theory, since the V C-dimension of a halfplane is m+1, any hypothesis that is consistent with all N samples has generalization error O( √ (m+ 1) logN/N). Once we have enough samples, finding a consistent halfplane is a linear program. The full algorithm is given in Algorithm 8\nLemma 21 Given N = O(`2mn2 log n/η2) samples of (h, y), where h is chosen from distribution D2 and y = sgn(Gh), with high probability over the choice of samples, Algorithm 8 outputs a matrix G′ that satisfies yi = sgn(G′hi) for all samples (yi, hi). Furthermore,\nPr h∼Dt\n[sgn(G′h) 6= sgn(Gh)] < η/`.\nProof: It is clear that the LP in Algorithm 8 is feasible, because G is a feasible solution. On the other hand, every feasible solution G′ of the LP are consistent with all the samples.\nFor each output node v, since the family of m-dimensional half-planes has VCdimension m+1, we know with high probability any row vector G′v that is consistent with all the N i.i.d. samples has small generalization error\nPr h∼Dh\n[sgn(G′vh) 6= sgn(Gvh)] ≤ √ 2m log(eN/d)\nN +O(\n√ log(n)\n2N ) ≤ η/n`\nTaking a union bound for all coordinates of y, we have\nPr h∼Dt [sgn(G′h) 6= sgn(Gh)] < η/`. 2\nAlgorithm 8. LearnDecoder\nInput: N = O(nl2m2 log n/η2) samples (h1, y1), (h2, y2), . . . , (hN , yN), where h is from distribution Dt and y = sgn(Gh) Output: A graph G′ such that Prh∼Dt [sgn(G ′h) 6= sgn(Gh)] ≤ .\nSolve the linear program\n∀j, { G′hj ≤ 0 if yj = 0 G′hj > 0 if yj = 1\nHere ≤ and > are coordinate-wise. return a feasible solution G′, the decoder is y = D(h) = sgn(G′h).\nWhen decoders for all layers are learned by Algorithm 8, the deep network composed by the decoders generates a distribution that is close to the original deep network: Theorem 22 Given a deep network R(`, ρ, {Gi}), suppose decoders D2, D3, ..., Dl are learned for layers G2, G3, ..., G`, then for a random ρn` sparse vector h (l),\nPr h [D2(D3(· · ·Dl(h(l)) · · · ) 6= h(1)] ≤ `− 1 ` · η.\nIn particular, let R′ be the network generated by stacking the decoders, the outputs of the two networks are `−1\n` · η-close in statistical distance.\nProof: Let bad event Badi be the event h (t) = Dt+1(· · ·Dl(h(l)) · · · ) for all t ≥ i, and h(i−1) 6= Di(· · ·Dl(h(l)) · · · ). Clearly the events are disjoint, and the event of interest is the union of Bad2, Bad3, ..., Badl. By Lemma 21, each Badi happens with probability at most η/`. Union bound gives the result.\n2"
    }, {
      "heading" : "H Layer with Real-valued Output",
      "text" : "In previous sections, we considered hidden layers with sparse binary input and output. However, in most of applications of deep learning, the observed vector is dense and real-valued. Bengio et al.[BCV13] suggested a variant auto-encoder with linear decoder, which is particularly useful in this case.\nWe first show for a random weighted graph G, the linear decoder function D(h) = Gh and the encoder function E(y) = sgn(GTy + b) form a denoising autoencoder.\nTheorem 23 If G is a random graph with random weights in either {+1,−1} or [−1, 1], the encoder E(y) = sgn(GTy−0.1d′~1) and linear decoder D(h) = Wh form an sparse autoencoder: that is, for any vector h of support size at most k, E(D(h)) = h.\nFurther, the autoencoder satisfy the denoising property E(D(h)+η) = h when the noise vector η has independent components each with variance at most o(d′0/ log 2 n).\nProof: When there is no noise, we know E(D(~h)) = sgn(GTG~h − 0.2d′0~1). With high probability the matrix GTG has value at least 0.9d′0 on diagonals (for {+1,−1} weights, for [−1, 1] weights this is at least 0.2d′0). For any fixed ~h, the support of the i-th row of GTG and the support of ~h have a intersection of size at most d′0 log\n2 n with high probability. Also, at all these intersections, the entries of GTG has random signs, and variance bounded by O(1). Hence if ~hi = 1 (G TG~h)i ≥ 0.9d′0 − O( √ d′0 log 2 n)\n(or 0.2d′0 − O( √ d′0 log 2 n) for [−1, 1] weights) ; if ~hi = 0 (GTG~h)i ≤ O( √ d′0 log\n2 n). Setting the threshold at 0.1d′0 easily distinguishes between these two cases.\nEven if we add noise, since the inner product of Gi and the noise vector is bounded by o(d0) with high probability, the autoencoder still works.2\nWe use similar ideas as in Section D to learn the last layer. The algorithm collects the correlation-structure of the observed variables, and use this information to reconstruct E+. We inherent most of the notations used in Section C. Formally, the last layer consists of a graph G(U, V,E,w), chosen randomly from distribution Gm,n,p. The weights w are chosen uniform randomly from ±1. Although our result extends to the distribution on h generated by the deep network, and the weights are in [−1, 1] for simplicity, we restrict to the uniform ρm sparse distribution and random ±1 weights.\nH.1 Learning network using correlation\nWhen we observe real-valued output in the last layer, we can learn whether three nodes u,v and s have a common cause from their correlation measured by E[xuxvxs], even if the output vector is fully dense. Note that compared to Theorem 4 in Section D, the main difference here is that we allow ρpm to be any constant (before ρpm 1/d). Theorem 24 When ρpm = O(1), pn = Ω(log3 n), and the input h is a uniformly random ρm-sparse vector, then with high probability over the choice of the weights and the choice of the graph, for any three nodes u, v, s at the output side (side of y)\n1. If u, v and s have no common neighbor, then |Eh[yuyvys]| ≤ ρ/3\n2. If u, v and s have a unique common neighbor, then |Eh[yuyvys]| ≥ 2ρ/3\nThe rigorous proof of Theorem 24 is similar to Lemma 7. We defer the proof in Appendix L.\nAlthough the hypergraph given by Theorem 24 is not exactly the same as the hypergraph given by Theorem 11, the same Algorithm 7 can find the edges of the graph. Notice that in this case we get the support of all edges (instead of just positive edges). Using the sign of Eh[yuyvys], we can easily distinguish positive and negative edges (see Appendix L for more details). This idea can be generalized to [−1, 1] weights, and we prove the following theorem in Appendix L.\nTheorem 25 If h is from the distribution D1, with parameters satisfying ρ1d = O(1), d log3 n), d3/2/(d1d2) log−3/2 n and √ d/d1 log−3/2 n, then with high probability over the choice of the weights and choice of the graph, there is an algorithm such that given O(log2 n/ρ21) samples of the output y, learns the network exactly in time O(m(d\n3 + n) + log2 n/ρ21).\nReal weights from [−1, 1] Finally, the above results can be extended to the case when edge weights are uniformly random from [−1, 1]. In this case there are several differences:\n1. The correlation is large only if the three vertices share a parent with relatively large edge weights. A slight variant of Algorithm 7 learns the large weights.\n2. It is still possible to use PartialEncoder even if we only have the large weights.\n3. After applying PartialEncoder, we get (h, y) pairs. By solving the system of linear equations yi = Ghi we learn G.\nTheorem 26 If the weight of the network is chosen independently from [−1, 1], and all the others parameters satisfy the same condition as in Theorem 25 (but with different universal constant), there is an algorithm that learns the graph and the weights using O(log2 n/ρ21) samples and O(log 2 n/ρ21 +m(d 3 + n)) time."
    }, {
      "heading" : "I Lower bound",
      "text" : "I.1 Improper learning is necessary when weights are reals\nIn this section, we showcase an example of real weights in which we cannot hope to recover either the weights (even approximately), or to find some set of weights that agrees with the true one on all the inputs, using less than exp(Ω(d)) samples. This justifies the necessity of the improper/PAC learning of the real weights.\nLemma 27 There exist two vectors w,w′ ∈ {0, 1}d such that the two functions f = sgn(wTh) and f ′ = sgn(w′Th) for h ∈ {0, 1}d only differ at the point h = ~1. Thus it is necessary to\nhave exp(Ω(d)) samples of h from uniform ρ-sparse distribution to recover the true value of function f = sgn(wTh) at point h = ~1.\nProof: We construct the following w and w′. WLOG, let d be an odd number with d = 2s + 1. Let z be the vector with first s coordinates being 1/(s + 1), and the remaining coordinates being 1/s. Let w = z + ~1 and w′ = z − ~1, where 0 ≤ < (s(s+ 1)(2s+ 1))−1. Note that zTh is away from zero by at least (s(s+ 1))−1 if h 6= ~1 and zTh = 0 if h = ~1. Therefore, wTh and w′Th agree on the sign for all h but h = ~1. 2\nI.2 Representational power: One layer net can’t do two layers net\nIn this section we show that a two-layer network with ±1 weights is more expressive than one layer network with arbitrary weights. A two-layer network (G1, G2) consists of random graphs G1 and G2 with random ±1 weights on the edges. Viewed as a generative model, its input is h(3) and the output is h(1) = sgn(G1 sgn(G2h\n(3))). We will show that a single-layer network even with arbitrary weights and arbitrary threshold functions must generate a fairly different distribution.\nLemma 28 For almost all choices of (G1, G2), the following is true. For every one layer network with matrix A and vector b, if h(3) is chosen to be a random ρ3n-sparse vector with ρ3d2d1 1, the probability (over the choice of h(3)) is at least Ω(ρ23) that sgn(G1 sgn(G1h (3))) 6= sgn(Ah(3) + b).\nThe idea is that the cancellations possible in the two-layer network simply cannot all be accomodated in a single-layer network even using arbitrary weights. More precisely, even the bit at a single output node v cannot be well-represented by a simple threshold function.\nFirst, observe that the output at v is determined by values of d1d2 nodes at the top layer that are its ancestors. Wlog, in the one layer net (A, b), there should be no edge between v and any node u that is not its ancestor. The reason is that these edges between v and its ancestors in (A, b) can be viewed collectively as a single random variables that is not correlated with values at v’s ancestors, and either these edges are “influential” with probability at least ρ23/4 in which case it causes a wrong bit at v; or else it is not influential and removing it will not change the function computed on ρ23/4 fraction of probability mass. Similarly, if there is a path from u to v then there must be a corresponding edge in the one-layer network. The question is what weight it should have, and we show that no weight assignment can avoid producing erroneous answers.\nThe reason is that with probability at least ρ3/2, among all ancestors of v in the input layer, only u is 1. Thus in order to produce the same output in all these cases,\nin the one-layer net the edge between u and v should be positive iff the path from u to v consists of two positive edges. But now we show that with high probability there is a cancellation effect involving a local structure in the two layer net whose effect cannot be duplicated by such a single-layer net (See the Figure 3 and 4).\nh(1)\nh(2)\nv\n+1 -1\n+1\nu1 u2 u3 h(3)\n+1\n+1\nG2\nG1\ns s′\nu4\n-1\nFigure 3: Two-layer network(G1, G2)\nAs drawn in Figure 3, suppose the nodes u1, u2 connect to s in h (2) via +1 and −1 edge, and s connects to v via a +1 edge. Similarly, the nodes u3, u4 connect to s′ in h(2) via +1 and −1 edge, and s connects to v via a +1 edge.\nNow assume all other ancestors of v are off, and consider the following four values of (u1, u2, u3, u4): (1, 1, 0, 0), (0, 0, 1, 1), (1, 0, 0, 1), (0, 1, 1, 0). In the two-layer network, h(1)v should be 0 for the first two inputs and 1 for the last two inputs. Now we are going to see the contradiction. For single-layer network, these values imply constraints Au1,v + Au2,v + bv ≤ 0, Au3,v + Au4,v + bv ≤ 0, Au1,v + Au4,v + bv > 0, Au2,v + Au3,v + bv > 0. However, there can be no choices of (A, b) that satisfies all four constraints! To see that, simply add the first two and the last two, the lefthand-sides are all ∑4 i=1Aui,v + 2bv, but the right-hand-sides are ≤ 0 and > 0. The single-layer network cannot agree on all four inputs. Each of the four inputs occurs with probability at least Ω(ρ23). Therefore the outputs of two networks must disagree with probability Ω(ρ23).\nRemark: It is well-known in complexity theory that such simple arguments do not suffice to prove lowerbounds on neural nets with more than one layer."
    }, {
      "heading" : "J Random Graph Properties",
      "text" : "In this Section we state the properties of random graphs that are used by the algorithm. We first describe the unique-neighbor property, which is central to our analysis. In the next part we list the properties required by different steps of the algorithm."
    }, {
      "heading" : "J.1 Unique neighbor property",
      "text" : "Recall that in bipartite graph G(U, V,E,w), the set F (u) denotes the neighbors of u ∈ U , and the set B(v) denotes the neighbors of v ∈ V .\nFor any node u ∈ U and any subset S ⊂ U , let UF (u, S) be the sets of unique neighbors of u with respect to S,\nUF (u, S) , {v ∈ V : v ∈ F (u), v 6∈ F (S \\ {u})}\nProperty 1 In a bipartite graph G(U, V,E,w), a node u ∈ U has (1 − )-unique neighbor property with respect to S if\n∑\nv∈UF (u,S) |w(u, v)| ≥ (1− )\n∑\nv∈F (u) |w(u, v)| (2)\nThe set S has (1 − )-strong unique neighbor property if for every u ∈ U , u has (1− )-unique neighbor property with respect to S. Remark 4 Notice that in our definition, u does not need to be inside S. This is not much stronger than the usual definition where u has to be in S: if u is not in S we are simply saying u has unique neighbor property with respect to S ∪ {u}. When all (or most) sets of size |S|+ 1 have the “usual” unique neighbor properties, all (or most) sets of size |S| have the unique neighbor property according to our definition. Lemma 29 If G(U, V,E) is from distribution Gm,n,p, and for any u ∈ U , the weight vector w(u, ·) (i.i.d. on edges) satisfies that |w(u, ·)|21 ≥ C · ‖w(u, ·)‖22 for some C. Then for every subset S of U with (1 − p)|S| > 1 − /2 (note that p|S| = d′|S|/n is the expected density of F (S)), with probability 1 − 2m exp(− 2C) over the randomness of the graph, S has the (1− )-strong unique neighbor property.\nNote: The weights usually comes from one of the following: 1. all weights are 1; 2. weights are random in {±1}; 3. weights are random in [−1, 1]. In all these cases C is Θ(d′). Proof:\nFix the vertex u, first sample the edges incident to u. Without loss of generality assume u has neighbors v1, . . . , vk (where k ≈ pn). Now sample the edges incident to the vi’s. For each vi, with probability (1− p)|S| ≥ 1− /2, vi is a unique neighbor of u. Call this event Goodi (and we also use Goodi as the indicator for this event).\nBy the construction of the graph we know Goodi’s are independent, hence by Hoeffding inequality (see Theorem 45), we have that with probability 1−2 exp(− 2( ∑ v∈F (u) w(u, v)) 2\n∑ v∈F (u) w(u, v) 2 ) = 1−2 exp(− 2|w(u, ·)|21/‖w(u, ·)‖22) = 1−2 exp(− 2C),\nthe following holds ∑\nv∈F (u) w(u, v)Goodi ≥ (1− )\n∑\nv∈F (u) w(u, v).\nBy union bound, every u satisfies this property with probability at least 1 − 2m exp(− 2C). 2 Corollary 30 IfG(U, V,E) is chosen from Gm,n,p and all the weights w(u, v) have the same magnitude for (u, v) ∈ E , then for fixed S with p|S| < /2, with probability 1− exp(−Ω(d′)), S has the (1− )-strong unique neighbor property.\nJ.2 Properties required by each steps\nWe now list the properties required in our analysis. These properties hold with high probability for random graphs."
    }, {
      "heading" : "J.2.1 Single layer",
      "text" : "The algorithm PairwisGraph requires the following properties Psing\n1. For any u in the observed layer, |B(u)| ∈ [0.9d, 1.1d], (if G has negative weights, we also need |B+(u)| ∈ [0.9d/2, 1.1d/2])\n2. For any z in the hidden layer, |F (z)| ∈ [0.9d′, 1.1d′], (if G has negative weights, we also need |F+(z)| ∈ [0.45d′, 0.55d′])\nThe algorithm 3-WiseGraph needs Psing, and additionally Psing+ 1. For any u, v in the observed layer, |B+(u) ∪B+(v)| ≤ 10.\nLemma 31 If graph G is chosen from Gm,n,p with expected degrees d, d′ log n, with high probability over the choice of the graph, Psing is satisfied. If in addition d2 ≤ n4/5, the property Psing+ is also satisfied with high probability."
    }, {
      "heading" : "J.2.2 Multi-layer",
      "text" : "For the multi-layer setting, the algorithm PairwisGraph requires the following expansion properties Pmul.\n1. For any node u at the layer i, |Fi−1(u)| ∈ [0.9d′i−1, 1.1d′i−1], |Bi(u)| ∈ [0.9di, 1.1di],∣∣F+i−1(u) ∣∣ ∈ [0.45d′i−1, 0.55d′i−1], ∣∣B+i (u) ∣∣ ∈ [0.45di, 0.55di]\n2. For any node u at the layer i, |B(t)+ (u)| ≥ 0.8ρi/ρt, and |B(`)(u)| ≤ 2`−i+1ρi/ρ`.\n3. For any pair of nodes u, v at layer i,\n∣∣∣B(`)+ (u) ∩B(`)+ (v) ∣∣∣ ≤ 2ρi+1/ρ` · ( log n/di+1 + 1/(ρ`n`) + ∣∣∣B(i+1)+ (u) ∩B(i+1)+ (v) ∣∣∣ )\nIn particular, if u and v have no common positive parent at layer i+1 ( ∣∣∣B(i+1)+ (u) ∩B(i+1)+ (v) ∣∣∣ = 0), then ∣∣∣B(`)+ (u) ∩B(`)+ (v) ∣∣∣ ≤ o(ρi+1/ρ`)\nThe algorithm 3-WiseGraph needs property Pmul+:\n1. Properties 1 and 2 in Pmul 2. For any pair of nodes u, v at layer i, ∣∣∣B(`)+ (u) ∩B(`)+ (v) ∣∣∣ ≤ 10ρi+1/ρ`\n3. For any three nodes u, v and s at layer i, if they don’t have a common positive neighbor in layer i+ 1,\n∣∣∣B(`)+ (u) ∩B(`)+ (v) ∩B(`)+ (s) ∣∣∣\n≤ 2ρi+1/ρ` · ( log n/di+1 + ρ`/(ρ`n`) + 1/(ρ 2 `n 2 `) ) ≤ o(ρi+1/ρ`)\nLemma 32 If the network D(`, ρ`, {Gi}) have parameters satisfying di log n, and ρ2i ρi+1, then with high probability over the randomness of the graphs, {Gi}’s satisfy Pmul. Additionally, if di log n and ρ3i ρi+1, then {Gi}’s satisfy Pmul+ with high probability.\nIn order to prove Lemma 32 we need the following claim:\nClaim 33 If the graph G ∼ Gm,n,p with d = pm being the expected back degree, and d log n. For two arbitrary sets T1 and T2, with d|T1| m, d|T2| m, we have with high probability\n|B(T1) ∩B(T2)| ≤ (1 + )d|T1 ∩ T2|+ (1 + )d2|T1||T2|/m+ 5 log n\nThis Claim simply follows from simple concentration bounds. Now we are ready to prove Lemma 32. Proof:[Proof of Lemma 32] Property 1 in Pmul follows directly from di log n.\nProperty 2 in Pmul follows from unique neighbor properties (when we view the bipartite graph from Ni to Ni+1).\nFor Property 3, we prove the following proposition by induction on t:\nProposition 34 For any two nodes u, v at layer 1, ∣∣∣B(t)+ (u) ∩B(t)+ (v) ∣∣∣ ≤ (1+ )ttρ21/(ρ2tnt)+6t(1+ )tρ3 log n/ρt+(1+ )tρ2 ∣∣∣B(2)+ (u) ∩B(2)(v) ∣∣∣ /ρt\nThis is true for t = 2 (by Claim 33). Suppose it is true for all the values less than t − 1. By Claim 33, we know with high probability (notice that we only need to do union bound on n2 pairs)\n∣∣∣B(t+1)+ (u) ∩B(t+1)+ (v) ∣∣∣ ≤ (1 + )dt/2 · ρ2 ∣∣∣B(t)+ (u) ∩B(t)+ (v) ∣∣∣ /ρt\n+(1 + )d2t/4 · |B(t)+ (u)||B(t)+ (v)|+ 5 log n ≤ (1 + )t+1ρ2 ∣∣∣B(2)+ (u) ∩B(2)(v) ∣∣∣ /ρt+1 + 6t(1 + )t+1ρ3 log n/ρt+1\n+(1 + )ttρ21/(ρ 2 tnt) + (1 + )d 2 t/4 · ρ21/(ρ2tnt+1) + 5 log\n≤ (1 + )t+1ρ2 ∣∣∣B(2)+ (u) ∩B(2)(v) ∣∣∣ /ρt+1 + 6(t+ 1)(1 + )t+1ρ3 log n/ρt+1 +2(1 + )t+1(t+ 1)ρ21/(ρ 2 t+1nt+1),\nwhere the last inequality uses the fact that ρ21/(ρ 2 tnt) ≤ d2t/4 · ρ21/(ρ2tnt+1). This is because ntdt/nt+1 = d ′ t 1.\nProposition 34 implies that when ρ21 ρ2, and ` is a constant, ∣∣∣B(t)+ (u) ∩B(t)+ (v) ∣∣∣ ≤ 2ρi+1/ρ` · ( 1/di+1 + 1/(ρ`n`) + ∣∣∣B(i+1)+ (u) ∩B(i+1)+ (v) ∣∣∣ )\nProperty 3 in Pmul+ is similar but more complicated. 2"
    }, {
      "heading" : "J.2.3 Properties for Graph Reovery",
      "text" : "For the algorithm RecoverGraph3Wise to work, the hypergraph generated from the random graph should have the following properties Precovery+.\n1’ For any (v1, v2, v3) ∈ E, if S is the set defined as in the algorithm, then |S\\F (B(v1) ∩B(v2) ∩B(v3))| < d′/20.\n2’ For any u1, u2 ∈ U , |F (u1) ∪ F (u2)| > 1.5d′.\n3’ For any u ∈ U , v ∈ V and v 6∈ F (u), v is correlated with at most d′2/40 pairs in F (u).\n4’ For any u ∈ U , at least 0.1 fraction of triples v1, v2, v3 ∈ F (u) does not have a common neighbor other than u.\n5’ For any u ∈ U , its degree is in [0.8d′, 1.2d′]"
    }, {
      "heading" : "J.2.4 Properties for Partial Encoder",
      "text" : "For the Partial Encoder algorithm to work, we only need the support of h satisfying the strong unique-neighbor property."
    }, {
      "heading" : "J.2.5 Properties for Learning the graph",
      "text" : "In order to learn the −1 weights, the conditions we need are similar to Pmul and Pmul+.\nIn the case with real weights, we don’t need any assumptions on the graph, because we are relying on VC-dimension arguments."
    }, {
      "heading" : "K Auxiliary Lemmas for uniform ρ`n` sparse vec-",
      "text" : "tors\nIn this subsection, we provide lemmas about the uniform ρn-sparse vector of dimension n.\nThese Lemmas suggest that when looking at intersections with relatively small sets, uniform ρn sparse vector behaves like i.i.d. Bernoulli variables with probability ρ. Also, in this section the sparse vector is identified by its support S.\nLemma 35 If S is a uniformly random ρn-sparse subset of [n], then for any fix set T with ρ|T | 1 and |T | n,\n1− ρ|T |/2 ≥ Pr[T ∩ S = ∅] ≥ 1− ρ|T |\nProof: For each t ∈ T , Pr[t ∈ S] = ρ. By union bound, Pr[T ∩ S = ∅] ≥ 1−∑t∈S Pr[t ∈ S] = 1− ρ|T |.\nOn the other hand, let ρn = k, we have,\nPr[T ∩ S = ∅] = (1− k n ) · (1− k n− 1) . . . (1−\nk n− |T |) ≤ (1− k n )|T | ≤ 1− ρ|T |/2\n2\nLemma 36 If S is a uniformly random ρn-sparse subset of [n], then for any fix set T with ρ|T | 1 and |T | n, ρ|T | ≥ Pr[|S ∩ T | = 1] ≥ ρ|T |/2\nProof: For any t ∈ T , we compute Pr[S ∩ T = {t}]. Conditioned on t ∈ T , S \\ {t} is uniform (ρn− 1)-sparse subset of [n] \\ {t}, thus by the Lemma above,\nPr[S ∩ T = {t}] = Pr[t ∈ S] · Pr[S ∩ (T \\ {t}) = ∅] ∈ [ρ/2, ρ]\nAll the events S ∩ T = {t} for t ∈ T are disjoint, thus ρ|T | ≥ Pr[|S ∩ T | = 1] ≥ ρ|T |/2 2 Lemma 37 If S is uniform random ρn-sparse subset of [n], then for any fix set T with ρ|T | 1 and |T | n,\nPr[|T ∩ S| > ρn/2] < (2ρ|T |)ρn/2\nProof: Let k = ρn. For any subsets M of T of size k, Pr[M ⊂ S] = k n ·\nk n−1 . . . k n−(k/2) ≤ (2ρ)k/2. Taking a union bound over all M , we have Pr[|T ∩ S| ≥\nk/2] ≤ ( |T | k/2 ) (2ρ)k/2 ≤ (2ρ|T |)k/2. 2\nLemma 38 If S is a uniformly random ρn-sparse set, then for any two disjoint sets T1 and T2 with ρ|T1| 1, ρ|T2| 1 and |T1|, |T2| n,\nρ2|T1||T2|/5 ≤ Pr[S ∩ T1 6= ∅, S ∩ T2 6= ∅] ≤ 2ρ2|T1||T2| Proof: From Lemma 36 we know Pr[|S ∩ T1| = 1] ∈ [ρ|T1|/2, ρ|T1|].Conditioned on |S ∩ T1| = 1, S \\ T1 is a random ρn − 1 sparse vector in n − |T1| dimension. Apply Lemma 36 with ρ′ = (ρn − 1)/(n − |T1|) we have Pr[|S ∩ T2| = 1 | |S ∩ T1| = 1] = Θ(ρ′|T2|).\nOn the other hand, ρ|T1| is an upperbound on Pr[S ∩ T1 6= ∅], ρ′|T2| is an upperbound on Pr[S ∩ T2 6= ∅|S ∩ T1 6= ∅], so we get the other direction. 2 Lemma 39 If S is a uniformly random ρn-sparse subset of [n], then for two disjoint sets T1, T2 with ρ|T1|, ρ|T2| 1 and |T1|, |T2| n,\nρ|T1|/2 ≤ Pr[S ∩ T1 6= ∅, S ∩ T2 = ∅] ≤ 2ρ|T1|\nThe proof is very similar to Lemma 38.\nLemma 40 If S is a uniform random ρn-sparse subset of [n], then for any three sets T1, T2 and T3 with ρ|T1|, ρ|T2|, ρ|T3| 1 and |T1|, |T2|, |T3| n. Let max{|Ti|} ≤ A,max1≤i<j≤3 |Ti∩ Tj| = B, and |T1 ∩ T2 ∩ T3| = C, then\nPr[(S ∩ Ti 6= ∅) for i = 1, 2, 3] ≤ O(ρ3A3 + ρ2AB + ρC)\nProof: The proof is not hard but tedious. It enumerates all the possible ways S can intersect with all three Ti’s.\nLet K1 = T1 − T2 ∪ T3, K2 = T2 − T1 ∪ T3, K3 = T3 − T1 ∪ T2. Also, let K12 = T1 ∩ T2 − T3 (similarly define K23, K31). Finally let K123 = T1 ∩ T2 ∩ T3.\nLet E be the event that S ∩ K1 6= ∅, S ∩ K2 6= ∅, S ∩ K3 6= ∅; E ′ be the event that S ∩K123 6= ∅; E12 be the event that S ∩K12 6= ∅, S ∩K3 6= ∅ (similarly define E23 and E31). The event S ∩ Ti 6= ∅ for all i = 1, 2, 3 is contained in the union of these events E ∪ E ′ ∪ E12 ∪ E23 ∪ E31. By previous Lemmas it is easy to bound Pr[E] ≤ 2ρ3|T1||T2||T3| ≤ 2ρ3A3, Pr[E ′] ≤ ρC, and Pr[E12] ≤ 2ρ2|K12||K3| ≤ 2ρ2AB. Hence Pr[(S ∩ Ti 6= ∅) for i = 1, 2, 3] ≤ O(ρ3A3 + ρ2AB + ρC) 2"
    }, {
      "heading" : "L Omitted Proofs in Section H",
      "text" : "In this Section we shall prove Theorem 24 and Theorem 25.\nProving Theorem 24 By the structure of the network we know\nE[yuyvys] = ∑\ni∈B(u),j∈B(v),k∈B(s) wi,uwj,vwk,s E[hihjhk] (3)\nObserve that wi,u and wj,v are different random variables, no matter whether i is equal to j. Thus by 3-order Hoeffding inequality stated in Lemma 47, with high probability over the choice of the weights\n|E[yuyvys]| ≤ √ ∑\ni∈B(u),j∈B(v),k∈B(s) E[hihjhk]2 log3 n (4)\nLet V be the main term in the bound:\nV , ∑\ni∈B(u),j∈B(v),k∈B(s) E[hihjhk]2.\nDefine Buv = B(u) ∩ B(v), Bvs = B(v) ∩ B(s), Bsu = B(s) ∩ B(u). Similar to Property Psing+, for a random graph we know all these sets have size at most 10.\nThe following claim gives bound on the variance term (so we know E[yuyvys] has small absolute value when they don’t share a common neighbor):\nClaim 41 If h is uniform random ρm-sparse vector, and B(u)∩B(v)∩B(s) = ∅, and |Buv| ≤ 10, |Bvs| ≤ 10 and |Bsu| ≤ 10, then\nV = ∑\ni∈B(u),j∈B(v),k∈B(s) E[hihjhk]2 ≤ O(ρ6d3 + ρ4d)\nProof:First of all, for any triple (i, j, k) such that i, j, k are distinct, the expectation E[hihjhk] ≤ ρ3. The total number of such triples is at most O(d3). The contribution to the sum V is O(d3ρ6)\nThen we count the triples (i, j, k) with i = j, and i 6= k (i ∈ Buv and k ∈ B(s)). The total number of such triples (i, j, k) is bounded by 10d. In this case, E[hihjhk] ≤ ρ2. Thus the total contribution to the V is O(dρ4). The other two symmetric cases have similar contribution to the V as well.\nHence the total sum is bounded by O(ρ6d3 + ρ4d). 2 When u, v, s share a common neighbor, the following claim shows E[yuyvys] has a\nlarge absolute value.\nClaim 42 If h is uniform random ρm-sparse vector , and B(u) ∩ B(v) ∩ B(s) = {z}, and |Buv| ≤ 10, |Bvs| ≤ 10 and |Bsu| ≤ 10, then with high probability over the choice of the weights\n|E[yuyvys]| ≥ ρ−O( √ (ρ6d3 + ρ4d) log3 n)\nProof: Let Az,z,z = 0, and Ai,j,k = E[hihjhk] when (i, j, k) 6= (z, z, z). Thus\nE[yuyvys] = ∑\ni∈B(u),j∈B(v),k∈B(s) wi,uwj,vwk,s E[hihjhk]\n= wz,uwz,vwz,s E[hz] + ∑\ni∈B(u),j∈B(v),k∈B(s) wi,uwj,vwk,sAi,j,k (5)\nSimilar to the previous case, we know that ∑\ni∈B(u),j∈B(v),k∈B(s) A2i,j,k ≤ O(ρ6d3 + ρ4d)\nand thus with high probability,\n| ∑\ni∈B(u),j∈B(v),k∈B(s) wi,uwj,vwk,sAi,j,k| ≤\n√ ∑\ni∈B(u),j∈B(v),k∈B(s) A2i,j,k log 3 n\n≤ O( √ (ρ6d3 + ρ4d) log3 n)\nBy Equation (5), the expectation can be bounded by\nE[yuyvys] = wz,uwz,vwz,s E[hz]±O( √ (ρ6d3 + ρ4d) log3 n),\nwhich implies\n|E[yuyvys]| ≥ ρ−O( √ (ρ6d3 + ρ4d) log3 n)\n2\nThe Theorem follows directly from these two claims.\nExtending Theorem 24 to distribution D1 Using similar ideas, and the bounds from Section E, we can prove Theorem 25.\nWe shall first extend Theorem 25 to the setting when h is generated by the upper levels of the network (call the distribution D1). In order to do this we need to bound E[hihjhk] carefully. Following similar ideas as Lemma 12, we have\nProposition 43 If h ∼ D1, then\nE[hihjhk] is { ≤ O(ρ31 + ρ2ρ1 + ρ2) if B+(i) ∩B+(j) ∩B+(k) 6= ∅, ≤ O(ρ31 + ρ2ρ1 + ρ3) if B+(i) ∩B+(j) ∩B+(k) = ∅,\nand\nE[hihj] is { ≤ O(ρ21 + ρ2) if B+(i) ∩B+(j) 6= ∅, ≤ O(ρ21 + ρ3) if B+(i) ∩B+(j) = ∅,\nBasically, most pairs/triples still have small expectation. The following Proposition shows there are only a very small number of pairs that are highly correlated.\nProposition 44 For a fixed set T of size at most 3d in layer 1, when d51 < n, with high probability, the number of pairs i, j ∈ T, i 6= j such that B+(i) ∩B+(j) 6= ∅ is at most 10. Proof: For any node q in layer 2, let Cq = ∣∣{(i, j) : i, j ∈ T, (q, i), (q, j) ∈ E+1\n}∣∣. By randomness of the graph we know E[Cq] ≤ d4/n2. Summing over all nodes in layer 2, we have E[ ∑ q Cq] ≤ d4/n ≤ n−0.2. With high probability, ∑ q Cq ≤ 10. (Note that Cq are independent variables.) 2 With Proposition 43 and 44, we are ready to extend Theorem 24 to the more general distribution. As before we need to bound the variance. When B(u) ∩ B(v) ∩ B(s) = ∅, we bound the variance V = ∑\ni∈B(u),j∈B(v),k∈B(s) E[hihjhk]2 as follows: First of all, there are at most O(d3) triples of (i, j, k) such that i, j, k are distinct. Typically, each one contribute to V with E[hihjhk] ≤ O((ρ31 + ρ2ρ1 + ρ3)2). However, there might be at most 10 triples with higher expectation (by Proposition 44), so the final bound is O((ρ31 + ρ2ρ1 + ρ2) 2). The total contribution of these triples is\nv1 = O(d 3(ρ31 + ρ2ρ1 + ρ3) 2) +O((ρ31 + ρ2ρ1 + ρ2) 2)\nThere are at most 10d triples of (i, j, k) such that i = j ∈ Buv and i 6= k. The contribution of these triples is at most ρ21 +ρ2. Thus the total contribution is at most\nv2 ≤ 10d(ρ21 + ρ2)2). Combining all these we know √ V ≤ √v1 + v2 ≤ d3/2(ρ31 +ρ2ρ1 +ρ3)+(ρ31 +ρ2ρ1 +\nρ2) + 10d 1/2(ρ21 + ρ2).\nUnder the assumption of Theorem 25, √ V log3 n ≤ ρ1/3. With high probability, when u, v, s do not share a common neighbor, we know |E[yuyvys]| ≤ ρ1/3 ; when u, v, s share a unique neighbor z, we know |E[yuyvys]| ≥ 2ρ1/3. Therefore the algorithm can distinguish them.\nModifications in Graph Recovery If edge weights are {±1}, the same algorithm works because with high probability there are no more than a constant number of triples u, v, s in the observed layer that share more than 1 neighbor (this is implied by Proposition 43).\nWhen edge weights are in [−1, 1], the hypergraph only satisfy the following approximate conditions: if u, v, s share a common parent z and the edges from z to u, v, s all have weight outside [−0.01, 0.01], (u, v, s) is an edge in the hypergraph; if u, v, s do not share a common parent, then (u, v, s) is not an edge. All other cases are ambiguous. Graph Recovery in this case can again be done by the same algorithm, because there are only a small constant fraction of ambiguous edges, and the thresholds in the algorithm tolerates a small constant fraction of error.\nModifications in PartialEncoder Note that in this case graph recovery gives all edges, instead of positive edges, so we cannot directly use those for PartialEncoder. However, the expectation E[yuyvys] is positive and large, even if they share a common parent z and there are even number of positive edges from z to u, v, s. Using this information it is easy to separate the positive and negative edges, see Algorithm 9.\nAlgorithm 9. LearnLastLayer\nInput: E, the set of edges Output: E+, E−, the set of positive and negative edges\nfor each node x ∈ h(1) do Pick u, v in F (x) Let S be the set {s : Eh[yuyvys] > 0}. for each t : (x, t) ∈ E do\nif most triples (t, u′, v′)(u′, v′ ∈ S) have positive Eh[ytyu′yv′ ] then add (x, t) to E+ else add (x, t) to E−\nend if end for\nend for\nWhen weights are {+1,−1} we are already done. When the weights are in [−1, 1] we don’t find all the positive edges (1% of the edges are missing because they have small weights), but this is again OK because the PartialEncoder can tolerate small constant fraction of error.\nModifications in Learning Decoder For learning decoder, since we do not have thresholds at the last layer, we only need to solve a system of linear equations."
    }, {
      "heading" : "M Standard Concentration Bounds",
      "text" : "We use the following version of Hoeffding’s inequality.\nTheorem 45 [Hoe63] X1, . . . , Xn are independent random variables such that Xi ∈ [ai, bi] almost surely. Let S = X1 + · · ·+Xn. Then\nPr[|S − E[S]| ≥ t] ≤ 2 exp( −2t 2\n∑n i=1(ai − bi)2 )\nUsing this (and with union bound) it is easy to prove the following bilinear and trilinear bounds (these are not tight but we don’t mind losing log n factors).\nLemma 46 If w, w′ are independent random vectors in {−1, 1}d, then with high probability\n| ∑\ni,j\nwiw ′ jAi,j| ≤ O\n  √∑\ni,j\nA2i,j log 2 n\n \nProof: We are going to bound xTAy, which is equal to\nxTAy = d∑\ni=1\nxi\n(∑\nj\nAi,jyj\n)\nWith high probability over the choice of y, for any i, ∑ j Ai,jyj ≤ O( √∑ j Ai,j 2 log n).\nWhen these events happen, we have that\nxTAy = d∑\ni=1\nxi\n(∑\nj\nAi,jyj\n) ≤ ∑\ni\nxiO(\n√∑\nj\nAi,j 2 log n).\nBy Hoeffding inequality again, we have that with high probability over the choice of x,\nxTAy ≤ ∑\ni\nxiO\n  √∑\nj\nAi,j 2 log n   ≤ O   √∑\ni,j\nAi,j 2 log2 n\n \n2\nLemma 47 If w, w′, w′′ are independent random vectors in {−1, 1}d, then with high probability\n| ∑\ni,j,k\nwiw ′ jw ′′ kAi,j,k| ≤ O\n  √∑\ni,j,k\nA2i,j,k log 3 n\n \nThe proof is very similar to Lemma 46."
    } ],
    "references" : [ {
      "title" : "A spectral algorithm for latent Dirichlet allocation",
      "author" : [ "Anima Anandkumar", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Yi-Kai Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning topic models – going beyond svd",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra" ],
      "venue" : "In IEEE 53rd Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "New algorithms for learning incoherent and overcomplete dictionaries",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra" ],
      "venue" : "ArXiv, 1308.6273,",
      "citeRegEx" : "Arora et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2013
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bengio.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2009
    }, {
      "title" : "Combining geometry and combinatorics: a unified approach to sparse signal recovery",
      "author" : [ "R. Berinde", "A.C. Gilbert", "P. Indyk", "H. Karloff", "M.J. Strauss" ],
      "venue" : "In 46th Annual Allerton Conference on Communication,",
      "citeRegEx" : "Berinde et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Berinde et al\\.",
      "year" : 2008
    }, {
      "title" : "Graph reconstructiona survey",
      "author" : [ "J Adrian Bondy", "Robert L Hemminger" ],
      "venue" : "Journal of Graph Theory,",
      "citeRegEx" : "Bondy and Hemminger.,? \\Q1977\\E",
      "shortCiteRegEx" : "Bondy and Hemminger.",
      "year" : 1977
    }, {
      "title" : "Kernel methods for deep learning",
      "author" : [ "Youngmin Cho", "Lawrence Saul" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Cho and Saul.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cho and Saul.",
      "year" : 2009
    }, {
      "title" : "Compressed sensing",
      "author" : [ "David L Donoho" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Donoho.,? \\Q2006\\E",
      "shortCiteRegEx" : "Donoho.",
      "year" : 2006
    }, {
      "title" : "The Organization of Behavior: A Neuropsychological Theory. Wiley, new edition",
      "author" : [ "Donald O. Hebb" ],
      "venue" : null,
      "citeRegEx" : "Hebb.,? \\Q1949\\E",
      "shortCiteRegEx" : "Hebb.",
      "year" : 1949
    }, {
      "title" : "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
      "author" : [ "Daniel Hsu", "Sham M. Kakade" ],
      "venue" : "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Hsu and Kakade.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hsu and Kakade.",
      "year" : 2013
    }, {
      "title" : "A spectral algorithm for learning hidden Markov models",
      "author" : [ "Daniel Hsu", "Sham M. Kakade", "Tong Zhang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2012
    }, {
      "title" : "Learnability beyond ac",
      "author" : [ "Jeffrey C Jackson", "Adam R Klivans", "Rocco A Servedio" ],
      "venue" : "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Jackson et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Jackson et al\\.",
      "year" : 2002
    }, {
      "title" : "Cryptographic hardness for learning intersections of halfspaces",
      "author" : [ "Adam R Klivans", "Alexander A Sherstov" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Klivans and Sherstov.,? \\Q2009\\E",
      "shortCiteRegEx" : "Klivans and Sherstov.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "A provably efficient algorithm for training deep",
      "author" : [ "Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir" ],
      "venue" : "networks. ArXiv,",
      "citeRegEx" : "Livni et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Livni et al\\.",
      "year" : 2013
    }, {
      "title" : "Settling the polynomial learnability of mixtures of gaussians",
      "author" : [ "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "In the 51st Annual Symposium on the Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Moitra and Valiant.,? \\Q2010\\E",
      "shortCiteRegEx" : "Moitra and Valiant.",
      "year" : 2010
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an n node multilayer neural net that has degree at most nγ for some γ < 1 and each edge has a random edge weight in [−1, 1]. Our algorithm learns almost all networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights. 1",
    "creator" : "LaTeX with hyperref package"
  }
}