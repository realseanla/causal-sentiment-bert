{
  "name" : "1703.01203.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic Separation Theorems",
    "authors" : [ "A.N. Gorbana", "I.Y. Tyukin" ],
    "emails" : [ "ag153@le.ac.uk", "it37@le.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n01 20\n3v 3\n[ cs\n.L G\n] 3\nA ug\n2 01\n7\nThe problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Based on fundamental properties of measure concentration, we show that for M < a exp(bn) random Melement sets in Rn are linearly separable with probability p, p > 1 − ϑ, where 1 > ϑ > 0 is a given small constant. Exact values of a, b > 0 depend on the probability distribution that determines how the random M-element sets are drawn, and on the constant ϑ. These stochastic separation theorems provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples.\nKeywords: Fisher’s discriminant, random set, measure concentration, linear separability, machine learning, extreme point"
    }, {
      "heading" : "1. Introduction",
      "text" : "Artificial Intelligence (AI) systems make errors. They should be corrected without damage of existing skills. The problem of non-destructive correction arises in many areas of research and development, from AI to mathematical neuroscience, where the reverse engineering of the brain ability to learn on-the-fly remains a great challenge. It is very desirable that the corrector of errors is non-iterative (one-shot) because iterative re-training of a large system requires much time and resource and cannot be done immediately without impeding activity.\nThe non-desrructive correction requires separation of the situations (samples) with errors from the samples corresponding to correct behavior by a simple and robust classifier. Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data. Rosenblatt (1962) revived the common interest in linear classifiers. His works sparked intensive scientific debate (Minsky and Papert, 1969) and gave rise to development of numerous crucial concepts such as e.g. Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002). Linear functionals (adaptive summators) are basic building blocks of significantly more sophisticated AI systems such as e.g. multilayer perceptrons, (Rumelhart et al., 1986), Convolutional Neural Networks (Le Cun and Bengio, 1995), (LeCun et al., 2015) and their derivatives. Much is known about linear functionals\n∗Corresponding author Email addresses: ag153@le.ac.uk (A.N. Gorban), it37@le.ac.uk\n(I.Y. Tyukin)\nas “stand-alone” learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.\nIn this work, we demonstrate that in high dimensions and even for exponentially large samples, linear classifiers in their classical Fisher’s form are powerful enough to separate errors from correct responses with high probability and to provide efficient solution to the non-destructive corrector problem. We prove that linear functionals, as learning machines, have surprising and, as far as we are concerned, new peculiar extremal properties: in high dimension, with probability p > 1 − ϑ and for M < a exp(bn) with a, b > 0 every point in random i.i.d. drawn M-element sets in Rn is linearly separable from the rest. Moreover, the separating linear functional can be found explicitly, without iterations. This property holds for a broad set of relevant distributions, including products of probability measures with bounded support and equidistribution in a unit ball, providing mathematical foundations for one-trial correction of legacy AI systems (cf. (Gorban et al., 2016a)).\nA problem of data fusion in multiagent systems has clear similarity to the problem of non-destructive correction. According to Forney et al. (2017), data collected by different agents may not be naively combined due to changes in the context, and special procedures for their assimilation without damage of gained skills are needed. The proven stochastic separation effects can be used to approach this problem. They also shed light on the possible origins of remarkable selectivity to stimuli observed in-vivo in the real brain (Quian Quiroga et al., 2005).\nPreprint submitted to Elsevier August 4, 2017"
    }, {
      "heading" : "2. Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1. Notation",
      "text" : "Throughout the text, Rn is the n-dimensional linear real vector space. Unless stated otherwise, symbols xi = (xi,1, . . . , xi,n) denote elements of Rn, and ( xi, x j ) = ∑ k xi,kx j,k is the inner product of xi and x j in R n. Symbol Bn stands for the unit ball in Rn centered at the origin: Bn = {x ∈ Rn| (x, x) ≤ 1}."
    }, {
      "heading" : "2.2. Linear Separability of Sets",
      "text" : "Definition A set S ⊂ Rn is linearly separable if for each x ∈ S there exists a linear functional l such that l(x) > l(y) for all y ∈ S , y , x.\nRecall that x ∈ Rn is an extreme point of a convex compact K if there exist no points y, z ∈ K, y , z such that x = (y + z)/2. The basic examples of linearly separable sets are extreme points of a convex compacts: vertices of convex polyhedra or points on the n-dimensional sphere. The sets of extreme points of a compact may be not linearly separable as is demonstrated by simple 2D examples (Simon, 2011).\nProposition 1. Every compact linearly separable set S ⊂ Rn is a set of extreme points of a convex compact K = convS .\nThe proof follows immediately from the previous definitions, the Krein-Milman theorem (Simon, 2011) (its finitedimensional form was known to Minkovsky) and classical theorems about separation of a point from a convex set.\nIf n + 1 points in Rn do not belong to a hyperplane then they are vertices of a simplex and are, obviously, linearly separable. If the lengths of the edges are bounded then the volume of the simplex decreases with n not slower than 1/n!. Therefore, we can expect that for a sufficiently regular distribution of points, a random point does not belong to the simplex, and n+2 independently chosen random points are also linearly separable, with high probability, which tends to 1 as 1 − c/n! (n → ∞, c > 0). This fast convergence allows us to hypothesize that even a large random finite set is linearly separable in high dimension with high probability, if the distribution is regular enough. We prove this statement below for i.i.d. random points from equidistributions in a ball and a cube, and from distributions that are products of measures with bounded support."
    }, {
      "heading" : "3. Main Results",
      "text" : "Let us start from the equidistribution in the unit ball Bn in R n. The probability p that a random point belongs to a layer Bn \\ rBn (0 < r < 1) between spheres of radius 1 and of radius r is p = 1 − rn. Let us take a unit vector v. The probability that the projection of a random vector x on v, (x, v), exceeds r can be estimated from above by half of the ratio of volumes of balls of radii ρ = √ 1 − r2 and 1 (see Fig. 1 with ε = 1 − r): P((x, v) > r) ≤ 0.5ρn.\nTheorem 1. Let {x1, . . . , xM} be a set of M i.i.d. random points from the equidustribution in the unit ball Bn, 0 < r < 1. Then\nP\n( ‖xM‖ > r and ( xi, xM\n‖xM‖\n)\n< r for all i , M\n)\n≥ 1 − rn − 0.5(M − 1)ρn; (1)\nP\n( ‖x j‖ > r and ( xi, x j\n‖x j‖\n)\n< r for all i, j, i , j\n)\n≥ 1 − Mrn − 0.5M(M − 1)ρn; (2)\nP\n( ‖x j‖ > r and ( xi\n‖xi‖ ,\nx j\n‖x j‖\n)\n< r for all i, j, i , j\n)\n≥ 1 − Mrn − M(M − 1)ρn. (3)\nThe proof is based on the independence of random points {x1, . . . , xM}, on the geometric picture presented in Fig. 1, and on an elementary inequality P(A1&A2& . . .&Am) ≥ 1 − ∑\ni(1 − P(Ai)) for any events A1, . . . , Am. In Fig. 1 we should take ε = 1 − r and the external radius of the spherical layer A is 1. Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r. The idea of almost orthogonal bases was introduced by Kainen and Kůrková (1993) and used efficiently by Kůrková and Sanguineti (2007) for estimation of the cardinality of ε-nets in compact convex subsets of Hilbert spaces including the sets of functions computable by perceptrons.\nThe following corollary gives simple estimates of exponential growth of the maximal possible M for which inequalities (1) and (2) hold with a given probability value.\nCorollary 1. Let {x1, . . . , xM} be a set of M i.i.d. random points from the equidustribution in the unit ball Bn and 0 < r, ϑ < 1. If\nM < 2(ϑ − rn)/ρn, (4)\nthen P((xi, xM) < r‖xM‖ for all i = 1, . . . ,M − 1) > 1 − ϑ. If\nM < (r/ρ)n ( −1 + √ 1 + 2ϑρn/r2n ) , (5)\nthen P((xi, x j) < r‖xi‖ for all i, j = 1, . . . ,M, i , j) ≥ 1 − ϑ. In particular, if inequality (5) holds then the set {x1, . . . , xM} is linearly separable with probability p > 1 − ϑ.\nA weaker and simpler estimate (sufficient condition) follows\nimmediately from (5):\nϑ/M2 > rn + 0.5ρn. (6)\nRemark 1. According to (6) the pre exponential factor in the estimate for M2 may be chosen as ϑ, while the exponent depends on r only. For example, for r = 1/ √ 2 the simple sufficient condition (6) gives M2 < 2 3 ϑ2n/2. For ϑ = 0.01 (or specificity 99%) and n = 100 we get M < 2, 740, 000.\nThus, if we select 2,700,000 i.i.d. points from an equidistribution in a unit ball in R100 then with probability p > 0.99 all these points will be vertices of their convex hull.\nEstimates similar to (3), (5), and (6) are useful for the equidistribution of the normalized data on a unit sphere too. This is because they not only establish the fact of separability but also specify separation margins.\nConsider a product distribution in an n-dimensional unit cube. Let the coordinates of a random point, X1, . . . , Xn (0 ≤ Xi ≤ 1) be independent random variables with expectations Xi and variances σ2\ni > σ2 0 > 0. Let x be a vector with coordinates\nXi. For large n, this distribution is concentrated in a relatively small vicinity of a sphere with an arbitrary centre c with coordinates ci and radius R, where\nR2 = E\n       ∑\ni\n(Xi − ci)2      \n\n= ∑\ni\nσ2i + ‖x − c‖2. (7)\nConcentration near the spheres with different centres implies concentration in the vicinity of their intersection (an example of the ‘waist concentration’ (Gromov, 2003)). The vicinity of the spheres, where the distribution is concentrated, can be estimated by the Hoeffding inequality (Hoeffding, 1963). Let Y1, . . . , Yn be independent bounded random variables: 0 ≤ Yi ≤ 1. The empirical mean of these variables is defined as Y = 1\nn (Y1 + · · · + Yn). Then\nP ( Y − E [ Y ] ≥ t ) ≤ exp ( −2nt2 ) ;\nP\n(∣\n∣ ∣ ∣ Y − E [ Y ]\n∣ ∣ ∣ ∣ ≥ t ) ≤ 2 exp ( −2nt2 ) . (8)\nLet us take Yi = (Xi − ci)2. Consider the centres located in the cube, 0 ≤ ci ≤ 1. Then 0 ≤ Yi ≤ 1 and E [ Y ] = 1 n R2. In particular, if ci = Xi then E [ Y ] = 1 n R2 0 (the minimal possible value), where R2 0 = ∑ i σ 2 i ≥ nσ2 0 . In general, nσ2 0 ≤ R2 ≤ n.\nWith probability p > 1 − 2 exp ( −2nt2 ) a random point x\nbelongs to the spherical layer (δ = nt/R2 0 , t = δR2 0 /n):\n1 − δ ≤ ‖x − x‖2/R20 ≤ 1 + δ. (9)\nConsider M i.i.d. points {x1, . . . , xM} from the product distribution. With probability p > 1−2M exp ( −2nt2 ) they all belong to the spherical layer (9). Therefore, with this probability we return to the situation presented in Fig. 1 with internal radius√ 1 − δR0 and external radius √ 1 + δR0. The difference from the equidistribution in the ball is that the volume of the ball is concentrated near the external sphere, while the distribution in the layer (9) is concentrated around the sphere ‖x − x‖2 = R2 0 .\nThe radius of ball B is defined by ρ2 = (1+δ)R2 0 − (1−δ)R2 0 = 2δR2 0 . The concentration radius (7) for the spheres concentric with the ball B (Fig. 1) is defined by R2 = R2 0 + (1 − δ)R2 0 = (2−δ)R2 0 . Therefore, a random point does not belong to the ball Bwith probability p > 1−exp ( −2nτ2 )\n, where τ = 1 n (R2−ρ2) =\n1 n (2 − 3δ)R2 0 . Thus, we get the following statement.\nTheorem 2. Let {x1, . . . , xM} be i.i.d. random points from the product distribution in a unit cube, 0 < δ < 2/3. Then\nP\n      1 − δ ≤ ‖x j − x‖2\nR2 0 ≤ 1 + δ and (\nxi − x R0 , xM − x ‖xM − x‖\n) < √ 1 − δ for all i, j, i , M )\n≥ 1 − 2M exp ( −2δ2R40/n ) − (M − 1) exp ( −2R40(2 − 3δ)2/n ) ;\n(10)\nP\n      1 − δ ≤ ‖x j − x‖2\nR2 0 ≤ 1 + δ and (\nxi − x R0 , x j − x ‖x j − x‖\n) < √ 1 − δ for all i, j, i , j )\n≥ 1 − 2M exp ( −2δ2R40/n ) − M(M − 1) exp ( −2R40(2 − 3δ)2/n )\n(11)\nWhen the value of delta is chosen as δ = 0.5 and R0 is replaced with its estimate from below, R2 0 ≥ nσ2 0 , inequalities (10) and (11) result in the following estimates:\nP\n      1\n2 ≤\n‖x j − x‖2\nR2 0\n≤ 3 2 and\n(\nxi − x R0 , xM − x ‖xM − x‖\n) < √ 1 − δ for all i, j, i , M )\n≥ 1 − 3M exp ( −0.5nσ40 ) ;\n(12)\nP\n      1\n2 ≤\n‖x j − x‖2\nR2 0\n≤ 3 2 and\n(\nxi − x R0 , x j − x ‖x j − x‖\n) < √ 1 − δ for all i, j, i , j )\n≥ 1 − M(M + 1) exp ( −0.5nσ40 ) .\n(13)\nCorollary 2. Let {x1, . . . , xM} be i.i.d. random points from the product distribution in a unit cube and 0 < ϑ < 1. If\nM < 1\n3 ϑ exp\n(\n0.5nσ40\n)\n, (14)\nthen with probability p > 1 − ϑ\n0.5 ≤ ‖x j − x‖2\nR2 0\n≤ 1.5 and ( xi − x R0 , xM − x ‖xM − x‖ ) <\n√ 2\n2\nfor all i, j, i , M.\nIf\n(M + 1)2 < 1\n3 ϑ exp\n(\n0.5nσ40\n)\n, (15)\nthen with probability p > 1 − ϑ\n0.5 ≤ ‖x j − x‖2\nR2 0\n≤ 1.5 and ( xi − x R0 , x j − x ‖x j − x‖ ) <\n√ 2\n2\nfor all i, j, i , j.\nIn particular, if inequality (15) holds then the set {x1, . . . , xM} is linearly separable with probability p > 1 − ϑ.\nThe estimates (14), (15) are far from being optimal and can be improved. The main message here is their exponential dependence on n: the upper boundary of M can grow with n exponentially. Numerical experiments show that the equidistribution in cube is not worse, from the practical point of view, than the uniform distribution in a ball. To illustrate this, we empirically assessed linear separability of samples drawn from equidistributions in the unit n-cubes. For selected values of n from the set {1, . . . , 5000} we generated 100 samples S of M = 20000 random points from [0, 1]n. For each sample, a sub-sample S ⊂ S of N = 4000 points was randomly chosen, and for each point xi in this sub-sample linear functionals l(x) = (xi − x̄, x − x̄) − ‖xi − x̄‖2 were constructed. Sings of l(x j), x j ∈ S , x j , xi were calculated, and the numbers N− of instances when l(x j) < 0 where recorded. Empirical frequencies N−/N were then derived. Outcomes of this experiment are summarized in Fig. 2. These experiments demonstrate that the probability that a randomly selected point in a sample is linearly separable from the rest could be significantly higher than the simple exponential estimates provided. This, however, is not surprising as the estimates are based on the values of means and variances, and do not take into account other quantitative properties of the sample distribution.\nIn general position, a set of n points in Rn−1 is linearly separable. Therefore, if n−1 or less points fromM = {x1, . . . , xM−1} are not separated from x by the hyperplane L (Fig. 1) then they can be separated by an additional hyperplane orthogonal to L. This means that x can be separated from the wholeM by a conjunction of two linear inequalities, (•, x/‖x‖) > r& (•, y) > q, for some 0 < r < 0, q > 0, and y, (y, x) = 0. This system can be considered as a cascade of two independent neurons (Gorban et al., 2016a). The probability of such a two-neuron separability is higher than of linear separability. (Compare inequality (16) in the following theorem to (1).)\nTheorem 3. Let S = {x1, . . . , xM} be a set of M i.i.d. random points from the equidustribution in the unit ball Bn, 0 < r < 1. Then\nP\n( ‖xM‖ > r& ( xi, xM\n‖xM‖\n) < r for at least M−n points xi ∈ S )\n≥ (1 − rn)(1 − 0.5ρn)M−1\n× (\n1 − 1 n!\n(\n0.5(M − n)ρn 1 − 0.5ρn\n)n)\nexp\n[\n0.5(M − n)ρn 1 − 0.5ρn\n]\n,\n(16)\nwhere ρ = √ 1 − r2.\nFor r = 1/ √ 2, n = 100, and M = 2, 74 · 106, (16) gives:\nP ( ‖xM‖ > r& ( xi, xM\n‖xM‖\n) < r for at least M−n xi ∈ S )\n≥ 1 − θ with θ < 5 · 10−14. The probability stays close to 1 for much larger values ofM, as settingM = 7·1016 results in the estimate: P ( ‖xM‖ > r& ( xi, xM\n‖xM‖\n) < r for at least M−n xi ∈ S )\n≥ 1 − θ with θ < 5 · 10−9."
    }, {
      "heading" : "4. Conclusion",
      "text" : "Classical measure concentration theorems state that random points are concentrated in a thin layer near a surface (a sphere, an average or median level set of energy or another function, etc.). The stochastic separation theorems describe thin structure of these thin layers: the random points are not only concentrated in a thin layer but are all linearly separable from the rest of the set even for exponentially large random sets. The estimates are produced for two classes of distributions in high dimension: for equidistributions in balls or ellipsoids or for the product distributions with compact support (i.e. for the case when coordinates are bounded independent random variables). Numerous generalisations are possible, for example:\n• Relax the requirement of independent coordinates in Theorem 2 to that of weakly dependent vector-valued vari-\nables;\n• Instead of equidistributions, consider distributions with strongly log-concave probability densities;\n• Use various simple and robust nonlinear classifiers like small neural cascades (compare to Theorem 3), algebraic\nclassifiers and other families. For these generalisations, the VC dimension is expected to play the role similar to dimension n in Theorems 1 and 2.\nStochastic separation Theorems 1–3 are important for synthesis and one-shot correction of AI systems. For example, inequalities (1) and (10) evaluate the probability that a randomly selected point xM is linearly separable from all other M − 1 points by the linear functional l(x) = (x, xM − x). This separation is sufficient to correct a mistake of a legacy AI system without any re-learning and modification of existing skills (Gorban et al., 2016a). Such measure concentration effects reveal the hidden geometric background of the reported success of randomized neural networks models (Scardapane and Wang, 2017).\nStochastic separation theorems can simplify highdimensional data analysis and generate the ’blessing of dimensionality’ (Gorban et al., 2016c). For example, according to (6), in a dataset with 100 attributes and M < 2.7 · 106 samples we should not be surprised to observe the linear separability of each sample from the rest of the database by the inequalities 〈xi, x j〉 < √ 1 2 〈xi, xi〉 (i , j) in the Mahalanobis inner product 〈x, y〉 = (x, S −1y), where S is the empirical covariance matrix. The Mahalanobis inner product is used for ‘whitening’, i.e. for transformation of the data cloud into the spherical form. Of course, these attributes should not be highly correlated and the empiric covariance matrix should be invertible.\nWe analysed separation of random points from random sets. This is the problem of single correction of a legacy AI system. The question of generalisability of this correction is of great practical importance. It leads to a problem of separation of two random sets. A simple series of generalisations can be immediately produced from Theorems 1-3 for separation of an Melement random set S = {x1, . . . , xM} ⊂ Rn from a k-element one {y1, . . . , yk} for k < n. For this purpose, we can consider a linear space E = span{yi− y1 | i = 2, . . . , k} and study separation of a point from an M-element set in the projection onto the quotient space Rn/E. If y1, . . . , yk are independent then separation would likely be limited to sets of small cardinality k < n. If, in contrast, y1, . . . , yk are pair-wise positively correlated then we can expect that a single functional would separate them from S , with reasonable probability even for some k ≥ n. This naturally gives rise to generalization of corrections.\nThe reported extreme separation capabilities of linear functionals offer new insights into the Grandmother cell or concept cell phenomena that are broadly reported in neuroscience (Quian Quiroga et al., 2005), (Viskontasa et al., 2009). The essence of the phenomenon is that some neurons in the human brain respond unexpectedly selective to particular persons or objects. Strikingly, not only is the brain able to respond selectively to “rare” individual stimuli but also such selectivity\ncan be learnt very rapidly from a limited number of experiences (Ison et al., 2015). The question is: Why small ensembles of neurons may deliver such a sophisticated functionality reliably? Stochastic separation Theorems 1-3 provide a possible answer. If we accept that a) linear functionals followed by nonlinear threshold-modulated response as phenomenological models of cells whose activity was measured, b) the number of inputs converging to these cells is large enough, and c) they are statistically independent, then extreme selectivity of responses of such models follows immediately from Theorem 2."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Authors are grateful to M. Gromov and S. Utev for seminal questions and comments. The work was partially supported by Innovate UK (KTP009890 and KTP010522)."
    } ],
    "references" : [ {
      "title" : "An elementary introduction to modern convex geometry",
      "author" : [ "K. Ball" ],
      "venue" : "Flavors of geometry",
      "citeRegEx" : "Ball,? \\Q1997\\E",
      "shortCiteRegEx" : "Ball",
      "year" : 1997
    }, {
      "title" : "Stability and generalization",
      "author" : [ "O. Bousquet", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research ,",
      "citeRegEx" : "Bousquet and Elisseeff,? \\Q2002\\E",
      "shortCiteRegEx" : "Bousquet and Elisseeff",
      "year" : 2002
    }, {
      "title" : "The use of multiple measurements in taxonomic problems",
      "author" : [ "R.A. Fisher" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Fisher,? \\Q1936\\E",
      "shortCiteRegEx" : "Fisher",
      "year" : 1936
    }, {
      "title" : "Counterfactual DataFusion for Online Reinforcement Learners. A talk at the 1st Workshop on Transfer in Reinforcement Learning at AAMAS 2017",
      "author" : [ "A. Forney", "J. Pearl", "E. Bareinboim" ],
      "venue" : "May 8–9,",
      "citeRegEx" : "Forney et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Forney et al\\.",
      "year" : 2017
    }, {
      "title" : "Large margin classification using the perceptron algorithm",
      "author" : [ "Y. Freund", "R. Schapire" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Freund and Schapire,? \\Q1999\\E",
      "shortCiteRegEx" : "Freund and Schapire",
      "year" : 1999
    }, {
      "title" : "2016a. Onetrial correction of legacy AI systems and stochastic separation theorems",
      "author" : [ "A.N. Gorban", "I. Romanenko", "R. Burton", "I. Tyukin" ],
      "venue" : null,
      "citeRegEx" : "Gorban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gorban et al\\.",
      "year" : 2016
    }, {
      "title" : "2016b. Approximation with random bases: Pro et contra",
      "author" : [ "A.N. Gorban", "I. Tyukin", "D. Prokhorov", "K. Sofeikov" ],
      "venue" : "Information Sciences",
      "citeRegEx" : "Gorban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gorban et al\\.",
      "year" : 2016
    }, {
      "title" : "The blessing of dimensionality: Separation theorems in the thermodynamic limit",
      "author" : [ "A.N. Gorban", "I.Y. Tyukin", "I. Romanenko" ],
      "venue" : "IFACPapersOnLine, 49,",
      "citeRegEx" : "Gorban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gorban et al\\.",
      "year" : 2016
    }, {
      "title" : "Isoperimetry of waists and concentration of maps. GAFA",
      "author" : [ "M. Gromov" ],
      "venue" : "Geomteric and Functional Analysis",
      "citeRegEx" : "Gromov,? \\Q2003\\E",
      "shortCiteRegEx" : "Gromov",
      "year" : 2003
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "J. Amer. Statist. Assoc",
      "citeRegEx" : "Hoeffding,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding",
      "year" : 1963
    }, {
      "title" : "Rapid encoding of new memories by individual neurons in the human",
      "author" : [ "M. Ison", "R. Quian Quiroga", "I. Fried" ],
      "venue" : "brain. Neuron",
      "citeRegEx" : "Ison et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ison et al\\.",
      "year" : 2015
    }, {
      "title" : "Quasiorthogonal dimension of euclidian spaces",
      "author" : [ "P. Kainen", "V. Kůrková" ],
      "venue" : "Appl. Math. Lett",
      "citeRegEx" : "Kainen and Kůrková,? \\Q1993\\E",
      "shortCiteRegEx" : "Kainen and Kůrková",
      "year" : 1993
    }, {
      "title" : "Estimates of covering numbers of convex sets with slowly decaying orthogonal subsets",
      "author" : [ "V. Kůrková", "M. Sanguineti" ],
      "venue" : "Discrete Applied Mathematics",
      "citeRegEx" : "Kůrková and Sanguineti,? \\Q2007\\E",
      "shortCiteRegEx" : "Kůrková and Sanguineti",
      "year" : 2007
    }, {
      "title" : "Convolutional networks for images, speech, and time series",
      "author" : [ "Y. Le Cun", "T. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Cun and Bengio,? \\Q1995\\E",
      "shortCiteRegEx" : "Cun and Bengio",
      "year" : 1995
    }, {
      "title" : "On learning sets and functions",
      "author" : [ "B. Natarajan" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Natarajan,? \\Q1989\\E",
      "shortCiteRegEx" : "Natarajan",
      "year" : 1989
    }, {
      "title" : "Invariant visual representation by single neurons in the human brain",
      "author" : [ "R. Quian Quiroga", "L. Reddy", "G. Kreiman", "C. Koch", "I. Fried" ],
      "venue" : "Nature 435,",
      "citeRegEx" : "Quiroga et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Quiroga et al\\.",
      "year" : 2005
    }, {
      "title" : "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
      "author" : [ "F. Rosenblatt" ],
      "venue" : "Spartan Books",
      "citeRegEx" : "Rosenblatt,? \\Q1962\\E",
      "shortCiteRegEx" : "Rosenblatt",
      "year" : 1962
    }, {
      "title" : "Learning internal representations by error propagation, in: Rumelhart, D.E. McClelland, J., the PDP research group (Eds.), Parallel distributed processing: Explorations",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "Randomness in neural networks: an overview",
      "author" : [ "S. Scardapane", "D. Wang" ],
      "venue" : "WIREs Data Mining Knowl. Discov. 7,",
      "citeRegEx" : "Scardapane and Wang,? \\Q2017\\E",
      "shortCiteRegEx" : "Scardapane and Wang",
      "year" : 2017
    }, {
      "title" : "Convexity, An Analytic Viewpoint, Cambridge Tracts in Mathematics, V. 187",
      "author" : [ "B. Simon" ],
      "venue" : null,
      "citeRegEx" : "Simon,? \\Q2011\\E",
      "shortCiteRegEx" : "Simon",
      "year" : 2011
    }, {
      "title" : "The Nature of Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik,? \\Q2000\\E",
      "shortCiteRegEx" : "Vapnik",
      "year" : 2000
    }, {
      "title" : "Estimation of Dependences Based on Empirical Data",
      "author" : [ "V.N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik,? \\Q1982\\E",
      "shortCiteRegEx" : "Vapnik",
      "year" : 1982
    }, {
      "title" : "On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "V.N. Vapnik", "A.Y. Chervonenkis" ],
      "venue" : "Theory of Probability and Its Applications",
      "citeRegEx" : "Vapnik and Chervonenkis,? \\Q1971\\E",
      "shortCiteRegEx" : "Vapnik and Chervonenkis",
      "year" : 1971
    }, {
      "title" : "Human medial temporal lobe neurons respond preferentially to personally relevant images",
      "author" : [ "I. Viskontasa", "R. Quian Quiroga", "I. Fried" ],
      "venue" : "Proc. Nat. Acad. Sci. 120,",
      "citeRegEx" : "Viskontasa et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Viskontasa et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).",
      "startOffset" : 27,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).",
      "startOffset" : 143,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).",
      "startOffset" : 159,
      "endOffset" : 189
    }, {
      "referenceID" : 17,
      "context" : "multilayer perceptrons, (Rumelhart et al., 1986), Convolutional Neural Networks (Le Cun and Bengio, 1995), (LeCun et al.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data.",
      "startOffset" : 35,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data. Rosenblatt (1962) revived the common interest in linear classifiers.",
      "startOffset" : 35,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "Tyukin) as “stand-alone” learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.",
      "startOffset" : 83,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "Tyukin) as “stand-alone” learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.",
      "startOffset" : 112,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : "Tyukin) as “stand-alone” learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.",
      "startOffset" : 258,
      "endOffset" : 272
    }, {
      "referenceID" : 3,
      "context" : "According to Forney et al. (2017), data collected by different agents may not be naively combined due to changes in the context, and special procedures for their assimilation without damage of gained skills are needed.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "The sets of extreme points of a compact may be not linearly separable as is demonstrated by simple 2D examples (Simon, 2011).",
      "startOffset" : 111,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "The proof follows immediately from the previous definitions, the Krein-Milman theorem (Simon, 2011) (its finitedimensional form was known to Minkovsky) and classical theorems about separation of a point from a convex set.",
      "startOffset" : 86,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r.",
      "startOffset" : 0,
      "endOffset" : 227
    }, {
      "referenceID" : 0,
      "context" : "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r. The idea of almost orthogonal bases was introduced by Kainen and Kůrková (1993) and used efficiently by Kůrková and Sanguineti (2007) for estimation of the cardinality of ε-nets in compact convex subsets of Hilbert spaces including the sets of functions computable by perceptrons.",
      "startOffset" : 0,
      "endOffset" : 374
    }, {
      "referenceID" : 0,
      "context" : "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r. The idea of almost orthogonal bases was introduced by Kainen and Kůrková (1993) and used efficiently by Kůrková and Sanguineti (2007) for estimation of the cardinality of ε-nets in compact convex subsets of Hilbert spaces including the sets of functions computable by perceptrons.",
      "startOffset" : 0,
      "endOffset" : 428
    }, {
      "referenceID" : 8,
      "context" : "Concentration near the spheres with different centres implies concentration in the vicinity of their intersection (an example of the ‘waist concentration’ (Gromov, 2003)).",
      "startOffset" : 155,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "The vicinity of the spheres, where the distribution is concentrated, can be estimated by the Hoeffding inequality (Hoeffding, 1963).",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 18,
      "context" : "Such measure concentration effects reveal the hidden geometric background of the reported success of randomized neural networks models (Scardapane and Wang, 2017).",
      "startOffset" : 135,
      "endOffset" : 162
    }, {
      "referenceID" : 23,
      "context" : ", 2005), (Viskontasa et al., 2009).",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "Strikingly, not only is the brain able to respond selectively to “rare” individual stimuli but also such selectivity can be learnt very rapidly from a limited number of experiences (Ison et al., 2015).",
      "startOffset" : 181,
      "endOffset" : 200
    } ],
    "year" : 2017,
    "abstractText" : "The problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Based on fundamental properties of measure concentration, we show that for M < a exp(bn) random Melement sets in R are linearly separable with probability p, p > 1 − θ, where 1 > θ > 0 is a given small constant. Exact values of a, b > 0 depend on the probability distribution that determines how the random M-element sets are drawn, and on the constant θ. These stochastic separation theorems provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples.",
    "creator" : "LaTeX with hyperref package"
  }
}