{
  "name" : "1411.5007.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Unified View of Large-scale Zero-sum Equilibrium Computation",
    "authors" : [ "Kevin Waugh", "J. Andrew Bagnell" ],
    "emails" : [ "waugh@cs.cmu.edu", "dbagnell@ri.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n50 07\nv1 [\ncs .A\nI] 1\n8 N\nov 2\n01 4"
    }, {
      "heading" : "Introduction",
      "text" : "The first annual Computer Poker Competition was held in 2007 providing a testbed for adversarial decision-making with imperfect information. Though incredible advancements have been made since then, the solution of an abstract game still remains a critical component of the top agents. The strength of such a strategy correlates with how well the abstract game models the intractably large full game. Algorithmic improvements in equilibrium computation enable teams to solve larger abstract games and thus improve the overall strength of their agents.\nThe first agents used off-the-shelf linear programming packages to solve the abstract games. Shortly after, two specialized equilibrium-finding techniques emerged drastically reducing resource requirements by allowing implicit game representations. One method, counterfactual regret minimization (CFR), combines many simple no-regret learners together to minimize overall regret and as a whole they converge to an equilibrium (Zinkevich et al. 2008). The other, an application of Nesterov’s excessive gap technique (EGT, 2005), is a gradient method that attacks the convex-concave saddle-point formulation directly (Gilpin et al. 2007).\nCurrently, the two techniques are thought of as simply different and competing. Though both improved, the advancements and the methods remained isolated. At this point, CFR has more widespread adoption due to its simplicity and the power of the sampling schemes available to it.\nCopyright c© 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nIn this paper we connect CFR and EGT. Specifically, both define Bregman divergences with the same structure. This viewpoint allows us to make important connections between the two as well as current research on convex optimization and no-regret learning. In particular, we show that CFR can be thought of as smoothed fictitious play and its dual weights are a function of the opponent’s average strategy; with the appropriate step-size the primal iterate converges to the solution (as opposed to the average); and that a convergence rate of O(1/T ) can be achieved while sampling."
    }, {
      "heading" : "Zero-Sum Extensive-form Games",
      "text" : "A extensive-form game is a tuple Γ = (N,H, p, σc, I, u) (see, e.g., Osborne and Rubinstein). The game has N players, the set of which is [N ]. The set of histories, H, form a tree rooted at φ, the empty history. For a history h ∈ H, we denote the set of available actions by A(h). For any action a ∈ A(h), the history ha ∈ H is a child of h. The tree’s leaves, or terminal histories, are denoted by Z ⊆ H. At a non-terminal history the player choice function, p : H\\Z → [N ]∪{c}, determines who is to act, either a player or nature. Nature’s policy is denoted σc defines a distribution over actions when it is to act σc(·|h) ∈ ∆A(h). The information partition, I = ∪i∈[N ]Ii, is a partition the players’ histories. All histories in an information set are indistinguishable to the player to act. We have ∀I ∈ I, h, h′ ∈ I , that p(h) = p(h′) and A(h) = A(h′). Finally, at a terminal history, z ∈ Z , the utility for player i, ui : Z → R, determines the reward for player i reaching terminal z.\nEach player plays the game by means of a behavioral strategy. A behavioral strategy for player i, σi ∈ Σi, maps histories to distributions over actions, σi(·|h) ∈ ∆A(h). When the player is to act, their choice is drawn from this distribution. A strategy must respect the information partition, ∀I ∈ Ii, h, h′ ∈ I, σi(·|h) = σi(·|h′). We call the tuple of N strategies, (σ1, . . . , σN ), a strategy profile.\nThere are two additional properties we require of the extensive-form games we examine. First, we consider twoplayer zero-sum games. That is, N = 2 and u2(z) = −u1(z); what one player wins the other loses.\nSecond, we will consider games of perfect recall— neither player is forced forget any information they once knew. Mathematically this requirement is that all histories in an information set share the same sequence of information\nsets and actions from the point-of-view of the acting player. With these additional restrictions, we can conveniently represent a game in its sequence form, Γ = (A,E, F ). Following from perfect recall, any sequence of information set/action pairs, henceforth simply sequence, is uniquely identified by its final element. Consequently, we can represent behavioral strategies as vectors, called realization plans, such that the expected utility of the game is a bilinear product xTAy. In particular, a realization plan for the row player is a non-negative vector x indexed by sequences such that ∑\na∈A(I) x(I, a) = x(parent(I)) for all I ∈ Ii and where x(φ) = 1. In words, the probability mass flowing out of an information set equals the probability of playing to reach that information set. The constraint on the empty sequence, φ, normalizes the probability. We represent these linear equality constraints with the matrix E, i.e., Ex = e1, and thus Σ1 = {x | Ex = e, x ≥ 0}. For the column player, we have corresponding concepts y, F , and Σ2. The matrix A is the payoff matrix. Entry ai,j is the expected payoff to the row player over all terminals reach by sequences i and j.\nA pair of strategies, (x, y) is said to be an ε-Nash equilibrium if neither player can benefit by more than ε by deviating to another strategy. In particular,\nxTAy + ε ≥ x′Ay, and ∀x′ ∈ Σ1 −xTAy + ε ≥ −xTAy′. ∀y′ ∈ Σ2\nRemarkably, a Nash equilibrium always exists and in our case we can efficiently find ε-equilibria. In the next sections we will discuss and relate large-scale methods to do so."
    }, {
      "heading" : "Counterfactual Regret Minimization",
      "text" : "Online learning is a powerful framework for analyzing the performance of adaptive algorithms. At time t ∈ [T ], an online algorithm chooses a policy xt ∈ Σ and then receives reward vector ut ∈ K. It aims to minimize external regret,\nmax x∗∈Σ\nT ∑\nt=1\nut · x∗ − ut · xt,\nits utility relative to the best fixed policy in hindsight. An algorithm is said to be no-regret if its regret grows sublinear in T for any sequence of u’s from bounded set K. That is, if the bound on its time-averaged regret approaches zero in the worst-case (Cesa-Bianchi and Lugosi 2006).\nThere is an important connection between no-regret learning and zero-sum equilibrium computation. Two no-regret algorithms in self-play converge to a Nash equilibrium. Operationally, the row player gets reward reward ut = Ayt and the column player ut = −ATxt.\nTheorem 1. If two no-regret algorithms play a zero-sum game against one and other for T iterations and have timeaveraged regret less than ε, then their average strategies (x̄, ȳ) are a 2ε-Nash equilibrium. Here x̄ = ∑T\nt=1 x t/T .\nProof. For any x′ ∈ Σ1 and y′ ∈ Σ2,\n1\nT\nT ∑\nt=1\nx′ · Ayt − xt · Ayt ≤ ε, and\n1\nT\nT ∑\nt=1\n(−xt · Ay′)− (−xt · Ayt) ≤ ε\nadding the two inequalities together\n1\nT\nT ∑\nt=1\nx′ ·Ayt − xt · Ay′ ≤ 2ε\nsubstituting in the definitions of x̄ and ȳ\nx′ · Aȳ − x̄ · Ay′ ≤ 2ε choosing y′ = ȳ, we get for all x′ ∈ Σ1 x′ ·Aȳ ≤ x̄ ·Aȳ + 2ε Similarly, if instead we choose x′ = x̄, we get the second inequality in the definition of a 2ε-Nash.\nThe space of mixed strategies, Σ1 and Σ2, though structured, is complicated. Zinkevich et al. overcome this and describe a no-regret algorithm over the space of realization plans. Their approach minimizes a new notion of regret— counterfactual regret—at each information set using simple no-regret algorithms for the probability simplex. They show that counterfactual regret bounds external regret, thus their approach computes an equilibrium in self-play.\nThe counterfactual utility for action a at information set I is the expected utility given that the player tries to and successfully plays action a. That is, we weight a terminal by the total probability of the opponent and chance reaching it, but only by the remaining probability for the acting player. Computing counterfactual utilities is done by traversing the game tree, or one sparse matrix-vector product.\nThere are numerous no-regret algorithms operating over the probability simplex, Σ = ∆. Let us present two. The first, regret-matching (Hart and Mas-Colell 2000), is also known as the polynomially-weighted forecaster. It is the most commonly used algorithm for zero-sum equilibrium computation. Notationally, we call rt = ut − ut · xte the instantaneous regret and Rt = ∑t\ni=1 r i the cumulative\nregret at time t. Let L = supu∈K ‖u‖∞. Definition 1 (Regret-matching). Choose xt+1 ∝ (Rt)+.\nHere, (x)+ = max{0, x}. Theorem 2 (from (Hart and Mas-Colell 2000)). If xt’s are chosen using regret-matching, then the external regret is no more than L √ NT .\nThe second algorithm is Hedge (Freund and Schapire 1997). It is also known as the exponentially-weighted forecaster, exponential weights, or weighted majority (Littlestone and Warmuth 1994).\nDefinition 2 (Hedge). Choose xt+1 ∝ exp(ηRt). Theorem 3 (from (Freund and Schapire 1997)). If xt’s are chosen using Hedge with η = √\n2 log(N)/T/L, then the external regret is no more than L √ 2T logN .\nAlgorithm 1 CFR Update with Hedge function UPDATEREGRETI (R, g)\nfor a ∈ A(I) do for I ′ ∈ child(I, a) do\nu′, RI′ ← UPDATEREGRETI′ (RI′ , gI′) ga ← ga + u′\nend for end for xa ∝ exp(RI,a) for a ∈ A(I) do\nRI,a ← RI,a + ga − g · x end for return g · x,R\nend function function UPDATEREGRET(R, g)\nfor I ∈ child(Φ) do , RI ← UPDATEREGRETI′ (RI , gI)\nend for end function\nFor equilibrium-finding, the regret-matching algorithm of Hart and Mas-Colell is common place. Though Hedge has a slightly better theoretical bound than regret-matching, it is computationally more expensive due to exp and choosing η can be tricky in practice. As we will see, the use of Hedge here leads to an insightful connection between the two equilibrium-finding approaches in question. We show the counterfactual regret update in Algorithm 1."
    }, {
      "heading" : "Connection to Convex Optimization",
      "text" : "A different view from the no-regret learning approach is simply to write the equilibrium computation as a nonsmooth minimization. This minimization is convex, as both the objective and the set of realization plans are.\nTheorem 4. Let f(x) = maxy∈Σ2 −xTAy then f is convex on Σ1. Furthermore, if x∗ ∈ argminx∈Σ1 f(x) then x∗ is a minimax optimal strategy for the row player.\nProof. First, let y′ ∈ argmaxy∈Σ2 −xTAy. Claim f ′(x) = −Ay′ ∈ ∂f(x). For any x′ ∈ Σ1,\nf(x′)− f(x) = max y∈Σ2 −x′ ·Ay − max y∈Σ2 −x · Ay\n≥ −x′ · Ay′ − max y∈Σ2 −x · Ay = −x′ · Ay′ + x ·Ay′\n= (−Ay′) · (x′ − x) This is an instantiation of Danskin’s theorem (Bertsekas 1999). By the optimality of x∗, for any x̂ ∈ Σ1:\nx∗ ·Ay∗ = −f(x∗) ≥ −f(x′) = x̂ · Aŷ. where y∗ maximizes −x∗ ·Ay and ŷ maximizes−x̂·Ay.\nNote, the subgradient computation is precisely y’s best response. The CFR-BR technique is exactly CFR applied to this non-smooth optimization (Johanson et al. 2012).\nAs f is convex, and we can efficiently evaluate its subgradient via a best response calculation, we can use any subgradient method to find an ε-equilibrium strategy. Unfortunately, the most basic approach, the projected subgradient method, requires a complicated and costly projection onto the set of realization plans. We can avoid this projection by employing the proper Bregman divergence. In particular, if h : D → R is a strongly convex such that we can quickly solve the minimization\nargmin x∈D\ng · x+ h(x) (1)\nwe say h fits D. In these cases, we can often use h in place of the squared l2 distance and avoid any projections.\nHoda, Gilpin, and Peña (2008) describe a family of diverences, or distance generating functions, for the set of realization plans. They construct their family of distance generating functions inductively. One such h is as follows:\nhI(x, y) = ∑\na∈A(I) xI,a log xI,a+ ∀I ∈ Ii ∑\nI′∈child(I,a)\nxI,a [hI′(yI′/xI,a)]\nh(x) = ∑\nI∈child(φ)\nhI(xI)\nA few things worth noting. First, by child(x) we mean the set of information sets that are immediate children of sequence x. Second, we slightly abuse of notation above in that hI(x, y) depends on the immediate realization weights, x, and all child weights y. We denote the child weights belonging to information set I ′ as yI′ . Second, due to perfect recall, this recursion does bottom out; there are information sets with no children and there are no cycles. At a terminal information set, hI is the negative entropy function. The recursion makes use of the dilation or perspective operator.\nGordon (2006) introduces the same function in his supplementary materials, but does not provide a closed-form solution to its minimization. The closed-form solution is shown in Algorithm 2. Note that this algorithm is the same as a best response calculation where we replace the max operator with the softmax operator. The normalization step afterwards restores the sequence form constraints, i.e., converts from a behavioral strategy to a realization plan.\nThe computational structure of Algorithm 1 and 2 are identical. CFR, too, must normalize when computing an equilibrium to properly average the strategies. Both use the softmax operator to on the expected utility to define the current policy. In particular, if R = 0, then the computation is equivalent with the exception of the initial sign of g."
    }, {
      "heading" : "Dual Averaging",
      "text" : "Nesterov’s dual averaging (2009) is a subgradient method, and thus we can use to find equilibria. As we shall see, it has close connections to counterfactual regret when equipped with Hoda, Gilpin, and Peña distance function.\nDual averaging defines two sequences, xt, the query points, and gt, the corresponding sequence of subgradients. The averages x̄ and ḡ converge to a primal-dual solution.\nAlgorithm 2 Smoothed Best Response function MINIMIZE hI (u)\nfor a ∈ A(I) do for I ′ ∈ child(I, a) do\nu′, xI′ ← MINIMIZE hI′ (uI′) uI,a ← uI,a + u′\nend for end for xI,a ∝ exp(uI,a) ⊲ softmax instead of max return uI · x, x\nend function function NORMALIZEI (x, Z)\nfor a ∈ A(I) do xI,a ← xI,a/Z for I ′ ∈ child(I, a) do\nNORMALIZEI′(xI′ , xI,a) end for\nend for end function function MINIMIZE h(g)\nfor I ∈ child(Φ) do , xI ← MINIMIZE hI′ (−gI)\nNORMALIZEI (x, 1) end for return x\nend function\nDefinition 3. Let βt > 0 be a sequence of step sizes and h : D → R be a strongly convex distance generating function. The sequence xt is given by\nxt+1 = argmin x∈D\n1\nt\nt ∑\ni=1\n[ f(xi) + f ′(xi) · (x− xi) ] + βth(x)\n= argmin x∈D\nḡt · x+ βth(x) (2)\nThe method convergences for step sizes O( √ t), or for an appropriately chosen constant step size when the number if iterations is known ahead of time.\nInterestingly, Hedge and regret-matching over the simplex are operationally equivalent to dual averaging.\nTheorem 5. Hedge is equivalent to Dual Averaging over the simplex with σ(x) =\n∑n i=1 xi log xi, β t = 1/(tη) and gt = −ut. Proof. The dual averaging minimization can be written as the convex conjugate of the negative entropy (Boyd and Vandenberghe 2004):\nmin x∈∆\nḡt · x+ βt n ∑\ni=1\nxi log xi\n= −βt max x∈∆\n−ḡt · x/βt − n ∑\ni=1\nxi log xi\n= −βt log n ∑\ni=1\nexp ( −ĝi/βt )\nThe gradient of the conjugate minimizes the objective (Rockafeller 1970),\nxt+1 ∝ exp ( −ḡt/βt ) = exp\n(\n−η t ∑\ni=1\ngt\n)\n∝ exp ( η t ∑\ni=1\nut − ut · xte ) = exp ( ηRt )\nThe last step follows from exp(x) ∝ exp(b) exp(x) = exp(be+ x) for any vector x and constant b.\nIn particular, note subtracting the expected utility in the regret update does not at all alter the iterates. We need only accumulate counterfactual utility when using Hedge.\nTheorem 6. Regret-matching is equivalent to Dual Averaging over the simplex with σ(x) = ‖x+‖22/2, βt = eTRt+/t and gt = ut · xte− ut.\nProof. Consider the dual averaging minimization without the normalization constraint:\nxt+1 = argmin x≥0 ḡt · x+ βt‖x+‖22/2\n= (−ḡt)+\nβt =\nt(1 t ∑t i=1 u i − ui · xie)+ eTRt+\n= Rt+\neTRt+\nNote that by construction xt+1 sums to one, therefore the normalization constraint holds. In order for dual averaging to converge, we need 1/βt ∈ O( √ T ). This follows from the no-regret bound on regret-matching, eTRT+ ≤ L √ NT .\nFollowing from Theorem 6, we see that CFR with regretmatching is dual averaging with a Hoda, Gilpin, and Peñastyle divergence built on ‖x+‖22. Note that the step sizes must be chosen appropriately to avoid projection. That is, this divergence may not be appropriate for other gradient methods that rely on more stringent step size choices.\nLet us explicitly instantiate dual averaging for solving the convex-concave equilibrium saddle-point optimization.\nxt+1 = argmin x∈Σ1\n1\nt\nt ∑\ni=1\n−Ayi + βth(x),\n= argmin x∈Σ1\n−Aȳt + βth(x),\nyt+1 = argmin y∈Σ2\n1\nt\nt ∑\ni=1\nATxi + βth(y).\n= argmin y∈Σ2\nAT x̄t + βth(y).\nIn words, dual averaging applied to the saddle-point problem can be thought of as fictitious play with a smoothed best response as opposed to an actual best response (Brown 1951).\nDual averaging and Hedge are operationally equivalent at terminal information sets, that is, ones where all sequences\nhave no children. At a non-terminal information set, dual averaging responds as if its current policy is played against the opponent’s average strategy in future decisions. Counterfactual regret minimization, on the other hand, plays against the opponent’s current policy. The no-regret property of the algorithm guarantees that these two quantities remain close. In rough mathematical terms, we have\n∑T t=1 x t · Ayt ≈ xT ·∑Tt=1 Ayt within O( √ T ). Conceptually, we can conclude that counterfactual regret minimization, too, is roughly equivalent to smoothed fictitious play.\nOperationally, the difference is counterfactual regret minimization propagates and accumulates the expected utilities from the child information sets in the regrets. Dual averaging, in spirit, is lazy and re-propagates these utilities on each iteration. We note that this re-propagation is not algorithmically necessary when we have sparse stochastic updates as the expected value of an information set only changes if play goes through it. That is, we can memoize and update this value in a sparse fashion.\nThis understanding of CFR hints at why it outperforms its bound in practice and why unprincipled speedups may indeed be reasonable. In particular, we can achieve faster rates of convergence, O(L/T ) as opposed to O(L/ √ T ), when minimizing smooth functions with gradient descent and the appropriate step size. Two no-regret learners in self-play are essentially smoothing the non-smooth objective for one and other. The smooth objective itself is changing from iteration to iteration, but this suggests we can choose more aggressive step sizes than necessary. Further evidence of this is that the convergence behavior for CFR-BR, a minimization of a known non-smooth objective, is exhibits more volatile behavior that is closer to the true CFR regret bound.\nDual averaging with the Hoda, Gilpin, and Peña divergence is itself a no-regret learner over the set of realization plans (Xiao 2010). The regret bound itself is a function of the strong convexity parameter of the distance function. The bound on which appears to be quite loose. The above analysis suggests that it should be similar, or perhaps in some cases better, than the counterfactual regret bound on external regret. This is not shocking as counterfactual regret minimization is agnostic to no-regret algorithm in use."
    }, {
      "heading" : "Initialization with a Prior",
      "text" : "When computing an equilibrium or in an online setting, typically the initial strategy is uniformly random. Though the initial error does drop quite rapidly, it is often the case that we have available good priors available. Particularly in an online setting, it is preferable start with a good strategy instead of essentially learning both how to play the game and how to exploit an opponent at the same time. From the optimization perspective, we now discuss sound approaches.\nFirst, let us investigate how to initialize the dual weights—the cumulative counterfactual regrets. In dual averaging, the dual weights to x, ḡ = Aȳ, is the utilities to x of the opponent’s optimal strategy. If we have guesses at x∗ and y∗, we can use those to initialize the dual weights. This view of the dual weights is a simplification what is being done by Brown and Sandholm (2014), where counterfactual\nregret minimization is started from a known good solution. From the convex optimization view-point this is immediate.\nAn appealing property of this is that the representation of the opponent’s policy need not be known. That is, so long as we can play against it we can estimate the dual weights. In some cases, the variance of this estimate may be quite poor. With more structured representations, or domain knowledge it may be possible to greatly improve the variance.\nIt is quite common when considering convex optimization problems to recenter the problem. In particular, note that dual averaging starts with the policy x0 = argminx∈D h(x). By instead choosing h′(x) = ∇h(x) −∇h(x′) · x, we shift the starting policy to x′. In the case of the negative entropy over the simplex, this is equivalent to instead using a Kullback-Leibler divergence. Note, the step size schedule is an important practical factor that needs to be considered more carefully. In particular, smaller step sizes keep iterates closer too the initial policy."
    }, {
      "heading" : "Convergence of the Current Policy",
      "text" : "Some work has considered using CFR’s final policy as opposed to the average policy. Experimentally, it is shown that the current policy works quite well in practice despite the lack of bounds on its performance.\nA large assortment of recent work on stochastic convex optimization has considered this problem in depth exploring different averaging and step size selection schemes. When we view the problem from the convex optimization viewpoint these results transfer without modification. In particular, it has been shown that using a step size decreasing like 1/ √ t will lead to convergence of the current iterate for nonsmooth optimization. That is, if the opponent plays a best response, like in CFR-BR, we need not store the average policy reducing the memory requirement by 50%."
    }, {
      "heading" : "Acceleration and Sampling",
      "text" : "An important reason that no-regret algorithms have emerged as the dominant approach for large-scale equilibrium computation is their amenability to a various forms of sampling. At a high level, by introducing stochasticity we can drastically reduce the computation on each iteration by introducing different types of sparsity while only marginally increasing the number of necessary iterations.\nTheorem 7 (from (Cesa-Bianchi and Lugosi 2006)). Let the sequence xt be chosen according to a no-regret algorithm with regret bound √ CT and let x̃t ∼ xt. For all δ ∈ (0, 1), with probability at least 1 − δ the regret of the sequence x̃t is bounded by √ CT + √ T/2 log 1/δ.\nBy sampling ỹ from y, now we choose ut+1 = Aỹt. That is, ỹt is a standard basis vector and ut+1 is just a column of A (Lanctot et al. 2009). This has a number of important computational consequences. First, we no longer require a matrix-vector multiplication reducing the complexity of each iteration to linear from quadratic. In addition to the asymptotic reduction in complexity, we improve the constants since selecting a column of A requires no arithmetic\noperations. In fact, we may not even need to store ut+1 if we can index directly into A.\nA second important computational improvement is we can now often use integral numbers in place of floatingpoint numbers (Gibson 2013). In particular, note that rt = ut−ut ·x̃te is integral so long as ut is. Furthermore, x̄ can be represented as a vector of counts–the number of times each action is sampled. By using regret-matching, even computing xt can be done completely with integers as no floatingpoint math is required to convert RT to xt+1.\nAnother form of sampling is possible when nature participates in the game (Zinkevich et al. 2008). For example, imagine that A = ∑p\ni=1 Ai and we can implicitly access each Ai. This is the case when nature rolls a die or draws a card from a shuffled deck. Instead of explicitly forming and storing A, or examining each Ai on every iteration, we can choose one randomly. That is, Ãt = pAit , where it ∼ Uniform([p]). When we cannot store A this form of sampling reduces each iteration’s asymptotic complexity from linear in p to constant.\nNesterov’s excessive gap technique (2005) cannot handle stochastic gradients. This is the primary reason that it and the Hoda, Gilpin, and Peña divergence fell out of favor for equilibrium computation. As we note here, the divergence itself has nothing to do with the inability to handle stochasticity. It is a power tool enabling us to consider a wide variety of gradient methods. The stochastic mirror prox (SMP) algorithm of Juditsky, Nemirovski, and Tauvel (2011) is an extension of the extra-gradient method that achieves a rate of O(L/T + σ/ √ T ) on saddle-point problems like ours. Here, σ is the variance of the gradient. This rate is optimal. Specifically, it enables us to trade off between the variance of the sampling technique and the slower 1/ √ T rate that CFR achieves. Current sampling procedures favors low computation with sparse updates. It is unlikely that using them along side SMP will work well out-of-the-box. Further investigation is needed to determine if a successful compromise can practically improve performance."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "The task of computing approximate Nash equilibria in large zero-sum extensive-form games has received a tremendous amount of attention due mainly to the Annual Computer Poker Competition. Immediately after its inception, two competing and seemingly different approaches emerged—one an application of noregret online learning, the other a sophisticated gradient method applied to a convex-concave saddle-point formulation. Since then, both approaches have grown in relative isolation with advancements on one side not effecting the other. In this paper, we rectify this by dissecting and, in a sense, unify the two views.",
    "creator" : "LaTeX with hyperref package"
  }
}