{
  "name" : "1604.02774.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "[miguel.melro.leandro@gmail.com]" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 4.\n02 77\n4v 1\n[ cs\nOur method trains a neural network using Levenderg-Marquardt algorithm, where we restrict the knowledge dissemination in the network structure. We show how this reduces neural networks plasticity without damage drastically the learning performance. Making the descriptive power of produced neural networks similar to the descriptive power of Lukasiewicz logic language, simplifying the translation between symbolic and connectionist structures.\nThis method is used in the reverse engineering problem of finding the formula used on generation of a truth table for a multi-valued Lukasiewicz logic. For real data sets the method is particulary useful for attribute selection, on binary classification problems defined using nominal attribute. After attribute selection and possible data set completion in the resulting connectionist model: neurons are directly representable using a disjunctive or conjunctive formulas, in the Lukasiewicz logic, or neurons are interpretations which can be approximated by symbolic rules. This fact is exemplified, extracting symbolic knowledge from connectionist models generated for the data set Mushroom from UCI Machine Learning Repository.\nIntroduction\nThere are essentially two representation paradigms, namely, connectionist representations and symbolic-based representations, usually taken as very different. On one hand, symbolic based descriptions is specified through a grammar having a fairly clear semantics, can codify structured objects, in some cases support various forms of automated reasoning and can be transparent to users. On the other hand the usual way to see information presented using connectionist description, is its codification on a neural network. Artificial neural networks in principle combine, among other things, the ability to learn (and be trained) with massive parallelism and robustness or insensitivity to perturbations of input data. But neural networks are usually taken as black boxes providing little insight into how the information is codified. They have no explicit, declarative knowledge structure that allows the representation and generation of explanation structures. Thus, knowledge captured by neural networks is not transparente to users and cannot be verified by domain experts. To solve this problem, researchers have been interested in developing a humanly understandable representation for neural networks.\nIt is natural to seek a synergy integrating the white-box character of symbolic base representation and the learning power of artificial neuro networks. Such neuro-symbolic model are currently a very active area of research. One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.\nOur approach to neuro-symbolic models and knowledge extraction is based on trying to find a comprehensive language for humans representable directly in a neural network topology. This has been done for some types of neuro networks like Knowledge-based networks [10] [27]. These constitute a special class of artificial neural network that consider crude symbolic domain knowledge to generate the initial network architecture, which is later refined in the presence of training data. In the other direction there has been widespread activity aimed at translating neural language in the form of symbolic relations [11] [12] [26]. This processes served to identify the most significant determinants of decision or classification. However this is a hard problem since often an artificial neural network with good generalization does not necessarily imply involvement of hidden units with distinct meaning. Hence any individual unit cannot essentially be associated with a single concept of feature of the problem domain. This the archetype of connectionist approaches, where all information is stored in a distributed manner among the processing units and their associated connectivity. In this work we searched for a language, based on fuzzy logic, where the formulas are simple to inject in a multilayer feedforward network, but free from the need of given interpretation to hidden units in the problem domain.\nFor that we selected the language associated to a many-valued logic, the Lukasiewicz logic. We inject and extract knowledge from a neural network using it. This type of logic have a very useful property motivated by the ”linearity” of logic connectives. Every logic connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and\none [6]. Allowing the direct codification of formulas in the network architecture, and simplifying the extraction of rules. This type of back-propagation neural network can be trained efficiently using the Levenderg-Marquardt algorithm, when the configuration of each neuron is conditioned to converge to predefined patters associated or having directed representation in Lukasiewicz logic.\nThis strategy presented good performance when applied to the reconstruction of formulas from truth tables. If the truth table is generated using a formula from the language Lukasiewicz first order logic the optimum solution is defined using only units directly translated in formulas. In this type of reverse engineering problem we presuppose no noise. However the process is stable for the introduction of Gaussian noise on the input data. This motivate the application of this methodology to extract comprehensible symbolic rules from real data. However this is a hard problem since often an artificial neural network with good generalization does not necessarily imply that neural units can be translated in a symbolic formula. We describe, in this work, a simple rule to generate symbolic approximation to these unrepresentable configurations.\nThe presented process, for reverse engineering, can be applied to data sets characterizing a property of an entity by the truth value for a set of propositional features. And, it proved to be an excelente procedure for attribute selection. Allowing the data set simplification, by removing irrelevant attributes. The process when applied to real data generates potencial unrepresentable models. We used the relevant inputs attributes on this models as relevante attributes to the knowledge extraction problem, deleting others. This reduces the problem dimension allowing the potencial convergence to a less complex neuronal network topology.\nOverview of the paper: After present the basic notions about may valued logic and how can Lukasiewicz formulas be injected in a neural network. We describe the methodology for training a neural network having dynamic topology and having by activation function the identity truncated to zero and one. This methodology uses the Levenderg-Marquardt algorithm, with a special procedure called smooth crystallization to restrict the knowledge dissemination in the network structure. The resulting configuration is pruned used a crystallization process, where only links with values near 1 or -1 survive. The complexity of the generate network is reduced by applying the ”Optimal Brain Surgeon” algorithm proposed by B. Hassibi, D. G. Stork and G.J. Stork. If the simplified network doesn’t satisfies the stoping criteria, the methodology is repeated in a new network, possibly with a new topology. If the data used on the network train was generated by a formula, and have sufficient cases, the process converge for a prefect connectionist presentation and every neural unit in the neural network can be reconverted to a formula. We finish this work by presenting how the describe methodology could be used to extract symbolic knowledge from real data, and how the generated model could be used as a attribute selection procedure."
    }, {
      "heading" : "1 Preliminaries",
      "text" : "We begin by presenting the basic notions we need from the subjects of many valued logics, and how formulas in its language can be injected and extracted from a back-propagations neural network."
    }, {
      "heading" : "1.1 Many valued logic",
      "text" : "Classical propositional logic is one of the earliest formal systems of logic. The algebraic semantics of this logic is given by Boolean algebra. Both, the logic and the algebraic semantics have been generalized in many directions. The generalization of Boolean algebra can be based in the relationship between conjunction and implication given by\nx ∧ y ≤ z ⇔ x ≤ y → z ⇔ y ≤ x→ z.\nThese equivalences, called residuation equivalences, imply the properties of logic operators in a Boolean algebras. They can be used to present implication as a generalize inverse for the conjunction.\nIn application of fuzzy logic the properties of Boolean conjunction are too rigid, hence it is extended a new binary connective ⊗, usually called fusion. Extending the commutativity to the fusion operation, the residuation equivalences define an implication denoted in this work by ⇒ :\nx⊗ y ≤ z ⇔ x ≤ y ⇒ z ⇔ y ≤ x⇒ z.\nThis two operators are supposed defined in a partially ordered set of truth values (P,≤), extending the two valued set of an Boolean algebra. This defines a residuated poset (P,⊗,⇒,≤), where we interprete P as a set of truth values. This structure have been used on the definition of many types of logics. If P have more than two values the associated logics are called many-valued logics. An infinite-valued logic is a many valued logic with P infinite.\nWe focused our attention on many-valued logics having [0, 1] as set of truth values. In this logics the fusion operator ⊗ is known as a t -norm. In [13] it is defined as a binary operator defined in [0, 1] commutative and associative, non-decreasing in both arguments and 1⊗ x = x and 0⊗ x = 0.\nThe following are example of t-norms. All are continuous t-norms\n1. Lukasiewicz t-norm: x⊗ y = max(0, x+ y − 1).\n2. Product t-norm: x⊗ y = xy usual product between real numbers.\n3. Gödel t-norm: x⊗ y = min(x, y).\nIn [9] all continuous t-norms are characterized as ordinal sums of Lukasiewicz, Gödel and product t-norms.\nMany-valued logics can be conceived as a set of formal representation languages that proven to be useful for both real world and computer science applications. And when they are defined by continuous t-norms they are known as fuzzy logics."
    }, {
      "heading" : "1.2 Processing units",
      "text" : "As mention in [1] there is a lack of a deep investigation of the relationships between logics and neural networks. In this work we present a methodology using neural networks to learn formulas from data. And where neural networks are trate as circuital counterparts of (functions represented by) formulas. They are either easy to implement and high parallel objects.\nIn [6] it is shown what, by taking as activation function ψ the identity truncated to zero and one, also named saturating linear transfer function\nψ(x) = min(1,max(x, 0))\nit is possible to represent the corresponding neural network as combination of propositions of Lukasiewicz calculus and viceversa[1].\nUsually Lukasiewicz logic sentences are built, as in first-order logic languages, from a (countable) set of propositional variables, a conjunction ⊗ (the fusion operator), an implication ⇒ and the truth constant 0. Further connectives are defined as follows:\n1. ϕ1 ∧ ϕ2 is ϕ1 ⊗ (ϕ1 ⇒ ϕ2),\n2. ϕ1 ∨ ϕ2 is ((ϕ1 ⇒ ϕ2) ⇒ ϕ2) ∧ ((ϕ2 ⇒ ϕ1) ⇒ ϕ1)\n3. ¬ϕ1 is ϕ1 ⇒ 0\n4. ϕ1 ⇔ ϕ2 is (ϕ1 ⇒ ϕ2)⊗ (ϕ2 ⇒ ϕ1)\n5. 1 is 0 ⇒ 0\nThe usual interpretation for a well formed formula ϕ is defined recursive defining by the assignment of truth values to each proposicional variable. However the application of neural network to learn Lukasiewicz sentences seems more promisor using a non recursive approach to proposition evaluation. We can do this by defining the first order language as a graphic language. In this language, words are generate using the atomic componentes presented on figure 2, they are networks defined linking this sort of neurons. This is made gluing atomic componentes, satisfy the neuron signature, i.e. it is an unit having several inputs\nand one output. This task of construct complex structures based on simplest ones can be formalized using generalized programming [8].\nIn other words Lukasiewicz logic language is defined by the set of all neural networks, where its neurons assume one of the configuration presented in figure 2.\nA networks of this type can be interpreted as a function, see figure 3, generically denoted by ψb(w1x1, w2x2). In this context a network is the functional interpretation for a sentence when its interpretation is the sentence truth table. The fact of networks and interpretation have a similar structure preserved by\na graph homomorphism, called the translation morphism, simplifies the transformation between string base representations and the network representation, allowing to write:\nProposition 1 Every well formed formula in Lukasiewicz logic language can be codified using a neural network.\nFor instance, the semantic for sentence ϕ = (x ⊗ y ⇒ z)⊕ (z ⇒ w) can be described using the bellow network or codified by the bellow set of matrixes. We must note, in the example, what the partial interpretation of each unit is a simple exercise of pattern checking, relating the weights signs and the neuron bias.\nx 1 ❈❈ ❈❈ −1\n?>=<89:;⊗ −1\n❈❈ ❈❈ ❈ 1\ny 1 ⑧⑧⑧⑧⑧ 76540123= 1 ?>=<89:;⇒\n1 ❇❇\n❇❇ ❇\n0\nz\n−1 ❄❄\n❄❄ ❄\n1 ⑧⑧⑧⑧⑧\n1\n0\n0\n?>=<89:;⊕\n?>=<89:;⇒ 1 76540123= 1 ⑤⑤⑤⑤⑤\nw\n1 ③③③③\nx y z w b’s partial interpretation i1 i2 i3   1 1 0 0 0 0 1 0 0 0 −1 1     −1 0 1   x⊗ y z\nz ⇒ w i1 i2 i3\nj1 j2\n[\n−1 1 0 0 0 1\n] [\n1 0\n]\ni1 ⇒ i2 i3\nj1 j2 [\n1 1 ] [ 0 ]\nj1 ⊕ j2\nINTERPRETATION: j1 ⊕ j2 = (i1 ⇒ i2) ⊕ (i3) = = ((x ⊗ y) ⇒ z) ⊕ (z ⇒ w)\nIn this sense this neural network can be seen as an interpretation for sentence ϕ, it codifies fϕ, the proposition truth table. And it can be presented in string base notation by writing:\nfϕ(x, y, z, w) = ψ0(ψ0(ψ1(−z, w)), ψ1(ψ0(z),−ψ−1(x, y)))\nHowever fϕ is a continuo structure, for our propose, it must be discretized using a finite structure but having suficiente information to describe the original formula. A truth table fϕ for a formula ϕ is a map fϕ : [0, 1]\nm → [0, 1], where m is the number of propositional variables used in ϕ. For each integer n > 0, let Sn be the set {0, 1 n , . . . , n−1 n , 1}. Each n > 0, defines a subtable for fϕ defined by f (n) ϕ : (Sn) m → Sn, and given by f (n) ϕ (v̄) = fϕ(v̄), and called the ϕ (n+1)-valued truth subtable."
    }, {
      "heading" : "1.3 Similarity between a configuration and a formula",
      "text" : "We called Castro neural network to a neural network having as activation function ψ the identity truncated to zero and one and where its weights are -1, 0 or 1 and having by bias an integer. And a Carlos neural network is called representable if it is codified as a binary neural network i.e. a Castro neural network where each neuron don’t have more than two inputs. A network is called unrepresentable if it can’t be codified using a binary Castro neural network. In figure 4, we present an example of an unrepresentable network configuration, as we will see in the following.\nNote what binary Castro neural network can be translated directory in the Lukasiewicz first order language, and in this sense are called them Lukasiewicz neural network. Bellow we presented examples of the functional interpretation for formulas with two propositional variables. They can be organized in two class:\nx −1 ❆❆ ❆❆ 0 y 1 ?>=<89:;ψ w ⇔ w = ψ0(−x, y, z)\nz\n1 ⑦⑦⑦⑦\nAnd correspond to all possible configurations of neurons with two inputs. The other possible configurations are constant and can also be seen as representable configurations. For instance ψb(x1, x2) = 0, if b < −1, and ψb(−x1,−x2) = 1, if b > 1.\nIn this sense every representable network can be codified by a neural network where the neural units satisfy the above patterns. Bellow we present examples of representable configurations with three inputs and how they can be codified using representable neural networks having units with two inputs.\nDisjunctive configurations\nψ−2(x1, x2, x3) = ψ−1(x1, ψ−1(x2, x3)) = fx1⊗x2⊗x3 ψ−1(x1, x2,−x3) = ψ−1(x1, ψ0(x2,−x3)) = fx1⊗x2⊗¬x3\nψ0(x1,−x2,−x3) = ψ−1(x1, ψ1(−x2,−x3)) = fx1⊗¬x2⊗¬x3 ψ1(−x1,−x2,−x3) = ψ0(−x1, ψ1(−x2,−x3)) = f¬x1⊗¬x2⊗¬x3\nConjunctive interpretations\nψ0(x1, x2, x3) = ψ0(x1, ψ0(x2, x3)) = fx1⊕x2⊕x3 ψ1(x1, x2,−x3) = ψ0(x1, ψ1(x2,−x3)) = fx1⊕x2⊕¬x3\nψ2(x1,−x2,−x3) = ψ0(x1, ψ2(−x2,−x3)) = fx1⊕¬x2⊕¬x3 ψ3(−x1,−x2,−x3) = ψ1(−x1, ψ2(−x2,−x3)) = f¬x1⊕¬x2⊕¬x3\nConstant configurations ψb(x1, x2, x3) = 0, if b < −2, and ψb(−x1,−x2,−x3) = 1, if b > 3, are also representable. However there are example an unrepresentable network with three inputs in fig. 4.\nNaturally, a neuron configuration when representable can by codified by different structures using Lukasiewicz neural network. Particularly we have:\nProposition 2 If the neuron configuration α = ψb(x1, x2, . . . , xn−1, xn) is representable, but not constant, it can be codified in a Lukasiewicz neural network with structure:\nβ = ψb1(x1, ψb2(x2, . . . , ψbn−1(xn−1, xn) . . .)).\nAnd since the n-nary operator ψb is comutativa in function β variables could interchange its position without change operator output. By this we mean what, in the string based representation, variable permutation generate equivalent formulas. From this we can concluded what:\nProposition 3 If α = ψb(x1, x2, . . . , xn−1, xn) is representable, but not constant, it is the interpretation of a disjunctive formula or of a conjunctive formula.\nRecall that disjunctive formulas are written using only disjunctions and negations, and conjunctive formulas are written using only conjunctions and negations. This live us with the task of classify a neuron configuration according with its representation. For that, we established a relation using the configuration bias and the number of negative and positive inputs.\nProposition 4 Given the neuron configuration\nα = ψb(−x1,−x2, . . . ,−xn, xn+1, . . . , xm)\nwith m = n + p inputs and where n and p are, respectively, the number of negative weights and the number of positive, on the neuron configuration.\n1. If b = −(m− 1) + n (i.e. b = −p+ 1) the neuron is called a conjunction and it is a interpretation for\n¬x1 ⊗ . . .⊗ ¬xn ⊗ xn+1 ⊗ . . .⊗ xm\n2. When b = n the neuron is called a disjunction and it is a interpretation of\n¬x1 ⊕ . . .⊕ ¬xn ⊕ xn+1 ⊕ . . .⊕ xm\nFrom this we proposed the following estrutural characterization for representable neurons.\nProposition 5 Every conjunctive or disjunctive configuration α = ψb(x1, x2, . . . , xn−1, xn), can be codified by a Lukasiewicz neural network\nβ = ψb1(x1, ψb2(x2, . . . , ψbn−1(xn−1, xn) . . .)),\nwhere b = b1 + b2 + · · ·+ bn−1 and b1 ≤ b2 ≤ · · · ≤ bn−1.\nThis can be translated in the following neuron rewriting rule,\nw1 ❂ ❂❂\n❂❂ ❂\nb\n. . . ?>=<89:;ψ R // wn\n✁✁✁✁✁✁\nw1 ❂ ❂❂\n❂❂ ❂\nb0\n. . . ?>=<89:;ψ 1\n❁❁ ❁❁\n❁❁ ❁\nb1 wn−1 ✝✝✝✝✝✝✝ ?>=<89:;ψ wn\n♠♠♠♠♠♠♠♠♠♠♠\nlinking networks, where values b0 and b1 satisfy b = b0 + b1 and b1 ≤ b0, and such that neither involved neurons have constant output. This rewriting rule can be used to like equivalent configurations like:\nx −1 ❇❇ ❇❇ 2 y 1 76540123ϕ R //\nz\n−1 ⑤⑤⑤⑤⑤\nw\n1 ☞☞☞☞☞☞☞☞\nx −1 ❇❇ ❇❇ 2 y 1 76540123ϕ\n1 ❆❆ ❆❆ ❆ 0\nR //\nz\n−1 ✁✁✁✁✁ 76540123ϕ\nw\n1 ♠♠♠♠♠♠♠♠♠\nx −1 ❇❇ ❇❇ 2\nz −176540123ϕ 1\n❆❆ ❆❆ ❆ 0\ny 1 76540123ϕ\n1 ❆❆ ❆❆ ❆ 0\nw 1 76540123ϕ\nNote what a representable Castro neural network can been transformed by the application of rule R in a set of equivalente Lukasiewicz neural network having less complex neurons. Then we have:\nProposition 6 Unrepresentable neuron configurations are those transformed by rule R in, at least, two not equivalent neural networks.\nFor instance unrepresentable configuration ψ0(−x1, x2, x3) is transform by rule R in three not equivalent configurations:\n1. ψ0(x3, ψ0(−x1, x2)) = fx3⊕(¬x1⊗x2) ,\n2. ψ−1(x3, ψ1(−x, x2)) = fx3⊗(¬x1⊗x2) , or\n3. ψ0(−x1, ψ0(x2, x3)) = f¬x1⊗(x2⊕x3) .\nThe representable configuration ψ2(−x1,−x2, x3) is transform by rule R on only two distinct but equivalent configurations:\n1. ψ0(x3, ψ2(−x1,−x2)) = fx3⊕¬(x1⊗x2) , or\n2. ψ1(−x2, ψ1(−x1, x3)) = f¬x2⊕(¬x1⊕x3)\nFrom this we concluded that Castro neural networks have more expressive power than Lukasiewicz logic language. There are structures defined using Castro neural networks but not codified in the Lukasiewicz logic language.\nWe also want meed reverse the knowledge injection process. We want extracted knowledge from trained neural networks. For it we need translate neuron configuration in propositional connectives or formulas. However, we as just said, not all neuron configurations can be translated in formulas, but they can be approximate by formulas. To quantify the approximation quality we defined the notion of interpretations and formulas λ-similar.\nTwo neuron configurations α = ψb(x1, x2, . . . , xn) and β = ψb′(y1, y2, . . . , yn) are called λ-similar in a (m+ 1)-valued Lukasiewicz logic if λ is the mean absolute error by taken the truth subtable given by α as an approximation to the truth subtable given by β. When this is the case we write\nα ∼λ β.\nIf α is unrepresentable and β is representable, the second configuration is called a representable approximation to the first.\nWe have for instance, on the 2-valued Lukasiewicz logic (the Boolean logic case), the unrepresentable configuration α = ψ0(−x1, x2, x3) satisfies:\n1. ψ0(−x1, x2, x3) ∼0.125 ψ0(x3, ψ0(−x1, x2)),\n2. ψ0(−x1, x2, x3) ∼0.125 ψ−1(x3, ψ1(−x1, x2)), and\n3. ψ0(−x1, x2, x3) ∼0.125 ψ0(−x1, ψ0(x2, x3)).\nAnd in this case, the truth subtables of, formulas α1 = x3 ⊕ (¬x1 ⊗ x2), α1 = x3⊗ (¬x1 ⊗x2) and α1 = ¬x1 ⊗ (x2⊕x3) are both λ-similar to ψ0(−x1, x2, x3), where λ = 0.125 since they differ in one position on 8 possible positions. This mean that both formulas are 12.3% accurate. The quality of this approximations was checked by presenting values of similarity levels λ on other finite Lukasiewicz logics. For every selected logic both formulas α1, α2 and α3 have the some similarity level when compared to α:\n• 3-valued logic, λ = 0.1302,\n• 4-valued logic, λ = 0.1300,\n• 5-valued logic, λ = 0.1296,\n• 10-valued logic, λ = 0.1281,\n• 20-valued logic, λ = 0.1268,\n• 30-valued logic, λ = 0.1263,\n• 50-valued logic, λ = 0.1258.\nLets see a more complex configuration α = ψ0(−x1, x2,−x3, x4,−x5). From it we can derive, through rule R, configurations:\n1. β1 = ψ0(−x5, ψ0(x4, ψ0(−x3, ψ0(x2,−x1))))\n2. β2 = ψ−1(x4, ψ−1(x2, ψ0(−x5, ψ0(−x3,−x1))))\n3. β3 = ψ−1(x4, ψ0(−x5, ψ0(x2, ψ1(−x3,−x1))))\n4. β4 = ψ−1(x4, ψ0(x2, ψ0(−x5, ψ1(−x3,−x1))))\nsince this configurations are not equivalents we concluded that α is unrepresentable. When we compute the similarity level between α and each βi using different finite logics we have:\n• 2-valued logic α ∼0.156 β1, α ∼0.094 β2, α ∼0.656 β3 and α ∼0.531 β4,\n• 3-valued logic α ∼0.134 β1, α ∼0.082 β2, α ∼0.728 β3 and α ∼0.601 β4,\n• 4-valued logic α ∼0.121 β1, α ∼0.076 β2, α ∼0.762 β3 and α ∼0.635 β4,\n• 5-valued logic α ∼0.112 β1, α ∼0.071 β2, α ∼0.781 β3 and α ∼0.655 β4,\n• 10-valued logic α ∼0.096 β1, α ∼0.062 β2, α ∼0.817 β3 and α ∼0.695 β4,\nFrom this we may concluded that β2 is a good approximation to α and its quality improve when we increase the number of truth values. The error increase at a low rate that the number of cases.\nIn this sense we will also use rule R in the case of unrepresentable configurations. From an unrepresentable configuration α we can generate the finite set S(α), with representable networks similar to α, using rule R. Note what from S(α) we may select as approximation to α the formula having the interpretation more similar to α, denoted by s(α). This identification of unrepresentable configuration by representable approximations can be used to transform network with unrepresentable neurons into representable neural networks. The stress associated to this transformation caracterizes the translation accuracy."
    }, {
      "heading" : "1.4 A neural network crystallization",
      "text" : "Weights in Castro neural networks assume the values -1 or 1. However the usual learning algorithms process neural networks weights presupposing the continuity of weights domain. Naturally, every neural network with weighs in [−1, 1] can be seen as an approximation to a Castro neural networks. The process of identify a neural network with weighs in [−1, 1] with a Lukasiewicz neural networks was called crystallization. And essentially consists in rounding each neural weight wi to the nearest integer less than or equal to wi, denoted by ⌊wi⌋.\nIn this sense the crystallization process can be seen as a pruning on the network structure, where links between neurons with weights near 0 are removed and weights near -1 or 1 are consolidated. However this process is very crispy. We need a smooth procedure to crystallize a network in each learning iteration to avoid the drastic reduction on learning performance. In each iteration we want restrict the neural network representation bias, making the network representation bias converge to a structure similar to a Castro neural networks. For that, we defined by representation error for a network N with weights w1, . . . , wn, as\n∆(N) =\nn∑\ni=1\n(wi − ⌊wi⌋).\nWhen N is a Castro neural networks we have ∆(N) = 0. And we defined a smooth crystallization process by iterating the function:\nΥn(w) = sign(w).((cos(1− abs(w) − ⌊abs(w)⌋). π\n2 )n + ⌊abs(w)⌋)\nwhere sign(w) is the sign of w and abs(w) its absolute value. We denote by Υn(N) the function having by input and output a neural network defined extending Υ(wi) to all network weights and neurons bias. Since, for every network N and n > 0, ∆(N) ≥ ∆(Υn(N)), we have:\nProposition 7 Given a neural networks N with weights in the interval [0, 1]. For every n > 0 the function Υn(N) have by fixed points Castro neural networks N ′.\nThe convergence speed dependes on parameter n. Increasing n speedup crystallization but reduces the network plasticity to the training data. For our applications, we selected n = 2 based on the learning efficiency on a set of test formulas. For grater values for n imposes stronger restritivos to learning. This induces a quick convergence to an admissible configuration of Castro neural network."
    }, {
      "heading" : "2 Learning propositions",
      "text" : "We began the study of Castro neural network generation trying to do reverse engineering on a truth table. By this we mean what given a truth table from a\n(n+1)-valued Lukasiewicz logic, generated by a formula in the Lukasiewicz logic language, we will try to find its interpretation in the form of a Lukasiewicz neural network. And from it rediscover the original formula.\nFor that we trained a Backpropagation neural networks using the truth table. Our methodology trains networks having progressively more complex topologies, until a crystalized network with good performance have been found. Note that this methodology convergence dependes on the selected training algorithm.\nThe bellow Algorithm 1 described our process for truth table reverse engineering:\nAlgorithm 1 Reverse Engineering algorithm\n1: Given a (n+1)-valued truth subtable for a Lukasiewicz logic proposition 2: Define an inicial network complexity 3: Generate an inicial neural network 4: Apply the Backpropagation algorithm using the data set 5: if the generated network have bad performance then 6: If need increase network complexity 7: Try a new network. Go to 3 8: end if\n9: Do neural network crystallization using the crisp process. 10: if crystalized network have bad performance then 11: Try a new network. Go to 3 12: end if 13: Refine the crystalized network\nGiven a part of a truth table we try to find a Lukasiewicz neural network what codifies the data. For that we generated neural networks with a fixed number of hidden layers, on our implementation we used three. When the process detects bad learning performances, it aborts the training, and generates a new network with random heights. After a fixed number of tries the network topology is change. This number of tries dependes of the network inputs number. After trying configure a set of networks with a given complexity and bad learning performance, the system tries to apply the selected Backpropagation algorithm to a more complex set of networks. In the following we presented a short description for the selected learning algorithm.\nIf the continuous optimization process converges, i.e. if the system finds a network codifying the data, the network is crystalized. If the error associated to this process increase the original network error the crystalized network is throwaway, and the system returns to the learning fase trying configure a new network.\nWhen the process converges and the resulting network can be codified as a crisp Lukasiewicz neural network the system prunes the network. The goal of this fase is the network simplification. For that we selected the Optimal Brain Surgeon algorithm proposed by G.J. Wolf, B. Hassibi and D.G. Stork in [16].\nThe figure 5 presents an example of the Reverse Engineering algoritmo input data set (a truth table) and output neural network structure."
    }, {
      "heading" : "2.1 Training the neural network",
      "text" : "Standard Error Backpropagation algorithm (EBP) is a gradient descent algorithm, in which the network weights are moved along the negative of the gradient of the performance function. EBP algorithm has been a significant improvement in neural network research, but it has a weak convergence rate. Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21]. The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods. It gives a good exchange between the speed of Newton algorithm and the stability of the steepest descent method [3].\nThe basic EBP algorithm adjusts the weights in the steepest descent direction. This is the direction in which the performance function is decreasing most rapidly. In the EBP algorithm, the performance index F (w) to be minimized is defined as the sum of squared erros between the target output and the network’s simulated outputs, namely:\nF (wk) = e T k ek\nwhere the vector wk = [w1, w2, . . . , wn] consists of all the current weights of the network, ek is the current error vector comprising the error for all the training examples.\nWhen training with the EBP method, an iteration of the algorithm define the change of weights and have the form\nwk+1 = wk − αGk\nwhere Gk is the gradient of F on wk, and α is the learning rate. Note that, the basic step of the Newton’s method can be derived dom Taylor formula and is as: wk+1 = wk −H −1 k Gk\nwhere Hk is the Hessian matrix of the performance index at the current values of the weights.\nSince Newton’s method implicitly uses quadratic assumptions (arising from the neglect of higher order terms in a Taylor series), The Hessian need not to be evaluated exactly. Rather an approximation can be used like\nHk ≈ J T k Jk\nwhere Jk is the Jacobian matrix that contains first derivatives of the network errors with respect to the weights wk. The Jacobian matrix Jk can be computed through a standard back propagation technique [22] that is much less complex than computing the Hessian matrix. The current gradient take the form Gk = JTk ek, where ek is a vector of current network errors. Note what Hk = J T k Jk in linear case. The main advantage of this technique is rapid convergence. However, the rate of convergence is sensitive to the starting location, or more precisely, the linearity around the starting location.\nIt can be seen that simple gradient descent and Newton iteration are complementary in advantages they provide. Levenberg proposed an algorithm based on this observation, whose update rule is blend mentioned algorithms and is given as\nwk+1 = wk − [J T k Jk + µI] −1JTk ek\nwhere Jk is the Jacobian matrix evaluated at wk and µ is the learning rate. This update rule is used as follow. If the error goes down following an update, it implies that our quadratic assumption on the function is working and we reduce µ (usually by a factor of 10) to reduce the influence of gradient descent. In this way, the performance function is always reduced at each iteration of the algorithm [14]. On the other hand, if the error up, we would like to follow the gradient more and so µ is increased by the same factor. The Levenberg algorithm is thus\n1. Do an update as directed by the rule above.\n2. Evaluated the error at the new weight vector.\n3. If error has increased as the result the update reset the weights to their previous values and increase µ by a factor β. Then try an update again\n4. If error has decreased as a result of the update, then accept the set and decrease µ by a factor β.\nThe above algorithm has the disadvantage that if the value of µ is large, the approximation to Hessian matrix is not used at all. We can derive some advantage out of the second derivative even in such cases by scaling each component of the gradient according to the curvature. This should result in larger movement along the direction where the gradient is smaller so the classic ”error valley” problem does not occur any more. This crucial insight was provided by Marquardt. He replaced the identity matrix in the Levenberg update rule with the diagonal of Hessian matrix approximation resulting in the LevenbergMarquardt update rule.\nwk+1 = wk − [J T k Jk + µ.diag(J T k Jk)] −1JTk ek\nSince the Hessian is proportional to the curvature this rule implies a larger step in the direction with low curvatures and big step in the direction with high curvature.\nAlgorithm 2 Levenberg-Marquardt algorithm with soft crystallization\n1: Initialize the weights w and parameters µ = .01 and β = .1 2: Compute e the sum of the squared error over all inputs F (w) 3: Compute J the Jacobian of F in w 4: Compute the increment of weight ∆w = −[JTJ + µdiag(JTk Jk)]\n−1JT e 5: Let w∗ be the result of applying to w +∆w the soft crystallization process\nΥ2. 6: if F (w∗) < F (w) then 7: w = w +∆w 8: µ = µ.β 9: Go back to step 2\n10: else 11: µ = µ/β 12: Go back to step 4 13: end if\nThe standard LM training algorithm can be illustrated in the following pseudo-codes:\nIt is to be notes while LM method is no way optimal but is just a heuristic, it works extremely well for learn Lukasiewicz neural network. The only flaw is its need for matrix inversion as part of the update. Even thought the inverse is usually implemented using pseudo-inverse methods such as singular value decomposition, the cost of update become prohibitive after the model size increases to a few thousand weights.\nThe application of a soft cristalizador step in each iteration accelerates the convergence to a Castro neural network."
    }, {
      "heading" : "3 Applying reverse engineering on truth tables",
      "text" : "Given a Lukasiewicz neural network it can be translated in the form of a string base formula if every neuron is representable. Proposition 4 defines a way to translate from the connectionist representation to a symbolic representation. And it is remarkable the fact that, when the truth table used in the learning is generate by a formula in a adequate n-valued Lukasiewicz logic the Reverse Engineering algorithm converges to a representable Lukasiewicz neural network and it is equivalent to the original formula.\nWhen we generate a truth table in the 4-valued Lukasiewicz logic using formula\n(x4 ⊗ x5 ⇒ x6)⊗ (x1 ⊗ x5 ⇒ x2)⊗ (x1 ⊗ x2 ⇒ x3)⊗ (x6 ⇒ x4)\nit have 4096 cases, the result of applying the algorithm is the 100% accurate\nneural network. \n   0 0 0 −1 0 1 0 0 0 1 1 −1 1 1 −1 0 0 0\n−1 1 0 0 −1 0\n\n  \n\n   0 −1 −1 2\n\n  \n¬x4 ⊗ x6 x4 ⊗ x5 ⊗ ¬x6 x1 ⊗ x2 ⊗ ¬x3 ¬x1 ⊕ x2 ⊕ ¬x5\n[ −1 −1 −1 1 ] [ 0 ] ¬i1 ⊗ ¬i2 ⊗ ¬i3 ⊗ i4 [\n1 ] [ 0 ]\nj1\nFrom it we may reconstructed the formula:\nj1 = ¬i1 ⊗ ¬i2 ⊗ ¬i3 ⊗ i4 = ¬(¬x4 ⊗ x6) ⊗ ¬(x4 ⊗ x5 ⊗ ¬x6) ⊗ ¬(x1 ⊗ x2 ⊗ ¬x3) ⊗ (¬x1 ⊕ x2 ⊕ ¬x5) = = (x4 ⊕ ¬x6) ⊗ (¬x4 ⊕ ¬x5 ⊕ x6) ⊗ (¬x1 ⊕ ¬x2 ⊕ x3) ⊗ (¬x1 ⊕ x2 ⊕ ¬x5) =\n= (x6 ⇒ x4) ⊗ (x4 ⊗ x5 ⇒ x6) ⊗ (x1 ⊗ x2 ⇒ x3) ⊗ (x1 ⊗ x5 ⇒ x2)\nNote however the restriction imposed, in our implementation, to three hidden layers having the least hidden layer only one neuron, impose restriction to the complexity of reconstructed formula. For instance\n((x4 ⊗ x5 ⇒ x6)⊕ (x1 ⊗ x5 ⇒ x2))⊗ (x1 ⊗ x2 ⇒ x3)⊗ (x6 ⇒ x4)\nto be codified in a three hidden layer network the last layer needs two neurons one to codify the disjunction and the other to codify the conjunctions. When the algorithm was applied to the truth table generated in the 4-valued Lukasiewicz logic having by stoping criterium a mean square error less than 0.0007 it produced the representable network:\n\n 0 0 0 1 0 −1 1 −1 0 1 1 −1 1 1 −1 0 0 0\n\n\n\n 1 −2 −1\n\n\nx4 ⊕ ¬x6 x1 ⊗ ¬x2 ⊗ x4 ⊗ x5 ⊗ ¬x6 x1 ⊗ x2 ⊗ ¬x3\n[ 1 −1 −1 ] [ 0 ] i1 ⊗ ¬i2 ⊗ ¬i3 [\n1 ] [ 0 ]\nj1\nBy this we may conclude what original formula can be approximate, or is λsimilar with λ = 0.002 to:\nj1 = i1 ⊗ ¬i2 ⊗ ¬i3 = (x4 ⊕ ¬x6) ⊗ ¬(x1 ⊗ ¬x2 ⊗ x4 ⊗ x5 ⊗ ¬x6) ⊗ ¬(x1 ⊗ x2 ⊗ ¬x3) = = (x4 ⊕ ¬x6) ⊗ (¬x1 ⊕ x2 ⊕ ¬x4 ⊕ ¬x5 ⊕ x6) ⊗ (¬x1 ⊕ ¬x2 ⊕ x3) =\n= (x6 ⇒ x4) ⊗ ((x1 ⊗ x4 ⊗ x5) ⇒ (x2 ⊕ x6)) ⊗ (x1 ⊗ x2 ⇒ x3)\nNote that j1 is 0.002-similar to the original formula in the 4-valued Lukasiewicz logic but it is equivalente to the original in the 2-valued Lukasiewicz logic, i.e. in Boolean logic.\nThe fixed number of layer also impose restrictions to reconstruction of formula. A table generated by:\n(((i1 ⊗ i2)⊕ (i2 ⊗ i3))⊗ ((i3 ⊗ i4)⊕ (i4 ⊗ i5)))⊕ (i5 ⊗ i6)\nrequires at least 4 hidden layers, to be reconstructed, this is the number os levels required by the associated parsing tree.\nBellow we can see all the fixed points found by the process, when applied on the 5-valued truth table for\nx ∧ y := min(x, y).\nThese reversed formulas are equivalent in the 5-valued Lukasiewicz logic, and\nwhere find for different executions.\n0 −1\ny −176540123ϕ −1 76540123ϕ\nx\n1 ✁✁✁✁✁−176540123ϕ −1 ⑥⑥⑥⑥⑥\n1\n0 0\ny −176540123ϕ −1 76540123ϕ\nx\n1 ✁✁✁✁✁ 1 76540123ϕ 1 ⑥⑥⑥⑥⑥\n0\n0 0\ny 1 76540123ϕ −1 76540123ϕ\nx\n−1 ✁✁✁✁✁ 1 76540123ϕ 1 ⑥⑥⑥⑥⑥\n0\n1 0\ny 1 76540123ϕ 1 76540123ϕ\nx\n−1 ✁✁✁✁✁−176540123ϕ −1 ⑥⑥⑥⑥⑥\n1\n¬(¬¬(x ⇒ y) ⇒ ¬x) ¬(x ⇒ ¬(x ⇒ y)) ¬(¬(y ⇒ x) ⇒ x) ¬((x ⇒ y) ⇒ ¬x)\n0 0\nx −176540123ϕ −1 76540123ϕ\ny\n1 ✁✁✁✁✁ 1 76540123ϕ 1 ⑥⑥⑥⑥⑥\n0\n0 −1\nx 1 76540123ϕ 1 76540123ϕ\ny\n1 ✁✁✁✁✁−176540123ϕ 1 ⑥⑥⑥⑥⑥\n1\n1 1\nx −176540123ϕ −1 76540123ϕ\ny\n−1 ✁✁✁✁✁ 1 76540123ϕ −1 ⑥⑥⑥⑥⑥\n0\n1 0\nx 1 76540123ϕ 1 76540123ϕ\ny\n−1 ✁✁✁✁✁−176540123ϕ −1 ⑥⑥⑥⑥⑥\n1\n¬(y ⇒ ¬(y ⇒ x)) (y ⇒ x) ⊗ y ¬(¬(y ⇒ x) ⇒ ¬y) ¬((y ⇒ x) ⇒ ¬y)\nThe bellow table presents mean times need to find a configuration with a mean square error less than 0.002. Then mean time is computed using a 6 tries for some formulas on the 5-valued truth Lukasiewicz logic. We implementation the algorithm using the MatLab neural network package and run it in a AMD Athalon 64 X2 Dual-Core Processor TK-53 at 1.70 GH on a Windows Vista system with 1G of memory.\nformula mean variance\n1 i1 ⊗ i3 ⇒ i6 5.68 39.33 2 i4 ⇒ i6 ⊗ i6 ⇒ i2 26.64 124.02 3 ((i1 ⇒ i4) ⊕ (i6 ⇒ i2)) ⊗ (i6 ⇒ i1) 39.48 202.94 4 (i4 ⊗ i5 ⇒ i6) ⊗ (i1 ⊗ i5 ⇒ i2) 51.67 483.85 5 ((i4 ⊗ i5 ⇒ i6) ⊕ (i1 ⊗ i5 ⇒ i2)) ⊗ (i1 ⊗ i3 ⇒ i2) 224.74 36475.47 6 ((i4 ⊗ i5 ⇒ i6) ⊕ (i1 ⊗ i5 ⇒ i2)) ⊗ (i1 ⊗ i3 ⇒ i2) ⊗ (i6 ⇒ i4) 368.32 55468.66"
    }, {
      "heading" : "4 Applying the process on real data",
      "text" : "The extraction of a rule from a data set is very different from the task of reverse engineering the rule used on the generation of a data set. In sense what, in the reverse engineering task we know the existence of a prefect description for the information, we know the adequate logic language to describe it and we have lack of noise. The extraction of a rule from a data set is made establishing a stopping criterium base on a language fixed by the extraction process. The expressive power of this language caracterize the learning algorithm plasticity. However very expressive languages produce good fitness to the trained data, but with bad generalization, and its sentences are usually difficult to understand.\nWith the application of our process to real data we try to catch information in the data similar to the information described using sentences in Lukasiewicz logic language. This naturally means what, in this case, we will try to search for simple and understandable models for the data. And for this make sense strategy followed of train of progressively more complex models and subjected to a strong criteria of pruning. When the mean squared error stopping criteria is satisfied it has big probability of be the simplest one. However some of its\nneuron configurations may be unrepresentable and must be approximated by a formula without damage drastically the model performance.\nNote however the fact what the use of the presented process can be prohibitive to train complex models having a grate number of attributes, i.e. learn formulas with many connectives and propositional variables. In this sense our process use must be preceded by a fase of attribute selection.\n4.0.1 Mushrooms\nMushroom is a data set available in UCI Machine Learning Repository. Its records drawn from The Audubon Society Filed Guide to North American Mushrooms (1981) G. H. Lincoff (Pres.), New York, was donate by Jeff Schlimmer. This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom. However we will try to find a one using the data set as a truth table.\nThe data set have 8124 instances defined using 22 nominally valued attributes presented in the table bellow. It has missing attribute values, 2480, all for attribute #11. 4208 instances (51.8%) are classified as editable and 3916 (48.2%) has classified poisonous.\nN. Attribute Values 0 classes edible=e, poisonous=p 1 cap.shape bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s 2 cap.surface fibrous=f,grooves=g,scaly=y,smooth=s 3 cap.color brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w, yellow=y 4 bruises? bruises=t,no=f 5 odor almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p, spicy=s 6 gill.attachment attached=a,descending=d,free=f,notched=n 7 gill.spacing close=c,crowded=w,distant=d 8 gill.size broad=b,narrow=n 9 gill.color black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p, purple=u,red=e,white=w,yellow=y 10 stalk.shape enlarging=e,tapering=t 11 stalk.root bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=? 12 stalk.surface.above.ring ibrous=f,scaly=y,silky=k,smooth=s 13 stalk.surface.below.ring ibrous=f,scaly=y,silky=k,smooth=s 14 stalk.color.above.ring brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y 15 stalk.color.below.ring brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y 16 veil.type partial=p,universal=u 17 veil.color brown=n,orange=o,white=w,yellow=y 18 ring.number none=n,one=o,two=t 19 ring.type cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s, zone=z 20 spore.print.color black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w, yellow=y 21 population abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y 22 habitat grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\nWe used a unsupervised filter converting all nominal attributes into binary numeric attributes. An attribute with k values is transformed into k binary attributes if the class is nominal. This produces a data set with 111 binary attributes.\nAfter the binarization we used the presented algorithm to selected relevante attributes for mushrooms classification. After 4231.8 seconds the system pro-\nduced a model, having an architecture (2,1,1), a quite complex rule with 100% accuracy depending on 23 binary attributes defined by values of\n{odor,gill.size,stalk.surface.above.ring, ring.type, spore.print.color}\nWith the values assumed by this attributes we produce a new data set. After some tries the simples model generated was the following:\nA1 − bruises? = t\n1\n❑❑ ❑❑\n❑❑ ❑❑\n❑❑ ❑❑\n❑❑ ❑❑\n❑❑ ❑❑\n❑\nA2 − odor ∈ {a, l, n}\n1\n◗◗◗ ◗◗◗\n◗◗◗ ◗◗◗\n◗◗◗ ◗◗◗\n1A3 − odor = c −1\n❳❳❳❳❳ ❳❳❳❳❳\n❳❳❳❳❳\nA4 − ring.type = e\n−1 76540123ϕ\nA5 − spore.print.color = r\n−1 ❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢❢\nA6 − population = c\n−1 ♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠♠\nA7 − habitat = w\n1 ssssssssssssssssssssss\nA8 − habitat ∈ {g,m, u, d, p, l}\n−1 ②②②②②②②②②②②②②②②②②②②②②②②②\nThis model have an accuracy of 100%. From it, and since attribute values in A2 and A3, and in A7 and A8 are auto exclusive, we used propositions A1, A2, A3, A4, A5, A6, A7 to define a new data set. This new data set was enriched with new negative cases by introduction for each original case a new one where the truth value of each attribute was multiplied by 0.5. For instance the ”eatable” mushroom case:\n(A1=0, A2=1, A3=0, A4=0, A5=0, A6=0, A7=0,A8=1,A9=0)\nwas used on the definition of a new ”poison” case\n(A1=0, A2=0.5, A3=0, A4=0, A5=0, A6=0, A7=0,A8=0.5,A9=0)\nThis resulted in a convergence speed increase and reduced the occurrence of no representable configurations.\nWhen we applied our ”reverse engineering” algoritmo to the enriched data set, having by stoping criteria the mean square error less than mse. For mse = 0.003 the system produced the model:\n[\n0 1 0 0 −1 0 1 0 1 0 1 0 0 −1\n] [\n−1 −1\n]\nA2 ⊗ ¬A5 ⊗ A7 A2 ⊗ A4 ⊗ ¬A7\n[ 1 1 ] [ 0 ] i1 ⊕ i2 [\n1 ] [ 0 ]\nThis model codifies the proposition\n(A2 ⊗ ¬A5 ⊗ A7) ⊕ (A2 ⊗ A4 ⊗ ¬A8)\nand misses the classification of 48 cases. It have 98.9% accuracy. More precise model can be produced, by restring the stoping criteria. However this, in general, produce more complex propositions and more dificulte to\nunderstand. For instance with mse = 0.002 the systems generated the bellow model. It misses 32 cases, having an accuracy of 99.2%, and easy to convert in a proposition.\n\n   0 0 0 −1 0 0 1 1 1 0 −1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 −1 −1 1\n\n  \n\n   1 −1 0\n−1\n\n  \n¬A4 ⊕ A7 A1 ⊗ A2 ⊗ ¬A4 A7 A2 ⊗ ¬A5 ⊗ ¬A6 ⊗ A7\n[\n−1 0 1 0 1 −1 0 −1\n] [\n1 0\n]\n¬i1 ⊕ i3 i1 ⊗ ¬i2 ⊗ ¬i4\n[ 1 −1 ] [ 0 ]\nj1 ⊗ ¬j2\nThis neural network codifies\nj1 ⊗ ¬j2 = (¬i1 ⊕ i3) ⊗ ¬(i1 ⊗ ¬i2 ⊗ ¬i4) = = (¬(¬A4 ⊕ A7) ⊕ A7) ⊗ ¬((¬A4 ⊕ A7) ⊗ ¬(A1 ⊗ A2 ⊗ ¬A4) ⊗ ¬(A2 ⊗ ¬A5 ⊗ ¬A6 ⊗ A7)) =\n= ((A4 ⊗ ¬A7) ⊕ A7) ⊗ ((A4 ⊗ ¬A7) ⊕ (A1 ⊗ A2 ⊗ ¬A4) ⊕ (A2 ⊗ ¬A5 ⊗ ¬A6 ⊗ A7))\nSome times the algorithm converged to unrepresentable configurations like the one presented bellow, having however 100% accuracy. The frequency of this type of configurations increases with the increase of required accuracy.\n\n −1 1 −1 1 0 −1 0 0 0 0 1 1 0 −1 1 1 0 0 0 0 −1\n\n\n\n 0 1 0\n\n\ni1 unrepresentable A4 ⊗ A5 ⊗ ¬A6 i3 unrepresentable\n[ 1 −1 1 ] [ 0 ] j1unrepresentable [\n1 ] [ 0 ]\nSince, for the similarity evaluation on data set, we have:\n1. i1 ∼0.0729 ((¬A1 ⊗ A4) ⊕ A2) ⊗ ¬A3 ⊗ ¬A6\n2. i3 ∼0.0 (A1 ⊕ ¬A7) ⊗ A2\n3. j1 ∼0.0049 (i1 ⊗ ¬i2) ⊕ i3\nThe formula\nα = (((((¬A1 ⊗ A4) ⊕ A2) ⊗ ¬A3 ⊗ ¬A6) ⊗ ¬(A4 ⊗ A5 ⊗ ¬A6)) ⊕ ((A1 ⊕ ¬A7) ⊗ A2)\nis λ-similar, with λ = 0.0049 to the original neural network. Formula α misses the classification for 40 cases. Note what the symbolic model is stable to the bad performance of i1 representation.\nOther example of unrepresentable is given bellow. This network structure can be simplified during the symbolic translation.\n\n 1 1 −1 1 0 0 1 0 0 1 −1 0 0 0 0 −1 0 1 1 0 −1\n\n\n\n −1 0 2\n\n\ni1 unrepresentable A3 ⊗ ¬A4 ¬A2 ⊗ A4 ⊗ A5 ⊗ ¬A7\n[\n−1 0 1 0 1 1\n] [\n0 1\n]\n¬i1 ⊗ i3 1\n[ −1 1 ] [ 1 ]\n¬j1 ⊗ j2\nSince i1 ∼0.0668 (A1 ⊗A2⊗A7)⊕ ¬A3⊕A4\nthe neural network is similar to,\nα = ¬j1 ⊗ j2 = ¬(¬i1 ⊗ i3) ⊗ 1 = ((A1 ⊗ A2 ⊗ A7) ⊕ ¬A3 ⊕ A4) ⊕ ¬(¬A2 ⊗ A4 ⊗ A5 ⊗ ¬A7)\nand the degree of similarity is λ = 0, i.e. the neural network interpretation is equivalent to formula α in the Mushrooms data set, in the sense what both produce equal classifications."
    }, {
      "heading" : "5 Conclusions",
      "text" : "This methodology to codify and extract symbolic knowledge from a neuro network is very simple and efficient for the extraction of simple rules from medium sized data sets. From our experience the described algorithm is a very good tool for attribute selection, particulary when we have low noise and classification problems depending from few nominal attributes to be selected from a huge set of possible attributes.\nIn the theoretic point of view it is particularly interesting the fact what restricting the values assumed by neurons weights restrict the information propagation in the network. Allowing the emergence of patterns in the neuronal network structure. For the case of linear neuronal networks these structures are characterized by the occurrence of patterns in neuron configuration with a direct symbolic presentation in a Lukasiewicz logic."
    } ],
    "references" : [ {
      "title" : "Neural networks and rational lukasiewicz logic",
      "author" : [ "P. Amato", "A.D. Nola", "B. Gerla" ],
      "venue" : "IEEE Transaction on Neural Networks, vol. 5 no. 6, 506- 510, 2002. ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A modified regression algorithm for fast one layer neural network training",
      "author" : [ "T.J. Andersen", "B.M. Wilamowski" ],
      "venue" : "World Congress of Neural Networks, Washington DC, USA, Vol. 1 no. 4, CA, (1995)687-690. ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Frist- and second-order methods for learning between steepest descent and newton’s method",
      "author" : [ "R. Battiti" ],
      "venue" : "Neural Computation, Vol. 4 no. 2, 141-166, 1992. ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Enhanced training algorithms",
      "author" : [ "M.G. Bello" ],
      "venue" : "and intehrated training/architecture selection for multi layer perceptron network, IEEE Transaction on Neural Networks, vol. 3, 864-875, 1992. ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Massively parallel reasoning",
      "author" : [ "S.E. Bornscheuer", "S. Hölldobler", "Y. Kalinke", "A. Strohmaier" ],
      "venue" : "in: Automated Deduction - A Basis foe Applications, Vol. II, Kluwer Academic Publisher, 291-321, 1998. ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The logic of neural networks",
      "author" : [ "J.L. Castro", "E. Trillas" ],
      "venue" : "Mathware and Soft Computing, vol. 5, 23-27, 1998. ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Conjugate gradient algorithm for efficient training of artificial neural networks",
      "author" : [ "C. Charalambous" ],
      "venue" : "IEEE Proceedings, Vol. 139 no. 3, 301-310, 1992. ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Semantics of architectural connectors",
      "author" : [ "J. Fiadeiro", "A. Lopes" ],
      "venue" : "TAP- SOFT’97 LNCS, v.1214, p.505-519, Springer-Verlag, 1997. ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "On the simultaneous associativity of f(x",
      "author" : [ "M.J. Frank" ],
      "venue" : "y) and x+y−f(x, y), Aequations Math., vol. 19, 194-226, 1979. ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Knowledge-based connectionism from revising domain theories",
      "author" : [ "L.M. Fu" ],
      "venue" : "IEEE Trans. Syst. Man. Cybern, Vol. 23, 173-182, 1993. ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Connectionist expert systems",
      "author" : [ "S.I. Gallant" ],
      "venue" : "Commun. ACM, Vol. 31 ,(1988)152-169. ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Functional representation of many-valued logics based on continuous t-norms",
      "author" : [ "B. Gerla" ],
      "venue" : "PhD thesis, University of Milano, 2000. ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Neural network design",
      "author" : [ "M.T. Hagan", "H.B. Demuth", "M.H. Beal" ],
      "venue" : "PWS Publishing Company, Boston.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Training feed-forward networks with marquardt algorithm",
      "author" : [ "M.T. Hagan", "M. Menhaj" ],
      "venue" : "IEEE Transaction on Neural Networks, vol. 5 no. 6, 989- 993, 1999. ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Optimal brain surgeon and general network pruning",
      "author" : [ "B. Hassibi", "D.G. Stork", "G.J. Wolf" ],
      "venue" : "IEEE International Conference on Neural Network, vol. 4 no. 5, 740-747, 1993. ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Logic programs and connectionist networks",
      "author" : [ "P. Hitzler", "S. Hölldobler", "A.K. Seda" ],
      "venue" : "Journal of Applied Logic, 2, 245-272, 2004. ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Challenge problems for the integration of logic and connectionist systems",
      "author" : [ "S. Hölldobler" ],
      "venue" : "in: F. Bry, U.Geske and D. Seipel, editors, Proceedings 14. Workshop Logische Programmierung, GMD Report 90, 161-171, 2000. ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Towards a new massively parallel computational model for logic programming",
      "author" : [ "S. Hölldobler", "Y. Kalinke" ],
      "venue" : "in: Proceedings ECAI94 Workshop on Combining symbolic and Connectionist Processing, 68-77, 1994. ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Approximating the semantics of logic programs by recurrent neural networks",
      "author" : [ "S. Hölldobler", "Y. Kalinke", "H.P. Störr" ],
      "venue" : "Applied Intelligence 11, 45-58, 1999. ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Increased rates of convergence through learning rate adaptation",
      "author" : [ "R.A. Jacobs" ],
      "venue" : "Neural Networks, Vol. 1 no. 4, CA, 295-308, 1988. ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "and S",
      "author" : [ "K. Mehrotra", "C.K. Mohan" ],
      "venue" : "Ranka, Elements of artificial neural networks,, The MIT Press.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Acceleration of back-propagation through learning rate and momentum adaptation",
      "author" : [ "A.A. Miniani", "R.D. Williams" ],
      "venue" : "Proceedings of International Joint Conference on Neural Networks, San Diego, CA, 676-679, 1990. ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Back-propagation improvements based on heuristic arguments",
      "author" : [ "T. Samad" ],
      "venue" : "Proceedings of International Joint Conference on Neural Networks, Washington, 565-568, 1990. ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Accelerated learning in layered neural networks",
      "author" : [ "S.A. Solla", "E. Levin", "M. Fleisher" ],
      "venue" : "Complex Sustems, 2, 625-639, 1988. ",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Extracting refined rules from knowledgebased neural networks",
      "author" : [ "G.G. Towell", "J.W. Shavlik" ],
      "venue" : "Mach. Learn., Vol. 13 ,71-101, 1993. ",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Our approach uses finite truth valued Lukasiewicz logic, where we take advantage of fact, presented by Castro in [6], what in this type of logics every connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and one.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 18,
      "context" : "One particular aspect of this problem which been considered in a number of papers, see [5] [17] [18] [19] [20], is the extraction of logic programs from trained networks.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "This has been done for some types of neuro networks like Knowledge-based networks [10] [27].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "In the other direction there has been widespread activity aimed at translating neural language in the form of symbolic relations [11] [12] [26].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "In the other direction there has been widespread activity aimed at translating neural language in the form of symbolic relations [11] [12] [26].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "one [6].",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "We focused our attention on many-valued logics having [0, 1] as set of truth values.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "In [13] it is defined as a binary operator defined in [0, 1] commutative and associative, non-decreasing in both arguments and 1⊗ x = x and 0⊗ x = 0.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "In [13] it is defined as a binary operator defined in [0, 1] commutative and associative, non-decreasing in both arguments and 1⊗ x = x and 0⊗ x = 0.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "In [9] all continuous t-norms are characterized as ordinal sums of Lukasiewicz, Gödel and product t-norms.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "2 Processing units As mention in [1] there is a lack of a deep investigation of the relationships between logics and neural networks.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "In [6] it is shown what, by taking as activation function ψ the identity truncated to zero and one, also named saturating linear transfer function ψ(x) = min(1,max(x, 0)) it is possible to represent the corresponding neural network as combination of propositions of Lukasiewicz calculus and viceversa[1].",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "In [6] it is shown what, by taking as activation function ψ the identity truncated to zero and one, also named saturating linear transfer function ψ(x) = min(1,max(x, 0)) it is possible to represent the corresponding neural network as combination of propositions of Lukasiewicz calculus and viceversa[1].",
      "startOffset" : 300,
      "endOffset" : 303
    }, {
      "referenceID" : 7,
      "context" : "This task of construct complex structures based on simplest ones can be formalized using generalized programming [8].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "A truth table fφ for a formula φ is a map fφ : [0, 1] m → [0, 1], where m is the number of propositional variables used in φ.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "A truth table fφ for a formula φ is a map fφ : [0, 1] m → [0, 1], where m is the number of propositional variables used in φ.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Since, for every network N and n > 0, ∆(N) ≥ ∆(Υn(N)), we have: Proposition 7 Given a neural networks N with weights in the interval [0, 1].",
      "startOffset" : 133,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "Stork in [16].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : "Many efforts have been made to speed up EBP algorithm [4] [24] [25] [23] [21].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "The Levenderg-Marquardt algorithm (LM) [15] [2] [3] [7] ensued from development of EBP algorithm dependent methods.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "It gives a good exchange between the speed of Newton algorithm and the stability of the steepest descent method [3].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "The Jacobian matrix Jk can be computed through a standard back propagation technique [22] that is much less complex than computing the Hessian matrix.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "In this way, the performance function is always reduced at each iteration of the algorithm [14].",
      "startOffset" : 91,
      "endOffset" : 95
    } ],
    "year" : 2016,
    "abstractText" : "This work describes a methodology to combine logic-based systems and connectionist systems. Our approach uses finite truth valued Lukasiewicz logic, where we take advantage of fact, presented by Castro in [6], what in this type of logics every connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and one. This allowed the injection of first-order formulas in a network architecture, and also simplified symbolic rule extraction. Our method trains a neural network using Levenderg-Marquardt algorithm, where we restrict the knowledge dissemination in the network structure. We show how this reduces neural networks plasticity without damage drastically the learning performance. Making the descriptive power of produced neural networks similar to the descriptive power of Lukasiewicz logic language, simplifying the translation between symbolic and connectionist structures. This method is used in the reverse engineering problem of finding the formula used on generation of a truth table for a multi-valued Lukasiewicz logic. For real data sets the method is particulary useful for attribute selection, on binary classification problems defined using nominal attribute. After attribute selection and possible data set completion in the resulting connectionist model: neurons are directly representable using a disjunctive or conjunctive formulas, in the Lukasiewicz logic, or neurons are interpretations which can be approximated by symbolic rules. This fact is exemplified, extracting symbolic knowledge from connectionist models generated for the data set Mushroom from UCI Machine Learning Repository.",
    "creator" : "LaTeX with hyperref package"
  }
}