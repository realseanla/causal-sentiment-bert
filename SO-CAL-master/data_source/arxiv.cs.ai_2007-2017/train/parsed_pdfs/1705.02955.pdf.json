{
  "name" : "1705.02955.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Safe and Nested Subgame Solving for Imperfect-Information Games∗",
    "authors" : [ "Noam Brown" ],
    "emails" : [ "noamb@cs.cmu.edu", "sandholm@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Imperfect-information games model strategic settings that have hidden information. They have a myriad of applications including negotiation, auctions, cybersecurity, and physical security. In such games, the typical goal is to find a Nash equilibrium [22], which is a profile of strategies—one for each player—such that no player can improve by unilaterally deviating to a different strategy.\nSubgame solving is a standard technique in perfect-information games such as chess and checkers [1] in which a piece of the game is solved in isolation. This can be accomplished in perfect-information games because the exact state of the game is known, which allows the remaining subgame to be solved independently from the rest of the game. For example, in chess determining the optimal response to the Queen’s Gambit requires no knowledge of the optimal response to the Sicilian Defense. This decomposition was key to AIs being able to defeat top humans in chess [5] and Go [29]. In checkers, the ability to decompose the game into smaller independent subgames was even used to solve the entire game [27].\nIn contrast, imperfect-information games cannot be solved via decomposition as perfect-information games can because the optimal strategy in a subgame may depend on strategies and outcomes in other, unreached subgames. Although this is a counter-intuitive idea, we provide a demonstration of this in Section 2.\nRather than rely on decomposition, typical past approaches for imperfect-information games involved solving the game as a whole. For example, heads-up limit Texas hold’em, a relatively simple form of poker with 1013 decision points, was essentially solved without decomposition. However, this approach cannot extend to large games, such as heads-up no-limit Texas hold’em—the primary ∗A version of this paper was posted on the authors’ web pages in 2016, submitted to the AAAI-17 Workshop on Computer Poker and Imperfect Information Games in October 2016, and published in that workshop on February 5th, 2017.\nar X\niv :1\n70 5.\n02 95\n5v 1\n[ cs\n.A I]\n8 M\nay 2\n01 7\nbenchmark problem in imperfect-information game solving—which has 10161 decision points, or can even be infinite in size if fractional bets are allowed [14].2 The standard approach to computing strategies in such large games is to first generate an abstraction of the game, which is a smaller version of the game that retains as much as possible the strategic characteristics of the original game [24, 26, 25]. For example, a continuous action space might be discretized. This abstract game is solved and its solution is used when playing the full game by mapping states in the full game to states in the abstract game (for example, by rounding to the nearest discrete action in the case of a discretized continuous action space). In extremely large games, a small abstraction may not capture all the strategic complexity of the game, and its solution may be far from a Nash equilibrium in the original game.\nFor this reason, it seems natural to attempt to improve the strategy as we descend the game tree and the remaining subgame becomes smaller, even though—as explained previously—this may not lead to a Nash equilibrium. For example, at the start of a game of poker we could include in the abstraction a large number of bet sizes for the early stages of the game, but only a few different bet sizes for the final rounds. When we reach the final rounds of the game, we could calculate a new strategy in the subgame we are in that has a large number of bet sizes in the final rounds.\nWhile it may not be possible to arrive at an exact equilibrium by analyzing subgames independently in this way, it may be possible to improve the strategies in those subgames when the original (trunk) strategy is suboptimal.\nIn Section 2 we first present an intuitive example demonstrating why imperfect-information subgames cannot be solved in isolation, unlike perfect-information games. Section 3 defines notation and provides background that is used in the remaining paper. In Section 4 we review prior forms of subgame solving for imperfect-information games. Then in Section 5 we propose a new form of subgame solving that retains the theoretical guarantees of the best prior methods while performing better in practice. Next, in Section 6 we present an alternative form of subgame solving that is more robust to errors in the assumptions of the model. This weakens the theoretical guarantees of the algorithm, but improves practical performance dramatically. While this section is important for those who wish to implement and build upon the algorithms in this paper, it is not necessary for the more casual reader who wishes to gain a high-level understanding of subgame solving. In Section 7 we introduce a method for subgame solving to be nested as players descend the game tree, leading to substantially better performance compared to action translation, the prior state-of-the-art approach. Finally, in section 8 we show experimentally that these new subgame solving techniques lead to substantially lower exploitability compared to past techniques. We also present experimental results from the 2017 Brains vs. AI competition in which Libratus, our AI which uses the techniques presented in this paper, defeated top human specialists in heads-up no-limit Texas hold’em poker, the primary challenge problem for imperfect-information games. This was the first time an AI defeated top humans in heads-up no-limit Texas hold’em."
    }, {
      "heading" : "2 Coin Toss",
      "text" : "In this section we provide intuition for why an imperfect-information subgame cannot be solved in isolation. We demonstrate this in a simple game we call Coin Toss, shown in Figure 1a, which will be used as a running example throughout the paper.\nCoin Toss is played between players P1 and P2. A coin is flipped and lands either Heads or Tails with equal probability; only P1 sees the outcome. P1 can then choose between actions “Sell” and “Play.” The Sell action leads to a subgame whose details are not important, but the expected value to P1 of choosing the Sell action will be important. (For simplicity, one can equivalently assume in this section that Sell leads to an immediate terminal reward, where the value depends on whether the coin landed Heads or Tails). If the coin lands Heads, it is considered lucky and P1 can receive an expected\n2The version of heads-up no-limit Texas hold’em we refer to, which is the standard in the AI community, allows bets in increments of $1, with each player having $20,000. This version has 10161 decision points. However, this number can be made arbitrarily large by allowing finer-grained bet sizes. For example, allowing bets in increments of $0.01 would multiply the branching factor of the game by 100, without meaningfully changing the strategic complexity of the game. For this reason, it is more appropriate to view no-limit Texas hold’em as a game with continuous action spaces in which traditional measurements of game size do not directly apply.\nvalue of $0.50 for choosing Sell. On the other hand, if the coin lands Tails, it is considered unlucky and P1 receives an expected value of −$0.50. (that is, P1 must on average pay someone $0.50 to get rid of the coin). If P1 instead chooses Play, then P2 has the opportunity to guess how the coin landed. If P2 guesses correctly, P1 receives a reward of −$1. The figure shows rewards only for P1; P2 always receives the negation of P1’s reward. P2 also has the option to forfeit, which should never be chosen but will be relevant in later sections. We wish to determine the optimal strategy for P2 in the subgame S that occurs after P1 chooses Play, shown in Figure 1a.\nWere P2 to always guess Heads, P1 would receive $0.50 for choosing Sell when the coin lands Heads, and $1 for choosing Play when it lands Tails. This would result in an average of $0.75 for P1. Alternatively, were P2 to always guess Tails, P1 would receive $1 for choosing Play when it lands Heads, and −$0.50 for choosing Sell when it lands Tails. This would result in an average reward of $0.25 for P1. However, P2 would do even better by guessing Heads with 25% probability and Tails with 75% probability. In that case, P1 could only receive $0.50 (on average) by choosing Play when the coin lands Heads—the same value for choosing Sell. Similarly, P1 could only receive −$0.50 by choosing Play when the coin lands Tails, which is the same value received for choosing Sell. This would yield an average reward of $0 for P1. It is easy to see that this is the best P2 could do, because P1 can receive at least $0 in expectation by always choosing Sell. Therefore, choosing Heads with 75% probability and Tails with 25% probability is an optimal strategy for P2 in the “Play” subgame.\nNow suppose the coin is considered lucky if it lands Tails and unlucky if it lands Heads. That is, the expected reward for selling the coin when it lands Heads is now −$0.50 and when it lands Tails is now $0.50. It is easy to see that P2’s optimal strategy for the “Play” subgame is now to guess Heads with 75% probability and Tails with 25% probability. This shows that a player’s optimal strategy in a subgame can depend on the strategies and outcomes in other parts of the game. Thus, one cannot solve a subgame using information about that subgame alone. This is the central challenge of playing imperfect-information games as opposed to perfect-information games."
    }, {
      "heading" : "3 Notation and Background",
      "text" : "In an imperfect-information extensive-form game there is a finite set of players, P . H is the set of all possible histories (nodes) in the game tree, represented as a sequence of actions, and includes the empty history. A(h) is the actions available in a history and P (h) ∈ P ∪ c is the player who acts at that history, where c denotes chance. Chance plays an action a ∈ A(h) with a fixed probability that is known to all players. The history h′ reached after an action is taken in h is a child of h, represented by h · a = h′, while h is the parent of h′. If there exists a sequence of actions from h to h′, then h is an ancestor of h′ (and h′ is a descendant of h) denoted h @ h′. Z ⊆ H are terminal histories from\nwhich no actions are available. For each player i ∈ P , there is a payoff function ui : Z → <. If P = {1, 2} and u1 = −u2, the game is two-player zero-sum. Imperfect information is represented by information sets (infosets) for each player i ∈ P by a partition Ii of h ∈ H : P (h) = i. For any infoset I ∈ Ii, all histories h, h′ ∈ I are indistinguishable to i, so A(h) = A(h′). I(h) is the infoset I where h ∈ I . P (I) is the player i such that I ∈ Ii. A(I) is the set of actions such that for all h ∈ I , A(I) = A(h). A strategy σi(I) is a probability vector over A(I) for player i in I . The probability of a particular action a is denoted by σi(I, a). Since all histories in an infoset belonging to player i are indistinguishable, the strategies in each of them must be identical. That is, for all h ∈ I , σi(h) = σi(I) and σi(h, a) = σi(I, a). A full-game strategy σi ∈ Σi defines a strategy for each infoset belonging to player i. A strategy profile σ is a tuple of strategies, one for each player. The expected payoff for player i if all players play according to the strategy profile 〈σi, σ−i〉 is ui(σi, σ−i), where σ−i denotes the strategies in σ of all players other than i.\nLet πσ(h) = ∏ h′·avh σP (h′)(h\n′, a) denote the joint probability of reaching h if all players play according to σ. πσi (h) is the contribution of player i to this probability (that is, the probability of reaching h if all players other than i, and chance, always chose actions leading to h). πσ−i(h) is the contribution of all players other than i, and chance. πσ(h, h′) is the probability of reaching h′ given that h has been reached, and 0 if h 6@ h′. In a perfect-recall game, ∀h, h′ ∈ I ∈ Ii, πi(h) = πi(h′). In this paper we focus specifically on two-player zero-sum perfect-recall games. Therefore, for i = P (I) we define πi(I) = πi(h) for h ∈ I . Moreover, I ′ @ I if for some h′ ∈ I ′ and some h ∈ I , h′ @ h. Similarly, I ′ · a @ I if h′ · a @ h. Finally, πσ(I, I ′) is probability of reaching I ′ from I according to the strategy σ.\nWe define an imperfect-information subgame, which we refer to simply as a subgame in this paper. A subgame is easily defined in perfect-information games as containing a history (the root) and all its descendants. The existence of infosets complicates this, because it does not make sense to include only some of the histories from an infoset and not others. An imperfect-information subgame overcomes this problem by expanding the subgame to include all histories (and their descendants) which share an infoset with a history already in the subgame. In most cases (but not all) an imperfectinformation subgame can intuitively be described as including all histories which share public actions (that is, actions viewable to both players). That is, we can construct a game tree consisting only of public actions by players or chance, where a node in the tree is a set that contains all the histories which involve that sequence of public actions (as well as any sequence of private actions). An imperfect-information subgame is defined as containing all the histories in a single node (the root) in this public-action game tree, as well as all their descendants. In poker, for example, a subgame is uniquely defined by a sequence of bets (viewable to both players) and public board cards, but not by private player cards. Figure 1b shows the public game tree of Coin Toss. While this view of subgames is intuitive and covers many common cases, it is possible to construct subgames that do not fit into this formulation.3 Formally, an imperfect-information subgame is a set of histories S ⊆ H such that for all h ∈ S, if h @ h′, then h′ ∈ S, and for all h ∈ S, if h′ ∈ I(h) for some I ∈ IP (h) then h′ ∈ S. A Nash equilibrium [21] is a strategy profile σ∗ such that ∀i, ui(σ∗i , σ∗−i) = maxσ′i∈Σi ui(σ ′ i, σ ∗ −i). In two-player zero-sum games, all Nash equilibria give identical expected values for a player. A best response BRi(σ−i) is a strategy for player i such that ui(BRi(σ−i), σ−i) = maxσ′i∈Σi ui(σ ′ i, σ−i). The exploitability exp(σ−i) of a strategy σ−i is defined as ui(BRi(σ−i), σ−i)− ui(σ∗), where σ∗ is a Nash equilibrium.\nCounterfactual value vσ(I) is the value player i expects to achieve by playing according to σ, having already reached infoset I . Formally, vσi (I, a) =\n1 πσ−i(I)\n∑ h∈I ( πσ−i(h) ∑ z∈Z ( πσ(h · a, z)ui(z) )) and vσi (I) = maxa∈A(I) v σ i (I, a)\nA counterfactual best response [19] CBRi(σ−i) is similar to a best response, but additionally maximizes counterfactual value at every infoset. Specifically, a counterfactual best response is a strategy σi that is a best response with the additional condition that if σi(I, a) > 0 then vσi (I, a) = maxa′ v\nσ(I, a′). 3For example, games in which information is revealed at different times to each player, so that no action can\nbe described as “public.”\nWe further define counterfactual best response value CBV σ−i(I) as the value player i expects to achieve by playing according to CBRi(σ−i), having already reached infoset I . Formally, CBV σ−i(I, a) = v\n〈CBRi(σ−i),σ−i〉 i (I, a) and CBV σ−i(I) = maxa∈A(I) CBV σ−i(I, a)."
    }, {
      "heading" : "4 Prior Approaches to Subgame Solving in Imperfect-Information Games",
      "text" : "This section reviews prior techniques for subgame solving in imperfect-information games. Our new algorithm then builds on some of the ideas and notation.\nThroughout this section, we refer to the Coin Toss game shown in Figure 1a. We focus on the Play subgame. If P1 chooses Sell, the game continues to a separate subgame (not shown).\nAs discussed in Section 1, a standard approach to dealing with large imperfect-information games is to solve an abstract, simplified version of the game. This abstract solution is a strategy profile in the full game (which is typically quite far from a Nash equilibrium, despite being a Nash equilibrium in the abstract game). We refer to this strategy profile in the full game as the trunk. The goal of subgame solving is to improve the trunk by changing the strategy only in a subgame. While the trunk is frequently a Nash equilibrium (or approximate Nash equilibrium) in some abstraction of the full game, our techniques do not assume this. The trunk can in fact be any arbitrary strategy in the full game.\nAssume that a trunk strategy profile σ (shown in Figure 2) has already been computed for Coin Toss in which P1 chooses Play 34 of the time with Heads and 1 2 of the time with Tails, and P2 chooses Heads 12 of the time, Tails 1 4 of the time, and Forfeit 1 4 of the time after P1 chooses Play.\n4 The details of the trunk strategy in the Sell subgame are not relevant in this section, but the expected value for choosing the Sell action is relevant. We assume that if P1 chose the Sell action and played optimally thereafter, then she would receive an expected payoff of 0.5 if the coin is Heads, and −0.5 if the coin is Tails. We will attempt to improve P2’s strategy in the subgame S that follows P1 choosing Play."
    }, {
      "heading" : "4.1 Unsafe Subgame Solving",
      "text" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [2, 10, 11, 8]. This form of subgame solving assumes that both players will play according to their trunk strategies outside of the subgame. In other words, all nodes outside the subgame are fixed and can be treated as chance nodes with probabilities determined by the trunk strategy. Thus, the different roots of the subgame are reached with probabilities determined from the trunk strategies using Bayes’ rule. A strategy is then computed for the subgame—independently from the rest of the game.\n4In many large games the trunk strategy is far from optimal either because the equilibrium-finding algorithm did not sufficiently converge or because the game was too large and had to be abstracted. Clearly the example trunk strategy shown here could be trivially improved; we use it for simplicity of exposition.\nIn all subgame solving algorithms, an augmented subgame containing S, but much smaller than the original game, is solved to determine the strategy for S. Applying Unsafe subgame solving to the trunk strategy in Coin Toss (after P1 chooses Play) means solving the augmented subgame shown in Figure 3.\nSpecifically, we define R as the set of earliest-reachable histories in S. That is, h ∈ R if h ∈ S and h′ 6∈ S for any h′ @ h. We then calculate πσ(h) for each h ∈ R. The augmented subgame is constructed consisting only of an initial chance node and S. The initial chance node reaches h ∈ R with probability π\nσ(h)∑ h′∈R π\nσ(h′) . The augmented subgame is solved and its strategy is then used whenever S is encountered.\nUnsafe subgame solving lacks theoretical solution quality guarantees and there are many situations where it performs extremely poorly, because it makes strong assumptions about P1’s strategy outside S that may not be true. Indeed, if it were applied to the trunk strategy of Coin Toss, it would produce a strategy in which P2 always chooses Heads—which P1 could exploit severely by only choosing Play with Tails. Despite the lack of theoretical guarantees and potentially bad performance, Unsafe subgame solving is simple and can sometimes produce low-exploitability strategies, as we show later.\nWe now move to discussing safe subgame solving techniques, that is, ones that ensure that the exploitability of the strategy is no higher than that of the trunk strategy."
    }, {
      "heading" : "4.2 Subgame Re-Solving",
      "text" : "In subgame re-solving [4], a safe strategy is computed for P2 in the subgame by constructing the augmented subgame shown in Figure 4, and computing an equilibrium strategy σS for it. The augmented subgame differs from Unsafe subgame solving by giving P1 the option to “opt out” from entering the subgame and instead receive the value she could get for entering the subgame if P2 played according to the trunk strategy. Specifically, for each earliest-reachable history h in the subgame (that is, each h ∈ R), let hr be its parent and aS be the action leading to h such that hr · aS = h. We require hr to be a P1 history; if it is not, then we can simply insert a P1 history with only a single action between hr and h. These “parent” histories hr form the head of the subgame Sr. Sr is not included in S. Instead, every earliest-reachable history in S has a parent in Sr (and the parent is a P1 history).\nThe augmented subgame consists of a starting chance node that connects to each history hr in Sr in proportion to the probability that player P1 could reach hr if P1 tried to do so (that is, in proportion to πσ−1(hr)). Let aS be the action in hr that connects to S in the original game (that is, hr · aS ∈ S). After the initial chance node in the augmented subgame, P1 has two possible actions. Action a′S , the augmented-subgame equivalent of aS , leads into S, while action a′T leads to a terminal payoff that awards the best response value of entering the subgame if P2 plays according to the trunk strategy (that is, CBV σ−1(I(hr), aS)). In the trunk strategy of Coin Toss, P1 choosing Play after the coin lands Heads results in an expected value of 0, and 12 if the coin is Tails. Therefore, a ′ T leads to a terminal payoff of 0 for Heads and 12 for Tails. After the equilibrium strategy σ S is computed in the\naugmented subgame, P2 plays according to the computed subgame strategy σS2 rather than the trunk strategy when in S. The P1 strategy σS1 is not used.\nClearly P1 cannot do worse than always picking action a′T (which awards the same expected value as P1 playing a best response against P2’s trunk strategy). But P1 also cannot do better than always picking a′T , because P2 could simply play according to the trunk strategy in S, which means action a′S would give the same expected value to P1 as action a ′ T (if P1 played optimally in S). In this way, the strategy for P2 in S is pressured to be no worse than that of the trunk strategy. In the example game Coin Toss, if P2 were to always choose Heads (as was the case in Unsafe subgame solving), then P1 would always choose a′T with Heads and a ′ S with Tails.\nRe-solving guarantees that P2’s strategy will be no worse than the trunk (and may be better). However, it may miss out on opportunities for improvement. For example, if we apply re-solving to the example trunk strategy in Coin Toss, one possible solution to the augmented subgame is the trunk strategy itself, so we may arrive at the same exact strategy as the trunk in which Player 2 chooses Forfeit 25% of the time, even though Heads and Tails dominate that action. The next subgame solving technique addresses this shortcoming by adding a stronger condition for a solution of the augmented subgame."
    }, {
      "heading" : "4.3 Maxmargin Solving",
      "text" : "Maxmargin solving [19] is similar to Re-solving, except that it seeks to improve P2’s strategy in the subgame strategy as much as possible. While Re-solving seeks a strategy for P2 in S that would simply dissuade P1 from entering S, Maxmargin solving additionally seeks to punish P1 as much as possible if P1 nevertheless chooses to enter S. A subgame margin is defined for each infoset in Sr, which represents the difference in value between entering the subgame versus choosing the alternative payoff. Specifically, for each infoset I ∈ Sr and action aS leading to S, the subgame margin is Mσ S\n(I, aS) = v σS (I, a′T )− vσ S (I, a′S), or equivalently\nMσ S (I, aS) = CBV σ−i(I, aS)− vσ S (I, a′S) (1)\nIn Maxmargin solving, a Nash equilibrium strategy is computed such that the minimum margin over all I ∈ Sr is maximized. Formally, Maxmargin finds a Nash equilibrium strategy profile σS for the augmented subgame described in Re-solve subgame solving, with the additional condition that σS = arg maxσ′S{minI∈Sr Mσ ′S (I, a′S) ) }. Aside from maximizing the minimum margin, the augmented subgames used in Re-solving and Maxmargin solving are identical.\nGiven our base strategy in Coin Toss, Maxmargin solving would result in P2 choosing Heads with probability 38 , Tails with probability 5 8 , and Forfeit with probability 0.\nThe augmented subgame can be solved in a way that maximizes the minimum margin by using a standard LP solver. In order to use iterative algorithms such as the Excessive Gap Technique [23, 9, 17] or Counterfactual Regret Minimization (CFR) [32], one can use the gadget game described by Moravcik et al. [19]. Details on the gadget game are provided in the Appendix. Our experiments used CFR.\nMaxmargin solving is safe. Furthermore, it guarantees that if every Player 1 best response reaches the subgame with positive probability through some infoset(s) that have positive margin, then exploitability is strictly lower than that of the trunk strategy. While the theoretical guarantees are stronger, Maxmargin may lead to worse practical performance relative to Re-solving when combined with the techniques discussed in Section 6, due to Maxmargin’s greater tendency to overfit to assumptions in the model.\nStill, none of the prior techniques consider that in Coin Toss P1 can achieve a payoff of 0.5 by choosing Sell with Heads, and thus has more incentive to reach S when in the Tails state. The next section introduces our new technique, Reach subgame solving, which addresses this problem."
    }, {
      "heading" : "5 Reach Subgame Solving",
      "text" : "In this section we introduce Reach subgame solving, an improvement to both Re-solving and Maxmargin subgame solving that considers what payoffs are achievable from other paths in the game. We first consider the case of solving a single subgame. We then cover independently solving multiple subgames."
    }, {
      "heading" : "5.1 Solving a Single Subgame",
      "text" : "All of the subgame-solving techniques described in Section 4 only consider the target subgame in isolation. This can be improved by incorporating information about what payoffs the players could receive by not reaching the subgame. For example in the Coin Toss trunk strategy, P1 can receive an expected value (EV) of 0.5 by choosing Sell in the Heads state, and −0.5 in the Tails state. The solution that Maxmargin solving produces would result in P1 receiving an EV of − 14 by choosing Play in the Heads state, and 14 in the Tails state. Thus, P1 could simply always choose Sell in the Heads state and Play in the Tails state against P2’s strategy and receive an EV of 38 .\nReach subgame solving improves upon Re-solve and Maxmargin subgame solving by considering all the actions P1 could take along the path to the subgame. If there was an action leading away from the subgame that had a higher expected value than the action leading to the subgame, then P1 would be making a mistake by choosing to reach the subgame. This difference in value is a gift to P2 that allows P2 to be less concerned with P1 reaching the subgame along that path. The gift can be added to the P1 infoset in Sr that would be reached along the path. Since in Maxmargin we want to maximize the minimum margin, and in Re-solve we want all margins to be nonnegative, this gives us greater flexibility to increase the margin for other infosets instead.\nThe augmented subgame used in Reach-Maxmargin and Reach-Resolve requires additional definitions. Define the pathQS(I) to an infoset I ∈ Sr to be the set of infosets I ′ such that I ′ v I and I ′ is not an ancestor of any other information set in Sr. Let I0 be the earliest infoset in QS(I). At each P1 infoset along the path from I0 to I , we add to the margin of I the difference between the value of the optimal action (that is, the counterfactual best response value) and the value of the action taken to continue on the path, divided by the probability of reaching the subgame. If the optimal action is to continue along the path, then nothing is added to the margin. Specifically, for each I ′ ∈ QS(I) and action a′ ∈ A(I ′) that leads to S, we add g(I ′, a′) = CBV\nσ2 (I′)−CBV σ2 (I′,a′) πσ−i(I ′,I) to the margin of I . We divied\nby πσ−i(I ′, I) because even if the counterfactual value of I increased by g(I ′, a′), the infoset I is only reached from I ′ with probability πσ−i(I ′, I). So the counterfactual value of action a′ in I ′ would still be at mostCBV σ2(I ′, a′)+πσ−i(I ′, I)g(I ′, a′) ≤ CBV σ2(I ′, a′)+CBV σ2(I ′)−CBV σ2(I ′, a′) ≤ CBV σ2(I ′).\nFormally, we define a single subgame reach margin as\nMσSssr(I, aS) = M σS (I, aS) + ∑ I′·a′vI·aS |I′∈QS(I) CBV σ2(I ′)− CBV σ2(I ′, a′) πσ−i(I ′, I)\nTheorem 1 shows that Reach-Maxmargin results in a combined strategy with exploitability lower than or equal to the trunk strategy. If the opponent reaches the subgame with positive probability and\nthe margin of the reached infoset is positive, then exploitability is strictly lower than that of the trunk strategy. 5\nTheorem 1. Given a strategy σ2, an imperfect-information subgame S for P2, and a solved subgame Nash equilibrium strategy σS2 , let σ ′ 2 be the strategy that plays according to σ S 2 in subgame S and σ2 elsewhere. If minIMssr(I, aS) ≥ 0 for S, then exp(σ′2) ≤ exp(σ2). Furthermore, if π〈BR\nσ′2 ,σ′2〉(I) > 0 for some I ∈ Sr for a subgame S, then exp(σ′2) ≤ exp(σ2) − π σ′2 −1(I) minIMssr(I, aS).\nThis theorem statement is similar to that of Maxmargin [19], but the margins here are higher than (or equal to) those in Maxmargin."
    }, {
      "heading" : "5.2 Solving Multiple Subgames Independently",
      "text" : "Other subgames solving methods have also considered the cost of reaching a subgame [31, 13]. However, those approaches (and the version of Reach subgame solving we described above) are only correct in theory when applied to a single subgame. Typically, we want to solve multiple subgames independently—or, equivalently, any subgame that is reached at run time. This poses a problem because the construction of the augmented subgame assumes that all P2 nodes outside the subgame have strategies that are fixed according to the trunk strategy. If this assumption is violated by changing the strategy in multiple subgames, then the safety of Reach subgame solving (that is, the guarantee that exploitability will be no worse than the trunk) may no longer hold.\nTo address this issue, we make two changes. First, we must ensure that we do not “double count” gifts that P1 gives us by using an entire gift in multiple subgames. For example, consider the game shown in Figure 5 which contains two subgames S1 and S2. The Sell action leads to an expected value of 0.5 from the Heads state, while Play leads to an expected value of 0. When solving just one of these subgames, P2 can afford to always choose Tails, thereby letting P1 achieve a value of 1 for reaching that subgame from Heads, because due to the chance node it would only increase P1’s value for choosing Play by 0.5. But if the same reasoning is applied to both subgames independently then both subgames would always choose Tails, and P1’s value for choosing Play from Heads would become 1 while the value for Sell would only be 0.5.\nTo avoid this double-counting issue, gifts from any P1 action a′ in an infoset I ′ must be divided among all the subgames that can be reached from that point on. For a division to be valid, it must\n5When solving only a single subgame, techniques that are similar to Reach subgame solving exist that achieve even better theoretical performance, and indeed are provably optimal [13]. However, those techniques do not generally apply when solving multiple subgames independently, which is typically the more relevant problem (though they still apply in rare cases when solving multiple subgames independently). We use the single-subgame technique described in this section because it can be more easily extend to the case of solving multiple independent subgames.\nensure that if all the reachable subgames made full use of their share of the gift, the counterfactual value of a′ in I ′ would still be no higher than the counterfactual value of the best action in I ′. Any division satisfying this condition is sufficient, and ideally the gift would be divided primarily among the subgames that would make the greatest use of it. However, it may be difficult to know beforehand which subgames are in greatest need of a gift. In this paper, we divide gifts among subgames according to the probability that P1 could reach the subgame. In other words, assume action I ′ · a′ results in a gift and action I is a descendant of I ′ and has an action aS that leads to a subgame S. We increase the margin of I in the subgame S by CBV σ(I ′)− CBV σ(I ′, a′). We refer to this division as splitting gifts by reach. Later we prove in Theorem 2 that splitting by reach is theoretically sound.\nWhile in theory this splitting of gifts is necessary to guarantee exploitability does not increase, in practice it is not always necessary. Since many subgames will not use the gifts they are given, one can heuristically increase the size of the gifts and rely on the double-counting not to occur in practice (or at least not to occur to the fullest extent possible). In our experiments we show both the theoretically correct splitting of gifts and the more aggressive scaling up of gifts. We use one additional improvement in the experiments when splitting the gifts: if a subgame consists of only a terminal node (such as a fold action in poker) then clearly any assigned gift will not be used. Thus, we do not consider those subgames when dividing gifts.\nThe second issue when solving subgames independently is that gifts we assumed were present may actually not exist. For example, in Coin Toss suppose solving the Sell subgame results in P1’s value for Sell from the Heads state dropping from 0.5 to 0.25. If we independently solve the Play subgame then we have no way of knowing that the gift from the Sell action dropped, so we may still assume there is a gift of 0.5 from the Heads state based on the trunk strategy. Thus, in order to guarantee a theoretical result on exploitability that is stronger than Maxmargin solving, we must use a lower bound on gifts. In our experiments we use the minimum reachable payoff as a lower bound.6\nUsing a lower bound for gifts guarantees that exploitability will never be higher than Maxmargin solving (and may be lower). Still, even if we do not use a lower bound and instead simply assume that the gifts from the trunk strategies are accurate (that is, a gift is determined by the counterfactual best response values against the P2 trunk strategy), then the resulting P2 strategy is still guaranteed to have exploitability no higher than the trunk strategy (and therefore retain the same theoretical guarantees as Re-solving). But the stronger theoretical guarantee from incorporating gifts is lost in that case. In practice, it may be best to use an accurate estimate of what the gifts would be after all subgames are solved, if such an estimate exists (that is, an estimate of CBV σ ′ 2\n1 (I ′) for an infoset I ′,\nwhere σ′2 is the P2 strategy after solving all subgames). The idea of using estimates is covered in more detail in Section 6.\nLet ĝ(I ′, a′) be an estimate of CBV σ ′ 2(I ′) − CBV σ′2(I ′, a′) such that CBV σ2(I ′) − CBV σ2(I ′, a′) ≥ ĝ(I ′, a′) ≥ bCBV σ′2(I ′) − CBV σ′2(I ′, a′)c. When solving multiple subgames independently, the augmented subgame is identical to that in Re-solve and Maxmargin except the value of the alternative payoff for infoset I ∈ Sr is increased by ∑ I′·a′vI·a ĝ(I\n′, a′). Formally, we define a reach margin as7\nMr(I, aS) = M(I, aS) + ∑\nI′·a′vI·a\nĝ(I ′, a′) (2)\nThis margin is larger than or equal to the one used in Maxmargin, because ĝ(I ′, a′) is nonnegative. We also define bMr(I, aS)c similarly, except it uses lower bounds on CBV σ ′ 2(I ′)− CBV σ′2(I ′, a′) for gifts. Theorem 2 shows that when subgames are solved independently and splitting gifts as described, and a subgame has positive minimum margin and is reached with positive probability, then Reach-Maxmargin solving will produce a strategy with lower exploitability than the trunk.\nTheorem 2. Given a strategy σ2, a set of disjoint subgames S for P2, and a subgame strategy σS2 for each subgame S ∈ S produced via Reach-Maxmargin solving using lower bounds for gifts and splitting gifts by reach, let σ′2 be the strategy that plays according to σ S 2 in each subgame S, respectively, and σ2 elsewhere. Moreover, let σ−S2 be the strategy that plays according to σ ′ 2\n6While this may seem like a loose lower bound, there are many situations where the off-path action simply leads to a terminal node. For these cases, the lower bound we use is optimal.\n7The definition of Mr(I, aS) uses gifts from all I ′ · a′ v I · a, not just those in Q(I).\neverywhere except for P2 nodes in S, where it instead plays according to σ2. If π〈BR σ′2 ,σ′2〉(I) > 0 for some I ∈ Sr, then exp(σ′2) ≤ exp(σ−S2 )− π σ′2 −1(I) minIbMr(I, aS)c.\nSo far we have given techniques that guarantee a reduction in exploitability by setting a′T equal to the best response value of the trunk. Relaxing this guarantee may lead to lower exploitability in practice, particularly when the original trunk strategy is far from equilibrium. We discuss this approach in the next section."
    }, {
      "heading" : "6 Modeling Error in a Subgame",
      "text" : "In this section we consider the case where we have a good estimate of what the counterfactual values of subgames would look like in a Nash equilibrium. We then bound exploitability as a function of this estimate. Unlike previous sections, exploitability might be higher than the trunk; the solution quality ultimately depends on the estimates used. However, in practice this approach leads to significantly lower exploitability.\nWhen solving multiple P2 subgames, there is a minimally-exploitable strategy σ∗2 that could, in theory, be computed by changing only the strategies in the subgames. (σ∗2 may not be a Nash equilibrium because P2’s strategy outside the subgames is fixed, but it is the closest that can be achieved by changing the strategy only in the subgames). However, σ∗2 can only be guaranteed to be produced by solving all the subgames together, because the optimal strategy in one subgame depends on the optimal strategy in other subgames.\nStill, suppose that we know CBV σ ∗ 2 (I) for every infoset I ∈ Sr for every subgame. By setting the P1 alternative payoff for every infoset I in the head of a subgame to v(I, a′T ) = CBV σ∗2 (I), safe subgame solving guarantees a strategy will be produced with exploitability no worse than σ∗2 . So achieving a strategy equivalent to σ∗2 does not require knowledge of σ ∗ 2 ; rather, it only requires knowledge of CBV σ ∗ 2 (I) for infosets I in the heads of the subgames.\nWhile we may not know CBV σ ∗ 2 (I) exactly without knowing σ∗2 itself, we may nevertheless be able to produce (or learn) good estimates of CBV σ ∗ 2 (I). For example, in Section 8 we compute the solution to the game of no-limit Flop hold’em (NLFH), and find that in perfect play P2 can expect to win about 37 mbb/h8 (that is, if P1 plays perfectly against the computed P2 strategy, then P1 earns −37; if P2 plays perfectly against the computed P1 strategy, then P2 earns 37). An abstraction of the game which is only 0.02% of the size of the full game produces a P1 strategy that can be beaten by 112 mbb/h, and a P2 strategy that can be beaten by 21 mbb/h. Still, the abstract strategy estimates that at equilibrium, P2 can expect to win 35 mbb/h. So even though the abstraction produces a very poor estimate of the strategy σ∗, it produces a good estimate of the value of σ∗. In our experiments, we estimate CBV σ ∗ 2 (I) by using the P1 counterfactual value from the trunk strategy CBV σ2(I).\nTheorem 3 proves that if we use estimates of CBV σ ∗ 2 (I) as the alternative payoffs in Re-solve subgame solving, then we can bound exploitability by the distance of the estimates from the true values. This is in contrast to the previous algorithms which guaranteed exploitability no worse than the trunk.\nTheorem 3. Let S be a set of subgames being solved. Let σ∗2 be a minimally-exploitable P2 strategy that differs from the trunk strategy only in S. Let d = maxS∈S,I∈Sr |CBV σ ∗ 2 (I) − v(I, a′T )|. Applying Re-solving to each subgame produces a P2 strategy with exploitability no higher than exp(σ∗2) + d.\nSo if one can accurately estimate what the P1 counterfactual values would be against an optimal P2 strategy in the subgames, then that may be a better option than using the counterfactual best response values from the trunk.9 In our experiments, this approach tends to be do better than the theoretically safe options described in Section 4.10\n8In poker, the performance of one strategy against another depends on how much money is being wagered. For this reason, expected value and exploitability are measured in milli big blinds per hand (mbb/h). A big blind is the amount of money one of the players is required to put into the pot at the beginning of each hand.\n9It is also possible to combine the safety of past approaches with some of the better performance of using estimates by adding the original Re-solve conditions as additional constraints.\n10Subsequent to our study, the AI DeepStack used a technique similar to this form of subgame solving [20]."
    }, {
      "heading" : "6.1 Distributional Alternative Payoffs",
      "text" : "One problem with existing safe subgame solving techniques is that they may “overfit” to the alternative payoffs, even when we use estimates. Consider for instance a subgame that P1 could enter from two different information sets I1 and I2. Assume P1’s counterfactual value for I1 is estimated to be 1, and for I2 is 10. Now suppose during subgame solving, P2 has a choice between two different strategies. The first sets P1’s value for entering the subgame from I1 to 0.99 and from I2 to 9.99. The second lowers P1’s value for entering the subgame from I1 to 1.01 and from I2 to 0. The safe subgame solving methods described so far would choose the first strategy, because the second strategy leaves one of the margins negative. However, intuitively, the second strategy is likely the better option, because it is more robust to errors in the model. For example, perhaps we are not confident that 10 is the exact counterfactual value, but instead believe its true value is normally distributed with 10 as the mean and a standard deviation of 1. In this case, we would prefer the strategy that lowers the value for I2 to 0.\nTo address this problem, we introduce a way to incorporate the modeling uncertainty into the game itself. Specifically, we introduce a new augmented subgame that makes subgame solving more robust to errors in the model. We change the augmented subgame used in Re-solving (shown in Figure 4) so that the alternative payoffs are random variables, and P1 is informed at the start of the augmented subgame of the values drawn from the random variables (but P2 is not). The augmented subgame is otherwise identical. A visualization of this change is shown in Figure 6. As the distributions of the random variables narrow, the augmented subgame converges to the re-solve augmented subgame (but still maximizes the minimum margin when all margins are positive). As the distributions widen, P2 seeks to maximize the sum over all margins, regardless of which are positive or negative.\nThis modification makes the augmented subgame infinite in size because the random variables may be real-valued and P1 could have a unique strategy for each outcome of the random variable. Fortunately, the special structure of the game allows us to arrive at a P2 Nash equilibrium strategy for this infinite-sized augmented subgame by solving a much simpler gadget game.\nThe gadget game is identical to the augmented subgame used in Re-solve subgame solving (shown in Figure 4), except at each initial P1 information set in Sr, P1 chooses action a′S (that is, chooses to enter the subgame rather than take the alternative payoff) with probability P ( XI ≤ v(I, a′S) ) , where v(I, a′S) is the expected value of action a ′ S . (When solving via CFR, it is the expected value on each iteration, as described in CFR-BR [15]). This leads to Theorem 4, which proves that solving this simplified gadget game produces a P2 strategy that is a Nash equilibrium in the infinite-sized augmented subgame illustrated in Figure 6. Theorem 4. Let S′ be a Re-solve subgame and S′r its root. Let S be a Distributional subgame similar to S′, except at each infoset I ∈ Sr, P1 observes the outcome of a random variable XI and the alternative payoff is equal to that outcome. If CFR is used to solve S′ except that the action leading to S′ is taken from each I ∈ S′r with probability P ( XI ≤ vt(I, a′S) ) , where vt(I, a′S) is the counterfactual value on iteration t of action a′S , then the resulting P2 strategy σ S′ 2 in S ′ is a P2 Nash equilibrium strategy in S.\nAnother option which also solves the game but has better empirical performance relies on the softmax (also known as Hedge) algorithm [18]. This gadget game is more complicated, and is described in detail in Appendix B. We use the softmax gadget game in our experiments.\nThe correct distribution to use for the random variables ultimately depends on the actual unknown errors in the model. In our experiments for this technique, we set XI ∼ N ( µI , s 2 I ) , where µI is the trunk counterfactual value (plus any gifts). sI is set as the difference between the trunk counterfactual value of I , and the true (that is, unabstracted) counterfactual best response value of I . Our experiments show that this heuristic works well, and future research could yield even better options."
    }, {
      "heading" : "7 Nested Subgame Solving",
      "text" : "As we have discussed, large games must be abstracted to reduce the game to a tractable size. This is particularly common in games with large or continuous action spaces, such as an auction where a bidder can choose any price in some range. Typically the action space is discretized by action abstraction so only a few actions are included in the abstraction. While we might limit ourselves to the actions we included in the abstraction, an opponent might choose actions that are not in the abstraction. In that case, the off-tree action can be mapped to an action that is in the abstraction, and the strategy from that in-abstraction action can be used. For example, in an auction game we might include a bid of $100 in our abstraction. If a player bids $101, we simply treat that as a bid of $100. This is referred to as action translation [12, 28, 6]. Action translation is the state-of-the-art prior approach to dealing with this issue. It has been used, for example, by all the leading competitors in the Annual Computer Poker Competition (ACPC). The leading action translation mapping used by most of the top teams in the ACPC is the pseudoharmonic mapping [6]. That is the action mapping that we will benchmark against in our experiments.\nIn this section, we develop techniques for applying subgame solving to calculate responses to opponent’s off-tree actions, thereby obviating the need for action translation. In other words, rather than simply treat a bid of $101 as $100, we calculate in real time a unique response to the bid of $101. The approach can also be used in a nested fashion in response to subsequent opponent off-tree actions. We present two methods that dramatically outperform the leading action translation technique. The same techniques can also be used more generally to calculate finer-grained card or action abstractions as play progresses down the game tree. In this section, for exposition, we assume that P2 wishes to respond to P1 choosing an off-tree action.\nWe refer to the first method as the inexpensive method. When P1 chooses an off-tree action a in infoset I , a subgame S is generated such that I ∈ Sr and I · a leads to S. This subgame may itself be an abstraction. S is solved using any of the safe subgame solving techniques discussed earlier, except that we use CBV σ−1(I) in place of CBV σ−1(I, a) for the alternative payoff (since a is not a valid action in I according to σ). The solution σS is combined with σ to form σ′. Counterfactual values are then updated for every infoset I ′ ∈ S and each I ′ ∈ QS(I) (that is, on the path to I). The process repeats whenever P1 again chooses an off-tree action.\nBy using CBV σ−1(I) in place of CBV σ ′ −1(I ′, a), we can retain some of the theoretical guarantees of Reach-Maxmargin and Maxmargin. For example, if in every information set I P1 is better off taking an existing action than the new action that was added, then the new strategy has exploitability no higher than the original strategy. More generally, Proposition 1 proves that we can bound the increase in exploitability of the expanded strategy by the sum of the positive margins. The proposition follows trivially from the definition of M(I, aS).\nProposition 1. Let aS be an action in information sets in Sr that leads to a subgame S. Let CBR1(σ−1)\n6→Sr·aS be a P1 strategy that maximizes counterfactual value in every information set, except that it never chooses action aS in Sr, and define its value as CBV σ−1 6→Sr·aS . If S is solved via nested subgame solving, then exploitability is bounded as CBV σ−1 ≤ CBV σ−1 6→Sr·aS +∑ I∈Sr max ( 0,M(I, aS) ) .\nThe “inexpensive” approach cannot be combined with Unsafe subgame solving because the probability of reaching an action outside of a player’s abstraction is undefined. That is, πσ(h · a) is undefined when a is not considered a valid action in h according to the abstraction. Nevertheless, a similar but more expensive approach is possible with Unsafe subgame solving (as well as all the other subgame-solving techniques) by starting the subgame solving at h rather than at h · a. In other words,\nif action a taken in history h is not in the abstraction, then Unsafe subgame solving is conducted in the smallest subgame containing h (and action a is added to that abstraction). This increases the size of the subgame compared to the inexpensive method because a strategy must be recomputed for every action a′ ∈ A(h) in addition to a. For example, if an off-tree action is chosen by the opponent as the first action in the game, then the strategy for the entire game must be recomputed. We therefore call this method the expensive method. We present experiments with both methods."
    }, {
      "heading" : "8 Experiments",
      "text" : "Our experiments were conducted on two poker games we call no-limit flop hold’em (NLFH) and no-limit turn hold’em (NLTH). NLFH is similar to the popular poker game of heads-up no-limit Texas hold’em except that there are only two rounds, called the pre-flop and flop. Poker was chosen because we can leverage certain domain-specific optimizations to speed up computation by multiple orders of magnitude, allowing us to solve and calculate exploitability for large-scale games [16]. At the beginning of both NLFH and NLTH, each player receives two private cards from a 52-card deck. Player 1 puts in the “big blind” of 100 chips, and Player 2 puts in the “small blind” of 50 chips. A round of betting then proceeds starting with Player 2, referred to as the preflop, in which a specific number of bets or raises are allowed (where the number varies depending on the version of NLFH or NLTH). Either player may fold on their turn, in which case the game immediately ends and the other player wins the pot. After the first betting round is completed, three community cards are dealt out, and another round of betting is conducted (starting with Player 1), referred to as the flop. That is the final round in NLFH, but NLTH has an additional round in which another single community card is dealt and another round of betting occurs, referred to as the turn. At the end of the final round of betting, both players form the best possible five-card poker hand using their two private cards and the community cards. The player with the better hand wins the pot. For equilibrium finding, we used a version of CFR called CFR+ [30]. There is no randomness in our experiments.\nOur first experiment compares the performance of the subgame solving techniques when applied to information abstraction (which is card abstraction in the case of poker). Specifically, we solve NLFH with no information abstraction on the preflop. On the flop, there are 1,286,792 infosets for each betting sequence; the abstraction buckets them into 200, 2,000, or 30,000 abstract ones (using a leading information abstraction algorithm [7]). We then apply subgame solving immediately after the flop community cards are dealt. We experiment with two versions of the game, one small and one large, which include only a few of the available actions in each infoset. The small game requires 1.1 GB to store the unabstracted strategy as double-precision floats. The large game requires 4 GB. We also experimented on abstractions of NLTH. In that case, we solve NLTH with no information abstraction on the preflop or flop. On the turn, there are 55,190,538 infosets for each betting sequence; the abstraction buckets them into 200, 2,000, or 20,000 abstract ones. We apply subgame solving immediately after the turn community card is dealt. NLTH requires 35 GB to store the unabstracted strategy. Tables 1, 2, and 3 show the performance of each technique. In all our experiments, exploitability is measured in the standard units used in this field: milli big blinds per hand (mbb/h).\nWe use a normal distribution in the Distributional subgame solving experiments, with standard deviation determined by the heuristic presented in Section 6.1. Since subgame solving begins immediately after a chance node with an extremely high branching factor (1, 755 in NLFH), the gifts\nfor the Reach algorithms are divided inefficiently. Many subgames do not use the gifts at all, while others would make use of more. The result is that the theoretically safe version of Reach splits gifts very conservatively. In the experiments we show results both for this theoretically safe splitting of gifts, as well as a more aggressive version where gifts are not split at all, but instead are given in full to each subgame. This weakens the theoretical guarantees of the algorithm, but performs better empirically and is still potentially safe if only a few of the subgames make full use of the gifts.\nDespite lacking theoretical guarantees, Unsafe subgame solving does surprisingly well in most games. However, it did substantially worse in Large NLFH with 30,000 buckets. This exemplifies its variability. Among the safe methods, all of the changes we introduce show improvement over past techniques. The Reach-Estimated + Distributional algorithm generally resulted in the lowest exploitability among the various choices, and in most cases beat unsafe subgame solving.\nIn general, not splitting the gifts did better than splitting gifts in a theoretically correct manner. However, this is not universally true. Appendix D shows that in at least one case, exploitability increased when gifts were scaled up too aggressively. In all cases, using Reach subgame solving in the theoretical safe method led to lower exploitability.\nIn all but one case, using estimated counterfactual values lowered exploitability more than Maxmargin and Re-solving. Also, in all but one case using distributional alternative payoffs lowered exploitability.\nThe second experiment evaluates nested subgame solving, and compares it to action translation. In order to also evaluate action translation, in this experiment, we create an NLFH game that includes 3 bet sizes at every point in the game tree (0.5, 0.75, and 1.0 times the size of the pot); a player can also decide not to bet. Only one bet (i.e., no raises) is allowed on the preflop, and three bets are allowed on the flop. There is no information abstraction anywhere in the game. We also created a second, smaller abstraction of the game in which there is still no information abstraction, but the 0.75× pot bet is never available. We calculate the exploitability of one player using the smaller abstraction, while the other player uses the larger abstraction. Whenever the large-abstraction player chooses a 0.75× pot bet, the small-abstraction player generates and solves a subgame for the remainder of the game (which again does not include any 0.75× pot bets) using the nested subgame solving techniques\ndescribed above. This subgame strategy is then used as long as the large-abstraction player plays within the small abstraction, but if she chooses the 0.75× pot bet later again, then the subgame solving is used again, and so on.\nTable 4 shows that all the subgame solving techniques substantially outperform action translation. Resolve, Maxmargin, and Reach-Maxmargin use inexpensive nested subgame solving, while Unsafe and “Reach-Maxmargin (expensive)” use the expensive approach. We did not test distributional alternative payoffs in this experiment, since the calculated best response values are likely quite accurate. Reach-Maxmargin performed the best, outperforming Maxmargin and unsafe subgame solving. These results suggest that nested subgame solving is preferable to action translation (if there is sufficient time to solve the subgame)."
    }, {
      "heading" : "8.1 Evaluation Against Top Humans",
      "text" : "The techniques in this paper are a key component of the AI Libratus, which defeated four top human specialists in heads-up no-limit Texas hold’em in the January 2017 Brains vs. AI competition. Headsup no-limit Texas hold’em has been the primary benchmark challenge for AI in imperfect-information games. The competition was played over the course of 20 days, and involved 120,000 hands of poker. A prize pool of $200,000 was split among the four human professionals based on their performance against the AI to incentivize strong play. The AI decisively defeated the team of human players by a margin of 147 mbb / hand, with a p-value of 0.0002 and 99.98% statistical significance (see Figure 7). This was the first time an AI defeated top humans in heads-up no-limit Texas hold’em poker."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We introduced a subgame solving technique for imperfect-information games that has stronger theoretical guarantees and better practical performance than prior subgame-solving methods. We presented results on exploitability of both safe and unsafe subgame solving techniques. We also introduced a method for nested subgame solving in response to the opponent’s off-tree actions, and demonstrated that this leads to dramatically better performance than the usual approach of action translation. This is, to our knowledge, the first time that exploitability of subgame solving techniques has been measured in large games.\nFinally, we demonstrated the effectiveness of these techniques in practice against top human professionals in the game of heads-up no-limit Texas hold’em poker, the main benchmark challenge for AI in imperfect-information games. In the 2017 Brains vs. AI competition, our AI Libratus became the first AI to reach the milestone of defeating top humans in heads-up no-limit Texas hold’em."
    }, {
      "heading" : "Appendix: Supplementary Material",
      "text" : ""
    }, {
      "heading" : "A Description of Gadget Game",
      "text" : "Solving the augmented subgame described in Maxmargin solving and Reach-Maxmargin solving will not, by itself, necessarily maximize the minimum margin. While LP solvers can easily handle this objective, the process is more difficult for iterative algorithms such as Counterfactual Regret Minimization (CFR) and the Excessive Gap Technique (EGT). For these iterative algorithms, the augmented subgame can be modified into a gadget game that, when solved, will provide a Nash equilibrium to the augmented subgame and will also maximize the minimum margin [19]. This gadget game is unnecessary when using distributional alternative payoffs, which is introduced in section 6.1.\nThe gadget game differs from the augmented subgame in two ways. First, all P1 payoffs that are reached from the initial information set of I ∈ Sr are shifted by the alternative payoff of I . Second, rather than the game starting with a chance node that determines P1’s starting state, P1 decides for herself which state to begin the game in. Specifically, the game begins with a P1 node where each action in the node corresponds to an information set I in Sr. After P1 chooses to enter an information set I , chance chooses the precise history h ∈ I in proportion to πσ−1(h). By shifting all payoffs in the game by the size of the alternative payoff, the gadget game forces P1 to focus on improving the performance of each information set over some baseline, which is the goal of Maxmargin and Reach-Maxmargin solving. Moreover, by allowing P1 to choose the state in which to enter the game, the gadget game forces P2 to focus on maximizing the minimum margin.\nFigure 8 illustrates the gadget game used in Maxmargin and Reach-Maxmargin."
    }, {
      "heading" : "B Hedge for Distributional Subgame Solving",
      "text" : "In this paper we use CFR [32] with Hedge in Sr, which allows us to leverage a useful property of the Hedge algorithm [18] to update all the information sets resulting from outcomes of XI simultaneously.11 When using Hedge, action a′S is chosen on iteration t with probability eηtv̂(I,a ′ S)\neηtv̂(I,a ′ S )+eηtv̂(I,a ′ T\n) .\n11Another option is to apply CFR-BR [15] only at the initial P1 nodes when deciding between a′T and a ′ S .\nWhere v̂(I, a′T ) is the observed expected value of action a ′ T , v̂(I, a ′ S) is the observed expected value of action a′S , and ηt is a tuning parameter. Since, action a ′ S leads to identical play by both players for all outcomes of X , v̂(I, a′S) is identical for all outcomes of X . Moreover, v̂(I, a ′ T ) is simply the outcome of XI . So the probability that a′S is taken across all information sets on iteration t is∫ ∞ −∞ eηtv̂(a ′ S) eηtv̂(a ′ S) + eηtx fXI (x)dx (3)\nwhere fXI (x) is the pdf of XI . In other words, if CFR is used to solve the augmented subgame, then the game being solved is identical to Figure 4 except that action a′S is always chosen in information set I on iteration t with probability given by (3). In our experiments, we set the Hedge tuning parameter η as suggested in [3]: ηt = √\nln(|A(I)|) 3 √ V AR(I)t √ t , where V AR(I)t is the observed variance in\nthe payoffs the information set has received across all iterations up to t. In the subgame that follows Sr, we use CFR+ as the solving algorithm."
    }, {
      "heading" : "C Proof of Theorem 1",
      "text" : "Proof. Assume Mssr(I, aS) ≥ 0 for every information set I in Sr for a subgame S and let = minIMssr(I, aS).\nFor an information set I ∈ Sr, let I ′ be the earliest information set in QS(I). Then πσ−1(I ′)CBV σ−1(I ′) ≥ πσ−1(I ′)CBV σ ′ −i→I·a ′ S (I ′) + .\nFirst suppose that π〈BR(σ ′ 2),σ ′ 2〉(I) = 0. Then either π〈BR(σ ′ 2),σ ′ 2〉(I ′) = 0 or π〈BR(σ ′ 2),σ ′ 2〉(I ′, I) = 0. If it is the former case, then CBV σ−1(I ′) does not affect exp(σ′2). If it is the latter case, then since I is the only information set in Sr reachable from I ′, so in any best response I ′ only reaches nodes outside of S with positive probability. The nodes outside S belonging to P2 were unchanged between σ and σ′, so CBV σ ′ −1(I ′) ≤ CBV σ−1(I ′).\nNow suppose that π〈BR(σ ′ 2),σ ′ 2〉(I) > 0. Since BR(σ′2) already reaches I ′ on its own, so CBV σ ′ −i(I ′) = CBV σ ′ −i→I·a ′ S (I ′). Since πσ−1(I ′)CBV σ−1(I ′) ≥ πσ−1(I ′)CBV σ ′ −i→I·a ′ S (I ′)+ , so we get πσ−1(I ′)CBV σ−1(I ′) ≥ πσ−1(I ′)CBV σ ′ −i(I ′) + . Following Theorem 1 in Moravcik et al. [19], we get that exp(σ′2) ≤ exp(σ2)− . Now consider any information set I ′′ @ I ′. Before encountering any P2 nodes whose strategies are different in σ′ (that is, P2 nodes in S), P1 must first traverse a I ′ information set as previously defined. But for every I ′ information set, CBV σ ′ −1(I ′) ≤ CBV σ−1(I ′). Therefore, CBV σ ′ −1(I ′′) ≤ CBV σ−1(I ′′)."
    }, {
      "heading" : "D Scaling of Gifts",
      "text" : "To retain the theoretical guarantees of Reach subgame solving, it is necessary to divide gifts among reachable subgames. However, empirical performance may increase if these gifts are scaled up by some factor. In most games we experimented on, exploitability decreased the further the gifts were scaled. However, Figure 9 shows one case in which we observe the exploitability increasing when the gifts are scaled up too far. The graph shows exploitability when the gifts are scaled by various factors. At 0, the algorithm is identical to Maxmargin. at 1, the algorithm is the theoretically correct form of Reach-Maxmargin. Optimal performance in this game occurs when the gifts are scaled by a factor of about 1,000. Scaling the gifts by 100,000 leads to performance that is worse than Maxmargin subgame solving. This empirically demonstrates that while scaling up gifts may lead to better performance in some cases (because an entire gift is unlikely to be used in every subgame that receives one), it may also lead to far worse performance in some cases."
    }, {
      "heading" : "E Proof of Theorem 2",
      "text" : "Proof. Similar to Theorem 1, assume Mr(I, aS) ≥ 0 for every information set I and let = minI∗∈Sr Mr(I ∗, aS).\nWe show that for every P1 infoset I v I∗, CBV σ ′ 2(I) ≤ CBV σ2(I) + ∑ I′′·a′′vI ( bCBV σ′2(I ′′)− CBV σ ′ 2(I ′′, a′′)c ) − πσ2−1(I, I∗) . Clearly this holds for I∗ itself. Moreover, the condition holds for every other I ∈ Sr, because by assumption every margin is nonnegative and πσ2−1(I, I∗) = 0 for any other I in the root of a subgame. The condition also clearly holds for any I with no descendants in S because then πσ2−1(I, I ∗) = 0 and CBV σ2(I) = CBV σ S 2 (I) since nothing has changed.\nNext, consider an information set I ′ v I∗ such that every descendant satisfies the inductive step. Let Succ(I ′, a′) be the set of immediate successor information sets from I ′. Then CBV σ ′ 2(I ′, a′) =\nCBV σ2(I ′, a′) + ∑ I∈Succ(I′,a′) π σ′2 −1(I ′, I)(CBV σ ′ 2(I) − CBV σ2(I)). Since every successor of I ′ satisfies the inductive step, and exactly one of them leads to I∗, so\nCBV σ ′ 2(I ′, a′) ≤ CBV σ2(I ′, a′)− πσ2−1(I ′, I∗) +∑\nI∈Succ(I′,a′)\nπ σ′2 −1(I ′, I) ( ∑ I′′·a′′vI ( bCBV σ ′ 2(I ′′)− CBV σ ′ 2(I ′′, a′′)c ))\nCBV σ ′ 2(I ′, a′) ≤ CBV σ2(I ′, a′)− πσ2−1(I ′, I∗) + ∑ I′′·a′′vI ( bCBV σ ′ 2(I ′′)− CBV σ ′ 2(I ′′, a′′)c ) Since bCBV σ′2(I ′) − CBV σ′2(I ′, a′)c ≤ dCBV σ′2(I ′) − CBV σ′2(I ′, a′)e ≤ CBV σ2(I) − CBV σ2(I, a) so we get\nCBV σ ′ 2(I ′, a′) ≤ CBV σ2(I ′)− πσ2−1(I ′, I∗) + ∑ I′′·a′′vI′ ( bCBV σ ′ 2(I ′′)− CBV σ ′ 2(I ′′, a′′)c ) Since CBV σ ′ 2(I ′) = maxa′∈A(I′) CBV σ′2(I ′, a′), we have\nCBV σ ′ 2(I ′) ≤ CBV σ2(I ′)− πσ2−1(I ′, I∗) + ∑ I′′·a′′vI′ ( bCBV σ ′ 2(I ′′)− CBV σ ′ 2(I ′′, a′′)c ) which satisfies the inductive step. Thus, CBV σ\n′ 2 ≤ CBV σ2 − πσ2−1(I∗) ."
    }, {
      "heading" : "F Proof of Theorem 3",
      "text" : "Proof. Let σ′ be the strategy produced after Re-solving is applied to each subgame. Consider a subgame S ∈ S. We prove that ∑ I∈Sr π σ′2 −1(I) max{0, CBV σ\n′ 2(I) − CBV σ∗2 (I)|} ≤∑\nI∈Sr π σ′2 −1(I) ( |v(I, a′T )− CBV σ ∗ 2 (I)| ) .\nLet IU be the set of I ∈ Sr such that CBV σ ′ 2(I) ≥ CBV σ∗2 (I) and v(I, a′T ) ≤ CBV σ ′ 2(I). Let IL be the set of I ∈ Sr such that CBV σ ′ 2(I) < CBV σ ∗ 2 (I) and v(I, a′T ) ≤ CBV σ ∗ 2 (I). Then we must\nprove that ∑ I∈IU π σ′2 −1(I) ( CBV σ ′ 2(I)−CBV σ∗2 (I) ) ≤ ∑ I∈IU π σ′2 −1(I) ( |v(I, a′T )−CBV σ ∗ 2 (I)| ) +∑\nI∈IL π σ′2 −1(I) ( |v(I, a′T )− CBV σ ∗ 2 (I)| ) .\nWe will further split IU into two cases: IUA ⊆ IU such that v(I, a′T ) ≥ CBV σ ∗ 2 (I) and IUB ⊆ IU such that v(I, a′T ) < CBV σ∗2 (I). Re-solve converges to a P2 strategy such that σ′2 = arg minσ2 (∑ I∈Sr π σ′2 −1(I) ( max{0, CBV σ2(I) − v(I, a′T )} )) . Thus, if σ′2 is such a minimum,\nthen ∑ I∈Sr π σ′2 −1(I) ( max{0, CBV σ′2(I) − v(I, a′T ) ) ≤ ∑ I∈Sr π σ′2 −1(I) ( max{0, CBV σ∗2 (I) −\nv(I, a′T ) ) . So ∑ I∈IU π σ′2 −1(I) ( CBV σ ′ 2(I) − max{v(I, a′T ), CBV σ ∗ 2 (I)} ) ≤ π σ′2 −1(I) (∑ I∈IL CBV σ∗2 (I)−max{v(I, a′T ), CBV σ ′ 2(I)} ) .\nSince for all I ∈ IU we have CBV σ′2(I) − max{v(I, a′T ), CBV σ ∗ 2 (I)} ≤ CBV σ′2(I) − CBV σ ∗ 2 (I) + |v(I, a′T ) − CBV σ ∗ 2 (I)|, so we get ∑ I∈IU π σ′2 −1(I) ( CBV σ ′ 2(I) − CBV σ ∗ 2 (I) ) ≤ ∑ I∈IU π σ′2 −1(I) ( |v(I, a′T ) − CBV σ ∗ (I)| ) + ∑ I∈IL π σ′2 −1(I) ( CBV σ ∗ 2 (I) − max{v(I, a′T ), CBV σ ′ 2(I)} ) .\nSince for all I ∈ IL we have that CBV σ∗2 (I) ≥ v(I, a′T ) so we get ∑ I∈IU π σ′2 −1(I) ( CBV σ ′ 2(I)− CBV σ ∗ 2 (I) ) ≤ ∑ I∈IU π σ′2 −1(I) ( |v(I, a′T ) − CBV σ ∗ (I)| ) + ∑ I∈IL π σ′2 −1(I) ( |v(I, a′T ) − CBV σ ∗ 2 (I)| ) .\nThen we need only sum over all subgames to arrive at exp(σ′2) ≤ exp(σ∗2) +∑ S∈S ∑ I∈Sr π σ′2 −1(I)|CBV σ ∗ 2 (I) − v(I, a′T )|. Let d = maxS∈S,I∈Sr{|CBV σ ∗ 2 (I) − v(I, a′T )|}. Then this gives us exp(σ′2) ≤ exp(σ∗2) + d."
    }, {
      "heading" : "G Proof of Theorem 4",
      "text" : "Proof. We prove inductively that using CFR in S′ while choosing the action leading to S′ from each I ∈ S′r with probability P ( XI ≤ vt(I, a′S) ) results in play that is identical to CFR in S and CFR-BR [15] in Sr, which converges to a Nash equilibrium.\nFor each P2 information set I ′2 in S ′, there is exactly one corresponding information set I2 in S that is reached via the same actions, ignoring random variables. Each P1 information set I ′1 in S ′ corresponds to a set of information sets in S that are reached via the same actions, where the elements in the set differ only by the outcome of the random variables. We prove that on each iteration, the instantaneous regret for these corresponding information sets is identical (and therefore the average strategy played in the P2 information sets over all iterations is identical).\nAt the start of the first iteration of CFR, all regrets are zero. Therefore, the base case is trivially true. Now assume that on iteration t, regrets are identical for all corresponding information sets. Then the strategies played on iteration t in S are identical as well.\nFirst consider an information set I ′1 in S ′ and a corresponding information set I1 in S. Since the remaining structure of the game is identical beyond I ′1 and I1, and because P2’s strategies are identical in all P2 information sets encountered, so the immediate regret for I ′1 and I1 is identical as well.\nNext consider a P1 information set I1,x in Sr in which the random variable XI has an observed value of x. Let the corresponding P1 information set in S′r be I ′ 1. Since CFR-BR is played in\nthis information set, and since action a′T leads to a payoff of x, so P1 will choose action a ′ S with probability 1 if x ≥ a′T and with probability 0 otherwise. Thus, for all information sets in Sr corresponding to I ′1, action a ′ S is chosen with probability P ( XI ≤ v(I, a′S) ) .\nFinally consider a P2 information set I2 in S and its corresponding information set I ′2 in S ′. Since in both cases action a′T is taken in Sr with probability P ( XI ≤ v(I, a′S) ) , and because P1 plays identically between corresponding information sets in S and S′, and because the structure of the game is otherwise identical, so the immediate regret for I ′1 and I1 is identical as well."
    } ],
    "references" : [ {
      "title" : "On the application of dynamic programming to the determination of optimal play in chess and checkers",
      "author" : [ "Richard Bellman" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1965
    }, {
      "title" : "Approximating game-theoretic optimal strategies for fullscale poker",
      "author" : [ "Darse Billings", "Neil Burch", "Aaron Davidson", "Robert Holte", "Jonathan Schaeffer", "Terence Schauenberg", "Duane Szafron" ],
      "venue" : "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Dynamic thresholding and pruning for regret minimization",
      "author" : [ "Noam Brown", "Christian Kroer", "Tuomas Sandholm" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "Solving imperfect information games using decomposition",
      "author" : [ "Neil Burch", "Michael Johanson", "Michael Bowling" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Action translation in extensive-form games with large action spaces: Axioms, paradoxes, and the pseudo-harmonic mapping",
      "author" : [ "Sam Ganzfried", "Tuomas Sandholm" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Potential-aware imperfect-recall abstraction with earth mover’s distance in imperfect-information games",
      "author" : [ "Sam Ganzfried", "Tuomas Sandholm" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Endgame solving in large imperfect-information games",
      "author" : [ "Sam Ganzfried", "Tuomas Sandholm" ],
      "venue" : "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "First-order algorithm with O(ln(1/ )) convergence for -equilibrium in two-person zero-sum games",
      "author" : [ "Andrew Gilpin", "Javier Peña", "Tuomas Sandholm" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "A competitive Texas Hold’em poker player via automated abstraction and real-time equilibrium computation",
      "author" : [ "Andrew Gilpin", "Tuomas Sandholm" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Better automated abstraction techniques for imperfect information games, with application to Texas Hold’em poker",
      "author" : [ "Andrew Gilpin", "Tuomas Sandholm" ],
      "venue" : "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "A heads-up no-limit Texas Hold’em poker player: Discretized betting models and automatically generated equilibriumfinding programs",
      "author" : [ "Andrew Gilpin", "Tuomas Sandholm", "Troels Bjerre Sørensen" ],
      "venue" : "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "A time and space efficient algorithm for approximately solving large imperfect information games",
      "author" : [ "Eric Jackson" ],
      "venue" : "In AAAI Workshop on Computer Poker and Imperfect Information,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Measuring the size of large no-limit poker games",
      "author" : [ "Michael Johanson" ],
      "venue" : "Technical report, University of Alberta,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Finding optimal abstract strategies in extensive-form games",
      "author" : [ "Michael Johanson", "Nolan Bard", "Neil Burch", "Michael Bowling" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Accelerating best response calculation in large extensive games",
      "author" : [ "Michael Johanson", "Kevin Waugh", "Michael Bowling", "Martin Zinkevich" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Theoretical and practical advances on smoothing for extensive-form games",
      "author" : [ "Christian Kroer", "Kevin Waugh", "Fatma Kilinc-Karzan", "Tuomas Sandholm" ],
      "venue" : "arXiv preprint arXiv:1702.04849,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2017
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "Nick Littlestone", "M.K. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1994
    }, {
      "title" : "Refining subgames in large imperfect information games",
      "author" : [ "Matej Moravcik", "Martin Schmid", "Karel Ha", "Milan Hladik", "Stephen Gaukrodger" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Deepstack: Expert-level artificial intelligence in no-limit",
      "author" : [ "M. Moravčík", "M. Schmid", "N. Burch", "V. Lisý", "D. Morrill", "N. Bard", "T. Davis", "K. Waugh", "M. Johanson", "M. Bowling" ],
      "venue" : "poker. Science,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2017
    }, {
      "title" : "Equilibrium points in n-person games",
      "author" : [ "John Nash" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1950
    }, {
      "title" : "Non-cooperative games",
      "author" : [ "John Nash" ],
      "venue" : "PhD thesis, Priceton University,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1950
    }, {
      "title" : "Excessive gap technique in nonsmooth convex minimization",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "SIAM Journal of Optimization,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "The state of solving large incomplete-information games, and application to poker",
      "author" : [ "Tuomas Sandholm" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Abstraction for solving large incomplete-information games",
      "author" : [ "Tuomas Sandholm" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Probabilistic state translation in extensive games with large action sets",
      "author" : [ "David Schnizlein", "Michael Bowling", "Duane Szafron" ],
      "venue" : "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2009
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Solving heads-up limit Texas hold’em",
      "author" : [ "Oskari Tammelin", "Neil Burch", "Michael Johanson", "Michael Bowling" ],
      "venue" : "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Strategy grafting in extensive games",
      "author" : [ "Kevin Waugh", "Nolan Bard", "Michael Bowling" ],
      "venue" : "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "In such games, the typical goal is to find a Nash equilibrium [22], which is a profile of strategies—one for each player—such that no player can improve by unilaterally deviating to a different strategy.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "Subgame solving is a standard technique in perfect-information games such as chess and checkers [1] in which a piece of the game is solved in isolation.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "This decomposition was key to AIs being able to defeat top humans in chess [5] and Go [29].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "benchmark problem in imperfect-information game solving—which has 10 decision points, or can even be infinite in size if fractional bets are allowed [14].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 22,
      "context" : "2 The standard approach to computing strategies in such large games is to first generate an abstraction of the game, which is a smaller version of the game that retains as much as possible the strategic characteristics of the original game [24, 26, 25].",
      "startOffset" : 240,
      "endOffset" : 252
    }, {
      "referenceID" : 23,
      "context" : "2 The standard approach to computing strategies in such large games is to first generate an abstraction of the game, which is a smaller version of the game that retains as much as possible the strategic characteristics of the original game [24, 26, 25].",
      "startOffset" : 240,
      "endOffset" : 252
    }, {
      "referenceID" : 19,
      "context" : "A Nash equilibrium [21] is a strategy profile σ∗ such that ∀i, ui(σ i , σ∗ −i) = maxσ′ i∈Σi ui(σ ′ i, σ ∗ −i).",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "A counterfactual best response [19] CBRi(σ−i) is similar to a best response, but additionally maximizes counterfactual value at every infoset.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [2, 10, 11, 8].",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [2, 10, 11, 8].",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [2, 10, 11, 8].",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [2, 10, 11, 8].",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "In subgame re-solving [4], a safe strategy is computed for P2 in the subgame by constructing the augmented subgame shown in Figure 4, and computing an equilibrium strategy σ for it.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "Maxmargin solving [19] is similar to Re-solving, except that it seeks to improve P2’s strategy in the subgame strategy as much as possible.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 21,
      "context" : "In order to use iterative algorithms such as the Excessive Gap Technique [23, 9, 17] or Counterfactual Regret Minimization (CFR) [32], one can use the gadget game described by Moravcik et al.",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "In order to use iterative algorithms such as the Excessive Gap Technique [23, 9, 17] or Counterfactual Regret Minimization (CFR) [32], one can use the gadget game described by Moravcik et al.",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "In order to use iterative algorithms such as the Excessive Gap Technique [23, 9, 17] or Counterfactual Regret Minimization (CFR) [32], one can use the gadget game described by Moravcik et al.",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "[19].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "This theorem statement is similar to that of Maxmargin [19], but the margins here are higher than (or equal to) those in Maxmargin.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "Other subgames solving methods have also considered the cost of reaching a subgame [31, 13].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "Other subgames solving methods have also considered the cost of reaching a subgame [31, 13].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "When solving only a single subgame, techniques that are similar to Reach subgame solving exist that achieve even better theoretical performance, and indeed are provably optimal [13].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "Subsequent to our study, the AI DeepStack used a technique similar to this form of subgame solving [20].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "(When solving via CFR, it is the expected value on each iteration, as described in CFR-BR [15]).",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : "Another option which also solves the game but has better empirical performance relies on the softmax (also known as Hedge) algorithm [18].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "This is referred to as action translation [12, 28, 6].",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "This is referred to as action translation [12, 28, 6].",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "This is referred to as action translation [12, 28, 6].",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "The leading action translation mapping used by most of the top teams in the ACPC is the pseudoharmonic mapping [6].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "Poker was chosen because we can leverage certain domain-specific optimizations to speed up computation by multiple orders of magnitude, allowing us to solve and calculate exploitability for large-scale games [16].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 26,
      "context" : "For equilibrium finding, we used a version of CFR called CFR+ [30].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "On the flop, there are 1,286,792 infosets for each betting sequence; the abstraction buckets them into 200, 2,000, or 30,000 abstract ones (using a leading information abstraction algorithm [7]).",
      "startOffset" : 190,
      "endOffset" : 193
    } ],
    "year" : 2017,
    "abstractText" : "Unlike perfect-information games, imperfect-information games cannot be solved by decomposing the game into subgames that are solved independently. Instead, all decisions must consider the strategy of the game as a whole, and more computationally intensive algorithms are used. While it is not possible to solve an imperfect-information game exactly through decomposition, it is possible to approximate solutions, or improve existing strategies, by solving disjoint subgames. This process is referred to as subgame solving. We introduce subgame solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the tree, leading to lower exploitability. Subgame solving is a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold’em poker.",
    "creator" : "LaTeX with hyperref package"
  }
}