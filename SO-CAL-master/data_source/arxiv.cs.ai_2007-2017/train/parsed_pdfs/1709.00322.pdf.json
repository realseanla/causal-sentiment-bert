{
  "name" : "1709.00322.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Disintegration and Bayesian Inversion, Both Abstractly and Concretely†\nKENTA CHO and BART JACOBS\nInstitute for Computing and Information Sciences Radboud University P.O.Box 9010, 6500 GL Nijmegen, the Netherlands\nReceived 4 September 2017\nThe notions of disintegration and Bayesian inversion are fundamental in conditional probability theory. They produce channels, as conditional probabilities, from a joint state, or from an already given channel (in opposite direction). These notions exist in the literature, in concrete situations, but are presented here in abstract graphical formulations. The resulting abstract descriptions are used for proving basic results in conditional probability theory. The existence of disintegration and Bayesian inversion is discussed for discrete probability, and also for measure-theoretic probability — via standard Borel spaces and via likelihoods. Finally, the usefulness of disintegration and Bayesian inversion is illustrated in several non-trivial examples."
    }, {
      "heading" : "1. Introduction",
      "text" : "The essence of conditional probability can be summarised informally in the following equation about probability distributions:\njoint = conditional · marginal.\nA bit more precisely, when we have joint probabilities P(x, y) for elements x, y ranging over two sample spaces, the above equation splits into two equations,\nP(y | x) · P(x) = P(x, y) = P(x | y) · P(y), (1)\nwhere P(x) and P(y) describe the marginals, which are obtained by discarding variables. We see that conditional probabilities P(y | x) and P(x | y) can be constructed in two directions, namely y given x, and x given y. We also see that we need to copy variables: x on the left-hand-side of the equations (1), and y on the right-hand-side.\nConditional probabilities play a crucial role in Bayesian probability theory. They form the nodes of Bayesian networks (Pearl, 1988; Bernardo and Smith, 2000; Barber, 2012),\n† The research leading to these results has received funding from the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013) / ERC grant agreement nr. 320571.\nar X\niv :1\n70 9.\n00 32\n2v 1\n[ cs\n.A I]\n2 9\nA ug\n2 01\n7\nwhich reflect the conditional independences of the underlying joint distribution via their graph structure. As part of our approach, we shall capture conditional independence in an abstract manner. The main notion of this paper is disintegration. It is the process of extracting a conditional probability from a joint probability. Disintegration, as we shall formalise it here, gives a structural description of the above equation (1) in terms of states and channels. In general terms, a state is a probability distribution of some sort (discrete, measure-theoretic, or even quantum) and a channel is a map or morphism in a probabilistic setting, like P(y | x) and P(x | y) as used above. It can take the form of a stochastic matrix, probabilistic transition system, Markov kernel, conditional probability table (in a Bayesian network), or morphism in a Kleisli category of a ‘probability monad’ (Jacobs, 2017). A state is a special kind of channel, with trivial domain. Thus we can work in a monoidal category of channels, where we need discarding and copying — more formally, a comonoid structure on each object — in order to express the above conditional probability equations (1). In this article we abstract away from interpretation details and will describe disintegration pictorially, in the language of string diagrams. This language can be seen as the internal language of symmetric monoidal categories (Selinger, 2010) — with comonoids in our case. The essence of disintegration becomes: extracting a conditional probability channel from a joint state.\nA categorical approach to Bayesian conditioning has appeared for instance in (Culbertson and Sturtz, 2014; Staton et al., 2016; Clerc et al., 2017) and in (Jacobs et al., 2015; Jacobs and Zanasi, 2016; Jacobs, 2017). The latter references use effectus theory (Jacobs, 2015; Cho et al., 2015), a new comprehensive approach aimed at covering the logic of both quantum theory and probability theory, supported by a Python-based tool ‘EfProb’, for ‘effectus probability’. This tool is used for the (computationally extensive) examples in this paper.\nDisintegration, also known as regular conditional probability, is a notoriously difficult operation in measure-theoretic probability, see e.g. (Pollard, 2002; Panangaden, 2009; Chang and Pollard, 1997): it may not exist (Stoyanov, 2014); even if it exists it may be determined only up to negligible sets; and it may not be continuous or computable (Ackerman et al., 2011). Disintegration has been studied using categorical language in (Culbertson and Sturtz, 2014), which focuses on a specific category of probabilistic mappings. Our approach here is more axiomatic. We thus describe disintegration as going from a joint state to a channel. A closely related concept is Bayesian inversion: it turns a channel (with a state) into a channel in opposite direction. We show how Bayesian inversion can be understood and expressed easily in terms of disintegration — and also how, in the other direction, disintegration can be obtained from Bayesian inversion. Bayesian inversion is taken as primitive notion in (Clerc et al., 2017). Here we start from disintegration. The difference is a matter of choice.\nBayesian inversion is crucial for backward inference. We explain it informally: let σ be a state of a domain/type X, and c : X → Y be a channel; Bayesian inversion yields a channel d : Y → X. Informally, it produces for an element y ∈ Y , seen as singleton/point\npredicate {y}, the conditioning of the state σ with the pulled back evidence c−1({y}). A concrete example involving such ‘point observations’ will be described at the end of Section 8. More generally, disintegration and Bayesian inversion are used to structurally organise state updates in presence of new evidence in probabilistic programming, see e.g. (Gordon et al., 2014; Borgström et al., 2013; Staton et al., 2016; Katoen et al., 2015). See also (Shan and Ramsey, 2017), where disintegration is handled via symbolic manipulation.\nDisintegration and Bayesian inversion are relatively easy to define in discrete probability theory. The situation is much more difficult in measure-theoretic probability theory, first of all because point predicates {y} do not make much sense there, see also (Chang and Pollard, 1997). A common solution to the problem of the existence of disintegration / Bayesian inversion is to restrict ourselves to standard Borel spaces, as in (Clerc et al., 2017). We take this approach too. There is still an issue that disintegration is determined only up to negligible sets. We address this by defining ‘almost equality’ in our abstract pictorial formulation. This allows us to present a fundamental result from (Clerc et al., 2017) abstractly in our setting, see Section 5.\nAnother common, more concrete solution is to assume a likelihood, that is, a probabilistic relation X × Y → R≥0. Such a likelihood gives rise to probability density function (pdf), providing a good handle on the situation, see (Pawitan, 2001). The technical core of Section 8 is a generalisation of this likelihood-based approach.\nThe paper is organised as follows. It starts with a brief introduction to the graphical language that we shall be using, and to the underlying monoidal categories with discarding and copying. Then, Section 3 introduces both disintegration and Bayesian inversion in this graphical language, and relates the two notions. Subsequently, Section 4 contains an elaborated example, namely of naive Bayesian classification. A standard example from the literature (Witten et al., 2011) is redescribed in the current setting: first, channels are extracted via disintegration from a table with given data; next, Bayesian inversion is applied to the combined extracted channels, giving the required classification. This is illustrated in both the discrete and the continuous version of the example.\nNext, Section 5 is more technical and elaborates the standard equality notion of ‘equal almost everywhere’ in the current setting. This is used for describing Bayesian inversion in a more formal way, following (Clerc et al., 2017). Section 6 uses our graphical approach to review conditional independence and to prove at an abstract level several known results, namely equivalence of various formulations of conditional independence, and the ‘graphoid’ axioms from (Verma and Pearl, 1988; Geiger et al., 1990). Section 7 relaxes the requirement that maps are causal, so that ‘effects’ can be used as the duals of states for validity and conditioning. The main result relates conditioning of joint states to forward and backward inference via the extracted channels, in the style of (Jacobs and Zanasi, 2016); it is illustrated in a concrete example, where a Bayesian network is seen as a graph in a Kleisli category — following (Fong, 2012). Finally, Section 8 gives the likelihood formulation of disintegration and inversion, as briefly described above."
    }, {
      "heading" : "2. Graphical language",
      "text" : "The basic idea underlying this paper is to describe probability theory in terms of channels. A channel f : X → Y is a (stochastic) process from a system of type X into that of Y . Concretely, it may be a probability matrix or kernel. Our standing assumption is that types (as objects) and channels (as arrows) form a symmetric monoidal category. For the formal definition we refer to (Mac Lane, 1998). We informally summarise that we have the following constructions.\n1 Sequential composition g ◦ f : X → Z for appropriately typed channels f : X → Y and g : Y → Z.\n2 Parallel composition f ⊗ g : X ⊗ Z → Y ⊗W for f : X → Y and g : Z → W . This involves composition of types X ⊗ Z.\n3 Identity channels idX : X → X, which ‘do nothing’. Thus id ◦ f = f = id ◦ f . 4 A unit type I, which represents ‘no system’. Thus I ⊗X ∼= X ∼= X ⊗ I. 5 Swap isomorphisms X ⊗ Y ∼= Y ⊗X and associativity isomorphisms (X ⊗ Y )⊗ Z ∼=\nX ⊗ (Y ⊗ Z), so the ordering in composed types does not matter.\nRepresentation of such channels in the ordinary ‘formula’ notation easily becomes complex and thus reasoning becomes hard to follow. A graphical language known as string diagrams offers a more convenient and intuitive way of reasoning in a symmetric monoidal category.\nIn string diagrams, types/objects are represented as wires ‘ ’, with information flowing bottom to top. The composition of types is depicted by juxtaposition of wires, and the unit type is ‘no diagram’ as below.\nX ⊗ Y = X Y I =\nChannels/arrows are represented by boxes with an input wire(s) and an output wire(s), in upward direction. When a box does not have input or output, we write it as a triangle or diamond. For example, f : X → Y ⊗ Z, ω : I → X, p : X → I, and s : I → I are respectively depicted as:\nf\nX\nY Z\nω\nX p\nX s\nThe identity channels are represented by ‘no box’, i.e. just wires, and the swap isomorphisms are represented by crossing of wires:\nid\nX\nX\n= X X\nX\nY\nY\nFinally, the sequential composition of channels is depicted by connecting the input and\noutput wires, and the parallel composition is given by juxtaposition, respectively as below:\ng ◦ f = f\ng h⊗ k = h k\nThe use of string diagrams is justified by the following ‘coherence’ theorem, see (Selinger, 2010).\nTheorem 2.1. A well-formed equation between composites of arrows in a symmetric monoidal category follows from the axioms of symmetric monoidal categories if and only if the string diagrams of both sides are equal up to isomorphism of diagrams.\nWe further assume the following structure in our category. For each type X there are a discarder X : X → I and a copier X : X → X ⊗X. They are required to satisfy the following equations:\n= = = =\nThis says that ( X , X) forms a commutative comonoid on X. By the associativity we may write:\n. . . := . . .\nMoreover we assume that the comonoid structures ( X , X) are compatible with the monoidal structure (⊗, I), in the sense that the following equations hold.\nX ⊗ Y = X Y I = X ⊗ Y = X Y I =\nNote that we do not assume that these maps are natural. Explicitly, we do not necessarily have ◦ f = (f ⊗ f) ◦ or ◦ f = . We will use these symmetric monoidal categories throughout in the paper. For convenience, we introduce a term for them.\nDefinition 2.2. A CD-category is a symmetric monoidal category (C,⊗, I) with a commutative comonoid ( X , X) for each X ∈ C, suitably compatible as described above.\nHere ‘CD’ stands for Copy/Discard.\nDefinition 2.3. An arrow f : X → Y in a CD-category is said to be causal if\nf = .\nA CD-category is affine if all the arrows are causal, or equivalently, the tensor unit I is a final object.\nThe term ‘causal’ comes from (categorical) quantum foundation (Coecke and Kissinger, 2017; D’Ariano et al., 2017), and is related to relativistic causality, see e.g. (Coecke, 2016).\nWe reserve the term ‘channel’ for causal arrows. Explicitly, causal arrows c : X → Y in a CD-category are called channels. A channel ω : I → X with input type I is called a state (on X). For the time being we only consider affine CD-categories, where all arrows are channels.\nExample 2.4. Our main examples of affine CD-categories are two Kleisli categories K`(D) and K`(G), respectively, for discrete probability, and more general, measure-theoretic probability. 1 What we call a distribution or a state over a set X is a finite subset {x1, x2, . . . , xn} ⊆\nX, called the support where each element xi occurs with a multiplicity ri ∈ [0, 1], such that ∑ i ri = 1. Such a convex combination is often written as r1|x1〉+ · · ·+ rn|xn〉 with ri ∈ [0, 1]. The ket notation |−〉 is meaningless syntactic sugar that is used to distinguish elements x ∈ X from occurrences in such formal sums. Notice that a distribution can also be written as a function ω : X → [0, 1] with finite support supp(ω) = {x ∈ X | ω(x) 6= 0}. We shall write D(X) for the set of distributions over X. This D is a monad on the category Set of sets and functions. A function f : X → D(Y ) is called a Kleisli map; it forms a channel X → Y . Such maps can be composed as matrices, for which we use special notation ◦· .\n(g ◦· f)(x)(z) = ∑ y∈Y f(x)(y) · g(y)(z) for g : Y → D(Z) with x ∈ X, z ∈ Z.\nWe write 1 = {∗} for a singleton set, and 2 = 1 + 1 = {0, 1}. Notice that D(1) ∼= 1 and D(2) ∼= [0, 1]. We can identify a state on X with a channel 1→ X. The monad D is known to be commutative. This implies that finite products of sets X × Y give rise to a symmetric monoidal structure on the Kleisli category K`(D). Specifically, for two maps f : X → D(Y ) and g : Z → D(W ), the tensor product / parallel composition f ⊗ g : X × Z → D(Y ×W ) is given by:\n(f ⊗ g)(x, z)(y, w) = f(x, y) · g(z, w).\nFor each set X there are a copier X : X → D(X×X) and a discarder X : X → D(1) given by X(x) = 1|x, x〉 and X(x) = 1|∗〉, respectively. They come from the cartesian (finite product) structure of the base category Set, through the obvious functor Set→ K`(D). Therefore K`(D) is a CD-category. It is moreover affine, since the monad is affine in the sense that D(1) ∼= 1. 2 Let X = (X,ΣX) be a measurable space, where ΣX is a σ-algebra on X. A probability measure, also called a state, on X is a function ω : ΣX → [0, 1] which is countably additive and satisfies ω(X) = 1. We write G(X) for the collection of all such probability measures on X. This set G(X) is itself a measurable space. Notice that G(X) ∼= D(X) when X is a finite set (as discrete space). In particular, G(2) ∼= D(2) ∼= [0, 1]. This G is a monad on the category Meas of measurable spaces, with measurable functions between them; it is called the Giry monad, after (Giry, 1982; Jacobs, 2017). A Kleisli map, that is, a measurable function f : X → G(Y ) is a channel (or a probability kernel, see Example 7.1). These channels can be composed, via Kleisli composition ◦· ,\nusing integration: (g ◦· f)(x)(C) = ∫ Y g(y)(C) f(x)(dy) where g : Y → G(Z) and x ∈ X,C ∈ ΣZ .\nIt is well-known that the monad G is commutative and affine, see also (Jacobs, 2017). Thus, in a similar manner to the previous example, the Kleisli category K`(G) is an affine CD-category. The parallel composition f ⊗ g : X × Z → G(Y × W ) for f : X → G(Y ) and g : Z → G(W ) is given as:\n(f ⊗ g)(x, z)(B ×D) = f(x)(B) · g(z)(D),\nfor x ∈ X, z ∈ Z, B ∈ ΣY , and D ∈ ΣW . This indeed determines a unique measure (f ⊗ g)(x, z) ∈ G(Y ×W ), which is a product measure of probability measures f(x) and g(z)."
    }, {
      "heading" : "3. Marginalisation, integration and disintegration",
      "text" : "Let C be an affine CD-category. We think of states ω : I → X in C as abstract (probability) distributions on type X. States of the form ω : I → X ⊗ Y , often called (bipartite) joint states, are seen as joint distributions on X and Y . Later on we shall also consider npartite joint states, but for the time being we restrict ourselves to bipartite ones. For a joint distribution P(x, y) in ordinary discrete probability, we can calculate the marginal distribution on X by summing (or marginalising) Y out, as P(x) = ∑ y P(x, y). The\nmarginal distribution on Y is also calculated by P(y) = ∑ x P(x, y). In our abstract setting, given a joint state ω : I → X ⊗ Y , we can obtain marginal states simply by discarding wires, as in:\nω X marginal on X←−−−−−−−−− [ ω X Y marginal on Y7−−−−−−−−−→ ω Y\nIn other words, the marginal states are the state ω composed with the projection maps π1 : X ⊗ Y → X and π2 : X ⊗ Y → Y , as below.\nπ1 := X Y\nπ2 := X Y\nExample 3.1. For a joint state ω ∈ D(X ×X) in K`(D), the first marginal ω1 = π1 ◦· ω is given by ω1(x) = ∑ y∈Y ω(x, y), as expected. For a joint state ω ∈ G(X ×X) in K`(G), the first marginal is given by ω1(A) = ω(A× Y ) for A ∈ ΣX . The second marginals are similar.\nA channel c : X → Y is seen as an abstract conditional distribution P(y|x). In ordinary probability theory, we can calculate a joint distribution P(x, y) from a distribution P(x) and a conditional distribution P(y|x) by the formula P(x, y) = P(y|x) · P(x), which is often called the product rule. Similarly we have P(x, y) = P(x|y) · P(y). In our setting, starting from a state σ : I → X and a channel c : X → Y , or a state τ : I → Y and a channel d : Y → X, we can ‘integrate’ them into a joint state on X ⊗ Y as follows,\nrespectively:\nσ\nc\nX Y\nor\nτ\nd\nX Y\n(2)\nExample 3.2. Let σ ∈ D(X) and c : X → D(Y ) be a state and a channel in K`(D). An easy calculation verifies that ω = (id⊗ c) ◦· ◦· σ, the joint state on X × Y defined as in (2), satisfies ω(x, y) = c(x)(y) · σ(x), as we expect from the product rule.\nFor a state σ ∈ G(X) and a channel c : X → G(Y ) in K`(G), the joint state ω = (id⊗ c) ◦· ◦· σ is given by ω(A×B) = ∫ A c(x)(B)σ(dx) for A ∈ ΣX and B ∈ ΣY . This ‘integration’ construction of a joint probability measure is standard, see e.g. (Pollard, 2002; Panangaden, 2009).\nDisintegration is an inverse operation of the ‘integration’ of a state and a channel into a joint state, as in (2). More specifically, it starts from a joint state ω : I → X ⊗ Y and extracts either a state ω1 : I → X and a channel c1 : X → Y , or a state ω2 : I → Y and a channel c2 : Y → X as below,\n( ω1 X , c1 Y\nX\n) disintegration←−−−−−−−− [\nω\nX Y disintegration7−−−−−−−−→ (\nω2\nY\n, c2\nX\nY\n)\nsuch that the equation on the left or right below holds, respectively.\nω1\nc1\nX Y\n= ω\nX Y\n=\nω2\nc2\nX Y\n(3)\nWe immediately see from the equation that ω1 and ω2 must be marginals of ω:\nω =\nω1\nc1\n=\nω1\n= ω1\nand similarly\nω =\nω2 .\nWe are thus led to the following definition.\nDefinition 3.3. Let ω : I → X⊗Y be a joint state. A channel c1 : X → Y (or c2 : Y → X) is called a disintegration of ω if it satisfies the equation (3) with ωi the marginals of ω.\nExample 3.4. Let ω ∈ D(X × Y ) be a joint state in K`(D). We write ω1 ∈ D(X) for\nthe first marginal, given by ω1(x) = ∑ y ω(x, y). Then a channel c : X → D(Y ) is a disintegration of ω if and only if ω(x, y) = c(x)(y) · ω1(x) for all x ∈ X and y ∈ Y . It turns out that there is always such a channel c. We define a channel c by:\nc(x)(y) := ω(x, y) ω1(x)\nif ω1(x) 6= 0 ,\nand c(x) := τ if ω1(x) = 0, for an arbitrary state τ ∈ D(Y ). (We may assume that Y is nonempty.) This indeed defines a channel c satisfying the required equation. Roughly speaking, disintegration in discrete probability is nothing but the ‘definition’ of conditional probability: P(y|x) = P(x, y)/P(x). There is still some subtlety — disintegrations need not be unique, when there are x ∈ X with ω1(x) = 0. Disintegrations in measure-theoretic probability, in K`(G), are far more difficult. Let ω ∈ G(X×Y ) be a joint state, with ω1 ∈ G(X) the first marginal. A channel c : X → G(Y ) is a disintegration of ω if and only if\nω(A×B) = ∫ A c(x)(B)ω1(dx)\nfor all A ∈ ΣX and B ∈ ΣY . This is the ordinary notion of disintegration (of probability measures), also known as regular conditional probability; see e.g. (Faden, 1985; Pollard, 2002; Panangaden, 2009). We see that there is no obvious way to obtain a channel c here, unlike the discrete case. In fact, a disintegration may not exist (Stoyanov, 2014). There are, however, a number of results that guarantee the existence of a disintegration in certain situations. We will come back to this issue later in the section.\nBayesian inversion is a special form of disintegration, occurring frequently. We start from a state σ : I → X and a channel c : X → Y . We then integrate them into a joint state on X and Y , and disintegrate it in the other direction, as below.\n( σ X , c Y\nX\n) integration7−−−−−−−→\nσ\nc\nX Y\ndisintegration7−−−−−−−−→ (\nσ\nc\nY\n, d\nX\nY\n)\nWe call the disintegration d : Y → X a Bayesian inversion for σ : I → X along c : X → Y . By unfolding the definitions, a channel d : Y → X is a Bayesian inversion if and only if\nσ\nc\n=\nσ\nd\nc . (4)\nExample 3.5. Let σ ∈ D(X) and c : X → D(Y ) be a state and a channel in K`(D). Then a channel d : Y → D(X) is a Bayesian inversion for σ along c if and only if c(x)(y) · σ(x) = d(y)(x) · c∗(σ)(y), where c∗(σ)(y) = ∑ x′ c(x′)(y) · σ(x′). In a similar\nmanner to Example 3.4, we can obtain such a d by:\nd(y)(x) := c(x)(y) · σ(x) c∗(σ)(y) = c(x)(y) · σ(x)∑ x′ c(x′)(y) · σ(x′)\nfor y ∈ Y with c∗(σ)(y) 6= 0. For y ∈ Y with c∗(σ)(y) = 0, we may define d(y) to be an arbitrary state in D(X). We can recognise the above formula as the Bayes formula:\nP(x|y) = P(y|x) · P(x)P(y) = P(y|x) · P(x)∑ x′ P(y|x′) · P(x′) .\nLet σ ∈ G(X) and c : X → G(Y ) be a state and a channel in K`(G). A channel d : Y → G(X) is a Bayesian inversion if and only if∫\nA c(x)(B)σ(dx) = ∫ B d(y)(A) c∗(σ)(dy)\nfor all A ∈ ΣX and B ∈ ΣY . Here c∗(σ) ∈ G(Y ) is the measure given by c∗(σ)(B) =∫ X c(x)(B)σ(dx). As we see below, Bayesian inversions are in some sense equivalent to disintegrations, and thus, they are as difficult as disintegrations. In particular, a Bayesian inversion need not exist. In practice, however, the state σ and channel c are often given via density functions. This setting, so-called (absolutely) continuous probability, makes it easy to compute a Bayesian inversion. Suppose that X and Y are subspaces of R, and that σ and c admit density functions as\nσ(A) = ∫ A f(x) dx c(x)(B) = ∫ B `(x, y) dy\nfor measurable functions f : X → R≥0 and ` : X × Y → R≥0. The conditional probability density `(x, y) of y given x is often called the likelihood of x given y. By the familiar Bayes formula for densities — see e.g. (Bernardo and Smith, 2000) — the conditional density of x given y is:\nk(y, x) := `(x, y) · f(x)∫ X `(x′, y) · f(x′) dx′ .\nThis k then gives a channel d : Y → G(X) by d(y)(A) = ∫ A k(y, x) dx\nfor each y ∈ Y such that ∫ X `(x′, y) · f(x′) dx′ 6= 0. For the other y’s we define d(y) to be some fixed state in G(X). An elementary calculation verifies that d is indeed a Bayesian inversion for σ along c. Later, in Section 8, we generalise this calculation into our abstract setting.\nAlthough Bayesian inversions are a special case of disintegrations, we can conversely obtain disintegrations from Bayesian inversions, as in the proposition below. Therefore, in some sense the two notions are equivalent.\nProposition 3.6. Let ω be a state on X⊗Y . Let d : X → X⊗Y be a Bayesian inversion\nfor ω along the first projection π1 : X ⊗ Y → X on the left below.\nπ1 = X Y\nπ2 ◦ d = d X\nY\nThen the composite π2 ◦ d : X → Y shown on the right above is a disintegration of ω.\nProof. We prove that the first equation in (3) holds for c1 = π2 ◦ d, as follows.\nω\nd\n=\nω\nd\n=\nω\nd ∗=\nω\n=\nω\n= ω\nFor the marked equality ∗= we used the equation (4) for the Bayesian inversion d.\nWe say that an affine CD-category C admits disintegration if for every bipartite state ω : I → X ⊗ Y there exist a disintegration c1 : X → Y of ω. Note that in such categories there also exists a disintegration c2 : Y → X of ω in the other direction, since it can be obtained as a disintegration of the following state:\nω\nY X\n.\nBy Proposition 3.6, admitting disintegration is equivalent to admitting Bayesian inversion. In Example 3.4, we have seen that K`(D) admits disintegration, but that in measuretheoretic probability, in K`(G), disintegrations may not exist. There are however a number of results that guarantee the existence of disintegrations in specific situations, see e.g. (Pachl, 1978; Faden, 1985). We here invoke one of these results and show that there is a subcategory of K`(G) that admits disintegration. A measurable space is called a standard Borel space if it is measurably isomorphic to a Polish space with its Borel σ-algebra, or equivalently, if it is measurably isomorphic to a Borel subspace of R. Then the following theorem is standard, see e.g. (Pollard, 2002, §5.2) or (Faden, 1985, §5).\nTheorem 3.7. Let X be any measurable space and Y be a standard Borel space. Then for any state (i.e. a probability measure) ω ∈ G(X×Y ) in K`(G), there exists a disintegration c1 : X → G(Y ) of ω.\nLet pKrnsb be the full subcategory of K`(G) consisting of standard Borel spaces as objects. It is easy to see that pKrnsb is an affine CD-category. Then the previous theorem immediately shows:\nCorollary 3.8. The category pKrnsb admits disintegration.\nWe note that pKrnsb can also be seen as the Kleisli category of the Giry monad restricted on the category of standard Borel spaces.\nSince there are various ‘existence’ theorems like Theorem 3.7, there may be other subcategories of K`(G) that admit disintegration. A likely candidate is the category of perfect probabilistic mappings in (Culbertson and Sturtz, 2014). We do not go into this question here, since pKrnsb suffices for the present paper."
    }, {
      "heading" : "4. Example: naive Bayesian classifiers via inversion",
      "text" : "Bayesian classification is a well-known technique in machine learning that produces a distribution over data classifications, given certain sample data. The distribution describes the probability, for each data (classification) category, that the sample data is in that category. Here we consider an example of ‘naive’ Bayesian classification, where the features are assumed to be independent. We consider a standard classification example from the literature which forms an ideal setting to illustrate the use of both disintegration and Bayesian inversion. Disintegration is used to extract channels from a given table, and subsequently Bayesian inversion is applied to (the tuple of) these channels to obtain the actual classification. The use of channels and disintegration/inversion in this classification setting is new, as far as we know.\nFor the calculations in this example we use the EfProb library (Cho and Jacobs, 2017), of which we explain the notation for marginalisation and disintegration. There are many ways to marginalise an n-partite state, namely one for each subset of the wires {1, 2, . . . , n}. In EfProb such a subset is described as a mask, consisting of a list of n zero’s or one’s, where a zero at position i means that the i-th wire/component is marginalised out, and a one at position i means that it remains. Such a mask M = [b1, . . . , bn] with bi ∈ {0, 1} is used in EfProb as a post-fix operation in ω % M on an n-partite state ω. An example explains it all:\nif ω = then ω % [1, 0, 1, 0, 0] =\nIn a similar way one can disintegrate an n-partite state in 2n may ways, where a mask of length n is now used to describe which wires are used as input to the extracted channel and which ones as output. In EfProb this is written as ω M , where M is a mask, as above. A systematic description will be given in Section 6 below.\nIn practice it is often useful to be able to marginalise first, and disintegrate next. The general description in n-ary form is a bit complicated, so we use an example for n = 5. We shall label the wires with xi, as on the left below. We seek the conditional probability written conventionally as c = ω[x1, x4 | x2, x5] on the right below.\nω\nx1 x2 x3 x4 x5\nc\nx1 x4\nx2 x5\nThis channel c must satisfy:\nω =\nω\nc\nThis picture shows how to obtain the channel c from ω: we first marginalise to restrict to the relevant wires x1, x2, x4, x5. This is written as ω % [1, 1, 0, 1, 1]. Subsequently we disintegrate with x1, x4 as output and x2, x5 as input. Hence:\nc := ω % [1, 1, 0, 1, 1] [0, 1, 0, 1] = ω [ [1, 0, 0, 1, 0] : [0, 1, 0, 0, 1] ] in EfProb notation.\nWe see that the latter EfProb post-fix [ [1, 0, 0, 1, 0] : [0, 1, 0, 0, 1] ] is a ‘variable free’ version of the traditional notation [x1, x4 | x2, x5], selecting the relevant positions — with | replaced by :.\nWe have now prepared the ground and can turn to the classification example that we announced. It involves the classification of ‘playing’ (yes or no) for certain weather data, used in (Witten et al., 2011). We shall first go through the discrete example in some detail. The relevant data are in the table in Figure 1. The question is: given this table, what can be said about the probability of playing if the outlook is Sunny, the temperature is Cold, the humidity is High and it is Windy? Our plan is to first organise these table data into four channels dO, dT , dH , dW in a\nnetwork of the form: Outlook Temperature Humidity Windy PlaydO gg dH AA dW 88 dT __\n(5)\nThe abstraction of these channels works by disintegration. The representation in EfProb starts by defining the relevant domains for the categories in the table in Figure 1. We choose abbreviations for the entries in each of the categories.\n>>> Outlook = [’S’, ’O’, ’R’] >>> Temp = [’H’, ’M’, ’C’] >>> Humidity = [’H’, ’N’] >>> Windy = [’t’, ’f’] >>> Play = [’y’, ’n’] >>> D = [Outlook, Temp, Humidity, Windy, Play]\nThis last domain D combines the previous ones into a single domain. It is used for the representation of the table, where each of the 14 lines in Figure 1 gets a probability 1/14.\n>>> table = 1/14 ∗ point_state((’S’,’H’,’H’,’f’,’n’), D) \\ ... + 1/14 ∗ point_state((’S’,’H’,’H’,’t’,’n’), D) \\ ... + 1/14 ∗ point_state((’O’,’H’,’H’,’f’,’y’), D) \\ ... + 1/14 ∗ point_state((’R’,’M’,’H’,’f’,’y’), D) \\ ... + 1/14 ∗ point_state((’R’,’C’,’N’,’f’,’y’), D) \\ ... + 1/14 ∗ point_state((’R’,’C’,’N’,’t’,’n’), D) \\ ... + 1/14 ∗ point_state((’O’,’C’,’N’,’t’,’y’), D) \\ ... + 1/14 ∗ point_state((’S’,’M’,’H’,’f’,’n’), D) \\ ... + 1/14 ∗ point_state((’S’,’C’,’N’,’f’,’y’), D) \\ ... + 1/14 ∗ point_state((’R’,’M’,’N’,’f’,’y’), D) \\ ... + 1/14 ∗ point_state((’S’,’M’,’N’,’t’,’y’), D) \\ ... + 1/14 ∗ point_state((’O’,’M’,’H’,’t’,’y’), D) \\ ... + 1/14 ∗ point_state((’O’,’H’,’N’,’f’,’y’), D) \\ ... + 1/14 ∗ point_state((’R’,’M’,’H’,’t’,’n’), D)\nIn this way the table is transformed into a joint probability distribution on the (5-ary) domain D. Here, the transformation from table to distribution was done by hand, but it is easy enough to automate this process.\nWe extract the four channels in Diagram (5) via appropriate disintegrations, from the Play column to the Outlook / Temperature / Humidity / Windy columns.\n>>> dO = table[[1,0,0,0,0] : [0,0,0,0,1]] >>> dT = table[[0,1,0,0,0] : [0,0,0,0,1]] >>> dH = table[[0,0,1,0,0] : [0,0,0,0,1]]\n>>> dW = table[[0,0,0,1,0] : [0,0,0,0,1]]\nThus, as described in the beginning of this section, the ‘outlook’ channel dO is extracted by first marginalising the table to the relevant wires, and then disintegrating. Explicitly, dO is table % [1,0,0,0,1] // [0,1].\nIn a next step we combine these four channels into a single channel d via tupling. The answer that we are looking for will be obtained by Bayesian inversion of this channel d. But Bayesian inversion requires an additional initial state. For this we take the ‘Play’ marginal of the table, in the fifth position.\n>>> d = (dO @ dT @ dH @ dW) ∗ copy(Play,4) >>> prior_play = table % [0,0,0,0,1] >>> prior_play 0.643|y> + 0.357|n>\n>>> posterior_play = d.inversion(prior_play)(’S’,’C’,’H’,’t’) >>> posterior_play 0.205|y> + 0.795|n>\nNotice that the assumptions — Sunny outlook, Cold temperature, High humidity, true windiness — are used as input to the inversion of d. The resulting classification probability of 0.205 coincides with the probability of 20.5% that is computed in (Witten et al., 2011) — in a rather ad hoc manner, without much of a theoretical basis.\nOne could complain that our approach is ‘too’ abstract, since it remains magical what these extracted channels do. We elaborate the outlook channel dO, going from the Play to the Outlook domain. We print the two probability distributions for the two values y and n of the Play domain:\n>>> dO(’y’) 0.222|S> + 0.444|O> + 0.333|R>\n>>> dO(’n’) 0.6|S> + 0|O> + 0.4|R>\nThe first distribution 29 |S〉+ 4 9 |O〉+ 3 9 |R〉 arises as follows. We need to concentrate on the 9 lines in Figure 1 for which Play is yes; in these lines, in the first Outlook column, 2 out of 9 entries are Sunny, 4 out of 9 are Overcast, and 3 out of 9 are Rainy. This corresponds to the first distribution dO(’y’). Similarly, the second distribution captures the Outlook for the 5 lines where Play is no: 3 out of 5 are Sunny and 2 out of 5 are Rainy. There is a ‘continuous’ variation of this example where numerical values are used for temperature and humidity. We shall not repeat the table and refer to (Witten et al., 2011) for details. We use Bayesian inversion, as before, for classification, but we need a slightly different approach to extract channels for the ‘continuous’ features. One first computes the mean and standard deviation of the given numerical values, separately when Play is yes, and when Play is no. These means and standard deviations are used as parameters for Gaussian (normal) distributions in the EfProb channel definitions below. The mean and standard deviation values are copied from (Witten et al., 2011). We only define continuous channels for temperature and humidity, and re-use the discrete channels\nfor outlook and windiness to form a single ‘hybrid’ channel c that will be inverted for classification:\n>>> cT = chan_from_states([gaussian_state(73, 6.2), ... gaussian_state(74.6, 7.9)], Play) >>> cH = chan_from_states([gaussian_state(79.1, 10.2), ... gaussian_state(86.2, 9.7)], Play) >>> c = (dO @ cT @ cH @ dW) ∗ copy(Play,4) >>> c.inversion(prior_play)(’S’,66,90,’t’) 0.207|y> + 0.793|n>\nThe latter inversion computation produces the probability of 0.207 for playing when the outlook is Sunny, the temperature is 66 (Fahrenheit), the humidity is 90% and the windiness is true. The value computed in (Witten et al., 2011) is 20.8%. The minor difference of 0.001 with our outcome can be attributed to (intermediate) rounding errors."
    }, {
      "heading" : "5. Almost equality of channels",
      "text" : "This section explains how the standard notion of ‘equal up to negligible sets’ or ‘equal almost everywhere’ (with respect to a measure) can be expressed abstractly using string diagrams. Via this equality relation Bayesian inversion can be characterised very neatly, following (Clerc et al., 2017). We consider an affine CD-category, continuing in the setting of Section 3.\nDefinition 5.1. Let c, d : X → Y be two parallel channels, and σ : I → X be a state on their domain. We say that c is σ-almost equal to d, written as c σ∼ d if\nσ\nc\n=\nσ\nd\n.\nIt is obvious that σ∼ is an equivalence relation on channels of type X → Y . When S is a set of arrows of type X → Y , we write S/σ for the quotient S/ σ∼.\nTo put it more intuitively, we have c σ∼ d iff c and d can be identified whenever the input wires are connected to σ, possibly through copiers. For instance, using the associativity and commutativity of copiers, by c σ∼ d we may reason as:\nσ\nc\n=\nσ\nc =\nσ\nd =\nσ\nd\n.\nIn particular, c σ∼ d if and only if\nσ\nc\n=\nσ\nd\n.\nNow the following is an obvious consequence from the definition.\nProposition 5.2. If both c, d : X → Y are disintegrations of a joint state ω : I → X⊗Y , then c ω1∼ d, where ω1 : I → X is the first marginal of ω.\nFor channels f, g : X → D(Y ) and a state σ ∈ D(X) in K`(D), it is easy to see that f σ∼ g if and only if f(x)(y) · σ(x) = g(x)(y) · σ(x) for all x ∈ X and y ∈ Y if and only if f(x) = g(x) for any x ∈ X with σ(x) 6= 0. Almost equality in K`(G) is less trivial but characterised in an expected way.\nProposition 5.3. Let f, g : X → G(Y ) be channels and µ ∈ G(X) a state in K`(G). Then f µ∼ g if and only if for any B ∈ ΣY , f(−)(B) = g(−)(B) µ-almost everywhere.\nProof. By expanding the definition, f µ∼ g if and only if∫ A f(x)(B)µ(dx) = ∫ A g(x)(B)µ(dx)\nfor all A ∈ ΣX and B ∈ ΣY . This is equivalent to f(−)(B) = g(−)(B) µ-almost everywhere for all B ∈ ΣY , see (Fremlin, 2000, 131H).\nAlmost-everywhere equality of probability kernels f, g : X → G(Y ) is often formulated by the stronger condition that f = g µ-almost everywhere. The next proposition shows that the stronger variant is equivalent under a reasonable assumption (any standard Borel space is countably generated, for example).\nProposition 5.4. In the setting of the previous proposition, additionally assume that the measurable space Y is countably generated. Then f µ∼ g if and only if f = g µ-almost everywhere.\nProof. Let the σ-algebra ΣY on Y be generated by a countable family (Bn)n. We may assume that (Bn)n is a π-system, i.e. a family closed under binary intersections. Let An = {x ∈ X | f(x)(Bn) = g(x)(Bn)}, and A = ⋂ nAn. Each An is µ-conegligible, and thus A is µ-conegligible. For each x ∈ A, we have f(x)(Bn) = g(x)(Bn) for all n. By application of the Dynkin π-λ theorem, it follows that f(x) = g(x). Therefore f = g µ-almost everywhere.\nWe can now present a fundamental result from (Clerc et al., 2017, §3.3) in our abstract setting. Let (I ↓ C) be the comma (coslice) category. for an affine CD-category C. Objects in (I ↓ C) are states in C, formally pairs (X,σ) of objects X ∈ C and states σ : I → X. Arrows from (X,σ) to (Y, τ) are state-preserving channels c : X → Y in C\nsatisfying c ◦ σ = τ . A joint state (X ⊗ Y, ω) ∈ (I ↓C) is called a coupling of two states (X,σ), (Y, τ) ∈ (I ↓C) if\nω\nX\n= σ\nX and\nω\nY\n= τ\nY .\nWe write Coupl((X,σ), (Y, τ)) for the set of couplings of (X,σ) and (Y, τ).\nTheorem 5.5. Let C be an affine CD-category that admits disintegration. For each pair of states (X,σ), (Y, τ) ∈ (I ↓C), there is the following bijection:\n(I ↓C) ( (X,σ), (Y, τ) ) /σ ∼= Coupl((X,σ), (Y, τ))\nProof. For each c ∈ (I ↓C) ( (X,σ), (Y, τ) ) , we define a joint state I → X ⊗ Y to be\nthe ‘integration’ of σ and c as below, for which we use the following ad hoc notation:\nσ = c :=\nσ\nc\nX Y\nIt is easy to check that σ = c is a coupling of σ and τ . For two channels c, d : X → Y , we have σ = c = σ = d if and only if c σ∼ d, by the definition of σ∼. This means the mapping\nc 7−→ σ = c , (I ↓C) ( (X,σ), (Y, τ) ) /σ −→ Coupl((X,σ), (Y, τ))\nis well-defined and injective. To prove the surjectivity let (X⊗Y, ω) ∈ Coupl((X,σ), (Y, τ)). Let c : X → Y be a disintegration of ω. Then c is state-preserving since\nτ = ω =\nω\nc\n= ω\nc =\nσ\nc\nMoreover we have σ = c = ω, as desired.\nVia the symmetry X⊗Y ∼=→ Y ⊗X we have the obvious bijection Coupl((X,σ), (Y, τ)) ∼=\nCoupl((Y, τ), (X,σ)). This immediately gives the following corollary.\nCorollary 5.6. Let C be an affine CD-category that admits disintegration. For any states (X,σ), (Y, τ) ∈ (I ↓C) we have\n(I ↓C) ( (X,σ), (Y, τ) ) /σ ∼= (I ↓C) ( (Y, τ), (X,σ) ) /τ\nThe bijection sends a channel c : X → Y to a Bayesian inversion d : Y → X for σ along c.\nTheorem 2 of (Clerc et al., 2017) is obtained as an instance, for the category pKrnsb. This bijective correspondence yields a ‘dagger’ (−)† functor on (a suitable quotient of) the comma category (I ↓C) — as noted by the authors of (Clerc et al., 2017).\n5.1. Equality extension property\nThe section concerns a property that allows us to extend almost-equality w.r.t. some state to a larger state which has the smaller state as marginal. This property is convenient for equational reasoning between string diagrams, and used later in the proof of Theorem 8.3.\nDefinition 5.7. We say an affine CD-category has the equality extension property if for any joint state ω : I → X ⊗ Z and for any channels c, d : X → Y , c ω1∼ d implies c⊗ idZ ω∼ d⊗ idZ , where ω1 = π1 ◦ ω is the first marginal of ω.\nLemma 5.8. An affine CD-category has the equality extension property if and only if for any joint state ω : I → X ⊗ Z, and for any channels c, d : X → Y ,\nc ω1∼ d implies\nω\nc =\nω\nd\nwhere ω1 = π1 ◦ ω.\nProof. The ‘only if’ is obvious, since c⊗idZ ω∼ d⊗idZ implies (c⊗idZ)◦ω = (d⊗idZ)◦ω. To prove the ‘if’, assume c ω1∼ d for a state ω : I → X ⊗ Z and for channels c, d : X → Y . We have to prove c⊗ idZ ω∼ d⊗ idZ , i.e.\nω\nc\n= ω\nd\nThis follows from the latter condition applied to the following state:\nX Z⊗X⊗Z :=\nω\nX Z X Z\n.\nIn fact, any category admitting disintegration has the equality extension property.\nProposition 5.9. If an affine CD-category admits disintegration, then it has the equality extension property.\nProof. Assume c ω1∼ d for a state ω : I → X ⊗ Z and for channels c, d : X → Y , with ω1 = π1 ◦ ω. Suppose that ω is disintegrated as:\nω =\nω1\ne\nThen\nω\nc =\nω1\nc e\n=\nω1\nd e\n= ω\nd\nThis concludes the proof by Lemma 5.8.\nRecall that K`(G) does not admit disintegration. Nevertheless, the category has the equality extension property.\nProposition 5.10. The category K`(G) satisfies the equality extension property.\nProof. We prove the claim using Proposition 5.3 and Lemma 5.8. Assume c ω1∼ d for ω ∈ G(X ⊗ Z) and c, d : X → G(Y ), with ω1 the first marginal of ω. The equality (c⊗ ηZ) ◦· ω = (d⊗ ηZ) ◦· ω is equivalent to:∫\nX×Z c(x)(A)1C(z)ω(d(x, z)) = ∫ X×Z d(x)(A)1C(z)ω(d(x, z)) (6)\nfor all A ∈ ΣX and C ∈ ΣZ . Using∣∣c(x)(A)1C(z)− d(x)(A)1C(z)∣∣ = ∣∣c(x)(A)− d(x)(A)∣∣1C(z) ≤ ∣∣c(x)(A)− d(x)(A)∣∣ , we have ∣∣∣∫\nX×Z\n( c(x)(A)1C(z)− d(x)(A)1C(z) ) ω(d(x, z)) ∣∣∣ ≤ ∫ X×Z\n∣∣c(x)(A)1C(z)− d(x)(A)1C(z)∣∣ω(d(x, z)) ≤ ∫ X×Z\n∣∣c(x)(A)− d(x)(A)∣∣ω(d(x, z)) = ∫ X\n∣∣c(x)(A)− d(x)(A)∣∣ (π1)∗(ω)(dx) = ∫ X\n∣∣c(x)(A)− d(x)(A)∣∣ω1(dx) = 0 .\nThis proves the desired equality (6)."
    }, {
      "heading" : "6. Conditional independence",
      "text" : "Throughout this section, we consider an affine CD-category that admits disintegration.\n6.1. Disintegration of multipartite states\nSo far we have concentrated on bipartite states — except in the classification example in Section 4. In order to deal with a general n-partite state ω : I → X1 ⊗ · · · ⊗Xn, we will\nintroduce several notations and conventions in Definitions 6.1, 6.2 and 6.3 below; they are in line with standard practice in probability theory. In the conventions, an n-partite state, as below, is fixed, and used implicitly.\nω\n. . .X1 X2 Xn\nDefinition 6.1. When we write . . .Xi1Xi2 Xik\nwhere i1, . . . , ik are distinct, it denotes the state I → Xi1 ⊗ · · · ⊗Xik obtained from ω by marginalisation and permutation of wires (if necessary). Let us give a couple of examples, for n = 5.\nX1 X4\n:= ω\nX1\nX2 X3\nX4\nX5\nX4X2X5\n:= ω X1\nX2\nX3\nX4 X5\nWe permute wires via a combination of crossing. This is unambiguous by the coherence theorem.\nBelow we will use symbols X,Y, Z,W, . . . to denote not only a single wire Xi but also multiple wires Xi ⊗Xj ⊗ · · · . Disintegrations more general than in the bipartite case are now introduced as follows.\nDefinition 6.2. ForX = Xi1⊗· · ·⊗Xik and Y = Xj1⊗· · ·⊗Xjl , with all i1, . . . , ik, j1, . . . , jl distinct, a disintegration X → Y is defined to be a disintegration of\nX Y\n,\nthe marginal state given by the previous convention. We denote the disintegration simply as on the left below,\nY\nX\nYX\n= X Y\nBy definition, it must satisfy the equation on the right above. Let us give an example. The disintegration X1 ⊗X4 → X5 ⊗X2 on the left below is defined by the equation on\nthe right.\nX1\nX5 X2\nX4 X1 X5 X2X4\n= X1X4X5X2\nMore specifically, assuming n = 5 and expanding the notation for marginals, the equation is:\nω\nX1 X2\nX1 X3 X4\nX4X5\n= ω\nX1 X2\nX3\nX4 X5\nNote that disintegrations need not be unique. Thus when we write Y\nX\n, we in fact choose\none of them. Nevertheless, such disintegrations are unique up to almost-equality with\nrespect to X , which is good enough for our purpose.\nFinally we make a convention about almost equality (Definition 5.1).\nDefinition 6.3. Let S and T be string diagrams of type X → Y that are made from marginals and disintegrations of ω as defined in Definitions 6.1 and 6.2. When we say S is almost equal to T (or write S ∼ T ) without reference to a state, it means that S is\nalmost equal to T with respect to the state X .\nWe shall make use of the following auxiliary equations involving discarding and composition of disintegrations.\nProposition 6.4. In the conventions and notations above, the following hold. 1 X\nY\nZ\n∼\nX\nZ\n2 X\nY W\nZ\n∼\nX\nY W\nZ\nProof. By the definition of almost equality, 1 is proved by:\nX\nY\nZ\n= X\nY\nZ\n= X Z =\nX Z\n.\nSimilarly, we prove 2 as follows.\nX Y WZ\n=\nX Y WZ\n=\nX Y WZ\n=\nX Y WZ\n?= X Z Y W\n= X Z Y W\n=\nX YZ W\nFor the marked equality ?=, we used the equality extension property, which is valid in a category with disintegration.\nThe equations correspond respectively to ∑ y P(x, y|z) = P(x|z) and P(x|y, z)·P(z|y, w) =\nP(x, z|y, w) in discrete probability.\nRemark 6.5. In this section, we use symbols X1, X2, . . . or X,Y, Z, . . . in order to specify wires in string diagrams. These symbols should not mean mere objects/types, since objects need not be distinct and thus we cannot distinguish wires by objects. As a consequence, our notations here are somewhat informal. One way to make our notations more formal is to introduce labels for wires (cf. (Kissinger, 2014)). For the present paper, however, our informal notations seem to be sufficient.\n6.2. Conditional independence\nWe continue using the notations in the previous subsection. Recall that we fix an n-partite state\nω\n. . .X1 X2 Xn\nand use symbols X,Y, Z,W, . . . to denote a wire Xi or multiple wires Xi ⊗Xj ⊗ · · · . We now introduce the notion of conditional independence. Although it is defined\nwith respect to the underlying state ω, we leave the state ω implicit, like an underlying probability space Ω in ordinary probability theory.\nDefinition 6.6. LetX,Y, Z denote distinct wires. Then we sayX and Y are conditionally independent given Z, written as X ‚ Y | Z, if\nX Y\nZ\n∼\nX Y\nZ\n.\nThe definition is analogous to the condition P(x, y|z) = P(x|z) P(y|z) in ordinary probability theory. Indeed our definition coincides with this ordinary one, as explained below.\nExample 6.7. In K`(D), let cX|Z : Z → D(X), cY |Z : Z → D(Y ), cXY |Z : Z → D(X×Y ) be disintegrations of some joint state, say ω ∈ D(X × Y × Z). Let ωZ ∈ D(Z) be the marginal on Z. Then X ‚ Y | Z if and only if\ncXY |Z(z)(x, y) = cX|Z(z)(x) · cY |Z(z)(y) whenever ωZ(z) 6= 0\nfor all x ∈ X, y ∈ Y and z ∈ Z. If we write P(x, y|z) = cXY |Z(z)(x, y), P(x|z) = cX|Z(z)(x), P(y|z) = cY |Z(z)(y), and P(z) = ωZ(z), then the condition will look more familiar:\nP(x, y|z) = P(x|z) · P(y|z) whenever P(z) 6= 0 . Similarly, in K`(G), let cX|Z : Z → G(X), cY |Z : Z → G(Y ), cXY |Z : Z → G(X × Y ), and ωZ ∈ G(Z) be appropriate disintegrations and a marginal of some joint probability measure ω. Then X ‚ Y | Z if and only if\ncXY |Z(z)(A×B) = cX|Z(z)(A) · cY |Z(z)(B) for ωZ-almost all z ∈ Z\nfor all A ∈ ΣX and B ∈ ΣY .\nThe equivalences in the next result are well-known in conditional probability. Our contribution is that we formulate and prove them at an abstract, graphical level.\nProposition 6.8. The following are equivalent. 1 X ‚ Y | Z 2\nX Y Z\n=\nX Y Z\n3 X\nY Z\n∼\nX\nY Z\n4\nX Y Z\n=\nX Y Z\n5 X Y Z\n=\nX Y Z\nProof. By definition of almost equality, 1 is equivalent to\nX Y Z\n=\nX Y Z\n=:\nX Y Z\nWe then have 1 ⇔ 2, since the identity below holds by the definition of disintegration.\nX Y Z\n= X Y Z\nSimilarly, 3 ⇔ 4 follows by the definitions of almost equality and disintegration. We have 2 ⇔ 4 because\nX Y Z\n=\nX Y Z\n=\nX Y Z\n,\nand similarly 2 ⇔ 5.\nNote that the condition 3 of the proposition is an analogue of P(x|y, z) = P(x|z). The other conditions 2, 4 and 5 say that the joint state can be factorised in certain ways, corresponding to the following equations:\nP(x, y, z) = P(x|z) P(y|z) P(z) = P(x|z) P(y, z) = P(y|z) P(x, z).\nThe proposition below shows that our abstract formulation of conditional independence does satisfy the basic ‘rules’ of conditional independence, which are known as (semi-) graphoids axioms (Verma and Pearl, 1988; Geiger et al., 1990).\nProposition 6.9. Conditional independence (−) ‚ (−) | (−) satisfies: 1 (Symmetry) X ‚ Y | Z if and only if Y ‚ X | Z. 2 (Decomposition) X ‚ Y ⊗ Z |W implies X ‚ Y |W and X ‚ Z |W . 3 (Weak union) X ‚ Y ⊗ Z |W implies X ‚ Y | Z ⊗W . 4 (Contraction) X ‚ Z |W and X ‚ Y | Z ⊗W imply X ‚ Y ⊗ Z |W . Proof. We will freely use Proposition 6.8. (1) Suppose X ‚ Y | Z. Then\nY X Z\n= XY Z\n(X‚Y |Z)=\nY X Z\n=\nY X Z\n.\nThis means Y ‚ X | Z. (2) Suppose X ‚ Y ⊗ Z |W , namely:\nX Y ZW\n=\nX Y Z W\nMarginalising Z, we obtain\nX Y W\n= X Y Z W\n=\nX Y Z W\n=\nX Y W\n,\nby Proposition 6.4.1. Thus X ‚ Y |W . Similarly we prove X ‚ Z |W . Finally, we prove 3 and 4 at the same time. Note that X ‚ Y ⊗ Z | W implies X ‚ Z |W , as shown above. Therefore what we need to prove is that X ‚ Y ⊗Z |W if and only if X ‚ Y | Z ⊗W , under X ‚ Z |W . Assume X ‚ Z |W , so we have\nX\nZ W\n∼\nX\nZ W\nThen\nX Y Z W\n=\nX Y Z W\n=\nX Y Z W\n=\nX Y Z W\n=\nX Y Z W\n=\nX Y Z W\nThis proves X ‚ Y ⊗ Z |W if and only if X ‚ Y | Z ⊗W . The four properties from the graphoid axioms are essential in reasoning of conditional independence with DAGs or Bayesian networks (Verma and Pearl, 1988; Geiger et al., 1990). We leave further details to future work."
    }, {
      "heading" : "7. Beyond causal channels",
      "text" : "All CD-categories C that we have considered so far are affine in the sense that all arrows f : X → Y are causal: ◦ f = . We now drop the affineness, in order to enlarge our category to include ‘non-causal’ arrows. Essentially, we lose nothing by this change: all the arguments so far can still be applied to the subcategory Caus(C) ⊆ C containing all the objects and causal arrows. The category Caus(C) inherits the monoidal structure of C and the comonoid structures on each objects, so that Caus(C) is an affine CD-category.\nRecall that channels in C are causal arrows, i.e. arrows in Caus(C). States are channels of the form σ : I → X. We call endomaps I → I on the tensor unit scalars. The set C(I, I) of scalars forms a monoid via the composition s · t = s◦ t and 1 = idI . The monoid of scalars is always commutative — in fact, this is the case for any monoidal category, see e.g. (Abramsky and Coecke, 2009, §3.2). In string diagram scalars are written as s or simply as s. We can multiply scalars s to any arrows f : X → Y by the parallel composition, or diagrammatically by juxtaposition:\nfs\nWe call an arrow σ : I → X is normalisable if the scalar ◦σ : I → I is (multiplicatively) invertible. In that case we can normalise σ into a proper state as follows.\nnrm(σ) := σ σ ( )−1 Effects in C are arrows of the form p : X → I; they correspond to observables, with\npredicates as special case. Diagrammatically they are written as on the left below.\np ω |= p :=\nσ\np\nOn the right the validity σ |= p of a state σ : I → X and a effect p : X → I is defined. It is the scalar given by composition. Note that effects are not causal in general; by\ndefinition, only discarders are causal ones. States σ : I → X can be conditioned by effects p : X → I via normalisation, as follows.\nσ|p := nrm (\nσ\np ) =\nσ\np ( )−1\nσ\np\nThe conditional state σ|p is defined if the validity σ |= p is invertible.\nExample 7.1. Recall that our previous examples K`(D) and K`(G) are both affine. We give two non-affine CD-categories that have K`(D) and K`(G) as subcategories, respectively. 1 For discrete probability, we use multisets (or unnormalised distributions) over nonneg-\native real numbers R≥0 = [0,∞), such as\n1|x〉+ 0.5|y〉+ 3|z〉 on a set X = {x, y, z, . . . }\nWe denote byM(X) the set of multisets over R≥0 on X. More formally:\nM(X) = {φ : X → R≥0 | φ has finite support} .\nIt extends to a commutative monadM : Set→ Set, see (Coumans and Jacobs, 2013). In a similar way to the distribution monad D, we can check that the Kleisli category K`(M) is a CD-category. For a Kleisli map f : X → M(Y ), causality ◦ f = amounts to the condition ∑ y f(x)(y) = 1 for all x ∈ X. It is thus easy to see that Caus(K`(M)) ∼= K`(D). In fact, the distribution monad D can be obtained fromM as its affine submonad, see (Jacobs, 2017). An effect p : X → 1 in K`(M) is a function p : X → R≥0. Its validity σ |= p in a state ω is given by the expected value ∑ x σ(x) · p(x). The state σ|p updated with ‘evidence’\np is defined as σ|p(x) = σ(x)·p(x)σ|=p . 2 For general, measure-theoretic probability, we use s-finite kernels between measurable\nspaces (Kallenberg, 2017; Staton, 2017). Let X and Y be measurable spaces. A function f : X × ΣY → [0,∞] is called a kernel from X to Y if — f(x,−) : ΣY → [0,∞] is a measure for each X; and — f(−, B) : X → [0,∞] is measurable for each B ∈ ΣY . We write f : X Y when f is a kernel from X to Y . A probability kernel is a kernel f : X Y with f(x, Y ) = 1 for all x ∈ X. A kernel f : X Y is finite if there exists r ∈ [0,∞) such that for all x ∈ X, f(x, Y ) ≤ r. (Note that it must be ‘uniformly’ finite.) A kernel f : X Y is s-finite if f = ∑ n fn for some countable family (fn : X → Y )n∈N of finite kernels. For two s-finite kernels f : X Y and g : Y Z, we define the (sequential) composite g ◦ f : X Z by\n(g ◦ f)(x,C) = ∫ Y g(y, C) f(x, dy)\nfor x ∈ X and C ∈ ΣZ . There are identity kernels ηX : X X given by ηX(x,A) = 1A(x). With these data, measurable spaces and s-finite kernels form a category, which\nwe denote by sfKrn. There is a monoidal structure on sfKrn. For measurable spaces X,Y we define the tensor product X ⊗ Y = X × Y to be the cartesian product of measurable spaces. The tensor unit I = 1 is the singleton space. For s-finite kernels f : X Y and g : Z W , we define f ⊗ g : X × Z Y ×W by\n(f ⊗ g)((x, z), E) = ∫ Y (∫ W 1E(y, w) g(z,dw) ) f(x, dy)\n= ∫ W (∫ Y 1E(y, w) f(x,dy) ) g(z,dw)\nfor x ∈ X, z ∈ Z,E ∈ ΣY×W . The latter equality holds by the Fubini-Tonelli theorem for s-finite measures. These make the category sfKrn symmetric monoidal. Finally, for each measurable spaceX there is a ‘copier’ : X X×X and a ‘discarder’ : X 1, given by (x,E) = 1E(x, x) and (x, 1) = 1, so that sfKrn is a CD-category. For more technical details we refer to (Kallenberg, 2017; Staton, 2017). Note that an s-finite kernel f : X Y is causal if and only if it is a probability kernel, which is nothing but a Kleisli map X → G(Y ) for the Giry monad. Therefore the causal subcategory of sfKrn is the Kleisli category of the Giry monad: Caus(sfKrn) ∼= K`(G). In particular, states in sfKrn are probability measures σ ∈ G(X). An effect p : X 1 in sfKrn, i.e. an s-finite kernel p : X × Σ1 → [0,∞], can be identified with a measurable function p : X → [0,∞]. The validity σ |= p is then the integral ∫ X p(x)σ(dx), defined in [0,∞]. The conditional state σ|p ∈ G(X) is defined by:\nσ|p(A) = ∫ A p(x)σ(dx) σ |= p\nfor A ∈ ΣX , when the validity σ |= p is neither 0 nor ∞.\nWith these notions in place we return to the original description of disintegration in Section 3. We assume a joint state ω with its two disintegrations c1 and c2 in:\nω1 = ω\nc1\nX Y\n= ω\nX Y\n=\nω = ω2\nc2\nX Y\n(7)\nAs indicated, we write ω1 and ω2 for the first and second marginals of ω. Let q be an effect on Y . It can be extended to an effect 1⊗ q on X ⊗ Y , where:\n1⊗ q := q X Y\nThen we can form the conditioned state ω|1⊗q. In a next step we take its first marginal, written as ( ω|1⊗q ) 1. It turns out that, in general, this first marginal is different from the original first marginal ω1, even though the effect q only applies to the second coordinate. This is called ‘crossover influence’ in (Jacobs and Zanasi, 2017). It happens when the state ω is ‘entwined’, that is, when its two coordinates are correlated.\nA fundamental result in this context is that this crossover influence can also be captured via the channels c1, c2 that are extracted from ω via disintegrations. This works via effect transformation c∗(p) := p ◦ c and state transformation c∗(σ) := c ◦ σ along a channel.\nTheorem 7.2. In the above setting, assuming that the relevant conditioned states exist, there are equalities of states:\nω1|c∗1(q) = ( ω|1⊗q ) 1 = ( c2 ) ∗(ω2|q). (8)\nFollowing (Jacobs and Zanasi, 2016) we can say that the expression on the left in (8) uses backward inference, and the one on the right uses forward inference.\nProof. We first note that the state in the middle of (8) is the first marginal of:\nω\nq ( )−1\nω\nq\nHence: ( ω|1⊗q ) 1 = ω q ( )−1\nω\nq\n(9)\nWe note that the above scalar (that is inverted) can also be obtained as:\nω\nc1\nq\n=\nω\nc1\nq\n(7)= ω\nq\n(10)\nHence we can prove the equation on the left in (8):\nω1|c∗1(q) =\nω\nc1 q( )−1 ω c1\nq\n(7),(10)= ω\nq ( )−1\nω\nq\nIn a similar way we prove the equation on the right in (8), since ( c2 ) ∗(ω2|q) equals:\nω\nq ( )−1\nω\nq\nc2\n= ω\nq ( )−1\nω\nq\nc2 (7)= ω\nq ( )−1\nω\nq\nThe two equations in Theorem 8 will be illustrated in the ‘disease and mood’ example below, where a particular state (probability) will be calculated in three different ways.\nDisease and mood example\nWe describe a non-trivial example of probabilistic (Bayesian) reasoning. The setting is the following. We consider a joint state about the occurrence and non-occurrence of a disease, written as D and ∼D, jointly with the occurrence and non-occurrence of a good mood, written as M and ∼M . The joint distribution that we start from is of the form:\n0.05|M,D〉+ 0.4|M,∼D〉+ 0.5|∼M,D〉+ 0.05|∼M,∼D〉. (11)\nSuppose there is a test for the disease, which is positive in 90% of all cases of people having the disease, and still 5% positive for people without the disease. Suppose the disease comes out positive. What is then the mood? It is expected that the mood will deteriorate, since the disease and the mood are ‘entwined’ (correlated) in the above joint state (11): a high likelihood of disease corresponds to a low mood. We formalise this example in the EfProb language (Cho and Jacobs, 2017), see also Section 4. In EfProb one writes conditioning s|p as s/p, state transformation c∗(s) = c ◦ s as c >> s and predicate (effect) transformation c∗(q) = q ◦ c as c << q. We recall from Section 4 that marginalisation and disintegration of a state s are written as s % M and s // M respectively, where M is a mask of zeros and ones. The above joint (prior) state (11) is defined in EfProb as follows, starting with the relevant domains (types).\n>>> mood_dom = [’M’, ’~M’] >>> disease_dom = [’D’, ’~D’] >>> w = State([0.05, 0.4, 0.5, 0.05], [mood_dom, disease_dom]) >>> w 0.05|M,D> + 0.4|M,~D> + 0.5|~M,D> + 0.05|~M,~D>\nWe see that the latter state w in EfProb corresponds to the above state (11). We can concentrate on the disease or mood separately, via marginalisations. In EfProb this is done as follows, via post-fix selection operations.\n>>> w1 = w % [1,0] >>> w2 = w % [0,1] >>> w1 0.45|M> + 0.55|~M>\n>>> w2 0.55|D> + 0.45|~D>\nThe sensitivity of the test is defined as a channel, called sens for ‘sensitivity’ below. If the disease is present, the test gives a positive outcome in 90% of the cases. But if the disease is absent, the test still has a 5% change of being positive. This is captured in the definition of the sensitivity channel below. It is applied to the first marginal of the\nprior state w to see what the a priori likelihood of a positive test is. This is done via state transformation, which is written in EfProb as >>.\n>>> sens = chan_from_states([flip(9/10),flip(1/20)],disease_dom) >>> sens >> w2 0.518|True> + 0.482|False>\nAs explained above, we are interested in the mood after a positive test. This requires updating the prior state. Below we first define the positive-test predicate, and then condition (update, revise) the state, via the EfProb-notation /. We introduce a positivetest predicate (effect) pos_test via predicate transformation, written as << in EfProb. In order to use it for conditioning the joint state w, we have to ‘weaken’ (extend) the predicate pos_test to the whole domain of w. This is done via parallel conjunction @ with the truth predicate. Finally, the second marginal of the updated state s gives the new mood, after the test. This corresponds to the middle expression in (8).\n>>> pos_test = sens << yes_pred >>> pos_test"
    }, {
      "heading" : "D: 0.9 | ~D: 0.05",
      "text" : ">>> s = w / (pos_test @ truth(mood_dom)) >>> s % [0,1] 0.126|M> + 0.874|~M>\nClearly, a positive test leads to a lower mood: a reduction from 0.45 to 0.126. Next we show how this result can also be obtained via disintegration of the prior joint state w. There are two ways to do this, via disintegration in the first component and predicate transformation (backward inference), or via disintegration in the second component and state transformation (forward inference). The first way corresponds to the expression on the left in (8):\n>>> c1 = w // [1,0] >>> w1 / (c1 << pos_test) 0.126|M> + 0.874|~M>\nVia disintegration in the second component we get the same result, as on the right in (8):\n>>> c2 = w // [0,1] >>> c2 >> (w2 / pos_test) 0.126|M> + 0.874|~M>\nThe fact that these three approaches lead to the same mood distribution follows from Theorem 7.2."
    }, {
      "heading" : "8. Disintegration via likelihoods",
      "text" : "We continue in the setting of Section 7 in a CD-category that is not necessarily affine. The goal of this section is to present Theorem 8.3, which generalises a construction of Bayesian inversions using densities/likelihoods shown in Example 3.5.\nWe first introduce ‘likelihoods’ in our setting.\nDefinition 8.1. We say a channel c : X → Y is represented by a effect ` on X ⊗ Y with respect to an arrow ν : I → Y if\nc = `\nν\n(12)\nWe call ` a likelihood relation for the channel c with respect to ν.\nInterpreted in the category sfKrn, the definition says: a kernel c : X Y satisfies c(x,B) = ∫ B `(x, y) ν(dy)\nfor a kernel ` : X × Y 1 (identified with a measurable function ` : X × Y → [0,∞]) and a measure ν : 1 Y . This is basically the same as what we have in Example 3.5, but here ν is not necessarily the Lebesgue measure. We use the σ-almost equality σ∼ also for non-causal arrows.\nDefinition 8.2. Let σ : I → X be a state. We say that an arrow c : X → Y is σ-almost causal if ◦c σ∼ . An effect p : X → I is σ-almost invertible if there is an effect q : X → I such that:\np q σ∼ .\nThe definition allows us to normalise an arrow f : X → Y into an almost causal one, as follows. If an effect ◦ f is σ-almost inverted by q, as on the left below,\nf q σ∼ f q\nthen clearly the arrow X → Y on the right is σ-almost causal. We can now formulate and prove our main technical result.\nTheorem 8.3. Let σ be a state on X, and c : X → Y be a channel represented by a likelihood relation ` with respect to ν as in (12) above. Assume that the category has the equality extension property, and that the effect\n`\nσ\n= `\nσ is almost invertible w.r.t. c∗(σ) =\n`\nνσ\n.\nThen, writing q : Y → I for an almost inverse to the effect, the channel\nd : Y → X := `\nσ\nq\nis a Bayesian inversion for σ along c : X → Y . Namely, together they satisfy the equation (4).\nNote that we now allow a non-causal arrow d : Y → X to be a Bayesian inversion. Nonetheless, it follows from the definition that any Bayesian inversion is c∗(σ)-almost causal. Similarly, we use the equality extension property for almost causal arrows.\nProof. We reason as follows.\nσ\nc\nd\n=\nσ\n`\nν\n`\nσ\nq\n(i)=\nσ\nq`\nν\n`\nσ\n(ii)=\nν\n`\nσ\n=\nσ\n`\nν =\nσ\nc\nFor the equality (i)= we use associativity and commutativity of copiers . The equality (ii)= follows by\nσ\nq`\n∼ w.r.t. `\nνσ\nusing the equality extension property.\nExample 8.4. We instantiate the Theorem 8.3 in sfKrn. Let c : X Y be a probability kernel represented by a likelihood relation ` : X × Y 1 with respect to ν : 1 Y . The relation ` is identified with a measurable function ` : X × Y → [0,∞] and ν with a measure ν : ΣY → [0,∞]. The equation (12) amounts to\nc(x,B) = ∫ B `(x, y) ν(dy) .\nIn particular, each `(x,−) is a probability density function, satisfying ∫ Y `(x, y) ν(dy) = 1. Typically, we use the Lebesgue measure as ν, with Y a subspace of R. Let σ : 1 X be a probability measure. Then c∗(σ) : 1 Y is given as:\nc∗(σ)(B) = ∫ X c(x,B)σ(dx) = ∫ X ∫ B `(x, y) ν(dy)σ(dx)\nThe effect\np : Y 1 = `\nσ is given as: p(y) = ∫ X `(x, y)σ(dx) .\nTo define an inverse of p, we claim that 0 < p <∞, c∗(σ)-almost everywhere. We prove that p−1({0,∞}) = p−1(0) ∪ p−1(∞) is c∗(σ)-negligible, as:\nc∗(σ) ( p−1(0) ) = ∫ X ∫ p−1(0) `(x, y) ν(dy)σ(dx)\n= ∫ p−1(0) p(x) ν(dy)\n= ∫ p−1(0) 0 ν(dy) = 0\nand, similarly we have∫ p−1(∞) ∞ ν(dy) = ∫ p−1(∞) p(x) ν(dy) = c∗(σ) ( p−1(∞) ) ≤ 1\nbut this is possible only when ν(p−1(∞)) = 0, hence c∗(σ) ( p−1(∞) ) = ∫ p−1(∞) p(x) ν(dy) = 0. Now define an effect q : Y 1 by\nq(y) = { p(y)−1 if 0 < p(y) <∞ 0 otherwise.\nThen p is c∗(σ)-almost inverted by q. By Theorem 8.3, the Bayesian inversion for σ along c is given by\nd : Y → X := `\nσ\nq\n,\nnamely, d(y,A) = q(y) ∫ A `(x, y)σ(dx)\n= ∫ A `(x, y)σ(dx)∫\nX `(x, y)σ(dx) whenever 0 < ∫ X `(x, y)σ(dx) <∞ (13)\nThis may be seen as a variant of the Bayes formula. The calculation in Example 3.5 is reproduced when σ is also given via a density function.\nWe conclude with another example in which the likelihood-based calculation of Bayesian inversion, specifically the formula (13) above, is used to condition with respect to point observations.\nCustomers calling\nImagine a call centre that is open for 8 hours on each day of the week. The distribution of calls is different on weekends (Sat-Sun) from other days (Mon-Fri). What can we then learn from a single call at a given time of the day regarding whether it is weekend or not?\nThe formalisation in EfProb starts by defining a domain with label W for weekend and ~W for non-weekend, together with a prior state that expresses a change of 27 of being in a weekend.\n>>> weekend_dom = [’W’, ’~W’] >>> prior = State([2/7, 5/7], weekend_dom) >>> prior 0.286|W> + 0.714|~W>\nNext we have a channel that assigns a different Gaussian distribution to W and to ~W.\n>>> hours_dom = R(0,8) >>> c = chan_from_states([gaussian_state(5,4,hours_dom), ... gaussian_state(2,4,hours_dom)], ... weekend_dom)\nThe probability density functions of the two distributions look as follows.\nIn the weekend diagram on the left we see that the calls start coming in later. Now we ask ourselves the question: suppose we see one call at (hour) 6. How does this affect the prior distribution? Of course, the updated distribution should have a higher likelihood for ‘weekend’ since 6 is relatively late.\nWe construct an inversion d of the above channel c, together with the prior state, in order to compute the updated distribution. It yields the channel d, going from hours_dom to weekend_dom. The distribution at time 6 is obtained by applying channel d to the value 6.\n>>> d = c.inversion(prior) >>> d(6) 0.374|W> + 0.626|~W>\nWe see that the weekend probability has increased indeed."
    } ],
    "references" : [ {
      "title" : "Categorical quantum mechanics",
      "author" : [ "S. Abramsky", "B. Coecke" ],
      "venue" : "Handbook of Quantum",
      "citeRegEx" : "Abramsky and Coecke,? 2009",
      "shortCiteRegEx" : "Abramsky and Coecke",
      "year" : 2009
    }, {
      "title" : "Logic and Quantum Structures: Quantum Logic, pages 261–323",
      "author" : [ "N.L. Ackerman", "C.E. Freer", "D.M. Roy" ],
      "venue" : null,
      "citeRegEx" : "Ackerman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ackerman et al\\.",
      "year" : 2011
    }, {
      "title" : "Bayesian machine learning",
      "author" : [ "J. T", "D. Pollard" ],
      "venue" : "Logical Methods in Comp. Sci.,",
      "citeRegEx" : "T. and Pollard,? \\Q1997\\E",
      "shortCiteRegEx" : "T. and Pollard",
      "year" : 1997
    }, {
      "title" : "The EfProb library for probabilistic calculations",
      "author" : [ "K. Cho", "B. Jacobs" ],
      "venue" : null,
      "citeRegEx" : "Cho and Jacobs,? \\Q2017\\E",
      "shortCiteRegEx" : "Cho and Jacobs",
      "year" : 2017
    }, {
      "title" : "Algebra and Coalgebra in Computer Science, volume 72 of LIPIcs",
      "author" : [ "K. Dagstuhl. Cho", "B. Jacobs", "A. Westerbaan", "B. Westerbaan" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2015
    }, {
      "title" : "Terminality implies no-signalling ...and much more than that",
      "author" : [ "B. 355–369. Springer. Coecke" ],
      "venue" : null,
      "citeRegEx" : "Coecke,? \\Q2016\\E",
      "shortCiteRegEx" : "Coecke",
      "year" : 2016
    }, {
      "title" : "Picturing Quantum Processes: A First Course",
      "author" : [ "B. Coecke", "A. Kissinger" ],
      "venue" : null,
      "citeRegEx" : "Coecke and Kissinger,? \\Q2017\\E",
      "shortCiteRegEx" : "Coecke and Kissinger",
      "year" : 2017
    }, {
      "title" : "Scalars, monads and categories",
      "author" : [ "Theory", "D. Diagrammatic Reasoning. Cambridge University Press. Coumans", "B. Jacobs" ],
      "venue" : "Heunen, C., Sadrzadeh,",
      "citeRegEx" : "Theory et al\\.,? 2013",
      "shortCiteRegEx" : "Theory et al\\.",
      "year" : 2013
    }, {
      "title" : "Quantum Theory from First Principles",
      "author" : [ "G.M. D’Ariano", "G. Chiribella", "P. Perinotti" ],
      "venue" : "Categorical Structures,",
      "citeRegEx" : "D.Ariano et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "D.Ariano et al\\.",
      "year" : 2017
    }, {
      "title" : "Identifying independence in Bayesian networks",
      "author" : [ "T. Verma", "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Geiger et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2000
    }, {
      "title" : "A categorical approach to probability theory",
      "author" : [ "M. Giry" ],
      "venue" : "In Categorical Aspects of Topology",
      "citeRegEx" : "Giry,? \\Q1982\\E",
      "shortCiteRegEx" : "Giry",
      "year" : 1982
    }, {
      "title" : "New directions in categorical logic, for classical, probabilistic and quantum",
      "author" : [ "B. ACM. Jacobs" ],
      "venue" : "Future of Software Engineering,",
      "citeRegEx" : "Jacobs,? \\Q2015\\E",
      "shortCiteRegEx" : "Jacobs",
      "year" : 2015
    }, {
      "title" : "A predicate/state transformer semantics for Bayesian learning",
      "author" : [ "B. Jacobs", "F. Zanasi" ],
      "venue" : null,
      "citeRegEx" : "Jacobs and Zanasi,? \\Q2016\\E",
      "shortCiteRegEx" : "Jacobs and Zanasi",
      "year" : 2016
    }, {
      "title" : "A formal semantics of influence in Bayesian reasoning",
      "author" : [ "B. Elsevier. Jacobs", "F. Zanasi" ],
      "venue" : null,
      "citeRegEx" : "Jacobs and Zanasi,? \\Q2017\\E",
      "shortCiteRegEx" : "Jacobs and Zanasi",
      "year" : 2017
    }, {
      "title" : "Random Measures, Theory and Applications",
      "author" : [ "Math. Found" ],
      "venue" : "LIPIcs. Schloss Dagstuhl. Kallenberg, O",
      "citeRegEx" : "Found.,? \\Q2017\\E",
      "shortCiteRegEx" : "Found.",
      "year" : 2017
    }, {
      "title" : "Abstract tensor systems as monoidal categories. In Categories and Types",
      "author" : [ "A. Springer. Kissinger" ],
      "venue" : null,
      "citeRegEx" : "Kissinger,? \\Q2014\\E",
      "shortCiteRegEx" : "Kissinger",
      "year" : 2014
    }, {
      "title" : "Categories for the Working Mathematician. Springer, second edition",
      "author" : [ "S. Springer. Mac Lane" ],
      "venue" : "Pachl, J. K",
      "citeRegEx" : "Lane,? \\Q1998\\E",
      "shortCiteRegEx" : "Lane",
      "year" : 1998
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems",
      "author" : [ "J. Clarendon Press. Pearl" ],
      "venue" : "Morgan Kaufmann. Pollard, D. (2002). A User’s Guide to Measure Theoretic Probability. Cambridge University",
      "citeRegEx" : "Pearl,? 1988",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "A survey of graphical languages for monoidal categories",
      "author" : [ "P. Press. Selinger" ],
      "venue" : "New Structures",
      "citeRegEx" : "Selinger,? 2010",
      "shortCiteRegEx" : "Selinger",
      "year" : 2010
    }, {
      "title" : "Exact Bayesian inference by symbolic disintegration",
      "author" : [ "Springer. Shan", "C.-c", "N. Ramsey" ],
      "venue" : "Notes in Physics,",
      "citeRegEx" : "Shan et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Shan et al\\.",
      "year" : 2017
    }, {
      "title" : "Commutative semantics for probabilistic programming",
      "author" : [ "S. ACM. Staton" ],
      "venue" : "Princ. of Programming Languages,",
      "citeRegEx" : "Staton,? \\Q2017\\E",
      "shortCiteRegEx" : "Staton",
      "year" : 2017
    }, {
      "title" : "Counterexamples in Probability",
      "author" : [ "J.M. ACM. Stoyanov" ],
      "venue" : "Computer Science,",
      "citeRegEx" : "Stoyanov,? \\Q2014\\E",
      "shortCiteRegEx" : "Stoyanov",
      "year" : 2014
    }, {
      "title" : "Data Mining Practical Machine Learning Tools",
      "author" : [ "I. Intelligence. Witten", "E. Frank", "M. Hall" ],
      "venue" : null,
      "citeRegEx" : "Witten et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Witten et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "They form the nodes of Bayesian networks (Pearl, 1988; Bernardo and Smith, 2000; Barber, 2012),",
      "startOffset" : 41,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "This language can be seen as the internal language of symmetric monoidal categories (Selinger, 2010) — with comonoids in our case.",
      "startOffset" : 84,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : ", 2017) and in (Jacobs et al., 2015; Jacobs and Zanasi, 2016; Jacobs, 2017).",
      "startOffset" : 15,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "The latter references use effectus theory (Jacobs, 2015; Cho et al., 2015), a new comprehensive approach aimed at covering the logic of both quantum theory and probability theory, supported by a Python-based tool ‘EfProb’, for ‘effectus probability’.",
      "startOffset" : 42,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "The latter references use effectus theory (Jacobs, 2015; Cho et al., 2015), a new comprehensive approach aimed at covering the logic of both quantum theory and probability theory, supported by a Python-based tool ‘EfProb’, for ‘effectus probability’.",
      "startOffset" : 42,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "(Pollard, 2002; Panangaden, 2009; Chang and Pollard, 1997): it may not exist (Stoyanov, 2014); even if it exists it may be determined only up to negligible sets; and it may not be continuous or computable (Ackerman et al.",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "(Pollard, 2002; Panangaden, 2009; Chang and Pollard, 1997): it may not exist (Stoyanov, 2014); even if it exists it may be determined only up to negligible sets; and it may not be continuous or computable (Ackerman et al., 2011).",
      "startOffset" : 205,
      "endOffset" : 228
    }, {
      "referenceID" : 22,
      "context" : "A standard example from the literature (Witten et al., 2011) is redescribed in the current setting: first, channels are extracted via disintegration from a table with given data; next, Bayesian inversion is applied to the combined extracted channels, giving the required classification.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "The main result relates conditioning of joint states to forward and backward inference via the extracted channels, in the style of (Jacobs and Zanasi, 2016); it is illustrated in a concrete example, where a Bayesian network is seen as a graph in a Kleisli category — following (Fong, 2012).",
      "startOffset" : 131,
      "endOffset" : 156
    }, {
      "referenceID" : 18,
      "context" : "The use of string diagrams is justified by the following ‘coherence’ theorem, see (Selinger, 2010).",
      "startOffset" : 82,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "Jacobs 6 The term ‘causal’ comes from (categorical) quantum foundation (Coecke and Kissinger, 2017; D’Ariano et al., 2017), and is related to relativistic causality, see e.",
      "startOffset" : 71,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "Jacobs 6 The term ‘causal’ comes from (categorical) quantum foundation (Coecke and Kissinger, 2017; D’Ariano et al., 2017), and is related to relativistic causality, see e.",
      "startOffset" : 71,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "(Coecke, 2016).",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "This G is a monad on the category Meas of measurable spaces, with measurable functions between them; it is called the Giry monad, after (Giry, 1982; Jacobs, 2017).",
      "startOffset" : 136,
      "endOffset" : 162
    }, {
      "referenceID" : 21,
      "context" : "In fact, a disintegration may not exist (Stoyanov, 2014).",
      "startOffset" : 40,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "For the calculations in this example we use the EfProb library (Cho and Jacobs, 2017), of which we explain the notation for marginalisation and disintegration.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "Weather and play data, copied from (Witten et al., 2011).",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "It involves the classification of ‘playing’ (yes or no) for certain weather data, used in (Witten et al., 2011).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 22,
      "context" : "5% that is computed in (Witten et al., 2011) — in a rather ad hoc manner, without much of a theoretical basis.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "We shall not repeat the table and refer to (Witten et al., 2011) for details.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "The mean and standard deviation values are copied from (Witten et al., 2011).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "The value computed in (Witten et al., 2011) is 20.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "(Kissinger, 2014)).",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : "2 For general, measure-theoretic probability, we use s-finite kernels between measurable spaces (Kallenberg, 2017; Staton, 2017).",
      "startOffset" : 96,
      "endOffset" : 128
    }, {
      "referenceID" : 20,
      "context" : "For more technical details we refer to (Kallenberg, 2017; Staton, 2017).",
      "startOffset" : 39,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "This is called ‘crossover influence’ in (Jacobs and Zanasi, 2017).",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "(8) Following (Jacobs and Zanasi, 2016) we can say that the expression on the left in (8) uses backward inference, and the one on the right uses forward inference.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "We formalise this example in the EfProb language (Cho and Jacobs, 2017), see also Section 4.",
      "startOffset" : 49,
      "endOffset" : 71
    } ],
    "year" : 2017,
    "abstractText" : "The notions of disintegration and Bayesian inversion are fundamental in conditional probability theory. They produce channels, as conditional probabilities, from a joint state, or from an already given channel (in opposite direction). These notions exist in the literature, in concrete situations, but are presented here in abstract graphical formulations. The resulting abstract descriptions are used for proving basic results in conditional probability theory. The existence of disintegration and Bayesian inversion is discussed for discrete probability, and also for measure-theoretic probability — via standard Borel spaces and via likelihoods. Finally, the usefulness of disintegration and Bayesian inversion is illustrated in several non-trivial examples.",
    "creator" : "LaTeX with hyperref package"
  }
}