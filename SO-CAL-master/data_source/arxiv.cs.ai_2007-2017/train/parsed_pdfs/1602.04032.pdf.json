{
  "name" : "1602.04032.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Truthful Mechanism with Biparameter Learning for Online Crowdsourcing",
    "authors" : [ "Satyanath Bhat", "Divya Padmanabhan", "Shweta Jain" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Crowdsourcing is widely used in procuring labels and services for traditional AI applications. Often many of the tasks crowdsourced are more readily accomplished by humans than computers. An additional advantage is the scalable and cost-effective nature of crowdsourcing. However, typical crowdsourcing platforms may not consider several important aspects of traditional\nar X\niv :1\n60 2.\n04 03\n2v 1\n[ cs\n.A I]\n1 2\nplanning such as ensuring work completion within a strict deadline and with assured guarantees on the quality.\nAs a motivating example for this paper, consider a sequence of jobs arriving online where each job corresponds to translating a large document which has to be completed within a deadline and with an assured level of accuracy. It may not be possible for a single individual worker to accomplish this job, so the requester could split such a job into tasks (either at chapter or section or any other level) and allocate each task to a crowd worker. Due to the very nature of the task, a worker, if employed for a long duration, might start committing errors. We refer to the duration until which an agent works without committing any error as the time to failure (TTF). Also, each worker differs in the time taken to complete the entire job (if the entire job is executed by the worker). The time taken by a worker to complete the job all by himself is called the job completion time (JCT) of that worker. Each worker incurs a certain cost to complete the entire job. Note that the workers are heterogeneous in terms of their costs, JCT, and TTF. Moreover, JCT and TTF of the workers are stochastic. An additional non-trivial challenge occurs when crowd workers are strategic and may misrepresent their costs in the hope of gaining higher utility. This setting occurs in other problems such as tagging of a large repository of images, audio transcriptions, etc.\nIn this work, we consider jobs which (a) arrive online, (b) are divisible (into tasks), (c) have strict completion deadlines, and (d) are to be completed with an assured accuracy. We propose a multi-armed bandit (MAB) mechanism which learns the two parameters (mean job completion time (MJCT) and mean time to failure (MTTF)) of the workers while eliciting their privately held costs truthfully. We show that the proposed MAB mechanism minimizes the regret while meeting the deadline and accuracy requirements on every job. The following are the specific contributions of this work.\n1. Non-strategic, with learning: We look at the problem of allocating divisible online jobs to crowd workers so as to meet the constraints on deadline and accuracy (Section 4). The underlying optimization problem turns out to be non-trivial since the parameters MJCT and MTTF of the workers are unknown. We overcome this challenge by devising a biparameter learning scheme based on the Robust UCB algorithm [5]. Further, we embed this learning scheme into our social welfare maximizing algorithm, which we refer to as SW-GREEDY.\n2. Strategic, with learning: We next non-trivially extend the results above to the setting where worker costs are privately held (Section 5) by designing a mechanism (TD-UCB). This mechanism is is dominant strategy incentive compatible and ex-post individually rational (Theorem 1).\n3. Regret Analysis: In Section 6, we show, for non-strategic as well as strategic settings, that the number of jobs for which a non-optimal worker\nset is chosen, is upper bounded by O(log T ) (Theorem 2), where T is the total number of jobs to be completed. Moreover, once an optimal worker set is selected, the allocation algorithm converges asymptotically to an efficient allocation, ensuring that the average regret goes to zero in the limit (Theorem 3).\n4. Simulations: Finally, we show the practical efficacy of our learning mechanism via simulations in Section 7."
    }, {
      "heading" : "2 Previous Work",
      "text" : "We now look at previous work related to our setting. We group the relevant literature based on whether or not crowd workers are strategic.\nIn the non-strategic case, most of the work in crowdsourcing has focused on models for aggregating labels and building classifiers [13, 12]. Many efforts also address problems similar to the one considered in our paper. Faradani et al. [9] look at the design of pricing schemes dependent on the completion times of the workers. The strategic nature of the workers is not considered here. The problem of completing tasks within a deadline is also investigated by Yu et al. [16]. The authors consider the setting where the workers delegate tasks to other workers when they are unable to complete the work within a deadline. Here the costs to workers are assumed to be known and workers are non-strategic. Under a different setup, Ding et al. [8] look at the budgeted multi-armed bandit problem where the two parameters stochastic costs and stochastic rewards are learnt. However, they do not consider strategic workers.\nIn the strategic case, Chandra et al. [6] look at allocating indivisible tasks to strategic crowd workers under deadline constraints with the assumption that the reliability (in terms of completion of the task) of the agents is common knowledge and not estimated. Singer and Mittal [14] and Biswas et al. [3] look at pricing mechanisms in the presence of budget constraints and task completion deadlines. However, the heterogeneity with respect to time to failure is not modelled. Tran-Thanh et al. [15] look at crowdsourcing classification tasks with the goal of trading off cost and accuracy of the estimation. However the TTF and JCT of the workers is not modeled here. Choosing an optimal worker set in order to obtain an assured accuracy level has been studied in Jain et al. [11]. The allocation algorithm makes use of the multi-armed bandits abstraction Auer et al. [1]. A version of their allocation algorithm was designed for the case where workers are strategic with respect to bidding their costs. However, their setting does not look at the completion of tasks within a deadline. The problem of allocating tasks concurrently to several workers in order to meet deadlines is looked at by Gerding et al. [10]. The work uses a variant of VCG mechanism to elicit\nthe costs truthfully from the workers. They consider stochastic completion times of tasks but do not consider the time to failure during the allocation.\nOur work differs from all the work listed above in that, we design an allocation scheme to complete jobs within a deadline while simultaneously learning the mean completion time as well as the mean time to failure of the workers. We also design a mechanism to elicit the costs of the workers truthfully."
    }, {
      "heading" : "3 The Model",
      "text" : "Let N = {1, . . . , n} denote the set of crowd workers (also referred to as agents) available to the requester. A sequence of T homogeneous jobs arrives at the platform, one at a time. Following are some of the design issues pertaining to the requester.\n1. Job Parameters\n(a) Deadline: The clock starts ticking for a job as soon as it arrives. We use D to denote the deadline. The deadline D on each job is an upper bound on the duration, starting from the arrival of that job, before which the job is required to be completed in expectation.\n(b) Task creation: The requester can divide a current job t (t = 1, . . . , T ) into a certain number of tasks so as to facilitate completion of the job by\nthe deadline D. We use x (t) i to denote the fraction of the job t assigned as a task to the worker i. Therefore, 0 ≤ x(t)i ≤ 1 and ∑n i=1 x (t) i = 1. We assume arbitrary division of a given job into tasks for ease of exposition. However, this assumption can be relaxed to capture meaningful constraints such as the size of the task.\n(c) Threshold on probability of failure for tasks: A worker is more likely to commit an error if he works for a longer duration on a task. We say a worker has failed when he commits an error. We use ε to denote (the common) threshold on probability of failure for any task. This threshold allows the requester to control the overall “quality” of the job.\n2. Worker Parameters\n(a) Job Completion Time (JCT): A worker has a stochastic job completion time, which is the time he requires to complete the entire job by himself. JCT for a worker is random variable with a fixed but unknown mean. We refer to the mean job completion time as MJCT. The requester wishes to learn the MJCT for each worker. If ρi is the MJCT of worker i, then\nthe task allocation x (t) i will meet the deadline constraint in expectation if x (t) i × ρi ≤ D.\n(b) Time to Failure (TTF): A worker is also characterized by a stochastic time to failure, which denotes the duration for which a worker would work without a failure. Like JCT, TTF also has a fixed yet unknown mean, which the requester wishes to learn. If Fi is the CDF of TTF\nfor agent i, who workers for a expected duration x (t) i × ρi on the task allocation given by the fraction x (t) i of job t, the requirement on threshold probability error dictates Fi(x (t) i × ρi) ≤ ε. (c) Cost Incurred: Worker i has a privately held cost ci ∈ [c, c̄] which represents the cost incurred by worker i to complete the job entirely on his\nown. Therefore, the cost involved to complete x (t) i fraction of the job by the worker i is cix (t) i .\n3. Goal of Optimization Problem: The constraints on deadline and threshold on probability of failure for every task has to be met in a cost optimal way for every online job t. Thus, the underlying optimization problem for the entire collection of jobs {1, 2, . . . , T} is given by eq. (1).\nmin x (t) i ∈[0,1] T∑ t=1 n∑ i=1 cix (t) i ,\nsubject to,∑n i=1 x (t) i = 1,∀t\nCompletion time(x (t) i ) ≤ D ∀i ∈ N, ∀t,\nProbability of failure(x (t) i ) ≤ ε ∀i ∈ N, ∀t.\n(1)\nAs mentioned earlier, the JCT and the TTF of the workers are stochastic in nature. We assume the JCT of each worker follows a log-normal distribution with unknown yet fixed mean ρi ∈ [ρ, ρ̄] while the TTF for each worker follows an exponential distribution with mean βi ∈ [β, β̄].\nRemark 1 (Choice of Distributions). The choice of log-normal distribution is due to its wide applicability in social sciences and economics to model similar quantities. However any suitable non-negative random variable whose distribution is sub-Gaussian (or sub-exponential) may be used. As discussed, the errors in this setting are introduced due to higher working duration on the task. This is analogous to the modelling of failure as function of time, in biological or computer or reliability literature, as exponential distributions. Hence, we model the TTF of the workers as exponential.\nThe optimization problem stated in eq. (1) involves a learning scheme along with cost minimization across all the T online jobs. However, due to independence across the jobs, the problem can be decomposed into a sequential cost minimization problem corresponding to each job (t). Therefore, in\neq. (1) the summation over the jobs can be omitted. This enables us to use xi in place of x (t) i for the sequential optimization problem for each job."
    }, {
      "heading" : "4 The Case of Non-Strategic Workers",
      "text" : "We first study the scenario where the costs ci incurred by the workers are common knowledge. If the means (ρi and βi) are known to the requester, no feasible allocation xi to the worker i should exceed D/ρi. The additional requirement on accuracy requires that the probability of a worker failing in the duration ρixi does not exceed ε. This is equivalent to the constraint Fi(ρixi) ≤ ε where Fi is the CDF of the random variable TTF of worker i which we model as the exponential distribution with mean βi. On simplification, the requester’s optimization problem reduces to eq. (2).\nmin xi∈[0,1] n∑ i=1 cixi,\nsubject to,∑n i=1 xi = 1,\nxi ≤ 1ρi min ( D,βi ln ( 1 1−ε )) ∀i ∈ N,\n(2)\nIn practice, ρi and βi are not known and need to be learnt. We make use of the multi-armed bandit (MAB) abstraction for learning these parameters. More specifically, since ρi and βi are sub-exponential distributions, we appeal to the Robust UCB technique [5]. While ψ-UCB algorithm [4] is a regret minimizing scheme for learning the mean of sub-Gaussian distributions, for heavy tailed distributions (e.g. log normal and exponential), Robust UCB has been shown to be regret minimizing [5]. We adopt the Robust UCB scheme with truncated empirical mean as the estimator."
    }, {
      "heading" : "4.1 Difficulty in Learning βi",
      "text" : "If a worker i, allocated a fraction xi of the job, takes time τ for completion, then τ/xi is a sample from the distribution log-normal(ρi). Therefore, every allocation contributes one such sample for the Robust UCB algorithm estimating ρi. However, for estimating βi, each sample allocation fed to the Robust UCB algorithm must correspond to a failure, but this is not practical as we do not observe failure at every instance of allocation. To handle this difficulty, we propose to use a surrogate random variable. Consider the experiment where a worker i is allocated a task (fraction of a job) on which the worker spends a duration of at least δ. The experiment is deemed to have failed if the worker i fails in the first δ duration of allocation, otherwise\nit is deemed a success. Let N (i) δ be the number of such independent experiments till a failure is encountered. We propose to use the random variable β ′ δ,i = δ ×N (i) δ to construct a sample from exponential(βi). To obtain such a sample, for every job t, we observe for a duration δ to see if any of the allocated workers have failed. Let η (i) δ be the number of contiguous instances (of jobs) of allocation during which a worker i does not fail in the interval δ. Note that η (i) δ is a sample from N (i) δ . Therefore, the value δ×η (i) δ forms a sample of interest. Once a sample is obtained, η (i) δ is reset and the process is again repeated to collect more samples. The expectation of the surrogate random variable in the limit coincides with βi due to Lemma 1.\nLemma 1. limδ→0 E[β ′ δ,i] = βi\nProof. By definition, β ′ δ,i = N (i) δ ×δ. Note, Nδ ∼ Geometric(1−exp(−δ/βi)) and therefore, E[Nδ] = 11−exp(−δ/βi) .\nlim δ→0\nE[β ′ δ,i] = lim\nδ→0\nδ\n1− exp(−δ/βi) = βi (3)\nwhere eq. (3) follows by applying the L’Hospital’s rule."
    }, {
      "heading" : "4.2 SW-GREEDY: A Greedy Allocation",
      "text" : "The workers are indexed in an increasing order of their costs and each worker i is allocated the largest possible fraction xPSTi which does not violate the constraints in eq. (2) till all tasks of the job are allocated.\nThe constraint 1ρi min ( D,βi ln ( 1 1−ε )) , involves means which are unknown. As mentioned earlier, we use Robust UCB to learn estimates for ρi and βi. ρ̂ + i and β̂ + i are the upper confidence indices while ρ̂ − i and β̂ − i are the lower confidence indices of MJCT and MTTF respectively, obtained from Robust UCB. ρ̂i and β̂i are the empirical estimates of MJCT and MTTF respectively for worker i. We could substitute ρ̂−i , ρ̂i or ρ̂ + i as the estimate for ρi in our constraint. A higher value of ρi enforces a lower allocation to worker i compared to when a lower value of ρi is used. Hence we refer to ρ̂+i as a pessimistic estimate for ρi. By a similar reasoning, we refer to β̂−i as the pessimistic estimate for βi. The use of the pessimistic estimates ensures that even with the true underlying means the constraint in eq. (2) is satisfied.\nThe allocation algorithm discussed above ensures that the social welfare regret of the learning scheme is optimized, hence we refer to the above allocation as SW-GREEDY (Algorithm 1). The social welfare is defined as follows.\nDefinition 1. Social Welfare: Social welfare of a feasible (i.e. satisfying eq. (2)) allocation x is the sum of valuations of the agents under that allocation. In this setting, the valuation of a crowd agent is −cixi. Therefore, social welfare is given by ∑n i=1−cixi.\nEvery worker i is paid an amount equal to the cost incurred, i.e. xPSTi (t)× ci, where x PST i is the allocation to agent i given by Algorithm 1.\nRemark 2 (Pessimistic Selection). The fundamental underlying philosophy of the UCB family of algorithms is “optimism under uncertainty.” Intuitively, this optimism helps in adequate exploration realtive to a naive scheme which just uses the empirical estimate. In our work, we do not use this philosophy implicitly, however, due to the greedy nature of the allocation scheme, the pessimistic allocation set is a superset of the optimistic allocation.\nALGORITHM 1: SW-GREEDY Allocation Algorithm\nInput: Set of workers N , number of jobs T , deadline D, accuracy level ε, input cost vector: c1 ≤ c2 ≤ . . . ≤ cn (By re-indexing N)\n1 ∀i ∈ N , ρ̂i = ρ̄, ρ̂+i = ρ̄, ρ̂ − i = ρ, Ni,t=0 2 β̂i = β, β̂ + i = β̄, β̂ − i = β, N β i,t = 0, 3 η (i) δ = 0\n4 for Online job arrival t = 1, . . . , T do\n5 xPST(t) = {xPST1 (t), . . . , xPSTn (t)} = {0, . . . , 0} 6 i = 1 7 while ∑n j=1 x PST j (t) < 1 do 8 xPSTi (t) = 1 ρ̂+i min ( D,β−i ln [ 1 1−ε\n]) 9 if 1− ∑i−1 j=1 x PST j (t) < x PST i (t) then\n10 xPSTi (t) = 1− ∑i−1 j=1 x PST j (t)\n11 i = i+ 1\n12 Define k̄t = max{i : xPSTi > 0} 13 Allocate the job t as per xPSTi 14 Observe τ̃i, the time of completion of x PST i by i 15 ∀i ∈ {1, . . . , k̄t}, Ni,t = Ni,t−1 + 1 16 ρ̂i = ( Ni,t−1 × ρ̂i + τ̃ixPSTi (t) ) × 1Ni,t 17 for i ∈ {1, . . . , k̄t} do 18 if Worker i made an error during δ then 19 β̂i = β̂i×Nβi,t−1+(δ×η (i) δ )\nNβi,t−1+1\n20 Nβi,t = N β i,t−1 + 1 21 η (i) δ = 0\n22 else 23 η (i) δ = η (i) δ + 1 24 Nβi,t = N β i,t−1\nInitialize\nPessimistic Selection\nUpdates for Surrogate of βi"
    }, {
      "heading" : "5 The Case of Strategic Workers: TD-UCB",
      "text" : "Here, before an allocation is performed, the agents announce their bids. These bids may or may not be equal to their true private costs. We denote the bid profile by (bi, b−i), where bi is the bid of agent i and b−i denotes the collection of bids of all agents except agent i. In order to ensure that the agents bid their costs truthfully, we introduce a mechanism TD-UCB. The allocation rule remains the same as the one for the case where the workers are non-strategic. We use the allocation given in Algorithm 1 replacing the input costs with the bids."
    }, {
      "heading" : "5.1 Payment Scheme",
      "text" : "Let ξt denote a tuple of allocation and performance of the allocated workers for the job t. The learning until job t is captured in the history ht = {ξk}tk=0. In order to specify the payment scheme, we require the notion of ‘externality’ imposed by an agent on another. We denote the externality imposed by agent i on j as xEXTi,j (bi, b−i;ht, t), which signifies the additional fraction of the job allocated to the agent j in the absence of agent i. The externality for the job t depends on the bid profile (bi, b−i) as well as the history of allocations till job t. Let kt be the agent with the largest reported bid in the worker set chosen by the allocation scheme. Figure 1 provides a schematic diagram indicating the position of the bids and the agents chosen by our algorithm.\nAgent 1\nb1 ≤\nAgent 2\nb2 ≤ . . .\nAgent kt\nbkt≤ ≤\nAgent n\nbn≤. . .\nWorkers chosen by the allocation\nBids by the workers\nWe now propose a payment structure in eq. (5) that ensures truthful bidding and positive utility to the participating agents (Theorem 1).\npi(bi, b−i; t) =\n{ 0 if i > kt,\nZ3 otherwise, where, (5)\nZ3 = n∑ s=kt [ xEXTi,s (t)× bs ] + xPSTi (t)− n∑ s=kt xEXTi,s (t) × c̄ Remark 3 (Notation). All the mechanism side parameters such as xPSTi or pi are a function of (bi, b−i;ht, t). Similarly, the agent side parameters such as utility ui depend on the tuple (bi, b−i; ci, ht, t). Note that the agent side parameters have an additional dependency on the true cost ci. Whenever clear from the context, we drop one or more of these dependencies for ease of notation.\nRemark 4 (Externality). Our mechanism is an externality based scheme like the VCG mechanism. We now set about the task of proving that the mechanism is truthul, regret minimizing, and individually rational, while learning the associated stochastic parameters. Earlier works have shown the non-triviality involved in the design of such learning mechanisms [2, 7]."
    }, {
      "heading" : "5.2 Properties of TD-UCB Mechanism",
      "text" : "Definition 2. Utility of an Agent: The utility of an agent in this setting is the difference between the valuation of an allocation and the payment made. The utility is given by the following.\nui(bi, b−i; ci, ht, t) = −ci × xPSTi (bi, b−i; ci, ht, t) + pi(bi, b−i; ci, ht, t)\nDefinition 3. Dominant Strategy Incentive Compatible (DSIC): A mechanism is DSIC if the utility ui(ci, b−i; ci) ≥ ui(bi, b−i; ci) ∀bi ∈ [c, c], ∀b−i ∈ [c, c]n−1, ∀i ∈ N , where bi and ci are the bid and true cost incurred by the worker i respectively, b−i is the bid profile of all agents other than i.\nA DSIC mechanism ensures that an agent obtains the highest utility by bidding his true cost, irrespective of the bids of other agents.\nDefinition 4. Ex-post Individually Rational (IR): A mechanism is ex-post individually rational if ui(ci, b−i; ci) ≥ 0), ∀b−i ∈ [c, c]n−1 ∀i ∈ N .\nAn IR mechanism ensures that for every agent, the utility obtained from truthful bidding of the costs is non-negative.\nTheorem 1. The TD-UCB mechanism is DSIC and IR.\nProof. IR is immediate and follows from the definition of the payment scheme of the mechanism (eq. (5)). We prove the DSIC property by examining different possible scenarios of allocation for an agent. In each of these scenarios, we compute the utilities with truthful bids as against strategic misreports of bids.\nFor performing any job t, utility of a worker i is defined as follows.\nui(bi, b−i; ci) = pi(bi, b−i)− ci × xPSTi (bi, b−i) (6)\nwhere xPSTi (bi, b−i) and pi(bi, b−i) are the allocation and the payment to the worker i respectively. We consider the following three possible scenarios for the positioning of each worker i in the increasing order of ranking of the bids of the workers. We refer to the set of workers with non-zero task allocation as the active set in this proof. Throughout the proof, we denote by A the active set of allocated workers when agent i bids his true cost ci. We denote by A′ the active set when the agent bids untruthfully.\n• Case 1: i > kt In this scenario, when the agent bids truthfully,\nb1 < b2 < · · · < bkt︸ ︷︷ ︸ Bids fromA < · · · < bi−1 < ci < bi+1 < · · · < cN\nWhen the worker reports his cost truthfully (i.e, bi = ci), he does not receive any allocation and therefore ui(ci, b−i; ci) = 0. Now we consider the following two cases when he misreports his cost.\na) Overbid of cost (bi > ci) : Since xPSTi (ci, b−i; ci) = 0, a higher bid bi would only place the agent at a position oi(bi, b−i) ≥ oi(ci, b−i) = i in the revised ranking order. At the position oi(bi, b−i), again the allocation to him would be zero, that is, xPSTi (bi, b−i; ci) = 0 and thereby the utility from overbidding would be same as the utility from truthful bidding. Hence, he does not benefit from overbidding his cost.\nb) Underbid of cost (bi < ci): Here there could be two possibilities:\n(i) bi ≥ bkt : This scenario is identical to case 1(a) shown above and hence there is no incentive for the agent to bid in this manner.\n(ii) bi < bkt : With such a bid, the agent i is able to enter the active set of allocated workers. Let the position of the agent i in the new active set A′ be j, that is, oi(bi, b−i) = j, and the agent with the highest bid in A ′ is kt ′ ≤ kt.\nTherefore, by underbidding his cost, agent i is able to move the workers\np ∈ {kt ′ + 1, · · · , kt} out of the active set. We now show that such a bid does not fetch agent i an increased utility. As per the payment structure,\npi(bi, b−i) = kt∑ s=kt ′ xEXTi,s (bi, b−i)bs\n+ N∑ s=kt+1 xEXTi,s (bi, b−i)bs\n+ xPSTi (bi, b−i)− N∑ s=kt ′ ,s 6=i xEXTi,s (bi, b−i)  c (7) The second term in eq. (7) is zero, this is due to the fact that in absence of agent i {1, 2, . . . , kt} can complete the current job t. Therefore, with even an underbid i has no externality on agents {kt + 1, . . . , n}. The third term in eq. (7) is also zero as the allocation with truthful bidding was enough to complete the job t by agents {1, 2, . . . , kt}. Hence in absence of i the allocation with underbid xPSTi (bi, b−i) is met by the externality sum. By underbidding, the agent i is therefore able to obtain the portions of the job which would have been allocated to s ∈ {kt ′ + 1, · · · , kt}. For all such agents s, xEXTi,s (bi, b−i) > 0, since in the absence of i, these agents would have received an allocation. But note that bs < ci and so these agents contribute towards a negative utility. Therefore the net utility ui(bi, b−i; ci) < ui(ci, b−i; ci).\n• Case 2: i = kt. When agent i bids truthfully, the active set is as follows:\nb1 ≤ · · · ≤ bj ≤ · · · ≤ ci︸ ︷︷ ︸ Bids fromA ≤ bi+1 · · ·\nand the payment to agent i\npi(ci,b−i) = N∑ s=i+1 xEXTi,s (ci, b−i)bs\n+ ( xPSTi (ci, b−i)− n∑ s=i+1 xEXTi,s (ci, b−i) ) c (8)\na) Overbid of cost (bi > ci): Here we look at two possible values of the range of the bids.\n(i) An overbid such that agent i no longer belongs to the active set A′: At the position oi(bi, b−i), the allocation to him is zero, that is, x PST i (bi, b−i; ci) =\n0 and thereby the utility from overbidding would be less than the utility\nfrom truthful bidding. Hence, he does not benefit from overbidding his cost in this manner.\n(ii) An overbid such that agent i remains in the active set but brings in other higher cost agents into the active set : Suppose the active set A′\ncontains the agents {i + 1, · · · , p} in addition to the set A, such that, without loss of generality,\nb1 < · · · < bi−1 < bi+1 < · · · < bp < bi︸ ︷︷ ︸ Bids fromA′ < bp+1 < · · ·\nThe payment to agent i with overbid is,\npi(bi,b−i) = N∑\ns=p+1\nxEXTi,s (bi, b−i)bs\n+ (xPSTi (bi, b−i)− n∑\ns=p+1\nxEXTi,s (bi, b−i))c (9)\nSince the agents {i+ 1, · · · , p} have moved before i in the ordering of the bids, those agents do not contribute to pi(bi, b−i) further. However, for the agents s ∈ {p + 1, · · · , N}, xEXTi,s (bi, b−i) = xEXTi,s (ci, b−i) because the same proportion of job must be reassigned to the agent s when i bids bi as well as when i is truthful. The first term in eq. (8) therefore strictly exceeds first term in eq. (9). We now show that the second terms in eq. (8) and eq. (9) are equal. Observe that,\nxPSTi (ci, b−i) = x PST i (bi, b−i) + p∑ s=i+1 xPSTj (bi, b−i)\n= xPSTi (bi, b−i) + p∑ s=i+1 xEXTi,s (bi, b−i)\nA simple substitution for xPSTi (bi, b−i) in eq. (9) shows that the second terms in eq. (8) and eq. (9) are equal. Therefore the overall payment pi(bi, b−i) < pi(ci, b−i) and further ui(bi, b−i; ci) < ui(ci, b−i; ci).\nb) Underbid of cost ( bi < ci): Note that in this scenario, there are the following two possibilities.\n(i) The active set A′ = A. The agent i moves to a new position j, that is, oi(bi, b−i) = j. Without loss of generality, we can consider that the agent with the highest bid in A′ is now agent i − 1. The ordering of the agents is now,\nb1 ≤ · · · ≤ bj−1 ≤ bi ≤ bj ≤ bi−1︸ ︷︷ ︸ Bids fromA′ ≤ bi+1 · · · .\nBy our payment structure,\npi(bi,b−i) = N∑ s=i−1 s 6=i xEXTi,s (bi, b−i)bs\n+ (xPSTi (bi, b−i)− N∑\ns=i−1 s 6=i\nxEXTi,s (bi, b−i))c (10)\nSince the active set remains the same in spite of underbidding, ∀s, i+ 1 ≤ s ≤ N , xEXTi,s (bi, b−i) = xEXTi,s (ci, b−i) and in addition, xEXTi,i−1(bi, b−i) > 0, but, bi−1 < ci. Therefore, the first term in eq. (8) exceeds the first term in eq. (10). We also know that,\nxPSTi (bi, b−i) = x PST i (ci, b−i) + x EXT i,i−1(bi, b−i) (11)\nsince the additional allocation that i gets due to an overbid would be allocated to the last agent i − 1 in A′, in the absence of i. A simple substitution in eq. (10) shows that the second terms in eq. (8) and eq. (10) are equal. Therefore ui(bi, b−i; ci) < ui(ci, c−i; ci). (ii) The active set A′ due to underbidding by agent i is smaller than the active set A due to truthful bidding by agent i: This means that some agents get removed from A. Suppose the agents s ∈ {j + 1, · · · , i − 1} get pushed out in the active set A′. Then by a similar argument as in the case 2 (b) (i) above, xEXTi,s (bi, b−i) > 0, but bs < ci. Therefore these agents contribute towards a negative utility and hence, ui(bi, b−i; ci) < ui(ci, c−i; ci).\n• Case 3: i < kt\na) Overbid of cost (bi > ci): If the agent i bids a higher cost, the position of i in the ranking order changes to one of the following.\n(i) i ≤ oi(bi, b−i) < kt: The allocation to the worker remains the same as when he is truthful, that is, xPSTi (bi, b−i) = x PST i (ci, b−i). Our pay-\nment structure ensures that the payment pi(bi, b−i) = pi(ci, b−i) and hence ui(bi, b−i; ci) = ui(ci, b−i; ci). (ii) oi(bi, b−i) = kt: In this case, agent i ends up losing a part of x PST i (ci, b−i)\nto the worker kt. This scenario is analogous to Case 2 (a) (ii) where a worker who bids truthfully would have been at the last position kt, but by overbidding ends up sharing his allocation with other agents. Therefore ui(bi, b−i; ci) < ui(ci, b−i; ci). (iii) oi(bi, b−i) > kt: Here, agent i does not receive any allocation and thereby his payment as well as utility are both zero.\nb) Underbid of cost (bi < ci): Upon bidding a lower cost, the agent moves further up in the ranking order, that is oi(bi, b−i) ≤ i. The allocation also does not change, that is, xPSTi (bi, b−i) = x PST i (ci, b−i). Our payment structure ensures that the pay-\nment pi(bi, b−i) = pi(ci, b−i) and hence ui(bi, b−i; ci) = ui(ci, b−i; ci).\nFuture rounds: If the agent i ignores the loss incurred in the current job t and chooses to manipulate the current bid for future utility, the resulting argument rolls back to one the above three cases."
    }, {
      "heading" : "6 Regret Analysis",
      "text" : "In the strategic as well as the non-strategic settings, the underlying optimization problem involves parameters that are learnt in tandem. Hence regret is an important notion which we analyse in this section. Following are some relevant definitions. A problem instance in this space is characterized by a set of crowd agents N , the vector c of their costs, the mean vectors (ρ, β), and the design parameters – Deadline(D), accuracy (ε).\nDefinition 5. Optimal worker set: For a problem instance with all the parameters known, in the solution to the optimization problem of eq. (2), we refer to the set of agents allocated non-zero fraction of the job as the the optimal worker set.\nDefinition 6. Optimal allocation: We refer to the solution of eq. (2) as the optimal allocation.\nDefinition 7. ∆-Separation: Let k∗ be the agent in the optimal worker set with the highest bid. In the optimal allocation (social welfare maximizing), all workers’ allocation except k∗ would meet the constraints in eq. (2) with equality. We refer to the ∆-separation as the additional fraction of the job which agent k∗ can take without violating any of the constraints. As all the stochastic parameters in this space are continuous, almost surely ∆ > 0.\nDefinition 8. Regret: A learning mechanism in this space suffers a loss in social welfare due to either a) non-optimal set selection or b) due to suboptimal allocation within the optimal set. Formally, regret of a mechanism A, is given by\nR(A) = T n∑ i=1 cix ∗ i − T∑ t=1 n∑ i=1 cix (A) i (t),\nwhere x (A) i (t) is the allocation to the agent i for the job t by the mechanism A.\nWe use the truncated empirical estimator within our Robust UCB scheme. Through an invocation of the Bernstein inequality, we have, with high confidence (probability > 1− t−4 for the tth job), the true mean lies within the Robust UCB and LCB indices(see Lemma 1 in [5]). With enough samples, the symmetric indices of the Robust UCB scheme shrinks small enough so that no additional agents than the optimal set are required to meet the spill-over even due to the pessimistic strategy used.\nTheorem 2. The TD-UCB mechanism selects an optimal set after the job t′ ∈ O(log T ).\nProof. We denote k∗ as the costliest agent in the optimal set. Let x∗ = {x∗1, . . . , x∗k∗} denote the allocations when the means are known. Consider ∆,\n∆ = min(D,βk∗ log(\n1 1−ε))\nρk∗ − xk∗(t, β)\n∆ denotes the additional fraction of work the agent k∗ can take up without violating the constraints. Following is a sufficient condition on t when the set selected by the pessimistic estimate matches the optimal set.\nNeed to get :x∗i (ρ, β)− xPSTi (t) ≤ ∆n , ∀i ≤ k ∗ (12)\nWe denote ρRI(t) and βRI(t) as the Robust UCB indices of the JCT and the TTF of a worker in the active set. Recall, the active set for job t denotes an agent allocated a non-zero fraction of the job. The expression for the pessimistic allocation ∀i ∈ {1, . . . , k∗} at job t is given by\nxPSTi (t)× (ρ̂i + ρRI) = min ( D, (β̂i − βRI) log ( 1\n1− ε\n)) (13)\nThe allocation xPSTi is determined by equality in eq. (13) whenever the set chosen is not optimal. We analyse this allocation via two cases to determine the job t when condition in eq. (12) is met. Case (i): xPSTi (t)× (ρ̂i + ρRI) = D or xPSTi (t) = Dρ̂i+ρRI Consider,\nx∗i (ρ, β)− xPSTi (t)\n= min\n( D,βi log ( 1\n1−ε )) ρi − D ρ̂i + ρRI\n≤ D ρi − D ρ̂i + ρRI ≤ D ρ̂i − ρRI − D ρ̂i + ρRI [∵ w.h.p. ρi ≥ ρ̂i − ρRI] ≤ 2DρRI ρ2\nIn the current case, we have that the sufficiency condition is met whenever ρRI ≤ ∆ρ2\n2nD . In terms of the job t, through the expression of robust UCB index ρRI, the case is met whenever,\nt ≥ 336uρn 2D2 log(T )\n∆2 ρ4 (14)\nwhere uρ is an upper bound on the second moment of completion time (can be shown via easy computation). Case (ii): xPSTi (t)×(ρ̂i+ρRI) = (β̂i−βRI) log ( 1 1−ε ) or xPSTi (t) = β̂i−βRI ρ̂i+ρRI log ( 1 1−ε ) . Unlike ρi where samples are obtained for every t, the samples from surrogate are obtained after multiple (yet finite due to bounded β) jobs. To simplify our analysis, we consider as if a sample of βi is obtained for every job, the difference due to this simplification is only within constant factors. Consider,\nx∗i (ρ, β)− xPSTi (t)\n= min\n( D,βi log ( 1\n1−ε )) ρi − β̂i − βRI ρ̂i + ρRI log ( 1 1− ε ) ≤ ( βi ρi − β̂i − βRI ρ̂i + ρRI ) log ( 1 1− ε )\n≤ ( β̂i + βRI ρ̂i − ρRI − β̂i − βRI ρ̂i + ρRI ) log ( 1 1− ε )\nW.h.p ρi ≥ ρ̂i − ρRI and βi ≤ β̂i + βRI. As the surrogate observes at most one sample for every sample of ρi, we have βRI ≥ ρRI\n≤ βRI × 2ρ̄+ 2β̄\nρ2 log\n( 1\n1− ε\n)\nThis gives us that the sufficiency condition is met whenever βRI ≤ ∆ρ2\nn(2ρ̄+2β̄) log( 11−ε) .\nIn terms of the round t, the sufficiency condition is met whenever\nt ≥ 64n2uβ log(T )(2ρ̄+ 2β̄)\n2 log2 (\n1 1−ε ) ∆2 ρ4\n(15)\nwhere uβ is an upper bound on the second moment on the TTF. From eq. (13) and eq. (15), the optimal set is chosen after O(log(T )) online jobs.\nAs mentioned earlier, the regret in this setting arises first out of sub-optimal set selection and thereon out of sub-optimal allocation. Through theorem 2, we bound the number of jobs where sub-optimal set is chosen. The following theorem establishes the asymptotic efficiency of our learning scheme.\nTheorem 3. Average regret of TD-UCB mechanism approaches zero asymptotically.\nProof: WLOG due to Theorem 2, we will consider the case where the active set is the optimal set. Let k∗ be the last member in the active set. The average regret, for the job t, is then given by\nRavg,t = n∑ i=1 ck ∗ (x∗i − xPSTi (t)) t\nThrough steps similar to the proof of theorem 2, we have for a job t for an agent i either\nck ∗ (x∗i − xPSTi (t)) t ≤ 2DρRI t× ρ2\nor ck ∗ (x∗i − xPSTi (t)) t ≤ βRI t × 2ρ̄+ 2β̄ ρ2 log\n( 1\n1− ε\n) .\nAs both βRI/t and ρRI/t approach zero, we have\nlim t→∞ Ravg,t = 0."
    }, {
      "heading" : "7 Simulations",
      "text" : "We have shown theoretical guarantees on the regret in the asymptotic sense. In practice, the constants pertaining to the same are unknown. Also, our algorithm focusses on the optimization of social welfare. However, the payments to the workers involve externality to the workers which depend on the Robust UCB learning scheme. The learning scheme does not provide guarantees on the stochastic parameters for the sub-optimal workers. But the payments to the optimal worker set may also involve these parameters of the sub-optimal workers. Hence the simulations help us to form a fair idea about the same. Therefore, in order to investigate the efficacy of our algorithm in practical terms, we tested our method on synthetically generated datasets.\nWe simulated a set of 400 diverse workers with different costs, completion times and error rates. We fixed the following values, ρ = 50, ρ = 100, β = 25,\nβ = 35, c = 10 and c = 100. Out of the 400 workers, 250 high performing workers were simulated with ρ values uniformly sampled from [50, 75], β values uniformly from [30, 35] and costs from [10, 50]. A set of 150 mediocre\nworkers were also simulated. These workers were simulated with ρ values set to 100, β to 25, and costs to 100. The deadline D was fixed as 50 and the error probability threshold (ε) for an agent allocated a task was set to 0.01.\nWe checked the performance of our algorithm for a total of 106 jobs. The baseline used for comparison is the omniscient greedy allocation scheme with the parameters β and ρ known. We refer to this baseline as ‘Optimal’. In the context of payments, the ‘Optimal’ baseline refers to the minimum payment for inducing truthful reports for the greedy allocation aware of the means.\nWith increasing number of jobs, we observed that the total payment reduced and the negative social welfare decreased. The plots for the same are shown in fig. 2(a) and fig. 2(b). The performance of TD-UCB improves towards ‘Optimal’ when executed over more jobs. This shows that the learning improves over time and converges to the optimal set / optimal allocation."
    }, {
      "heading" : "8 Future Work",
      "text" : "Following are some directions for future work.\na) The derivation of theoretical bounds on the payments to the workers is still an open question. It would be interesting to see how these learned values affect the payment.\nb) Investigate the use of other models for the JCT and TTF. For instance, the error committed by a worker could be modelled as a Bernoulli random variable the bias of which could depend on the time to completion. Such models would pose more challenges for the learning due to the interdependency between the stochastic parameters.\nc) In our formulation of the problem in eq. (2), we have posed the constraint on meeting the deadline as a requirement to be met in expectation. We would also be interested in satisfying the constraints in a probabilistic sense."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Journal of Machine Learning, 47(2-3): 235–256",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Characterizing truthful multiarmed bandit mechanisms: extended abstract",
      "author" : [ "M. Babaioff", "Y. Sharma", "A. Slivkins" ],
      "venue" : "Tenth ACM Conference on Electronic Commerce, pages 79–88. ACM",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A truthful budget feasible multi-armed bandit mechanism for crowdsourcing time critical tasks",
      "author" : [ "A. Biswas", "S. Jain", "D. Mandal", "Y. Narahari" ],
      "venue" : "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning, 5(1):1–122",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bandits with heavy tail",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "IEEE Transactions on Information Theory, 59(11):7711–7717",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Novel mechanisms for online crowdsourcing with unreliable",
      "author" : [ "P. Chandra", "Y. Narahari", "D. Mandal", "P. Dey" ],
      "venue" : "strategic agents. In Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 1256–1262",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The price of truthfulness for payper-click auctions",
      "author" : [ "N.R. Devanur", "S.M. Kakade" ],
      "venue" : "Tenth ACM Conference on Electronic Commerce, pages 99–106",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multi-armed bandit with budget constraint and variable costs",
      "author" : [ "W. Ding", "T. Qin", "X.-D. Zhang", "T.-Y. Liu" ],
      "venue" : "AAAI, pages 232–238",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "What’s the right price? pricing tasks for finishing on time",
      "author" : [ "S. Faradani", "B. Hartmann", "P.G. Ipeirotis" ],
      "venue" : "Human Computation, volume 11, pages 26–31",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Scalable mechanism design for the procurement of services with uncertain durations",
      "author" : [ "E. Gerding", "S. Stein", "K. Larson", "A. Rogers", "N.R. Jennings" ],
      "venue" : "Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1, pages 649–656",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An incentive compatible multi-armed-bandit crowdsourcing mechanism with quality assurance",
      "author" : [ "S. Jain", "S. Gujar", "S. Bhat", "O. Zoeter", "Y. Narahari" ],
      "venue" : "CoRR, abs/1406.7157",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Budget-optimal crowdsourcing using low-rank matrix approximations",
      "author" : [ "O.S. Karger David R", "S. Devavrat" ],
      "venue" : "In 49th Annual Conference on Communication, Control, and Computing (Allerton),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Learning from crowds",
      "author" : [ "V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy" ],
      "venue" : "The Journal of Machine Learning Research, 11:1297–1322",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Pricing mechanisms for crowdsourcing markets",
      "author" : [ "Y. Singer", "M. Mittal" ],
      "venue" : "Twenty Second Internation World Wide Web Conference, pages 1157–1166",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks",
      "author" : [ "L. Tran-Thanh", "M. Venanzi", "A. Rogers", "N.R. Jennings" ],
      "venue" : "Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient task sub-delegation for crowdsourcing",
      "author" : [ "H. Yu", "C. Miao", "Z. Shen", "C. Leung", "Y. Chen", "Q. Yang" ],
      "venue" : "Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "We overcome this challenge by devising a biparameter learning scheme based on the Robust UCB algorithm [5].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "In the non-strategic case, most of the work in crowdsourcing has focused on models for aggregating labels and building classifiers [13, 12].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "In the non-strategic case, most of the work in crowdsourcing has focused on models for aggregating labels and building classifiers [13, 12].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : "[9] look at the design of pricing schemes dependent on the completion times of the workers.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "[16].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[8] look at the budgeted multi-armed bandit problem where the two parameters stochastic costs and stochastic rewards are learnt.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] look at allocating indivisible tasks to strategic crowd workers under deadline constraints with the assumption that the reliability (in terms of completion of the task) of the agents is common knowledge and not estimated.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "Singer and Mittal [14] and Biswas et al.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "[3] look at pricing mechanisms in the presence of budget constraints and task completion deadlines.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "[15] look at crowdsourcing classification tasks with the goal of trading off cost and accuracy of the estimation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "min x (t) i ∈[0,1] T ∑",
      "startOffset" : 13,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "min xi∈[0,1] n ∑",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "More specifically, since ρi and βi are sub-exponential distributions, we appeal to the Robust UCB technique [5].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "While ψ-UCB algorithm [4] is a regret minimizing scheme for learning the mean of sub-Gaussian distributions, for heavy tailed distributions (e.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "log normal and exponential), Robust UCB has been shown to be regret minimizing [5].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "Earlier works have shown the non-triviality involved in the design of such learning mechanisms [2, 7].",
      "startOffset" : 95,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "Earlier works have shown the non-triviality involved in the design of such learning mechanisms [2, 7].",
      "startOffset" : 95,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "Through an invocation of the Bernstein inequality, we have, with high confidence (probability > 1− t−4 for the tth job), the true mean lies within the Robust UCB and LCB indices(see Lemma 1 in [5]).",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "Out of the 400 workers, 250 high performing workers were simulated with ρ values uniformly sampled from [50, 75], β values uniformly from [30, 35] and costs from [10, 50].",
      "startOffset" : 162,
      "endOffset" : 170
    } ],
    "year" : 2016,
    "abstractText" : "We study a problem of allocating divisible jobs, arriving online, to workers in a crowdsourcing setting which involves learning two parameters of strategically behaving workers. Each job is split into a certain number of tasks that are then allocated to workers. Each arriving job has to be completed within a deadline and each task has to be completed satisfying an upper bound on probability of failure. The job population is homogeneous while the workers are heterogeneous in terms of costs, completion times, and times to failure. The job completion time and time to failure of each worker are stochastic with fixed but unknown means. The requester is faced with the challenge of learning two separate parameters of each (strategically behaving) worker simultaneously, namely, the mean job completion time and the mean time to failure. The time to failure of a worker depends on the duration of the task handled by the worker. Assuming non-strategic workers to start with, we solve this biparameter learning problem by applying the Robust UCB algorithm. Then, we non-trivially extend this algorithm to the setting where the workers are strategic about their costs. Our proposed mechanism is dominant strategy incentive compatible and ex-post individually rational with asymptotically optimal regret performance.",
    "creator" : "LaTeX with hyperref package"
  }
}