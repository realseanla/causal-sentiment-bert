{
  "name" : "1408.6350.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Definition and properties to assess multi-agent environments as social intelligence tests",
    "authors" : [ "Javier Insa-Cabrera", "José Hernández-Orallo" ],
    "emails" : [ "jinsa@dsic.upv.es", "jorallo@dsic.upv.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: social intelligence, artificial intelligence, multi-agent systems, cooperation, competition, interaction, game theory, teams, rewards, intelligence testing, universal psychometrics.\nar X\niv :1\n40 8.\n63 50\nv1 [\ncs .M\nA ]\n2 7\nA ug\n2 01"
    }, {
      "heading" : "Contents",
      "text" : ""
    }, {
      "heading" : "1 Introduction 3",
      "text" : ""
    }, {
      "heading" : "2 Background 3",
      "text" : ""
    }, {
      "heading" : "3 Defining Social Intelligence Universally 6",
      "text" : "3.1 Multi-agent environments and team rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Teams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.3 A formal definition of social intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.4 Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
    }, {
      "heading" : "4 Properties about social intelligence testbeds 12",
      "text" : "4.1 Boundedness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Interactivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.3 Non-neutralism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.4 Secernment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.5 Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.6 Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.7 Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.8 Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.9 Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.10 Summary of properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    }, {
      "heading" : "5 Degree of compliance of several multi-agent and social scenarios 26",
      "text" : "5.1 Graphical analysis for the properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.2 Matching pennies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.3 Prisoner’s dilemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 5.4 Predator-prey (Pursuit game) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.5 Pac-Man . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5.6 RoboCup Soccer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"
    }, {
      "heading" : "6 Conclusions and Future work 48",
      "text" : ""
    }, {
      "heading" : "A Matching Pennies properties 53",
      "text" : "A.1 Action Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 A.2 Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 A.3 Fine Discrimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 A.4 Strict Total Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 A.5 Partial Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 A.6 Slot Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 A.7 Competitive Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84"
    }, {
      "heading" : "B Prisoner’s Dilemma properties 86",
      "text" : "B.1 Action Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 B.2 Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 B.3 Fine Discrimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 B.4 Strict Total Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 B.5 Partial Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 B.6 Slot Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 B.7 Competitive Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106"
    }, {
      "heading" : "C Predator-prey properties 108",
      "text" : "C.1 Action Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 C.2 Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 C.3 Fine Discrimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 C.4 Strict Total Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 C.5 Partial Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 C.6 Slot Reward Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 C.7 Competitive Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 C.8 Cooperative Anticipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216"
    }, {
      "heading" : "1 Introduction",
      "text" : "Evaluation tools are crucial in any discipline as a way to assess its progress and creations. Artificial intelligence, as a discipline, lacks general, well-grounded and universally accepted intelligence measurement tools. In fact, artificial intelligence is a paradigmatic case of how useful these tools would be and how impeding this lack is. There are, of course, some tools, benchmarks and contests, aimed at the measurement of humanoid intelligence or the performance in a particular set of tasks. However, the evolution and state of the art of artificial intelligence is now more focussed towards social abilities, and here the measuring tools are still rather incipient.\nIn the past two decades, the notion of agent and the area of multi-agent systems have shifted artificial intelligence to problems and solutions where ‘social’ intelligence is more relevant. This shift towards a more social-oriented AI is related to the modern view of human intelligence as highly social, actually one of the most distinctive features of human intelligence over other kinds of animal intelligence. Some significant questions that appear here are then whether we are able to properly evaluate social intelligence in general (not only in AI, but universally) and whether we can develop measurement tools that distinguish between social intelligence and general intelligence.\nIn this paper, we address these questions by attempting a formalisation of social intelligence and some of its associated properties, such as social dependency and anticipation, and the properties a good test should have, such as discrimination, grading, reliability, boundedness, symmetry, efficiency, validity, etc. These properties help to: identify the components of social intelligence and its varieties; make clear that the mere appearance of other agents does not make a context social; pave the way for the analysis of whether many social environments, games and tests found in the literature are useful for measuring social intelligence. In particular, we will analyse a representative selection of environments, some usual in game theory, such as matching pennies and the prisoner’s dilemma, and some more sophisticated (and realistic) social scenarios, such as the predator-prey (a pursuit game), Pac-Man or RoboCup Soccer. We will see whether they meet the properties a social intelligence testbed should have.\nOne side question that we will try to analyse here is whether social intelligence can be fully separated from general intelligence or, conversely, whether general intelligence can be seen as a special case of social intelligence where the presence and intelligence of other agents is not so relevant.\nThe paper is organised as follows. Section 2 gives an overview of the approaches to social intelligence from several disciplines. Section 3 presents a formal, parametric definition of social intelligence and specifies how a test can be derived from the definition. Section 4 discusses several properties that are needed (or desirable) for a good test of social intelligence. Section 5 discusses current social environments and games in AI, and their suitability to evaluate social intelligence following the definition and the properties. Finally, section 6 closes the paper with some discussion and future work."
    }, {
      "heading" : "2 Background",
      "text" : "Social intelligence (and its true distinction from general intelligence) has been a matter of study for many years. Many definitions have been proposed such as the “ability to understand and manage men and women, boys and girls – to act wisely in human relations” [56], the “ability to get along with others” [41], the “facility in dealing with human beings” [60], or more specific definitions including “[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers” [58]. Nonetheless, none of these definitions is sufficiently formal and operational to provide a clear measurement procedure. In fact, all these definitions require the definition of many other new concepts that appear in the definition.\nDespite the ambiguity of what social intelligence is, many tests have been proposed to measure social intelligence in humans (see [62] for a survey). Typically there are two kinds of tests: 1) some use paper and pencil tests to measure social knowledge, and 2) others use more real situation tests (such as viewing photographies) in order to find out how people react in social situations. Some examples of these tests propose storylines which must be ordered to make sense, find the best end to a given joke, or select a correct emotion to a given face. However, there is a third, but unusual way: to measure social intelligence in terms of the definitions above, by confronting a human against other humans and see whether the subject deals with them or get along well. Apart from practical reasons that make this approach more difficult for testing, there are\nimportant questions, such as the selection and role of the other humans in order to make the test objective and effective.\nBut social intelligence is not only present in humans. Other animal species have also demonstrated this kind of intelligence. The evaluation of animals is more difficult, as we cannot ask them to perform a test as we do with humans, so the third type of tests is more common in this setting. Also, tests include some food as rewards in order to motivate the animals to perform the tasks. This is the same configuration as in reinforcement learning (RL) [54], where rewards are provided in order to encourage agents to perform tasks. In social intelligence tests for animals, especially for those focussing on cooperation, animals must obtain some food or reward that cannot be obtained by one individual alone, but two or more animals interacting are needed to get their reward. Some of the capabilities evaluated with those tests are their predisposition to deal with others, and their selfishness or altruism.\nAlthough these tests measure some aspects of social intelligence, many have been devised to evaluate social intelligence for a particular species and for very specific tasks. In these tests, it is highly debatable whether the tasks are representative of a broader view of social intelligence. Also, it is usually very difficult to compare the results with those of other species. Fortunately, there have been some exceptions to this (species) specialisation, and they are proliferating in the past decade. For instance, some recent work has shown that social abilities can be compared in a systematic way between human children and apes [28, 29].\nEven in the cases when the tests are generalised, they are still composed of a set of tasks that have been associated to social cognition indirectly, by observation or correlation, according to decades of experiments in the comparative cognition literature (see, e.g., [59, 51]). As a result, they cannot directly relate to the common definitions of social intelligence. For instance, one typical task used in social intelligence is to establish eye contact or to recognise oneself in a mirror (see, e.g., [52, pp. 452-453]). These tasks do not seem to be derived from any definition of social intelligence.\nIn order to elude this gap, many studies in ethology, comparative cognition and psychology just focus on specific issues, such as competition, cooperation, symbiosis, communication, group/swarm abilities, etc. However, in these scenarios, the emphasis is usually put on detecting and observing some of these phenomena, rather than properly evaluating abilities. For instance, prey-predator interaction and behaviour have been studied from many different points of view (including game theory [43]), but it is not clear how the ability of each subject can be objectively evaluated, especially because the interaction depends on the cognitive abilities of both prey and predator. For instance, lions are well prepared to predict zebra’s movements and chase them, but they may be less able for other kinds of animals.\nDespite these difficulties, comparative cognition [59, 51] is more and more concerned about performing tests that compare the abilities of many different species and also the abilities of individuals of different species. From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test). To achieve this, such a test should be able to evaluate any level and spectrum of social intelligence, instead of focussing on the specific range and particular abilities of a single species.\nWhen thinking about social tests and making them more species-independent, we can take the most general perspective, which leads us to the consideration of machines as well. However, evaluating social intelligence in machines has been quite different to the assessment of human and animal social intelligence. Nonetheless, as occurs with animals, rewards or scores may be used as a measure of their performance and a way of giving feedback to make them show their abilities. Besides, environments must be presented in such a way that a machine can process the observations and perform a set of actions. This is done by providing them with sensors and actuators that interact with our physical world, or provide them with a logical or virtual environment.\nThe environments used in social tests for machines tend to represent tasks that the agents must perform by interacting with other agents, so the performance is calculated as their capability to successfully cooperate with and/or compete against them to achieve some goals (see [3, 38] for two testbeds in multi-agent environments). In this way, the evaluation is a simulation in a social context, which is more directly linked to the definition of social intelligence.\nIn the context of social sciences (stretching from economics to AI), game theory [43] has also studied the interaction of different agents in formalised structures (called games). For this purpose, game theory uses a formal approach to define a utility function, and the effort is made for finding the best strategy among all possible strategies, assuming that the rest of agents also try to obtain their best results following some kind of rational actions. Although game theory needs the interaction of several agents, the goal is not to evaluate social intelligence but rather to analyse how the agents (or just policies) behave in these games and whether they\nreach some kind of equilibrium. Several kinds of games try to represent or to analyse a variety of properties: cooperative or non-cooperative games, simultaneous or sequential games, normal-form or extensive-form games, zero-sum or general-sum games, and symmetric or asymmetric games [43, 42]. However, games that may have the most interesting properties or applications are not necessarily useful for testing. For instance, a game where equilibria are easy may be inappropriate if we want a discriminative test. Similarly, asymmetric games make it more difficult to assess agent performance, as they depend on the role each agent takes.\nOne important concept in game theory is the notion of zero-sum vs. non-zero-sum games. Zero-sum games are a particular set of games where a player’s gains (or losses) are equally balanced by the other players’ losses (or gains). These kinds of games are known as competitive games, since one’s gains reduces the gains (or increases the losses) of the other player(s), making the players having opposed interests. When a zero-sum game only has two players it becomes a pure competitive game. But zero-sum games can also contain cooperation in games with three or more players. Two players can cooperate in order to compete against a third or more players. As the number of players increase, cooperation becomes more important. In contrast, general-sum games are those games where the payoffs sum more or less than zero, and games can be cooperative even for two players. Finally, another particular feature in game theory is that environments are generally simple (without objects or spaces) and it is just the continuous interaction between agents that matters.\nMulti-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44]. Agents are usually evaluated according to their performance in some tasks interacting with other agents. Environments are usually selected to represent some particular problems for which techniques are developed and evaluated. However, these evaluations lack some important features. They do not evaluate social intelligence in a general way, but they are typically designed to evaluate one kind of task. However, the most important problem is that they usually require very specific abilities, or when they require many, it is not clear how to disentangle them. For instance, if a MAS setting requires both competition and cooperation to solve a problem, it is not always easy to select or gauge the degree of relevance of each one in order to give more relevance to competition over cooperation, or vice versa. Nonetheless, the major issue is that many capabilities other than social intelligence also contaminate the results, which makes many MAS scenarios unsuitable if we want to measure social abilities only.\nAs an alternative1, can we start from a formal definition of an ability and derive tests from it? This approach has been investigated for machine intelligence evaluation. Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17]. Dobrev [7] suggested that machine intelligence should be measured by evaluating agent performance in a range of worlds, an idea that was independently developed in [39] under the name “Universal Intelligence”. This definition treats intelligence as a general notion, calculating it as the performance of the agent in a wide range of environments. Following this definition, a framework to evaluate intelligence [27] and an environment following the framework [24] were proposed. In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations. One of the possible reasons is that these environments lacked the richness of interaction. From the formal definition, it is virtually impossible to randomly generate an environment that contains some kind of social behaviour. Therefore, in order to overcome some of the limitations of these tests, some other agents need to be included in the environment to generate social situations. This was the goal in [36], where other agents were directly included in the environment. Some simple experiments were performed to evaluate machine social behaviour in environments where the agents were forced to compete and/or cooperate with other agents. The results of these experiments showed the impact on agents’ performance when other agents are directly introduced in a test of general intelligence. These experiments were performed using the framework in [27], which was originally designed to evaluate general intelligence, by simply including other agents in the environment. Nonetheless, a general environment such as this one does not seem enough to evaluate social intelligence, since some abilities other than social intelligence are also evaluated in these kinds of environments. In order to measure social intelligence in isolation, we need to provide an appropriate environment class where only social intelligence is needed (or at least, where the degree of social intelligence needed can be fine tuned).\nIn this context, the key issue is to determine what kind of agents we must include in the test to interact with and what their roles are. This boils down to choosing a distribution of agents. However, in order to provide an environment with some rich social situations, we need first to know the level of social intelligence of the agents provided by the distribution. This circular problem is turned into a recursive one in [18], where different levels of distributions are recursively provided by constructing a new distribution of agents from a prior distribution\n1Not only as an alternative to MAS scenarios, but also to the Turing Test, CAPTCHAs and IQ tests (see [9] for a discussion).\nby selecting (or increasing the probability of) those agents with higher performance. However, it is not easy to derive a definition of social intelligence from here or a procedure to create environments that would be the base for social intelligence tests.\nOverall, there are many different approaches for the study and evaluation of social intelligence, but we lack a comprehensive theory, well-grounded tools and wide comparisons to better understand the problem and find better measurement devices."
    }, {
      "heading" : "3 Defining Social Intelligence Universally",
      "text" : "One way of reaching a universal definition of social intelligence is to consider more specific definitions and generalising them for any kind of subject. Thorndike’s definition of social intelligence refers to “men, women, boys and girs” [56]. So this approach would generalise this view with the variety of species in animal cognition, but also including machines, robots and other artificial systems. This is in the spirit of universal psychometrics [19], where we must consider any kind of agent (natural or artificial). Any of these systems can, in principle, be evaluated and can also be subject of interaction with the evaluee.\nThis can take us to definitions such as “performance of an agent in a wide range of environments while interacting with other agents” [36]. As a result, we see clearly that social intelligence is a relative property, where we need to specify these other agents (and the range of environments).\nWith this approach, we distinguish those traits that have positive consequences on the performance (rewards) from those that are associated to social intelligence but do not necessarily lead to better performance (such as being generous, open, extroverted, etc.). In other words, we understand that an agent is socially intelligent if it has the ability to perform better in a social environment, but not if it is very sociable but showing very poor performance. In the end, we want an operational definition such that its measurement can be directly linked to it, and not derived by some other traits that are usually associated to social intelligence in humans and animals.\nSo we must focus our attention on the specification of the set of environments used for measuring and, most especially, on the characteristics of other agents. Nonetheless, it is important to determine the role these agents take in the environment relative to the evaluated agent. For instance, the environment can be populated by very intelligent agents, but the possibilities of an evaluated agent to achieve its goals will depend on whether these agents are allies or enemies, or more generally if they are cooperative or competitive. The key issue is to establish whether the other agents goals and interests are compatible with one’s goals. The concept is complex, as alliances can be created and broken even if no clear teams are established from the beginning (and this is an interesting property of social intelligence). Nonetheless we have to consider the notion of role from the beginning and make it visible at the top, jointly with the kind of environment and the kind of agents.\nThese roles or alliances determine two major social behaviours: cooperation and competition. These are in fact linked to the issue that some agents share some goals while some other agents compete or are against other agents’ goals. If we think of rewards (or any other kind of utility function) as a general way of expressing goals, interests and even resources they share or compete for, we can distinguish two major kinds of social intelligence:\nDefinition 1. Competitive social intelligence is the capability to obtain the best performance in an environment where other agents compete for the same rewards.\nDefinition 2. Cooperative social intelligence is the capability to obtain the best performance in an environment where other agents share the same rewards.\nNote that both definitions are not exclusive, as there are environments where both competitive and cooperative behaviours are possible. This is similar to the several degrees of general-sum games in game theory. Nonetheless, it would be very useful to have some way to analyse competition and cooperation separately (as two main facets of social intelligence). How clear-cut this separation can be done is an open question, as both abilities are occasionally correlated. For instance, the creation of alliances in a purely competitive scenario leads to temporary or permanent cooperation, where the other agents are seen in an instrumental way.\nIn what follows, we will see how these informal definitions can be formalised and integrated."
    }, {
      "heading" : "3.1 Multi-agent environments and team rewards",
      "text" : "Before addressing a formal integration of definitions 1 and 2, we need to give a definition of (multi-agent) environment. An environment is a world where an agent can interact through actions, rewards and observations\nas seen in figure 1 (left). This general view of the interaction between an agent and an environment can be extended to multi-agent systems by letting various agents interact simultaneously with the environment as seen in figure 1 (right).\nA multi-agent environment is an interactive scenario with several agents. An environment accepting n agents defines n parameters (one for each agent) denoted as slots. We use i = 1, . . . , n to denote the agent slots. Ai is the action set for agent in slot i and a = (a1, . . . , an) ∈ A1 × · · · ×An is a joint action profile of the n agents in the available discrete set of actions. Oi is the observation set that the agent in slot i can perceive and Ri ⊆ Q represents the possible rewards obtained by the agent in slot i. Both joint observation and reward profiles are denoted as o = (o1, . . . , on) and r = (r1, . . . , rn) respectively, similarly as we did with actions. For each step, every agent must receive an observation oi ∈ Oi and a reward ri ∈ Ri, and perform an action ai ∈ Ai. We will use ai,k, ri,k and oi,k to respectively denote the action, reward and observation at step k for the agent in slot i. We use ak, rk and ok respectively to denote the joint actions, rewards and observations of all the agents at step k (i.e., a1 = (a1,1, . . . , an,1) represents the joint actions at step k = 1). The order of events is always: observation, action and reward. For example, a sequence of two steps in a multi-agent environment is then a string such as o1a1r1o2a2r2 and the string o1,1a1,1r1,1o1,2a1,2r1,2 denotes the sequence of observations, actions and rewards for the agent in slot 1.\nBoth the environment and the agents are defined as probabilistic measures. For the agent in slot i, the term π(ai,k|oi,1ai,1ri,1 . . . oi,k) → [0, 1] denotes its probability to perform action ai,k after the sequence of events oi,1ai,1ri,1 . . . oi,k. The observations provided by the environment to the agent in slot i also have a probabilistic measure ω(oi,k|o1a1r1 . . . ok−1ak−1rk−1) → [0, 1]. As with observations, rewards are provided to the agent in slot i depending on observations and actions on previous steps ρ(ri,k|o1a1r1 . . . okak) → [0, 1]. Note that the rewards obtained by each agent depend on the joint actions, observations and rewards of all the agents interacting in the environment, and not only on their own. A random agent (usually denoted by πr) in slot i is an agent that chooses its actions from Ai using a uniform distribution.\nWe use ĂKi (π, µ) to denote the distribution (a probability measure) of strings representing the sequences of actions that π performs in slot i of µ during K steps. If K is omitted, we assume that K is infinite, i.e., infinite sequences of actions for an endless episode. If agents and environment are deterministic (not stochastic) then this boils down to a probability measure giving probability 1 to one single string, the sequence of actions performed by π on µ.\nSimilarly, we use R̆Ki (π, µ) to denote the distribution of reward strings of π in slot i of µ during K steps. If K is omitted, we assume that K is infinite. Again, if neither agents nor environment are stochastic, this is just a string. In the general case, we use RKi (π, µ) to denote the expected average reward (or value) with a discount factor γ. For instance, for K = ∞ this is Ri(π, µ) , E( lim K→∞ ∑K k=1 γ k−1ri,k∑K k=1 γ k−1 ). Unless stated otherwise, we assume γ = 1."
    }, {
      "heading" : "3.2 Teams",
      "text" : "We need to address a characterisation of slots, such that we can specify how agents participate in the environment. This actually means that we need to decide how the environment distributes rewards among the agents. An easy possibility will be to make each agent get its rewards without further constraints over other\nagents’ rewards. With this configuration (e.g., general-sum games), both competition and cooperation may be completely useless for most environments, as the rewards are not limited or linked to the other agents. In contrast, if we set that the total set of rewards is limited in some way, we will foster competition, as happens in zero-sum games. But in any of these two cases cooperation will hardly take place. Alliances could arise sporadically between at least two agents in order to bother (or defend against) a third agent. However, with low levels of social intelligence this seems unlikely to happen. For this reason we need to find a way to make agents cooperate, or at least to make it more likely before any (sophisticated) alliance can emerge on its own. One possible answer is the use of teams, defined as follows:\nDefinition 3. Agent slots i and j are in the same team iff ∀k : ri,k = rj,k\nwhich means that all agents in a team receive exactly the same rewards. This differs from alliances, where the agents could receive different rewards. Note that teams are not alliances as usually understood in game theory. In fact, teams are fixed and cannot be changed by the agents. Also, we do not use the term alliance as we do not use any sophisticated mechanism to award rewards, related to the contribution of each agent in the team, as it is done with the Shapley Value [48]. We just set rewards uniformly.\nAt this moment, we are ready to define an environment with parametrised agents by only specifying their slots and their team arrangement.\nDefinition 4. A multi-agent environment µ accepting N(µ) agents (i.e., the number of slots in µ) is a tuple ⟨A,R,O, ω, ρ, τ⟩, where A, R, O represent the action sets, reward sets and observation sets respectively (i.e., A = A1×· · ·×AN(µ), R = R1×· · ·×RN(µ) and O = O1×· · ·×ON(µ)) and ω and ρ are the observation function and reward function respectively as defined in section 3.1. τ is a partition on the set of slots {1, . . . , N(µ)}, where each set in τ represents a team.\nNote that with this definition the agents are not included in the environment. For instance, noughts and crosses could be defined as an environment µnc with two agents, where the partition set τ is defined as {{1}, {2}}, which represents that this game allows two teams, and one agent in each. Another example is RoboCup Soccer [38], denoted by µrc, whose τ would be {{1, 2, 3, 4, 5}, {6, 7, 8, 9, 10}}, which represents that there are two teams, with slots {1, 2, 3, 4, 5} in the first team and slots {6, 7, 8, 9, 10} in the second team.\nOnce environments are defined, without including the agents, now we can define an instantiation for a particular agent setup. Formally, a team line-up l is a list of agents. For instance, if we have a set of agents Π = {π1, π2, π3, π4}, a line-up from this set could be l1 = (π2, π3). The use of the same agent twice is allowed, so l2 = (π1, π1) is also a line-up. We denote by µ[l] the instantiation of an environment µ with a line-up l, provided that the length of l is greater than or equal to the number of agents allowed by µ (if l has more agents, the excess is ignored). The slots of the environment are then matched with the corresponding elements of l following their order. For instance, for the noughts and crosses, an instantiation would be µnc[l1]. Note that different instantiations over the same environment would normally lead to different results. A line-up pattern l̇ is a list of agents where one or more elements are not instantiated. We can use instantiation to create more specific patterns or even to convert a pattern into a line-up. The instantiation of an agent π at position i on line-up pattern l̇ of length n is denoted by l̇ i← π, which is exactly l̇1:(i−1) · π · l̇(i+1):n, where l · m denotes the concatenation of lists l and m and lj:k denotes the elements in l from position j to k. This notation can be extended to instantiate several agents simultaneously using l̇ i,...,j← π1, . . . , πn to represent l̇ i← π1 · · · j← πn. Once a line-up pattern l̇ has all its elements instantiated becomes a line-up l. Note that environments can only be instantiated with line-ups, so first we need to instantiate all the elements from a line-up pattern to convert it to a line-up, and then use it to instantiate an environment. For instance, a line-up pattern for the set of agents Π could be l̇3 = (π3, ∗), where ∗ represents an element that is not instantiated, and l̇3 2← π4 instantiates position 2 with agent π4, converting the line-up pattern into the line-up l3 = (π3, π4). We will use Ln(Π) to specify the set of all the line-ups of length n with agents of Π, and L̇n−i,...,j(Π) to denote the set of all the line-up patterns of length n with agents of Π where positions i, . . . , j are not instantiated. For instance, L̇n−i(Π) defines the set of all possible line-up patterns {l̇1:i−1 · ∗ · l̇i+1:n} from the set of agents Π.\nWe will use line-up patterns with positions i, . . . , j not being instantiated to evaluate agents in these positions, while the rest of the line-up pattern will be instantiated with the agents they will have to interact with.\nwL denotes weights to line-ups formed with agents from a certain set Π, giving weights to the agents in the line-up and their positions. Similarly wL̇ denotes weights for line-up patterns formed with agents from Π. As line-ups and line-up patterns are clearly related, we will assume that wL̇ can be derived from wL.\nAssumption 1. The value of wL̇ for line-up patterns with only one element that is not instantiated is derived from wL as:\n∀i, n,Π, l̇ ∈ L̇n−i(Π) : wL̇(l̇) = ∑ π∈Π wL(l̇ i← π)\nand wL̇ for line-up patterns with 2 or more non-instantiated elements is recursively derived as: ∀i, . . . , j, n,Π, l̇ ∈ L̇n−i,...,j(Π) : wL̇(l̇) = ∑ π∈Π wL̇(l̇ i← π) = · · · = ∑ π∈Π wL̇(l̇ j← π)\nFinally, note that we can calculate the expected average reward for any agent in line-up l in an environment µ. We will use the notation Ri(µ[l]), which gives us the expected average reward of the ith agent in line-up l for environment µ (also in slot i). We can extend this notation to distributions as well (i.e. Ăi(µ[l]) for distribution of action sequences and R̆i(µ[l]) for distribution of reward sequences)."
    }, {
      "heading" : "3.3 A formal definition of social intelligence",
      "text" : "Having these ideas in mind we can now attempt a first definition of social intelligence. We first fix the line-up and vary on the possible environments.\nDefinition 5. We define the ability of an agent π interacting in line-up l which contains π at position i, over a set of environments M accepting at least i agents and at most |l| agents (being | · | the length of a list), weighting the environments by wM as:\nΥi(l,M,wM ) , ∑ µ∈M wM (µ)Ri(µ[l1:N(µ)]) (1)\nwhere |M | ≥ 1.\nAlternatively, we can think about a definition of the ability of an agent for a varying set of line-ups while fixing the environment.\nDefinition 6. We define the ability of an agent π in slot i, interacting in an environment µ accepting at least i agents, with a set of line-up patterns defined over agent set Π and wL̇ as a weight for line-up patterns for the environment µ:\nΥi(π,Π, wL̇, µ) , ∑\nl̇∈L̇N(µ)−i (Π)\nwL̇(l̇, µ)Ri(µ[l̇ i← π]) (2)\nwhere N(µ) ≥ 1 and if N(µ) > 1 then |Π| ≥ 1 otherwise |Π| ≥ 0.\nIn the definition we have used a weight wL̇ that depends on µ (i.e., wL̇(l̇, µ)). Note that wL̇ applies to l̇, giving weights to the agents (and their positions) such that the evaluated agent π interacts in the environment µ when it is located at position i. Note also that now l̇ has always N(µ) elements when instantiated, so now no upper-restriction exists over the number of slots of µ.\nIn fact, both wM and wL̇ could be integrated into a single weight for instantiated environments. However, we want to decouple agents and environments and use both of them as independent parameters. To make the agents and the environment independent we work with the next assumption.\nAssumption 2. If wL(l, µ) and wL̇(l̇, µ), where l ∈ LN(µ)(Π) and l̇ ∈ L̇ N(µ) −i (Π), are independent of µ then:\n∀l, µ : wL(l, µ) = wL(l) ∀l̇, µ : wL̇(l̇, µ) = wL̇(l̇)\nAssumption 2 gives the same weight to any line-up and line-up pattern whatever the environment. In what follows we will use wL(l) and wL̇(l̇) as a weight for line-up l and line-up pattern l̇ respectively.\nUnder the assumption 2 we can integrate both equations 1 and 2 as follows:\nDefinition 7. The social intelligence of an agent π in slot i, interacting with the class of agents Π with a weight for line-up patterns wL̇, in a set of environments M with a weight for environments wM is defined as:\nΥi(π,Π, wL̇,M,wM ) , ∑ µ∈M wM (µ) ∑\nl̇∈L̇N(µ)−i (Π)\nwL̇(l̇)Ri(µ[l̇ i← π]) (3)\nwhere |M | ≥ 1,∀µ ∈ M then N(µ) ≥ 1 and if ∃µ ∈ M |N(µ) > 1 then |Π| ≥ 1 otherwise |Π| ≥ 0.\nAnd summing the performance over all possible slots of the environments of M , weighting the slots of each environment by wS , we have:\nDefinition 8. The social intelligence of π interacting with the class of agents Π with a weight for line-up patterns wL̇, in a set of environments M with a weight for environments wM and a weight for slots wS is defined as:\nΥ(π,Π, wL̇,M,wM , wS) , ∑ µ∈M wM (µ) N(µ)∑ i=1 wS(i, µ) ∑\nl̇∈L̇N(µ)−i (Π)\nwL̇(l̇)Ri(µ[l̇ i← π]) (4)\nwhere |M | ≥ 1,∀µ ∈ M then N(µ) ≥ 1 and if ∃µ ∈ M |N(µ) > 1 then |Π| ≥ 1 otherwise |Π| ≥ 0.\nThis equation now removes the lower-restriction of the number of slots on the environments. Finally the positions of the agents in the line-up patterns can be assumed independent:\nAssumption 3. If wΠ(π, i) defines the weight for the agent π appearing at position i in a line-up or line-up pattern, we assume wΠ(π) to appear in all positions in a line-up or line-up pattern. Formally:\n∀π, i : wΠ(π, i) = wΠ(π)\nUnder the assumption 3, wL and wL̇ can be derived as a function of terms from wΠ. Finally, we assume that the probability of an agent in a line-up and line-up pattern is independent of its position.\nAssumption 4. We calculate wL as a product of agents weights wΠ as: ∀n,Π, l ∈ Ln(Π) : wL(l) = ∏\n1≤k≤n\nwΠ(lk:k)\nand wL̇ is calculated as a product of agent weights wΠ as: ∀i, n,Π, l̇ ∈ L̇n−i(Π) : wL̇(l̇) = ∏\n1≤k<i\nwΠ(l̇k:k) ∏\ni<k≤n\nwΠ(l̇k:k) (5)\nThis assumption is also extended to line-up patterns with 2 or more elements not instantiated as well.\nProposition 1. Under assumptions 3 and 4, social intelligence as per equation 4 is also defined as:\nΥ(π,Π, wL̇,M,wM , wS) = Υ(π,Π, wΠ,M,wM , wS) =\n= ∞∑ j=1 j∑ i=1 ∑ l̇∈L̇j−i(Π)  ∏ 1≤k<i wΠ(l̇k:k) ∏ i<k≤j wΠ(l̇k:k)  ∑ µ∈Mj wM (µ)wS(i, µ)Ri(µ[l̇ i← π])\nwhere M j denotes all the environments in M with j agent slots, |M | ≥ 1, and if ∃µ ∈ M |N(µ) > 1 then |Π| ≥ 1 otherwise |Π| ≥ 0.\nProof. Definition 8 ranges over environments, their slots and then over line-up patterns, but we could express an equivalent equation by ranging over line-up patterns first and environments and their slots next:\nΥ(π,Π, wL̇,M,wM , wS) = ∑ µ∈M wM (µ) N(µ)∑ i=1 wS(i, µ) ∑\nl̇∈L̇N(µ)−i (Π)\nwL̇(l̇)Ri(µ[l̇ i← π]) =\n= ∑ µ∈M wM (µ) N(µ)∑ i=1 wS(i, µ) ∑\nl̇∈L̇N(µ)−i (Π)\n ∏ 1≤k<i wΠ(l̇k:k) ∏ i<k≤N(µ) wΠ(l̇k:k) Ri(µ[l̇ i← π]) = =\n∞∑ j=1 ∑ µ∈Mj wM (µ) j∑ i=1 wS(i, µ) ∑\nl̇∈L̇j−i(Π)\n ∏ 1≤k<i wΠ(l̇k:k) ∏ i<k≤j wΠ(l̇k:k) Ri(µ[l̇ i← π]) = = Υ(π,Π, wΠ,M,wM , wS) =\n∞∑ j=1 j∑ i=1 ∑ l̇∈L̇j−i(Π)  ∏ 1≤k<i wΠ(l̇k:k) ∏ i<k≤j wΠ(l̇k:k)  ∑ µ∈Mj wM (µ)wS(i, µ)Ri(µ[l̇ i← π])\nThis shows how we can parametrise the definition in terms of the weight of the other participants (wΠ) independently of their order in line-up patterns. For instance, the weight for each agent could depend on its (social) intelligence, provided we are able to estimate this value. The use of a product of weights makes sense if wΠ is a unit measure.\nThe interpretation of the above definition is the expected performance of agent π interacting with all possible line-up patterns generated using the set of agents Π, and in a set of environments M with π playing at all possible slots in each environment2.\nProposition 1 is not only useful for parametrising the definition in terms of the agents in isolation, but also because it decouples agents from environments. This makes sense in the context of (social) intelligence evaluation, as we want to consider other agents that are able to work in different environments, and not very specific agents that only work in one environment.\nDefinition 8 and its reformulation by proposition 1 integrates all kinds of social behaviour, as it does not distinguish between agents appearing in the same team or opponent teams. For instance, if we consider a set Π with very intelligent agents, some environments (and line-ups) will be easier if many of these agents appear in the same team, but will be harder if they appear in opponent teams. Also, the aggregation may consider many other environments where no social behaviour takes place. This means that the above equations are a skeleton for the definition, but we need to better analyse the pair (Π, wL̇) or (Π, wΠ) and the triplet (M , wM , wS)."
    }, {
      "heading" : "3.4 Tests",
      "text" : "A definition is not a test, most especially because many definitions range over infinite sets or an infinite number of steps. A test must be a finite procedure that can be feasibly applicable to an agent. For the moment, we will focus on non-adaptive tests, which are based on performing just a finite number of finite experiments or trials (episodes), which are independent of the previous ones.\nConsequently, a test is defined using the definition of Υ(π,Π, wL̇,M,wM , wS), where Π is sampled with some distribution, M is sampled with some distribution and the number of steps for each experiment is limited in some way. Sampling is understood to be without replacement when there is determinism (it does not make sense to repeat the same episode if the result is already known) but is understood to be with replacement for non-deterministic agents or environments. We denote by S ∼n [A]p a sample S of n elements from set A using probability distribution p for the powerset of A, i.e., for 2A. The use of a distribution over samples instead of a distribution over exercises gives more flexibility about the conditions that we could establish on the sampling procedure. For instance, we could define a sample probability such that high diversity is ensured (apart from high accumulated relevance of the exercises that are chosen) or such that a range of difficulties is covered. Keep\n2The above definitions could be simplified if for every environment µ1 and slot i in it, there is always an environment µ2 with exactly the same behaviour where slot i becomes 1. That means that we could easily get rid of the summation over slots and work just with slot 1 for agent π. In other words, this would be like considering that the evaluated agent always plays with slot 1. As we will discuss later on, if the environments are symmetric, this problem also vanishes. Another option is to consider a uniform distribution for slots.\nin mind that with this definition, the issues of with replacement or without replacement is re-understood as whether these samples allow repeated values or not. With this notation, we can give the following definition of test:\nDefinition 9. A test over Υ (definition 8 in the previous subsection), denoted by Υ̂[pΠ, pM , pS , pK , nE ], is a sample of nE episodes (or exercises) from all those summed in the definition, using agent distribution pΠ, an environment distribution pM , a slot distribution pS , and a distribution on the number of steps pK .\nΥ̂[pΠ, pM , pS , pK , nE ](π,Π, wL̇,M,wM , wS) , ηE ∑\n⟨µ,i,l̇⟩∈E wM (µ)wS(i, µ)wL̇(l̇)R\nK i (µ[l̇ i← π])\nwhere ηE normalises the formula with ηE = 1∑\n⟨µ,i,l̇⟩∈E wM (µ)wS(i,µ)wL̇(l̇) , K is chosen using probability distribution\npK and the episodes (or exercises) E are sampled as: E ∼nE  ⋃ µ∈M N(µ)⋃ i=1 {〈 µ, i, l̇ 〉 : l̇ ∈ L̇N(µ)−i (Π) } pE\nwith pE being a distribution on the set of triplets 〈 µ, i, l̇ 〉 based on pM , pS and pΠ.\nNote that we use pΠ for the line-up patterns, which could be the line-up pattern probability derived as the product of the probabilities of each agent in the line-up pattern following assumption 3, as in equation 5. As we will see, if the environments are symmetric, we can get rid of pS and wS and just evaluate for slot 1.\nIt is important not to confuse the probabilities of sampling the line-up patterns, environments, slots and number of steps (pΠ, pM , pS , pK) with any weight defined on them, in particular, the weights wL̇, wM and wS defined on line-up patterns, environments and slots respectively. Weights represent the relevance of an environment, their slots and line-up pattern for the definition (so it determines the abilities, roles and agents that have higher weight in the formula), while the distributions are just a way of sampling the usually large or infinite set of environments, slots and line-ups of agents. Weights and distributions might be related (or may be equal in order to ensure fast convergence to the actual value), but some other considerations may suggest that a less relevant case (low weight) can be sampled with high probability, as it may be highly representative or more robust, for instance. Actually, we want that a diversity of cases is sampled, rather than similar cases that will provide redundant information. This is why we use a distribution on 2A and not on A because otherwise we would not be able to measure how good (e.g., informative) a set of trials is. We will discuss this issue again in the following section in the context of reliability and efficiency."
    }, {
      "heading" : "4 Properties about social intelligence testbeds",
      "text" : "In order to evaluate social intelligence and distinguish it from general intelligence, we need tests where social ability has to be used and, also, where we can perceive its consequences. This means that not every environment is useful for measuring social intelligence and not every subset of agents is also useful. We want tests such that the evaluated agent must use its social intelligence to understand and/or have influence over other agents’ policies in such a way that this is useful to accomplish the evaluated agent’s goals. We also need situations where common general intelligence is not enough. In a way, we want to subtract (from the summation of all environments and line-up patterns) those problems (as defined by classes of environments and agents) where general intelligence is enough (and social intelligence is useless) and those where intelligence (of any kind, social or non-social) is useless.\nWe will investigate some properties that are hence desirable (or necessary) for a testbed of environments3 and agents to actually measure social intelligence. Some other properties are more of a practical nature, such as the degree of discrimination and grading of the environment, the symmetry of slots, its reliability and efficiency.\nHereinafter, we will differentiate two kinds of set of agents: the set of agents we want to evaluate, denoted by Πe, and the set of agents we want the environment to be populated with as opponents and team players,\n3In what follows, we will explore some properties that can be applied to sets of environments rather than single environments. A definition or test can be composed of just one environment if M has only one element (as for definition 8 having |M | = 1). Actually, we will give the definitions for one environment but they are easily extensible to a family or distribution of environments with a weight function used as a testbed.\ndenoted by Πo 4. On many occasions both sets can be set equal, but it will be useful to keep them as separate sets. Figure 2 gives a summary of the properties we will introduce in this section. They have different purposes and will reach different levels of formalisation. Actually, many of the properties (the quantitative ones) will be of the form Prop(Πe, wΠe ,Πo, wL̇, µ, wS), i.e., given these two sets Πe and Πo, and the weights for them (in the form of agent weight wΠe and line-up weight wL̇ respectively), they give a value for a given environment µ and slot weight wS .\nFinally, some of the properties below are presented for two slots but they could be extended to three or more slots as well. Similarly, all the following definitions are introduced for one environment, but they can be easily extended for a set of environments as well.\nNext we will analyse and formalise these properties."
    }, {
      "heading" : "4.1 Boundedness",
      "text" : "One property that we need to impose in order to make many of the previous definitions meaningful is that rewards must be bounded (otherwise, some summations will diverge). Any arbitrary choice of upper and lower bounds can be scaled to any other choice so, without loss of generality, we can assume that all of them are bounded between −1 and 1. Formally:\n∀i, k : −1 ≤ ri,k ≤ 1\nNote that they are bounded for every step. As Ri(·) is an average, then it is bounded as well if the rewards are bounded.\nHowever, bounded rewards do not ensure that the measurement from definition 8 is bounded. In order to ensure a bounded result we also need to consider that weights are bounded, i.e., there are constants cM , cS , cL̇ and cΠ such that:\n4Instead of using only one set for opponents and team players, it can also be extended by using two different sets; one for opponent players and another for team players.\n∀M : ∑ µ∈M wM (µ) = cM (6)\n∀µ : N(µ)∑ i=1 wS(i, µ) = cS (7)\n∀i, n,Πo : ∑\nl̇∈L̇n−i(Πo)\nwL̇(l̇) = cL̇ (8)\n∀Π : ∑ π∈Π wΠ(π) = cΠ (9)\nEquation 8 can also be applied for two or more non-instantiated slots and equation 9 is used when assumption 4 is made.\nA convenient choice would be to have cM = cS = cL̇ = cΠ = 1, and these weights would become unit measures (which should not be confused with the probabilities used in definition 9). The expression of wL̇ in terms of a product of wΠ make more sense if wΠ is a unit measure. With these conditions Υ and Υ̂ are bounded 5\nAn optional property that might be interesting occasionally is to consider environments whose reward sum is constant. Without loss of generality, we can take this constant to be zero, which leads to the well-known notion of zero-sum games in game theory.\nDefinition 10. An environment µ is zero-sum if and only if:\n∀k : N(µ)∑ i=1 ri,k = 0 (10)\nThe above definition may be too strict when we have environments with an episode goal at the end, but we want some positive or negative rewards to be given while agents approach the goal. A more convenient version follows:\nDefinition 11. An environment µ is zero-sum in the limit iff:\nlim K→∞ K∑ k=1 N(µ)∑ i=1 ri,k = 0 (11)\nNote that if we have teams, the previous definition could be changed in such a way that:\nlim K→∞ K∑ k=1 ∑ t∈τ ∑ i∈t ri,k = 0 (12)\nSo the sum of the agents’ rewards in a team (or team’s reward) does not need to be zero but the sum of all teams’ rewards does. For instance, if we have a team with agents {1, 2} and another team with agents {3, 4, 5}, then a reward (in the limit) of 1/4 for agents 1 and 2 will imply −1/6 for each of the agents in the other team.\nThe zero-sum properties are appropriate for competition between teams. In fact, if teams have only one agent then we have pure competitive environments. We can have both competition and cooperation by using teams in a zero-sum game, where agents in a team cooperate and agents in different teams compete. If we want to evaluate pure cooperation (with one or more teams) then zero-sum games will not be appropriate."
    }, {
      "heading" : "4.2 Interactivity",
      "text" : "By interactivity we mean the property that agents’ actions have implications on the actions (and rewards) of the other agents. This is a key property as the existence of several agents in an environment does not ensure, per se, any social behaviour. In fact, it is important to realise that the use of several agents and their arrangement into\n5Note that we are talking about the measure. For instance, Υ can be a measure that represents the, e.g., sigmoid function of an unbounded magnitude, easily recovered with a logit or probit function.\nteams does not ensure that some social behaviour can ever take place. Imagine a non-social environment, such as finding the way out of a maze with no other agents. Rewards depend on the agent finding the way out or not. While this is clearly non-social, we can use this environment as a building block and create an environment that takes two agents but makes them play separately on two equal mazes. We can generate rewards in at least four different ways: (1) we can give rewards separately without any modification on the outputs of the building blocks, (2) we can normalise them to a constant or a zero-sum (in the spirit of definition 1), (3) we can average both rewards and give them to both agents (in the spirit of definition 2) or (4) any other combination of the rewards, including a stochastic (non-deterministic) combination. Note that none of these four options would contain or foster any kind of social behaviour. In fact, no agent is aware of the other (apart from the effect on rewards). However, the rewards can get highly correlated (as in ways 2 or 3 above) and do so in a non-additive or functional way.\nThe explanation of why there is no social behaviour in this case is that there is no influence between the actions of both agents. As a result, the big issue about choosing social contexts is how to determine that an agent has influence on other’s actions. In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37]. Some other approaches have looked for some common information content between the peers. However, as pointed out by [12], “this may originate from a common source”, so common or mutual information is not sufficient for interaction to have taken place.\nSo we need a measure of interaction that is not based on correlation or common information content. However, the degree of influence that other agents may have on the actions of the agent we are evaluating is difficult to grasp; As environments and agents can be non-deterministic, changes can appear just randomly."
    }, {
      "heading" : "4.2.1 Action Dependency",
      "text" : "We need to take a different approach. The key idea defines interaction in terms of sensitivity to other agents or, in other words, whether the inclusion of different agents in the environment has an effect on what the evaluated agent does. One formalisation of this idea goes as follows:\nDefinition 12. The action dependency degree for evaluated agent π playing in slot i in environment µ with a class of opponents and team players Πo with a weight of line-up patterns wL̇, is given by:\nADi(π,Πo, wL̇, µ) , ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−i (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ăi(µ[u̇ i← π]), Ăi(µ[v̇ i← π])) (13)\nwhere ηL̇2 normalises the formula with ηL̇2 = 1∑\nu̇,v̇∈L̇N(µ)−i (Πo)|u̇ ̸=v̇ wL̇(u̇)wL̇(v̇)\n, ∆S is a divergence function between\nstring distributions, |Πo| ≥ 2, N(µ) ≥ 2 and ∃u̇, v̇ ∈ L̇N(µ)−i (Πo)|u̇ ̸= v̇, wL̇(u̇) > 0 and wL̇(v̇) > 0.\nNote that wL̇ can be written in terms of wΠo if we assume independence for the environment and agent positions (assumption 2 and 3), as we did in assumption 4. Note also that Ă returns a distribution of action sequences if the environment or any of the agents is non-deterministic. If ADi is high, then the proportion of cases where two team line-up patterns lead to different sequences of actions for π is high. This means that π is highly sensitive in this environment about who else is in the environment. Conversely, if we have that for many pairs of line-up patterns the sequences of actions of π are similar, this means that π’s actions are not affected by other agents.\nThe previous definition is relative to a distribution of line-up patterns on a population of agents, but it is given for a particular evaluated agent π. We may have that one evaluated agent can be very insensitive to line-up pattern changes, but other evaluated agents can be more sensitive in the same environment. If we want to generalise this for a class of evaluated agents Πe we have:\nDefinition 13. The action dependency degree for a set of evaluated agents Πe with associated weight wΠe playing in slot i in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nADi(Πe, wΠe ,Πo, wL̇, µ) , ∑ π∈Πe wΠe(π)ADi(π,Πo, wL̇, µ) (14)\nwhere |Πe| ≥ 1.\nThis definition is given only for slot i. Finally, we need to aggregate the action dependency degree for all slots:\nDefinition 14. The action dependency degree for a set of evaluated agents Πe with associated weight wΠe in environment µ with weight of slots wS , a class of opponents and team players Πo and a weight of line-up patterns wL̇, is given by:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) , N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) (15)\nwhere N(µ) ≥ 1.\nIt certainly remains to clarify what ∆S can be. For deterministic cases an edit distance can be used. However, for non-deterministic cases we need to find alignments between the distributions or aggregate strings into some prototypes and compare them. One simple approach for both the deterministic and non-deterministic cases could be based on comparing action frequencies (independently of their order) or, alternatively, n-grams. We will see some specific examples in section 5. Note that different ∆S functions may lead to different interpretations of action influence. For instance, there can be environments where a first few actions are interactive, but then no interaction takes places any more. In this case, the strings may be very different, but the degree, or more precisely, the timespan of interaction is small (like a butterfly effect)."
    }, {
      "heading" : "4.3 Non-neutralism",
      "text" : "The existence of interaction between agents does not ensure that these interactions are meaningful in terms of rewards. For instance, two agents can interact, but they may not affect each other’s rewards. This, in ecological terms, is known as ‘neutralism’. In fact, in ecology, given two species, there are seven possible combinations of positive, negative or no effect between them, leading to six forms of symbiosis [40]: neutralism (0,0), amensalism (0,-), commensalism (+,0), competition (-,-), mutualism (+,+), and predation/parasitism (+,-). In our case, as we want to characterise environments that may contain individuals (possibly more than two), we can simplify this to neutralism, cooperation (including commensalism and mutualism) and competition (including the rest). In other words, we want to analyse whether interaction has no effect on rewards, has a positive relation or a negative one."
    }, {
      "heading" : "4.3.1 Reward Dependency",
      "text" : "So, the first thing that we need to determine is whether there is a dependency in rewards. This is very similar to the action dependency seen above:\nDefinition 15. The reward dependency degree for evaluated agent π playing in slot i in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇, is given by:\nRDi(π,Πo, wL̇, µ) , ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−i (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(Ri(µ[u̇ i← π]), Ri(µ[v̇ i← π])) (16)\nwhere ηL̇2 normalises the formula with ηL̇2 = 1∑\nu̇,v̇∈L̇N(µ)−i (Πo)|u̸̇=v̇ wL̇(u̇)wL̇(v̇)\n, ∆Q is a divergence function for\nrational numbers, |Πo| ≥ 2, N(µ) ≥ 2 and ∃u̇, v̇ ∈ L̇N(µ)−i (Πo)|u̇ ̸= v̇, wL̇(u̇) > 0 and wL̇(v̇) > 0.\nNote that we use expected average rewards instead of a history of rewards. So, here the divergence compares numbers. For instance, we can use ∆Q(a, b) = 1 − δ(a, b), where δ is the Kronecker delta function (δ(a, b) = 1 if a = b and 0 otherwise). With this choice, the previous function would boil down to the probability that by taking two team line-up patterns (using a weight or distribution wL̇), after instantiating both with π in slot i, the expected average rewards of π are different. Another option could be relative absolute difference, i.e., ∆Q(a, b) = |a−b| |a|+|b| .\nNow, we can generalise this for a set of evaluated agents Πe:\nDefinition 16. The reward dependency degree for a set of evaluated agents Πe with associated weight wΠe playing in slot i in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nRDi(Πe, wΠe ,Πo, wL̇, µ) , ∑ π∈Πe wΠe(π)RDi(π,Πo, wL̇, µ) (17)\nwhere |Πe| ≥ 1.\nSo now we measure how dependent the rewards are in general (for any evaluated agent in Πe). The previous definition may slightly resemble the Shapley Value [48] in cooperative game theory, but here we are not concerned about how relevant each agent is in a team (whether its contribution is higher than the contribution of its teammates), but to see whether there is effect on the rewards.\nFinally, we aggregate for all slots:\nDefinition 17. The reward dependency degree for a set of evaluated agents Πe with associated weight wΠe in environment µ with weight of slots wS , a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) , N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) (18)\nwhere N(µ) ≥ 1."
    }, {
      "heading" : "4.3.2 Slot Reward Dependency",
      "text" : "Both definitions 14 and 17 are necessary as we can have reward dependency without action dependency and action dependency without reward dependency.\nNow, we are interested in telling the sign of this dependency, i.e., how much cooperative or competitive this is.\nDefinition 18. The slot reward dependency for evaluated agent π playing in slot i with slot j (with i ̸= j) in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nSRDi,j(π,Πo, wL̇, µ) , corrl̇∈L̇N(µ)−i (Πo) [wL̇(l̇)](Ri(µ[l̇\ni← π]), Rj(µ[l̇ i← π])) (19)\nwhere corrx∈X [w](a, b) is a weighted (w) correlation function between a and b for the elements generated by X, |Πo| ≥ 1 and N(µ) ≥ 2.\nAny correlation function can be used here (Pearson, Spearman, etc.). Clearly, if two slots are in the same team, from definition 3, we have that SRD is 1. In the case of a zero-sum game with only two teams, any two slots of different teams have a SRD of −1 (provided there is at least one ‘match’ which is not a tie). Note that as usual with correlation measures, if we have that two slots are reward independent, then SRD is 0. However, having SRD = 0 does not necessarily imply independency. This means that we need to calculate the reward dependency degree and then ask whether this dependency is positive or negative for pairs of slots.\nNow, we can generalise this for a set of evaluated agents Πe:\nDefinition 19. The slot reward dependency for a set of evaluated agents Πe with associated weight wΠe playing in slot i with slot j (with i ̸= j) in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nSRDi,j(Πe, wΠe ,Πo, wL̇, µ) , ∑ π∈Πe wΠe(π)SRDi,j(π,Πo, wL̇, µ) (20)\nwhere |Πe| ≥ 1.\nFinally we aggregate all combinations of pairs of slots:\nDefinition 20. The slot reward dependency for a set of evaluated agents Πe with associated weight wΠe in environment µ with weight of slots wS , with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nSRD(Πe, wΠe ,Πo, wL̇, µ, wS) , ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ)  (21)\nwhere ηS21 normalises the formula with ηS21 = 1∑N(µ) i=1 wS(i,µ) (∑i−1 j=1 wS(j,µ)+ ∑N(µ) j=i+1 wS(j,µ) ) , N(µ) ≥ 2 and ∃i, j|1 ≤ i ≤ N(µ), 1 ≤ j ≤ N(µ), i ̸= j, wS(i, µ) > 0 and wS(j, µ) > 0.\nIn practice, in order to evaluate social abilities, we require environments with high RD. Then, depending on the use of teams and normalisations, we can gauge whether we want to evaluate competition or cooperation, and have some positive SRD with some slots and some negative SRD with some other slots. This is easily obtained by using teams."
    }, {
      "heading" : "4.4 Secernment",
      "text" : "It is an important characteristic for a test to be able to give different values for different evaluated agents. Otherwise, if the results are the same (or very similar) for most evaluated agents, we get little information. In other words, we want tests (i.e., environment and set of agents populating it) to secern, to be discriminative. Although there are many approaches to the idea of discriminating power (see e.g., [26]), one simple notion that accounts for this concept quite well is the variance of results."
    }, {
      "heading" : "4.4.1 Fine and Coarse Discrimination",
      "text" : "In order to formalise this notion of variance (or number of different values) of the expected average rewards of the set of evaluated agents, we can just compare pairs of values as follows:\nDefinition 21. The fine discriminating power for evaluated agents π1 and π2 playing in slot i in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nFDi(π1, π2,Πo, wL̇, µ) , ∑\nl̇∈L̇N(µ)−i (Πo)\nwL̇(l̇)∆Q(Ri(µ[l̇ i← π1]), Ri(µ[l̇ i← π2])) (22)\nwhere ∆Q is a divergence function for rational numbers, N(µ) ≥ 1 and if N(µ) > 1 then |Πo| ≥ 1 otherwise |Πo| ≥ 0.\nThis measures the expected average reward divergence of two evaluated agents placed both in slot i in the same line-up patterns. If ∆Q is some kind of numeric difference (e.g., the absolute difference or the squared difference), then this measure would be similar to some kind of dispersion of expected average rewards (like a variance). If ∆Q equals 1 − δ (with δ being the Kronecker delta function) we have that this measures the number of times two different evaluated agents score differently.\nWe can generalise this for a set of evaluated agents Πe:\nDefinition 22. The fine discriminating power for a set of evaluated agents Πe with associated weight wΠe playing in slot i in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nFDi(Πe, wΠe ,Πo, wL̇, µ) , ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FDi(π1, π2,Πo, wL̇, µ) (23)\nwhere ηΠ2 normalises the formula with ηΠ2 = 1∑\nπ1,π2∈Πe|π1 ̸=π2 wΠe (π1)wΠe (π2) , |Πe| ≥ 2 and ∃π1, π2 ∈ Πe|π1 ̸= π2, wΠe(π1) > 0 and wΠe(π2) > 0.\nAgain, we can sum over all slots:\nDefinition 23. The fine discriminating power for a set of evaluated agents Πe with associated weight wΠe in environment µ with weight of slots wS , with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) , N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ) (24)\nwhere N(µ) ≥ 1.\nBeing able to discriminate in terms of pair of agents for each line-up pattern in an environment can be generalised with the overall result of a measure Υ by considering the aggregated values on this measure, namely:\nDefinition 24. The coarse discriminating power for evaluated agents π1 and π2 playing in slot i in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nCDi(π1, π2,Πo, wL̇, µ) , ∆Q(Υi(π1,Πo, wL̇, µ),Υi(π2,Πo, wL̇, µ)) (25)\nwhere Υi(π,Π, wL̇, µ) is defined in equation 2 and ∆Q is a divergence function for rational numbers.\nWe generalise this for a set of evaluated agents Πe:\nDefinition 25. The coarse discriminating power for a set of evaluated agents Πe with associated weight wΠe playing in slot i in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nCDi(Πe, wΠe ,Πo, wL̇, µ) , ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)CDi(π1, π2,Πo, wL̇, µ) (26)\nwhere ηΠ2 normalises the formula with ηΠ2 = 1∑\nπ1,π2∈Πe|π1 ̸=π2 wΠe (π1)wΠe (π2) , |Πe| ≥ 2 and ∃π1, π2 ∈ Πe|π1 ̸= π2, wΠe(π1) > 0 and wΠe(π2) > 0.\nAnd summing over all slots:\nDefinition 26. The coarse discriminating power for a set of evaluated agents Πe with associated weight wΠe in environment µ with weight of slots wS , with a class of opponents and team players Πo and a weight of line-ups pattern wL̇ is given by:\nCD(Πe, wΠe ,Πo, wL̇, µ, wS) , N(µ)∑ i=1 wS(i, µ)CDi(Πe, wΠe ,Πo, wL̇, µ) (27)\nwhere N(µ) ≥ 1.\nIn both fine and coarse discrimination the goal is to check if two evaluated agents obtain similar results. The difference resides at the level we analyse their results. While the fine checks the similarities for each line-up pattern, the coarse is more oriented to seeing the overall similarity."
    }, {
      "heading" : "4.4.2 Strict Total and Partial Grading",
      "text" : "An environment and set of agents populating it being discriminative when comparing evaluated agents does not mean that there is a gradation or order between the results for a set of evaluated agents. For instance, if we have three agents π1, π2 and π3 that we want to evaluate in a competitive environment with two agent slots and two teams, and we get that π1 scores better when interacts with π2, π2 scores better when interacts with π3 and π3 scores better when interacts with π1, then there is no way to establish a gradation for these three agents. Idealistically, we would like to have a strict total order, but this is unrealistic for many environments.\nSo the idea we will pursue is to evaluate how close an environment and set of agents populating it are to this ideal situation from the expected average rewards of the evaluated agents (without an aggregated rating system6):\n6A common approach is to create a rating when we have many experiments, as done with sport ratings, such as the ELO rating [13] in chess.\nDefinition 27. The strict total grading quality for evaluated agents π1, π2 and π3 playing in slots i and j (with i ̸= j) in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nSTGi,j(π1, π2, π3,Πo, wL̇, µ) , ∑\nl̇∈L̇N(µ)−i,j (Πo)\nwL̇(l̇)STOi,j(π1, π2, π3, l̇, µ) (28)\nwhere N(µ) ≥ 2, if N(µ) > 2 then |Πo| ≥ 1 otherwise |Πo| ≥ 0 and STOi,j(π1, π2, π3, l̇, µ) (where l̇ has all its elements instantiated except positions i and j) is 1 if there is a permutation of the three evaluated agents such that there is a strict total order in their expected average rewards when placed by pairs in l̇ interacting in environment µ in slots i and j, and 0 otherwise. Formally, it is 1 iff there is a permutation (π′1, π ′ 2, π ′ 3) such that: Ri(µ[l̇ i,j← π′1, π′2]) < Rj(µ[l̇ i,j← π′1, π′2]), Ri(µ[l̇ i,j← π′2, π′3]) < Rj(µ[l̇ i,j← π′2, π′3]) and Ri(µ[l̇ i,j← π′1, π′3]) < Rj(µ[l̇\ni,j← π′1, π′3]). For instance, if we have three agents a, b and c in an environment µ and line-up pattern l̇ for slots i and j, and their expected average rewards when placed by pairs in l̇ shows us that b < a, a < c and b < c, then we have STOi,j(a, b, c, l̇, µ) = 1 with the permutation (b, a, c). This property is related to reliability, which we will see later on.\nWe generalise this for a set of evaluated agents Πe:\nDefinition 28. The strict total grading quality for a set of evaluated agents Πe with associated weight wΠe playing in slots i and j (with i ̸= j) in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nSTGi,j(Πe, wΠe ,Πo, wL̇, µ) , ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STGi,j(π1, π2, π3,Πo, wL̇, µ)\n(29) where ηΠ3 normalises the formula with ηΠ3 = 1∑ π1,π2,π3∈Πe|π1 ̸=π2 ̸=π3 wΠe (π1)wΠe (π2)wΠe (π3) , |Πe| ≥ 3 and ∃π1, π2, π3 ∈ Πe|π1 ̸= π2 ̸= π3, wΠe(π1) > 0, wΠe(π2) > 0 and wΠe(π3) > 0.\nNow, if we aggregate all combinations of pairs of slots, we have:\nDefinition 29. The strict total grading quality for a set of evaluated agents Πe with associated weight wΠe in environment µ with weight of slots wS , with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) , ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  (30)\nwhere ηS21 normalises the formula with ηS21 = 1∑N(µ) i=1 wS(i,µ) (∑i−1 j=1 wS(j,µ)+ ∑N(µ) j=i+1 wS(j,µ) ) , N(µ) ≥ 2 and ∃i, j|1 ≤ i ≤ N(µ), 1 ≤ j ≤ N(µ), i ̸= j, wS(i, µ) > 0 and wS(j, µ) > 0.\nThe previous definition only considers strict total orders, and is useful to determine whether we can find a strict total order for the evaluated agents. However, this does not say much about the existence of grading ‘inconsistencies’, such as non-discriminative cases such as π1 = π2, π2 = π3 and π1 = π3 which, for the above definition, are considered in the same way as not ordering cases such as π1 > π2, π2 > π3 and π1 < π3. In order to distinguish these cases, we can give a new definition as follows:\nDefinition 30. The partial grading quality for a set of evaluated agents Πe with associated weight wΠe in environment µ with weight of slots wS , with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is defined as in definition 29 with the use of a partial order with ≤ instead of a strict total order with <. It is denoted by PG.\nIf STG and PG are high, this means that a derivation of a rating is highly consistent to what we see when using evaluated agents from Πe on slots i and j. A very similar property is known as monotonicity in [21, sec. 5][22], showing an agent set for the matching pennies game that is non-monotonic. Nonetheless, a partial order can still be constructed for the agent set of all finite state machines for this game [30].\nThe existence of a meaningful rating allows for subselections of Πo according to this rating, which can be used to furbish the tests with high-rank agents that can lead to more sophisticated social environments (which can be detrimental or beneficial, so making it more or less difficult, respectively, depending on whether it is used for the same team or for opponents)."
    }, {
      "heading" : "4.5 Anticipation",
      "text" : "One crucial property that is related to social intelligence is anticipation, which means that in both competition and cooperation, evaluated agents can benefit from anticipating other agents’ moves or, in more general terms, by having a theory of other’s minds. While a formalisation of this concept is very elusive, we can at least introduce an approximation."
    }, {
      "heading" : "4.5.1 Competitive Anticipation",
      "text" : "The first thing we need to do is to distinguish between competition and cooperation. In competitive anticipation we usually expect that evaluated agents can perform better if their opponents can be well anticipated. An example of this is a predator-prey situation. This phenomenon is difficult to define in general, but we can introduce a simplified approach based on the idea that one evaluated agent anticipates competitively if its expected average reward competing against a (generally) non-random agent is higher than its expected average reward competing against a random agent. This can be generalised as follows:\nDefinition 31. The anticipation benefit for evaluated agent π1 against agent π2, playing in slots i and j respectively (with i and j in different teams) when competing in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nACompi,j(π1, π2,Πo, wL̇, µ) , ∑\nl̇∈L̇N(µ)−i,j (Πo)\nwL̇(l̇) 1\n2\n( Ri(µ[l̇ i,j← π1, π2])−Ri(µ[l̇ i,j← π1, πr]) ) (31)\nwhere πr is a random agent, 1 2 normalises the result of ACompi,j to be between −1 and 1, N(µ) ≥ 2 and if N(µ) > 2 then |Πo| ≥ 1 otherwise |Πo| ≥ 0.\nWe generalise this for a set of evaluated agents Πe:\nDefinition 32. The anticipation benefit for a set of evaluated agents Πe with associated weight wΠe playing in slot i when competing against slot j (with i and j in different teams) in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nACompi,j(Πe, wΠe ,Πo, wL̇, µ) , ∑\nπ1∈Πe\nwΠe(π1) max π2∈Πo ACompi,j(π1, π2,Πo, wL̇, µ) (32)\nwhere |Πe| ≥ 1 and |Πo| ≥ 1.\nBy aggregating all combinations of pairs of slots, we have:\nDefinition 33. The anticipation benefit for a set of evaluated agents Πe with associated weight wΠe when competing in environment µ with weight of slots wS , with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nAComp(Πe, wΠe ,Πo, wL̇, µ, wS) , ηS22 ∑ t1,t2∈τ |t1 ̸=t2 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACompi,j(Πe, wΠe ,Πo, wL̇, µ) (33)\nwhere ηS22 normalises the formula with ηS22 = 1∑\nt1,t2∈τ|t1 ̸=t2 ∑ i∈t1 wS(i,µ) ∑ j∈t2 wS(j,µ) , τ is the partition of slots\non teams for environment µ, N(µ) ≥ 2 and ∃t1, t2 ∈ τ |t1 ̸= t2,∃i ∈ t1, j ∈ t2|wS(i, µ) > 0 and wS(j, µ) > 0.\nIf AComp is positive this means that evaluated agents behave better against non-random agents (in general) than against random agents.One good example of the above definition is when slot i is a predator and j is a prey. If, in general, evaluated agents in a class perform better with non-random preys than with random preys then the environment shows a benefit for this evaluated agent class (for these slots). Of course, this depends on Πo, but if we include non-random opponents with some movement patterns and/or some degree of intelligence, the definition becomes more meaningful."
    }, {
      "heading" : "4.5.2 Cooperative Anticipation",
      "text" : "On the other hand, in cooperative anticipation (or coordination) good rewards can only be obtained with both agents performing some actions together. Again a general definition accounting for all possible coordination situations is difficult, but we can introduce a simplified approach based on the following intuitive definition: two agents π1 and π2 coordinate cooperatively if the sum of the expected average rewards of π1 and π2 are higher when they interact together than the sum of each one interacting with a random agent.\nDefinition 34. The anticipation benefit for evaluated agent π1 and agent π2 playing in slots i and j respectively (with i ̸= j but in the same team) when cooperating in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nACoopi,j(π1, π2,Πo, wL̇, µ) , ∑\nl̇∈L̇N(µ)−i,j (Πo)\nwL̇(l̇) 1\n4 ×\n× ( Ri(µ[l̇ i,j← π1, π2]) +Rj(µ[l̇ i,j← π1, π2])−Ri(µ[l̇ i,j← π1, πr])−Rj(µ[l̇ i,j← πr, π2]) ) (34) where πr is a random agent, 1 4 normalises the result of ACoopi,j to be between −1 and 1, N(µ) ≥ 2 and if N(µ) > 2 then |Πo| ≥ 1 otherwise |Πo| ≥ 0.\nThe use of random agents for the above definition may not work in some cases if there are more than two elements in a team, as coordination may only take place when all of them coordinate and not only a pair (if a random agent is included, this can be very disruptive). In this case, the above definition could be extended to reach the number of elements in the team instead.\nWe can generalise this for a set of evaluated agents Πe:\nDefinition 35. The anticipation benefit for a set of evaluated agents Πe with associated weight wΠe playing in slot i when cooperating with slot j (with i ̸= j but in the same team) in environment µ with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nACoopi,j(Πe, wΠe ,Πo, wL̇, µ) , ∑\nπ1∈Πe\nwΠe(π1) max π2∈Πo ACoopi,j(π1, π2,Πo, wL̇, µ) (35)\nwhere |Πe| ≥ 1 and |Πo| ≥ 1.\nFinally, if we aggregate all combinations of pairs of slots, we have:\nDefinition 36. The anticipation benefit for a set of evaluated agents Πe with associated weight wΠe when cooperating in environment µ with weight of slots wS , with a class of opponents and team players Πo and a weight of line-up patterns wL̇ is given by:\nACoop(Πe, wΠe ,Πo, wL̇, µ, wS) , ηS23 ∑ t∈τ ∑ i,j∈t|i ̸=j wS(i, µ)wS(j, µ)ACoopi,j(Πe, wΠe ,Πo, wL̇, µ)+\n+ ∑\nt1,t2,t3∈τ |t1 ̸=t2 ̸=t3 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACoopi,j(Πe, wΠe ,Πo, wL̇, µ) (36)\nwhere ηS23 normalises the formula with ηS23 = 1∑ t∈τ ∑ i,j∈t|i̸=j wS(i,µ)wS(j,µ)+ ∑\nt1,t2,t3∈τ|t1 ̸=t2 ̸=t3 ∑ i∈t1 wS(i,µ) ∑ j∈t2 wS(j,µ) ,\nτ is the partition of slots on teams for environment µ, N(µ) ≥ 2 and ∃i, j ∈ t ∈ τ |i ̸= j, wS(i, µ) > 0 and wS(j, µ) > 0 or ∃t1, t2, t3 ∈ τ |t1 ̸= t2 ̸= t3,∃i ∈ t1, j ∈ t2|wS(i, µ) > 0 and wS(j, µ) > 0.\nThe previous definition includes slots in two different teams when the environment contains three or more teams. This is because two agents from different teams can cooperate against another agent in a third team.\nWe have defined the anticipation with only two slots, but competition and cooperation can also appear with three or more agents."
    }, {
      "heading" : "4.6 Symmetry",
      "text" : "In game theory, a symmetric game is a game where the payoffs for playing a particular strategy depend only on the other strategies employed by the rest of agents, not on who is playing them. This property is very useful for evaluating purposes. Since we can change the positions of the agents and they will maintain their results, we only need to evaluate the agent playing in one position of the environment.\nFor our definition of multi-agent environments this definition of symmetry must be reconsidered. The previous definition means that for each pair of line-ups with the same agents but in different order, the agents obtain exactly the same results. But with the inclusion of teams this definition is not appropriate. For example, using an environment with the partition of slots on teams τ = {{1, 2}, {3, 4}} and line-up l = (π1, π2, π3, π4), we have that agents π1 and π2 must both obtain the same result, as π3 and π4 as well. Following the definition and switching the positions of π2 and π3 we obtain line-up l\n′ = (π1, π3, π2, π4), which now means that agents π1 and π3 must have the same rewards (since they are now in the same team) while maintaining their previous results, as π2 and π4 as well. This situation can only occur when all slots (and therefore teams) obtain equal results.\nInstead, we extend this definition of symmetry to include teams. First, we denote by σ(l) the set of line-ups permuting the agent positions in line-up l. This set will correspond with the one used in game theory to define symmetry. To adapt this set to our definition, we must select a subset of line-ups from σ(l) respecting the teams defined in τ . We denote this subset with σ(l, τ), where we only select line-ups from σ(l) if original teams are maintained. Following the example, line-up l′ is not included in σ(l, τ) since π1 and π3 from l\n′ were not in the same team in l (as π2 and π4 as well). However, l\n′′ = (π3, π4, π2, π1) is included in σ(l, τ), since both pair of agents (π1, π2) and (π3, π4) are still in the same team. From here, we define symmetry for a multi-agent environment as follows:\nDefinition 37. We say a multi-agent environment µ is symmetric if and only if every team in τ has the same number of elements and:\n∀i,K,Π, l ∈ LN(µ)(Π), l′ ∈ σ(l, τ) : R̆Ki (µ[l]) = R̆Ki′ (µ[l′]) (37)\nwhere i′ represents the slot of agent li:i in l ′.\nNote that we impose that each set in τ must have the same number of elements. This is because we only consider environments to be symmetric if we can evaluate an agent in every slot and obtain the same result. Having teams with different number of elements will not allow us to do this.\nThis definition now fits our goal of symmetry. But too few environments will fit this definition of symmetry because it is too restrictive. However, we could divide this definition of symmetry of teams into two parts depending on the relation between the slots:\nFor the first part we look at the relation between the slots within each team:\nDefinition 38. We say an environment is Intra-Team Symmetric when the agents within every team can be swapped without affecting their results.\nThis kind of symmetry will allow us to evaluate a team in an environment without taking into account their positions within every set of τ .\nFor the second part we look at the relation between the slots in different teams. This can be also divided into two parts:\nDefinition 39. We say an environment is Partial Inter-Team Symmetric if every pair of teams having the same number of elements can be swapped without affecting their results.\nDefinition 40. We say an environment is Total Inter-Team Symmetric if every pair of teams can be swapped without affecting their results.\nThis kind of symmetry will allow us to evaluate a team in an environment without taking into account in which set of τ they are situated.\nDefinition 37 will correspond with an Intra-Team and Total Inter-Team Symmetry, where every team of agents can be located in every set of τ and in different order, maintaining their performance expectation.\nSymmetry is not a necessary condition for social behaviours, but it is a very practical one for measurement as the result does not depend on the slot we use to evaluate and all slots are useful for evaluating the same ability. This simplifies all previous calculations (for social intelligence and properties) as we can obtain the same results by calculating them for only one slot or pair of slots."
    }, {
      "heading" : "4.7 Validity",
      "text" : "Validity is the most important property of a cognitive test in psychometrics. In our context, the validity of a definition is that it accounts for the notion we expect it to grasp. For instance, if we say that a given definition of Υ measures social intelligence but it actually measures arithmetic abilities then the definition is not valid. In our case, this depends on the choice of Πo (Π) and M as the core of definition 8.\nPoor validity may have two sources (or may appear in two different variants): a definition may be too specific (it does not account for all the abilities the notion is thought to consider) or it is too general (it includes some abilities that are not part of the notion to be measured). In other words, the measure should account for all, but not more, of the concept it tries to represent. We refer to these two issues of validity as the generality of the measure and the specificity of the measure.\nRegarding generality, we should be careful about the use of very restrictive choices for Πo and M . It could be possible to find a single environment that meets all the properties seen in the previous subsections. However, using just one environment is prone to specialisation, as usual in many AI benchmarks. For instance, if we use a particular maze as an environment with a set of particular agents, then we can have good scores by using a very specialised agent for this situation, which may be unable to succeed in other social contexts. For instance, chess with current chess players is an example where a specialised system (e.g., Deep Blue) is able to score well, while it is clearly useless for other problems. A similar over-specialisation may happen if the agent class is too small. This is usual in biology, where some species specialise for predating (or establishing a symbiosis) with other species.\nConsequently, the environment class and the agent class must be general enough to avoid that some predefined or hardwired policies could be optimal for these classes. This is the key issue of a (social) intelligence test; it must be as general as possible. We need to choose a diverse environment class. One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).\nSimilarly, as happens with the environment diversity and most especially while evaluating social intelligence, we need to consider a class of agents that leads to a diversity in line-up patterns. This class of agents should incorporate many different types of agents: random agents, agents with some predetermined policies, agents that are able to learn, human agents, agents with low social intelligence, agents with high social intelligence, etc. The set of all possible agents (either artificial or biological) is known as machine kingdom in [19, 20] and raises many questions about the feasibility of any test considering this astronomically large set. Also, there are doubts about what the weight for this universal set should be. Instead, some representative kinds of agents could be chosen. In this way, we could aim at social intelligence relative to a smaller (and well-defined) set of agents, possibly specialising the definition by limiting the resources, the program size [25] or the intelligence of the agents [18].\nRegarding specificity, it is equally important for a measurement to only include those environments and agents that really reflect what we want to measure. For instance, it is desirable that the evaluation of an ability is done in an environment where no other abilities are required, or in other words, we want that the environment evaluates the ability in isolation. Otherwise, it will not be clear which part of the result comes from the ability to be evaluated, and which part comes from other abilities. Although it is very difficult to avoid any contamination, the idea is to ensure that the role of these other abilities are minor, or are taken for granted for all agents.\nThe properties of dependency (interactivity, non-neutralism) and anticipation (both competitive and cooperative) seen in previous subsections have been included for the sake of specificity. We are certainly not interested in non-social environments as this would contaminate the measure with other abilities. In fact, one of the recurrent issues in defining and measuring social intelligence is to be specific enough to distinguish it from\ngeneral intelligence. Unlike all other properties in this section, validity is not a property for which a formal account can be given, as it precisely accounts for how well the definition reflects the natural or intuitive notion that is to be measured. The assurement about validity must come then from the use of a formal definition (as for definition 8) with a meaningful instantiation for the agent and environment classes, and also from the experimental results that can be obtained through the tests derived from the definition. Note that in psychometrics there is usually a lack of a proper definition of the cognitive abilities of interest (e.g., psychometrics has not presented an unambiguous definition of intelligence), so validity is applied to a test and not to the definition of a cognitive ability. In fact, the concept is frequently derived from the test, as has happened with the modern view of intelligence, as “the ability measured by IQ tests”.\nAs discussed in the introduction, we are interested in a formal definition whose validity we can discuss and appraise, and from which tests are derived, and not the other way round."
    }, {
      "heading" : "4.8 Reliability",
      "text" : "Another key issue in psychometric tests is the notion of reliability, which means that the measurement is close to the actual value. Note that this is different to validity, which refers about the true identification or definition of the actual value. In other words, if we assume validity, i.e., that our definition is correct7, reliability refers to the quality of the measurement with respect to the actual value. More technically, if the actual value of π for an ability φ is v then we want a test to give a value which is close to v. The cause of the divergence may be systematic (bias), non-systematic (variance) or both.\nFirst, we need to consider that reliability applies to tests, as introduced in section 3.4. Reliability is then defined by considering that a test can be repeated many times, so becoming a random variable that we can compare to the true value. Formally:\nDefinition 41. Given a definition of a cognitive ability Υ and a test Υ̂, the test error is given by:\nTE(Υ̂) , Mean((Υ̂−Υ)2) (38)\nwhere the mean is calculated over the repeated application of the test (to one subject or more subjects).\nThe reliability Rel(Υ̂) can be defined as a decreasing function over TE(Υ̂), such as Rel(Υ̂) = e−TE(Υ̂). The reason for defining test error as the mean squared error (and not an absolute error) is a customary choice in many measures of error, as we can decompose it into the squared bias: (Mean(Υ̂)−Υ)2 and the variance of the error V ar(Υ̂−Υ). These values can be calculated for just one evaluated agent or for all evaluated agents.\nIf the bias is not zero this means that the mechanism to sample the exercises and/or the number of iterations is inappropriate, and the choices for pΠo (pΠ), pM , pS and pK in definition 9 must be revised. If there is a high variance, this suggests that the number of episodes nE is too small, and we need more (i.e., exercises in a tests) to get a less volatile result.\nIn this sense, note that some of the properties studied in previous subsections can hold for Υ but may be significantly different for unreliable tests (i.e., approximations) Υ̂.\nThe estimation of TE(Υ̂) or Rel(Υ̂) depends on knowing the true value of Υ. This is not possible in practice for most environments, so Υ will need to be estimated for large samples and compared with an actual test (working with a small sample). Because of the difficulties of estimating this, in what follows we will just give a qualitative assessment."
    }, {
      "heading" : "4.9 Efficiency",
      "text" : "This property refers to how efficient a test is in terms of the (computational) time required to get a reliable score. It is easy to see that efficiency and reliability are opposed. If we were able to perform an infinitely number of infinite episodes, then we would have Υ̂ = Υ, with perfect reliability, as we would exhaust Πo and M . However, as we try to make tests not only finite but more efficient, we lose reliability because of the sampling procedure. If done properly, it is usually the variance component of the reliability decomposition that is affected if it is possible to keep the bias close to 0 even with very low values of the number of episodes nE in definition 9.\n7The lack of a proper definition for many abilities makes reliability refer to the quality of the result of a single application of the test in comparison to the idealised average result if the test could be repeated indefinitely.\nThe definition of efficiency then needs to be defined as a ratio between the reliability and the time taken by the test (depending mostly on nE and pK , but also on pΠo (pΠ), pM and pS).\nDefinition 42. Given a definition of a cognitive ability Υ and a test Υ̂, the efficiency is given by:\nEff(Υ̂) , Rel(Υ̂)/T ime(Υ̂) (39)\nwhere Time is the average time taken by test Υ̂. Time can be measured as physical (real) time or as computational time (steps).\nWhile this is the way it should be measured, the big issue is how to choose environments and agents such that a high efficiency is attained. Clearly, if the selected environments are insensitive to agents actions or require too many actions to affect rewards, then this will negatively affect efficiency. As we are interested in social abilities, interactivity and non-neutralism must be high, as otherwise most steps will be useless to get information about the evaluated agent. This of course includes cases where the evaluated agent is stuck or bored because their opponents (or teammates) are too good or too bad, or the environment leads the evaluated agent to heaven or hell situations where actions are almost irrelevant,\nNaturally, a way of making tests more efficient is by the use of adaptive tests, as in computerised adaptive testing. We will not explore this possibility in this paper as our definition of test in section 3.4 is not adaptive (for adaptive versions of universal tests, the reader is referred to [27, 19]).\nSimilarly to reliability, we will just give a qualitative assessment of efficiency."
    }, {
      "heading" : "4.10 Summary of properties",
      "text" : "In tables 1 and 2 we can see a summary of all previous properties. Table 1 shows the quantitative properties, while table 2 shows the qualitative8 properties. This completes our picture jointly with figure 2.\nWith these properties we managed to represent how appropriate an environment µ and the set of agents Πo we use are in order to evaluate the social intelligence of a given set of evaluated agents Πe. The set of properties we propose provides key information about the testbed we are analysing. First, we can measure the influence that a set of agents Πo produces in a set of evaluated agents Πe. Second, we can analyse to what extent the anticipation abilities are useful for a set of evaluated agents Πe interacting with a set of agents Πo. Third, we can determine whether cooperation or competition is given more importance in the testbed. Fourth, we estimate the discriminative power that the testbed has for the evaluation of different agents. Fifth, the grading power of the testbed indicates how effective it is to rank agents. And sixth, we have some instrumental properties that are convenient to convert the definition into a practical test.\nWe think that these properties can be useful to characterise a testbed. For the quantitative properties we find two kinds of properties. The properties whose values range from 0 to 1 determine the percentage of fulfilment that the environment µ and the set of agents Πo have about this property when evaluating a set of agents Πe. Therefore, the lower the value the worse the system is in regard to this property, and the higher the value the better. On the contrary, the kind of properties whose values range from −1 to 1 must not be interpreted in the same way. Instead, these properties measure to which kind of type the system is more focussed on, and not a level of accomplishment or quality."
    }, {
      "heading" : "5 Degree of compliance of several multi-agent and social scenarios",
      "text" : "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66]. Typically these games and environments are created or selected to represent a specific problem or family to analyse or solve. Since we are interested in developing social intelligence tests, it is first mandatory to evaluate whether these other previous testbeds could be valid as they are (or with minor modifications). If not, they can still be a good source of inspiration to figure out new environment classes by reusing some of their ideas or hybridising some of their features.\nWe would have liked to explore many games and environments, but we can just practically do a selection of some of the most common and representative in the area of multi-agent systems, game theory and (social) computer games. We will focus on some testbeds whose specification is complete, so we can analyse the level of\n8Some of them can in principle be quantified, but we only give a qualitative assessment in this paper.\ncompliance of these testbeds for the properties seen in the previous section. In particular, the testbeds that we are going to analyse in this section are: matching pennies, prisoner’s dilemma, predator-prey (a pursuit game), Pac-Man and RoboCup Soccer."
    }, {
      "heading" : "5.1 Graphical analysis for the properties",
      "text" : "Before starting with the games and environments testbeds, we are going to introduce some indicators and a graphical representation that will be illustrated on a figurative environment. In table 3 we show a summary with the most important elements we are using in this section.\nIn order to assess compliance with interactivity, non-neutralism, anticipation and other properties for an environment µ we need to specify the evaluated agent class Πe with associated weight wΠe , the agent class Πo which populates the environment, line-up pattern weights wL̇ and slot weights wS . One choice for Πe, wΠe and Πo would be to consider any possible agent that is expressible using a given policy language. This, however, would make the calculation of most properties difficult (if not impossible). A better approach would be to use a (representative) sample of all agents or a sample of a meaningful class. Instead of that, and in order to give a more general picture of the environment itself, we will show the range of values that each property can have (independently of the agents), and how much this range can be restricted (for better or worse) depending on which Πe, wΠe and Πo we select. In fact, when evaluating a set of agents Πe in a certain setup, we should provide which set of agents Πo will populate the environment, but we could also let this set unfixed in order to measure the environment itself. Finally, the use of different weights can lead to different ranges for the properties, but in what follows we will assume uniform weights for line-up patterns wL̇ and slots wS .\nWe divided the properties into three types. In the first type we have the properties which have a quantitative value that can range between 0 and 1. In the second type we have the properties which have a quantitative value that can range between −1 and 1. And in the third type are the properties for which we provide a qualitative value.\nFor the first two types of properties we calculate the range that each property can have in an environment. For this, we need to calculate the lowest and highest values that this range can have for each quantitative property Prop. To achieve this, we will select Πe, wΠe and Πo (from the set of all possible Πe, wΠe and Πo such that Prop is defined) that obtain the lowest and highest values respectively. We define General as follows:\nDefinition 43. We denoteGeneral to be the range of values fromGeneralmin(Prop, µ) toGeneralmax(Prop, µ), where:\nGeneralmin(Prop, µ) , min Πe,wΠe ,Πo Prop(Πe, wΠe ,Πo, wL̇, µ, wS) (40)\nGeneralmax(Prop, µ) , max Πe,wΠe ,Πo Prop(Πe, wΠe ,Πo, wL̇, µ, wS) (41)\nwhere the weight for slots wS and weight for line-up patterns wL̇ are uniform weights and the triplet ⟨Πe, wΠe ,Πo⟩ is selected (from the set of all possible ⟨Πe, wΠe ,Πo⟩ such that Prop is defined) to minimise/maximise the values of a quantitative property Prop for environment µ.\nFor the first type of properties Prop we can select some set of agents Πo to obtain a situation where General is restricted in such a way that Generalmax(Prop, µ) decreases. In particular, we are interested in the setup with the “lowest maximum”, i.e., we consider those Πo that minimise this maximum. We define Left as follows:\nDefinition 44. We denote Left to be the most restricted range of values from Leftmin(Prop, µ) to Leftmax(Prop, µ) that we can obtain when Πo is selected (from the set of all possible Πo such that Prop is defined) to decrease the values of a quantitative property Prop which range is between 0 and 1 for environment µ, where:\nLeftmin(Prop, µ) , Generalmin(Prop, µ) (42)\nLeftmax(Prop, µ) , min Πo max Πe,wΠe Prop(Πe, wΠe ,Πo, wL̇, µ, wS) (43)\nIn the same way, for the first type of properties Prop we can select some set of agents Πo to obtain a situation where General is restricted in such a way that Generalmin(Prop, µ) increases. In particular, we are interested in the setup with the “highest minimum”, i.e., we consider those Πo that maximise this minimum. We define Right as follows:\nDefinition 45. We denoteRight to be the most restricted range of values fromRightmin(Prop, µ) toRightmax(Prop, µ) that we can obtain when Πo is selected (from the set of all possible Πo such that Prop is defined) to increase the values of a quantitative property Prop which range is between 0 and 1 for environment µ, where:\nRightmin(Prop, µ) , max Πo min Πe,wΠe Prop(Πe, wΠe ,Πo, wL̇, µ, wS) (44)\nRightmax(Prop, µ) , Generalmax(Prop, µ) (45)\nFor the first type of properties, the General, Left and Right ranges become better as long as their minimum and maximum values become higher. If the Left range values are lower, this would mean that a bad selection of Πo is disastrous for the quality of the testbed. If Right range values are higher would mean that there is a good selection of Πo which improves the quality of the testbed. The comparison between Left and Right with General shows us the importance that a good selection for the set of opponents and team players Πo has for a property Prop in an environment µ. As these three ranges are more different, the selection of agents Πo becomes more important in order to provide a better quality for the testbed.\nIn figure 3 we present the properties of a figurative environment divided in three sections. The top section represents five quantitative properties whose range can be between 0 and 19. The middle section represents the three quantitative properties whose range can be between −1 and 1. Finally, the bottom section represents the five qualitative properties.\nWe can see that each property of the first type has the early mentioned General, Left and Right ranges represented with three bands. The first property (Action Dependency) has a General range from 0 to 1,\n9Since FD and CD are similar properties, we decided to just calculate the FD property in order to simplify our analysis.\nrepresented with the first band. This is the broadest range that this kind of property can have. This means that this environment can have any value for this property depending on the set of agents Πo, the set of evaluated agents Πe and its weight distribution wΠe . The second band represents its Left range, which is equal to [0, 0]. In this case, there exists a set of agents Πo that restricts this range to the minimum possible range. The third band represents its Right range, which remains from 0 to 1. Now, no set of agents Πo can be selected to restrict this range. In the next four properties we see some other examples for the three ranges that this type of property can have. As we can see in the last property of this type (PG), we use a lighter color to represent that (part of) a range is not formally calculated, but instead we provide an estimation.\nNext we arrive at the second type of properties. Here the values for the General range can be between −1 and 1. Unlike the previous case, we do not provide Left and Right ranges for this type of properties. This is because these properties represent for which kind of social intelligence the environment is more oriented, so there are not really good or bad ranges for this property.\nIn the first property of this second type (Slot Reward Dependency), we see that the General range for this property is equal to [−1,−1], indicating that this environment is purely competitive. The next property shows another example for this type of properties. Meanwhile, the last property has a label with the text “Not Defined”. This is because for this environment the property is not defined, so we cannot represent it.\nFinally, we arrived at the last type of properties. Here, we denote whether the environment meets the properties by using a tick (X) or cross (×) mark respectively.\nNow that we have explained how we will represent the properties, let us start analysing some true environments."
    }, {
      "heading" : "5.2 Matching pennies",
      "text" : "Matching pennies [61] can be considered the simplest game in game theory featuring competition. This game consists of two players (or agents) each flipping a coin. If both coins match player 1 wins, otherwise player 2\nwins. This game is played as a repeated game, which means that the game is played on a single iteration and the game is repeated for several iterations. Each player can see the actions performed by the other player. The game is usually repeated during K steps (i.e., it is the iterated matching pennies), so players can use past steps in order to predict the other player’s strategy. Following definition 4, for agent slot i this environment only allows two actions Ai = {Head, Tail} and only provides two rewards Ri = {−1, 1}, which correspond to lose and win respectively. Clearly, in this game, τ = {{1}, {2}} represents the partition of slots in teams, i.e., has two teams and only one slot in each. For agent in slot i the environment provides an observation set Oi = Aj ∪ {null} (where agent slot j represents the slot of the other agent) and the observation function ω returns to each agent the action performed by the other agent in the previous iteration or null if it is the first iteration. Figure 4 shows the reward function ρ as a reward matrix, which has the actions of both agents as input and their rewards as outputs.\nNext we discuss the level of compliance of the matching pennies environment with respect to the properties seen in section 4. We can see a summary of the social properties for the matching pennies in figure 5. In appendix A we prove how we obtained these values.\nIf we start with the properties, we see that this game is bounded, as rewards are always between −1 and 1. This means that if the weight functions wS and wL̇ are bounded, the value of Υ (and many other properties)\nwill be bounded. Also, matching pennies is a well known zero-sum game, which means that the payoffs of the agents always sum zero (as we can see in figure 4) and, therefore, agents have totally opposed interests.\nWe next move to the symmetry property. This game has only two teams with one agent on each team. So, in order to prove that this environment is not symmetric, we only need to find a pair of line-ups l1 = (π1, π2) and l2 = (π2, π1) where the sequence of rewards for π1 and/or π2 differs in both line-ups. This becomes trivial by using the same agent πt (which always performs Tail) as both π1 and π2 (i.e., l1 = l2 = (πt, πt)) and check whether the agent obtains the same result in both slots. Since πt gets a result of 1 in slot 1 and a result of −1 in slot 2, we can conclude that this environment is not symmetric. This forces us to calculate some other properties for all the slots.\nIf we look to the General range for the action dependency (AD) property, we see that it goes from 0 to 1 (propositions 2 and 3). That means that the evaluated agents can either interact without noticing the other agent or can perform actions depending on which agent they encounter. But some particular selection of Πo could make this environment to have a too restrictive Left range with respect to this property, making it equal to [0, 0] (proposition 4), so no evaluated agent could perform different actions depending on which agent it interacts with. In addition, we see that no particular selection of Πo can restrict the Right range, which remains from 0 to 1 (proposition 5).\nWhen we look at the General range for the reward dependency (RD) property, we see that it goes from 0 to 1 (propositions 6 and 7). This means that the evaluated agents can either obtain the same expected average reward or can obtain different expected average rewards depending on which agent they encounter. But some particular selection of Πo could make this environment have a too restrictive Left range with respect to this property, making it equal to [0, 0] (proposition 8). In addition, we see that no particular selection of Πo can restrict the Right range, which remains from 0 to 1 (proposition 9).\nThe General range for the fine discrimination (FD) property goes from 0 to 1 (propositions 10 and 11). This means that two different evaluated agents can either obtain the same expected average reward or can obtain different expected average rewards. The Left range can be restricted to be equal to [0, 0] (proposition 12), and the Right range goes from 0 to 1 (proposition 13). This means that, with a bad selection of Πo, no pair of evaluated agents can be differentiated in terms of performance, and it does not exist a Πo to always differentiate any pair of evaluated agents.\nThe General range for the strict total grading (STG) and partial grading (PG) properties has, as in all previous properties, a minimum value of 0 (propositions 14 and 18) and a maximum value of 1 (propositions 15 and 19). This means that we cannot provide an ordering for some sets of evaluated agents, but we can provide it for some other sets of evaluated agents. In both STG and PG, Left cannot be restricted by any Πo, remaining from 0 to 1 (propositions 16 and 20), and the same occurs with Right, which cannot be restricted by any Πo, remaining from 0 to 1 (propositions 17 and 21).\nThis environment always has a General range for the slot reward dependency (SRD) property equal to [−1,−1] (proposition 22). This means that both agent slots have opposed interests, i.e., it is entirely competitive.\nThe General range for the competitive anticipation (AComp) property goes from − 12 to 1 2 (propositions 23 and 24). These values tell us that an evaluated agent can improve its results by correctly anticipating the actions of the other agent, but an incorrect anticipation will worsen its results.\nOn the contrary, we cannot evaluate the cooperative anticipation (ACoop) property in this environment. This is because each of the two teams has only one slot (in addition, this is also a zero-sum game), so the formula cannot be applied.\nWe now discuss the validity property. Matching pennies is a very simple game. As a result, it seems clear that it is not general enough to be used (alone) as the basis of a social intelligent test. Nonetheless, there are different opinions about this, as it has been suggested that matching pennies could be an intelligence test on its own, under the name ‘Adversarial Sequence Prediction’ [30, 31]. In fact, a tournament was organised in 2011 where computer algorithms competed10 and very interesting emergence phenomena were observed. Only strategies that were able to see patterns in the other players scored well (better than random). There are of course some counter-intuitive things about this game. Actually, random agents score exactly the same with any opponent, even with very intelligent ones. This raises concerns about the validity and reliability of this environment for a test since, in an intelligence test, the average score of a random policy should not obtain a good result, since random agents are clearly not intelligent. Also, matching pennies results are non-monotonic for a set of agents. In [21] there is an example of an agent set for matching pennies that is non-monotonic (so PG < 1). Nonetheless, partial orders can still be constructed for the agent set of all finite state machines\n10See http://matchingpennies.com/tournament/.\n[31]. Another important problem about matching pennies is that it only evaluates pure competition (it is a zero-sum game), and no form of cooperation can be found (although some versions, or the ternary extension, rock-paper-scissors, could allow for cooperation). Finally, another strong argument against the validity of this game as a good environment (alone) for a social intelligence test is that some current systems may score better than humans, even though these systems are not (socially) intelligent at all and they are designed to play matching pennies only.\nFinally, efficiency is a property where matching pennies excels, since every action has immediate consequences. This means that in most cases, it can be enough to design a test where only a few dozen steps are performed in order to have a good approximation. Of course, there might be agents that may change their behaviour in step 10,000 so their assessment up to this point will be completely different to their assessment in the limit (if no discounting factor is used). For some agents, with our definition of reliability, we get that some pairs of agents are stable from step 2, so high degrees of reliability (low test error) can be achieved very soon. When a random agent is involved, the approximation is slower, but not much slower, depending on a binomial distribution.\nOverall, matching pennies is an interesting game, but it lacks the generality that a social intelligence test should have. Nonetheless, it is a very simple game that illustrates how the range values of several properties can be calculated and provide very useful information about how an environment or game behaves. In the end, it has been a useful exercise before analysing more sophisticated scenarios below."
    }, {
      "heading" : "5.3 Prisoner’s dilemma",
      "text" : "In the prisoner’s dilemma [46] two prisoners (or agents) are suspects of a crime, and are asked if the other prisoner is guilty of that crime. If both cooperate and do not blame the other, both spend a short time in prison. If one cooperates but not the other, the one who blamed reduces its time in prison to the minimum sentence, but the other prisoner obtains the maximum sentence. Finally, if both prisoners blame the other, both spend a long time in prison.\nAs happens with the matching pennies, this game is played as a repeated game, which means that the game is played on a single iteration and the game is repeated for several iterations. Each player can see the actions performed by the other player. The game is usually repeated during K steps (i.e., it is the iterated prisoner’s dilemma), so players can use past steps in order to predict the other player’s strategy. Following definition 4, for agent slot i this environment only allows two actions Ai = {Cooperate, Blame} and provides four rewards Ri = {1, 2, 3, 4}, which correspond to the time spent in prison. In this game, τ = {{1}, {2}} represents the partition of slots in teams, which has two teams and only one slot in each. For agent in slot i the environment provides an observation set Oi = Aj ∪{null} (where agent slot j represents the slot of the other agent) and the observation function ω returns to each agent the action performed by the other agent in the previous iteration or null if it is the first iteration. Figure 6 shows the reward function ρ as a reward matrix, which has the actions of both agents as input and their rewards as outputs.\nThe payoff matrix in figure 6 is not normalised. We can normalise this matrix to be between −1 and 1, giving the lowest reward to the highest period in prison and vice versa. Once rewards are normalised, for agent slot i they are set to Ri = {−1,−0.33, 0.33, 1}. In figure 7 we can see this payoff matrix normalised.\nPrisoner’s dilemma differs from matching pennies by including some cooperation, since both agents can choose to cooperate to spend a relatively little time in prison.\nWe can see a summary of the social properties for the prisoner’s dilemma in figure 8. In appendix B we prove how we obtained these values.\nWith the normalised payoff matrix we can see that this game is bounded, since 1K·n\n∑K\nk=1\n∑n\ni=1 ri,k = c where −0.33 ≤ c ≤ 0.33. In this game some cooperation can appear, as for example, when both agents always decide to cooperate, so they obtain the maximum joint reward (0.33). But blaming can provide the best reward to one player if the other player still cooperates, so cooperating now provides the worst reward. Finally, if both decide to blame, both obtain the worst joint reward (−0.33). As in matching pennies, if the weight functions wS and wL̇ are bounded, the value of Υ (and many properties) will be bounded.\nAnalysing the symmetry property, we can see that the payoff matrix is clearly symmetric for both players. This makes that the payoffs of any strategies made by the agents do not depend on which slots they are, since they will obtain the same rewards. From this observation we can conclude that this environment is symmetric, which allows us to calculate some other properties only for one slot and assume that it is maintained for the other.\nIf we look at the ranges for the action dependency (AD) property, we encounter exactly the same scenario than in the matching pennies. That means that evaluated agents can either interact without noticing the other agent, obtaining a value of 0 (proposition 25), or can always perform actions depending on the agent they encounter, obtaining a value of 1 (proposition 26). But some particular selections of Πo can provide a too restricted Left range, forcing this value to be equal to [0, 0] (proposition 27), so no evaluated agent from Πe can behave differently depending on the agents of Πo. In addition, no particular selection of Πo can restrict the Right range, remaining from 0 to 1 (proposition 28).\nWe start to find some differences with the matching pennies when we analyse the reward dependency (RD) property. As in matching pennies, the General range for this property goes from 0 to 1 (propositions 29 and\n30), so the expected average rewards of the evaluated agents can either depend or not on which agent they encounter. Also, some particular selection of Πo could make this environment to have a too restrictive Left range with respect to this property, making it equal to [0, 0] (proposition 31), so no evaluated agent obtains different expected average rewards depending on which agent it interacts with. But, a good selection of Πo can restrict the Right range making it equal to [1, 1] (proposition 32). This means that the evaluated agents will obtain different expected average rewards depending on which agent they interact with.\nWe now move to the fine discrimination (FD) property. The General range for this property goes from 0 to 1 (propositions 33 and 34). This means that two evaluated agents can either obtain the same expected average reward or different expected average rewards depending on the set of agents Πo we select. It exists a particular Πo which restricts the Left range to be equal to [0, 0] (conjecture 1), meaning that the environment will not be discriminating the evaluated agents, since every evaluated agent will obtain the same expected average reward. It is not possible to restrict the Right range in such a way that we can always discriminate every pair of evaluated agents, so it remains from 0 to 1 (proposition 35), so it is possible to find two different evaluated agents from some particular Πe obtaining the same result.\nThe General range for the strict total grading (STG) and partial grading (PG) properties can, as in all previous properties, reach a minimum value of 0 (propositions 36 and 40) and a maximum value of 1 (propositions 37 and 41) for this environment. This means that we cannot provide an ordering for some sets of evaluated agents, but we can provide it for some other sets of evaluated agents. In both STG and PG, the Left range cannot be restricted by any Πo, remaining from 0 to 1 (propositions 38 and 42), and the same occurs with the Right range, which cannot be restricted by any Πo, remaining from 0 to 1 (propositions 39 and 43).\nThe General range for the slot reward dependency (SRD) property goes from −1 to 1 (propositions 44 and 45). This provides very different distributions of expected average rewards depending on the strategies used by the agents.\nThe General range for the competitive anticipation (AComp) property goes from − 23 to 2 3 (propositions 46 and 47). Anticipating the strategy of the other agent can be really useful to obtain a good expected average reward, but it can also provide a really bad expected average reward if the strategy is not correctly anticipated.\nWe cannot evaluate the cooperative anticipation (ACoop) property in this environment. This is because, as in the matching pennies environment, each of the two teams has only one slot, so the formula cannot be applied. This seems counter-intuitive since one of the actions is named to cooperate, but this cooperation is not meant to improve the agent’s own rewards, but to improve the other agent’s rewards. Indeed, if we reframe the game by using only one team and calculating the team reward as the mean of the agents’ rewards, then both agents can cooperate to obtain the best joint reward. This will lead us to a situation where a bad cooperative anticipation between the agents’ actions can negatively affect the team reward, but also, a good cooperative anticipation will have good benefits for the team.\nWe now discuss the validity property. The prisoner’s dilemma is similar in simplicity to the matching pennies game. But in this game, competition is not so strong, providing some cooperation between the two teams and making this game more general than the matching pennies. But in this game we cannot evaluate cooperation within a team, so it is not general enough to evaluate social intelligence. Actually, some simple strategies can clearly make the adversary’s results get stuck, forcing it to obtain bad rewards independently of its strategy. This raises concerns about the validity of this environment for a test, since an intelligence test should not give bad results to intelligent agents.\nFinally, the efficiency property is almost as good as in the matching pennies environment. Every action has immediate consequences on the agents’ results, having good approximations for their results in few steps. Therefore, reliable values for the ability measured will be reached in short time.\nThe prisoner’s dilemma resembles the matching pennies in many aspects, but it is slightly a more complex environment. As many similarities exists between both environments many properties remain equal.\nLet us see a more complex environment."
    }, {
      "heading" : "5.4 Predator-prey (Pursuit game)",
      "text" : "One typical environment for cooperation that uses a 2D discrete space is a pursuit game called Predator-prey [3], where the evaluee acts as a predator and has to cooperate/coordinate with other two predators in order to chase a prey. If they succeed chasing the prey, the goal is achieved. Figure 9 shows an example of a predator-prey environment.\nMany variants have been proposed about this scenario, which provides a high diversity of environments. Some\nexamples include spaces with and without obstacles or boundaries, and many variants about the parameters have been considered: the distance of the scenario that the agents can perceive, the number of predators or preys, the speed of the agents, etc. Even the definition of how the prey is chased has been modified, e.g., the prey is surrounded by the predators, or one predator chases the prey by occupying the same position. Some of these variants add a variety of social complexity to the game, such as different levels of cooperation/competition by having to interact with different numbers of predators or preys, or having faster preys.\nThese and other pursuit games have been widely studied and used in multi-agent systems (i.e., [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.\nSince we cannot analyse all the variants we just select one of them to analyse it, but different variants could have different properties’ values. For this analysis we will use the environment shown in figure 9, which also shows its initial observation. The game is typically performed in episodes. We will make an episode to end after six iterations are performed.\nFollowing definition 4, for agent slot i this environment allows four actions Ai = {Up, Right, Down, Left}, which leads the agent to the cell facing this direction (when an agent performs an action leading to a block or boundary, the agent does not move). For agent slot i the environment provides three rewards Ri = {0,−6, 6}, which correspond to ‘the episode is not finished’, ‘lose the chase’ and ‘win the chase’ respectively. τ = {{1}, {2, 3, 4}} represents the partition of slots in teams. The first team {1} contains the prey (which is located in the upper left corner) and the second team {2, 3, 4} contains three predators (which are located in the upper right, bottom left and bottom right corners respectively). For agent in slot i the environment provides an observation set Oi which corresponds to the set of spaces with any possible location of the agents, and the observation function ω returns for every agent a description of the space as, for example, figure 9. For the analysis of this environment we will follow the same procedure as for previous environments. This is the reason why we allow the evaluated agent to play in every slot, even as the prey. But, as mentioned above, if we only let the agent play as a predator, the values of the properties will be different. Figure 10 shows the reward function ρ as a payoff matrix which has the current iteration and the chasing situation as input and the agents’ rewards as output. Note that after the six iterations, the average reward will be −1 or 1 depending on whether the agent wins the chase or not.\nWe can see a summary of the social properties for the predator-prey in figure 11. In appendix C we prove how we obtained these values.\nWhen we start with the properties we see that this game is bounded since its rewards are between −6 and 6, so average rewards are between −1 and 1. As with previous environments, if weight functions wS and wL̇ are bounded, the value of Υ and many properties will be bounded.\nAnalysing the symmetry property, we clearly see that this game is not symmetric. First, prey and predator\nteams do not have the same number of agents. And second, changing the slots of the agents within the predator team does not provide the same sequences of rewards for the agents, owing to they start in different positions. This forces us to calculate all the other properties for all the slots.\nIf we look at the action dependency (AD) property, we cannot see any difference with previous environments, even having a so different environment. For the General range, evaluated agents can either interact without noticing the other agents, providing a minimum value of 0 (proposition 48), or can perform different actions depending on the agents they encounter, providing a maximum value of 1 (proposition 49). But some particular bad selections of Πo could restrict Left range to be equal to [0, 0] (proposition 50). On the contrary, no particular selection of Πo can restrict the Right range for this property, remaining from 0 to 1 (proposition 51).\nWhen we look at the ranges for the reward dependency (RD) property, we can see that the General range goes from 0 to 1 (proposition 52 and conjecture 2) so the rewards of the evaluated agents can either differ or not depending on the agents they encounter. A really bad selection of Πo can make the Left range too restrictive, staying on [0, 0] (proposition 53) no matter which Πe we are evaluating. But we can restrict the Right range by selecting a properly Πo, obtaining a range from 13 28 to 1 (approximation 1). This would force that almost half of the results of the evaluated agents will be different depending on the line-up pattern they interact with. We now move to the ranges for the fine discrimination (FD) property. The General range goes from 0 to 1 (propositions 54 and 55), i.e., the result of one evaluated agent can be different or not from the result of another evaluated agent depending on which set of agents Πo is selected. But a bad selection of Πo can restrict too much the Left range, making this value equal to [0, 0] (proposition 56), so every evaluated agent will obtain the same result. However, the Right range cannot be restricted, remaining from 0 to 1 (proposition 57), since there are always two different evaluated agents that can obtain the same expected average reward, independently of the set of agents Πo.\nStrict total grading (STG) and partial grading (PG) properties are clearly different from previous environments. The General range for the strict total grading property can only go from 0 to 12 (propositions 58 and\n59), so we cannot even have a strict total ordering for all the evaluated agents. In addition, its Left range can be restricted to be from 0 to 14 (approximation 2), so it would be possible to obtain some strict total ordering. At least, its Right range can be restricted to be from 14 to 1 2 (approximation 3), which somehow alleviates this situation. But instead, its partial grading has really good ranges. Its General range goes from 12 to 1 (propositions 60 and 61), its Left range can be restricted to be from 12 to 3 4 (approximation 4) and its Right range can be restricted to be from 34 to 1 (approximation 5). This makes this environment good for grading the evaluated agents, so even with a bad selection of Πo we will still be able to obtain some partial orders between some of the evaluated agents, and provides a promising partial grading for the evaluated agents if Πo is well selected.\nThe slot reward dependency (SRD) has a General range equal to [0, 0] (proposition 62). This particular value comes from the opposite results of preys and predators. As early mentioned, if we had only used the slots of the predators for evaluating the agents, we would have had another range.\nThe General range for the competitive anticipation (AComp) property goes from −1 to 12 (propositions 63 and 64). Anticipating the strategy of the other team can be useful to improve the expected average reward, but only when playing in the predator team, since playing as the prey does not have this benefit. But conversely, a bad anticipation does really penalise the expected average reward of an evaluated agent either playing in the prey or predator team.\nIn this environment we can find cooperation between the agents within the predator team. The General range for the cooperative anticipation (ACoop) goes from −1 to 1 (propositions 65 and 66). This makes a good coordination within the predator team to always chase the prey, but a bad coordination can let the prey to escape.\nWe now discuss the validity property. In the predator-prey environment we can encounter both competition between the prey and predators, and cooperation among the predators, which makes it a complex game to evaluate social intelligence. Both competition and cooperation seem important in this game, giving more importance to competition rather than cooperation as we can see from AComp and ACoop, where a bad result can come from incorrectly anticipating while an incorrect anticipation in cooperation does not necessarily provide bad rewards. But this game gives us a general situation to evaluate social intelligence in a broad way. Also, agents that are evaluated in this environment can have good orderings as properties STG and PG reflect, making it a good environment to classify the agents. However, the abilities that the agents need to accomplish their goals are not balanced. It is easier for the predator team to win the chase if they cooperate adequately, where the prey will not have a chance to survive. Also, the Left ranges of the properties are usually very restrictive when Πo is not selected carefully. So it is really necessary to select a correct set of agents, but still, once a certain level of intelligence is reached we cannot evaluate higher levels of intelligence, since their result will remain equal. From here, we can say that the social intelligence that this environment is evaluating is clearly limited to a certain level, and once this level is reached the results will not vary as happens in the Left range for the RD property. Summarizing, this environment allows us to evaluate both competition and cooperation which makes it a good environment to evaluate social intelligence, but this can only be evaluated until a certain level of intelligence. Over this level, the results will not reflect the true intelligence, so the environment will not be evaluating its actual value. As a result, the game is not valid for interesting levels of social intelligence.\nFinally, this environment is not reliable for just one exercise. It has a high variability as only 6 iteration are allowed and the space is small, so small changes in the first movements may have very different consequences. With respect to its efficiency, we just need a few interactions to provide us a fast result for the evaluated agent, which makes this environment efficient. This means that reliability can be obtained in reasonable time by the repetition of many episodes.\nThe predator-prey gives us a more complex environment than previous environments. However, it can still provide us a good environment to evaluate social intelligence if Πo is wisely chosen. But we must be careful, since we could obtain a really poor environment if they are not properly selected. The results we have obtained came from our choice to include the possibility to evaluate the agent also playing as the prey. A more classical approach could have given us a different picture for this environment."
    }, {
      "heading" : "5.5 Pac-Man",
      "text" : "Computer games are also used as mainstream environments to evaluate AI systems. One example of the use of games for evaluating AI is the ALE (Arcade Learning Environment) [2], a framework where a set of arcade computer games are used to evaluate the performance of current AI algorithms. Here, we will analyse Pac-Man,\na simple and well known game, but still complex enough to the state of the art in AI, which uses a 2D maze. The AI community has used this environment as a testbed in order to evaluate their algorithms (e.g., [57, 15]). This game resembles a pursuit game, but this time the player represents the prey role (most of the time), so it must avoid being caught by the enemies (represented by ghosts). In order to win, Pac-Man must also collect all the pills that are present in the environment, which also provide some points. On the other hand, ghosts are appearing in the environment one by one over time, and they win if at least one of them is able to chase Pac-Man. If Pac-Man is able to reach certain locations in the environment and eat specific pills, it becomes invulnerable for a short period of time, and receives additional points by chasing the ghosts. Figure 12 shows a Pac-Man game screenshot.\nFrom the huge diversity of possible situations that can occur in this environment, it is difficult to formally analyse some of the properties as we did with previous environments. As long as the systems are more complex, it becomes more difficult to determine their actual levels of cooperation and competition, and more effort is needed to formalise them and find some Πe, wΠe and Πo to find the environment ranges. Instead, we will analyse this environment in an informal way. As in previous environments we will assume uniform weights for wL̇ and wS .\nIn figure 13 we show a summary with an estimation of the properties for Pac-Man. When we start with the properties we see that this game is not bounded, since Pac-Man can obtain more and more points (or rewards) as long as it continues surpassing levels. This makes that Υ will not be bounded for this environment. Reframing the game by calculating an average of points by time as rewards in order to make it bounded will change the goal of the game significantly.\nAlso, the game is not symmetric. On one hand, both teams do not have the same number of slots, which makes the game not Total Inter-Team Symmetric. On the other hand, we could say that the environment is Intra-Team Symmetric, since every ghost has the same probability to chase Pac-Man, but this is not exact, since each ghost appears in different moments of the game, so swapping their behaviour could not provide exactly the same results, making the environment not Intra-Team Symmetric.\nThe action dependency (AD) property seems to be as in previous environments. All agents have the possibility to ignore the actions of the other agents or act according to what they did in previous interactions.\nWhen we look at the reward dependency (RD) property, it could obtain some different values. Indeed, it is too easy to chase Pac-Man if the four ghosts cooperate coherently, but a bad behaviour for the ghosts can facilitate the game for Pac-Man. In addition, small differences in the behaviour of the agents can provide very different results as, for example, a ghost passes near Pac-Man and decides to chase or to avoid it. This small difference in behaviour will provide high differences in their results. Also, when a ghost is far from Pac-Man, small differences in its behaviour will probably lead to similar results.\nThe fine discrimination (FD) property also has a huge range of values. As mentioned above, the behaviour\nof two different evaluated agents can both obtain the same or very different results, highly depending on the behaviour of the other agents.\nIt seems difficult to know whether we can establish a grading between the evaluated agents in this environment. But we venture that the grading properties could be similar to the ones provided in the predator-prey environment, since both environments have many similarities.\nIt is also difficult to provide a slot reward dependency, since rewards obtained by one team typically do not reflect on the other team. For example, every point obtained by Pac-Man does not directly have influence on the ghost team’s rewards, and chasing Pac-Man only prevents it from obtaining more points. But just assuming that the rewards of each team are always different, we can obtain a value as we did in the SRD for the predator-prey environment (which has a similar configuration) to obtain an approximated value, meaning that the slot reward dependency is more focused on cooperation than competition.\nIf we look at the anticipation properties, it is possible that competitive anticipation does not have a huge reflect on rewards, but still anticipating competitors will provide some good rewards. In cooperative anticipation, it is possible that one ghost can do worse than a random agent, leading to really bad values in this case, but a good anticipation can make chasing Pac-Man easier.\nThis game is not very reliable as it depends on many small details. Also, the game is not efficient. We will need to run the game at least for dozens of iterations to get some stability in the agents’ expected average rewards, since many of the first actions obtain the same rewards but the crucial part of the game comes when the pills become scattered.\nFinally, with this game we can both evaluate competition and cooperation. We can find competition, since each team can only gain rewards by making the other team lose rewards. Additionally, in the ghost team cooperation is also needed to properly chase Pac-Man. For this game, the selection of agents is crucial, a set of predators with high level of intelligence can make Pac-Man efforts useless, which will always obtain bad results. This makes the game somewhat valid, but only for lower levels of (social) intelligence if the selection of agents is wisely chosen, but not for agents with high levels of (social) intelligence where the game becomes not valid."
    }, {
      "heading" : "5.6 RoboCup Soccer",
      "text" : "As an example of a 3D space game we find the RoboCup Soccer competition [38]. Here, two artificial multi-agent systems (or teams) have to compete against each other in order to win a soccer match. The agents in each team must cooperate to make the ball reach the adversary’s goal, while cooperate to avoid the adversary to score a goal. The game follows the rules of a typical soccer match. Figure 14 shows a RoboCup Soccer match.\nAs happens with the previous environment, this game has a huge diversity of possible situations that can occur (including physical and virtual versions), which makes difficult to formally analyse it. Again, in such complex scenario, it becomes more difficult to determine the levels of cooperation and competition. But also, due to the high level of complexity of this game, teams tend to some specialisation, with each player focussing on some specific aspects of the game instead of focusing on the problem in a general way. Again, we will analyse this environment in an informal way. As in previous environments we will assume uniform weights for wL̇ and wS .\nAs in the previous environment, in figure 15 we show a summary with an estimation of the social properties for the RoboCup Soccer.\nWhen we start with the properties we see that this game is bounded, since rewards (1 for win, 0 for a tie and −1 for lose) are bounded. This makes that Υ will be bounded if weights are also bounded.\nWhen we look at the symmetry property, both teams have the same number of slots and, if we ignore which team starts with the possession of the ball on each half, it makes the environment Total Inter-Team Symmetric, so both teams can swap their slots and the result will remain the same. But, if we want to swap the slots of two players within the same team, they will not obtain the same results (as for example the goalkeeper has different rules), so the environment is not Intra-Team Symmetric. Since for the symmetry condition we need the environment to be both Total Inter-Team Symmetric and Intra-Team Symmetric, we can conclude that the RoboCup Soccer is not symmetric.\nThe action dependency (AD) property is similar to previous environments. All the evaluated agents have the possibility to act differently depending on the agents they encounter, but, at least, in this game an agent can affect the actions of the evaluated agents. For example, one agent can knock the evaluated agent down to the ground, so now it will only be able to stand up.\nWe now see the reward dependency (RD) property, which can have a huge range. A change in the line-up of course can change the result of the match. Conversely, only changing one agent in the line-up can completely change the match result to make a team lose. changing the agents’ rewards. And we can reason in the same way for the fine discrimination (FD) property, since the behaviour of only one agent (the agent to be evaluated) can also make its team to either win/lose the game, or obtain the same result.\nIt is not easy to determine if there exists some grading between the evaluated agents in the RoboCup Soccer. Instead, we can take a look at some professional (human) Soccer leagues. It is not unusual to see situations where two teams repeatedly tie, and a third team beats one of them while loses against the other one repeatedly. This situation shows us that there is no strict grading between teams (and neither is between their players) for this game.\nThe slot reward dependency is straightforward for this environment. We have two teams with five players\non each team. Every agent within a team will obtain the same reward, while the other agents within the other team will obtain the opposite reward. Using a correlation function over these rewards and over all slots, we obtain a slot reward dependency equal to 0.\nIf we look at the anticipation properties, correctly anticipating both competitive and cooperative can provide a high advantage for each of the teams, so the team of the evaluated agent can score more goals and win the game more easily. A bad competitive anticipation can lead the opponent to win the game, while a good one can provide good results. In cooperative anticipation, the evaluated agent could play worse if it is not correctly anticipating its teammate, but a good anticipation can provide them really good results.\nLet us now consider the validity of this game. First, with this game we can both evaluate competition and cooperation. We can find competition, since both teams must compete to win the game. Additionally, the agents within each team can cooperate to mislead the other team and score more goals. Second, increasing the social intelligence of the agents will typically increase the difficulty of the match, since more skilled agents will score goals more easily, and also defend better, preventing the other team to score. This makes the game useful to match a high variety of skill levels. But conversely, this game also evaluates some other skills than social intelligence, such as for example, their ability to predict the movement that the ball will do when it is kicked. In principle, there are reasons to consider this game a valid scenario to evaluate social intelligence. However, since the agents will need more than their social intelligence in order to play the game, we think that an evaluation using this kind of environment will not only evaluate social intelligence, but also other abilities such as motion understanding. For this reason we consider this environment not valid to evaluate social intelligence.\nThis game is not reliable. In this game, it is widely known that two teams that are facing each other do not necessarily obtain the same result in every match. This depends highly on the decisions made by the agents, since a small decision change can lead to a very different result on the match. Also, the result of the game depends on luck (at least for the physical version), since the players cannot always predict the correct movement of the ball."
    }, {
      "heading" : "5.7 Summary",
      "text" : "So far, we have analysed some ranges of values that the properties presented in this paper can have for the five environments we have selected. Next, let us group the environment properties by the different ranges in order to make a comparison between the environments through their properties.\nIn figure 16 we see the General range of the quantitative properties for the five environments.\nAs we can see in the top part of figure 16, there are few differences with respect to this range for the environments. In the first three properties all the environments have the broadest possible range. We only see some difference for the grading properties in the last three environments. While the first two environments have the broadest possible range, the last three environments seems that as long as the strict total grading gets worse, the partial grading gets a better range. This is simply explained because agents in the same team obtain the same rewards, so it is not possible to obtain a strict total ordering for them while it is still possible for agents in different teams. Obviously, the agents in the same team always have a partial ordering, making this range higher.\nIn the bottom part of figure 16 we can see more differences. In the slot reward dependency property we can see the first big difference between the environments. The majority of the environments have a unique (and usually different) value for this property. While the matching pennies is completely competitive, Pac-Man is slightly more oriented to cooperation, and predator-prey and RoboCup Soccer are neutral (i.e., provides both competition and cooperation to the same extent). But, prisoner’s dilemma is the only one that has the broadest possible range instead of having a predetermined configuration, so this environment allows the agents to dynamically cooperate and compete with agents in the other team depending on which actions they perform.\nFinally we have the two anticipation properties. In both of them, the agents obtain better rewards when\nthey correctly anticipate, but their rewards get worse when they incorrectly anticipate. In the competitive anticipation we see some small differences between the environments. While a correct competitive anticipation provides good rewards, some environments usually provide much worse rewards when incorrectly anticipating competitive agents, as we can see for the predator-prey and RoboCup Soccer. For the cooperative anticipation we see another different picture. In this case two of the environments do not have this property, since they do not provide teams where the agents will be able to cooperate. However, the last three environments provide teams where cooperative anticipation can be useful between cooperative agents, and also they (almost) provide the broadest range for this property.\nIt is much more interesting to see what happens with these environments if we make a bad selection of Πo. In figure 17 we can see the Left ranges of the quantitative properties which range is between 0 and 1 for the five environments.\nWe can see that both the action and reward dependency properties have the worst possible range for all the environments. This means that a bad selection of Πo will be disastrous with respect to these properties. In fact, this is not surprising, since Πo could be populated only with agents having the same exact behaviour, so the evaluated agents will not be able to behave or obtain different rewards depending on which agents from Πo they are interacting with.\nFor the fine discrimination property, we can see that (almost) all the environments have a very low range, so the evaluated agents can hardly be discriminated.\nThe strict total grading property clearly gives us an order of their compliance for the environments. RoboCup Soccer is clearly the worst environment, while predator-prey and Pac-Man follow it. Meanwhile, matching pennies and prisoner’s dilemma cannot be restricted with any particular Πo, obtaining the best range for this property.\nFinally, the partial grading has different ranges for the environments. Predator-prey and Pac-Man have the same range, and they also have a clearly better range than the RoboCup Soccer. Matching pennies and prisoner’s dilemma have the broadest possible range. At this point, it is not clear which pair matching pennies and prisoner’s dilemma, or predator-prey and Pac-Man has a better range. From one side, the range for matching pennies and prisoner’s dilemma goes from 0 to 1, so the selection of Πo does not necessarily worsens the property, but still it is possible to have a bad value. From the other side, the worst value for the predatorprey and Pac-Man cannot be as bad as in matching pennies and prisoner’s dilemma. However, the selection of Πo does worsen the values for this property.\nLet us next see the effect that a good selection of Πo can have on some quantitative properties for the environments. This is possibly the most interesting picture, because it gives us the best we can do with a right\nchoice of Πo. In figure 18 we can see the Right ranges of the quantitative properties which range is between 0 and 1 for the five environments.\nWe can see that the action dependency and fine discrimination properties cannot improve much by selecting an appropriate Πo. At least RoboCup Soccer can slightly restrict its action dependency range, which means that even with the best possible choice of Πo we cannot ensure that there will be a high action dependency.\nFor the reward dependency property the ranges vary. Matching pennies and RoboCup Soccer cannot restrict their respective ranges. Pac-Man can slightly restrict this range and predator-prey does not have a bad range, so depending on which line-up pattern the evaluated agents encounter, they will certainly obtain some differences in their expected average rewards. But prisoner’s dilemma can ace this property, making the expected average rewards different for every evaluated agent depending on the line-up pattern they interact with.\nFor the strict total grading we find more differences. It is not clear which environment has a better range, but at least we can say that RoboCup Soccer has the worst range among the five environments. However, it is not as clear which of the other four environment has a better range. From one side, the range of matching pennies and prisoners’ dilemma goes from 0 to 1, so the selection of Πo does not necessarily improve their property, but still it is possible to have a good value. On the other hand, the best value for the predator-prey and Pac-Man cannot be as good as in matching pennies and prisoner’s dilemma. However, the selection of Πo does improve the values for this property.\nFinally, the partial grading gives us more information about the orderings. RoboCup Soccer has improved its range, but it is still worse than the predator-prey and Pac-Man, which now are clearly the best to obtain an ordering. However matching pennies and prisoner’s dilemma cannot restrict this range, making them the worst of the five environments.\nOverall, the Right range is more informative. Note that the Πo that we use to calculate the values for each environment’s property is not necessarily the same. We just obtain the values locally for each property. That means that some points could not be achievable at the same time.\nLastly, in figure 19 we see a summary of the qualitative properties to obtain a practical test for the environments.\nAs we can see, almost all the environments have bounded rewards. This provides a bounded value for Υ. But only the prisoner’s dilemma is symmetric, so in order to evaluate an agent in the other environments, we will need to evaluate them in all the slots. With respect to the validity property, no environment is correctly evaluating social intelligence. Some of them are not sufficiently general, as happens with the matching pennies or the prisoners’ dilemma. We have the opposite situation with RoboCup Soccer, where more abilities are evaluated and it seems difficult to isolate social intelligence from these other abilities. Finally, other environments can only\nevaluate the social intelligence to a certain degree, as happens in the predator-prey or Pac-Man. With respect to the reliable property, the prisoners’ dilemma can evaluate every level of performance with a good result, but we cannot say the same for the rest of the environments. In matching pennies every agent interacting against a random agent will obtain the same result independently of their intelligence, while the rest of environments will not be able to provide a correct value for this property over certain level of intelligence. Lastly, due to its simplicity, matching pennies, prisoners’ dilemma and predator-prey are really efficient to provide a result for the agents, but the rest of environments will need to run the game during several iterations and episodes to converge to their results.\nFrom this analysis and comparison between the properties of the environments we made in this section, we can provide some insights. First, we give some findings about the five environments.\n• As we have seen in our analysis, these environments are typically covering anticipation well. Competitive anticipation is well covered in all the environments, while cooperative anticipation is not defined for the first two environments, but the last three are covering it very well. It also seems that the partial grading is generally well covered, so we can find some partial orderings between the evaluated agents.\n• We can find some other properties that the environments are not covering well. One example is the action dependency, where (almost) every environment analysed in this paper obtained the same poor ranges. This property is something which is not usually thought about when designing an environment, but the possibility of having influence on the actions that other players can do is an interesting thing to consider when designing a multi-agent environment and, in particular, if we want to evaluate social intelligence. Another property which is not usually well covered is the slot reward dependency, where these environments are typically only giving one value. We do not mean that the environments do not have good values, but instead, an environment having a broader range of values will provide us a more interesting scenario, where the relations of competition and cooperation between agents can change dynamically.\n• As we can see from their qualitative properties, four of the five environments we selected have some difficulties to be used as a practical test. None of them provides symmetry to simplify the evaluation. The range of abilities required to succeed in these environments are not appropriate to be a valid test to evaluate social intelligence accurately, as well as their reliability is compromised depending on the agents we use to populate the environment. At least some of them are efficient enough to obtain results in a short period of time, and they usually provide bounded rewards, so we can calculate a bounded value of the (social) intelligence of the agents.\n• With these properties, we obtained different ranges of General, Left and Right for each of the five environments. In addition, we could see that some little changes over the definition of an environment (as occurs with the matching pennies and prisoner’s dilemma) are clearly reflected with these properties. In fact, every kind of environment will have particular ranges of values for these properties, with which we will be able to select the (social) environment(s) that best fits our goals (e.g., select an environment focused on anticipating other agents).\n• As we have seen, a good selection of Πo is crucial in order to obtain appropriate social environments. But, how could we provide a Πo which is appropriate to evaluate the social ability of an agent in a large number of environments? This is a difficult task. When starting the evaluation, since the intelligence of the evaluated agent is not known, it would be appropriate to use one Πo which agents are not too smart during the first environments. As long as the evaluation goes ahead and the intelligence of the evaluee is\nbetter known, it would be better to use another Πo whose agents are conforming to this level of intelligence. In order to solve this problem, we could provide a unique Πo and use some kind of distribution which is continuously evolving, giving more probability to the agents which are obtaining better results on these environments (e.g., as in the spirit of the Darwin-Wallace distribution [18]).\nFrom the previous analysis we can now distinguish the features of the environments that could be reused for the design of better environments to measure social intelligence more effectively. The first environment we saw is matching pennies, but it does not seem to have any particular useful feature from the properties we analysed. Next we saw the prisoner’s dilemma environment, which is similar to the matching pennies with some little modifications. This prisoner’s dilemma offers some nice features to include in a social intelligence test. First, we notice its capability to dynamically change the relation between the slots, providing a competitive and cooperative environment at the same time depending on the agents’ actions. Second, the evaluated agent can obtain drastically different results when it interacts with very different agents from Πo. And third, the symmetry of the payoff matrix, its reliability and efficiency makes this environment a good candidate to provide a simple test. The third environment was the predator-prey. This is the first environment that we analysed providing several agents in (at least) one team. From this team of agents, it is possible to anticipate cooperative agents in such a way that really good performance can be achieved when it is done correctly, and an incorrect anticipation can provide really bad rewards. The same occurs while anticipating competitive agents, but in this case, both teams can anticipate the agents in the other team. Also, we can obtain good partial gradings for the evaluated agents and it is a really efficient game, providing a result in a short period of time. However, Pac-Man and RoboCup Soccer do not provide significant features beyond those provided by predator-prey. At least, in RoboCup Soccer it is possible to exert a slight influence on other agents’ actions, but only to some extent.\nConversely, we also distinguish those features that we do not want to appear in environments for social intelligent tests. The first feature we distinguish is that none of these environments are valid to evaluate social intelligence since they are evaluating: 1) more abilities than necessary, as in RoboCup Soccer where the agents need their motion understanding to play the game, 2) not enough abilities, as in matching pennies and prisoner’s dilemma where the agents cannot cooperate with agents in the same team, or 3) is only valid for lower levels of intelligence, as in predator-prey and Pac-Man where the predators and ghost can easily chase the prey and Pac-Man respectively once they reach a certain level of (social) intelligence. Also, the majority of the environments do not provide a reliable result due to many reasons. For example, the results of the agents can be easily restricted to always obtain the same results (as occurs in matching pennies by using a random agent). Also, little changes in the behaviour of the agents can create a butterfly effect, making the agents to obtain very different results (as occurs in the last three environments). Also, the environments are typically not symmetric, which will force to evaluate the agents in all slots, and complex environments usually need a lot of iterations to provide a “reliable” result. In these five environments, it is weird (if not impossible) to find a situation where an agent can directly influence on which actions are available for one (or more) of the other agents. The capability to directly influence on the available actions of the rest of agents could provide us a richer social environment. Also, some environments (matching pennies and prisoner’s dilemma) are not suitable to let the agents anticipate cooperation within a team, since they do not provide the agents a team of agents to cooperate with them. Finally, when we see in more detail some environments, we notice that predator-prey and Pac-Man provide a really difficult/hostile environment, where the predators and ghosts respectively have an enormous advantage to win the game.\nEven if it is not the goal of this paper (but a future work), we consider that a good environment measuring social intelligence would have (at least) these characteristics: 1) It provides two or more teams to interact with, and two or more agents on each team. By having this, the agents will be able to compete against the other team(s), cooperate with the agents within the same team and, in the case where more than two teams are presented in the environment, cooperate with other teams. This will provide anticipation to the environment, so the agents will be able to competitive and cooperative anticipate other agents. 2) The agents should influence in some way the rewards obtained by the other agents, providing reward dependency to the environment. 3) There should be limited rewards that the agents can obtain and the payoff of the agents will only depend on the actions they perform. This will provide us a bounded and symmetric environment, which will be ideal to create a practical test. 4) There should not be easy equilibria in the environment. If such circumstance occurs, most of the agents (the intelligent ones) will always perform the same actions, which will limit the results obtained by the agents. Avoiding easy equilibria will provide to the environment more discriminative power, reward dependency and grading for the agents. 5) The environment should provide different kind of spaces where the agents can move. This will avoid the agents to specialise to a particular space. This will make the environment\nmore valid to evaluate (social) intelligence in a more broader way. 6) The agents must be able to influence in some extent the actions that the other agents can perform, creating richer social situations and providing some action dependency to the environment.\nFinally, what can we say about the properties? Are they sufficient to characterise any environment? How should they be used? Are the plots useful? Is the Left and Right ranges more meaningful than the General ranges? Some insights below.\n• With these properties we are able to obtain different values for each environment. This gives us some idea about the strengths and weakness of each environment.\n• Here, we only evaluated one agent from a set of evaluated agents Πe interacting with a set of opponents and team players Πo. But more specialised properties (and even a definition of social collective intelligence) can be easily extended by, for example, dividing the set of agents Πo into two sets (i.e., one for opponent players and one for team players) or, instead of evaluating the agents in isolation, evaluating together a group (or collective) of agents.\n• There are also other issues which may not be covered on these properties, as for example communication among teammates. They do not provide information about misleading opponents, or the possibility of the agents to influence the actions of other agents on its benefit. Also, the properties do not show us the contribution of the agents to their teams’ rewards, as well as the impact that their inclusion in the line-up has in the results of the other teams.\n• We proposed some useful properties to measure the appropriateness of the environments to evaluate social intelligence, which are also useful to characterise these environments. These properties provide us some interesting information about the environments such as the fine and coarse discrimination, which give us a measure of their discriminative power. Other interesting properties are the action, reward and slot reward dependency, giving us an idea about the existing dependency between the actions/rewards of the agents, and which relation between the slots is given more importance in the environment (i.e., it is a more competitive- or cooperative-oriented environment), and if this relation is static or can change during the evaluation.\n• We used the Left and Right ranges in order to compare for each property how a particular good or bad selection of Πo can affect that property in an environment. Conversely, a real test shall provide a unique Πo to evaluate the agents, obtaining a unique range for each property. This selection of Πo will (most probably) make the properties to barely look like the Left or Right ranges we calculated for this five environments, providing instead more varied ranges. Therefore, a comparison between some testbeds with their Πo fixed will give us a more clear idea about their differences."
    }, {
      "heading" : "6 Conclusions and Future work",
      "text" : "Social intelligence has been an important area of study in psychology, comparative cognition and economics for more than a century, and more recently, in artificial intelligence. However, despite the fact that other tests have been created to evaluate other cognitive abilities, nowadays it is still difficult to find a proper test to evaluate social intelligence. Also, current tests tend to be focussed on evaluating the ability of a single species and it is even more complicated to find a test to evaluate social intelligence that is applicable to machines. In fact, tests designed to succeed on a task that requires social intelligence usually also require other abilities to succeed in the task, making them not appropriate for this purpose. This lack of general socially-oriented tests may be due to the absence of a precise (and formal) definition of social intelligence.\nIn this paper we formalise a definition of social intelligence and some useful social properties for multi-agent environments. In particular, the contributions are:\n• We analysed what social behaviour implies, reviewing what it means from several disciplines.\n• We have considered various options for a definition, and we finally proposed a parametrised definition that formalises the notion of performing well in an environment with other (social) agents. We also indicate how a test can be constructed using this definition.\n• We proposed some properties along with their formal definitions in order to better analyse the appropriateness of an environment to evaluate social intelligence.\n• With this definition of social intelligence and the properties proposed, we analysed and compared several tests and games from artificial intelligence and game theory where social intelligence has an important role to see which properties they meet and which can be improved in order to evaluate social intelligence.\nThis definition of social intelligence along with the properties proposed here are a first attempt in order to determine whether a testbed is useful for the assessment of social intelligence. As far as we know, this is the first approximation to provide a formal definition of social intelligence along with some useful properties to judge a certain testbed. More research will provide us more information about which properties can be improved and information about other properties to complement the ones presented here.\nThe characterisation of social intelligence proposed here along with the properties we propose to determine whether an environment is useful to evaluate social intelligence has some open features to be solved.\n• First, it is not clear which utility function the agents must have. Should they be regulated using a discount factor as usual in reinforcement learning? Should we give more importance to later rewards, when the agents are supposed to understand how to behave? Or is it better to use an average (as we did in this paper), giving the same importance to all the rewards?\n• Second, every test should provide which level of complexity it is evaluating. We postulated that the level of complexity should be determined by the agents included in the environment (and their intelligence), the agent setting that determines how teams are formed and the environment where the agent is evaluated. We determined how the first two parameters should influence the formula, but without indicating the formula itself. Should the level of intelligence of the agents weight more than the agent setting, or should it be otherwise? How can we consider the environment in this formula?\n• Third, we noticed that every space used to define an environment necessarily evaluates some spatial intelligence. In theory, we should calculate which part of the result comes from spatial intelligence and subtract it, but this seems very difficult. Alternatively, we could figure out an environment class where no other abilities needed to interact will be really useful. Or at least with a wide variety such that, on average, these other abilities do not bias the result.\n• Fourth, we could also relax the definition of social intelligence by letting the agents cooperate without placing them in a team. Life has taught us that alliances can arise from several agents, even when they do not share the same objectives, to improve the chances of success. It would be interesting to analyse whether a test evaluating only competition can indirectly evaluate this spontaneous cooperation.\n• Fifth, social intelligence is linked to communication and language. We have not included any property or feature in the definitions to account for the presence of communication and language, or to facilitate that. Clearly, communication is possible through actions. Even language can be transmitted by the agent actions with a proper coding. However, this could be rendered more easily to agents. Nonetheless, any particular communication protocol can make the test non-universal. Instead we think that some extra actions that could be observed immediately by other agents (or by a subset of them) could be basic enough as a signal.\nThis article goes beyond the simple properties of game theory in many ways, opening a number of possibilities for the evaluation of multi-agent environments. Of course, further research is needed to clarify all these questions. We have set some formal principles and made the difficulties arise. In fact, the evaluation of non-social intelligence itself has not been fully achieved and it is still being investigated. The evaluation of social intelligence is still more convoluted. This article provides the basis of how we can evaluate social intelligence in a formal way following these principles."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by the MEC projects EXPLORA-INGENIO TIN 2009-06078-E, CONSOLIDERINGENIO 26706 and TINs 2010-21062-C02-02, 2013-45732-C4-1-P and GVA projects PROMETEO/2008/051 and PROMETEO/2011/052. Javier Insa-Cabrera was sponsored by Spanish MEC-FPU grant AP2010-4389."
    }, {
      "heading" : "Appendix",
      "text" : "Before starting with each of the environments, we will prove a lemma that will be helpful for the Left and Right ranges.\nWe could calculate Left and Right using Πe and Πo with a high number of agents. However, the more agents we include the more difficult the calculation becomes. Instead of this, and in order to simplify calculations, we can just use the minimum necessary number of agents in Πe and Πo for that property to obtain the maximum/minimum value following the idea on lemma 1:\nLemma 1. In order to calculate Left/Right maximum/minimum value for a property Prop, the length of the set of evaluated agents |Πe| and the length of the set of opponents and team players |Πo| can be respectively equal to the minimum number of evaluated agents n and agents to populate the environment m needed to calculate Prop.\nProof. Let Πe = {π1, . . . , πn, . . . , πp} be the set of agents evaluated with weight wΠe in an environment µ with weight of slots wS using a set of opponents and team players Πo and wL̇ as a weight for line-up patterns.\nLet us suppose that we want to calculate the value for a property Prop which needs n evaluated agents to be defined, its definition calculates first the value for each evaluated agent π and then these values are weighted using wΠe(π) to provide the property value. Following this definition we obtain a list of values (v1, . . . , vn, . . . , vp) (one for each evaluated agent) that will be weighted with wΠe to obtain the property value v. If we get rid of the evaluated agent which obtains the maximum value for Prop and we rearrange wΠe to sum 1, then the new property value v′ will always be lower than v. We can repeat this process until n agents remain in Πe (i.e., |Πe| = n) to obtain vmin for this set of evaluated agents. An analogous process applies to obtaining vmax by getting rid of the evaluated agent which obtains the minimum value for Prop.\nThe same reasoning applies to the properties that calculate the values for pairs of evaluated agents, but in this case we get rid of the agent whose sum of values (the values of the pairs where this agent is used) is highest/lowest. Also, the same reasoning applies for Πo."
    }, {
      "heading" : "A Matching Pennies properties",
      "text" : "In this section we prove how we obtained the values for the properties for the matching pennies environment (section 5.2). To calculate some of the values for the properties, we make use of lemma 2.\nLemma 2. In the matching pennies environment and for any slot, introducing a random agent πr in a line-up will always provide an expected average reward equal to 0 for both agents.\nProof. A random agent πr has a probability of p j r,h = p j r,t = 1 2 to perform both Head and Tail at iteration j. Let us denote with πs the agent that πr is interacting with, and denote with p 1 s,h the probability of performing Head and p1s,t = 1− p1s,h the probability of performing Tail at the first iteration for πs. To calculate the expected reward of an agent, we sum the possible rewards that this agent can obtain multiplied by the probability that these rewards occurs. When we calculate the expected reward for πr for the first iteration in the matching pennies environment µ in any slot i, we obtain:\n∀i : R1i (πr, µ) = p1r,h ( p1s,h × r1h,h,i + p1s,t × r1h,t,i ) + p1r,t ( p1s,h × r1t,h,i + p1s,t × r1t,t,i ) where rja1,a2,i is the reward that the agent in slot i obtains at iteration j when one agent performs a1 and the other agent performs a2.\nFrom the matching pennies’ payoff matrix (figure 4), we can see that for every slot i, rh,h,i = rt,t,i and rh,t,i = rt,h,i, so we name them re,i and rd,i respectively. We can also see that the reward values are the inverse of each other, having rd,i = −re,i. Renaming the rewards in the formula and rearranging it we obtain:\n∀i : R1i (πr, µ) = p1r,h ( p1s,h × re,i + p1s,t × rd,i ) + p1r,t ( p1s,h × rd,i + p1s,t × re,i ) =\n= p1r,h ( p1s,h × re,i + p1s,t × (−re,i) ) + p1r,t ( p1s,h × (−re,i) + p1s,t × re,i ) =\n= p1r,h ( re,i × ( p1s,h − p1s,t )) + p1r,t ( (−re,i)× ( p1s,h − p1s,t )) =\n= p1r,h ( re,i × ( p1s,h − p1s,t )) − p1r,t ( re,i × ( p1s,h − p1s,t )) =\n= ( p1r,h − p1r,t ) × ( re,i × ( p1s,h − p1s,t )) And since πr gives the same probability to both Head and Tail (p 1 r,h = p 1 r,t = 1 2 ) we obtain the following\nexpected reward:\n∀i : R1i (πr, µ) = ( 1\n2 − 1 2\n) × ( re,i × ( p1s,h − p1s,t )) = 0\nWe calculated the expected reward for the first iteration. At this point πs could change its behaviour depending on what happened on the previous iteration, using different probabilities p2s,h and p 2 s,t for iteration 2. But note that it does not matter which probabilities pns,h and p n s,t we use, the result will still be 0, and averaging over the iterations we obtain an expected average reward equal to 0. Obviously, since this is a zero-sum game, when πr obtains an expected average reward of 0, πs obtains the same expected average reward of 0."
    }, {
      "heading" : "A.1 Action Dependency",
      "text" : "We start with the action dependency (AD) property. As given in section 4.2.1, we want to know if the evaluated agents behave differently depending on which line-up they interact with. We use ∆S(a, b) = 1 if distributions a and b are equal and 0 otherwise.\nProposition 2. Generalmin for the action dependency (AD) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πt} with wΠe(πt) = 1 and Πo = {πh1, πh2} (a πh agent always performs Head and a πt agent always performs Tail).\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(πt,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πt]), Ă1(µ[v̇ 1← πt])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă1(µ[πt, πh1]), Ă1(µ[πt, πh2]))\nNote that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nIn this case, we only need to calculate ∆S(Ă1(µ[πt, πh1 ]), Ă1(µ[πt, πh2 ])), where the agent in both slots 1 (πt) will perform the same sequence of actions (always Tail) independently of the line-up. So:\nAD1(πt,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 2, the agent in both slots 2 (πt) will also perform the same sequence of actions (always Tail) independently of the line-up. So:\nAD2(πt,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă2(µ[u̇ 2← πt]), Ă2(µ[v̇ 2← πt])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă2(µ[πh1, πt]), Ă2(µ[πh2, πt])) =\n= 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)ADi(πt,Πo, wL̇, µ) = = 1\n2 {AD1(πt,Πo, wL̇, µ) +AD2(πt,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSince 0 is the lowest possible value for the action dependency property, therefore matching pennies has Generalmin = 0 for this property.\nProposition 3. Generalmax for the action dependency (AD) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πm} with wΠe(πm) = 1 and Πo = {πh, πt} (a πm agent first acts randomly and then always mimics the other agent’s last action, a πh agent always performs Head and a πt agent always performs Tail).\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(πm,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πm]), Ă1(µ[v̇ 1← πm])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă1(µ[πm, πh]), Ă1(µ[πm, πt]))\nNote that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nFrom iteration 2, πm will mimic the last action of the agent in slot 2, and since πh will always perform Head and πt will always perform Tail, the agent in both slots 1 (πm) will perform different sequences of actions on each line-up. So:\nAD1(πm,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd for slot 2, the agent in both slots 2 (πm) will also perform different sequences of actions on each line-up. So:\nAD2(πm,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă2(µ[u̇ 2← πm]), Ă2(µ[v̇ 2← πm])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă2(µ[πh, πm]), Ă2(µ[πt, πm])) =\n= 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, we weight over the slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)ADi(πm,Πo, wL̇, µ) = = 1\n2 {AD1(πm,Πo, wL̇, µ) +AD2(πm,Πo, wL̇, µ)} =\n= 1\n2 {1 + 1} = 1\nSince 1 is the highest possible value for the action dependency property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 4. Leftmax for the action dependency (AD) property is equal to 0 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πo = {πh1, πh2} (a πh agent always performs Head) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we can calculate its AD value for each slot. We start with slot 1:\nAD1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)AD1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate AD1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 12 to calculate this value for a figurative evaluated agent π from Πe:\nAD1(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← π]), Ă1(µ[v̇ 1← π])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă1(µ[π, πh1]), Ă1(µ[π, πh2]))\nNote that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nA πh agent will always perform Head, so we obtain a situation where the agent in both slots 1 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent’s behaviour. So:\nAD1(π,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nAD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd for slot 2:\nAD2(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)AD2(π,Πo, wL̇, µ)\nAgain, we do not know which Πe we have, but we know that we will need to evaluate AD2(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 12 to calculate this value for a figurative evaluated agent π from Πe:\nAD2(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă2(µ[u̇ 2← π]), Ă2(µ[v̇ 2← π])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă2(µ[πh1, π]), Ă2(µ[πh2, π]))\nAgain, a πh agent will always perform Head, so we obtain a situation where the agent in both slots 2 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent’s behaviour. So:\nAD2(π,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nAD2(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\n2 {AD1(Πe, wΠe ,Πo, wL̇, µ) +AD2(Πe, wΠe ,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : AD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, matching pennies has Leftmax = 0 for this property.\nProposition 5. Rightmin for the action dependency (AD) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πh} with wΠe(πh) = 1 (a πh agent always performs Head) we find this situation no matter which Πo we use.\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(πh,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πh]), Ă1(µ[v̇ 1← πh]))\nWe do not know which Πo we have, but we know that we will need to obtain two different line-up patterns\nu̇ and v̇ from L̇ N(µ) −1 (Πo) to calculate ∆S(Ă1(µ[u̇ 1← πh]), Ă1(µ[v̇ 1← πh])). We calculate this value for two figurative line-up patterns u̇ = (∗, π1) and v̇ = (∗, π2) from L̇N(µ)−1 (Πo):\n∆S(Ă1(µ[u̇ 1← πh]), Ă1(µ[v̇ 1← πh])) = ∆S(Ă1(µ[πh, π1]), Ă1(µ[πh, π2]))\nHere, the agent in both slots 1 (πh) will perform the same sequence of actions (always Head) independently of the line-up. So no matter which agents are in Πo we obtain:\nAD1(πh,Πo, wL̇, µ) = 0\nAnd for slot 2: AD2(πh,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă2(µ[u̇ 2← πh]), Ă2(µ[v̇ 2← πh]))\nAgain, we do not know which Πo we have, but we know that we will need to obtain two different line-up\npatterns u̇ and v̇ from L̇ N(µ) −2 (Πo) to calculate ∆S(Ă2(µ[u̇ 2← πh]), Ă2(µ[v̇ 2← πh])). We calculate this value for two figurative line-up patterns u̇ = (∗, π1) and v̇ = (∗, π2) from L̇N(µ)−2 (Πo):\n∆S(Ă2(µ[u̇ 2← πh]), Ă2(µ[v̇ 2← πh])) = ∆S(Ă2(µ[π1, πh]), Ă2(µ[π2, πh]))\nAgain, the agent in both slots 2 (πh) will perform the same sequence of actions (always Head) independently of the line-up. So no matter which agents are in Πo we obtain:\nAD2(πh,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)ADi(πh,Πo, wL̇, µ) = = 1\n2 {AD1(πh,Πo, wL̇, µ) +AD2(πh,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSo, for every Πo we obtain the same result:\n∀Πo : AD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, matching pennies has Rightmin = 0 for this property."
    }, {
      "heading" : "A.2 Reward Dependency",
      "text" : "We continue with the reward dependency (RD) property. As given in section 4.3.1, we want to know if the evaluated agents obtain different expected average rewards depending on which line-up they interact with. We use ∆Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 6. Generalmin for the reward dependency (RD) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πt} with wΠe(πt) = 1 and Πo = {πh1, πh2} (a πh agent always performs Head and a πt agent always performs Tail).\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(πt,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← πt]), R1(µ[v̇ 1← πt])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R1(µ[πt, πh1]), R1(µ[πt, πh2]))\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nIn this case, we only need to calculate ∆Q(R1(µ[πt, πh1]), R1(µ[πt, πh2])), where πt will always perform Tail and a πh agent will always perform Head, so the agent in both slots 1 (πt) will obtain the same expected average reward (−1) independently of the line-up. So:\nRD1(πt,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 2, the agent in both slots 2 (πt) will also obtain the same expected average reward (1) independently of the line-up. So:\nRD2(πt,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R2(µ[u̇ 2← πt]), R2(µ[v̇ 2← πt])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R2(µ[πh1, πt]), R2(µ[πh2, πt])) =\n= 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)RDi(πt,Πo, wL̇, µ) = = 1\n2 {RD1(πt,Πo, wL̇, µ) +RD2(πt,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSince 0 is the lowest possible value for the reward dependency property, therefore matching pennies has Generalmin = 0 for this property.\nProposition 7. Generalmax for the reward dependency (RD) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πt} with wΠe(πt) = 1 and Πo = {πh, πt} (a πh agent always performs Head and a πt agent always performs Tail).\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(πt,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← πt]), R1(µ[v̇ 1← πt])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R1(µ[πt, πh]), R1(µ[πt, πt]))\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nIn line-up (πt, πh), where πt will always perform Tail and πh will always perform Head, the agent in slot 1 (πt) will obtain one expected average reward (−1), while in line-up (πt, πt), where both πt will always perform Tail, the agent in slot 1 (πt) will obtain a different expected average reward (1). So:\nRD1(πt,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd for slot 2, the agent in slot 2 (πt) will also obtain different expected average rewards depending on the line-up. So:\nRD2(πt,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R2(µ[u̇ 2← πt]), R2(µ[v̇ 2← πt])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R2(µ[πh, πt]), R2(µ[πt, πt]))\n= 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, we weight over the slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)RDi(πt,Πo, wL̇, µ) = = 1\n2 {RD1(πt,Πo, wL̇, µ) +RD2(πt,Πo, wL̇, µ)} =\n= 1\n2 {1 + 1} = 1\nSince 1 is the highest possible value for the reward dependency property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 8. Leftmax for the reward dependency (RD) property is equal to 0 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πo = {πh1, πh2} (a πh agent always performs Head) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we can calculate its RD value for each slot. We start with slot 1:\nRD1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)RD1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate RD1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 15 to calculate this value for a figurative evaluated agent π from Πe:\nRD1(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← π]), R1(µ[v̇ 1← π])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R1(µ[π, πh1]), R1(µ[π, πh2]))\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nA πh agent will always perform Head, so we obtain a situation where the agent in both slots 1 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action\nsequences depending on the opponent’s behaviour, obtaining agent in both slots 1 (any π) the same expected average reward. So:\nRD1(π,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nRD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd for slot 2:\nRD2(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)RD2(π,Πo, wL̇, µ)\nAgain, we do not know which Πe we have, but we know that we will need to evaluate RD2(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 15 to calculate this value for a figurative evaluated agent π from Πe:\nRD2(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R2(µ[u̇ 2← π]), R2(µ[v̇ 2← π])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R2(µ[πh1, π]), R2(µ[πh2, π]))\nAgain, a πh agent will always perform Head, so we obtain a situation where the agent in both slots 2 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent’s behaviour, obtaining agent in both slots 2 (any π) the same expected average reward. So:\nRD2(π,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nRD2(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\n2 {RD1(Πe, wΠe ,Πo, wL̇, µ) +RD2(Πe, wΠe ,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : RD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, matching pennies has Leftmax = 0 for this property.\nProposition 9. Rightmin for the reward dependency (RD) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πr} with wΠe(πr) = 1 (a πr agent always acts randomly) we find this situation no matter which Πo we use.\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(πr,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← πr]), R1(µ[v̇ 1← πr]))\nWe do not know which Πo we have, but we know that we will need to obtain two different line-up patterns u̇\nand v̇ from L̇ N(µ) −1 (Πo) to calculate ∆Q(R1(µ[u̇ 1← πr]), R1(µ[v̇ 1← πr])). We calculate this value for two figurative line-up patterns u̇ = (∗, π1) and v̇ = (∗, π2) from L̇N(µ)−1 (Πo):\n∆Q(R1(µ[u̇ 1← πr]), R1(µ[v̇ 1← πr])) = ∆Q(R1(µ[πr, π1]), R1(µ[πr, π2]))\nHere, the agent in both slots 1 (πr) makes its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So no matter which agents are in Πo we obtain:\nRD1(πr,Πo, wL̇, µ) = 0\nAnd for slot 2: RD2(πr,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R2(µ[u̇ 2← πr]), R2(µ[v̇ 2← πr]))\nAgain, we do not know which Πo we have, but we know that we will need to obtain two different line-up\npatterns u̇ and v̇ from L̇ N(µ) −2 (Πo) to calculate ∆Q(R2(µ[u̇ 2← πr]), R2(µ[v̇ 2← πr])). We calculate this value for two figurative line-up patterns u̇ = (∗, π1) and v̇ = (∗, π2) from L̇N(µ)−2 (Πo):\n∆Q(R2(µ[u̇ 2← πr]), R2(µ[v̇ 2← πr])) = ∆Q(R2(µ[π1, πr]), R2(µ[π2, πr]))\nAgain, the agent in both slots 2 (πr) makes its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So no matter which agents are in Πo we obtain:\nRD2(πr,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)RDi(πr,Πo, wL̇, µ) = = 1\n2 {RD1(πr,Πo, wL̇, µ) +RD2(πr,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSo, for every Πo we obtain the same result:\n∀Πo : RD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, matching pennies has Rightmin = 0 for this property."
    }, {
      "heading" : "A.3 Fine Discrimination",
      "text" : "Now we move to the fine discrimination (FD) property. As given in section 4.4.1, we want to know if different evaluated agents obtain different expected average rewards when interacting in the environment. We use ∆Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 10. Generalmin for the fine discrimination (FD) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πt1, πt2} with uniform weight for wΠe and Πo = {πh} (a πh agent always performs Head and a πt agent always performs Tail).\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πt1, πt2,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πt1, πt2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD1(πt1, πt2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πt1]), R1(µ[l̇ 1← πt2])) =\n= ∆Q(R1(µ[πt1, πh]), R1(µ[πt2, πh]))\nHere, a πt agent will always perform Tail and πh will always perform Head, so both agents in slot 1 (πt1 and πt2) will obtain the same expected average reward (−1). So:\nFD1(πt1, πt2,Πo, wL̇, µ) = 0\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 2: FD2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD2(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD2(πt1, πt2,Πo, wL̇, µ)\nIn this case, we only need to calculate FD2(πt1, πt2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD2(πt1, πt2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2 (Πo)\nwL̇(l̇)∆Q(R2(µ[l̇ 2← πt1]), R2(µ[l̇ 2← πt2])) =\n= ∆Q(R2(µ[πh, πt1]), R2(µ[πh, πt2]))\nHere, πh will always perform Head and a πt agent will always perform Tail, so both agents in slot 2 (πt1 and πt2) will obtain the same expected average reward (1). So:\nFD2(πt1, πt2,Πo, wL̇, µ) = 0\nTherefore:\nFD2(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\n2 {FD1(Πe, wΠe ,Πo, wL̇, µ) + FD2(Πe, wΠe ,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSince 0 is the lowest possible value for the fine discriminative property, therefore matching pennies has Generalmin = 0 for this property.\nProposition 11. Generalmax for the fine discrimination (FD) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πh, πt} with uniform weight for wΠe and Πo = {πh} (a πh agent always performs Head and a πt agent always performs Tail).\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πh, πt,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πh, πt,Πo, wL̇, µ). We follow definition 21 to calculate this value: FD1(πh, πt,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πh]), R1(µ[l̇ 1← πt])) =\n= ∆Q(R1(µ[πh, πh]), R1(µ[πt, πh]))\nIn line-up (πh, πh), where both πh will always perform Head, the agent in slot 1 (πh) will obtain one expected average reward (1), while in line-up (πt, πh), where πt will always perform Tail and πh will always perform Head, the agent in slot 1 (πt) will obtain a different expected average reward (−1). So:\nFD1(πh, πt,Πo, wL̇, µ) = 1\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd for slot 2: FD2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD2(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD2(πh, πt,Πo, wL̇, µ)\nIn this case, we only need to calculate FD2(πh, πt,Πo, wL̇, µ). We follow definition 21 to calculate this value: FD2(πh, πt,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2 (Πo)\nwL̇(l̇)∆Q(R2(µ[l̇ 2← πh]), R2(µ[l̇ 2← πt])) =\n= ∆Q(R2(µ[πh, πh]), R2(µ[πh, πt]))\nIn line-up (πh, πh), where both πh will always perform Head, the agent in slot 2 (πh) will obtain one expected average reward (−1), while in line-up (πh, πt), where πh will always perform Head and πt will always perform Head, the agent in slot 2 (πt) will obtain a different expected average reward (1). So:\nFD2(πh, πt,Πo, wL̇, µ) = 1\nTherefore:\nFD2(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, we weight over the slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\n2 {FD1(Πe, wΠe ,Πo, wL̇, µ) + FD2(Πe, wΠe ,Πo, wL̇, µ)} =\n= 1\n2 {1 + 1} = 1\nSince 1 is the highest possible value for the fine discriminative property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 12. Leftmax for the fine discrimination (FD) property is equal to 0 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πo = {πr} (a πr agent always acts randomly) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate FD1(π1, π2,Πo, wL̇, µ) for all pair of evaluated agents π1, π2 ∈ Πe|π1 ̸= π2. We follow definition 21 to calculate this value for two figurative evaluated agents π1 and π2 from Πe such that π1 ̸= π2:\nFD1(π1, π2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← π1]), R1(µ[l̇ 1← π2])) =\n= ∆Q(R1(µ[π1, πr]), R1(µ[π2, πr]))\nHere, the agent in both slots 2 (πr) makes its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So no matter which agents π1 and π2 are we obtain:\nFD1(π1, π2,Πo, wL̇, µ) = 0\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd for slot 2: FD2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD2(π1, π2,Πo, wL̇, µ)\nAgain, we do not know which Πe we have, but we know that we will need to evaluate FD2(π1, π2,Πo, wL̇, µ) for all pair of evaluated agents π1, π2 ∈ Πe|π1 ̸= π2. We follow definition 21 to calculate this value for two figurative evaluated agents π1 and π2 from Πe such that π1 ̸= π2:\nFD2(π1, π2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2 (Πo)\nwL̇(l̇)∆Q(R2(µ[l̇ 2← π1]), R1(µ[l̇ 2← π2])) =\n= ∆Q(R2(µ[πr, π1]), R2(µ[πr, π2]))\nAgain, the agent in both slots 1 (πr) makes its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So no matter which agents π1 and π2 are we obtain:\nFD2(π1, π2,Πo, wL̇, µ) = 0\nTherefore:\nFD2(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ)\n= 1\n2 {FD1(Πe, wΠe ,Πo, wL̇, µ) + FD2(Πe, wΠe ,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : FD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, matching pennies has Leftmax = 0 for this property.\nProposition 13. Rightmin for the fine discrimination (FD) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πt1, πt2} with uniform weight for wΠe (a πt agent always performs Tail) we find this situation no matter which Πo we use.\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πt1, πt2,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πt1, πt2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD1(πt1, πt2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πt1]), R1(µ[l̇ 1← πt2]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −1 (Πo) to calculate ∆Q(R1(µ[l̇ 1← πt1]), R1(µ[l̇ 1← πt2])). We calculate this value for a figurative line-up pattern l̇ = (∗, π) from L̇N(µ)−1 (Πo):\n∆Q(R1(µ[l̇ 1← πt1]), R1(µ[l̇ 1← πt2])) = ∆Q(R1(µ[πt1, π]), R1(µ[πt2, π]))\nA πt agent will always perform Tail, so we obtain a situation where the agent in both slots 2 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent’s behaviour, obtaining both agents in slot 1 (πt1 and πt2) the same expected average reward. So:\nFD1(πt1, πt2,Πo, wL̇, µ) = 0\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 2: FD2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD2(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD2(πt1, πt2,Πo, wL̇, µ)\nIn this case, we only need to calculate FD2(πt1, πt2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD2(πt1, πt2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2 (Πo)\nwL̇(l̇)∆Q(R2(µ[l̇ 2← πt1]), R2(µ[l̇ 2← πt2]))\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −2 (Πo) to calculate ∆Q(R2(µ[l̇ 2← πt1]), R2(µ[l̇ 2← πt2])). We calculate this value for a figurative line-up pattern l̇ = (π, ∗) from L̇N(µ)−2 (Πo):\n∆Q(R2(µ[l̇ 2← πt1]), R2(µ[l̇ 2← πt2])) = ∆Q(R2(µ[π, πt1]), R2(µ[π, πt2]))\nAgain, a πt agent will always perform Tail, so we obtain a situation where the agent in both slots 1 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent’s behaviour, obtaining both agents in slot 2 (πt1 and πt2) the same expected average reward. So:\nFD2(πt1, πt2,Πo, wL̇, µ) = 0\nTherefore:\nFD2(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ)\n= 1\n2 {FD1(Πe, wΠe ,Πo, wL̇, µ) + FD2(Πe, wΠe ,Πo, wL̇, µ)} =\n= 1\n2 {0 + 0} = 0\nSo, for every Πo we obtain the same result:\n∀Πo : FD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, matching pennies has Rightmin = 0 for this property."
    }, {
      "heading" : "A.4 Strict Total Grading",
      "text" : "We arrive to the strict total grading (STG) property. As given in section 4.4.2, we want to know if there exists a strict ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the STO: Ri(µ[l̇ i,j← π1, π2]) < Rj(µ[l̇ i,j← π1, π2]), Ri(µ[l̇ i,j← π2, π3]) < Rj(µ[l̇ i,j← π2, π3]) and Ri(µ[l̇ i,j← π1, π3]) < Rj(µ[l̇ i,j← π1, π3]).\nSlot i Slot j π1 < π2 π2 < π3 π1 < π3\nProposition 14. Generalmin for the strict total grading (STG) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πr1, πr2, πr3} with uniform weight for wΠe and Πo = ∅ (a πr agent always acts randomly).\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πr1, πr2, πr3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πr1, πr2, πr3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πr1, πr2, πr3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πr1, πr2, πr3, l̇, µ) =\n= STO1,2(πr1, πr2, πr3, (∗, ∗), µ)\nThe following table shows us STO1,2 for all the permutations of πr1, πr2, πr3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πr1 < πr2 πr1 < πr3 πr2 < πr1 πr2 < πr3 πr3 < πr2 πr1 < πr3 πr1 < πr3 πr1 < πr2 πr2 < πr3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πr2 < πr3 πr3 < πr1 πr3 < πr2 πr3 < πr1 πr1 < πr2 πr2 < πr1 πr2 < πr1 πr3 < πr2 πr3 < πr1\nBut, it is not possible to find a STO, since for every permutation we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nSTG1,2(πr1, πr2, πr3,Πo, wL̇, µ) = 0\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 2 and 1:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(πr1, πr2, πr3,Πo, wL̇, µ)\nAgain, we only need to calculate STG2,1(πr1, πr2, πr3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,1(πr1, πr2, πr3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)STO2,1(πr1, πr2, πr3, l̇, µ) =\n= STO2,1(πr1, πr2, πr3, (∗, ∗), µ) The following table shows us STO2,1 for all the permutations of πr1, πr2, πr3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πr1 < πr2 πr1 < πr3 πr2 < πr1 πr2 < πr3 πr3 < πr2 πr1 < πr3 πr1 < πr3 πr1 < πr2 πr2 < πr3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πr2 < πr3 πr3 < πr1 πr3 < πr2 πr3 < πr1 πr1 < πr2 πr2 < πr1 πr2 < πr1 πr3 < πr2 πr3 < πr1\nAgain, it is not possible to find a STO, since for every permutation we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nSTG2,1(πr1, πr2, πr3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {STG1,2(Πe, wΠe ,Πo, wL̇, µ) + STG2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {0 + 0} = 0\nSince 0 is the lowest possible value for the strict total grading property, therefore matching pennies has Generalmin = 0 for this property.\nProposition 15. Generalmax for the strict total grading (STG) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πh, πh/t, πm/o} with uniform weight for wΠe and Πo = ∅ (a πh agent always performs Head, a πh/t agent always performs Head when playing in slot 1 and always performs Tail when playing in slot 2, and a πm/o agent always mimics the last action of its opponent when playing in slot 1 and always performs the opposite of this action when playing in slot 2)11.\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πh, πh/t, πm/o,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πh, πh/t, πm/o,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πh, πh/t, πm/o,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πh, πh/t, πm/o, l̇, µ) =\n= STO1,2(πh, πh/t, πm/o, (∗, ∗), µ)\nThe following table shows us STO1,2 for all the permutations of πh, πh/t, πm/o.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πh < πh/t πh < πm/o πh/t < πh πh/t < πm/o πm/o < πh/t πh < πm/o πh < πm/o πh < πh/t πh/t < πm/o Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πh/t < πm/o πm/o < πh πm/o < πh/t πm/o < πh πh < πh/t πh/t < πh πh/t < πh πm/o < πh/t πm/o < πh\nIt is possible to find a STO for the first permutation. In πh < πh/t, πh will always perform Head and πh/t will always perform Tail, so they will obtain an expected average reward of −1 and 1 respectively. In πh/t < πm/o, πh/t will always perform Head and πm/o will always perform Tail, so they will obtain an expected average reward of −1 and 1 respectively. In πh < πm/o, πh will always perform Head and πm/o will always perform Tail, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG1,2(πh, πh/t, πm/o,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 2 and 1:\n11πh/t and πm/o have to know in which slot they are playing. To infer this, they start with a random action at the first iteration and then look at the action of their opponent and the reward they obtain.\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(πh, πh/t, πm/o,Πo, wL̇, µ)\nAgain, we only need to calculate STG2,1(πh, πh/t, πm/o,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,1(πh, πh/t, πm/o,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)STO2,1(πh, πh/t, πm/o, l̇, µ) =\n= STO2,1(πh, πh/t, πm/o, (∗, ∗), µ)\nThe following table shows us STO2,1 for all the permutations of πh, πh/t, πm/o.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πh < πh/t πh < πm/o πh/t < πh πh/t < πm/o πm/o < πh/t πh < πm/o πh < πm/o πh < πh/t πh/t < πm/o Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πh/t < πm/o πm/o < πh πm/o < πh/t πm/o < πh πh < πh/t πh/t < πh πh/t < πh πm/o < πh/t πm/o < πh\nAgain, it is possible to find a STO for the first permutation. In πh < πh/t, πh will always perform Head and πh/t will always perform Head, so they will obtain an expected average reward of −1 and 1 respectively. In πh/t < πm/o, πh/t will always perform Tail and πm/o will always perform Tail, so they will obtain an expected average reward of −1 and 1 respectively. In πh < πm/o, πh will always perform Head and πm/o will always perform Head, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG2,1(πh, πh/t, πm/o,Πo, wL̇, µ) = 1\nTherefore:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {STG1,2(Πe, wΠe ,Πo, wL̇, µ) + STG2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {1 + 1} = 1\nSince 1 is the highest possible value for the strict total grading property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 16. Leftmax for the strict total grading (STG) property is equal to 1 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πe = {πh, πh/t, πm/o} with uniform weight for wΠe (a πh agent always performs Head, a πh/t agent always performs Head when playing in slot 1 and always performs Tail when playing in slot 2, and a πm/o agent always mimics the last action of its opponent when playing in slot 1 and always performs the opposite of this action when playing in slot 2)12 we find this situation no matter which Πo we use.\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πh, πh/t, πm/o,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πh, πh/t, πm/o,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πh, πh/t, πm/o,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πh, πh/t, πm/o, l̇, µ) =\n= STO1,2(πh, πh/t, πm/o, (∗, ∗), µ)\nNote that the choice of Πo does not affect the result of STG1,2. The following table shows us STO1,2 for all the permutations of πh, πh/t, πm/o.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πh < πh/t πh < πm/o πh/t < πh πh/t < πm/o πm/o < πh/t πh < πm/o πh < πm/o πh < πh/t πh/t < πm/o Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πh/t < πm/o πm/o < πh πm/o < πh/t πm/o < πh πh < πh/t πh/t < πh πh/t < πh πm/o < πh/t πm/o < πh\nIt is possible to find a STO for the first permutation. In πh < πh/t, πh will always perform Head and πh/t will always perform Tail, so they will obtain an expected average reward of −1 and 1 respectively. In πh/t < πm/o, πh/t will always perform Head and πm/o will always perform Tail, so they will obtain an expected average reward of −1 and 1 respectively. In πh < πm/o, πh will always perform Head and πm/o will always perform Tail, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG1,2(πh, πh/t, πm/o,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 2 and 1:\n12πh/t and πm/o have to know in which slot they are playing. To infer this, they start with a random action at the first iteration and then look at the action of their opponent and the reward they obtain.\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(πh, πh/t, πm/o,Πo, wL̇, µ)\nAgain, we only need to calculate STG2,1(πh, πh/t, πm/o,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,1(πh, πh/t, πm/o,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)STO2,1(πh, πh/t, πm/o, l̇, µ) =\n= STO2,1(πh, πh/t, πm/o, (∗, ∗), µ)\nNote again that the choice of Πo does not affect the result of STG2,1. The following table shows us STO2,1 for all the permutations of πh, πh/t, πm/o.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πh < πh/t πh < πm/o πh/t < πh πh/t < πm/o πm/o < πh/t πh < πm/o πh < πm/o πh < πh/t πh/t < πm/o Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πh/t < πm/o πm/o < πh πm/o < πh/t πm/o < πh πh < πh/t πh/t < πh πh/t < πh πm/o < πh/t πm/o < πh\nAgain, it is possible to find a STO for the first permutation. In πh < πh/t, πh will always perform Head and πh/t will always perform Head, so they will obtain an expected average reward of −1 and 1 respectively. In πh/t < πm/o, πh/t will always perform Tail and πm/o will always perform Tail, so they will obtain an expected average reward of −1 and 1 respectively. In πh < πm/o, πh will always perform Head and πm/o will always perform Head, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG2,1(πh, πh/t, πm/o,Πo, wL̇, µ) = 1\nTherefore:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {STG1,2(Πe, wΠe ,Πo, wL̇, µ) + STG2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {1 + 1} = 1\nSince 1 is the highest possible value for the strict total grading property and no Πo is able to influence this result, therefore matching pennies has Leftmax = 1 for this property.\nProposition 17. Rightmin for the strict total grading (STG) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πr1, πr2, πr3} with uniform weight for wΠe (a πr agent always acts randomly) we find this situation no matter which Πo we use.\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πr1, πr2, πr3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πr1, πr2, πr3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πr1, πr2, πr3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πr1, πr2, πr3, l̇, µ) =\n= STO1,2(πr1, πr2, πr3, (∗, ∗), µ)\nNote that the choice of Πo does not affect the result of STG1,2. The following table shows us STO1,2 for all the permutations of πr1, πr2, πr3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πr1 < πr2 πr1 < πr3 πr2 < πr1 πr2 < πr3 πr3 < πr2 πr1 < πr3 πr1 < πr3 πr1 < πr2 πr2 < πr3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πr2 < πr3 πr3 < πr1 πr3 < πr2 πr3 < πr1 πr1 < πr2 πr2 < πr1 πr2 < πr1 πr3 < πr2 πr3 < πr1\nBut, it is not possible to find a STO, since for every permutation we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nSTG1,2(πr1, πr2, πr3,Πo, wL̇, µ) = 0\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 2 and 1:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(πr1, πr2, πr3,Πo, wL̇, µ)\nAgain, we only need to calculate STG2,1(πr1, πr2, πr3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,1(πr1, πr2, πr3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)STO2,1(πr1, πr2, πr3, l̇, µ) =\n= STO2,1(πr1, πr2, πr3, (∗, ∗), µ)\nNote again that the choice of Πo does not affect the result of STG2,1. The following table shows us STO2,1 for all the permutations of πr1, πr2, πr3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πr1 < πr2 πr1 < πr3 πr2 < πr1 πr2 < πr3 πr3 < πr2 πr1 < πr3 πr1 < πr3 πr1 < πr2 πr2 < πr3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πr2 < πr3 πr3 < πr1 πr3 < πr2 πr3 < πr1 πr1 < πr2 πr2 < πr1 πr2 < πr1 πr3 < πr2 πr3 < πr1\nAgain, it is not possible to find a STO, since for every permutation we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nSTG2,1(πr1, πr2, πr3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {STG1,2(Πe, wΠe ,Πo, wL̇, µ) + STG2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {0 + 0} = 0\nSince 0 is the lowest possible value for the strict total grading property and no Πo is able to influence this result, therefore matching pennies has Rightmin = 0 for this property."
    }, {
      "heading" : "A.5 Partial Grading",
      "text" : "Now we arrive to the partial grading (PG) property. As given in section 4.4.2, we want to know if there exists a partial ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the PO: Ri(µ[l̇ i,j← π1, π2]) ≤ Rj(µ[l̇ i,j← π1, π2]), Ri(µ[l̇ i,j← π2, π3]) ≤ Rj(µ[l̇ i,j← π2, π3]) and Ri(µ[l̇ i,j← π1, π3]) ≤ Rj(µ[l̇ i,j← π1, π3]).\nSlot i Slot j π1 ≤ π2 π2 ≤ π3 π1 ≤ π3\nProposition 18. Generalmin for the partial grading (PG) property is equal to 0 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πh1, πh2, πt} with uniform weight for wΠe and Πo = ∅ (a πh agent always performs Head and a πt agent always performs Tail).\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πh1, πh2, πt,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πh1, πh2, πt,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πh1, πh2, πt,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πh1, πh2, πt, l̇, µ) =\n= PO1,2(πh1, πh2, πt, (∗, ∗), µ)\nThe following table shows us PO1,2 for all the permutations of πh1, πh2, πt.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πh1 ≤ πh2 πh1 ≤ πt πh2 ≤ πh1 πh2 ≤ πt πt ≤ πh2 πh1 ≤ πt πh1 ≤ πt πh1 ≤ πh2 πh2 ≤ πt Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πh2 ≤ πt πt ≤ πh1 πt ≤ πh2 πt ≤ πh1 πh1 ≤ πh2 πh2 ≤ πh1 πh2 ≤ πh1 πt ≤ πh2 πt ≤ πh1\nBut, it is not possible to find a PO, since for every permutation we have either πh1 ≤ πh2 or πh2 ≤ πh1. In both cases, a πh agent will always perform Head, so they will obtain an expected average reward of 1 and −1 respectively. So:\nPG1,2(πh1, πh2, πt,Πo, wL̇, µ) = 0\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 2 and 1:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(πh1, πh2, πt,Πo, wL̇, µ)\nAgain, we only need to calculate PG2,1(πh1, πh2, πt,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,1(πh1, πh2, πt,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)PO2,1(πh1, πh2, πt, l̇, µ) =\n= PO2,1(πh1, πh2, πt, (∗, ∗), µ) The following table shows us PO2,1 for all the permutations of πh1, πh2, πt.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πh1 ≤ πh2 πh1 ≤ πt πh2 ≤ πh1 πh2 ≤ πt πt ≤ πh2 πh1 ≤ πt πh1 ≤ πt πh1 ≤ πh2 πh2 ≤ πt Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πh2 ≤ πt πt ≤ πh1 πt ≤ πh2 πt ≤ πh1 πh1 ≤ πh2 πh2 ≤ πh1 πh2 ≤ πh1 πt ≤ πh2 πt ≤ πh1\nAgain, it is not possible to find a PO, since for every permutation we have either πh1 ≤ πt or πt ≤ πh1. In πh1 ≤ πt, πh1 will always perform Head and πt will always perform Tail, so they will obtain an expected average reward of 1 and −1 respectively. In πt ≤ πh1, πt will always perform Tail and πh1 will always perform Head, so they will obtain an expected average reward of 1 and −1 respectively. So:\nPG2,1(πh1, πh2, πt,Πo, wL̇, µ) = 0\nTherefore:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {PG1,2(Πe, wΠe ,Πo, wL̇, µ) + PG2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {0 + 0} = 0\nSince 0 is the lowest possible value for the partial grading property, therefore matching pennies hasGeneralmin = 0 for this property.\nProposition 19. Generalmax for the partial grading (PG) property is equal to 1 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πr1, πr2, πr3} with uniform weight for wΠe and Πo = ∅ (a πr agent always acts randomly).\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πr1, πr2, πr3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πr1, πr2, πr3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πr1, πr2, πr3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πh, πh/t, πm/o, l̇, µ) =\n= PO1,2(πr1, πr2, πr3, (∗, ∗), µ)\nThe following table shows us PO1,2 for all the permutations of πr1, πr2, πr3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πr1 ≤ πr2 πr1 ≤ πr3 πr2 ≤ πr1 πr2 ≤ πr3 πr3 ≤ πr2 πr1 ≤ πr3 πr1 ≤ πr3 πr1 ≤ πr2 πr2 ≤ πr3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πr2 ≤ πr3 πr3 ≤ πr1 πr3 ≤ πr2 πr3 ≤ πr1 πr1 ≤ πr2 πr2 ≤ πr1 πr2 ≤ πr1 πr3 ≤ πr2 πr3 ≤ πr1\nIt is possible to find a PO for every permutation, since we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nPG1,2(πr1, πr2, πr3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 2 and 1:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(πr1, πr2, πr3,Πo, wL̇, µ)\nAgain, we only need to calculate PG2,1(πr1, πr2, πr3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,1(πr1, πr2, πr3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)PO2,1(πr1, πr2, πr3, l̇, µ) =\n= PO2,1(πr1, πr2, πr3, (∗, ∗), µ)\nThe following table shows us PO2,1 for all the permutations of πr1, πr2, πr3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πr1 ≤ πr2 πr1 ≤ πr3 πr2 ≤ πr1 πr2 ≤ πr3 πr3 ≤ πr2 πr1 ≤ πr3 πr1 ≤ πr3 πr1 ≤ πr2 πr2 ≤ πr3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πr2 ≤ πr3 πr3 ≤ πr1 πr3 ≤ πr2 πr3 ≤ πr1 πr1 ≤ πr2 πr2 ≤ πr1 πr2 ≤ πr1 πr3 ≤ πr2 πr3 ≤ πr1\nAgain, it is possible to find a PO for every permutation, since we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nPG2,1(πr1, πr2, πr3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {PG1,2(Πe, wΠe ,Πo, wL̇, µ) + PG2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {1 + 1} = 1\nSince 1 is the highest possible value for the partial grading property, therefore matching pennies has Generalmax = 1 for this property.\nProposition 20. Leftmax for the partial grading (PG) property is equal to 1 for the matching pennies environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πe = {πr1, πr2, πr3} with uniform weight for wΠe (a πr agent always acts randomly) we find this situation no matter which Πo we use.\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πr1, πr2, πr3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πr1, πr2, πr3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πr1, πr2, πr3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πh, πh/t, πm/o, l̇, µ) =\n= PO1,2(πr1, πr2, πr3, (∗, ∗), µ)\nNote that the choice of Πo does not affect the result of PG1,2. The following table shows us PO1,2 for all the permutations of πr1, πr2, πr3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πr1 ≤ πr2 πr1 ≤ πr3 πr2 ≤ πr1 πr2 ≤ πr3 πr3 ≤ πr2 πr1 ≤ πr3 πr1 ≤ πr3 πr1 ≤ πr2 πr2 ≤ πr3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πr2 ≤ πr3 πr3 ≤ πr1 πr3 ≤ πr2 πr3 ≤ πr1 πr1 ≤ πr2 πr2 ≤ πr1 πr2 ≤ πr1 πr3 ≤ πr2 πr3 ≤ πr1\nIt is possible to find a PO for every permutation, since we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nPG1,2(πr1, πr2, πr3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 2 and 1:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(πr1, πr2, πr3,Πo, wL̇, µ)\nAgain, we only need to calculate PG2,1(πr1, πr2, πr3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,1(πr1, πr2, πr3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)PO2,1(πr1, πr2, πr3, l̇, µ) =\n= PO2,1(πr1, πr2, πr3, (∗, ∗), µ)\nNote again that the choice of Πo does not affect the result of PG2,1. The following table shows us PO2,1 for all the permutations of πr1, πr2, πr3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πr1 ≤ πr2 πr1 ≤ πr3 πr2 ≤ πr1 πr2 ≤ πr3 πr3 ≤ πr2 πr1 ≤ πr3 πr1 ≤ πr3 πr1 ≤ πr2 πr2 ≤ πr3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πr2 ≤ πr3 πr3 ≤ πr1 πr3 ≤ πr2 πr3 ≤ πr1 πr1 ≤ πr2 πr2 ≤ πr1 πr2 ≤ πr1 πr3 ≤ πr2 πr3 ≤ πr1\nAgain, it is possible to find a PO for every permutation, since we have at least one random agent making its expected average reward equal to its opponent expected average reward (both have an expected average reward of 0 as proved in lemma 2). So:\nPG2,1(πr1, πr2, πr3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {PG1,2(Πe, wΠe ,Πo, wL̇, µ) + PG2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {1 + 1} = 1\nSince 1 is the highest possible value for the partial grading property and no Πo is able to influence this result, therefore matching pennies has Leftmax = 1 for this property.\nProposition 21. Rightmin for the partial grading (PG) property is equal to 0 for the matching pennies environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πh1, πh2, πt} with uniform weight for wΠe (a πh agent always perform Head and a πt agent always perform Tail) we find this situation no matter which Πo we use.\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πh1, πh2, πt,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πh1, πh2, πt,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πh1, πh2, πt,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πh1, πh2, πt, l̇, µ) =\n= PO1,2(πh1, πh2, πt, (∗, ∗), µ) Note that the choice of Πo does not affect the result of PG1,2.\nThe following table shows us PO1,2 for all the permutations of πh1, πh2, πt.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πh1 ≤ πh2 πh1 ≤ πt πh2 ≤ πh1 πh2 ≤ πt πt ≤ πh2 πh1 ≤ πt πh1 ≤ πt πh1 ≤ πh2 πh2 ≤ πt Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πh2 ≤ πt πt ≤ πh1 πt ≤ πh2 πt ≤ πh1 πh1 ≤ πh2 πh2 ≤ πh1 πh2 ≤ πh1 πt ≤ πh2 πt ≤ πh1\nBut, it is not possible to find a PO, since for every permutation we have either πh1 ≤ πh2 or πh2 ≤ πh1. In both cases, πh will always perform Head, so they will obtain an expected average reward of 1 and −1 respectively. So:\nPG1,2(πh1, πh2, πt,Πo, wL̇, µ) = 0\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 2 and 1:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(πh1, πh2, πt,Πo, wL̇, µ)\nAgain, we only need to calculate PG2,1(πh1, πh2, πt,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,1(πh1, πh2, πt,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)PO2,1(πh1, πh2, πt, l̇, µ) =\n= PO2,1(πh1, πh2, πt, (∗, ∗), µ) Note again that the choice of Πo does not affect the result of PG2,1.\nThe following table shows us PO2,1 for all the permutations of πh1, πh2, πt.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πh1 ≤ πh2 πh1 ≤ πt πh2 ≤ πh1 πh2 ≤ πt πt ≤ πh2 πh1 ≤ πt πh1 ≤ πt πh1 ≤ πh2 πh2 ≤ πt Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πh2 ≤ πt πt ≤ πh1 πt ≤ πh2 πt ≤ πh1 πh1 ≤ πh2 πh2 ≤ πh1 πh2 ≤ πh1 πt ≤ πh2 πt ≤ πh1\nAgain, it is not possible to find a PO, since for every permutation we have either πh1 ≤ πt or πt ≤ πh1. In πh1 ≤ πt, πh1 will always perform Head and πt will always perform Tail, so they will obtain an expected average reward of 1 and −1 respectively. In πt ≤ πh1, πt will always perform Tail and πh1 will always perform Head, so they will obtain an expected average reward of 1 and −1 respectively. So:\nPG2,1(πh1, πh2, πt,Πo, wL̇, µ) = 0\nTherefore:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {PG1,2(Πe, wΠe ,Πo, wL̇, µ) + PG2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {0 + 0} = 0\nSince 0 is the lowest possible value for the partial grading property and no Πo is able to influence this result, therefore matching pennies has Rightmin = 0 for this property."
    }, {
      "heading" : "A.6 Slot Reward Dependency",
      "text" : "Next we see the slot reward dependency (SRD) property. As given in section 4.3.2, we want to know how much competitive or cooperative the environment is.\nProposition 22. General range for the slot reward dependency (SRD) property is equal to [−1,−1] for the matching pennies environment.\nProof. Following definition 20, we obtain the SRD value for any ⟨Πe, wΠe ,Πo⟩ (where Πe, wΠe and Πo are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 19, we can calculate its SRD value for each pair of slots. We start with slots 1 and 2:\nSRD1,2(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD1,2(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD1,2(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD1,2(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−1 (Πo) [wL̇(l̇)](R1(µ[l̇\n1← π1]), R2(µ[l̇ 1← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −1 (Πo) to calculate corr(R1(µ[l̇ 1← π1]), R2(µ[l̇ 1← π1])). We calculate this value for a figurative line-up pattern l̇ = (∗, π2) from L̇N(µ)−1 (Πo):\ncorr(R1(µ[l̇ 1← π1]), R2(µ[l̇ 1← π1])) = corr(R1(µ[π1, π2]), R2(µ[π1, π2]))\nThis game is a zero-sum game with two agents. That means that, in every game, the sum of both agents’ rewards will always be zero, or in other words, when the agent in slot 1 (any π1) obtains a reward r the agent in slot 2 (any π2) obtains −r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 2 always obtain opposite expected average reward, then the correlation function will always obtain the same value13 of −1. So:\nSRD1,2(π1,Πo, wL̇, µ) = −1\nTherefore:\nSRD1,2(Πe, wΠe ,Πo, wL̇, µ) = −1\nAnd for slots 2 and 1: SRD2,1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD2,1(π,Πo, wL̇, µ)\nAgain, we do not know which Πe we have, but we know that we will need to evaluate SRD2,1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD2,1(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−2 (Πo) [wL̇(l̇)](R2(µ[l̇\n2← π1]), R1(µ[l̇ 2← π1]))\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇\nfrom L̇ N(µ) −2 (Πo) to calculate corr(R2(µ[l̇ 2← π1]), R1(µ[l̇ 2← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, ∗) from L̇N(µ)−2 (Πo):\ncorr(R2(µ[l̇ 2← π1]), R1(µ[l̇ 2← π1])) = corr(R2(µ[π2, π1]), R1(µ[π2, π1])) 13Provided there is at least one game which is not a tie.\nAgain, this game is a zero-sum game with two agents. That means that, in every game, the sum of both agents’ rewards will always be zero, or in other words, when the agent in slot 2 (any π1) obtains a reward r the agent in slot 1 (any π2) obtains −r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 2 and 1 always obtain opposite expected average reward, then the correlation function will always obtain the same value of −1. So:\nSRD2,1(π1,Πo, wL̇, µ) = −1\nTherefore:\nSRD2,1(Πe, wΠe ,Πo, wL̇, µ) = −1\nAnd finally, we weight over the slots:\nSRD(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 2\n1\n1\n2\n1 2 {SRD1,2(Πe, wΠe ,Πo, wL̇, µ) + SRD2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {−1 + (−1)} = −1\nSo, for every trio ⟨Πe, wΠe ,Πo⟩ we obtain the same result:\n∀Πe, wΠe ,Πo : SRD(Πe, wΠe ,Πo, wL̇, µ, wS) = −1\nTherefore, matching pennies has General = [−1,−1] for this property."
    }, {
      "heading" : "A.7 Competitive Anticipation",
      "text" : "Finally, we follow with the competitive anticipation (AComp) property. As given in section 4.5.1, we want to know how much benefit the evaluated agents obtain when they anticipate competing agents.\nProposition 23. Generalmin for the competitive anticipation (AComp) property is equal to − 12 for the matching pennies environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πt/h} with wΠe(πt/h) = 1 and Πo = {πh} (a πh agent always performs Head and a πt/h agent always performs Tail when playing in slot 1 and always performs Head when playing in slot 2)14.\nFollowing definition 33, we obtain the AComp value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots in different teams. Following definition 32, we could calculate its AComp value for each pair of slots but, since Πe has only one agent, its weight is equal to 1 and Πo also has only one agent, it is equivalent to use directly definition 31. We start with slots 1 and 2:\nAComp1,2(πt/h, πh,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,2← πt/h, πh])−R1(µ[l̇ 1,2← πt/h, πr]) ) =\n= 1\n2\n( R1(µ[πt/h, πh])−R1(µ[πt/h, πr]) ) 14πt/h has to know in which slot it is playing. To infer this, it starts with a random action at the first iteration and then look\nat the action of its opponent and the reward it obtains.\nWe know from lemma 2 that R1(µ[πt/h, πr]) = 0, so we only need to calculate R1(µ[πt/h, πh]), where πt/h will always perform Tail and πh will always perform Head, so the agent in slot 1 (πt/h) will obtain an expected average reward of −1. So:\nAComp1,2(πt/h, πh,Πo, wL̇, µ) = 1 2 ((−1)− 0) = −1\n2 Note that this is the minimum possible value for AComp1,2(π1, π2,Πo, wL̇, µ) since R1(µ[π1, πr]) will always be equal to 0 for this environment.\nAnd for slots 2 and 1:\nAComp2,1(πt/h, πh,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇) 1\n2\n( R2(µ[l̇ 2,1← πt/h, πh])−R2(µ[l̇ 2,1← πt/h, πr]) ) =\n= 1\n2\n( R2(µ[πh, πt/h])−R2(µ[πr, πt/h]) ) Again, we know from lemma 2 that R2(µ[πr, πt/h]) = 0, so we only need to calculate R2(µ[πh, πt/h]), where πh and πt/h will always perform Head, so the agent in slot 2 (πt/h) will obtain an expected average reward of −1. So:\nAComp2,1(πt/h, πh,Πo, wL̇, µ) = 1 2 ((−1)− 0) = −1\n2 Note again that this is the minimum possible value for AComp2,1(π1, π2,Πo, wL̇, µ) since R2(µ[πr, π1]) will always be equal to 0 for this environment.\nAnd finally, we weight over the slots:\nAComp(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS22 ∑ t1,t2∈τ |t1 ̸=t2 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACompi,j(Πe, wΠe ,Πo, wL̇, µ) =\n= 2\n1\n1\n2\n1 2 {AComp1,2(Πe, wΠe ,Πo, wL̇, µ) +AComp2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {AComp1,2(πt/h, πh,Πo, wL̇, µ) +AComp2,1(πt/h, πh,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1\n2 { −1 2 + ( −1 2 )} = −1 2\nSince − 12 is the lowest possible value that we can obtain for the competitive anticipation property, therefore matching pennies has Generalmin = − 12 for this property.\nProposition 24. Generalmax for the competitive anticipation (AComp) property is equal to 1 2 for the matching pennies environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πh/t} with wΠe(πh/t) = 1 and Πo = {πh} (a πh agent always performs Head and a πh/t agent always performs Head when playing in slot 1 and always performs Tail when playing in slot 2)15.\nFollowing definition 33, we obtain the AComp value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots in different teams. Following definition 32, we could calculate its AComp value for each pair of slots but, since Πe has only one agent, its weight is equal to 1 and Πo also has only one agent, it is equivalent to use directly definition 31. We start with slots 1 and 2:\nAComp1,2(πh/t, πh,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,2← πh/t, πh])−R1(µ[l̇ 1,2← πh/t, πr]) ) =\n= 1\n2\n( R1(µ[πh/t, πh])−R1(µ[πh/t, πr]) ) 15πh/t has to know in which slot it is playing. To infer this, it starts with a random action at the first iteration and then look\nat the action of its opponent and the reward it obtains.\nWe know from lemma 2 that R1(µ[πh/t, πr]) = 0, so we only need to calculate R1(µ[πh/t, πh]), where πh/t and πh will always perform Head, so the agent in slot 1 (πh/t) will obtain an expected average reward of 1. So:\nAComp1,2(πh/t, πh,Πo, wL̇, µ) = 1 2 (1− 0) = 1 2\nNote that this is the maximum possible value for AComp1,2(π1, π2,Πo, wL̇, µ) since R1(µ[π1, πr]) will always be equal to 0 for this environment.\nAnd for slots 2 and 1:\nAComp2,1(πh/t, πh,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇) 1\n2\n( R2(µ[l̇ 2,1← πh/t, πh])−R2(µ[l̇ 2,1← πh/t, πr]) ) =\n= 1\n2\n( R2(µ[πh, πh/t])−R2(µ[πr, πh/t]) ) Again, we know from lemma 2 that R2(µ[πr, πh/t]) = 0, so we only need to calculate R2(µ[πh, πh/t]), where πh will always perform Head and πh/t will always perform Tail, so the agent in slot 2 (πh/t) will obtain an expected average reward of 1. So:\nAComp2,1(πh/t, πh,Πo, wL̇, µ) = 1 2 (1− 0) = 1 2\nNote again that this is the maximum possible value for AComp2,1(π1, π2,Πo, wL̇, µ) since R2(µ[πr, π1]) will always be equal to 0 for this environment.\nAnd finally, we weight over the slots:\nAComp(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS22 ∑ t1,t2∈τ |t1 ̸=t2 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACompi,j(Πe, wΠe ,Πo, wL̇, µ) =\n= 2\n1\n1\n2\n1 2 {AComp1,2(Πe, wΠe ,Πo, wL̇, µ) +AComp2,1(Πe, wΠe ,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1 2 {AComp1,2(πh/t, πh,Πo, wL̇, µ) +AComp2,1(πh/t, πh,Πo, wL̇, µ)} =\n= 2\n1\n1\n2\n1\n2\n{ 1\n2 +\n1\n2\n} = 1\n2\nSince 12 is the highest possible value that we can obtain for the competitive anticipation property, therefore matching pennies has Generalmax = 1 2 for this property."
    }, {
      "heading" : "B Prisoner’s Dilemma properties",
      "text" : "In this section we prove how we obtained the values for the properties for the prisoner’s dilemma environment (section 5.3)."
    }, {
      "heading" : "B.1 Action Dependency",
      "text" : "We start with the action dependency (AD) property. As given in section 4.2.1, we want to know if the evaluated agents behave differently depending on which line-up they interact with. We use ∆S(a, b) = 1 if distributions a and b are equal and 0 otherwise.\nProposition 25. Generalmin for the action dependency (AD) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πb} with wΠe(πb) = 1 and Πo = {πc1, πc2} (a πc agent always performs Cooperate and a πb agent always performs Blame).\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 13, we could calculate its AD value for slot 1 but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12 for slot 1:\nAD1(πb,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πb]), Ă1(µ[v̇ 1← πb])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă1(µ[πb, πc1]), Ă1(µ[πb, πc2]))\nNote that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nIn this case, we only need to calculate ∆S(Ă1(µ[πb, πc1 ]), Ă1(µ[πb, πc2 ])), where the agent in both slots 1 (πb) will perform the same sequence of actions (always Blame) independently of the line-up. So:\nAD1(πb,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, generalising for all slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= AD1(Πe, wΠe ,Πo, wL̇, µ) = = AD1(πb,Πo, wL̇, µ) = = 0\nSince 0 is the lowest possible value for the action dependency property, therefore prisoner’s dilemma has Generalmin = 0 for this property.\nProposition 26. Generalmax for the action dependency (AD) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πm} with wΠe(πm) = 1 and Πo = {πc, πb} (a πm agent first acts randomly and then always mimics the other agent’s last action, a πc agent always performs Cooperate and a πb agent always performs Blame).\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 13, we could calculate its AD value for slot 1 but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12 for slot 1:\nAD1(πm,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πm]), Ă1(µ[v̇ 1← πm])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă1(µ[πm, πc]), Ă1(µ[πm, πb]))\nNote that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nFrom iteration 2, πm will mimic the last action of the agent in slot 2, and since πc will always perform Cooperate and πb will always perform Blame, the agent in both slots 1 (πm) will perform different sequences of actions on each line-up. So:\nAD1(πm,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, generalising for all slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= AD1(Πe, wΠe ,Πo, wL̇, µ) = = AD1(πm,Πo, wL̇, µ) = = 1\nSince 1 is the highest possible value for the action dependency property, therefore prisoner’s dilemma has Generalmax = 1 for this property.\nProposition 27. Leftmax for the action dependency (AD) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πo = {πc1, πc2} (a πc agent always performs Cooperate) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 13, we can calculate its AD value for slot 1:\nAD1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)AD1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate AD1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 12 to calculate this value for a figurative evaluated agent π from Πe:\nAD1(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← π]), Ă1(µ[v̇ 1← π])) =\n= 2 2\n1\n1\n2\n1 2 ∆S(Ă1(µ[π, πc1]), Ă1(µ[π, πc2]))\nNote that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nA πc agent will always perform Cooperate, so we obtain a situation where the agent in both slots 1 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent’s behaviour. So:\nAD1(π,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nAD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd finally, generalising for all slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= AD1(Πe, wΠe ,Πo, wL̇, µ) = = 0\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : AD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, prisoner’s dilemma has Leftmax = 0 for this property.\nProposition 28. Rightmin for the action dependency (AD) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πc} with wΠe(πc) = 1 (a πc agent always performs Cooperate) we find this situation no matter which Πo we use.\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 13, we could calculate its AD value for slot 1 but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12 for slot 1:\nAD1(πc,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πc]), Ă1(µ[v̇ 1← πc]))\nWe do not know which Πo we have, but we know that we will need to obtain two different line-up patterns u̇\nand v̇ from L̇ N(µ) −1 (Πo) to calculate ∆S(Ă1(µ[u̇ 1← πc]), Ă1(µ[v̇ 1← πc])). We calculate this value for two figurative line-up patterns u̇ = (∗, π1) and v̇ = (∗, π2) from L̇N(µ)−1 (Πo):\n∆S(Ă1(µ[u̇ 1← πc]), Ă1(µ[v̇ 1← πc])) = ∆S(Ă1(µ[πc, π1]), Ă1(µ[πc, π2]))\nHere, the agent in both slots 1 (πc) will perform the same sequence of actions (always Cooperate) independently of the line-up. So no matter which agents are in Πo we obtain:\nAD1(πc,Πo, wL̇, µ) = 0\nAnd finally, generalising for all slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= AD1(Πe, wΠe ,Πo, wL̇, µ) = = AD1(πc,Πo, wL̇, µ) = = 0\nSo, for every Πo we obtain the same result:\n∀Πo : AD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, prisoner’s dilemma has Rightmin = 0 for this property."
    }, {
      "heading" : "B.2 Reward Dependency",
      "text" : "We continue with the reward dependency (RD) property. As given in section 4.3.1, we want to know if the evaluated agents obtain different expected average rewards depending on which line-up they interact with. We use ∆Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 29. Generalmin for the reward dependency (RD) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πb} with wΠe(πb) = 1 and Πo = {πc1, πc2} (a πc agent always performs Cooperate and a πb agent always performs Blame).\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 16, we could calculate its RD value for slot 1 but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15 for slot 1:\nRD1(πb,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← πb]), R1(µ[v̇ 1← πb])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R1(µ[πb, πc1]), R1(µ[πb, πc2]))\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nIn this case, we only need to calculate ∆Q(R1(µ[πb, πc1]), R1(µ[πb, πc2])), where πb will always perform Blame and πc will always perform Cooperate, so the agent in both slots 1 (πb) will obtain the same expected average reward (1) independently of the line-up. So:\nRD1(πb,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, generalising for all slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= RD1(Πe, wΠe ,Πo, wL̇, µ) = = RD1(πb,Πo, wL̇, µ) = = 0\nSince 0 is the lowest possible value for the reward dependency property, therefore prisoner’s dilemma has Generalmin = 0 for this property.\nProposition 30. Generalmax for the reward dependency (RD) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πb} with wΠe(πb) = 1 and Πo = {πc, πb} (a πc agent always performs Cooperate and a πb agent always performs Blame).\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 16, we could calculate its RD value for slot 1 but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15 for slot 1:\nRD1(πb,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← πb]), R1(µ[v̇ 1← πb])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R1(µ[πb, πc]), R1(µ[πb, πb]))\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nIn line-up (πb, πc), where πb will always perform Blame and πc will always perform Cooperate, the agent in slot 1 (πb) will obtain one expected average reward (1), while in line-up (πb, πb), where both πb will always perform Blame, the agent in slot 1 (πb) will obtain a different expected average reward (− 13 ). So:\nRD1(πb,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, generalising for all slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= RD1(Πe, wΠe ,Πo, wL̇, µ) = = RD1(πb,Πo, wL̇, µ) = = 1\nSince 1 is the highest possible value for the reward dependency property, therefore prisoner’s dilemma has Generalmax = 1 for this property.\nProposition 31. Leftmax for the reward dependency (RD) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πo = {πc1, πc2} (a πc agent always performs Cooperate) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 16, we can calculate its RD value for slot 1:\nRD1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)RD1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate RD1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 15 to calculate this value for a figurative evaluated agent π from Πe:\nRD1(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← π]), R1(µ[v̇ 1← π])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R1(µ[π, πc1]), R1(µ[π, πc2]))\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nA πc agent will always perform Cooperate, so we obtain a situation where the agent in both slots 1 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent’s behaviour, obtaining agent in both slots 1 (any π) the same expected average reward. So:\nRD1(π,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nRD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd finally, generalising for all slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= RD1(Πe, wΠe ,Πo, wL̇, µ) = = 0\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : RD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, prisoner’s dilemma has Leftmax = 0 for this property.\nProposition 32. Rightmin for the reward dependency (RD) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πo = {πc, πb} (a πc agent always performs Cooperate and a πb agent always performs Blame) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 16, we can calculate its RD value for slot 1:\nRD1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)RD1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate RD1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 15 to calculate this value for a figurative evaluated agent π from Πe:\nRD1(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← π]), R1(µ[v̇ 1← π])) =\n= 2 2\n1\n1\n2\n1 2 ∆Q(R1(µ[π, πc]), R1(µ[π, πb]))\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2. From the matching pennies’ payoff matrix (figure 4)\nIn line-up (π, πc), where πc will always perform Cooperate, the agent in slot 1 (π) will obtain an expected average reward between 13 and 1, while in line-up (π, πb), where πb will always perform Blame, the agent in slot 1 (π) will obtain another expected average reward between −1 and − 13 . So:\nRD1(π,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nRD1(Πe, wΠe ,Πo, wL̇, µ) = 1\nAnd finally, generalising for all slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= RD1(Πe, wΠe ,Πo, wL̇, µ) = = 1\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : RD(Πe, wΠe ,Πo, wL̇, µ, wS) = 1\nTherefore, prisoner’s dilemma has Rightmin = 1 for this property."
    }, {
      "heading" : "B.3 Fine Discrimination",
      "text" : "Now we move to the fine discrimination (FD) property. As given in section 4.4.1, we want to know if different evaluated agents obtain different expected average rewards when interacting in the environment. We use ∆Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 33. Generalmin for the fine discrimination (FD) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πb1, πb2} with uniform weight for wΠe and Πo = {πc} (a πc agent always performs Cooperate and a πb agent always performs Blame).\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 22, we can calculate its FD value for slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πb1, πb2,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πb1, πb2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD1(πb1, πb2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πb1]), R1(µ[l̇ 1← πb2])) =\n= ∆Q(R1(µ[πb1, πc]), R1(µ[πb2, πc]))\nHere, a πb agent will always perform Blame and πc will always perform Cooperate, so both agents in slot 1 (πb1 and πb2) will obtain the same expected average reward (1). So:\nFD1(πb1, πb2,Πo, wL̇, µ) = 0\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, generalising for all slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ) =\n= FD1(Πe, wΠe ,Πo, wL̇, µ) = = 0\nSince 0 is the lowest possible value for the fine discriminative property, therefore prisoner’s dilemma has Generalmin = 0 for this property.\nProposition 34. Generalmax for the fine discrimination (FD) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πc, πb} with uniform weight for wΠe and Πo = {πc} (a πc agent always performs Cooperate and a πb agent always performs Blame).\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 22, we can calculate its FD value for slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πc, πb,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πc, πb,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD1(πc, πb,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πc]), R1(µ[l̇ 1← πb])) =\n= ∆Q(R1(µ[πc, πc]), R1(µ[πb, πc]))\nIn line-up (πc, πc), where both πc will always perform Cooperate, the agent in slot 1 (πc) will obtain one expected average reward ( 13 ), while in line-up (πb, πc), where πb will always perform Blame and πc will always perform Cooperate, the agent in slot 1 (πb) will obtain a different expected average reward (1). So:\nFD1(πc, πb,Πo, wL̇, µ) = 1\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, generalising for all slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ) =\n= FD1(Πe, wΠe ,Πo, wL̇, µ) = = 1\nSince 1 is the highest possible value for the fine discriminative property, therefore prisoner’s dilemma has Generalmax = 1 for this property.\nConjecture 1. Leftmax for the fine discrimination (FD) property is equal to 0 for the prisoner’s dilemma environment.\nAn agent π ∈ Πo can force every evaluated agent to obtain an expected average reward equal to 0 (in the limit). The procedure is simple. While the evaluated agent has an expected average reward lower/greater than 0, π performs Cooperate/Blame forcing the evaluated agent to increase/decrease its expected average reward. If this procedure is repeated indefinitely, the expected average reward of any evaluated agent will tend to 0. So:\n∀π1, π2∃Πo : FD(π1, π2,Πo, wL̇, µ, wS) = 0 Therefore, prisoner’s dilemma has Leftmax = 0 for this property.\nProposition 35. Rightmin for the fine discrimination (FD) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πb1, πb2} with uniform weight for wΠe (a πb agent always performs Blame) we find this situation no matter which Πo we use.\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one slot and generalise its result to all slots. Following definition 22, we can calculate its FD value for slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πb1, πb2,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πb1, πb2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD1(πb1, πb2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πb1]), R1(µ[l̇ 1← πb2]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −1 (Πo) to calculate ∆Q(R1(µ[l̇ 1← πb1]), R1(µ[l̇ 1← πb2])). We calculate this value for a figurative line-up pattern l̇ = (∗, π) from L̇N(µ)−1 (Πo):\n∆Q(R1(µ[l̇ 1← πb1]), R1(µ[l̇ 1← πb2])) = ∆Q(R1(µ[πb1, π]), R1(µ[πb2, π]))\nA πb agent will always perform Blame, so we obtain a situation where the agent in both slots 2 (any π) will be able to differentiate with which agent is interacting, so it will not be able to change its distribution of action sequences depending on the opponent’s behaviour, obtaining both agents in slot 1 (πb1 and πb2) the same expected average reward. So:\nFD1(πb1, πb2,Πo, wL̇, µ) = 0\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, generalising for all slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ)\n= FD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nSo, for every Πo we obtain the same result:\n∀Πo : FD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, prisoner’s dilemma has Rightmin = 0 for this property."
    }, {
      "heading" : "B.4 Strict Total Grading",
      "text" : "We arrive to the strict total grading (STG) property. As given in section 4.4.2, we want to know if there exists a strict ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the STO: Ri(µ[l̇ i,j← π1, π2]) < Rj(µ[l̇ i,j← π1, π2]), Ri(µ[l̇ i,j← π2, π3]) < Rj(µ[l̇ i,j← π2, π3]) and Ri(µ[l̇ i,j← π1, π3]) < Rj(µ[l̇ i,j← π1, π3]).\nSlot i Slot j π1 < π2 π2 < π3 π1 < π3\nProposition 36. Generalmin for the strict total grading (STG) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πb1, πb2, πb3} with uniform weight for wΠe and Πo = ∅ (a πb agent always performs Blame).\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28, we can calculate its STG value for slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πb1, πb2, πb3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πb1, πb2, πb3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πb1, πb2, πb3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πb1, πb2, πb3, l̇, µ) =\n= STO1,2(πb1, πb2, πb3, (∗, ∗), µ)\nThe following table shows us STO1,2 for all the permutations of πb1, πb2, πb3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb1 < πb2 πb1 < πb3 πb2 < πb1 πb2 < πb3 πb3 < πb2 πb1 < πb3 πb1 < πb3 πb1 < πb2 πb2 < πb3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb2 < πb3 πb3 < πb1 πb3 < πb2 πb3 < πb1 πb1 < πb2 πb2 < πb1 πb2 < πb1 πb3 < πb2 πb3 < πb1\nBut, it is not possible to find a STO, since for every permutation we always have πbi < πbj , where a πb agent will always perform Blame, so they will both obtain an expected average reward of − 13 . So:\nSTG1,2(πb1, πb2, πb3,Πo, wL̇, µ) = 0\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, generalising for all slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = STG1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= 0\nSince 0 is the lowest possible value for the strict total grading property, therefore prisoner’s dilemma has Generalmin = 0 for this property.\nProposition 37. Generalmax for the strict total grading (STG) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πc, πb, πr} with uniform weight for wΠe and Πo = ∅ (a πc agent always performs Cooperate, a πb agent always performs Blame and a πr agent always acts randomly).\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28, we can calculate its STG value for slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πc, πb, πr,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πc, πb, πr,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πc, πb, πr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πc, πb, πr, l̇, µ) =\n= STO1,2(πc, πb, πr, (∗, ∗), µ)\nThe following table shows us STO1,2 for all the permutations of πc, πb, πr.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πc < πb πc < πr πb < πc πb < πr πr < πb πc < πr πc < πr πc < πb πb < πr Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb < πr πr < πc πr < πb πr < πc πc < πb πb < πc πb < πc πr < πb πr < πc\nIt is possible to find a STO for the second permutation. In πc < πr, πc will always perform Cooperate and πr will always act randomly, so they will obtain an expected average reward of − 13 and 2 3 respectively. In πr < πb, πr will always act randomly and πb will always perform Blame, so they will obtain an expected average reward of − 23 and 1 3 respectively. In πc < πb, πc will always perform Cooperate and πb will always perform Blame, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG1,2(πc, πb, πr,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, generalising for all slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = STG1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\nSince 1 is the highest possible value for the strict total grading property, therefore prisoner’s dilemma has Generalmax = 1 for this property.\nProposition 38. Leftmax for the strict total grading (STG) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πe = {πc, πb, πr} with uniform weight for wΠe (a πc agent always performs Cooperate, a πb agent always performs Blame and a πr agent always acts randomly) we find this situation no matter which Πo we use.\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28, we can calculate its STG value for slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πc, πb, πr,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πc, πb, πr,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πc, πb, πr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πc, πb, πr, l̇, µ) =\n= STO1,2(πc, πb, πr, (∗, ∗), µ)\nNote that the choice of Πo does not affect the result of STG1,2. The following table shows us STO1,2 for all the permutations of πc, πb, πr.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πc < πb πc < πr πb < πc πb < πr πr < πb πc < πr πc < πr πc < πb πb < πr Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb < πr πr < πc πr < πb πr < πc πc < πb πb < πc πb < πc πr < πb πr < πc\nIt is possible to find a STO for the second permutation. In πc < πr, πc will always perform Cooperate and πr will always act randomly, so they will obtain an expected average reward of − 13 and 2 3 respectively. In πr < πb, πr will always act randomly and πb will always perform Blame, so they will obtain an expected average reward of − 23 and 1 3 respectively. In πc < πb, πc will always perform Cooperate and πb will always perform Blame, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG1,2(πc, πb, πr,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, generalising for all slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = STG1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\nSo, for every Πo we obtain the same result:\n∀Πo : STG(Πe, wΠe ,Πo, wL̇, µ, wS) = 1\nTherefore, prisoner’s dilemma has Leftmax = 1 for this property.\nProposition 39. Rightmin for the strict total grading (STG) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πb1, πb2, πb3} with uniform weight for wΠe (a πb agent always performs Blame) we find this situation no matter which Πo we use.\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28, we can calculate its STG value for slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πb1, πb2, πb3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πb1, πb2, πb3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πb1, πb2, πb3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πb1, πb2, πb3, l̇, µ) =\n= STO1,2(πb1, πb2, πb3, (∗, ∗), µ)\nNote that the choice of Πo does not affect the result of STG1,2. The following table shows us STO1,2 for all the permutations of πb1, πb2, πb3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb1 < πb2 πb1 < πb3 πb2 < πb1 πb2 < πb3 πb3 < πb2 πb1 < πb3 πb1 < πb3 πb1 < πb2 πb2 < πb3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb2 < πb3 πb3 < πb1 πb3 < πb2 πb3 < πb1 πb1 < πb2 πb2 < πb1 πb2 < πb1 πb3 < πb2 πb3 < πb1\nBut, it is not possible to find a STO, since for every permutation we always have πbi < πbj , where a πb agent will always perform Blame, so they will both obtain an expected average reward of − 13 . So:\nSTG1,2(πb1, πb2, πb3,Πo, wL̇, µ) = 0\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, generalising for all slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = STG1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= 0\nSo, for every Πo we obtain the same result:\n∀Πo : STG(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, prisoner’s dilemma has Rightmin = 0 for this property."
    }, {
      "heading" : "B.5 Partial Grading",
      "text" : "Now we arrive to the partial grading (PG) property. As given in section 4.4.2, we want to know if there exists a partial ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the PO: Ri(µ[l̇ i,j← π1, π2]) ≤ Rj(µ[l̇ i,j← π1, π2]), Ri(µ[l̇ i,j← π2, π3]) ≤ Rj(µ[l̇ i,j← π2, π3]) and Ri(µ[l̇ i,j← π1, π3]) ≤ Rj(µ[l̇ i,j← π1, π3]).\nSlot i Slot j π1 ≤ π2 π2 ≤ π3 π1 ≤ π3\nProposition 40. Generalmin for the partial grading (PG) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {π−ccbb , π −bbc b , πm} with uniform weight for wΠe and Πo = ∅ (a πm agent first acts randomly and then always mimics the other agent’s last action, a π−ccbb agent always performs Blame except for the last three actions where it performs Cooperate twice and finalises performing Blame, and a π−bbcb agent always performs Blame except for the last three actions where it performs Blame twice and finalises performing Cooperate).\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28 (for PG), we can calculate its PG value for slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(π −ccb b , π −bbc b , πm,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(π −ccb b , π −bbc b , πm,Πo, wL̇, µ). We follow definition 27 (for PG)\nto calculate this value:\nPG1,2(π −ccb b , π −bbc b , πm,Πo, wL̇, µ) = ∑ l̇∈L̇N(µ)−1,2 (Πo) wL̇(l̇)PO1,2(π −ccb b , π −bbc b , πm, l̇, µ) =\n= PO1,2(π −ccb b , π −bbc b , πm, (∗, ∗), µ)\nThe following table shows us PO1,2 for all the permutations of π −ccb b , π −bbc b , πm.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 π−ccbb ≤ π −bbc b π −ccb b ≤ πm π −bbc b ≤ π −ccb b π−bbcb ≤ πm πm ≤ π −bbc b π −ccb b ≤ πm π−ccbb ≤ πm π −ccb b ≤ π −bbc b π −bbc b ≤ πm Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 π−bbcb ≤ πm πm ≤ π −ccb b πm ≤ π −bbc b\nπm ≤ π−ccbb π −ccb b ≤ π −bbc b π −bbc b ≤ π −ccb b\nπ−bbcb ≤ π −ccb b πm ≤ π −bbc b πm ≤ π −ccb b\nBut, it is not possible to find a PO for any permutation, since π−ccbb > πm, π −bbc b > π −ccb b and πm > π −bbc b .\nSo:\nPG1,2(π −ccb b , π −bbc b , πm,Πo, wL̇, µ) = 0\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, generalising for all slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = PG1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= 0\nSince 0 is the lowest possible value for the partial grading property, therefore prisoner’s dilemma has Generalmin = 0 for this property.\nProposition 41. Generalmax for the partial grading (PG) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πb1, πb2, πb3} with uniform weight for wΠe and Πo = ∅ (a πb agent always performs Blame).\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28 (for PG), we can calculate its PG value for slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πb1, πb2, πb3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πb1, πb2, πb3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πb1, πb2, πb3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πb1, πb2, πb3, l̇, µ) =\n= PO1,2(πb1, πb2, πb3, (∗, ∗), µ)\nThe following table shows us PO1,2 for all the permutations of πb1, πb2, πb3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb1 ≤ πb2 πb1 ≤ πb3 πb2 ≤ πb1 πb2 ≤ πb3 πb3 ≤ πb2 πb1 ≤ πb3 πb1 ≤ πb3 πb1 ≤ πb2 πb2 ≤ πb3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb2 ≤ πb3 πb3 ≤ πb1 πb3 ≤ πb2 πb3 ≤ πb1 πb1 ≤ πb2 πb2 ≤ πb1 πb2 ≤ πb1 πb3 ≤ πb2 πb3 ≤ πb1\nIt is possible to find a PO for every permutation, since we always have πbi ≤ πbj , where a πb agent will always perform Blame, so they will both obtain an expected average reward of − 13 . So:\nPG1,2(πb1, πb2, πb3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, generalising for all slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = PG1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\nSince 1 is the highest possible value for the partial grading property, therefore prisoner’s dilemma has Generalmax = 1 for this property.\nProposition 42. Leftmax for the partial grading (PG) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πe = {πb1, πb2, πb3} with uniform weight for wΠe (a πb agent always performs Blame) we find this situation no matter which Πo we use.\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28 (for PG), we can calculate its PG value for slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πb1, πb2, πb3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πb1, πb2, πb3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πb1, πb2, πb3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πb1, πb2, πb3, l̇, µ) =\n= PO1,2(πb1, πb2, πb3, (∗, ∗), µ) Note that the choice of Πo does not affect the result of PG1,2.\nThe following table shows us PO1,2 for all the permutations of πb1, πb2, πb3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb1 ≤ πb2 πb1 ≤ πb3 πb2 ≤ πb1 πb2 ≤ πb3 πb3 ≤ πb2 πb1 ≤ πb3 πb1 ≤ πb3 πb1 ≤ πb2 πb2 ≤ πb3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πb2 ≤ πb3 πb3 ≤ πb1 πb3 ≤ πb2 πb3 ≤ πb1 πb1 ≤ πb2 πb2 ≤ πb1 πb2 ≤ πb1 πb3 ≤ πb2 πb3 ≤ πb1\nIt is possible to find a PO for every permutation, since we always have πbi ≤ πbj , where a πb agent will always perform Blame, so they will both obtain an expected average reward of − 13 . So:\nPG1,2(πb1, πb2, πb3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, generalising for all slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = PG1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\nSo, for every Πo we obtain the same result:\n∀Πo : PG(Πe, wΠe ,Πo, wL̇, µ, wS) = 1\nTherefore, prisoner’s dilemma has Leftmax = 1 for this property.\nProposition 43. Rightmin for the partial grading (PG) property is equal to 0 for the prisoner’s dilemma environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {π−ccbb , π −bbc b , πm} with uniform weight for wΠe (a πm agent first acts randomly and then always mimics the other agent’s last action, a π−ccbb agent always performs Blame except for the last three actions where it performs Cooperate twice and finalises performing Blame, and a π−bbcb agent always performs Blame except for the last three actions where it performs Blame twice and finalises performing Cooperate) we find this situation no matter which Πo we use.\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted values). Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 28 (for PG), we can calculate its PG value for slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(π −ccb b , π −bbc b , πm,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(π −ccb b , π −bbc b , πm,Πo, wL̇, µ). We follow definition 27 (for PG)\nto calculate this value:\nPG1,2(π −ccb b , π −bbc b , πm,Πo, wL̇, µ) = ∑ l̇∈L̇N(µ)−1,2 (Πo) wL̇(l̇)PO1,2(π −ccb b , π −bbc b , πm, l̇, µ) =\n= PO1,2(π −ccb b , π −bbc b , πm, (∗, ∗), µ)\nNote that the choice of Πo does not affect the result of PG1,2. The following table shows us PO1,2 for all the permutations of π −ccb b , π −bbc b , πm.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 π−ccbb ≤ π −bbc b π −ccb b ≤ πm π −bbc b ≤ π −ccb b π−bbcb ≤ πm πm ≤ π −bbc b π −ccb b ≤ πm π−ccbb ≤ πm π −ccb b ≤ π −bbc b π −bbc b ≤ πm Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 π−bbcb ≤ πm πm ≤ π −ccb b πm ≤ π −bbc b\nπm ≤ π−ccbb π −ccb b ≤ π −bbc b π −bbc b ≤ π −ccb b\nπ−bbcb ≤ π −ccb b πm ≤ π −bbc b πm ≤ π −ccb b\nBut, it is not possible to find a PO for any permutation, since π−ccbb < π −bbc b , π −bbc b < πm but π −ccb b > πm.\nSo:\nPG1,2(π −ccb b , π −bbc b , πm,Πo, wL̇, µ) = 0\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, generalising for all slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = PG1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= 0\nSo, for every Πo we obtain the same result:\n∀Πo : PG(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, prisoner’s dilemma has Rightmin = 0 for this property."
    }, {
      "heading" : "B.6 Slot Reward Dependency",
      "text" : "Next we see the slot reward dependency (SRD) property. As given in section 4.3.2, we want to know how much competitive or cooperative the environment is.\nProposition 44. Generalmin for the slot reward dependency (SRD) property is equal to −1 for the prisoner’s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πb} with wΠe(πb) = 1 and Πo = {πc} (a πc agent always performs Cooperate and a πb agent always performs Blame).\nFollowing definition 20, we obtain the SRD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 19, we could calculate its SRD value for slots 1 and 2 but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 18 for slots 1 and 2:\nSRD1,2(πb,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−1 (Πo) [wL̇(l̇)](R1(µ[l̇\n1← πb]), R2(µ[l̇ 1← πb])) =\n= corr(R1(µ[πb, πc]), R2(µ[πb, πc]))\nIn line-up (πb, πc), where πb will always perform Blame and πc will always perform Cooperate, they will obtain an expected average reward of 1 and −1 respectively. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 2 obtain an expected average reward of −1 and 1 respectively, then the correlation function will obtain the value of −1. So:\nSRD1,2(πb,Πo, wL̇, µ) = −1\nAnd finally:\nSRD(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ)  = = SRD1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= SRD1,2(πb,Πo, wL̇, µ) = = −1\nSince −1 is the lowest possible value for the slot reward dependency property, therefore prisoner’s dilemma has Generalmin = −1 for this property.\nProposition 45. Generalmax for the slot reward dependency (SRD) property is equal to 1 for the prisoner’s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πb} with wΠe(πb) = 1 and Πo = {πb} (a πb agent always performs Blame).\nFollowing definition 20, we obtain the SRD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one pair of slots and generalise its result to all pair of slots. Following definition 19, we could calculate its SRD value for slots 1 and 2 but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 18 for slots 1 and 2:\nSRD1,2(πb,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−1 (Πo) [wL̇(l̇)](R1(µ[l̇\n1← πb]), R2(µ[l̇ 1← πb])) =\n= corr(R1(µ[πb, πb]), R2(µ[πb, πb]))\nIn line-up (πb, πb), where both πb will always perform Blame, they will both obtain an expected average reward of − 13 . Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 2 obtain the same expected average reward of − 13 , then the correlation function will obtain the value of 1. So:\nSRD1,2(πb,Πo, wL̇, µ) = 1\nAnd finally:\nSRD(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ)  = = SRD1,2(Πe, wΠe ,Πo, wL̇, µ) =\n= SRD1,2(πb,Πo, wL̇, µ) = = 1\nSince 1 is the highest possible value for the slot reward dependency property, therefore prisoner’s dilemma has Generalmax = 1 for this property."
    }, {
      "heading" : "B.7 Competitive Anticipation",
      "text" : "Finally, we follow with the competitive anticipation (AComp) property. As given in section 4.5.1, we want to know how much benefit the evaluated agents obtain when they anticipate competing agents.\nProposition 46. Generalmin for the competitive anticipation (AComp) property is equal to − 23 for the prisoner’s dilemma environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πc/b} with wΠe(πc/b) = 1 and Πo = {πb} (a πc/b agent performs Cooperate until the other agent also performs Cooperate, then it starts to perform Blame, and a πb agent always performs Blame).\nFollowing definition 33, we obtain the AComp value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one pair of slots in different teams, and generalise its result to all pair of slots. Following definition 32, we could calculate its AComp value for slots 1 and 2 but, since Πe has only one agent, its weight is equal to 1 and Πo also has only one agent, it is equivalent to use directly definition 31 for slots 1 and 2:\nAComp1,2(πc/b, πb,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,2← πc/b, πb])−R1(µ[l̇ 1,2← πc/b, πr]) ) =\n= 1\n2\n( R1(µ[πc/b, πb])−R1(µ[πc/b, πr]) ) In line-up (πc/b, πb), where πc/b will always perform Cooperate and πb will always perform Blame, the agent in slot 1 (πc/b) will obtain an expected average reward of −1. In line-up (πc/b, πr), where πc/b will start with Cooperate and then will continue with Blame once πr performs Cooperate, and πr will always act randomly, the agent in slot 1 (πc/b) will obtain an expected average reward of 1 3 (in the limit). So:\nAComp1,2(πc/b, πb,Πo, wL̇, µ) = 1\n2\n( (−1)− 1\n3\n) = −2\n3\nNote that this is the minimum possible value for AComp1,2(π1, π2,Πo, wL̇, µ) since interacting with a random agent cannot provide a greater result than 13 for this environment.\nAnd finally, generalising for all slots:\nAComp(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS22 ∑ t1,t2∈τ |t1 ̸=t2 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACompi,j(Πe, wΠe ,Πo, wL̇, µ) =\n= AComp1,2(Πe, wΠe ,Πo, wL̇, µ) = = AComp1,2(πc/b, πb,Πo, wL̇, µ) = = −2 3\nSince − 23 is the lowest possible value that we can obtain for the competitive anticipation property, therefore prisoner’s dilemma has Generalmin = − 23 for this property.\nProposition 47. Generalmax for the competitive anticipation (AComp) property is equal to 2 3 for the prisoner’s dilemma environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πb/c} with wΠe(πb/c) = 1 and Πo = {πc} (a πb/c agent performs Blame until the other agent also performs Blame, then it starts to perform Cooperate, and a πc agent always performs Cooperate).\nFollowing definition 33, we obtain the AComp value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is symmetric, we just need to calculate this property for one pair of slots in different teams, and generalise its result to all pair of slots. Following definition 32, we could calculate its AComp value for slots 1 and 2 but, since Πe has only one agent, its weight is equal to 1 and Πo also has only one agent, it is equivalent to use directly definition 31 for slots 1 and 2:\nAComp1,2(πb/c, πc,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,2← πb/c, πc])−R1(µ[l̇ 1,2← πb/c, πr]) ) =\n= 1\n2\n( R1(µ[πb/c, πc])−R1(µ[πb/c, πr]) ) In line-up (πb/c, πc), where πb/c will always perform Blame and πc will always perform Cooperate, the agent in slot 1 (πb/c) will obtain an expected average reward of 1. In line-up (πb/c, πr), where πb/c will start with Blame and then will continue with Cooperate once πr performs Blame, and πr will always act randomly, the agent in slot 1 (πb/c) will obtain an expected average reward of − 13 (in the limit). So:\nAComp1,2(πb/c, πc,Πo, wL̇, µ) = 1\n2\n( 1− ( −1 3 )) = 2 3\nNote that this is the maximum possible value for AComp1,2(π1, π2,Πo, wL̇, µ) since interacting with a random agent cannot provide a lower result than − 13 for this environment.\nAnd finally, generalising for all slots:\nAComp(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS22 ∑ t1,t2∈τ |t1 ̸=t2 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACompi,j(Πe, wΠe ,Πo, wL̇, µ) =\n= AComp1,2(Πe, wΠe ,Πo, wL̇, µ) = = AComp1,2(πb/c, πc,Πo, wL̇, µ) = = 2\n3\nSince 23 is the highest possible value that we can obtain for the competitive anticipation property, therefore prisoner’s dilemma has Generalmax = 2 3 for this property."
    }, {
      "heading" : "C Predator-prey properties",
      "text" : "In this section we prove how we obtained the values for the properties for the predator-prey environment (section 5.4). To calculate some of the values for the properties, we make use of lemma 3.\nLemma 3. When three well coordinated predators are trying to chase the prey, it will always be chased in 5 iterations or less no matter the behaviour of the prey.\nSince there exists a lot of variants to chase the prey, we cannot show them all. Instead, here we show one of the largest sequences of actions to chase the prey in 5 iterations when the prey is trying to escape and the predators are well coordinated.\n⊗\nOther behaviours of the prey will lead it closer to the boundaries, which would be easier for the predators to chase it."
    }, {
      "heading" : "C.1 Action Dependency",
      "text" : "We start with the action dependency (AD) property. As given in section 4.2.1, we want to know if the evaluated agents behave differently depending on which line-up they interact with. We use ∆S(a, b) = 1 if distributions a and b are equal and 0 otherwise.\nProposition 48. Generalmin for the action dependency (AD) property is equal to 0 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πu} with wΠe(πu) = 1 and Πo = {πd1, πd2} (a πd agent always performs Down and a πu agent always performs Up).\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πu]), Ă1(µ[v̇ 1← πu])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă1(µ[πu, πd1, πd1, πd1]), Ă1(µ[πu, πd1, πd1, πd2]))+\n+∆S(Ă1(µ[πu, πd1, πd1, πd1]), Ă1(µ[πu, πd1, πd2, πd1]))+\n...\n+ ∆S(Ă1(µ[πu, πd2, πd2, πd1]), Ă1(µ[πu, πd2, πd2, πd2]))}\nNote that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nIn this case, we have 28 possible pairs of line-ups where the agent in both slots 1 (πu) will perform the same sequence of actions (always Up) independently of the line-up. So:\nAD1(πu,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nFor slot 2, the agent in both slots 2 (πu) will also perform the same sequence of actions (always Up) independently of the line-up. So:\nAD2(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă2(µ[u̇ 2← πu]), Ă2(µ[v̇ 2← πu])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă2(µ[πd1, πu, πd1, πd1]), Ă2(µ[πd1, πu, πd1, πd2]))+\n+∆S(Ă2(µ[πd1, πu, πd1, πd1]), Ă2(µ[πd1, πu, πd2, πd1]))+\n...\n+ ∆S(Ă2(µ[πd2, πu, πd2, πd1]), Ă2(µ[πd2, πu, πd2, πd2]))} = = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nFor slot 3, the agent in both slots 3 (πu) will also perform the same sequence of actions (always Up) independently of the line-up. So:\nAD3(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−3 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă3(µ[u̇ 3← πu]), Ă3(µ[v̇ 3← πu])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă3(µ[πd1, πd1, πu, πd1]), Ă3(µ[πd1, πd1, πu, πd2]))+\n+∆S(Ă3(µ[πd1, πd1, πu, πd1]), Ă3(µ[πd1, πd2, πu, πd1]))+\n...\n+ ∆S(Ă3(µ[πd2, πd2, πu, πd1]), Ă3(µ[πd2, πd2, πu, πd2]))} = = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nAnd for slot 4, the agent in both slots 4 (πu) will also perform the same sequence of actions (always Up) independently of the line-up. So:\nAD4(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−4 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă4(µ[u̇ 4← πu]), Ă4(µ[v̇ 4← πu])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă4(µ[πd1, πd1, πd1, πu]), Ă4(µ[πd1, πd1, πd2, πu]))+\n+∆S(Ă4(µ[πd1, πd1, πd1, πu]), Ă4(µ[πd1, πd2, πd1, πu]))+\n...\n+ ∆S(Ă4(µ[πd2, πd2, πd1, πu]), Ă4(µ[πd2, πd2, πd2, πu]))} = = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nAnd finally, we weight over the slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)ADi(πu,Πo, wL̇, µ) = = 1\n4 {AD1(πu,Πo, wL̇, µ) +AD2(πu,Πo, wL̇, µ)+\n+AD3(πu,Πo, wL̇, µ) +AD4(πu,Πo, wL̇, µ)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSince 0 is the lowest possible value for the action dependency property, therefore predator-prey hasGeneralmin = 0 for this property.\nProposition 49. Generalmax for the action dependency (AD) property is equal to 1 for the predator-prey environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πm} with wΠe(πm) = 1 and Πo = {πu, πd} (a πm agent first acts randomly and then always mimics sequentially the other agents’ last action, a πu agent always performs Up and a πd agent always performs Down).\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(πm,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πm]), Ă1(µ[v̇ 1← πm])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă1(µ[πm, πu, πu, πu]), Ă1(µ[πm, πu, πu, πd]))+\n+∆S(Ă1(µ[πm, πu, πu, πu]), Ă1(µ[πm, πu, πd, πu]))+\n...\n+ ∆S(Ă1(µ[πm, πd, πd, πu]), Ă1(µ[πm, πd, πd, πd]))} Note that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nIn this case, we have 28 possible pairs of line-ups where the agent in both slots 1 (πm) will perform different sequences of actions depending on the line-up. So:\nAD1(πm,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 1} = 1\nFor slot 2, the agent in both slots 2 (πm) will also perform different sequences of actions depending on the line-up. So:\nAD2(πm,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă2(µ[u̇ 2← πm]), Ă2(µ[v̇ 2← πm])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă2(µ[πu, πm, πu, πu]), Ă2(µ[πu, πm, πu, πd]))+\n+∆S(Ă2(µ[πu, πm, πu, πu]), Ă2(µ[πu, πm, πd, πu]))+\n...\n+ ∆S(Ă2(µ[πd, πm, πd, πu]), Ă2(µ[πd, πm, πd, πd]))} =\n= 2 8\n7\n1\n8\n1 8 {28× 1} = 1\nFor slot 3, the agent in both slots 3 (πm) will also perform different sequences of actions depending on the line-up. So:\nAD3(πm,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−3 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă3(µ[u̇ 3← πm]), Ă3(µ[v̇ 3← πm])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă3(µ[πu, πu, πm, πu]), Ă3(µ[πu, πu, πm, πd]))+\n+∆S(Ă3(µ[πu, πu, πm, πu]), Ă3(µ[πu, πd, πm, πu]))+\n...\n+ ∆S(Ă3(µ[πd, πd, πm, πu]), Ă3(µ[πd, πd, πm, πd]))} =\n= 2 8\n7\n1\n8\n1 8 {28× 1} = 1\nAnd for slot 4, the agent in both slots 4 (πm) will also perform different sequences of actions depending on the line-up. So:\nAD4(πm,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−4 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă4(µ[u̇ 4← πm]), Ă4(µ[v̇ 4← πm])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă4(µ[πu, πu, πu, πm]), Ă4(µ[πu, πu, πd, πm]))+\n+∆S(Ă4(µ[πu, πu, πu, πm]), Ă4(µ[πu, πd, πu, πm]))+\n...\n+ ∆S(Ă4(µ[πd, πd, πu, πm]), Ă4(µ[πd, πd, πd, πm]))} =\n= 2 8\n7\n1\n8\n1 8 {28× 1} = 1\nAnd finally, we weight over the slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)ADi(πm,Πo, wL̇, µ) = = 1\n4 {AD1(πm,Πo, wL̇, µ) +AD2(πm,Πo, wL̇, µ)+\n+AD3(πm,Πo, wL̇, µ) +AD4(πm,Πo, wL̇, µ)} = = 1\n4 {1 + 1 + 1 + 1} = 1\nSince 1 is the highest possible value for the action dependency property, therefore predator-prey has Generalmax = 1 for this property.\nProposition 50. Leftmax for the action dependency (AD) property is equal to 0 for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πo = {πd1, πd2} (a πd agent always performs Down) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we can calculate its AD value for each slot. We start with slot 1:\nAD1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)AD1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate AD1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 12 to calculate this value for a figurative evaluated agent π from Πe:\nAD1(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← π]), Ă1(µ[v̇ 1← π])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă1(µ[π, πd1, πd1, πd1]), Ă1(µ[π, πd1, πd1, πd2]))+\n+∆S(Ă1(µ[π, πd1, πd1, πd1]), Ă1(µ[π, πd1, πd2, πd1]))+\n...\n+ ∆S(Ă1(µ[π, πd2, πd2, πd1]), Ă1(µ[π, πd2, πd2, πd2]))}\nNote that we avoided to calculate both ∆S(a, b) and ∆S(b, a) since they provide the same result, by calculating only ∆S(a, b) and multiplying the result by 2.\nA πd agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 1 (any π) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up. So:\nAD1(π,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nAD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nFor slot 2:\nAD2(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)AD2(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate AD2(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 12 to calculate this value for a figurative evaluated agent π from Πe:\nAD2(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă2(µ[u̇ 2← π]), Ă2(µ[v̇ 2← π])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă2(µ[πd1, π, πd1, πd1]), Ă2(µ[πd1, π, πd1, πd2]))+\n+∆S(Ă2(µ[πd1, π, πd1, πd1]), Ă2(µ[πd1, π, πd2, πd1]))+\n...\n+ ∆S(Ă2(µ[πd2, π, πd2, πd1]), Ă2(µ[πd2, π, πd2, πd2]))}\nA πd agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 2 (any π) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up. So:\nAD2(π,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nAD2(Πe, wΠe ,Πo, wL̇, µ) = 0\nFor slot 3:\nAD3(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)AD3(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate AD3(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 12 to calculate this value for a figurative evaluated agent π from Πe:\nAD3(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−3 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă3(µ[u̇ 3← π]), Ă3(µ[v̇ 3← π])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă3(µ[πd1, πd1, π, πd1]), Ă3(µ[πd1, πd1, π, πd2]))+\n+∆S(Ă3(µ[πd1, πd1, π, πd1]), Ă3(µ[πd1, πd2, π, πd1]))+\n...\n+ ∆S(Ă3(µ[πd2, πd2, π, πd1]), Ă3(µ[πd2, πd2, π, πd2]))}\nA πd agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 3 (any π) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up. So:\nAD3(π,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nAD3(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd for slot 4:\nAD4(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)AD4(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate AD4(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 12 to calculate this value for a figurative evaluated agent π from Πe:\nAD4(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−4 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă4(µ[u̇ 4← π]), Ă4(µ[v̇ 4← π])) =\n= 2 8\n7\n1\n8\n1 8 {∆S(Ă4(µ[πd1, πd1, πd1, π]), Ă4(µ[πd1, πd1, πd2, π]))+\n+∆S(Ă4(µ[πd1, πd1, πd1, π]), Ă4(µ[πd1, πd2, πd1, π]))+\n...\n+ ∆S(Ă4(µ[πd2, πd2, πd1, π]), Ă4(µ[πd2, πd2, πd2, π]))}\nA πd agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 4 (any π) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up. So:\nAD4(π,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nAD4(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\n4 {AD1(Πe, wΠe ,Πo, wL̇, µ) +AD2(Πe, wΠe ,Πo, wL̇, µ)+\n+AD3(Πe, wΠe ,Πo, wL̇, µ) +AD4(Πe, wΠe ,Πo, wL̇, µ)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : AD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, predator-prey has Leftmax = 0 for this property.\nProposition 51. Rightmin for the action dependency (AD) property is equal to 0 for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πu} with wΠe(πu) = 1 (a πu agent always performs Up) we find this situation no matter which Πo we use.\nFollowing definition 14, we obtain the AD value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 13, we could calculate its AD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 12. We start with slot 1:\nAD1(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă1(µ[u̇ 1← πu]), Ă1(µ[v̇ 1← πu]))\nWe do not know which Πo we have, but we know that we will need to obtain two different line-up patterns\nu̇ and v̇ from L̇ N(µ) −1 (Πo) to calculate ∆S(Ă1(µ[u̇ 1← πu]), Ă1(µ[v̇ 1← πu])). We calculate this value for two figurative line-up patterns u̇ = (∗, π1, π2, π3) and v̇ = (∗, π4, π5, π6) from L̇N(µ)−1 (Πo):\n∆S(Ă1(µ[u̇ 1← πu]), Ă1(µ[v̇ 1← πu])) = ∆S(Ă1(µ[πu, π1, π2, π3]), Ă1(µ[πu, π4, π5, π6]))\nHere, the agent in both slots 1 (πu) will perform the same sequence of actions (always Up) independently of the line-up. So no matter which agents are in Πo we obtain:\nAD1(πu,Πo, wL̇, µ) = 0\nFor slot 2: AD2(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă2(µ[u̇ 2← πu]), Ă2(µ[v̇ 2← πu]))\nWe do not know which Πo we have, but we know that we will need to obtain two different line-up patterns\nu̇ and v̇ from L̇ N(µ) −2 (Πo) to calculate ∆S(Ă2(µ[u̇ 2← πu]), Ă2(µ[v̇ 2← πu])). We calculate this value for two figurative line-up patterns u̇ = (π1, ∗, π2, π3) and v̇ = (π4, ∗, π5, π6) from L̇N(µ)−2 (Πo):\n∆S(Ă2(µ[u̇ 2← πu]), Ă2(µ[v̇ 2← πu])) = ∆S(Ă2(µ[π1, πu, π2, π3]), Ă2(µ[π4, πu, π5, π6]))\nHere, the agent in both slots 2 (πu) will perform the same sequence of actions (always Up) independently of the line-up. So no matter which agents are in Πo we obtain:\nAD2(πu,Πo, wL̇, µ) = 0\nFor slot 3: AD3(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−3 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă3(µ[u̇ 3← πu]), Ă3(µ[v̇ 3← πu]))\nWe do not know which Πo we have, but we know that we will need to obtain two different line-up patterns\nu̇ and v̇ from L̇ N(µ) −3 (Πo) to calculate ∆S(Ă3(µ[u̇ 3← πu]), Ă3(µ[v̇ 3← πu])). We calculate this value for two figurative line-up patterns u̇ = (π1, π2, ∗, π3) and v̇ = (π4, π5, ∗, π6) from L̇N(µ)−3 (Πo):\n∆S(Ă3(µ[u̇ 3← πu]), Ă3(µ[v̇ 3← πu])) = ∆S(Ă3(µ[π1, π2, πu, π3]), Ă3(µ[π4, π5, πu, π6]))\nHere, the agent in both slots 3 (πu) will perform the same sequence of actions (always Up) independently of the line-up. So no matter which agents are in Πo we obtain:\nAD3(πu,Πo, wL̇, µ) = 0\nAnd for slot 4: AD4(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−4 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆S(Ă4(µ[u̇ 4← πu]), Ă4(µ[v̇ 4← πu]))\nWe do not know which Πo we have, but we know that we will need to obtain two different line-up patterns\nu̇ and v̇ from L̇ N(µ) −4 (Πo) to calculate ∆S(Ă4(µ[u̇ 4← πu]), Ă4(µ[v̇ 4← πu])). We calculate this value for two figurative line-up patterns u̇ = (π1, π2, π3, ∗) and v̇ = (π4, π5, π6, ∗) from L̇N(µ)−4 (Πo):\n∆S(Ă4(µ[u̇ 4← πu]), Ă4(µ[v̇ 4← πu])) = ∆S(Ă4(µ[π1, π2, π3, πu]), Ă4(µ[π4, π5, π6, πu]))\nHere, the agent in both slots 4 (πu) will perform the same sequence of actions (always Up) independently of the line-up. So no matter which agents are in Πo we obtain:\nAD4(πu,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nAD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)ADi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)ADi(πu,Πo, wL̇, µ) = = 1\n4 {AD1(πu,Πo, wL̇, µ) +AD2(πu,Πo, wL̇, µ)+\n+AD3(πu,Πo, wL̇, µ) +AD4(πu,Πo, wL̇, µ)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every Πo we obtain the same result:\n∀Πo : AD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, predator-prey has Rightmin = 0 for this property."
    }, {
      "heading" : "C.2 Reward Dependency",
      "text" : "We continue with the reward dependency (RD) property. As given in section 4.3.1, we want to know if the evaluated agents obtain different expected average rewards depending on which line-up they interact with. We use ∆Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 52. Generalmin for the reward dependency (RD) property is equal to 0 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πu} with wΠe(πu) = 1 and Πo = {πd1, πd2} (a πd agent always performs Down and a πu agent always performs Up).\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← πu]), R1(µ[v̇ 1← πu])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R1(µ[πu, πd1, πd1, πd1]), R1(µ[πu, πd1, πd1, πd2]))+\n+∆Q(R1(µ[πu, πd1, πd1, πd1]), R1(µ[πu, πd1, πd2, πd1]))+\n...\n+ ∆Q(R1(µ[πu, πd2, πd2, πd1]), R1(µ[πu, πd2, πd2, πd2]))}\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nIn this case, we have 28 possible pairs of line-ups, where πu will always perform Up and a πd agent will always perform Down, so the agent in both slots 1 (πu) will obtain the same expected average reward (1) independently of the line-up. So:\nRD1(πu,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nFor slot 2, the agent in both slots 2 (πu) will also obtain the same expected average reward (1) independently of the line-up. So:\nRD2(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R2(µ[u̇ 2← πu]), R2(µ[v̇ 2← πu])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R2(µ[πd1, πu, πd1, πd1]), R2(µ[πd1, πu, πd1, πd2]))+\n+∆Q(R2(µ[πd1, πu, πd1, πd1]), R2(µ[πd1, πu, πd2, πd1]))+\n...\n+ ∆Q(R2(µ[πd2, πu, πd2, πd1]), R2(µ[πd2, πu, πd2, πd2]))} = = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nFor slot 3, the agent in both slots 3 (πu) will also obtain the same expected average reward (−1) independently of the line-up. So:\nRD3(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−3 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R3(µ[u̇ 3← πu]), R3(µ[v̇ 3← πu])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R3(µ[πd1, πd1, πu, πd1]), R3(µ[πd1, πd1, πu, πd2]))+\n+∆Q(R3(µ[πd1, πd1, πu, πd1]), R3(µ[πd1, πd2, πu, πd1]))+\n...\n+ ∆Q(R3(µ[πd2, πd2, πu, πd1]), R3(µ[πd2, πd2, πu, πd2]))} = = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nAnd for slot 4, the agent in both slots 4 (πu) will also obtain the same expected average reward (1) independently of the line-up. So:\nRD4(πu,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−4 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R4(µ[u̇ 4← πu]), R4(µ[v̇ 4← πu])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R4(µ[πd1, πd1, πd1, πu]), R4(µ[πd1, πd1, πd2, πu]))+\n+∆Q(R4(µ[πd1, πd1, πd1, πu]), R4(µ[πd1, πd2, πd1, πu]))+\n...\n+ ∆Q(R4(µ[πd2, πd2, πd1, πu]), R4(µ[πd2, πd2, πd2, πu]))} = = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nAnd finally, we weight over the slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)RDi(πu,Πo, wL̇, µ) = = 1\n4 {RD1(πu,Πo, wL̇, µ) +RD2(πu,Πo, wL̇, µ)+\n+RD3(πu,Πo, wL̇, µ) +RD4(πu,Πo, wL̇, µ)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSince 0 is the lowest possible value for the reward dependency property, therefore predator-prey hasGeneralmin = 0 for this property.\nConjecture 2. Generalmax for the reward dependency (RD) property is equal to 1 for the predator-prey environment.\nTo find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πr} with wΠe(πr) = 1 and Πo = {πs, πr} (a πr agent always acts randomly and a πs agent always stays in the same cell\n16). Following definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(πr,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← πr]), R1(µ[v̇ 1← πr])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R1(µ[πr, πs, πs, πs]), R1(µ[πr, πs, πs, πr]))+\n+∆Q(R1(µ[πr, πs, πs, πs]), R1(µ[πr, πs, πr, πs]))+\n...\n+ ∆Q(R1(µ[πr, πr, πr, πs]), R1(µ[πr, πr, πr, πr]))}\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nThe expected average reward of these line-ups highly depends on the agents’ positions, obtaining an expected average reward from −1 to 1 (exclusive, since there always exists some probability that the prey will either be chased or not) to the agent in slot 1 (πr). One reason is the stochastic behaviour of the πr agents, which makes that no pair of line-ups can obtain exactly the same result. Another reason is that the positions of the slots where the random agents play as a predator do not have a symmetric place in the space (blocks are not symmetrically located in the space) which, for each πr in a different slot, provides (most likely) different probabilities to chase the prey17. This makes every pair of line-up to have different expected average rewards, making its reward dependency equal to 1.\nRD1(πr,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 1} = 1\nFor slot 2, also the result of these line-ups highly depends on the agents’ positions. So:\nRD2(πr,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R2(µ[u̇ 2← πr]), R2(µ[v̇ 2← πr])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R2(µ[πs, πr, πs, πs]), R2(µ[πs, πr, πs, πr]))+\n+∆Q(R2(µ[πs, πr, πs, πs]), R2(µ[πs, πr, πr, πs]))+\n...\n+ ∆Q(R2(µ[πr, πr, πr, πs]), R2(µ[πr, πr, πr, πr]))} =\n= 2 8\n7\n1\n8\n1 8 {28× 1} = 1\nFor slot 3, also the result of these line-ups highly depends on the agents’ positions. So:\n16Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\n17It is more likely that the prey will be chased by the lower left predator than the upper right predator, and the lower right predator will have the lowest chance to chase the prey.\nRD3(πr,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−3 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R3(µ[u̇ 3← πr]), R3(µ[v̇ 3← πr])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R3(µ[πs, πs, πr, πs]), R3(µ[πs, πs, πr, πr]))+\n+∆Q(R3(µ[πs, πs, πr, πs]), R3(µ[πs, πr, πr, πs]))+\n...\n+ ∆Q(R3(µ[πr, πr, πr, πs]), R3(µ[πr, πr, πr, πr]))} =\n= 2 8\n7\n1\n8\n1 8 {28× 1} = 1\nAnd for slot 4, also the result of these line-ups highly depends on the agents’ positions. So:\nRD4(πr,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−4 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R4(µ[u̇ 4← πr]), R4(µ[v̇ 4← πr])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R4(µ[πs, πs, πs, πr]), R4(µ[πs, πs, πr, πr]))+\n+∆Q(R4(µ[πs, πs, πs, πr]), R4(µ[πs, πr, πs, πr]))+\n...\n+ ∆Q(R4(µ[πr, πr, πs, πr]), R4(µ[πr, πr, πr, πr]))} =\n= 2 8\n7\n1\n8\n1 8 {28× 1} = 1\nAnd finally, we weight over the slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)RDi(πr,Πo, wL̇, µ) = = 1\n4 {RD1(πr,Πo, wL̇, µ) +RD2(πr,Πo, wL̇, µ)+\n+RD3(πr,Πo, wL̇, µ) +RD4(πr,Πo, wL̇, µ)} = = 1\n4 {1 + 1 + 1 + 1} = 1\nSince 1 is the highest possible value for the reward dependency property, therefore predator-prey has Generalmax = 1 for this property.\nProposition 53. Leftmax for the reward dependency (RD) property is equal to 0 for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πo = {πd1, πd2} (a πd agent always performs Down) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we can calculate its RD value for slot 1:\nRD1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)RD1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate RD1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 15 to calculate this value for a figurative evaluated agent π from Πe:\nRD1(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← π]), R1(µ[v̇ 1← π])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R1(µ[π, πd1, πd1, πd1]), R1(µ[π, πd1, πd1, πd2]))+\n+∆Q(R1(µ[π, πd1, πd1, πd1]), R1(µ[π, πd1, πd2, πd1]))+\n...\n+ ∆Q(R1(µ[π, πd2, πd2, πd1]), R1(µ[π, πd2, πd2, πd2]))}\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nA πd agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 1 (any π) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up, obtaining agent in both slots 1 (any π) the same expected average reward. So:\nRD1(π,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nRD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nFor slot 2:\nRD2(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)RD2(π,Πo, wL̇, µ)\nAgain, we do not know which Πe we have, but we know that we will need to evaluate RD2(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 15 to calculate this value for a figurative evaluated agent π from Πe:\nRD2(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R2(µ[u̇ 2← π]), R2(µ[v̇ 2← π])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R2(µ[πd1, π, πd1, πd1]), R2(µ[πd1, π, πd1, πd2]))+\n+∆Q(R2(µ[πd1, π, πd1, πd1]), R2(µ[πd1, π, πd2, πd1]))+\n...\n+ ∆Q(R2(µ[πd2, π, πd2, πd1]), R2(µ[πd2, π, πd2, πd2]))}\nAgain, a πd agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 2 (any π) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up, obtaining agent in both slots 2 (any π) the same expected average reward. So:\nRD2(π,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nRD2(Πe, wΠe ,Πo, wL̇, µ) = 0\nFor slot 3:\nRD3(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)RD3(π,Πo, wL̇, µ)\nAgain, we do not know which Πe we have, but we know that we will need to evaluate RD3(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 15 to calculate this value for a figurative evaluated agent π from Πe:\nRD3(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−3 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R3(µ[u̇ 3← π]), R3(µ[v̇ 3← π])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R3(µ[πd1, πd1, π, πd1]), R3(µ[πd1, πd1, π, πd2]))+\n+∆Q(R3(µ[πd1, πd1, π, πd1]), R3(µ[πd1, πd2, π, πd1]))+\n...\n+ ∆Q(R3(µ[πd2, πd2, π, πd1]), R3(µ[πd2, πd2, π, πd2]))}\nAgain, a πd agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 3 (any π) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up, obtaining agent in both slots 3 (any π) the same expected average reward. So:\nRD3(π,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nRD3(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd for slot 4:\nRD4(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)RD4(π,Πo, wL̇, µ)\nAgain, we do not know which Πe we have, but we know that we will need to evaluate RD4(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 15 to calculate this value for a figurative evaluated agent π from Πe:\nRD4(π,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−4 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R4(µ[u̇ 4← π]), R4(µ[v̇ 4← π])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R4(µ[πd1, πd1, πd1, π]), R4(µ[πd1, πd1, πd2, π]))+\n+∆Q(R4(µ[πd1, πd1, πd1, π]), R4(µ[πd1, πd2, πd1, π]))+\n...\n+ ∆Q(R4(µ[πd2, πd2, πd1, π]), R4(µ[πd2, πd2, πd2, π]))}\nAgain, a πd agent will always perform Down, so for the 28 possible pairs of line-ups we obtain a situation where the agent in both slots 4 (any π) will be able to differentiate with which agents is interacting, so it will not be able to change its distribution of action sequences depending on the line-up, obtaining agent in both slots 4 (any π) the same expected average reward. So:\nRD4(π,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {28× 0} = 0\nTherefore, no matter which agents are in Πe and their weights wΠe we obtain:\nRD4(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\n4 {RD1(Πe, wΠe ,Πo, wL̇, µ) +RD2(Πe, wΠe ,Πo, wL̇, µ)+\n+RD3(Πe, wΠe ,Πo, wL̇, µ) +RD4(Πe, wΠe ,Πo, wL̇, µ)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : RD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, predator-prey has Leftmax = 0 for this property.\nApproximation 1. Rightmin for the reward dependency (RD) property is equal to 13 28 (as a lower approximation) for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πchase} with wΠe(πchase) = 1 and Πo = {πlose, πwin} (a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, a πlose agent always tries to be chased when playing as the prey and tries to do not chase when playing as the predator, and a πwin agent always tries to not be chased when playing as the prey and tries to chase when playing as the predator) we find a lower approximation with this situation.\nFollowing definition 17, we obtain the RD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 16, we could calculate its RD value for each slot but, since Πe has only one agent and its weight is equal to 1, it is equivalent to use directly definition 15. We start with slot 1:\nRD1(πchase,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−1 (Πo)|u̇ ̸=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R1(µ[u̇ 1← πchase]), R1(µ[v̇ 1← πchase])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R1(µ[πchase, πlose, πlose, πlose]), R1(µ[πchase, πlose, πlose, πwin]))+\n+∆Q(R1(µ[πchase, πlose, πlose, πlose]), R1(µ[πchase, πlose, πwin, πlose]))+\n...\n+ ∆Q(R1(µ[πchase, πwin, πwin, πlose]), R1(µ[πchase, πwin, πwin, πwin]))}\nNote that we avoided to calculate both ∆Q(a, b) and ∆Q(b, a) since they provide the same result, by calculating only ∆Q(a, b) and multiplying the result by 2.\nFrom the 28 possible pairs of line-ups that we obtained, πchase tries to make equal the maximum number of pairs, while πwin and πlose try to diverge the maximum number of pairs. In this case, the agents from Πo can only assure that two line-up patterns obtain different results ((∗, πlose, πlose, πlose) and (∗, πwin, πwin, πwin)), therefore the agent from Πe (πchase) can make equal seven of the eight line-ups 18. So:\nRD1(πchase,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {7× 1 + 21× 0} = 1 4\nFor slot 2:\n18Note that only one predator trying to win is enough to chase a prey who wants to be chased.\nRD2(πchase,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−2 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R2(µ[u̇ 2← πchase]), R2(µ[v̇ 2← πchase])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R2(µ[πlose, πchase, πlose, πlose]), R2(µ[πlose, πchase, πlose, πwin]))+\n+∆Q(R2(µ[πlose, πchase, πlose, πlose]), R2(µ[πlose, πchase, πwin, πlose]))+\n...\n+ ∆Q(R2(µ[πwin, πchase, πwin, πlose]), R2(µ[πwin, πchase, πwin, πwin]))}\nFrom the 28 possible pairs of line-ups that we obtained, πchase tries to make equal the maximum number of pairs, while πwin and πlose try to diverge the maximum number of pairs. In this case, the agents from Πo can assure that three line-up patterns obtain the same result ((πlose, ∗, πlose, πwin), (πlose, ∗, πwin, πlose) and (πlose, ∗, πwin, πwin)) and other three line-up patterns obtain a different result ((πwin, ∗, πlose, πlose), (πwin, ∗, πlose, πwin) and (πwin, ∗, πwin, πlose)), therefore the agent from Πe (πchase) can only make equal five of the eight line-ups. So:\nRD2(πchase,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {15× 1 + 13× 0} = 15 28\nFor slot 3:\nRD3(πchase,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−3 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R3(µ[u̇ 3← πchase]), R3(µ[v̇ 3← πchase])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R3(µ[πlose, πlose, πchase, πlose]), R3(µ[πlose, πlose, πchase, πwin]))+\n+∆Q(R3(µ[πlose, πlose, πchase, πlose]), R3(µ[πlose, πwin, πchase, πlose]))+\n...\n+ ∆Q(R3(µ[πwin, πwin, πchase, πlose]), R3(µ[πwin, πwin, πchase, πwin]))}\nFrom the 28 possible pairs of line-ups that we obtained, πchase tries to make equal the maximum number of pairs, while πwin and πlose try to diverge the maximum number of pairs. In this case, the agents from Πo can assure that three line-up patterns obtain the same result ((πlose, πlose, ∗, πwin), (πlose, πwin, ∗, πlose) and (πlose, πwin, ∗, πwin)) and other three line-up patterns obtain a different result ((πwin, πlose, ∗, πlose), (πwin, πlose, ∗, πwin) and (πwin, πwin, ∗, πlose)), therefore the agent from Πe (πchase) can only make equal five of the eight line-ups. So:\nRD3(πchase,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {15× 1 + 13× 0} = 15 28\nAnd for slot 4:\nRD4(πchase,Πo, wL̇, µ) = ηL̇2 ∑\nu̇,v̇∈L̇N(µ)−4 (Πo)|u̸̇=v̇\nwL̇(u̇)wL̇(v̇)∆Q(R4(µ[u̇ 4← πchase]), R4(µ[v̇ 4← πchase])) =\n= 2 8\n7\n1\n8\n1 8 {∆Q(R4(µ[πlose, πlose, πlose, πchase]), R4(µ[πlose, πlose, πwin, πchase]))+\n+∆Q(R4(µ[πlose, πlose, πlose, πchase]), R4(µ[πlose, πwin, πlose, πchase]))+\n...\n+ ∆Q(R4(µ[πwin, πwin, πlose, πchase]), R4(µ[πwin, πwin, πwin, πchase]))}\nFrom the 28 possible pairs of line-ups that we obtained, πchase tries to make equal the maximum number of pairs, while πwin and πlose try to diverge the maximum number of pairs. In this case, the agents from\nΠo can assure that three line-up patterns obtain the same result ((πlose, πlose, πwin, ∗), (πlose, πwin, πlose, ∗) and (πlose, πwin, πwin, ∗)) and other three line-up patterns obtain a different result ((πwin, πlose, πlose, ∗), (πwin, πlose, πwin, ∗) and (πwin, πwin, πlose, ∗)), therefore the agent from Πe (πchase) can only make equal five of the eight line-ups. So:\nRD4(πchase,Πo, wL̇, µ) = 2 8\n7\n1\n8\n1 8 {15× 1 + 13× 0} = 15 28\nAnd finally, we weight over the slots:\nRD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)RDi(Πe, wΠe ,Πo, wL̇, µ) =\n= N(µ)∑ i=1 wS(i, µ)RDi(πchase,Πo, wL̇, µ) = = 1\n4 {RD1(πchase,Πo, wL̇, µ) +RD2(πchase,Πo, wL̇, µ)+\n+RD3(πchase,Πo, wL̇, µ) +RD4(πchase,Πo, wL̇, µ)} = = 1\n4\n{ 1\n4 +\n15 28 + 15 28 + 15 28\n} = 13\n28\nTherefore, predator-prey has Rightmin = 13 28 (as a lower approximation) for this property."
    }, {
      "heading" : "C.3 Fine Discrimination",
      "text" : "Now we move to the fine discrimination (FD) property. As given in section 4.4.1, we want to know if different evaluated agents obtain different expected average rewards when interacting in the environment. We use ∆Q(a, b) = 1 if numbers a and b are equal and 0 otherwise.\nProposition 54. Generalmin for the fine discrimination (FD) property is equal to 0 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πu1, πu2} with uniform weight for wΠe and Πo = {πd} (a πu agent always performs Up and a πd agent always performs Down).\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πu1, πu2,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πu1, πu2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD1(πu1, πu2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πu1]), R1(µ[l̇ 1← πu2])) =\n= ∆Q(R1(µ[πu1, πd, πd, πd]), R1(µ[πu2, πd, πd, πd]))\nHere, a πu agent will always perform Up and πd will always perform Down, so both agents in slot 1 (πu1 and πu2) will obtain the same expected average reward (1). So:\nFD1(πu1, πu2,Πo, wL̇, µ) = 0\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nFor slot 2: FD2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD2(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD2(πu1, πu2,Πo, wL̇, µ)\nAgain, we only need to calculate FD2(πu1, πu2,Πo, wL̇, µ). We follow definition 21 to calculate this value: FD2(πu1, πu2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2 (Πo)\nwL̇(l̇)∆Q(R2(µ[l̇ 2← πu1]), R2(µ[l̇ 2← πu2])) =\n= ∆Q(R2(µ[πd, πu1, πd, πd]), R2(µ[πd, πu2, πd, πd]))\nAgain, a πu agent will always perform Up and πd will always perform Down, so both agents in slot 2 (πu1 and πu2) will obtain the same expected average reward (1). So:\nFD2(πu1, πu2,Πo, wL̇, µ) = 0\nTherefore:\nFD2(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nFor slot 3: FD3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD3(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD3(πu1, πu2,Πo, wL̇, µ)\nAgain, we only need to calculate FD3(πu1, πu2,Πo, wL̇, µ). We follow definition 21 to calculate this value: FD3(πu1, πu2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3 (Πo)\nwL̇(l̇)∆Q(R3(µ[l̇ 3← πu1]), R3(µ[l̇ 3← πu2])) =\n= ∆Q(R3(µ[πd, πd, πu1, πd]), R3(µ[πd, πd, πu2, πd]))\nAgain, a πu agent will always perform Up and πd will always perform Down, so both agents in slot 3 (πu1 and πu2) will obtain the same expected average reward (−1). So:\nFD3(πu1, πu2,Πo, wL̇, µ) = 0\nTherefore:\nFD3(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 4: FD4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD4(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD4(πu1, πu2,Πo, wL̇, µ)\nAgain, we only need to calculate FD4(πu1, πu2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD4(πu1, πu2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4 (Πo)\nwL̇(l̇)∆Q(R4(µ[l̇ 4← πu1]), R4(µ[l̇ 4← πu2])) =\n= ∆Q(R4(µ[πd, πd, πd, πu1]), R4(µ[πd, πd, πd, πu2]))\nAgain, a πu agent will always perform Up and πd will always perform Down, so both agents in slot 4 (πu1 and πu2) will obtain the same expected average reward (1). So:\nFD4(πu1, πu2,Πo, wL̇, µ) = 0\nTherefore:\nFD4(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\n4 {FD1(Πe, wΠe ,Πo, wL̇, µ) + FD2(Πe, wΠe ,Πo, wL̇, µ)+\n+ FD3(Πe, wΠe ,Πo, wL̇, µ) + FD4(Πe, wΠe ,Πo, wL̇, µ)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSince 0 is the lowest possible value for the fine discriminative property, therefore predator-prey hasGeneralmin = 0 for this property.\nProposition 55. Generalmax for the fine discrimination (FD) property is equal to 1 for the predator-prey environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πs, πr} with uniform weight for wΠe and Πo = {πs} (a πr agent always acts randomly and a πs agent always stays in the same cell19).\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πs, πr,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πs, πr,Πo, wL̇, µ). We follow definition 21 to calculate this value: FD1(πs, πr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πs]), R1(µ[l̇ 1← πr])) =\n= ∆Q(R1(µ[πs, πs, πs, πs]), R1(µ[πr, πs, πs, πs]))\nHere, a πs agent will always stay in the same cell and πr will always act randomly. In this case, πs will never been chased but πr will have a possibility to be chased, so they will obtain different expected average rewards. So:\nFD1(πs, πr,Πo, wL̇, µ) = 1\n19Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nFor slot 2: FD2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD2(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD2(πs, πr,Πo, wL̇, µ)\nAgain, we only need to calculate FD2(πs, πr,Πo, wL̇, µ). We follow definition 21 to calculate this value: FD2(πs, πr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2 (Πo)\nwL̇(l̇)∆Q(R2(µ[l̇ 2← πs]), R2(µ[l̇ 2← πr])) =\n= ∆Q(R2(µ[πs, πs, πs, πs]), R2(µ[πs, πr, πs, πs]))\nAgain, a πs agent will always stay in the same cell and πr will always act randomly. In this case, πs will never chase the prey but πr will have a possibility to chase the prey, so they will obtain different expected average rewards. So:\nFD2(πs, πr,Πo, wL̇, µ) = 1\nTherefore:\nFD2(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nFor slot 3: FD3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD3(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD3(πs, πr,Πo, wL̇, µ)\nAgain, we only need to calculate FD3(πs, πr,Πo, wL̇, µ). We follow definition 21 to calculate this value: FD3(πs, πr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3 (Πo)\nwL̇(l̇)∆Q(R3(µ[l̇ 3← πs]), R3(µ[l̇ 3← πr])) =\n= ∆Q(R3(µ[πs, πs, πs, πs]), R3(µ[πs, πs, πr, πs]))\nAgain, a πs agent will always stay in the same cell and πr will always act randomly. In this case, πs will never chase the prey but πr will have a possibility to chase the prey, so they will obtain different expected average rewards. So:\nFD3(πs, πr,Πo, wL̇, µ) = 1\nTherefore:\nFD3(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd for slot 4: FD4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD4(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD4(πs, πr,Πo, wL̇, µ)\nAgain, we only need to calculate FD4(πs, πr,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD4(πs, πr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4 (Πo)\nwL̇(l̇)∆Q(R4(µ[l̇ 4← πs]), R4(µ[l̇ 4← πr])) =\n= ∆Q(R4(µ[πs, πs, πs, πs]), R4(µ[πs, πs, πs, πr]))\nAgain, a πs agent will always stay in the same cell and πr will always act randomly. In this case, πs will never chase the prey but πr will have a possibility to chase the prey, so they will obtain different expected average rewards. So:\nFD4(πs, πr,Πo, wL̇, µ) = 1\nTherefore:\nFD4(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 1 = 1\nAnd finally, we weight over the slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ) =\n= 1\n4 {FD1(Πe, wΠe ,Πo, wL̇, µ) + FD2(Πe, wΠe ,Πo, wL̇, µ)+\n+ FD3(Πe, wΠe ,Πo, wL̇, µ) + FD4(Πe, wΠe ,Πo, wL̇, µ)} = = 1\n4 {1 + 1 + 1 + 1} = 1\nSince 1 is the highest possible value for the fine discriminative property, therefore predator-prey hasGeneralmax = 1 for this property.\nProposition 56. Leftmax for the fine discrimination (FD) property is equal to 0 for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πo = {πchase} (a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find this situation no matter which pair ⟨Πe, wΠe⟩ we use.\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩ (where Πe and wΠe are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate FD1(π1, π2,Πo, wL̇, µ) for all pair of evaluated agents π1, π2 ∈ Πe|π1 ̸= π2. We follow definition 21 to calculate this value for two figurative evaluated agents π1 and π2 from Πe such that π1 ̸= π2:\nFD1(π1, π2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← π1]), R1(µ[l̇ 1← π2])) =\n= ∆Q(R1(µ[π1, πchase, πchase, πchase]), R1(µ[π2, πchase, πchase, πchase]))\nHere, the predators will coordinate to always chase the prey as seen in lemma 3. So no matter which agents π1 and π2 are we obtain:\nFD1(π1, π2,Πo, wL̇, µ) = 0\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 0\nFor slot 2: FD2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD2(π1, π2,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate FD2(π1, π2,Πo, wL̇, µ) for all pair of evaluated agents π1, π2 ∈ Πe|π1 ̸= π2. We follow definition 21 to calculate this value for two figurative evaluated agents π1 and π2 from Πe such that π1 ̸= π2:\nFD2(π1, π2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2 (Πo)\nwL̇(l̇)∆Q(R2(µ[l̇ 2← π1]), R2(µ[l̇ 2← π2])) =\n= ∆Q(R2(µ[πchase, π1, πchase, πchase]), R2(µ[πchase, π2, πchase, πchase]))\nHere, the prey will always try to be chased, and also at least one predator is trying to chase they prey, therefore the prey will always been chased. So no matter which agents π1 and π2 are we obtain:\nFD2(π1, π2,Πo, wL̇, µ) = 0\nTherefore:\nFD2(Πe, wΠe ,Πo, wL̇, µ) = 0\nFor slot 3: FD3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD3(π1, π2,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate FD3(π1, π2,Πo, wL̇, µ) for all pair of evaluated agents π1, π2 ∈ Πe|π1 ̸= π2. We follow definition 21 to calculate this value for two figurative evaluated agents π1 and π2 from Πe such that π1 ̸= π2:\nFD3(π1, π2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3 (Πo)\nwL̇(l̇)∆Q(R3(µ[l̇ 3← π1]), R3(µ[l̇ 3← π2])) =\n= ∆Q(R3(µ[πchase, πchase, π1, πchase]), R3(µ[πchase, πchase, π2, πchase]))\nHere, the prey will always try to be chased, and also at least one predator is trying to chase they prey, therefore the prey will always been chased. So no matter which agents π1 and π2 are we obtain:\nFD3(π1, π2,Πo, wL̇, µ) = 0\nTherefore:\nFD3(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd for slot 4: FD4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD4(π1, π2,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate FD4(π1, π2,Πo, wL̇, µ) for all pair of evaluated agents π1, π2 ∈ Πe|π1 ̸= π2. We follow definition 21 to calculate this value for two figurative evaluated agents π1 and π2 from Πe such that π1 ̸= π2:\nFD4(π1, π2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4 (Πo)\nwL̇(l̇)∆Q(R4(µ[l̇ 4← π1]), R4(µ[l̇ 4← π2])) =\n= ∆Q(R4(µ[πchase, πchase, πchase, π1]), R4(µ[πchase, πchase, πchase, π2]))\nHere, the prey will always try to be chased, and also at least one predator is trying to chase they prey, therefore the prey will always been chased. So no matter which agents π1 and π2 are we obtain:\nFD4(π1, π2,Πo, wL̇, µ) = 0\nTherefore:\nFD4(Πe, wΠe ,Πo, wL̇, µ) = 0\nAnd finally, we weight over the slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ)\n= 1\n4 {FD1(Πe, wΠe ,Πo, wL̇, µ) + FD2(Πe, wΠe ,Πo, wL̇, µ)+\n+ FD3(Πe, wΠe ,Πo, wL̇, µ) + FD4(Πe, wΠe ,Πo, wL̇, µ)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every pair ⟨Πe, wΠe⟩ we obtain the same result:\n∀Πe, wΠe : FD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, predator-prey has Leftmax = 0 for this property.\nProposition 57. Rightmin for the fine discrimination (FD) property is equal to 0 for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πu1, πu2} with uniform weight for wΠe (a πu agent always performs Up) we find this situation no matter which Πo we use.\nFollowing definition 23, we obtain the FD value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every slot. Following definition 22, we can calculate its FD value for each slot. We start with slot 1:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD1(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD1(πu1, πu2,Πo, wL̇, µ)\nNote that we avoided to calculate both FDi(π1, π2,Πo, wL̇, µ)} and FDi(π2, π1,Πo, wL̇, µ)} since they provide the same result, by calculating only FDi(π1, π2,Πo, wL̇, µ)} and multiplying the result by 2.\nIn this case, we only need to calculate FD1(πu1, πu2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD1(πu1, πu2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1 (Πo)\nwL̇(l̇)∆Q(R1(µ[l̇ 1← πu1]), R1(µ[l̇ 1← πu2]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −1 (Πo) to calculate ∆Q(R1(µ[l̇ 1← πu1]), R1(µ[l̇ 1← πu2])). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, π3) from L̇N(µ)−1 (Πo):\n∆Q(R1(µ[l̇ 1← πu1]), R1(µ[l̇ 1← πu2])) = ∆Q(R1(µ[πu1, π1, π2, π3]), R1(µ[πu2, π1, π2, π3]))\nA πu agent will always perform Up, so we obtain a situation where the other agents (any π1, π2, π3) will be able to differentiate with which agent they are interacting, so they will not be able to change their distribution of action sequences depending on the behaviour of the agent in slot 1, obtaining both agents in slot 1 (πu1 and πu2) the same expected average reward. So:\nFD1(πu1, πu2,Πo, wL̇, µ) = 0\nTherefore:\nFD1(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nFor slot 2: FD2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD2(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD2(πu1, πu2,Πo, wL̇, µ)\nIn this case, we only need to calculate FD2(πu1, πu2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD2(πu1, πu2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2 (Πo)\nwL̇(l̇)∆Q(R2(µ[l̇ 2← πu1]), R2(µ[l̇ 2← πu2]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −2 (Πo) to calculate ∆Q(R2(µ[l̇ 2← πu1]), R2(µ[l̇ 2← πu2])). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, π3) from L̇N(µ)−2 (Πo):\n∆Q(R2(µ[l̇ 2← πu1]), R2(µ[l̇ 2← πu2])) = ∆Q(R2(µ[π1, πu1, π2, π3]), R2(µ[π1, πu2, π2, π3]))\nA πu agent will always perform Up, so we obtain a situation where the other agents (any π1, π2, π3) will be able to differentiate with which agent they are interacting, so they will not be able to change their distribution of action sequences depending on the behaviour of the agent in slot 2, obtaining both agents in slot 2 (πu1 and πu2) the same expected average reward. So:\nFD2(πu1, πu2,Πo, wL̇, µ) = 0\nTherefore:\nFD2(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nFor slot 3: FD3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD3(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD3(πu1, πu2,Πo, wL̇, µ)\nIn this case, we only need to calculate FD3(πu1, πu2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD3(πu1, πu2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3 (Πo)\nwL̇(l̇)∆Q(R3(µ[l̇ 3← πu1]), R3(µ[l̇ 3← πu2]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −3 (Πo) to calculate ∆Q(R3(µ[l̇ 3← πu1]), R3(µ[l̇ 3← πu2])). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, π3) from L̇N(µ)−3 (Πo):\n∆Q(R3(µ[l̇ 3← πu1]), R3(µ[l̇ 3← πu2])) = ∆Q(R3(µ[π1, π2, πu1, π3]), R3(µ[π1, π2, πu2, π3]))\nA πu agent will always perform Up, so we obtain a situation where the other agents (any π1, π2, π3) will be able to differentiate with which agent they are interacting, so they will not be able to change their distribution\nof action sequences depending on the behaviour of the agent in slot 3, obtaining both agents in slot 3 (πu1 and πu2) the same expected average reward. So:\nFD3(πu1, πu2,Πo, wL̇, µ) = 0\nTherefore:\nFD3(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd for slot 4: FD4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ2 ∑\nπ1,π2∈Πe|π1 ̸=π2\nwΠe(π1)wΠe(π2)FD4(π1, π2,Πo, wL̇, µ) =\n= 2 2\n1\n1\n2\n1 2 FD4(πu1, πu2,Πo, wL̇, µ)\nIn this case, we only need to calculate FD4(πu1, πu2,Πo, wL̇, µ). We follow definition 21 to calculate this value:\nFD4(πu1, πu2,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4 (Πo)\nwL̇(l̇)∆Q(R4(µ[l̇ 4← πu1]), R4(µ[l̇ 4← πu2]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −4 (Πo) to calculate ∆Q(R4(µ[l̇ 4← πu1]), R4(µ[l̇ 4← πu2])). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, π3, ∗) from L̇N(µ)−4 (Πo):\n∆Q(R4(µ[l̇ 4← πu1]), R4(µ[l̇ 4← πu2])) = ∆Q(R4(µ[π1, π2, π3, πu1]), R4(µ[π1, π2, π3, πu2]))\nA πu agent will always perform Up, so we obtain a situation where the other agents (any π1, π2, π3) will be able to differentiate with which agent they are interacting, so they will not be able to change their distribution of action sequences depending on the behaviour of the agent in slot 4, obtaining both agents in slot 4 (πu1 and πu2) the same expected average reward. So:\nFD4(πu1, πu2,Πo, wL̇, µ) = 0\nTherefore:\nFD4(Πe, wΠe ,Πo, wL̇, µ) = 2 2\n1\n1\n2\n1 2 0 = 0\nAnd finally, we weight over the slots:\nFD(Πe, wΠe ,Πo, wL̇, µ, wS) = N(µ)∑ i=1 wS(i, µ)FDi(Πe, wΠe ,Πo, wL̇, µ)\n= 1\n4 {FD1(Πe, wΠe ,Πo, wL̇, µ) + FD2(Πe, wΠe ,Πo, wL̇, µ)+\n+ FD3(Πe, wΠe ,Πo, wL̇, µ) + FD4(Πe, wΠe ,Πo, wL̇, µ)} = = 1\n4 {0 + 0 + 0 + 0} = 0\nSo, for every Πo we obtain the same result:\n∀Πo : FD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0\nTherefore, predator-prey has Rightmin = 0 for this property."
    }, {
      "heading" : "C.4 Strict Total Grading",
      "text" : "We arrive to the strict total grading (STG) property. As given in section 4.4.2, we want to know if there exists a strict ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the STO: Ri(µ[l̇ i,j← π1, π2]) < Rj(µ[l̇ i,j← π1, π2]), Ri(µ[l̇ i,j← π2, π3]) < Rj(µ[l̇ i,j← π2, π3]) and Ri(µ[l̇ i,j← π1, π3]) < Rj(µ[l̇ i,j← π1, π3]).\nSlot i Slot j π1 < π2 π2 < π3 π1 < π3\nProposition 58. Generalmin for the strict total grading (STG) property is equal to 0 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πs1, πs2, πs3} with uniform weight for wΠe and Πo = {πx} (a πs agent always stays in the same cell20, and a πx agent acts stochastically with a probability of 1/ √ 2 to do not reach the upper left corner and a probability of 1− 1/ √ 2 to reach this corner).\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πs1, πs2, πs3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πs1, πs2, πs3, l̇, µ) =\n= STO1,2(πs1, πs2, πs3, (∗, ∗, πx, πx), µ)\nThe following table shows us STO1,2 for all the permutations of πs1, πs2, πs3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 (πx) and 4 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1−1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any πs) and 2 (any πs) to obtain the same expected average reward (0). So:\n20Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nSTG1,2(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 1 and 3:\nSTG1,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,3(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG1,3(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,3(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇)STO1,3(πs1, πs2, πs3, l̇, µ) =\n= STO1,3(πs1, πs2, πs3, (∗, πx, ∗, πx), µ) The following table shows us STO1,3 for all the permutations of πs1, πs2, πs3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (πx) and 4 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1−1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any πs) and 3 (any πs) to obtain the same expected average reward (0). So:\nSTG1,3(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG1,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 1 and 4:\nSTG1,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,4(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG1,4(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,4(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇)STO1,4(πs1, πs2, πs3, l̇, µ) =\n= STO1,4(πs1, πs2, πs3, (∗, πx, πx, ∗), µ) The following table shows us STO1,4 for all the permutations of πs1, πs2, πs3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (πx) and 3 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1−1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any πs) and 4 (any πs) to obtain the same expected average reward (0). So:\nSTG1,4(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG1,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 1:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,1(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,1(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)STO2,1(πs1, πs2, πs3, l̇, µ) =\n= STO2,1(πs1, πs2, πs3, (∗, ∗, πx, πx), µ)\nThe following table shows us STO2,1 for all the permutations of πs1, πs2, πs3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 (πx) and 4 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1−1/2 = 1/2 to chase the prey, making for both agents in slots 2 (any πs) and 1 (any πs) to obtain the same expected average reward (0). So:\nSTG2,1(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nSTG2,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,3(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,3(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,3(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇)STO2,3(πs1, πs2, πs3, l̇, µ) =\n= STO2,3(πs1, πs2, πs3, (πx, ∗, ∗, πx), µ) The following table shows us STO2,3 for all the permutations of πs1, πs2, πs3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (any πs) and 3 (any πs) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG2,3(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 4:\nSTG2,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,4(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,4(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,4(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇)STO2,4(πs1, πs2, πs3, l̇, µ) =\n= STO2,4(πs1, πs2, πs3, (πx, ∗, πx, ∗), µ) The following table shows us STO2,4 for all the permutations of πs1, πs2, πs3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (any πs) and 4 (any πs) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG2,4(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 1:\nSTG3,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,1(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,1(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,1(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇)STO3,1(πs1, πs2, πs3, l̇, µ) =\n= STO3,1(πs1, πs2, πs3, (∗, πx, ∗, πx), µ)\nThe following table shows us STO3,1 for all the permutations of πs1, πs2, πs3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (πx) and 4 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1−1/2 = 1/2 to chase the prey, making for both agents in slots 3 (any πs) and 1 (any πs) to obtain the same expected average reward (0). So:\nSTG3,1(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nSTG3,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,2(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,2(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,2(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇)STO3,2(πs1, πs2, πs3, l̇, µ) =\n= STO3,2(πs1, πs2, πs3, (πx, ∗, ∗, πx), µ)\nThe following table shows us STO3,2 for all the permutations of πs1, πs2, πs3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 (any πs) and 2 (any πs) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG3,2(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 4:\nSTG3,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,4(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,4(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,4(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇)STO3,4(πs1, πs2, πs3, l̇, µ) =\n= STO3,4(πs1, πs2, πs3, (πx, πx, ∗, ∗), µ)\nThe following table shows us STO3,4 for all the permutations of πs1, πs2, πs3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 (any πs) and 4 (any πs) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG3,4(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 1:\nSTG4,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,1(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,1(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,1(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇)STO4,1(πs1, πs2, πs3, l̇, µ) =\n= STO4,1(πs1, πs2, πs3, (∗, πx, πx, ∗), µ)\nThe following table shows us STO4,1 for all the permutations of πs1, πs2, πs3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 (πx) and 3 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1−1/2 = 1/2 to chase the prey, making for both agents in slots 4 (any πs) and 1 (any πs) to obtain the same expected average reward (0). So:\nSTG4,1(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nSTG4,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,2(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,2(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,2(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇)STO4,2(πs1, πs2, πs3, l̇, µ) =\n= STO4,2(πs1, πs2, πs3, (πx, ∗, πx, ∗), µ)\nThe following table shows us STO4,2 for all the permutations of πs1, πs2, πs3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 (any πs) and 2 (any πs) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG4,2(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 4 and 3:\nSTG4,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,3(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,3(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,3(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇)STO4,3(πs1, πs2, πs3, l̇, µ) =\n= STO4,3(πs1, πs2, πs3, (πx, πx, ∗, ∗), µ)\nThe following table shows us STO4,3 for all the permutations of πs1, πs2, πs3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πs1 < πs2 πs1 < πs3 πs2 < πs1 πs2 < πs3 πs3 < πs2 πs1 < πs3 πs1 < πs3 πs1 < πs2 πs2 < πs3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πs2 < πs3 πs3 < πs1 πs3 < πs2 πs3 < πs1 πs1 < πs2 πs2 < πs1 πs2 < πs1 πs3 < πs2 πs3 < πs1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 (any πs) and 3 (any πs) share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG4,3(πs1, πs2, πs3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 4\n3\n1\n4\n1 4 {STG1,2(Πe, wΠe ,Πo, wL̇, µ) + STG1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ STG1,4(Πe, wΠe ,Πo, wL̇, µ) + STG2,1(Πe, wΠe ,Πo, wL̇, µ)+ + STG2,3(Πe, wΠe ,Πo, wL̇, µ) + STG2,4(Πe, wΠe ,Πo, wL̇, µ)+ + STG3,1(Πe, wΠe ,Πo, wL̇, µ) + STG3,2(Πe, wΠe ,Πo, wL̇, µ)+ + STG3,4(Πe, wΠe ,Πo, wL̇, µ) + STG4,1(Πe, wΠe ,Πo, wL̇, µ)+ + STG4,2(Πe, wΠe ,Πo, wL̇, µ) + STG4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {12× 0} = 0\nSince 0 is the lowest possible value for the strict total grading property, therefore predator-prey hasGeneralmin = 0 for this property.\nProposition 59. Generalmax for the strict total grading (STG) property is equal to 1 2 for the predator-prey environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πx, πy, πz} with uniform weight for wΠe and Πo = {πs} (a πs agent always stays in the same cell21).\nπx behaves as shown in figure 21 when playing on each of the 4 slots.\nπy behaves as shown in figure 22 when playing on each of the 4 slots.\nπz behaves as shown in figure 23 when playing on each of the 4 slots. Following definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\n21Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πx, πy, πz,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πx, πy, πz, l̇, µ) =\n= STO1,2(πx, πy, πz, (∗, ∗, πs, πs), µ)\nThe following table shows us STO1,2 for all the permutations of πx, πy, πz.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nIt is possible to find a STO for the first permutation. In πx < πy, πx will stay at the upper left corner and πy will chase it in that cell in the 3rd iteration, so they will obtain an expected average reward of −1 and 1 respectively. In πy < πz, πy will reach the 2nd row 2nd column cell and πz will chase it in that cell in the 3rd iteration, so they will obtain an expected average reward of −1 and 1 respectively. In πx < πz, πx will stay at the upper left corner and πz will chase it in that cell in the 5th iteration, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG1,2(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nSTG1,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,3(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG1,3(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,3(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇)STO1,3(πx, πy, πz, l̇, µ) =\n= STO1,3(πx, πy, πz, (∗, πs, ∗, πs), µ)\nThe following table shows us STO1,3 for all the permutations of πx, πy, πz.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nIt is possible to find a STO for the first permutation. In πx < πy, πx will stay at the upper left corner and πy will chase it in that cell in the 3rd iteration, so they will obtain an expected average reward of −1 and 1 respectively. In πy < πz, πy will reach the 2nd row 2nd column cell and πz will chase it in that cell in the 3rd iteration, so they will obtain an expected average reward of −1 and 1 respectively. In πx < πz, πx will stay at the upper left corner and πz will chase it in that cell in the 5th iteration, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG1,3(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nSTG1,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,4(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG1,4(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,4(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇)STO1,4(πx, πy, πz, l̇, µ) =\n= STO1,4(πx, πy, πz, (∗, πs, πs, ∗), µ)\nThe following table shows us STO1,4 for all the permutations of πx, πy, πz.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nIt is possible to find a STO for the first permutation. In πx < πy, πx will stay at the upper left corner and πy will chase it in that cell in the 6th iteration, so they will obtain an expected average reward of −1 and 1 respectively. In πy < πz, πy will reach the 2nd row 2nd column cell and πz will chase it in that cell in the 4th iteration, so they will obtain an expected average reward of −1 and 1 respectively. In πx < πz, πx will stay at the upper left corner and πz will chase it in that cell in the 6th iteration, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG1,4(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,1(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,1(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)STO2,1(πx, πy, πz, l̇, µ) =\n= STO2,1(πx, πy, πz, (∗, ∗, πs, πs), µ) The following table shows us STO2,1 for all the permutations of πx, πy, πz.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nIt is possible to find a STO for the first permutation. In πx < πy, πy will reach the 2nd row 2nd column cell and πx will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. In πy < πz, πz will reach the 2nd row 2nd column cell and πy will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. In πx < πz, πz will reach the 2nd row 2nd column cell and πx will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG2,1(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 3:\nSTG2,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,3(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,3(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,3(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇)STO2,3(πx, πy, πz, l̇, µ) =\n= STO2,3(πx, πy, πz, (πs, ∗, ∗, πs), µ)\nThe following table shows us STO2,3 for all the permutations of πx, πy, πz.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG2,3(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 4:\nSTG2,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,4(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,4(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,4(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇)STO2,4(πx, πy, πz, l̇, µ) =\n= STO2,4(πx, πy, πz, (πs, ∗, πs, ∗), µ)\nThe following table shows us STO2,4 for all the permutations of πx, πy, πz.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG2,4(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 1:\nSTG3,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,1(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,1(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,1(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇)STO3,1(πx, πy, πz, l̇, µ) =\n= STO3,1(πx, πy, πz, (∗, πs, ∗, πs), µ) The following table shows us STO3,1 for all the permutations of πx, πy, πz.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nIt is possible to find a STO for the first permutation. In πx < πy, πy will reach the 2nd row 2nd column cell and πx will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. In πy < πz, πz will reach the 2nd row 2nd column cell and πy will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. In πx < πz, πz will reach the 2nd row 2nd column cell and πx will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG3,1(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nSTG3,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 2:\nSTG3,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,2(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,2(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,2(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇)STO3,2(πx, πy, πz, l̇, µ) =\n= STO3,2(πx, πy, πz, (πs, ∗, ∗, πs), µ) The following table shows us STO3,2 for all the permutations of πx, πy, πz.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG3,2(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 4:\nSTG3,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,4(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,4(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,4(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇)STO3,4(πx, πy, πz, l̇, µ) =\n= STO3,4(πx, πy, πz, (πs, πs, ∗, ∗), µ) The following table shows us STO3,4 for all the permutations of πx, πy, πz.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG3,4(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 1:\nSTG4,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,1(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,1(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,1(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇)STO4,1(πx, πy, πz, l̇, µ) =\n= STO4,1(πx, πy, πz, (∗, πs, πs, ∗), µ)\nThe following table shows us STO4,1 for all the permutations of πx, πy, πz.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nIt is possible to find a STO for the first permutation. In πx < πy, πy will reach the 2nd row 2nd column cell and πx will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. In πy < πz, πz will reach the 2nd row 2nd column cell and πy will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. In πx < πz, πz will reach the 2nd row 2nd column cell and πx will never chase it in that cell, so they will obtain an expected average reward of −1 and 1 respectively. So:\nSTG4,1(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nSTG4,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 2:\nSTG4,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,2(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,2(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,2(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇)STO4,2(πx, πy, πz, l̇, µ) =\n= STO4,2(πx, πy, πz, (πs, ∗, πs, ∗), µ)\nThe following table shows us STO4,2 for all the permutations of πx, πy, πz.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG4,2(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 4 and 3:\nSTG4,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,3(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,3(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,3(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇)STO4,3(πx, πy, πz, l̇, µ) =\n= STO4,3(πx, πy, πz, (πs, πs, ∗, ∗), µ)\nThe following table shows us STO4,3 for all the permutations of πx, πy, πz.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πx < πy πx < πz πy < πx πy < πz πz < πy πx < πz πx < πz πx < πy πy < πz Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πy < πz πz < πx πz < πy πz < πx πx < πy πy < πx πy < πx πz < πy πz < πx\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nSTG4,3(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 4\n3\n1\n4\n1 4 {STG1,2(Πe, wΠe ,Πo, wL̇, µ) + STG1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ STG1,4(Πe, wΠe ,Πo, wL̇, µ) + STG2,1(Πe, wΠe ,Πo, wL̇, µ)+ + STG2,3(Πe, wΠe ,Πo, wL̇, µ) + STG2,4(Πe, wΠe ,Πo, wL̇, µ)+ + STG3,1(Πe, wΠe ,Πo, wL̇, µ) + STG3,2(Πe, wΠe ,Πo, wL̇, µ)+ + STG3,4(Πe, wΠe ,Πo, wL̇, µ) + STG4,1(Πe, wΠe ,Πo, wL̇, µ)+ + STG4,2(Πe, wΠe ,Πo, wL̇, µ) + STG4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {6× 1 + 6× 0} = 1 2\nSince 12 is the highest possible value that we can obtain for the strict total grading property, therefore predator-prey has Generalmax = 1 2 for this property.\nApproximation 2. Leftmax for the strict total grading (STG) property is equal to 1 4 (as a lower approximation) for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πe = {πchase1, πchase2, πchase3} with uniform weight for wΠe (a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find a lower approximation of this situation no matter which Πo we use.\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πchase1, πchase2, πchase3, l̇, µ)\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,2 (Πo) to calculate STO1,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, ∗, π1, π2) from L̇N(µ)−1,2 (Πo):\nSTO1,2(πchase1, πchase2, πchase3, l̇, µ) = STO1,2(πchase1, πchase2, πchase3, (∗, ∗, π1, π2), µ)\nThe following table shows us STO1,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nIt is possible to find a STO for every permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 2 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of STO1,2, so no matter which agents are in Πo we obtain:\nSTG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nSTG1,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇)STO1,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,3 (Πo) to calculate STO1,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, ∗, π2) from L̇N(µ)−1,3 (Πo):\nSTO1,3(πchase1, πchase2, πchase3, l̇, µ) = STO1,3(πchase1, πchase2, πchase3, (∗, π1, ∗, π2), µ)\nThe following table shows us STO1,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nAgain, it is possible to find a STO for every permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 3 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of STO1,3, so no matter which agents are in Πo we obtain:\nSTG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nSTG1,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇)STO1,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,4 (Πo) to calculate STO1,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, ∗) from L̇N(µ)−1,4 (Πo):\nSTO1,4(πchase1, πchase2, πchase3, l̇, µ) = STO1,4(πchase1, πchase2, πchase3, (∗, π1, π2, ∗), µ)\nThe following table shows us STO1,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nAgain, it is possible to find a STO for every permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 4 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of STO1,4, so no matter which agents are in Πo we obtain:\nSTG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)STO2,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,1 (Πo) to calculate STO2,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, ∗, π1, π2) from L̇N(µ)−2,1 (Πo):\nSTO2,1(πchase1, πchase2, πchase3, l̇, µ) = STO2,1(πchase1, πchase2, πchase3, (∗, ∗, π1, π2), µ)\nThe following table shows us STO2,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nIt is not possible to find a STO for any permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 2 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of STO2,1, so no matter which agents are in Πo we obtain:\nSTG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nSTG2,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇)STO2,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,3 (Πo) to calculate STO2,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, ∗, π2) from L̇N(µ)−2,3 (Πo):\nSTO2,3(πchase1, πchase2, πchase3, l̇, µ) = STO2,3(πchase1, πchase2, πchase3, (π1, ∗, ∗, π2), µ)\nThe following table shows us STO2,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 4:\nSTG2,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇)STO2,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,4 (Πo) to calculate STO2,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, ∗) from L̇N(µ)−2,4 (Πo):\nSTO2,4(πchase1, πchase2, πchase3, l̇, µ) = STO2,4(πchase1, πchase2, πchase3, (π1, ∗, π2, ∗), µ)\nThe following table shows us STO2,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 1:\nSTG3,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇)STO3,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,1 (Πo) to calculate STO3,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, ∗, π2) from L̇N(µ)−3,1 (Πo):\nSTO3,1(πchase1, πchase2, πchase3, l̇, µ) = STO3,1(πchase1, πchase2, πchase3, (∗, π1, ∗, π2), µ)\nThe following table shows us STO3,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nAgain, it is not possible to find a STO for any permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 3 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of STO3,1, so no matter which agents are in Πo we obtain:\nSTG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nSTG3,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇)STO3,2(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,2 (Πo) to calculate STO3,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, ∗, π2) from L̇N(µ)−3,2 (Πo):\nSTO3,2(πchase1, πchase2, πchase3, l̇, µ) = STO3,2(πchase1, πchase2, πchase3, (π1, ∗, ∗, π2), µ)\nThe following table shows us STO3,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 4:\nSTG3,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇)STO3,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,4 (Πo) to calculate STO3,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, ∗) from L̇N(µ)−3,4 (Πo):\nSTO3,4(πchase1, πchase2, πchase3, l̇, µ) = STO3,4(πchase1, πchase2, πchase3, (π1, π2, ∗, ∗), µ)\nThe following table shows us STO3,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 1:\nSTG4,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇)STO4,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,1 (Πo) to calculate STO4,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, ∗) from L̇N(µ)−4,1 (Πo):\nSTO4,1(πchase1, πchase2, πchase3, l̇, µ) = STO4,1(πchase1, πchase2, πchase3, (∗, π1, π2, ∗), µ)\nThe following table shows us STO4,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nAgain, it is not possible to find a STO for any permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 4 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of STO4,1, so no matter which agents are in Πo we obtain:\nSTG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nSTG4,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇)STO4,2(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,2 (Πo) to calculate STO4,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, ∗) from L̇N(µ)−4,2 (Πo):\nSTO4,2(πchase1, πchase2, πchase3, l̇, µ) = STO4,2(πchase1, πchase2, πchase3, (π1, ∗, π2, ∗), µ)\nThe following table shows us STO4,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 4 and 3:\nSTG4,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇)STO4,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,3 (Πo) to calculate STO4,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, ∗) from L̇N(µ)−4,3 (Πo):\nSTO4,3(πchase1, πchase2, πchase3, l̇, µ) = STO4,3(πchase1, πchase2, πchase3, (π1, π2, ∗, ∗), µ)\nThe following table shows us STO4,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 4\n3\n1\n4\n1 4 {STG1,2(Πe, wΠe ,Πo, wL̇, µ) + STG1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ STG1,4(Πe, wΠe ,Πo, wL̇, µ) + STG2,1(Πe, wΠe ,Πo, wL̇, µ)+ + STG2,3(Πe, wΠe ,Πo, wL̇, µ) + STG2,4(Πe, wΠe ,Πo, wL̇, µ)+ + STG3,1(Πe, wΠe ,Πo, wL̇, µ) + STG3,2(Πe, wΠe ,Πo, wL̇, µ)+ + STG3,4(Πe, wΠe ,Πo, wL̇, µ) + STG4,1(Πe, wΠe ,Πo, wL̇, µ)+ + STG4,2(Πe, wΠe ,Πo, wL̇, µ) + STG4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {3× 1 + 9× 0} = 1 4\nSo, for every Πo we obtain the same result:\n∀Πo : STG(Πe, wΠe ,Πo, wL̇, µ, wS) = 1\n4\nTherefore, predator-prey has Leftmax = 1 4 (as a lower approximation) for this property.\nApproximation 3. Rightmin for the strict total grading (STG) property is equal to 1 4 (as a higher approximation) for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πchase1, πchase2, πchase3} with uniform weight for wΠe (a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find a higher approximation of this situation no matter which Πo we use.\nFollowing definition 29, we obtain the STG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28, we can calculate its STG value for each pair of slots. We start with slots 1 and 2:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for STGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate STG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)STO1,2(πchase1, πchase2, πchase3, l̇, µ)\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,2 (Πo) to calculate STO1,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, ∗, π1, π2) from L̇N(µ)−1,2 (Πo):\nSTO1,2(πchase1, πchase2, πchase3, l̇, µ) = STO1,2(πchase1, πchase2, πchase3, (∗, ∗, π1, π2), µ)\nThe following table shows us STO1,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nIt is possible to find a STO for every permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 2 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of STO1,2, so no matter which agents are in Πo we obtain:\nSTG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nSTG1,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇)STO1,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,3 (Πo) to calculate STO1,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, ∗, π2) from L̇N(µ)−1,3 (Πo):\nSTO1,3(πchase1, πchase2, πchase3, l̇, µ) = STO1,3(πchase1, πchase2, πchase3, (∗, π1, ∗, π2), µ)\nThe following table shows us STO1,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nAgain, it is possible to find a STO for every permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 3 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of STO1,3, so no matter which agents are in Πo we obtain:\nSTG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nSTG1,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG1,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇)STO1,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,4 (Πo) to calculate STO1,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, ∗) from L̇N(µ)−1,4 (Πo):\nSTO1,4(πchase1, πchase2, πchase3, l̇, µ) = STO1,4(πchase1, πchase2, πchase3, (∗, π1, π2, ∗), µ) The following table shows us STO1,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nAgain, it is possible to find a STO for every permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 4 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of STO1,4, so no matter which agents are in Πo we obtain:\nSTG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nSTG1,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)STO2,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,1 (Πo) to calculate STO2,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, ∗, π1, π2) from L̇N(µ)−2,1 (Πo):\nSTO2,1(πchase1, πchase2, πchase3, l̇, µ) = STO2,1(πchase1, πchase2, πchase3, (∗, ∗, π1, π2), µ) The following table shows us STO2,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nIt is not possible to find a STO for any permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 2 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of STO2,1, so no matter which agents are in Πo we obtain:\nSTG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nSTG2,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇)STO2,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,3 (Πo) to calculate STO2,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, ∗, π2) from L̇N(µ)−2,3 (Πo):\nSTO2,3(πchase1, πchase2, πchase3, l̇, µ) = STO2,3(πchase1, πchase2, πchase3, (π1, ∗, ∗, π2), µ) The following table shows us STO2,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 4:\nSTG2,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG2,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇)STO2,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,4 (Πo) to calculate STO2,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, ∗) from L̇N(µ)−2,4 (Πo):\nSTO2,4(πchase1, πchase2, πchase3, l̇, µ) = STO2,4(πchase1, πchase2, πchase3, (π1, ∗, π2, ∗), µ)\nThe following table shows us STO2,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG2,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 1:\nSTG3,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇)STO3,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,1 (Πo) to calculate STO3,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, ∗, π2) from L̇N(µ)−3,1 (Πo):\nSTO3,1(πchase1, πchase2, πchase3, l̇, µ) = STO3,1(πchase1, πchase2, πchase3, (∗, π1, ∗, π2), µ)\nThe following table shows us STO3,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nAgain, it is not possible to find a STO for any permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 3 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of STO3,1, so no matter which agents are in Πo we obtain:\nSTG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nSTG3,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇)STO3,2(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,2 (Πo) to calculate STO3,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, ∗, π2) from L̇N(µ)−3,2 (Πo):\nSTO3,2(πchase1, πchase2, πchase3, l̇, µ) = STO3,2(πchase1, πchase2, πchase3, (π1, ∗, ∗, π2), µ)\nThe following table shows us STO3,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 4:\nSTG3,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG3,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇)STO3,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,4 (Πo) to calculate STO3,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, ∗) from L̇N(µ)−3,4 (Πo):\nSTO3,4(πchase1, πchase2, πchase3, l̇, µ) = STO3,4(πchase1, πchase2, πchase3, (π1, π2, ∗, ∗), µ)\nThe following table shows us STO3,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG3,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 1:\nSTG4,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇)STO4,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,1 (Πo) to calculate STO4,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, ∗) from L̇N(µ)−4,1 (Πo):\nSTO4,1(πchase1, πchase2, πchase3, l̇, µ) = STO4,1(πchase1, πchase2, πchase3, (∗, π1, π2, ∗), µ)\nThe following table shows us STO4,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nAgain, it is not possible to find a STO for any permutation, since we always have πchasei < πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 4 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of STO4,1, so no matter which agents are in Πo we obtain:\nSTG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nSTG4,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇)STO4,2(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,2 (Πo) to calculate STO4,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, ∗) from L̇N(µ)−4,2 (Πo):\nSTO4,2(πchase1, πchase2, πchase3, l̇, µ) = STO4,2(πchase1, πchase2, πchase3, (π1, ∗, π2, ∗), µ)\nThe following table shows us STO4,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd for slots 4 and 3:\nSTG4,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)STG4,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 STG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate STG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 to calculate this value:\nSTG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇)STO4,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,3 (Πo) to calculate STO4,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, ∗) from L̇N(µ)−4,3 (Πo):\nSTO4,3(πchase1, πchase2, πchase3, l̇, µ) = STO4,3(πchase1, πchase2, πchase3, (π1, π2, ∗, ∗), µ)\nThe following table shows us STO4,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πchase1 < πchase2 πchase1 < πchase3 πchase2 < πchase1 πchase2 < πchase3 πchase3 < πchase2 πchase1 < πchase3 πchase1 < πchase3 πchase1 < πchase2 πchase2 < πchase3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πchase2 < πchase3 πchase3 < πchase1 πchase3 < πchase2 πchase3 < πchase1 πchase1 < πchase2 πchase2 < πchase1 πchase2 < πchase1 πchase3 < πchase2 πchase3 < πchase1\nBut, it is not possible to find a STO, since for every permutation the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nSTG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nSTG4,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nAnd finally, we weight over the slots:\nSTG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)STGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 4\n3\n1\n4\n1 4 {STG1,2(Πe, wΠe ,Πo, wL̇, µ) + STG1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ STG1,4(Πe, wΠe ,Πo, wL̇, µ) + STG2,1(Πe, wΠe ,Πo, wL̇, µ)+ + STG2,3(Πe, wΠe ,Πo, wL̇, µ) + STG2,4(Πe, wΠe ,Πo, wL̇, µ)+ + STG3,1(Πe, wΠe ,Πo, wL̇, µ) + STG3,2(Πe, wΠe ,Πo, wL̇, µ)+ + STG3,4(Πe, wΠe ,Πo, wL̇, µ) + STG4,1(Πe, wΠe ,Πo, wL̇, µ)+ + STG4,2(Πe, wΠe ,Πo, wL̇, µ) + STG4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {3× 1 + 9× 0} = 1 4\nSo, for every Πo we obtain the same result:\n∀Πo : STG(Πe, wΠe ,Πo, wL̇, µ, wS) = 1\n4\nTherefore, predator-prey has Rightmin = 1 4 (as a higher approximation) for this property."
    }, {
      "heading" : "C.5 Partial Grading",
      "text" : "Now we arrive to the partial grading (PG) property. As given in section 4.4.2, we want to know if there exists a partial ordering between the evaluated agents when interacting in the environment.\nTo simplify the notation, we use the next table to represent the PO: Ri(µ[l̇ i,j← π1, π2]) ≤ Rj(µ[l̇ i,j← π1, π2]), Ri(µ[l̇ i,j← π2, π3]) ≤ Rj(µ[l̇ i,j← π2, π3]) and Ri(µ[l̇ i,j← π1, π3]) ≤ Rj(µ[l̇ i,j← π1, π3]).\nSlot i Slot j π1 ≤ π2 π2 ≤ π3 π1 ≤ π3\nProposition 60. Generalmin for the partial grading (PG) property is equal to 1 2 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πx, πy, πz} with uniform weight for wΠe and Πo = {πs} (a πs agent always stays in the same cell22).\nπx behaves as shown in figure 24 when playing on each of the 4 slots. πy behaves as shown in figure 25 when playing on each of the 4 slots. πz behaves as shown in figure 26 when playing on each of the 4 slots. Following definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\n22Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πx, πy, πz,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πx, πy, πz, l̇, µ) =\n= PO1,2(πx, πy, πz, (∗, ∗, πs, πs), µ)\nThe following table shows us PO1,2 for all the permutations of πx, πy, πz.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nBut, it is not possible to find a PO for any permutation, since πx > πz, πy > πx and πz > πy. So:\nPG1,2(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 1 and 3:\nPG1,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,3(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG1,3(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,3(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇)PO1,3(πx, πy, πz, l̇, µ) =\n= PO1,3(πx, πy, πz, (∗, πs, ∗, πs), µ)\nThe following table shows us PO1,3 for all the permutations of πx, πy, πz.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nBut, it is not possible to find a PO for any permutation, since πx > πz, πy > πx and πz > πy. So:\nPG1,3(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nPG1,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 1 and 4:\nPG1,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,4(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG1,4(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,4(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇)PO1,4(πx, πy, πz, l̇, µ) =\n= PO1,4(πx, πy, πz, (∗, πs, πs, ∗), µ)\nThe following table shows us PO1,4 for all the permutations of πx, πy, πz.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nBut, it is not possible to find a PO for any permutation, since πx > πz, πy > πx and πz > πy. So:\nPG1,4(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nPG1,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 1:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,1(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,1(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)PO2,1(πx, πy, πz, l̇, µ) =\n= PO2,1(πx, πy, πz, (∗, ∗, πs, πs), µ)\nThe following table shows us PO2,1 for all the permutations of πx, πy, πz.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nBut, it is not possible to find a PO for any permutation, since πx > πz, πy > πx and πz > πy. So:\nPG2,1(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nPG2,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,3(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,3(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,3(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇)PO2,3(πx, πy, πz, l̇, µ) =\n= PO2,3(πx, πy, πz, (πs, ∗, ∗, πs), µ)\nThe following table shows us PO2,3 for all the permutations of πx, πy, πz.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG2,3(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nPG2,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 4:\nPG2,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,4(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,4(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,4(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇)PO2,4(πx, πy, πz, l̇, µ) =\n= PO2,4(πx, πy, πz, (πs, ∗, πs, ∗), µ)\nThe following table shows us PO2,4 for all the permutations of πx, πy, πz.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG2,4(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nPG2,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 1:\nPG3,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,1(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,1(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,1(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇)PO3,1(πx, πy, πz, l̇, µ) =\n= PO3,1(πx, πy, πz, (∗, πs, ∗, πs), µ)\nThe following table shows us PO3,1 for all the permutations of πx, πy, πz.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nBut, it is not possible to find a PO for any permutation, since πx > πz, πy > πx and πz > πy. So:\nPG3,1(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nPG3,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nPG3,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,2(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,2(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,2(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇)PO3,2(πx, πy, πz, l̇, µ) =\n= PO3,2(πx, πy, πz, (πs, ∗, ∗, πs), µ)\nThe following table shows us PO3,2 for all the permutations of πx, πy, πz.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG3,2(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nPG3,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 4:\nPG3,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,4(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,4(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,4(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇)PO3,4(πx, πy, πz, l̇, µ) =\n= PO3,4(πx, πy, πz, (πs, πs, ∗, ∗), µ) The following table shows us PO3,4 for all the permutations of πx, πy, πz.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG3,4(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nPG3,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 1:\nPG4,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,1(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,1(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,1(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇)PO4,1(πx, πy, πz, l̇, µ) =\n= PO4,1(πx, πy, πz, (∗, πs, πs, ∗), µ)\nThe following table shows us PO4,1 for all the permutations of πx, πy, πz.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nBut, it is not possible to find a PO for any permutation, since πx > πz, πy > πx and πz > πy. So:\nPG4,1(πx, πy, πz,Πo, wL̇, µ) = 0\nTherefore:\nPG4,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nPG4,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,2(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,2(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,2(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇)PO4,2(πx, πy, πz, l̇, µ) =\n= PO4,2(πx, πy, πz, (πs, ∗, πs, ∗), µ)\nThe following table shows us PO4,2 for all the permutations of πx, πy, πz.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG4,2(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nPG4,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 4 and 3:\nPG4,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,3(πx, πy, πz,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,3(πx, πy, πz,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,3(πx, πy, πz,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇)PO4,3(πx, πy, πz, l̇, µ) =\n= PO4,3(πx, πy, πz, (πs, πs, ∗, ∗), µ) The following table shows us PO4,3 for all the permutations of πx, πy, πz.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πx ≤ πy πx ≤ πz πy ≤ πx πy ≤ πz πz ≤ πy πx ≤ πz πx ≤ πz πx ≤ πy πy ≤ πz Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πy ≤ πz πz ≤ πx πz ≤ πy πz ≤ πx πx ≤ πy πy ≤ πx πy ≤ πx πz ≤ πy πz ≤ πx\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG4,3(πx, πy, πz,Πo, wL̇, µ) = 1\nTherefore:\nPG4,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 4\n3\n1\n4\n1 4 {PG1,2(Πe, wΠe ,Πo, wL̇, µ) + PG1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ PG1,4(Πe, wΠe ,Πo, wL̇, µ) + PG2,1(Πe, wΠe ,Πo, wL̇, µ)+ + PG2,3(Πe, wΠe ,Πo, wL̇, µ) + PG2,4(Πe, wΠe ,Πo, wL̇, µ)+ + PG3,1(Πe, wΠe ,Πo, wL̇, µ) + PG3,2(Πe, wΠe ,Πo, wL̇, µ)+ + PG3,4(Πe, wΠe ,Πo, wL̇, µ) + PG4,1(Πe, wΠe ,Πo, wL̇, µ)+ + PG4,2(Πe, wΠe ,Πo, wL̇, µ) + PG4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {6× 1 + 6× 0} = 1 2\nSince 12 is the lowest possible value that we can obtain for the partial grading property, therefore predatorprey has Generalmin = 1 2 for this property.\nProposition 61. Generalmax for the partial grading (PG) property is equal to 1 for the predator-prey environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πs1, πs2, πs3} with uniform weight for wΠe and Πo = {πx} (a πs agent always stays in the same cell23, and a πx agent acts stochastically with a probability of 1/ √ 2 to do not reach the upper left corner and a probability of 1− 1/ √ 2 to reach this corner).\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πs1, πs2, πs3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πs1, πs2, πs3, l̇, µ) =\n= PO1,2(πs1, πs2, πs3, (∗, ∗, πx, πx), µ) The following table shows us PO1,2 for all the permutations of πs1, πs2, πs3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 3 (πx) and 4 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1− 1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any πs) and 2 (any πs) to obtain the same expected average reward (0). So:\nPG1,2(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nPG1,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,3(πs1, πs2, πs3,Πo, wL̇, µ)\n23Note that every cell has an action which is blocked by a block or a boundary, therefore an agent performing this action will stay at its current cell.\nIn this case, we only need to calculate PG1,3(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,3(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇)PO1,3(πs1, πs2, πs3, l̇, µ) =\n= PO1,3(πs1, πs2, πs3, (∗, πx, ∗, πx), µ)\nThe following table shows us PO1,3 for all the permutations of πs1, πs2, πs3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 2 (πx) and 4 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1− 1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any πs) and 3 (any πs) to obtain the same expected average reward (0). So:\nPG1,3(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nPG1,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,4(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG1,4(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,4(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇)PO1,4(πs1, πs2, πs3, l̇, µ) =\n= PO1,4(πs1, πs2, πs3, (∗, πx, πx, ∗), µ)\nThe following table shows us PO1,4 for all the permutations of πs1, πs2, πs3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 2 (πx) and 3 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1− 1/2 = 1/2 to chase the prey, making for both agents in slots 1 (any πs) and 4 (any πs) to obtain the same expected average reward (0). So:\nPG1,4(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,1(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,1(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)PO2,1(πs1, πs2, πs3, l̇, µ) =\n= PO2,1(πs1, πs2, πs3, (∗, ∗, πx, πx), µ)\nThe following table shows us PO2,1 for all the permutations of πs1, πs2, πs3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 3 (πx) and 4 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1− 1/2 = 1/2 to chase the prey, making for both agents in slots 2 (any πs) and 1 (any πs) to obtain the same expected average reward (0). So:\nPG2,1(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 3:\nPG2,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,3(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,3(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,3(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇)PO2,3(πs1, πs2, πs3, l̇, µ) =\n= PO2,3(πs1, πs2, πs3, (πx, ∗, ∗, πx), µ)\nThe following table shows us PO2,3 for all the permutations of πs1, πs2, πs3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG2,3(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 4:\nPG2,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,4(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,4(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,4(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇)PO2,4(πs1, πs2, πs3, l̇, µ) =\n= PO2,4(πs1, πs2, πs3, (πx, ∗, πx, ∗), µ)\nThe following table shows us PO2,4 for all the permutations of πs1, πs2, πs3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG2,4(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 1:\nPG3,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,1(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,1(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,1(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇)PO3,1(πs1, πs2, πs3, l̇, µ) =\n= PO3,1(πs1, πs2, πs3, (∗, πx, ∗, πx), µ)\nThe following table shows us PO3,1 for all the permutations of πs1, πs2, πs3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 2 (πx) and 4 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1− 1/2 = 1/2 to chase the prey, making for both agents in slots 3 (any πs) and 1 (any πs) to obtain the same expected average reward (0). So:\nPG3,1(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG3,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 2:\nPG3,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,2(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,2(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,2(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇)PO3,2(πs1, πs2, πs3, l̇, µ) =\n= PO3,2(πs1, πs2, πs3, (πx, ∗, ∗, πx), µ)\nThe following table shows us PO3,2 for all the permutations of πs1, πs2, πs3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG3,2(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG3,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 4:\nPG3,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,4(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,4(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,4(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇)PO3,4(πs1, πs2, πs3, l̇, µ) =\n= PO3,4(πs1, πs2, πs3, (πx, πx, ∗, ∗), µ) The following table shows us PO3,4 for all the permutations of πs1, πs2, πs3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG3,4(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG3,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 1:\nPG4,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,1(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,1(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,1(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇)PO4,1(πs1, πs2, πs3, l̇, µ) =\n= PO4,1(πs1, πs2, πs3, (∗, πx, πx, ∗), µ)\nThe following table shows us PO4,1 for all the permutations of πs1, πs2, πs3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 2 (πx) and 3 (πx) have a probability of (1/ √ 2)× (1/ √ 2) = 1/2 to do not chase the prey (any πs) and the same probability 1− 1/2 = 1/2 to chase the prey, making for both agents in slots 4 (any πs) and 1 (any πs) to obtain the same expected average reward (0). So:\nPG4,1(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG4,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 2:\nPG4,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,2(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,2(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,2(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇)PO4,2(πs1, πs2, πs3, l̇, µ) =\n= PO4,2(πs1, πs2, πs3, (πx, ∗, πx, ∗), µ)\nThe following table shows us PO4,2 for all the permutations of πs1, πs2, πs3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG4,2(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG4,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 4 and 3:\nPG4,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,3(πs1, πs2, πs3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,3(πs1, πs2, πs3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,3(πs1, πs2, πs3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇)PO4,3(πs1, πs2, πs3, l̇, µ) =\n= PO4,3(πs1, πs2, πs3, (πx, πx, ∗, ∗), µ)\nThe following table shows us PO4,3 for all the permutations of πs1, πs2, πs3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πs1 ≤ πs2 πs1 ≤ πs3 πs2 ≤ πs1 πs2 ≤ πs3 πs3 ≤ πs2 πs1 ≤ πs3 πs1 ≤ πs3 πs1 ≤ πs2 πs2 ≤ πs3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πs2 ≤ πs3 πs3 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1 πs1 ≤ πs2 πs2 ≤ πs1 πs2 ≤ πs1 πs3 ≤ πs2 πs3 ≤ πs1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So:\nPG4,3(πs1, πs2, πs3,Πo, wL̇, µ) = 1\nTherefore:\nPG4,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 4\n3\n1\n4\n1 4 {PG1,2(Πe, wΠe ,Πo, wL̇, µ) + PG1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ PG1,4(Πe, wΠe ,Πo, wL̇, µ) + PG2,1(Πe, wΠe ,Πo, wL̇, µ)+ + PG2,3(Πe, wΠe ,Πo, wL̇, µ) + PG2,4(Πe, wΠe ,Πo, wL̇, µ)+ + PG3,1(Πe, wΠe ,Πo, wL̇, µ) + PG3,2(Πe, wΠe ,Πo, wL̇, µ)+ + PG3,4(Πe, wΠe ,Πo, wL̇, µ) + PG4,1(Πe, wΠe ,Πo, wL̇, µ)+ + PG4,2(Πe, wΠe ,Πo, wL̇, µ) + PG4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {12× 1} = 1\nSince 1 is the highest possible value for the partial grading property, therefore predator-prey hasGeneralmax = 1 for this property.\nApproximation 4. Leftmax for the partial grading (PG) property is equal to 3 4 (as a lower approximation) for the predator-prey environment.\nProof. To find Leftmax (equation 43), we need to find a pair ⟨Πe, wΠe⟩ which maximises the property as much as possible while Πo minimises it. Using Πe = {πchase1, πchase2, πchase3} with uniform weight for wΠe (a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find a lower approximation of this situation no matter which Πo we use.\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πchase1, πchase2, πchase3, l̇, µ)\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,2 (Πo) to calculate PO1,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, ∗, π1, π2) from L̇N(µ)−1,2 (Πo):\nPO1,2(πchase1, πchase2, πchase3, l̇, µ) = PO1,2(πchase1, πchase2, πchase3, (∗, ∗, π1, π2), µ)\nThe following table shows us PO1,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 2 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of PO1,2, so no matter which agents are in Πo we obtain:\nPG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nPG1,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇)PO1,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,3 (Πo) to calculate PO1,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, ∗, π2) from L̇N(µ)−1,3 (Πo):\nPO1,3(πchase1, πchase2, πchase3, l̇, µ) = PO1,3(πchase1, πchase2, πchase3, (∗, π1, ∗, π2), µ)\nThe following table shows us PO1,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nAgain, it is possible to find a PO for every permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 3 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of PO1,3, so no matter which agents are in Πo we obtain:\nPG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nPG1,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇)PO1,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,4 (Πo) to calculate PO1,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, ∗) from L̇N(µ)−1,4 (Πo):\nPO1,4(πchase1, πchase2, πchase3, l̇, µ) = PO1,4(πchase1, πchase2, πchase3, (∗, π1, π2, ∗), µ)\nThe following table shows us PO1,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nAgain, it is possible to find a PO for every permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 4 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of PO1,4, so no matter which agents are in Πo we obtain:\nPG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)PO2,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,1 (Πo) to calculate PO2,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, ∗, π1, π2) from L̇N(µ)−2,1 (Πo):\nPO2,1(πchase1, πchase2, πchase3, l̇, µ) = PO2,1(πchase1, πchase2, πchase3, (∗, ∗, π1, π2), µ)\nThe following table shows us PO2,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is not possible to find a PO for any permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 2 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of PO2,1, so no matter which agents are in Πo we obtain:\nPG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nPG2,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇)PO2,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,3 (Πo) to calculate PO2,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, ∗, π2) from L̇N(µ)−2,3 (Πo):\nPO2,3(πchase1, πchase2, πchase3, l̇, µ) = PO2,3(πchase1, πchase2, πchase3, (π1, ∗, ∗, π2), µ)\nThe following table shows us PO2,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 4:\nPG2,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇)PO2,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,4 (Πo) to calculate PO2,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, ∗) from L̇N(µ)−2,4 (Πo):\nPO2,4(πchase1, πchase2, πchase3, l̇, µ) = PO2,4(πchase1, πchase2, πchase3, (π1, ∗, π2, ∗), µ)\nThe following table shows us PO2,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 1:\nPG3,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇)PO3,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,1 (Πo) to calculate PO3,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, ∗, π2) from L̇N(µ)−3,1 (Πo):\nPO3,1(πchase1, πchase2, πchase3, l̇, µ) = PO3,1(πchase1, πchase2, πchase3, (∗, π1, ∗, π2), µ)\nThe following table shows us PO3,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nAgain, it is not possible to find a PO for any permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 3 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of PO3,1, so no matter which agents are in Πo we obtain:\nPG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nPG3,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 0\nFor slots 3 and 2:\nPG3,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇)PO3,2(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,2 (Πo) to calculate PO3,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, ∗, π2) from L̇N(µ)−3,2 (Πo):\nPO3,2(πchase1, πchase2, πchase3, l̇, µ) = PO3,2(πchase1, πchase2, πchase3, (π1, ∗, ∗, π2), µ)\nThe following table shows us PO3,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG3,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 4:\nPG3,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇)PO3,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,4 (Πo) to calculate PO3,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, ∗) from L̇N(µ)−3,4 (Πo):\nPO3,4(πchase1, πchase2, πchase3, l̇, µ) = PO3,4(πchase1, πchase2, πchase3, (π1, π2, ∗, ∗), µ)\nThe following table shows us PO3,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG3,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 1:\nPG4,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇)PO4,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,1 (Πo) to calculate PO4,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, ∗) from L̇N(µ)−4,1 (Πo):\nPO4,1(πchase1, πchase2, πchase3, l̇, µ) = PO4,1(πchase1, πchase2, πchase3, (∗, π1, π2, ∗), µ)\nThe following table shows us PO4,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nAgain, it is not possible to find a PO for any permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 4 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of PO4,1, so no matter which agents are in Πo we obtain:\nPG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nPG4,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 0\nFor slots 4 and 2:\nPG4,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇)PO4,2(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,2 (Πo) to calculate PO4,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, ∗) from L̇N(µ)−4,2 (Πo):\nPO4,2(πchase1, πchase2, πchase3, l̇, µ) = PO4,2(πchase1, πchase2, πchase3, (π1, ∗, π2, ∗), µ)\nThe following table shows us PO4,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG4,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 4 and 3:\nPG4,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇)PO4,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,3 (Πo) to calculate PO4,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, ∗) from L̇N(µ)−4,3 (Πo):\nPO4,3(πchase1, πchase2, πchase3, l̇, µ) = PO4,3(πchase1, πchase2, πchase3, (π1, π2, ∗, ∗), µ)\nThe following table shows us PO4,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG4,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 4\n3\n1\n4\n1 4 {PG1,2(Πe, wΠe ,Πo, wL̇, µ) + PG1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ PG1,4(Πe, wΠe ,Πo, wL̇, µ) + PG2,1(Πe, wΠe ,Πo, wL̇, µ)+ + PG2,3(Πe, wΠe ,Πo, wL̇, µ) + PG2,4(Πe, wΠe ,Πo, wL̇, µ)+ + PG3,1(Πe, wΠe ,Πo, wL̇, µ) + PG3,2(Πe, wΠe ,Πo, wL̇, µ)+ + PG3,4(Πe, wΠe ,Πo, wL̇, µ) + PG4,1(Πe, wΠe ,Πo, wL̇, µ)+ + PG4,2(Πe, wΠe ,Πo, wL̇, µ) + PG4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {9× 1 + 3× 0} = 3 4\nSo, for every Πo we obtain the same result:\n∀Πo : PG(Πe, wΠe ,Πo, wL̇, µ, wS) = 3\n4\nTherefore, predator-prey has Leftmax = 3 4 (as a lower approximation) for this property.\nApproximation 5. Rightmin for the partial grading (PG) property is equal to 3 4 (as a higher approximation) for the predator-prey environment.\nProof. To find Rightmin (equation 44), we need to find a pair ⟨Πe, wΠe⟩ which minimises the property as much as possible while Πo maximises it. Using Πe = {πchase1, πchase2, πchase3} with uniform weight for wΠe (a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator) we find a higher approximation of this situation no matter which Πo we use.\nFollowing definition 30, we obtain the PG value for this ⟨Πe, wΠe ,Πo⟩ (where Πo is instantiated with any permitted value). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 28 (for PG), we can calculate its PG value for each pair of slots. We start with slots 1 and 2:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nNote that we avoided to calculate all the permutations of π1, π2, π3 for PGi,j(π1, π2, π3,Πo, wL̇, µ) since they provide the same result, by calculating only one permutation and multiplying the result by the number of permutations 6.\nIn this case, we only need to calculate PG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇)PO1,2(πchase1, πchase2, πchase3, l̇, µ)\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,2 (Πo) to calculate PO1,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, ∗, π1, π2) from L̇N(µ)−1,2 (Πo):\nPO1,2(πchase1, πchase2, πchase3, l̇, µ) = PO1,2(πchase1, πchase2, πchase3, (∗, ∗, π1, π2), µ)\nThe following table shows us PO1,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 2 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of PO1,2, so no matter which agents are in Πo we obtain:\nPG1,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 3:\nPG1,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇)PO1,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,3 (Πo) to calculate PO1,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, ∗, π2) from L̇N(µ)−1,3 (Πo):\nPO1,3(πchase1, πchase2, πchase3, l̇, µ) = PO1,3(πchase1, πchase2, πchase3, (∗, π1, ∗, π2), µ)\nThe following table shows us PO1,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nAgain, it is possible to find a PO for every permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 3 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of PO1,3, so no matter which agents are in Πo we obtain:\nPG1,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 1 and 4:\nPG1,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG1,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇)PO1,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −1,4 (Πo) to calculate PO1,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, ∗) from L̇N(µ)−1,4 (Πo):\nPO1,4(πchase1, πchase2, πchase3, l̇, µ) = PO1,4(πchase1, πchase2, πchase3, (∗, π1, π2, ∗), µ) The following table shows us PO1,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nAgain, it is possible to find a PO for every permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 1 and 4 will obtain an expected average reward of −1 and 1 respectively. Note that the choice of Πo does not affect the result of PO1,4, so no matter which agents are in Πo we obtain:\nPG1,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG1,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 1:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇)PO2,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,1 (Πo) to calculate PO2,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, ∗, π1, π2) from L̇N(µ)−2,1 (Πo):\nPO2,1(πchase1, πchase2, πchase3, l̇, µ) = PO2,1(πchase1, πchase2, πchase3, (∗, ∗, π1, π2), µ) The following table shows us PO2,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 2 Slot 1 Slot 2 Slot 1 Slot 2 Slot 1 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is not possible to find a PO for any permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 2 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of PO2,1, so no matter which agents are in Πo we obtain:\nPG2,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nPG2,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 2 and 3:\nPG2,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇)PO2,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,3 (Πo) to calculate PO2,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, ∗, π2) from L̇N(µ)−2,3 (Πo):\nPO2,3(πchase1, πchase2, πchase3, l̇, µ) = PO2,3(πchase1, πchase2, πchase3, (π1, ∗, ∗, π2), µ) The following table shows us PO2,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG2,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 2 and 4:\nPG2,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG2,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇)PO2,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −2,4 (Πo) to calculate PO2,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, ∗) from L̇N(µ)−2,4 (Πo):\nPO2,4(πchase1, πchase2, πchase3, l̇, µ) = PO2,4(πchase1, πchase2, πchase3, (π1, ∗, π2, ∗), µ)\nThe following table shows us PO2,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 2 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG2,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG2,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 1:\nPG3,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇)PO3,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,1 (Πo) to calculate PO3,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, ∗, π2) from L̇N(µ)−3,1 (Πo):\nPO3,1(πchase1, πchase2, πchase3, l̇, µ) = PO3,1(πchase1, πchase2, πchase3, (∗, π1, ∗, π2), µ)\nThe following table shows us PO3,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 3 Slot 1 Slot 3 Slot 1 Slot 3 Slot 1 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nAgain, it is not possible to find a PO for any permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 3 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of PO3,1, so no matter which agents are in Πo we obtain:\nPG3,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nPG3,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 3 and 2:\nPG3,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇)PO3,2(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,2 (Πo) to calculate PO3,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, ∗, π2) from L̇N(µ)−3,2 (Πo):\nPO3,2(πchase1, πchase2, πchase3, l̇, µ) = PO3,2(πchase1, πchase2, πchase3, (π1, ∗, ∗, π2), µ)\nThe following table shows us PO3,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 3 Slot 2 Slot 3 Slot 2 Slot 3 Slot 2 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG3,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG3,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 3 and 4:\nPG3,4(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG3,4(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇)PO3,4(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −3,4 (Πo) to calculate PO3,4(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, ∗) from L̇N(µ)−3,4 (Πo):\nPO3,4(πchase1, πchase2, πchase3, l̇, µ) = PO3,4(πchase1, πchase2, πchase3, (π1, π2, ∗, ∗), µ)\nThe following table shows us PO3,4 for all the permutations of πchase1, πchase2, πchase3.\nSlot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 3 and 4 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG3,4(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG3,4(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nFor slots 4 and 1:\nPG4,1(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,1(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇)PO4,1(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,1 (Πo) to calculate PO4,1(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (∗, π1, π2, ∗) from L̇N(µ)−4,1 (Πo):\nPO4,1(πchase1, πchase2, πchase3, l̇, µ) = PO4,1(πchase1, πchase2, πchase3, (∗, π1, π2, ∗), µ)\nThe following table shows us PO4,1 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 4 Slot 1 Slot 4 Slot 1 Slot 4 Slot 1 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nAgain, it is not possible to find a PO for any permutation, since we always have πchasei ≤ πchasej , where a πchase agent always tries to be chased when playing as the prey and tries to chase when playing as a predator, so the agents in slots 4 and 1 will obtain an expected average reward of 1 and −1 respectively. Note that the choice of Πo does not affect the result of PO4,1, so no matter which agents are in Πo we obtain:\nPG4,1(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 0\nTherefore:\nPG4,1(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 0 = 0\nFor slots 4 and 2:\nPG4,2(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,2(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇)PO4,2(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,2 (Πo) to calculate PO4,2(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, ∗, π2, ∗) from L̇N(µ)−4,2 (Πo):\nPO4,2(πchase1, πchase2, πchase3, l̇, µ) = PO4,2(πchase1, πchase2, πchase3, (π1, ∗, π2, ∗), µ)\nThe following table shows us PO4,2 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 4 Slot 2 Slot 4 Slot 2 Slot 4 Slot 2 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 2 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG4,2(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG4,2(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd for slots 4 and 3:\nPG4,3(Πe, wΠe ,Πo, wL̇, µ) = ηΠ3 ∑\nπ1,π2,π3∈Πe|π1 ̸=π2 ̸=π3\nwΠe(π1)wΠe(π2)wΠe(π3)PG4,3(π1, π2, π3,Πo, wL̇, µ) =\n= 6 9\n2\n1\n3\n1\n3\n1 3 PG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ)\nIn this case, we only need to calculate PG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ). We follow definition 27 (for PG) to calculate this value:\nPG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇)PO4,3(πchase1, πchase2, πchase3, l̇, µ)\nAgain, we do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from L̇ N(µ) −4,3 (Πo) to calculate PO4,3(πchase1, πchase2, πchase3, l̇, µ). We calculate this value for a figurative line-up pattern l̇ = (π1, π2, ∗, ∗) from L̇N(µ)−4,3 (Πo):\nPO4,3(πchase1, πchase2, πchase3, l̇, µ) = PO4,3(πchase1, πchase2, πchase3, (π1, π2, ∗, ∗), µ)\nThe following table shows us PO4,3 for all the permutations of πchase1, πchase2, πchase3.\nSlot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πchase1 ≤ πchase2 πchase1 ≤ πchase3 πchase2 ≤ πchase1 πchase2 ≤ πchase3 πchase3 ≤ πchase2 πchase1 ≤ πchase3 πchase1 ≤ πchase3 πchase1 ≤ πchase2 πchase2 ≤ πchase3 Slot 4 Slot 3 Slot 4 Slot 3 Slot 4 Slot 3 πchase2 ≤ πchase3 πchase3 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1 πchase1 ≤ πchase2 πchase2 ≤ πchase1 πchase2 ≤ πchase1 πchase3 ≤ πchase2 πchase3 ≤ πchase1\nIt is possible to find a PO for every permutation, since the agents in slots 4 and 3 share rewards (and expected average rewards as well) due they are in the same team. So no matter which agents are in Πo we obtain:\nPG4,3(πchase1, πchase2, πchase3,Πo, wL̇, µ) = 1\nTherefore:\nPG4,3(Πe, wΠe ,Πo, wL̇, µ) = 6 9\n2\n1\n3\n1\n3\n1 3 1 = 1\nAnd finally, we weight over the slots:\nPG(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)PGi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 4\n3\n1\n4\n1 4 {PG1,2(Πe, wΠe ,Πo, wL̇, µ) + PG1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ PG1,4(Πe, wΠe ,Πo, wL̇, µ) + PG2,1(Πe, wΠe ,Πo, wL̇, µ)+ + PG2,3(Πe, wΠe ,Πo, wL̇, µ) + PG2,4(Πe, wΠe ,Πo, wL̇, µ)+ + PG3,1(Πe, wΠe ,Πo, wL̇, µ) + PG3,2(Πe, wΠe ,Πo, wL̇, µ)+ + PG3,4(Πe, wΠe ,Πo, wL̇, µ) + PG4,1(Πe, wΠe ,Πo, wL̇, µ)+ + PG4,2(Πe, wΠe ,Πo, wL̇, µ) + PG4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {9× 1 + 3× 0} = 3 4\nSo, for every Πo we obtain the same result:\n∀Πo : PG(Πe, wΠe ,Πo, wL̇, µ, wS) = 3\n4\nTherefore, predator-prey has Rightmin = 3 4 (as a higher approximation) for this property."
    }, {
      "heading" : "C.6 Slot Reward Dependency",
      "text" : "Next we see the slot reward dependency (SRD) property. As given in section 4.3.2, we want to know how much competitive or cooperative the environment is.\nProposition 62. General range for the slot reward dependency (SRD) property is equal to [0, 0] for the predator-prey environment.\nProof. Following definition 20, we obtain the SRD value for any ⟨Πe, wΠe ,Πo⟩ (where Πe, wΠe and Πo are instantiated with any permitted values). Since the environment is not symmetric, we need to calculate this property for every pair of slots. Following definition 19, we can calculate its SRD value for each pair of slots. We start with slots 1 and 2:\nSRD1,2(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD1,2(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD1,2(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD1,2(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−1 (Πo) [wL̇(l̇)](R1(µ[l̇\n1← π1]), R2(µ[l̇ 1← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −1 (Πo) to calculate corr(R1(µ[l̇ 1← π1]), R2(µ[l̇ 1← π1])). We calculate this value for a figurative line-up pattern l̇ = (∗, π2, π3, π4) from L̇N(µ)−1 (Πo):\ncorr(R1(µ[l̇ 1← π1]), R2(µ[l̇ 1← π1])) = corr(R1(µ[π1, π2, π3, π4]), R2(µ[π1, π2, π3, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 1 (any π1) obtains a reward r the agent in slot 2 (any π2) obtains −r as reward, and this relation is propagated to expected average\nrewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 2 always obtain opposite expected average reward, then the correlation function will always obtain the same value24 of −1. So:\nSRD1,2(π1,Πo, wL̇, µ) = −1\nTherefore:\nSRD1,2(Πe, wΠe ,Πo, wL̇, µ) = −1\nFor slots 1 and 3: SRD1,3(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD1,3(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD1,3(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD1,3(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−1 (Πo) [wL̇(l̇)](R1(µ[l̇\n1← π1]), R3(µ[l̇ 1← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −1 (Πo) to calculate corr(R1(µ[l̇ 1← π1]), R3(µ[l̇ 1← π1])). We calculate this value for a figurative line-up pattern l̇ = (∗, π2, π3, π4) from L̇N(µ)−1 (Πo):\ncorr(R1(µ[l̇ 1← π1]), R3(µ[l̇ 1← π1])) = corr(R1(µ[π1, π2, π3, π4]), R3(µ[π1, π2, π3, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 1 (any π1) obtains a reward r the agent in slot 3 (any π3) obtains −r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 3 always obtain opposite expected average reward, then the correlation function will always obtain the same value of −1. So:\nSRD1,3(π1,Πo, wL̇, µ) = −1\nTherefore:\nSRD1,3(Πe, wΠe ,Πo, wL̇, µ) = −1\nFor slots 1 and 4: SRD1,4(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD1,4(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD1,4(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD1,4(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−1 (Πo) [wL̇(l̇)](R1(µ[l̇\n1← π1]), R4(µ[l̇ 1← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −1 (Πo) to calculate corr(R1(µ[l̇ 1← π1]), R4(µ[l̇ 1← π1])). We calculate this value for a figurative line-up pattern l̇ = (∗, π2, π3, π4) from L̇N(µ)−1 (Πo):\ncorr(R1(µ[l̇ 1← π1]), R4(µ[l̇ 1← π1])) = corr(R1(µ[π1, π2, π3, π4]), R4(µ[π1, π2, π3, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 1 (any π1) obtains a reward r the agent in slot 4 (any π4) obtains −r as reward, and this relation is propagated to expected average\n24Provided there is at least one game which is not a tie.\nrewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 1 and 4 always obtain opposite expected average reward, then the correlation function will always obtain the same value of −1. So:\nSRD1,4(π1,Πo, wL̇, µ) = −1\nTherefore:\nSRD1,4(Πe, wΠe ,Πo, wL̇, µ) = −1\nFor slots 2 and 1: SRD2,1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD2,1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD2,1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD2,1(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−2 (Πo) [wL̇(l̇)](R2(µ[l̇\n2← π1]), R1(µ[l̇ 2← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −2 (Πo) to calculate corr(R2(µ[l̇ 2← π1]), R1(µ[l̇ 2← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, ∗, π3, π4) from L̇N(µ)−2 (Πo):\ncorr(R2(µ[l̇ 2← π1]), R1(µ[l̇ 2← π1])) = corr(R2(µ[π2, π1, π3, π4]), R1(µ[π2, π1, π3, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 2 (any π1) obtains a reward r the agent in slot 1 (any π2) obtains −r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 2 and 1 always obtain opposite expected average reward, then the correlation function will always obtain the same value of −1. So:\nSRD2,1(π1,Πo, wL̇, µ) = −1\nTherefore:\nSRD2,1(Πe, wΠe ,Πo, wL̇, µ) = −1\nFor slots 2 and 3: SRD2,3(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD2,3(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD2,3(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD2,3(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−2 (Πo) [wL̇(l̇)](R2(µ[l̇\n2← π1]), R3(µ[l̇ 2← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −2 (Πo) to calculate corr(R2(µ[l̇ 2← π1]), R3(µ[l̇ 2← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, ∗, π3, π4) from L̇N(µ)−2 (Πo):\ncorr(R2(µ[l̇ 2← π1]), R3(µ[l̇ 2← π1])) = corr(R2(µ[π2, π1, π3, π4]), R3(µ[π2, π1, π3, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 2 (any π1) obtains a reward r the agent in slot 3 (any π3) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between\nthe expected average rewards, and the agents in slots 2 and 3 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD2,3(π1,Πo, wL̇, µ) = 1\nTherefore:\nSRD2,3(Πe, wΠe ,Πo, wL̇, µ) = 1\nFor slots 2 and 4: SRD2,4(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD2,4(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD2,4(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD2,4(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−2 (Πo) [wL̇(l̇)](R2(µ[l̇\n2← π1]), R4(µ[l̇ 2← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −2 (Πo) to calculate corr(R2(µ[l̇ 2← π1]), R4(µ[l̇ 2← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, ∗, π3, π4) from L̇N(µ)−2 (Πo):\ncorr(R2(µ[l̇ 2← π1]), R4(µ[l̇ 2← π1])) = corr(R2(µ[π2, π1, π3, π4]), R4(µ[π2, π1, π3, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 2 (any π1) obtains a reward r the agent in slot 4 (any π4) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 2 and 4 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD2,4(π1,Πo, wL̇, µ) = 1\nTherefore:\nSRD2,4(Πe, wΠe ,Πo, wL̇, µ) = 1\nFor slots 3 and 1: SRD3,1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD3,1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD3,1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD3,1(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−3 (Πo) [wL̇(l̇)](R3(µ[l̇\n3← π1]), R1(µ[l̇ 3← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −3 (Πo) to calculate corr(R3(µ[l̇ 3← π1]), R1(µ[l̇ 3← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, π3, ∗, π4) from L̇N(µ)−3 (Πo):\ncorr(R3(µ[l̇ 3← π1]), R1(µ[l̇ 3← π1])) = corr(R3(µ[π2, π3, π1, π4]), R1(µ[π2, π3, π1, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 3 (any π1) obtains a reward r the agent in slot 1 (any π2) obtains −r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in\nslots 3 and 1 always obtain opposite expected average reward, then the correlation function will always obtain the same value of −1. So:\nSRD3,1(π1,Πo, wL̇, µ) = −1\nTherefore:\nSRD3,1(Πe, wΠe ,Πo, wL̇, µ) = −1\nFor slots 3 and 2: SRD3,2(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD3,2(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD3,2(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD3,2(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−3 (Πo) [wL̇(l̇)](R3(µ[l̇\n3← π1]), R2(µ[l̇ 3← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −3 (Πo) to calculate corr(R3(µ[l̇ 3← π1]), R2(µ[l̇ 3← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, π3, ∗, π4) from L̇N(µ)−3 (Πo):\ncorr(R3(µ[l̇ 3← π1]), R2(µ[l̇ 3← π1])) = corr(R3(µ[π2, π3, π1, π4]), R2(µ[π2, π3, π1, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 3 (any π1) obtains a reward r the agent in slot 2 (any π3) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 3 and 2 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD3,2(π1,Πo, wL̇, µ) = 1\nTherefore:\nSRD3,2(Πe, wΠe ,Πo, wL̇, µ) = 1\nFor slots 3 and 4: SRD3,4(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD3,4(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD3,4(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD3,4(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−3 (Πo) [wL̇(l̇)](R3(µ[l̇\n3← π1]), R4(µ[l̇ 3← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −3 (Πo) to calculate corr(R3(µ[l̇ 3← π1]), R4(µ[l̇ 3← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, π3, ∗, π4) from L̇N(µ)−3 (Πo):\ncorr(R3(µ[l̇ 3← π1]), R4(µ[l̇ 3← π1])) = corr(R3(µ[π2, π3, π1, π4]), R4(µ[π2, π3, π1, π4]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 3 (any π1) obtains a reward r the agent in slot 4 (any π4) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between\nthe expected average rewards, and the agents in slots 3 and 4 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD3,4(π1,Πo, wL̇, µ) = 1\nTherefore:\nSRD3,4(Πe, wΠe ,Πo, wL̇, µ) = 1\nFor slots 4 and 1: SRD4,1(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD4,1(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD4,1(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD4,1(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−4 (Πo) [wL̇(l̇)](R4(µ[l̇\n4← π1]), R1(µ[l̇ 4← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −4 (Πo) to calculate corr(R4(µ[l̇ 4← π1]), R1(µ[l̇ 4← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, π3, π4, ∗) from L̇N(µ)−4 (Πo):\ncorr(R4(µ[l̇ 4← π1]), R1(µ[l̇ 4← π1])) = corr(R4(µ[π2, π3, π4, π1]), R1(µ[π2, π3, π4, π1]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 4 (any π1) obtains a reward r the agent in slot 1 (any π2) obtains −r as reward, and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 4 and 1 always obtain opposite expected average reward, then the correlation function will always obtain the same value of −1. So:\nSRD4,1(π1,Πo, wL̇, µ) = −1\nTherefore:\nSRD4,1(Πe, wΠe ,Πo, wL̇, µ) = −1\nFor slots 4 and 2: SRD4,2(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD4,2(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD4,2(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD4,2(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−4 (Πo) [wL̇(l̇)](R4(µ[l̇\n4← π1]), R2(µ[l̇ 4← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −4 (Πo) to calculate corr(R4(µ[l̇ 4← π1]), R2(µ[l̇ 4← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, π3, π4, ∗) from L̇N(µ)−4 (Πo):\ncorr(R4(µ[l̇ 4← π1]), R2(µ[l̇ 4← π1])) = corr(R4(µ[π2, π3, π4, π1]), R2(µ[π2, π3, π4, π1]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 4 (any π1) obtains a reward r the agent in slot 2 (any π3) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between\nthe expected average rewards, and the agents in slots 4 and 2 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD4,2(π1,Πo, wL̇, µ) = 1\nTherefore:\nSRD4,2(Πe, wΠe ,Πo, wL̇, µ) = 1\nAnd for slots 4 and 3: SRD4,3(Πe, wΠe ,Πo, wL̇, µ) = ∑ π∈Πe wΠe(π)SRD4,3(π,Πo, wL̇, µ)\nWe do not know which Πe we have, but we know that we will need to evaluate SRD4,3(π,Πo, wL̇, µ) for all evaluated agent π ∈ Πe. We follow definition 18 to calculate this value for a figurative evaluated agent π1 from Πe:\nSRD4,3(π1,Πo, wL̇, µ) = corrl̇∈L̇N(µ)−4 (Πo) [wL̇(l̇)](R4(µ[l̇\n4← π1]), R3(µ[l̇ 4← π1]))\nWe do not know which Πo we have, but we know that we will need to obtain a line-up pattern l̇ from\nL̇ N(µ) −4 (Πo) to calculate corr(R4(µ[l̇ 4← π1]), R3(µ[l̇ 4← π1])). We calculate this value for a figurative line-up pattern l̇ = (π2, π3, π4, ∗) from L̇N(µ)−4 (Πo):\ncorr(R4(µ[l̇ 4← π1]), R3(µ[l̇ 4← π1])) = corr(R4(µ[π2, π3, π4, π1]), R3(µ[π2, π3, π4, π1]))\nFrom the predator-prey’s payoff matrix (figure 10), we can see that when the agent in slot 4 (any π1) obtains a reward r the agent in slot 3 (any π4) obtains the same value r as reward (since they are in the same team), and this relation is propagated to expected average rewards as well. Since we use a correlation function between the expected average rewards, and the agents in slots 4 and 3 always obtain the same expected average reward, then the correlation function will always obtain the same value of 1. So:\nSRD4,3(π1,Πo, wL̇, µ) = 1\nTherefore:\nSRD4,3(Πe, wΠe ,Πo, wL̇, µ) = 1\nAnd finally, we weight over the slots:\nSRD(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS21 N(µ)∑ i=1 wS(i, µ)×\n× i−1∑ j=1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ) + N(µ)∑ j=i+1 wS(j, µ)SRDi,j(Πe, wΠe ,Πo, wL̇, µ)  = = 8\n3\n1\n4\n1 4 {SRD1,2(Πe, wΠe ,Πo, wL̇, µ) + SRD1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+ SRD1,4(Πe, wΠe ,Πo, wL̇, µ) + SRD2,1(Πe, wΠe ,Πo, wL̇, µ)+ + SRD2,3(Πe, wΠe ,Πo, wL̇, µ) + SRD2,4(Πe, wΠe ,Πo, wL̇, µ)+ + SRD3,1(Πe, wΠe ,Πo, wL̇, µ) + SRD3,2(Πe, wΠe ,Πo, wL̇, µ)+ + SRD3,4(Πe, wΠe ,Πo, wL̇, µ) + SRD4,1(Πe, wΠe ,Πo, wL̇, µ)+ + SRD4,2(Πe, wΠe ,Πo, wL̇, µ) + SRD4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 4\n3\n1\n4\n1 4 {6× (−1) + 6× 1} = 0\nSo, for every trio ⟨Πe, wΠe ,Πo⟩ we obtain the same result:\n∀Πe, wΠe ,Πo : SRD(Πe, wΠe ,Πo, wL̇, µ, wS) = 0 Therefore, predator-prey has General = [0, 0] for this property."
    }, {
      "heading" : "C.7 Competitive Anticipation",
      "text" : "Then, we follow with the competitive anticipation (AComp) property. As given in section 4.5.1, we want to know how much benefit the evaluated agents obtain when they anticipate competing agents.\nProposition 63. Generalmin for the competitive anticipation (AComp) property is equal to −1 for the predator-prey environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πwin/win′} with wΠe(πwin/win′) = 1 and Πo = {πwin} (a πwin agent always tries to not be chased when playing as the prey and tries to chase when playing as the predator, and a πwin/win′ agent always tries to not be chased when playing as the prey and tries to chase when playing as the predator but from the fifth iteration stops chasing the prey).\nFollowing definition 33, we obtain the AComp value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots in different teams. Following definition 32, we could calculate its AComp value for each pair of slots but, since Πe has only one agent, its weight is equal to 1 and Πo also has only one agent, it is equivalent to use directly definition 31. We start with slots 1 and 2: AComp1,2(πwin/win′ , πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,2← πwin/win′ , πwin])−R1(µ[l̇ 1,2← πwin/win′ , πr]) ) =\n= 1\n2\n( R1(µ[πwin/win′ , πwin, πwin, πwin])−R1(µ[πwin/win′ , πr, πwin, πwin]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(πwin/win′ , πwin, πwin, πwin) the agent in slot 1 (πwin/win′) will obtain an expected average reward of −1, while in line-up (πwin/win′ , πr, πwin, πwin) the agent in slot 1 (πwin/win′), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\nAComp1,2(πwin/win′ , πwin,Πo, wL̇, µ) = 1\n2 ((−1)− 1) = −1\nFor slots 1 and 3:\nAComp1,3(πwin/win′ , πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,3← πwin/win′ , πwin])−R1(µ[l̇ 1,3← πwin/win′ , πr]) ) =\n= 1\n2\n( R1(µ[πwin/win′ , πwin, πwin, πwin])−R1(µ[πwin/win′ , πwin, πr, πwin]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(πwin/win′ , πwin, πwin, πwin) the agent in slot 1 (πwin/win′) will obtain an expected average reward of −1, while in line-up (πwin/win′ , πwin, πr, πwin) the agent in slot 1 (πwin/win′), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\nAComp1,3(πwin/win′ , πwin,Πo, wL̇, µ) = 1\n2 ((−1)− 1) = −1\nFor slots 1 and 4:\nAComp1,4(πwin/win′ , πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,4← πwin/win′ , πwin])−R1(µ[l̇ 1,4← πwin/win′ , πr]) ) =\n= 1\n2\n( R1(µ[πwin/win′ , πwin, πwin, πwin])−R1(µ[πwin/win′ , πwin, πwin, πr]) )\nWe know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up (πwin/win′ , πwin, πwin, πwin) the agent in slot 1 (πwin/win′) will obtain an expected average reward of −1, while in line-up (πwin/win′ , πwin, πwin, πr) the agent in slot 1 (πwin/win′), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\nAComp1,4(πwin/win′ , πwin,Πo, wL̇, µ) = 1\n2 ((−1)− 1) = −1\nFor slots 2 and 1:\nAComp2,1(πwin/win′ , πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇) 1\n2\n( R2(µ[l̇ 2,1← πwin/win′ , πwin])−R2(µ[l̇ 2,1← πwin/win′ , πr]) ) =\n= 1\n2\n( R2(µ[πwin, πwin/win′ , πwin, πwin])−R2(µ[πr, πwin/win′ , πwin, πwin]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(πwin, πwin/win′ , πwin, πwin) the agent in slot 2 (πwin/win′), where they prey will not be chased due to the misscoordination of πwin/win′ in the last iterations, will obtain an expected average reward of −1, while in line-up (πr, πwin/win′ , πwin, πwin) the agent in slot 2 (πwin/win′), where the prey will almost always be chased by the predators, will almost obtain an expected average reward of 1. So:\nAComp2,1(πwin/win′ , πwin,Πo, wL̇, µ) = 1\n2 ((−1)− 1) = −1\nFor slots 3 and 1:\nAComp3,1(πwin/win′ , πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇) 1\n2\n( R3(µ[l̇ 3,1← πwin/win′ , πwin])−R3(µ[l̇ 3,1← πwin/win′ , πr]) ) =\n= 1\n2\n( R3(µ[πwin, πwin, πwin/win′ , πwin])−R3(µ[πr, πwin, πwin/win′ , πwin]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(πwin, πwin, πwin/win′ , πwin) the agent in slot 3 (πwin/win′), where they prey will not be chased due to the misscoordination of πwin/win′ in the last iterations, will obtain an expected average reward of −1, while in line-up (πr, πwin, πwin/win′ , πwin) the agent in slot 3 (πwin/win′), where the prey will almost always be chased by the predators, will almost obtain an expected average reward of 1. So:\nAComp3,1(πwin/win′ , πwin,Πo, wL̇, µ) = 1\n2 ((−1)− 1) = −1\nAnd for slots 4 and 1:\nAComp4,1(πwin/win′ , πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇) 1\n2\n( R4(µ[l̇ 4,1← πwin/win′ , πwin])−R4(µ[l̇ 4,1← πwin/win′ , πr]) ) =\n= 1\n2\n( R4(µ[πwin, πwin, πwin, πwin/win′ ])−R4(µ[πr, πwin, πwin, πwin/win′ ]) ) We know from lemma 3 that three predators correctly coordinating will always chase the prey. In line-up\n(πwin, πwin, πwin, πwin/win′) the agent in slot 4 (πwin/win′), where they prey will not be chased due to the misscoordination of πwin/win′ in the last iterations, will obtain an expected average reward of −1, while in line-up (πr, πwin, πwin, πwin/win′) the agent in slot 4 (πwin/win′), where the prey will almost always be chased by the predators, will almost obtain an expected average reward of 1. So:\nAComp4,1(πwin/win′ , πwin,Πo, wL̇, µ) = 1\n2 ((−1)− 1) = −1\nAnd finally, we weight over the slots:\nAComp(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS22 ∑ t1,t2∈τ |t1 ̸=t2 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACompi,j(Πe, wΠe ,Πo, wL̇, µ) =\n= 8\n3\n1\n4\n1 4 {AComp1,2(Πe, wΠe ,Πo, wL̇, µ) +AComp1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+AComp1,4(Πe, wΠe ,Πo, wL̇, µ) +AComp2,1(Πe, wΠe ,Πo, wL̇, µ)+ +AComp3,1(Πe, wΠe ,Πo, wL̇, µ) +AComp4,1(Πe, wΠe ,Πo, wL̇, µ)} = = 8\n3\n1\n4\n1 4 {AComp1,2(πwin/win′ , πwin,Πo, wL̇, µ) +AComp1,3(πwin/win′ , πwin,Πo, wL̇, µ)+\n+AComp1,4(πwin/win′ , πwin,Πo, wL̇, µ) +AComp2,1(πwin/win′ , πwin,Πo, wL̇, µ)+ +AComp3,1(πwin/win′ , πwin,Πo, wL̇, µ) +AComp4,1(πwin/win′ , πwin,Πo, wL̇, µ)} = = 8\n3\n1\n4\n1 4 {6× (−1)} = −1\nSince −1 is the lowest possible value for the competitive anticipation property, therefore predator-prey has Generalmin = −1 for this property.\nProposition 64. Generalmax for the competitive anticipation (AComp) property is equal to 1 2 for the predatorprey environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πtl/br} with wΠe(πtl/br) = 1 and Πo = {πbr} (a πtl/br agent always stays in the top left corner when playing as the prey and always goes to the bottom right corner when playing as a predator, and a πbr agent always goes to the bottom right corner).\nFollowing definition 33, we obtain the AComp value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of slots in different teams. Following definition 32, we could calculate its AComp value for each pair of slots but, since Πe has only one agent, its weight is equal to 1 and Πo also has only one agent, it is equivalent to use directly definition 31. We start with slots 1 and 2:\nAComp1,2(πtl/br, πbr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,2 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,2← πtl/br, πbr])−R1(µ[l̇ 1,2← πtl/br, πr]) ) =\n= 1\n2\n( R1(µ[πtl/br, πbr, πbr, πbr])−R1(µ[πtl/br, πr, πbr, πbr]) ) In line-up (πtl/br, πbr, πbr, πbr) the agent in slot 1 (πtl/br) will obtain an expected average reward of 1, while in line-up (πtl/br, πr, πbr, πbr) the agent in slot 1 (πtl/br), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 125. So:\nAComp1,2(πtl/br, πbr,Πo, wL̇, µ) = 1\n2 (1− 1) = 0\nFor slots 1 and 3:\nAComp1,3(πtl/br, πbr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,3 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,3← πtl/br, πbr])−R1(µ[l̇ 1,3← πtl/br, πr]) ) =\n= 1\n2\n( R1(µ[πtl/br, πbr, πbr, πbr])−R1(µ[πtl/br, πbr, πr, πbr]) ) In line-up (πtl/br, πbr, πbr, πbr) the agent in slot 1 (πtl/br) will obtain an expected average reward of 1, while in line-up (πtl/br, πbr, πr, πbr) the agent in slot 1 (πtl/br), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\n25It is arguably that some behaviours for the prey could try to be chased by the random agent, providing a greater value for this property, but this probability of been chased will still remain too low, which would not increase too much the value.\nAComp1,3(πtl/br, πbr,Πo, wL̇, µ) = 1\n2 (1− 1) = 0\nFor slots 1 and 4:\nAComp1,4(πtl/br, πbr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−1,4 (Πo)\nwL̇(l̇) 1\n2\n( R1(µ[l̇ 1,4← πtl/br, πbr])−R1(µ[l̇ 1,4← πtl/br, πr]) ) =\n= 1\n2\n( R1(µ[πtl/br, πbr, πbr, πbr])−R1(µ[πtl/br, πbr, πbr, πr]) ) In line-up (πtl/br, πbr, πbr, πbr) the agent in slot 1 (πtl/br) will obtain an expected average reward of 1, while in line-up (πtl/br, πbr, πbr, πr) the agent in slot 1 (πtl/br), where the prey will almost always be able to avoid the predators, will almost obtain an expected average reward of 1. So:\nAComp1,4(πtl/br, πbr,Πo, wL̇, µ) = 1\n2 (1− 1) = 0\nFor slots 2 and 1:\nAComp2,1(πtl/br, πbr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,1 (Πo)\nwL̇(l̇) 1\n2\n( R2(µ[l̇ 2,1← πtl/br, πbr])−R2(µ[l̇ 2,1← πtl/br, πr]) ) =\n= 1\n2\n( R2(µ[πbr, πtl/br, πbr, πbr])−R2(µ[πr, πtl/br, πbr, πbr]) ) In line-up (πbr, πtl/br, πbr, πbr), where πtl/br and πbr will always go to the bottom right corner, the agent in slot 2 (πtl/br) will obtain an expected average reward of 1, since the prey will always be chased. In line-up (πr, πtl/br, πbr, πbr), where πtl/br and πbr will always go to the bottom right corner and πr will act randomly, the agent in slot 2 (πtl/br) will obtain an expected average reward of −1, since the prey will rarely be chased. So:\nAComp2,1(πtl/br, πbr,Πo, wL̇, µ) = 1\n2 (1− (−1)) = 1\nFor slots 3 and 1:\nAComp3,1(πtl/br, πbr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,1 (Πo)\nwL̇(l̇) 1\n2\n( R3(µ[l̇ 3,1← πtl/br, πbr])−R3(µ[l̇ 3,1← πtl/br, πr]) ) =\n= 1\n2\n( R3(µ[πbr, πbr, πtl/br, πbr])−R3(µ[πr, πbr, πtl/br, πbr]) ) In line-up (πbr, πbr, πtl/br, πbr), where πtl/br and πbr will always go to the bottom right corner, the agent in slot 3 (πtl/br) will obtain an expected average reward of 1, since the prey will always be chased. In line-up (πr, πbr, πtl/br, πbr), where πtl/br and πbr will always go to the bottom right corner and πr will act randomly, the agent in slot 3 (πtl/br) will obtain an expected average reward of −1, since the prey will rarely be chased. So:\nAComp3,1(πtl/br, πbr,Πo, wL̇, µ) = 1\n2 (1− (−1)) = 1\nAnd for slots 4 and 1:\nAComp4,1(πtl/br, πbr,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,1 (Πo)\nwL̇(l̇) 1\n2\n( R4(µ[l̇ 4,1← πtl/br, πbr])−R4(µ[l̇ 4,1← πtl/br, πr]) ) =\n= 1\n2\n( R4(µ[πbr, πbr, πbr, πtl/br])−R4(µ[πr, πbr, πbr, πtl/br]) )\nIn line-up (πbr, πbr, πbr, πtl/br), where πtl/br and πbr will always go to the bottom right corner, the agent in slot 4 (πtl/br) will obtain an expected average reward of 1, since the prey will always be chased. In line-up (πr, πbr, πbr, πtl/br), where πtl/br and πbr will always go to the bottom right corner and πr will act randomly, the agent in slot 4 (πtl/br) will obtain an expected average reward of −1, since the prey will rarely be chased. So:\nAComp4,1(πtl/br, πbr,Πo, wL̇, µ) = 1\n2 (1− (−1)) = 1\nAnd finally, we weight over the slots:\nAComp(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS22 ∑ t1,t2∈τ |t1 ̸=t2 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACompi,j(Πe, wΠe ,Πo, wL̇, µ) =\n= 8\n3\n1\n4\n1 4 {AComp1,2(Πe, wΠe ,Πo, wL̇, µ) +AComp1,3(Πe, wΠe ,Πo, wL̇, µ)+\n+AComp1,4(Πe, wΠe ,Πo, wL̇, µ) +AComp2,1(Πe, wΠe ,Πo, wL̇, µ)+ +AComp3,1(Πe, wΠe ,Πo, wL̇, µ) +AComp4,1(Πe, wΠe ,Πo, wL̇, µ)} = = 8\n3\n1\n4\n1 4 {AComp1,2(πtl/br, πbr,Πo, wL̇, µ) +AComp1,3(πtl/br, πbr,Πo, wL̇, µ)+\n+AComp1,4(πtl/br, πbr,Πo, wL̇, µ) +AComp2,1(πtl/br, πbr,Πo, wL̇, µ)+ +AComp3,1(πtl/br, πbr,Πo, wL̇, µ) +AComp4,1(πtl/br, πbr,Πo, wL̇, µ)} = = 8\n3\n1\n4\n1 4 {3× 0 + 3× 1} = 1 2\nSince 12 is the highest possible value that we can obtain for the competitive anticipation property, therefore predator-prey has Generalmax = 1 2 for this property."
    }, {
      "heading" : "C.8 Cooperative Anticipation",
      "text" : "Finally, we follow with the cooperative anticipation (ACoop) property. As given in section 4.5.2, we want to know how much benefit the evaluated agents obtain when they anticipate cooperating agents.\nProposition 65. Generalmin for the cooperative anticipation (ACoop) property is equal to −1 for the predatorprey environment.\nProof. To find Generalmin (equation 40), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which minimises the property as much as possible. We can have this situation by selecting Πe = {πbr′} with wΠe(πbr′) = 1 and Πo = {π33/br′} (a πbr′ agent always goes to the bottom right corner, but if it notices that not all the predators are going directly to this corner, then it will go to the cell in the 3rd row and 3rd column, and a π33/br′ agent always goes to the cell in the 3rd row and 3rd column when playing as the prey and will go directly to the bottom right corner when playing as a predator, but if it notices that not all the predators are going directly to this corner, then it will go to the cell in the 3rd row and 3rd column).\nFollowing definition 36, we obtain the ACoop value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of different slots in the same team. Following definition 35, we could calculate its ACoop value for each pair of slots but, since Πe has only one agent, its weight is equal to 1 and Πo also has only one agent, it is equivalent to use directly definition 34. We start with slots 2 and 3: ACoop2,3(πbr′ , π33/br′ ,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R2(µ[l̇ 2,3← πbr′ , π33/br′ ]) +R3(µ[l̇ 2,3← πbr′ , π33/br′ ])−R2(µ[l̇ 2,3← πbr′ , πr])−R3(µ[l̇ 2,3← πr, π33/br′ ]) ) =\n= 1\n4\n( R2(µ[π33/br′ , πbr′ , π33/br′ , π33/br′ ]) +R3(µ[π33/br′ , πbr′ , π33/br′ , π33/br′ ])−\n−R2(µ[π33/br′ , πbr′ , πr, π33/br′ ])−R3(µ[π33/br′ , πr, π33/br′ , π33/br′ ]) )\nIn both line-ups (π33/br′ , πbr′ , π33/br′ , π33/br′), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 2 (πbr′) and slot 3 (π33/br′) will both obtain an expected average reward of −1. In line-up (π33/br′ , πbr′ , πr, π33/br′), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 2 (πbr′) will almost obtain an expected average reward of 1. In line-up (π33/br′ , πr, π33/br′ , π33/br′), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 3 (π33/br′) will almost obtain an expected average reward of 1. So:\nACoop2,3(πbr′ , π33/br′ ,Πo, wL̇, µ) = 1\n4 ((−1) + (−1)− 1− 1) = −1\nFor slots 2 and 4:\nACoop2,4(πbr′ , π33/br′ ,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R2(µ[l̇ 2,4← πbr′ , π33/br′ ]) +R4(µ[l̇ 2,4← πbr′ , π33/br′ ])−R2(µ[l̇ 2,4← πbr′ , πr])−R4(µ[l̇ 2,4← πr, π33/br′ ]) ) =\n= 1\n4\n( R2(µ[π33/br′ , πbr′ , π33/br′ , π33/br′ ]) +R4(µ[π33/br′ , πbr′ , π33/br′ , π33/br′ ])−\n−R2(µ[π33/br′ , πbr′ , π33/br′ , πr])−R4(µ[π33/br′ , πr, π33/br′ , π33/br′ ]) )\nIn both line-ups (π33/br′ , πbr′ , π33/br′ , π33/br′), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 2 (πbr′) and slot 4 (π33/br′) will both obtain an expected average reward of −1. In line-up (π33/br′ , πbr′ , π33/br′ , πr), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 2 (πbr′) will almost obtain an expected average reward of 1. In line-up (π33/br′ , πr, π33/br′ , π33/br′), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 4 (π33/br′) will almost obtain an expected average reward of 1. So:\nACoop2,4(πbr′ , π33/br′ ,Πo, wL̇, µ) = 1\n4 ((−1) + (−1)− 1− 1) = −1\nFor slots 3 and 2:\nACoop3,2(πbr′ , π33/br′ ,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R3(µ[l̇ 3,2← πbr′ , π33/br′ ]) +R2(µ[l̇ 3,2← πbr′ , π33/br′ ])−R3(µ[l̇ 3,2← πbr′ , πr])−R2(µ[l̇ 3,2← πr, π33/br′ ]) ) =\n= 1\n4\n( R3(µ[π33/br′ , π33/br′ , πbr′ , π33/br′ ]) +R2(µ[π33/br′ , π33/br′ , πbr′ , π33/br′ ])−\n−R3(µ[π33/br′ , πr, πbr′ , π33/br′ ])−R2(µ[π33/br′ , π33/br′ , πr, π33/br′ ]) )\nIn both line-ups (π33/br′ , π33/br′ , πbr′ , π33/br′), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 3 (πbr′) and slot 2 (π33/br′) will both obtain an expected average reward of −1. In line-up (π33/br′ , πr, πbr′ , π33/br′), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 3 (πbr′) will almost obtain an expected average reward of 1. In line-up (π33/br′ , π33/br′ , πr, π33/br′), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 2 (π33/br′) will almost obtain an expected average reward of 1. So:\nACoop3,2(πbr′ , π33/br′ ,Πo, wL̇, µ) = 1\n4 ((−1) + (−1)− 1− 1) = −1\nFor slots 3 and 4:\nACoop3,4(πbr′ , π33/br′ ,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R3(µ[l̇ 3,4← πbr′ , π33/br′ ]) +R4(µ[l̇ 3,4← πbr′ , π33/br′ ])−R3(µ[l̇ 3,4← πbr′ , πr])−R4(µ[l̇ 3,4← πr, π33/br′ ]) ) =\n= 1\n4\n( R3(µ[π33/br′ , π33/br′ , πbr′ , π33/br′ ]) +R4(µ[π33/br′ , π33/br′ , πbr′ , π33/br′ ])−\n−R3(µ[π33/br′ , π33/br′ , πbr′ , πr])−R4(µ[π33/br′ , π33/br′ , πr, π33/br′ ]) )\nIn both line-ups (π33/br′ , π33/br′ , πbr′ , π33/br′), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 3 (πbr′) and slot 4 (π33/br′) will both obtain an expected average reward of −1. In line-up (π33/br′ , π33/br′ , πbr′ , πr), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 3 (πbr′) will almost obtain an expected average reward of 1. In line-up (π33/br′ , π33/br′ , πr, π33/br′), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 4 (π33/br′) will almost obtain an expected average reward of 1. So:\nACoop3,4(πbr′ , π33/br′ ,Πo, wL̇, µ) = 1\n4 ((−1) + (−1)− 1− 1) = −1\nFor slots 4 and 2:\nACoop4,2(πbr′ , π33/br′ ,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R4(µ[l̇ 4,2← πbr′ , π33/br′ ]) +R2(µ[l̇ 4,2← πbr′ , π33/br′ ])−R4(µ[l̇ 4,2← πbr′ , πr])−R2(µ[l̇ 4,2← πr, π33/br′ ]) ) =\n= 1\n4\n( R4(µ[π33/br′ , π33/br′ , π33/br′ , πbr′ ]) +R2(µ[π33/br′ , π33/br′ , π33/br′ , πbr′ ])−\n−R4(µ[π33/br′ , πr, π33/br′ , πbr′ ])−R2(µ[π33/br′ , π33/br′ , π33/br′ , πr]) )\nIn both line-ups (π33/br′ , π33/br′ , π33/br′ , πbr′), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 4 (πbr′) and slot 2 (π33/br′) will both obtain an expected average reward of −1. In line-up (π33/br′ , πr, π33/br′ , πbr′), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 4 (πbr′) will almost obtain an expected average reward of 1. In line-up (π33/br′ , π33/br′ , π33/br′ , πr), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 2 (π33/br′) will almost obtain an expected average reward of 1. So:\nACoop4,2(πbr′ , π33/br′ ,Πo, wL̇, µ) = 1\n4 ((−1) + (−1)− 1− 1) = −1\nAnd for slots 4 and 3:\nACoop4,3(πbr′ , π33/br′ ,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R4(µ[l̇ 4,3← πbr′ , π33/br′ ]) +R3(µ[l̇ 4,3← πbr′ , π33/br′ ])−R4(µ[l̇ 4,3← πbr′ , πr])−R3(µ[l̇ 4,3← πr, π33/br′ ]) ) =\n= 1\n4\n( R4(µ[π33/br′ , π33/br′ , π33/br′ , πbr′ ]) +R3(µ[π33/br′ , π33/br′ , π33/br′ , πbr′ ])−\n−R4(µ[π33/br′ , π33/br′ , πr, πbr′ ])−R3(µ[π33/br′ , π33/br′ , π33/br′ , πr]) )\nIn both line-ups (π33/br′ , π33/br′ , π33/br′ , πbr′), where the predators will go directly to the bottom right cell and the prey will go to the cell in the 3rd row and 3rd column, the agents in slot 4 (πbr′) and slot 3 (π33/br′) will both obtain an expected average reward of −1. In line-up (π33/br′ , π33/br′ , πr, πbr′), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 4 (πbr′) will almost obtain an expected average reward of 1. In line-up (π33/br′ , π33/br′ , π33/br′ , πr), where πr will act randomly so the predators will almost always notice this random movement and then they will go to the cell in the 3rd row and 3rd column, the agent in slot 3 (π33/br′) will almost obtain an expected average reward of 1. So:\nACoop4,3(πbr′ , π33/br′ ,Πo, wL̇, µ) = 1\n4 ((−1) + (−1)− 1− 1) = −1\nAnd finally, we weight over the slots:\nACoop(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS23 ∑ t∈τ ∑ i,j∈t|i̸=j wS(i, µ)wS(j, µ)ACoopi,j(Πe, wΠe ,Πo, wL̇, µ)+\n+ ∑\nt1,t2,t3∈τ |t1 ̸=t2 ̸=t3 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACoopi,j(Πe, wΠe ,Πo, wL̇, µ) =\n= 8\n3\n1\n4\n1 4 {ACoop2,3(Πe, wΠe ,Πo, wL̇, µ) +ACoop2,4(Πe, wΠe ,Πo, wL̇, µ)+\n+ACoop3,2(Πe, wΠe ,Πo, wL̇, µ) +ACoop3,4(Πe, wΠe ,Πo, wL̇, µ)+ +ACoop4,2(Πe, wΠe ,Πo, wL̇, µ) +ACoop4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 8\n3\n1\n4\n1 4 {ACoop2,3(πbr′ , π33/br′ ,Πo, wL̇, µ) +ACoop2,4(πbr′ , π33/br′ ,Πo, wL̇, µ)+\n+ACoop3,2(πbr′ , π33/br′ ,Πo, wL̇, µ) +ACoop3,4(πbr′ , π33/br′ ,Πo, wL̇, µ)+ +ACoop4,2(πbr′ , π33/br′ ,Πo, wL̇, µ) +ACoop4,3(πbr′ , π33/br′ ,Πo, wL̇, µ)} = = 8\n3\n1\n4\n1 4 {6× (−1)} = −1\nSince −1 is the lowest possible value for the cooperative anticipation property, therefore predator-prey has Generalmin = −1 for this property.\nProposition 66. Generalmax for the cooperative anticipation (ACoop) property is equal to 1 for the predatorprey environment.\nProof. To find Generalmax (equation 41), we need to find a trio ⟨Πe, wΠe ,Πo⟩ which maximises the property as much as possible. We can have this situation by selecting Πe = {πwin} with wΠe(πwin) = 1 and Πo = {πwin} (a πwin agent always tries to not be chased when playing as the prey and tries to chase when playing as the predator).\nFollowing definition 36, we obtain the ACoop value for this ⟨Πe, wΠe ,Πo⟩. Since the environment is not symmetric, we need to calculate this property for every pair of different slots in the same team. Following definition 35, we could calculate its ACoop value for each pair of slots but, since Πe has only one agent, its weight is equal to 1 and Πo also has only one agent, it is equivalent to use directly definition 34. We start with slots 2 and 3:\nACoop2,3(πwin, πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,3 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R2(µ[l̇ 2,3← πwin, πwin]) +R3(µ[l̇ 2,3← πwin, πwin])−R2(µ[l̇ 2,3← πwin, πr])−R3(µ[l̇ 2,3← πr, πwin]) ) =\n= 1\n4\n( R2(µ[πwin, πwin, πwin, πwin]) +R3(µ[πwin, πwin, πwin, πwin])−\n−R2(µ[πwin, πwin, πr, πwin])−R3(µ[πwin, πr, πwin, πwin]) )\nIn both line-ups (πwin, πwin, πwin, πwin), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 2 (πwin) and slot 3 (πwin) will both obtain an expected average reward of 1. In line-up (πwin, πwin, πr, πwin), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 2 (πwin) will almost obtain an expected average reward of −1. In line-up (πwin, πr, πwin, πwin), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 3 (πwin) will almost obtain an expected average reward of −1. So:\nACoop2,3(πwin, πwin,Πo, wL̇, µ) = 1\n4 (1 + 1− (−1)− (−1)) = 1\nFor slots 2 and 4:\nACoop2,4(πwin, πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−2,4 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R2(µ[l̇ 2,4← πwin, πwin]) +R4(µ[l̇ 2,4← πwin, πwin])−R2(µ[l̇ 2,4← πwin, πr])−R4(µ[l̇ 2,4← πr, πwin]) ) =\n= 1\n4\n( R2(µ[πwin, πwin, πwin, πwin]) +R4(µ[πwin, πwin, πwin, πwin])−\n−R2(µ[πwin, πwin, πwin, πr])−R4(µ[πwin, πr, πwin, πwin]) )\nIn both line-ups (πwin, πwin, πwin, πwin), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 2 (πwin) and slot 4 (πwin) will both obtain an expected average reward of 1. In line-up (πwin, πwin, πwin, πr), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 2 (πwin) will almost obtain an expected average reward of −1. In line-up (πwin, πr, πwin, πwin), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 4 (πwin) will almost obtain an expected average reward of −1. So:\nACoop2,4(πwin, πwin,Πo, wL̇, µ) = 1\n4 (1 + 1− (−1)− (−1)) = 1\nFor slots 3 and 2:\nACoop3,2(πwin, πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,2 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R3(µ[l̇ 3,2← πwin, πwin]) +R2(µ[l̇ 3,2← πwin, πwin])−R3(µ[l̇ 3,2← πwin, πr])−R2(µ[l̇ 3,2← πr, πwin]) ) =\n= 1\n4\n( R3(µ[πwin, πwin, πwin, πwin]) +R2(µ[πwin, πwin, πwin, πwin])−\n−R3(µ[πwin, πr, πwin, πwin])−R2(µ[πwin, πwin, πr, πwin]) )\nIn both line-ups (πwin, πwin, πwin, πwin), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 3 (πwin) and slot 2 (πwin) will both obtain an expected average reward of 1. In line-up (πwin, πr, πwin, πwin), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 3 (πwin) will almost obtain an expected average reward of −1. In line-up (πwin, πwin, πr, πwin), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 2 (πwin) will almost obtain an expected average reward of −1. So:\nACoop3,2(πwin, πwin,Πo, wL̇, µ) = 1\n4 (1 + 1− (−1)− (−1)) = 1\nFor slots 3 and 4:\nACoop3,4(πwin, πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−3,4 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R3(µ[l̇ 3,4← πwin, πwin]) +R4(µ[l̇ 3,4← πwin, πwin])−R3(µ[l̇ 3,4← πwin, πr])−R4(µ[l̇ 3,4← πr, πwin]) ) =\n= 1\n4\n( R3(µ[πwin, πwin, πwin, πwin]) +R4(µ[πwin, πwin, πwin, πwin])−\n−R3(µ[πwin, πwin, πwin, πr])−R4(µ[πwin, πwin, πr, πwin]) )\nIn both line-ups (πwin, πwin, πwin, πwin), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 3 (πwin) and slot 4 (πwin) will both obtain an expected average reward of 1. In line-up (πwin, πwin, πwin, πr), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 3 (πwin) will almost obtain an expected average reward of −1. In line-up (πwin, πwin, πr, πwin), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 4 (πwin) will almost obtain an expected average reward of −1. So:\nACoop3,4(πwin, πwin,Πo, wL̇, µ) = 1\n4 (1 + 1− (−1)− (−1)) = 1\nFor slots 4 and 2:\nACoop4,2(πwin, πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,2 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R4(µ[l̇ 4,2← πwin, πwin]) +R2(µ[l̇ 4,2← πwin, πwin])−R4(µ[l̇ 4,2← πwin, πr])−R2(µ[l̇ 4,2← πr, πwin]) ) =\n= 1\n4\n( R4(µ[πwin, πwin, πwin, πwin]) +R2(µ[πwin, πwin, πwin, πwin])−\n−R4(µ[πwin, πr, πwin, πwin])−R2(µ[πwin, πwin, πwin, πr]) )\nIn both line-ups (πwin, πwin, πwin, πwin), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 4 (πwin) and slot 2 (πwin) will both obtain an expected average reward of 1. In line-up (πwin, πr, πwin, πwin), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 4 (πwin) will almost obtain an expected average reward of −1. In line-up (πwin, πwin, πwin, πr), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 2 (πwin) will almost obtain an expected average reward of −1. So:\nACoop4,2(πwin, πwin,Πo, wL̇, µ) = 1\n4 (1 + 1− (−1)− (−1)) = 1\nAnd for slots 4 and 3:\nACoop4,3(πwin, πwin,Πo, wL̇, µ) = ∑\nl̇∈L̇N(µ)−4,3 (Πo)\nwL̇(l̇) 1\n4 ×\n× ( R4(µ[l̇ 4,3← πwin, πwin]) +R3(µ[l̇ 4,3← πwin, πwin])−R4(µ[l̇ 4,3← πwin, πr])−R3(µ[l̇ 4,3← πr, πwin]) ) =\n= 1\n4\n( R4(µ[πwin, πwin, πwin, πwin]) +R3(µ[πwin, πwin, πwin, πwin])−\n−R4(µ[πwin, πwin, πr, πwin])−R3(µ[πwin, πwin, πwin, πr]) )\nIn both line-ups (πwin, πwin, πwin, πwin), where the predators will coordinate to always chase the prey as seen in lemma 3, the agents in slot 4 (πwin) and slot 3 (πwin) will both obtain an expected average reward of 1.\nIn line-up (πwin, πwin, πwin, πr), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 4 (πwin) will almost obtain an expected average reward of −1. In line-up (πwin, πwin, πr, πwin), where the random agent will almost never coordinate with the other predators so the prey will almost always survive, the agent in slot 3 (πwin) will almost obtain an expected average reward of −1. So:\nACoop4,3(πwin, πwin,Πo, wL̇, µ) = 1\n4 (1 + 1− (−1)− (−1)) = 1\nAnd finally, we weight over the slots:\nACoop(Πe, wΠe ,Πo, wL̇, µ, wS) = ηS23 ∑ t∈τ ∑ i,j∈t|i ̸=j wS(i, µ)wS(j, µ)ACoopi,j(Πe, wΠe ,Πo, wL̇, µ)+\n+ ∑\nt1,t2,t3∈τ |t1 ̸=t2 ̸=t3 ∑ i∈t1 wS(i, µ) ∑ j∈t2 wS(j, µ)ACoopi,j(Πe, wΠe ,Πo, wL̇, µ) =\n= 8\n3\n1\n4\n1 4 {ACoop2,3(Πe, wΠe ,Πo, wL̇, µ) +ACoop2,4(Πe, wΠe ,Πo, wL̇, µ)+\n+ACoop3,2(Πe, wΠe ,Πo, wL̇, µ) +ACoop3,4(Πe, wΠe ,Πo, wL̇, µ)+ +ACoop4,2(Πe, wΠe ,Πo, wL̇, µ) +ACoop4,3(Πe, wΠe ,Πo, wL̇, µ)} = = 8\n3\n1\n4\n1 4 {ACoop2,3(πwin, πwin,Πo, wL̇, µ) +ACoop2,4(πwin, πwin,Πo, wL̇, µ)+\n+ACoop3,2(πwin, πwin,Πo, wL̇, µ) +ACoop3,4(πwin, πwin,Πo, wL̇, µ)+ +ACoop4,2(πwin, πwin,Πo, wL̇, µ) +ACoop4,3(πwin, πwin,Πo, wL̇, µ)} = = 8\n3\n1\n4\n1 4 {6× 1} = 1\nSince 1 is the highest possible value for the cooperative anticipation property, therefore predator-prey has Generalmax = 1 for this property."
    } ],
    "references" : [ {
      "title" : "Expertness based cooperative q-learning. Systems, Man, and Cybernetics, Part B: Cybernetics",
      "author" : [ "Majid Nili Ahmadabadi", "Masoud Asadpour" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "The arcade learning environment",
      "author" : [ "MG Bellemare", "Y Naddaf", "J Veness", "M Bowling" ],
      "venue" : "J. Artificial Intelligence Res,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "On optimal cooperation of knowledge sources",
      "author" : [ "Miroslav Benda" ],
      "venue" : "Technical Report,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1985
    }, {
      "title" : "The first international roshambo programming competition",
      "author" : [ "Darse Billings" ],
      "venue" : "International Computer Games Association Journal,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "Evolutionary online learning of cooperative behavior with situationaction pairs",
      "author" : [ "Joerg Denzinger", "Michael Kordt" ],
      "venue" : "InMultiAgent Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "Experiments in learning prototypical situations for variants of the pursuit game",
      "author" : [ "Jörg Denzinger", "Matthias Fuchs" ],
      "venue" : "In Proc. ICMAS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1996
    }, {
      "title" : "AI - What is this? A definition of artificial intelligence",
      "author" : [ "D. Dobrev" ],
      "venue" : "PC Magazine Bulgaria (in Bulgarian, English version at http://www.dobrev.com/AI),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "A non-behavioural, computational extension to the Turing Test",
      "author" : [ "D.L. Dowe", "A.R. Hajek" ],
      "venue" : "In Intl. Conf. on Computational Intelligence & multimedia applications (ICCIMA’98), Gippsland,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "IQ tests are not for machines",
      "author" : [ "D.L. Dowe", "J. Hernández-Orallo" ],
      "venue" : "yet. Intelligence,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "How universal can an intelligence test be",
      "author" : [ "D.L. Dowe", "J. Hernández-Orallo" ],
      "venue" : "Adaptive Behavior,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Compression and intelligence: social environments and communication",
      "author" : [ "D.L. Dowe", "J. Hernández-Orallo", "P.K. Das" ],
      "venue" : "Artificial General Intelligence 2011,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "On interaction complexity, (space-time) resolution and intelligence",
      "author" : [ "D.L. Dowe", "J. Hernández-Orallo" ],
      "venue" : "In ReteCog interaction Workshop,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "The rating of chessplayers",
      "author" : [ "A.E. Elo" ],
      "venue" : "past and present,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1978
    }, {
      "title" : "On a pursuit game on cayley",
      "author" : [ "Peter Frankl" ],
      "venue" : "graphs. Combinatorica,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1987
    }, {
      "title" : "Learning to play pac-man: An evolutionary, rule-based approach",
      "author" : [ "Marcus Gallagher", "Amanda Ryan" ],
      "venue" : "In Evolutionary Computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2003
    }, {
      "title" : "Beyond the Turing Test",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : "J. Logic, Language & Information,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "On the computational measurement of intelligence factors",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : "National Institute of Standards and Technology,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2000
    }, {
      "title" : "On more realistic environment distributions for defining, evaluating and developing intelligence",
      "author" : [ "J. Hernández-Orallo", "D.L. Dowe", "S. España-Cubillo", "M.V. Hernández-Lloreda", "J. Insa-Cabrera" ],
      "venue" : "Artificial General Intelligence 2011,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Universal psychometrics: Measuring cognitive abilities in the machine kingdom",
      "author" : [ "J. Hernández-Orallo", "David L. Dowe", "M.Victoria Hernández-Lloreda" ],
      "venue" : "Cognitive Systems Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "On potential cognitive abilities in the machine kingdom",
      "author" : [ "J. Hernández-Orallo", "D.L. Dowe" ],
      "venue" : "Minds and Machines,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Turing tests with Turing machines",
      "author" : [ "J. Hernández-Orallo", "J. Insa", "D.L. Dowe", "B. Hibbard" ],
      "venue" : "The Alan Turing Centenary Conference,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Turing machines and recursive Turing tests",
      "author" : [ "J. Hernandez-Orallo", "J. Insa-Cabrera", "D.L. Dowe", "B. Hibbard" ],
      "venue" : "AISB/IACAP 2012 Symposium ”Revisiting Turing and his Test”,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity",
      "author" : [ "J. Hernández-Orallo", "N. Minaya-Collado" ],
      "venue" : "In Proc. Intl Symposium of Engineering of Intelligent Systems",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1998
    }, {
      "title" : "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems",
      "author" : [ "José Hernández-Orallo" ],
      "venue" : "In Artificial General Intelligence, 3rd Intl Conf,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Complexity distribution of agent policies",
      "author" : [ "José Hernández-Orallo" ],
      "venue" : "CoRR, abs/1302.2056,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "On environment difficulty and discriminating power",
      "author" : [ "José Hernández-Orallo" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Measuring universal intelligence: Towards an anytime intelligence test",
      "author" : [ "José Hernández-Orallo", "David L. Dowe" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Humans have evolved specialized skills of social cognition: The cultural intelligence hypothesis",
      "author" : [ "E. Herrmann", "J. Call", "M.V. Hernández-Lloreda", "B. Hare", "M. Tomasello" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    }, {
      "title" : "The structure of individual differences in the cognitive abilities of children and chimpanzees",
      "author" : [ "E. Herrmann", "M.V. Hernández-Lloreda", "J. Call", "B. Hare", "M. Tomasello" ],
      "venue" : "Psychological Science,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "Adversarial sequence prediction",
      "author" : [ "B. Hibbard" ],
      "venue" : "In Proceeding of the 2008 conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Measuring agent intelligence via hierarchies of environments",
      "author" : [ "B. Hibbard" ],
      "venue" : "Artificial General Intelligence,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Comparing humans and AI agents",
      "author" : [ "J. Insa-Cabrera", "D.L. Dowe", "S. España-Cubillo", "M.V. Hernández-Lloreda", "J. Hernández-Orallo" ],
      "venue" : "Artificial General Intelligence 2011,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2011
    }, {
      "title" : "Evaluating a reinforcement learning algorithm with a general intelligence test",
      "author" : [ "J. Insa-Cabrera", "D.L. Dowe", "J. Hernandez-Orallo" ],
      "venue" : "Advances in Artificial Intelligence - 14th Conference of the Spanish Association for Artificial Intelligence,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "Interaction settings for measuring (social) intelligence in multiagent systems",
      "author" : [ "J. Insa-Cabrera", "J. Hernández-Orallo" ],
      "venue" : "In ReteCog interaction Workshop,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2013
    }, {
      "title" : "Hernandez-Lloreda. The anynt project intelligence test : Lambda - one",
      "author" : [ "J. Insa-Cabrera", "J. Hernandez-Orallo", "D.L. Dowe", "S. Espaa", "M.V" ],
      "venue" : "AISB/IACAP 2012 Symposium ”Revisiting Turing and his Test”,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2012
    }, {
      "title" : "On measuring social intelligence: experiments on competition and cooperation",
      "author" : [ "Javier Insa-Cabrera", "José-Luis Benacloch-Ayuso", "José Hernández-Orallo" ],
      "venue" : "In Proceedings of the 5th international conference on Artificial General Intelligence,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2012
    }, {
      "title" : "Indirect interaction in environments for multi-agent systems",
      "author" : [ "David Keil", "Dina Q. Goldin" ],
      "venue" : "editors, E4MAS,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2005
    }, {
      "title" : "Robocup: The robot world cup initiative",
      "author" : [ "Hiroaki Kitano", "Minoru Asada", "Yasuo Kuniyoshi", "Itsuki Noda", "Eiichi Osawa" ],
      "venue" : "In Proceedings of the first international conference on Autonomous agents,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1997
    }, {
      "title" : "Universal intelligence: A definition of machine intelligence",
      "author" : [ "Shane Legg", "Marcus Hutter" ],
      "venue" : "Minds and Machines,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2007
    }, {
      "title" : "Environmental science: systems and solutions",
      "author" : [ "Michael L Mac Kinney", "Robert Milton Schoch" ],
      "venue" : "Jones & Bartlett Learning,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2003
    }, {
      "title" : "Are you socially intelligent",
      "author" : [ "F.A. Moss", "T. Hunt" ],
      "venue" : "Scientific American,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1927
    }, {
      "title" : "Game theory: analysis of conflict",
      "author" : [ "Roger B Myerson" ],
      "venue" : "Harvard university press,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2013
    }, {
      "title" : "An introduction to game theory",
      "author" : [ "Martin J Osborne" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2004
    }, {
      "title" : "Emergent bucket brigading: a simple mechanisms for improving performance in multi-robot constrained-space foraging tasks",
      "author" : [ "Esben H Ostergaard", "Gaurav S Sukhatme", "Maja J Matari" ],
      "venue" : "In Proceedings of the fifth international conference on Autonomous agents,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2001
    }, {
      "title" : "A strategic metagame player for general chess-like games",
      "author" : [ "Barney Pell" ],
      "venue" : "Computational Intelligence,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1996
    }, {
      "title" : "Using genetic programming with multiple data types and automatic modularization to evolve decentralized and coordinated navigation in multiagent systems",
      "author" : [ "Alan Robinson", "Lee Spector" ],
      "venue" : "In Late Breaking Papers at the Genetic and Evolutionary Computation Conference",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2002
    }, {
      "title" : "The Shapley value: essays in honor of Lloyd S",
      "author" : [ "Alvin E Roth" ],
      "venue" : null,
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1988
    }, {
      "title" : "Behaviour of trading automata in a computerized double auction",
      "author" : [ "John Rust", "Richard Palmer", "John H Miller" ],
      "venue" : null,
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1992
    }, {
      "title" : "Learning to coordinate without sharing information",
      "author" : [ "Sandip Sen", "Ip Sen", "Mahendra Sekaran", "John Hale" ],
      "venue" : "Proceedings of the Twelfth National Conference on Artificial Intelligence,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1994
    }, {
      "title" : "Fundamentals of Comparative Cognition",
      "author" : [ "S.J. Shettleworth", "P. Bloom", "L. Nadel" ],
      "venue" : null,
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2013
    }, {
      "title" : "Shettleworth. Cognition, evolution, and behavior",
      "author" : [ "J Sara" ],
      "venue" : null,
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2009
    }, {
      "title" : "Multiagent systems: algorithmic, game-theoretic, and logical foundations",
      "author" : [ "Y. Shoham", "K. Leyton-Brown" ],
      "venue" : "Cambridge Univ Pr,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2008
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "The MIT press,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1998
    }, {
      "title" : "Multi-agent reinforcement learning: Independent vs. cooperative agents",
      "author" : [ "Ming Tan" ],
      "venue" : "In Proceedings of the Tenth International Conference on Machine Learning,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 1993
    }, {
      "title" : "A monte-carlo aixi approximation",
      "author" : [ "Joel Veness", "Kee Siong Ng", "Marcus Hutter", "William Uther", "David Silver" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2011
    }, {
      "title" : "Some characteristics of the good judge of personality",
      "author" : [ "Philip E. Vernon" ],
      "venue" : "The Journal of Social Psychology,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 1933
    }, {
      "title" : "Comparative cognition: Experimental explorations of animal intelligence",
      "author" : [ "Edward A Wasserman", "Thomas R Zentall" ],
      "venue" : null,
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2006
    }, {
      "title" : "The measurement and appraisal of adult intelligence",
      "author" : [ "David Wechsler" ],
      "venue" : "Academic Medicine,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 1958
    }, {
      "title" : "Evolutionary game theory",
      "author" : [ "Jörgen W Weibull" ],
      "venue" : "The MIT press,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 1995
    }, {
      "title" : "Theory and Measurement of Social Intelligence as a Cognitive Performance Construct",
      "author" : [ "Susanne Weis" ],
      "venue" : "PhD thesis, Otto-von-Guericke-Universität Magdeburg, Universitätsbibliothek,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2008
    }, {
      "title" : "Designing the market game for a trading agent competition",
      "author" : [ "Michael P. Wellman", "Peter R. Wurman", "Kevin O’Malley", "Roshan Bangera", "S-d Lin", "Daniel Reeves", "William E. Walsh" ],
      "venue" : "Internet Computing,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2001
    }, {
      "title" : "Environments for multi-agent systems, state-of-the-art and research challenges",
      "author" : [ "D. Weyns", "H.V.D. Parunak", "F. Michel", "T. Holvoet", "J. Ferber" ],
      "venue" : "In Environments for MAS,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2005
    }, {
      "title" : "Generalized measures of information transfer",
      "author" : [ "P.L. Williams", "R.D. Beer" ],
      "venue" : "arXiv preprint arXiv:1102.1507,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2011
    }, {
      "title" : "Learning mazes with aliasing states: An LCS algorithm with associative perception",
      "author" : [ "Z. Zatuchna", "A. Bagnall" ],
      "venue" : "Adaptive Behavior,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2009
    }, {
      "title" : "co-evolutionary fitness switching: Learning complex collective behaviors using genetic programming",
      "author" : [ "Byoung-Tak Zhang", "Dong-Yeon Cho" ],
      "venue" : "Advances in genetic programming,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 40,
      "context" : "Many definitions have been proposed such as the “ability to understand and manage men and women, boys and girls – to act wisely in human relations” [56], the “ability to get along with others” [41], the “facility in dealing with human beings” [60], or more specific definitions including “[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers” [58].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 57,
      "context" : "Many definitions have been proposed such as the “ability to understand and manage men and women, boys and girls – to act wisely in human relations” [56], the “ability to get along with others” [41], the “facility in dealing with human beings” [60], or more specific definitions including “[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers” [58].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 55,
      "context" : "Many definitions have been proposed such as the “ability to understand and manage men and women, boys and girls – to act wisely in human relations” [56], the “ability to get along with others” [41], the “facility in dealing with human beings” [60], or more specific definitions including “[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers” [58].",
      "startOffset" : 573,
      "endOffset" : 577
    }, {
      "referenceID" : 59,
      "context" : "Despite the ambiguity of what social intelligence is, many tests have been proposed to measure social intelligence in humans (see [62] for a survey).",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 52,
      "context" : "This is the same configuration as in reinforcement learning (RL) [54], where rewards are provided in order to encourage agents to perform tasks.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 27,
      "context" : "For instance, some recent work has shown that social abilities can be compared in a systematic way between human children and apes [28, 29].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 28,
      "context" : "For instance, some recent work has shown that social abilities can be compared in a systematic way between human children and apes [28, 29].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 56,
      "context" : ", [59, 51]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 49,
      "context" : ", [59, 51]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 42,
      "context" : "For instance, prey-predator interaction and behaviour have been studied from many different points of view (including game theory [43]), but it is not clear how the ability of each subject can be objectively evaluated, especially because the interaction depends on the cognitive abilities of both prey and predator.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 56,
      "context" : "Despite these difficulties, comparative cognition [59, 51] is more and more concerned about performing tests that compare the abilities of many different species and also the abilities of individuals of different species.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 49,
      "context" : "Despite these difficulties, comparative cognition [59, 51] is more and more concerned about performing tests that compare the abilities of many different species and also the abilities of individuals of different species.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 26,
      "context" : "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).",
      "startOffset" : 223,
      "endOffset" : 235
    }, {
      "referenceID" : 18,
      "context" : "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).",
      "startOffset" : 223,
      "endOffset" : 235
    }, {
      "referenceID" : 9,
      "context" : "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).",
      "startOffset" : 223,
      "endOffset" : 235
    }, {
      "referenceID" : 2,
      "context" : "The environments used in social tests for machines tend to represent tasks that the agents must perform by interacting with other agents, so the performance is calculated as their capability to successfully cooperate with and/or compete against them to achieve some goals (see [3, 38] for two testbeds in multi-agent environments).",
      "startOffset" : 277,
      "endOffset" : 284
    }, {
      "referenceID" : 37,
      "context" : "The environments used in social tests for machines tend to represent tasks that the agents must perform by interacting with other agents, so the performance is calculated as their capability to successfully cooperate with and/or compete against them to achieve some goals (see [3, 38] for two testbeds in multi-agent environments).",
      "startOffset" : 277,
      "endOffset" : 284
    }, {
      "referenceID" : 42,
      "context" : "In the context of social sciences (stretching from economics to AI), game theory [43] has also studied the interaction of different agents in formalised structures (called games).",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 42,
      "context" : "Several kinds of games try to represent or to analyse a variety of properties: cooperative or non-cooperative games, simultaneous or sequential games, normal-form or extensive-form games, zero-sum or general-sum games, and symmetric or asymmetric games [43, 42].",
      "startOffset" : 253,
      "endOffset" : 261
    }, {
      "referenceID" : 41,
      "context" : "Several kinds of games try to represent or to analyse a variety of properties: cooperative or non-cooperative games, simultaneous or sequential games, normal-form or extensive-form games, zero-sum or general-sum games, and symmetric or asymmetric games [43, 42].",
      "startOffset" : 253,
      "endOffset" : 261
    }, {
      "referenceID" : 47,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 37,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 60,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 147,
      "endOffset" : 170
    }, {
      "referenceID" : 53,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 147,
      "endOffset" : 170
    }, {
      "referenceID" : 48,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 147,
      "endOffset" : 170
    }, {
      "referenceID" : 45,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 147,
      "endOffset" : 170
    }, {
      "referenceID" : 64,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 147,
      "endOffset" : 170
    }, {
      "referenceID" : 43,
      "context" : "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].",
      "startOffset" : 147,
      "endOffset" : 170
    }, {
      "referenceID" : 7,
      "context" : "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].",
      "startOffset" : 130,
      "endOffset" : 145
    }, {
      "referenceID" : 22,
      "context" : "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].",
      "startOffset" : 130,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].",
      "startOffset" : 130,
      "endOffset" : 145
    }, {
      "referenceID" : 16,
      "context" : "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].",
      "startOffset" : 130,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : "Dobrev [7] suggested that machine intelligence should be measured by evaluating agent performance in a range of worlds, an idea that was independently developed in [39] under the name “Universal Intelligence”.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 38,
      "context" : "Dobrev [7] suggested that machine intelligence should be measured by evaluating agent performance in a range of worlds, an idea that was independently developed in [39] under the name “Universal Intelligence”.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 26,
      "context" : "Following this definition, a framework to evaluate intelligence [27] and an environment following the framework [24] were proposed.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "Following this definition, a framework to evaluate intelligence [27] and an environment following the framework [24] were proposed.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 32,
      "context" : "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 34,
      "context" : "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 35,
      "context" : "This was the goal in [36], where other agents were directly included in the environment.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 26,
      "context" : "These experiments were performed using the framework in [27], which was originally designed to evaluate general intelligence, by simply including other agents in the environment.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "This circular problem is turned into a recursive one in [18], where different levels of distributions are recursively provided by constructing a new distribution of agents from a prior distribution 1Not only as an alternative to MAS scenarios, but also to the Turing Test, CAPTCHAs and IQ tests (see [9] for a discussion).",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "This circular problem is turned into a recursive one in [18], where different levels of distributions are recursively provided by constructing a new distribution of agents from a prior distribution 1Not only as an alternative to MAS scenarios, but also to the Turing Test, CAPTCHAs and IQ tests (see [9] for a discussion).",
      "startOffset" : 300,
      "endOffset" : 303
    }, {
      "referenceID" : 18,
      "context" : "This is in the spirit of universal psychometrics [19], where we must consider any kind of agent (natural or artificial).",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 35,
      "context" : "This can take us to definitions such as “performance of an agent in a wide range of environments while interacting with other agents” [36].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "oi,k) → [0, 1] denotes its probability to perform action ai,k after the sequence of events oi,1ai,1ri,1 .",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "ok−1ak−1rk−1) → [0, 1].",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "okak) → [0, 1].",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 46,
      "context" : "Also, we do not use the term alliance as we do not use any sophisticated mechanism to award rewards, related to the contribution of each agent in the team, as it is done with the Shapley Value [48].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 37,
      "context" : "Another example is RoboCup Soccer [38], denoted by μrc, whose τ would be {{1, 2, 3, 4, 5}, {6, 7, 8, 9, 10}}, which represents that there are two teams, with slots {1, 2, 3, 4, 5} in the first team and slots {6, 7, 8, 9, 10} in the second team.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 62,
      "context" : "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].",
      "startOffset" : 60,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].",
      "startOffset" : 60,
      "endOffset" : 76
    }, {
      "referenceID" : 33,
      "context" : "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].",
      "startOffset" : 60,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].",
      "startOffset" : 60,
      "endOffset" : 76
    }, {
      "referenceID" : 36,
      "context" : "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "However, as pointed out by [12], “this may originate from a common source”, so common or mutual information is not sufficient for interaction to have taken place.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 39,
      "context" : "In fact, in ecology, given two species, there are seven possible combinations of positive, negative or no effect between them, leading to six forms of symbiosis [40]: neutralism (0,0), amensalism (0,-), commensalism (+,0), competition (-,-), mutualism (+,+), and predation/parasitism (+,-).",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 46,
      "context" : "The previous definition may slightly resemble the Shapley Value [48] in cooperative game theory, but here we are not concerned about how relevant each agent is in a team (whether its contribution is higher than the contribution of its teammates), but to see whether there is effect on the rewards.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 25,
      "context" : ", [26]), one simple notion that accounts for this concept quite well is the variance of results.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : "So the idea we will pursue is to evaluate how close an environment and set of agents populating it are to this ideal situation from the expected average rewards of the evaluated agents (without an aggregated rating system): 6A common approach is to create a rating when we have many experiments, as done with sport ratings, such as the ELO rating [13] in chess.",
      "startOffset" : 347,
      "endOffset" : 351
    }, {
      "referenceID" : 21,
      "context" : "5][22], showing an agent set for the matching pennies game that is non-monotonic.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 29,
      "context" : "Nonetheless, a partial order can still be constructed for the agent set of all finite state machines for this game [30].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 38,
      "context" : "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "The set of all possible agents (either artificial or biological) is known as machine kingdom in [19, 20] and raises many questions about the feasibility of any test considering this astronomically large set.",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "The set of all possible agents (either artificial or biological) is known as machine kingdom in [19, 20] and raises many questions about the feasibility of any test considering this astronomically large set.",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : "In this way, we could aim at social intelligence relative to a smaller (and well-defined) set of agents, possibly specialising the definition by limiting the resources, the program size [25] or the intelligence of the agents [18].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "In this way, we could aim at social intelligence relative to a smaller (and well-defined) set of agents, possibly specialising the definition by limiting the resources, the program size [25] or the intelligence of the agents [18].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 26,
      "context" : "4 is not adaptive (for adaptive versions of universal tests, the reader is referred to [27, 19]).",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : "4 is not adaptive (for adaptive versions of universal tests, the reader is referred to [27, 19]).",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 44,
      "context" : "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].",
      "startOffset" : 112,
      "endOffset" : 128
    }, {
      "referenceID" : 61,
      "context" : "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].",
      "startOffset" : 112,
      "endOffset" : 128
    }, {
      "referenceID" : 51,
      "context" : "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].",
      "startOffset" : 112,
      "endOffset" : 128
    }, {
      "referenceID" : 63,
      "context" : "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].",
      "startOffset" : 112,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] The evaluated agents do not take into account other agents’ actions in their behaviour.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] Each evaluated agent obtains the same expected average reward independently of the line-up pattern.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] Every evaluated agent obtains the same expected average reward for each line-up pattern.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] Every evaluated agent obtains the same (social) intelligence value.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] There is no strict total order between the evaluated agents.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "[0, 1] There is no partial order between the evaluated agents.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 58,
      "context" : "2 Matching pennies Matching pennies [61] can be considered the simplest game in game theory featuring competition.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 29,
      "context" : "Nonetheless, there are different opinions about this, as it has been suggested that matching pennies could be an intelligence test on its own, under the name ‘Adversarial Sequence Prediction’ [30, 31].",
      "startOffset" : 192,
      "endOffset" : 200
    }, {
      "referenceID" : 30,
      "context" : "Nonetheless, there are different opinions about this, as it has been suggested that matching pennies could be an intelligence test on its own, under the name ‘Adversarial Sequence Prediction’ [30, 31].",
      "startOffset" : 192,
      "endOffset" : 200
    }, {
      "referenceID" : 20,
      "context" : "In [21] there is an example of an agent set for matching pennies that is non-monotonic (so PG < 1).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "[31].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "But, a good selection of Πo can restrict the Right range making it equal to [1, 1] (proposition 32).",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "But, a good selection of Πo can restrict the Right range making it equal to [1, 1] (proposition 32).",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "4 Predator-prey (Pursuit game) One typical environment for cooperation that uses a 2D discrete space is a pursuit game called Predator-prey [3], where the evaluee acts as a predator and has to cooperate/coordinate with other two predators in order to chase a prey.",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 53,
      "context" : ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.",
      "startOffset" : 2,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.",
      "startOffset" : 2,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.",
      "startOffset" : 2,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.",
      "startOffset" : 2,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.",
      "startOffset" : 2,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "One example of the use of games for evaluating AI is the ALE (Arcade Learning Environment) [2], a framework where a set of arcade computer games are used to evaluate the performance of current AI algorithms.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 54,
      "context" : ", [57, 15]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 14,
      "context" : ", [57, 15]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 37,
      "context" : "6 RoboCup Soccer As an example of a 3D space game we find the RoboCup Soccer competition [38].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : ", as in the spirit of the Darwin-Wallace distribution [18]).",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "[1] Majid Nili Ahmadabadi and Masoud Asadpour.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] MG Bellemare, Y Naddaf, J Veness, and M Bowling.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Miroslav Benda.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Darse Billings.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Joerg Denzinger and Michael Kordt.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Jörg Denzinger and Matthias Fuchs.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Peter Frankl.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Marcus Gallagher and Amanda Ryan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] José Hernández-Orallo.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] José Hernández-Orallo.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] José Hernández-Orallo.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] José Hernández-Orallo and David L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[28] E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[29] E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[30] B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31] B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[32] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[33] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[35] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[36] Javier Insa-Cabrera, José-Luis Benacloch-Ayuso, and José Hernández-Orallo.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[37] David Keil and Dina Q.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[38] Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, and Eiichi Osawa.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[39] Shane Legg and Marcus Hutter.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[40] Michael L Mac Kinney and Robert Milton Schoch.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "[41] F.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "[42] Roger B Myerson.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 42,
      "context" : "[43] Martin J Osborne.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 43,
      "context" : "[44] Esben H Ostergaard, Gaurav S Sukhatme, and Maja J Matari.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : "[45] Barney Pell.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "[47] Alan Robinson and Lee Spector.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 46,
      "context" : "[48] Alvin E Roth.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 47,
      "context" : "[49] John Rust, Richard Palmer, and John H Miller.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "[50] Sandip Sen, Ip Sen, Mahendra Sekaran, and John Hale.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 49,
      "context" : "[51] S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 50,
      "context" : "[52] Sara J Shettleworth.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 51,
      "context" : "[53] Y.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 52,
      "context" : "[54] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 53,
      "context" : "[55] Ming Tan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 54,
      "context" : "[57] Joel Veness, Kee Siong Ng, Marcus Hutter, William Uther, and David Silver.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 55,
      "context" : "[58] Philip E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 56,
      "context" : "[59] Edward A Wasserman and Thomas R Zentall.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 57,
      "context" : "[60] David Wechsler.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 58,
      "context" : "[61] Jörgen W Weibull.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 59,
      "context" : "[62] Susanne Weis.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 60,
      "context" : "[63] Michael P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 61,
      "context" : "[64] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 62,
      "context" : "[65] P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 63,
      "context" : "[66] Z.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 64,
      "context" : "[67] Byoung-Tak Zhang and Dong-Yeon Cho.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2014,
    "abstractText" : "Social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour. The amalgamation of these traits is then used to configure the intuitive notion of social intelligence. Instead, in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents, and we assess and derive tests from it. This definition makes several dependencies explicit: (1) the definition depends on the choice (and weight) of environments and agents, (2) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams, (3) the definition mostly depends on the abilities of other agents, and (4) the actual difference between social intelligence and general intelligence (or other abilities) depends on these choices. As a result, we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour (such as action and reward dependency and anticipation on competitive/cooperative behaviours) are met as well as some other more instrumental properties (such as secernment, boundedness, symmetry, validity, reliability, efficiency), which are convenient to convert the definition into a practical test. From the definition and the formalised properties, we take a look at several representative multi-agent environments, tests and games to see whether they meet these properties.",
    "creator" : "LaTeX with hyperref package"
  }
}