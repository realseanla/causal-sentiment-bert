{
  "name" : "1609.05796.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Enabling Dark Energy Science with Deep Generative Models of Galaxy Images",
    "authors" : [ "Siamak Ravanbakhsh", "François Lanusse", "Rachel Mandelbaum", "Jeff Schneider", "Barnabás Póczos" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The last two decades have greatly clarified the contents of the Universe, while leaving several large mysteries in our cosmological model. We now have compelling evidence that the expansion rate of the Universe is accelerating, suggesting that the vast majority of the total energy content of the Universe is the so-called dark energy. Yet we lack an understanding of what dark energy actually is, which provides one of the main motivations behind the next generation of cosmological surveys such as LSST LSST Science Collaboration et al. (2009), Euclid Laureijs et al. (2011) and WFIRST Green et al. (2012). These billion dollar projects are specifically designed to shed light on the nature of dark energy by probing the Universe through the weak gravitational lensing effect –i.e., the minute deflection of the light from distant objects by the intervening massive large scale structures of the Universe. On cosmological scales, this lensing effect causes very small but coherent deformations of background galaxy images, which appear slightly sheared, providing a way to statistically map the matter distribution in the Universe. To measure the lensing signal, future surveys will image and measure the shapes of billions of galaxies, significantly driving down statistical errors compared to the current generation of surveys, to the level where dark energy models may become distinguishable.\nHowever, the quality of this analysis hinges on the accuracy of the shape measurement algorithms tasked with estimating the ellipticities of the galaxies in the survey. This point is particularly crucial to the success of these missions, as any unaccounted for measurement biases in their ensemble\naverages would impact the final cosmological analysis and potentially lead to false conclusions. In order to detect and/or calibrate any such biases, future surveys will heavily rely on image simulations, closely mimicking real observations but with a known ground truth lensing signal.\nProducing these image simulations, however, is challenging in itself as they require high quality galaxy images as the input of the simulation pipeline. Such observations can only be obtained by extremely expensive space-based imaging surveys, which will remain a scarce resource for the foreseeable future. The largest current survey being used for image simulation purposes is the COSMOS survey Scoville et al. (2007), carried out using the Hubble Space Telescope (HST). Despite being the largest available dataset, COSMOS is relatively small, and there is great interest in increasing the size of our galaxy image samples to improve the quality of this crucial calibration process.\nIn this work, we propose an alternative to the expensive acquisition of more high quality calibration data using deep conditional generative models. In recent years, these models have achieved remarkable success in modeling complex highdimensional distributions, producing natural images that can pass the visual Turing test. Two prominent approaches for training these models are variational autoencoder (VAE) Kingma and Welling (2013); Rezende et al. (2014) and generative adversarial network (GAN) Goodfellow et al. (2014). Our aim is to train a coditional variation of these models using existing HST data and generate new galaxy\nar X\niv :1\n60 9.\n05 79\n6v 2\n[ as\ntr o-\nph .I\nM ]\n3 0\nN ov\n2 01\n2\nimages “conditioned” on statistics of interest such as the brightness or size of the galaxy. This will allow us to synthesize calibration datasets for specific galaxy populations, with objects exhibiting realistic morphologies. In related works in machine learning literature Regier et al. (2015b) use a convex combination of smooth and spiral templates in an (unconditioned) generative model of galaxy images and Regier et al. (2015a) propose using VAE for this task.1\nIn the following, Section I gives a brief background on the image generation for calibration and its significance for modern cosmology. We then review the current approaches to deep conditional generative models and introduce new techniques for our problem setting in Sections II and III. In Section IV we assess the quality of the generated images by comparing the conditional distributions of shape and morphology parameters between simulated and real galaxies, and find good agreement."
    }, {
      "heading" : "I. WEAK GRAVITATIONAL LENSING",
      "text" : "In the weak regime of gravitational lensing, the distortion of background galaxy images can be modeled by an anisotropic shear, noted γ, whose amplitude and orientation depend on the matter distribution between the observer and these distant galaxies. This shear affects in particular the apparent ellipticity of galaxies, denoted e. Measuring this weak lensing effect is made possible under the assumption that background galaxies are randomly oriented, so that the ensemble average of the shapes would average to zero in the absence of lensing. Their apparent ellipticity e can then be used as a noisy but unbiased estimator of the shear field γ: E[e] = γ. The cosmological\n1The current approach to address this problem in cosmology literature is to fit analytic parametric light profiles (defined by size, intensity, ellipticity and steepness parameters) to the observed galaxies, followed by a simple modelling of the distribution of the fitted parameters as a function of a quantity of interest, such as the galaxy brightness. This modelling usually simply involves fitting a linear dependence of mean and standard deviation of a Gaussian distribution – e.g., see Hoekstra et al. (2016); Appendix A. However, simple parametric models of galaxy light profiles do not have the complex morphologies needed for calibration task. The only currently available alternative, if realistic galaxy morphologies are needed, is to use the training set images themselves as the input of the simulation pipeline. This involves subsampling the training set to match the distribution of size, redshift and brightness of the target galaxy simulations, leaving only a relatively small number of objects, reused several hundred times to simulate a large survey – e.g., see Jarvis et al. (2016); Section 6.1.\nanalysis then involves computing auto- and cross-correlations of the measured ellipticities for galaxies at different distances. These correlation functions are compared to theoretical predictions in order to constrain cosmological models and shed light on the nature of dark energy.\nHowever, measuring galaxy ellipticities such that their ensemble average (used for the cosmological analysis) is unbiased is an extremely challenging task. Fig. 1 illustrates the main steps involved in the acquisition of the science images. The weakly sheared galaxy images undergo additional distortions (essentially blurring) as they go through the atmosphere and telescope optics, before being acquired by the imaging sensor which pixelates the noisy image. As this figure illustrates, the cosmological shear is clearly a subdominant effect in the final image and needs to be disentangled from subsequent blurring by the atmosphere and telescope options. This blurring, or Point Spread Function (PSF), can be directly measured by using stars as point sources, as shown at the top of Fig. 1.\nOnce the image is acquired, shape measurement algorithms are used to estimate the ellipticity of the galaxy while correcting for the PSF. However, despite the best efforts of the weak lensing community for nearly two decades, all current stateof-the-art shape measurement algorithms are still susceptible to biases in the inferred shears. These measurement biases are commonly modeled in terms of additive and multiplicative bias parameters c and m defined as:\nE[e] = (1 +m) γ + c (1)\nwhere γ is the true shear. Depending on the shape measurement method being used, m and c can depend on factors such as the PSF size/shape, the level of noise in the images or, more generally, intrinsic properties of the galaxy population (like their size and ellipticity distributions, etc. ). Calibration of these biases can be achieved using image simulations, closely mimicking real observations for a given survey but using galaxy images distorted with a known shear, thus allowing the measurement of the bias parameters in Eq. (1).\nImage simulation pipelines, such as the GalSim package Rowe et al. (2015), use a forward modeling of the observations, reproducing all the steps of the image acquisition pro-\n3\ncess in Fig. 1, and therefore require as a starting point galaxy images with high resolution and S/N. The main difficulty in these image simulations is therefore the need for a calibration sample of high quality galaxy images representative of the galaxy population of the survey being simulated. Our aim in this work is to train a deep generative model which can be used to cheaply synthesize such data sets for specific galaxy populations, by conditioning the samples on measurable quantities."
    }, {
      "heading" : "A. Data set",
      "text" : "As our main dataset, we use the COSMOS survey to build a training and validation set of galaxy images and extract from the corresponding catalog a condition vector y with three features: half-light radius (measure of size), magnitude (measure of brightness) and redshift (cosmological measure of distance). To facilitate the training, we align all galaxies along their major axis and produce 85,000 instances of 64x64 image stamps using the GalSim package.\nWe also use the GALAXY-ZOO dataset Willett et al. (2013) to demonstrate the abilities of our alternative conditional adversarial objective. Each of the 61,000 galaxy images in this dataset is accompanied by y ∈ [0, 1]37 features produced using a crowd-sourced set of questions that form a decision tree. We cropped the central 50% of these images and resized them to 128 × 128 pixels. We augmented both datasets by flipping the images along the vertical and horizontal axes."
    }, {
      "heading" : "II. CONDITIONAL VARIATIONAL AUTOENCODER",
      "text" : "Applications in semi-supervised learning and structured prediction have motivated different versions of the “conditional” variational autoencoder (C-VAE) in the past Kingma et al. (2014); Sohn et al. (2015). Although the architecture that we discuss here resembles to those of Kingma et al. (2014); Sohn et al. (2015), there are some differences due to different objectives.\nWe are interested in learning the conditional density p∗(x | y) for x ∈ X and y ∈ Y , given a set of observations D = (x̂1, ŷ1), . . . , (x̂N , ŷN ), by learning model parameters θ that maximizes the conditional likelihood ∏ (x̂,ŷ)∈D pθ(x̂ | ŷ) – e.g., for the COSMOS dataset X = <64×64 and Y = <3. In a latent-variable model, an auxiliary variable z ∈ Z is\nintroduced to increase the expressive power of pθ(x, z | y), such that ∫ Z pθ(x, z | y)dz is the marginal of interest. Here, different assignments to z can explain variations and complex statistical dependencies in p(x | y).\nTo enable efficient (ancestral) sampling from this model, pθ can be a directed model pθ(x, z | y) = pθ1(z | y) pθ2(x | z, y), where we first sample z ∼ pθ1(· | y) followed by x ∼ pθ2(· | z, y). An expressive form for the conditional distributions pθ1 and pθ2 is a deep neural network, that can represent complex directed graphical models. Here, for example, we model pθ2(x | z, y) using multi-layered convolutional or densely connected neural networks that encode the mean and variance of a multi-variate Gaussian for the COSMOS dataset and the expectation of Bernoulli variables for the GALAXYZOO dataset.\nTo learn the parameters θ one needs to estimate the posterior pθ(z | x, y), which is often intractable in directed models. An elegant solution to this problem is to introduce a second directed model q(z | x, y), called inference or recognition model. This conditional distribution is also encoded as a deep neural network and it is tasked with estimating the intractable posterior pθ(z | x, y).\nThis is achieved through a variational bound on the conditional log-likelihood:\nlog(pθ(x̂ | ŷ)) ≥− DKL(qφ(z | x̂, ŷ)‖pθ1(z | ŷ)) (2) + Ez∼qφ(·|x̂,ŷ)[log pθ2(x̂ | z, ŷ)]\nwhere the first term is the KL-divergence between the posterior qφ and the conditional prior pθ1 and the second term is the reconstruction error – that is we want the model to achieve low reconstruction error while encoding the dataset. At the same time the KL-divergence term encourages the code to follow a distribution, dictated by the the condition y. Fortunately, the reparametrization-trick by Kingma and Welling (2013); Rezende et al. (2014); Williams (1992) enables the maximization of this lower-bound (i.e., learn θ1, θ2 and φ) using stochastic back-propagation through the layers of these three neural networks. This enables maximizing the log-likelihood of an expressive model with large number of parameters through variations of stochastic gradient descent.\n4 0 50 100 150 200 iteration 10−3 10−2 10−1 100 101 102 103 av g. cr os sco rr el at io n II (train.) II (valid.) III (train.) III (valid.)\nFig. 4: Cross-correlation between y and z in C-VAE when p(z | y) = p(z), with and without cross-correlation penalty."
    }, {
      "heading" : "A. Cross-Correlation",
      "text" : "Inspired by the application of cross-correlation in disentangling the factors in an autoencoder by Cheung et al. (2014), we also consider an alternative method of conditioning in VAE. Let us proceed with a simple question: what happens here if we simplify the prior pθ1(z | y) ⇒ pθ1(z)? In principle, the simplified C-VAE would try to make the posterior qφ(z | x, y) independent of y.2 In this case, for generating samples x ∼ pθ(· | y), we could still sample z ∼ pθ1(·) and then generate x ∼ pθ2(· | y, z).\nIn practice, we observe z and y become more and more decorrelated during the training, but this happens at a slow pace. We can further enforce this decorrelation using a minibatch cross-correlation penalty\nC({ŷ}, {z}) def= 1 2 ∑ i,j ( 1 N N∑ n=1 (ŷ (n) i − ȳi)(z (n) i − z̄i) )2 where {ŷ}/{z} are conditions/codes in a mini-batch of size N , where z ∼ qψ(· | x̂) and i, j index dimensions of ŷ, z respectively. Here ȳi and z̄i are mini-batch average values.\nLack of cross-correlation only entails independence, if both yi and zi have Gaussian distribution. Although pθ1(z) is by design a standard Gaussian, the condition y may have an arbitrary distribution. To resolve this, we transform ŷi → F−1N (Fyi(ŷi)), where Fyi is the empirical cumulative distribution function (CDF) for ŷ ∈ D and F−1N is the (numerically\n2This is because the information content of y is already available to the generative model pθ(x | y, z) for reconstruction and reducing the information exchange through z should reduce the KL-divergence penalty DKL(qφ(z | x̂, ŷ)‖p(z)).\napproximated) inverse CDF of Gaussian. The transformed variable has a Gaussian distribution."
    }, {
      "heading" : "B. Experiments",
      "text" : "Figure 4 compares the reduction in the average crosscorrelation between ŷ and z for the same network, with and without the cross-correlation penalty. For numerical stability we linearly increase the penalty coefficient from 0 to 1000 over iterations. These results are for the COSMOS dataset. All C-VAE results are using the log-pixel-intensity, also for numerical stability.\nFigure 5 compares − log(pθ(x̂ | ŷ)) for three models: I using a neural network to encode pθ1(z | y)\nII using pθ1(z | y) = pθ1(z) III pθ1(z | y) = pθ1(z) plus cross-correlation penalty\nThe figure suggests that the first scheme eventually produces better models. It also shows that enforcing the independence of z and y only slightly decreases the likelihood, compared to the baseline II where z and y remain highly dependent."
    }, {
      "heading" : "III. A NEW OBJECTIVE FOR ADVERSARIAL TRAINING",
      "text" : "A major problem with VAE-generated images is their blurriness. A few recent works address this issue Kingma et al. (2016); Larsen et al. (2015); Dosovitskiy and Brox (2016) – e.g., by defining a more expressive reconstruction loss. Fortunately, the noise model is available for COSMOS images, and the added noise to some extent reduces this problem in our application (see Section IV).\nAn alternative to generative modeling that does not suffer from this problem is offered by adversarial training of generative networks Goodfellow et al. (2014). In the adversarial setting, a generator Gω : Z → X attempts to fool the discriminator Dψ : X → [0, 1] into classifying its fake instances x = G(z) as real, while the discriminator’s objective is to correctly classify the two sources of real versus generated instances. Deep networks representing these adversaries are trained alternatively, and under some conditions pG (the implicit distribution of the generator Gω for z ∼ U(0, 1)) converges to p∗ –i.e., at this fixed-point, the generator produces realistic images that are indistinguishable by the discriminator.\nThe conditional variation of this method was first introduced by Mirza and Osindero (2014) and used in a cascade\n5 CVAE sample CVAE sample + noise COSMOS image\nFig. 7: Comparison of a C-VAE sample before and after adding noise and a real COSMOS image with corresponding size, magnitude and redshift.\nof conditional models with increasing resolution in Denton et al. (2015). In these conditional models, the generator Gω : Z×Y → X and the discriminator Dψ : X ×Y → [0, 1], are both deep neural networks that are now conditioned on the same observed variable ŷ ∈ D. The min-max formulation of this adversarial setting seeks a saddle-point for\nmin ω max ψ\nEx̂,ŷ∈D,z∈U [ log(Dψ(x̂, ŷ))\n+ log(1−Dψ(Gω(z, ŷ), ŷ)) ]\nIn practice it is much more efficient to use a different loss function for the generator as it produces stronger gradients for the generator at the beginning Goodfellow et al. (2014):\nmax ψ\nEx̂,ŷ∈D,z∈U [ log(Dψ(x̂, ŷ))+\nlog(1−Dψ(Gω(z, ŷ), ŷ)) ]\nmax ω\nEz∈U [ log(Dψ(Gω(z, ŷ), ŷ)) ]\nHere, one must carefully adjust the expressive power of G and D to avoid oscillations, and domination of either adversary. The choice of hyper-parameters is known to be a major hurdle in training of adversarial networks and using this scheme, despite much effort, we could not train a generator for our problem that uses continuous conditional variables.\nWe introduce an alternative adversarial objective for conditional generative modeling that in our experience is more stable and did not require any hyper-parameter tuning in our application. The basic idea is simple: A predictor R : X → Y replaces the discriminator D : X × Y → [0, 1]. The predictor attempts to produce predictions of the condition ŷ ∈ D for the real data, that are at least as good as its predictions for generated instances. The generator’s objective is to produce instances with low prediction error\nPredictor: min ψ\nmin{0, (3) Ex̂,ŷ∈D,z∈U [ `(Rψ(Gω(z, ŷ)), ŷ)− `(Rψ(x), ŷ) ] }\nGenerator: min ω\nEŷ∈D,z∈U [ `(Rψ(Gω(z, ŷ)), ŷ) ] (4)\nwhere in our application `(y, ŷ) = ‖y − ŷ‖22. Why should the generator produces realistic images at all as long as the predictor makes equally bad predictions for both real and generated images? Both errors Eqs. (3) and (4) will be low in this case. The key here is that the generator always seeks to improve its samples to increase their prediction\n(a) Galaxy sizes (b) Galaxy brightness\nFig. 8: Comparison of galaxy sizes and brightness between real COSMOS images and C-VAE samples. Colors indicate the value of the relevant variable used to condition the generated images (half-light radius for size and magnitude for brightness)\naccuracy and therefore the dynamics of this adversarial setting does not allow this mode of failure.\nThis scheme, also relaxes the constraint on the expressive power of the adversaries. This is because the predictor has no incentive to lower the error for the real data, as long as its prediction errors are not worse that those of the generated data. Therefore, it is only the generator that fuels the competition and training is practically finished when the generator is unable to improve.\nA mode of failure that our scheme does not resolve is the collapse of generator, where generator G(y, z) repeats few output patterns by solely relying on y and basically ignoring the random feed z. The predictor eventually realizes this repeating pattern in generated data but gradient descent can no longer rescue the generator from this local optima. A solution to this problem called mini-batch discrimination was recently proposed by Salimans et al. (2016), where each instance in the mini-batch is augmented with information about its differences with other instances in the same mini-batch. The Predictor can therefore detect this tendency of the generator early on, and the generator incurs a loss for its behavior before its complete collapse. For better mini-batch statistics, we use relatively larger mini-batches with 128/256 instances."
    }, {
      "heading" : "A. Experiments",
      "text" : "Following Radford et al. (2015) we use (de)convolutional layers with (fractional) stride, batch normalization Ioffe and Szegedy (2015) and leaky-ReLU activation functions in our deep networks. For optimization, we use Adam Kingma and Ba (2014) with reduced exponential decay rate of .5 for the first moment estimates.\nFigure 6 reports the prediction loss `(Rψ(Gω(z, ŷ), ŷ)) and `(Rψ(x), ŷ) for the COSMOS dataset, were we use 4 (de)convolution layers. The figure suggests that the predictor tends to keep the prediction error of the real images slightly higher than that of generated images. Both of these quantities reduce over time, and their agreement with validation errors could monitor convergence. The fact that the error is decreasing over time and prediction error for both real and generated data remains close to each other is due to having a “laid back” predictor – i.e., by removing the min(0, .) operation in predictor’s loss, we would lose both of these properties.\nFor illustration purposes, we applied the same method to the GALAXY-ZOO dataset. Figure 2 shows some instances\n6 in the test-set accompanied by C-GAN generated image conditioned on the same ŷ. For this dataset we used 5- layer fully (de)convolutional generator and predictor, minibatch discrimination, batch-normalization and tanh activation function for the final layer of the generator.\nIV. VALIDATION\nIn this section, we assess the quality of the model generated galaxy images by comparing common image statistics used in weak lensing analyses. Our aim is to consistently measure the same statistics on real COSMOS images and images generated by our model for the same set of input variables y. These statistics are affected by the presence of noise in the image, but as was noted in the previous section, our C-VAE generates essentially noiseless images, which prevents direct comparison with real images. We limit this analysis to C-VAE generated images (as we found it to produce more consistent results compared to C-GAN) and add a noise field to our generated images. This noise model, calibrated for COSMOS observations, is provided by the GalSim package; see Fig. 7.\nThe most commonly used image statistics in weak lensing analyses rely on the second moments of the galaxy’s intensity profile I(u1, u2), where (u1, u2) are pixel coordinates. The second moment tensor Q is defined as:\nQαβ =\n∫ du1du2 W (u1, u2) I(u1, u2) uαuβ∫\ndu1du2 W (u1, u2) I(u1, u2) ,\nwith (α, β) ∈ {1, 2} and where W is a weighting function. This tensor can be used to define a size measurement σ = |det(Q)|1/4 which reduces to the standard deviation if the light profile is a Gaussian. More importantly, the second moments are commonly used to measure galaxy ellipticities which can be defined as:\ne = e1 + ie2 = Q11 −Q22 − 2iQ12\nQ11 +Q22 + 2(Q11Q22 −Q212)1/2 To measure Q in practice, we use the adaptive moments method Hirata and Seljak (2003); Mandelbaum et al. (2005) which estimates the second order moments by fitting an elliptical Gaussian profile to the galaxy light profile. As a side product of this method, we can also use the amplitude of the best fit Gaussian model as a proxy for the brightness of the galaxy.\nWe compare real COSMOS images to C-VAE samples by processing the images in pairs, where every COSMOS galaxy in our validation set is associated to a C-VAE sample conditioned on the half-light radius, magnitude and redshift of the real galaxy. Fig. 8a shows for each pair of images the galaxy size σ, as measured using second order moments; see also Fig. 3. The color of the points indicates the half-light radius of the COSMOS galaxy in the pair, also used to condition the C-VAE sample. As can be seen, the sizes of generated galaxies are generally unbiased. Fig. 8b shows the similar results for brightness; C-VAE is generating samples of the correct brightness without any significant bias.\nThe most relevant image statistics for weak lensing science are the ellipticity and size distributions of a given galaxy sample. Fig. 9 compares these overall distributions measured\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Ellipticity |e|\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0 COSMOS galaxies CVAE samples\n0 2 4 6 8 10 12 14 16 Size σ [pix]\n0.00\n0.05\n0.10\n0.15\n0.20 COSMOS galaxies CVAE samples\nFig. 9: Comparison of galaxy ellipticity (left) and size (right) distributions measured from second moments between real COSMOS images and CVAE samples.\non real and generated galaxies. Note that contrary to the previous test where the quantities considered (size and brightness) were part of the condition variable y, the ellipticity is not. Therefore, this test allows us to check how well the model is able to blindly learn correct galaxy shapes. This figure shows that despite being slightly more elliptical than real galaxies, the ellipticity distribution of the C-VAE samples is broadly consistent with the COSMOS distribution. Fig. 9 also compares size distributions which are in good agreement. This comes as no surprise however as C-VAE samples are explicitly conditioned on galaxy sizes and the previous test has shown these samples to be largely unbiased.\nCONCLUSION\nIn this paper, we proposed novel techniques and studied the application of two most promising methods for deep conditional generative modeling in producing galaxy images. In the future, we plan to measure more subtle morphological statistics in generated images and find ways for simultaneous learning of the noise model. We are also investigating the application of our variation on adversarial training in other settings and assessing the effectiveness of the predictor as a stand-alone classification/regression model."
    } ],
    "references" : [ {
      "title" : "Discovering hidden factors of variation in deep networks",
      "author" : [ "Brian Cheung", "Jesse A Livezey", "Arjun K Bansal", "Bruno A Olshausen" ],
      "venue" : "arXiv preprint arXiv:1412.6583,",
      "citeRegEx" : "Cheung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cheung et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Emily L Denton", "Soumith Chintala", "Rob Fergus" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating images with perceptual similarity metrics based on deep networks",
      "author" : [ "Alexey Dosovitskiy", "Thomas Brox" ],
      "venue" : "arXiv preprint arXiv:1602.02644,",
      "citeRegEx" : "Dosovitskiy and Brox.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dosovitskiy and Brox.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Wide-Field InfraRed Survey Telescope (WFIRST) Final Report",
      "author" : [ "J. Green", "P. Schechter", "C. Baltay", "R. Bean", "D. Bennett", "R. Brown", "C. Conselice", "M. Donahue" ],
      "venue" : null,
      "citeRegEx" : "Green et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Green et al\\.",
      "year" : 2012
    }, {
      "title" : "Shear calibration biases in weaklensing surveys",
      "author" : [ "C. Hirata", "U. Seljak" ],
      "venue" : "Monthly Notices of the Royal Astronomical Society,",
      "citeRegEx" : "Hirata and Seljak.,? \\Q2003\\E",
      "shortCiteRegEx" : "Hirata and Seljak.",
      "year" : 2003
    }, {
      "title" : "A study of the sensitivity of shape measurements to the input parameters of weak lensing image simulations",
      "author" : [ "Henk Hoekstra", "Massimo Viola", "Ricardo Herbonnet" ],
      "venue" : "arXiv preprint arXiv:1609.03281,",
      "citeRegEx" : "Hoekstra et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hoekstra et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "The des science verification weak lensing shear catalogues",
      "author" : [ "M Jarvis", "E Sheldon", "J Zuntz", "T Kacprzak", "SL Bridle", "A Amara", "R Armstrong", "MR Becker", "GM Bernstein", "C Bonnett" ],
      "venue" : "Monthly Notices of the Royal Astronomical Society,",
      "citeRegEx" : "Jarvis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jarvis et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving variational inference with inverse autoregressive flow",
      "author" : [ "Diederik P. Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2016
    }, {
      "title" : "Gravitational Lensing Accuracy Testing 2010 (GREAT10) Challenge Handbook",
      "author" : [ "T. Kitching", "S. Balan", "G. Bernstein", "M. Bethge", "S. Bridle", "F. Courbin", "M. Gentile", "A. Heavens" ],
      "venue" : null,
      "citeRegEx" : "Kitching et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kitching et al\\.",
      "year" : 2010
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity metric",
      "author" : [ "Anders Boesen Lindbo Larsen", "Søren Kaae Sønderby", "Ole Winther" ],
      "venue" : "arXiv preprint arXiv:1512.09300,",
      "citeRegEx" : "Larsen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 2015
    }, {
      "title" : "Euclid Definition Study Report",
      "author" : [ "R. Laureijs", "J. Amiaux", "S. Arduini", "J. . Auguères", "J. Brinchmann", "R. Cole", "M. Cropper", "C. Dabin", "L. Duvet", "A. Ealet" ],
      "venue" : null,
      "citeRegEx" : "Laureijs et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Laureijs et al\\.",
      "year" : 2011
    }, {
      "title" : "The Third Gravitational Lensing Accuracy Testing (GREAT3) Challenge Handbook",
      "author" : [ "R. Mandelbaum", "B. Rowe", "J. Bosch", "C. Chang", "F. Courbin", "M. Gill", "M. Jarvis", "A. Kannawadi" ],
      "venue" : "The Astrophysical Journal Supplement,",
      "citeRegEx" : "Mandelbaum et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mandelbaum et al\\.",
      "year" : 2014
    }, {
      "title" : "A deep generative model",
      "author" : [ "J Regier", "J McAuliffe", "Prabhat" ],
      "venue" : null,
      "citeRegEx" : "Regier et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Regier et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "(2009), Euclid Laureijs et al. (2011) and WFIRST Green et al.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "(2011) and WFIRST Green et al. (2012). These billion dollar projects are specifically designed to shed light on the nature of dark energy by probing the Universe through the weak gravitational lensing effect –i.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "Image credit: Mandelbaum et al. (2014), adapted",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "from Kitching et al. (2010).",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "Two prominent approaches for training these models are variational autoencoder (VAE) Kingma and Welling (2013); Rezende et al.",
      "startOffset" : 85,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "Two prominent approaches for training these models are variational autoencoder (VAE) Kingma and Welling (2013); Rezende et al. (2014) and generative adversarial network (GAN) Goodfellow et al.",
      "startOffset" : 85,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "(2014) and generative adversarial network (GAN) Goodfellow et al. (2014). Our aim is to train a coditional variation of these models using existing HST data and generate new galaxy ar X iv :1 60 9.",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "In related works in machine learning literature Regier et al. (2015b) use a convex combination of smooth and spiral templates in an (unconditioned) generative model of galaxy images and Regier et al.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "In related works in machine learning literature Regier et al. (2015b) use a convex combination of smooth and spiral templates in an (unconditioned) generative model of galaxy images and Regier et al. (2015a) propose using VAE for this task.",
      "startOffset" : 48,
      "endOffset" : 208
    }, {
      "referenceID" : 6,
      "context" : ", see Hoekstra et al. (2016); Appendix A.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : ", see Hoekstra et al. (2016); Appendix A. However, simple parametric models of galaxy light profiles do not have the complex morphologies needed for calibration task. The only currently available alternative, if realistic galaxy morphologies are needed, is to use the training set images themselves as the input of the simulation pipeline. This involves subsampling the training set to match the distribution of size, redshift and brightness of the target galaxy simulations, leaving only a relatively small number of objects, reused several hundred times to simulate a large survey – e.g., see Jarvis et al. (2016); Section 6.",
      "startOffset" : 6,
      "endOffset" : 616
    }, {
      "referenceID" : 11,
      "context" : "Applications in semi-supervised learning and structured prediction have motivated different versions of the “conditional” variational autoencoder (C-VAE) in the past Kingma et al. (2014); Sohn et al.",
      "startOffset" : 166,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : "Applications in semi-supervised learning and structured prediction have motivated different versions of the “conditional” variational autoencoder (C-VAE) in the past Kingma et al. (2014); Sohn et al. (2015). Although the architecture that we discuss here resembles to those of Kingma et al.",
      "startOffset" : 166,
      "endOffset" : 207
    }, {
      "referenceID" : 11,
      "context" : "Applications in semi-supervised learning and structured prediction have motivated different versions of the “conditional” variational autoencoder (C-VAE) in the past Kingma et al. (2014); Sohn et al. (2015). Although the architecture that we discuss here resembles to those of Kingma et al. (2014); Sohn et al.",
      "startOffset" : 166,
      "endOffset" : 298
    }, {
      "referenceID" : 11,
      "context" : "Applications in semi-supervised learning and structured prediction have motivated different versions of the “conditional” variational autoencoder (C-VAE) in the past Kingma et al. (2014); Sohn et al. (2015). Although the architecture that we discuss here resembles to those of Kingma et al. (2014); Sohn et al. (2015), there are some differences due to different objectives.",
      "startOffset" : 166,
      "endOffset" : 318
    }, {
      "referenceID" : 10,
      "context" : "Fortunately, the reparametrization-trick by Kingma and Welling (2013); Rezende et al.",
      "startOffset" : 44,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "Fortunately, the reparametrization-trick by Kingma and Welling (2013); Rezende et al. (2014); Williams (1992) enables the maximization of this lower-bound (i.",
      "startOffset" : 44,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "Fortunately, the reparametrization-trick by Kingma and Welling (2013); Rezende et al. (2014); Williams (1992) enables the maximization of this lower-bound (i.",
      "startOffset" : 44,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Inspired by the application of cross-correlation in disentangling the factors in an autoencoder by Cheung et al. (2014), we also consider an alternative method of conditioning in VAE.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "A few recent works address this issue Kingma et al. (2016); Larsen et al.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "A few recent works address this issue Kingma et al. (2016); Larsen et al. (2015); Dosovitskiy and Brox (2016) – e.",
      "startOffset" : 38,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "(2015); Dosovitskiy and Brox (2016) – e.",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "An alternative to generative modeling that does not suffer from this problem is offered by adversarial training of generative networks Goodfellow et al. (2014). In the adversarial setting, a generator Gω : Z → X attempts to fool the discriminator Dψ : X → [0, 1] into classifying its fake instances x = G(z) as real, while the discriminator’s objective is to correctly classify the two sources of real versus generated instances.",
      "startOffset" : 135,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "An alternative to generative modeling that does not suffer from this problem is offered by adversarial training of generative networks Goodfellow et al. (2014). In the adversarial setting, a generator Gω : Z → X attempts to fool the discriminator Dψ : X → [0, 1] into classifying its fake instances x = G(z) as real, while the discriminator’s objective is to correctly classify the two sources of real versus generated instances. Deep networks representing these adversaries are trained alternatively, and under some conditions pG (the implicit distribution of the generator Gω for z ∼ U(0, 1)) converges to p∗ –i.e., at this fixed-point, the generator produces realistic images that are indistinguishable by the discriminator. The conditional variation of this method was first introduced by Mirza and Osindero (2014) and used in a cascade",
      "startOffset" : 135,
      "endOffset" : 819
    }, {
      "referenceID" : 1,
      "context" : "of conditional models with increasing resolution in Denton et al. (2015). In these conditional models, the generator Gω : Z×Y → X and the discriminator Dψ : X ×Y → [0, 1], are both deep neural networks that are now conditioned on the same observed variable ŷ ∈ D.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "In practice it is much more efficient to use a different loss function for the generator as it produces stronger gradients for the generator at the beginning Goodfellow et al. (2014):",
      "startOffset" : 158,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "(2015) we use (de)convolutional layers with (fractional) stride, batch normalization Ioffe and Szegedy (2015) and leaky-ReLU activation functions in our deep networks.",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "(2015) we use (de)convolutional layers with (fractional) stride, batch normalization Ioffe and Szegedy (2015) and leaky-ReLU activation functions in our deep networks. For optimization, we use Adam Kingma and Ba (2014) with reduced exponential decay rate of .",
      "startOffset" : 85,
      "endOffset" : 219
    }, {
      "referenceID" : 5,
      "context" : "To measure Q in practice, we use the adaptive moments method Hirata and Seljak (2003); Mandelbaum et al.",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "To measure Q in practice, we use the adaptive moments method Hirata and Seljak (2003); Mandelbaum et al. (2005) which estimates the second order moments by fitting an elliptical Gaussian profile to the galaxy light profile.",
      "startOffset" : 61,
      "endOffset" : 112
    } ],
    "year" : 2016,
    "abstractText" : "Understanding the nature of dark energy, the mysterious force driving the accelerated expansion of the Universe, is a major challenge of modern cosmology. The next generation of cosmological surveys, specifically designed to address this issue, rely on accurate measurements of the apparent shapes of distant galaxies. However, shape measurement methods suffer from various unavoidable biases and therefore will rely on a precise calibration to meet the accuracy requirements of the science analysis. This calibration process remains an open challenge as it requires large sets of high quality galaxy images. To this end, we study the application of deep conditional generative models in generating realistic galaxy images. In particular we consider variations on conditional variational autoencoder and introduce a new adversarial objective for training of conditional generative networks. Our results suggest a reliable alternative to the acquisition of expensive high quality observations for generating the calibration data needed by the next generation of cosmological surveys. The last two decades have greatly clarified the contents of the Universe, while leaving several large mysteries in our cosmological model. We now have compelling evidence that the expansion rate of the Universe is accelerating, suggesting that the vast majority of the total energy content of the Universe is the so-called dark energy. Yet we lack an understanding of what dark energy actually is, which provides one of the main motivations behind the next generation of cosmological surveys such as LSST LSST Science Collaboration et al. (2009), Euclid Laureijs et al. (2011) and WFIRST Green et al. (2012). These billion dollar projects are specifically designed to shed light on the nature of dark energy by probing the Universe through the weak gravitational lensing effect –i.e., the minute deflection of the light from distant objects by the intervening massive large scale structures of the Universe. On cosmological scales, this lensing effect causes very small but coherent deformations of background galaxy images, which appear slightly sheared, providing a way to statistically map the matter distribution in the Universe. To measure the lensing signal, future surveys will image and measure the shapes of billions of galaxies, significantly driving down statistical errors compared to the current generation of surveys, to the level where dark energy models may become distinguishable. However, the quality of this analysis hinges on the accuracy of the shape measurement algorithms tasked with estimating the ellipticities of the galaxies in the survey. This point is particularly crucial to the success of these missions, as any unaccounted for measurement biases in their ensemble averages would impact the final cosmological analysis and potentially lead to false conclusions. In order to detect and/or calibrate any such biases, future surveys will heavily rely on image simulations, closely mimicking real observations but with a known ground truth lensing signal.",
    "creator" : "LaTeX with hyperref package"
  }
}