{
  "name" : "1610.06540.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "JOINTLY LEARNING TO ALIGN AND CONVERT GRAPHEMES TO PHONEMES WITH NEURAL ATTENTION MODELS",
    "authors" : [ "Shubham Toshniwal", "Karen Livescu" ],
    "emails" : [ "klivescu}@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1. INTRODUCTION\nGrapheme-to-phoneme conversion (“G2P”) is the task of converting from a spelling (a grapheme sequence) to its pronunciation (a phoneme or phone sequence1). For example, given the word paste, the task is to output its pronunciation [ P EY S T ]. This conversion is a frequent component of text-to-speech (TTS) and automatic speech recognition (ASR) systems. While static dictionaries exist, they are finite; G2P models are essential for handling new words.\nOne of the challenges in G2P conversion is that the pronunciation of any grapheme depends on a variety of factors including its context and the etymology of the word. For example, defiance is pronounced [ D IH F AY AH N S ], while the similar grapheme sequence fiance is pronounced in a very different way [ F IY AA N S EY ] because of its origin in French. Another complication is that the output phone sequence can be either shorter than or longer than the input grapheme sequence. For example, knife −→ [ N AY F ] has two “silent” graphemes at the beginning and end, while exit −→ [ EH G Z IH T ] has more phones than graphemes. The alignment between grapheme and phoneme sequences is therefore non-trivial.\nA typical approach to G2P involves using joint sequence models, where the alignment is provided via some external aligner [1, 2, 3]. However, since the alignment is a latent varible—a means to an end rather than the end itself—it is interesting to consider whether we can do away with such explicit alignments.\nSome recent work on the G2P problem has used neural networkbased approaches. Specifically, long short-term memory (LSTM) networks [4] have recently been explored [5, 6]. LSTMs (and, more generally, recurrent neural networks) can model varying contexts (“memory”) and have been successful for a number of sequence prediction tasks. When used in an encoder-decoder approach, as in [6], they in principle require no alignment between the input (grapheme\n1We will generally use the term “phoneme” to encompass any subword unit of pronunciation.\nsequence) and output (phoneme sequence) and are therefore quite natural for this task. However, to date the best-performing G2P approaches still use alignments.\nIn this paper we explore an extension of encoder-decoder networks based on an attention mechanism, which has proven useful in other sequence prediction tasks [7, 8, 9]. The attention mechanism endows encoder-decoder networks with the ability to consider “soft” alignments, and to learn these alignments jointly with the sequence prediction task. We show that we can improve over the state-ofthe-art G2P results on three standard data sets with such attentionenabled encoder-decoder models, removing entirely the dependency on alignments.2\n2. RELATED WORK\nMost previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10]. For example, an alignment between the grapheme sequence phoneme and the phoneme sequence [ F OW N IY M ] might be as follows:\nph o n e m e | | | | | | f ow n iy m -\nGiven a grapheme sequence (which has been aligned/chunked) G = g1, · · · , gm, conditional models estimate the probability Pr(P |G) of a phoneme sequence P = p1, · · · , pm by:\nPr(P |A,G) ≈ m∏ i=1 Pr(pi|pi−1i−k, g i+k i−k)\nfor some appropriate alignment A and a context window of size k. The factored terms in the distribution can be modeled using a maximum entropy classifier [1], or the full product can be modeled as a conditional random field (CRF) [11].\nAnother approach is to do joint sequence modeling by constructing a vocabulary of aligned grapheme and phoneme pairs, or graphones. The graphone sequence can be modeled via n-gram models [2, 10] or maximum entropy models [1].\nRecently there has been some work using recurrent neural networks (RNNs) for this task. Rao et al. [5] use bidirectional LSTMs with a connectionist temporal classification (CTC) layer [12] which doesn’t require the data to be aligned. Yao and Zweig [6] explore both an encoder-decoder architechture [13] and an input-output\n2Code available at https://github.com/shtoshni92/g2p\nar X\niv :1\n61 0.\n06 54\n0v 1\n[ cs\n.C L\n] 2\n0 O\nct 2\n01 6\nLSTM network with explicit alignments. Among these previous approaches, the best performance by a single model is obtained by Yao and Zweig’s alignment-based approach, although Rao et al. obtain even better performance on one data set by combining their LSTM model with an (alignment-based) n-gram model.\nIn this paper, we explore the use of attention in the encoderdecoder framework as a way of removing the dependency on alignments. The use of a neural attention model was first explored by Bahdanau et al. for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9]. The G2P problem is in fact largely analogous to the translation problem, with a many-to-many mapping between subsequences of input labels and subsequences of output labels and with potentially long-range dependencies (as in the effect of the final “e” in paste on the pronunciation of the “a”). In experiments presented below, we find that this type of attention model indeed removes our dependency on an external aligner and achieves improved performance on standard data sets.\n3. MODEL\nWe next describe the main components of our models both without and with attention."
    }, {
      "heading" : "3.1. Encoder-decoder models",
      "text" : "We briefly describe the encoder-decoder (“sequence-to-sequence”) approach, as proposed by [13]. An encoder-decoder model includes an encoder, which reads in the input (grapheme) sequence, and a decoder, which generates the output (phoneme) sequence. A typical encoder-decoder model is shown in Figure 1. In our model, the encoder is a bidirectional long short-term memory (BiLSTM) network; we use a bidirectional network in order to capture the context on both sides of each grapheme. The encoder takes as input the grapheme sequence, represented as a sequence of vectors x = (x1, · · · ,xTg ), obtained by multiplying the one-hot vectors representing the input characters with a character embedding matrix which is learned jointly with the rest of the model. The encoder computes a sequence of hidden state vectors, h = (h1, · · · ,hTg ),\ngiven by: −→ hi = f(xi, −−→ hi−1)\n←− hi = f ′(xi, ←−− hi+1) hi = ( −→ hi; ←− hi)\nWe use separate stacked (deep) LSTMs to model f and f ′.3 A “context vector” c is computed from the encoder’s state sequence:\nc = q({h1, · · · ,hTg})\nIn our case, we use a linear combination of −−→ hTg and ←− h1, with parameters learned during training. Since our models are stacked, we carry out this linear combination at every layer.\nThis context vector is passed as an input to the decoder. The decoder, g(·), is modeled as another stacked (unidirectional) LSTM, which predicts each phoneme yt given the context vector c and all of the previously predicted phonemes {y1, · · · , yt−1} in the following way:\ndt = g(ỹt−1,dt−1, c)\np(yt|y<t,x) = softmax(Wsdt + bs)\nwhere dt−1 is the hidden state of the decoder LSTM and ỹt−1 is the vector obtained by projecting the one hot vector corresponding to yt−1 using a phoneme embedding matrixE. The embedding matrix E is jointly learned with other parameters of the model. In basic encoder-decoder models, the context vector c is just used as an initial state for the decoder LSTM, d0 = c, and is not used after that."
    }, {
      "heading" : "3.2. Global Attention",
      "text" : "One of the important extensions of encoder-decoder models is the use of attention mechanism to adapt the context vector c for every output label prediction [7]. Rather than just using the context vector as an initial state for the decoder LSTM, we use a different context vector ct at every decoder time step, where ct is a linear combination of all of the encoder hidden states. The choice of initial state for the decoder LSTM is now less important; we simply use the last hidden state of the encoder’s backward LSTM. The ability to attend to different encoder states when decoding each output label means that the attention mechanism can be seen as a soft alignment between the input (grapheme) sequence and output (phoneme) sequence. We use the attention mechanism proposed by [16], where the context vector ct at time t is given by:\nuit = v T tanh(W1hi +W2dt + ba)\nαt = softmax(ut)\nct = Tg∑ i=1 αithi\nwhere the vectors v, ba and the matrices W1,W2 are parameters learned jointly with the rest of the encoder-decoder model. The score αit is a weight that represents the importance of the hidden encoder state hi in generating the phoneme yt. It should be noted that the vector hi is really a stack of vectors and for attention calculations we only use its top layer.\nThe decoder then uses ct in the following way:\np(yt|y<t,x) = softmax(Ws[ct;dt] + bs) 3For brevity we exclude the LSTM equations. The details can be found\nin Zaremba et al. [15]."
    }, {
      "heading" : "3.3. Local Attention",
      "text" : "The global attention mechanism considers all of the encoder states in predicting each decoder output. It can be argued, however, that the G2P problem has an inherently local nature; that is, each output typically depends on only a small window of input labels. Therefore, limiting attention to an appropriate window of encoder states should be sufficient to predict each phoneme. Local attention models operate by first finding an aligned position pt at each timsestep t, which is an index into the input sequence, and then considers a context window [pt −D, pt +D] of indices around it. In our experiments, we find D = 3 to work well. The context vector ct is then calculated in a similar fashion as in the global attention model described above but while considering only the encoder states falling within the context window. We consider two variants of local attention, based on [17], described below.\n3.3.1. Monotonic Alignment (local-m)\nThe local-m model makes the assumption of a simple monotonic alignment between the source and target sequence: pt = t. The attention weights are then computed as in the global attention model, but the context vector is computed by summing only over [pt −D, pt +D]. This type of attention model is a reasonable one for tasks where the input and output sequences can be expected to proceed almost in lock step, which is arguably the case in the G2P task.\n3.3.2. Predictive Alignment (local-p)\nThe local-m model may be too restrictive. An alternative is to include the prediction of the attention center pt as part of the model. The local-p model predicts pt in the following way:\npt = Tg · σ(vpT tanh(Wpdt))\nwhere Tg is the length of the input sequence, σ(·) is a logistic function, and vp,Wp are learned parameters. The σ(·) ensures that the predicted index does not exceed the source length. In addition, the model also favors the input positions near pt by reweighting the attention weights with a Gaussian prior centered around pt. The new attention weights α̃it are then given by:\nα̃it = αit · exp ( − (i− pt) 2\n2σ2 ) for encoder positions i falling within the context window, where αit is as computed for the global attention model and the Gaussian standard deviation σ = D\n2 . Finally, the new attention weights α̃it are\nused in place of αit in computing the context vector."
    }, {
      "heading" : "3.4. Input Feeding",
      "text" : "One of the issues with attention models is that at each decoder step, the attention model decides what to attend to independently of the decisions at other time steps. Therefore, there is no explicit global constraint to ensure that all of source sequence is “covered”, i.e. that all of the inputs are being attended to at one point or another. One indirect way of addressing this potential problem is by concatenating the previous attention-weighted context vector with the input for the next decoder time step, as done in [17]. This “feeding” of previous attention makes the model more aware of the attention decisions being made in previous time steps. We treat the choice of using input feeding as a hyperparameter of our models.\n4. EXPERIMENTS"
    }, {
      "heading" : "4.1. Data",
      "text" : "In order to compare directly with earlier results, we use three standard data sets typically used for evaluating G2P models, namely CMUDict, Pronlex and NetTalk. For all three data sets, we use the same experimental setup as in [1], which we briefly describe below.4\nThe CMUDict data set is split into a 106,837-word training set and a 12,000-word test set. As in previous work, we sample 2,670 words from the training set to create a development set for the purpose of hyperparameter tuning.\nThe Pronlex data set has 83,182 words in the training set, 2,400 in the development set, and 4,800 in the test set. The development and test sets are split into 3 categories. We report only the overall results and not category-wise ones for Pronlex.\nThe NetTalk data set is quite small compared to the other two data sets, with only 14,851 words for training and 4,951 words for testing. For the purpose of hyperparameter tuning, we create a development set of 1,000 words sampled from the training set; however, after tuning, the final model is trained on the original training set."
    }, {
      "heading" : "4.2. Evaluation",
      "text" : "We evaluate performance using the standard measures of word error rate (WER) and phoneme error rate (PER), reported as percentages. PER is equal to the Levenshtein distance of the predicted phoneme sequence from the ground truth divided by the total number of phonemes in the ground truth. WER is equal to the total number of word in which there is at least one phone error, divided by the total number of words. As in prior work, for words with multiple ground-truth pronunciations, we choose the ground truth that results in the lowest PER."
    }, {
      "heading" : "4.3. Training",
      "text" : "Our stacked LSTMs have 3 layers, each with 512 units. We use 512- dimensional embedding vectors for representing both the characters and the phonemes. We use minibatch stochastic gradient descent (SGD) together with Adam [18] using a minibatch size of 256. We use an initial learning rate of 0.001 and reduce this learning rate by a multiplicative factor of 0.8 whenever the WER for development data does not decrease at the end of an epoch. We train the model for 100 epochs but update/save it at the end of an epoch only when it decreases the WER on development data. To prevent overfitting we: (a) introduce a dropout [19] layer between every pair of consecutive layers of the stacked LSTMs, and (b) use scheduled sampling [20], with a linear decay, on the decoder side.\nWe tune the dropout probability over the range {0, 0.1, 0.2, 0.3, 0.4}. We also tune over the choice of using input feeding or not. The combination of hyperparameters that gives the best result on the development set is chosen. It should be noted that the performance is quite stable with respect to dropout and input feeding: We observed only a minor drop in performance even if we choose slightly “sub-optimal” parameters. All of the models are implemented using TensorFlow [21].\n4We are grateful to Stan Chen for providing the data. 5The high variance is due to 1 of the 5 runs being unusually bad, resulting\nin a WER of 33.47%"
    }, {
      "heading" : "4.4. Inference",
      "text" : "We use a greedy decoder (beam size = 1) to decode the phoneme sequence during inference. That is, at each decoding time step we consider the output phone to be the argmax of the softmax output of the decoder at that time frame. We got no reliable gains by using beam search with any beam size greater than 1."
    }, {
      "heading" : "4.5. Results",
      "text" : "Table 1 presents the main results with our tuned models on the three test sets, compared to the best previously reported results. For all three data sets, the best prior results to our knowledge with a single model are those of Yao and Zweig [6] with an alignment-based deep bidirectional LSTM. For CMUDict, a better WER was obtained by [5] by ensembling their CTC bidirectional LSTM with an alignment-based 5-gram model, but no corresponding PER was reported.\nOur best attention models clearly outperform all of the previous best models in terms of PER. In terms of WER, the attention model outperforms all prior single (non-ensembled) models. For CMUDict, we also include the result of ensembling five of our global attention models using different random initializations, by voting on their outputs (with random tie-breaking), which outperforms the ensemble of Rao et al..\nAmong the three attention models, global and local-m attention model perform well across all three data sets, while the local-p model performs well on CMUDict and Pronlex but not on NetTalk. The success of the global model may be explained by the fact that the source sequence length in this task (i.e., the word length) is rather short, always less than 25 characters in these three data sets. Therefore, it is feasible for the global attention model to consider all of the encoder states and weight them in an appropriate manner.\nThe local-m attention model, even with its simplistic assumption about alignment, outperforms the local-p model on every data set and is a clear best performer on Pronlex. Although the assumption of monotonic alignment turns out to be too simplistic for other tasks, such as machine translation [17], it is a reasonable choice for G2P.\nSurprisingly, the local-p attention model remains a distant third among the three attention models across all three data sets. Moreover, it suffers a higher PER even when it obtains comparable WERs,\nas is the case for Pronlex. This means that words with errors tend to have a large number of errors. This seems to suggest that if an alignment error is made near the beginning of the word, then it is hard for the local-p model to recover. This points towards a need for a better alignment prediction strategy. The particular poor performance of local-p on NetTalk also suggests that it may need a larger training set for learning the alignment prediction parameters.\nFor all of our results, we make the choice of dropout probability and input feeding based on tuning on the development set. The typical choice of dropout probability tends to be around 0.2-0.3 while the decision of using input feeding or not tends to vary with the choice of attention model and the data set used. However, performance does not vary greatly with these two parameters."
    }, {
      "heading" : "4.5.1. Ablation analysis for CMUDict",
      "text" : "In order to measure the contributions of various components of our attention models, we performed an ablation analysis for the global attention model evaluated on the CMUDict development data. The results are shown in Table 2. As can be seen from the table, the removal of input feeding results in a very minor drop in performance. The use of regularization in the form of dropout and scheduled sampling also provides a minor boost. The importance of attention is reflected in almost a 1% absolute drop in performance when atten-\ntion is removed. As we will later see, this boost is more prominent for longer words.\nIn addition, we also tested the effect of bidirectionality by comparing against a “reverse unidirectional” LSTM, which drops the bidirectionality and takes the input in reverse order. Interestingly, the reverse unidirectional non-attention model is better than its bidirectional counterpart. This might be due to how we initialize our decoder LSTM for the bidirectional case, where we use a linear combination of hidden states of the forward and backward encoder LSTMs. In fact, bidirectional LSTMs may not be needed for a task with such short input sequences, as is reflected in the very minor drop in performance for the reverse unidirectional global attention model.\nA large number of LSTM hidden units is crucial, as can be seen from the almost 11% absolute drop in performance when we reduce it to 50. The use of a 3-layer stacked LSTM is also justified, as a smaller number of stacked layers results in significant performance losses. The importance of LSTM units can be gauged by the performance drop when they are replaced by Gated Recurrent Units (GRUs) [22] in a reverse unidirectional global attention model.\n5. ANALYSIS\nIn this section, we analyze the models and their errors on the CMUDict data set."
    }, {
      "heading" : "5.1. Error Comparison Based on Word Length",
      "text" : "We compare the errors made by the global attention model and noattention model as a function of word length. For this purpose, we categorize the word length into 4 categories: short (length ≤ 6), medium (length∈ {7, 8}), long (length∈ {9, 10}), very long (length ≥ 11). Figure 2 provides the results of this comparison. There are two main observations that can be made from the plot: (i) Both models make fewer errors on longer words; this is in some ways counter-intuitive, but is perhaps because the longer a word is, the more opportunity the model has to discern certain important global properties such as its origin. (ii) The global attention model has an even larger advantage over the no-attention model for longer words. The second observation is quite intuitive: The no-attention model is forced to represent all of the information about a word with a single hidden vector, which naturally fails for longer sequences."
    }, {
      "heading" : "5.2. Phoneme Error Comparison Between Attention Models",
      "text" : "We next analyze the per-word PER, defined as the proportion of phoneme errors per word. We are interested to analyze the distribution of phoneme errors across words. That is, how often are word errors due to very minor phone errors, and how often are they due to a catastrophic failure? We consider this question by analyzing all of the word errors made by the global and local-p attention models. We categorize the per-word PER into 4 categories: small (≤ 10% of phones are incorrect), medium (10-20% incorrect), large (20-30%) and very large (≥ 30 %).\nWe find that the numbers of small and medium errors are comparable for the two types of attention models. However, the local-p makes many more large and very large errors. As might be expected, the local-p attention model depends a great deal on the predicted alignment position; if that prediction is incorrect, this can lead to cascading error effect and make it hard for the model to recover."
    }, {
      "heading" : "5.3. Error Classification",
      "text" : "The low PER of the global attention model means that even most of the incorrect predictions are still very close to the ground truth. We analyze the cases where the prediction is still very far from the ground truth, in order to understand the kinds of phenomena that still need to be addressed.\nWe consider the following four most common categories of errors, also illustrated in Table 3:\n• Foreign Origin Names: These examples can be very challenging, and at the same time fairly common as well. To give an idea of their importance, among the 13 words in the CMUDict development set with a phonetic edit distance of at least 4 between the ground truth and prediction, 11 of them were from this category. In the examples presented in Table 3, the word QUIXOTE is of Spanish origin and MACIOCE is an Italian last name.\n• Under/Over Production: In this type of error, the attention model either fails to attend to a character or attends to the same one too many times. For example, for the word KITTIWAKE, the model outputs [ K IH T AH W W K K ], as shown in Table 3, which seems to be due to translating the characters W, K twice. Conversely, for LASTS the model fails to convert the characters T, S at all, resulting in the output prediction [ L AE S ]. This problem has also been observed in neural machine translation, and there has been some recent work to address it [23]. The key idea is to maintain a coverage vector during decoding which indicates the part of source that has been translated. This approach may be a useful avenue for future work on G2P as well.\n• Abbreviations: This category is extremely difficult, but a bit rarer than the Foreign Names category. As shown in the table, for the word BLVD, which is an abbreviation of BOULEVARD, the model outputs [ B L AH D ]. Some of the abbreviations in the data set are extremely challenging, requiring word knowledge as in this case, and the attention model seemed to struggle a lot with them.\n6In the updated cmudict-0.7b, the model prediction is one of the pronunciations while the given ground truth has been removed\n• Wrong Ground Truth: This is a very rare category but interestingly, for all of the instances of this type of error made by the attention model, the model’s prediction matches the corrected version in the updated version of CMUDict. For instance, for the word STACIE, the model correctly outputs [ S T EY S IY ] but the ground truth is labeled as [ S T AE K IY ]. Another example is the misspelled word COMMERICAL, shown in Table 3, with letters I, C interchanged. Even for this non-existent word, the attention model outputs quite a convincing pronunciation. These errors suggest the need to update the version of CMUDict used in G2P evaluation for more meaningful results."
    }, {
      "heading" : "5.4. Phoneme Embedding Visualization",
      "text" : "Most of the model parameters do not have a clear meaning. However, the learned phone embeddings should be meaningful; for example, similar phones should be assigned similar embeddings and vice versa. In order to confirm this, we plot in Figure 3 a two-dimensional visualization, generated with t-SNE [24], of the phone embeddings learned by the global attention model.\nThe embeddings for the most part behave as expected. The phones are broadly clustered according to manner classes: vowels, diphthongs, fricatives, and stops generally each have their own portion of the space (with some intermingling of stops and fricatives), and diphthongs form a tight sub-group within the vowel region. Voiced/voiceless pairs of stops and fricatives are grouped close together, and vowels are placed in reasonable locations corresponding their height/frontness. Nasals and semivowels are the least wellclustered, as might be expected.\n6. CONCLUSION\nIn this work, we have applied an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Our attention models achieve state-of-the-art results on three standard data sets, thus eliminating the need for explicit alignments which have been used by the best previous approaches. We compare several attention models and find that global attention is a reasonable choice given the short source sequence length for the G2P task. We also show that a simple monotonic alignment-based local attention is also well-suited for this task. Exploration of additional local attention models may therefore be a good direction for future work.\nOur error analysis indicates that foreign names are one of the biggest remaining sources of error. This issue has been studied in some prior work [25], but has not been explicitly addressed in current state-of-the-art methods. It may therefore be helpful to consider extending approaches such as ours with joint models of word origin and pronunciation.\n7. REFERENCES\n[1] Stanley F. Chen, “Conditional and joint models for graphemeto-phoneme conversion,” in Eurospeech, 2003.\n[2] Maximilian Bisani and Hermann Ney, “Joint-sequence models for Grapheme-to-Phoneme Conversion,” Speech Commun., vol. 50, no. 5, pp. 434–451, May 2008.\n[3] Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif, “Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion,” in NAACL-HLT, 2007.\n[4] Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\n[5] Kanishka Rao, Fuchun Peng, Hasim Sak, and Françoise Beaufays, “Grapheme-to-phoneme conversion using Long ShortTerm Memory Recurrent Neural Networks,” in ICASSP, 2015.\n[6] Kaisheng Yao and Geoffrey Zweig, “Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion,” in Interspeech, 2015.\n[7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” CoRR, vol. abs/1409.0473, 2014.\n[8] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals, “Listen, Attend and Spell,” CoRR, vol. abs/1508.01211, 2015.\n[9] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio, “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,” in ICML, 2015.\n[10] Lucian Galescu and James F. Allen, “Bi-directional conversion between graphemes and phonemes using a joint n-gram model,” in 4th ITRW on Speech Synthesis, 2001.\n[11] Dong Wang and Simon King, “Letter-to-Sound Pronunciation Prediction using Conditional Random Fields,” IEEE Signal Process. Lett., vol. 18, no. 2, pp. 122–125, 2011.\n[12] Alex Graves, Santiago Fernández, and Faustino Gomez, “Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,” in ICML, 2006.\n[13] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, “Sequence to Sequence Learning with Neural Networks,” in NIPS, 2014.\n[14] Alex Graves, “Generating Sequences with Recurrent Neural Networks,” CoRR, vol. abs/1308.0850, 2013.\n[15] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals, “Recurrent Neural Network Regularization,” CoRR, vol. abs/1409.2329, 2014.\n[16] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton, “Grammar as a Foreign Language,” in NIPS, 2015.\n[17] Thang Luong, Hieu Pham, and Christopher D. Manning, “Effective Approaches to Attention-based Neural Machine Translation,” in EMNLP, 2015.\n[18] John Duchi, Elad Hazan, and Yoram Singer, “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” JMLR, vol. 12, pp. 2121–2159, July 2011.\n[19] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, “Dropout: a simple way to prevent neural networks from overfitting,” JMLR, vol. 15, no. 1, pp. 1929–1958, 2014.\n[20] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer, “Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks,” CoRR, vol. abs/1506.03099, 2015.\n[21] Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng, “TensorFlow: Large-scale machine learning on heterogeneous systems,” 2015.\n[22] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, “Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,” in EMNLP, 2014.\n[23] Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li, “Coverage-based neural machine translation,” CoRR, vol. abs/1601.04811, 2016.\n[24] Laurens van der Maaten and Geoffrey Hinton, “Visualizing data using t-SNE,” JMLR, vol. 9, no. Nov, pp. 2579–2605, 2008.\n[25] Sonjia Waxmonsky and Sravana Reddy, “G2P conversion of proper names using word origin information,” Stroudsburg, PA, USA, NAACL-HLT, 2012, pp. 367–371."
    } ],
    "references" : [ {
      "title" : "Conditional and joint models for graphemeto-phoneme conversion",
      "author" : [ "Stanley F. Chen" ],
      "venue" : "Eurospeech, 2003.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Joint-sequence models for Grapheme-to-Phoneme Conversion",
      "author" : [ "Maximilian Bisani", "Hermann Ney" ],
      "venue" : "Speech Commun., vol. 50, no. 5, pp. 434–451, May 2008.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion",
      "author" : [ "Sittichai Jiampojamarn", "Grzegorz Kondrak", "Tarek Sherif" ],
      "venue" : "NAACL-HLT, 2007.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Comput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Grapheme-to-phoneme conversion using Long Short- Term Memory Recurrent Neural Networks",
      "author" : [ "Kanishka Rao", "Fuchun Peng", "Hasim Sak", "Françoise Beaufays" ],
      "venue" : "ICASSP, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion",
      "author" : [ "Kaisheng Yao", "Geoffrey Zweig" ],
      "venue" : "Interspeech, 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neural Machine Translation by Jointly Learning to Align and Translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, vol. abs/1409.0473, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Listen, Attend and Spell",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals" ],
      "venue" : "CoRR, vol. abs/1508.01211, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio" ],
      "venue" : "ICML, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bi-directional conversion between graphemes and phonemes using a joint n-gram model",
      "author" : [ "Lucian Galescu", "James F. Allen" ],
      "venue" : "4th ITRW on Speech Synthesis, 2001.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Letter-to-Sound Pronunciation Prediction using Conditional Random Fields",
      "author" : [ "Dong Wang", "Simon King" ],
      "venue" : "IEEE Signal Process. Lett., vol. 18, no. 2, pp. 122–125, 2011.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez" ],
      "venue" : "ICML, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Sequence to Sequence Learning with Neural Networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Generating Sequences with Recurrent Neural Networks",
      "author" : [ "Alex Graves" ],
      "venue" : "CoRR, vol. abs/1308.0850, 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Recurrent Neural Network Regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals" ],
      "venue" : "CoRR, vol. abs/1409.2329, 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Grammar as a Foreign Language",
      "author" : [ "Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "NIPS, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Effective Approaches to Attention-based Neural Machine Translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning" ],
      "venue" : "EMNLP, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "JMLR, vol. 12, pp. 2121–2159, July 2011.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "JMLR, vol. 15, no. 1, pp. 1929–1958, 2014.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1929
    }, {
      "title" : "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "author" : [ "Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer" ],
      "venue" : "CoRR, vol. abs/1506.03099, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Martı́n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Mané", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : "2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "EMNLP, 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Coverage-based neural machine translation",
      "author" : [ "Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li" ],
      "venue" : "CoRR, vol. abs/1601.04811, 2016.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton" ],
      "venue" : "JMLR, vol. 9, no. Nov, pp. 2579–2605, 2008.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "G2P conversion of proper names using word origin information",
      "author" : [ "Sonjia Waxmonsky", "Sravana Reddy" ],
      "venue" : "Stroudsburg, PA, USA, NAACL-HLT, 2012, pp. 367–371.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A typical approach to G2P involves using joint sequence models, where the alignment is provided via some external aligner [1, 2, 3].",
      "startOffset" : 122,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "A typical approach to G2P involves using joint sequence models, where the alignment is provided via some external aligner [1, 2, 3].",
      "startOffset" : 122,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "A typical approach to G2P involves using joint sequence models, where the alignment is provided via some external aligner [1, 2, 3].",
      "startOffset" : 122,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "Specifically, long short-term memory (LSTM) networks [4] have recently been explored [5, 6].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Specifically, long short-term memory (LSTM) networks [4] have recently been explored [5, 6].",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "Specifically, long short-term memory (LSTM) networks [4] have recently been explored [5, 6].",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "When used in an encoder-decoder approach, as in [6], they in principle require no alignment between the input (grapheme",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "In this paper we explore an extension of encoder-decoder networks based on an attention mechanism, which has proven useful in other sequence prediction tasks [7, 8, 9].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "In this paper we explore an extension of encoder-decoder networks based on an attention mechanism, which has proven useful in other sequence prediction tasks [7, 8, 9].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "In this paper we explore an extension of encoder-decoder networks based on an attention mechanism, which has proven useful in other sequence prediction tasks [7, 8, 9].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : "Most previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10].",
      "startOffset" : 191,
      "endOffset" : 204
    }, {
      "referenceID" : 1,
      "context" : "Most previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10].",
      "startOffset" : 191,
      "endOffset" : 204
    }, {
      "referenceID" : 2,
      "context" : "Most previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10].",
      "startOffset" : 191,
      "endOffset" : 204
    }, {
      "referenceID" : 9,
      "context" : "Most previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10].",
      "startOffset" : 191,
      "endOffset" : 204
    }, {
      "referenceID" : 0,
      "context" : "The factored terms in the distribution can be modeled using a maximum entropy classifier [1], or the full product can be modeled as a conditional random field (CRF) [11].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "The factored terms in the distribution can be modeled using a maximum entropy classifier [1], or the full product can be modeled as a conditional random field (CRF) [11].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "The graphone sequence can be modeled via n-gram models [2, 10] or maximum entropy models [1].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "The graphone sequence can be modeled via n-gram models [2, 10] or maximum entropy models [1].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "The graphone sequence can be modeled via n-gram models [2, 10] or maximum entropy models [1].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "[5] use bidirectional LSTMs with a connectionist temporal classification (CTC) layer [12] which doesn’t require the data to be aligned.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[5] use bidirectional LSTMs with a connectionist temporal classification (CTC) layer [12] which doesn’t require the data to be aligned.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "Yao and Zweig [6] explore both an encoder-decoder architechture [13] and an input-output",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "Yao and Zweig [6] explore both an encoder-decoder architechture [13] and an input-output",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 12,
      "context" : "We briefly describe the encoder-decoder (“sequence-to-sequence”) approach, as proposed by [13].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "One of the important extensions of encoder-decoder models is the use of attention mechanism to adapt the context vector c for every output label prediction [7].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "We use the attention mechanism proposed by [16], where the context vector ct at time t is given by:",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "[15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "We consider two variants of local attention, based on [17], described below.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "One indirect way of addressing this potential problem is by concatenating the previous attention-weighted context vector with the input for the next decoder time step, as done in [17].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "For all three data sets, we use the same experimental setup as in [1], which we briefly describe below.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "We use minibatch stochastic gradient descent (SGD) together with Adam [18] using a minibatch size of 256.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "To prevent overfitting we: (a) introduce a dropout [19] layer between every pair of consecutive layers of the stacked LSTMs, and (b) use scheduled sampling [20], with a linear decay, on the decoder side.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "To prevent overfitting we: (a) introduce a dropout [19] layer between every pair of consecutive layers of the stacked LSTMs, and (b) use scheduled sampling [20], with a linear decay, on the decoder side.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "All of the models are implemented using TensorFlow [21].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "CMUDict BiDir LSTM + Alignment [6] 5.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "55 DBLSTM-CTC [5] 25.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "8 DBLSTM-CTC + 5-gram model [5] 21.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "Pronlex BiDir LSTM + Alignment [6] 6.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "NetTalk BiDir LSTM + Alignment [6] 7.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "For all three data sets, the best prior results to our knowledge with a single model are those of Yao and Zweig [6] with an alignment-based deep bidirectional LSTM.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "For CMUDict, a better WER was obtained by [5] by ensembling their CTC bidirectional LSTM with an alignment-based 5-gram model, but no corresponding PER was reported.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : "Although the assumption of monotonic alignment turns out to be too simplistic for other tasks, such as machine translation [17], it is a reasonable choice for G2P.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "The importance of LSTM units can be gauged by the performance drop when they are replaced by Gated Recurrent Units (GRUs) [22] in a reverse unidirectional global attention model.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "This problem has also been observed in neural machine translation, and there has been some recent work to address it [23].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "In order to confirm this, we plot in Figure 3 a two-dimensional visualization, generated with t-SNE [24], of the phone embeddings learned by the global attention model.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : "This issue has been studied in some prior work [25], but has not been explicitly addressed in current state-of-the-art methods.",
      "startOffset" : 47,
      "endOffset" : 51
    } ],
    "year" : 2016,
    "abstractText" : "We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk).",
    "creator" : "LaTeX with hyperref package"
  }
}