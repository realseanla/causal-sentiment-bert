{
  "name" : "1406.3339.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Algorithms for CVaR Optimization in MDPs",
    "authors" : [ "Yinlam Chow" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n33 39\nv3 [\ncs .A\nI] 1\n0 Ju"
    }, {
      "heading" : "1 Introduction",
      "text" : "A standard optimization criterion for an infinite horizon Markov decision process (MDP) is the expected sum of (discounted) costs (i.e., finding a policy that minimizes the value function of the initial state of the system). However in many applications, we may prefer to minimize some measure of risk in addition to this standard optimization criterion. In such cases, we would like to use a criterion that incorporates a penalty for the variability (due to the stochastic nature of the system) induced by a given policy. In risk-sensitive MDPs [18], the objective is to minimize a risk-sensitive criterion such as the expected exponential utility [18], a variance-related measure [32, 16], or the percentile performance [17]. The issue of how to construct such criteria in a manner that will be both conceptually meaningful and mathematically tractable is still an open question.\nAlthough most losses (returns) are not normally distributed, the typical Markiowitz mean-variance optimization [22], that relies on the first two moments of the loss (return) distribution, has dominated the risk management for over 50 years. Numerous alternatives to mean-variance optimization have emerged in the literature, but there is no clear leader amongst these alternative risk-sensitive objective functions. Value-atrisk (VaR) and conditional value-at-risk (CVaR) are two promising such alternatives\n∗Mohammad Ghavamzadeh is at Adobe Research, on leave of absence from INRIA Lille - Team SequeL.\nthat quantify the losses that might be encountered in the tail of the loss distribution, and thus, have received high status in risk management. For (continuous) loss distributions, while VaR measures risk as the maximum loss that might be incurred w.r.t. a given confidence level α, CVaR measures it as the expected loss given that the loss is greater or equal to VaRα. Although VaR is a popular risk measure, CVaR’s computational advantages over VaR has boosted the development of CVaR optimization techniques. We provide the exact definitions of these two risk measures and briefly discuss some of the VaR’s shortcomings in Section 2. CVaR minimization was first developed by Rockafellar and Uryasev [29] and its numerical effectiveness was demonstrated in portfolio optimization and option hedging problems. Their work was then extended to objective functions consist of different combinations of the expected loss and the CVaR, such as the minimization of the expected loss subject to a constraint on CVaR. This is the objective function that we study in this paper, although we believe that our proposed algorithms can be easily extended to several other CVaR-related objective functions. Boda and Filar [10] and Bäuerle and Ott [25, 4] extended the results of [29] to MDPs (sequential decision-making). While the former proposed to use dynamic programming (DP) to optimize CVaR, an approach that is limited to small problems, the latter showed that in both finite and infinite horizon MDPs, there exists a deterministic history-dependent optimal policy for CVaR optimization (see Section 3 for more details).\nMost of the work in risk-sensitive sequential decision-making has been in the context of MDPs (when the model is known) and much less work has been done within the reinforcement learning (RL) framework. In risk-sensitive RL, we can mention the work by Borkar [11, 12] who considered the expected exponential utility and those by Tamar et al. [34] and Prashanth and Ghavamzadeh [21] on several variance-related risk measures. CVaR optimization in RL is a rather novel subject. Morimura et al. [24] estimate the return distribution while exploring using a CVaR-based risk-sensitive policy. Their algorithm does not scale to large problems. Petrik and Subramanian [27] propose a method based on stochastic dual DP to optimize CVaR in large-scale MDPs. However, their method is limited to linearly controllable problems. Borkar and Jain [15] consider a finite-horizon MDP with CVaR constraint and sketch a stochastic approximation algorithm to solve it. Finally, Tamar et al. [35] have recently proposed a policy gradient algorithm for CVaR optimization.\nIn this paper, we develop policy gradient (PG) and actor-critic (AC) algorithms for mean-CVaR optimization in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then propose several methods to estimate this gradient both incrementally and using system trajectories (update at each time-step vs. update after observing one or more trajectories). We then use these gradient estimations to devise PG and AC algorithms that update the policy parameters in the descent direction. Using the ordinary differential equations (ODE) approach, we establish the asymptotic convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem. In comparison to [35], while they develop a PG algorithm for CVaR optimization in stochastic shortest path problems that only considers continuous loss distributions, uses a biased estimator for VaR, is not incremental, and has no convergence proof, here we study mean-CVaR optimization, consider both discrete and\ncontinuous loss distributions, devise both PG and (several) AC algorithms (trajectorybased and incremental – plus AC helps in reducing the variance of PG algorithms), and establish convergence proof for our algorithms."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We consider problems in which the agent’s interaction with the environment is modeled as a MDP. A MDP is a tuple M = (X ,A, C, P, P0), where X = {1, . . . , n} and A = {1, . . . ,m} are the state and action spaces; C(x, a) ∈ [−Cmax, Cmax] is the bounded cost random variable whose expectation is denoted by c(x, a) = E [ C(x, a) ] ; P (·|x, a) is the transition probability distribution; and P0(·) is the initial state distribution. For simplicity, we assume that the system has a single initial state x0, i.e., P0(x) = 1{x = x0}. All the results of the paper can be easily extended to the case that the system has more than one initial state. We also need to specify the rule according to which the agent selects actions at each state. A stationary policy µ(·|x) is a probability distribution over actions, conditioned on the current state. In policy gradient and actor-critic methods, we define a class of parameterized stochastic policies { µ(·|x; θ), x ∈ X , θ ∈ Θ ⊆ Rκ1 } , estimate the gradient of a performance measure w.r.t. the policy parameters θ from the observed system trajectories, and then improve the policy by adjusting its parameters in the direction of the gradient. Since in this setting a policy µ is represented by its κ1-dimensional parameter vector θ, policy dependent functions can be written as a function of θ in place of µ. So, we use µ and θ interchangeably in the paper. We denote by dµγ(x|x0) = (1 − γ) ∑∞ k=0 γ k P(xk = x|x0 = x0;µ) and πµγ (x, a|x0) = dµγ(x|x0)µ(a|x) the γ-discounted visiting distribution of state x and state-action pair (x, a) under policy µ, respectively.\nLet Z be a bounded-mean random variable, i.e., E[|Z|] < ∞, with the cumulative distribution function F (z) = P(Z ≤ z) (e.g., one may think of Z as the loss of an investment strategy µ). We define the value-at-risk at the confidence level α ∈ (0, 1) as VaRα(Z) = min { z | F (z) ≥ α } . Here the minimum is attained because F is nondecreasing and right-continuous in z. When F is continuous and strictly increasing, VaRα(Z) is the unique z satisfying F (z) = α, otherwise, the VaR equation can have no solution or a whole range of solutions. Although VaR is a popular risk measure, it suffers from being unstable and difficult to work with numerically when Z is not normally distributed, which is often the case as loss distributions tend to exhibit fat tails or empirical discreteness. Moreover, VaR is not a coherent risk measure [2] and more importantly does not quantify the losses that might be suffered beyond its value at the α-tail of the distribution [28]. An alternative measure that addresses most of the VaR’s shortcomings is conditional value-at-risk, CVARα(Z), which is the mean of the α-tail distribution of Z . If there is no probability atom at VaRα(Z), CVaRα(Z) has a unique value that is defined as CVaRα(Z) = E [ Z | Z ≥ VaRα(Z) ] . Rockafellar and Uryasev [29] showed that CVaRα(Z) = min\nν∈R Hα(Z, ν)\n△ = min\nν∈R\n{ ν + 1 1− αE [ (Z − ν)+ ]} . (1)\nNote that as a function of ν, Hα(·, ν) is finite and convex (hence continuous)."
    }, {
      "heading" : "3 CVaR Optimization in MDPs",
      "text" : "For a policy µ, we define the loss of a state x (state-action pair (x, a)) as the sum of (discounted) costs encountered by the agent when it starts at state x (state-action pair (x, a)) and then follows policy µ, i.e., Dθ(x) = ∑∞ k=0 γ\nkC(xk, ak) | x0 = x, µ and Dθ(x, a) = ∑∞ k=0 γ\nkC(xk, ak) | x0 = x, a0 = a, µ. The expected value of these two random variables are the value and action-value functions of policy µ, i.e., V θ(x) = E [ Dθ(x) ] and Qθ(x, a) = E [ Dθ(x, a) ] . The goal in the standard discounted formulation is to find an optimal policy θ∗ = argminθ V θ(x0).\nFor CVaR optimization in MDPs, we consider the following optimization problem: For a given confidence level α ∈ (0, 1) and loss tolerance β ∈ R,\nmin θ\nV θ(x0) subject to CVaRα ( Dθ(x0) ) ≤ β. (2)\nBy Theorem 16 in [28], the optimization problem (2) is equivalent to (Hα is defined by (1))\nmin θ,ν\nV θ(x0) subject to Hα ( Dθ(x0), ν ) ≤ β. (3)\nTo solve (3), we employ the Lagrangian relaxation procedure [5] to convert it to the following unconstrained problem:\nmax λ min θ,ν\n( L(θ, ν, λ) △ = V θ(x0) + λ ( Hα ( Dθ(x0), ν ) − β )) , (4)\nwhere λ is the Lagrange multiplier. The goal here is to find the saddle point of L(θ, ν, λ), i.e., a point (θ∗, ν∗, λ∗) that satisfies L(θ, ν, λ∗) ≥ L(θ∗, ν∗, λ∗) ≥ L(θ∗, ν∗, λ),∀θ, ν,∀λ > 0. This is achieved by descending in (θ, ν) and ascending in λ using the gradients of L(θ, ν, λ) w.r.t. θ, ν, and λ, i.e.,1\n∇θL(θ, ν, λ) = ∇θV θ(x0) + λ (1− α)∇θE [( Dθ(x0)− ν )+] , (5)\n∂νL(θ, ν, λ) = λ ( 1 +\n1 (1− α)∂νE [( Dθ(x0)− ν )+] ) ∋ λ ( 1− 1 (1− α)P ( Dθ(x0) ≥ ν )) ,\n(6)\n∇λL(θ, ν, λ) = ν + 1 (1− α)E [( Dθ(x0)− ν )+]− β. (7)\nWe assume that there exists a policy µ(·|·; θ) such that CVaRα ( Dθ(x0) ) ≤ β (feasibility assumption). As discussed in Section 1, Bäuerle and Ott [25, 4] showed that there exists a deterministic history-dependent optimal policy for CVaR optimization. The important point is that this policy does not depend on the complete history, but only on the current time step k, current state of the system xk, and accumulated discounted cost ∑k i=0 γ\nic(xi, ai). In the following, we present a policy gradient (PG) algorithm (Sec. 4) and several actor-critic (AC) algorithms (Sec. 5.5) to optimize (4). While the PG algorithm updates its parameters after observing several trajectories, the AC algorithms are incremental and update their parameters at each time-step.\n1The notation ∋ in (6) means that the right-most term is a member of the sub-gradient set ∂νL(θ, ν, λ)."
    }, {
      "heading" : "4 A Trajectory-based Policy Gradient Algorithm",
      "text" : "In this section, we present a policy gradient algorithm to solve the optimization problem (4). The unit of observation in this algorithm is a system trajectory generated by following the current policy. At each iteration, the algorithm generates N trajectories by following the current policy, use them to estimate the gradients in (5)-(7), and then use these estimates to update the parameters θ, ν, λ.\nLet ξ = {x0, a0, c0, x1, a1, c1, . . . , xT−1, aT−1, cT−1, xT } be a trajectory generated by following the policy θ, where x0 = x0 and xT is usually a terminal state of the system. After xk visits the terminal state, it enters a recurring sink state xR at the next time step, incurring zero cost, i.e., C(xR, a) = 0, ∀a ∈ A. Time index T is referred as the stopping time of the MDP. Since the transition is stochastic, T is a non-deterministic quantity. Here we assume that the policy µ is proper, i.e.,∑∞\nk=0 P(xk = x|x0 = x0, µ) < ∞ for every x 6∈ {xS , xT }. This further means that with probability 1, the MDP exits the transient states and hits xT (and stays in xS) in finite time T . For simplicity, we assume that the agent incurs zero cost in the terminal state. Analogous results for the general case with a non-zero terminal cost can be derived using identical arguments. The loss and probability of ξ are defined as D(ξ) = ∑T−1 k=0 γ kc(xk, ak) and Pθ(ξ) = P0(x0) ∏T−1\nk=0 µ(ak|xk; θ)P (xk+1|xk, ak), respectively. It can be easily shown that ∇θ logPθ(ξ) = ∑T−1 k=0 ∇θ logµ(ak|xk; θ).\nAlgorithm 1 contains the pseudo-code of our proposed policy gradient algorithm. What appears inside the parentheses on the right-hand-side of the update equations are the estimates of the gradients of L(θ, ν, λ) w.r.t. θ, ν, λ (estimates of (5)-(7)) (see Appendix A.2). ΓΘ is an operator that projects a vector θ ∈ Rκ1 to the closest point in a compact and convex set Θ ⊂ Rκ1 , and ΓN and ΓΛ are projection operators to [−Cmax1−γ , Cmax 1−γ ] and [0, λmax], respectively. These projection operators are necessary to ensure the convergence of the algorithm. The step-size schedules satisfy the standard conditions for stochastic approximation algorithms, and ensures that the VaR parameter ν update is on the fastest time-scale { ζ3(i) } , the policy parameter θ update is on the intermediate time-scale { ζ2(i) } , and the Lagrange multiplier λ update is on the slowest time-scale { ζ1(i) } (see Appendix A.1 for the conditions on the step-size schedules). This results in a three time-scale stochastic approximation algorithm. We prove that our policy gradient algorithm converges to a (local) saddle point of the risk-sensitive objective function L(θ, ν, λ) (see Appendix A.3)."
    }, {
      "heading" : "5 Incremental Actor-Critic Algorithms",
      "text" : "As mentioned in Section 4, the unit of observation in our policy gradient algorithm (Algorithm 1) is a system trajectory. This may result in high variance for the gradient estimates, especially when the length of the trajectories is long. To address this issue, in this section, we propose actor-critic algorithms that use linear approximation for some quantities in the gradient estimates and update the parameters incrementally (after each state-action transition). To develop our actor-critic algorithms, we should show how the gradients of (5)-(7) are estimated in an incremental fashion. We show this in the next four subsections, followed by a subsection that contains the algorithms.\nAlgorithm 1 Trajectory-based Policy Gradient Algorithm for CVaR Optimization Input: parameterized policy µ(·|·; θ), confidence level α, loss tolerance β and Lagrangian threshold λmax Initialization: policy parameter θ = θ0, VaR parameter ν = ν0, and the Lagrangian parameter λ = λ0 while 1 do\nfor i = 0, 1, 2, . . . do for j = 1, 2, . . . do\nGenerate N trajectories {ξj,i}Nj=1 by starting at x0 = x0 and following the current policy θi.\nend for\nν Update: νi+1 = ΓN [ νi − ζ3(i) ( λi −\nλi (1− α)N\nN∑\nj=1\n1 { D(ξj,i) ≥ νi\n})]\nθ Update: θi+1 = ΓΘ [ θi − ζ2(i) ( 1\nN\nN∑\nj=1\n∇θ log Pθ(ξj,i)|θ=θiD(ξj,i)\n+ λi\n(1− α)N\nN∑\nj=1\n∇θ log Pθ(ξj,i)|θ=θi ( D(ξj,i)− νi ) 1 { D(ξj,i) ≥ νi\n})]\nλ Update: λi+1 = ΓΛ [ λi + ζ1(i) ( νi − β + 1\n(1− α)N\nN∑\nj=1\n( D(ξj,i)− νi ) 1 { D(ξj,i) ≥ νi\n})]\nend for if {λi} converges to λmax then\nSet λmax ← 2λmax. else\nreturn parameters ν, θ, λ and break end if\nend while"
    }, {
      "heading" : "5.1 Gradient w.r.t. the Policy Parameters θ",
      "text" : "The gradient of our objective function w.r.t. the policy parameters θ in (5) may be rewritten as\n∇θL(θ, ν, λ) = ∇θ ( E [ Dθ(x0) ] +\nλ (1− α)E [( Dθ(x0)− ν )+] ) . (8)\nGiven the original MDP M = (X ,A, C, P, P0) and the parameter λ, we define the augmented MDP M̄ = (X̄ , Ā, C̄, P̄ , P̄0) as X̄ = X × R, Ā = A, P̄0(x, s) = P0(x)1{s = s0}, and\nC̄(x, s, a) =\n{ λ(−s)+/(1− α) if x = xT\nC(x, a) otherwise , P̄ (x′, s′|x, s, a) =\n{ P (x′|x, a) if s′ = ( s−C(x, a) ) /γ\n0 otherwise\nwhere xT is any terminal state of the original MDP M and sT is the value of the s part of the state when a policy θ reaches a terminal state xT after T steps, i.e., sT = 1 γT ( s0 − ∑T−1k=0 γkC(xk, ak) ) . We define a class of parameterized stochastic\npolicies { µ(·|x, s; θ), (x, s) ∈ X̄ , θ ∈ Θ ⊆ Rκ1 } for this augmented MDP. Thus, the total (discounted) loss of this trajectory can be written as\nT−1∑\nk=0\nγkC(xk, ak) + γ T C̄(xT , sT , a) = D θ(x0) + λ (1− α) ( Dθ(x0)− s0 )+ . (9)\nFrom (9), it is clear that the quantity in the parenthesis of (8) is the value function of the policy θ at state (x0, s0 = ν) in the augmented MDP M̄, i.e., V θ(x0, ν). Thus, it is easy to show that (the proof of the second equality can be found in the literature, e.g., [26])\n∇θL(θ, ν, λ) = ∇θV θ(x0, ν) = 1 1− γ ∑\nx,s,a\nπθγ(x, s, a|x0, ν) ∇ log µ(a|x, s; θ) Qθ(x, s, a),\n(10)\nwhere πθγ is the discounted visiting distribution (defined in Section 2) and Q θ is the action-value function of policy θ in the augmented MDP M̄. We can show that 1 1−γ∇ logµ(ak|xk, sk; θ) · δk is an unbiased estimate of ∇θL(θ, ν, λ), where δk = C̄(xk, sk, ak) + γV̂ (xk+1, sk+1)− V̂ (xk, sk) is the temporal-difference (TD) error in M̄, and V̂ is an unbiased estimator of V θ (see e.g. [8]). In our actor-critic algorithms, the critic uses linear approximation for the value function V θ(x, s) ≈ v⊤φ(x, s) = Ṽ θ,v(x, s), where the feature vector φ(·) is from low-dimensional space Rκ2 ."
    }, {
      "heading" : "5.2 Gradient w.r.t. the Lagrangian Parameter λ",
      "text" : "We may rewrite the gradient of our objective function w.r.t. the Lagrangian parameters λ in (7) as ∇λL(θ, ν, λ) = ν−β+∇λ ( E [ Dθ(x0) ] + λ\n(1− α)E [( Dθ(x0)− ν )+] ) (a) = ν−β+∇λV θ(x0, ν).\n(11) Similar to Section 5.1, (a) comes from the fact that the quantity in the parenthesis in (11) is V θ(x0, ν), the value function of the policy θ at state (x0, ν) in the augmented MDP M̄. Note that the dependence of V θ(x0, ν) on λ comes from the definition of the cost function C̄ in M̄. We now derive an expression for ∇λV θ(x0, ν), which in turn will give us an expression for ∇λL(θ, ν, λ).\nLemma 1 The gradient of V θ(x0, ν) w.r.t. the Lagrangian parameter λ may be written as\n∇λV θ(x0, ν) = 1 1− γ ∑\nx,s,a\nπθγ(x, s, a|x0, ν) 1\n(1− α)1{x = xT }(−s) +. (12)\nProof. See Appendix B.2. From Lemma 1 and (11), it is easy to see that ν−β+ 1(1−γ)(1−α)1{x = xT }(−s)+ is an unbiased estimate of ∇λL(θ, ν, λ). An issue with this estimator is that its value is fixed to νk − β all along a system trajectory, and only changes at the end to νk − β + 1 (1−γ)(1−α)(−sT )+. This may affect the incremental nature of our actor-critic algorithm. To address this issue, we propose a different approach to estimate the gradients w.r.t. θ and λ in Sec. 5.4 (of course this does not come for free).\nAnother important issue is that the above estimator is unbiased only if the samples are generated from the distribution πθγ(·|x0, ν). If we just follow the policy, then we may use νk −β+ γ k\n(1−α)1{xk = xT }(−sk)+ as an estimate for ∇λL(θ, ν, λ) (see (20) and (22) in Algorithm 2). Note that this is an issue for all discounted actor-critic algorithms that their (likelihood ratio based) estimate for the gradient is unbiased only if the samples are generated from πθγ , and not just when we simply follow the policy. Although this issue was known in the community, there is a recent paper that investigates it in details [36]. Moreover, this might be a main reason that we have no convergence analysis (to the best of our knowledge) for (likelihood ratio based) discounted actorcritic algorithms.2"
    }, {
      "heading" : "5.3 Sub-Gradient w.r.t. the VaR Parameter ν",
      "text" : "We may rewrite the sub-gradient of our objective function w.r.t. the VaR parameters ν in (6) as\n∂νL(θ, ν, λ) ∋ λ ( 1− 1 (1− α)P ( ∞∑\nk=0\nγkC(xk, ak) ≥ ν | x0 = x0; θ )) . (13)\nFrom the definition of the augmented MDP M̄, the probability in (13) may be written as P(sT ≤ 0 | x0 = x0, s0 = ν; θ), where sT is the s part of the state in M̄ when we reach a terminal state, i.e., x = xT (see Section 5.1). Thus, we may rewrite (13) as\n∂νL(θ, ν, λ) ∋ λ ( 1− 1 (1− α)P ( sT ≤ 0 | x0 = x0, s0 = ν; θ )) . (14)\nFrom (14), it is easy to see that λ−λ1{sT ≤ 0}/(1−α) is an unbiased estimate of the sub-gradient of L(θ, ν, λ) w.r.t. ν. An issue with this (unbiased) estimator is that it can be only applied at the end of a system trajectory (i.e., when we reach the terminal state xT ), and thus, using it prevents us of having a fully incremental algorithm. In fact, this is the estimator that we use in our semi trajectory-based actor-critic algorithm (see (21) in Algorithm 2).\nOne approach to estimate this sub-gradient incrementally, hence having a fully incremental algorithm, is to use simultaneous perturbation stochastic approximation (SPSA) method [9]. The idea of SPSA is to estimate the sub-gradient g(ν) ∈ ∂νL(θ, ν, λ) using two values of g at ν− = ν − ∆ and ν+ = ν + ∆, where ∆ > 0 is a positive\n2Note that the discounted actor-critic algorithm with convergence proof in [6] is based on SPSA.\nperturbation (see Sec. 5.5 for the detailed description of ∆).3 In order to see how SPSA can help us to estimate our sub-gradient incrementally, note that\n∂νL(θ, ν, λ) = λ+ ∂ν ( E [ Dθ(x0) ] +\nλ (1− α)E [( Dθ(x0)− ν )+] ) (a) = λ+ ∂νV θ(x0, ν).\n(15)\nSimilar to Sections 5.1 and 5.2, (a) comes from the fact that the quantity in the parenthesis in (15) is V θ(x0, ν), the value function of the policy θ at state (x0, ν) in the augmented MDP M̄. Since the critic uses a linear approximation for the value function, i.e., V θ(x, s) ≈ v⊤φ(x, s), in our actor-critic algorithms (see Section 5.1 and Algorithm 2), the SPSA estimate of the sub-gradient would be of the form g(ν) ≈ λ+ v⊤ [ φ(x0, ν+)− φ(x0, ν−) ] /2∆ (see (18) in Algorithm 2)."
    }, {
      "heading" : "5.4 An Alternative Approach to Compute the Gradients",
      "text" : "In this section, we present an alternative way to compute the gradients, especially those w.r.t. θ and λ. This allows us to estimate the gradient w.r.t. λ in a (more) incremental fashion (compared to the method of Section 5.2), with the cost of the need to use two different linear function approximators (instead of one used in Algorithm 2). In this approach, we define the augmented MDP slightly different than the one in Section 5.2. The only difference is in the definition of the cost function, which is defined here as (note that C(x, a) has been replaced by 0 and λ has been removed)\nC̄(x, s, a) =\n{ (−s)+/(1− α) if x = xT ,\n0 otherwise, where xT is any terminal state of the original MDP M. It is easy to see that the term\n1 (1−α)E\n[( Dθ(x0)− ν )+] appearing in the gradients of (5)-(7) is the value function of\nthe policy θ at state (x0, ν) in this augmented MDP. As a result, we have Gradient w.r.t. θ: It is easy to see that now this gradient (5) is the gradient of the value function of the original MDP, ∇θV θ(x0), plus λ times the gradient of the value function of the augmented MDP, ∇θV θ(x0, ν), both at the initial states of these MDPs (with abuse of notation, we use V for the value function of both MDPs). Thus, using linear approximators u⊤f(x, s) and v⊤φ(x, s) for the value functions of the original and augmented MDPs, ∇θL(θ, ν, λ) can be estimated as ∇θ logµ(ak|xk, sk; θ) · (ǫk + λδk), where ǫk and δk are the TD-errors of these MDPs. Gradient w.r.t. λ: Similar to the case for θ, it is easy to see that this gradient (7) is ν − β plus the value function of the augmented MDP, V θ(x0, ν), and thus, can be estimated incrementally as ∇λL(θ, ν, λ) ≈ ν − β + v⊤φ(x, s). Sub-Gradient w.r.t. ν: This sub-gradient (6) is λ times one plus the gradient w.r.t. ν of the value function of the augmented MDP, ∇νV θ(x0, ν), and thus using SPSA, can be estimated incrementally as λ ( 1 + v⊤ [ φ(x0,ν+)−φ(x0,ν−) ] 2∆ ) . Algorithm 3 in Appendix B.3 contains the pseudo-code of the resulting algorithm. 3SPSA-based gradient estimate was first proposed in [33] and has been widely used in various settings, especially those involving high-dimensional parameter. The SPSA estimate described above is two-sided. It can also be implemented single-sided, where we use the values of the function at ν and ν+. We refer the readers to [9] for more details on SPSA and to [21] for its application in learning in risk-sensitive MDPs."
    }, {
      "heading" : "5.5 Actor-Critic Algorithms",
      "text" : "In this section, we present two actor-critic algorithms for optimizing the risk-sensitive measure (4). These algorithms are based on the gradient estimates of Sections 5.1-5.3. While the first algorithm (SPSA-based) is fully incremental and updates all the parameters θ, ν, λ at each time-step, the second one updates θ at each time-step and updates ν and λ only at the end of each trajectory, thus given the name semi trajectory-based. Algorithm 2 contains the pseudo-code of these algorithms. The projection operators ΓΘ, ΓN , and ΓΛ are defined as in Section 4 and are necessary to ensure the convergence of the algorithms. The step-size schedules satisfy the standard conditions for stochastic approximation algorithms, and ensures that the critic update is on the fastest time-scale{ ζ4(k) } , the policy and VaR parameter updates are on the intermediate time-scale, with ν-update { ζ3(k) } being faster than θ-update { ζ2(k) } , and finally the Lagrange multiplier update is on the slowest time-scale { ζ1(k) } (see Appendix B.1 for the conditions on these step-size schedules). This results in four time-scale stochastic approximation algorithms. We prove that these actor-critic algorithms converge to a (local) saddle point of the risk-sensitive objective function L(θ, ν, λ) (see Appendix B.4)."
    }, {
      "heading" : "6 Experimental Results",
      "text" : "We consider an optimal stopping problem in which the state at each time step k ≤ T consists of the cost ck and time k, i.e., x = (ck, k), where T is the stopping time. The agent (buyer) should decide either to accept the present cost or wait. If she accepts or when k = T , the system reaches a terminal state and the cost ck is received, otherwise, she receives the cost ph and the new state is (ck+1, k + 1), where ck+1 is fuck w.p. p and fdck w.p. 1−p (fu > 1 and fd < 1 are constants). Moreover, there is a discounted factor γ ∈ (0, 1) to account for the increase in the buyer’s affordability. The problem has been described in more details in Appendix C. Note that if we change cost to reward and minimization to maximization, this is exactly the American option pricing problem, a standard testbed to evaluate risk-sensitive algorithms (e.g., [34]). Since the state space is continuous, solving for an exact solution via DP is infeasible, and thus, it requires approximation and sampling techniques.\nWe compare the performance of our risk-sensitive policy gradient Alg. 1 (PGCVaR) and two actor-critic Algs. 2 (AC-CVaR-SPSA,AC-CVaR-Semi-Traj) with their riskneutral counterparts (PG and AC) (see Appendix C for the details of these experiments). Fig. 1 shows the distribution of the discounted cumulative cost Dθ(x0) for the policy θ learned by each of these algorithms. From left to right, the columns display the first two moments, the whole (distribution), and zoom on the right-tail of these distributions. The results indicate that the risk-sensitive algorithms yield a higher expected loss, but less variance, compared to the risk-neutral methods. More precisely, the loss distributions of the risk-sensitive algorithms have lower right-tail than their risk-neutral counterparts. Table 1 summarizes the performance of these algorithms. The numbers reiterate what we concluded from Fig. 1.\nAlgorithm 2 Actor-Critic Algorithm for CVaR Optimization Input: Parameterized policy µ(·|·; θ) and value function feature vector φ(·) (both over the augmented MDP M̄), confidence level α, loss tolerance β and Lagrangian threshold λmax Initialization: policy parameters θ = θ0; VaR parameter ν = ν0; Lagrangian parameter λ = λ0; value function weight vector v = v0 while 1 do\n// (1) SPSA-based Algorithm: for k = 0, 1, 2, . . . do\nDraw action ak ∼ µ(·|xk, sk; θk); Observe cost C̄(xk, sk, ak); Observe next state (xk+1, sk+1) ∼ P̄ (·|xk, sk, ak); // note that sk+1 = (sk − C ( xk, ak) ) /γ (see Sec. 5.1)\nTD Error: δk(vk) = C̄(xk, sk, ak) + γv ⊤ k φ(xk+1, sk+1)− v⊤k φ(xk, sk)\n(16)\nCritic Update: vk+1 = vk + ζ4(k)δk(vk)φ(xk, sk) (17)\nActor Updates: νk+1 = ΓN ( νk − ζ3(k) ( λk + v⊤k [ φ ( x0, νk +∆k ) − φ(x0, νk −∆k) ]\n2∆k\n))\n(18)\nθk+1 = ΓΘ ( θk − ζ2(k) 1− γ∇θ log µ(ak|xk, sk; θ)|θ=θk · δk(vk) )\n(19)\nλk+1 = ΓΛ ( λk + ζ1(k) ( νk − β + γk 1− α1{xk = xT }(−sk) + ))\n(20)\nend for // (2) Semi Trajectory-based Algorithm: for i = 0, 1, 2, . . . do\nSet k = 0 and (xk, sk) = (x0, νi) while xk 6= xT do\nDraw action ak ∼ µ(·|xk, sk; θk); Observe C̄(xk, sk, ak) and (xk+1, sk+1) ∼ P̄ (·|xk, sk, ak) For fixed values of νi and λi, execute (16)-(17) and (19) with (ζ4(k), ζ2(k)) replaced by (ζ4(i), ζ2(i)); k ← k + 1;\nend while // we reach a terminal state (xT , sT ) (end of the trajectory)\nν Update: νi+1 = ΓN ( νi − ζ2(i) ( λi −\nλi 1− α1\n{ sT ≤ 0 })) (21)\nλ Update: λi+1 = ΓΛ ( λi + ζ1(i) ( νi − β + γ T (1− α) (−sT ) +)) (22)\nend for if {λi} converges to λmax then\nSet λmax ← 2λmax. else\nreturn policy and value function parameters v, ν, θ, λ and break end if\nend while"
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "We proposed novel policy gradient and actor critic (AC) algorithms for CVaR optimization in MDPs. We provided proofs of convergence (in the appendix) to locally risk-sensitive optimal policies for the proposed algorithms. Further, using an optimal stopping problem, we observed that our algorithms resulted in policies whose loss distributions have lower right-tail compared to their risk-neutral counterparts. This is extremely important for a risk averse decision-maker, especially if the right-tail contains catastrophic losses. Future work includes: 1) Providing convergence proofs for our AC algorithms when the samples are generated by following the policy and not from its discounted visiting distribution (this can be wasteful in terms of samples), 2) Here we established asymptotic limits for our algorithms. To the best of our knowledge, there are no convergence rate results available for multi-timescale stochastic approximation schemes, and hence, for AC algorithms. This is true even for the AC algorithms that\ndo not incorporate any risk criterion. It would be an interesting research direction to obtain finite-time bounds on the quality of the solution obtained by these algorithms, 3) Since interesting losses in the CVaR optimization problems are those that exceed the VaR, in order to compute more accurate estimates of the gradients, it is necessary to generate more samples in the right-tail of the loss distribution (events that are observed with a very low probability). Although importance sampling methods have been used to address this problem [3, 35], several issues, particularly related to the choice of the sampling distribution, have remained unsolved that are needed to be investigated, and finally, 4) Evaluating our algorithms in more challenging problems."
    }, {
      "heading" : "A Technical Details of the Trajectory-based Policy Gradient Algorithm",
      "text" : ""
    }, {
      "heading" : "A.1 Assumptions",
      "text" : "We make the following assumptions for the step-size schedules in our algorithms:\n(A1) For any state-action pair (x, a), µ(a|x; θ) is continuously differentiable in θ and ∇θµ(a|x; θ) is a Lipschitz function in θ for every a ∈ A and x ∈ X .\n(A2) The Markov chain induced by any policy θ is irreducible and aperiodic.\n(A3) The step size schedules {ζ3(i)}, {ζ2(i)}, and {ζ1(i)} satisfy\n∑\ni\nζ1(i) = ∑\ni\nζ2(i) = ∑\ni ζ3(i) = ∞, (23) ∑\ni\nζ1(i) 2,\n∑\ni\nζ2(i) 2,\n∑\ni\nζ3(i) 2 < ∞, (24)\nζ1(i) = o ( ζ2(i) ) , ζ2(i) = o ( ζ3(i) ) . (25)\n(23) and (24) are standard step-size conditions in stochastic approximation algorithms, and (25) indicates that the update corresponds to {ζ3(i)} is on the fastest timescale, the update corresponds to {ζ2(i)} is on the intermediate time-scale, and the update corresponds to {ζ1(i)} is on the slowest time-scale."
    }, {
      "heading" : "A.2 Computing the Gradients",
      "text" : "i) ∇θL(θ, ν, λ): Gradient of L(θ, ν, λ) w.r.t. θ By expanding the expectations in the definition of the objective function L(θ, ν, λ) in (4), we obtain\nL(θ, ν, λ) = ∑\nξ\nPθ(ξ)D(ξ) + λν + λ 1− α ∑\nξ\nPθ(ξ) ( D(ξ) − ν )+ − λβ.\nBy taking gradient with respect to θ, we have\n∇θL(θ, ν, λ) = ∑\nξ\n∇θPθ(ξ)D(ξ) + λ 1− α ∑\nξ\n∇θPθ(ξ) ( D(ξ)− ν )+ .\nThis gradient can rewritten as\n∇θL(θ, ν, λ) = ∑\nξ\nPθ(ξ)·∇θ logPθ(ξ) ( D(ξ) + λ\n1− α ( D(ξ) − ν ) 1 { D(ξ) ≥ ν\n}) ,\n(26)\nwhere\n∇θ logPθ(ξ) =∇θ { T−1∑\nk=0\nlogP (xk+1|xk, ak) + logµ(ak|xk; θ) + log 1{x0 = x0} }\n= T−1∑\nk=0\n1\nµ(ak|xk; θ) ∇θµ(ak|xk; θ)\n=\nT−1∑\nk=0\n∇θ log µ(ak|xk; θ).\nii) ∂νL(θ, ν, λ): Sub-differential of L(θ, ν, λ) w.r.t. ν From the definition of L(θ, ν, λ), we can easily see that L(θ, ν, λ) is a convex\nfunction in ν for any fixed θ ∈ Θ. Note that for every fixed ν and any ν′, we have ( D(ξ)− ν′ )+ − ( D(ξ)− ν )+ ≥ g · (ν′ − ν),\nwhere g is any element in the set of sub-derivatives:\ng ∈ ∂ν ( D(ξ)− ν )+ △ =    −1 if ν < D(ξ), −q : q ∈ [0, 1] if ν = D(ξ), 0 otherwise.\nSince L(θ, ν, λ) is finite-valued for any ν ∈ R, by the additive rule of sub-derivatives, we have\n∂νL(θ, ν, λ) =   − λ 1− α ∑\nξ\nPθ(ξ)1 { D(ξ) > ν } − λq 1− α ∑\nξ\nPθ(ξ)1 { D(ξ) = ν } + λ | q ∈ [0, 1]    .\n(27) In particular for q = 1, we may write the sub-gradient of L(θ, ν, λ) w.r.t. ν as\n∂νL(θ, ν, λ)|q=0 = λ− λ 1− α\n∑\nξ\nPθ(ξ)·1 { D(ξ) ≥ ν } or λ− λ 1− α ∑\nξ\nPθ(ξ)·1 { D(ξ) ≥ ν } ∈ ∂νL(θ, ν, λ).\niii) ∇λL(θ, ν, λ): Gradient of L(θ, ν, λ) w.r.t. λ Since L(θ, ν, λ) is a linear function in λ, obviously one can express the gradient of\nL(θ, ν, λ) w.r.t. λ as follows:\n∇λL(θ, ν, λ) = ν − β + 1 1− α ∑\nξ\nPθ(ξ) · ( D(ξ) − ν ) 1 { D(ξ) ≥ ν } . (28)"
    }, {
      "heading" : "A.3 Proof of Convergence of the Policy Gradient Algorithm",
      "text" : "In this section, we prove the convergence of our policy gradient algorithm (Algorithm 1).\nTheorem 2 Suppose λ∗ ∈ [0, λmax). Then the sequence of (θ, λ)−updates in Algorithm 1 converges to a (local) saddle point (θ∗, ν∗, λ∗) of our objective function L(θ, ν, λ) almost surely, i.e., it satisfies L(θ, ν, λ∗) ≥ L(θ∗, ν∗, λ∗) ≥ L(θ∗, ν∗, λ), ∀(θ, ν) ∈ Θ × [−Cmax1−γ , Cmax 1−γ ] ∩ B(θ∗,ν∗)(r) for some r > 0 and ∀λ ∈ [0, λmax]. Note that B(θ∗,ν∗)(r) represents a hyper-dimensional ball centered at (θ∗, ν∗) with radius r.\nSince ν converges on the faster timescale than θ and λ, the ν-update can be rewritten by assuming (θ, λ) as invariant quantities, i.e.,\nνi+1 = ΓN [ νi − ζ3(i) ( λ− λ\n(1 − α)N\nN∑\nj=1\n1 { D(ξj,i) ≥ νi })] . (29)\nConsider the continuous time dynamics of ν defined using differential inclusion\nν̇ ∈ Υν [−g(ν)] , ∀g(ν) ∈ ∂νL(θ, ν, λ), (30)\nwhere\nΥν [K(ν)] := lim 0<η→0 ΓN (ν + ηK(ν))− ΓN (ν) η .\nand ΓN is the Euclidean projection operator of ν to [−Cmax1−γ , Cmax 1−γ ], i.e., ΓN (ν) = argmin ν̂∈[−Cmax1−γ , Cmax 1−γ ] 1 2‖ν− ν̂‖22. In general ΓN (ν) is not necessarily differentiable. Υν [K(ν)] is the left directional derivative of the function ΓN (ν) in the direction of K(ν). By using the left directional derivative Υν [−g(ν)] in the sub-gradient descent algorithm for ν, the gradient will point at the descent direction along the boundary of ν whenever the ν−update hits its boundary.\nFurthermore, since ν converges on the faster timescale than θ, and λ is on the slowest time-scale, the θ-update can be rewritten using the converged ν∗(θ) and assuming λ as an invariant quantity, i.e.,\nθi+1 =ΓΘ [ θi − ζ2(i) ( 1\nN\nN∑\nj=1\n∇θ logPθ(ξj,i)|θ=θiD(ξj,i)\n+ λ\n(1 − α)N\nN∑\nj=1\n∇θ logPθ(ξj,i)|θ=θi ( D(ξj,i)− ν ) 1 { D(ξj,i) ≥ ν∗(θi) })] .\nConsider the continuous time dynamics of θ ∈ Θ:\nθ̇ = Υθ [−∇θL(θ, ν, λ)] |ν=ν∗(θ), (31)\nwhere\nΥθ[K(θ)] := lim 0<η→0 ΓΘ(θ + ηK(θ))− ΓΘ(θ) η .\nand ΓΘ is the Euclidean projection operator of θ to Θ, i.e., ΓΘ(θ) = argminθ̂∈Θ 1 2‖θ− θ̂‖22. Similar to the analysis of ν, Υθ[K(θ)] is the left directional derivative of the\nfunction ΓΘ(θ) in the direction of K(θ). By using the left directional derivative Υθ [−∇θL(θ, ν, λ)] in the gradient descent algorithm for θ, the gradient will point at the descent direction along the boundary of Θ whenever the θ−update hits its boundary.\nFinally, since λ-update converges in a slowest time-scale, the λ-update can be rewritten using the converged θ∗(λ) and ν∗(λ), i.e.,\nλi+1 = ΓΛ  λi + ζ1(i) ( ν∗(λi) + 1\n1− α 1 N\nN∑\nj=1\n( D(ξj,i)− ν∗(λi) )+ − β )  .\n(32) Consider the continuous time system\nλ̇(t) = Υλ [ ∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ) ] , λ(t) ≥ 0, (33)\nwhere\nΥλ[K(λ)] := lim 0<η→0\nΓΛ ( λ+ ηK(λ) ) − ΓΛ(λ)\nη .\nandΓΛ is the Euclidean projection operator of λ to [0, λmax], i.e., ΓΛ(λ) = argminλ̂∈[0,λmax] 1 2‖λ− λ̂‖22. Similar to the analysis of (ν, θ), Υλ[K(λ)] is the left directional derivative of the function ΓΛ(λ) in the direction of K(λ). By using the left directional derivative Υλ [∇λL(θ, ν, λ)] in the gradient ascent algorithm for λ, the gradient will point at the ascent direction along the boundary of [0, λmax] whenever the λ−update hits its boundary.\nDefine L∗(λ) = L(θ∗(λ), ν∗(λ), λ),\nfor λ ≥ 0 where (θ∗(λ), ν∗(λ)) ∈ Θ× [−Cmax1−γ , Cmax 1−γ ] is a local minimum of L(θ, ν, λ) for fixed λ ≥ 0, i.e., L(θ, ν, λ) ≥ L(θ∗(λ), ν∗(λ), λ) for any (θ, ν) ∈ Θ×[−Cmax1−γ , Cmax 1−γ ]∩ B(θ∗(λ),ν∗(λ))(r) for some r > 0. Next, we want to show that the ODE (33) is actually a gradient ascent of the Lagrangian function using the envelope theorem in mathematical economics [23]. The envelope theorem describes sufficient conditions for the derivative of L∗ with respect to λ where it equals to the partial derivative of the objective function L with respect to λ, holding (θ, ν) at its local optimum (θ, ν) = (θ∗(λ), ν∗(λ)). We will show that ∇λL∗(λ) coincides with with ∇λL(θ, ν, λ)|θ=θ∗(λ),ν=ν∗(λ) as follows.\nTheorem 3 The value function L∗ is absolutely continuous. Furthermore,\nL∗(λ) = L∗(0) +\n∫ λ\n0\n∇λ′L(θ, ν, λ′) ∣∣∣ θ=θ∗(s),ν=ν∗(s),λ′=s ds, λ ≥ 0. (34)\nProof. The proof follows from analogous arguments of Lemma 4.3 in [13]. From the definition of L∗, observe that for any λ′, λ′′ ≥ 0 with λ′ < λ′′, |L∗(λ′′)− L∗(λ′)| ≤ sup\nθ∈Θ,ν∈[−Cmax 1−γ , Cmax 1−γ ]\n|L(θ, ν, λ′′)− L(θ, ν, λ′)|\n= sup θ∈Θ,ν∈[−Cmax\n1−γ , Cmax 1−γ ]\n∣∣∣∣∣ ∫ λ′′ λ′ ∇λL(θ, ν, s)ds ∣∣∣∣∣\n≤ ∫ λ′′\nλ′ sup\nθ∈Θ,ν∈[−Cmax 1−γ , Cmax 1−γ ]\n|∇λL(θ, ν, s)| ds ≤ 3Cmax\n(1− α)(1− γ) (λ ′′ − λ′).\nThis implies that L∗ is absolutely continuous. Therefore, L∗ is continuous everywhere and differentiable almost everywhere.\nBy the Milgrom-Segal envelope theorem of mathematical economics (Theorem 1 of [23]), one can conclude that the derivative of L∗(λ) coincides with the derivative of L(θ, ν, λ) at the point of differentiability λ and θ = θ∗(λ), ν = ν∗(λ). Also since L∗ is absolutely continuous, the limit of (L∗(λ) − L∗(λ′))/(λ − λ′) at λ ↑ λ′ (or λ ↓ λ′) coincides with the lower/upper directional derivatives if λ′ is a point of nondifferentiability. Thus, there is only a countable number of non-differentiable points in L∗ and each point of non-differentiability has the same directional derivatives as the point slightly beneath (in the case of λ ↓ λ′) or above (in the case of λ ↑ λ′) it. As the set of non-differentiable points of L∗ has measure zero, it can then be interpreted that ∇λL∗(λ) coincides with ∇λL(θ, ν, λ)|θ=θ∗(λ),ν=ν∗(λ), i.e., expression (34) holds. Remark 1 It can be easily shown that L∗(λ) is a concave function. Since for given θ and ν, L(θ, ν, λ) is a linear function in λ. Therefore, for any α′ ∈ [0, 1], α′L∗(λ1) + (1−α′)L∗(λ2) ≤ L∗(α′λ1+(1−α′)λ2), i.e., L∗(λ) is a concave function. Concavity of L∗ implies that it is continuous and directionally (both left hand and right hand) differentiable in int dom(L∗). Furthermore at any λ = λ̃ such that the derivative of L(θ, ν, λ) with respect of λ at θ = θ∗(λ), ν = ν∗(λ) exists, by Theorem 1 of [23], ∇λL∗(λ)|λ=λ̃+ = (L\n∗(λ̃+)−L∗(λ̃))/(λ̃+−λ̃) ≥ ∇λL(θ, ν, λ)|θ=θ∗(λ),ν=ν∗(λ),λ=λ̃ ≥ (L∗(λ̃−) − L∗(λ̃))/(λ̃− − λ̃) = ∇λL∗(λ)|λ=λ̃− . Furthermore concavity of L\n∗ implies ∇λL∗(λ)|λ=λ̃+ ≤ ∇λL\n∗(λ)|λ=λ̃− . Combining these arguments, one obtains ∇λL∗(λ)|λ=λ̃+ = ∇λL(θ, ν, λ)|θ=θ∗(λ),ν=ν∗(λ),λ=λ̃ = ∇λL\n∗(λ)|λ=λ̃− . In order to prove the main convergence result, we need the following standard\nassumptions and remarks. Assumption 4 For any given x0 ∈ X and θ ∈ Θ, the set {( ν, g(ν) )\n| g(ν) ∈ ∂νL(θ, ν, λ) } is closed.\nRemark 2 For any given θ ∈ Θ, λ ≥ 0, and g(ν) ∈ ∂νL(θ, ν, λ), we have |g(ν)| ≤ 3λ(1 + |ν|)/(1 − α). (35)\nTo see this, recall from definition that g can be parameterized by q as, for q ∈ [0, 1],\ng(ν) = − λ (1− α)\n∑\nξ\nPθ(ξ)1 {D(ξ) > ν} − λq 1− α ∑\nξ\nPθ(ξ)1 {D(ξ) = ν}+ λ.\nIt is obvious that |1 {D(ξ) = ν}| , |1 {D(ξ) > ν}| ≤ 1+|ν|. Thus, ∣∣∣ ∑ ξ Pθ(ξ)1 {D(ξ) > ν} ∣∣∣ ≤ supξ |1 {D(ξ) > ν}| ≤ 1 + |ν|, and ∣∣∣ ∑ ξ Pθ(ξ)1 {D(ξ) = ν} ∣∣∣ ≤ 1 + |ν|. Recalling 0 < (1− q), (1 − α) < 1, these arguments imply the claim of (35).\nBefore getting into the main result, we need the following technical proposition.\nProposition 5 ∇θL(θ, ν, λ) is Lipschitz in θ.\nProof. Recall that\n∇θL(θ, ν, λ) = ∑\nξ\nPθ(ξ) · ∇θ logPθ(ξ) ( D(ξ) + λ\n1− α ( D(ξ)− ν ) 1 { D(ξ) ≥ ν\n})\nand∇θ logPθ(ξ) = ∑T−1\nk=0 ∇θµ(ak|xk; θ)/µ(ak|xk; θ) wheneverµ(ak|xk; θ) ∈ (0, 1]. Now Assumption (A1) implies that ∇θµ(ak|xk; θ) is a Lipschitz function in θ for any a ∈ A and k ∈ {0, . . . , T − 1} and µ(ak|xk; θ) is differentiable in θ. Therefore, by recalling that Pθ(ξ) = ∏T−1 k=0 P (xk+1|xk, ak)µ(ak|xk; θ)1{x0 = x0} and by combining these arguments and noting that the sum of products of Lipschitz functions is Lipschitz, one concludes that ∇θL(θ, ν, λ) is Lipschitz in θ.\nRemark 3 ∇θL(θ, ν, λ) is Lipschitz in θ implies that ‖∇θL(θ, ν, λ)‖2 ≤ 2(‖∇θL(θ0, ν, λ)‖+ ‖θ0‖)2 + 2‖θ‖2 which further implies that\n‖∇θL(θ, ν, λ)‖2 ≤ K1(1 + ‖θ‖2).\nfor K1 = 2max(1, (‖∇θL(θ0, ν, λ)‖ + ‖θ0‖)2) > 0. Similarly, ∇θ logPθ(ξ) is Lipschitz implies that ‖∇θ logPθ(ξ)‖2 ≤ K2(ξ)(1 + ‖θ‖2). for a positive random variableK2(ξ). Furthermore, since T < ∞ w.p. 1, µ(ak|xk; θ) ∈ (0, 1] and ∇θµ(ak|xk; θ) is Lipschitz for any k < T , K2(ξ) < ∞ w.p. 1.\nWe are now in a position to prove the convergence analysis of Theorem 2.\nProof. [Proof of Theorem 2] We split the proof into the following four steps:\nStep 1 (Convergence of ν−update) Since ν converges in a faster time scale than θ and λ, one can assume both θ and λ as fixed quantities in the ν-update, i.e.,\nνi+1 = ΓN  νi + ζ3(i)   λ (1− α)N N∑\nj=1\n1 { D(ξj,i) ≥ νi } − λ+ δνi+1     , (36)\nand\nδνi+1 = λ\n1− α\n − 1\nN\nN∑\nj=1\n1 { D(ξj,i) ≥ νi } + ∑\nξ\nPθ(ξ)1{D(ξ) ≥ νi}   . (37)\nFirst, one can show that δνi+1 is square integrable, i.e.\nE[‖δνi+1‖2 | Fν,i] ≤ 4 ( λmax 1− α )2\nwhere Fν,i = σ ( νm, δνm, m ≤ i ) is the filtration of νi generated by different independent trajectories. Second, since the history trajectories are generated based on the sampling probability mass function Pθ(ξ), expression (27) implies that E [δνi+1 | Fν,i] = 0. Therefore, the ν-update is a stochastic approximation of the ODE (30) with a Martingale difference error term, i.e.,\nλ 1− α ∑\nξ\nPθ(ξ)1{D(ξ) ≥ νi} − λ ∈ −∂νL(θ, ν, λ)|ν=νi .\nThen one can invoke Corollary 4 in Chapter 5 of [14] (stochastic approximation theory for non-differentiable systems) to show that the sequence {νi}, νi ∈ [−Cmax1−γ , Cmax1−γ ] converges almost surely to a fixed point ν∗ ∈ [−Cmax1−γ , Cmax1−γ ] of differential inclusion (31), where ν∗ ∈ Nc := {ν ∈ [−Cmax1−γ , Cmax1−γ ] : Υν [−g(ν)] = 0, g(ν) ∈ ∂νL(θ, ν, λ)}. To justify the assumptions of this theorem, 1) from Remark 2, the Lipschitz property is satisfied, i.e., supg(ν)∈∂νL(θ,ν,λ) |g(ν)| ≤ 3λ(1 + |ν|)/(1 − α), 2) ∂νL(θ, ν, λ) is a convex compact set by definition, 3) Assumption 4 implies that {(ν, g(ν)) | g(ν) ∈ ∂νL(θ, ν, λ)} is a closed set. This implies ∂νL(θ, ν, λ) is an upper semi-continuous set valued mapping 4) the step-size rule follows from (A.1), 5) the Martingale difference assumption follows from (37), and 6) νi ∈ [−Cmax1−γ , Cmax 1−γ ], ∀i implies that supi ‖νi‖ < ∞ almost surely. Consider the ODE of ν ∈ R in (30), we define the set-valued derivative of L as follows:\nDtL(θ, ν, λ) = { g(ν)Υν [ − g(ν) ] | ∀g(ν) ∈ ∂νL(θ, ν, λ) } .\nOne may conclude that\nmax g(ν)\nDtL(θ, ν, λ) = max { g(ν)Υν [ − g(ν) ] | g(ν) ∈ ∂νL(θ, ν, λ) } .\nWe have the following cases: Case 1: When ν ∈ (−Cmax1−γ , Cmax 1−γ ). For every g(ν) ∈ ∂νL(θ, ν, λ), there exists a sufficiently small η0 > 0 such that ν − η0g(ν) ∈ [−Cmax1−γ , Cmax 1−γ ] and\nΓN ( θ − η0g(ν) ) − θ = −η0g(ν).\nTherefore, the definition of Υθ[−g(ν)] implies\nmax g(ν)\nDtL(θ, ν, λ) = max { − g2(ν) | g(ν) ∈ ∂νL(θ, ν, λ) } ≤ 0. (38)\nThe maximum is attained because ∂νL(θ, ν, λ) is a convex compact set and g(ν)Υν [ − g(ν) ]\nis a continuous function. At the same time, we have maxg(ν) DtL(θ, ν, λ) < 0 whenever 0 6∈ ∂νL(θ, ν, λ). Case 2: When ν ∈ {−Cmax1−γ , Cmax 1−γ } and for any g(ν) ∈ ∂Lν(θ, ν, λ) such that ν − ηg(ν) ∈ [−Cmax1−γ , Cmax 1−γ ], for any η ∈ (0, η0] and some η0 > 0. The condition ν − ηg(ν) ∈ [−Cmax1−γ , Cmax 1−γ ] implies that\nΥν [ − g(ν) ] = −g(ν).\nThen we obtain\nmax g(ν)\nDtL(θ, ν, λ) = max { − g2(ν) | g(ν) ∈ ∂νL(θ, ν, λ) } ≤ 0. (39)\nFurthermore, we have maxg(ν) DtL(θ, ν, λ) < 0 whenever 0 6∈ ∂νL(θ, ν, λ). Case 3: When ν ∈ {−Cmax1−γ , Cmax1−γ } and there exists a non-empty set G(ν) := {g(ν) ∈ ∂Lν(θ, ν, λ) | θ − ηg(ν) 6∈ [−Cmax1−γ , Cmax1−γ ], ∃η ∈ (0, η0], ∀η0 > 0}. First, consider any g(ν) ∈ G(ν). For any η > 0, define νη := ν − ηg(ν). The above condition implies that when 0 < η → 0, ΓN [ νη ]\nis the projection of νη to the tangent space of [−Cmax1−γ , Cmax 1−γ ]. For any elements ν̂ ∈ [− Cmax 1−γ , Cmax 1−γ ], since the following set {ν ∈ [−Cmax1−γ , Cmax 1−γ ] : ‖ν − νη‖2 ≤ ‖ν̂ − νη‖2} is compact, the projection of νη on [−Cmax1−γ , Cmax 1−γ ] exists. Furthermore, since f(ν) := 1 2 (ν − νη)2 is a strongly convex function and ∇f(ν) = ν − νη, by first order optimality condition, one obtains\n∇f(ν∗η)(ν − ν∗η) = (ν∗η − νη)(ν − ν∗η ) ≥ 0, ∀ν ∈ [ −Cmax 1− γ , Cmax 1− γ ]\nwhere ν∗η is an unique projection of νη (the projection is unique because f(ν) is strongly convex and [−Cmax1−γ , Cmax1−γ ] is a convex compact set). Since the projection (minimizer) is unique, the above equality holds if and only if ν = ν∗η .\nTherefore, for any ν ∈ [−Cmax1−γ , Cmax 1−γ ] and η > 0,\ng(ν)Υν [ − g(ν) ] = g(ν) ( lim\n0<η→0 ν∗η − ν η\n)\n= ( lim\n0<η→0 ν − νη η\n)( lim\n0<η→0 ν∗η − ν η\n) = lim\n0<η→0 −‖ν∗η − ν‖2 η2 + lim 0<η→0 ( ν∗η − νη )(ν∗η − ν η2 ) ≤ 0.\nSecond, for any g(ν) ∈ ∂νL(θ, ν, λ)∩G(ν)c , one obtains ν−ηg(ν) ∈ [−Cmax1−γ , Cmax 1−γ ], for any η ∈ (0, η0] and some η0 > 0. In this case, the arguments follow from case 2 and the following expression holds, Υν [ − g(ν) ] = −g(ν).\nCombining these arguments, one concludes that\nmax g(ν) DtL(θ, ν, λ)\n≤max { max { g(ν) Υν [ − g(ν) ] | g(ν) ∈ G(ν) } ,max { − g2(ν) | g(ν) ∈ ∂νL(θ, ν, λ) ∩ G(ν)c }} ≤ 0.\n(40)\nThis quantity is non-zero whenever 0 6∈ {g(ν) Υν [ − g(ν) ] | ∀g(ν) ∈ ∂νL(θ, ν, λ)} (this is because, for any g(ν) ∈ ∂νL(θ, ν, λ)∩G(ν)c , one obtains g(ν) Υν [ − g(ν) ] = −g(ν)2). Thus, by similar arguments one may conclude that maxg(ν) DtL(θ, ν, λ) ≤ 0 and\nit is non-zero if Υν [ − g(ν) ] 6= 0 for every g(ν) ∈ ∂νL(θ, ν, λ). Therefore, by Lasalle’s invariance principle for differential inclusion (see Theorem 2.11 [30]), the above arguments imply that with any initial condition θ(0), the state trajectory ν(t) of (31) converges to a stable stationary point ν∗ in the positive invariant set Nc. Since maxg(ν) DtL(θ, ν, λ) ≤ 0, ν̇ is a descent direction of L(θ, ν, λ) for fixed θ and λ, i.e., L(θ, ν∗, λ) ≤ L(θ, ν(t), λ) ≤ L(θ, ν(0), λ) for any t ≥ 0.\nStep 2 (Convergence of θ−update) Since θ converges in a faster time scale than λ and ν converges faster than θ, one can assume λ as a fixed quantity and ν as a converged quantity ν∗(θ) in the θ-update. The θ-update can be rewritten as a stochastic approximation, i.e.,\nθi+1 = ΓΘ ( θi + ζ2(i) ( −∇θL(θ, ν, λ)|θ=θi,ν=ν∗(θi) + δθi+1 )) , (41)\nwhere\nδθi+1 =∇θL(θ, ν, λ)|θ=θi,ν=ν∗(θi)− 1\nN\nN∑\nj=1\n∇θ logPθ(ξj,i) |θ=θi D(ξj,i)\n− λ (1− α)N\nN∑\nj=1\n∇θ logPθ(ξj,i)|θ=θi ( D(ξj,i)− ν∗(θi) ) 1 { D(ξj,i) ≥ ν∗(θi) } .\n(42)\nFirst, one can show that δθi+1 is square integrable, i.e., E[‖δθi+1‖2 | Fθ,i] ≤ Ki(1 + ‖θi‖2) for some Ki > 0, where Fθ,i = σ ( θm, δθm, m ≤ i ) is the filtration of θi generated by different independent trajectories. To see this, notice that\n‖δθi+1‖2 ≤2 ( ∇θL(θ, ν, λ)|θ=θi,ν=ν∗(θi) )2 + 2\nN2 ( Cmax 1− γ + 2λCmax (1− α)(1− γ)\n)2 ( N∑\nj=1\n∇θ log Pθ(ξj,i) |θ=θi\n)2\n≤2K1,i(1 + ‖θi‖2) + 2 N\nN2 ( Cmax 1− γ + 2λmaxCmax (1− α)(1− γ)\n)2 ( N∑\nj=1\n‖∇θ log Pθ(ξj,i) |θ=θi‖2 )\n≤2K1,i(1 + ‖θi‖2) + 2N\nN2 ( Cmax 1− γ + 2λmaxCmax (1− α)(1− γ)\n)2 ( N∑\nj=1\nK2(ξj,i)(1 + ‖θi‖2) )\n≤2 ( K1,i+ 2N−1\nN ( Cmax 1− γ + 2λmaxCmax (1− α)(1− γ) )2 max 1≤j≤N K2(ξj,i) ) (1+‖θi‖2)\nThe Lipschitz upper bounds are due to results in Remark 3. Since K2(ξj,i) < ∞ w.p. 1, there exists K2,i < ∞ such that max1≤j≤N K2(ξj,i) ≤ K2,i. Furthermore, T < ∞\nw.p. 1 implies E[T 2 | Fθ,i] < ∞. By combining these results, one concludes that E[‖δθi+1‖2 | Fθ,i] ≤ Ki(1+‖θi‖2) where\nKi = 2 ( K1,i+\n2N−1K2,i N ( Cmax 1− γ + 2λmaxCmax (1 − α)(1 − γ) )2) < ∞.\nSecond, since the history trajectories are generated based on the sampling probability mass function Pθi(ξ), expression (26) implies that E [δθi+1 | Fθ,i] = 0. Therefore, the θ-update is a stochastic approximation of the ODE (31) with a Martingale difference error term. In addition, from the convergence analysis of ν−update, ν∗(θ) is an asymptotically stable equilibrium point of {νi}. From (27), ∂νL(θ, ν, λ) is a Lipschitz set-valued mapping in θ (since Pθ(ξ) is Lipschitz in θ), it can be easily seen that ν∗(θ) is a Lipschitz continuous mapping of θ.\nNow consider the continuous time system θ ∈ Θ in (31). We may write\ndL(θ, ν, λ)\ndt\n∣∣∣∣ ν=ν∗(θ) = ( ∇θL(θ, ν, λ)|ν=ν∗(θ) )⊤ Υθ [ −∇θL(θ, ν, λ)|ν=ν∗(θ) ] . (43)\nWe have the following cases: Case 1: When θ ∈ Θ◦. Since Θ◦ is the interior of the set Θ and Θ is a convex compact set, there exists a sufficiently small η0 > 0 such that θ − η0∇θL(θ, ν, λ)|ν=ν∗(θ) ∈ Θ and\nΓΘ ( θ − η0∇θL(θ, ν, λ)|ν=ν∗(θ) ) − θ = −η0∇θL(θ, ν, λ)|ν=ν∗(θ).\nTherefore, the definition of Υθ [ −∇θL(θ, ν, λ)|ν=ν∗(θ) ] implies\ndL(θ, ν, λ)\ndt\n∣∣∣∣ ν=ν∗(θ) = − ∥∥∇θL(θ, ν, λ)|ν=ν∗(θ) ∥∥2 ≤ 0. (44)\nAt the same time, we have dL(θ, ν, λ)/dt|ν=ν∗(θ) < 0whenever ‖∇θL(θ, ν, λ)|ν=ν∗(θ)‖ 6= 0. Case 2: When θ ∈ ∂Θ and θ − η∇θL(θ, ν, λ)|ν=ν∗(θ) ∈ Θ for any η ∈ (0, η0] and some η0 > 0. The condition θ − η∇θL(θ, ν, λ)|ν=ν∗(θ) ∈ Θ implies that\nΥθ [ −∇θL(θ, ν, λ)|ν=ν∗(θ) ] = −∇θL(θ, ν, λ)|ν=ν∗(θ).\nThen we obtain\ndL(θ, ν, λ)\ndt\n∣∣∣∣ ν=ν∗(θ) = − ∥∥∇θL(θ, ν, λ)|ν=ν∗(θ) ∥∥2 ≤ 0. (45)\nFurthermore, dL(θ, ν, λ)/dt|ν=ν∗(θ) < 0 when ‖∇θL(θ, ν, λ)|ν=ν∗(θ)‖ 6= 0. Case 3: When θ ∈ ∂Θ and θ − η∇θL(θ, ν, λ)|ν=ν∗(θ) 6∈ Θ for some η ∈ (0, η0] and any η0 > 0. For any η > 0, define θη := θ − η∇θL(θ, ν, λ)|ν=ν∗(θ). The above condition implies\nthat when 0 < η → 0, ΓΘ [ θη ] is the projection of θη to the tangent space of Θ. For any elements θ̂ ∈ Θ, since the following set {θ ∈ Θ : ‖θ − θη‖2 ≤ ‖θ̂ − θη‖2} is compact, the projection of θη on Θ exists. Furthermore, since f(θ) := 12‖θ− θη‖22 is a strongly convex function and ∇f(θ) = θ− θη, by first order optimality condition, one obtains ∇f(θ∗η)⊤(θ − θ∗η) = (θ∗η − θη)⊤(θ − θ∗η) ≥ 0, ∀θ ∈ Θ where θ∗η is an unique projection of θη (the projection is unique because f(θ) is strongly convex and Θ is a convex compact set). Since the projection (minimizer) is unique, the above equality holds if and only if θ = θ∗η .\nTherefore, for any θ ∈ Θ and η > 0,\n( ∇θL(θ, ν, λ)|ν=ν∗(θ) )⊤ Υθ [ −∇θL(θ, ν, λ)|ν=ν∗(θ) ] = ( ∇θL(θ, ν, λ)|ν=ν∗(θ)\n)⊤ (\nlim 0<η→0 θ∗η − θ η\n)\n= ( lim\n0<η→0 θ − θη η\n)⊤ ( lim\n0<η→0 θ∗η − θ η\n) = lim\n0<η→0 −‖θ∗η − θ‖2 η2 + lim 0<η→0 ( θ∗η − θη )⊤ ( θ∗η − θ η2 ) ≤ 0.\nFrom these arguments, one concludes that dL(θ, ν, λ)/dt|ν=ν∗(θ) ≤ 0 and this quantity is non-zero whenever ∥∥Υθ [ −∇θL(θ, ν, λ)|ν=ν∗(θ)\n]∥∥ 6= 0. Therefore, by Lasalle’s invariance principle [19], the above arguments imply that\nwith any initial condition θ(0), the state trajectory θ(t) of (31) converges to a stable stationary point θ∗ in the positive invariant set Θc andL(θ∗, ν∗(θ∗), λ) ≤ L(θ(t), ν∗(θ(t)), λ) ≤ L(θ(0), ν∗(θ(0)), λ) for any t ≥ 0.\nBased on the above properties and noting that 1) from Proposition 5, ∇θL(θ, ν, λ) is a Lipschitz function in θ, 2) the step-size rule follows from Section A.1, 3) expression (47) implies that δθi+1 is a square integrable Martingale difference, and 4) θi ∈ Θ, ∀i implies that supi ‖θi‖ < ∞ almost surely, one can invoke Theorem 2 in Chapter 6 of [14] (multi-time scale stochastic approximation theory) to show that the sequence {θi}, θi ∈ Θ converges almost surely to a fixed point θ∗ ∈ Θ of ODE (31), where θ∗ ∈ Θc := {θ ∈ Θ : Υθ[−∇θL(θ, ν, λ)|ν=ν∗(θ)] = 0}. Also, it can be easily seen that Θc is a closed subset of the compact set Θ, which is a compact set as well.\nStep 3 (Local Minimum) Now, we want to show that {θi, νi} converges to a local minimum ofL(θ, ν, λ) for fixed λ. Recall {θi, νi} converges to (θ∗, ν∗) := (θ∗, ν∗(θ∗)). From previous arguments on (ν, θ) convergence analysis imply that with any initial condition (θ(0), ν(0)), the state trajectories θ(t) and ν(t)) of (30) and (31) converge to the set of stationary points (θ∗, ν∗) in the positive invariant set Θc×Nc andL(θ∗, ν∗, λ) ≤ L(θ(t), ν∗(θ(t)), λ) ≤ L(θ(0), ν∗(θ(0)), λ) ≤ L(θ(0), ν(t), λ) ≤ L(θ(0), ν(0), λ) for any t ≥ 0.\nBy contradiction, suppose (θ∗, ν∗) is not a local minimum. Then there exists (θ̄, ν̄) ∈ Θ×[−Cmax1−γ , Cmax1−γ ]∩B(θ∗,ν∗)(r) such that L(θ̄, ν̄, λ) = min(θ,ν)∈Θ×[−Cmax1−γ ,Cmax1−γ ]∩B(θ∗,ν∗)(r) L(θ, ν, λ). The minimum is attained by Weierstrass extreme value theorem. By putting θ(0) = θ̄, the above arguments imply that\nL(θ̄, ν̄, λ) = min (θ,ν)∈Θ×[−Cmax1−γ , Cmax 1−γ ]∩B(θ∗,ν∗)(r)\nL(θ, ν, λ) < L(θ∗, ν∗, λ) ≤ L(θ̄, ν̄, λ)\nwhich is clearly a contradiction. Therefore, the stationary point (θ∗, ν∗) is a local minimum of L(θ, ν, λ) as well.\nStep 4 (Convergence of λ−update) Since λ-update converges in the slowest time scale, it can be rewritten using the converged θ∗(λ) = θ∗(ν∗(λ), λ) and ν∗(λ), i.e.,\nλi+1 = ΓΛ ( λi + ζ1(i) ( ∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λi),ν=ν∗(λi),λ=λi + δλi+1 )) (46)\nwhere δλi+1 = −∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ),λ=λi + ( ν∗(λi) + 1 1− α 1 N N∑\nj=1\n( D(ξj,i)− ν∗(λi) )+ − β ) .\n(47)\nFrom (28), it is obvious that ∇λL(θ, ν, λ) is a constant function of λ. Similar to θ−update, one can easily show that δλi+1 is square integrable, i.e.,\nE[‖δλi+1‖2 | Fλ,i] ≤ 2 ( β +\n3Cmax (1− γ)(1− α)\n)2 ,\nwhere Fλ,i = σ ( λm, δλm, m ≤ i ) is the filtration of λ generated by different independent trajectories. Furthermore, expression (28) implies that E [δλi+1 | Fλ,i] = 0. Therefore, the λ-update is a stochastic approximation of the ODE (33) with a Martingale difference error term. In addition, from the convergence analysis of (θ, ν)−update, (θ∗(λ), ν∗(λ)) is an asymptotically stable equilibrium point of {θi, νi}. From (26), ∇θL(θ, ν, λ) is a linear mapping in λ, it can be easily seen that (θ∗(λ), ν∗(λ)) is a Lipschitz continuous mapping of λ.\nConsider the ODE of λ ∈ [0, λmax] in (33). Analogous to the arguments in the θ−update, we may write\ndL(θ, ν, λ)\ndt\n∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ) = ∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ) Υλ [ ∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ) ] .\nand show that dL(θ, ν, λ)/dt|θ=θ∗(λ),ν=ν∗(λ) ≤ 0, this quantity is non-zero whenever∥∥Υλ [ dL(θ, ν, λ)/dλ|θ=θ∗(λ),ν=ν∗(λ)\n]∥∥ 6= 0. Lasalle’s invariance principle implies that λ∗ ∈ Λc := {λ ∈ [0, λmax] : Υλ[∇λL(θ, ν, λ) |ν=ν∗(λ),θ=θ∗(λ)] = 0} is a stable equilibrium point.\nBased on the above properties and noting that the step size rule follows from Section A.1, one can apply the multi-time scale stochastic approximation theory (Theorem 2 in Chapter 6 of [14]) to show that the sequence {λi} converges almost surely to a fixed point λ∗ ∈ [0, λmax] of ODE (33), where λ∗ ∈ Λc := {λ ∈ [0, λmax] : Υλ[∇λL(θ, ν, λ) |θ=θ∗(λ),ν=ν∗(λ)] = 0}. Since Λc is a closed set of [0, λmax], it is a compact set as well. Following the same lines of arguments and recalling the envelope theorem (Theorem 3) for local optimum, one further concludes that λ∗ is a local maximum of L(θ∗(λ), ν∗(λ), λ) = L∗(λ).\nStep 5 (Saddle Point) By letting θ∗ = θ∗ ( ν∗(λ∗), λ∗ ) and ν∗ = ν∗(λ∗), we will show that (θ∗, ν∗, λ∗) is a (local) saddle point of the objective function L(θ, ν, λ) if λ∗ ∈ [0, λmax).\nNow suppose the sequence {λi} generated from (46) converges to a stationary point λ∗ ∈ [0, λmax). Since step 3 implies that (θ∗, ν∗) is a local minimum of L(θ, ν, λ∗) over feasible set (θ, ν) ∈ Θ× [−Cmax1−γ , Cmax 1−γ ], there exists a r > 0 such that\nL(θ∗, ν∗, λ∗) ≤ L(θ, ν, λ∗), ∀(θ, ν) ∈ Θ× [ −Cmax 1− γ , Cmax 1− γ ] ∩B(θ∗,ν∗)(r).\nIn order to complete the proof, we must show\nν∗ + 1 1− αE [( Dθ ∗ (x0)− ν∗ )+] ≤ β, (48)\nand\nλ∗ ( ν∗ + 1 1− αE [( Dθ ∗ (x0)− ν∗ )+]− β ) = 0. (49)\nThese two equations imply\nL(θ∗, ν∗, λ∗) =V θ ∗ (x0)+λ∗ ( ν∗ + 1 1− αE [( Dθ ∗ (x0)− ν∗ )+]− β )\n=V θ ∗ (x0) ≥V θ∗(x0)+λ ( ν∗ + 1\n1− αE [( Dθ ∗ (x0)− ν∗ )+]− β ) = L(θ∗, ν∗, λ),\nwhich further implies that (θ∗, ν∗, λ∗) is a saddle point of L(θ, ν, λ). We now show that (48) and (49) hold.\nRecall that Υλ [ ∇λL(θ, ν, λ)|θ=θ∗(λ),ν=ν∗(λ),λ=λ∗ ] |λ=λ∗ = 0. We show (48) by\ncontradiction. Suppose ν∗ + 11−αE [( Dθ ∗ (x0)− ν∗ )+] > β. This then implies that for λ∗ ∈ [0, λmax), we have\nΓΛ ( λ∗ − η ( β − ( ν∗ + 1\n1− αE [( Dθ ∗ (x0)− ν∗ )+]) )) = λ∗−η ( β− ( ν∗+ 1 1− αE [( Dθ ∗ (x0)−ν∗ )+]) )\nfor any η ∈ (0, ηmax] for some sufficiently small ηmax > 0. Therefore,\nΥλ [ ∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ),λ=λ∗ ] ∣∣∣∣∣ λ=λ∗ = ν∗+ 1 1− αE [( Dθ ∗ (x0)− ν∗ )+]−β > 0. This contradicts with Υλ [ ∇λL(θ, ν, λ)|θ=θ∗(λ),ν=ν∗(λ),λ=λ∗ ] |λ=λ∗ = 0. Therefore, (48) holds. To show that (49) holds, we only need to show that λ∗ = 0 if ν∗+ 11−αE [( Dθ ∗ (x0)− ν∗ )+] < β. Suppose λ∗ ∈ (0, λmax), then there exists a sufficiently small η0 > 0 such that 1\nη0\n( ΓΛ ( λ∗ − η0 ( β − ( ν∗ + 1\n1− αE [( Dθ ∗ (x0)− ν∗ )+])) ) − ΓΛ(λ∗) )\n=ν∗ + 1 1− αE [( Dθ ∗ (x0)− ν∗ )+]− β < 0.\nThis again contradicts with the assumptionΥλ [ ∇λL(θ, ν, λ)|θ=θ∗(λ),ν=ν∗(λ),λ=λ∗ ] |λ=λ∗ = 0 from (70). Therefore (49) holds. Combining the above arguments, we finally conclude that (θ∗, ν∗, λ∗) is a (local) saddle point of L(θ, ν, λ) if λ∗ ∈ [0, λmax). Remark 4 When λ∗ = λmax and ν∗ + 11−αE [( Dθ ∗ (x0)− ν∗ )+] > β,\nΓΛ\n( λ∗ − η ( β − ( ν∗ + 1\n1− αE [( Dθ ∗ (x0)− ν∗ )+]) )) = λmax\nfor any η > 0 and\nΥλ [ ∇λL(θ, ν, λ)|θ=θ∗(λ),ν=ν∗(λ),λ=λ∗ ] |λ=λ∗= 0.\nIn this case one cannot guarantee feasibility using the above analysis, and (θ∗, ν∗, λ∗) is not a local saddle point. Such λ∗ is referred as a spurious fixed point [20]. Practically, by incrementally increasing λmax (see Algorithm 1 for more details), when λmax becomes sufficiently large, one can ensure that the policy gradient algorithm will not get stuck at the spurious fixed point."
    }, {
      "heading" : "B Technical Details of the Actor-Critic Algorithms",
      "text" : ""
    }, {
      "heading" : "B.1 Assumptions",
      "text" : "We make the following assumptions for the proof of our actor-critic algorithms: (B1) For any state-action pair (x, s, a) in the augmented MDP M̄, µ(a|x, s; θ) is continuously differentiable in θ and ∇θµ(a|x, s; θ) is a Lipschitz function in θ for every a ∈ A, x ∈ X and s ∈ R.\n(B2) The augmented Markov chain induced by any policy θ, M̄θ, is irreducible and aperiodic.\n(B3) The basis functions { φ(i) }κ2 i=1\nare linearly independent. In particular, κ2 ≪ n andΦ is full rank.4 Moreover, for every v ∈ Rκ2 , Φv 6= e, where e is the n-dimensional vector with all entries equal to one.\n(B4) For each (x′, s′, a′) ∈ X̄ × Ā, there is a positive probability of being visited, i.e., πθγ(x\n′, s′, a′|x, s) > 0. Note that from the definition of the augmented MDP M̄, X̄ = X × R and Ā = A. (B5) The step size schedules {ζ4(k)}, {ζ3(k)}, {ζ2(k)}, and {ζ1(k)} satisfy\n∑\nk\nζ1(k) = ∑\nk\nζ2(k) = ∑\nk\nζ3(k) = ∑\nk ζ4(k) = ∞, (50) ∑\nk\nζ1(k) 2,\n∑\nk\nζ2(k) 2,\n∑\nk\nζ3(k) 2,\n∑\nk\nζ4(k) 2 < ∞, (51)\nζ1(k) = o ( ζ2(k) ) , ζ2(k) = o ( ζ3(k) ) , ζ3(k) = o ( ζ4(k) ) . (52)\nThis indicates that the updates correspond to {ζ4(k)} is on the fastest time-scale, the update corresponds to {ζ3(k)}, {ζ2(k)} are on the intermediate time-scale, where ζ3(k) converges faster than ζ2(k), and the update corresponds to {ζ1(k)} is on the slowest time-scale. (B6) The SPSA step size {∆k} satisfies ∆k → ∞ as k → ∞ and ∑ k(ζ2(k)/∆k)\n2 < ∞.\nTechnical assumptions for the convergence of the actor-critic algorithm will be given in the section for the proof of convergence."
    }, {
      "heading" : "B.2 Gradient with Respect to λ (Proof of Lemma 1)",
      "text" : "Proof. By taking the gradient of V θ(x0, ν) w.r.t. λ (just a reminder that both V and Q are related to λ through the dependence of the cost function C̄ of the augmented MDP M̄ on λ), we obtain\n4We may write this as: In particular, the (row) infinite dimensional matrix Φ has column rank κ2.\n∇λV θ(x0, ν) = ∑\na∈Ā\nµ(a|x0, ν; θ)∇λQθ(x0, ν, a)\n= ∑\na∈Ā\nµ(a|x0, ν; θ)∇λ [ C̄(x0, ν, a) + ∑\n(x′,s′)∈X̄\nγP̄ (x′, s′|x0, ν, a)V θ(x′, s′) ]\n= ∑\na µ(a|x0, ν; θ)∇λC̄(x0, ν, a) ︸ ︷︷ ︸\nh(x0,ν)\n+γ ∑\na,x′,s′\nµ(a|x0, ν; θ)P̄ (x′, s′|x0, ν, a)∇λV θ(x′, s′)\n= h(x0, ν) + γ ∑\na,x′,s′\nµ(a|x0, ν; θ)P̄ (x′, s′|x0, ν, a)∇λV θ(x′, s′) (53)\n= h(x0, ν) + γ ∑\na,x′,s′\nµ(a|x0, ν; θ)P̄ (x′, s′|x0, ν, a) [ h(x′, s′)\n+ γ ∑\na′,x′′,s′′\nµ(a′|x′, s′; θ)P̄ (x′′, s′′|x′, s′, a′)∇λV θ(x′′, s′′) ]\nBy unrolling the last equation using the definition of ∇λV θ(x, s) from (53), we obtain\n∇λV θ(x0, ν) = ∞∑\nk=0\nγk ∑\nx,s\nP(xk = x, sk = s | x0 = x0, s0 = ν; θ)h(x, s)\n= 1 1− γ ∑\nx,s\ndθγ(x, s|x0, ν)h(x, s) = 1 1− γ ∑\nx,s,a\ndθγ(x, s|x0, ν)µ(a|x, s)∇λC̄(x, s, a)\n= 1 1− γ ∑\nx,s,a\nπθγ(x, s, a|x0, ν)∇λC̄(x, s, a)\n= 1 1− γ ∑\nx,s,a\nπθγ(x, s, a|x0, ν) 1\n1− α1{x = xT }(−s) +.\nB.3 Actor-Critic Algorithm with the Alternative Approach to Compute the Gradients"
    }, {
      "heading" : "B.4 Convergence of the Actor Critic Algorithms",
      "text" : "In this section we want to derive the following convergence results.\nTheorem 6 Suppose v∗ ∈ argminv ‖Tθ[Φv]− Φv‖2dθγ , where\nTθ[V ](x, s) = ∑\na\nµ(a|x, s; θ)   C̄(x, s, a) + ∑\nx′,s′\nP̄ (x′, s′|x, s, a)V (x′, s′)   \nand Ṽ ∗(x, s) = φ⊤(x, s)v∗ is the projected Bellman fixed point of V θ(x, s), i.e., Ṽ ∗(x, s) = ΠTθ[Ṽ ∗](x, s). Also suppose the γ−stationary distribution πθγ is used\nAlgorithm 3 Actor-Critic Algorithm for CVaR Optimization (Alternative Gradient Computation)\nwhile 1 do Input: Parameterized policy µ(·|·; θ), value function feature vectors f(·) and φ(·), confidence level α, and loss tolerance β Initialization: policy parameters θ = θ0; VaR parameter ν = ν0; Lagrangian parameter λ = λ0; value function weight vectors u = u0 and v = v0 for k = 0, 1, 2, . . . do\nDraw action ak ∼ µ(·|xk, sk; θk) Observe next state (xk+1, sk+1) ∼ P̄ (·|xk, sk, ak); // note that sk+1 = (sk − C ( xk, ak) )\n/γ (see Sec. 5.1) Observe costs C(xk, ak) and C̄(xk, sk, ak) // C̄ and P̄ are the cost and transition functions of the\n// augmented MDP M̄ defined in Sec. 5.4, while C is the cost function of the original MDP M\nTD Errors: ǫk(uk) = C(xk, ak) + γu ⊤ k f(xk+1)− u⊤k f(xk) (54)\nδk(vk) = C̄(xk, sk, ak) + γv ⊤ k φ(xk+1, sk+1)− v⊤k φ(xk, sk)\n(55)\nCritic Updates: uk+1 = uk + ζ4(k)ǫk(uk)f(xk) (56)\nvk+1 = vk + ζ4(k)δk(vk)φ(xk, sk) (57)\nActor Updates: νk+1 = ΓN ( νk − ζ3(k)λk ( 1 + v⊤k [ φ ( x0, νk +∆k ) − φ(x0, νk −∆k) ]\n2(1− α)∆k\n))\n(58)\nθk+1 = ΓΘ ( θk − ζ2(k) 1− γ∇θ log µ(ak|xk, sk; θ)|θ=θk · ( ǫk(uk) + λk 1− αδk(vk) ))\n(59)\nλk+1 = ΓΛ ( λk + ζ1(k) ( νk − β + v⊤φ(xk, sk)\n1− α\n)) (60)\nend for if {λi} converges to λmax then\nSet λmax ← 2λmax. else\nreturn policy and value function parameters v, u, ν, θ, λ and break end if\nend while\nto generate samples of (xk, sk, ak) for any k ∈ {0, 1, . . . , }. Then the v−updates in the actor critic algorithms converge to v∗ almost surely.\nNext define ǫθ(vk) = ‖Tθ[Φvk]− Φvk‖2dθγ\nas the residue of the value function approximation at step k induced by policyµ(·|·, ·; θ). By triangular inequality and fixed point theorem Tθ[V ∗] = V ∗, it can be easily seen that ‖V ∗−Φvk‖2dθγ ≤ ǫθ(vk)+‖Tθ[Φvk]−Tθ[V ∗]‖2 dθγ\n≤ ǫθ(vk)+γ‖Φvk−V ∗‖2dθγ . The last inequality follows from the contraction mapping argument. Thus, one concludes that ‖V ∗ − Φvk‖2dθγ ≤ ǫθ(vk)/(1− γ).\nTheorem 7 Suppose λ∗ ∈ [0, λmax), ǫθk(vk) → 0 as t goes to infinity and the γ−stationary distribution πθγ is used to generate samples of (xk, sk, ak) for any k ∈ {0, 1, . . . , }. For SPSA based algorithm, also suppose the perturbation sequence {∆k} satisfies ǫθk(vk)E[1/∆k] → 0. Then the sequence of (θ, ν, λ)-updates in Algorithm 2 converges to a (local) saddle point (θ∗, ν∗, λ∗) of our objective function L(θ, ν, λ) almost surely, it satisfies L(θ, ν, λ∗) ≥ L(θ∗, ν∗, λ∗) ≥ L(θ∗, ν∗, λ), ∀(θ, ν) ∈ Θ × [−Cmax/(1 − γ), Cmax/(1 − γ)] ∩ B(θ∗,ν∗)(r) for some r > 0 and ∀λ ∈ [0, λmax]. Note that B(θ∗,ν∗)(r) represents a hyper-dimensional ball centered at (θ\n∗, ν∗) with radius r.\nSince the proof of the Multi-loop algorithm and the SPSA based algorithm is almost identical (except the ν−update), we will focus on proving the SPSA based actor critic algorithm."
    }, {
      "heading" : "B.4.1 Proof of Theorem 6: TD(0) Critic Update (v−update)",
      "text" : "By the step length conditions, one notices that {vk} converges in a faster time scale than {θk}, {νk} and {λk}, one can assume (θ, ν, λ) in the v−update as fixed quantities. The critic update can be re-written as follows:\nvk+1 = vk + ζ4(k)φ(xk , sk)δk(vk) (61)\nwhere the scaler\nδk (v) = −φ⊤(xk, sk)v + γφ⊤ (xk+1, sk+1) v + C̄(xk, sk, ak).\nis known as the temporal difference (TD). Define\nA = ∑\ny,a′,s′\nπθγ(y, s ′, a′|x, s)φ(y, s′)\n φ⊤(y, s′)− γ ∑\nz,s′′\nP̄ (z, s′′|y, s′, a)φ⊤ (z, s′′)\n \n(62)\nand b = ∑\nyX,a′,s′\nπθγ(y, s ′, a′|x, s)φ(y, s′)C̄(y, s′, a′). (63)\nBased on the definitions of matrices A and b, it is easy to see that the TD(0) critic update vk in (61) can be re-written as the following stochastic approximation scheme:\nvk+1 = vk + ζ4(k)(b −Avk + δAk+1) (64)\nwhere the noise term δAk+1 is a square integrable Martingale difference, i.e, E[δAk+1 | Fk] = 0 if the γ−stationary distribution πθγ used to generate samples of (xk, sk, ak). Fk is the filtration generated by different independent trajectories. By writing\nδAk+1 = −(b−Avk) + φ(xk, sk)δk(vk)\nand noting Eπθγ [φ(xk, sk)δk(vk) | Fk] = −Avk + b, one can easily check that the stochastic approximation scheme in (61) is equivalent to the TD(0) iterates in (61) and δAk+1 is a Martingale difference, i.e., Eπθγ [δAk+1 | Fk] = 0. Let\nh (v) = −Av + b.\nBefore getting into the convergence analysis, we have the following technical lemma.\nLemma 8 Every eigenvalues of matrix A has positive real part.\nProof. To complete this proof, we need to show that for any vector v ∈ Rκ2 , v⊤Av > 0. Now, for any fixed v ∈ Rκ2 , define y(x, s) = v⊤φ⊤(x, s). It can be easily seen from the definition of A that v⊤Av = ∑\nx,x′,a,s,s′\ny(x, s)πθγ(x, s, a|x0 = x0, s0 = ν)·(1{x′ = x, s′ = s}−γP̄ (x′, s′|x, s, a))y(x′, s′).\nBy convexity of quadratic functions and Jensen’s inequality, one can derive the following expressions:\n∑\nx,x′,a,s,s′\ny(x, s)πθγ(x, s, a|x0 = x0, s0 = ν)γP̄ (x′, s′|x, s, a)y(x′, s′)\n≤‖y‖dθγ √ γ\n√ ∑\nx,x′,a,s,s′\ndθγ(x, s|x0 = x0, s0 = ν)γµ(a|x, s; θ)P (x′, s′|x, s, a)(y(x′, s′))2\n=‖y‖dθγ √∑\ny,s′\n( dθγ(y, s′|x0, ν)− (1− γ)1{x0 = y, ν = s′} ) (y(x′, s′))2\n<‖y‖2dθγ\nwhere dθγ(x, s|x0 = x0, s0 = ν)µ(a|x, s; θ) = πθγ(x, s, a|x0 = x0, s0 = ν) and\n‖y‖2dθγ = ∑\nx,s\ndθγ(x, s|x0 = x0, s0 = ν)(y(x, s))2.\nThe first inequality is due to the fact that µ(a|x, s; θ), P̄ (y, s′|x, s, a) ∈ [0, 1] and convexity of quadratic function, the second equality is based on the stationarity property of a γ−visiting distribution: dθγ(y, s′|x0, ν) ≥ 0, ∑ y,s′ d θ γ(y, s\n′|x0, ν) = 1 and ∑\nx′,s,a\nπθγ(x ′, s, a|x0 = x0, s0 = ν)γP̄ (y, s′|x′, s, a′) = dθγ(y, s′|x0, ν)−(1−γ)1{x0 = y, ν = s′}.\nAs the above argument holds for any v ∈ Rκ2 and y(x, s) = v⊤φ(x, s), one shows that v⊤Av > 0 for any v ∈ Rκ2 . This further implies v⊤A⊤v > 0 and v⊤(A⊤+A)v > 0 for any v ∈ Rκ2 . Therefore, A + A⊤ is a symmetric positive definite matrix, i.e. there exists a ǫ > 0 such that A + A⊤ > ǫI . To complete the proof, suppose by contradiction that there exists an eigenvalue λ of A which has a non-positive real-part. Let vλ be the corresponding eigenvector of λ. Then, by pre- and post-multiplying v∗λ and vλ to A + A⊤ > ǫI and noting that the hermitian of a real matrix A is A⊤, one obtains 2Re(λ)‖vλ‖2 = v∗λ(A + A⊤)vλ = v∗λ(A + A∗)vλ > ǫ‖vλ‖2. This implies Re(λ) > 0, i.e., a contradiction. By combining all previous arguments, one concludes that every eigenvalues A has positive real part.\nWe now turn to the analysis of the TD(0) iteration. Note that the following properties hold for the TD(0) update scheme in (61):\n1. h (v) is Lipschitz.\n2. The step size satisfies the following properties in Appendix B.1.\n3. The noise term δAk+1 is a square integrable Martingale difference.\n4. The function hc (v) := h (cv) /c, c ≥ 1\nconverges uniformly to a continuous function h∞ (v) for any w in a compact set, i.e., hc (v) → h∞ (v) as c → ∞.\n5. The ordinary differential equation (ODE)\nv̇ = h∞ (v)\nhas the origin as its unique globally asymptotically stable equilibrium.\nThe fourth property can be easily verified from the fact that the magnitude of b is finite and h∞ (v) = v. The fifth property follows directly from the facts that h∞ (v) = −Av and all eigenvalues of A have positive real parts. Therefore, by Theorem 3.1 in [14], these five properties imply the following condition:\nThe TD iterates {vk} is bounded almost surely, i.e., sup k ‖vk‖ < ∞ almost surely.\nFinally, from the standard stochastic approximation result, from the above conditions, the convergence of the TD(0) iterates in (61) can be related to the asymptotic behavior of the ODE v̇ = h (v) = b−Av. (65) By Theorem 2 in Chapter 2 of [14], when property (1) to (3) in (65) hold, then vk → v∗ with probability 1 where the limit v∗ depends on (θ, ν, λ) and is the unique solution satisfying h (v∗) = 0, i.e., Av∗ = b. Therefore, the TD(0) iterates converges to the unique fixed point v∗ almost surely, at k → ∞."
    }, {
      "heading" : "B.4.2 Proof of Theorem 7",
      "text" : "Step 1 (Convergence of v−update) The proof of the critic parameter convergence follows directly from Theorem 6.\nStep 2 (Convergence of SPSA based ν−update) In this section, we present the ν−update for the incremental actor critic method. This update is based on the SPSA perturbation method. The idea of this method is to estimate the sub-gradient g(ν) ∈ ∂νL(θ, ν, λ) using two simulated value functions corresponding to ν− = ν − ∆ and ν+ = ν + ∆. Here ∆ ≥ 0 is a positive random perturbation that vanishes asymptotically.\nThe SPSA-based estimate for a sub-gradient g(ν) ∈ ∂νL(θ, ν, λ) is given by:\ng(ν) ≈ λ+ 1 2∆\n( φ⊤ ( x0, ν +∆ ) − φ⊤ ( x0, ν −∆ )) v\nwhere ∆ ≥ 0 is a “small” random perturbation of the finite difference sub-gradient approximation.\nNow, we turn to the convergence analysis of sub-gradient estimation and ν−update. Since v converges faster than ν, and ν converges faster then θ and λ, the ν−update in (18) can be rewritten using the converged critic-parameter v∗(ν) and (θ, λ) in this expression is viewed as constant quantities, i.e.,\nνk+1 = ΓN ( νk − ζ3(k) ( λ+ 1\n2∆k\n( φ⊤ ( x0, νk +∆k ) − φ⊤ ( x0, νk −∆k )) v∗(νk) )) .\n(66) First, we have the following assumption on the feature functions in order to prove\nthe SPSA approximation is asymptotically unbiased.\nAssumption 9 For any v ∈ Rκ1 , the feature function satisfies the following conditions\n|φ⊤V ( x0, ν +∆ ) v − φ⊤V ( x0, ν −∆ ) v| ≤ K1(v)(1 + ∆).\nFurthermore, the Lipschitz constants are uniformly bounded, i.e., supv∈Rκ1 K 2 1(v) < ∞. This assumption is mild because the expected utility objective function implies that L(θ, ν, λ) is Lipschitz in ν, and φ⊤V ( x0, ν ) v is just a linear function approximation of V θ(x0, ν). Then, we establish the bias and convergence of stochastic sub-gradient estimates. Let g(νk) ∈ argmax {g : g ∈ ∂νL(θ, ν, λ)|ν=νk} and\nΛ1,k+1 =\n(( φ⊤ ( x0, νk +∆k ) − φ⊤ ( x0, νk −∆k )) v∗(νk)\n2∆k − EM (k)\n) ,\nΛ2,k =λk + E L M (k)− g(νk), Λ3,k =EM (k)− ELM (k),\nwhere\nEM (k) :=E\n[ 1\n2∆k\n( φ⊤ ( x0, νk +∆k ) − φ⊤ ( x0, νk −∆k )) v∗(νk) | ∆k\n]\nELM (k) :=E\n[ 1\n2∆k\n( V θ ( x0, νk +∆k ) − V θ ( x0, νk −∆k )) | ∆k ] .\nNote that (66) is equivalent to\nνk+1 = νk − ζ3(k) (g(νk) + Λ1,k+1 + Λ2,k + Λ3,k) (67)\nFirst, it is obvious that Λ1,k+1 is a Martingale difference as E[Λ1,k+1 | Fk] = 0, which implies\nMk+1 =\nk∑\nj=0\nζ3(j)Λ1,j+1\nis a Martingale with respect to filtration Fk. By Martingale convergence theorem, we can show that if supk≥0 E[M 2 k ] < ∞, when k → ∞, Mk converges almost surely and ζ3(k)Λ1,k+1 → 0 almost surely. To show that supk≥0 E[M2k ] < ∞, for any t ≥ 0 one observes that,\nE[M2k+1] =\nk∑\nj=0\n(ζ3(j)) 2 E[E[Λ21,j+1 | ∆j ]]\n≤2 k∑\nj=0\nE\n[( ζ3(j)\n2∆j\n)2 { E [( ( φ⊤ ( x0, νj +∆j ) − φ⊤ ( x0, νj −∆j ) ) v∗(νj) )2 | ∆j ]\n+E [( φ⊤ ( x0, νj +∆j ) − φ⊤ ( x0, νj −∆j ) ) v∗(νj) | ∆j\n]2} ]\nNow based on Assumption 9, the above expression implies\nE[M2k+1] ≤2 k∑\nj=0\nE\n[( ζ3(j)\n2∆j\n)2 2K21 (1 + ∆j) 2\n]\nCombining the above results with the step length conditions, there exists K = 4K21 > 0 such that\nsup k≥0\nE[M2k+1] ≤ K ∞∑\nj=0\nE\n[( ζ3(j)\n2∆j\n)2] + (ζ2(j)) 2 < ∞.\nSecond, by the “Min Common/Max Crossing” theorem, one can show ∂νL(θ, ν, λ)|ν=νk is a non-empty, convex and compact set. Therefore, by duality of directional directives and sub-differentials, i.e.,\nmax {g : g ∈ ∂νL(θ, ν, λ)|ν=νk} = lim ξ↓0 L(θ, νk + ξ, λ)− L(θ, νk − ξ, λ) 2ξ ,\none concludes that for λk = λ (converges in a slower time scale),\nλ+ ELM (k) = g(νk) +O(∆k), almost surely.\nThis further implies that\nΛ2,k = O(∆k), i.e., Λ2,k → 0 as k → ∞, almost surely.\nThird, since dθγ(x 0, ν|x0, ν) = 1, from definition of ǫθ(v∗(νk)) it is obvious that |Λ3,k| ≤ 2ǫθ(v∗(νk))E[1/∆k]. When t goes to infinity, ǫθ(v∗(νk))E[1/∆k] → 0 by assumption and Λ3,k → 0. Finally, as we have just showed that ζ2(k)Λ1,k+1 → 0, Λ2,k → 0 and Λ3,k → 0 almost surely, the ν−update in (67) is a stochastic approximations of an element in the differential inclusion\nNow we turn to the convergence analysis of ν. It can be easily seen that the ν−update in (18) is a noisy sub-gradient descent update with vanishing disturbance bias. This update can be viewed as an Euler discretization of the following differential inclusion ν̇ ∈ Υν [−g(ν)] , ∀g(ν) ∈ ∂νL(θ, ν, λ), (68) Thus, the ν−convergence analysis follows from analogous convergence analysis in step 1 of Theorem 2’s proof.\nStep 3 (Convergence of θ−update) We first analyze the actor update (θ−update). Since θ converges in a faster time scale than λ, one can assume λ in the θ−update as a fixed quantity. Furthermore, since v and ν converge in a faster scale than θ, one can also replace v and ν with their limits v∗(θ) and ν∗(θ) in the convergence analysis. In the following analysis, we assume that the initial state x0 ∈ X is given. Then the θ−update in (19) can be re-written as follows:\nθk+1 = ΓΘ ( θk − ζ2(k) ( ∇θ logµ(ak|xk, sk; θ)|θ=θk δk(v ∗(θk))\n1− γ\n)) . (69)\nSimilar to the trajectory based algorithm, we need to show that the approximation of ∇θL(θ, ν, λ) is Lipschitz in θ in order to show the convergence of the θ parameter. This result is generalized in the following proposition.\nProposition 10 The following function is a Lipschitz function in θ:\n1 1− γ ∑\nx,a,s πθγ(x, s, a|x0 = x0, s0 = ν)∇θ logµ(a|x, s; θ)  −v⊤φ(x, s) + γ ∑\nx′,s′\nP̄ (x′, s′|x, s, a)v⊤φ(x′, s′) + C̄(x, s, a)   .\nProof. First consider the feature vector v. Recall that the feature vector satisfies the linear equation Av = b where A and b are functions of θ found from the Hilbert space projection of Bellman operator. It has been shown in Lemma 1 of [7] that, by exploiting the inverse of A using Cramer’s rule, one can show that v is continuously differentiable\nof θ. Next, consider the γ− visiting distribution πθγ . From an application of Theorem 2 of [1] (or Theorem 3.1 of [31]), it can be seen that the stationary distribution πθγ of the process (xk, sk) is continuously differentiable in θ. Recall from Assumption (B1) that ∇θµ(ak|xk, sk; θ) is a Lipschitz function in θ for any a ∈ A and k ∈ {0, . . . , T − 1} and µ(ak|xk, sk; θ) is differentiable in θ. Therefore, by combining these arguments and noting that the sum of products of Lipschitz functions is Lipschitz, one concludes that ∇θL(θ, ν, λ) is Lipschitz in θ.\nConsider the case in which the value function for a fixed policy µ is approximated by a learned function approximator, φ⊤(x, s)v∗. If the approximation is sufficiently good, we might hope to use it in place of V θ(x, s) and still point roughly in the direction of the true gradient. Recall the temporal difference error (random variable) for given (xk, sk) ∈ X × R\nδk (v) = −v⊤φ(xk, sk) + γv⊤φ (xk+1, sk+1) + C̄(xk, sk, ak).\nDefine the v−dependent approximated advantage function\nÃθ,v(x, s, a) = Q̃θ,v(x, s, a)− v⊤φ(x, s),\nwhere Q̃θ,v(x, s, a) = γ ∑\nx′,s′\nP̄ (x′, s′|x, s, a)v⊤φ(x′, s′) + C̄(x, s, a).\nThe following Lemma first shows that δk(v) is an unbiased estimator of Ãθ,v.\nLemma 11 For any given policy µ and v ∈ Rκ2 , we have\nÃθ,v(x, s, a) = E[δk(v) | xk = x, sk = s, ak = a].\nProof. Note that for any v ∈ Rκ2 , E[δk(v) | xk = x, sk = s, ak = a, µ] = C̄(x, s, a)−v⊤φ(x, s)+γE [ v⊤φ(xk+1, sk+1) | xk = x, sk = s, ak = a ] .\nwhere\nE [ v⊤φ(xk+1, sk+1) | xk = x, sk = s, ak = a ] = ∑\nx′,s′\nP̄ (x′, s′|x, s, a)v⊤φ(x′, s′).\nBy recalling the definition of Q̃θ,v(x, s, a), the proof is completed. Now, we turn to the convergence proof of θ.\nTheorem 12 Suppose θ∗ is the equilibrium point of the continuous system θ satisfying\nΥθ [ −∇θL(θ, ν, λ)|ν=ν∗(θ) ] = 0. (70)\nThen the sequence of θ−updates in (19) converges to θ∗ almost surely.\nProof. First, the θ−update from (69) can be re-written as follows:\nθk+1 = ΓΘ ( θk + ζ2(k) ( −∇θL(θ, ν, λ)|ν=ν∗(θ),θ=θk + δθk+1 + δθǫ ))\nwhere\nδθk+1 = ∑\nx′,a′,s′\nπθkγ (x ′, s′, a′|x0 = x0, s0 = ν∗(θk))∇θ log µ(a′|x′, s′; θ)|θ=θk\nÃθk,v ∗(θk)(x′, s′, a′)\n1− γ\n−∇θ log µ(ak|xk, sk; θ)|θ=θk δk(v\n∗(θk))\n1− γ . (71)\nis a square integrable stochastic term of the θ−update and\nδθǫ = ∑\nx′,a′,s′\nπθkγ (x ′, s′, a′|x0 = x0, s0 = ν∗(θk)) ∇θ log µ(a′|x′, s′; θ)|θ=θk 1− γ (A θk (x′, s′, a′)− Ãθk,v ∗(θk)(x′, s′, a′))\n≤‖ψθk‖∞ 1− γ\n√( 1 + γ\n1− γ\n) ǫθk (v ∗(θk)).\nwhere ψθ(x, s, a) = ∇θ logµ(a|x, s; θ) is the “compatible feature”. The last inequality is due to the fact that for πθγ being a probability measure, convexity of quadratic functions implies\n∑\nx′,a′,s′\nπθγ(x ′, s′, a′|x0 = x0, s0 = ν∗(θ))(Aθ(x′, s′, a′)− Ãθ,v(x′, s′, a′))\n≤ ∑\nx′,a′,s′\nπθγ(x ′, s′, a′|x0 = x0, s0 = ν∗(θ))(Qθ(x′, s′, a′)− Q̃θ,v(x′, s′, a′))\n+ ∑\nx′,s′\ndθγ(x ′, s′|x0 = x0, s0 = ν∗(θ))(V θ(x′, s′)− Ṽ θ,v(x′, s′))\n=γ ∑\nx′,a′,s′\nπθγ(x ′, s′, a′|x0 = x0, s0 = ν∗(θ))\n∑\nx′′,s′′\nP̄ (x′′, s′′|x′, s′, a′)(V θ(x′′, s′′)− φ⊤(x′′, s′′)v)\n+\n√∑\nx′,s′\ndθγ(x′, s′|x0 = x0, s0 = ν∗(θ))(V θ(x′, s′)− Ṽ θ,v(x′, s′))2\n≤γ √ ∑\nx′,a′,s′\nπθγ(x′, s′, a′|x0 = x0, s0 = ν∗(θ)) ∑\nx′′,s′′\nP̄ (x′′, s′′|x′, s′, a′)(V θ(x′′, s′′)− φ⊤(x′′, s′′)v)2\n+\n√ ǫθ(v)\n1− γ\n≤√γ √ ∑\nx′′,s′′\n( dθγ(x′′, s′′|x0, ν∗(θ))− (1− γ)1{x0 = x′′, ν = s′′} ) (V θ(x′′, s′′)− φ⊤(x′′, s′′)v)2 +\n√ ǫθ(v)\n1− γ\n≤ √( 1 + γ\n1− γ\n) ǫθ(v)\nThen by Lemma 11, if the γ−stationary distribution πθγ is used to generate samples of (xk, sk, ak), one obtains E [δθk+1 | Fθ,k] = 0, where Fθ,k = σ(θm, δθm, m ≤ k) is the filtration generated by different independent trajectories. On the other hand, |δθǫ| → 0 as ǫθk(v∗(θk)) → 0. Therefore, the θ−update in (69) is a stochastic approximation of the ODE\nθ̇ = Υθ [ −∇θL(θ, ν, λ)|ν=ν∗(θ) ]\nwith an error term that is a sum of a vanishing bias and a Martingale difference. Thus, the convergence analysis of θ follows analogously from the step 2 of Theorem 2’s proof.\nStep 4 (Local Minimum) The proof of local minimum of (θ∗, ν∗) follows directly from the arguments in Step 3 of Theorem 2’s proof.\nStep 5 (The λ−update and Convergence to Saddle Point) Notice that λ−update converges in a slowest time scale, (18) can be rewritten using the converged v∗(λ), θ∗(λ) and ν∗(λ), i.e.,\nλk+1 = ΓΛ ( λk + ζ1(k) ( ∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ),λ=λk + δλk+1 )) (72)\nwhere δλk+1 = −∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ),λ=λk + ( ν∗(λk) + (−sk)+ (1− α)(1 − γ)1{xk = xT } − β ) (73) is a square integrable stochastic term of the λ−update. Similar to the θ−update, by using the γ−stationary distribution πθγ , one obtains E [δλk+1 | Fλ,k] = 0 where Fλ,k = σ(λm, δλm, m ≤ k) is the filtration of λ generated by different independent trajectories. As above, the λ−update is a stochastic approximation of the ODE\nλ̇ = Υλ [ ∇λL(θ, ν, λ) ∣∣∣∣ θ=θ∗(λ),ν=ν∗(λ) ]\nwith an error term that is a Martingale difference. Then the λ−convergence and the (local) saddle point analysis follows from analogous arguments in step 4 and 5 of Theorem 2’s proof.\nStep 2′ (Convergence of Multi-loop ν−update) Since ν converges on a faster timescale than θ and λ, the ν−update in (21) can be rewritten using the fixed (θ, λ), i.e.,\nνi+1 = ΓN ( νi − ζ2(i) ( λ− λ\n1− α ( P ( sT ≤ 0 | x0 = x0, s0 = νi, µ ) + δνM,i+1\n)))\n(74) and\nδνM,i+1 = −P ( sT ≤ 0 | x0 = x0, s0 = νi, µ ) + 1 {sT ≤ 0} (75)\nis a square integrable stochastic term of the ν−update. It is obvious thatE [δνM,i+1 | Fν,i] = 0, where Fν,i = σ(νm, δνm, m ≤ i) is the corresponding filtration of ν, the ν−update in (21) is a stochastic approximations of an element in the differential inclusion ∂νL(θ, ν, λ)|ν=νi for any i with an error term that is a Martingale difference, i.e.,\nλ 1− αP ( sT ≤ 0 | x0 = x0, s0 = νi, µ ) − λ ∈ −∂νL(θ, ν, λ)|ν=νi .\nThus, the ν−update in (74) can be viewed as an Euler discretization of the differential inclusion in (68), and the ν−convergence analysis follows from analogous convergence analysis in step 1 of Theorem 2’s proof."
    }, {
      "heading" : "C Experimental Results",
      "text" : ""
    }, {
      "heading" : "C.1 Problem Setup and Parameters",
      "text" : "The house purchasing problem can be reformulated as follows\nmin θ\nE [ Dθ(x0) ] subject to CVaRα ( Dθ(x0 ) ≤ β. (76)\nwhere Dθ(x0) = ∑T\nk=0 γ k (1{uk = 1}ck + 1{uk = 0}ph) | x0 = x, µ. We will set\nthe parameters of the MDP as follows: x0 = [1; 0], ph = 0.1, T = 20, γ = 0.95, fu = 1.5, fd = 0.8 and p = 0.65. For the risk constrained policy gradient algorithm, the step-length sequence is given as follows,\nζ1(i) = 0.1\ni , ζ2(i) =\n0.05 i0.8 , ζ3(i) = 0.01 i0.55 , ∀i.\nThe CVaR parameter and constraint threshold are given by α = 0.9 and β = 1.9. The number of sample trajectories N is set to 100.\nFor the risk constrained actor critic algorithm, the step-length sequence is given as follows,\nζ1(i) = 1\ni , ζ2(i) =\n1\ni0.85 , ζ3(i) =\n0.5 i0.7 , ζ3(i) = 0.5 i0.55 , ∆k = 0.5 i0.1 , ∀i.\nThe CVaR parameter and constraint threshold are given by α = 0.9 and β = 2.5. One can later see that the difference in risk thresholds is due to the different family of parametrized Boltzmann policies.\nThe parameter bounds are given as follows: λmax = 1000, Θ = [−60, 60]κ1 and Cmax = 4000 > x0 × fTu ."
    }, {
      "heading" : "C.2 Trajectory Based Algorithms",
      "text" : "In this section, we have implemented the following trajectory based algorithms.\n1. PG: This is a policy gradient algorithm that minimizes the expected discounted cost function, without considering any risk criteria.\n2. PG-CVaR: This is the CVaR constrained simulated trajectory based policy gradient algorithm that is given in Section 4.\nIt is well known that a near-optimal policy µ was obtained using the LSPI algorithm with 2-dimensional radial basis function (RBF) features. We will also implement the 2-dimensional RBF feature function φ and consider the family Boltzmann policies for policy parametrization\nµ(a|x; θ) = exp(θ ⊤φ(x, a))∑\na′∈A exp(θ ⊤φ(x, a′))\n.\nThe experiments for each algorithm comprised of the following two phases:\n1. Tuning phase: Here each iteration involved the simulation run with the nominal policy parameter θ where the run length for a particular policy parameter is at most T steps. We run the algorithm for 1000 iterations and stop when the parameter (θ, ν, λ) converges.\n2. Converged run: Followed by the tuning phase, we obtained the converged policy parameter θ∗. In the converged run phase, we perform simulation with this policy parameter for 1000 runs where each simulation generates a trajectory of at most T steps. The results reported are averages over these iterations.\nC.3 Incremental Based Algorithm\nOn the other hand, we have also implemented the following incremental based algorithms.\n1. AC: This is an actor critic algorithm that minimizes the expected discounted cost function, without considering any risk criteria. This is similar to Algorithm 1 in [6].\n2. AC-CVaR-Semi-Traj.: This is the CVaR constrained multi-loop actor critic algorithm that is given in Section 5.\n3. AC-CVaR-SPSA: This is the CVaR constrained SPSA actor critic algorithm that is given in Section 5.\nSimilar to the trajectory based algorithms, we will implement the RBFs as feature functions for [x; s] and consider the family of augmented state Boltzmann policies,\nµ(a|(x, s); θ) = exp(θ ⊤φ(x, s, a))∑\na′∈A exp(θ ⊤φ(x, s, a′))\n.\nSimilarly, the experiments also comprise of two phases: 1) the tuning phase where the set of parameters (v, θ, ν, λ) is obtained after the algorithm converges, and 2) the converged run where the policy parameter is simulated for 1000 runs."
    }, {
      "heading" : "D Bellman Equation and Projected Bellman Equation for Expected Utility Function",
      "text" : ""
    }, {
      "heading" : "D.1 Bellman Operator for Expected Utility Functions",
      "text" : "First, we want find the Bellman equation for the objective function\nE [ Dθ(x0) | x0 = x0, s0 = s0, µ ] + λ 1− αE [[ Dθ(x0)− s0 ]+ | x0 = x0, s0 = s0, µ ] (77) where λ and (x0, s0) ∈ X × R are given.\nFor any function V : X × R → R, recall the following Bellman operator on the augmented space X × R:\nTθ[V ](x, s) := ∑\na∈A\nµ(a|x, s; θ)   C̄(x, s, a) + ∑\nx′,s′\nγP̄ (x′, s′|x, s, a)V (x′, s′)    .\nFirst, it is easy to show that this Bellman operator satisfies the following properties.\nProposition 13 The Bellman operator Tθ[V ] has the following properties:\n• (Monotonicity) If V1(x, s) ≥ V2(x, s), for anyx ∈ X , s ∈ R, then Tθ[V1](x, s) ≥ Tθ[V2](x, s).\n• (Constant shift) For K ∈ R, Tθ[V +K](x, s) = Tθ[V ](x, s) + γK .\n• (Contraction) ‖Tθ[V1]− Tθ[V2]‖∞ ≤ γ‖V1 − V2‖∞,\nwhere ‖f‖∞ = maxx∈X ,s∈R |f(x, s)|.\nProof. The proof of monotonicity and constant shift properties follow directly from the definitions of the Bellman operator. Furthermore, denote c = ‖V1 − V2‖∞. Since\nV2(x, s) − ‖V1 − V2‖∞ ≤ V1(x, s) ≤ V2(x, s) + ‖V1 − V2‖∞, ∀x ∈ X , s ∈ R,\nby monotonicity and constant shift property,\nTθ[V2](x, s)−γ‖V1−V2‖∞ ≤ Tθ[V1](x, s) ≤ Tθ[V2](x, s)+γ‖V1−V2‖∞ ∀x ∈ X , s ∈ R.\nThis further implies that\n|Tθ[V1](x, s) − Tθ[V2](x, s)| ≤ γ‖V1 − V2‖∞ ∀x ∈ X , s ∈ R\nand the contraction property follows. The following theorems show there exists a unique fixed point solution to Tθ[V ](x, s) = V (x, s), where the solution equals to the value function expected utility.\nTheorem 14 (Equivalence Condition) For any bounded function V0 : X × R → R, there exists a limit function V θ such that V θ(x, s) = limN→∞ TNθ [V0](x, s). Furthermore,\nV θ(x0, s0) = E [ Dθ(x0) | x0 = x0, µ ] + λ 1− αE [[ Dθ(x0)− s0 ]+ | x0 = x0, s0 = s0, µ ] .\nProof. The first part of the proof is to show that for any x ∈ X and s ∈ R,\nVn(x, s) := T n θ [V0](x 0, s0) = E\n[ n−1∑\nk=0\nγkC̄(xk, sk, ak) + γ nV0(xn, sn) | x0 = x, s0 = s, µ\n]\n(78) by induction. For n = 1, V1(x, s) = Tθ[V0](x, s) = E [ C̄(x0, s0, a0) + γV0(x1, s1) | x0 = x, s0 = s, µ ] . By induction hypothesis, assume (78) holds at n = k. For n = k + 1,\nVk+1(x, s) :=T k+1 θ [V0](x, s) = Tθ[Vk](x, s)\n= ∑\na∈Ā\nµ(a|x, s; θ)   C̄(x, s, a) + ∑\nx′,s′\nγP̄ (x′, s′|x, s, a)Vk ( x′, s′ )   \n= ∑\na∈Ā\nµ(a|x, s; θ)\n \nC̄(x, s, a) + ∑\nx′,s′\nγP̄ (x′, s′|x, s, a)\nE\n[ k−1∑\nk=0\nγkC̄(xk, sk, ak) + γ kV0(xk, sk) | x0 = x′, s0 = s′, µ\n]}\n= ∑\na∈Ā\nµ(a|x, s; θ)   C̄(x, s, a) + ∑\nx′,s′\nγP̄ (x′, s′|x, s, a)\nE\n[ k∑\nt=1\nγkC̄(xk, sk, ak) + γ kV0(xk+1, sk+1) | x1 = x′, s1 = s′, µ\n]}\n=E\n[ k∑\nk=0\nγkC̄(xk, sk, ak) + γ k+1V0(xk+1, sk+1) | x0 = x, s0 = s, µ ] .\nThus, the equality in (78) is proved by induction. The second part of the proof is to show that V θ(x0, s0) := limn→∞ Vn(x0, s0) and\nV θ(x0, s0) = E [ Dθ(x0) | x0 = x0, µ ] + λ 1− αE [[ Dθ(x0)− s0 ]+ | x0 = x0, s0 = s0, µ ] .\nFrom the assumption of transient policies, one note that for any ǫ > 0 there exists a sufficiently large k > N(ǫ) such that ∑∞ t=k P(xn = z|x0, µ) < ǫ for z ∈ X . This implies P(T < ∞) > 1 − ǫ. Since V0(x, s) is bounded for any x ∈ X and s ∈ R, the\nabove arguments imply V θ(x0, s0) ≤E [ T−1∑\nk=0\nγkC̄(xk, sk, ak) | x0 = x0, s0 = s0, µ ] (1− ǫ) + ǫ ( λ\n1− α (|s 0|+ Cmax) + Cmax 1− γ\n)\n+ lim n→∞ E\n[ n−1∑\nt=T\nγkC̄(xk, sk, ak) + γ nV0(xn, sn) | x0 = x0, s0 = s0, µ ] (1− ǫ)\n≤ lim n→∞ E\n[ T−1∑\nk=0\nγkC(xk, ak) | x0 = x0, s0 = s0, µ ] (1− ǫ) + ǫ ( 1− ǫ ǫ γn‖V0‖∞ + λ 1− α (|s 0|+ Cmax) + Cmax 1− γ )\n+ E [ γT C̄(xT , sT , aT ) | x0 = x0, s0 = s0, µ ] (1− ǫ)\n=E [ Dθ(x0) | x0 = x0, s0 = s0, µ ] (1− ǫ)\n+ λ 1− αE [ γT (−sT )+ | x0 = x0, s0 = s0, µ ] (1− ǫ) + ǫ ( λ 1− α (|s 0|+ Cmax) + Cmax 1− γ )\n=E [ Dθ(x0) | x0 = x0, s0 = s0, µ ] (1− ǫ)\n+ λ 1− αE [[ Dθ(x0)− s0 ]+ | x0 = x0, s0 = s0, µ ] (1− ǫ) + ǫ ( λ 1− α (|s 0|+ Cmax) + Cmax 1− γ ) .\nThe first inequality is due to the fact for x0 = x0, s0 = s0,\nlim n→∞\nn∑\nk=0\nγkC̄(xk, sk, ak) ≤ λ\n1− α |s 0|+\n( 1 + λ\n1− α\n) ∞∑\nk=0\nγk|c(xk, ak)| ≤ λ\n1− α (|s 0|+Cmax)+ Cmax 1− γ ,\nthe second inequality is due to 1) V0 is bounded, C̄(x, s, a) = C(x, a) when x 6= xT and 2) for sufficiently large k > N(ǫ) and any z ∈ X , ∞∑\nt=k\n∑\ns\nP(xk = z, sk = s|x0 = x0, s0 = s0, µ)ds = ∞∑\nt=k\nP(xk = z|x0 = x0, s0 = s0, µ) < ǫ.\nThe first equality follows from the definition of transient policies and the second equality follows from the definition of stage-wise cost in the ν−augmented MDP.\nBy similar arguments, one can also show that\nV θ(x0, s0) ≥ ǫ ( − lim\nn→∞ (1− ǫ)γn‖V0‖∞/ǫ− Cmax/(1− γ)\n) + (1 − ǫ)\n( E [ Dθ(x0) | x0 = x0, s0 = s0, µ ] + λ 1− αE [[ Dθ(x0)− s0 ]+ | x0 = x0, s0 = s0, µ ]) .\nTherefore, by taking ǫ → 0, we have just shown that for any (x0, s0) ∈ X × R, V θ(s0, s0) = E [ Dθ(x0) | x0 = x0, s0 = s0, µ ] +λ/(1−α)E [[ Dθ(x0)− s0 ]+ | x0 = x0, s0 = s0, µ ] .\nApart from the analysis in [4] where a fixed point result is defined based on the following specific set of functions Vθ , we are going to provide the fixed point theorem for general spaces of augmented value functions.\nTheorem 15 (Fixed Point Theorem) There exists a unique solution to the fixed point equation: Tθ[V ](x, s) = V (x, s), ∀x ∈ X and s ∈ R. Let V ∗ : X × R → R be such unique fixed point solution. Then,\nV ∗(x, s) = V θ(x, s), ∀x ∈ X , s ∈ R.\nProof. For Vk+1(x, s) = Tθ[Vk](x, s) starting at V0 : X × R → R one obtains by contraction that ‖Vk+1 − Vk‖∞ ≤ γ‖Vk − Vk−1‖∞. By the recursive property, this implies ‖Vk+1 − Vk‖∞ ≤ γk‖V1 − V0‖∞. It follows that for every k ≥ 0 and m ≥ 1,\n‖Vk+m − Vk‖∞ ≤ m∑\ni=1\n‖Vk+i − Vk+i−1‖∞ ≤ γk(1 + γ + . . .+ γm−1)‖V1 − V0‖∞\n≤ γ k\n1− γ ‖V1 − V0‖∞.\nTherefore, {Vk} is a Cauchy sequence and must converge to V ∗ since (B(X × R), ‖ · ‖∞) is a complete space. Thus, we have for k ≥ 1,\n‖Tθ[V ∗]−V ∗‖∞ ≤ ‖Tθ[V ∗]−Vk‖∞+‖Vk−V ∗‖∞ ≤ γ‖Vk−1−V ∗‖∞+‖Vk−V ∗‖∞.\nSince Vk converges to V ∗, the above expression implies Tθ[V ∗](x, s) = V ∗(x, s) for any (x, s) ∈ X ×R. Therefore, V ∗ is a fixed point. Suppose there exists another fixed point Ṽ ∗. Then,\n‖Ṽ ∗ − V ∗‖∞ = ‖Tθ[Ṽ θ]− Tθ[V θ]‖∞ ≤ γ‖Ṽ θ − V θ‖∞\nfor γ ∈ (0, 1). This implies that Ṽ ∗ = V ∗. Furthermore, since V θ(x, s) = limn→∞ T nθ [V0](x, s) with V0 : X × R → R being an arbitrary initial value function. By the following convergence rate bound inequality\n‖T kθ [V0]− V ∗‖∞ = ‖T kθ [V0]− T kθ [V ∗]‖∞ ≤ γk‖V0 − V ∗‖∞, γ ∈ (0, 1),\none concludes that V θ(x, s) = V ∗(x, s) for any (x, s) ∈ X × R."
    }, {
      "heading" : "D.2 The Projected Bellman Operator",
      "text" : "Consider the v−dependent linear value function approximation of V θ(x, s), in the form of φ⊤(x, s)v, where φ(x, s) ∈ Rκ2 represents the state-dependent feature. The feature vectors can also be dependent on θ as well. But for notational convenience, we drop the indices corresponding to θ. The low dimensional subspace is therefore SV = {Φv|v ∈ Rκ2} where φ : X × R → Rκ2 is a function mapping such that Φ(x, s) = φ⊤(x, s). We also make the following standard assumption on the rank of matrix φ. More information relating to the feature mappings and function approximation φ can be found in Appendix. Let v∗ ∈ Rκ2 be the best approximation parameter vector. Then Ṽ ∗(x, s) = (v∗)⊤φ(x, s) is the best linear approximation of V θ(x, s).\nOur goal is to estimate v∗ from simulated trajectories of the MDP. Thus, it is reasonable to consider the projections from R onto SV with respect to a norm that is weighted according to the occupation measure dθγ(x\n′, s′|x, s), where (x0, s0) = (x, s) is the initial condition of the augmented MDP. For a function y : X × R → R, we introduce the weighted norm: ‖y‖d = √∑ x,s d(x\n′, s′|x, s)(y(x′, s′))2 where d is the occupation measure (with non-negative elements). We also denote by Π the projection from X ×R to SV . We are now ready to describe the approximation scheme. Consider the following projected fixed point equation\nV (x, s) = ΠTθ[V ](x, s)\nwhere Tθ is the Bellman operator with respect to policy θ and let Ṽ ∗ denote the solution of the above equation. We will show the existence of this unique fixed point by the following contraction property of the projected Bellman operator: ΠTθ.\nLemma 16 There exists κ ∈ (0, 1) such that\n‖ΠTθ[V1]−ΠTθ[V2]‖d ≤ κ‖V1 − V2‖d.\nProof. Note that the projection operator Π is non-expansive:\n‖ΠTθ[V1]−ΠTθ[V2]‖2d ≤ ‖Tθ[V1]− Tθ[V2]‖2d.\nOne further obtains the following expression:\n‖Tθ [V1]− Tθ[V2]‖2d\n= ∑\nx,s\nd(x, s|x, s)\n  ∑\ny,a,s′\nγµ(a|x, s; θ)P̄ (y, s′|x, s, a)(V1(y, s′)− V2(y, s′))\n  2\n≤ ∑\nx,s\nd(x, s|x, s)\n  ∑\ny,a,s′\nγ2µ(a|x, s; θ)P̄ (y, s′|x, s, a)(V1(y, s′)− V2(y, s′))2  \n= ∑\ny,s′\n( d(y, s′|x, s)− (1− γ)1{x = y, s = s′} ) γ(V1(y, s ′)− V2(y, s′))2\n≤γ‖V1 − V2‖2d.\nThe first inequality is due to the fact that µ(a|x, s; θ), P̄ (y, s′|x, s, a) ∈ [0, 1] and convexity of quadratic function, the second equality is based on the property of γ−visiting distribution. Thus, we have just shown that ΠTθ is contractive with κ = √ γ ∈ (0, 1).\nTherefore, by Banach fixed point theorem, a unique fixed point solution exists for equation: ΠTθ[V ](x, s) = V (x, s) for any x ∈ X , s ∈ R. Denote by Ṽ ∗ the fixed point solution and v∗ be the corresponding weight, which is unique by the full rank assumption. From Lemma 16, one obtains a unique value function estimates from the following projected Bellman equation:\nΠTθ[Ṽ ∗](x, s) = Ṽ ∗(x, s), Ṽ ∗(x, s, a) = (v∗)⊤φ(x, s). (79)\nAlso we have the following error bound of the value function approximation.\nLemma 17 Let V ∗ be the fixed point solution of Tθ[V ](x, s) = V (x, s) and v∗ be the unique solution for ΠTθ[Φv](x, s) = φ⊤(x, s)v. Then, for some κ ∈ (0, 1),\n‖V ∗ − Ṽ ∗‖d = ‖V ∗ − Φv∗‖d ≤ 1√ 1− γ ‖V ∗ −ΠV ∗‖d.\nProof. Note that by the Pythagorean theorem of projection,\n‖V ∗ − Φv∗‖2d = ‖V ∗ − ΠV ∗‖2d + ‖ΠV ∗ −Φv∗‖2d = ‖V ∗ − ΠV ∗‖2d + ‖ΠTθ[V ∗]− ΠTθ[Φv∗]‖2d ≤ ‖V ∗ − ΠV ∗‖2d + κ2‖V ∗ − Φv∗‖2d\nTherefore, by recalling κ = √ γ, the proof is completed by rearranging the above inequality. This implies that if V ∗ ∈ SV , V ∗(x, s) = Ṽ ∗(x, s) for any (x, s) ∈ X × R.\nNote that we can re-write the projected Bellman equation in explicit form as follows:\nΠTθ[Φv ∗] = Φv∗\n⇐⇒ Π\n     ∑\na∈A\nµ(a|x, s; θ)  C̄(x, s, a) + γ ∑\ny,s′\nP̄ (y, s′|x, s, a)(v∗)⊤φ ( y, s′ )     \nx∈X ,s∈R\n  = Φv∗.\nBy the definition of projection, the unique solution v∗ ∈ Rℓ satisfies v∗ ∈ argmin\nv ‖Tθ[Φv]− Φv‖2dθγ\n⇐⇒ v∗ ∈ argmin v\n∑\ny,s′\ndθγ(y, s ′|x, s)·\n  ∑\na′∈A\nµ(a′|y, s′; θ)  C̄(y, s′, a′) + γ ∑\nz,s′′\nP̄ (z, s′′|y, s′, a′)φ⊤ ( z, s′′ ) vds′′  − φ⊤(y, s′)v   2 .\nBy the projection theorem on Hilbert space, the orthogonality condition for v∗ becomes:\n∑\ny,a′,s′\nπθγ(y, s ′, a′|x, s)φ(y, s′)(v∗)⊤φ(y, s′)\n= ∑\ny,a′,s′\n{ πθγ(y, s ′, a′|x, s)φ(y, s′)C̄(y, s′, a′) + γ ∑\nz,s′′\nπθγ(y, s ′, a′|x, s)P̄ (z, s′′|y, s′, a′)φ(y, s′)φ⊤ ( z, s′′ )} v∗.\nThis condition can be written as Av∗ = b where\nA = ∑\ny,a′,s′\nπθγ(y, s ′, a′|x, s)φ(y, s′)\n φ⊤(y, s′)− γ ∑\nz,s′′\nP̄ (z, s′′|y, s′, a)φ⊤ (z, s′′) ds′′  \n(80) is a finite dimensional matrix in Rκ2×κ2 and\nb = ∑\ny,a′,s′\nπθγ(y, s ′, a′|x, s)φ(y, s′)C̄(y, s′, a′). (81)\nis a finite dimensional vector in Rκ2 . The matrix A is invertible since Lemma 16 guarantees that (79) has a unique solution v∗. Note that the projected equation Av = b can be re-written as v = v − ξ(Av − b) for any positive scaler ξ ≥ 0. Specifically, since\nAv−b = ∑\ny,a′,s′\nπθγ(y, s ′, a′|x, s)φ(y, s′)\n v⊤φ(y, s′)− ∑\nz,s′′\nP̄ (z, s′′|y, s′, a′)(γv⊤φ (z, s′′) + C̄(y, s′, a′))   ,\none obtains\nAv − b = Eπθγ [ φ(xk, sk) ( v⊤φ(xk, sk)− γv⊤φ (xk+1, sk+1)− C̄(xk, sk, ak) )]\nwhere the occupation measure πθγ(x, s, a|x0, ν) is a valid probability measure. Recall from the definitions of (A, b) that\nA =Eπ θ γ [ φ(xk, sk) ( φ⊤(xk, sk)− γφ⊤ (xk+1, sk+1) )] ,\nb =Eπ θ γ [ φ(xk, sk)C̄(xk, sk, ak) ]\nwhere Eπ θ γ is the expectation induced by the occupation measure (which is a valid probability measure)."
    }, {
      "heading" : "E Supplementary: Gradient with Respect to θ",
      "text" : "By taking gradient of V θ with respect to θ, one obtains\n∇θV θ(x0, ν) = ∑\na\n∇θµ(a|x0, ν; θ)Qθ(x0, ν, a) + µ(a|x0, ν; θ)∇θQθ(x0, ν, a)\n= ∑\na\n∇θµ(a|x0, ν; θ)Qθ(x0, ν, a) + µ(a|x0, ν; θ)∇θ  C̄(x0, ν, a) + ∑\nx′,s′\nγP̄ (x′, s′|x0, ν, a)V θ ( x′, s′ )  \n= ∑\na\n∇θµ(a|x0, ν; θ)Qθ(x0, ν, a) + γµ(a|x0, ν; θ)\n  ∑\nx1,s1\nγP̄ (x1, s1|x0, ν, a)∇θV θ ( x1, s1\n) \n\n=hθ(x0, ν) + γ ∑\nx1,s1,a0\nµ(a0|x0, ν; θ)P̄ (x1, s1|x0, ν, a0)∇θV θ ( x1, s1 )\nwhere hθ(x0, ν) = ∑\na\n∇θµ(a|x0, ν; θ)Qθ(x0, ν, a).\nSince the above expression is a recursion, one further obtains ∇θV θ(x0, ν) =hθ(x0, ν) + γ ∑\na,x1,s1\nµ(a|x0, ν; θ)P̄ (x1, s1|x0, ν, a)\n hθ(x1, s1) + γ ∑\na1,x2,s2\nµ(a1|x1, s1; θ)P̄ (x2, s2|x1, s1, a1)∇θV θ ( x2, s2\n) \n .\nBy the definition of occupation measures, the above expression becomes\n∇θV θ(x0, ν) = ∞∑\nk=0\nγk ∑\nx′,a′,s′\nµ(a′|x′, s′; θ)P̄ (xk = x′, sk = s′|x0 = x0, s0 = ν)hθ(x′, s′)\n= 1 1− γ ∑\nx′,s′\ndθγ(x ′, s′|x0 = x0, s0 = ν)hθ(x′, s′)\n= 1 1− γ ∑\nx′,s′\ndθγ(x ′, s′|x0 = x0, s0 = ν)\n∑\na′∈A\n∇θµ(a′|x′, s′; θ)Qθ(x′, s′, a′)\n= 1 1− γ ∑\nx′,a′,s′\nπθγ(x ′, s′, a′|x0 = x0, s0 = ν)∇θ logµ(a′|x′, s′; θ)Qθ(x′, s′, a′)\n= 1 1− γ ∑\nx′,a′,s′\nπθγ(x ′, s′, a′|x0 = x0, s0 = ν)∇θ logµ(a′|x′, s′; θ)Aθ(x′, s′, a′)\n(82)\nwhere Aθ(x, s, a) = Qθ(x, s, a)− V θ(x, s)\nis the advantage function. The last equality is due to the fact that ∑\na\nµ(a|x, s; θ)∇θ logµ(x|s, a; θ)V θ(x, s) =V θ(x, s) · ∑\na\n∇θµ(a|x, s; θ)\n=V θ(x, s) · ∇θ ∑\na\nµ(a|x, s; θ) = ∇θ(1) · V θ(x, s) = 0.\nThus, the gradient of the Lagrangian function is\n∇θL(θ, ν, λ) = ∇θV θ(x, s) ∣∣∣∣ x=x0,s=ν ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.",
    "creator" : "LaTeX with hyperref package"
  }
}