{
  "name" : "1706.00061.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The Sample Complexity of Online One-Class Collaborative Filtering",
    "authors" : [ "Reinhard Heckel", "Kannan Ramchandran" ],
    "emails" : [ "<heckel@berkeley.edu>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Recommender systems seek to identify the subset of a large collection of items that a user likes (Aggarwal, 2016). In practice, recommender systems often use collaborative filtering (CF) (Ekstrand et al., 2011) to identify items a given\n1University of California, Berkeley, California, USA. Correspondence to: Reinhard Heckel <heckel@berkeley.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s).\nuser likes, based on ratings that this user and a large number of other users have provided in the past. To this end, a user-based CF algorithm first identifies similar users, and then predicts the ratings of a given user from the ratings provided by similar users. In practice, recommender systems typically operate in an online fashion, i.e., items are recommended to users over time, and the ratings obtained in response to recommendations are used to improve future recommendations.\nHowever, in many application areas of recommender systems, users only occasionally rate what they ‘like’, and never what they ‘dislike’. E.g., in e-commerce, such as Amazon’s recommender system, an item (or a set of items) is recommended to a user, and the user either purchases the item, which indicates a ‘like’, or the user does not purchase the item. Not purchasing the item, however, does not necessarily indicate a ‘dislike’, since the user might not even have considered the recommendation. Other examples of such feedback include implicit ratings such as viewing a webpage and listening to a song (Hu et al., 2008), and business-to-business recommender systems (Heckel et al., 2017). The problem of generating recommendations based on positive ratings only is known as one-class CF (Pan et al., 2008). The lack of negative ratings is often considered to make this problem challenging (Pan et al., 2008). However, it is unclear whether it is fundamentally more difficult in the absence of negative ratings to identify the user’s preferences, in the sense that the sample complexity (i.e., the number of ratings required to make good recommendations) is fundamentally larger. Additionally, there is little theoretical understanding on how the probability of a user responding to a recommendation, pf , affects the sample complexity of a one-class CF algorithm and in particular its cold start time, i.e., the number of recommendations the algorithm needs to invest in learning the user’s preferences before being able to make good recommendations. In this paper, we address those two questions, that turn out to be closely related.\nTo this end, we introduce a probabilistic model for a oneclass online recommender system and a corresponding online user-based CF algorithm, termed User-CF, and analyze its performance. Our model, and elements of our algorithm, are inspired by a related model and algorithm by Bresler et al. (2014) for the two-class CF problem, i.e.,\nar X\niv :1\n70 6.\n00 06\n1v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\n01 7\nfor a setup where positive and negative ratings are available. In a nutshell, each user in our model has a latent probability preference vector which describes the extent to which she likes or dislikes each item. Similar users have similar preference vectors. At a given time step t = 0, 1, . . ., the User-CF algorithm recommends a single item to each user, typically different for each user. With probability specified by a corresponding preference vector, the user likes or dislikes the recommended item. If the user likes the item, the user rates it with probability pf , and if the user does not like the item, no rating is given. An item that has been rated cannot be recommended again, since a rating often corresponds to consuming an item, and there is little point in, e.g., recommending a product that has been previously purchased in the past for a second time. While in practice the probability pf could be different for each user, for ease of presentation, we assume that pf is constant over all users. The goal of the User-CF algorithm is to maximize the number of recommendations that a users likes.\nThe User-CF algorithm consists of an exploitation step that recommends items that similar users have rated positively, and two kinds of exploration steps; one to learn the preferences of the users and the other to explore similarity between users.\nOur main result, stated in Section 4, guarantees that after a certain cold start time in which the User-CF algorithm recommends the order of (log(N)/p2f ) 1 1−α items to each user, a fraction 1−c/pf of the remaining recommendations given by the User-CF algorithm are optimal. Here, α is a learning rate that can be chosen very close to zero, N is the number of users, and c a numerical constant. The cold start time is required to identify similar users and learn their preferences regarding a few items. We also show that any algorithm has to make on the order of 1/p2f recommendations before it can make good recommendations, therefore the User-CF algorithm is near optimal. The fraction c/pf of the remaining time steps is associated with learning the preferences of the users. This ‘cost’ of c/pf does not have to be paid upfront, but is paid continuously: After the cold start time, the User-CF algorithm starts exploiting successfully. Again, a fraction of the recommendations proportional to 1/pf is necessary to learn the preference of the users. Our numerical results in Section 5 show that even if our data is not generated from the probabilistic model, but is based on real data, the cold start time and the fraction of time steps required to learn the preferences of the users are nearly proportional to 1/p2f and 1/pf , respectively.\nAs a consequence of this result, we find that obtaining positive and negative ratings instead of only positive ones, improves the number of ratings required for the initial cold start period by a factor of pf . To see this, note that the ex-\npected number of ratings obtained by a user in a given number of time steps or equivalently after a given number of recommendations is proportional to pf . Thus, the number of ratings required for the initial cold start time is inversely proportional to pf , and the number of ratings required for continuously learning the preferences is independent of pf . Since pf = 1 corresponds to users giving positive and negative feedback (no positive feedback implies dislike when pf = 1), the number of ratings required for the cold-start time is by a factor of 1/pf larger than the number of ratings required by a user-based CF algorithm that obtains positive and negative ratings.\nThose findings are relevant for the design of recommender systems, since both pf and whether positive, or negative and positive ratings are obtained can often be incorporated in the design of a recommender system. Therefore an understanding of the associated benefits and costs in terms of sample complexity, as provided in this paper, is important. We finally note that the goal of this paper is not to improve upon state-of-the art algorithms, but rather to inform the design of algorithms and what to expect in terms of sample complexity as a function of the various parameters involved.\nRelated literature: While to the best of our knowledge, this is the first work that analytically studies one-class CF in an online setting, theoretical results have been established for the two or multiple class CF problems. One of the first analytical results on user-based CF algorithms, an asymptotic performance guarantee under a probabilistic model, was established by Biau et al. (2010). Most related to our approach is the Collaborative-Greedy algorithm studied by Bresler et al. (2014) for the online twoclass CF problem. The Collaborative-Greedy algorithm differs from our User-CF algorithm in selecting the nearest neighbors based on thresholding similarity, instead of selecting the k most similar users, and in the way preferences of the users are explored. This difference in the exploration steps is crucial for establishing that after the cold start period, our User-CF algorithm makes optimal recommendations in a fraction 1 − c/pf of the remaining time steps. Dabeer (2013) studies a probabilistic model in an online setup, and Barman & Dabeer (2012) study a probabilistic model in an offline setup, and state performance guarantees for a two-class user-based CF algorithm. Closely related to user-based CF is item-based CF. Item-based CF exploits similarity in item space by recommending items similar to those a given user has rated positively in the past. Our results do not extend trivially to item-based CF, since a corresponding analysis requires assumptions on the similarity in item space, and additionally the exploration strategies of item-based CF algorithms are considerably different. We do not discuss item based CF algorithms here, but refer to (Bresler et al., 2015) for a recent analysis of an item\nbased CF algorithm for the two-class CF problem. Next, we note that Deshpande & Montanari (2012) study recommender systems in the context of multi-armed bandits (Bubeck & Cesa-Bianchi, 2012). Specifically, Deshpande & Montanari (2012) consider a model where the (continuous) ratings are described by the inner product of a user and item feature vector, and assume the item feature vectors to be given.\nA conceptually related online learning problem are multiarmed bandits with dependent arms (Pandey et al., 2007). Specifically, in this variant of the multi-armed bandit problem, the arms are grouped into clusters, and the arms within each cluster are dependent. The assignments of arms to clusters are assumed known. In our paper, we assume that users cluster in user types that have similar distributions. Therefore, the learning problem in our paper can be viewed as an multi-armed bandit problem with dependent arms, but the assignment of the arms to clusters is unknown.\nFinally, we note that a class of learning problems reminiscent to that considered here is partial monitoring (Bartók et al., 2014). While partial monitoring has been studied in the context of recommender systems (Kveton et al., 2015), we are not aware of papers on partial monitoring in collaborative filtering.\nOutline: In Section 2, we formally specify our model, motivate it, and state the CF problem. Sections 3 and 4 contain the User-CF algorithm and corresponding performance guarantee, respectively. In Section 5 we provide numerical results on real data. The proof of our main result can be found in the supplementary material."
    }, {
      "heading" : "2. Model and learning problem",
      "text" : "In this section we introduce the probabilistic model and learning problem considered in this paper. As mentioned previously, this model is inspired by that in (Bresler et al., 2014) for the two-class CF problem.\nModel: Consider N users and M items. A user may like an item (+1), or dislike an item (−1). Associated with each user is an (unknown) latent preference vector pu ∈ [0, 1]M whose entries pui are the probabilities of user u liking item i. We assume that an item i is either “likable” for user u, i.e., pui > 1/2 + ∆, for some ∆ ∈ (0, 1/2], or “not likable”, i.e., pui < 1/2 −∆. The hidden ranking Rhiddenui is obtained at random as Rhiddenui = 1 (like) with probability pui, and Rhiddenui = −1 (dislike) with probability 1 − pui. The ratings are stochastic to model that users are not fully consistent in their rating; the parameter ∆ quantifies the inconsistency (or uncertainty or noise). The one-class aspect is incorporated in our model by assuming that users never reveal that they dislike an item.\nSpecifically, an CF algorithm operates on the model as fol-\nlows. At each time step t = 0, 1, . . . the algorithm recommends a single item i = i(t, u) to each user u—typically this item is different for each user—and obtains an realization of the binary random variable\nRui = { Zui ∼ Bernoulli(pf ), if Rhiddenui = 1, 0, if Rhiddenui = −1\nin response, independently across u and i. It follows that P [Rui = 1] = puipf and P [Rui = 0] = 1 − puipf ,. Here, pf corresponds to the probability of a user reporting a positive rating. As mentioned before, while one might treat the slightly more general case of the probability pf being different for each user, for ease of presentation, we assume that it is constant over the users. If Rui = 1, user u consumes item i, and i will not be recommended to u in subsequent time steps. Note that Rui = 0 means that either user u does not like item i (Rhiddenui = −1), or user u did not respond to the recommendation. Therefore, if Rui = 0, i may be recommended to u again in subsequent time steps. Finally, observe that if pf = 1, the user provides positive and negative ratings, since Rhiddenui = 0 implies Rui = −1 if pf = 1.\nIn order to make recommendations based on the user’s preferences, we must assume some relation between the users. Following (Bresler et al., 2014), we assume that each user belongs to one of K < N user types. Two users u and v belong to the same type if they find the same items likable, i.e., if 1 {pui > 1/2 + ∆} = 1 {pvi > 1/2 + ∆}, for all items i. This does not require the preference vectors pu and pv of two users corresponding to the same user type to be equivalent. We note that this assumption could be relaxed by only assuming that users of the same type share a large fraction of the items that they find likable. We assume that the preference vectors belonging to the same type are more similar than those belonging to other types. Specifically, assume that for all u ∈ [N ], [N ] := {0, 1, . . . , N − 1}, and for some γ ∈ [0, 1),\nγ min v∈Tu 〈pu,pv〉 ≥ max v/∈Tu 〈pu,pv〉 , (1)\nwhere Tu ⊂ [N ] is the subset of all users that are of the same type as u. The smaller γ, the more distinct users of the same type are from users of another type. We further assume that each user likes at least a fraction ν of the items. This assumption is made to avoid degenerate situations were a user u does not like any item. Assuming that users cluster in the user-item space in different user types is common and is implicitly used by user-based CF algorithms (Sarwar et al., 2000), which perform well in practice. To further justify this assumption empirically, we plot in Figure 1 the clustering of user ratings of the Movielens 10 Million dataset (Harper & Konstan, 2015). Figure 1 shows that the user’s ratings cluster both in user and in item space.\nLearning problem and reward: The goal of a CF algorithm is to maximize reward. A reasonable reward for the online CF problem is the expected number of recommendations that a user rates positively, i.e., the pseudo-reward\nT−1∑ t=0 N−1∑ u=0 E [ Rui(u,t) ] .\nHere, i(u, t) is the item recommended to u at time t. In an e-commerce setting this corresponds to the number of recommended products that a user buys. Note that due to the uncertainty of a user liking an item (the random rating Rhiddenui might be −1 even when i is likable by u), we cannot expect to do better than maximizing the pseudo-reward.\nIn this paper our focus is on recommending likable items. Following (Bresler et al., 2014) we therefore consider the closely related accumulated reward defined as the expected total number of likable items (pui > 1/2) that are recommended by an algorithm up to time T :\nE [reward(T )] := T−1∑ t=0 N−1∑ u=0 E [ Xui(u,t) ] . (2)\nHere, Xui(u,t) = 1 {pui > 1/2} is the indicator random variable that is equal to one if item i(u, t) recommended to user u at time t is likable and zero otherwise (note that item i is chosen by the CF algorithm as a function of the responses Ru′i′ to recommendations (i′, u′) made at previous time steps, and is therefore a random variable)."
    }, {
      "heading" : "3. User-CF algorithm",
      "text" : "In this section we present our user-based CF algorithm (User-CF). In order to maximize reward the User-CF algorithm balances exploring, i.e., learning about the users,\nand exploiting, i.e., recommending items predicted to be likable based on previous ratings. To this end, the UserCF algorithm, formally introduced below, performs at time t = 0, 1, . . ., either an preference exploration, similarity exploration, or exploitation step.\nAn exploitation step first identifies the k most similar users in terms of their rating vectors rv ∈ {0, 1}M , for a given user u. The rating vectors consist of the responses Rui of users to recommendations (u, i) made by the User-CF algorithm at previous time steps. The exploitation step proceeds by recommending the item that has received the largest number of positive ratings from the nearest neighbors of u in previous time steps. For an exploitation step to be successful, it is crucial to find similar users and learn their preferences effectively. This is accomplished with similarity exploration steps, that recommend the same items to all users, and preference exploration steps that recommend random items to certain subsets of the users. Before formally stating the User-CF algorithm, we illustrate its main steps using a toy example.\nExample 1 Consider N = 6 users and M = 5 items, with preference vectors pu and rating vectors ru at time t = 2 given by \npT0 pT1 pT2 pT3 pT4 pT5  =  .9 .8 .9 .1 .1 .9 .8 .9 .2 .3 .9 .8 .9 .1 .2 .1 .3 .1 .8 .7 .2 .2 .1 .9 .9 .1 .1 .3 .7 .8  ,  rT0 rT1 rT2 rT3 rT4 rT5  =  1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n0 0 0 0 0\n .\nUsers 0, 1, 2 are of the same type as they find items 0, 1, 2 likable, and users 3, 4, 5 belong to a second type as they find items 3, 4 likable. The preference vectors are obtained by executing at time step 0 a preference exploration step, that recommends the randomly chosen items 4, 1, 1, 2, 3, 3 to users 0, 1, . . . , 5, respectively, and a similarity exploration step that recommends item 0 to all users. The responses Rui obtained from the similarity and preference exploration step are marked with rectangles and circles, respectively. Consider recommending an item to user 0 with an exploitation step at time 2. The k = 2 nearest neighbors of u = 0 are Nu = {1, 2} (〈r0, rv〉 = 1 for v = 1, 2 and 〈r0, rv〉 = 0 for v = 3, 4, 5). Since ∑ v∈Nu rvi is maximized for i = 1 (rvi is the i-th entry of ru), item 1 is recommended to user u, which happens to be a likable\nitem, as desired.\nWe next formally describe the User-CF algorithm, and explain the intuition behind the specific steps. Input parameters of the User-CF algorithm are learning rates α and η ∈ (0, 1) (e.g., η = 1/2) relevant for similarity and preference exploration steps, respectively, a batch sizeQ relevant for preference exploration steps, and finally the number of nearest neighbors k, relevant for exploitation steps. Our results guarantee that for a range of input parameters that depends on properties of the model such as the number of user types and pf , the User-CF algorithm performs essentially optimally. While we may or may not have prior knowledge of those model parameters, in practice, we can optimize for the hyper-parameters of the User-CF algorithm by using cross validation.\nAt initialization, the User-CF algorithm generates a random permutation π of the items [M ] required by the similarity exploration step. Furthermore, it splits the item space into M/Q random, and equally sized1 subsets of cardinality Q (batches), denoted by Qq ⊂ [M ], q = 0, . . . ,M/Q− 1.\nAt time steps t = bηQqc, q = 0, . . . ,M/Q − 1, the UserCF algorithm performs preference exploration steps. At all other time steps, with probabilities pJ = 1(t−q)α , q = bt/(ηQ)c, and pE = 1− pJ , the algorithm performs similarity exploration and exploitation steps, respectively.\nSimilarity exploration step: For each user u, recommend the first item i in the permutation π that has not been recommended to user u in previous steps of the algorithm. This step explores the item space and its important for selecting ‘good’ neighborhoods. Performing a sufficient number of similarity exploration steps allows to guarantee that the nearest neighbors of a given user u are of the same type.\nPreference exploration step: At time t = bηQqc, recommend to each user u an item, chosen independently and uniformly at random fromQq , that has not been rated by u in previous time steps. This step is important to learn the preferences of users.\nExploitation step: For all users u, estimate the probability of u liking a given item i as\np̂ui =\n{ 1 nui ∑ v∈Nu Rvi, if nui > 0,\n0, if nui = 0. (3)\nHere, Rvi is the rating of user v for item i obtained in previous time steps (we use the convention Rvi = 0 if no rating was obtained at a previous time step). Next, Nu is the set of users corresponding to the k largest values of〈 rsimu , r sim v 〉 , v ∈ [N ]. Here, rsimu ∈ {0, 1}M is the vector\n1We assume for simplicity that M is divisible by Q, if this is not the case, batch dM/Qe − 1 may simply contain less than Q items.\ncontaining only the responses Rui of user u to recommendations i given in previous similarity exploration steps up to time t, and is zero otherwise. Moreover, nui is the number of users inNu that received recommendation i. Finally, for each user u, recommend an item i that maximizes p̂ui′ over all items i′ that have not been rated yet.\nThe idea behind the User-CF algorithm is as follows. An exploitation step recommends likable items to u if a) most of the neighbors of u are of the same user type as u, and if b) the items are sufficiently well explored so that p̂ui indicates whether i is likable by u (i.e., pui > 1/2) or not, for all i. A large portion of the first few steps is likely to be spent on similarity exploration. This is sensible, as we need to ensure that a) is satisfied in order to make good recommendations. As time evolves, the User-CF algorithm randomly explores batches of items, one batch at a time, in order to estimate the preferences of the users regarding the items in the corresponding batches. Note that if the UserCF algorithm would explore the entire item space at once, e.g., by recommending an item chosen at random from all items, the time required for the algorithm to make ‘good’ recommendations would grow linearly in M , and M might be very large. By splitting the item space into batches Qq , the User-CF algorithm can start exploiting without having learned the preferences of the users regarding all items. Finally note that the User-CF algorithm may recommend the same item at several time steps (unless the item is rated by the user); this is sensible in particular if pf is small."
    }, {
      "heading" : "4. Main result",
      "text" : "Our main result, stated below, shows that after a certain cold start time, the User-CF algorithm produces essentially optimal recommendations.\nTheorem 1 Suppose that there are at least N2K users of the same type, for all user types, and that condition (1) holds for some γ ∈ [0, 1], which ensures that user types are distinct. Moreover, assume that at least a fraction ν of all items is likable to a given user, for all users. Pick δ > 0 and suppose that there are sufficiently many users per user type:\nN K ≥ c νpf∆2 log(M/δ) log(4/δ). (4)\nSet\nTstart :=( c̃ log(N/δ)\np2f (1− γ)2ν)\n) 1 1−α ( 1−max ( 1\nT , K N c log(M/δ) pf∆2\n)) ,\nwhere c̃ is a numerical constant. Then, for appropriate choices2 of the parameters η, k, and Q, the expected reward accumulated by the User-CF algorithm up to time\n2Specifically, η = c1ν, k = c2 NK , and Q =\nT ∈ [Tstart, 45νMpf ] satisfies\nE [reward(T )] NT\n≥ (\n1− Tstart + 1 T\n− 2α (T − Tstart) 1−α\nT (1− α)\n−K N c log(M/δ) pf∆2\n) (1− δ). (5)\nTheorem 1 states that after an initial cold start time on the order of Tstart, the User-CF algorithm recommends only likable items up to a fraction KN c log(M/δ) pf∆2\nof the time steps. This follows since a oracle that only recommends likable items obtains an reward of E [reward(T )] = NT . This yields the claim from the introduction, that after the cold start time, a fraction 1−c/pf of all recommendations made by the User-CF algorithm are likable. Note that condition (4) allows the number of user types, K, to be near linear in the number of users, N .\nWe note that the particular choice of the parameters of the User-CF algorithm in Theorem 1 is mainly out of expositional convenience; the supplementary material contains a more general statement.\nTheorem 1 is proven by showing that after the initial cold start time, exploration steps recommend likable items with very high probability. An exploration step recommends a likable item to user u provided that\na) most of the nearest neighbors of u are of the same user type, and\nb) the items are sufficiently well explored so that the maximum of p̂ui over items not rated yet corresponds to a likable item.\nFor a), we use that the user types are sufficiently distinct and that most of the nearest neighbors of u are of the same user type. The former is ensured by condition (1), and the latter holds after the initial cold start time which ensures that sufficiently many similarity exploration steps have been executed. After the initial cold start time, most of the nearest neighbors of u are of the same user type, and essentially no further cost is required to learn the neighborhoods. This is reflected in the lower bound (5), by the terms depending on T becoming negligible as T becomes large compared to Tstart. Note the dependence of Tstart on γ; the more similar the user types are (i.e., the closer γ is to 1), the longer it takes till User-CF is guaranteed to find good neighborhoods."
    }, {
      "heading" : "4.1. Dependence on pf is nearly optimal",
      "text" : "The cold start time of the User-CF algorithm guaranteed by Theorem 1 is proportional to (1/p2f ) 1 1−α . For small learning rates α, this scaling can not be improved significantly, as the following results shows.\nc3 kpf∆\n2\nlog(M/δ) , for numerical constants c1, c2, and c3.\nProposition 1 Suppose that there are more items than user types, i.e., M ≥ K. Fix λ ∈ (0, 1). Then there is a set of users with at least N2K users of the same type, for each user type, with preference vectors such that for all T ≤ λ\np2f , the\nexpected reward of any online algorithm is upper bounded by E[reward(T )]TN ≤ λ+ 1 K .\nProposition 1 shows that if the cold start time is significantly smaller than 1/p2f , then there are problem instances for which any algorithm mostly recommends non-likable items. The proposition is a consequence of the fact that after making on the order of 1/p2f recommendations, for many users we did not obtain any rating. A consequence of the proposition is that the cold start time of the User-CF algorithm is near optimal.\nRecall that even after the initial cold start time, a constant fraction of KN c log(M/δ) pf∆2\nof the recommendations might be non-likable. This fraction is the cost for establishing b). Specifically, in order to ensure b), the User-CF algorithm needs to recommend sufficiently many items to the k neighbors of u so that p̂ui indicates whether user u likes item i or not, or more precisely such that the maximum of p̂ui over items not rated yet corresponds to a likable item. This is established by showing that p̂ui > pf/2 for all likable items i ∈ Qq, q = 0, . . . , tηQ − 1, and p̂ui < pf/2 for all other items. Since the expected number of positive ratings per recommendation is proportional to pf , the number of ratings required to ensure that pui > pf/2 is proportional to 1/pf ."
    }, {
      "heading" : "4.2. One versus two class CF",
      "text" : "Recall that pf = 1 implies that users provide positive and negative ratings, and that the User-CF algorithm is nearly optimal in pf . Since the expected number of ratings obtained by a user in a given number of time steps is proportional to pf , a consequence of our result is that receiving positive and negative ratings instead of only positive ones improves the number of ratings required for initial exploration by a factor of 1/pf , which can be significant.\nWe finally note that Bresler et al. (2014) proved a performance guarantee for a closely related two-class collaborative CF algorithm termed Collaborative-Greedy. Bresler et al. (2014) consider the regime where the number of users is much larger than the number of items, i.e., N = O(MC), C > 1 and additionally the number of user types obeys (N = O(KM)). For this regime, Theorem 1 particularized to pf = 1 essentially reduces to Theorem 1 in (Bresler et al., 2014) (there are some further minor differences). However, our result particularized to the two-class case also holds when the number of items is much larger than the number of users, and allows the number of user types to be near linear in the number of users. This improvement is due to differences in the preference explo-\nration strategies of the algorithms."
    }, {
      "heading" : "4.3. Alternative exploration strategies",
      "text" : "While there are other sensible preference exploration strategies, the essential element of our approach is to split up the item space into subsets of itemsQ0,Q1, . . ., start by exploring Q0, then allow for exploitation steps, continue with exploring Q1, again allow for exploitation steps and so forth. If one explores instead the whole item space [M ] at the beginning, the learning time required for p̂ui to indicate whether an item is likable or not, is proportional to M , and can therefore be very large. To see this, consider a preference exploration step that recommends a single item to all users, chosen uniformly at random from the set of all items [M ]. The expected number of ratings obtained by executing Tr such preference exploration steps relevant for estimating whether pui > 1/2, is the expected number of neighbors of u to which i has been recommend, and is therefore proportional to Trk/M . To ensure that this expectation is larger than 0, Tr has to be on the order or M (provided that the other parameters are fixed)."
    }, {
      "heading" : "5. Numerical results",
      "text" : "In this section, we simulate an online recommender system based on real-world data in order to understand whether the User-CF algorithm behaves as predicted by Theorem 1, even when the data is not generated by the probabilistic model, but is based on real data. While an ideal dataset to validate our algorithm would consist of the ratings from all users for all items, the vast majority of ratings in standard CF datasets such as the Netflix or Movielens dataset (consisting of movie ratings) are unknown. To obtain a dataset with a higher proportion of ratings, following (Bresler et al., 2014), we consider the subset of the Movielens dataset corresponding to frequently rated items and to users that have rated many items. The Movielens dataset consists of 10 Million movie ratings in {1, 2, 3, 4, 5}; we took the ratings ≥ 4 as 1, ratings ≤ 3 as −1, and missing ratings as 0. Many of the items (movies) in the dataset have a significant bias towards a positive or negative rating. To make sure our results are not due to exploiting such biases, we only select frequently rated items out of the (approximately) unbiased items. Note that a nearest neighbor based algorithm, like the User-CF algorithm, performs well in case the item ratings are very biased even when the neighborhoods are randomly selected. Of the resulting dataset, denoted by RML ∈ {−1, 0, 1}1000×500, 18.1% of the ratings are 1, 17.1% are−1, and the remaining ones are unknown and therefore set to 0.\nOne class versus two class CF: We start with comparing the qualitative behavior of the User-CF algorithm to a two-class version of the User-CF algorithm. The two-class version of the User-CF algorithm differs from the one-class version in taking into account the negative ratings. Specif-\nically, the ratings Rvi in (3) for the two-class version or the User-CF algorithm are set to −1, 0, and 1, if a negative, none, or a positive rating was obtained as a response to a recommendation. We performed the following experiment. If the User-CF algorithm recommends item i to user u, it obtains the rating Rui = 1 in response provided that [RML]ui = 1 ([RML]ui denotes the (u, i)-th entry of RML), and Rui = 0 otherwise, while the two-class User-CF algorithm obtains Rui = [RML]ui in response. We allow both algorithms to only recommend an item to a given user once, so after M = 500 time steps, all items have been recommend to all users. We measure performance in terms of the accumulated reward, defined as\nacc-reward(T ) := T−1∑ t=0 reward(t),\nreward(t) := 1\nN N−1∑ u=0 [RML]ui(u,t), (6)\nwhere i(u, t) is the item recommended to user u by the corresponding variant of the User-CF algorithm. The results, depicted in Figure 3, show that the two-class recommender performs better, as expected, since it obtains significantly more ratings. Specifically, the expected number of ratings it obtains is almost twice the expected number of ratings the one-class User-CF algorithm obtains. After having recommend most of the likable items, mostly non-likable are left to recommend, which explains the inverse U -shape in Figure 3.\nDependence of User-CF on pf : We next validate empirically that the cold start time and number of preference exploration steps (needed to learn the preferences of the users) scale as 1/p2f and 1/pf , respectively. We start with the former. To this end, we split the items into two random disjoint sets I1 ⊂ [M ] and I2 ⊂ [M ] of equal cardinality. We then perform the following experiment for\npf ∈ {1, 0.75, 0.5}. We start by recommending 3Mkpf items, chosen uniformly at random from I2 to each user, and, provided the corresponding rating is positive ([RML]ui = 1), we provide this rating to the User-CF algorithm with probability pf . The expected number of positive ratings obtained is therefore independent of pf . Those preference exploration steps make sure that the preferences of the items in I2 are explored well. We then perform Ts similarity exploration steps on the items in the sets I1 = {i0, . . . , iM/2−1}, by recommending item it to user u at t = 0, . . . , Ts − 1. If [RML]uit = 1 then we provide the rating [RML]uit to the User-CF algorithm with probability pf . After Ts such similarity exploration steps, we perform an exploitation step. In Figure 3 we plot the reward defined in (6) obtained by the exploitation step, over Ts/p2f . The results confirm that the cold start time required to find ‘good’ neighborhoods scales inversely proportional to p2f , since all three curves lie on top of each other.\nNext, we demonstrate that the number of preference exploration steps required to learn the preferences of the users is proportional to 1/pf . To this end, we perform the same experiment as above, this time, however, we first perform Ts = 25/p 2 f similarity exploration steps on the items in the set I1, and then perform Tr preference exploration steps by recommending Tr items, chosen uniformly at random from I2 to each user. As before, if [RML]ui = 1, the rating [RML]ui is provided to the algorithm with probability pf . In Figure 3 we plot the reward obtained from performing a single exploitation step after Tr such preference exploration steps over Tr/pf . The results indicate that, as predicted by our theory, the number of preference exploration steps required to learn the preferences is proportional to pf , as the curves for different pf lie on top of each other. As mentioned previously, this is not surprising, as the number of positive ratings obtained is proportional to pf ."
    }, {
      "heading" : "6. Proof of Theorem 1",
      "text" : "Theorem 1 follows immediately from the following result.\nTheorem 2 Suppose that there are at least N2K users of the same type, for all user types, and assume that at least a fraction ν of all items is likable to a given user, for all users. Moreover, suppose that for some γ ∈ [0, 1), all users satisfy condition (1). Pick δ > 0 and suppose that the number of nearest neighbors k, the batch size Q, and the parameter η, are chosen such that k ≤ 9N40K , η ≤ ν/2,\nk Q ≥ 64 log(8M/δ) pf∆2 , (7)\nand\nQ ≥ 10 ν log(4/δ). (8)\nThen the reward accumulated by the User-CF algorithm up to time T ∈ [Tstart, 45νMpf ] with\nTstart =\n( 512 max ( log (\n4NQ k∆\n) , log ( 88 δ ))) 11−α (3p2f (1− γ)2ν) 1 1−α ( 1−max ( 1 T , 2 ηQ\n)) satisfies\nE [reward(T )] NT\n≥ (\n1− Tstart T − 2α (T − Tstart)\n1−α\nT (1− α) −max\n( 1\nT ,\n2\nηQ\n)) (1− δ). (9)\nTheorem 1 follows by choosing the parameter of the User-CF algorithm as follows:\nη = ν\n2 , k =\n9\n40\nN K , and Q = k\npf∆ 2\n64 log(8M/δ) .\nTo see this, note that by definition, the conditions on k and η and condition (7) on Q are satisfied. By (4), condition (8) holds and 2ηQ = K N c′ log(M/δ) pf∆2 . Moreover, max ( log ( 4NQ k∆ ) , log ( 88 δ )) ≤ c̃ log(N/δ)."
    }, {
      "heading" : "6.1. Proof of Theorem 2",
      "text" : "Theorem 2 is proven by showing that at time t ≥ Tstart the following holds for all users u:\ni) the neighborhood of u is sufficiently well explored by similarity exploration steps so that most of the nearest neighbors of u are good, i.e., are of the same user type as u (similarly, neighbors are called bad if they are of a different user type than u),\nii) for t ≥ Tstart, the estimates p̂ui, for all i ∈ Qq, q = 0, . . . , tηQ − 1 correctly predict whether i is likable by u or not, and\niii) there exist items in the sets Qq, q = 0, . . . , tηQ − 1 that are likable by u and that have not been rated by u at previous times steps.\nConditions i, ii, and iii guarantee that an exploitation step recommends a likable item.\nFormally, we start by defining the following events:\nGβ(t) = {At time t, no more than βk of the k-nearest neighbors of u are bad}, (10)\nL(t) ={at time t, there exists an item i ∈ Qq, q = 0, . . . , t/(ηQ)− 1 that is likable by u}, (11)\nand\nE(t) = ⋃\nq=0,..., tηQ−1\nEq(t), (12)\nwith\nEq(t) ={Conditioned on G ∆ 4Q (t), for all i ∈ Qq ,\np̂ui > pf/2, if pui > 1/2 + ∆, and p̂ui < pf/2, if pui < 1/2−∆}. (13)\nFor convenience, we omit in the notion of L(t), Gβ(t), E(t), and Eq(t) the dependence on u. The significance of those definitions is that ifL(t), G ∆\n4Q (t), and E(t) hold simultaneously, then the recommendation made to user u by an exploitation\nstep at time t is likable. We can therefore lower-bound the reward E [reward(T )] as follows:\nE [reward(T )] NT = 1 NT T−1∑ t=0 N−1∑ u=0 P [ Xui(u,t) = 1 ] ≥ 1 NT N−1∑ u=0 T−1∑ t=0,t/∈{ηQq : q=0,1,...} P [exploitation at t]P [ Xui(u,t) = 1|exploitation at t ] (14)\n≥ 1 N N−1∑ u=0\n( 1\nT T−1∑ t=0 (1− (2/t)α)P [ Xui(u,t) = 1|exploitation at t ] −max ( 1 T , 2 ηQ )) (15)\n≥ 1 N N−1∑ u=0\n( 1\nT T−1∑ t=Tstart (1− δ)(1− (2/t)α)−max ( 1 T , 2 ηQ )) (16)\n≥ (1− δ) (\n1− Tstart T − 2α (T − Tstart)\n1−α\nT (1− α) −max\n( 1\nT ,\n2\nηQ\n)) . (17)\nHere, (14) follows from P [ Xui(u,t) = 1|preference exploration at t ] ≥ 0 and P [ Xui(u,t) = 1|similarity exploration at t ] ≥ 0.\nFor (15) we used, for t 6= ηQq,\nP [exploration at t] = 1− (t− bt/(ηQ)c)−α ≥ 1− (t(1− 1/(ηQ)))−α ≥ 1− (2/t)α\nwhich follows from ηQ ≥ 2. Moreover we used for (15) that the fraction of preference exploration steps up to time T is at most max( 1T , 2 ηQ ). To see that, note that at T ∈ {ηQq, . . . , ηQ(q + 1)} we have performed q + 1 preference exploration steps. It follows that, for q ≥ 1, the fraction of preference exploration steps performed up to T is given by q+1qηQ ≤ 2 ηQ . Thus, for any T ≥ 1, the fraction of preference exploration steps is ≤ max( 1T , 2 ηQ ). Equality (16) follows from\nP [ Xui(u,t) = 1|exploitation at t ] ≥ P [ E(t) ∩ G ∆\n4Q (t) ∩ L(t) ] ≥ 1− δ. (18)\nHere, inequality (18) holds for t ≥ Tstart and is established below. Finally, inequality (17) follows from T−1∑ t=Tstart t−α ≤ ∫ T−1 Tstart−1 t−α = 1 1− α t1−α|T−1t=Tstart−1\n= (T − 1)1−α − (Tstart − 1)1−α 1− α ≤ (T − Tstart) 1−α 1− α .\nIt remains to establish (18). To this end, define for notational convenience\nA := 256 max\n( log (\n4NQ k∆\n) , log ( 88 δ )) 3p2f (1− γ)2ν ,\nand let Ts be the number of similarity exploration steps executed up to time T . Inequality (18) follows by noting that, for all t ≥ Tstart, by the union bound,\nP [ (E(t) ∩ G ∆\n4Q (t) ∩ L(t))c\n] ≤ P [Ec(t)] + P [ Gc∆ 4Q (t) ] + P [Lc(t)]\n≤ P [Ec(t)] + P [ Gc∆\n4Q (t)|Ts ≥ A\n] + P [Ts ≤ A] + P [Lc(t)] (19)\n≤ δ 4 + δ 4 + δ 4 + δ 4 = δ. (20)\nHere, inequality (19) follows since for two events C,B we have that\nP [C] = P [C ∩B] + P [C ∩Bc] = P [C|B]P [B] + P [C|Bc]P [Bc] ≤ P [C|B] + P [Bc] . (21)\nInequality (20) follows from\nP [Ec(t)] ≤ δ/4 (22) P [ Gc∆\n4Q (t)|Ts ≥ A\n] ≤ δ/4 (23)\nP [Ts ≤ A] ≤ δ/4 (24) P [Lc(t)] ≤ δ/4. (25)\nIn the remainder of this proof, we establish the inequalities (22)-(25). The key ingredient for these bounds are concentration inequalities, in particular a version of Bernstein’s inequality (Bardenet & Maillard, 2015).\nProof of (22): By the union bound, we have, for all t = 0, . . . ,M − 1, that\nP [Ec(t)] ≤ M/Q−1∑ q=0 P [ Ecq (t) ] ≤ δ 4\nas desired. Here, we used P [ Ecq (t) ] ≤ δQ4M , which follows from Lemma 1 stated below with δ\n′ = δQ4M and Tr = 1 (note that the assumption (26) of Lemma 1 is implied by the assumption (7) of Theorem 2).\nLemma 1 (Preference exploration) Suppose we recommend Tr random items to each user, chosen uniformly at random from a set Q ⊆ [M ] of Q items. Suppose that pvi is ∆-bounded away from 1/2, for all i ∈ Q and for all v ∈ Nu, where Nu is a set of k users, of which no more than βk, with β ≤ ∆Tr4Q , of the users are of a different type than u. Fix δ ′ > 0. If\nTr k\nQ\npf∆ 2\n64 log(2Q/δ′) ≥ 1 (26)\nthen, with probability at least 1− δ′, for all i ∈ Q, p̂ui > pf2 if pui ≥ 1/2 + ∆ and p̂ui < pf 2 if pui ≤ 1/2−∆.\nProof of (23): Inequality (23) follows from Lemma 2 below, which ensures that a user has many good and only few bad neighbors.\nLemma 2 (Many good and few bad neighbors) Let Tu be the subsets of all users [N ] that are of the same type of u and suppose its cardinality satisfies≥ N2K . Suppose that, for some constant γ ∈ [0, 1), condition (1) holds, and that the number of nearest neighbors k satisfies k ≤ 9N40K . Choose β ∈ (0, 1), and suppose\nTs ≥ 64 log(N/(βk))\n3p2f (1− γ)2 1 M minv∈Tu 〈pu,pv〉\n(27)\nsimilarity exploration steps have been performed. Then, with probability at least 1− 11e− 364Tsp2f (1−γ)2 1M minv∈Tu 〈pu,pv〉, the set of nearest neighbors Nu of user u (defined in Section 3), contains no more than βk bad neighbors.\nTo see that inequality (23) follows from Lemma 2, we first note that Ts ≥ A guarantees that condition (27) of Lemma 2 is satisfied (with β = ∆4Q ). To see this, note that since each user likes at least a fraction ν of the items, we have\n1\nM min v∈Tu\n〈pu,pv〉 ≥ ν ( 1\n2 + ∆\n)2 ≥ ν\n4 . (28)\nLemma 2 therefore implies P [ Gc∆\n4Q (t)|Ts ≥ A\n] ≤ 11e− 364Tsp 2 f (1−γ) 2 1 M minv∈Tu 〈pu,pv〉 ≤ 11e− log(88/δ) = δ\n8 ,\nas desired. For the second inequality above we used (28) and Ts ≥ A.\nProof of (24): We next establish the inequality P [Ts ≤ A] ≤ δ/4. To this end, recall that a similarity exploration step is carried out at t = 0, . . . , T − 1, t 6= ηQq, q = 0, 1, . . . with probability 1/(t − bt/(ηQ)c). Recall from the discussion below inequality (17), that the fraction of time steps up to time T for which t = ηQq, for some q, is at most max( 1T , 2 ηQ ). It follows that the number of similarity exploration steps, Ts, carried out after t ≥ Tstart steps of the User-CF algorithm, stochastically dominates the random variable S = ∑T̃ t=1 Zt, T̃ = Tstart(1 − max( 1 T , 2 ηQ )), where Zt is a binary random variable with P [Zt = 1] = 1/tα. It follows that\nP [Ts ≤ A] = P [ Ts ≤ T̃ 1−α/2 ] ≤ e− T̃ 1−α 20 ≤ δ/4, (29)\nwhere the first inequality holds by definition of Tstart, i.e.,\nTstart = (2A) 1 1−α / ( 1−max ( 1\nT ,\n2\nηQ\n)) ,\nand the second inequality holds by Lemma 3 stated below. Finally, the last inequality in (29) follows from\nT̃ = (2A) 1 1−α ≥ 128 3 log(44/δ).\nThe following lemma appears in (Bresler et al., 2014). Lemma 3 Let S = ∑T̃ t=1 Zt where Zt is a binary random variable with P [Zt = 1] = 1/tα, α ∈ (0, 4/7). We have that\nP [ ST ≤ T̃ 1−α/2 ] ≤ e− T̃ 1−α 20 .\nProof of (25): Suppose t < ηQ, consider user u, and letN0 be the total number of items likable by u in the setQ0 (recall that Q0 is choosen uniformly at random from the subset of items [M ] of cardinality Q). Note that N0 > ηQ implies that at t < ηQ, there exist items that are likable by u inQ0 that have not been recommended to u yet. Therefore, we can upper bound the probability that no likable items are left to recommend, for t < ηQ, by\nP [Lc(t)] ≤ P [N0 ≤ ηQ] ≤ P [N0 ≤ Qν/2] ≤ P [N0 ≤ E [N0]−Qν/2] (30)\n≤ e −Q (ν/2)\n2\n2ν(1−ν)+ 2 3 ν 2 = e\n−Q ν/4 2(1−ν)+ 1 3 ≤ e−Q ν10 ≤ δ 4 . (31)\nHere, the first inequality in (30) follows from η ≤ ν/2, by assumption; the second inequality in (30) follows from E [N0] ≥ νQ (since at least a fraction of ν of the items is likable by u), the first inequality in (31) follows from Bernstein’s inequality (Bardenet & Maillard, 2015), and finally the last inequality in (30) holds by assumption (8). We have established that P [Lc(t)] ≤ δ/4, for t < ηQ. Using the exact same line of arguments yields the same bound for t ∈ [ηQ, ηM ].\nIt remains to upper bound P [Lc(t)] for t ∈ [ηM, 45νMpf ]. To this end, let N c u(T ) be the number of (likable) items that have been rated by user u after T time steps, and note that if N cu(T ) is strictly smaller than the (minimum) number of likable items, then there are likable items left to recommend. Formally,\nP [Lc(t)] ≤ P [N cu(T ) ≥ νM ] (32)\nwhere we used that for each user u, at least νM items are likable. Recall that with probability puipf ≤ pf a likable item i is rated if it is recommended to u. Once rated, an item is not recommended again.\nNote that N cu(T ) is statistically dominated by a sum of independent binary random variables Zt with P [Zt = 1] = pf . We therefore have that\nP [N cu(T ) ≥ νM ] ≤ P [ N cu(T ) ≥ T (pf +\npf 4\n) ] ≤ e− Tp2f 2 ≤ e− Tstartp 2 f\n2 ≤ δ 4 . (33)\nHere, the first inequality holds by the assumption T ≤ 45νMpf , the second inequality follows by Hoeffding’s inequality, the third inequality follows by T ≥ Tstart, and the last inequality follows from Tstart ≥ 2p2f log(4/δ), which holds by definition of Tstart. Application of (33) on (32) concludes the proof of P [Lc(t)] ≤ δ/4."
    }, {
      "heading" : "6.2. Proof of Lemma 2",
      "text" : "Recall that rsimu ∈ {0, 1}M is the vector containing the responsesRui of user u to previous similarity exploration steps up to time t, and that we assume in Lemma 2, that Ts similarity exploration steps have been performed up to time t. To establish Lemma 2, we show that there are more than k users v that are of the same user type as u and satisfy 1Ts 〈 rsimu , r sim v 〉 ≥ θ,\nand at the same time, there are fewer than kβ users of a different user type as u that satisfy 1Ts 〈 rsimu , r sim v 〉 ≥ θ for a certain threshold θ chosen below. This is accomplished by the following two lemmas.\nLemma 4 (Many good neighbors) Suppose there are at least N2K users of the type as user u (including u), and suppose that Ts similarity exploration steps have been performed. Then, with probability at least 1− 10pgood,\npgood := e − 316Tspg(1−θ/pg) 2 , pg := p 2 f\n1\nM min v∈Tu\n〈pu,pv〉 ,\nat least 9N40K users v of the same user type as u obey 1 Ts 〈 rsimu , r sim v 〉 ≥ θ.\nLemma 5 (Few bad neighbors) Suppose that Ts similarity exploration steps have been performed. Then, with probability at least 1− pbad, where\npbad = e −Tspb(θ/pb−1)\n2/4\n1+(θ/pb−1)/3 , pb := p 2 f max v/∈Tu\n1\nM 〈pv,pu〉 ,\nat most Npbad users v of a different user type than u obey 1Ts 〈 rsimu , r sim v 〉 ≥ θ.\nWe set θ =\npg + pb 2 .\nWith this choice, by Lemma 4, there are more than 9N40K ≥ k (the inequality holds by assumption) users v of the same type as u that satisfy 1Ts 〈 rsimu , r sim v 〉 ≥ θ, with probability at least 1 − 10pgood. By Lemma 5, there are no more than Npbad\nusers v of a different type as u with 1Ts 〈 rsimu , r sim v 〉 ≥ θ. Thus, by the union bound, Nu contains less than pbadN bad neighbors with probability at least\n1− 10pgood − pbad ≥ 1− 11e− 3 64Tspg(1−γ) 2 .\nHere, we used pgood = e − 364Tspg(1−pb/pg) 2 ≤ e− 364Tspg(1−γ) 2\nwhere the inequality follows by pb/pg ≤ γ, by (1). Moreover, we used\npbad = e −Tspb(θ/pb−1)\n2/4 1+(θ/pb−1)/3 = e −Tspb(pg/pb−1) 2/16 1+(pg/pb−1)/6 = e −Tspg(\n√ pg/pb− √ pb/pg) 2/16\n1+(pg/pb−1)/6 ≤ e− Tspg(\n√ 1/γ−√γ)2/16\n1+(1/γ−1)/6\n≤ e− Tspg(\n√ 1/γ−√γ)2/16 1+(1/γ−1) =e −Tspg(1−γ)2/16\n. (34)\nHere, the first inequality follows from the absolute value of the exponent being decreasing in pb/pg , and from the assumption pb/pg ≤ γ, by (1).\nTo conclude the proof, we needed to establish that the maximum number of bad neighbors Npbad satisfies Npbad ≤ βk. This follows directly by noting that, by assumption (27), the RHS of (34) is upper-bounded by βkN ."
    }, {
      "heading" : "6.2.1. PROOF OF LEMMA 4",
      "text" : "Consider u and assume there are exactly N2K users from the same user type. There could be more, but it is sufficient to consider N2K . Let v be of the same user type. We start by showing that 1 Ts 〈 rsimu , r sim v 〉 ≥ θ with high probability. To\nthis end, note that 〈 rsimu , r sim v 〉 = ∑Ts−1 t=0 Ruπ(t)Rvπ(t) where π is the random permutation of the item space drawn by the User-CF algorithm at initialization, and Ruπ(t)Rvπ(t) is a binary random variable, independent across t, with success probability p2f puπ(t)pvπ(t). Setting a := p 2 f 1 M 〈pu,pv〉, for notational convenience, it follows that\nP [ 1\nTs\n〈 rsimu , r sim v 〉 ≤ θ ] = P [ 1\nTs\n〈 rsimu , r sim v 〉 ≤ a− (a− θ) ] (35)\n≤ e− Ts(a−θ)2/2 a+(a−θ)/3 (36)\n= e− Tsa(1−θ/a)2/2 1+(1−θ/a)/3 ≤ e− 38Tsa(1−θ/a) 2\n(37)\n≤ e− 38Tspg(1−θ/pg) 2 ≤ pgood. (38)\nHere, (36) follows from Bernstein’s inequality (Bardenet & Maillard, 2015), and for (38) we used that the RHS of (37) is decreasing in a.\nNext, consider the random variable\nW = ∑ v∈Tu Gv, Gv = 1 { 1 Ts 〈 rsimu , r sim v 〉 ≥ θ } ,\nwhere Tu is the subset of all users [N ] that are of the same time as user u, as before. By Chebyshev’s inequality, P [ W − E [W ] ≤ −E [W ]\n2\n] ≤ Var(W )\n(E [W ] /2)2 . (39)\nSince there are at least N2K users of the same type, the carnality of Tu is lower bounded by N 2K −1. It follows with (38) that E [W ] ≥ (1− pgood) ( N 2K − 1 ) .\nNext, we upper bound the variance of W . We have Var(W ) = ∑ v∈Tu Var(Gv) + ∑ v,w∈Tu,v 6=w Cov(Gv, Gw).\nWith Gv = G2v , Var(Gv) = E [ G2v ] − E [Gv]2 = E [Gv] (1− E [Gv]) ≤ 1− E [Gv] ≤ pgood.\nSimilarly, Cov(Gv, Gw) = E [GvGw]− E [Gv]E [Gw] ≤ 1− (1− q)2 ≤ 2pgood.\nThus, we obtain Var(W ) ≤ ( N 2K − 1 ) pgood + ( N 2K − 1 )( N 2K − 2 ) 2pgood ≤ ( N 2K − 1 )2 2pgood.\nPlugging this into (39) yields P [ W − E [W ] ≤ −E [W ]\n2\n] ≤ 8pgood\n(1− pgood)2 ≤ 10pgood,\nfor pgood ≤ 1/10. It follows that the number of good neighbors is larger than\nW ≥ E [W ] /2 ≥ (1− pgood) N 4K ≥ 9N 40K\nwith probability at least 1− 10pgood."
    }, {
      "heading" : "6.2.2. PROOF OF LEMMA 5",
      "text" : "Let u and v be two fixed users of different user types. Similarly as in the proof of Lemma 4, we start by showing that 1 Ts 〈 rsimu , r sim v 〉 ≤ θ with high probability. To this end, note that 〈 rsimu , r sim v 〉 = ∑Ts−1 t=0 Ruπ(t)Rvπ(t) where π is a random permutation of the item space andRuπ(t)Rvπ(t) is a binary random variable, independent across t, with success probability p2f puπ(t)pvπ(t). Setting a = p 2 f 1 M 〈pu,pv〉, for notational convenience, it follows that\nP [ 1\nTs\n〈 rsimu , r sim v 〉 ≥ θ ] = P [ 1\nTs\n〈 rsimu , r sim v′ 〉 ≥ a+ (θ − a) ] ≤ e− Ts(θ−a)2/2 a+(θ−a)/3 (40)\n≤ e− Tspb(θ/pb−1)\n2/2\n1+(θ/pb−1)/3 = p2bad. (41)\nHere, (40) follows from Bernstein’s inequality. Specifically, we use that π is a random permutation of the item space as well as that RuiRvi are binary random variables independent across i (note that Bernstein’s inequality also applies to sampling without replacement, see e.g., (Bardenet & Maillard, 2015)). Finally, for inequality (41), we used that a ≤ pb = p2f maxv/∈Tu 1 M 〈pv,pu〉.\nSet Nbad = ∑ v/∈Tu 1 {u and v are declared neighbors}. By inequality (41), we have E [Nbad] ≤ p 2 badN. Thus, by Markov’s inequality,\nP [Nbad ≥ Npbad] ≤ E [Nbad] Npbad ≤ p 2 badN Npbad = pbad,\nwhich concludes the proof."
    }, {
      "heading" : "6.3. Proof of Lemma 1 (preference exploration)",
      "text" : "Assume w.l.o.g. that pui > 1/2 + ∆, for all i ∈ Q. The case where some of the pui satisfy pui < 1/2 − ∆ is treated analogously. To prove Lemma 1, we may further assume that pui = 12 + ∆, for all i ∈ Q, since P [ p̂ui > pf 2 ] is increasing in pui.\nConsider a fixed item i ∈ Q, and let N goodu be the subset of Nu corresponding to users that are of the same type as u and to which additionally an recommendation has been made by drawing Tr items uniformly from Q for each user u. Let Ng be the cardinality of N goodu . In order to upper-bound P [ p̂ui ≤ pf2 ] , we first note that by (21),\nP [ p̂ui ≤\npf 2\n] ≤ P [ p̂ui ≤\npf 2 ∣∣∣Ng ≥ ng]+ P [Ng ≤ ng] . (42) Here, we defined\nng := Trk\nQ (1/2− β). (43)\nWe next upper bound the probabilities on the RHS of (42). We start with the first probability on the RHS of (42):\nP [ p̂ui ≤\npf 2\n∣∣∣Ng = n′g] ≤ P [∑ v∈N goodu Rvi\nn′g + βk ≤ pf 2\n∣∣∣Ng = n′g ]\n(44)\n= P  1 n′g ∑ v∈N goodu Rvi ≤ pf 2 n′g + βk n′g ∣∣∣Ng = n′g \n= P  1 n′g ∑ v∈N goodu Rvi ≤ pf ( 1 2 + ∆ ) − pf ( ∆− βk 2n′g ) ∣∣∣Ng = n′g \n= P  ∑ v∈N goodu ( Rvi − pf ( 1 2 + ∆ )) ≤ −n′gpf ( ∆− βk 2n′g ) ∣∣∣Ng = n′g \n≤ e −\nn′gpf (∆−βk/(2n ′ g)) 2/2\n(1/2+∆)+(∆−βk/(2n′g))/3 (45)\nwhere (44) follows from the number of users nui inNu that received recommendation i being upper bounded by Ng + βk (recall that βk is the maximum number of bad neighbors in Nu), and by assuming adversarially that all recommendations given to bad neighbors did yieldRvi = 0. Finally, (45) follows from Bernstein’s inequality; to apply Bernstein’s inequality, we used that E [Rvi] = pf(1/2 + ∆), and that the variance of Rvi is upper bounded by pf(1/2 + ∆), for v ∈ N goodu . Next, note that by Bayes theorem,\nP [p̂ui ≤ 1/2|Ng ≥ ng] = P [{p̂ui ≤ 1/2} ∩ {Ng ≥ ng}]\nP [Ng ≥ ng]\n=\n∑ n′g≥ng P [ p̂ui ≤ 1/2 ∣∣Ng ≥ ng]P [Ng = n′g] P [Ng ≥ ng]\n≤ e −\nngpf (∆−βk/(2n ′ g)) 2/2\n(1/2+∆)+(∆−βk/(2n′g))/3 (46)\n≤ e− ngpf∆\n2/8\n1/2+∆+∆/6 ≤ e− ngpf∆\n2\n16 ≤ e− Trkpf∆\n2\nQ64 . (47)\nHere, inequality (46) follows from inequality (45) and using that the RHS of inequality (45) is increasing in n′g . For inequality (47) we used the definition of ng in (43), and that\nβk ng = βk Trk Q (1/2− β) = Q Tr\nβ\n1/2− β ≤ ∆. (48)\nHere, the inequality (48) holds by β ≤ ∆Tr4Q , by assumption, and β ≤ 1/4, due to ∆ ≤ 1/2 and Tr ≤ Q (since we recommend each item at most once).\nWe proceed with upper bounding P [Ng ≤ ng] in (42). Recall thatNg is the number of times item i has been recommended to one of the ≥ (1− β)k good neighbors in Nu.\nWe will only consider the Tr random items recommended to each user; this yields an upper bound on P [Ng ≤ ng]. Recall that those items are chosen from the Q items in Q, and that, by assumption, of the k neighbors at least (1− β)k are good. By Bernstein’s inequality,\nP [Ng ≤ ng] = P [ Ng ≤ Tr\n(1− β)k Q − Trk 2Q\n]\n≤ e −\nTrk( 1 2Q )2/2\n1−β Q (1− 1−β Q )+ 1 3 1 2Q ≤ e\n− Trk(\n1 2Q )2/2\n1−β Q (1− 1−β Q )+ 1 3 1 2Q ≤ e−\nTrk 1\n8Q 1+1/6 ≤ e− Trk 10Q . (49)\nApplication of inequalities (47) and (49) to inequality (42) together with a union bound yields P [p̂ui ≤ 1/2, for one or more i ∈ Q] ≤ Q ( e− Trkpf∆ 2 Q64 + e− Trk 10Q ) ≤ 2Qe− Trkpf∆ 2 Q64 , (50)\nwhere we used that pf∆2 ≤ 1. By (26), the RHS above is smaller than δ′. This concludes the proof."
    }, {
      "heading" : "7. Proof of Proposition 1",
      "text" : "Consider a set of users with K user types that are non-overlapping in their preferences, specifically, consider a set of users where every user u belonging to the k-th user type has preference vector\n[pu]i = { 1, if i ∈ [k(M − 1)/K, . . . , kM/K] 0, otherwise.\nConsider a given user u. At time T , the expected number of ratings obtained by u is upper bounded by p2f . Thus, for all T ≤ λ\np2f in at least a fraction λ of the runs of the algorithm, the algorithm has no information on the user u, and the\nbest it can do is to recommend a random item. For our choice of preference vectors, with probability at most 1/K, it will recommend a likable item. Therefore, an upper bound on the expected regret is given by (λ+ 1/K)NT ."
    } ],
    "references" : [ {
      "title" : "Recommender systems: The textbook",
      "author" : [ "Aggarwal", "Charu C" ],
      "venue" : null,
      "citeRegEx" : "Aggarwal and C.,? \\Q2016\\E",
      "shortCiteRegEx" : "Aggarwal and C.",
      "year" : 2016
    }, {
      "title" : "Concentration inequalities for sampling without replacement",
      "author" : [ "Bardenet", "Rémi", "Maillard", "Odalric-Ambrym" ],
      "venue" : null,
      "citeRegEx" : "Bardenet et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bardenet et al\\.",
      "year" : 2015
    }, {
      "title" : "Analysis of a collaborative filter based on popularity amongst neighbors",
      "author" : [ "Barman", "Kishor", "Dabeer", "Onkar" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "Barman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Barman et al\\.",
      "year" : 2012
    }, {
      "title" : "Multi-armed bandit problems with dependent arms",
      "author" : [ "Bartók", "Gábor", "Foster", "Dean P", "Pál", "Dávid", "Rakhlin", "Alexander", "Szepesvári", "Csaba" ],
      "venue" : "Math. Oper. Res.,",
      "citeRegEx" : "Bartók et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bartók et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistical analysis of k-nearest neighbor collaborative recommendation",
      "author" : [ "Biau", "Gérard", "Cadre", "Benoît", "Rouvière", "Laurent" ],
      "venue" : "Ann. Stat.,",
      "citeRegEx" : "Biau et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Biau et al\\.",
      "year" : 2010
    }, {
      "title" : "A latent source model for online collaborative filtering",
      "author" : [ "Bresler", "Guy", "Chen", "George H", "Shah", "Devavrat" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bresler et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bresler et al\\.",
      "year" : 2014
    }, {
      "title" : "Regret guarantees for item-item collaborative filtering",
      "author" : [ "Bresler", "Guy", "Shah", "Devavrat", "Voloch", "Luis F" ],
      "venue" : null,
      "citeRegEx" : "Bresler et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bresler et al\\.",
      "year" : 2015
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Bubeck", "Sébastien", "Cesa-Bianchi", "Nicolò" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2012
    }, {
      "title" : "Adaptive collaborating filtering: The low noise regime",
      "author" : [ "O. Dabeer" ],
      "venue" : "In IEEE International Symposium on Information Theory, pp",
      "citeRegEx" : "Dabeer,? \\Q2013\\E",
      "shortCiteRegEx" : "Dabeer",
      "year" : 2013
    }, {
      "title" : "Linear bandits in high dimension and recommendation systems",
      "author" : [ "Deshpande", "Yash", "Montanari", "Andrea" ],
      "venue" : "In Annual Allerton Conference on Communication, Control, and Computing,",
      "citeRegEx" : "Deshpande et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Deshpande et al\\.",
      "year" : 2012
    }, {
      "title" : "Collaborative filtering recommender systems",
      "author" : [ "Ekstrand", "Michael D", "Riedl", "John T", "Konstan", "Joseph A" ],
      "venue" : "Found. Trends Hum.-Comput. Interact.,",
      "citeRegEx" : "Ekstrand et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ekstrand et al\\.",
      "year" : 2011
    }, {
      "title" : "The MovieLens datasets: History and context",
      "author" : [ "Harper", "F. Maxwell", "Konstan", "Joseph A" ],
      "venue" : "ACM Trans. Interact. Intell. Syst.,",
      "citeRegEx" : "Harper et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Harper et al\\.",
      "year" : 2015
    }, {
      "title" : "Scalable and interpretable product recommendations via overlapping co-clustering",
      "author" : [ "Heckel", "Reinhard", "Vlachos", "Michail", "Parnell", "Thomas", "Dünner", "Celestine" ],
      "venue" : "In IEEE International Conference on Data Engineering,",
      "citeRegEx" : "Heckel et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Heckel et al\\.",
      "year" : 2017
    }, {
      "title" : "Collaborative filtering for implicit feedback datasets",
      "author" : [ "Hu", "Yifan", "Koren", "Yehuda", "Volinsky", "Chris" ],
      "venue" : "In IEEE International Conference on Data Mining, pp",
      "citeRegEx" : "Hu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2008
    }, {
      "title" : "Cascading Bandits: Learning to Rank in the Cascade Model",
      "author" : [ "Kveton", "Branislav", "Szepesvari", "Csaba", "Wen", "Zheng", "Ashkan", "Azin" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Kveton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kveton et al\\.",
      "year" : 2015
    }, {
      "title" : "One-class collaborative filtering",
      "author" : [ "Pan", "Rong", "Zhou", "Yunhong", "Cao", "Bin", "N.N. Liu", "R. Lukose", "M. Scholz", "Yang", "Qiang" ],
      "venue" : "In IEEE International Conference on Data Mining, pp",
      "citeRegEx" : "Pan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2008
    }, {
      "title" : "Multi-armed Bandit Problems with Dependent Arms",
      "author" : [ "Pandey", "Sandeep", "Chakrabarti", "Deepayan", "Agarwal", "Deepak" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Pandey et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Pandey et al\\.",
      "year" : 2007
    }, {
      "title" : "Analysis of recommendation algorithms for e-commerce",
      "author" : [ "Sarwar", "Badrul", "Karypis", "George", "Konstan", "Joseph", "Riedl", "John" ],
      "venue" : "In ACM Conference on Electronic Commerce,",
      "citeRegEx" : "Sarwar et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Sarwar et al\\.",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In practice, recommender systems often use collaborative filtering (CF) (Ekstrand et al., 2011) to identify items a given",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "Other examples of such feedback include implicit ratings such as viewing a webpage and listening to a song (Hu et al., 2008), and business-to-business recommender systems (Heckel et al.",
      "startOffset" : 107,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : ", 2008), and business-to-business recommender systems (Heckel et al., 2017).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "The problem of generating recommendations based on positive ratings only is known as one-class CF (Pan et al., 2008).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "The lack of negative ratings is often considered to make this problem challenging (Pan et al., 2008).",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "Our model, and elements of our algorithm, are inspired by a related model and algorithm by Bresler et al. (2014) for the two-class CF problem, i.",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "We do not discuss item based CF algorithms here, but refer to (Bresler et al., 2015) for a recent analysis of an item",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "One of the first analytical results on user-based CF algorithms, an asymptotic performance guarantee under a probabilistic model, was established by Biau et al. (2010). Most related to our approach is the Collaborative-Greedy algorithm studied by Bresler et al.",
      "startOffset" : 149,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "One of the first analytical results on user-based CF algorithms, an asymptotic performance guarantee under a probabilistic model, was established by Biau et al. (2010). Most related to our approach is the Collaborative-Greedy algorithm studied by Bresler et al. (2014) for the online twoclass CF problem.",
      "startOffset" : 149,
      "endOffset" : 269
    }, {
      "referenceID" : 4,
      "context" : "One of the first analytical results on user-based CF algorithms, an asymptotic performance guarantee under a probabilistic model, was established by Biau et al. (2010). Most related to our approach is the Collaborative-Greedy algorithm studied by Bresler et al. (2014) for the online twoclass CF problem. The Collaborative-Greedy algorithm differs from our User-CF algorithm in selecting the nearest neighbors based on thresholding similarity, instead of selecting the k most similar users, and in the way preferences of the users are explored. This difference in the exploration steps is crucial for establishing that after the cold start period, our User-CF algorithm makes optimal recommendations in a fraction 1 − c/pf of the remaining time steps. Dabeer (2013) studies a probabilistic model in an online setup, and Barman & Dabeer (2012) study a probabilistic model in an offline setup, and state performance guarantees for a two-class user-based CF algorithm.",
      "startOffset" : 149,
      "endOffset" : 766
    }, {
      "referenceID" : 4,
      "context" : "One of the first analytical results on user-based CF algorithms, an asymptotic performance guarantee under a probabilistic model, was established by Biau et al. (2010). Most related to our approach is the Collaborative-Greedy algorithm studied by Bresler et al. (2014) for the online twoclass CF problem. The Collaborative-Greedy algorithm differs from our User-CF algorithm in selecting the nearest neighbors based on thresholding similarity, instead of selecting the k most similar users, and in the way preferences of the users are explored. This difference in the exploration steps is crucial for establishing that after the cold start period, our User-CF algorithm makes optimal recommendations in a fraction 1 − c/pf of the remaining time steps. Dabeer (2013) studies a probabilistic model in an online setup, and Barman & Dabeer (2012) study a probabilistic model in an offline setup, and state performance guarantees for a two-class user-based CF algorithm.",
      "startOffset" : 149,
      "endOffset" : 843
    }, {
      "referenceID" : 16,
      "context" : "A conceptually related online learning problem are multiarmed bandits with dependent arms (Pandey et al., 2007).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Finally, we note that a class of learning problems reminiscent to that considered here is partial monitoring (Bartók et al., 2014).",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 14,
      "context" : "While partial monitoring has been studied in the context of recommender systems (Kveton et al., 2015), we are not aware of papers on partial monitoring in collaborative filtering.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "As mentioned previously, this model is inspired by that in (Bresler et al., 2014) for the two-class CF problem.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "Following (Bresler et al., 2014), we assume that each user belongs to one of K < N user types.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "Assuming that users cluster in the user-item space in different user types is common and is implicitly used by user-based CF algorithms (Sarwar et al., 2000), which perform well in practice.",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "Following (Bresler et al., 2014) we therefore consider the closely related accumulated reward defined as the expected total number of likable items (pui > 1/2) that are recommended by an algorithm up to time T :",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "For this regime, Theorem 1 particularized to pf = 1 essentially reduces to Theorem 1 in (Bresler et al., 2014) (there are some further minor differences).",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "We finally note that Bresler et al. (2014) proved a performance guarantee for a closely related two-class collaborative CF algorithm termed Collaborative-Greedy.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "We finally note that Bresler et al. (2014) proved a performance guarantee for a closely related two-class collaborative CF algorithm termed Collaborative-Greedy. Bresler et al. (2014) consider the regime where the number of users is much larger than the number of items, i.",
      "startOffset" : 21,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "To obtain a dataset with a higher proportion of ratings, following (Bresler et al., 2014), we consider the subset of the Movielens dataset corresponding to frequently rated items and to users that have rated many items.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "The following lemma appears in (Bresler et al., 2014).",
      "startOffset" : 31,
      "endOffset" : 53
    } ],
    "year" : 2017,
    "abstractText" : "We consider the online one-class collaborative filtering (CF) problem that consists of recommending items to users over time in an online fashion based on positive ratings only. This problem arises when users respond only occasionally to a recommendation with a positive rating, and never with a negative one. We study the impact of the probability of a user responding to a recommendation, pf , on the sample complexity, i.e., the number of ratings required to make ‘good’ recommendations, and ask whether receiving positive and negative ratings, instead of positive ratings only, improves the sample complexity. Both questions arise in the design of recommender systems. We introduce a simple probabilistic user model, and analyze the performance of an online user-based CF algorithm. We prove that after an initial cold start phase, where recommendations are invested in exploring the user’s preferences, this algorithm makes—up to a fraction of the recommendations required for updating the user’s preferences—perfect recommendations. The number of ratings required for the cold start phase is nearly proportional to 1/pf , and that for updating the user’s preferences is essentially independent of pf . As a consequence we find that, receiving positive and negative ratings instead of only positive ones improves the number of ratings required for initial exploration by a factor of 1/pf , which can be significant.",
    "creator" : "LaTeX with hyperref package"
  }
}