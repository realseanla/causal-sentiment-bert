{
  "name" : "1305.2218.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic gradient descent algorithms for strongly convex functions at O(1/T ) convergence rates",
    "authors" : [ "Shenghuo Zhu" ],
    "emails" : [ "zsh@nec-labs.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nConsider a stochastic optimization problem\nmin x∈X {f(x) := EξF (x, ξ)}\nwhere X ⊂ Rd is a nonempty bounded closed convex set, ξ is a random variable, F is a smooth convex function, f is a smooth strongly-convex function. The requirement of smoothness simplifies the analysis. If the objective function is nonsmooth but satisfies Lipschitz continuity, stochastic gradient descent algorithms can replace gradients with subgradients, but the analysis has to introduce an additional term in the same order as the variance term. Some nonsmooth cases have been studied in (Lan, 2008) and (Ghadimi & Lan, 2012).\nAssume that the domain is bounded, i.e. supx,y∈X ‖x−y‖2 ≤ D2. Let G(x, ξ) be a stochastic gradient of function f at x with a random variable ξ. Then g(x) := EξG(x, ξ) is a gradient of f(x). Assume that ‖g(x)−g(y)‖∗ ≤ L‖x−y‖, where L is known as the Lipschitz constant. We only consider strongly convex function in this note, thus assume that there is µ > 0, such that f(y) ≥ f(x) + 〈g(x), y − x〉 + µ2 ‖y − x‖\n2. We assume that stochastic gradients are bounded, i.e., there exists Q > 0, such that supξ ‖G(x, ξ)− g(x)‖∗ ≤ Q.\nWe are interested in the conditional number κ, which is defined as L/µ. The conditional number, κ, could be as large as √ N , where N is the number of samples and T = N . One reference case is regularized linear classifiers (Smale & Zhou, 2003), where the regularization factor could be as large as √ N . The other reference case is the conditional number of a N × n random matrix (Rudelson & Vershynin, 2009), where the smallest singular value is O( √ N − √ n− 1). When κ = Θ( √ T ), O(κ/T ) = O(1/ √ T ), which bridges the gap between the convergence rate for strongly convex functions and that for those without strongly convex condition. In this note, we assume κ = O(T ). We use big-O notation in term of T and κ and hide the factors D2L, Q2/L and DQ besides constants.\nNotation\nDenote {1 · · ·T} by [T ]. Let {ξt : t ∈ [T ]} be a sequence of independent random variables. Denote E|t−1{·} := E{·|ξ1, · · · , ξt−1}. We define l̃n(T, t) = ∑T τ=t+1 1 τ . Then l̃n(T, t) ≤ 1 t+1 + ln(T/(t + 1)), and for t ≥ 1, l̃n(T, t) ≤ ln(T/t).\n2 Stochastic gradient descent algorithm\nAlgorithm 1 shows the stochastic gradient descent method. Unlike the conventional averaging by equal weights wt = 1/T , we use a weighting scheme wt = αt ∏T τ=t+1(1−ατ ) = t/(2T (T+1)), where αt = 2/(t+1) . Theorem 1 shows a convergence rate of O(κ/T ), assuming that T > κ. Let At = ‖xt − x∗‖2, Bt = 〈δt, xt−1 − x∗〉 /Q, Ct = ‖δt‖2∗/Q2,\nar X\niv :1\n30 5.\n22 18\nv1 [\ncs .L\nG ]\n9 M\nay 2\n01 3\nAlgorithm 1 Stochastic gradient descent algorithm\n1: Input: initial solution x0, step sizes {γt > 0 : t ∈ [T ]} and averaging factor {αt > 0 : t ∈ [T ]}. 2: for t ∈ [T ] do 3: Let sample gradient ĝk = G(xt−1, ξt), where ξt is independent from {ξτ : τ ∈ [t− 1]}.\n4: Let xt = arg min x∈X\n{ 〈ĝt, x〉+ 1\n2γt ‖x− xt−1‖2\n} ;\n5: Set x̄t = x̄t−1 + αt(xt − x̄t−1); 6: end for 7: Output: x̄T .\nand the coefficients bt = O(1) and ct = O(1/t). The informal argument is that the weighting scheme equalizes the variance of each iteration, since var(btBt) and ctCt are O(1/t) assuming that At = O(1/t).\nTheorem 1. Assume that the underlying function f is strongly convex, i.e., µ > 0. Let κ = L/µ. If αt = 2 t+1 , γt = 2\nµ(t+2κ) , then it holds for Algorithm 1 that for θ > 0,\nPr{f(x̄T )− f(x∗) ≥ K̄(T ) + √ 2θK̃(T ) + θK̂(T )} ≤ exp{−θ}, (1)\nwhere\nK̄(T ) := D2L\nT +\n2κQ2\nLT = O(κ/T ),\nK̃(T ) := 4DQ(κ+ 1)\nT 3/2 +\n2 √ 2κQ2\nLT +\n4 √ 2κ3/2Q2 √ 1 + lnT\nLT 3/2 = O(κ/T ),\nK̂(T ) := 10κQ2\nLT = O(κ/T ).\nSimilarly with traditional equal weighting scheme, wt = 1/T , we have a convergence rate of O(κ ln(T )/T ) in Proposition 2. Informally, var( ∑ t wtbtBt) = ln(T )/T implies a convergence rate of O(ln(T )/T ). Proposition 2. Assume that µ > 0. Let κ = L/µ. If αt = 1 t , γt = 1 µ(t+κ) , then for θ > 0,\nPr{f(x̄T )− f(x∗) ≥ K̄(T ) + √ 2θK̃(T ) + θK̂(T )} ≤ exp{−θ},\nwhere\nK̄(T ) := LD2 2T + κQ2 2LT (1 + lnT ), K̃(T ) :=\nDQ √ κ+ 1\nT + κQ2 LT √ 1 + lnT , K̂(T ) := 6κQ2 LT .\nProposition 3 shows that if the optimal solution x∗ is an interior point, it is possible to simply take the nonaveraged solution, xT . The convergence rate is O(κ 2/T ). However, if κ = Θ( √ T ), O(κ2/T ) means not convergent, just like the non-averaged SGD solution without strongly convex conditions.\nProposition 3. Assume that µ > 0 and the optimal solution x∗ is an interior point. Let κ = L/µ. If γt = 1\nµ(t+κ) ,\nthen for θ > 0, Pr{f(xT )− f(x∗) ≥ K̄(T ) + √ 2θK̃(T ) + θK̂(T )} ≤ exp{−θ},\nwhere\nK̄(T ) := D2L(κ+ 1)2 2(T + κ)2 + κ2Q2(T + κ(1 + lnT )) 2L(T + κ)2 = O(κ2/T ),\nK̃(T ) := DQ(κ+ 1)2√ 2(T + κ)3/2 + κ2Q2 2L(T + κ) + κ2Q2\n√ κT (1 + ln(T )) 2L(T + κ)2 = O(κ2/T ),\nK̂(T ) := 6κ2Q2\nL(T + κ) = O(κ2/T ).\nRemark 1. There are studies on the high probability convergence rate of stochastic algorithm on strongly convex functions, such as (Rakhlin et al., 2012). The convergence rate usefully is O(polylog(T )/T ). Here, we prove a convergence rate of O( κT ) with proper weighting scheme.\n3 Accelerated Stochastic Gradient Descent Algorithm\nAlgorithm 2 Accelerated Stochastic Gradient Descent algorithm\n1: Input: x0, µ, {αt ≥ 0}, {γt > 0}; 2: Let x̄0 = x0; 3: for k ∈ [T ] do 4: Let yt−1 = αtxt−1 + (1− αt)x̄t−1; 5: Let ĝt = G(yt−1, ξt), where {ξt} is a sample;\n6: Let xt = arg min x∈X\n{ 〈ĝt − µ(yt−1 − xt−1), x〉+ 1\n2γt ‖x− xt−1‖2\n} ;\n7: Set x̄t = x̄t−1 + αt(xt − x̄t−1); 8: end for 9: Output: x̄t.\nAlgorithm 2 is a stochastic variant of Nesterov’s accelerated methods. The convergence rate is also O(κ/T ).\nComparing with Theorem 1, the determinant part in Theorem 4 have a better rate, i.e. LD 2\nT 2 .\nTheorem 4. Assume that µ > 0. If αt = 2 t+1 , γt = 1 µ(2κ/t+1/αt) , then for θ > 0,\nPr{f(x̄T )− f(x∗) > K̄(T ) + √ 2θK̃(T ) + θK̂(T )} ≤ exp{−θ},\nwhere\nK̄(T ) := 2D2L\nT 2 +\n2κQ2\nLT , K̃(T ) :=\n√ 20κDQ\nT 3/2 +\n√ 10κQ2\n2LT , K̂(T ) :=\n8κQ2\nLT .\nRemark 2. The paper (Ghadimi & Lan, 2012) has its strongly convex version for AC-SA for sub-Gaussian gradient assumption, but its proof relies on a multi-stage algorithm.\nAlthough SAGE (Hu et al., 2009) also provided a stochastic algorithm based on Nesterov’s method for strongly convexity, the high probability bound was not given in the paper.\n4 A note on weighting schemes\nIn this study, we find the interesting property of weighting scheme with αt = 2 t+1 , i.e. wt = 2t T (T+1) . The scheme takes advantage of a sequence with variance at the decay rate of 1t . Now let informally investigate a sequence with homogeneous variance, say 1. With a constant weighting scheme, αt = 1/t, i.e. wt = 1/T , the averaged variance is 1/T . With an exponential weighting scheme, α1 = 1, αt = α, i.e. w1 = (1 − α)T−1 and wt = α(1 − α)T−t, the averaged variance is α2−α (1+(1−α)\n2T−1) ≈ α2−α , which is translated to that the number of effective tail samples is a constant 2α − 1. With the weighting scheme αt = 2 t+1 or wt = 2t/(T (T + 1)), the averaged variance is 2(2T+1) 3T (T+1) ≈ 4 3T , which is translated to 3T4 effective tail samples. This is a trade-off between sample efficiency and recency. To make\nother trade-offs, We can use a generalized scheme1, αt = tr∑t τ=1 τ r or wt = tr∑T τ=1 τ r . Then the averaged variance is approximately (1+r) 2\n(1+2r)T .\n5 Proofs\nThe proof strategy is first to construct inequalities from the algorithms in Lemma 6 and 7, then to apply Lemma 5 to derive the probability inequalities.\n1An alternative scheme is αt = 1+r t+r or wt = (1+r)Γ(t+r;t) Γ(T+r+1;T ) , where Γ(T ; t) := Γ(T )/Γ(t).\nLemma 5. Assume that Bt is martingale difference, wt ≥ 0, ãt ≥ 0, c̃t ≥ 0, at ≥ 0, ct ≥ 0, dt > 0, A0 ≤ D2, At ≥ 0, and\nXt = wt(ãtAt−1 + 2b̃tBt + c̃tCt), (2) At ≤ dt(atAt−1 + 2btBt + ctCt), (3) B2t ≤ At−1Ct, Ct ≤ 1.\nIf the following conditions hold\n1. for u ∈ (0, 1 2R̂T ),\nE|T exp(uXT+1) ≤ exp((uP̄T + 2u2P̃ 2T\n1− uR̂T )AT + uR̄T + 2u2R̃2T 1− uR̂T ), (4)\n2. for t ∈ [T ], atdtP̄t + wtãt ≤ P̄t−1, R̄t + wtc̃t + ctdtP̄t ≤ R̄t−1, atdtP̃ 2 t + 4(wtb̃t + btdtP̄t)\n2 ≤ P̃ 2t−1, R̃2t + ctdtP̃ 2 t ≤ R̃2t−1,\nR̂t ≤ R̂t−1, atdtP̃ 2 t R̂t + 4btdt(wtb̃t + btdtP̄t)P̃ 2 t ≤ P̃ 2t−1R̂t−1, atdtP̃ 2 t R̂ 2 t + 4btdt(wtb̃t + btdtP̄t)P̃ 2 t R̂t + 2b 2 td 2 t P̃ 4 t ≤ P̃ 2t−1R̂2t−1,\n(5)\nthen for θ > 0,\nPr{ T+1∑ t=1 Xt ≥ P̄0D2 + R̄0 + √ 2θ(P̃0D 2 + R̃0) + 2θR̂0} ≤ exp{−θ}. (6)\nProof. We will prove the following inequality by induction,\nE|t exp(u T+1∑ τ=t+1 Xτ ) ≤ exp((uP̄t + 2u2P̃ 2t 1− uR̂t )At + uR̄t + 2u2R̃2t 1− uR̂t ), ∀u ∈ (0, 1 2R̂t ). (7)\nEq. 4 implies that Eq. (7) holds for t = T . For u ∈ (0, 1 2R̂t−1 ), E|t−1 exp(u T+1∑ τ=t Xτ ) ≤ E|t−1 exp(uXt + (uP̄t + u2P̃ 2t 2(1− uR̂t) )At + uR̄t +\nu2R̃2t\n2(1− uR̂t) ) (8)\n≤ E|t−1 exp(uwt(ãtAt−1 + 2b̃tBt + c̃tCt) + (uP̄t + u2P̃ 2t\n2(1− uR̂t) )dt(atAt−1 + 2btBt + ctCt) + uR̄t +\nu2R̃2t\n2(1− uR̂t) )\n(9)\n≤ exp((u(P̄tdtat + ptãt) + u2P̃ 2t dtat\n2(1− uR̂t) )At−1 + u(R̄t + ptct + P̄tdtct) +\nu2R̃2t\n2(1− uR̂t) +\nu2P̃ 2t dtct\n2(1− uR̂t) ) (10)\n× E|t−1 exp(2u(wtb̃t + btdtP̄t + ubtdtP̃\n2 t\n2(1− uR̂t) )Bt)\n≤ exp((u(P̄tdtat + ptãt) + u2P̃ 2t dtat\n2(1− uR̂t) + 2u2(wtb̃t + btdtP̄t +\nubtdtP̃ 2 t\n2(1− uR̂t) )2)At−1\n+ u(R̄t + wtc̃t + P̄tdtct) + u2R̃2t\n2(1− uR̂t) +\nu2P̃ 2t dtct\n2(1− uR̂t) ) (11)\n≤ exp((u(P̄tdtat + ptãt) + u2P̃ 2t dtat\n2(1− uR̂t) + 2u2(wtb̃t + btdtP̄t)\n2 + u3(wtb̃t + btdtP̄t)btdtP̃ 2 t\n2(1− uR̂t) +\n2u4b2td 2 t P̃ 4 t 2(1− uR̂t) )At−1\n+ u(R̄t + wtc̃t + P̄tdtct) + u2R̃2t\n2(1− uR̂t) +\nu2P̃ 2t dtct\n2(1− uR̂t) ) (12)\n≤ exp((uP̄t−1 + u2P̃ 2t−1\n2(1− uR̂t−1) )At−1 + uR̄t−1 +\nu2R̃2t−1\n2(1− uR̂t−1) ), (13)\nwhere Eq. (8) is due to the assumption of induction; Eq. (9) is due to Eq. (2,3); Eq. (10) is due to Ct ≤ 1; Eq. (11) is due to E|t−1Bt = 0, B2t ≤ At−1Ct ≤ At−1, and Hoeffding’s lemma, thus E|t−1 exp(2vBt) ≤ exp(2v2At−1); Eq. (12) is due to 1\n1−uR̂t ≤ 2R̂t−1 2R̂t−1−R̂t ≤ 2; Eq. (13) is due to Eqs. (5). Then for u ∈ (0, 1 2R̂t ),\nE exp(u T+1∑ τ=1 Xτ ) ≤ exp((uP̄0 + u2P̃ 20 2(1− uR̂0) )A0 + uR̄0 +\nu2R̃20\n2(1− uR̂0) ) ≤ exp(u(P̄0D2 + R̄0) +\nu2(P̃ 20D 2 + R̃20)\n2(1− 2uR̂0) ).\nEq. (6) follows Lemma 8.\nWe prove Lemma 6, which is the same as Lemma 7 of (Lan, 2008) except for the strong convexity.\nLemma 6. Let δt = G(xt−1, ξt)− g(xt−1), At = ‖xt − x∗‖2, Bt = 〈δt, xt−1 − x∗〉 /Q, Ct = ‖δt‖2∗/Q2. If γt > 0 and γtL < 1, it holds for Algorithm 1 that\nf(xt)− f(x∗) ≤ 1− γtµ\n2γt At−1 −\n1\n2γt At −QBt + γt 2(1− γtL) Q2Ct.\nProof. Let dt = xt − xt−1.\nf(xt) ≤ f(xt−1) + 〈g(xt−1), dt〉+ L\n2 ‖dt‖2 (14)\n≤ f(x∗) + 〈g(xt−1), xt − x∗〉 − µ\n2 ‖xt−1 − x∗‖2 +\nL 2 ‖dt‖2 (15)\n= f(x∗) + 〈ĝt, xt − x∗〉 − µ\n2 ‖xt−1 − x∗‖2 +\nL 2 ‖dt‖2 − 〈δt, xt − x∗〉\n≤ f(x∗) + 1− γtµ\n2γt ‖xt−1 − x∗‖2 −\n1\n2γt ‖xt − x∗‖2 − 1− γtL 2γt ‖dt‖2 − 〈δt, dt〉 − 〈δt, xt−1 − x∗〉 (16)\n≤ f(x∗) + 1− γtµ\n2γt ‖xt−1 − x∗‖2 −\n1\n2γt ‖xt − x∗‖2 + γt 2(1− γtL) ‖δt‖2∗ − 〈δt, xt−1 − x∗〉 . (17)\nEq. (14) is due to the Lipschitz continuity of f , Eq. (15) due to the strong convexity of f , Eq. (16) due to the optimality of Step 4.\nProof of Theorem 1. Because γtL = 2κ t+2κ < 1, it follows Lemma 6 that\nf(xt)− f(x∗) ≤ 1− γtµ\n2γt At−1 −\n1\n2γt At −QBt +\nγtQ 2\n2(1− γtL)\n≤ (t+ 2κ− 2)µAt−1 4 − (t+ 2κ)µAt 4 −QBt +\nQ2 µt .\nAs f(xt)− f(x∗) ≥ µ2At it follows Lemma 6 that\nAt ≤ dt(atAt−1 + 2btBt + ctCt),\nwhere at = µ(t+2κ−2) 4 , bt = − Q 2 , ct = Q2 µt and dt = 4 µ(t+2κ+2) . Let wt = αt ∏T τ=t+1(1 − ατ ) = 2t T (T+1) . Assume that α0 = 0 and γ0 = 1. Then\nf(x̄T )− f(x∗) ≤ T∑ t=1 wt(f(xt)− f(x∗)) ≤ T∑ t=1 wt ( 1− γtµ 2γt At−1 − 1 2γt At −QBt +\nγtQ 2\n2(1− γtL)\n)\n≤ T∑ t=1 wt ( 1− γtµ 2γt − wt−1 2wtγt−1 ) At−1 − T∑ t=1 wtQBt + T∑ t=1 wt γtQ 2 2(1− γtL)\n≤ T∑ t=1 wt ( L 2t At−1 −QBt + Q2 µt ) ≤ LD 2 T + T∑ t=1 wt ( −QBt + Q2 µt ) .\nNote that we use the factor At−1 ≤ D2 for simplicity. Let ãt = 0, b̃t = bt, c̃t = ct, XT+1 = LD 2\nT , and\nP̄t = 0, R̄t = LD2\nT + 2κQ2(T − t) LT 2 ,\nP̃ 2t = 4Q2(T − t)(t+ 2κ+ 2)(t+ 2κ− 1)\nT 2(T + 1)2 ,\nR̃2t = Q4κ2\nL2T 2(T + 1)2 (8(T − t)(T − t− 1) + 32κT l̃n(T, t)),\nR̂t = 5κQ2(T − t)\nLT 2 .\nGiven the facts that κ ≥ 1, (t + 2κ − 2)(t + 2κ − 1) ≤ (t + 2κ + 1)(t + 2κ − 2), (T − t + 1) − (T − t) = 1, (T − t+ 1)2 − (T − t)2 ≥ 2(T − t), (T − t+ 1)3 − (T − t)3 ≥ 3(T − t)2, the proof of Eq. (23) follows from Lemma 5, because for t ≥ 1,\natdtP̄t + wtãt = 0 = P̄t−1,\nR̄t + wtct + ctdtP̄t ≤ R̄t + 2t T 2 Q2 µt ≤ R̄t−1,\natdtP̃ 2 t + 4(wtb̃t + btdtP̄t) 2 ≤ t+ 2κ− 2 t+ 2κ+ 2 P̃ 2t + 4t2Q2 T 2(T + 1)2 ≤ Q 2 T 2(T + 1)2 (4(T − t)(t+ 2κ+ 1)(t+ 2κ− 2) + 4t2)\n≤ P̃ 2t−1 − Q2\nT 2(T + 1)2 (4(t+ 2κ+ 1)(t+ 2κ− 2)− 4t2)\n= P̃ 2t−1 − Q2\nT 2(T + 1)2 (4(2κ− 1)t+ 16κ2 − 8κ− 8) ≤ P̃ 2t−1,\nR̃2t + ctdtP̃ 2 t ≤ R̃2t + 16Q4(T − t)(t+ 2κ+ 2)(t+ 2κ− 1) µ2T 2(T + 1)2t(t+ 2κ+ 2)\n≤ Q 4 µ2T 2(T + 1)2 (8(T − t)(T − t− 1) + 32κT l̃n(T, t) + 16(T − t) + 16(2κ− 1) t ) ≤ R̃2t−1,\nand\natdtP̃ 2 t R̂t + 4btdt(wtb̃t + btdtP̄t)P̃ 2 t ≤ 4Q2(T − t)(t+ 2κ− 1)(t+ 2κ− 2) T 4 R̂t + 32Q4t(T − t)(t+ 2κ− 1) µT 6\n≤ Q 4\nµT 6 (20(T − t)2(t+ 2κ− 1)(t+ 2κ− 2) + 32t(T − t)(t+ 2κ− 1))\n≤ P̃ 2t−1R̂t−1 − Q4(T − t) µT 6 (2× 20(t+ 2κ+ 1)(t+ 2κ− 2)− 32t(t+ 2κ− 1)) = P̃ 2t−1R̂t−1 − Q4(T − t) µT 6 (8t2 − 8t+ 16κ(6t− 5) + 160κ2 − 80) ≤ P̃ 2t−1R̂t−1.\natdtP̃ 2 t R̂ 2 t + 4btdt(wtb̃t + btdtP̄t)P̃ 2 t R̂t + 2b 2 td 2 t P̃ 4 t\n≤ 4Q 2(T − t)(t+ 2κ+ 1)(t+ 2κ− 2)\nT 4 R̂2t + 32Q4t(T − t)(t+ 2κ− 1) µT 6 R̂t + 128Q6(T − t)2(t+ 2κ− 1)2 µ2T 8\n≤ P̃ 2t−1R̂2t−1 − Q6(T − t)2\nµ2T 8 (3× 100(t+ 2κ+ 1)(t+ 2κ− 2)− 160t(t+ 2κ− 1)− 128(t+ 2κ− 1)2)\n= P̃ 2t−1R̂ 2 t−1 −\nQ6(T − t)2\nµ2T 8 (12(t− 1)2 + 368(t− 1)(κ− 1) + 688(κ− 1)2 + 508(t− 1) + 1656(κ− 1) + 368)\n≤ P̃ 2t−1R̂2t−1.\nProof of Proposition 2. Because γtL < 1, it follows Lemma 6 that\nf(xt)− f(x∗) ≤ 1− γtµ\n2γt At−1 −\n1\n2γt At −QBt +\nγtQ 2\n2(1− γtL)\n≤ (L+ µ(2t− 1))At−1 2 − (L+ 2µt)At 2 −QBt +\nQ2 4µt .\nAs the strong convexity implies that f(xt)− f(x∗) ≥ µ2At, it follows Lemma 6 that At ≤ dt(atAt−1 + 2btBt + ctCt),\nwhere at = µ(t+κ−1) 2 , bt = − Q 2 , ct = Q2 2µt and dt = 2 µ(t+κ+1) . Let wt = αt ∏T τ=t+1(1− ατ ) = 1 T . Assume that α0 = 0 and γ0 = 1. Then\nf(x̄T )− f(x∗) ≤ T∑ t=1 wt(f(xt)− f(x∗)) ≤ T∑ t=1 wt ( 1− γtµ 2γt At−1 − 1 2γt At −QBt +\nγtQ 2\n2(1− γtL)\n)\n≤ T∑ t=1 wt ( 1− γtµ 2γt − wt−1 2wtγt−1 ) At−1 − T∑ t=1 wtQBt + T∑ t=1 wt γtQ 2 2(1− γtL)\n≤ LA0 2T + T∑ t=1 wt ( −QBt + Q2 4µt ) .\nLet ãt = 0, b̃t = bt, c̃t = ct, XT+1 = LD2\n2T , and\nP̄t = 0, R̄t = Q2\n2µT l̃n(T, t),\nP̃ 2t = Q2(t+ κ+ 1)\nT 2 ,\nR̃2t = Q4\nµ2T 2 l̃n(T, t),\nR̂t = 3Q2\nµT .\nThe proof follows from Lemma 5, because for k ≥ 1,\nP̄tdtat + ptãt = 0 = P̄t−1, R̄t + wtct + P̄tdtct ≤ Q2\n2µT ln T t + 1 T\nQ2 2µt ≤ R̄t−1,\nP̃ 2t dtat + 4(wt + P̄tdt) 2b2t ≤\nQ2(κ+ t+ 1) T 2 t+ κ− 1 t+ κ+ 1 + Q2 T 2 = Q2(t+ κ) T 2(t+ κ+ 1) = P̃ 2t−1,\nR̃2t + P̃ 2 t dtct ≤\nQ4\nµ2T 2 ln T t +\n2Q2 µT 2 Q2 2µt ≤ R̃2t−1,\nand\natdtP̃ 2 t R̂t + 4btdt(wtb̃t + btdtP̄t)P̃ 2 t ≤ Q2(t+ κ− 1)(t+ κ+ 1) T 2(t+ κ+ 1) R̂t + 2Q4(t+ κ+ 1) µT 3(t+ κ+ 1) ≤ Q 2(t+ κ) T 2 R̂t−1.\natdtP̃ 2 t R̂ 2 t + 4btdt(wtb̃t + btdtP̄t)P̃ 2 t R̂t + 2b 2 td 2 t P̃ 4 t\n≤ Q 2(t+ κ− 1)(t+ κ+ 1)\nT 2(t+ κ+ 1) R̂2t +\n2Q4(t+ κ+ 1) µT 3(t+ κ+ 1) R̂t + 2Q6(t+ κ+ 1)2 µ2T 4(t+ κ+ 1)2 ≤ Q 2(t+ κ) T 2 R̂2t−1.\nProof of Proposition 3. Because γtL < 1, it follows Lemma 6 that\nf(xt)− f(x∗) ≤ 1− γtµ\n2γt At−1 −\n1\n2γt At −QBt +\nγtQ 2\n2(1− γtL)\n≤ (L+ µ(t− 1))At−1 2 − (L+ µt)At 2 −QBt +\nQ2 2µt .\nAs the strong convexity implies that f(xt)− f(x∗) ≥ µ2At, it follows Lemma 6 that\nAt ≤ dt(atAt−1 + 2btBt + ctCt),\nwhere at = µ(t+κ−1) 2 , bt = − Q 2 , ct = Q2 2µt and dt = 2 µ(t+κ+1) . Because the solution is an interior point, we have\nf(xT )− f(x∗) ≤ L\n2 AT .\nLet wt = 0, XT+1 = L 2AT , and\nP̄t = L(t+ κ)(t+ κ+ 1)\n2(T + κ)(T + κ+ 1) ,\nR̄t = κ2Q2\n2L(T + κ)(T + κ+ 1) (T − t+ κl̃n(T, t)),\nP̃ 2t = Q2κ2(T − t)(t+ κ)(t+ κ+ 1)\n2(T + κ)2(T + κ+ 1)2 ,\nR̃2t = κ4Q4\n4L2(T + κ)2(T + κ+ 1)2 ((T − t)(T − t− 1) + κT l̃n(T, t)),\nR̂t = 2κ2Q2(T − t)\nL(T + κ)(T + κ+ 1) .\nThe proof follows from Lemma 5, because\nP̄tdtat = L(t+ κ)(t+ κ− 1) 2(T + κ)(T + κ+ 1) = P̄t−1,\nR̄t + P̄tdtct ≤ R̄t + L(t+ κ)(t+ κ+ 1)\n2(T + κ)(T + κ+ 1)\n2\nµ(t+ κ+ 1)\nQ2\n2µt\n≤ κ 2Q2 2L(T + κ)(T + κ+ 1) (T − t+ κl̃n(T, t) + t+ κ t ) ≤ R̄t−1,\nP̃ 2t dtat + P̄ 2 t d 2 t b 2 t ≤ t+ κ− 1 t+ κ+ 1 P̃ 2t + κ2Q2(t+ κ)2 4(T + κ)2(T + κ+ 1)2\n≤ P̃ 2t−1 − κ2Q2 (T + κ)2(T + κ+ 1)2 ( 1 2 (t+ κ− 1)(t+ κ)− 1 4 (t+ κ)2) ≤ P̃ 2t−1 − κ2Q2\n(T + κ)2(T + κ+ 1)2 ( 1 4 (t+ κ)(t+ κ− 2)) ≤ P̃ 2t−1, [t ≥ 1 and κ ≥ 1]\nR̃2t + P̃ 2 t dtct ≤ R̃2t + Q4κ2(T − t)(t+ κ) 2µ2(T + κ)2(T + κ+ 1)2t\n≤ Q 4κ2 4µ2(T + κ)2(T + κ+ 1)2 ((T − t)(T − t− 1) + κT l̃n(T, t) + 2(T − t) + (T − t)κ t ) ≤ R̃2t−1,\nand\natdtP̃ 2 t R̂t + 4b 2 td 2 t P̄tP̃ 2 t\n≤ Q 2κ2\n(T + κ)2(T + κ+ 1)2\n( 1\n2 (T − t)(t+ κ)(t+ κ− 1)R̂t + (T − t)(t+ κ)\nLQ2(t+ κ)\nµ2(T + κ)(T + κ+ 1) ) ≤ P̃ 2t−1R̂t−1 − Q4κ4 L(T + κ)3(T + κ+ 1)3 ( 2(T − t)(t+ κ)(t+ κ− 1)− (T − t)(t+ κ)2\n) ≤ P̃ 2t−1R̂t−1 − Q4κ4\nL(T + κ)3(T + κ+ 1)3(T − t) (t+ κ)(t+ κ− 2) ≤ P̃ 2t−1R̂t−1.\natdtP̃ 2 t R̂ 2 t + 4b 2 td 2 t P̄tP̃ 2 t R̂t + 2b 2 td 2 t P̃ 4 t\n≤ Q 2κ2 (T + κ)2(T + κ+ 1)2 ( 1 2 (T − t)(t+ κ)(t+ κ− 1)R̂2t + (T − t)(t+ κ)\nLQ2(t+ κ)\nµ2(T + κ)(T + κ+ 1) R̂t\n+ Q4κ2(T − t)2(t+ κ)2\n4µ2(T + κ)2(T + κ+ 1)2 )\n≤ P̃ 2t−1R̂2t−1 − Q6κ6 L2(T + κ)4(T + κ+ 1)4 (6(T − t)2(t+ κ)(t+ κ− 1)− 2(T − t)2(t+ κ)− 1 4 (T − t)2(t+ κ)2) ≤ P̃ 2t−1R̂2t−1 − Q6κ6(T − t)2(t+ κ)\nL2(T + κ)4(T + κ+ 1)4 ( 15 4 (t+ κ)− 6) ≤ P̃ 2t−1R̂2t−1.\nSimilar to Lemma 9 of (Lan, 2008), we have the following lemma for Algorithm 2 with the consideration of strongly convex cases.\nLemma 7. Let δt = G(yt−1, ξt)− g(yt−1), At = ‖xt − x∗‖2, Bt = 〈δt, xt−1 − x∗〉 /Q, Ct = ‖δt‖2∗/Q2. If 0 < αt < 1, γt > 0 and γt(αtL+ µ) < 1, it holds for Algorithm 2 that\nf(x̄t)− f(x∗) ≤ (1− αt)(f(x̄t−1)− f(x∗)) + αt(1− γtµ)\n2γt At−1 − αt 2γt At − αtQBt + αtγt 2(1− αtγtL− γtµ) Q2Ct.\nProof. Let dt = xt − xt−1 and vt = xt−1 + γtµ(yt−1 − xt−1). Note that x̄t − yt−1 = αtdt.\nf(x̄t) ≤ f(yt−1) + 〈g(yt−1), x̄t − yt−1〉+ L\n2 ‖x̄t − yt−1‖2 (18)\n= (1− αt)[f(yt−1) + 〈g(yt−1), x̄t−1 − yt−1〉] + αt[f(yt−1) + 〈g(yt−1), xt − yt−1〉] + α2tL\n2 ‖dt‖2\n≤ (1− αt)f(x̄t−1) + αtf(x∗) + αt 〈g(yt−1), xt − x∗〉 − αtµ\n2 ‖yt−1 − x∗‖2 +\nα2tL\n2 ‖dt‖2 (19)\n= (1− αt)f(x̄t−1) + αtf(x∗) + αt 〈ĝt, xt − x∗〉 − αtµ\n2 ‖yt−1 − x∗‖2 +\nα2tL\n2 ‖dt‖2 − αt 〈δt, xt − x∗〉\n≤ (1− αt)f(x̄t−1) + αtf(x∗) + αt γt 〈xt − vt, x∗ − xt〉 − αtµ 2 ‖yt−1 − x∗‖2 +\nα2tL\n2 ‖dt‖2 − αt 〈δt, xt − x∗〉 (20)\n= (1− αt)f(x̄t−1) + αtf(x∗) + αt(1− γtµ)\n2γt ‖xt−1 − x∗‖2 − αt 2γt ‖xt − x∗‖2 − αtµ 2 ‖yt−1 − xt‖2\n− αt(1− γtµ− αtγtL) 2γt ‖dt‖2 − αt 〈δt, dt〉 − αt 〈δt, xt−1 − x∗〉\n≤ (1− αt)f(x̄t−1) + αtf(x∗) + αt(1− γtµ)\n2γt ‖xt−1 − x∗‖2 − αt 2γt ‖xt − x∗‖2\n+ αtγt\n2(1− γtµ− αtγtL) ‖δt‖2∗ − αt 〈δt, xt−1 − x∗〉 . (21)\nEq. (24) is due to the Lipschitz continuity of f , Eq. (25) due to the strong convexity of f , Eq. (20) due to the optimality of Step 6.\nProof of Theorem 4. Let λt = ∏T τ=t+1(1− αt) = t(t+1) T (T+1) . We have and\nλtαt(1− γtµ) γt − λt−1αt−1 γt−1 = 2t T (T + 1) ( 2L t + µ(t+ 1) 2 − µ)− 2(t− 1) T (T + 1) ( 2L t− 1 + µt 2 ) = 0, ∀t > 1.\nLet at = µ(4κ+t(t−1)) 2t , bt = − Q 2 , ct = Q2 µt , and dt = 2t µ(4κ+t(t+1)) . Summing up the inequality in Lemma 7 weighted by λt, we have\nf(x̄t)− f(x∗) ≤ λ1α1(1− γ1µ)\n2γ1 A0 − λtαt 2γt At + t∑ τ=1 λτατ (−QBt + γτ 2(1− ατγτL− γτµ) Q2Cτ )\n≤ 2L T (T + 1) A0 − 2t T (T + 1) At dt + t∑ τ=1\n2τ\nT (T + 1) (2bτBτ + cτCτ ) .\n(22)\nLet Ãt := dt t { LA0 + ∑t τ=1 (2τbτBt + τctCt) } . Because f(x̄t)− f(x∗) ≥ 0, we have\nt\ndt At ≤\nt\ndt Ãt = t− 1 dt−1 Ãt−1 + 2tbtBt + tctCt = tatÃt−1 + 2tbtBt + tctCt\nThen At ≤ Ãt = dt(atÃt−1 + 2btBt + ctCt). (23)\nGiven Eq. (22) and Eq. (23), letting wt = 2t T (T+1) , ãt = 0, b̃t = bt, c̃t = ct, XT+1 = 2LD2 T (T+1) , and\nP̄t = 0, R̄t = 2LD2\nT 2 + 2κQ2(T − t) LT 2 ,\nP̃ 2t = 5Q2(T − t)(t(t+ 1) + 4κ)\nT 4 ,\nR̃2t = 5κ2Q4(T − t)(T − t− 1)\n2L2T 4 ,\nR̂t = 4κQ2(T − t)\nLT 2 ,\nthe proof follows from Lemma 5, because\natdtP̄t + wtãt = 0 = P̄t−1,\nR̄t + wtc̃t + ctdtP̄t ≤ R̄t + 2t T 2 Q2 µt ≤ R̄t−1,\natdtP̃ 2 t + 4(wtb̃t + btdtP̄t) 2 ≤ t(t− 1) + 4κ t(t+ 1) + 4κ P̃ 2t + 4t2Q2 T 4\n≤ Q 2\nT 4 (6(t(t− 1) + 4κ)(T − t) + 4t2) ≤ P̃ 2t−1 −\nQ2 T 4 (5(t(t− 1) + 4κ)− 4t2)\n≤ P̃ 2t−1 − Q2\nT 4 (t2 − 5t+ 20κ) ≤ P̃ 2t−1 −\nQ2 T 4 (3t) ≤ P̃ 2t−1,\nR̃2t + ctdtP̃ 2 t ≤ R̃2t + 5Q4(T − t) µ2T 4 ≤ R̃2t−1,\nand\natdtP̃ 2 t R̂t + 4btdt(wtb̃t + btdtP̄t)P̃ 2 t ≤ Q2(t(t− 1) + 4κ)(T − t) T 4 R̂t + 4t2Q4(T − t) µT 6\n≤ Q 4\nµT 6 (4(t(t− 1) + 4κ)(T − t)2 + 4t2(T − t))\n≤ P̃ 2t−1R̂t−1 − Q4(T − t) µT 6 (2× 4(t(t− 1) + 4κ)− 4t2) = P̃ 2t−1R̂t−1 − Q4(T − t) µT 6 (4t2 − 8t+ 32κ) ≤ P̃ 2t−1R̂t−1 − Q4(T − t) µT 6 (14t) ≤ P̃ 2t−1R̂t−1\natdtP̃ 2 t R̂ 2 t + 4btdt(wtb̃t + btdtP̄t)P̃ 2 t R̂t + 2b 2 td 2 t P̃ 4 t\n≤ 5Q 2(t(t− 1) + 4κ)(T − t)\nT 4 R̂2t + 20t2Q4(T − t) µT 6 R̂t + 100t2Q6(T − t)2 µ2T 8\n≤ Q 6\nµ2T 8 (80(t(t− 1) + 4κ)(T − t)3 + 80t2(T − t)2 + 100t2(T − t)2)\n≤ P̃ 2t−1R̂2t−1 − Q6(T − t)2\nµ2T 8 (3× 80(t(t− 1) + 4κ)− 80t2 − 100t2)\n= P̃ 2t−1R̂ 2 t−1 −\nQ6(T − t)2\nµ2T 8 (60t2 − 240t+ 960κ) ≤ P̃ 2t−1R̂2t−1 −\n(T − t)2Q6\nµ2T 8 (240t) ≤ P̃ 2t−1R̂2t−1.\nSupporting lemma\nWe use part of the proof of Lemma 8 in (Birgé & Massart, 1998).\nLemma 8. Let B > 0 and σ > 0. If the log-moment generating function satisfies\nlogE exp{uZ} ≤ σ 2u2\n2(1− uB) for all 0 ≤ u < 1/B,\nthen\nPr{Z ≥ } ≤ exp{− 2\n2σ2 + 2 B } for all ≥ 0, (24)\nand Pr{Z ≥ √ 2θσ2 + θB} ≤ exp{−θ} for all θ ≥ 0. (25)\nProof. It follows Markov’s inequality that\nPr{Z ≥ } ≤ inf u E exp{−u + uZ} = exp{−h( )},\nwhere h( ) := supu u − σ 2u2 2(1−uB) . Also, the supremum is achieved for\n= σ2u\n1− uB +\nσ2u2B\n2(1− uB)2 =\nσ2u\n2(1− uB) +\nσ2u\n2(1− uB)2 ,\ni.e. u = B−1[1− σ(2 B + σ2)−1/2] < 1/B. Then we prove Eq. (24), as\nh( ) = 2\nB + σ2 + σ2(1 + 2 B/σ2)1/2 ≥\n2\n2 B + 2σ2 .\nLet\nθ := σ2u2\n2(1− uB)2 = h( ).\nThen we prove Eq. (25), as\n√ 2θσ2 + θB =\nσ2u\n(1− uB) +\nσ2u2B\n2(1− uB)2 = .\nReferences\nBirgé, L., & Massart, P. (1998). Minimum contrast estimators on sieves: exponential bounds and rates of convergence. Bernoulli, 4, 329–375.\nGhadimi, S., & Lan, G. (2012). Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: a generic algorithmic framework. Optimization-online.\nHu, C., Kwok, J. T., & Pan, W. (2009). Accelerated gradient methods for stochastic optimization and online learning. NIPS’09: Neural Information Processing Systems.\nLan, G. (2008). Efficient methods for stochastic composite optimization. SIAM Journal on Optimization.\nRakhlin, A., Shamir, O., & Sridharan, K. (2012). Making gradient descent optimal for strongly convex stochastic optimization. ICML 2012.\nRudelson, M., & Vershynin, R. (2009). Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics, 62, 1707–1739.\nSmale, S., & Zhou, D.-X. (2003). Estimating the approximation error in learning theory. Anal. Appl. (Singap.), 1, 17–41."
    } ],
    "references" : [ {
      "title" : "Minimum contrast estimators on sieves: exponential bounds and rates of convergence",
      "author" : [ "L. Birgé", "P. Massart" ],
      "venue" : null,
      "citeRegEx" : "Birgé and Massart,? \\Q1998\\E",
      "shortCiteRegEx" : "Birgé and Massart",
      "year" : 1998
    }, {
      "title" : "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: a generic algorithmic framework. Optimization-online",
      "author" : [ "S. Ghadimi", "G. Lan" ],
      "venue" : null,
      "citeRegEx" : "Ghadimi and Lan,? \\Q2012\\E",
      "shortCiteRegEx" : "Ghadimi and Lan",
      "year" : 2012
    }, {
      "title" : "Accelerated gradient methods for stochastic optimization and online learning. NIPS’09",
      "author" : [ "C. Hu", "J.T. Kwok", "W. Pan" ],
      "venue" : "Neural Information Processing Systems",
      "citeRegEx" : "Hu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient methods for stochastic composite optimization",
      "author" : [ "G. Lan" ],
      "venue" : "SIAM Journal on Optimization",
      "citeRegEx" : "Lan,? \\Q2008\\E",
      "shortCiteRegEx" : "Lan",
      "year" : 2008
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "A. Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : null,
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2012
    }, {
      "title" : "Smallest singular value of a random rectangular matrix",
      "author" : [ "M. Rudelson", "R. Vershynin" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Rudelson and Vershynin,? \\Q2009\\E",
      "shortCiteRegEx" : "Rudelson and Vershynin",
      "year" : 2009
    }, {
      "title" : "Estimating the approximation error in learning theory",
      "author" : [ "S. Smale", "Zhou", "D.-X" ],
      "venue" : "Anal. Appl. (Singap.),",
      "citeRegEx" : "Smale et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Smale et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Some nonsmooth cases have been studied in (Lan, 2008) and (Ghadimi & Lan, 2012).",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "There are studies on the high probability convergence rate of stochastic algorithm on strongly convex functions, such as (Rakhlin et al., 2012).",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "Although SAGE (Hu et al., 2009) also provided a stochastic algorithm based on Nesterov’s method for strongly convexity, the high probability bound was not given in the paper.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "We prove Lemma 6, which is the same as Lemma 7 of (Lan, 2008) except for the strong convexity.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "Similar to Lemma 9 of (Lan, 2008), we have the following lemma for Algorithm 2 with the consideration of strongly convex cases.",
      "startOffset" : 22,
      "endOffset" : 33
    } ],
    "year" : 2013,
    "abstractText" : "With a weighting scheme proportional to t, a traditional stochastic gradient descent (SGD) algorithm achieves a high probability convergence rate of O(κ/T ) for strongly convex functions, instead of O(κ ln(T )/T ). We also prove that an accelerated SGD algorithm also achieves a rate of O(κ/T ).",
    "creator" : "TeX"
  }
}