{
  "name" : "1511.02619.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Decomposition Bounds for Marginal MAP",
    "authors" : [ "Wei Ping", "Qiang Liu", "Alexander Ihler" ],
    "emails" : [ "wping@ics.uci.edu", "ihler@ics.uci.edu", "qliu@cs.dartmouth.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods."
    }, {
      "heading" : "1 Introduction",
      "text" : "Probabilistic graphical models such as Bayesian networks and Markov random fields provide a useful framework and powerful tools for machine learning. Given a graphical model, inference refers to answering probabilistic queries about the model. There are three common types of inference tasks. The first are max-inference or maximum a posteriori (MAP) tasks, which aim to find the most probable state of the joint probability; exact and approximate MAP inference is widely used in structured prediction [26]. Sum-inference tasks include calculating marginal probabilities and the normalization constant of the distribution, and play a central role in many learning tasks (e.g., maximum likelihood). Finally, marginal MAP tasks are “mixed” inference problems, which generalize the first two types by marginalizing a subset of variables (e.g., hidden variables) before optimizing over the remainder.1 These tasks arise in latent variable models [e.g., 29, 25] and many decision-making problems [e.g., 13]. All three inference types are generally intractable; as a result, approximate inference, particularly convex relaxations or upper bounding methods, are of great interest.\nDecomposition methods provide a useful and computationally efficient class of bounds on inference problems. For example, dual decomposition methods for MAP [e.g., 31] give a class of easy-toevaluate upper bounds which can be directly optimized using coordinate descent [37, 6], subgradient updates [14], or other methods [e.g., 22]. It is easy to ensure both convergence, and that the objective is monotonically decreasing (so that more computation always provides a better bound). The resulting bounds can be used either as stand-alone approximation methods [6, 14], or as a component of search [11]. In summation problems, a notable decomposition bound is tree-reweighted BP (TRW), which bounds the partition function with a combination of trees [e.g., 34, 21, 12, 3]. These bounds are useful in joint inference and learning (or “inferning”) frameworks, allowing learning with approximate inference to be framed as a joint optimization over the model parameters and decomposition bound, often leading to more efficient learning [e.g., 23]. However, far fewer methods have been developed for marginal MAP problems.\n1In some literature [e.g., 28], marginal MAP is simply called MAP, and the joint MAP task is called MPE.\nar X\niv :1\n51 1.\n02 61\n9v 1\n[ cs\n.L G\n] 9\nN ov\n2 01\nIn this work, we deveop a decomposition bound that has a number of desirable properties: (1) Generality: our bound is sufficiently general to be applied easily to marginal MAP. (2) Any-time: it yields a bound at any point during the optimization (not just at convergence), so it can be used in an anytime way. (3) Monotonic and convergent: more computational effort gives strictly tighter bounds; note that (2) and (3) are particularly important for high-width approximations, which are expensive to represent and update. (4) Allows optimization over all parameters, including the “weights”, or fractional counting numbers, of the approximation; these parameters often have a significant effect on the tightness of the resulting bound. (5) Compact representation: within a given class of bounds, using fewer parameters to express the bound reduces memory and typically speeds up optimization.\nWe organize the rest of the paper as follows. Section 2 gives some background and notation, followed by connections to related work in Section 3. We derive our decomposed bound in Section 4, and present a (block) coordinate descent algorithm for monotonically tightening it in Section 5. We report experimental results in Section 6 and conclude the paper in Section 7."
    }, {
      "heading" : "2 Background",
      "text" : "Here, we review some background on graphical models and inference tasks. A Markov random field (MRF) on discrete random variables x = [x1, . . . , xn] ∈ Xn is a probability distribution,\np(x; θ) = exp [ ∑ α∈F θα(xα)− Φ(θ) ] ; Φ(θ) = log ∑ x∈Xn exp [ ∑ α∈F θα(xα) ] , (1)\nwhere F is a set of subsets of the variables, each associated with a factor θα, and Φ(θ) is the log partition function. We associate an undirected graph G = (V,E) with p(x) by mapping each xi to a node i ∈ V , and adding an edge ij ∈ E iff there exists α ∈ F such that {i, j} ⊆ α. We say node i and j are neighbors if ij ∈ E. Then, F is the subset of cliques (fully connected subgraphs) of G. The use and evaluation of a given MRF often involves different types of inference tasks. Marginalization, or sum-inference tasks perform a sum over the configurations to calculate the log partition function Φ in (1), marginal probabilities, or the probability of some observed evidence. On the other hand, the maximum a posteriori (MAP), or max-inference tasks perform joint maximization to find configurations with the highest probability, that is, Φ0(θ) = maxx ∑ α∈F θα(xα).\nA generalization of max- and sum- inference is marginal MAP, or mixed-inference, in which we are interested in first marginalizing a subsetA of variables (e.g., hidden variables), and then maximizing the remaining variables B (whose values are of direct interest), that is,\nΦAB(θ) = max xB Q(xB) = max xB log ∑ xA exp [ ∑ α∈F θα(xα) ] , (2)\nwhere A ∪B = V (all the variables) and A ∩B = ∅. Obviously, both sum- and max- inference are special cases of marginal MAP when A = V and B = V , respectively.\nIt will be useful to define an even more general inference task, based on a power sum operator: τi∑ xi f(xi) = [∑ xi f(xi) 1/τi ]τi ,\nwhere f(xi) is any non-negative function and τi is a temperature or weight parameter. The power sum reduces to a standard sum when τi = 1, and approaches maxx f(x) when τi → 0+, so that we define the power sum with τi = 0 to equal the max operator.\nThe power sum is helpful for unifying max- and sum- inference [e.g., 36], as well as marginal MAP [15]. Specifically, we can apply power sums with different weights τi to each variable xi along a predefined elimination order (e.g., [x1, . . . , xn]), to define the weighted log partition function:\nΦτ (θ) = log τ∑ x exp(θ(x)) = log τn∑ xn . . . τ1∑ x1 exp(θ(x)), (3)\nwhere we note that the value of (3) depends on the elimination order unless all the weights are equal. Obviously, (3) includes marginal MAP (2) as a special case by setting weights τA = 1 and τB = 0. This representation provides a useful tool for understanding and deriving new algorithms for general inference tasks, especially marginal MAP, for which relatively few efficient algorithms exist."
    }, {
      "heading" : "3 Related Work",
      "text" : "Variational upper bounds on MAP and the partition function, along with algorithms for providing fast, convergent optimization, have been widely studied in the last decade. In MAP, dual decomposition and linear programming methods have become a dominating approach, with numerous optimization techniques [37, 6, 32, 14, 38, 30, 22], and methods to tighten the approximations [33, 14].\nFor summation problems, most upper bounds are derived from the tree-reweighted (TRW) family of convex bounds [34], or more generally conditional entropy decompositions [5]. TRW bounds can be framed as optimizing over a convex combination of tree-structured models, or in a dual representation as a message-passing, TRW belief propagation algorithm. This illustrates a basic tension in the resulting bounds: in its primal form 2 (combination of trees), TRW is inefficient: it maintains a weight and O(|V |) parameters for each tree, and a large number of trees may be required to obtain a tight bound; this uses memory and makes optimization slow. On the other hand, the dual, or free energy, form uses onlyO(|E|) parameters (the TRW messages) to optimize over the set of all possible spanning trees – but, the resulting optimization is only guaranteed to be a bound at convergence, 3 making it difficult to use in an anytime fashion. Similarly, the gradient of the weights is only correct at convergence, making it difficult to optimize over these parameters; most implementations [e.g., 24] simply adopt fixed weights.\nThus, most algorithms do not satisfy all the desirable properties listed in the introduction. For example, many works have developed convergent message-passing algorithms for convex free energies [e.g., 9, 10]. However, by optimizing the dual they do not provide a bound until convergence, and the representation and constraints on the counting numbers do not facilitate optimizing the bound over these parameters. To optimize counting numbers, [8] adopt a more restrictive free energy form requiring positive counting numbers on the entropies; but this cannot represent marginal MAP, whose free energy involves conditional entropies (equivalent to the difference between two entropy terms).\nOn the other hand, working in the primal domain ensures a bound, but usually at the cost of enumerating a large number of trees. [12] heuristically select a small number of trees to avoid being too inefficient, while [21] focus on trying to speed up the updates on a given collection of trees. Another primal bound is weighted mini-bucket (WMB, [16]), which can represent a large collection of trees compactly and is easily applied to marginal MAP using the weighted log partition function viewpoint [15, 18]; however, existing optimization algorithms for WMB are non-monotonic, and often fail to converge, especially on marginal MAP tasks.\nWhile our focus is on variational bounds [16, 17], there are many non-variational approaches for marginal MAP as well. [27, 40] provide upper bounds on marginal MAP by reordering the order in which variables are eliminated, and using exact inference in the reordered join-tree; however, this is exponential in the size of the (unconstrained) treewidth, and can easily become intractable. [20] give an approximation closely related to mini-bucket [2] to bound the marginal MAP; however, unlike (weighted) mini-bucket, these bounds cannot be improved iteratively. The same is true for the algorithm of [19], which also has a strong dependence on treewidth. Other examples of marginal MAP algorithms include local search [e.g., 28] and Markov chain Monte Carlo methods [e.g., 4, 41]."
    }, {
      "heading" : "4 Fully Decomposed Upper Bound",
      "text" : "In this section, we develop a new general form of upper bound and provide an efficient, monotonically convergent optimization algorithm. Our new bound is based on fully decomposing the graph into disconnected cliques, allowing very efficient local computation, but can still be as tight as WMB or the TRW bound with a large collection of spanning trees once the weights and shifting variables are chosen or optimized properly. Our bound reduces to dual decomposition for MAP inference, but is applicable to more general mixed-inference settings.\nOur main result is based on the following generalization of the classical Hölder’s inequality [7]: 2Despite the term “dual decomposition” used in MAP tasks, in this work we refer to decomposition bounds as “primal” bounds, since they can be viewed as directly bounding the result of variable elimination. This is in contrast to, for example, the linear programming relaxation of MAP, which bounds the result only after optimization.\n3See an example in Supplement A.\nTheorem 4.1. For a given graphical model p(x; θ) in (1) with cliques F = {α} and a set of nonnegative weights τ = {τi ≥ 0, i ∈ V }, we define a set of “split weights” wα = {wαi ≥ 0, i ∈ α} on each variable-clique pair (i, α), that satisfies ∑ α|α3i w α i = τi. Then we have\nτ∑ x ∏ α∈F exp [ θα(xα) ] ≤ ∏ α∈F wα∑ xα exp [ θα(xα) ] , (4)\nwhere the left-hand side is the powered-sum along order [x1, . . . , xn] as defined in (3), and the right-hand side is the product of the powered-sums on subvector xα with weights wα along\nthe same elimination order; that is, ∑wα xα exp [ θα(xα) ] = ∑wαkc xkc · · · ∑wαk1 xk1 exp [ θα(xα) ] , where xα = [xk1 , . . . , xkc ] should be ranked with increasing index, consisting with the elimination order [x1, . . . , xn] as used in the left-hand side.\nProof details can be found in Section E of the supplement. A key advantage of the bound (4) is that it decomposes the joint power sum on x into a product of independent power sums over smaller cliques xα, which significantly reduces computational complexity and enables parallel computation."
    }, {
      "heading" : "4.1 Including Cost-shifting Variables",
      "text" : "In order to increase the flexibility of the upper bound, we introduce a set of cost-shifting or reparameterization variables δ = {δαi (xi) | ∀(i, α), i ∈ α} on each variable-factor pair (i, α), which can be optimized to provide a much tighter upper bound. Note that Φτ (θ) can be rewritten as,\nΦτ (θ) = log τ∑ x exp [∑ i∈V ∑ α∈Ni δαi (xi) + ∑ α∈F ( θα(xα)− ∑ i∈α δαi (xi) )] ,\nwhere Ni = {α | α 3 i} is the set of cliques incident to i. Applying inequality (4), we have that\nΦτ (θ) ≤ ∑ i∈V log wi∑ xi exp [ ∑ α∈Ni δαi (xi) ] + ∑ α∈F log wα∑ xα exp [ θα(xα)− ∑ i∈α δαi (xi) ] def == L(δ,w), (5)\nwhere the nodes i ∈ V are also treated as cliques within inequality (4), and a new weight wi is introduced on each variable i; the new weights w = {wi, wαi | ∀(i, α), i ∈ α} should satisfy\nwi + ∑ α∈Ni wαi = τi, wi ≥ 0, wαi ≥ 0, ∀(i, α). (6)\nThe bound L(δ,w) is convex w.r.t. the cost-shifting variables δ and weights w, enabling an efficient optimization algorithm that we present in Section 5. As we will discuss in Section 5.1, these shifting variables correspond to Lagrange multipliers that enforce a moment matching condition."
    }, {
      "heading" : "4.2 Dual Form and Connection With Existing Bounds",
      "text" : "It is straightforward to see that our bound in (5) reduces to dual decomposition [31] when applied on MAP inference with all τi = 0, and hence wi = wαi = 0. On the other hand, its connection with sum-inference bounds such as WMB and TRW is seen more clearly via a dual representation of (5):\nTheorem 4.2. The tightest upper bound obtainable by (5), that is,\nmin w min δ L(δ,w) = min w max b∈L(G)\n{ 〈θ, b〉+ ∑ i∈V wiH(xi; bi) + ∑ α∈F ∑ i∈α wαi H(xi|xpaαi ; bα) } , (7)\nwhere b = {bi(xi), bα(xα) | ∀(i, α), i ∈ α} is a set of pseudo-marginals (or beliefs) defined on the singleton variables and the cliques, and L is the corresponding local consistency polytope defined by L(G) = {b | bi(xi) = ∑ xα\\i bα(xα), ∑ xi bi(xi) = 1}. Here, H(·) are their corresponding marginal or conditional entropies, and paαi is the set of variables in α that rank later than i, that is, for the global elimination order [x1, . . . , xn], paαi = {j ∈ α | j i}. The proof details can be found in Section F of the supplement. It is useful to compare Theorem 4.2 with other dual representations. As the sum of non-negatively weighted conditional entropies, the bound is clearly convex and within the general class of conditional entropy decompositions (CED) [5], but unlike generic CED it has a simple and efficient primal form (5). 4 Comparing\n4The primal form derived in [5] (a geometric program) is computationally infeasible.\nto the dual form of WMB in Theorem 4.2 of [16], our bound is as tight as WMB, and hence the class of TRW / CED bounds attainable by WMB [16]. Most duality-based forms [e.g., 9, 10] are expressed in terms of joint entropies, 〈θ, b〉+ ∑ β cβH(bβ), rather than conditional entropies; while the two can be converted, the resulting counting numbers cβ will be differences of weights {wαi }, 5 which obfuscates its convexity, makes it harder to maintain the relative constraints on the counting numbers during optimization, and makes some counting numbers negative (rendering some methods inapplicable [8]). Finally, like most variational bounds in dual form, the RHS of (7) has a inner maximization and hence guaranteed to bound Φτ (θ) only at its optimum.\nIn contrast, our Eq. (5) is a primal bound (hence, a bound for any δ). It is similar to the primal form of TRW, except that (1) the individual regions are single cliques, rather than spanning trees of the graph, 6 and (2) the fraction weights wα associated with each region are vectors, rather than a single scalar. The representation’s efficiency can be seen with an example in Figure 1, which shows a 3×3 grid model and three relaxations that achieve the same bound. Assuming d states per variable and ignoring the equality constraints, our decomposition in Figure 1(c) uses 24d cost-shifting parameters (δ), and 24 weights. WMB (Figure 1(b)) is slightly more efficient, with only 8d parameters for δ and and 8 weights, but its lack of decomposition makes parallel and monotonic updates difficult. On the other hand, the equivalent primal TRW uses 16 spanning trees, shown in Figure 1(d), for 16 · 8 · d2 parameters, and 16 weights. The increased dimensionality of the optimization slows convergence, and updates are non-local, requiring full message-passing sweeps on the involved trees (although this cost can be amortized in some cases [21])."
    }, {
      "heading" : "5 Monotonically Tightening the Bound",
      "text" : "In this section, we propose a block coordinate descent algorithm (Algorithm 1) to minimize the upper bound L(δ,w) in (5) w.r.t. the shifting variables δ and weights w. Our algorithm has a monotonic convergence property, and allows efficient, distributable local computation due to the full decomposition of our bound. Our framework allows generic powered-sum inference, including max-, sum-, or mixed-inference as special cases by setting different weights."
    }, {
      "heading" : "5.1 Moment Matching and Entropy Matching",
      "text" : "We start with deriving the gradient of L(δ,w) w.r.t. δ and w. We show that the zero-gradient equation w.r.t. δ has a simple form of moment matching that enforces a consistency between the singleton beliefs with their related clique beliefs, and that of weights w enforces a consistency of marginal and conditional entropies.\nTheorem 5.1. (1) For L(δ,w) in (5), its zero-gradient w.r.t. δαi (xi) is ∂L\n∂δαi (xi) = µi(xi)− ∑ xα\\i µα(xα) = 0, (8)\n5See more details of this connection in Section F.3 of the supplement. 6While non-spanning subgraphs can be used in the primal TRW form, doing so leads to loose bounds; in\ncontrast, our decomposition’s terms consist of individual cliques.\nAlgorithm 1 Generalized Dual-decomposition (GDD) Input: weights {τi | i ∈ V }, elimination order o. Output: the optimal δ∗,w∗ giving tightest upper bound L(δ∗,w∗) for Φτ (θ) in (5). initialize δ = 0 and weights w = {wi, wαi }. repeat\nfor node i (in parallel with node j, (i, j) 6∈ E) do if τi = 0 then\nupdate δNi = {δαi |∀α ∈ Ni} with the closed-form update (11); else if τi 6= 0 then\nupdate δNi and wNi with gradient descent (8) and(12), combined with line search; end if\nend for until convergence δ∗ ← δ, w∗ ← w, and evaluate L(δ∗,w∗) by (5); Remark. GDD solves max-, sum- and mixed-inference by setting different values of weights {τi}.\nwhere µi(xi) ∝ exp [\n1 wi ∑ α∈Ni δ α i (xi) ] can be interpreted as a singleton belief on xi, and µα(xα)\ncan be viewed as clique belief on xα, defined with a chain rule (assuming xα = [x1, . . . , xc]), µα(xα) = ∏c i=1 µα(xi|xi+1:c); µα(xi|xi+1:c) = (Zi−1(xi:c)/Zi(xi+1:c))1/w α i , where Zi is the partial powered-sum up to x1:i on the clique, that is,\nZi(xi+1:c) = wαi∑ xi · · · wα1∑ x1 exp [ θα(xα)− ∑ i∈α δαi (xi) ] , Z0(xα) = exp [ θα(xα)− ∑ i∈α δαi (xi) ] ,\nwhere the summation order should be consistent with the global elimination order o = [x1, . . . , xn].\n(2) The gradients of L(δ,w) w.r.t. the weights {wi, wαi } are marginal and conditional entropies defined on the beliefs {µi, µα}, respectively,\n∂L ∂wi = H(xi;µi),\n∂L\n∂wαi = H(xi|xi+1:c;µα) = − ∑ xα µα(xα) logµα(xi|xi+1:c). (9)\nTherefore, the optimal weights should satisfy the following KKT condition wi ( H(xi;µi)− H̄i ) = 0, wαi ( H(xi|xi+1:c;µα)− H̄i ) = 0, ∀(i, α) (10)\nwhere H̄i = ( wiH(xi;µi) + ∑ α w α i H(xi|xi+1:c;µα) ) /τi is the average entropy on node i.\nThe proof details can be found in Section G of the supplement. The matching condition (8) enforces that µ = {µi, µα | ∀(i, α)} belong to the local consistency polytope L as defined in Theorem 4.2; similar moment matching results appear commonly in variational inference algorithms [e.g., 34]. [34] also derive a gradient of the weights, but it is based on the free energy form and is correct only after optimization; our form holds at any point, enabling efficient joint optimization of δ and w."
    }, {
      "heading" : "5.2 Block Coordinate Descent",
      "text" : "We derive a block coordinate descent method in Algorithm 1 to minimize our bound, in which we sweep through all the nodes i and update each block δNi = {δαi (xi) | ∀α ∈ Ni} and wNi = {wi, wαi | ∀α ∈ Ni} with the neighborhood parameters fixed. Our algorithm applies two update types, depending on whether the variables have zero weight: (1) For nodes with τi = 0 (corresponding to max nodes i ∈ B in marginal MAP), we derive a closed-form coordinate descent rule for the associated shifting variables δNi ; these nodes do not require to optimize wNi since it is fixed to be zero. (2) For nodes with τi 6= 0 (e.g., sum nodes i ∈ A in marginal MAP), we lack a closed form update for δNi and wNi , and optimize by local gradient descent combined with line search.\nThe lack of a closed form coordinate update for nodes τi 6= 0 is mainly because the order of power sums with different weights cannot be exchanged. However, the gradient descent inner loop is still efficient, because each gradient evaluation only involves the local variables in clique α.\nClosed-form Update. For any node i with τi = 0 (i.e., max nodes i ∈ B in marginal MAP), and its associated δNi = {δαi (xi) | ∀α ∈ Ni}, the following update gives a closed form solution for the zero (sub-)gradient equation in (8) (keeping the other {δαj |j 6= i,∀α ∈ Ni} fixed):\nδαi (xi)← |Ni| |Ni|+ 1 γαi (xi)− 1 |Ni|+ 1 ∑\nβ∈Ni\\α\nγβi (xi), (11)\nwhere |Ni| is the number of neighborhood cliques, and γαi (xi) = log ∑wα\\i xα\\i exp [ θα(xα) −∑\nj∈α\\i δ α j (xj) ] . Note that the update in (11) works regardless of the weights of nodes {τj | ∀j ∈ α, ∀α ∈ Ni} in the neighborhood cliques; when all the neighboring nodes also have zero weight (τj = 0 for ∀j ∈ α, ∀α ∈ Ni), it is analogous to the “star” update of dual decomposition for MAP [31]. The detailed derivation is shown in Proposition H.1 and H.2 in the supplement.\nThe update in (11) can be calculated with a cost of only O(|Ni| · d|α|), where d is the number of states of xi, and |α| is the clique size, by computing and saving all the shared {γαi (xi)} before updating δNi . Furthermore, the updates of δNi for different nodes i are independent if they are not directly connected by some clique α; this makes it easy to parallelize the coordinate descent process by partitioning the graph into independent sets, and parallelizing the updates within each set.\nLocal Gradient Descent. For nodes with τi 6= 0 (or i ∈ A in marginal MAP), there is no closedform solution for {δαi (xi)} and {wi, wαi } to minimize the upper bound. However, because of the fully decomposed form, the gradient w.r.t. δNi and wNi , (8)–(9), can be evaluated efficiently via local computation with O(|Ni| · d|α|), and again can be parallelized between nonadjacent nodes. To handle the normalization constraint (6) on wNi , we use an exponential gradient descent: let wi = exp(vi)/ [ exp(vi) + ∑ α exp(v α i ) ] and wαi = exp(v α i )/ [ exp(vi) + ∑ α exp(v α i ) ] ; taking the gradient w.r.t. vi and vαi and transforming back gives the following update wi ∝ wi exp [ − ηwi ( H(xi;µi)− H̄i )] , wαi ∝ wαi exp [ − ηwαi ( H(xi|xpaαi ;µα)− H̄i )] , (12) where η is the step size and paαi ={j∈α | j i}. In our implementation, we find that a few gradient steps (e.g., 5) with a backtracking line search using the Armijo rule works well in practice. Other more advanced optimization methods, such as L-BFGS and Newton’s method are also applicable."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section, we demonstrate our algorithm on a set of real-world graphical models from recent UAI inference challenges, including two diagnostic Bayesian networks with 203 and 359 variables and max domain sizes 7 and 6, respectively, and several MRFs for pedigree analysis with up to 1289 variables, max domain size of 7 and clique size 5.7 We construct marginal MAP problems on these models by randomly selecting half of the variables to be max nodes, and the rest as sum nodes.\nWe implement several algorithms that optimize the same primal marginal MAP bound, including our GDD (Algorithm 1), the WMB algorithm in [16] with ibound = 1, which uses the same cliques and a fixed point heuristic for optimization, and an off-the-shelf L-BFGS implementation that directly optimizes our decomposed bound. For comparison, we also computed several related primal bounds, including standard mini-bucket [2] and elimination reordering [27, 40], limited to the same computational limits (ibound = 1). We also tried MAS [20] but found its bounds extremely loose.8\nDecoding (finding a configuration x̂B) is more difficult in marginal MAP than in joint MAP. We use the same local decoding procedure that is standard in dual decomposition [31]. However, evaluating the objective Q(x̂B) involves a potentially difficult sum over xA, making it hard to score each decoding. For this reason, we evaluate the score of each decoding, but show the most recent decoding rather than the best (as is standard in MAP) to simulate behavior in practice.\nFigure 2 and Figure 3 compare the convergence of the different algorithms, where we define the iteration of each algorithm to correspond to a full sweep over the graph, with the same order of time complexity: one iteration for GDD is defined in Algorithm 1; for WMB is a full forward and backward message pass, as in Algorithm 2 of [16]; and for L-BFGS is a joint quasi-Newton step on all variables. The elimination order that we use is obtained by a weighted-min-fill heuristic [1] constrained to eliminate the sum nodes first.\nDiagnostic Bayesian Networks. Figure 2(a)-(b) shows that our GDD converges quickly and monotonically on both the networks, while WMB does not converge without proper damping; we\n7See http://graphmod.ics.uci.edu/uai08/Evaluation/Report/Benchmarks. 8The instances tested have many zero probabilities, which make finding lower bounds difficult; since MAS’\nbounds are symmetrized, this likely contributes to its upper bounds being loose.\nexperimented different damping ratios for WMB, and found that it is slower than GDD even with the best damping ratio found (e.g., in Figure 2(a), WMB works best with damping ratio 0.035 (WMB0.035), but is still significantly slower than GDD). Our GDD also gives better decoded marginal MAP solution xB (obtained by rounding the singleton beliefs). Both WMB and our GDD provide a much tighter bound than the non-iterative mini-bucket elimination (MBE) [2] or reordered elimination [27, 40] methods.\nGenetic Pedigree Instances. Figure 3 shows similar results on a set of pedigree instances. Again, GDD outperforms WMB even with the best possible damping, and out-performs the non-iterative bounds after only one iteration (pass through the graph)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we propose a new class of decomposition bounds for general powered-sum inference, which is capable of representing a large class of primal variational bounds but is much more computationally efficient. Unlike previous primal sum bounds, our bound decomposes into computations on small, local cliques, increasing efficiency and enabling parallel and monotonic optimization. We derive a block coordinate descent algorithm for optimizing our bound over both the cost-shifting parameters (reparameterization) and weights (fractional counting numbers), which generalizes dual decomposition and enjoy similar monotonic convergence property. Taking the advantage of its monotonic convergence, our new algorithm can be widely applied as a building block for improved heuristic construction in search, or more efficient learning algorithms."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is sponsored in part by NSF grants IIS-1065618 and IIS-1254071. Alexander Ihler is also funded in part by the United States Air Force under Contract No. FA8750-14-C-0011 under the DARPA PPAML program."
    }, {
      "heading" : "A Experiment on Ising grid",
      "text" : "Our GDD directly optimizes a primal bound, and is thus guaranteed to be an upper bound of the partition function even before the algorithm converges, enabling a desirable “any-time” property. In contrast, typical implementations of tree reweighted (TRW) belief propagation optimize the dual free energy function [34], and are not guaranteed to be a bound before convergence. We illustrate this point using an experiment on a toy 5×5 Ising grid, with parameters generated by normal ditribution N(0, 2) and half nodes selected as max-nodes for marginal MAP. Figure 4(a)-(b) shows the TRW free energy objective and GDD, WMB upper bounds across iterations; we can see that TRW does violate the upper bound property before convergence, while GDD and WMB always give valid upper bounds."
    }, {
      "heading" : "B More Results on Diagnostic Bayesian Networks",
      "text" : "In addtion to the marginal MAP results on BN-1 and BN-2 in main text, we vary the percentage of max-nodes when generating the marginal MAP problems; the reported results in Figure 5(a)-(b) are the best bound obtained by the different algorithms with the first 20 iterations. In all cases, GDD’s results are as good or better than WMB. WMB-0.5 (WMB with damping ratio 0.5) appears to work well on sum-only and max-only (MAP) problems, i.e., when the percentage of max-nodes equals 0% and 100% respectively, but performs very poorly on intermediate settings. The far more heavily damped WMB-0.04 or WMB-0.02 work better on average, but have much slower convergence."
    }, {
      "heading" : "C More Results on Pedigree Linkage Analysis",
      "text" : "We test our algorithm on additional 6 models of pedigree linkage analysis from the UAI08 inference challenge. We construct marginal MAP problems by randomly selected 50% of nodes to be max-nodes, and report all the results in Figure 6. We find that our algorithm consistently outperforms WMB with the best possible damping ratio."
    }, {
      "heading" : "D Extensions to Junction Graph",
      "text" : "Our bound (5) in the main text uses a standard “factor graph” representation in which the cost-shifts {δαi } are defined for each variable-factor pair (i, α), and are functions of single variables xi. We can extend our bound to use more general shifting parameters using a junction graph representation; this allows us to exploit higher order clique structures, leading to better performance.\nLet (C,S) be a junction graph of p(x; θ) where C = {c | c ⊂ V } is the set of clusters, and S = {s = ck ∩ cl | ck, cl ∈ C} is the set of separators. Assume p(x; θ) can be reparameterized into the form,\np(x; θ) = exp [∑ c∈C θc(xc)− Φ(θ) ] , (13)\nand the weighted log partition function is rewritten as Φτ (θ) = log ∑τ x exp [∑ c∈C θc(xc) ] . Similar to the derivation of bound (5) in the main text, we can apply Theorem 4.1, but with a set of more general cost-shifting variables δcs , defined on each adjacent separator-cluster pair (s, c); this gives the more general upper bound,\nΦτ (θ) ≤ ∑ s∈S log ws∑ xs exp [∑ c⊇s δcs(xs) ] + ∑ c∈C log wc∑ xc exp [ θc(xc)− ∑ s⊆c δcs(xs) ] , (14)\nwhere we introduce the set of non-negative weights ws = {wsi | i ∈ s} on each separator and wc = {wci | i ∈ c} on each cluster, which should satisfy ∑ s∈Nsei wsi + ∑ c∈Nci wci = τi, where N se i = {s | i ∈ s} are all the separators that include node i, and Nci = {c | i ∈ c} are all the clusters that include node i. Obviously, our earlier bound (5) in the main text can be viewed as a special case of (14) with a special junction graph whose separators consist of only single variables, that is, S = V .\nA block coordinate descent algorithm similar to Algorithm 1 can be derived to optimize the junction graph bound. In this case, we sweep through all the separators s and perform block coordinate update on all {δcs|∀c ⊇ s} at each iteration. Similarly to Algorithm 1, we can derive a close form update for separators with all-zero weights (that is, τi = 0, ∀i ∈ s, corresponding to s ⊆ B in marginal MAP), and perform local gradient descent otherwise."
    }, {
      "heading" : "E Proof of Thereom 4.1",
      "text" : "Proof. Note the Hölder’s inequality is[∑\nx ∏ j fj(x) 1/ξ0 ]ξ0 ≤∏ j [∑ x fj(x) 1/ξj ]ξj , where {fj(x)} are arbitrary positive functions, and {ξj} are non-negative numbers that satisfy ∑ j ξj = ξ0. Note we extend the inequality by defining power sum with ξj = 0 to equal the max operator. Our result follows by applying Hölder’s inequality on each xi sequentially along the elimination order [x1, x2, · · · , xn]."
    }, {
      "heading" : "F Dual Representations",
      "text" : "F.1 Background\nThe log-partition function Φ(θ) has the following variational (dual) form Φ(θ) = log ∑ x exp(θ(x)) = max b∈M(G) { 〈θ, b〉+H(x; b) } where M(G) is the marginal polytope [35]. Then, for any scalar ε > 0 (including ε→ 0+), we have\nΦε(θ) = ε log ∑ x exp( θ(x) ε ) = εmax b∈M { 〈θ ε , b〉+H(x; b) } = max b∈M { 〈θ, b〉+ εH(x; b) } .\nAs stated in [15, 16], we can further generalize the variational form of above scalar-weighted log partition function to the vector-weighted log partition function (3) in the main text,\nΦτ (θ) = log τn∑ xn . . . τ1∑ x1 exp(θ(x)) = max b∈M(G) { 〈θ, b〉+ ∑ i τiH(xi|xi+1:n; b) } , (15)\nwhere H(xi|xi+1:n; b) is the conditional entropy on b(x), and is defined as H(xi|xi+1:n; b) = − ∑ x b(x) log(b(xi|xi+1:n)). See more details of its derivation in Theorem 4.1 within [15].\nA notable special case of (15) is the dual form of marginal MAP\nΦAB(θ) = max xB log ∑ xA exp ( θ(x) ) = max b∈M(G) { 〈θ, b〉+H(xA|xB ; b) } , (16)\nby setting weights τA = 1 and τB = 0.\nF.2 Proof of Thereom 4.2\nWe now prove the following dual representation of our bound,\nmin δ L(δ,w) = max b∈L(G)\n{ 〈θ, b〉+ ∑ i∈V wiH(xi; bi) + ∑ α∈F ∑ i∈α wαi H(xi|xpaαi ; bα) } , (17)\nwhere L(G) = {b | bi(xi) = ∑ xα\\i bα(xα), ∑ xi bi(xi) = 1} is the local consistency polytope, and paαi = {j ∈ α|j i}. Thereom 4.2 follows directly from (17). Proof. In our primal bound L(δ,w) (5) in the main text, we let θ̃i(xi) = θi(xi) + ∑ α∈Ni δ α i (xi) (we add\ndummy singleton θi(xi) ≡ 0), and θ̃α(xα) = θα(xα)− ∑ i∈α δ α i (xi), then the bound can be rewritten as,\nL(θ̃,w) = ∑ i∈V log wi∑ xi exp [ θ̃i(xi) ] + ∑ α∈F log wα∑ xα exp [ θ̃α(xα) ] .\nNote, for any assignment x, we have ∑ i θ̃i(xi) + ∑ α θ̃α(xα) = ∑ α θα(xα).\nBy applying the dual form of the powered sum (15) on each node and clique respectively, we have L(θ̃,w) = ∑ i∈V max bi∈M(Gi) { 〈θ̃i, bi〉+ wiH(xi; bi) } + ∑ α∈F max bα∈M(Gα) { 〈θ̃α, bα〉+ ∑ i∈α wαi H(xi|xpaαi ; bα) } ,\nwhere paαi is the set of variables in α that are summed out later than i, M(Gi) and M(Gα) are the marginal polytopes on singleton node i and clique α respectively, which enforce {bi, bα} to be properly normalized. The above equation can be more compactly rewritten as,\nL(θ̃,w) = max b∈M̃\n{ 〈θ̃, b〉+ ∑ i∈V wiH(xi; bi) + ∑ α∈F ∑ i∈α wαi H(xi|xpaαi ; bα) } ,\nwhere M̃ = {M(Gi),M(Gα) | ∀ i ∈ V, α ∈ F}, and the elements {bi, bα} of b are independently optimized.\nThen, by tightening reparameterization θ̃ = {θ̃i, θ̃α}, we have\nmin θ̃ L(θ̃,w) = max b∈M̃ min θ̃\n{ 〈θ̃, b〉+ ∑ i∈V wiH(xi; bi) + ∑ α∈F ∑ i∈α wαi H(xi|xpaαi ; bα) }\nwhere the order of min and max are commuted according to the strong duality (it’s convex with θ̃, and concave with b). The inner minimization minθ̃〈θ̃, b〉 is a linear program, and it turns out can be solved analytically. To see this, given the relationship between θ̃ and δ, we rewrite the linear program as\nmin θ̃ 〈θ̃, b〉 = min δ\n{ 〈θ, b〉+ ∑ i∈V ∑ xi ∑ α∈Ni δαi (xi)bi(xi)− ∑ α∈F ∑ xα ∑ i∈α δαi (xi)bα(xα) } ,\n= min δ\n{ 〈θ, b〉+ ∑ (i,α) ∑ xi δαi (xi) ( bi(xi)− ∑ xα\\i bα(xα) )} .\nThen, it is easy to observe that the linear program is either equal to 〈θ, b〉 only if b satisfy the marginalization constraint ∑ xα\\i\nbα(xα) = bi(xi) for ∀(i, α), or it will become negative infinity. Considering the outer maximization, we have\nmin θ̃ L(θ̃,w) = max b∈L(G)\n{ 〈θ, b〉+ ∑ i∈V wiH(xi; bi) + ∑ α∈F ∑ i∈α wαi H(xi|xpaαi ; bα) } ,\nwhere L(G) is the local consistency polytope that is obtained by enforcing both M̃ and the marginalization constraint.\nF.3 Connection with Existing Free Energy Forms\nMost variational forms are expresssed in the following linear combination of local entropies [39, 10], 〈θ, b〉+ ∑ β cβH(bβ), (18)\nwhere β refers the region, cβ refers the general counting number, bβ(xβ) is the local belief.\nWe can rewrite our dual representations (17) as, 〈θ, b〉+ ∑ i∈V wiH(xi; bi) + ∑ α∈F ∑ i∈α wαi ( H(xi, xpaαi ; bα)−H(xpaαi ; bα) ) ,\nwhere paαi is the set of variables in α that rank later than i. Without loss of generality, assuming xα = [x1, · · · , xi, xj , · · ·xc], i.e. xi and xj are adjacent in the order, we can get\n〈θ, b〉+ ∑ i∈V wiH(xi; bi) + ∑ α∈F { wα1H(xα; bα) + ∑ [i,j]vα (wαj − wαi )H(xpaαi ; bpaαi ) }\n(19)\nwhere belief bpaαi is defined by bpaαi (xpaαi ) = ∑ xα\\paα\ni\nbα(xα).\nOne can view (19) in terms of (18), by selecting the region β ∈ {i ∈ V } ∪ {α ∈ F} ∪ {paαi | ∀(i, α)}; some counting numbers cβ will be the differences of weights wαj − wαi .\nF.4 Matching Our Bound to WMB\nAfter the weights are optimized, our GDD bound matches to WMB bound with optimum weights. A simple weight initialization method matches our bound to WMB with uniform weights on each mini-bucket, which often gives satisfactory result; a similar procedure can be used to match the bound with more general weights as in Section D. We first set wi = 0 for all nodes i. We then visit the nodes xi along the elimination order o = [x1, x2, · · · , xn], and divide xi’s neighborhood cliquesNi = {α|α 3 i} into two groups: (1) the children cliques in which all xα\\i have already been eliminated, that is, Nchi = {α | ∀j ∈ α\\i, j ≺ i in o}; (2) the other, parent cliques Npai = {α | ∃j ∈ α\\i, j i in o}. We set w α i = 0 for all the children cliques (α ∈ Nchi ), and uniformly split the weights, that is, wαi = τi/|Npai |, across the parent cliques."
    }, {
      "heading" : "G Proof of Therom 5.1",
      "text" : "Proof. For each δαi (xi), the involved terms in L(δ,w) are L α i (δ) = Φwi(δ) + Φwα(δ), where\nΦwi(δ) = log wi∑ xi exp [ ∑ α∈Ni δαi (xi) ] , Φwα(δ) = log wα∑ xα exp [ θα(xα)− ∑ i∈α δαi (xi) ] .\nOur result follows by showing that\n∂Φwi(δ) ∂δαi (xi) = µi(xi) and ∂Φwi(δ) ∂wi = H(xi;µi),\n∂Φwα(δ)\n∂δαi (xi) = − ∑ xα\\i µα(xα) and ∂Φwα(δ) ∂wαi = H(xi|xi+1:c;µα).\nThe gradient of Φwi(δ) is straightforward to calculate,\n∂Φwi ∂δαi (xi) = ∂ ∂δαi (xi)\n( wi log ∑ xi exp [∑ α∈Ni δ α i (xi) wi ]) = exp [∑ α∈Ni δαi (xi) wi ] Zwi = µi(xi),\nwhere Zwi = ∑ xi exp [∑ α∈Ni δαi (xi) wi ] , and\n∂Φwi ∂wi = logZwi + wi · 1 Zwi · ∑ xi { exp [∑ α∈Ni δ α i (xi) wi ] · ∑ α∈Ni δ α i (xi) −w2i } = logZwi −\n∑ xi { µi(xi) · ∑ α∈Ni δ α i (xi) wi } = −\n∑ xi { µi(xi) · [∑ α∈Ni δ α i (xi) wi − logZwi ]} = H(xi;µi).\nThe gradient of Φwα(δ) is more involved; see Proposition I.1 for a detailed derivation.\nGiven the gradients, the moment matching condition (8) in Therom 5.1 obviously holds. We now prove the entropy matching condition in (10). The constraint optimization is\nmin w L(w), s.t. wi ≥ 0, wαi ≥ 0, wi + ∑ α wαi = τi.\nNote, when τi = 0, the optimization is trival, so we simply assume τi > 0. We frame the Lagrangian as\nK(w,λ, g) def == L(w) + ∑ i λi ( wi + + ∑ α wαi − τi ) + ∑ i giwi + ∑ (i,α) gαi w α i .\nNote g ≤ 0 (dual feasibility), otherwise maxg,λK(w,λ, g) will approach infinity. The KKT conditions are\nstationarity: ∂K\n∂wi = H(xi;µi) + λi + gi = 0, (20)\n∂K ∂wαi = H(xi|xi+1:c;µα) + λi + gαi = 0, (21)\ncomplementary slackness: giwi = 0, gαi w α i = 0. (22)\nWe multiply wi and wαi to (20) and (21) respectively, then we can eliminate the KKT multipliers gi and g α i by applying the complementary slackness (22),\nwiH(xi;µi) + wiλi = 0, (23) wαi H(xi|xi+1:c;µα) + wαi λi = 0. (24)\nBy summing (24) over all α ∈ Ni and adding (23), we can solve the multiplier λi as\nλi = − wiH(xi;µi) +\n∑ α w α i H(xi|xi+1:c;µα)\nτi = −H̄i.\nWe plug it into (23) and (24), and obtain the entropy matching condition (10) in the Therom 5.1."
    }, {
      "heading" : "H Derivations of Closed-form Update",
      "text" : "We first derive the closed-form update rule for δαi (xi) in Proposition H.1. We derive the closed-form update rule for the block δNi = {δαi (xi) | ∀α ∈ Ni} in Proposition H.2.\nProposition H.1. Given max node i in marginal MAP (i.e., τi = 0 ) and one clique α 3 i (i.e. i ∈ α), keeping all δ fixed except δαi (xi), there is a closed-form update rule,\nδαi (xi)← 1\n2 log wα\\i∑ xα\\i exp [ θα(xα)− ∑ j∈α\\i δαj (xj) ] − 1 2 ∑ β∈Ni\\α δβi (xi), (25)\nwhere xα\\i = {xj : j ∈ α, j 6= i}, wα\\i = {wαj : j ∈ α, j 6= i}, and Ni = {α|α 3 i} is the set of all clique factors in the neighborhood of node i. Futhermore, this update will monotonically decrease the upper bound.\nProof. The terms within the bound L(δ,w) that depend on δαi (xi) are,\nmax xi [ ∑ α∈Ni δαi (xi) ] + max xi log wα\\i∑ xα\\i exp [ θα(xα)− ∑ i∈α δαi (xi) ] . (26)\nThe sub-gradient of (26) w.r.t. δαi (xi) equal to zero if and only if,\nx∗i = argmax xi [ ∑ α∈Ni δαi (xi) ] = argmax xi log wα\\i∑ xα\\i exp [ θα(xα)− ∑ i∈α δαi (xi) ] ,\nwhich is “argmax” matching. One sufficient condition of this matching is,\n∑ α∈Ni δαi (xi) = log wα\\i∑ xα\\i exp [ θα(xα)− ∑ i∈α δαi (xi) ]\nwhich impllies matching of “pseudo marginals”. Then, one can pull δαi (xi) outside from the operator log ∑wα\\i xα\\i exp, and get the closed-form equation\nδαi (xi) = 1\n2 log wα\\i∑ xα\\i exp [ θα(xα)− ∑ j∈α\\i δαj (xj) ] − 1 2 ∑ β∈Ni\\α δβi (xi).\nTo prove monotonicity, we substitute above update equation of δαi (xi) into (26); then we get,\nmax xi { ∑ β∈Ni\\α δβi (xi) + log wα\\i∑ xα\\i exp [ θα(xα)− ∑ j∈α\\i δαj (xj) ]} . (27)\nClearly, (27) ≤ (26) by using the fact that maxx[f(x) + g(x)] ≤ maxx f(x) + maxx g(x).\nProposition H.2. Given node i ∈ B (i.e., a max node) and all neighborhood cliques Ni = {α|α 3 i}, we can jointly optimize δNi = {δαi (xi) | ∀α ∈ Ni} in closed-form by keeping the other {δαj | j 6= i,∀α ∈ Ni} fixed. The update rule is,\nδαi (xi)← |Ni| |Ni|+ 1 γαi (xi)− 1 |Ni|+ 1 ∑\nβ∈Ni\\α\nγβi (xi), (28)\nwhere |Ni| is the number of neighborhood cliques, and {γαi (xi) | ∀α ∈ Ni} are defined by\nγαi (xi) = log wα\\i∑ xα\\i exp [ θα(xα)− ∑ j∈α\\i δαj (xj) ] . (29)\nFuthermore, this upate will monotonically decrease the upper bound.\nProof. For ∀α ∈ Ni, we have closed-form solutions for δαi (xi) as Proposition H.1. We rewrite it as,\n∀α ∈ Ni, 2δαi (xi) + ∑\nβ∈Ni\\α\nδβi (xi) = log wα\\i∑ xα\\i exp [ θα(xα)− ∑ j∈α\\i δαj (xj) ] . (30)\nNote, for ∀α, β ∈ Ni, there is a linear relationship between δαi (xi) and δβi (xi).\nWe denote column vector γi(xi) filled α-th element with\nγαi (xi) = log wα\\i∑ xα\\i exp [ θα(xα)− ∑ j∈α\\i δαj (xj) ] .\nWe also frame all {δαi (xi) | α ∈ Ni} into a column vector δNi(xi), and denote |Ni| × |Ni| matrix A\nA =  2 1 · · · 1 1 2 · · · 1 ... ... . . .\n... 1 1 · · · 2\n , and note A−1 =  |Ni| |Ni|+1 − 1|Ni|+1 · · · − 1 |Ni|+1 − 1|Ni|+1 |Ni| |Ni|+1 · · · − 1|Ni|+1 ... ... . . .\n... − 1|Ni|+1 − 1 |Ni|+1 · · · |Ni||Ni|+1 .  It is easy to verify AδNi(xi) = γi(xi). from (30). Since A is invertable, one can solve\nδNi(xi) = A −1γi(xi).\nThen, one can read out the closed-form update rule (28). The monotonicity holds directly by noticing that the update rule (28) are solutions which jointly satisfy equation (25)."
    }, {
      "heading" : "I Derivations of Gradient",
      "text" : "Proposition I.1. Given a weight vector wα = [wα1 , · · · , wαi , · · · , wαc ] associated with variables xα = {x1, · · · , xi, · · · , xc} on clique α, where c = |α| the power sum over clique α is,\nΦwα(δ) = log wα∑ xα exp [ θα(xα)− ∑ i∈α δαi (xi) ] = log wαc∑ xc · · · wαi∑ xi · · · wα1∑ x1 exp [ θα(xα)− ∑ i∈α δαi (xi) ] .\nWe recursively denote Zi as the partial power sum up to x1:i,\nZ0(xα) = exp [ θα(xα)− ∑ i∈α δαi (xi) ] and Zi(xi+1:c) = wαi∑ xi Zi−1(xi:c), (31)\nthus logZc = Φwα . We also denote the “pseudo marginal” (or, belief) on xα,\nµα(xα) = c∏ i=1 µα(xi|xi+1:c); µα(xi|xi+1:c) = (Zi−1(xi:c) Zi(xi+1:c) )1/wαi ,\nand it is easy to verify that µα(xi|xi+1:c) and µα(xα) are normalized.\nThen, the derivative of Φwα w.r.t. δαi (xi) can be written by beliefs,\n∂Φwα\n∂δαi (xi) = −µα(xi) = − ∑ xα\\i µα(xα) = − ∑ xc · · · ∑ xi+1 c∏ j=i µα(xj |xj+1:c) (32)\nIn addition, the derivative of Φwα w.r.t. wαi is the conditional entropy, ∂Φwα\n∂wαi = H(xi|xi+1:c;µα(xα)) = − ∑ xα µα(xα) logµα(xi|xi+1:c) (33)\nProof. Denote the reparameterization on clique α as θ̃α(xα) = θα(xα)− ∑ i∈α δ α i (xi). From the recursive definition of Zi(xi+1:c) (31), we have the following recursive rule for gradient, ∂ logZi(xi+1:c)\n∂θ̃α(xα) =\n∂\n∂θ̃α(xα)\n( wαi log ∑ xi [ Zi−1(xi:c) ]1/wαi )\n= wαi · 1 wαi · Zi−1(xi:c) 1 wα i∑\nxi\n[ Zi−1(xi:c) ] 1 wαc · Zi−1(xi:c)−1 · ∂Zi−1(xi:c) ∂θ̃α(xα)\n= µα(xi|xi+1:c) · ∂ logZi−1(xi:c)\n∂θ̃α(xα) . (34)\nIt should be noted, implicitly, xi+1:c within θ̃α(xα) should take the same value as xi+1:c in logZi(xi+1:c), otherwise, the derivative will equal 0.\nAs a result, we can calculate the derivatives of Φwα(θ̃α) w.r.t. θ̃α(xα) recursively as,\n∂Φwα\n∂θ̃α(xα) =\n∂ logZc ∂θ̃α(xα) = µα(xc) · ∂ logZc−1(xc) ∂θ̃α(xα) = · · · = c∏ i=1 µα(xi|xi+1:c) = µα(xα). (35)\nBy the chain rule,\n∂Φwα ∂δαi (xi) = ∑ xα\\i\n∂Φwα ∂θ̃α(xi, xα\\i) · ∂θ̃α(xi, xα\\i) ∂δαi (xi) = − ∑ xα\\i µα(xα),\nthen (32) has been proved.\nApplying the variational form of powered-sum (15) to Φwα , we have\nΦwα(θ̃α) = max bα∈Mα(G)\n{ 〈θ̃α, bα〉+ ∑ i wαi H(xi|xi+1:n; bα) } .\nAccording to Danskin’s theorem, the derivative ∂Φwα ∂θ̃α(xα) = b∗α(xα), which is the optimum of RHS. Combined with (35), we have b∗α = µα immediately, and the derivative w.r.t. wαi is,\n∂Φwα\n∂wαi = H(xi|xi+1:c;µα(xα)),\nthen (33) has been proved."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Marginal MAP inference involves making MAP predictions in systems defined<lb>with latent variables or missing information. It is significantly more difficult than<lb>pure marginalization and MAP tasks, for which a large class of efficient and con-<lb>vergent variational algorithms, such as dual decomposition, exist. In this work, we<lb>generalize dual decomposition to a generic power sum inference task, which in-<lb>cludes marginal MAP, along with pure marginalization and MAP, as special cases.<lb>Our method is based on a block coordinate descent algorithm on a new convex<lb>decomposition bound, that is guaranteed to converge monotonically, and can be<lb>parallelized efficiently. We demonstrate our approach on marginal MAP queries<lb>defined on real-world problems from the UAI approximate inference challenge,<lb>showing that our framework is faster and more reliable than previous methods.",
    "creator" : "LaTeX with hyperref package"
  }
}