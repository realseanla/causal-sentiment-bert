{
  "name" : "1501.05614.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Intégration d’une mesure d’indépendance pour la fusion d’informations",
    "authors" : [ "Mouloud Kharoune", "Arnaud Martin" ],
    "emails" : [ "Mouloud.Kharoune@univ-rennes1.fr,", "Arnaud.Martin@univ-rennes1.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Intégration d’une mesure d’indépendance pour la fusion d’informations\nMouloud Kharoune∗, Arnaud Martin∗\n∗UMR 6074 IRISA, Université de Rennes1 / IUT de Lannion, Rue Edouard Branly BP 3021, 22302 Lannion cedex\nMouloud.Kharoune@univ-rennes1.fr, Arnaud.Martin@univ-rennes1.fr\nRésumé. La fusion d’informations fait intervenir plusieurs sources d’informations afin d’améliorer la décision en terme de certitude et de précision. Quelle que soit l’approche retenue pour réaliser la fusion d’informations, l’hypothèse d’indépendance est généralement une hypothèse forte et incontournable. Nous proposons dans cet article une approche permettant d’intégrer une mesure d’indépendance avant de réaliser la combinaison des informations dans le cadre de la théorie des fonctions de croyance."
    }, {
      "heading" : "1 Introduction",
      "text" : "Tel que repris par Martin (2005), la fusion d’informations consiste à combiner des informations issues de plusieurs sources afin d’aider à la prise de décision. Les approches de fusion d’informations cherchent donc à tenir compte des redondances des informations issues des différentes sources. Les approches de fusion n’ont bien sûr d’intérêt que si les sources sont imparfaites et fournissent des informations peu sûres et précises qui se complètent. Ainsi il faut donc chercher à modéliser aux mieux les imperfections des sources et des données. Pour ce faire, différentes théories de l’incertain ont été sollicitées. Parmi elles, citons la théorie des probabilités, des sous-ensembles flous et des possibilités ou encore la théorie des fonctions de croyance. Quel que soit le cadre théorique retenu, lors de l’étape de combinaison de l’information, l’hypothèse d’indépendance des sources est généralement faite. Cette hypothèse est plus ou moins forte. Par exemple, l’indépendance statistique est généralement retenue pour appliquer plus aisément la combinaison bayésienne du cadre probabiliste. En effet, les estimations peuvent s’avérer très vite compliquées sans cette hypothèse. Dans le cas de la théorie des fonctions de croyance, il est question d’indépendance cognitive définie par Shafer (1976). Elle correspond à une absence de communication entre les sources sans que celles-ci soient pour autant indépendantes statistiquement. Malheureusement cette hypothèse d’indépendance est rarement vérifiée ou justifiée.\nDans ce travail nous nous intéressons plus particulièrement à la théorie des fonctions de croyance car elle offre un outil riche de modélisation et de gestion de l’information. En particulier, Chebbah et al. (2012, 2013) ont récemment proposé des approches pour mesurer l’indépendance des sources en distinguant de plus la dépendance positive et négative.\nNous poursuivons ainsi cet article en présentant les principes de base de la théorie des fonctions de croyance et en particulier la notion d’indépendance et d’affaiblissement. Nous ex-\nar X\niv :1\n50 1.\n05 61\n4v 1\n[ cs\n.A I]\n2 2\nJa n\n20 15\nposons ensuite l’approche proposée permettant de tenir compte d’une mesure d’indépendance avant la combinaison des fonctions de croyance. Nous illustrons ce principe à partir d’exemples générés."
    }, {
      "heading" : "2 Théorie des fonctions de croyance",
      "text" : "La théorie des fonctions de croyance issue des travaux de Dempster (1967), repris par Shafer (1976) est depuis quelques années employée dans des applications de fusion d’informations. Nous présentons ci-dessous les principes de cette théorie."
    }, {
      "heading" : "2.1 Principes de base",
      "text" : "Considérons le cadre de discernement Ω = {ω1, ω2, . . . , ωn} correspondant à l’ensemble de toutes les hypothèses possibles de décision d’un problème donné. Les éléments ωi représentent ainsi toutes les hypothèses exclusives et exhaustives.\nL’ensemble 2Ω = {A/A ⊆ Ω} = {∅, ω1, ω2, . . . , ωn, ω1 ∪ ω2, . . . ,Ω}, est composé de toutes les disjonctions de Ω. L’espace puissance 2Ω comporte 2|Ω| = 2n éléments.\nUne fonction de masse est une fonction de 2Ω vers l’intervalle [0, 1] qui affecte à chaque sous-ensemble de 2Ω une valeur de l’intervalle [0, 1] représentant sa masse de croyance élémentaire. Elle s’écrit :\nmΩ : 2Ω 7→ [0, 1] (1)\ntelle que : ∑ A⊆Ω mΩ(A) = 1 (2)\nUn sous-ensemble de 2Ω de masse de croyance non-nulle est un élément focal. La masse affectée à un élément focal A représente le degré de croyance élémentaire de la source à ce que la solution du problème soit A. Une fonction de masse permet ainsi de représenter des connaissances incertaines et imprécises d’une source d’informations. En général, nous manipulons des fonctions de masse non dogmatiques (i.e. dont l’ignorance Ω est élément focal), d’une part car elles permettent de systématiquement modéliser la part d’ignorance intrinsèque à toute source, mais également car toute fonction de masse non dogmatique est décomposable en fonctions de masse à support simple (i.e. qui ne comporte que deux éléments focaux dont Ω). Les fonctions de masse à support simple sont notées Aw telles que m(A) = 1 − w ∀A 6= Ω et m(Ω) = w. Ainsi une fonction de masse non dogmatique peut s’écrire :\nmΩ = ∩©A⊂ΩA w(A) (3)\noù ∩© est données par l’équation (5) ci-dessous. La crédibilité bel et la plausibilité pl sont des fonctions duales définies à partir de la fonctions de masse et représentent respectivement une fonction de croyance minimale et maximale. Ainsi la fonction de plausibilité est donnée par :\npl(X) = ∑\nY⊂Ω,Y ∩X 6=∅\nm(Y ) = bel(Ω)− bel(Xc) = 1−m(∅)− bel(Xc), (4)\noù Xc est le complémentaire de X . Une fois les fonctions de masse mΩj déterminées pour chaque source d’informations Sj , plusieurs opérateurs de combinaison sont envisageables en fonction des hypothèses initiales. Les opérateurs de type conjonctif peuvent être employés lorsque les sources sont fiables et indépendantes cognitivement. La combinaison conjonctive s’écrit pour deux fonctions de masse mΩ1 et m Ω 2 et pour tout X ∈ 2Ω par :\nmΩConj(X) = m1 ∩©m2 = ∑\nY1∩Y2=X mΩ1 (Y1)m Ω 2 (Y2). (5)\nNotons que l’élément neutre pour cette règle est la masse : mΩΩ(X) = 1 si X = Ω et 0 sinon. Lorsque cette hypothèse de fiabilité est trop forte et que l’on ne peut supposer que seule une des sources est fiable, la combinaison disjonctive peut alors être employée toujours sous l’hypothèse d’indépendance cognitive :\nmΩDis(X) = ∑\nY1∪Y2=X mΩ1 (Y1)m Ω 2 (Y2). (6)\nNotons que l’élément neutre pour cette règle est la masse : mΩ∅ (X) = 1 si X = ∅ et 0 sinon. La plupart des règles de combinaison issues des règles conjonctives et disjonctives, en particulier pour répartir le conflit, supposent que les sources sont indépendantes cognitivement. Martin (2010) en rappelle quelques unes.\nDenœux (2008) propose une famille de règles qui ne nécessitent pas l’hypothèse d’indépendance cognitive. Ainsi selon le comportement conjonctif ou disjonctif deux règles principales sont définies, la règle prudente et hardie. La règle prudente s’écrit pour les fonctions de masse non dogmatiques :\nmΩ1 ∧©m Ω 2 = ∩©A⊂ΩA w1(A)∧w2(A) (7)\noù ∧ est le maximum. La règle hardie s’écrit de même en considérant le minimum au lieu du maximum. Si ces règles sont efficaces lorsque les sources sont dépendantes, cette notion de dépendance ou d’indépendance n’est par clairement définie par Denœux (2008).\nLorsqu’une connaissance supplémentaire garantie un sous-ensembleA ⊂ Ω, nous pouvons définir une fonction de masse conditionnelle par :\nmΩ[A](X) = (mΩ ∩©mΩA)(X) (8)\noù mΩA(A) = 1 est la fonction de masse garantissant la réalisation de A."
    }, {
      "heading" : "2.2 Notion d’indépendance",
      "text" : "L’indépendance statistique est définie pour deux variables A et B par P (A|B) = P (A) ou de façon équivalente par P (A∩B) = P (A)P (B). Cette indépendance est étendue par Shafer (1976) dans le cadre de la théorie des fonctions de croyance et est donnée par : pl(A ∩ B) = pl(A)pl(B). Ben Yaghlane et al. (2002a,b) définissent une indépendance doxatique entre des variables définies sur des cadres de discernement différents éventuellement.\nCes définitions de l’indépendance ne correspondent pas à la notion d’indépendance cognitive entre les sources d’informations. Cette dernière se révèle très difficile à mesurer. Chebbah\net al. (2012, 2013) proposent une définition d’une mesure d’indépendance entre deux sources d’informations étendues à une mesure de dépendance positive et négative. La mesure d’indépendance entre deux sources est définie comme une sorte de corrélation entre deux sources issues d’un clustering (classification non-supervisée) sur les fonctions de masse de chacune des sources en associant ensuite les clusters. Si |Ω| = n, le clustering des fonctions de masse issues de la source S1 fourni n clusters, de même pour S2. Les clusters des deux sources (Clk1 , Clk2 ) sont associés de façon non symétrique en maximisant :\nαik1,k2 = |Clk1 ∩ Clk2 | |Clki | , i = 1, 2 (9)\nIl est ensuite possible de définir une fonction de masse sur ΩI = {I, Ī} représentant les deux possibilités : indépendant et dépendant (Ī) de la source S1 par rapport à la source S2 : mΩIk1k2(I) = β (1− α 1 k1k2 ) mΩIk1k2(Ī) = β α 1 k1k2\nmΩIk1k2(I ∪ Ī) = 1− β (10)\noù β est un facteur d’affaiblissement permettant de tenir compte du nombre d’observations dans chaque cluster. Ainsi la croyance élementaire que la source S1 est indépendante de S2 est donnée par la masse :\nmΩI (X) = 1\nn\n( n∑\nk1=1\nmΩIk1k2\n) (X) (11)\noù k2 est le cluster de la source S2 associé au cluster k1 de la source S1. La moyenne est ici employée du fait de la dépendance des fonctions de masse.\nChebbah et al. (2012, 2013) proposent un prolongement pour différencier la dépendance positive (la source S1 suit les avis de la source S2) et la dépendance négative (la source S1 dit le contraire des avis de la source S2). Ainsi, une fonction de masse conditionnelle est construite sur le cadre de discernement ΩP = {P, P̄} : mΩPk1k2 [Ī](P ) = 1−Dist(Clk1 , Clk2) mΩPk1k2 [Ī](P̄ ) = Dist(Clk1 , Clk2)\nmΩPk1k2 [Ī](P ∪ P̄ ) = 0 (12)\noùDist(Clk1 , Clk2) est la distance entre les deux clusters dépendantsClk1 etClk2 liés comme étant la moyenne des distances entre les fonctions de masse des objets en commun :\nDist(Clk1 , Clk2) = 1\n|Clk1 ∩ Clk2 | |Clk1∩Clk2 |∑ j=1 d(mΩ1,j ,m Ω 2,j) (13)\noù d est la distance proposée par Jousselme et al. (2001) entre les fonctions de masse de la source S1 et S2 respectivement.\nSi nous considérons que Ī = P ∪ P̄ , nous pouvons réécrire les deux fonctions de masse précédentes dans le cadre de discernement I = {I, P, P̄}. Nous définissons ainsi la fonction\nde masse entre deux clusters de S1 et S2 : mIk1k2(I) = m ΩI k1k2 (I) = β α1k1k2 mIk1k2(P ) = m ΩI k1k2 (Ī)mΩPk1k2 [Ī](P ) = β (1− α 1 k1k2 )(1−Dist(Clk1 , Clk2)) mIk1k2(P̄ ) = m ΩI k1k2 (Ī)mΩPk1k2 [Ī](P̄ ) = β (1− α 1 k1k2 )Dist(Clk1 , Clk2) mIk1k2(P ∪ P̄ ) = m ΩI k1k2\n(Ī)mΩPk1k2 [Ī](P ∪ P̄ ) = 0 mIk1k2(I ∪ P ∪ P̄ ) = m ΩI k1k2 (I ∪ Ī) = 1− β\n(14)\nLa fonction de masse sur la dépendance de la source S1 par rapport à S2 est donnée par :\nmI(X) = 1\nn\n( n∑\nk1=1\nmIk1k2\n) (X) (15)\noù k2 est le cluster de la source S2 associé au cluster k1 de la source S1. Cette fonction de masse représente ainsi l’ensemble des croyances élémentaires sur l’indépendance et dépendance positive et négative de la source S1 face à la source S2."
    }, {
      "heading" : "2.3 Notion d’affaiblissement",
      "text" : "Shafer (1976) a proposé la procédure d’affaiblissement suivante :\nαmΩ(X) = αmΩ(X) ∀X ∈ 2Ω \\ Ω (16) αmΩ(Ω) = 1− α(1−mΩ(Ω)) (17)\noù α est un facteur d’affaiblissement de [0, 1]. Cette procédure est généralement employée pour affaiblir les fonctions de masse par la fiabilité α des sources d’informations. Cette procédure a pour effet d’augmenter la masse sur l’ignorance Ω. Smets (1993) a justifié cette procédure en considérant que :\nmΩ[F ](X) = mΩ(X) (18) mΩ[F̄ ](X) = mΩΩ(X) (19)\noù mΩΩ(X) = 1 si X = Ω et 0 sinon, F et F̄ représentent la fiabilité et la non fiabilité et mΩ[F ] est une fonction de masse conditionnelement à la fiabilité F . Soit F = {F, F̄} le cadre de discernement correspondant, et la fonction de masse représentant la connaissance sur la fiabilité de la source : {\nmF (F ) = α mF (F) = 1− α. (20)\nAfin de combiner les deux sources d’informations fournissant les deux fonctions de masse mΩ[F ] et mF , il faut pouvoir les représenter dans le même espace Ω×F . Ainsi, nous devons effectuer une extention à vide sur la fonction de masse mF , opération que l’on note mF↑Ω×F :\nmF↑Ω×F (Y ) = { mF (X) si Y = Ω×X, X ⊆ F 0 sinon (21)\nDans le cas de la fonction de masse mΩ[F ], il faut déconditionner :\nmΩ [F ] ⇑Ω×F ( (A× F ) ∪ (Ω× F ) ) = mΩ [F ] (A) , A ⊆ Ω (22)\nIl est ainsi possible d’effectuer la combinaison :\nmΩ×FConj (Y ) = m F↑Ω×F ∩©mΩ [F ]⇑Ω×F (Y ), ∀Y ⊂ Ω×F (23)\nEnsuite il faut marginaliser la fonction de masse obtenue pour revenir dans l’espace Ω :\nmΩ×F↓Ω (X) = ∑\n{Y⊆Ω×F|Proj(Y ↓Ω)=X}\nmΩ×FConj (Y ) (24)\noù Proj (Y ↓ Ω) est la projection de Y sur Ω. Nous retrouvons ainsi :\nαmΩ(X) = mΩ×F↓Ω (X) (25)\nMercier (2006) a proposé une extension de cet affaiblissement en contextualisant le coefficient d’affaiblissement α en fonction de sous-ensembles de Ω."
    }, {
      "heading" : "3 Intégration de l’indépendance dans une fonction de masse",
      "text" : "Nous avons vu que la notion de l’indépendance est généralement une information supplémentaire nécessaire à la fusion d’informations, mais non prise en compte dans le formalisme choisi. La section 2.2 propose une modélisation et estimation d’une mesure d’indépendance dans le cadre de la théorie des fonctions de croyance. Nous allons ici nous appuyer sur le principe de l’affaiblissement présenté dans la section 2.3 afin de tenir compte de l’indépendance dans les fonctions de masse en vue de la combinaison.\nEn effet, lors de la combinaison conjonctive par exemple l’hypothèse d’indépendance cognitive des sources d’informations est nécessaire. Si les sources ne sont pas indépendantes on peut penser qu’elles ne devraient pas être combinées par ce biais. Cependant, comme le montre la section 2.2 les sources peuvent avoir des degrés de dépendance et d’indépendance. L’information fournie sur l’indépendance n’est pas catégorique. Ainsi, combiner deux sources indépendantes fortement devraient tendre vers le résultat de la combinaison de deux sources indépendantes. Si une source est dépendante d’une autre source, nous pouvons considérer que cette première source ne doit pas influer la combinaison avec une seconde source. Ainsi cette source doit représenter l’élément neutre de la combinaison.\nDans ce cas, il suffit d’appliquer la procédure d’affiblissement de la section 2.3 sur la fonction de masse mΩ de la source S1 en considérant l’indépendance donnée par la fonction de masse de l’équation (11) au lieu de celle de l’équation (20) dans le cas de la fiabilité.\nÀ présent, nous distinguons la dépendance positive de la dépendance négative. Si une source est dépendante positivement d’une autre source, il ne faut pas en tenir compte et donc tendre vers un résultat de combinaison qui prendrait cette première source comme un élément neutre. Enfin si une source est dépendante négativement d’une autre source, il peut être intéressant de marquer cette dépendance conflictuelle en augmentant la masse sur l’ensemble vide.\nPour réaliser ce schéma, nous proposons d’affaiblir les fonctions de masse d’une source S1 en fonction de sa mesure d’indépendance à une autre source S2, donnée par la fonction de massemI de l’équation (15). Nous réécrivons ici cette fonctions de masse de façon à simplifier\nles notations :  mI(I) = αβ mI(P ) = α(1− β)γ mI(P̄ ) = α(1− β)(1− γ) mI(I ∪ P ∪ P̄ ) = 1− α\n(26)\nAinsi, le paramètre α représente la fiabilité de la source S1, β l’indédendance de S1 face à S2 et γ la dépendance positive de S1 face à S2. Ces trois paramètres, α, β et γ sont compris entre 0 et 1.\nNous considérons ici une fonction de masse d’une source mΩ en fonction de son indépendance ou dépendance à une autre source. Ainsi nous définissons : m\nΩ[I](X) = mΩ(X) mΩ[P̄ ](X) = mΩ∅ (X) mΩ[P ](X) = mΩΩ(X)\n(27)\noù mΩΩ(X) = 1 si X = Ω et 0 sinon et m Ω ∅ (X) = 1 si X = ∅ et 0 sinon. Suivant la procédure d’affaiblissement, nous effectuons une extension à vide sur la fonction de masse mI :\nmI↑Ω×I (Y ) = { mI (X) si Y = Ω×X, X ⊆ I 0 sinon (28)\nLe déconditionnement des fonctions de masse mΩ[I], mΩ[P ] et mΩ[P̄ ] est donné par :\nmΩ [I] ⇑Ω×I ( (A× I) ∪ (Ω× I) ) = mΩ [I] (A) , A ⊆ Ω (29)\noù Ī = P ∪ P̄ .\nmΩ [ P̄ ]⇑Ω×I ( (A× P̄ ) ∪ (Ω× {I ∪ P}) ) = mΩ [ P̄ ] (A) , A ⊆ Ω (30)\nmΩ [P ] ⇑Ω×I ( (A× P ) ∪ (Ω× {I ∪ P̄}) ) = mΩ [P ] (A) , A ⊆ Ω (31)\nCe dernier déconditionnement mène en fait à la masse de l’ignorance et est l’élément neutre de la combinaison conjonctive.\nNous réalisons ensuite la combinaison conjonctive :\nmΩ×IConj(Y ) = m I↑Ω×I ∩©mΩ [I]⇑Ω×I ∩©mΩ\n[ P̄ ]⇑Ω×I (Y ), ∀Y ⊂ Ω× I (32)\nLa marginalisation de la fonction de masse permet ensuite de revenir dans l’espace Ω :\nmΩ×I↓Ω (X) = ∑\n{Y⊆Ω×I|Proj(Y ↓Ω)=X}\nmΩ×IConj (Y ) (33)\nCette procédure réalisée pour la source S1 en rapport à la source S2 peut être réalisée pour la source S2 au regard de la source S1. Ainsi les deux fonctions de masse obtenue peuvent être combinées par la règle de combinaison conjonctive qui suppose l’indépendance."
    }, {
      "heading" : "4 Illustration",
      "text" : ""
    }, {
      "heading" : "4.1 Fonctionnement de l’affaiblissement par la mesure d’indépendance",
      "text" : "Nous allons dans un premier temps illustrer le fonctionnement de l’affaiblissement par la mesure d’indépendance. Nous considérons ici un cadre de discernement Ω = {ω1, ω2, ω3}. Supposons que nous ayons deux sources S1 et S2 donnant deux fonctions de masse :\nmΩ1 (ω1) = 0.2, m Ω 1 (ω1 ∪ ω2) = 0.5, mΩ1 (Ω) = 0.3, (34)\nmΩ2 (ω2) = 0.1, m Ω 2 (ω1 ∪ ω2) = 0.6, mΩ2 (Ω) = 0.3 (35)\nLa combinaison conjonctive donne :\nmΩ1∩2(∅) = 0.02, mΩ1∩2(ω1) = 0.18, mΩ1∩2(ω2) = 0.08, mΩ1∩2(ω1 ∪ ω2) = 0.63, mΩ1∩2(Ω) = 0.09\nCette combinaison conjonctive est effectuée avec l’hypothèse d’indépendance cognitive des deux sources. Si une connaissance externe permet de mesurer la dépendance positive et négative de la source S1 par rapport à la source S2 telle que fournie par l’équation (36), nous devons en tenir compte avant la combinaison conjonctive. Supposons ainsi que α = 0.95, β = 0.05 et γ = 0.95 dans l’équation (36). Cette fonction de masse traduit donc une forte dépendance positive de S1 par rapport à S2. Nous avons ainsi la fonction de masse : mI(I) = 0.0475 mI(P ) = 0.8574 mI(P̄ ) = 0.0451 mI(I ∪ P ∪ P̄ ) = 0.05 (36)\nLe tableau 1 présente les différentes étapes d’extension à vide, de déconditionnement et de combinaison dans l’espace Ω × I. L’extension à vide et le déconditionnement transfèrent les masse sur les éléments focaux correspondant de l’espace Ω× I. La combinaison des trois fonctions de masse dans cet espace fait apparaître la masse sur l’ensemble vide qui correspond à la part de dépendance négative.\nLe tableau 2 présente ensuite la marginalisation et le résultat de combinaison avec la fonction de masse m2 non modifiée (i.e. que l’hypothèse d’indépendance totale de S2 par rapport à S1 est faite). Nous constatons que la masse transférée sur l’ignorance devient plus importante que lors de la combinaison conjonctive sans hypothèse sur la dépendance positive.\nAfin de bien illustrer le transfert de masse sur l’ensemble vide et sur l’ignorance, les figures 1 et 2 représentent les masses en fonction des variations de α, β et γ pour une fonction de masse dogmatique quelconque. Ainsi sur la figure 1 représentant les variations de masse sur l’ensemble vide, α est fixé à 1, β et γ variant, alors que sur la figure 2 représentant les variations de masse sur l’ignorance, γ est fixé à 1, α et β variant.\nLa figure 1 montre ainsi que plus β et γ sont petits plus on obtient une masse importante sur l’ensemble vide et donc une dépendance négative. La quantité β représente la part d’indépendance et la quantité γ représente la part de dépendance positive.\nLa figure 2 présente quand à elle, la variation de la masse sur Ω, l’ignorance. Cette masse est donnée directement par αβ qui contient donc la part d’indépendance β et la fiabilité α de la source.\nfocal mI↑Ω×I mΩ[I]⇑Ω×I mΩ[P̄ ]⇑Ω×I mΩ×IConj ∅ 0.0451\nω1 × I 0.0095 (ω1 ∪ ω2)× I 0.0237\nΩ× I 0.0475 0.0142 Ω× P 0.8574 0.8574\n(ω1 × I) ∪ (Ω× P ) 0.01 ((ω1 ∪ ω2)× I) ∪ (Ω× P ) 0.025\nΩ× P̄ 0.0451 Ω× (P ∪ P̄ ) 1\n(ω1 × I) ∪ (Ω× (P ∪ P̄ )) 0.2 ((ω1 ∪ ω2)× I) ∪ (Ω× (P ∪ P̄ )) 0.5\nΩ× I 0.05 0.3\nTAB. 1 – Détails de l’affaiblissement de la mesure d’indépendance : fonctions de masse dans Ω× I.\nfocal mΩ×I↓Ω1 m Ω 2 m Ω×I↓Ω 1 ∩©m Ω 2\n∅ 0.0451 0.0461 ω1 0.0095 0.0085 ω2 0.1 0.0945 ω1 ∪ ω2 0.0237 0.6 0.5743 Ω 0.9216 0.3 0.2765\nTAB. 2 – Détails de l’affaiblissement de la mesure d’indépendance : marginalisation et combinaison\nNous illustrons ainsi le résultat escompté de l’affaiblissement par la mesure d’indépendance, c’est-à-dire que nous retrouvons sur la masse de l’ensemble vide la quantité de dépendance négative et sur l’ignorance la quantité de fiabilité et de d’indépendance."
    }, {
      "heading" : "4.2 Influence sur le résultat de combinaison",
      "text" : "Afin d’illustrer l’influence de la prise en compte de la mesure d’indépendance sur les fonctions de masse, nous allons considérer ici les deux sources précédentes S1 et S2 qui fournissent les fonctions de masse données par les équations (34) et (35). Nous allons considérer trois cas pour chaque source avec un cas où la source S1 est plutôt indépendante de S2 (α = 0.95, β = 0.95, γ = 0.05), un cas où elle est plutôt dépendante positivement (α = 0.95, β = 0.05, γ = 0.95) et un cas où elle est plutôt dépendante négativement (α = 0.95, β = 0.05,\nγ = 0.05). Pour la source S2 nous considérons trois cas moins catégorique en fixant la fiabilité α = 0.9 : le cas plutôt indépendant (β = 0.9, γ = 0.1), le cas plutôt dépendant positivement (β = 0.1, γ = 0.9) et le cas plutôt dépendant négativement (β = 0.1, γ = 0.1).\nAinsi, le tableau 3 présente les résultats de la combinaison des deux sources en fonction des hypothèses d’indépendance et de dépendance, positive ou négative des deux sources S1 et S2. Nous constatons que lorsque les deux sources sont plutôt indépendantes l’une de l’autre, les résultats obtenus sont proches de ceux obtenus par la combinaison conjonctive directe sous l’hypothèse d’indépendance. Lorsqu’une des deux sources est dépendante négativement de l’autre, la masse transférée sur l’ensemble vide est importante. Lorsque l’une des deux sources est dépendante positivement la masse transférée sur l’ignorance mais de façon moins importante que pour la dépendance négative. En effet, l’ensemble vide est un élément absorbant\nS2 : α = 0.9 cas élément β = 0.9, γ = 0.1 β = 0.1, γ = 0.9 β = 0.1, γ = 0.1\nfocal mΩ×I↓Ω1 m Ω×I↓Ω 2 m Ω×I↓Ω 1∩2 m Ω×I↓Ω 2 m Ω×I↓Ω 1∩2 m Ω×I↓Ω 2 m Ω×I↓Ω 1∩2\nS1 ∅ 0.0451 0.081 0.1371 0.081 0.1240 0.729 0.7428 α = 0.95 ω1 0.1805 0.1513 0.1643 0.0473 β = 0.95 ω2 0.081 0.0627 0.009 0.007 0.009 0.007 γ = 0.05 ω1 ∪ ω2 0.4513 0.486 0.5352 0.054 0.4281 0.054 0.1357\nΩ 0.3231 0.352 0.1137 0.856 0.2766 0.208 0.0672 S1 ∅ 0.0451 0.081 0.1232 0.081 0.1226 0.729 0.7413 α = 0.95 ω1 0.0095 0.008 0.0086 0.0025 β = 0.05 ω2 0.081 0.0766 0.009 0.0085 0.009 0.0085 γ = 0.95 ω1 ∪ ω2 0.0238 0.486 0.4678 0.0054 0.0714 0.054 0.056\nΩ 0.9216 0.352 0.3244 0.856 0.7889 0.208 0.1917 S1 ∅ 0.8574 0.081 0.8697 0.081 0.869 0.729 0.9614 α = 0.95 ω1 0.0095 0.008 0.0087 0.0025 β = 0.05 ω2 0.081 0.0108 0.009 0.0012 0.009 0.0012 γ = 0.05 ω1 ∪ ω2 0.0237 0.486 0.073 0.054 0.0275 0.054 0.0121\nΩ 0.1094 0.352 0.0385 0.856 0.0936 0.208 0.0228\nTAB. 3 – Résultats de combinaison selon les hjypothèses de dépendance et d’indépendance des deux sources S1 et S2.\npour la combinaison conjonctive. Cette masse sur l’ensemble vide à l’issue de la combinaison conjonctive peut ainsi jouer un rôle d’alerte sur la dépendance négative. Une autre alternative serait d’envisager une autre règle de combinaison lorsque la masse issue de la dépendance négative est trop importante."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Cet article souligne l’importance de mesurer la réelle indépendance des sources et d’en tenir compte en vue de la combinaison des informations issues de celles-ci. Nous nous restreignons ici à la théorie des fonctions de croyance qui représente un cadre assez général pour la fusion d’informations. De ce contexte théorique, nous avons montré une approche originale pour intégrer une mesure d’indépendance exprimée sous la forme d’une fonction de masse. Nous avons ensuite explicité les fonctions de masse conditionnellement à leur indépendance mutuelle par un procédé d’affaiblissement des masses initiales. Ce principe doit être réalisé en vue d’une combinaison de ces fonctions de masse qui nécessite l’hypothèse d’indépendance.\nUne autre approche envisageable serait d’intégrer la mesure d’indépendance dans la combinaison des fonctions de masse.\nRéférences Ben Yaghlane, B., P. Smets, et K. Mellouli (2002a). Belief function independence : I. the\nmarginal case. International Journal of Approximate Reasoning 29(1), 47–70. Ben Yaghlane, B., P. Smets, et K. Mellouli (2002b). Belief function independence : II. the\nconditional case. International Journal of Approximate Reasoning 29(1), 47–70. Chebbah, M., A. Martin, et B. Ben Yaghlane (2012). Positive and negative dependence for\nevidential database enrichment. In IPMU, Italy, pp. 575–584. Chebbah, M., A. Martin, et B. Ben Yaghlane (2013). Dépendance et indépendance des sources\nimparfaites. In EGC, Toulouse, France. Dempster, A. P. (1967). Upper and Lower probabilities induced by a multivalued mapping.\nAnnals of Mathematical Statistics 38, 325–339. Denœux, T. (2008). Conjunctive and disjunctive combination of belief functions induced by\nnondistinct bodies of evidence. Artificial Intelligence 172, 234–264. Jousselme, A.-L., D. Grenier, et E. Bossé (2001). A new distance between two bodies of\nevidence. Information Fusion 2, 91–101. Martin, A. (2005). Fusion de classifieurs pour la classification d’images sonar. RNTI Extraction\ndes connaissances : Etat et perspectives E-5, 259–268. Martin, A. (2010). Le conflit dans la théorie des fonctions de croyance. In EGC, Hammamet,\nTunisie. Mercier, D. (2006). Fusion d’informations pour la reconnaissance automatique d’adresses\npostales dans le cadre de la théorie des fonctions de croyance. Ph. D. thesis, Université de Technologie de Compiègne.\nShafer, G. (1976). A mathematical theory of evidence. Princeton University Press. Smets, P. (1993). Belief Functions : the Disjunctive Rule of Combination and the Generalized\nBayesian Theorem. International Journal of Approximate Reasoning 9, 1–35."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Résumé. La fusion d’informations fait intervenir plusieurs sources d’informations afin d’améliorer la décision en terme de certitude et de précision. Quelle que soit l’approche retenue pour réaliser la fusion d’informations, l’hypothèse d’indépendance est généralement une hypothèse forte et incontournable. Nous proposons dans cet article une approche permettant d’intégrer une mesure d’indépendance avant de réaliser la combinaison des informations dans le cadre de la théorie des fonctions de croyance.",
    "creator" : "LaTeX with hyperref package"
  }
}