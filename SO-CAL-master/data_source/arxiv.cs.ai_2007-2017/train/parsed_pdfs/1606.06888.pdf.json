{
  "name" : "1606.06888.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Structure in the Value Function of Two-Player Zero-Sum Games of Incomplete Information",
    "authors" : [ "Auke J. Wiggers", "Diederik M. Roijers" ],
    "emails" : [ "auke@scyfer.nl", "Frans.Oliehoek@liverpool.ac.uk", "diederik.roijers@cs.ox.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n06 88\n8v 1\n[ cs\n.A I]\n2 2\nJu n\n20 16"
    }, {
      "heading" : "1 Introduction",
      "text" : "Modeling decision making for strictly competitive settings with incomplete information is a field with many promising applications for AI. Examples include games such as poker [24] and security settings [8]. In strictly competitive sequential games in which the environment can be influenced through actions, the problem of behaving rationally can be modeled as a zero-sum Partially Observable Stochastic Game (zs-POSG).\nReasoning about zs-POSGs poses a challenge for strategic agents: they need to reason about their own uncertainty regarding the state of the environment as well as uncertainty regarding the opposing agent. As this opponent is trying to minimize the reward that they are maximizing, behaving strategically typically requires stochastic strategies. A factor that further complicates the reasoning is that agents not only influence their immediate rewards, but also the future state of the environment and both agents’ future observations.\nIn this paper, we prove the existence of structural properties of zsPOSGs, that may be exploited to make reasoning about these models more tractable. We take inspiration from recent work for collaborative settings which has shown that it is possible to summarize the past joint policy using so called plan-time sufficient statistics [15], which\n1 Scyfer B.V., email: auke@scyfer.nl 2 University of Liverpool, Universiteit van Amsterdam, email:\nFrans.Oliehoek@liverpool.ac.uk 3 University of Oxford, email: diederik.roijers@cs.ox.ac.uk\ncan be interpreted as the belief of a special type of Partially Observable Markov Decision Process (POMDP) to which the collaborative Decentralized POMDP can be reduced [3, 10, 14]. This enabled tackling these problems using solution methods for POMDPs, leading to increases in scalability [3].\nThis paper provides a theoretical basis for enabling similar advancements for zs-POSGs. In particular, we extend results for DecPOMDPs to the zs-POSG setting by presenting three contributions:\n1. Two novel formulations of the value functions in POSGs, one based on past joint policies, and one based on distributions of information in the game called plan-time sufficient statistics. 2. A proof that the latter formulation allows for a generalization over the statistics: on every stage, the value function exhibits concavity and convexity in different subspaces of statistic space. 3. A reduction of the zs-POSG to a Non-Observable Stochastic Game, which in turn allows us to shows that certain properties previously proven for narrower classes of games generalize to the more general zs-POSG considered here.\nThis is the first work that gives insight in how the value function of a zs-POSG generalizes over the space of sufficient statistics. We argue that this result may open up the route for new solution methods."
    }, {
      "heading" : "2 Background",
      "text" : "In this section we provide the necessary background to explain our contributions. We defer a treatment of related work to Section 6, where we can more concisely point out the differences to our work.\nThis paper focuses on zero-sum games of incomplete information where the number of states, actions, observations and the horizon are finite. We examine games where the hidden state is static, and games with dynamic state (i.e., it changes over time). We assume perfect recall, i.e., agents recall their own past actions and observations, and assume that all elements of the game are common knowledge among the agents [19, Chapter 5]."
    }, {
      "heading" : "2.1 Zero-Sum One-Shot Games",
      "text" : "We start by describing one-shot (static) games.\nDefinition 1. A normal form game (NFG) is a tuple N = 〈I,A,R〉:\n• I = {1, 2} is the set of 2 agents, • A = A1 ×A2 is the set of joint actions a = 〈a1, a2〉, also called\n(joint) strategies, • R = {R1, R2} is the set of reward (or ‘payoff’) functions for the\nagents: Ri : A → R is the reward function for agent i,\nIn the case of a zero-sum NFG (zs-NFG), we have that R1(a) = −R2(a),∀a. In a zs-NFG, we will define the following quantities (and the associated strategies):\nDefinition 2. The maxmin value (for agent 1) of a 2 player zerosum game is defined as Vmaxmin(N ) = maxa1 mina2 R1(a1, a2).\nDefinition 3. The minmax value (for agent 1) of a 2 player zerosum game is defined as Vminmax(N ) = mina2 maxa1 R1(a1, a2).\nThe min-max theorem states that ∀N Vmaxmin(N ) ≤ Vminmax(N ) [2]. In case of equality, we say that V (N ) = Vmaxmin(N ) = Vminmax(N ) is the value of the game.\nA Nash Equilibrium (NE) is a joint strategy from which no agent has an incentive to unilaterally deviate. In zs-NFGs, if an NE exists, then it coincides with the value of the game. That is, the value of a game is the value attained when both agents follow the strategy specified by the NE. Moreover any NEs, also called saddle points in this context, correspond to maxmin-strategies for the players [19, Proposition 22.2]. We will refer to such strategies as rational strategies.\nThe value is not guaranteed to exist in all zs-NFGs. If the action sets are convex and compact, if ∀a2, the mapping a1 → R1(a1, a2) is a concave function, and if ∀a1, a2 → R1(a1, a2) is a convex function, then the value exists and the zs-NFG has a Nash equilibrium (i.e., a saddle point) [1, p. 134], [19, Proposition 20.3]. In games where actions are discrete, an NE (and corresponding value) may not exist. However, when mixed strategies are allowed, we can convert to a continuous action NFG: 〈I,M1 × M2,R′〉, where µi ∈ Mi specifies a probability distribution over actions, and where R′i(µ1, µ2) = ∑\na1 µ1(a1)\n∑\na2 µ2(a2)Ri(a1, a2). Furthermore, it\ncan now be shown that the utility function R′(µ1, µ2) in terms of mixed strategies is concave in µ1 for each µ2, and convex in µ2 for each µ1, such that the aforementioned assumptions needed for the existence of a saddle-point hold (i.e., an NE exists [2, p.239])."
    }, {
      "heading" : "2.2 Zero-Sum Bayesian Games",
      "text" : "Here we consider zero-sum Bayesian Games (zs-BGs), in which agents simultaneously select an action based on an individual observation (often referred to as their type).\nDefinition 4. A zs-BG is defined as a tuple B = 〈I,Θ,A, R, σ〉:\n• I = {1, 2} is the set of 2 agents, • Θ = Θ1 ×Θ2 is the finite set of joint types θ = 〈θ1, θ2〉, • A = A1 ×A2 is the finite set of joint actions a = 〈a1, a2〉, • R : Θ×A → R is the reward function for agent 1, • σ ∈ ∆(Θ) is the probability distribution over joint types.\nIn this paper we treat finite zs-BGs, where the sets of actions and types are finite. A pure strategy, to which we refer as a pure decision rule, is a mapping from types to actions. A stochastic decision rule δi ∈ ∆Si is a mapping from types to probability distributions over the set of actions, denoted as δi(ai|θi), ∆Si is the space of such mappings. Given a joint decision rule δ = 〈δ1, δ2〉, the value is:\nQBG(B, δ) , ∑\nθ\nσ(θ) ∑\na\nδ(a|θ)R(θ, a). (1)\nwhere δ(a|θ) , δ1(a1|θ1)δ2(a2|θ2). The case of pure decision rules is covered by treating them as degenerate stochastic policies.\nThere are two ways to reduce a zs-BG to a zs-NFG. First, we can simply reinterpret QBG(B, δ) as a payoff function R(δ1, δ2), such\nthat the BG corresponds to an NFG with continuous action sets ∆Si for each player i. The conditions for the existence of the value for such a game can be shown to hold, so if we suppose that 〈δ∗1 , δ ∗ 2〉 is an NE, then the value of the game can be defined as the maxmin (=minmax) value:\nVBG(B) , QBG(B, 〈δ ∗ 1 , δ ∗ 2〉) =\nmax δ1∈∆ S 1 min δ2∈∆ S 2 QBG(B, 〈δ1, δ2〉) = min δ2∈∆ S 2 max δ1∈∆ S 1 QBG(B, 〈δ1, δ2〉). (2)\nAlternatively, one can reinterpret the BG as an NFG with finite action sets corresponding to the pure decision rules. This then leads to a similar R′(µ1, µ2) formulation where the mixed strategies µi now are distributions over pure decision rules. Again this formulation will satisfy the required assumptions on concavity/convexity, such that this reinterpretation leads to the same logical conclusion that the value of the finite zs-BG exists (these dual perspectives are possible due to the one-one correspondence between stochastic decision rules and mixed strategies).\nThese reductions imply that solution methods for zs-NFGs (e.g., via linear programming [25]) can be used to solve zs-BGs. However, such an approach does not scale well — a more efficient solution method is to convert the zs-BG to sequence form [9]."
    }, {
      "heading" : "2.3 Zero-sum POSGs",
      "text" : "A zero-sum Partially Observable Stochastic Game (zs-POSG) is a model for multi-agent decision making under uncertainty in zerosum sequential games where the state changes over time, and the agents simultaneously choose actions at every stage.\nDefinition 5. A finite zs-POSG is defined as a tuple P = 〈h, I,S ,A,O, T, O,R, b0〉:\n• h is the horizon, • I = {1, 2} is the set of 2 agents, • S is the finite set of states s, • A = A1 ×A2 is the finite set of joint actions a = 〈a1, a2〉, • O = O1 ×O2 is the finite set of joint observations o = 〈o1, o2〉, • T is the transition function Pr(st+1|st, at), • O is the observation function Pr(ot+1|st+1, at), • R : S ×A× S → R is the reward function for agent 1, • b0 ∈ ∆(S) is the probability distribution over states.\nIn the zs-POSG, we aim to find maxmin-strategies and corresponding value. Let a pure policy for agent i be a mapping from individual action-observation histories (AOHs) ~θti = 〈a0i , o 1 i , . . . , a t−1 i , o t i〉 to actions. Let a stochastic policy for agent i be a mapping from individual AOHs to a probability distribution over actions, denoted as πi(ati|~θ t i). An individual policy defines action selection of one agent on every stage of the game, and is essentially a sequence of individual decision rules πi = 〈δ0i . . . δ h−1 i 〉. We define the past individual policy as a tuple of decision rules ϕti = 〈δ0i , . . . , δ t−1 i 〉, and define the tuple containing decision rules from stage t to h as the partial individual policy πti = 〈δ t i , . . . , δ h−1 i 〉.\nAs in zs-BGs, it is theoretically possible to convert a zs-POSG to a zs-NFG and solve using standard methods, but this is infeasible in practice. An alternative is to converting the zs-POSG to an extensive form game (EFG) and solve it in sequence form [9]. While this is more efficient than the NFG route, it is still intractable: the resulting EFG is huge since its size depends on the number of full histories (trajectories of joint actions, joint observations, and states) [18].\nWith the goal of opening paths to completely new approaches of tackling zs-POSGs, this paper focuses on the description of the value function of zs-POSGs, which (in contrast to the value of a game as defined above) is a function mapping from some notion of ‘state’ of a game to the expected value (for agent 1). Similar to how the ‘value’ is defined by rational strategies, we will use the term ‘value function’ for a function that captures the future expected rewards under a rational joint policy. However, since one can also reason about the value of non-rational policies, we will refer to the ‘rational value function’ if there is a need to clarify."
    }, {
      "heading" : "3 Structure in One-Shot Value",
      "text" : "In order to provide a value function definition for the sequential setting (in Section 4), we will rely on an intermediate result for one-shot games developed in this section. In particular, we introduce the concept of a Family of zero-sum Bayesian Games, for which we will introduce the joint type distribution as a suitable notion of ‘state’ and prove that its value function exhibits certain concave/convex properties with respect to this notion."
    }, {
      "heading" : "3.1 Families of Bayesian Games",
      "text" : "Here we consider the notion of a family of (zs)-BGs. Intuitively, different stages t of a POSG are similar to a BG: each agent has a privately observed history, which corresponds to its type. However, the probabilities of these histories might depend on how the game was played in earlier stages. As such, we will need to reason about families of BGs.\nDefinition 6. A Family of Bayesian Games, F = 〈I,Θ,A, R〉, is the set of Bayesian Games of the form 〈I,Θ,A, R, σ〉 for which I,Θ,A and R are identical.\nLet F be a Family of zero-sum Bayesian Games. By providing a joint type distribution, F(σ) indicates a particular zs-BG. We generalize (1) and (2) as follows:\nQF (σ, δ) , QBG(F(σ), δ), (3)\nV ∗ F(σ) , VBG(F(σ)). (4)\nAs each F(σ) is a zs-BG, the rational value function V ∗F(σ) can be written as:\nV ∗ F (σ) = max\nδ1∈∆ S 1 min δ2∈∆ S 2 QF(σ, 〈δ1, δ2〉). (5)\nWe define best-response value functions that give the best-response value to a decision rule of the opposing agent:\nV BR1 F (σ, δ2) , max\nδ1∈∆ S 1\nQF (σ, 〈δ1, δ2〉), (6)\nV BR2 F (σ, δ1) , min\nδ2∈∆ S 2\nQF (σ, 〈δ1, δ2〉). (7)\nWe remind the reader that, for each σ, QF(σ, 〈δ1, δ2〉) exhibits concavity and convexity in the space of decision rules, as discussed in Section 2.2."
    }, {
      "heading" : "3.2 Concavity/Convexity of the Value Function",
      "text" : "We saw in the previous section that the concave/convex shape of the utility function expressed in the space of decision rules follows di-\nrectly from known results. Here we provide a novel result: V ∗F exhibits a similar concave/convex shape in terms of type distributions. This is important, as it provides insight on how the distribution of information (in addition to the distribution of actions) affects the value of the game, which is key to enabling generalization of the value in different parts of sequential games. To provide this formulation, we decompose σ into a marginal term σm,i and a conditional term σc,i, here shown for i = 1:\nσm,1(θ1) , ∑\nθ2∈Θ2\nσ(θ1θ2), σc,1(θ2|θ1) , σ(θ1θ2)\nσm,1(θ1) . (8)\nThe terms σm,2 and σc,2 are defined similarly. We refer to the simplex ∆(Θi) containing marginals σm,i as the marginal-space of agent i. We write σt = σtm,iσ t c,i for short.\nWe will first show that the best-response value functions defined in (6) and (7) are linear in their respective marginal-spaces. Using this result, we prove that V ∗F exhibits concavity in ∆(Θ1) for every σc,1, and convexity in ∆(Θ2) for every σc,2. For this purpose, let us define a vector that contains the reward for agent 1 for each individual type θ1, given σc,1 and given that agent 2 follows decision rule δ2:\n~r[σc,1,δ2](θ1) , max a1∈A1\n[\n∑\nθ2\nσc,1(θ2|θ1) ∑\na2\nδ2(a2|θ2)R(θ, a)\n]\n. (9)\nThe vector ~r[σc,2,δ1] is defined analogously. Now we can state an important lemma that shows that the bestresponse value functions are linear functions in the marginal space.\nLemma 1. (1) V BR1F is linear in ∆(Θ1) for all σc,1 and δ2, and (2) V BR2F is linear in ∆(Θ2) for all σc,2 and δ1:\n1. V BR1F (σm,1σc,1, δ2) = ~σm,1 · ~r[σc,1,δ2], (10) 2. V BR2F (σm,2σc,2, δ1) = ~σm,2 · ~r[σc,2,δ1]. (11)\nProof. The proof is listed in Appendix A.\nThis lemma can now be used to show that the rational value function V ∗F possess concave and convex structure with respect to (appropriately chosen marginals of) the space of joint type distributions.\nTheorem 1. V ∗F is (1) concave in ∆(Θ1) for a given conditional distribution σc,1, and (2) convex in ∆(Θ2) for a given conditional distribution σc,2. More specifically, V ∗F is respectively a minimization over linear functions in ∆(Θ1) and a maximization over linear functions in ∆(Θ2):\n1. V ∗F (σm,1σc,1) = min δ2∈∆ S 2\n[\n~σm,1 · ~r[σc,1,δ2]\n]\n,\n2. V ∗F (σm,2σc,2) = max δ1∈∆ S 1\n[\n~σm,2 · ~r[σc,2,δ1]\n]\n.\nProof. Filling in the result of Lemma 1 gives:\nV ∗ F (σm,1σc,1) {5,6} = min\nδ2∈∆ S 2\nV BR1 F (σm,1σc,1, δ2)\n{10} = min\nδ2∈∆ S 2\n[\n~σm,1 · ~r[σc,1,δ2]\n]\n.\nThe proof for item 2 is analogous to that of item 1.\nThe importance of this theorem is that it gives direct ways to approximately generalize the value function using piecewise-linear and convex (concave) functions."
    }, {
      "heading" : "4 Structure in zs-POSG Value",
      "text" : "Given the result for one-shot games established in the previous section, we are now in the position to present our main contributions: novel formulations for the value function, and generalization of the structural result of Theorem 1 to the sequential setting. First, we will introduce a description of the rational value function based on past joint policies."
    }, {
      "heading" : "4.1 Past Joint Policies",
      "text" : "To make rational decisions at stage t in the zs-POSG, it is sufficient to know the past decisions, which are captured in the past joint policy ϕt. To show this, the zs-POSG value function in terms of the past joint policy ϕt can be defined in terms of ϕt by extending the formulation by Oliehoek [15].\nWe define the value function of the zs-POSG at a stage t in terms of a past joint policy ϕt. The definition that follows gives the value attained when all agents follow the joint decision rule δt, assuming that in future stages agents will act rationally. That is, the agents follow a rational joint future policy πt+1∗i = 〈δ\nt+1∗ . . . δh−1∗〉. We first define the Q-value function at the final stage t = h − 1, and give an inductive definition of the Q-value function at preceding stages. We then define the value function at every stage. Let immediate reward for a joint AOH and a joint decision rule be defined as:\nR(~θt, δt) , ∑\nat\nδ t(at|~θt)\n∑\nst\nPr(st|~θt, b0)R(st, at).\nFor the final stage t = h − 1, the Q-value function reduces to this immediate reward, as there is no future value:\nQ ∗ h−1(ϕ h−1 , ~θ h−1 , δ h−1) , R(~θh−1, δh−1). (12)\nGiven an AOH and decision rule at stage t, it is possible to find a probability distribution over joint AOHs at the next stage, as a joint AOH at t+ 1 is the joint AOH at t concatenated with joint action at and joint observation ot+1:\nPr(~θt+1|~θt, δt)=Pr(〈~θt, at, ot+1〉|~θt, δt)=Pr(ot+1|~θt, at)δt(at|~θt).\nFor all stages except the final stage t = h − 1, the value at future stages is propagated to the current stage as follows:\nQ ∗ t (ϕ t , ~θ t , δ t) , R(~θt, δt) + ∑\nat∈A\n∑\not+1\nPr(~θt+1|~θt, δt)\nQ ∗ t+1(ϕ t+1 , ~θ t+1 , δ t+1∗). (13)\nQ ∗ t (ϕ t , δ t) , ∑\n~θt∈~Θt\nPr(~θt|b0, ϕt)Q∗t (ϕ t , ~θ t , δ t). (14)\nWe use (14) to find rational decision rules for both agents. Consistent with (13), we show how to find δt+1∗ = 〈δt+1∗1 , δ t+1∗ 2 〉:\nδ t+1∗ 1 = argmax\nδ t+1 1 ∈∆S 1\nmin δ t+1 2 ∈∆S 2\nQ ∗ t+1(ϕ t+1 , 〈δt+11 , δ t+1 2 〉), (15)\nδ t+1∗ 2 = argmin\nδ t+1 2 ∈∆S 2\nmax δ t+1 1 ∈∆S 1\nQ ∗ t+1(ϕ t+1 , 〈δt+11 , δ t+1 2 〉). (16)\nUsing (12), (15) and (16), a rational joint decision rule δh−1∗ can be found by performing a maxminimization over immediate reward. Evaluation of Q∗h−1(ϕ t, δh−1∗) gives us the value at stage t = h −\n1, and (13) propagates the value to the preceding stages. As such, rationality for all stages follows by induction. The value function can now be defined in terms of the past joint policy as:\nV ∗ t (ϕ t) = max δt 1 ∈∆S 1 min δt 2 ∈∆S 2 Q ∗ t (ϕ t , 〈δt1, δ t 2〉). (17)\nBy (12), (15) and (16), δt∗ is dependent on δt+1∗, and thus on the rational future joint policy. However, δt+1∗ can only be found if past joint policy ϕt+1, which includes δt, is known. As such, (17) can not be used to form a backward inductive approach directly."
    }, {
      "heading" : "4.2 Plan-Time Sufficient Statistics",
      "text" : "A disadvantage of the value function definition from the previous section is that the value at a stage t depends on all the joint decisions from stage 0 to t. We propose to define the value function in terms of a plan-time sufficient statistic that summarizes many past joint policies. Furthermore, we give formal proof that this value function exhibits a concave/convex shape in statistic-space that may be exploitable.\nDefinition 7. The plan-time sufficient statistic for a general past joint policy ϕt, assuming b0 is known, is a distribution over joint AOHs: σt(~θt) , Pr(~θt|b0, ϕt).\nIn the collaborative Dec-POMDP case, a plan-time sufficient statistic fully captures the influence of the past joint policy. We prove that this also holds for the zs-POSG case, thereby validating the previous definition and the name ‘sufficient statistic‘. We aim to express the value for a given decision rule δt in terms of a plan-time sufficient statistic, given that the agents act rationally at later stages. First, we define the update rule for plan-time sufficient statistics:\nσ t+1(~θt+1) , Pr(ot+1|~θt, at)δt(at|~θt)σt(~θt). (18)\nThe Q-value function at stage h−1 reduces to the immediate reward:\nQ ∗ h−1(σ h−1 , ~θ h−1 , δ h−1) , R(~θh−1, δh−1). (19)\nWe then define the Q-value for all other stages similar to (13), (14):\nQ ∗ t (σ t , ~θ t , δ t) , R(~θt, δt) + ∑\nat\n∑\not+1\nPr(~θt+1|~θt, δt)\nQ ∗ t+1(σ t+1 , ~θ t+1 , δ t+1∗). (20)\nQ ∗ t (σ t , δ t) , ∑\n~θt\nσ t(~θt)Q∗t (σ t , ~θ t , δ t). (21)\nRational decision rules can be found using (21):\nδ t+1∗ 1 = argmax\nδ t+1 1 ∈∆S 1\nmin δ t+1 2 ∈∆S 2\nQ ∗ t+1(σ t+1 , 〈δt+11 , δ t+1 2 〉), (22)\nδ t+1∗ 2 = argmin\nδ t+1 2 ∈∆S 2\nmax δ t+1 1 ∈∆S 1\nQ ∗ t+1(σ t+1 , 〈δt+11 , δ t+1 2 〉). (23)\nWe formally prove that the statistic σt provides sufficient information for rational decision-making in the zs-POSG.\nLemma 2. σt is a sufficient statistic for the value of the zs-POSG, i.e. Q∗t (σ t, ~θt, δt) = Q∗t (ϕ t, ~θt, δt),∀t ∈ 0 . . . h− 1, ∀~θt ∈ ~Θt,∀δt.\nProof. The proof is listed in Appendix A.\nThis allows us to define the value function of the zs-POSG in terms of the sufficient statistic as follows:\nV ∗ t (σ t) , max δt 1 ∈∆S 1 min δt 2 ∈∆S 2 Q ∗ t (σ t , 〈δt1, δ t 2〉). (24)\nAlthough we have now identified the value at a single stage of the game, implementing a backwards inductive approach directly is still not possible, since the space of statistics is continuous and we do not know how to represent V ∗t (σ\nt). This paper takes a first step at resolving this problem by investigating the structure of V ∗t (σ t)."
    }, {
      "heading" : "4.3 Equivalence Final Stage Zero-Sum POSG and Family of Zero-Sum Bayesian Games",
      "text" : "We have already proven that the value function of a Family of zerosum Bayesian Games exhibits concavity and convexity in terms of the marginal parts of the type distribution σ for respectively agent 1 and 2. We show that the final stage of the zs-POSG P , t = h − 1, can be defined as a Family of zs-BGs as follows:\n• I = {1, 2} is the set of agents, • Θ = ~Θh−1 is the set of joint types corresponding to AOHs in\nzs-POSG P at stage h− 1, • A is the set of joint actions in zs-POSG P , • R(~θh−1, a) follows directly from the immediate reward function\nof P (as described in Section 4.1).\nBy the result of Theorem 1 (i.e., that V ∗F exhibits concavity and convexity in marginal-spaces of agent 1 and 2 respectively) the value function at the final stage of the zs-POSG, V ∗h−1, is concave in ∆(~Θh−11 ) for all σ h−1 c,1 , and convex in ∆(~Θ h−1 2 ) for all σ h−1 c,2 .\nNote that, even though the final stage is equal to a Family of Bayesian Games, our approach is substantially different from approaches that represent a POSG as a series of BGs [4] and derivative works [17]. In fact, all other stages (0 to h − 2) cannot be represented as a Family of BGs, as the rational value function for stages t = 0, ..., h− 2 cannot be expressed as a function R(~θt, a)."
    }, {
      "heading" : "4.4 Concavity/Convexity of the Value Function",
      "text" : "We continue to show that the value function for any stage exhibits the same type of structure. In particular, the plan-time sufficient statistic can be decomposed in marginals and conditionals, and the value function is concave in marginal-space for agent 1, ∆(~Θt1), and convex in marginal-space for agent 2, ∆(~Θt2). Figure 1 provides intuition on how the best-response value functions relate to the concave/convex value function: a ‘slice’ in statistic-space corresponds to a single conditional σtc,1 (σ t c,2) and exhibits concave (convex) shape of the value function made up by linear segments that each correspond to a partial policy of the other agent. As we will show, each segment corresponds exactly to a best-response value function.\nBest-response value functions in terms of σt and πti are defined as V BR1t and V BR2 t , similar to (6) and (7). Let Π t i be the space of all stochastic partial policies πti . We then have:\nV ∗ t (σ t) = min πt 2 ∈Πt 2 V BR1 t (σ t , π t 2) = max πt 1 ∈Πt 1 V BR2 t (σ t , π t 1). (25)\nWe first show that the best-response value functions V BR1t and V BR2t are linear in their respective marginal-spaces. Let us define a vector that contains the value (immediate reward and future value)\nfor agent 1 for each individual AOH ~θt1, given that agent 2 follows the partial policy πt2:\n~ν[σt c,1 ,πt 2 ](~θ t 1) , max\nat 1 ∈A1\n[\n∑\n~θt 2 ∈~Θt 2\nσ t c,1(~θ t 2|~θ t 1)\n∑\nat 2 ∈A2\nδ t 2(a t 2|~θ t 2)\n(\nR(~θt, at) + ∑\not+1∈O1\nPr(ot+1|~θt, at)~ν [σt+1\nc,1 ,π t+1 2\n] (~θt+11 )\n)]\n(26)\nNote that this is a recursive definition, and that ~θt+11 = 〈~θt1, a t 1, o\nt+1 1 〉. The vector ~ν[σtc,2,πt1] is defined analogously.\nLemma 3. (1) V BR1t is linear in ∆(~Θ t 1) for a given σ t c,1 and π t 2, and (2) V BR2t is linear in ∆(~Θ t 2) for a given σ t c,2 and π t 1, for all stages t = 0, . . . , h− 1:\n1. V BR1t (σ t m,1σ t c,1, π t 2) = ~σ t m,1 · ~ν[σt\nc,1 ,πt 2 ], (27)\n2. V BR2t (σ t m,2σ t c,2, π t 1) = ~σ t m,2 · ~ν[σt\nc,2 ,πt 1 ]. (28)\nProof. We prove this by induction. We know the zs-POSG value function at stage t = h − 1 to be equivalent to that of a Family of zs-BGs, which is concave/convex (Lemma 1). This is a base case for the proof. The full proof is listed in Appendix A.\nTheorem 2. V ∗t is (1) concave in ∆(~Θ t 1) for a given σ t c,1, and (2) convex in ∆(~Θt2) for a given σ t c,2. More specifically, V ∗ t is respectively a minimization over linear functions in ∆(~Θt1) and a maximization over linear functions in ∆(~Θt2):\n1. V ∗t (σ t m,1σ t c,1) = min\nπt 2 ∈Πt 2\n[\n~σtm,1 · ~ν[σt c,1 ,πt 2 ]\n]\n,\n2. V ∗t (σ t m,2σ t c,2) = max\nπt 1 ∈Πt 1\n[\n~σtm,2 · ~ν[σt c,2 ,πt 1 ]\n]\n.\nProof. Filling in the result of Lemma 3 gives:\nV ∗ t (σ t m,1σ t c,1) {25} = min\nπt 2 ∈Πt 2\nV BR1 t (σ t m,1σ t c,1, π t 2)\n{27} = min\nπt 2 ∈Πt 2\n[\n~σ t m,1 · ~ν[σt\nc,1 ,πt 2 ]\n]\n.\nThe proof for item 2 is analogous to that of item 1.\nThe importance of this theorem is that it suggests ways to (approximately) represent V ∗t (σ\nt). Thus, it may enable the development of new solution methods for zs-POSGs. To draw the parallel, many POMDP solution methods exploit the fact that a POMDP value function is piecewise-linear and convex (PWLC) in belief-space [20, 27] (which is similar to the statistic-space we consider), and recently\nsuch results have been extended to the decentralized cooperative (i.e., Dec-POMDP) case [3, 10].\nNote that our result is similar to, but different from the saddlepoint function that is typically associated with min-max equilibria. In particular, the value function in is defined in the space of plantime sufficient statistics, while the well-known saddle point function (see Section 2.1) is a function of complete strategies. This means that the latter is only defined for the reduction to a normal form game, which per definition destroys any of the specific structure of the game under concern. In contrast, our formulation preserves such structure and thus allows us to make statements about how value generalizes as a function of the information distribution. That is, the formulation allows us to give approximation bounds that generalize within each conditional statistic."
    }, {
      "heading" : "5 Reduction to NOSG",
      "text" : "The application of methods that exploit the PWLC structure of the value function of Dec-POMDPs was enabled by a reduction from Dec-POMDP to a non-observable MDP (NOMDP), which is a special type of (centralized) POMDP [3, 10, 14, 16]. This allows POMDP solution methods to be employed in the context of DecPOMDPs. The proposed plan-time statistics for Dec-POMDPs [15] precisely correspond to the belief in the centralized model.\nSince we have shown that it is possible to generalize the plantime statistics to the zs-POSG case, it is reasonable to expect that zsPOSGs can be reduced similarly. Here we present a reduction from zs-POSG to a special type of stochastic game where information is centralized, to which we refer as a Non-Observable Stochastic Game (NOSG). We do not provide the full background of the reduction for the Dec-POMDP case, but refer to [16]. The difference between the reduction from Dec-POMDP to NOMDP and the one we present next, is that the zs-POSG is reduced to a stochastic game where the joint AOH acts as the state.\nDefinition 8. A plan-time Non-Observable Stochastic Game for a zs-POSG is a tuple 〈Ṡ, Ȧ1, Ȧ1, Ȯ, Ṫ, Ȯ, Ṙ, ḃ0〉:\n• I = {1, 2} is the set of agents, • Ṡ is the set of augmented states ṡt, each corresponding to a joint\nAOH ~θt, • Ȧ = Ȧ1×Ȧ2 is the continuous action-space, containing stochas-\ntic decision rules δt = 〈δt1, δ t 2〉,\n• Ȯ = {NULL} is set of joint observations that only contains the NULL observation. • Ṫ is the transition function that specifies Ṫ (ṡt+1 | ṡt, 〈ȧt1, ȧ t 2〉) =\nPr(~θt+1 | ~θt, 〈δt1, δ t 2〉).\n• Ȯ is the observation function that specifies that observation NULL is received with probability 1. • Ṙ : Ṡ ×A → R is the reward function R(~θt, 〈δt1, δ t 2〉), • ḃ0 ∈ ∆(Ṡ) is the initial belief over states.\nIn the NOSG model, agents condition their choices on the joint belief over augmented states ḃ ∈ ∆(Ṡ), which corresponds to the belief over joint AOHs captured in the statistic σt ∈ ∆(Θt). As such, a value function formulation for the NOSG can be given in accordance with (24).\nIn order to avoid potential confusion, let us point out that a zsPOSG can also be converted to a best response POMDP by fixing the policies of one agent [12], which leads to a model where the information state b(s, θj) is a distribution over states and AOHs of the other agent. In contrast, our NOSG formulation maintains a belief\nover only joint AOHs. More importantly, however, our approach does not require fixing the policy of any agent, which would necessitate recomputing of all values when the fixed policy changes (if the past policy changes, the distributions over paths change, and thus future values are affected). Where the approach of Nair et al. [12] leads to a single-agent model that can be used to compute a best-response, our conversion leads to a multi-agent model that can be used to compute a Nash equilibrium directly.\nA key contribution of our NOSG formulation is that it directly indicates that properties of ‘zero-sum stochastic games with shared observations’ [5] also hold for zs-POSGs.\nDefinition 9. A zero-sum Shared Observation Stochastic Game (zs-SOSG) is a zs-POSG (cf. Def. 5) in which each agent receives the joint observation and can observe the other’s actions.\nGhosh et al. [5] show that, under some technical assumptions, a zsSOSG can be converted into a completely observable model (similar to the conversion of a POMDP into a belief MDP), and that, in the infinite-horizon case, both the value and a rational joint policy exists.\nOur claim is that these results in fact transfer to the more general class of zs-POSGs via our NOSG construction. We start by noting:\nLemma 4. The plan-time NOSG of a finite-horizon zs-POSG is a zs-SOSG.\nProof. In the finite-horizon case, the set of states in our plan-time NOSG is discrete. The action space, while continuous, is of finite dimensionality, and is a closed and bounded set. The set of shared observations only consists of a trivial NULL observation and by assuming rationality for both players, we can assume that they observe each others actions (corresponding to decision rules of the POSG).\nIn the infinite-horizon case, some assumptions on the class of decision rules δti are needed to be able to formulate the plan-time NOSG model and assuming infinite policy trees as the policies would violate certain technical requirements on which Ghosh’s results depend (e.g., the action-spaces are required to be metric and compact spaces). However, a straightforward extension of our reduction for the case where the agents use finite-state controllers (analogous to such formulations for Dec-POMDPs [10]), would satisfy these requirements, and as such we can infer the existence of a value for such games:\nCorollary 1. For infinite-horizon zs-POSGs where agents are restricted to use finite-state controllers for their policies, the value of the game exists.\nIn that way, our reduction shows that some of the properties established by Ghosh et al. for a limited subset of zero-sum stochastic games, in fact extend to a much broader class of problems."
    }, {
      "heading" : "6 Related Work",
      "text" : "There is rich body of literature on zero-sum games, and we provide pointers to the most relevant works here. The concave and convex structure we have found for the zs-POSG value function is similar to the saddle point structure associated with min-max equilibria [1]. Note, however, that we have defined the value function in terms of the distribution over information, rather than the substantially different space of joint strategies, solving of which requires flattening the game to normal form. That is, our results tell us something about the value of acting using the current information, thus they may give insight into games of general partial observability.\nA recent paper that is similar in spirit to ours is by Nayyar et al. [13] who introduce a so-called Common Information Based Conditional Belief — a probability distribution over AOHs and the state conditioned on common information — and use it to design a dynamic-programming approach for zs-POSGs. This method converts stages of the zs-POSG to Bayesian Games for which the type distribution corresponds to the statistic at that stage. However, since their proposed statistic is a distribution over joint AOHs and states, the statistic we propose in this paper is more compact. More importantly, Nayyar et al. do not provide any results regarding the structure of the value function, which is the main contribution of our paper.\nHansen et al. [7] present a dynamic-programming approach for finite-horizon (general sum) POSGs that iteratively constructs sets of one-step-longer (pure) policies for all agents. At every iteration, the sets of individual policies are pruned by removing dominated policies. This pruning is based on a different statistic called multi-agent belief : a distribution over states and policies of other agents. Such a multi-agent belief is sufficient from the perspective of an individual agent to determine its best response (or whether some of its policies are dominated). A more generalized investigation of individual statistics in decentralized settings is given by Wu & Lall [28]. However, these notions are not a sufficient statistic for the past joint policy from a designer perspective (as is the proposed plan-time sufficient statistic in this paper). In fact, they are complementary and we hypothesize that they can be fruitfully combined in future work.\nThere are works from game theory literature that present structural results on the value function of so-called ‘repeated zero-sum games with incomplete information’ [11, 21, 22, 23]. These can best be understood as a class of two-player extensive form games that lie in between Bayesian games and POSGs: at the start of the game, nature determines the state (a joint type) from which each agent makes a private observation (i.e., individual type), and subsequently the agents take actions in turns, thereby observing the actions of the opponent. The models for which these results have been proven are therefore substantially less general than the zs-POSG model we consider.\nFor various flavors of such games, it has been shown that the value function has a concave/convex structure: cases with incomplete information on one side [23, 26], and cases with incomplete information on both sides where ‘observations are independent’ (i.e., where the distribution over joint types is a product of individual type distributions) [21] or dependent (general joint type distributions) [11, 22]. These results, however, crucially depend on the alternating actions and the static state and therefore do not extend to zs-POSGs.\nA game-theoretic model that is closely related to the POSG model is the Interactive POMDP or I-POMDP [6]. In I-POMDPs, a (subjective) belief bi(s, ζj) is constructed from the perspective of a single agent as a probability distribution over states and the types, ζj , of the other agent. A level-k I-POMDP agent i reasons about level(k − 1) types ζj . Since each ζj fully determines the future policy of the other agent j, an I-POMDP can be interpreted as a best-response POMDP similar to the one introduced by Nair et al. [12] (discussed in Section 5), with the difference that the state also represents which policy the other agent uses. The differences mentioned in Section 5 also apply here; where an I-POMDP can be used to compute a bestresponse, our formulation is aimed at computing a Nash equilibrium."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "This paper presents a structural result on the shape of the value function of two-player zero-sum games of incomplete information, for games of static state and dynamic state, typically modeled as\na Bayesian Game (BG) and Partially Observable Stochastic Game (POSG) respectively. We formally defined the value function for both types of games in terms of an information distribution called the sufficient plan-time statistic: a probability distribution over joint sets of private information (originally used in the collaborative setting [15]). Using the fact that this probability distribution can be decomposed into a marginal and a conditional term, we presented that in the zero-sum case value functions of both types of games exhibit concavity (convexity) in the space of marginal statistics of the maximizing (minimizing) agent, for every conditional statistic. In the multi-stage game, this structure of the value function is preserved on every stage. Thus, our formulation enables us to make statements about how value generalizes as a function of the information distribution. Lastly, we showed how the results allow us to reduce our zs-POSG to a model with shared observations, thereby transferring properties of this narrower class of games to the zs-POSG.\nWe hope that this result leads to solution methods that exploit the structure of the value function at every stage, as recently such developments have been made in the field of cooperative multi-agent problems [10]. In particular, we believe that heuristic methods that identify useful (conditional) statistics to explore, or point-based methods that iteratively select statistics to evaluate [10, 27] may be adapted for the zs-POSG case.\nAcknowledgments This research is supported by the NWO Innovational Research Incentives Scheme Veni (#639.021.336) and NWO DTC-NCAP (#612.001.109) project."
    }, {
      "heading" : "A Appendix",
      "text" : "Proof of Lemma 1. We will prove item 1:\nV BR1 F (σm,1σc,1, δ2) = ~σm,1 · ~r[σc,1,δ2]\nThe Q-value definition is expanded in order to bring the marginal term σm,1 to the front of the equation:\nQF(σ, δ) {1,3} = ∑\nθ∈Θ\nσ(θ) ∑\na∈A\nδ(a|θ)R(θ, a)\n= ∑\nθ1∈Θ1\nσm,1(θ1) ∑\nθ2∈Θ2\nσc,1(θ2|θ1) ∑\na1∈A1\nδ1(a1|θ1)\n∑\na2∈A2\nδ2(a2|θ2)R(〈θ1, θ2〉, 〈a1, a2〉). (A.29)\nA maximization over stochastic decision rules conditioned on θ1 is equivalent to choosing a maximizing action for each θ1. Thus, we can rewrite the best-response value function as follows:\nV BR1 F (σ, δ2) = max\nδ1∈∆ S 1\nQ R t (σ, 〈δ1, δ2〉)\n{A.29} = max\nδ1∈∆ S 1\n[\n∑\nθ1∈Θ1\nσm,1(θ1) ∑\nθ2∈Θ2\nσc,1(θ2|θ1) ∑\na1∈A1\nδ1(a1|θ1)\n∑\na2∈A2\nδ2(a2|θ2)R(〈θ1, θ2〉, 〈a1, a2〉)\n]\n= ∑\nθ1∈Θ1\nσm,1(θ1)max a1\n[\n∑\nθ2∈Θ2\nσc,1(θ2|θ1) ∑\na2∈A2\nδ2(a2|θ2)R(θ, a)\n]\n{9} = ∑\nθ1∈Θ1\nσm,1(θ1)~r[σc,1,δ2](θ1) {vec. not.} = ~σm,1 · ~r[σc,1,δ2].\nAs it is possible to write V BR1F as an inner product of two vectors, V BR1F is linear in ∆(Θ1) for all σc,1 and δ2. Analogously, V BR2 F is linear in ∆(Θ2) for all σc,2 and δ1.\nProof of Lemma 2. The proof is largely identical to the proof of correctness of sufficient statistics in the collaborative setting [15]. For the final stage t = h − 1, we have the following: Q∗t (ϕ\nt, ~θt, δt) = R(~θt, δt) = Q∗t (σ\nt, ~θt, δt). As induction hypothesis, we assume that at stage t+ 1, σt+1 is a sufficient statistic, i.e.:\nQ ∗ t+1(ϕ t+1 , ~θ t+1 , δ t+1 pp ) = Q ∗ t+1(σ t+1 , ~θ t+1 , δ t+1 ss ). (A.30)\nWe aim to show that at stage t, σt is a sufficient statistic:\nQ ∗ t (ϕ t , ~θ t , δ t pp) = Q ∗ t (σ t , ~θ t , δ t ss). (A.31)\nWe substitute the induction hypothesis into (21):\nQ ∗ t (ϕ t , ~θ t , δ t pp) {21} = R(~θt, δtpp) + ∑\nat∈A\n∑\not+1\nPr(~θt+1|~θt, δtpp)\nQ ∗ t+1(ϕ t+1 , ~θ t+1 , δ t+1∗ pp )\n{A.30} = R(~θt, δtss) + ∑\nat∈A\n∑\not+1\nPr(~θt+1|~θt, δtss)\nQ ∗ t+1(σ t+1 , ~θ t+1 , δ t+1∗ ss ) = Q ∗ t (σ t , ~θ t , δ t ss).\nFurthermore, decision rules δt+1∗1,pp (based on the past joint policy) and δt+1∗1,ss (based on the sufficient statistic) are equal:\nδ t+1∗ 1,pp {(22)} = argmax\nδ t+1 1 ∈∆S 1\nmin δ t+1 2 ∈∆S 2\n[\n∑\n~θt+1∈~Θt+1\nPr(~θt+1|b0, ϕt+1)\nQ ∗ t+1(ϕ t+1 , ~θ t+1 , 〈δt+11 , δ t+1 2 〉)\n]\n{(7)} = argmax\nδ t+1 1 ∈∆S 1\nmin δ t+1 2 ∈∆S 2\n[\n∑\n~θt+1∈~Θt+1\nσ t+1(~θt+1)\nQ ∗ t+1(σ t+1 , ~θ t+1 , 〈δt+11 , δ t+1 2 〉)\n]\n{22} = δt+1∗1,ss .\nAnalogous reasoning holds for δt+1∗2,ss . Thus, by induction, σ t is a sufficient statistic for ϕt, ∀t ∈ 0 . . . h− 1.\nProof of Lemma 3. By the results of Lemma 1 and the results from Section 4.3, we know the best-response value function V BR1h−1 to be linear in ∆(~Θh−11 ). For all other stages, we assume the following induction hypothesis:\nV BR1 t+1 (σ t+1 m,1σ t+1 c,1 , π t+1 2 ) = σ t+1 m,1 · ~ν[σt+1\nc,1 ,π t+1 2\n] . (A.32)\nFor the inductive step we aim to prove that at the current stage t the following holds:\nV BR1 t (σ t m,1σ t c,1, π t 2) = ~σ t m,1 · ~ν[σt\nc,1 ,πt 2 ]. (A.33)\nLet QRt be a function similar to (3) that defines reward for a given statistic and joint decision rule. We expand the definition of V BR1t . For notational convenience we write σt instead of σtm,iσ t c,i, but keep in mind that we consider statistics corresponding to conditional σtc,i.\nV BR1 t (σ t , π t 2) {25} = max\nπt 1 ∈Πt 1\nVt(σ t , 〈πt1, π t 2〉)\n= max πt 1 ∈Πt 1\n[\nQ R t (σ t , 〈δt1, δ t 2〉) + Vt+1(Uss(σ t , δ t), 〈πt+11 , π t+1 2 〉)\n]\n= max δt 1 ∈∆S 1 max π t+1 1\n[\nQ R t (σ t+1 , 〈δt1, δ t 2〉) + Vt+1(σ t+1 , 〈πt+11 , π t+1 2 〉)\n]\n= max δt 1 ∈∆S 1\n[\nQ R t (σ t , 〈δt1, δ t 2〉) +max\nπ t+1 1\n[\nVt+1(σ t+1 , 〈πt+11 , π t+1 2 〉)\n]]\n{25} = max\nδt 1 ∈∆S 1\n[\nQ R t (σ t , 〈δt1, δ t 2〉) + V BR1 t+1 (σ t+1 , 〈πt+11 , π t+1 2 〉)\n]\n.(A.34)\nHere, Uss is the statistic update rule, defined in accordance with (18). We make the decomposition of σt into the marginal and conditional terms explicit again. Immediate reward QRt can be expanded similar to (A.29):\nQ R t (σ t m,1σ t c,1, 〈δ t 1, δ t 2〉) = σ t m,1(~θ t 1)σ t c,1(~θ t 2|~θ t 1)\n∑\nat 1 ∈A1\nδ t 1(a t 1|~θ t 1)\n∑\nat 2 ∈A2\nδ t 2(a t 2|~θ t 2)R(〈~θ t 1, ~θ t 2〉, 〈a t 1, a t 2〉). (A.35)\nWe expand V BR1t+1 using the induction hypothesis in order to bring the marginal distribution σtm,1 to the front:\nV BR1 t+1 (σ t+1 m,1σ t+1 c,1 , π t+1 2 ) {A.32} = σt+1m,1 · ~ν[σt+1\nc,1 ,π t+1 2 ]\n= ∑\n~θ t+1 1 ∈~Θt+1 1\nσ t+1 m,1(~θ t+1 1 )~ν[σt+1\nc,1 ,π t+1 2\n] (~θt+11 )\n{18} = ∑\n~θt 1 ∈~Θt 1\nσ t m,1(~θ t 1)\n∑\n~θt 1 ∈~Θt 2\nσ t c,1(~θ t 2|~θ t 1)\n∑\nat 1 ∈A1\nδ t 1(a t 1|~θ t 1)\n∑\nat 2 ∈A2\nδ t 2(a t 2|~θ t 2)\n∑\not+1∈O1\n∑\not+1∈O2\nPr(ot+11 o t+1 2 |〈~θ t 1, ~θ t 2〉, 〈a t 1, a t 2〉)~ν[σt+1\nc,1 ,π t+1 2\n] (~θt+11 ). (A.36)\nFilling the expanded equations into (A.34) and factorizing gives:\nV BR1 t (σ t m,1σ t c,1, π t 2) {25} = max\nδt 1 ∈∆S 1\n[\nQ R t (σ t m,, 〈δ t 1, δ t 2〉|σ t c,1)+\nV BR1 t+1 (σ t+1 m,1, π t+1 2 |σ t+1 c,1 )\n]\n{A.35,A.36} =\nmax δt 1 ∈∆S 1\n[\n∑\n~θt∈~Θt 1\nσ t m,1(~θ t 1)\n∑\n~θt∈~Θt 2\nσ t c,1(~θ t 2|~θ t 1)\n∑\nat 1 ∈A1\nδ t 1(a t 1|~θ t 1)\n∑\nat 1 ∈A2\nδ t 2(a t 2|~θ t 2)\n(\nR(〈~θt1, ~θ t 2〉, 〈a t 1, a t 2〉)+\n∑\not+1∈O\nPr(ot+1|~θt, at)~ν [σt+1\nc,1 ,π t+1 2\n] (~θt+11 )\n)]\n. (A.37)\nNote that the vector is indexed by the conditional σt+1c,1 . While this conditional is dependent on δt2, it is not dependent on δ t 1, allowing us to remove the maximization over decision rules δt1 from the equation. As a maximization over decision rules conditioned on individual AOH ~θt1 is equal to choosing the maximizing action for each of these AOHs, we can rewrite (A.37) as follows:\nV BR1 t (σ t , π t 2)\n= ∑\n~θt 1 ∈~Θt 1\nσ t m,1(~θ t 1)max\nat 1\n[\n∑\n~θt∈~Θt 2\nσ t c,1(~θ t 2|~θ t 1)\n∑\nat 1 ∈A2\nδ t 2(a t 2|~θ t 2)\n(\nR(~θt, at) + ∑\noFt+1∈O\nPr(ot+1|~θt, at)~ν [σt+1\nc,1 ,π t+1 2\n] (~θt+11 )\n)]\n{26} = ∑\n~θt 1 ∈~Θt 1\nσ t m,1(~θ t 1)~ν[σt\nc,1 ,πt 2 ](~θ t 1) {vec. not.} = ~σtm,1 · ~ν[σt c,1 ,πt 2 ]. (A.38)\nThis corresponds to (A.33). Therefore, by induction, best-response value function V BR1t is linear in ∆(~Θ t 1) for a given σ t c,1 and π t 2, for all stages t = 0 . . . h − 1. Analogously, V BR2t is a linear function in ∆(~Θt2) for a given σ t c,2 and π t 1, for all stages t = 0 . . . h− 1."
    } ],
    "references" : [ {
      "title" : "Optima and equilibria: an introduction to nonlinear analysis, volume 140",
      "author" : [ "Jean-Pierre Aubin" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Optimally solving Dec-POMDPs as continuous-state MDPs",
      "author" : [ "Jilles S. Dibangoye", "Christopher Amato", "Olivier Buffet", "François Charpillet" ],
      "venue" : "Proceedings of the International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Approximate solutions for partially observable stochastic games with common payoffs",
      "author" : [ "R. Emery-Montemerlo", "G. Gordon", "J. Schneider", "S. Thrun" ],
      "venue" : "Proceedings of International Joint Conference on Autonomous Agents and Multi Agent Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Zero-sum stochastic games with partial information",
      "author" : [ "MK Ghosh", "D McDonald", "S Sinha" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Interactive POMDPs: Properties and preliminary results",
      "author" : [ "Piotr J Gmytrasiewicz", "Prashant Doshi" ],
      "venue" : "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems-Volume",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Dynamic Programming for Partially Observable Stochastic Games",
      "author" : [ "Eric A Hansen", "Daniel S. Bernstein", "Shlomo Zilberstein" ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "A double oracle algorithm for zero-sum security games on graphs",
      "author" : [ "Manish Jain", "Dmytro Korzhyk", "Ondřej Vaněk", "Vincent Conitzer", "Michal Pěchouček", "Milind Tambe" ],
      "venue" : "Proceedings of the International Joint Conference on Autonomous Agents and Multi Agent Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Fast algorithms for finding randomized strategies in game trees",
      "author" : [ "Daphne Koller", "Nimrod Megiddo", "Bernhard Von Stengel" ],
      "venue" : "Proceedings of the twenty-sixth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1994
    }, {
      "title" : "Point based value iteration with optimal belief compression for Dec-POMDPs",
      "author" : [ "Liam C. MacDermed", "Charles Isbell" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "The value of two-person zero-sum repeated games with lack of information on both sides",
      "author" : [ "Jean-Francois Mertens", "Shmuel Zamir" ],
      "venue" : "International Journal of Game Theory,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1971
    }, {
      "title" : "Taming Decentralized POMDPs: Towards efficient policy computation for multiagent settings",
      "author" : [ "Ranjit Nair", "Milind Tambe", "Makoto Yokoo", "David Pynadath", "Stacy Marsella" ],
      "venue" : "Proceedings of the International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "Common information based Markov perfect equilibria for Stochastic Games with asymmetric information: Finite Games",
      "author" : [ "Ashutosh Nayyar", "Abhishek Gupta", "Cedric Langbort", "Tamer Basar" ],
      "venue" : "Automatic Control, IEEE Transactions on,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Decentralized stochastic control with partial history sharing: A common information approach",
      "author" : [ "Ashutosh Nayyar", "Aditya Mahajan", "Demosthenis Teneketzis" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Sufficient Plan-Time Statistics for Decentralized POMDPs",
      "author" : [ "Frans A. Oliehoek" ],
      "venue" : "Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Dec-POMDPs as nonobservable MDPs’, IAS technical report IAS-UVA-14-01, Intelligent Systems",
      "author" : [ "Frans A. Oliehoek", "Christopher Amato" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "A Vlassis, ‘Optimal and approximate q-value functions for Decentralized POMDPs.",
      "author" : [ "Frans A Oliehoek", "Matthijs TJ Spaan", "Nikos" ],
      "venue" : "Journal of AI Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Dec-POMDPs and extensive form games: equivalence of models and algorithms",
      "author" : [ "Frans A. Oliehoek", "Nikos Vlassis" ],
      "venue" : "Ias technical report IAS-UVA-06-02,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Anytime point-based approximations for large POMDPs.",
      "author" : [ "Joelle Pineau", "Geoffrey J Gordon", "Sebastian Thrun" ],
      "venue" : "Journal of AI Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Zero-sum games with ”almost” perfect information",
      "author" : [ "Jean-Pierre Ponssard" ],
      "venue" : "Management Science,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1975
    }, {
      "title" : "Some results on zero-sum  games with incomplete information: The dependent case",
      "author" : [ "Jean-Pierre Ponssard", "Sylvain Sorin" ],
      "venue" : "International Journal of Game Theory,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1980
    }, {
      "title" : "Zero-sum sequential games with incomplete information",
      "author" : [ "Jean-Pierre Ponssard", "Shmuel Zamir" ],
      "venue" : "International Journal of Game Theory,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1973
    }, {
      "title" : "Computer poker: A review",
      "author" : [ "Jonathan Rubin", "Ian Watson" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Multiagent systems: Algorithmic, game-theoretic, and logical foundations",
      "author" : [ "Yoav Shoham", "Kevin Leyton-Brown" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Stochastic Games with incomplete information",
      "author" : [ "Sylvain Sorin" ],
      "venue" : "Stochastic Games and applications, eds., Abraham Neyman and Sylvain Sorin,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2003
    }, {
      "title" : "Perseus: Randomized pointbased value iteration for POMDPs.",
      "author" : [ "Matthijs T.J. Spaan", "Nikos Vlassis" ],
      "venue" : "Journal of AI Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2005
    }, {
      "title" : "A theory of sufficient statistics for teams",
      "author" : [ "Jeffrey Wu", "Sanjay Lall" ],
      "venue" : "Proc. of the 53rd Annual Conference on Decision and Control,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Examples include games such as poker [24] and security settings [8].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "Examples include games such as poker [24] and security settings [8].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "We take inspiration from recent work for collaborative settings which has shown that it is possible to summarize the past joint policy using so called plan-time sufficient statistics [15], which",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 1,
      "context" : "uk can be interpreted as the belief of a special type of Partially Observable Markov Decision Process (POMDP) to which the collaborative Decentralized POMDP can be reduced [3, 10, 14].",
      "startOffset" : 172,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "uk can be interpreted as the belief of a special type of Partially Observable Markov Decision Process (POMDP) to which the collaborative Decentralized POMDP can be reduced [3, 10, 14].",
      "startOffset" : 172,
      "endOffset" : 183
    }, {
      "referenceID" : 12,
      "context" : "uk can be interpreted as the belief of a special type of Partially Observable Markov Decision Process (POMDP) to which the collaborative Decentralized POMDP can be reduced [3, 10, 14].",
      "startOffset" : 172,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "This enabled tackling these problems using solution methods for POMDPs, leading to increases in scalability [3].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 22,
      "context" : ", via linear programming [25]) can be used to solve zs-BGs.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "However, such an approach does not scale well — a more efficient solution method is to convert the zs-BG to sequence form [9].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "An alternative is to converting the zs-POSG to an extensive form game (EFG) and solve it in sequence form [9].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "While this is more efficient than the NFG route, it is still intractable: the resulting EFG is huge since its size depends on the number of full histories (trajectories of joint actions, joint observations, and states) [18].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 13,
      "context" : "To show this, the zs-POSG value function in terms of the past joint policy φ can be defined in terms of φ by extending the formulation by Oliehoek [15].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "Note that, even though the final stage is equal to a Family of Bayesian Games, our approach is substantially different from approaches that represent a POSG as a series of BGs [4] and derivative works [17].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "Note that, even though the final stage is equal to a Family of Bayesian Games, our approach is substantially different from approaches that represent a POSG as a series of BGs [4] and derivative works [17].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : "To draw the parallel, many POMDP solution methods exploit the fact that a POMDP value function is piecewise-linear and convex (PWLC) in belief-space [20, 27] (which is similar to the statistic-space we consider), and recently",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 24,
      "context" : "To draw the parallel, many POMDP solution methods exploit the fact that a POMDP value function is piecewise-linear and convex (PWLC) in belief-space [20, 27] (which is similar to the statistic-space we consider), and recently",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : ", Dec-POMDP) case [3, 10].",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : ", Dec-POMDP) case [3, 10].",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "5 Reduction to NOSG The application of methods that exploit the PWLC structure of the value function of Dec-POMDPs was enabled by a reduction from Dec-POMDP to a non-observable MDP (NOMDP), which is a special type of (centralized) POMDP [3, 10, 14, 16].",
      "startOffset" : 237,
      "endOffset" : 252
    }, {
      "referenceID" : 8,
      "context" : "5 Reduction to NOSG The application of methods that exploit the PWLC structure of the value function of Dec-POMDPs was enabled by a reduction from Dec-POMDP to a non-observable MDP (NOMDP), which is a special type of (centralized) POMDP [3, 10, 14, 16].",
      "startOffset" : 237,
      "endOffset" : 252
    }, {
      "referenceID" : 12,
      "context" : "5 Reduction to NOSG The application of methods that exploit the PWLC structure of the value function of Dec-POMDPs was enabled by a reduction from Dec-POMDP to a non-observable MDP (NOMDP), which is a special type of (centralized) POMDP [3, 10, 14, 16].",
      "startOffset" : 237,
      "endOffset" : 252
    }, {
      "referenceID" : 14,
      "context" : "5 Reduction to NOSG The application of methods that exploit the PWLC structure of the value function of Dec-POMDPs was enabled by a reduction from Dec-POMDP to a non-observable MDP (NOMDP), which is a special type of (centralized) POMDP [3, 10, 14, 16].",
      "startOffset" : 237,
      "endOffset" : 252
    }, {
      "referenceID" : 13,
      "context" : "The proposed plan-time statistics for Dec-POMDPs [15] precisely correspond to the belief in the centralized model.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "We do not provide the full background of the reduction for the Dec-POMDP case, but refer to [16].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "In order to avoid potential confusion, let us point out that a zsPOSG can also be converted to a best response POMDP by fixing the policies of one agent [12], which leads to a model where the information state b(s, θj) is a distribution over states and AOHs of the other agent.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "[12] leads to a single-agent model that can be used to compute a best-response, our conversion leads to a multi-agent model that can be used to compute a Nash equilibrium directly.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "A key contribution of our NOSG formulation is that it directly indicates that properties of ‘zero-sum stochastic games with shared observations’ [5] also hold for zs-POSGs.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "[5] show that, under some technical assumptions, a zsSOSG can be converted into a completely observable model (similar to the conversion of a POMDP into a belief MDP), and that, in the infinite-horizon case, both the value and a rational joint policy exists.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "However, a straightforward extension of our reduction for the case where the agents use finite-state controllers (analogous to such formulations for Dec-POMDPs [10]), would satisfy these requirements, and as such we can infer the existence of a value for such games: Corollary 1.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "The concave and convex structure we have found for the zs-POSG value function is similar to the saddle point structure associated with min-max equilibria [1].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "[13] who introduce a so-called Common Information Based Conditional Belief — a probability distribution over AOHs and the state conditioned on common information — and use it to design a dynamic-programming approach for zs-POSGs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[7] present a dynamic-programming approach for finite-horizon (general sum) POSGs that iteratively constructs sets of one-step-longer (pure) policies for all agents.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 25,
      "context" : "A more generalized investigation of individual statistics in decentralized settings is given by Wu & Lall [28].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "There are works from game theory literature that present structural results on the value function of so-called ‘repeated zero-sum games with incomplete information’ [11, 21, 22, 23].",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "There are works from game theory literature that present structural results on the value function of so-called ‘repeated zero-sum games with incomplete information’ [11, 21, 22, 23].",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : "There are works from game theory literature that present structural results on the value function of so-called ‘repeated zero-sum games with incomplete information’ [11, 21, 22, 23].",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 20,
      "context" : "There are works from game theory literature that present structural results on the value function of so-called ‘repeated zero-sum games with incomplete information’ [11, 21, 22, 23].",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 20,
      "context" : "For various flavors of such games, it has been shown that the value function has a concave/convex structure: cases with incomplete information on one side [23, 26], and cases with incomplete information on both sides where ‘observations are independent’ (i.",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "For various flavors of such games, it has been shown that the value function has a concave/convex structure: cases with incomplete information on one side [23, 26], and cases with incomplete information on both sides where ‘observations are independent’ (i.",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 18,
      "context" : ", where the distribution over joint types is a product of individual type distributions) [21] or dependent (general joint type distributions) [11, 22].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : ", where the distribution over joint types is a product of individual type distributions) [21] or dependent (general joint type distributions) [11, 22].",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 19,
      "context" : ", where the distribution over joint types is a product of individual type distributions) [21] or dependent (general joint type distributions) [11, 22].",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "A game-theoretic model that is closely related to the POSG model is the Interactive POMDP or I-POMDP [6].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "[12] (discussed in Section 5), with the difference that the state also represents which policy the other agent uses.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "We formally defined the value function for both types of games in terms of an information distribution called the sufficient plan-time statistic: a probability distribution over joint sets of private information (originally used in the collaborative setting [15]).",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 8,
      "context" : "We hope that this result leads to solution methods that exploit the structure of the value function at every stage, as recently such developments have been made in the field of cooperative multi-agent problems [10].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 8,
      "context" : "In particular, we believe that heuristic methods that identify useful (conditional) statistics to explore, or point-based methods that iteratively select statistics to evaluate [10, 27] may be adapted for the zs-POSG case.",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 24,
      "context" : "In particular, we believe that heuristic methods that identify useful (conditional) statistics to explore, or point-based methods that iteratively select statistics to evaluate [10, 27] may be adapted for the zs-POSG case.",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "The proof is largely identical to the proof of correctness of sufficient statistics in the collaborative setting [15].",
      "startOffset" : 113,
      "endOffset" : 117
    } ],
    "year" : 2016,
    "abstractText" : "Zero-sum stochastic games provide a rich model for competitive decision making. However, under general forms of state uncertainty as considered in the Partially Observable Stochastic Game (POSG), such decision making problems are still not very well understood. This paper makes a contribution to the theory of zero-sum POSGs by characterizing structure in their value function. In particular, we introduce a new formulation of the value function for zs-POSGs as a function of the ‘plan-time sufficient statistics’ (roughly speaking the information distribution in the POSG), which has the potential to enable generalization over such information distributions. We further delineate this generalization capability by proving a structural result on the shape of value function: it exhibits concavity and convexity with respect to appropriately chosen marginals of the statistic space. This result is a key pre-cursor for developing solution methods that may be able to exploit such structure. Finally, we show how these results allow us to reduce a zs-POSG to a ‘centralized’ model with shared observations, thereby transferring results for the latter, narrower class, to games with individual (private) observations.",
    "creator" : "LaTeX with hyperref package"
  }
}