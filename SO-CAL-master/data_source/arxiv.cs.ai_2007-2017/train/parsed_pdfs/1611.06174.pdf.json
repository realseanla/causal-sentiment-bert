{
  "name" : "1611.06174.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Jesse Davis", "Steven Schockaert" ],
    "emails" : [ "KuzelkaO@cardiff.ac.uk", "jesse.davis@cs.kuleuven.be", "SchockaertS1@cardiff.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The use of symbolic representations for encoding generative models is appealing, as they can be more transparent than black-box models such as neural networks. One notable example of such a symbolic framework is Markov logic networks (MLNs [5]), which use weighted first-order formulas to encode probabilistic models. These logical formulas can give us some important insights into what regularities have been identified in a given dataset. Unfortunately, the weights that are used in MLNs can be hard to interpret, as their intuitive meaning depends on the interplay with the weights of other formulas [6, 2]. This means that we cannot readily use MLNs to generate explanations for predictions (as the formulas influence cannot be interpreted in isolation), and this makes it difficult for domain experts to directly verify or improve learned MLNs.\nDensity estimation trees [4] encode probabilistic models in a symbolic way by explicitly defining sets of possible worlds (i.e. those that satisfy all the literals associated with a given branch of the tree) that have the same probability (i.e. the value that is associated with the leaf of that branch). Since each branch can be interpreted separately from the rest of the tree, a domain expert can readily verify whether a learned model is reasonable. However, the use of a tree structure means that relationships between variables are not encoded explicitly. For example, it may be that branches where a is true and b is false always have a very low probability, but this can only be verified by inspecting a large number of branches.\nThe approach we propose starts by converting a density estimation tree into a stratified logical theory, and then uses different forms of pruning to obtain an (often significantly) more compact theory, in which relationships between variables such as “if a then typically ¬b” are modelled explicitly [3]. This method is explained in more detail in the next section."
    }, {
      "heading" : "2 Logical encoding of density estimation trees",
      "text" : "We can straightforwardly transform a density estimation tree into a weighted logical theory, containing one formula for each branch of the tree. This transformation is illustrated in Figure 2. The density estimation tree on the left-hand side encodes, among others, that every world in which the\n∗This extended abstract describes our recent work [2, 3] from the perspective of interpretable machine learning.\nar X\niv :1\n61 1.\n06 17\n4v 1\n[ cs\n.A I]\n1 8\nN ov\natoms Bird and Flies are false has a probability of 0.25. The representation on the right-hand side of Figure 2 encodes the same knowledge using possibilistic logic [1]. Specifically, each branch of the density tree can be seen as a conjunction of literals; e.g. the left-most branch encodes the conjunction ¬Bird ∧ ¬Flies. For the conjunction α and the probability p associated with each branch in the tree, the possibilistic logic theory on the right-hand side contains the weighted formula (¬α, 1− p). For example, the left-most branch has α = ¬Bird ∧ ¬Flies and p = 0.25, and thus the weighted formula (Bird ∨ Flies, 0.75) appears in the theory. In general, a possibilistic logic theory {(α1, λ1), ..., (αn, λn)}, with each αi a formula and each λi a certainty weight (0 < λi ≤ 1), encodes a distribution π over possible worlds, defined for a possible world ω as π(ω) = 1 − λi, with λi the highest weight of a formula αi that is violated by ω, i.e. π(ω) = 1−max{λi |ω 6|= αi}. For a possible world ω, π(ω) is called the possibility degree of ω. Thus each weighted formula (αi, λi) can be seen as encoding an upper bound of 1− λi on the value of π(ω) for all worlds ω that violate αi.\nPossibility theory does not impose any particular meaning on the possibility degrees. Note in particular that π is typically not a probability distribution. In fact, in many applications, possibility degrees are interpreted in a purely ordinal fashion. A possibilistic logic theory is then simply seen as a ranking of formulas, i.e. a stratified knowledge base (SKB), which induces a ranking on possible worlds. However, when constructing possibilistic logic theories from density estimation trees, as in the example from Figure 2, the associated distribution π is actually a probability distribution. We will refer to possibilistic logic theories that induce a probability distribution as stratified probabilistic knowledge bases (SPKBs). We will use the terms SPKB and SKB to highlight whether we talk about possibilistic logic theories that induce a probability distribution or merely a ranking of possible worlds. As we will see in the next section, among others, SKBs can be used for maximum a posteriori (MAP) inference, while SPKBs can additionally be used for computing marginal probabilities.\nPruning An important advantage of possibilistic logic representations, over density estimation trees, is that we can use logical inference to derive representations which are more compact, and which encode the regularities underlying the data more explicitly. In particular, let us write Bλ for the set of classical formulas in a possibilistic logic theory B whose weight is at least λ. If a formula α can be derived from Bλ then B and B ∪ {(α, λ} are equivalent, in the sense that they induce the same possibility distribution π. Consequently, a standard SAT solver can be used to remove weighted formulas that are redundant and to simplify the remaining formulas (i.e. remove redundant literals). For example, the possibilistic logic theory from Figure 2 can be equivalently written as:\nB′ = {(¬Bird ∨ Antarctic ∨ Flies, 1), (¬Bird ∨ ¬Antarctic ∨ ¬Flies, 1), (¬Bird ∨ ¬Antarctic, 0.9375), (¬Flies ∨ Bird, 0.875), (¬Bird, 0.8125)}\nwhere e.g. (¬Bird ∨ ¬Antarctic ∨ Flies, 0.9375) was rewritten as (¬Bird ∨ ¬Antarctic, 0.9375), because we also have the formula (¬Bird ∨ ¬Antarctic ∨ ¬Flies, 1). In particular, because ¬Bird ∨ ¬Antarctic∨¬Flies and¬Bird∨¬Antarctic∨Flies imply¬Bird∨¬Antarctic, this means that (¬Bird∨ ¬Antarctic, 0.9375) can be added to B, after which the original formula (¬Bird ∨ ¬Antarctic ∨ Flies, 0.9375) becomes redundant. While the change between B and B′ is perhaps not drastic, we have shown in [3] that (in extreme cases) this method can lead to theories which are exponentially more compact than the initial density estimation tree. Moreover, apart from this exact form of logical simplification, there are also several approximate ways in which we can further simplify the possibilistic logic theory, such that the possibility distribution induced by the resulting theory stays\nclose to the distribution induced by the initial tree. For example, one effective strategy consists in merging the levels with the highest certainty degrees, whose only effect is that the probabilities of the least probable worlds can no longer be differentiated, which often does not matter much in applications. Experimental results in [3] show that thus reducing the possibilistic logic theories to 10% of their initial size usually does not lead to significantly lower predictive accuracies for MAPinference.\nInteraction with domain experts Another important advantage of possibilistic logic representations is that they can be easily modified by human users. It is then often convenient to convert the formulas into implications1, e.g.:\nB′′ = {(Bird ∧ ¬Antarctic→ Flies, 1), (Bird ∧ Antarctic→ ¬Flies, 1), (Bird→ ¬Antarctic, 0.9375), (Flies→ Bird, 0.875), (¬Bird, 0.8125)}\nWe may have an expert telling us that we should not be deriving with full certainty that Antarctic birds do not fly, as e.g. there are albatrosses living in Antarctica. It is easy to see that we can achieve this by simply removing the rule (Bird ∧ Antarctic → ¬Flies, 1), although some bookkeeping is needed to check whether any earlier pruning steps need to be revised. Similarly, we may have an expert telling us that the relative certainty of two formulas should be reversed. For example, if the expert tells us that it is more common to see an Antarctic bird than it is to see a flying creature that is not a bird, this tells us that the formula Flies→ Bird should appear with a higher certainty degree than the formula Bird → ¬Antarctic. In ordinal settings this is straightforward, but in SPKBs it requires us to relearn suitable certainty degrees, subject to the ranking constraint provided by the expert. In [3] we show how such weights can be found using geometric programming."
    }, {
      "heading" : "3 Inference",
      "text" : "Inference with SKBs From an SKB we can only obtain an ordering on possible worlds. However, if the SKB orders the possible worlds according to their probability, this is enough to evaluate MAP queries of the form “what formulas are true in the most probable models of α”. In fact, it turns out that MAP inference corresponds to a standard technique for inconsistency-tolerant reasoning in possibilistic logic. In particular, to verify whether a formula β is true in all the highest-ranked models of α, w.r.t. a possibilistic logic theory B, we can proceed as follows. First we add (α, 1) to B. Then we find the minimal weight w ∈ [0, 1] such that the set of formulas which appear with a weight that is strictly higher than w forms a consistent set Bα of classical formulas. It can be shown that a formula β holds in all the highest-ranked models of α iff it can be derived from Bα. If that is the case, we say that β is MAP-entailed by α and write α `B β. Note that we can check whether α `B β using a logarithmic number of calls to a SAT solver (in the number of different certainty degrees appearing in B). We now illustrate the procedure using the following toy example:\nB = {(Gardener→ ¬HayFever, 0.9), (Coughs→ HayFever, 0.8)}.\nIt holds that HayFever `B ¬Gardener, which follows from the fact that {HayFever,Gardener → ¬HayFever,Coughs→ HayFever} is consistent (i.e. the minimal weight w that ensures consistency is 0), and HayFever and Gardener → ¬HayFever entail ¬Gardener. We also have Gardener ∧ Coughs `B ¬HayFever, which can be seen as follows. First, if we add (Gardener∧Coughs, 1) to B then it becomes inconsistent as a propositional logic theory because Gardener implies ¬HayFever but Coughs implies HayFever, which is a contradiction. Following the aforementioned procedure, we find w = 0.8 and accordingly remove the rule (Coughs → HayFever, 0.8) and obtain Bα = {Gardener→ ¬HayFever,Gardener ∧ Coughs}, from which we can indeed derive ¬HayFever. Beyond MAP inference, we can also derive e.g. what is true in the top 20% most probable models of α using a model counter. Let w1, ..., wk = 1 be the certainty weights appearing in B ∪ {(α, 1)} that are strictly higher than the cut-off value w from the previous procedure. Let Ai be the set of formulas appearing with weight wi. A formula can be derived from Al ∪ ... ∪Ak iff it is true in the\n1When converting a clause C into an implication, we select the antecedent α and consequent β (where C = ¬α ∨ β) so that β would be true in all most probable worlds where α is true. In other words, using terminology of Section 3, we select α and β so that α would MAP-entail β, i.e. so that α would be sufficient evidence to derive β using MAP-inference. In cases when this is not possible, we leave C in the clausal form.\ntop θ% most probable models of α with θ = M(Al∪...∪Ak)M(α) , where for a set of formulas E we write M(E) for its number of models. This type of query is very natural, yielding conclusions which are similar in spirit to statements such as “90% of adult men weigh between 70 and 100kg”, “80% adult men weigh between 75 and 95 kg”, etc. Indeed, as we can consider nested intervals of increasingly typical weights, we can similarly characterize formulas that specify increasingly typical possible worlds.\nInference with SPKBs SPKBs support a number of additional query types. For example, for a probability threshold t, the worlds whose probability is greater than t are those which satisfy all formulas that appear in the considered SPKB B with a weight w ≥ 1 − t. Moreover, based on this insight, we can use a SPKB B to compute the marginal probability P (α) of any propositional formula. In particular, we can show that [3]:\nP (α) = k∑ i=1 (1− wi) · (M(Bi+1))−M(Bi))\nwhere w1, ..., wk are the certainty degrees appearing in B (sorted in increasing order); for i ≤ k, Bi is the set of formulas that appear in B ∪ {(α, 1)} with weight at least wi, while Bk+1 = {α}; M(.) again represents the model count operator."
    }, {
      "heading" : "4 Conclusions and future work",
      "text" : "Stratified (probabilistic) knowledge bases can encode the information captured by density estimation trees in a logical form. This leads to several important advantages for interpretable machine learning. First, SKBs can be used to generate logical justifications (proofs) to support the predictions that are made from them. Moreover the explicit symbolic nature of SKBs allows users, with a basic understanding of propositional logic, to relatively easily modify SKBs. These properties make SKBs stand out not only when compared to black-box models such as neural networks, but even when compared to other symbolic methods such as Markov logic networks.\nSeveral interesting directions for future work remain. For example, in [2] we have shown that, for any probability distribution, it is possible to construct a compact SKB which exactly captures MAPinference with evidence sets of bounded size. The techniques presented in [2] can naturally be applied for summarizing what can be derived from density estimation trees when only small evidence sets are considered. In practice, this can lead to even more compact and more easily interpretable SKBs, providing a more local view of the corresponding distributions. Another interesting direction would be to lift our approach into the first-order setting, similar to how we lifted the method in [2] to encode MLNs. Finally, note that while we have only considered binary properties in this paper, density estimation trees can also handle continuous variables, which means that adding support for continuous variables in our setting is relatively straightforward."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work has been supported by a grant from the Leverhulme Trust (RPG-2014-164). Jesse Davis is partially supported by the KU Leuven Research Fund (C22/15/015), and FWO-Vlaanderen (G.0356.12, SBO-150033)."
    } ],
    "references" : [ {
      "title" : "Possibilistic logic",
      "author" : [ "D. Dubois", "J. Lang", "H. Prade" ],
      "venue" : "D. N. D. Gabbay, C. Hogger J. Robinson, editor, Handbook of Logic in Artificial Intelligence and Logic Programming, volume 3, pages 439–513. Oxford University Press",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Encoding markov logic networks in possibilistic logic",
      "author" : [ "O. Kuželka", "J. Davis", "S. Schockaert" ],
      "venue" : "Proc. UAI",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Interpretable encoding of densities using possibilistic logic",
      "author" : [ "O. Kuželka", "J. Davis", "S. Schockaert" ],
      "venue" : "Proc. ECAI, pages 1239–1247",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Density estimation trees",
      "author" : [ "P. Ram", "A.G. Gray" ],
      "venue" : "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627–635. ACM",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Markov logic networks",
      "author" : [ "M. Richardson", "P. Domingos" ],
      "venue" : "Mach. learn., 62(1-2):107–136",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Coherence and compatibility of markov logic networks",
      "author" : [ "M. Thimm" ],
      "venue" : "Proc. ECAI, pages 891–896",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "One notable example of such a symbolic framework is Markov logic networks (MLNs [5]), which use weighted first-order formulas to encode probabilistic models.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "Unfortunately, the weights that are used in MLNs can be hard to interpret, as their intuitive meaning depends on the interplay with the weights of other formulas [6, 2].",
      "startOffset" : 162,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "Unfortunately, the weights that are used in MLNs can be hard to interpret, as their intuitive meaning depends on the interplay with the weights of other formulas [6, 2].",
      "startOffset" : 162,
      "endOffset" : 168
    }, {
      "referenceID" : 3,
      "context" : "Density estimation trees [4] encode probabilistic models in a symbolic way by explicitly defining sets of possible worlds (i.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "The approach we propose starts by converting a density estimation tree into a stratified logical theory, and then uses different forms of pruning to obtain an (often significantly) more compact theory, in which relationships between variables such as “if a then typically ¬b” are modelled explicitly [3].",
      "startOffset" : 300,
      "endOffset" : 303
    }, {
      "referenceID" : 1,
      "context" : "The density estimation tree on the left-hand side encodes, among others, that every world in which the ∗This extended abstract describes our recent work [2, 3] from the perspective of interpretable machine learning.",
      "startOffset" : 153,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "The density estimation tree on the left-hand side encodes, among others, that every world in which the ∗This extended abstract describes our recent work [2, 3] from the perspective of interpretable machine learning.",
      "startOffset" : 153,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "The representation on the right-hand side of Figure 2 encodes the same knowledge using possibilistic logic [1].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "While the change between B and B′ is perhaps not drastic, we have shown in [3] that (in extreme cases) this method can lead to theories which are exponentially more compact than the initial density estimation tree.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Experimental results in [3] show that thus reducing the possibilistic logic theories to 10% of their initial size usually does not lead to significantly lower predictive accuracies for MAPinference.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "In [3] we show how such weights can be found using geometric programming.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "Then we find the minimal weight w ∈ [0, 1] such that the set of formulas which appear with a weight that is strictly higher than w forms a consistent set Bα of classical formulas.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "In particular, we can show that [3]:",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "For example, in [2] we have shown that, for any probability distribution, it is possible to construct a compact SKB which exactly captures MAPinference with evidence sets of bounded size.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "The techniques presented in [2] can naturally be applied for summarizing what can be derived from density estimation trees when only small evidence sets are considered.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Another interesting direction would be to lift our approach into the first-order setting, similar to how we lifted the method in [2] to encode MLNs.",
      "startOffset" : 129,
      "endOffset" : 132
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we advocate the use of stratified logical theories for representing probabilistic models. We argue that such encodings can be more interpretable than those obtained in existing frameworks such as Markov logic networks. Among others, this allows for the use of domain experts to improve learned models by directly removing, adding, or modifying logical formulas.",
    "creator" : "LaTeX with hyperref package"
  }
}