{
  "name" : "1503.07220.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Individual Planning in Agent Populations: Exploiting Anonymity and Frame-Action Hypergraphs",
    "authors" : [ "Ekhlas Sonu", "Yingke Chen", "Prashant Doshi" ],
    "emails" : [ "esonu@uga.edu,", "ykchen@uga.edu,", "pdoshi@cs.uga.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n07 22\n0v 2\n[ cs\n.M A\n] 2\nA pr\n2 01"
    }, {
      "heading" : "Introduction",
      "text" : "We focus on the decision-making problem of an individual agent operating in the presence of other self-interested agents whose actions may affect the state of the environment and the subject agent’s rewards. In stochastic and partially observable environments, this problem is formalized by the interactive POMDP (I-POMDP) (Gmytrasiewicz and Doshi 2005). I-POMDPs cover an important portion of the multiagent planning problem space (Seuken and Zilberstein 2008; Doshi 2012), and applications in diverse areas such as security (Ng et al. 2010; Seymour and Peterson 2009), robotics (Wang 2013; Woodward and Wood 2012), ad hoc teams (Chandrasekaran et al. 2014) and human behavior modeling (Doshi et al. 2010; Wunder et al. 2011) testify to its wide appeal while critically motivating better scalability.\nPrevious I-POMDP solution approximations such as interactive particle filtering (Doshi and Gmytrasiewicz 2009), point-based value iteration (Doshi and Perez 2008) and interactive bounded policy iteration (IBPI) (Sonu and Doshi 2014) scale I-POMDP solutions to larger physical state, observation and model spaces. Hoang and Low (2013) introduced the specialized IPOMDP Lite framework that promotes efficiency by\nCopyright c© 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nmodeling other agents as nested MDPs. However, to the best of our knowledge no effort specifically scales I-POMDPs to many interacting agents – say, a population of more than a thousand – sharing the environment.\nFor illustration, consider the decision-making problem of the police when faced with a large protest. The degree of the police response is often decided by how many protestors of which type (disruptive or not) are participating. The individual identity of the protestor within each type seldom matters. This key observation of frame-action anonymity motivates us in how we model the agent population in the planning process. Furthermore, the planned degree of response at a protest site is influenced, in part, by how many disruptive protestors are predicted to converge at the site and much less by some other actions of protestors such as movement between other distant sites. Therefore, police actions depend on just a few actions of note for each type of agent.\nThe example above illustrates two known and powerful types of problem structure in domains involving many agents: action anonymity (Roughgarden and Tardos 2002) and context-specific independence (Boutilier et al. 1996). Action anonymity allows the exponentially large joint action space to be substituted with a much more compact space of action configurations where a configuration is a tuple representing the number of agents performing each action. Context-specific independence (wherein given a context such as the state and agent’s own action, not all actions performed by other agents are relevant) permits the space of configurations to be compressed by projecting counts over a limited set of others’ actions. We extend both action anonymity and context-specific independence to allow considerations of an agent’s frame as well. 1 We list the specific contributions of this paper below: 1. I-POMDPs are severely challenged by large numbers\nof agents sharing the environment, which cause an exponential growth in the space of joint models and actions. Exploiting problem structure in the form of frameaction anonymity and context-specific independence, we present a new method for considerably scaling the solution of I-POMDPs to an unprecedented number of\n1I-POMDPs distinguish between an agent’s frame and type with the latter including beliefs as well. Frames are similar in semantics to the colloquial use of types.\nagents. 2. We present a systematic way of modeling the prob-\nlem structure in transition, observation and reward functions, and integrating it in a simple method for solving IPOMDPs that models other agents using finite-state machines and builds reachability trees given an initial belief.\n3. We prove that the Bellman equation modified to include action configurations and frame-action independences continues to remain optimal given the I-POMDP with explicated problem structure.\n4. Finally, we theoretically verify the improved savings in computational time and memory, and empirically demonstrate it on a new problem of policing protest with over a thousand protestors.\nThe above problem structure allows us to emphatically mitigate the curse of dimensionality whose acute impact on I-POMDPs is well known. However, it does not lessen the impact of the curse of history. In this context, an additional step of sparse sampling of observations while generating the reachability tree allows sophisticated planning with a population of 1,000+ agents using about six hours."
    }, {
      "heading" : "Related Work",
      "text" : "Building on graphical games (Kearns, Littman, and Singh 2001), action graph games (AGG) (Jiang, Leyton-Brown, and Bhat 2011) utilize problem structures such as action anonymity and context-specific independence to concisely represent single shot complete-information games involving multiple agents and to scalably solve for Nash equilibrium. The independence is modeled using a directed action graph whose nodes are actions and an edge between two nodes indicates that the reward of an agent performing an action indicated by one node is affected by other agents performing action of the other node. Lack of edges between nodes encodes the context-specific independence where the context is the action. Action anonymity is useful when the action sets of agents overlap substantially. Subsequently, the vector of counts over the set of distinct actions, called a configuration, is much smaller than the space of action profiles.\nWe substantially build on AGGs in this paper by extending anonymity and context-specific independence to include agent frames, and generalizing their use to a partially observable stochastic game solved using decisiontheoretic planning as formalized by I-POMDPs. Indeed, Bayesian AGGs (Jiang and Leyton-Brown 2010) extend the original formulation to include agent types. These result in type-specific action sets with the benefit that the action graph structure does not change although the number of nodes grows with types: |Θ̂||A| nodes for agents with |Θ̂| types each having same |A| actions. If two actions from different type-action sets share a node, then these actions are interchangeable. A key difference in our representation is that we explicitly model frames in the graphs due to which context-specific independence is modeled using frame-action hypergraphs. Benefits are that we naturally maintain the distinction between two similar actions but performed by agents of different frames, and we add less additional nodes: |Θ̂| + |A|. However, a hypergraph\nis a more complex data structure for operation. Temporal AGGs (Jiang, Leyton-Brown, and Pfeffer 2009) extend AGGs to a repeated game setting and allow decisions to condition on chance nodes. These nodes may represent the action counts from previous step (similar to observing the actions in the previous game). Temporal AGGs come closest to multiagent influence diagrams (Koller and Milch 2001) although they can additionally model the anonymity and independence structure. Overall, I-POMDPs with frame-action anonymity and context-specific independence significantly augment the combination of Bayesian and temporal AGGs by utilizing the structures in a partially observable stochastic game setting with agent types.\nVarakantham et al. (2014) building on previous work (Varakantham et al. 2012) recently introduced a decentralized MDP that models a simple form of anonymous interactions: rewards and transition probabilities specific to a state-action pair are affected by the number of other agents regardless of their identities. The interaction influence is not further detailed into which actions of other agents are relevant (as in action anonymity) and thus configurations and hypergraphs are not used. Furthermore, agent types are not considered. Finally, the interaction hypergraphs in networked-distributed POMDPs (Nair et al. 2005) model complete reward independence between agents – analogous to graphical games – which differs from the hypergraphs in this paper (and action graphs) that model independence in reward (and transition, observation probabilities) along a different dimension: actions.\nBackground Interactive POMDPs allow a self-interested agent to plan individually in a partially observable stochastic environment in the presence of other agents of uncertain types. We briefly review the I-POMDP framework and refer the reader to (Gmytrasiewicz and Doshi 2005) for further details.\nA finitely-nested interactive I-POMDP for an agent (say agent 0) of strategy level l operating in a setting inhabited by one of more other interacting agents is defined as the following tuple:\nI-POMDP0,l = 〈IS0,l, A, T0,Ω0, O0, R0, OC0〉\n• IS0,l denotes the set of interactive states defined as, IS0,l = S × ∏N j=1Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪\nSMj}, for l ≥ 1, and ISi,0 = S, where S is the set of physical states. Θj,l−1 is the set of computable, intentional models ascribed to agent j: θj,l−1 = 〈bj,l−1, θ̂j〉, where bj,l−1 is agent j’s level l − 1 belief, bj,l−1 ∈ △(ISj,l−1), and θ̂j △ = 〈A, Tj ,Ωj , Oj , Rj , OCj〉, is j’s frame. Here, j is assumed to be Bayes-rational. At level 0, bj,0 ∈ △(S) and a level-0 intentional model reduces to a POMDP. SMj is the set of subintentional models of j, an example is a finite state automaton;\n• A = A0 ×A1 × . . .×AN is the set of joint actions of all agents;\n• T0 : S × A0 × ∏N\nj=1 Aj × S → [0, 1] is the transition function;\n• Ω0 is the set of agent 0’s observations; • O0 : S×A0 × ∏N\nj=1Aj ×Ω0 → [0, 1] is the observation function;\n• R0 : S×A0 × ∏N j=1 Aj → R is the reward function; and • OC0 is the optimality criterion, which is identical to that for POMDPs. In this paper, we consider a finite-horizon optimality criteria. Besides the physical state space, the I-POMDP’s interactive state space contains all possible models of other agents. In its belief update, an agent has to update its belief about the other agents’ models based on an estimation about the other agents’ observations and how they update their models. As the number of agents sharing the environment grows, the size of the joint action and joint model spaces increases exponentially. Therefore, the memory requirement for representing the transition, observation and reward functions grows exponentially as well as the complexity of performing belief update over the interactive states. In the context ofN agents, interactive bounded policy iteration (Sonu and Doshi 2014) generates good quality solutions for an agent interacting with 4 other agents (total of 5 agents) absent any problem structure. To the best of our knowledge, this result illustrates the best scalability so far to N > 2 agents.\nMany-Agent I-POMDP To facilitate understanding and experimentation, we introduce a pragmatic running example that also forms our evaluation domain.\nExample 1 (Policing Protest) Consider a policing scenario where police (agent 0) must maintain order in 3 geographically distributed and designated protest sites (labeled 0, 1, and 2) as shown in Fig. 1. A population of N agents are protesting at these sites. Police may dispatch one or two riot-control troops to either the same or different locations. Protests with differing intensities, low, medium and high (disruptive), occur at each of the three sites. The goal of the police is to deescalate protests to the low intensity at each site. Protest intensity at any site is influenced by the number of protestors and the number of police troops at that location. In the absence of adequate policing, we presume that\nthe protest intensity escalates. On the other hand, two police troops at a location are adequate for de-escalating protests."
    }, {
      "heading" : "Factored Beliefs and Update",
      "text" : "As we mentioned previously, the subject agent in an IPOMDP maintains a belief over the physical state and joint models of other agents, b0,l ∈ ∆(S × ∏N j=1Mj,l−1), where ∆(·) is the space of probability distributions. For settings such as Example 1 where N is large, the size of the interactive state space is exponentially larger, |IS0,l| = |S||Mj,l−1|\nN , and the belief representation unwieldy. However, the representation becomes manageable for large N if the belief is factored:\nb0,l(s,m1,l−1,m2,l−1, . . . ,mN,l−1) = Pr(s) Pr(m1,l−1|s)\n× Pr(m2,l−1|s)× . . .× Pr(mN,l−1|s) (1)\nThis factorization assumes conditional independence of models of different agents given the physical state. Consequently, beliefs that correlate agents may not be directly represented although correlation could be alternately supported by introducing models with a correlating device.\nThe memory consumed in storing a factored belief is O(|S| + N |S||M∗j |), where |M ∗ j | is the size of the largest model space among all other agents. This is linear in the number of agents, which is much less than the exponentially growing memory required to represent the belief as a joint distribution over the interactive state space, O(|S||M∗j | N ).\nGiven agent 0’s belief at time t, bt0,l, its action a t 0 and the subsequent observation it makes, ωt+10 , the updated belief at time step t+ 1, bt+10,l , may be obtained as:\nPr(st+1,mt+11,l−1, . . . ,m t+1 N,l−1|b t 0,l, a t 0, ω t+1 0 ) = Pr(s t+1|\nbt0,l, a t 0, ω t+1 0 ) Pr(m t+1 1,l−1|s t+1,mt+12,l−1, . . . ,m t+1 N,l−1,\nbt0,l, a t 0, ω t+1 0 )× . . .× Pr(m t+1 N,l−1|s t+1, bt0,l, a t 0, ω t+1 0 )\n(2)\nEach factor in the product of Eq. 2 may be obtained as follows. The update over the physical state is:\nPr(st+1|bt0,l, a t 0, ω t+1 0 ) ∝ Pr(s t+1, ωt+10 |b t 0,l, a t 0)\n= ∑\nst\nbt0,l(s t)\n∑\nmt−0\nbt0,l(m t 1,l−1|s t)× . . .× bt0,l(m t N,l−1|s t)\n× ∑\nat−0\nPr(at1|m t 1,l−1)× . . .× Pr(a t N |m t N,l−1)\n×Ot+10 (s t+1, 〈at0, a t −0〉, ω t+1 0 ) T0(s t, 〈at0, a t −0〉, s t+1) (3)\nand the update over the model of each other agent, j =\n1 . . .N , conditioned on the state at t+ 1 is:\nPr(mt+1j,l−1|s t+1,mt+1j+1,l−1, . . . ,m t+1 N,l−1, b t 0,l, a t 0, ω t+1 0 ) =\n∑\nst\nbt0(s t)\n∑\nmt−j,l−1\nbt0,l(m t 1,l−1|s t)× . . .× bt0,l(m t N,l−1|s t)\n∑\nat−j\nPr(at1|m t 1,l−1)× . . .× Pr(a t n|m t N,l−1)\n∑\nω t+1 j\nOj(s t+1, 〈aj , a t −j〉, ω t+1 j ) Pr(m t+1 j |m t j , a t j , ω t+1 j )\n(4)\nDerivations of Eqs. 3 and 4 are straightforward and not given here due to lack of space. In particular, note that models of agents other than j at t+1 do not impact j’s model update in the absence of correlated behavior. Thus, under the assumption of a factored prior as in Eq. 1 and absence of agent correlations, the I-POMDP belief update may be decomposed into an update of the physical state and update of the models of N agents conditioned on the state."
    }, {
      "heading" : "Frame-Action Anonymity",
      "text" : "As noted by Jiang et al. (2011), many noncooperative and cooperative problems exhibit the structure that rewards depend on the number of agents acting in particular ways rather than which agent is performing the act. This is particularly evident in Example 1 where the outcome of policing largely depends on the number of protestors that are peaceful and the number that are disruptive. Building on this, we additionally observe that the transient state of the protests and observations of the police at a site are also largely influenced by the number of peaceful and disruptive protestors moving from one location to another. This is noted in the example below:\nExample 2 (Frame-action anonymity of protestors) The transient state of protests reflecting the intensity of protests at each site depends on the previous intensity at a site and the number of peaceful and disruptive protestors entering the site. Police (noisily) observes the intensity of protest at each site which is again largely determined by the number of peaceful and disruptive protestors at a site. Finally, the outcome of policing at a site is contingent on whether the protest is largely peaceful or disruptive. Consequently, the identity of the individual protestors beyond their frame and action is disregarded.\nHere, peaceful and disruptive are different frames of others in agent 0’s I-POMDP, and the above definition may be extended to any number of frames. Frame-action anonymity is an important attribute of the above domain. We formally define it in the context of agent 0’s transition, observation and reward functions next:\nDefinition 1 (Frame-action anonymity) Let ap−0 be a joint action of all peaceful protestors and ad−0 be a joint action of all disruptive ones. Let ȧp−0 and ȧ d −0 be permutations of the two joint action profiles, respectively. An I-POMDP models frame-action anonymity iff for any a0, s, s′, a p −0 and a d −0: T0(s, a0, a p −0, a d −0, s ′) = T0(s, a0, ȧ p −0, ȧ d −0, s ′),\nO0(s ′, a0, a p −0, a d −0, ω0) = O0(s ′, a0, ȧ p −0, ȧ d −0, ω0), and R0(s, a0, a p −0, a d −0) = R0(s, a0, ȧ p −0, ȧ d −0) ∀ ȧ p −0, ȧ d −0.\nRecall the definition of an action configuration, C, as the vector of action counts of an agent population. A permutation of joint actions of others, say ȧp−0, assigns different actions to individual agents. Despite this, the fact that the transition and observation probabilities, and the reward remains unchanged indicates that the identity of the agent performing the action is irrelevant. Importantly, the configuration of the joint action and its permutation stays the same: C(ap−0) = C(ȧ p −0). This combined with Def. 1 allows redefining the transition, observation and reward functions to be over configurations as: T0(s, a0, C(a p −0), C(a d −0), s\n′), O0(s ′, a0, C(a p −0), C(a d −0), o) and R0(s, a0, C(a p −0), C(a d −0 )). Let Ap1, . . . , A p n be overlapping sets of actions of n peaceful protestors, and Ap−0 is the Cartesian product of these sets. Let C(Ap−0) be the set of all action configurations for A p −0. Observe that multiple joint actions from A p −0 may result in a single configuration; these joint actions are configuration equivalent. Consequently, the equivalence partitions the joint action set Ap−0 into |C(A p −0)| classes. Furthermore, when other agents of same frame have overlapping sets of actions, the number of configurations could be much smaller than the number of joint actions. Therefore, definitions of the transition, observation and reward functions involving configurations could be more compact."
    }, {
      "heading" : "Frame-Action Hypergraphs",
      "text" : "In addition to frame-action anonymity, domains involving agent populations often exhibit context-specific independences. This is a broad category and includes the contextspecific independence found in conditional probability tables of Bayesian networks (Boutilier et al. 1996) and in action-graph games. It offers significant additional structure for computational tractability. We begin by illustrating this in the context of Example 1.\nExample 3 (Context-specific independence in policing) At a low intensity protest site, reward for the police on passive policing is independent of the movement of the protestors to other sites. The transient intensity of the protest at a site given the level of policing at the site (context) is independent of the movement of protestors between other sites.\nThe context-specific independence above builds on the similar independence in action graphs in two ways: (i) We model such partial independence in the transitions of factored states and in the observation function as well, in addition to the reward function. (ii) We allow the contextspecific independence to be mediated by the frames of other agents in addition to their actions. For example, the rewards received from policing a site is independent of the number of protestors at another site, instead the rewards are influenced by the number of peaceful and disruptive protestors present at that site.\nThe latter difference generalizes the action graphs into frame-action hypergraphs, and specifically 3-uniform hyper-\ngraphs where each edge is a set of 3 nodes. We formally define it below:\nDefinition 2 (Frame-action hypergraph) A frame-action hypergraph for agent 0 is a 3-uniform hypergraph G = 〈Ψ, A−0, Θ̂−0, E〉, where Ψ is a set of nodes that represent the context, A−0 is a set of action nodes with each node representing an action that any other agent may take; Θ̂−0 is a set of frame nodes, each node representing a frame ascribed to an agent, and E is a 3-uniform hyperedge containing one node from each set Ψ, A−0, and Θ̂−0, respectively.\nBoth context and action nodes differ based on whether the hypergraph applies to the transition, observation or reward functions: • For the transition function, the context is the set of all\npairs of states between which a transition may occur and each action of agent 0, Ψ = S × A0 × S, and the action nodes includes actions of all other agents, A−0 = ⋃N j=1 Aj . Neighbors of a context node ψ = 〈s, a0, s\n′〉 are all the frame-action pairs that affect the probability of the transition. An edge (〈 s, a0, s′〉, a−0, θ̂) indicates that the probability of transitioning from s to s′ on performing a0 is affected (in part) by the other agents of frame θ̂ performing the particular action in A−0.\n• The context for agent 0’s observation function is the stateaction-observation triplet, Ψ = S × A0 × Ω0, and the action nodes are identical to those in the transition function. Neighbors of a context node, 〈s, a0, ω0〉, are all those frame-action pairs that affect the observation probability. Specifically, an edge (〈s, a0, ω0〉, a−0, θ̂) indicates that the probability of observing ω0 from state s on performing a0 is affected (in part) by the other agents performing action, a−0, who possess frame θ̂.\n• For agent 0’s reward function, the context is the set of pairs of state and action of agent 0, Ψ = S × A0, and the action nodes the same as those in transition and observation functions. An edge (〈 s, a0〉, a−0, θ̂−0) in this hyper-\ngraph indicates that the reward for agent 0 on performing action a0 at state s is affected (in part) by the agents of frame θ̂−0 who perform action in A−0. We illustrate a general frame-action hypergraph for contextspecific independence in a transition function and a reward function as bipartite Levi graphs in Figs. 2(a) and (b), respectively. We point out that the hypergraph for the reward function comes closest in semantics to the graph in action graph games (Jiang, Leyton-Brown, and Bhat 2011) although the former adds the state to the context and frames. Hypergraphs for the transition and observation functions differ substantially in semantics and form from action graphs.\nTo use these hypergraphs in our algorithms, we first define the general frame-action neighborhood of a context node.\nDefinition 3 (Frame-action neighborhood) The frameaction neighborhood of a context node ψ ∈ Ψ, ν(ψ), given a frame-action hypergraph G is defined as a subset of A × Θ̂ such that ν(ψ) = {(a−0, θ̂)|a−0 ∈ A−0, θ̂ ∈ Θ̂, (ψ, a−0, θ̂) ∈ E}.\nAs an example, the frame-action neighborhood of a stateaction pair, 〈s, a0〉 in a hypergraph for the reward function is the set of all action and frame nodes incident on each hyperedge anchored by the node 〈s, a0〉.\nWe move toward integrating frame-action anonymity introduced in the previous subsection with the context-specific independence as modeled above by introducing frameaction configurations.\nDefinition 4 (Frame-action configuration) A configuration over the frame-action neighborhood of a context node, ψ, given a frame-action hypergraph is a vector,\nCν(ψ) △ = 〈 C(Aθ̂1−0), C(A θ̂2 −0), . . . , C(A θ̂|Θ̂| −0 ), C(φ) 〉\nwhere each a included in Aθ̂−0 is an action in ν(ψ) with frame θ̂, and C(Aθ̂−0) is a configuration over actions by\nagents other than 0 whose frame is θ̂. All agents with frames other than those in the frame-action neighborhood are assumed to perform a dummy action, φ.\nDefinition 4 allows further inroads into compacting the transition, observation and rewards functions of the I-POMDP using context-specific independence. Specifically, we may redefine these functions one more time to limit the configurations only over the frame-action neighborhood of the context as, T0(s, a0, Cν(s,a0,s\n′), s′), O0(s ′, a0, C ν(s′,a0,ω0), ω0) and R0(s, a0, Cν(s,a0)). 2"
    }, {
      "heading" : "Revised Framework",
      "text" : "To benefit from structures of anonymity and context-specific independence, we redefine I-POMDP for agent 0 as:\nI-POMDP0,l = 〈IS0,l, A,Ω0, T0,O0,R0, OC0〉\nwhere:\n2Context in our transition function is 〈s, a0, s′〉 compared with the context of just 〈s, a0〉 in Varakantham et al’s (2014) transitions.\n• IS0,l, A, Ω0 and OC0 remain the same as before. The physical states are factored as, S =\n∏K k=1Xk.\n• T0 is the transition function, T0(x, a0, Cν(x,a0,x ′), x′)\nwhere Cν(x,a0,x ′) is the configuration over the frameaction neighborhood of context 〈x, a0, x′〉 obtained from a hypergraph that holds for the transition function. This transition function is significantly more compact than the original that occupies space O(|X |2|A0||A−0|N ) compared to the O(|X |2|A0|( N|ν∗| ) |ν∗|) of T0, where the fraction is the complexity of (\nN+|ν|∗+1 |ν∗|+1 )\n, |ν∗| is the maximum cardinality of the neighborhood of any context, and ( N|ν∗| ) |ν∗| ≪ |A−0| N . The value ( N+|ν|∗ |ν|∗ )\nis obtained from combinatorial compositions and represents the number of ways |ν∗| + 1 non-negative values can be weakly composed such that their sum is N .\n• The redefined observation function is O0(x ′, a0, C ν(x′,a0,ω0), ω0) where Cν(x\n′,a0,ω0) is the configuration over the frame-action neighborhood of context 〈x′, a0, ω0〉 obtained from a hypergraph that holds for the observation function. Analogously to the transition function, the original observation function consumes space O(|X ||Ω||A0||A−0|N ), which is much larger than space O(|X ||Ω||A0|( N|ν∗| )\n|ν∗|) occupied by this redefinition. • R0 is the reward function defined as R0(x, a0, Cν(x,a0)) where Cν(x,a0) is defined analogously to the configurations in the previous parameters. The reward for a state and actions may simply be the sum of rewards for the state factors and actions (or a more general function if needed). As with the transition and observation functions, this reward function is compact occupying space O(|X ||A0|( N|ν∗| ) |ν∗|) that is much less than\nO(|X ||A0||A−0| N ) of the original.\nBelief Update For this extended I-POMDP, we compute the updated belief over a physical state as a product of its factors using Eq. 5 and belief update over the models of each other agent using Eq 6 as shown below:\nPr(st+1|bt0,l, a t 0, ω t+1 0 ) ∝\n{\n∑\nst\nbt0,l(s t)\nK ∏\nk=1\n∑\nCν(x t+1 k ,at 0 ,ω t+1 0 )\nPr(Cν(x t+1 k ,at0,ω t+1 0 )|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t))\nO0(x t+1 k , a t 0, C\nν(xt+1 k ,at0,ω t+1 0 ), ωt+10 )\n}\n×\n{\n∑\nst\nbt0,l(s t)\nK ∏\nk=1 ∑\nCν(x t k ,at0,x t+1 k )\nPr(Cν(x t k,a t 0,x t+1 k\n)|bt0,l(M1,l−1|s t), . . . ,\nbt0,l(MN,l−1|s t))T0(x t k, C\nν(xtk,a t 0,x t+1 k\n), xt+1k )\n}\n(5)\nHere, the term, Pr(Cν(x t+1 k ,at0,ω t+1 0 )|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t)), is the probability of a frame-action configuration (see Def. 4) that is context specific to the triplet,\n〈xt+1, a0, ω t+1〉. It is computed from the factored beliefs over the models of all others. We discuss this computation in the next section. The second configuration term has an analogous meaning and is computed similarly.\nThe factored belief update over the models of each other agent, j = 1 . . .N , conditioned on the state at t+1 becomes:\nPr(mt+1j,l−1|s t+1,mt+1−j,l−1, b t 0,l, a t 0) =\n∑\nst\nbt0(s t) ∑\nmtj\nbt0(m t j\n|st) ∑\nat j\nPr(atj |m t j)\n∑\nC ν(xt+1,at j ,ωj)\nPr(Cν(x t+1,atj ,ωj)|\nbt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s\nt)) ∑\no t+1 j\nOj(x t+1,\natj , C ν(xt+1,atj ,ωj), ωt+1j ) Pr(m t+1 j |m t j , a t j, ω t+1 j ) (6)\nProofs for obtaining Eqs. 5 and 6 are omitted due to space restrictions. Notice that the distributions over configurations are computed using distributions over other agents’ models. Therefore, we must maintain and update conditional beliefs over other agents’ models. Hence, the problem cannot be reduced to a POMDP by including configurations with physical states.\nValue Function The finite-horizon value function of the many-agent I-POMDP continues to be the sum of agent 0’s immediate reward and the discounted expected reward over the future:\nV h(mt0,l) = max at0∈A0 ER0(b t 0,l, a t 0)+\nγ ∑\nω t+1 0\nPr(ωt+10 |b t 0,l, a t 0)V h−1(mt+10,l ) (7)\nwhere ER0(bt0,l, a t 0) is the expected immediate reward of agent 0 and γ is the discount factor. In the context of the redefined reward function of the many-agent I-POMDP framework in this section, the expected immediate reward is obtained as:\nER0(b t 0,l, a t 0) =\n∑\nst\nbt0,l(s t)\n( K ∑\nk=1\n∑\nCν(x t k ,at0)\nPr(Cν(x t k,a t 0)|\nbt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t))R0(x t k, a t 0, C\nν(xtk,a t 0)\n)\n(8)\nwhere the outermost sum is over all the state factors, st = 〈xt1, . . . , x t K〉, and the term, Pr(Cν(x t k,a\nt 0)|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t)) de-\nnotes the probability of a frame-action configuration that is context-specific to the factor, xtk. Importantly, Proposition 1 establishes that the Bellman equation above is exact. The proof is given in the extended version of this paper (Sonu, Chen, and Doshi 2015).\nProposition 1 (Optimality) The dynamic programming in Eq. 7 provides an exact computation of the value function for the many-agent I-POMDP."
    }, {
      "heading" : "Algorithms",
      "text" : "We present an algorithm that computes the distribution over frame-action configurations and outline our simple method for solving the many-agent I-POMDP defined previously."
    }, {
      "heading" : "Distribution Over Frame-Action Configurations",
      "text" : "Algorithm 1 generalizes an algorithm by Jiang and Lleyton-Brown (2011) for computing configurations over actions given mixed strategies of other agents to include frames and conditional beliefs over models of other agents. It computes the probability distribution of configurations over the frame-action neighborhood of an action given the belief over the agents’ models: Pr(Cν(x,a0,ω0)|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t)) and Pr(Cν(x,a0,x ′)|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t)) in Eq. 5, Pr(Cν(x,ωj)|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t)) in Eq. 6, and Pr(Cν(x,a0)|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t)) in Eq. 8.\nAlgorithm 1 Computing Pr(Cν(·)|b0,l(M1,l−1|s), . . . , b0,l(MN,l−1|s))\nInput: ν(·), 〈b0,l(M1,l−1|s), . . . , b0,l(MN,l−1|s)〉 Output: A trie Pn representing distribution over the frameaction configurations over ν(·)\n1: Initialize c0 ← (0, . . . , 0), one value for each frameaction pair in ν(·) and for φ. Insert into empty trie P0 2: Initialize P0[c0] ← 1 3: for j ← 1 to N do 4: Initialize Pj to be an empty trie 5: for all cj−1 from Pj−1 do 6: for all mj,l−1 ∈Mj,l−1 do 7: for all aj ∈ Aj such that Pr(aj |mj,l−1) > 0 do 8: cj ← cj−1 9: if 〈aj , θ̂j〉 ∈ ν(·) then 10: cj [aj ] ← cj [aj ] + 1 11: else"
    }, {
      "heading" : "12: cj [φ] ← cj [φ] + 1",
      "text" : "13: if Pj[cj ] does not exist then 14: Initialize Pj [cj ] ← 0 15: Pj [cj ] ← Pj [cj ] + Pj−1[cj−1] × Pr(aj |mj,l−1)× b0,l(mj,l−1|s) 16: return Pn\nAlgorithm 1 adds the actions of each agent one at a time. A Trie data structure enables efficient insertion and access of the configurations. We begin by initializing the configuration space for 0 agents (P0) to contain one tuple of integers (c0) with |ν| + 1 0s and assign its probability to be 1 (lines 1-2). Using the configurations of the previous step, we construct the configurations over the actions performed by j agents by adding 1 to a relevant element depending on j’s action and frame (lines 3-15). If an action aj performed by j with frame m̂j is in the frame-action neighborhood ν(·), then we increment its corresponding count by 1. Otherwise, it is considered as a dummy action and the count of φ is in-\ncremented (lines 9-12). Similarly, we update the probability of a configuration using the probability of aj and that of the base configuration cj−1 (line 15). This algorithm is invoked multiple times for different values of ν(·) as needed in the belief update and value function computation.\nWe utilize a simple method for solving the many-agent IPOMDP given an initial belief: each other agent is modeled using a finite-state controller as part of the interactive state space. A reachability tree of beliefs as nodes is projected for as many steps as the horizon (using Eqs. 5 and 6) and value iteration (Eq. 7) is performed on the tree. In order to mitigate the curse of history due to the branching factor that equals the number of agent 0’s actions and observations, we utilize the well-known technique of sampling observations from the propagated belief and obtain a sampled tree on which value iteration is run to get a policy. Action for any observation that does not appear in the sample is that which maximizes the immediate expected reward.\nComputational Savings The complexity of accessing an element in a ternary search trie is Θ(ν). The maximum number of configurations encountered at any iteration is upper bounded by total number of configurations for N agents, i.e. O(( N|ν∗| )\n|ν∗|). The complexity of Algorithm 1 is polynomial in N , O(N |M∗j ||A ∗ j ||ν ∗|( N|ν∗|) |ν∗|) where M∗j and A ∗ j are largest sets of models and actions for any agent. For the traditional I-POMDP belief update, the complexity of computing Eq. 3 is O(|S||M∗j | N |A∗j |\nN ) and that for computing Eq. 4 is O(|S||M∗j | N |A∗j | N |Ω∗j |) where ∗ denotes the maximum cardinality of a set for any agent. For a factored representation, belief update operator invokes Eq. 3 for each value of all state factors and it invokes Eq. 4 for each model of each agent j and for all values of updated states. Hence the total complexity of belief update is O(N |M∗j ||S| 2|M∗j | N |A∗j |\nN |Ω∗j |). The complexity of computing updated belief over state factor xt+1 using Eq. 5 is O(|S|NK|M∗j ||A ∗ j ||ν ∗|( N|ν∗| ) |ν∗|) (recall the complexity of Algorithm 1). Similarly, the complexity of computing updated model probability using Eq. 6 is O((|S|N |M∗j ||A ∗ j ||ν ∗| + |Ω∗j |)( N |ν∗| )\n|ν∗|). These complexity terms are polynomial in N for small values of |ν∗| as opposed to exponential in N as in Eqs. 3 and 4. The overall complexity of belief update is also polynomial in N .\nComplexity of computing the immediate expected reward in the absence of problem structure is O(|S|K|M∗j | N |A∗j | N ). On the other hand, the complexity of computing expected reward using Eq. 8 is O(|S|KN |M∗j ||A ∗ j ||ν ∗|( N|ν∗| ) |ν∗|), which is again polynomial in N for low values of |ν∗|. These complexities are discussed in greater detail in (Sonu, Chen, and Doshi 2015).\nExperiments We implemented a simple and systematic I-POMDP solving technique that computes reachable beliefs over the finite horizon and then calculates the optimal value at the\nroot node using the Bellman equation for the Many-Agents I-POMDP framework. We evaluate its performance in the aforementioned non-cooperative policing protest scenario (|S| = 27, |A0| = 9, |Aj | = 4, |Oj| = 8, |Oi| = 8). We model the other agents as POMDPs and solve them using bounded policy iteration (Poupart and Boutilier 2003), representing the models as finite state controllers. This representation enables us to have a compact model space. We set the maximum planning horizon to 4 throughout the experiments. The frame-action hypergraphs are encoded into the transition, observation and reward functions of the ManyAgent I-POMDP (Fig. 3). All computations are carried out on a RHEL platform with 2.80 GHz processor and 4 GB memory.\nTo evaluate the computational gain obtained by exploiting problem structures, we implemented a solution algorithm similar to the one described earlier that does not exploit any problem structure. A comparison of the Many-Agent I-POMDP with the original I-POMDP yields two important results: (i) When there are few other agents, the ManyAgent I-POMDP provides exactly the same solution as the original I-POMDP but with reduced running times by exploiting the problem structure. (ii) Many-Agent I-POMDP scales to larger agent populations, from 100 to 1,000+, and the new framework delivers promising results within reasonable time.\nIn the first setting, we consider up to 5 protestors with different frames. As shown in Table 1, both the traditional and the Many-Agent I-POMDP produce policies with the same expected value. However, as the Many-Agent I-POMDP losslessly projects joint actions to configurations, it requires much less running time.\nOur second setting considers a large number of protestors, for which the traditional I-POMDP does not scale. Instead, we first scale up the exact solution method using ManyAgent I-POMDP to deal with a few hundreds of other agents. Although the exploitation of the problem structures reduces the curse of dimensionality that plagues I-POMDPs, the curse of history is unaffected by such approaches. To mitigate the curse of history we use the well-known observation sampling method (Doshi and Gmytrasiewicz 2009), which allows us to scale to over 1,000 agents in a reasonable time of 4.5 hours as we show in Fig. 4(a). This increases to about 7 hours if we extend the horizon to 4 as shown in Fig. 4(b).\nConclusion The key contribution of the Many-Agent I-POMDP is its scalability beyond 1,000 agents by exploiting problem structures. We formalize widely existing problem structures – frame-action anonymity and context-specific independence – and encode it as frame-action hypergraphs. Other real-world examples exhibiting such problem structure are found in economics where the value of an asset depends on the num-\nber of agents vying to acquire it and their financial standing (frame), in real estate where the value of a property depends on its demand, the valuations of neighboring properties as well as the economic status of the neighbors because an upscale neighborhood is desirable. Compared to the previous best approach (Sonu and Doshi 2014), which scales to an extension of the simple tiger problem involving 5 agents only, the presented framework is far more scalable in terms of number of agents. Our future work includes exploring other types of problem structures and developing approximation algorithms for this I-POMDP. An integration with existing multiagent simulation platforms to illustrate the behavior of agent populations may be interesting."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is supported in part by a NSF CAREER grant, IIS-0845036, and a grant from ONR, N000141310870. We thank Brenda Ng for valuable feedback that led to improvements in the paper."
    }, {
      "heading" : "Games and Economic Behavior 71(1):141–173.",
      "text" : "[Jiang, Leyton-Brown, and Pfeffer 2009] Jiang, A. X.; Leyton-Brown, K.; and Pfeffer, A. 2009. Temporal actiongraph games: A new representation for dynamic games. In Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI), 268–276.\n[Kearns, Littman, and Singh 2001] Kearns, M.; Littman, M.; and Singh, S. 2001. Graphical models for game theory. In Uncertainty in Artificial Intelligence (UAI), 253–260.\n[Koller and Milch 2001] Koller, D., and Milch, B. 2001. Multi-agent influence diagrams for representing and solving games. In IJCAI, 1027–1034.\n[Nair et al. 2005] Nair, R.; Varakantham, P.; Tambe, M.; and Yokoo, M. 2005. Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs. In\nTwentieth AAAI Conference on Artificial Intelligence, 133– 139.\n[Ng et al. 2010] Ng, B.; Meyers, C.; Boakye, K.; and Nitao, J. 2010. Towards applying interactive POMDPs to real-world adversary modeling. In Innovative Applications in Artificial Intelligence (IAAI), 1814–1820.\n[Poupart and Boutilier 2003] Poupart, P., and Boutilier, C. 2003. Bounded finite state controllers. In Neural Information Processing Systems.\n[Roughgarden and Tardos 2002] Roughgarden, T., and Tardos, E. 2002. How bad is selfish routing? Journal of ACM 49(2):236–259.\n[Seuken and Zilberstein 2008] Seuken, S., and Zilberstein, S. 2008. Formal models and algorithms for decentralized decision making under uncertainty. Journal of Autonomous Agents and Multiagent Systems 17(2):190–250.\n[Seymour and Peterson 2009] Seymour, R., and Peterson, G. L. 2009. A trust-based multiagent system. In IEEE International Conference on Computational Science and Engineering, 109–116.\n[Sonu and Doshi 2014] Sonu, E., and Doshi, P. 2014. Scalable solutions of interactive POMDPs using generalized and bounded policy iteration. Journal of Autonomous Agents and Multi-Agent Systems DOI: 10.1007/s10458–014–9261–5, in press.\n[Sonu, Chen, and Doshi 2015] Sonu, E.; Chen, Y.; and Doshi, P. 2015. Individual planning in agent populations: Exploiting anonymity and frame-action hypergraphs. Technical Report http://arxiv.org/abs/1503.07220, arXiv.\n[Varakantham, Adulyasak, and Jaillet 2014] Varakantham, P.; Adulyasak, Y.; and Jaillet, P. 2014. Decentralized stochastic planning with anonymity in interactions. In AAAI Conference on Artificial Intelligence, 2505–2511.\n[Varakantham et al. 2012] Varakantham, P.; Cheng, S.; Gordon, G.; and Ahmed, A. 2012. Decision support for agent populations in uncertain and congested environments. In Uncertainty in artificial intelligence (UAI), 1471–1477.\n[Wang 2013] Wang, F. 2013. An I-POMDP based multiagent architecture for dialogue tutoring. In International Conference on Advanced ICT and Education (ICAICTE-13), 486–489.\n[Woodward and Wood 2012] Woodward, M. P., and Wood, R. J. 2012. Learning from humans as an i-pomdp. CoRR abs/1204.0274.\n[Wunder et al. 2011] Wunder, M.; Kaisers, M.; Yaros, J.; and Littman, M. 2011. Using iterated reasoning to predict opponent strategies. In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 593–600."
    }, {
      "heading" : "Appendix",
      "text" : "Factored Belief Update:\nb0,l(s t+1,mt+11 , . . . ,m t+1 n )\n= Pr(st+1,mt+11 , . . . ,m t+1 n |b t 0, a t 0, ω t+1 0 )\n= Pr(st+1|bt0, a t 0, ω t+1 0 )× Pr(m t+1 1 |s t+1, bt0, a t 0, ω t+1 0 )\n× · · · × Pr(mt+1n |s t+1,mt+11 , . . . ,m t+1 n−1b t 0, a t 0, ω t+1 0 )\nDerivation of equation 5: Starting with equation 3, we have:\nPr(st+1|bt0,l, a t 0, ω t+1 0 ) ∝ Pr(s t+1, ωt+10 |b t 0,l, a t 0)\n= Pr(st+1|bt0,l, a t 0)Pr(ω t+1 0 |s t+1, bt0,l, a t 0)\nThe update term for physical states may be represented as a product of its factors such that for any factor Xk:\nPr(xt+1k |b t 0,l, a t 0) =\n∑\nst\nbt0,l(s t)\n∑\nmt−0\nbt0,l(m t −0|s t)×\n∑\nat−0\nPr(at−0|m t −0) T0(x t k, 〈a t 0, a t −0〉, x t+1 k )\nwhere bt0,l(m t −0|s t) = bt0,l(m t 1|s t)× . . .× bt0,l(m t N |s t), and Pr(at−0|m t −0) = Pr(a t 1|m t 1)× . . .× Pr(a t N |m t N ).\nWe introduce a projection function δν(ψ) that maps joint actions to the corresponding frame-action configurations as defined in definition 4. Formally δν(ψ) : a → Cν(ψ), where C ν(ψ) is the set of all possible configurations such that for\nall agents j with frame θ̂, C(a, θ̂) = |{j : aj = a, θ̂j = θ̂, (aj , θ̂) ∈ ν(ψ)}|.\nNext we partition the set of joint action of all other agents A−0 into smaller subsets A1−0, . . . , A |Cν(ψ)| −0 such that the projection function δν(ψ) maps all joint actions belonging to any given partition Ac−0 to the same value configuration. Hence, we may rewrite the above equation as:\nPr(xt+1k |b t 0,l, a t 0) =\n∑\nst\nbt0,l(s t)\n|Cν(x t k ,at0,x t+1 k\n)| ∑\nc=1\n∑\nmt−0\nbt0,l(m t −0|s\nt) ∑\nat−0∈A c −0\nPr(at−0|m t −0)\nT0(x t k, 〈a t 0, a t −0〉, x t+1 k )\nUnder frame-action anonymity and frame-action independence, for all joint actions at−0 ∈ A c −0 T0(x t k, 〈a t 0, a t −0〉, x t+1 k ) = T0(x t k, a t 0, C ν(xt,at0,x t+1), xt+1k ), where Cν(x t k,a t 0,x t+1 k ) = δν(x t k,a t 0,x t+1 k\n)(at−0).\nPr(xt+1k |b t 0,l, a t 0) =\n∑\nst\nbt0,l(s t)\n|Cν(x t k ,at0,x t+1 k\n)| ∑\nc=1\n∑\nmt−0\nbt0,l(m t −0|s\nt) ∑\nat−0∈A c −0\nPr(at−0|m t −0)\nT0(x t k, a t 0, C\nν(xtk,a t 0,x t+1 k\n), xt+1k )\nThe cumulative probability of joint actions mapping to the same configuration, ∑\nmt−0\nbt0,l(m t −0|s t)\n∑\nat−0∈A c −0\nPr(at−0|m t −0), is computed tractably using algo-\nrithm 1 as Pr(Cν(x t k,a t 0,x t+1 k\n)|b0,l(M t 1|s t), . . . b0,l(M t n|s t)). Hence, the equation becomes:\nPr(xt+1k |b t 0,l, a t 0) =\n∑\nst\nbt0,l(s t)\n∑\nCν(x t k ,at0,x t+1 k )\nPr(Cν(x t k,a t 0,x t+1 k )|\nb0,l(M t 1|s t), . . . b0,l(M t n|s t)) × T0(x t, at0, C\nν(xtk,a t 0,x t+1 k\n), xt+1)\nSimilarly, the observation probability may also be obtained in a factored form and Cν(s t+1,at0,ω t+1 0 ) = δν(s t+1,at0,ω\nt+1 0 )(at−0) may be substitued instead of the joint\naction.\nPr(ωt+10 |s t+1, bt0,l, a t 0) =\n∑\nst\nbt0,l(s t)\nK ∏\nk=1\n∑\nCν(x t+1 k ,at0,ω t+1 0 )\nPr(Cν(x t+1 k ,at0,ω t+1 0 )|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t))\nO0(x t+1 k , a t 0, C\nν(xt+1 k ,at0,ω t+1 0 ), ωt+10 )\nTherefore, we may rewrite equation 3 as follows:\nPr(st+1|bt0,l, a t 0, ω t+1 0 ) ∝\n{\n∑\nst\nbt0,l(s t)\nK ∏\nk=1\n∑\nCν(x t+1 k ,at 0 ,ω t+1 0 )\nPr(Cν(x t+1 k ,at0,ω t+1 0 )|bt0,l(M1,l−1|s t), . . . , bt0,l(MN,l−1|s t))\nO0(x t+1 k , a t 0, C\nν(xt+1 k ,at0,ω t+1 0 ), ωt+10 )\n}\n×\n{\n∑\nst\nbt0,l(s t)\nK ∏\nk=1 ∑\nCν(x t k ,at 0 ,x t+1 k )\nPr(Cν(x t k,a t 0,x t+1 k\n)|bt0,l(M1,l−1|s t), . . . ,\nbt0,l(MN,l−1|s t))T0(x t k, C\nν(xtk,a t 0,x t+1 k\n), xt+1k )\n}\n(9)\nDerivation of equation 6: The belief update over the models of agent j shown in equa-\ntion 4 can be rewritten as follows:\nPr(mt+1j,l−1|s t+1,mt+1j+1,l−1, . . . ,m t+1 N,l−1, b t 0,l, a t 0, ω t+1 0 )\n= ∑\nst\nbt0(s t)\n∑\nmt1,l−1\nbt0,l(m t 1,l−1|s\nt) ∑\nat1\nPr(at1|m t 1,l−1)\n. . . ∑\nmt j−1,l−1\nbt0,l(m t j−1,l−1|s\nt) ∑\nat j−1\nPr(atj−1|m t j−1,l−1)\n∑\nmt j,l−1\nbt0,l(m t j,l−1|s\nt) ∑\nat j\nPr(atj |m t j,l−1)\n∑\nmt j+1,l−1\nbt0,l(m t j+1,l−1|s\nt) ∑\nat j+1\nPr(atj+1|m t j,l−1) . . .\n∑\nmt N,l−1\nbt0,l(m t N,l−1|s\nt) ∑\nat N\nPr(atN |m t N,l−1)\n∑\nω t+1 j\nOj(s t+1, 〈aj , a t −j〉, ω t+1 j ) Pr(m t+1 j |m t j , a t j, ω t+1 j )\nSubstituting frame-action configuration as in equation 9, we get:\nPr(mt+1j,l−1|s t+1,mt+1j+1,l−1, . . . ,m t+1 N,l−1, b t 0,l, a t 0, ω t+1 0 )\n= ∑\nst\nbt0(s t) ∑\nmt j\nbt0(m t j |s\nt) ∑\nat j\nPr(atj |m t j)\n∑\nC ν(xt+1,at j ,ωj)\nPr(Cν(x t+1,atj ,ωj)|at0, b t 0,l(M1|s t), . . . , bt0,l(Mj−1|s t),\nbt0,l(Mj+1|s t), . . . bt0,l(mN |s\nt))× ∑\no t+1 j\nOj(x t+1, atj,\nCν(x t+1,ωj), ωt+1j ) Pr(m t+1 j |m t j , a t j , ω t+1 j ) (10)\nWhere the probability over the configurations is computed as in algorithm 1 using belief over models of all other agents except j. In the end we add 1 to the count of action a0 in every configuration.\nComplexity of belief update: For the traditional I-POMDP belief update, the complexity of computing equation 3 is O(|S|(|M∗j |) N (|A∗j |) N ) and that for computing equation 4 is O(|S||M∗j | N |A∗j |\nN |Ω∗j |) where ∗ denotes the maximum cardianlity of a set for any agent. For factored representation, belief update operator invokes equation 3 for each value of all state factors and it invokes equation 4 for each model of each agent j and for all values of updated states. Hence the total complexity of belief update is O(K|X∗||S||M∗j | N |A∗j | N +N |M∗j ||S| 2(|M∗j |) N |A∗j | N |Ω∗j |).\nIn equation 5, algorithm is called once for all values of st. The two inner summations iterate over all possible configurations over transition and observation contexts. The number of configurations is upper bounded by (\nN+|ν∗| |ν∗| )\nwhere |ν∗| is the maximum cardinality of the frame-action neighborhood for any context. Hence the complexity of computing updated belief of state factor xt+1 using equation 5 is O(|S|K × {N |M∗j ||A ∗ j ||ν ∗| ( N+|ν∗| |ν∗| ) + ( N+|ν∗| |ν∗| )\n}) (recall the complexity of algorithm 1). Similarly, the complexity of computing updated model probability using equation 6 is\nO(|S|×{N |M∗j ||A ∗ j ||ν ∗| ( N+|ν∗| |ν∗| ) +|Ω∗j | ( N+|ν∗| |ν∗| )\n}). These complexity terms are polynomial in N for small values of |ν∗| as opposed to exponential in N as in equations 3 and 4. The overall complexity of belief update is also polynomial in N .\nProof of Proposition 1: The expected reward of agent 0 is obtained as the sum of reward factors.\nER0(b t 0,l, a t 0) =\n∑\nst\nbt0,l(s t)\n( K ∑\nk=1\n∑\nmt−0\nbt0,l(m t 1|s t)× . . .×\nbt0,l(m t N |s\nt)× ∑\nat−0\nPr(at1|m t 1,l−1)× . . .× Pr(a t N |m t N,l−1)\nR0(x t k, 〈a t 0, a t −0〉)\n)\nComplexity of computing expected reward using the above equation is O(|S|K(|M∗j |) N (|A∗j |) N ). Equation 8 is derived similarly to the belief update by substituting distribution over frame-action configurations for distributions over joint models and joint actions. This combined with the proofs for Eqs. 5 and 6 allow us to obtain Eq.7 from the Bellman equation of the original I-POMDP.\nThe complexity of computing expected reward using equation 8 is O(|S|K{N |M∗j ||A ∗ j ||ν ∗|+1} ( N+|ν∗| |ν∗| )\n) which is again polynomial in N for low values of |ν∗|."
    } ],
    "references" : [ {
      "title" : "Context-specific independence in bayesian networks",
      "author" : [ "Boutilier" ],
      "venue" : "In Twelfth international conference on Uncertainty in artificial intelligence (UAI),",
      "citeRegEx" : "Boutilier,? \\Q1996\\E",
      "shortCiteRegEx" : "Boutilier",
      "year" : 1996
    }, {
      "title" : "Team behavior in interactive dynamic influence diagrams with applications to ad hoc teams (extended abstract)",
      "author" : [ "Chandrasekaran" ],
      "venue" : "In Autonomous Agents and MultiAgent Systems Conference (AAMAS),",
      "citeRegEx" : "Chandrasekaran,? \\Q2014\\E",
      "shortCiteRegEx" : "Chandrasekaran",
      "year" : 2014
    }, {
      "title" : "P",
      "author" : [ "P. Doshi", "Gmytrasiewicz" ],
      "venue" : "J.",
      "citeRegEx" : "Doshi and Gmytrasiewicz 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Perez",
      "author" : [ "P. Doshi" ],
      "venue" : "D.",
      "citeRegEx" : "Doshi and Perez 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Modeling recursive reasoning in humans using empirically informed interactive POMDPs",
      "author" : [ "Doshi" ],
      "venue" : "In International Autonomous Agents and Multiagent Systems Conference (AAMAS),",
      "citeRegEx" : "Doshi,? \\Q2010\\E",
      "shortCiteRegEx" : "Doshi",
      "year" : 2010
    }, {
      "title" : "and Doshi",
      "author" : [ "P.J. Gmytrasiewicz" ],
      "venue" : "P.",
      "citeRegEx" : "Gmytrasiewicz and Doshi 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "K",
      "author" : [ "T.N. Hoang", "Low" ],
      "venue" : "H.",
      "citeRegEx" : "Hoang and Low 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and LeytonBrown",
      "author" : [ "A.X. Jiang" ],
      "venue" : "K.",
      "citeRegEx" : "Jiang and Leyton.Brown 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "N",
      "author" : [ "A.X. Jiang", "K. LeytonBrown", "Bhat" ],
      "venue" : "A.",
      "citeRegEx" : "Jiang. Leyton.Brown. and Bhat 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A",
      "author" : [ "Jiang" ],
      "venue" : "X.; Leyton-Brown, K.; and Pfeffer, A.",
      "citeRegEx" : "Jiang. Leyton.Brown. and Pfeffer 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Graphical models for game theory",
      "author" : [ "Littman Kearns", "M. Singh 2001] Kearns", "M. Littman", "S. Singh" ],
      "venue" : "In Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Kearns et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 2001
    }, {
      "title" : "and Milch",
      "author" : [ "D. Koller" ],
      "venue" : "B.",
      "citeRegEx" : "Koller and Milch 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs",
      "author" : [ "Nair" ],
      "venue" : null,
      "citeRegEx" : "Nair,? \\Q2005\\E",
      "shortCiteRegEx" : "Nair",
      "year" : 2005
    }, {
      "title" : "Towards applying interactive POMDPs to real-world adversary modeling",
      "author" : [ "Ng" ],
      "venue" : "In Innovative Applications in Artificial Intelligence (IAAI),",
      "citeRegEx" : "Ng,? \\Q2010\\E",
      "shortCiteRegEx" : "Ng",
      "year" : 2010
    }, {
      "title" : "and Boutilier",
      "author" : [ "P. Poupart" ],
      "venue" : "C.",
      "citeRegEx" : "Poupart and Boutilier 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "and Tardos",
      "author" : [ "T. Roughgarden" ],
      "venue" : "E.",
      "citeRegEx" : "Roughgarden and Tardos 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "and Zilberstein",
      "author" : [ "S. Seuken" ],
      "venue" : "S.",
      "citeRegEx" : "Seuken and Zilberstein 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "G",
      "author" : [ "R. Seymour", "Peterson" ],
      "venue" : "L.",
      "citeRegEx" : "Seymour and Peterson 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Doshi",
      "author" : [ "E. Sonu" ],
      "venue" : "P.",
      "citeRegEx" : "Sonu and Doshi 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Individual planning in agent populations: Exploiting anonymity and frame-action hypergraphs",
      "author" : [ "Chen Sonu", "E. Doshi 2015] Sonu", "Y. Chen", "P. Doshi" ],
      "venue" : "Technical Report http://arxiv.org/abs/1503.07220,",
      "citeRegEx" : "Sonu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sonu et al\\.",
      "year" : 2015
    }, {
      "title" : "Decentralized stochastic planning with anonymity in interactions",
      "author" : [ "Adulyasak Varakantham", "P. Jaillet 2014] Varakantham", "Y. Adulyasak", "P. Jaillet" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Varakantham et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Varakantham et al\\.",
      "year" : 2014
    }, {
      "title" : "Decision support for agent populations in uncertain and congested environments",
      "author" : [ "Varakantham" ],
      "venue" : "In Uncertainty in artificial intelligence (UAI),",
      "citeRegEx" : "Varakantham,? \\Q2012\\E",
      "shortCiteRegEx" : "Varakantham",
      "year" : 2012
    }, {
      "title" : "An I-POMDP based multiagent architecture for dialogue tutoring",
      "author" : [ "F. Wang" ],
      "venue" : "In International Conference on Advanced ICT and Education",
      "citeRegEx" : "Wang,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang",
      "year" : 2013
    }, {
      "title" : "R",
      "author" : [ "M.P. Woodward", "Wood" ],
      "venue" : "J.",
      "citeRegEx" : "Woodward and Wood 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Using iterated reasoning to predict opponent strategies",
      "author" : [ "Wunder" ],
      "venue" : "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),",
      "citeRegEx" : "Wunder,? \\Q2011\\E",
      "shortCiteRegEx" : "Wunder",
      "year" : 2011
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Interactive partially observable Markov decision processes (I-POMDP) provide a formal framework for planning for a self-interested agent in multiagent settings. An agent operating in a multiagent environment must deliberate about the actions that other agents may take and the effect these actions have on the environment and the rewards it receives. Traditional I-POMDPs model this dependence on the actions of other agents using joint action and model spaces. Therefore, the solution complexity grows exponentially with the number of agents thereby complicating scalability. In this paper, we model and extend anonymity and context-specific independence – problem structures often present in agent populations – for computational gain. We empirically demonstrate the efficiency from exploiting these problem structures by solving a new multiagent problem involving more than 1,000 agents. Introduction We focus on the decision-making problem of an individual agent operating in the presence of other self-interested agents whose actions may affect the state of the environment and the subject agent’s rewards. In stochastic and partially observable environments, this problem is formalized by the interactive POMDP (I-POMDP) (Gmytrasiewicz and Doshi 2005). I-POMDPs cover an important portion of the multiagent planning problem space (Seuken and Zilberstein 2008; Doshi 2012), and applications in diverse areas such as security (Ng et al. 2010; Seymour and Peterson 2009), robotics (Wang 2013; Woodward and Wood 2012), ad hoc teams (Chandrasekaran et al. 2014) and human behavior modeling (Doshi et al. 2010; Wunder et al. 2011) testify to its wide appeal while critically motivating better scalability. Previous I-POMDP solution approximations such as interactive particle filtering (Doshi and Gmytrasiewicz 2009), point-based value iteration (Doshi and Perez 2008) and interactive bounded policy iteration (IBPI) (Sonu and Doshi 2014) scale I-POMDP solutions to larger physical state, observation and model spaces. Hoang and Low (2013) introduced the specialized IPOMDP Lite framework that promotes efficiency by Copyright c © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. modeling other agents as nested MDPs. However, to the best of our knowledge no effort specifically scales I-POMDPs to many interacting agents – say, a population of more than a thousand – sharing the environment. For illustration, consider the decision-making problem of the police when faced with a large protest. The degree of the police response is often decided by how many protestors of which type (disruptive or not) are participating. The individual identity of the protestor within each type seldom matters. This key observation of frame-action anonymity motivates us in how we model the agent population in the planning process. Furthermore, the planned degree of response at a protest site is influenced, in part, by how many disruptive protestors are predicted to converge at the site and much less by some other actions of protestors such as movement between other distant sites. Therefore, police actions depend on just a few actions of note for each type of agent. The example above illustrates two known and powerful types of problem structure in domains involving many agents: action anonymity (Roughgarden and Tardos 2002) and context-specific independence (Boutilier et al. 1996). Action anonymity allows the exponentially large joint action space to be substituted with a much more compact space of action configurations where a configuration is a tuple representing the number of agents performing each action. Context-specific independence (wherein given a context such as the state and agent’s own action, not all actions performed by other agents are relevant) permits the space of configurations to be compressed by projecting counts over a limited set of others’ actions. We extend both action anonymity and context-specific independence to allow considerations of an agent’s frame as well. 1 We list the specific contributions of this paper below: 1. I-POMDPs are severely challenged by large numbers of agents sharing the environment, which cause an exponential growth in the space of joint models and actions. Exploiting problem structure in the form of frameaction anonymity and context-specific independence, we present a new method for considerably scaling the solution of I-POMDPs to an unprecedented number of I-POMDPs distinguish between an agent’s frame and type with the latter including beliefs as well. Frames are similar in semantics to the colloquial use of types. agents. 2. We present a systematic way of modeling the problem structure in transition, observation and reward functions, and integrating it in a simple method for solving IPOMDPs that models other agents using finite-state machines and builds reachability trees given an initial belief. 3. We prove that the Bellman equation modified to include action configurations and frame-action independences continues to remain optimal given the I-POMDP with explicated problem structure. 4. Finally, we theoretically verify the improved savings in computational time and memory, and empirically demonstrate it on a new problem of policing protest with over a thousand protestors. The above problem structure allows us to emphatically mitigate the curse of dimensionality whose acute impact on I-POMDPs is well known. However, it does not lessen the impact of the curse of history. In this context, an additional step of sparse sampling of observations while generating the reachability tree allows sophisticated planning with a population of 1,000+ agents using about six hours. Related Work Building on graphical games (Kearns, Littman, and Singh 2001), action graph games (AGG) (Jiang, Leyton-Brown, and Bhat 2011) utilize problem structures such as action anonymity and context-specific independence to concisely represent single shot complete-information games involving multiple agents and to scalably solve for Nash equilibrium. The independence is modeled using a directed action graph whose nodes are actions and an edge between two nodes indicates that the reward of an agent performing an action indicated by one node is affected by other agents performing action of the other node. Lack of edges between nodes encodes the context-specific independence where the context is the action. Action anonymity is useful when the action sets of agents overlap substantially. Subsequently, the vector of counts over the set of distinct actions, called a configuration, is much smaller than the space of action profiles. We substantially build on AGGs in this paper by extending anonymity and context-specific independence to include agent frames, and generalizing their use to a partially observable stochastic game solved using decisiontheoretic planning as formalized by I-POMDPs. Indeed, Bayesian AGGs (Jiang and Leyton-Brown 2010) extend the original formulation to include agent types. These result in type-specific action sets with the benefit that the action graph structure does not change although the number of nodes grows with types: |Θ̂||A| nodes for agents with |Θ̂| types each having same |A| actions. If two actions from different type-action sets share a node, then these actions are interchangeable. A key difference in our representation is that we explicitly model frames in the graphs due to which context-specific independence is modeled using frame-action hypergraphs. Benefits are that we naturally maintain the distinction between two similar actions but performed by agents of different frames, and we add less additional nodes: |Θ̂| + |A|. However, a hypergraph is a more complex data structure for operation. Temporal AGGs (Jiang, Leyton-Brown, and Pfeffer 2009) extend AGGs to a repeated game setting and allow decisions to condition on chance nodes. These nodes may represent the action counts from previous step (similar to observing the actions in the previous game). Temporal AGGs come closest to multiagent influence diagrams (Koller and Milch 2001) although they can additionally model the anonymity and independence structure. Overall, I-POMDPs with frame-action anonymity and context-specific independence significantly augment the combination of Bayesian and temporal AGGs by utilizing the structures in a partially observable stochastic game setting with agent types. Varakantham et al. (2014) building on previous work (Varakantham et al. 2012) recently introduced a decentralized MDP that models a simple form of anonymous interactions: rewards and transition probabilities specific to a state-action pair are affected by the number of other agents regardless of their identities. The interaction influence is not further detailed into which actions of other agents are relevant (as in action anonymity) and thus configurations and hypergraphs are not used. Furthermore, agent types are not considered. Finally, the interaction hypergraphs in networked-distributed POMDPs (Nair et al. 2005) model complete reward independence between agents – analogous to graphical games – which differs from the hypergraphs in this paper (and action graphs) that model independence in reward (and transition, observation probabilities) along a different dimension: actions. Background Interactive POMDPs allow a self-interested agent to plan individually in a partially observable stochastic environment in the presence of other agents of uncertain types. We briefly review the I-POMDP framework and refer the reader to (Gmytrasiewicz and Doshi 2005) for further details. A finitely-nested interactive I-POMDP for an agent (say agent 0) of strategy level l operating in a setting inhabited by one of more other interacting agents is defined as the following tuple: I-POMDP0,l = 〈IS0,l, A, T0,Ω0, O0, R0, OC0〉 • IS0,l denotes the set of interactive states defined as, IS0,l = S × ∏N j=1Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of physical states. Θj,l−1 is the set of computable, intentional models ascribed to agent j: θj,l−1 = 〈bj,l−1, θ̂j〉, where bj,l−1 is agent j’s level l − 1 belief, bj,l−1 ∈ △(ISj,l−1), and θ̂j △ = 〈A, Tj ,Ωj , Oj , Rj , OCj〉, is j’s frame. Here, j is assumed to be Bayes-rational. At level 0, bj,0 ∈ △(S) and a level-0 intentional model reduces to a POMDP. SMj is the set of subintentional models of j, an example is a finite state automaton; • A = A0 ×A1 × . . .×AN is the set of joint actions of all agents; • T0 : S × A0 × ∏N j=1 Aj × S → [0, 1] is the transition function; • Ω0 is the set of agent 0’s observations; • O0 : S×A0 × ∏N j=1Aj ×Ω0 → [0, 1] is the observation function; • R0 : S×A0 × ∏N j=1 Aj → R is the reward function; and • OC0 is the optimality criterion, which is identical to that for POMDPs. In this paper, we consider a finite-horizon optimality criteria. Besides the physical state space, the I-POMDP’s interactive state space contains all possible models of other agents. In its belief update, an agent has to update its belief about the other agents’ models based on an estimation about the other agents’ observations and how they update their models. As the number of agents sharing the environment grows, the size of the joint action and joint model spaces increases exponentially. Therefore, the memory requirement for representing the transition, observation and reward functions grows exponentially as well as the complexity of performing belief update over the interactive states. In the context ofN agents, interactive bounded policy iteration (Sonu and Doshi 2014) generates good quality solutions for an agent interacting with 4 other agents (total of 5 agents) absent any problem structure. To the best of our knowledge, this result illustrates the best scalability so far to N > 2 agents. Many-Agent I-POMDP To facilitate understanding and experimentation, we introduce a pragmatic running example that also forms our evaluation domain. Figure 1: Protestors of different frames (colors) and police troops at two of three sites in the policing protest domain. The state space of police decision making is factored into the protest intensity levels at the sites. Example 1 (Policing Protest) Consider a policing scenario where police (agent 0) must maintain order in 3 geographically distributed and designated protest sites (labeled 0, 1, and 2) as shown in Fig. 1. A population of N agents are protesting at these sites. Police may dispatch one or two riot-control troops to either the same or different locations. Protests with differing intensities, low, medium and high (disruptive), occur at each of the three sites. The goal of the police is to deescalate protests to the low intensity at each site. Protest intensity at any site is influenced by the number of protestors and the number of police troops at that location. In the absence of adequate policing, we presume that the protest intensity escalates. On the other hand, two police troops at a location are adequate for de-escalating protests. Factored Beliefs and Update As we mentioned previously, the subject agent in an IPOMDP maintains a belief over the physical state and joint models of other agents, b0,l ∈ ∆(S × ∏N j=1Mj,l−1), where ∆(·) is the space of probability distributions. For settings such as Example 1 where N is large, the size of the interactive state space is exponentially larger, |IS0,l| = |S||Mj,l−1| N , and the belief representation unwieldy. However, the representation becomes manageable for large N if the belief is factored: b0,l(s,m1,l−1,m2,l−1, . . . ,mN,l−1) = Pr(s) Pr(m1,l−1|s) × Pr(m2,l−1|s)× . . .× Pr(mN,l−1|s) (1) This factorization assumes conditional independence of models of different agents given the physical state. Consequently, beliefs that correlate agents may not be directly represented although correlation could be alternately supported by introducing models with a correlating device. The memory consumed in storing a factored belief is O(|S| + N |S||M j |), where |M ∗ j | is the size of the largest model space among all other agents. This is linear in the number of agents, which is much less than the exponentially growing memory required to represent the belief as a joint distribution over the interactive state space, O(|S||M j | N ). Given agent 0’s belief at time t, bt0,l, its action a t 0 and the subsequent observation it makes, ω 0 , the updated belief at time step t+ 1, b 0,l , may be obtained as: Pr(s,m 1,l−1, . . . ,m t+1 N,l−1|b t 0,l, a t 0, ω t+1 0 ) = Pr(s | bt0,l, a t 0, ω t+1 0 ) Pr(m t+1 1,l−1|s ,m 2,l−1, . . . ,m t+1 N,l−1, bt0,l, a t 0, ω t+1 0 )× . . .× Pr(m t+1 N,l−1|s , bt0,l, a t 0, ω t+1 0 ) (2) Each factor in the product of Eq. 2 may be obtained as follows. The update over the physical state is: Pr(s|b0,l, a t 0, ω t+1 0 ) ∝ Pr(s , ω 0 |b t 0,l, a t 0)",
    "creator" : "gnuplot 4.6 patchlevel 4"
  }
}