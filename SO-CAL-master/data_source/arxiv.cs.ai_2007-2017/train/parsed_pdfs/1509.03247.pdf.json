{
  "name" : "1509.03247.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "ar.chaudhuri@samsung.com)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "vector regression (-FTSVR) and -twin support vector regression (- TSVR). -FTSVR is achieved by incorporating trapezoidal fuzzy numbers to -TSVR which takes care of uncertainty existing in forecasting problems. -FTSVR determines a pair of -insensitive proximal functions by solving two related quadratic programming problems. The structural risk minimization principle is implemented by introducing regularization term in primal problems of -FTSVR. This yields dual stable positive definite problems which improves regression performance. -FTSVR is then reformulated as - HFTSVR consisting of a set of hierarchical layers each containing - FTSVR. Experimental results on both synthetic and real datasets reveal that -HFTSVR has remarkable generalization performance with minimum training time.\nKeywords—Regression, -TSVR, -FTSVR, -HFTSVR\nI. INTRODUCTION\nUPPORT vector machines (SVMs) are powerful tools for\npattern classification and regression [1]. They have been\nsuccessfully applied to several real world problems [2]. There\nexist some classical methods [3] where decision surface is found by maximizing the margin between parallel\nhyperplanes. Recently some nonparallel hyperplane classifiers such as twin support vector regression (TSVR) [4] are\ndeveloped where two nonparallel proximal hyperplanes are\nused such that each hyperplane is closest to one class and farther than other class. TSVR solves two smaller sized\nquadratic programming problems and is faster than classical approaches. It is excellent at dealing with cross planes dataset.\nOther methods like -support vector regression (-SVR) [5] finds a linear function such that more training samples locate\nin -insensitive tube and function is as flat as possible leading\nto structural risk minimization principle. Another commonly\nused regressor viz. -twin support vector regression (-TSVR) [6] behaves like TSVR but it minimizes structural risk by\nadding regularization term through two functions that are as flat as possible. The dual problems are derived without any\nextra assumption and need not be modified any more. The\nexperiments have shown that -TSVR is faster and has better\ngeneralization. Based on this motivation, we first illustrate -\nArindam Chaudhuri is with Samsung Research & Development Institute Delhi, Noida – 201304 INDIA (corresponding author phone: +919871466996; e-mail: ar.chaudhuri@samsung.com).\nextension of -TSVR. -FTSVR is then remodeled as -\nhierarchical fuzzy twin support vector regression (-HFTSVR)\nwhich consists of set of hierarchical layers each containing - FTSVR with gaussian kernel at given scale. On increasing\nscale layer by layer details are incorporated inside regression\nfunction. It adapts local scale to data keeping number of\nsupport vectors and comparable configuration time. The\napproach is based on interleaving regression estimate with\npruning activity. -HFTSVR is applied to noisy synthetic and real datasets. It denoises original data obtaining an effective reconstruction of better quality. The major contributions of this work include: (a) fuzzification [7] of -TSVR leading to\n-FTSVR (b) hierarchical formulation of -FTSVR for noisy mislabeled samples to bring robustness in classification\nresults. This paper is presented as follows. In section II, we\nintroduce -TSVR. This is followed by -FTSVR and - HFTSVR in sections III and IV respectively. In next section\nexperimental results are highlighted. Finally, in section VI\nconclusions are given."
    }, {
      "heading" : "II. -TWIN SUPPORT VECTOR REGRESSION",
      "text" : "The -TSVR is formalized based on TSVR [4] and -SVR\n[5]. The -TSVR concentrates on two -insensitive proximal linear functions:\nℎ1(\uD835\uDC65) = \uD835\uDC641 \uD835\uDC47\uD835\uDC65 + \uD835\uDC4F1 (1) ℎ2(\uD835\uDC65) = \uD835\uDC642 \uD835\uDC47\uD835\uDC65 + \uD835\uDC4F2 (2)\nThe empirical risks are measured by:\n\uD835\uDC45\uD835\uDC52\uD835\uDC5A \uD835\uDF001 [ℎ1] = ∑ max{0, (\uD835\uDC66\uD835\uDC56 − ℎ1(\uD835\uDC65\uD835\uDC56)) 2}\n\uD835\uDC5A\n\uD835\uDC56=1\n+ \uD835\uDC5D1 ∑ max{0, −(\uD835\uDC66\uD835\uDC56 − ℎ1(\uD835\uDC65\uD835\uDC56) + \uD835\uDF001)}\n\uD835\uDC5A\n\uD835\uDC56=1\n(3)\n\uD835\uDC45\uD835\uDC52\uD835\uDC5A \uD835\uDF002 [ℎ2] = ∑ max{0, (ℎ2(\uD835\uDC65\uD835\uDC56) − \uD835\uDC66\uD835\uDC56) 2}\n\uD835\uDC5A\n\uD835\uDC56=1\n+ \uD835\uDC5D2 ∑ max{0, −(ℎ2(\uD835\uDC65\uD835\uDC56) − \uD835\uDC66\uD835\uDC56 + \uD835\uDF002)}\n\uD835\uDC5A\n\uD835\uDC56=1\n(4)\nHere, \uD835\uDC5D1 > 0, \uD835\uDC5D2 > 0 and ∑ max{0, −(\uD835\uDC66\uD835\uDC56 − ℎ1(\uD835\uDC65\uD835\uDC56) + \uD835\uDC5A \uD835\uDC56=1 \uD835\uDF001)} and ∑ max{0, −(ℎ2(\uD835\uDC65\uD835\uDC56) − \uD835\uDC66\uD835\uDC56 + \uD835\uDF002)} \uD835\uDC5A \uD835\uDC56=1 are the one side - insensitive loss function [5]. By introducing regularization terms 1\n2 (\uD835\uDC641 \uD835\uDC47\uD835\uDC641 + \uD835\uDC4F1 2) and\n1 2 (\uD835\uDC642 \uD835\uDC47\uD835\uDC642 + \uD835\uDC4F2 2), slack variables\n\uD835\uDF09, \uD835\uDF09∗, \uD835\uDF02 and \uD835\uDF02∗, the primal problems are expressed as:\nS\nmin \uD835\uDC641,\uD835\uDC4F1,\uD835\uDF09,\uD835\uDF09 ∗\n1 2 \uD835\uDC5D3(\uD835\uDC641 \uD835\uDC47\uD835\uDC641 + \uD835\uDC4F1 2) + 1 2 \uD835\uDF09∗\uD835\uDC47\uD835\uDF09∗ + \uD835\uDC5D1\uD835\uDC52 \uD835\uDC47\uD835\uDF09 subject to: \uD835\uDC4C − (\uD835\uDC34\uD835\uDC641 + \uD835\uDC52\uD835\uDC4F1) = \uD835\uDF09 ∗ (5)\n\uD835\uDC4C − (\uD835\uDC34\uD835\uDC641 + \uD835\uDC52\uD835\uDC4F1) ≥ −\uD835\uDF001\uD835\uDC52 − \uD835\uDF09, \uD835\uDF09 ≥ 0 and\nmin \uD835\uDC642,\uD835\uDC4F1,\uD835\uDF09,\uD835\uDF09 ∗\n1 2 \uD835\uDC5D4(\uD835\uDC642 \uD835\uDC47\uD835\uDC642 + \uD835\uDC4F2 2) + 1 2 \uD835\uDF02∗\uD835\uDC47\uD835\uDF02∗ + \uD835\uDC5D2\uD835\uDC52 \uD835\uDC47\uD835\uDF02\nsubject to: (\uD835\uDC34\uD835\uDC642 + \uD835\uDC52\uD835\uDC4F2) − \uD835\uDC4C = \uD835\uDF02 ∗ (6) (\uD835\uDC34\uD835\uDC642 + \uD835\uDC52\uD835\uDC4F2) − \uD835\uDC4C ≥ −\uD835\uDF002\uD835\uDC52 − \uD835\uDF02, \uD835\uDF02 ≥ 0\nHere \uD835\uDC5D3 > 0, \uD835\uDC5D4 > 0, \uD835\uDF001 > 0 and \uD835\uDF002 > 0. The solutions of equations (5) and (6) are obtained by deriving their dual problems. The Lagrangian of equation (5) is given by: \uD835\uDC3F(\uD835\uDC641 , \uD835\uDC4F1, \uD835\uDF09, \uD835\uDEFC, \uD835\uDEFD) = 1\n2 (\uD835\uDC4C − (\uD835\uDC34\uD835\uDC641 + \uD835\uDC52\uD835\uDC4F1))\n\uD835\uDC47 (\uD835\uDC4C − (\uD835\uDC34\uD835\uDC641 + \uD835\uDC52\uD835\uDC4F1))\n+ 1\n2 \uD835\uDC5D3(‖\uD835\uDC641‖ 2 + \uD835\uDC4F1 2) + \uD835\uDC5D1\uD835\uDC52 \uD835\uDC47\uD835\uDF09 − \uD835\uDEFD\uD835\uDC47\uD835\uDF09\n−\uD835\uDEFC\uD835\uDC47(\uD835\uDC4C − (\uD835\uDC34\uD835\uDC641 + \uD835\uDC52\uD835\uDC4F1) + \uD835\uDF001\uD835\uDC52 + \uD835\uDF09) (7)\nHere \uD835\uDEFC = (\uD835\uDEFC1, … … . , \uD835\uDEFC\uD835\uDC5A) and \uD835\uDEFD = (\uD835\uDEFD1, … … . , \uD835\uDEFD\uD835\uDC5A) are vectors of Lagrange multipliers. The Karush Kuhn Tucker condition for \uD835\uDC641, \uD835\uDC4F1, \uD835\uDF09, \uD835\uDEFC and \uD835\uDEFD are given by: −\uD835\uDC34\uD835\uDC47(\uD835\uDC4C − \uD835\uDC34\uD835\uDC641 − \uD835\uDC52\uD835\uDC4F1) + \uD835\uDC5D3\uD835\uDC641 + \uD835\uDC34\n\uD835\uDC47\uD835\uDEFC = 0 (8) −\uD835\uDC52\uD835\uDC47(\uD835\uDC4C − \uD835\uDC34\uD835\uDC641 + \uD835\uDC52\uD835\uDC4F1) + \uD835\uDC5D3\uD835\uDC4F1 + \uD835\uDC52\n\uD835\uDC47\uD835\uDEFC = 0 (9) \uD835\uDC5D1\uD835\uDC52 − \uD835\uDEFD − \uD835\uDEFC = 0 (10)\n\uD835\uDC4C − (\uD835\uDC34\uD835\uDC641 + \uD835\uDC52\uD835\uDC4F1) ≥ −\uD835\uDF001\uD835\uDC52 − \uD835\uDF09, \uD835\uDF09 ≥ 0 (11) \uD835\uDEFC\uD835\uDC47(\uD835\uDC4C − (\uD835\uDC34\uD835\uDC641 + \uD835\uDC52\uD835\uDC4F1) + \uD835\uDF001\uD835\uDC52 + \uD835\uDF09) = 0, \uD835\uDEFD\n\uD835\uDC47\uD835\uDF09 = 0 (12) \uD835\uDEFC ≥ 0, \uD835\uDEFD ≥ 0 (13) Since \uD835\uDEFD ≥ 0 we have: 0 ≤ \uD835\uDEFC ≤ \uD835\uDC5D1\uD835\uDC52 (14)\nThe equations (8)-(9) imply that:\n− [\uD835\uDC34 \uD835\uDC47 \uD835\uDC52\uD835\uDC47 ] \uD835\uDC4C + ([\uD835\uDC34 \uD835\uDC47 \uD835\uDC52\uD835\uDC47 ] [\uD835\uDC34 \uD835\uDC52] + \uD835\uDC5D3\uD835\uDC3C) [ \uD835\uDC641 \uD835\uDC4F1 ] + [\uD835\uDC34 \uD835\uDC47 \uD835\uDC52\uD835\uDC47 ] \uD835\uDEFC = 0 (15)\nAssuming \uD835\uDC3D = [\uD835\uDC34 \uD835\uDC52], \uD835\uDC631 = [\uD835\uDC641 \uD835\uDC4F1] \uD835\uDC47 equation (15) is\nrewritten as:\n\uD835\uDC631 = (\uD835\uDC3D \uD835\uDC47\uD835\uDC3D + \uD835\uDC5D3\uD835\uDC3C) −1\uD835\uDC3D\uD835\uDC47(\uD835\uDC4C − \uD835\uDEFC) (16)\nThen substituting equation (16) into Lagrangian and using\nKarush Kuhn Tucker conditions the dual problem is:\nmax \uD835\uDEFC\n− 1\n2 \uD835\uDEFC\uD835\uDC47\uD835\uDC3D(\uD835\uDC3D\uD835\uDC47\uD835\uDC3D + \uD835\uDC5D3\uD835\uDC3C) −1\uD835\uDC3D\uD835\uDC47\uD835\uDEFC\uD835\uDC47 + \uD835\uDC4C\uD835\uDC47\uD835\uDC3D(\uD835\uDC3D\uD835\uDC47\uD835\uDC3D + \uD835\uDC5D3\uD835\uDC3C) −1\uD835\uDC3D\uD835\uDC47\uD835\uDEFC\n− (\uD835\uDC52\uD835\uDC47\uD835\uDF001 + \uD835\uDC4C \uD835\uDC47)\uD835\uDEFC\nsubject to: 0 ≤ \uD835\uDEFC ≤ \uD835\uDC5D1\uD835\uDC52 (17)\nIn equation (17) adjusting \uD835\uDC5D3 improves classification accuracy. In similar manner dual of equation (6) is obtained. Once solutions (\uD835\uDC641, \uD835\uDC4F1) and (\uD835\uDC642, \uD835\uDC4F2) of equations (5) and (6) are obtained from solutions of equation (17) and its dual, proximal functions ℎ1(\uD835\uDC65) and ℎ2(\uD835\uDC65) are achieved. The estimated regressor considered as approximation function is:\nℎ(\uD835\uDC65) = 1\n2 (ℎ1(\uD835\uDC65) + ℎ2(\uD835\uDC65))\nℎ(\uD835\uDC65) = 1\n2 (\uD835\uDC641 + \uD835\uDC642)\n\uD835\uDC47\uD835\uDC65 + 1\n2 (\uD835\uDC4F1 + \uD835\uDC4F2) (18)"
    }, {
      "heading" : "III. - FUZZY TWIN SUPPORT VECTOR REGRESSION",
      "text" : "In order to deal with problems of finite samples and uncertain data in existing in many forecasting situations, input\nvariables and output function ℎ(\uD835\uDC99) are described as crisp numbers by fuzzy comprehensive evaluation. To represent fuzzy degree of input variables, trapezoidal fuzzy membership function is adopted [7]. Suppose fuzzy training sample set {\uD835\uDC99\uD835\uDC8A, \uD835\uDC66\uD835\uDC56}\uD835\uDC56=1 \uD835\uDC5A with \uD835\uDC99\uD835\uDC8A = (\uD835\uDC94\uD835\uDC99\uD835\uDC8A , ∆\uD835\uDC94\uD835\uDC99\uD835\uDC8A , ∆\uD835\uDC94 ̅̅̅̅ \uD835\uDC99\uD835\uDC8A , ∆\uD835\uDC94 ̿̿̿̿ \uD835\uDC99\uD835\uDC8A) ∈ \uD835\uDC43(\uD835\uDC45) \uD835\uDC51 , \uD835\uDC66\uD835\uDC56 = (\uD835\uDC60\uD835\uDC66\uD835\uDC56 , ∆\uD835\uDC60\uD835\uDC66\uD835\uDC56 , ∆\uD835\uDC60 ̅̅ ̅ \uD835\uDC66\uD835\uDC56 , ∆\uD835\uDC60̿̿ ̿\uD835\uDC66\uD835\uDC56) ∈ \uD835\uDC43(\uD835\uDC45) and \uD835\uDC43(\uD835\uDC45) \uD835\uDC51 is \uD835\uDC51 dimensional vector set. In light of -FTSVR regression coefficients \uD835\uDC43(\uD835\uDC45) is estimated by following constrained optimization problems:\nmin \uD835̅\uDC98\uD835\uDFCF,\uD835\uDC4F1,\uD835\uDF09,\uD835\uDF09 ∗\n1 2 \uD835\uDC5D3(\uD835̅\uDC98\uD835\uDFCF \uD835\uDC47\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC4F1 2) + 1 2 \uD835\uDF09∗\uD835\uDC47\uD835\uDF09∗ + \uD835\uDC5D1\uD835\uDC52 \uD835\uDC47\uD835\uDF09\nsubject to: \uD835\uDC4C − (\uD835\uDC34\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC52\uD835\uDC4F1) = \uD835\uDF09 ∗ (19)\n\uD835\uDC4C − (\uD835\uDC34\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC52\uD835\uDC4F1) ≥ −\uD835\uDF001\uD835\uDC52 − \uD835\uDF09, \uD835\uDF09 ≥ 0 and\nmin \uD835\uDC98\uD835\uDFD0,\uD835\uDC4F1,\uD835\uDF09,\uD835\uDF09 ∗\n1 2 \uD835\uDC5D4(\uD835̅\uDC98\uD835\uDFD0 \uD835\uDC47\uD835̅\uDC98\uD835\uDFD0 + \uD835\uDC4F2 2) + 1 2 \uD835\uDF02∗\uD835\uDC47\uD835\uDF02∗ + \uD835\uDC5D2\uD835\uDC52 \uD835\uDC47\uD835\uDF02\nsubject to: (\uD835\uDC34\uD835̅\uDC98\uD835\uDFD0 + \uD835\uDC52\uD835\uDC4F2) − \uD835\uDC4C = \uD835\uDF02 ∗ (20)\n(\uD835\uDC34\uD835̅\uDC98\uD835\uDFD0 + \uD835\uDC52\uD835\uDC4F2) − \uD835\uDC4C ≥ −\uD835\uDF002\uD835\uDC52 − \uD835\uDF02, \uD835\uDF02 ≥ 0\nThe terms \uD835\uDC5D3 > 0, \uD835\uDC5D4 > 0, \uD835\uDF001 > 0 and \uD835\uDF002 > 0 are crisp numbers. The Lagrangian of equation (19) is given by:\n\uD835\uDC3F(\uD835̅\uDC98\uD835\uDFCF, \uD835\uDC4F1, \uD835\uDF09, \uD835\uDEFC, \uD835\uDEFD) = 1\n2 (\uD835\uDC4C − (\uD835\uDC34\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC52\uD835\uDC4F1))\n\uD835\uDC47 (\uD835\uDC4C − (\uD835\uDC34\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC52\uD835\uDC4F1))\n+ 1\n2 \uD835\uDC5D3(‖\uD835̅\uDC98\uD835\uDFCF‖ 2 + \uD835\uDC4F1 2) + \uD835\uDC5D1\uD835\uDC52 \uD835\uDC47\uD835\uDF09 − \uD835\uDEFD\uD835\uDC47\uD835\uDF09\n−\uD835\uDEFC\uD835\uDC47(\uD835\uDC4C − (\uD835\uDC34\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC52\uD835\uDC4F1) + \uD835\uDF001\uD835\uDC52 + \uD835\uDF09) (21)\nThe Lagrange multipliers are \uD835\uDEFC = (\uD835\uDEFC1, … … . , \uD835\uDEFC\uD835\uDC5A) and \uD835\uDEFD = (\uD835\uDEFD1, … … . , \uD835\uDEFD\uD835\uDC5A). The Karush Kuhn Tucker condition for \uD835̅\uDC98\uD835\uDFCF, \uD835\uDC4F1, \uD835\uDF09, \uD835\uDEFC and \uD835\uDEFD are given by:\n−\uD835\uDC34\uD835\uDC47(\uD835\uDC4C − \uD835\uDC34\uD835̅\uDC98\uD835\uDFCF − \uD835\uDC52\uD835\uDC4F1) + \uD835\uDC5D3\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC34 \uD835\uDC47\uD835\uDEFC = 0 (22) −\uD835\uDC52\uD835\uDC47(\uD835\uDC4C − \uD835\uDC34\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC52\uD835\uDC4F1) + \uD835\uDC5D3\uD835\uDC4F1 + \uD835\uDC52 \uD835\uDC47\uD835\uDEFC = 0 (23) \uD835\uDC5D1\uD835\uDC52 − \uD835\uDEFD − \uD835\uDEFC = 0 (24) \uD835\uDC4C − (\uD835\uDC34\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC52\uD835\uDC4F1) ≥ −\uD835\uDF001\uD835\uDC52 − \uD835\uDF09, \uD835\uDF09 ≥ 0 (25) \uD835\uDEFC\uD835\uDC47(\uD835\uDC4C − (\uD835\uDC34\uD835̅\uDC98\uD835\uDFCF + \uD835\uDC52\uD835\uDC4F1) + \uD835\uDF001\uD835\uDC52 + \uD835\uDF09) = 0, \uD835\uDEFD \uD835\uDC47\uD835\uDF09 = 0 (26) \uD835\uDEFC ≥ 0, \uD835\uDEFD ≥ 0 (27) Since \uD835\uDEFD ≥ 0 we have: 0 ≤ \uD835\uDEFC ≤ \uD835\uDC5D1\uD835\uDC52 (28)\nThe equations (22)-(23) imply that:\n− [\uD835\uDC34 \uD835\uDC47 \uD835\uDC52\uD835\uDC47 ] \uD835\uDC4C + ([\uD835\uDC34 \uD835\uDC47 \uD835\uDC52\uD835\uDC47 ] [\uD835\uDC34 \uD835\uDC52] + \uD835\uDC5D3\uD835\uDC3C) [ \uD835̅\uDC98\uD835\uDFCF \uD835\uDC4F1 ] + [\uD835\uDC34 \uD835\uDC47 \uD835\uDC52\uD835\uDC47 ] \uD835\uDEFC = 0 (29)\nAssuming \uD835\uDC3D = [\uD835\uDC34 \uD835\uDC52], \uD835\uDC631 = [\uD835̅\uDC98\uD835\uDFCF \uD835\uDC4F1] \uD835\uDC47 equation (29) is\nrewritten as:\n\uD835̅\uDC97\uD835\uDFCF = (\uD835\uDC3D \uD835\uDC47\uD835\uDC3D + \uD835\uDC5D3\uD835\uDC3C) −1\uD835\uDC3D\uD835\uDC47(\uD835\uDC4C − \uD835\uDEFC) (30)\nThen substituting equation (30) into Lagrangian and using\nKarush Kuhn Tucker conditions the dual problem is:\nmax \uD835\uDEFC\n− 1\n2 \uD835\uDEFC\uD835\uDC47\uD835\uDC3D(\uD835\uDC3D\uD835\uDC47\uD835\uDC3D + \uD835\uDC5D3\uD835\uDC3C) −1\uD835\uDC3D\uD835\uDC47\uD835\uDEFC\uD835\uDC47 + \uD835\uDC4C\uD835\uDC47\uD835\uDC3D(\uD835\uDC3D\uD835\uDC47\uD835\uDC3D + \uD835\uDC5D3\uD835\uDC3C) −1\uD835\uDC3D\uD835\uDC47\uD835\uDEFC\n− (\uD835\uDC52\uD835\uDC47\uD835\uDF001 + \uD835\uDC4C \uD835\uDC47)\uD835\uDEFC\nsubject to: 0 ≤ \uD835\uDEFC ≤ \uD835\uDC5D1\uD835\uDC52 (31)\nThe approximation function is:\nℎ(\uD835\uDC99) = 1\n2 (ℎ1(\uD835\uDC99) + ℎ2(\uD835\uDC99))\nℎ(\uD835\uDC99) = 1\n2 (\uD835̅\uDC98\uD835\uDFCF + \uD835̅\uDC98\uD835\uDFD0)\n\uD835\uDC47 ∙ \uD835\uDC99 + 1\n2 (\uD835\uDC4F1 + \uD835\uDC4F2) (32)\nIn equation (32) \uD835̅\uDC98\uD835\uDFCF = (\uD835̅\uDC6411, . . , \uD835̅\uDC641\uD835\uDC51) and \uD835̅\uDC98\uD835\uDFD0 = (\uD835̅\uDC6421, . . , \uD835̅\uDC642\uD835\uDC51) such that |\uD835̅\uDC98\uD835\uDFCF| = (|\uD835̅\uDC6411|, . . , |\uD835̅\uDC641\uD835\uDC51|) and |\uD835̅\uDC98\uD835\uDFD0| =\n(|\uD835̅\uDC6421|, . . , |\uD835̅\uDC642\uD835\uDC51|). The inner product of \uD835̅\uDC98\uD835\uDFCF and \uD835\uDC99 is \uD835̅\uDC98\uD835\uDFCF ∙ \uD835\uDC99. In \uD835\uDC43(\uD835\uDC45), ℎ(\uD835\uDC99) can be written as:\nℎ(\uD835\uDC99) = 1\n2 ((\uD835̅\uDC98\uD835\uDFCF + \uD835̅\uDC98\uD835\uDFD0) ∙ \uD835\uDC94\uD835\uDC99 + (\uD835\uDC4F1 + \uD835\uDC4F2)\uD835\uDF0C(∆\uD835\uDC94\uD835\uDC99)) (33)\nIn equation (33) \uD835\uDF0C(∆\uD835\uDC94\uD835\uDC99) = |(\uD835̅\uDC98\uD835\uDFCF + \uD835̅\uDC98\uD835\uDFD0) ∙ ∆\uD835\uDC94\uD835\uDC99| with \uD835̅\uDC98\uD835\uDFCF, \uD835̅\uDC98\uD835\uDFD0, \uD835\uDC94\uD835\uDC99, ∆\uD835\uDC94\uD835\uDC99 ∈ \uD835\uDC45 \uD835\uDC51 and \uD835\uDC4F1, \uD835\uDC4F2 ∈ \uD835\uDC45."
    }, {
      "heading" : "IV. - HIERARCHICAL FUZZY TWIN SUPPORT VECTOR REGRESSION",
      "text" : "Based on -FTSVR, -HFTSVR is formulated here. - HFTSVR is constituted into a pool of \uD835\uDC49 layers each comprising of single kernel -FTSVR {\uD835\uDC5A\uD835\uDC63(∘)} by suitable scale. The different layers are placed in hierarchy having scale determined by parameter \uD835\uDF0F\uD835\uDC63 which increases when layer number decreases (\uD835\uDF0F\uD835\uDC63 ≤ \uD835\uDF0F\uD835\uDC63+1). The output of -HFTSVR is: \uD835\uDC58(\uD835\uDC99) = ∑ \uD835\uDC5A\uD835\uDC63(\uD835\uDC99; \uD835\uDC49 \uD835\uDC63=1 \uD835\uDF0F\uD835\uDC63) (34)\n-HFTSVR configuration proceeds by adding and\nconfiguring one layer at a time. It initiates from layer featuring smallest scale to that featuring largest one. The first layer is trained such that distance between regression curve produced by first layer itself and data is minimized. It plays a significant role in its success. It is trained heuristically so that number of used layers reduces. All other layers are trained to approximate the residual. The residual for each layer is:\n\uD835\uDC5F\uD835\uDC5A\uD835\uDC63(\uD835\uDC99\uD835\uDC8A) = \uD835\uDC5F\uD835\uDC5A\uD835\uDC63−1(\uD835\uDC99\uD835\uDC8A) − \uD835\uDC5A\uD835\uDC63(\uD835\uDC99\uD835\uDC8A) (35)\nThe \uD835\uDC63\uD835\uDC61ℎ layer is configured with training set \uD835\uDC47\uD835\uDC46\uD835\uDC63 =\n{(\uD835\uDC99\uD835\uDFCF, \uD835\uDC5F\uD835\uDC5A\uD835\uDC63−1(\uD835\uDC99\uD835\uDFCF)), … … , (\uD835\uDC99\uD835\uDC8F, \uD835\uDC5F\uD835\uDC5A\uD835\uDC63−1(\uD835\uDC99\uD835\uDC8F))} . The value of scale parameter of first layer \uD835\uDF0F1 is proportional to input domain’s size. The parameter \uD835\uDF0F is decreased arbitrarily. The most preferred value of \uD835\uDF0F for each layer is \uD835\uDF0F\uD835\uDC63+1 = \uD835\uDF0F\uD835\uDC63 \uD835\uDC5B⁄ ; \uD835\uDC5B ≥ 2 producing satisfactory results. On decreasing \uD835\uDF0F slowly accuracy of solution improves but number of layers and number of support vectors increases. New layers are added during training until stopping criterion is satisfied. The two other parameters are defined for each layer: (a) \uD835\uDC35\uD835\uDC63 is tradeoff between regression error and smoothness of solution and (b) \uD835\uDF16 which controls amplitude of \uD835\uDF16-insensitivity tube around solution itself. The value of \uD835\uDC35 is usually set experimentally by trial and error. Here, \uD835\uDC35\uD835\uDC63 is chosen for each layer as \uD835\uDC46 times variance of residuals used to configure the \uD835\uDC63\uD835\uDC61ℎ layer as:\n\uD835\uDC35\uD835\uDC63 = \uD835\uDC46\uD835\uDC63\uD835\uDC4E\uD835\uDC5F(\uD835\uDC5F\uD835\uDC5A\uD835\uDC63−1(\uD835\uDC99\uD835\uDC8A)) (36)\nIn equation (36) \uD835\uDC35\uD835\uDC63 assumes value taken by Lagrange multipliers associated to support vectors of \uD835\uDC63\uD835\uDC61ℎ layer which represents maximum weight associated to each kernel. For input space regions where Gaussians associated to support vectors have no significant overlap. This depends both on Gaussian scale parameter and data density. The value of \uD835\uDC35\uD835\uDC63 is approximately maximum value that can be assumed by regression function in those regions as Gaussian kernel is 1. For this reason \uD835\uDC35\uD835\uDC63 is large enough to allow regression curve reaching maximum or minimum value of data points inside whole input domain. However, a larger \uD835\uDC35\uD835\uDC63 favors overfitting. The experimental results on different datasets suggest that \uD835\uDC46 lies in interval (0, 5] which represents a tradeoff. Similar to\n-FTSVR, parameter \uD835\uDF16 cannot be determined from dataset; rather \uD835\uDF16 is set proportional to accuracy required for regression. Experiments show that in -HFTSVR, layers with larger \uD835\uDF0F have number of support vectors similar to layers with smaller \uD835\uDF0F. There appears some contradiction as fewer units are required to realize a reconstruction at larger scale. Hence, in first layer where -HFTSVR output has low frequency content many data points lie far from curve and are still selected as support vectors. This leads to high number of support vectors. To avoid this after each layer has been configured, a pruning step is carried out to reduce number of support vectors. The cost function is then minimized a second time considering only reduced training set to obtain final approximation for each current layer. To reduce number of support vectors it is noticed that distance of training point from regression curve measures suitability of current curve to describe information conveyed. In this sense, points too distant from regression curve cannot be explained by curve. They can be regarded as outliers. For these reasons, acceptable approximation of regression curve is obtained using only those points that lie close to curve. This has been confirmed experimentally. It is observed that quality of regression at given scale does not degrade significantly if regression is computed considering only points close to \uD835\uDF16-tube. The closeness of point to \uD835\uDF16-tube can be assessed only after computation of regression itself considering all training points. In second pass, regression is computed again considering only points close to \uD835\uDF16-tube. Consider \uD835\uDC63\uD835\uDC61ℎ layer and regression computed for layer \uD835\uDC5A\uD835\uDC63(\uD835\uDC99) using complete training set \uD835\uDC47\uD835\uDC46\uD835\uDC63. Let us define \uD835\uDC47\uD835\uDC46\uD835\uDC63 ′ set constituting only of those support vectors that lie on border of \uD835\uDF16-tube and those whose distance from \uD835\uDC5A\uD835\uDC63(\uD835\uDC99) < \uD835\uDF16 \uD835\uDC5B⁄ as:\n\uD835\uDC47\uD835\uDC46\uD835\uDC63 ′ = {(\uD835̅\uDC99\uD835\uDC8A, \uD835\uDC5F\uD835\uDC5A\uD835\uDC63−1(\uD835̅\uDC99\uD835\uDC8A))|||\uD835\uDC5F\uD835\uDC5A\uD835\uDC63(\uD835̅\uDC99\uD835\uDC8A)| − \uD835\uDF16| < \uD835\uDC61\uD835\uDC5D ∨ |\uD835\uDC5F\uD835\uDC5A\uD835\uDC63(\uD835̅\uDC99\uD835\uDC8A)| <\n\uD835\uDF00 \uD835\uDC5B } (37)\nIn equation (37) \uD835\uDC61\uD835\uDC5D is tolerance parameter that determines thickness of \uD835\uDF16-tube margin. The configuration phase of each layer is structured in two sequential steps: (a) first provides regression curve \uD835\uDC5A\uD835\uDC63 considering all training points and (b) second \uD835\uDC5A\uD835\uDC63\n′ realizes an efficient regression curve by considering only selected subset of points. To cope with diminished point density in \uD835\uDC47\uD835\uDC46\uD835\uDC63 ′ , value of parameter \uD835\uDC35\uD835\uDC63 is increased proportionally in second optimization step as:\n\uD835\uDC35\uD835\uDC63 ′ = \uD835\uDC35\uD835\uDC63\n|\uD835\uDC47\uD835\uDC46\uD835\uDC63| |\uD835\uDC47\uD835\uDC46\uD835\uDC63 ′| = \uD835\uDC46\uD835\uDC63\uD835\uDC4E\uD835\uDC5F(\uD835\uDC5F\uD835\uDC5A\uD835\uDC63−1(\uD835\uDC99\uD835\uDC8A)) |\uD835\uDC47\uD835\uDC46\uD835\uDC63| |\uD835\uDC47\uD835\uDC46\uD835\uDC63 ′|\n(38)"
    }, {
      "heading" : "V. EXPERIMENTAL RESULTS",
      "text" : "In this section, some experiments are performed to\ndemonstrate performance of -HFTSVR compared with -\nFTSVR and -TSVR on synthetic and real datasets. All methods are implemented in Matlab 8.2 on PC having Intel P4 processor with 2.9 GHz and 1 GB RAM with 512 KB cache. The values of parameters are obtained through searching in range [2−9, 29] by tuning a set comprising of random 20 % of dataset. In experiments, we set \uD835\uDC5D1 = \uD835\uDC5D2, \uD835\uDC5D3 = \uD835\uDC5D4 and \uD835\uDF001 = \uD835\uDF002 to reduce computational complexity of parameter selection. In order to assess the performance of methods, evaluation criteria used are: (a) SSE (b) NMSE (c) \uD835\uDC452and (d) MAPE.\nA. Synthetic Datasets\nThe synthetic datasets are taken from [8]. Considering\nfunction \uD835\uDC66 = \uD835\uDC65 2 3⁄ training samples are distorted by Gaussian noise with 0 mean and 0.2 standard deviation so that:\n\uD835\uDC66\uD835\uDC56 = \uD835\uDC65\uD835\uDC56 2 3⁄ + \uD835\uDF09\uD835\uDC56 , \uD835\uDC65~\uD835\uDC48[−2, 2], \uD835\uDF09\uD835\uDC56~\uD835\uDC41(0, 0.2 2) (39)\nIn equation (39) \uD835\uDC48[\uD835\uDC4E, \uD835\uDC4F] and \uD835\uDC41(\uD835̅\uDC4E, \uD835̅\uDC4F2) represents uniform and gaussian random variable respectively. To avoid biased comparisons 10 independent groups of noisy samples are generated consisting of 200 training and 200 none noise test samples. It has been observed that -HFTSVR achieves best approximation. The results of performance criteria are given\n(see Table I). -HFTSVR derives smallest SSE, NMSE and largest \uD835\uDC452 among all methods. This indicates statistical information in training dataset is well explained by -\nHFTSVR with small regression errors. It is observed that - HFTSVR is fastest learning method improving training speed. Another datasets are generated by sine function distorted by gaussian noise with 0 mean and 0.2 standard deviation so that:\n\uD835\uDC66\uD835\uDC56 = \uD835\uDC60\uD835\uDC56\uD835\uDC5B(\uD835\uDC65\uD835\uDC56)\n\uD835\uDC65\uD835\uDC56 + \uD835\uDF09\uD835\uDC56 , \uD835\uDC65~\uD835\uDC48[−4\uD835\uDF0B, 4\uD835\uDF0B], \uD835\uDF09\uD835\uDC56~\uD835\uDC41(0, 0.2\n2) (40)\nThe dataset consists of 272 training samples and 526 test\nsamples. The results show the superiority of -HFTSVR is demonstrated (see Table I).\nFor further evaluation the experimental are also performed on UCI datasets such as Servo and Auto Price [9] in terms of NMSE, \uD835\uDC452, MAPE and CPU time. The given results further confirm the superiority of -HFTSVR (see Table II). The best\nparameters selected by -HFTSVR and -FTSVR on above UCI datasets are given (see Table III). It is observed that the values of \uD835\uDC5D3 and \uD835\uDC5D4 vary and usually do not consider smaller value in -HFTSVR. This implies that regularization in terms\nof -HFTSVR is significant."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "In this work we propose a novel regressor -HFTSVR by minimizing structural risk. The motivation towards developing\nthe regressor is attributed towards -FTSVR which is obtained\nby applying trapezoidal fuzzy numbers to -TSVR. -FTSVR takes care of uncertainty existing in forecasting problems. It\ndetermines a pair of -insensitive proximal functions by solving two related SVM type problems. The problem is solved by introducing regularization term in primal problems\nof -FTSVR and handling the dual alternative. This improves\noverall regression performance. Then -HFTSVR is\nformulated as set of hierarchical layers each containing - FTSVR. Experimental results on both synthetic and real\ndatasets reveal that -HFTSVR has superior compared to other\nregressor. However, suitable parameter selection of - HFTSVR remains a practical problem towards future research."
    } ],
    "references" : [ {
      "title" : "A Tutorial on Support Vector Machines for Pattern Recognition",
      "author" : [ "C. Burges" ],
      "venue" : "Data Mining and Knowledge Discovery, vol. 2, no. 2, pp. 121–167, 1998.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Fuzzy Support Vector Machine for Bankruptcy Prediction",
      "author" : [ "A. Chaudhuri", "K. De" ],
      "venue" : "Applied Soft Computing, vol. 11, no. 2, pp. 2472–2486, 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Support Vector Machines: Theory, Algorithms and Extensions.CRC",
      "author" : [ "N.Y. Deng", "Y.J. Tian", "C.H. Zhang" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Twin Support Vector Machines for Pattern Classification",
      "author" : [ "Jayadeva", "R. Khemchandani", "S. Chandra" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 5, pp. 905–910.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "A Tutorial on Support Vector Regression",
      "author" : [ "A. Samola", "B. Schöikopf" ],
      "venue" : "Statistics and Computing, vol. 14, pp. 199–222, 2004.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "An - Twin Support Vector Machine for Regression",
      "author" : [ "Y.H. Shao", "C.H. Zhang", "Z.M. Yang", "L. Jing", "N.Y. Deng" ],
      "venue" : "Neural Computing and Applications, vol. 23, no.1, pp. 175–185, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "TSVR: An efficient Twin Support Vector Machine for Regression",
      "author" : [ "X. Peng" ],
      "venue" : "Neural Networks, vol. 23, no. 3, pp. 365–372, 2010.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "UPPORT vector machines (SVMs) are powerful tools for pattern classification and regression [1].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "They have been successfully applied to several real world problems [2].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "There exist some classical methods [3] where decision surface is found by maximizing the margin between parallel hyperplanes.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Recently some nonparallel hyperplane classifiers such as twin support vector regression (TSVR) [4] are developed where two nonparallel proximal hyperplanes are used such that each hyperplane is closest to one class and farther than other class.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "Other methods like -support vector regression (-SVR) [5] finds a linear function such that more training samples locate in -insensitive tube and function is as flat as possible leading to structural risk minimization principle.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "-twin support vector regression (-TSVR) [6] behaves like TSVR but it minimizes structural risk by adding regularization term through two functions that are as flat as possible.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "The -TSVR is formalized based on TSVR [4] and -SVR [5].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "The -TSVR is formalized based on TSVR [4] and -SVR [5].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Here, p1 > 0, p2 > 0 and ∑ max{0, −(yi − h1(xi) + m i=1 ε1)} and ∑ max{0, −(h2(xi) − yi + ε2)} m i=1 are the one side insensitive loss function [5].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "The values of parameters are obtained through searching in range [2, 2] by tuning a set comprising of random 20 % of dataset.",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "The values of parameters are obtained through searching in range [2, 2] by tuning a set comprising of random 20 % of dataset.",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "The synthetic datasets are taken from [8].",
      "startOffset" : 38,
      "endOffset" : 41
    } ],
    "year" : 2015,
    "abstractText" : "The research presents -hierarchical fuzzy twin support vector regression (-HFTSVR) based on -fuzzy twin support vector regression (-FTSVR) and -twin support vector regression (TSVR). -FTSVR is achieved by incorporating trapezoidal fuzzy numbers to -TSVR which takes care of uncertainty existing in forecasting problems. -FTSVR determines a pair of -insensitive proximal functions by solving two related quadratic programming problems. The structural risk minimization principle is implemented by introducing regularization term in primal problems of -FTSVR. This yields dual stable positive definite problems which improves regression performance. -FTSVR is then reformulated as HFTSVR consisting of a set of hierarchical layers each containing FTSVR. Experimental results on both synthetic and real datasets reveal that -HFTSVR has remarkable generalization performance with minimum training time. Keywords—Regression, -TSVR, -FTSVR, -HFTSVR",
    "creator" : "Microsoft® Word 2013"
  }
}