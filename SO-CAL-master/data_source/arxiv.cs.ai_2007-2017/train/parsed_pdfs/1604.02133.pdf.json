{
  "name" : "1604.02133.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Revising Incompletely Specified Convex Probabilistic Belief Bases",
    "authors" : [ "Gavin Rens", "Thomas Meyer", "Giovanni Casini" ],
    "emails" : [ "gavinrens@gmail.com", "tmeyer@cs.uct.ac.za", "giovanni.casini@uni.lu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Suppose an agent represents its probabilistic knowledge with a set of statements; every statement says something about the probability of some features the agent is aware of. Ideally, the agent would want to have enough information to, at least, identify one probability distribution over all the situations (worlds) it deems possible. However, if the agent could not gather sufficient data or if it was not told or given sufficient information, it would not be able to pinpoint exactly one probability distribution. An agent with this sort of ignorance, can be thought of as having beliefs compatible with a set of distributions. Now, this agent might need to revise its beliefs when new (non-probabilistic) information is received, even though the agent’s beliefs do not characterize a particular probability distribution over its current possible worlds.\nSeveral researchers argue that using a single probability distribution requires the agent to make unrealistically precise uncertainty distinctions (Grove and Halpern, 1998; Voorbraak, 1999; Yue and Liu, 2008).1 “One widelyused approach to dealing with this has been to consider\n∗Centre for Artificial Intelligence Research Copyright c© 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1See also the references in these cited papers concerning criticisms against traditional probability theory.\nsets of probability measures as a way of modeling uncertainty,” (Grove and Halpern, 1998). However, simply applying standard probabilistic conditioning to each of the measures/distributions in the set individually and then combining the results is also not recommended. The framework presented in this paper proposes two ways to go from one ‘probabilistically incomplete’ belief base to another when new information is acquired.\nBoth belief revision methods presented, essentially follow this process: From the original belief base, determine a relatively small set of belief states / probability distributions ‘compatible’ with the belief base which is, in a sense, representative of the belief base. (We shall use the terms belief state, probability distribution, probability function and distribution interchangeably). Then revise every belief state in this representative set. Finally, induce a new, revised belief base from the revised representative set.\nWe shall present two approaches to determine the representative set of belief states from the current belief base: (i) The approach we focus on involves finding belief states which, in a sense, are at the boundaries of the constraints implied by the belief base. These ‘boundary belief states’ can be thought of as drawing the outline of the convex space of beliefs. This outline is then revised to form a new outline shape, which can be translated into a new belief base. (ii) As a possible alternative approach, the representative set is a single belief state which can be imagined to be at the center of the outline of the first approach. This ‘central’ belief state is found by determining the one in the space of beliefs which is least biased or most entropic in terms of information theory (Jaynes, 1978; Cover and Thomas, 1991).\nFor approach (i) – where the canonical set is the set of boundary belief states – we shall prove that the revised canonical set characterizes the set of all belief states which would have resulted from revising all (including interior) belief states compatible with the original belief base.\nThe relevant background theory and notations are now introduced.\nWe shall work with classical propositional logic. Let P be the finite set of atomic propositional variables (atoms, for short). Formally, a world is a unique assignment of truth values to all the atoms in P . There are thus 2n conceivable worlds. An agent may consider some non-empty subset W\nar X\niv :1\n60 4.\n02 13\n3v 1\n[ cs\n.A I]\n7 A\npr 2\n01 6\nof the conceivable worlds called the possible worlds. Often, in the exposition of this paper, a world will be referred to by its truth vector. For instance, if the vocabulary is placed in order 〈q, r〉 and w3 ¬q ∧ r, then w3 may be referred to as 01.2 Let L be all propositional formulae which can be formed from P and the logical connectives ∧ and ¬, with > abbreviating tautology and ⊥ abbreviating contradiction.\nLet β be a sentence in L. [β] denotes the set of β-worlds, that is, the elements ofW satisfying β. The worlds satisfying all sentences in a set of sentences K are denoted by [K].\nWe define the probabilistic language Lprob = {(α) ./ x | α ∈ L, ./∈ {≤,=,≥}, x ∈ [0, 1]}. Sentences with strict inequalities (<,>) are excluded from the language for now. Such sentences are more challenging to deal with and their inclusion is left for future work. We propose a belief base (BB) to be a consistent (logically satisfiable) subset ofLprob . A BB specifies an agent’s knowledge.\nThe basic semantic element of an agent’s beliefs is a probability distribution or a belief state\nb = {(w1, p1), (w2, p2), . . . , (wn, pn)},\nwhere pi is the probability that wi is the actual world in which the agent is. ∑ (w,p)∈b p = 1. We may also use c to refer to a belief state. For parsimony, let b = 〈p1, . . . , pn〉 be the probabilities that belief state b assigns to w1, . . . , wn where 〈w1, w2, w3, w4〉 = 〈11, 10, 01, 00〉, and 〈w1, w2, . . . , w8〉= 〈111, 110, . . . , 000〉. Let Π be the set of all belief states over W . b(α) abbreviates ∑ w∈W,w α b(w). b satisfies formula (α) ./ x (denoted b (α) ./ x) iff b(α) ./ x. If B is a set of formulae, then b satisfies B (denoted b B) iff ∀γ ∈ B, b γ. If B and B′ are sets of formulae, then B entailsB′ (denotedB |= B′) iff for all b ∈ Π, b B′ whenever b B. If B |= {γ} then we simply write B |= γ. B is logically equivalent to B′ (denoted B ≡ B′) iff B |= B′ and B′ |= B.\nInstead of an agent’s beliefs being represented by a single belief state, a BB B represents a set of belief-states: Let ΠB := {b ∈ Π | b B}. A BB B is satisfiable (consistent) iff ΠB 6= ∅.\nThe technique of Lewis imaging for the revision of belief states, requires a notion of distance between worlds to be defined. We use a pseudo-distance measure between worlds, as defined by Lehmann, Magidor, and Schlechta (2001) and adopted by Chhogyal et al. (2014).\nWe add a ‘faithfulness’ condition, which we feel is lacking from the definition of Lehmann, Magidor, and Schlechta (2001): without this condition, a pseudo-distance measure would allow all worlds to have zero distance between them. Boutilier (1998) mentions this condition, and we use his terminology: “faithfulness”. Definition 1. A pseudo-distance function d : W ×W → Z satisfies the following four conditions: for all worlds w,w′, w′′ ∈W ,\n1. d(w,w′) ≥ 0 (Non-negativity) 2w α is read ‘w is a model for/satisfies α’.\n2. d(w,w) = 0 (Identity) 3. d(w,w′) = d(w′, w) (Symmetry)"
    }, {
      "heading" : "4. d(w,w′)+d(w′, w′′) ≥ d(w,w′′) (Triangular Inequality)",
      "text" : "5. if w 6= w′, then d(w,w′) > 0 (Faithfulness)\nPresently, the foundation theory, or paradigm, for studying belief change operations is commonly known as AGM theory (Alchourrón, Gärdenfors, and Makinson, 1985; Gärdenfors, 1988). Typically, belief change (in a static world) can be categorized as expansion, revision or contraction, and is performed on a belief set, the set of sentences K closed under logical consequence. Expansion (denoted +) is the logical consequences of K ∪ {α}, where α is new information and K is the current belief set. Contraction of α is the removal of some sentences until α cannot be inferred from K. It is the reduction of beliefs. Revision is when α is (possibly) inconsistent with K and K is (minimally) modified so that the new K remains consistent and entails α. In this view, when the new information is consistent with the original beliefs, expansion and revision are equivalent.\nThe next section presents a generalized imaging method for revising probabilistic belief states. Then we describe the application of generalized imaging in our main contribution; revising boundary belief states instead of all belief states. The subsequent section explain another approaches of revising our belief bases, which prepares us for discussions in the rest of the paper. The latter method finds a single representative belief state through maximum entropy inference. Both the boundary belief state method and the maximum entropy method are reasonable, yet yield different results – a seeming paradox is thus uncovered. Then future possible directions of research are discussed. We end with a section on the related work and the concluding section.\nGeneralized Imaging It is not yet universally agreed what revision means in a probabilistic setting. One school of thought says that probabilistic expansion is equivalent to Bayesian conditioning. This is evidenced by Bayesian conditioning (BC) being defined only when b(α) 6= 0, thus making BC expansion equivalent to BC revision. In other words, one could define expansion (restricted revision) to be\nb BC α = {(w, p) | w ∈W,p = b(w | α), b(α) 6= 0}.\nTo accommodate cases where b(α) = 0, that is, where α contradicts the agent’s current beliefs and its beliefs need to be revised in the stronger sense, we shall make use of imaging. Imaging was introduced by Lewis (1976) as a means of revising a probability function. It has also been discussed in the work of, for instance, Gärdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015). Informally, Lewis’s original solution for accommodating contradicting evidence α is to move the probability of each world to its closest, α-world. Lewis made the strong assumption that every world has a unique closest α-world. More general versions of imaging allows worlds to have several, equally proximate, closest worlds.\nGärdenfors (1988) calls one of his generalizations of Lewis’s imaging general imaging. Our method is also a generalization. We thus refer to his as Gärdenfors’s general imaging and to our method as generalized imaging to distinguish them. It should be noted that all three these imaging methods are general revision methods and can be used in place of Bayesian conditioning for expansion. “Thus imaging is a more general method of describing belief changes than conditionalization,” (Gärdenfors, 1988, p. 112).\nLet Min(α,w, d) be the set of α-worlds closest to w with respect to pseudo-distance d. Formally,\nMin(α,w, d) :=\n{w′ ∈ [α] | ∀w′′ ∈ [α], d(w′, w) ≤ d(w′′, w)},\nwhere d(·) is some pseudo-distance measure between worlds (e.g., Hamming or Dalal distance). Example 1. Let the vocabulary be {q, r, s}. Let α be (q ∧ r) ∨ (q ∧ ¬r ∧ s). Suppose d is Hamming distance. Then\nMin((q ∧ r) ∨ (q ∧ ¬r ∧ s), 111, d) = {111} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 110, d) = {110} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 101, d) = {101} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 100, d) = {110, 101} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 011, d) = {111} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 010, d) = {110} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 001, d) = {101} Min((q ∧ r) ∨ (q ∧ ¬r ∧ s), 000, d) = {110, 101}\nDefinition 2 (GI). Then generalized imaging (denoted GI) is defined as\nb GI α := {(w, p) | w ∈W,p = 0 if w 6∈ [α], else p = ∑ w′∈W\nw∈Min(α,w′,d)\nb(w′)/|Min(α,w′, d)|}.\nIn words, b GI α is the new belief state produced by taking the generalized image of b with respect to α. Notice how the probability mass of non-α-worlds is shifted to their closest α-worlds. If a non-α-world w× with probability p has n closest α-worlds (equally distant), then each of these closest α-worlds gets p/n mass from w×.\nWe define b◦α := b ◦ α so that we can write b◦α(w), where ◦ is a revision operator. Example 2. Continuing on Example 1: Let b = 〈0, 0.1, 0, 0.2, 0, 0.3, 0, 0.4〉.\n(q ∧ r) ∨ (q ∧ ¬r ∧ s) is abbreviated as α. bGIα (111) = ∑\nw′∈W 111∈Min(α,w′,d)\nb(w′)/|Min(α,w′, d)|\n= b(111)/|Min(α, 111, d)| + b(011)/|Min(α, 011, d)| = 0/1 + 0/1 = 0.\nbGIα (110) = ∑\nw′∈W 110∈Min(α,w′,d)\nb(w′)/|Min(α,w′, d)|\n= b(110)/|Min(α, 110, d)| + b(100)/|Min(α, 100, d)| +\nb(010)/|Min(α, 010, d)| + b(000)/|Min(α, 000, d)| = 0.1/1 + 0.2/2 + 0.3/1 + 0.4/2 = 0.7.\nbGIα (101) = ∑\nw′∈W 101∈Min(α,w′,d)\nb(w′)/|Min(α,w′, d)|\n= b(101)/|Min(α, 101, d)| + b(100)/|Min(α, 100, d)| + b(001)/|Min(α, 001, d)| + b(000)/|Min(α, 000, d)| = 0/1 + 0.2/2 + 0/1 + 0.4/2 = 0.3.\nAnd bGIα (100) = b GI α (011) = b GI α (010) = b GI α (001) =\nbGIα (000) = 0.\nRevision via GI and boundary belief states Perhaps the most obvious way to revise a given belief base (BB) B is to revise every individual belief state in ΠB and then induce a new BB from the set of revised belief states. Formally, given observation α, first determine a new belief state bα for every b ∈ ΠB via the defined revision operation:\nΠB α = {bα ∈ Π | bα = b GI α, b ∈ ΠB}.\nIf there is more than only a single belief state in ΠB , then ΠB contains an infinite number of belief states. Then how can one compute ΠB α\n? And how would one subsequently determine Bα from ΠB α\n? In the rest of this section we shall present a finite method\nof determining ΠB α\n. What makes this method possible is the insight that ΠB can be represented by a finite set of ‘boundary’ belief states – those belief states which, in a sense, represent the limits or the convex hull of ΠB . We shall prove that the set of revised boundary belief states defines ΠB α\n. Inducing Bα from ΠB α\nis then relatively easy, as will be seen.\nLet W perm be every permutation on the ordering of worlds in W . For instance, if W = {w1, w2, w3, w4}, then W perm = {〈w1, w2, w3, w4〉, 〈w1, w2, w4, w3〉, 〈w1, w3, w2, w4〉, . . ., 〈w4, w3, w2, w1〉}. Given an ordering W# ∈ W perm , let W#(i) be the i-th element of W#; for instance, 〈w4, w3, w2, w1〉(2) = w3. Suppose we are given a BB B. We now define a function which, given a permutation of worlds, returns a belief state where worlds earlier in the ordering are assigned maximal probabilities according to the boundary values enforced by B.\nDefinition 3. MaxASAP(B,W#) is the b ∈ ΠB such that for i = 1, . . . , |W |, ∀b′ ∈ ΠB , if b′ 6= b, then∑i j=1 b(W #(j)) ≥ ∑i k=1 b ′(W#(k)).\nExample 3. Suppose the vocabulary is {q, r} and B1 = {(q) ≥ 0.6}. Then, for instance, MaxASAP(B1, 〈01, 00, 11, 10〉) = {(01, 0.4), (00, 0), (11, 0.6), (10, 0)} = {(11, 0.6), (10, 0), (01, 0.4), (00, 0)}. Definition 4. We define the boundary belief states of BB B as the set\nΠBbnd := {b ∈ ΠB | W# ∈W perm , b = MaxASAP(B,W#)}\nNote that |ΠBbnd | ≤ |W perm |.\nExample 4. Suppose the vocabulary is {q, r} and B1 = {(q) ≥ 0.6}. Then\nΠB1bnd = {{(11, 1.0), (10, 0.0), (01, 0.0), (00, 0.0)}, {(11, 0.0), (10, 1.0), (01, 0.0), (00, 0.0)}, {(11, 0.6), (10, 0.0), (01, 0.4), (00, 0.0)}, {(11, 0.6), (10, 0.0), (01, 0.0), (00, 0.4)}, {(11, 0.0), (10, 0.6), (01, 0.4), (00, 0.0)}, {(11, 0.0), (10, 0.6), (01, 0.0), (00, 0.4)}}.\nNext, the revision operation is applied to every belief state in ΠBbnd . Let (Π B bnd) GI α := {b′ ∈ Π | b′ = bGIα , b ∈ ΠBbnd}.\nExample 5. Suppose the vocabulary is {q, r} and B1 = {(q) ≥ 0.6}. Let α be (q ∧ ¬r) ∨ (¬q ∧ r). Then\n(ΠB1bnd) GI α = {{(11, 0.0), (10, 0.5), (01, 0.5), (00, 0.0)},\n{(11, 0.0), (10, 1.0), (01, 0.0), (00, 0.0)}, {(11, 0.0), (10, 0.3), (01, 0.7), (00, 0.0)}, {(11, 0.0), (10, 0.6), (01, 0.4), (00, 0.0)}, {(11, 0.0), (10, 0.8), (01, 0.2), (00, 0.0)}}.\n(Two revision operations produce {(11, 0), (10, 0.5), (01, 0.5), (00, 0)}.)\nTo induce the new BBBαbnd from (Π B bnd) GI α , the following procedure is executed. For every possible world, the procedure adds a sentence enforcing the upper (resp., lower) probability limit of the world, with respect to all the revised boundary belief states. Trivial limits are excepted.\nFor every w ∈ W , (φw) ≤ y ∈ Bα, where y = maxb∈(ΠBbnd )GIα b(w), except when y = 1, and (φw) ≥ y ∈ Bα, where y = minb∈(ΠBbnd )GIα b(w), except when y = 0.\nThe intention is that the procedure specifies Bα to represent the upper and lower probability envelopes of the set of revised boundary belief states – Bα thus defines the entire revised belief state space (cf. Theorem 1).\nExample 6. Continuing Example 5, using the translation procedure just above, we see that Bα1bnd = {(φ11) ≤ 0, (φ10) ≥ 0.3, (φ01) ≤ 0.7, (φ00) ≤ 0.0}.\nNote that if we let B′ = {((q ∧ ¬r) ∨ (¬q ∧ r)) = 1, (q ∧ ¬r) ≥ 0.3}, then ΠB′ = ΠBα1bnd . Example 7. Suppose the vocabulary is {q, r} and B2 = {(¬q ∧ ¬r) = 0.1}. Let α be ¬q. Then\nΠB2bnd = {{(11, 0.9), (10, 0), (01, 0), (00, 0.1)}, {(11, 0), (10, 0.9), (01, 0), (00, 0.1)}, {(11, 0), (10, 0), (01, 0.9), (00, 0.1)}},\n(ΠB2bnd) GI α = {{(11, 0), (10, 0), (01, 0.9), (00, 0.1)},\n{(11, 0), (10, 0), (01, 0), (00, 1)}} and\nBα2bnd = {(φ11) ≤ 0, (φ10) ≤ 0, (φ01) ≤ 0.9, (φ00) ≥ 0.1}.\nNote that if we letB′ = {(¬q) = 1, (¬q∧r) ≤ 0.9}, then ΠB ′ = ΠB α 2bnd .\nLet WMin(α,d) be a partition of W such that {wi1, . . . , wini} is a block in WMin(α,d) iff |Min(α,wi1, d)| = · · · = |Min(α,wini, d)|. Denote an element of block {wi1, . . . , wini} as wi, and the block of which wi is an element as [wi]. Let i = |Min(α,wi, d)|, in other words, the superscript in wi indicates the size of Min(α,wi, d). Let m := maxw∈W |Min(α,w, d)|. Observation 1. Let δ1, δ2, . . . , δm be positive integers such that i < j iff δi < δj . Let ν1, ν2, . . . , νm be values in [0, 1] such that ∑m k=1 νk = 1. Associate with every νi a maximum value it is allowed to take: most(νi). For every νi, we define the assignment value\nav(νi) :=\n{ most(νi) if ∑i k=1 ≤ 1\n1− ∑i−1 k=1 otherwise\nDetermine first av(ν1), then av(ν2) and so on. Then\nav(ν1) δ1 + · · ·+ av(νm) δm > ν′1 δ1 + · · ·+ ν ′ m δm\nwhenever ν′i 6= av(νi) for some i. For instance, let δ1 = 1, δ2 = 2, δ3 = 3, δ4 = 4. Let most(ν1) = 0.5, most(ν2) = 0.3, most(ν3) = 0.2, most(ν4) = 0.3. Then av(ν1) = 0.5, av(ν2) = 0.3, av(ν3) = 0.2, av(ν4) = 0 and\n0.5\n1 +\n0.3\n2 +\n0.2\n3 +\n0 4 = 0.716.\nBut 0.49\n1 +\n0.3\n2 +\n0.2\n3 +\n0.01\n4 = 0.709.\nAnd 0.5\n1 +\n0.29\n2 +\n0.2\n3 +\n0.01\n4 = 0.714.\nLemma 1 essentially says that the belief state in ΠB which causes a revised belief state to have a maximal value at world w (w.r.t. all belief states in ΠB), will be in ΠBbnd . Lemma 1. For all w ∈ W , arg maxbX∈ΠB ∑ w′∈W\nw∈Min(α,w′,d) bX(w\n′)/|Min(α,w′, d)| is\nin ΠBbnd .\nProof. Note that∑ w′∈W\nw∈Min(α,w′,d)\nb(w′)/|Min(α,w′, d)|\ncan be written in the form∑ w′∈[w1]\nw∈Min(α,w′,d) b(w′)\n1 + · · ·+\n∑ w′∈[wm]\nw∈Min(α,w′,d) b(w′)\nm .\nObserve that there must be a W# ∈ W perm such that W# = 〈w11, . . . , w1n1, . . . , wm1 , . . . , wmnm〉. Then by the\ndefinition of the set of boundary belief states (Def. 4), MaxASAP(B,W#) will assign maximal probability mass to [w1] = {w11, . . . , w1n1}, then to [w2] = {w21, . . . , wmn2} and so on.\nThat is, by Observation 1, for some bx ∈ ΠBbnd , bx(w) = maxbX∈ΠB ∑ w′∈W\nw∈Min(α,w′,d) bX(w\n′)/|Min(α,w′, d)|\nfor all w ∈ W . Therefore, arg maxbX∈ΠB ∑ w′∈W\nw∈Min(α,w′,d) bX(w\n′)/|Min(α,w′, d)| is\nin ΠBbnd .\nLet xw := maxb∈ΠBbnd b(w) X w := maxb∈ΠB b(w)\nyw := maxb∈(ΠBbnd )GIα b(w) Y w := maxb∈(ΠB)GIα b(w)\nxw := minb∈ΠBbnd b(w) X w := minb∈ΠB b(w)\nyw := minb∈(ΠBbnd )GIα b(w) Y w := minb∈(ΠB)GIα b(w)\nLemma 2 states that for every world, the upper/lower probability of the world with respect to ΠBbnd is equal to the upper/lower probability of the world with respect to ΠB . The proof requires Observation 1 and Lemma 1.\nLemma 2. For all w ∈W , yw = Y w and yw = Y w.\nProof. Note that if w 6∈ [α], then yw = Y w = 0 and yw = Y w = 0.\nWe now consider the cases where w ∈ [α].\nyw = Y w\niff max\nb∈(ΠBbnd ) b(w) = max b∈(ΠB) b(w)\niff\nmax bx∈ΠBbnd\n∑ w′∈W\nw∈Min(α,w′,d)\nbx(w ′)/|Min(α,w′, d)|\n= max bX∈ΠB\n∑ w′∈W\nw∈Min(α,w′,d)\nbX(w ′)/|Min(α,w′, d)|\nif bx(w) = bX(w), where\nbx(w) := max bx∈ΠBbnd\n∑ w′∈W\nw∈Min(α,w′,d)\nbx(w ′)/|Min(α,w′, d)|\nand\nbX(w) := max bX∈ΠB\n∑ w′∈W\nw∈Min(α,w′,d)\nbX(w ′)/|Min(α,w′, d)|.\nNote that ∑ w′∈W\nw∈Min(α,w′,d)\nb(w′)/|Min(α,w′, d)|\ncan be written in the form∑ w′∈[w1]\nw∈Min(α,w′,d) b(w′)\n1 + · · ·+\n∑ w′∈[wm]\nw∈Min(α,w′,d) b(w′)\nm .\nThen by Observation 1, bX(w) is in ΠBbnd . And also by Lemma 1, the belief state in ΠBbnd identified by bX(w) must be the one which maximizes∑\nw′∈W w∈Min(α,w′,d)\nbx(w ′)/|Min(α,w′, d)|,\nwhere bx ∈ ΠBbnd . That is, bx = bX . With a symmetrical argument, it can be shown that yw = Y w.\nIn intuitive language, the following theorem says that the BB determined through the method of revising boundary belief states captures exactly the same beliefs and ignorance as the belief states in ΠB which have been revised. This correspondence relies on the fact that the upper and lower probability envelopes of ΠB can be induce from ΠBbnd , which is what Lemma 2 states.\nTheorem 1. Let (ΠB)GIα := {bGIα ∈ Π | b ∈ ΠB}. Let Bαbnd be the BB induced from (ΠBbnd) GI α . Then Π Bαbnd = (ΠB)GIα .\nProof. We show that ∀b′∈Π, b′∈ΠBαbnd ⇐⇒ b′∈(ΠB)GIα . (⇒) b′ ∈ ΠBαbnd implies ∀w ∈ W , yw ≤ b′(w) ≤ yw (by definition of Bαbnd ). Lemma 2 states that for all w ∈W , yw = Y w and yw = Y w. Hence, ∀w ∈ W , Y w ≤ b′(w) ≤ Y w\nTherefore, b′(w) ∈ (ΠB)GIα . (⇐) b′(w) ∈ (ΠB)GIα implies ∀w ∈ W , Y\nw ≤ b′(w) ≤ Y w\n. Hence, by Lemma 2, ∀w ∈ W , yw ≤ b′(w) ≤ yw. Therefore, by definition of Bαbnd , b ′∈ΠBαbnd ."
    }, {
      "heading" : "Revising via a Representative Belief State",
      "text" : "Another approach to the revision of a belief base (BB) is to determine a representative of ΠB (call it brep), change the representative belief state via the the defined revision operation and then induce a new BB from the revised representative belief state. Selecting a representative probability function from a family of such functions is not new (Goldszmidt, Morris, and Pearl, 1990; Paris, 1994, e.g.). More formally, given observation α, first determine brep ∈ ΠB , then compute its revision bαrep , and finally induce B\nα from bαrep . We shall represent ΠB (and thus B) by the single ‘least biased’ belief state, that is, the belief state in ΠB with highest entropy:\nDefinition 5 (Shannon Entropy). H(b) := − ∑ w∈W b(w) ln b(w),\nwhere b is a belief state.\nDefinition 6 (Maximum Entropy). Traditionally, given some set of distributions Π, the most entropic distribution in Π is defined as\nbH := arg max b∈Π H(b).\nSuppose B2 = {(¬q ∧ ¬r) = 0.1}. Then the belief state b ∈ ΠB2 satisfying the constraints posed by B2 for which H(b) is maximized is brep = bH = 〈0.3, 0.3, 0.3, 0.1〉.\nThe above distribution can be found directly by applying the principle of maximum entropy: The true belief state is estimated to be the one consistent with known constraints, but is otherwise as unbiased as possible, or “Given no other knowledge, assume that everything is as random as possible. That is, the probabilities are distributed as uniformly as possible consistent with the available information,” (Poole and Mackworth, 2010). Obviously world 00 must be assigned probability 0.1. And the remaining 0.9 probability mass should be uniformly spread across the other three worlds.\nApplying GI to brep on evidence ¬q results in b¬qrep = 〈0, 0, 0.6, 0.4〉. Example 8. Suppose the vocabulary is {q, r}, B1 = {(q) ≥ 0.6} and α is (q ∧ ¬r) ∨ (¬q ∧ r). Then brep = arg maxb∈ΠB1 H(b) = 〈0.3, 0.3, 0.2, 0.2〉. Applying GI to brep on α results in bαrep = 〈0, 0.61, 0.39, 0〉. bαrep can be translated intoBα1rep as {(q∧¬r) = 0.61, (¬q∧r) = 0.39}.\nStill using α = (q ∧ ¬r) ∨ (¬q ∧ r), notice that ΠB\nα 1rep 6= ΠBα1bnd . But how different areBα1rep = {(q∧¬r) = 0.61, (¬q ∧ r) = 0.39} and Bα1bnd = {(q ∧ r) ≤ 0, (q∧¬r) ≥ 0.3, (¬q∧ r) ≤ 0.7, (¬q∧¬r) ≤ 0.0}? Perhaps one should ask, how different Bα1rep is from the representative of Bα1bnd : The least biased belief state satisfying B α 1bnd is 〈0, 0.5, 0.5, 0〉. That is, How different are 〈0, 0.61, 0.39, 0〉 and 〈0, 0.5, 0.5, 0〉?\nIn the case of B2, we could compare B ¬q 2bnd = {(φ11) ≤ 0, (φ10) ≤ 0, (φ01) ≤ 0.9, (φ00) ≥ 0.1} with b¬qrep = 〈0, 0, 0.6, 0.4〉. Or if we take the least biased belief state satisfying B¬q2bnd , we can compare 〈0, 0, 0.5, 0.5〉 with 〈0, 0, 0.6, 0.4〉.\nIt has been extensively argued (Jaynes, 1978; Shore and Johnson, 1980; Paris and Vencovsk, 1997) that maximum entropy is a reasonable inference mechanism, if not the most reasonable one (w.r.t. probability constraints). And in the sense that the boundary belief states method requires no compression / information loss, it also seems like a very reasonable inference mechanism for revising BBs as defined here. Resolving this misalignment in the results of the two methods is an obvious task for future research."
    }, {
      "heading" : "Future Directions",
      "text" : "Some important aspects still missing from our framework are the representation of conditional probabilistic information such as is done in the work of Kern-Isberner, and the association of information with its level of entrenchment. On the latter point, when one talks about probabilities or likelihoods, if one were to take a frequentist perspective, information observed more (less) often should become more\n(less) entrenched. Or, without considering observation frequencies, an agent could be designed to have, say, one or two sets of deeply entrenched background knowledge (e.g., domain constraints) which does not change or is more immune to change than ‘regular’ knowledge.\nGiven that we have found that the belief base resulting from revising via the boundary-belief-states approach differs from the belief base resulting from revising via the representative-belief-state approach, the question arises, When is it appropriate to use a representative belief state defined as the most entropic belief state of a given set ΠB? This is an important question, especially due to the popularity of employing the Maximum Entropy principle in cases of undespecified probabilistic knowledge (Jaynes, 1978; Goldszmidt, Morris, and Pearl, 1990; Hunter, 1991; Voorbraak, 1999; Kern-Isberner, 2001; Kern-Isberner and Rdder, 2004) and the principle’s well-behavedness (Shore and Johnson, 1980; Paris, 1994; Kern-Isberner, 1998).\nKatsuno and Mendelzon (1991) modified the eight AGM belief revision postulates (Alchourrón, Gärdenfors, and Makinson, 1985) to the following six (written in the notation of this paper), where ∗ is some revision operator.3\n• Bα∗ |= (α) = 1. • IfB∪{(α) = 1} is satisfiable, thenBα∗ ≡ B∪{(α) = 1}. • If (α) = 1 is satisfiable, then Bα∗ is also satisfiable. • If α ≡ β, then Bα∗ ≡ B β ∗ . • Bα∗ ∪ {(β) = 1} |= B α∧β ∗ . • If Bα∗ ∪ {(β) = 1} is satisfiable, then B α∧β ∗ |= Bα∗ ∪\n{(β) = 1}. Testing the various revision operations against these postulates is left for a sequel paper.\nAn extended version of maximum entropy is minimum cross-entropy (MCE) (Kullback, 1968; Csiszár, 1975):\nDefinition 7 (Minimum Cross-Entropy). The ‘directed divergence’ of distribution c from distribution b is defined as\nR(c, b) := ∑ w∈W c(w) ln c(w) b(w) .\nR(c, b) is undefined when b(w) = 0 while c(w) > 0; when c(w) = 0, R(c, b) = 0, because limx→0 ln(x) = 0. Given new evidence φ ∈ Lprob , the distribution c satisfying φ diverging least from current belief state b is\narg min c∈Π,c φ R(c, b).\nDefinition 8 (MCI). Then MCE inference (denoted (MCI)) is defined as\nbMCI α := arg min b′∈Π,b′ (α)=1\nR(b′, b).\nIn the following example, we interpret revision as MCE inference.\n3In these postulates, it is sometimes necessary to write an observation α as a BB, i.e., as {(α) = 1} – in the present framework, observations are regarded as certain.\nExample 9. Suppose the vocabulary is {q, r} and B1 = {(q) ≥ 0.6}. Let α be (q ∧ ¬r) ∨ (¬q ∧ r). Then\nΠB1bnd = {{(11, 1.0), (10, 0.0), (01, 0.0), (00, 0.0)}, {(11, 0.0), (10, 1.0), (01, 0.0), (00, 0.0)}, {(11, 0.6), (10, 0.0), (01, 0.4), (00, 0.0)}, {(11, 0.6), (10, 0.0), (01, 0.0), (00, 0.4)}, {(11, 0.0), (10, 0.6), (01, 0.4), (00, 0.0)}, {(11, 0.0), (10, 0.6), (01, 0.0), (00, 0.4)}},\n(ΠB1bnd) MCI α = {{(11, 0), (10, 0), (01, 1), (00, 0)},\n{(11, 0), (10, 1), (01, 0), (00, 0)}, {(11, 0), (10, 0.6), (01, 0.4), (00, 0)}} and\nBα1bnd = {(φ11) ≤ 0, (φ00) ≤ 0}. Note that if we let B′ = {((q ∧ ¬r) ∨ (¬q ∧ r)) = 1}, then ΠB ′ = ΠB α 1bnd .\nRecall from Example 6 that B′ included (q ∧ ¬r) ≥ 0.3. Hence, in this particular case, combining the boundary belief states approach with MCI results in a less informative revised belief base than when GI is used. The reason for the loss of information might be due to R(·, {(11, 1.0), (10, 0.0), (01, 0.0), (00, 0.0)}) and R(·, {(11, 0.6), (10, 0.0), (01, 0.0), (00, 0.4)}) being undefined: Recall that R(c, b) is undefined when b(w) = 0 while c(w) > 0. But then there is no belief state c for which c α and R(·) is defined (with these two belief states as arguments). Hence, there are no revised counterparts of these two belief states in (ΠB1bnd) MCI α . We would like to analyse MCI more within this framework. In particular, in the future, we would like to determine whether a statement like Theorem 1 holds for MCI too.\nIn MCE inference, b-consistency of evidence φ is defined as: There exists a belief state c such that c φ and c is totally continuous with respect to b (i.e., b(w) = 0 implies c(w) = 0). MCE is undefined when the evidence is not bconsistent. This is analogous to Bayesian conditioning being undefined for b(α) = 0. Obviously, this is a limitation of MCE because some belief states may not be considered as candidate revised belief states. Admittedly, we have not searched the literature on this topic due to it being out of the present scope.\nAs far as we know, imaging for belief change has never been applied to (conditional) probabilistic evidence. Due to issues with many revision methods required to be consistent with prior beliefs, and imaging not having this limitation, it might be worthwhile investigating.\nThe translation from the set of belief states back to a belief base is a mapping from every belief state to a probability formula. The size of the belief base is thus in the order of |W perm |, where |W | is already exponential in the size of P , the set of atoms. As we saw in several examples in this paper, the new belief base often has a more concise equivalent counterpart. It would be useful to find a way to consistently determine more concise belief bases than our present approach does.\nThe computational complexity of the process to revise a belief base is at least exponential. This work focused on theoretical issues. If the framework presented here is ever used in practice, computations will have to be optimized.\nThe following example illustrates how one might deal with strict inequalities.\nExample 10. Suppose the vocabulary is {q, r} and B3 = {(q) > 0.6}. Let α be (q ∧ ¬r) ∨ (¬q ∧ r). Let be a real number which tends to 0. Then ΠB3bnd =\n{{(11, 1.0), (10, 0.0), (01, 0.0), (00, 0.0)}, {(11, 0.0), (10, 1.0), (01, 0.0), (00, 0.0)}, {(11, 0.6 + ), (10, 0.0), (01, 0.4− ), (00, 0.0)}, {(11, 0.6 + ), (10, 0.0), (01, 0.0), (00, 0.4− )}, {(11, 0.0), (10, 0.6 + ), (01, 0.4− ), (00, 0.0)}, {(11, 0.0), (10, 0.6 + ), (01, 0.0), (00, 0.4− )}},\n(ΠB3bnd) GI α =\n{{(11, 0.0), (10, 0.5), (01, 0.5), (00, 0.0)}, {(11, 0.0), (10, 1.0), (01, 0.0), (00, 0.0)}, {(11, 0.0), (10, 0.3 + ), (01, 0.7− ), (00, 0.0)}, {(11, 0.0), (10, 0.6 + ), (01, 0.4− ), (00, 0.0)}, {(11, 0.0), (10, 0.8 + ), (01, 0.2− ), (00, 0.0)} and\nBα3bnd = {(φ11) ≤ 0, (φ10) ≥ 0.3 + , (φ01) ≤ 0.7 − , (φ00) ≤ 0.0}.\nNote that if we let B′ = {((q ∧ ¬r) ∨ (¬q ∧ r)) = 1, (q ∧ ¬r) > 0.3}, then ΠB′ = ΠBα3bnd .\nIt has been suggested by one of the reviewers that GI could be an affine map (i.t.o. geometry), thus allowing the proof of Theorem 1 to refer to existing results in the study of affine maps to significantly simplify the proof. The authors are not familiar with affine maps and thus leave investigation of the suggestion to other researchers."
    }, {
      "heading" : "Related Work",
      "text" : "Voorbraak (1999) proposed the partial probability theory (PTT), which allows probability assignments to be partially determined, and where there is a distinction between probabilistic information based on (i) hard background evidence and (ii) some assumptions. He does not explicitly define the “constraint language”, however, from his examples and discussions, one can infer that he has something like the language LPTT in mind: it contains all formulae which can be formed with sentences in our Lprob in combination with connectives ¬,∧ and ∨. A “belief state” in PTT is defined as the quadruple 〈Ω,B,A, C〉, where Ω is a sample space, B ⊂ LPTT is a sets of probability constraints,A ⊂ LPTT is a sets of assumptions and C ⊆W “represents specific information concerning the case at hand” (an observation or evidence).4 Our epistemic state can be expressed as a restricted PTT “belief state” by letting Ω = W , B = B, A = ∅ and\n4Voorbraak (1999)’s “belief state” would rather be called and epistemic state or knowledge structure in our language.\nC = {w ∈ W | w α}, where B is a belief base and α is an observation in our notation.\nVoorbraak (1999) mentions that he will only consider conditioning where the evidence does not contradict the current beliefs. He defines the set of belief states corresponding to the conditionalized PPT “belief state” as {b(· | C) ∈ Π | b ∈ ΠB∪A, b(C) > 0}. In our notation, this corresponds to {(b BC α) ∈ Π | b ∈ ΠB , b(α) > 0}, where α corresponds to C.\nVoorbraak (1999) proposes constraining as an alternative to conditioning: Let φ ∈ Lprob be a probability constraint. In our notation, constraining ΠB on φ produces ΠB∪{φ}.\nNote that expanding a belief set reduces the number of models (worlds) and expanding a PPT ”belief state” with extra constraints also reduces the number of models (belief states / probability functions).\nIn the context of belief sets, it is possible to obtain any belief state from the ignorant belief state by a series of expansions. In PPT, constraining, but not conditioning, has the analogous property. This is one of the main reasons we prefer to constraining and not conditioning to be the probabilistic version of expansion. (Voorbraak, 1999, p. 4)\nBut Voorbraak does not address the issue that C and φ are different kinds of observations, so constraining, as defined here, cannot be an alternative to conditioning. C cannot be used directly for constraining and φ cannot be used directly for conditioning.\nW.l.o.g., we can assume C is represented by α. If we take b GI α to be an expansion operation whenever b(α) > 0, then one might ask, Is it possible to obtain any belief base B′ from the ignorant belief baseB = ∅ by a series of expansions, using our approach? The answer is, No. For instance, there is no observation or series of observations which can change B = {} into B′ = {(q) ≥ 0.6}. But if we were to allow sentences (constraints) in Lprob to be observations, then we could obtain any B′ from the ignorant B.\nGrove and Halpern (1998) investigate what “update” (incorporation of an observation with current beliefs, such that the observation does not contradict the beliefs) means in a framework where beliefs are represented by a set of belief states. They state that the main purpose of their paper is to illustrate how different the set-of-distributions framework can be, “technically”, from the standard single-distribution framework. They propose six postulates characterizing what properties an update function should have. They say that some of the postulates are obvious, some arguable and one probably too strong. Out of seven (families of) update functions only the one based on conditioning (Updcond(·)) and the one based on constraining (Updconstrain(·)) satisfy all six postulates, where Updcond(Π\nB , α) := {(b BC α) ∈ Π | b ∈ ΠB , b(α) > 0} and where they interpret Voorbraak’s (1999) constraining as Updconstrain(Π\nB , α) := {b ∈ ΠB | b(α) = 1}. Grove and Halpern (1998) do not investigate the case when an observation must be incorporated while it is (possibly) inconsistent with the old beliefs (i.e., revision).\nKern-Isberner (2001) develops a new perspective of probabilistic belief change. Based on the ideas of Alchourrón,\nGärdenfors, and Makinson (1985) and Katsuno and Mendelzon (1991) (KM), the operations of revision and update, respectively, are investigated within a probabilistic framework. She employs as basic knowledge structure a belief base (b,R), where b is a probability distribution (belief state) of background knowledge and R is a set of probabilistic conditionals of the form A B[x] meaning ‘The probability of B, given A, is x. A universal inference operation – based on the techniques of optimum entropy – is introduced as an “adequate and powerful method to realize probabilistic belief change”.\nBy having a belief state available in the belief base, minimum cross-entropy can be used. The intention is then that an agent with belief base (b, T ) should always reason w.r.t. belief state bT := arg minc∈Π,c T R(c, b). Kern-Isberner (2001) then defines the probabilistic belief revision of (b,R) by evidence S as (b,R ∪ S). And the probabilistic belief update of (b,R) by evidence S is defined as (bR,S).5 She distinguishes between revision as a knowledge adding process, and updating as a change-recording process. KernIsberner (2001) sets up comparisons of maximum crossentropy belief change with AGM revision and KM update. Cases where, for update, new information R is inconsistent with the prior distribution b, or, for revision, is inconsistent with b or the context R, are not dealt with (Kern-Isberner, 2001, p. 399, 400).\nHaving a belief state available for modification when new evidence is to be adopted is quite convenient. As Voorbraak (1999) argues, however, an agent’s ignorance can hardly be represented in an epistemic state where a single belief state must always be chosen.\nThe reader may also refer to a later paper (Kern-Isberner, 2008) in which many of the results of the work just reviewed are generalized to belief bases of the form (Ψ,R), where Ψ denotes a general epistemic state. In that paper, she considers two instantiations of Ψ, namely as a probability distribution and as an ordinal conditional function (first introduced by Spohn (1988)).\nYue and Liu (2008) propose a probabilistic revision operation for imprecise probabilistic beliefs in the framework of Probabilistic Logic Programming (PLP). New evidence may be a probabilistic (conditional) formula and needs not be consistent with the original beliefs. Revision via imaging (e.g., GI) also overcomes this consistency issue.\nEssentially, their probabilistic epistemic states Ψ are induced from a PLP program which is a set of formulae, each formula having the form (ψ | φ)[l, u], meaning that the probability of the conditional (ψ | φ) lies in the interval [l, u].\nThe operator they propose has the characteristic that if an epistemic state Ψ represents a single probability distribution, revising collapses to Jeffrey’s rule and Bayesian conditioning.\nThey mention that it is required that the models (distributions) of Ψ is a convex set. There might thus be an opportunity to employ their revision operation on a representative set of boundary distributions as proposed in this paper.\n5This is a very simplified version of what she presents. Please refer to the paper for details."
    }, {
      "heading" : "Conclusion",
      "text" : "In this paper, we propose an approach how to generate a new probabilistic belief base from an old one, given a new piece of non-probabilistic information, where a belief base is a finite set of sentences, each sentence stating the likelihood of a proposition about the world. In this framework, an agent’s belief base represents the set of belief states compatible with the sentences in it. In this sense, the agent is able to represent its knowledge and ignorance about the true state of the world.\nWe used a version of the so-called imaging approach to implement the revision operation.\nTwo methods were proposed: revising a finite set of ‘boundary belief states’ and revising a least biased belief state. We focussed on the former and showed that the latter gives different results.\nThere were two main contribution of this paper. The first was to prove that the set of belief states satisfying Bnew is exactly those belief states satisfying the original belief base, revised. The second was to uncover an interesting conflict in the results of the two belief base revision methods. It is worth further understanding the reasons behind such a difference, as such an investigation could give more insight about the mechanisms behind the two methods and indicate possible pros and cons of each."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The work of Giovanni Casini has been supported by the Fonds National de la Recherche, Luxembourg, and cofunded by the Marie Curie Actions of the European Commission (FP7-COFUND) (AFR/9181001)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : null,
    "creator" : "LaTeX with hyperref package"
  }
}