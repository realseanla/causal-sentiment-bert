{
  "name" : "1605.03915.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "OPTIMIZING HUMAN-INTERPRETABLE DIALOG MANAGEMENT POLICY USING GENETIC ALGORITHM",
    "authors" : [ "Hang Ren", "Weiqun Xu", "Yonghong Yan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— dialog management, reinforcement learning, genetic algorithm\n1. INTRODUCTION\nDialog manager (DM) plays a central part in spoken dialog system (SDS) and its major functionalities include tracking dialog states and maintaining a dialog policy which decides how the system reacts given certain dialog state. Designing a dialog policy by hand is tedious and erroneous because of the uncertainty of underlying dialog states especially in noisy environment. In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5]. The main objective of RL is to learn an optimum policy conducted by an agent by maximizing its cumulative reward. One of the advantages of RL-based DMs is its robustness to noises from automatic speech recognizer (ASR) and spoken language understanding (SLU) modules. Also, it automates the optimization process by allowing the agent to discover the optimum policy through exploring the underlying state-action space and incrementally improve the controlling policy.\nDespite all the advantages, RL-based DMs are not widely deployed for commercial SDSs due to several reasons [6]. Firstly, RL algorithms are mostly data-demanding, which leaves dialog system designers in a dilemma since there is usually few or even no data available at the early stage of system development. Several methods have been proposed to mitigate this problem. A user simulator is often firstly built using wizard-of-oz dialog data, and then the simulator is used to train a RL-based DM. In recent studies it has been shown that by incorporating domain knowledge into the design of kernel functions, the GPSARSA [7, 8] algorithm exhibits much faster learning speed than conventional online RL methods. Secondly, RL algorithms usually use complex numerical models in optimizing the value function, which are usually beyond human comprehension. The learned policy is implicitly represented in the optimized value function (Q-function), which is difficult or even impossible for system designer to verify or modify, keeping back domain experts from setting necessary constraints over the system behavior.\nIn this paper we propose to use Genetic Algorithm (GA) [9] in optimizing DM policies (GA-DM) which are comprehensible to human designers and easy to verify and modify. The underlying idea is intuitive. We use human-readable domain language to sketch the basic structure of the DM policy, and leave the uncertain parameters for later tuning. According to our experiences in deploying SDSs, it is relatively easy to specify a basic DM policy, when engineering slot-filling or task-driven SDSs of a moderate scale. The most difficult part lies in setting various threshold parameters in dealing with ASR and SLU errors via repeatedly confirming and grounding. These parameters are usually set heuristically or by trailand-error. Automatic optimization of these parameters will be of great help. We hope to keep the trade-off between purely hand-designed rule-based policies and the ones automatically learned using black-box and data-driven RL methods while keeping the merits from both approaches. Two variants of the approach are proposed and evaluated, an on-line training method through interaction with a simulated user and an offline and sample-efficient version called on-corpus Q-points regression. ar X iv :1\n60 5.\n03 91\n5v 2\n[ cs\n.H C\n] 1\n3 M\nay 2\nListing 1 BNF grammar of dialog policy template 〈template〉 ::= ‘if’ 〈cond-exp〉 ‘then’ 〈action〉 ‘else’\n〈template〉 | ‘if’ 〈cond-exp〉 ‘then’ 〈action〉 ‘else’ 〈action〉\n〈cond-exp〉 ::= 〈cond-exp〉 〈logic-op〉 〈cond-exp〉 | 〈boolean-state-var〉 | 〈num-state-var〉 〈comparator〉 〈free-param〉\n〈comparator〉 ::= ‘<’ | ‘>’ | ‘==’\n〈logic-op〉 ::= ‘and’ | ‘or’\nIn the following sections we describe the algorithms and experiments in detail. In section 2.1 we briefly describe Genetic Algorithm and its application in DM policy optimization. We propose two different policy optimization methods based on simulation and dialog corpus in sections 2.2 and 2.3 respectively. In section 3 we give experimental results on simulated user and real human-machine dialog corpus.\n2. MODELS AND ALGORITHMS"
    }, {
      "heading" : "2.1. Genetic algorithm and dialog policy template",
      "text" : "Genetic algorithm is a general optimization framework. It simulates the evolution process of natural selection by keeping a population of candidate solutions (individuals) and incrementally improve the quality using various genetic operators. It is a global optimization method which can solve both numerical and combinatorial problems. The key constituent of GA is a fitness function evaluating the utility of each individual. GA has been proved to be effective in solving various problems, including optimizing controllers in AI games. The psudocode of optimizing DM using GA is given in Algorithm 1. We refer readers to [9] for a detailed description of GA. The concepts of genotype and phenotype are not discriminated here. In GA an individual directly carries all the information comprising a solution, which is a fixed-length\nAlgorithm 1 Genetic algorithm policy optimization 1: Input fitness function F ,Npop,Nmut, Tmax,K, σ, µmut 2: t← 0, P0 ← ∅ . the initial population 3: for i← 1, . . . , Npop do 4: P0.add(Random.generateIndividual()) . random\ninitialization 5: P0.evalFitness() . evaluate fitness of each individual 6: while fitness ft not converges and t < Tmax do 7: t← t+ 1, Pt ← ∅ . next generation 8: Pt.add(Pt−1.getFittest()) . elitism 9: for i← 1, . . . , Nmut do 10: Pt.add(mutate(Pt−1.getFittest(), σ, µmut)) . mutate the fittest 11: for i← 1, . . . , Npop −Nmut − 1 do 12: I1, I2 ← tournamentSelect(Pt−1,K) 13: Pt.add(mutate(crossover(I1, I2), σ, µmut)) .\nreproduction 14: Pt.evalFitness() 15: ft = Pt.getFittest().getFitness() 16: return Pt.getFittest()\n17: function MUTATE(I , σ, µmut) . mutate an individual I 18: for each parameter θi of I do 19: if Random.uniform() < µmut then 20: I.θi ← perturb(I.θi, σ) 21: return I\n22: function PERTURB(θ, σ) . add random noise to a single parameter 23: g ← abs(Random.stdGaussian()) 24: if Random.uniform() < θ then 25: v ← − gσ ∗ θ + θ 26: else 27: v ← gσ ∗ (1.0− θ) + θ 28: if v < 0.0 or v > 1.0 then 29: return perturb(θ, σ) 30: else 31: return v\n32: function TOURNAMENTSELECT(P , K) . tournament selection 33: choose a random subset PK of size K from P 34: return PK .getFittest()\n35: function CROSSOVER(I1, I2) . crossover of two parents 36: I ′ ← exchange random parts of I1 and I2 37: return I ′\nfloating-point array in our experiment and each number is in [0, 1] as a free parameter of the dialog policy template. An individual can instantiate a concrete DM policy, with a defined policy template. The policy template is composed of a set of prioritized condition-action expressions and used to specify the basic structure of a dialog policy. Given certain dialog\nstate, each condition expression is checked sequentially and the first matched one is selected with the associated action chosen as output. Listing 1 gives the BNF grammar of the proposed templates. The actions of the template is fixed and free parameters can be used to set thresholds for numerical state variables. Apart from the conditional expression, parameters can also be used to induce new state variables, for example a variable representing the number of slots whose top scores are above certain threshold. Although the general system action is fixed in the template, the ‘structure’ of the action (in this slot-filling setting, structure includes sub-dialogactions and the associated slots and values) can be controlled by parameters. For example, in the action ‘offer’, threshold can be used to filter the hypotheses that are used in searching for the queried information.\nNote that the template in Listing 1 has been proposed for its conciseness and simplicity and does not have to be fixed. The design of the dialog template requires knowledge in the dialog domain but does not need a exact model of the environmental noise, thus is very suitable for human experts. This engineering division is intentionally made in our proposed approach.\nIn GA two kinds of genetic operators are used, i.e. mutation and crossover, which are shown graphically in Figure 1 and as pseudo-code in Algorithm 1. During crossover, two parents are selected, then random parts of the two parents are exchanged, giving birth to a new child. The mutation operator checks each component of a chromosome sequentially, either leaving it intact or perturbing it randomly. In our implementation the perturbation is realized by sampling from a skewed normal distribution with the peak centered at the perturbed number. If the sampling result lies outside [0, 1], the process is repeated by calling the function perturb recursively. This sampling sub-routine is designed for a smooth distribution function. The mutation and crossover operators represent asexual and bisexual reproductions in GA respectively. Other reproducing strategy can be used as long as it effectively explores the underlying solution space. Tournament selection is used to select individuals for reproduction. It is a simple selection method where random K individuals are chosen from the population. We also use the elitism technique passing the fittest individual directly to the next generation, ensuring that the fitness of the population will never decrease. The fitness function is the most important part of GA since it guides the algorithm in searching for optimum solution. Two kinds of DM policy fitness evaluation methods are described in the following sections."
    }, {
      "heading" : "2.2. Policy optimization with a user simulator",
      "text" : "Since the fitness function should be consistent with the performance of the DM, the most straightforward way is to evaluate it online with users. But interacting with real user is timeconsuming and labor-intensive, thus an agenda-based user\nAlgorithm 2 Episodic fitted Q-iteration 1: Input {(si,t, ai,t+1, si,t+1)} where t ← 1, . . . , Tt − 1,\nand i← 1, . . . , N 2: initialize Q-function approximator Q̂(s, a) and arrayQi,t\nto 0 3: for l← 1, . . . , Lmax do 4: for i← 1, . . . , N do . for each dialog 5: for t← 1, . . . , Ti − 1 do . for each turn 6: r ← reward(si,t, ai,t+1, si,t+1) 7: if t == Ti − 1 then 8: Qi,t ← r . when the dialog ends 9: else\n10: Qi,t ← r + γmaxaQ̂(si,t+1, a) 11: Regress Q̂(s, a) on {(si,t, ai,t+1, Qi,t)} 12: Output: Q̂(s, a)\nsimulator is utilized [10] and N interactions are conducted between the simulated user and DM. Average cummulative reward is used as the fitness for the individual, which is similar to the objective of common RL algorithms.\nFR[πGA] = 1\nN N∑ i=1 li∑ j=1 γj−1rij (1)\nwhere rij is the immediate reward and γ the discounted coefficient.\nA noisy channel is designed to simulate ASR and SLU errors. For each dialog act {act, (slot, value)}, replacement and deletion are randomly applied to value given the assigned confidence scores, which are randomly generated too. The produced N-best hypotheses are then fed into DMs."
    }, {
      "heading" : "2.3. On-corpus Q-points regression",
      "text" : "Building a user simulator is not trivial and it is difficult to measure the consistency of the simulated user behavior to the real one. Learning a DM using a dialog corpus is appealing but there is very limited prior work on this subject [11, 12]. We propose to use an existing dialog corpus to estimate the fitness of a DM. First, an offline batch RL algorithm is applied on the corpus, inducing an optimum Q-function Q̂(s, a), and an implicitly defined policy πQ(s) = argmaxa Q̂(s, a) which is optimum with respect to the corpus. Then Q̂(s, a) is used to define the fitness function. We use fitted Q-iteration [13] to learn a nonparametric approximator Q̂(s, a), as described in Algorithm 2. The algorithm uses Bellman equation (line 10) to update the estimated Q-values. Extremely Random Trees (ExtraTrees) [14] are utilized for non-parametric regression. ExtraTrees are a powerful model for regression and classification as they are both flexible and less susceptible to over-fitting. The annotated dialog corpus is represented as state-action-state triplets in the form of {(st−1, at, st)}, and\nused as the training set. Two fitness estimation methods are proposed based on different heuristics. For an individual πGA whose fitness to be evaluated, the NPoints fitness function is used to calculates the number of triplets where the actions predicted by πGA and πQ are identical.\nFNPoints[πGA] = ∑ i δ(πGA(si), πQ(si)) (2)\nThe QVal fitness attempts to estimate the sum of the Q-values for the actions predicted by πGA on the training triplets. However, the Q-function trained on a fixed corpus is often inaccurate in unexplored regions of the state space [15, 11]. To mitigate the problem a supervised classifier P̂ (a|s) is built on the training set with the observed actions as targets. If the probability for an action is greater than a predefined threshold δ, the value produced by Q̂(s, a) is used, otherwise a constant R is used for punishment.\nFQVal[πGA] = ∑ i Q̃δ(si, πGA(si)) (3)\nQ̃δ(s, a) = { Q̂(s, a) if P̂ (a|s) > δ R otherwise\nThe two fitness functions are different in weighing the importance of training instances. FQVal will put a greater effort in optimizing instances with larger potential Q-value improvement while avoiding taking unobserved actions. Combining GA with the above two fitness functions leads to the on-corpus Q-points regression algorithm. One limitation of this algorithm compared to the on-line version is that no free parameter can be present in specifying the action structures since the fitness functions reply on the result of reinforcement learning, which does not support dynamical change of action structure."
    }, {
      "heading" : "2.4. On-corpus DM evaluation",
      "text" : "We describe a DM evaluation method on dialog corpus without the need for deploying the DM online. A held-out dialog corpus is used as testing set, and the estimated cumulative reward for the testing dialogs when following the target DM policy is used as metric for performance. A similar approach has been taken in evaluating the effect of different dialog state tracker on end-to-end performance of a DM [15]. The estimation of Q-function is similar to Algorithm 2. But rather than learning the optimum policy, the value function for the policy to be evaluated is estimated, with the Bellman iteration (line 10) in Algorithm 2 changed to:\nQi,t ← r + γQ̂(si,t+1, π(si,t+1)) (4)\nwhere π is the DM policy to evaluate. Then the average reward for starting turns 1N ∑ iQi,0 is used as a metric for performance.\n3. EXPERIMENTS\nWe devise a restaurant information domain for dialog simulation. There are 4 slots for the user simulator to fill before a database query. During simulations the DM interacts with the user simulator with a noise channel in between. The noise level of the channel can be adjusted to simulate different environmental noise conditions. Since the simulation process is stochastic, each experiment is conducted for 100 times, and the mean and standard deviation of testing performance are reported.\nThe reward function for the simulated environment is defined as follows. At each dialog turn the agent receives -1.0 reward. If correct restaurants are offered to users, 100.0 points are rewarded. But if the information is duplicate to that previously offered or the presented restaurants do not match user goal, -5.0 points are given. The reward discounting rate γ is set to 0.9.\nIn the on-corpus evaluation, DSTC2 dataset [16] is used for both DM policy learning and evaluation. The DSTC2 dataset was originally designed as a benchmark corpus for dialog state tracking. With the detailed annotation of dialog states, actions, SLU outputs and other information, it can be used as test set for end-to-end DM performance [15]. The dialog states used in both simulated and on-corpus experiments mainly comprise confident scores for each slot."
    }, {
      "heading" : "3.1. On-line learning experiment by simulation",
      "text" : "The dialog policy template used in the simulated experiments is shown as follows.\nc0 On dialog beginning: Welcome\nc1 There are no valid SLU results or the top SLU hypothesis score is less than θ0: Repeat\nc2 User has just denied a slot: Request that slot\nc3 There is a slot with score less than θ1 in the tracker: if the score is larger than θ2 then ExplicitConf else Request\nc4 The system has not yet output the action RequireMore: RequireMore\nc5 Otherwise: query the database with slot-value pairs whose scores are greater than θ3\nThe template has 6 condition-action clauses and contains 4 free parameters. Note that 3 free parameters lie in the condition expressions while θ3 is used to adapt the semantics of the macro action offer, which queries the database and presents results to the user.\nA rule-based DM policy is built by setting the 4 parameters heuristically. A RL-based policy trained using Q-learning with linear approximation is also built for comparison with the proposed methods. It is trained for over 100,000 dialog\nsessions to ensure that the state-action space is sufficiently explored and the optimal performance is reached. The probability for exploration is set to 0.3. In the training process of each kind of DM, the noise level is randomly set for a dialog session. The same reward function and discounting rate are also used in the fitness estimation of GA. We run GA for 30 generations in policy training.\nDuring testing 1000 dialog sessions are conducted and the noise level is adjusted in the same way as in training. We report both the overall performance (average reward received under a series of noise conditions) and the performance with a fixed noise level. The overall testing performance of each DM is shown in Fig.2. Performance when operating under fixed noise condition is shown in Fig.3 and 4. The level of environmental noise is measured using the semantic error rate of the top hypothesis of SLU results. It should be emphasized that the noise levels shown in the results are the same ones used in training. In addition to the GA-DM using the complete policy template, the utility of each individual clause in the template is evaluated. The four major clauses c1-c4 are disabled sequentially. The resulted DMs are evaluated using the same settings, and the testing results are shown along with the fullfledged GA-DM. The effects of different GA population size are explored and reported in Fig.5.\nSince the DMs are optimized against the average reward received under several noise conditions, the overall testing reward shown in Fig.2 should be taken as the direct metric of performance. The RL-based policy showed better overall\nperformance than the rule-based one, while GA-DM significantly outperformed both the rule-based and RL-based policies. From Fig.3, it can be seen that when the noise is low, the rule-based DM is very competitive and shows even better performance than the RL-based DM. But when the noise level of the environment increases, its performance degrades seriously, while the RL-based DM is much more robust. However, after tuning of the free parameters using GA, the GADM outperforms both the other DMs on nearly all noise conditions. Note the maximum noise level at which each DM could successfully complete a dialog, suggesting that the GADM is able to operate under more adverse environment. It is worth mentioning again that the rule-based DM and GA-DM are instantiations of the same policy template. The simulation results justify GA as an effective method for DM policy optimization and reveal the performance potential of simple and yet human-interpretable DM policies.\nIt is interesting to make a comparison between RL and GA policy learning. In DM policy optimization, the state space is often continuous and infinite. In conventional RL, a model of the underlying optimal value function of the environment has to be designated. The ability of the model to approximate the optimal value function is a key factor affecting the performance of the learnt policy. However, the design of the model is often non-intuitive and complicated since it oper-\nates in the value function space. Expert knowledge is often difficult to be directly applied. This fact can help to explain that in our experiments, the RL-based DM is not as competitive as the others when the noise level is low. Since the noise level is varied during training, the resulted learning environment is much more difficult to deal with than one with fixed noise condition. Thus the linear model used is unlikely to perfectly match the underlying optimal value function and cannot accommodate all types of condition. In our experiment the RL policy has learnt to make a trade-off and adapted to conditions with high environmental noise for a better overall performance. GA-DM tackles the problem from a different perspective. It operates in policy space directly and is much easier to incorporate expert knowledge. In GA-DM a policy model is developed instead. Equivalent assumptions about policy structure are often difficult to made in value function space. Thus the resulted policy model can be more powerful and expressive than one for value function.\nThe relative utility of each clause of the policy template on the performance is another interesting aspect to be investi-\ngated. According to the results shown in Fig.2 (b) and Fig.4, it can be observed that when C2 is disabled the performance drops seriously. But to our surprise, when C4 is disabled, the performance significantly boosts especially in high-noise regions. The results show the relative utility of each clause in the template and reveal the necessity to optimize the structure of policy template. This kind of structural optimization problem can also be solved using GA, and we plan to study this kind of optimization in future work.\nIn GA the population size often influences the optimization efficiency. The training fitness and testing performance using different population size is shown in Fig.5. We can observe that with an increasing population size, the training and testing performance nearly monotonically increases. This performance improvements are more obvious when the size is less than 100, and are not noticeable above 300. Because the elitism technique is used and the fitness of the elitist individual is cached, the training fitness improves steadily during training."
    }, {
      "heading" : "3.2. On-corpus learning experiment",
      "text" : "The DSTC2 testing corpus is used for on-corpus DM learning and evaluation [16], which is produced by a RL-based DM and consists of 1117 dialog sessions. The full annotations of the dataset are released after the conclusion of the DSTC2 challenge. The dialog state is the same as defined in the challenge, and we use the results produced by the ‘focus’ tracker using the scripts provided by the DSTC2 organizer. The dialog template used by GA comprises 9 condition-action clauses and 6 free parameters. The original corpus is equally split for training and testing.\nThe reward function is defined as follows. At each dialog turn the agent receives -10.0 reward. If correct restaurants are offered to users, 100.0 points are rewarded. But if the information is duplicate to that previously offered, -50.0 points are given. If the restaurants offered do not meet user’s demand, -100.0 points are given. The reward discounting rate γ is set to 0.9.\nIn addition to the GA-based DMs trained using QPointsregression described in section 2.3, results of 3 additional DMs are shown for comparison.\n1. SL-Original DM which is learned in a supervised way with the original dialog actions as training targets using the ExtraTrees classifier, represented as P̂ (a|s).\n2. SL-MaxQ supervised DM using the actions with maximum Q-value predicted by Q̂(s, a) as the supervised targets.\n3. ThresholdedQ DM as described in [15], which selects the action with the maximum Q-value predicted by Q̂(s, a) from the set of actions whose probabilities produced by P̂ (a|s) are greater than δ. The threshold is used to constrain the behavior of RL policy, in case of insufficient exploration.\nTo make full use of the available data and get a more stable estimation of the performance, we conducted 12 resampling experiments similar to the bootstrapping method, but avoid to use duplicate samples. In each sub-experiment, the dataset is reshuffled and split to get new training and testing instances. The average and standard deviation of the results are shown in Table 1.\nThe SL-MaxQ DM which acts greedily upon Q̂(s, a) has poor performance on the test set while being overrated on the training set, probably as a result of insufficient exploration. The ThresholdedQ DM mitigates the problem to a great degree by setting a simple threshold. That heuristic is shared with the QVal fitness function. GA-QVal outperforms all the other DMs and is very stable across the re-sampling experiments considering the relatively small standard deviation, while the behavior GA-NPoints which is less consistent results in an overall inferior performance. Although GA-QVal is trained under the guidance of an reinforcement learner Q̂(s, a), its performance is superior to both SL-MaxQ and\nThresholdedQ, which should be attributed to the prior domain knowledge incorporated into the policy template. The DMs in bold outperform SL-Original built by imitating the policy used in producing the corpus, indicating the possibility of building a better and yet human-comprehensible DM policy using a dialog corpus.\n4. RELATED WORK\nThe subject of automatically optimizing dialog policies is a hot topic, and many data-driven methods have been proposed among which RL-based ones are the most popular. There is some previous work on constraining the behavior of RL-based DM. In [17] Williams proposed to construct a hand-crafted DM and it produces a set of candidate actions for given dialog state, from which the best one is chosen by a POMDP-DM. Lison [18] proposed to use ‘probabilistic rule’ in specifying the transition and reward sub-models of the POMDP model. The probabilistic rules are human-readable and less parameterized than conventional probability distribution, thus reducing the free parameters of the POMDP model and allowing the system designers to make use of domain knowledge in designing DM. Our work bears some resemblance to [18]. But we used the dialog policy template to specify a policy directly and utilized GA to train the free parameters.\nHenderson et al. proposed a hybrid learning method in [11] to learn a policy on an existing dialog corpus by combining the results of supervised and reinforcement learning. Pure RL on fixed dataset often shows irregular behavior due to the insufficient exploration problem. Supervised learning (SL) is used to mitigate the problem and the hybrid method shows better performance than pure SL or RL. In this regard the QVal fitness function is similar in spirit and the use of policy template can further constrain the DM behavior, thus is suitable for off-line on-corpus learning.\nOne notable advantage of the GA-based DM over RLbased models is that the action structure can be changed during learning (only in on-line learning) as described in section 2.1. While in RL, each action ai ∈ A must be invariant otherwise the value function learned will be meaningless. This characteristic is suitable for SDS engineering since it can be difficult to determine the exact semantics of a dialog action beforehand. Further studies are needed in this regard.\n5. CONCLUSIONS AND FUTURE WORK\nIn this paper we described a framework to train humaninterpretable spoken dialog management policies using genetic algorithm. Two kinds of fitness functions were used, i.e., one based on interacting with a simulated user and the other on a dialog corpus which is more sample-efficient. We set up an online simulation environment and used the DSTC2 corpus for off-line on-corpus training and evaluation. The results show that by using domain language and setting appropriate\nfree parameters, the performance of simple rule-based DM policies can be largely improved, and can even outperforms those trained using reinforcement learning. According to our knowledge, this is the first time that genetic algorithm is applied to DM optimization. Another advantage is its ability to optimize the structure of system actions. This framework is very suitable to upgrade existing SDSs using rule-based DM, by using collected data to optimize the newly specified free parameters.\nThis research is still preliminary and several aspects need further investigation, especially the effects of fitness functions. The search space of dialog policy in GA can be expanded by allowing the condition-action expressions to be reordered and partially disabled. The structural learning in system actions also needs further studies. We hope this work can help to build better and practical spoken dialog systems.\n6. REFERENCES\n[1] Jason D. Williams and Steve Young, “Partially observable Markov decision processes for spoken dialog systems,” Computer Speech & Language, vol. 21, no. 2, pp. 393–422, 2007.\n[2] Steve Young, Milica Gašić, Simon Keizer, François Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu, “The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management,” Computer Speech & Language, vol. 24, no. 2, pp. 150–174, 2010.\n[3] Cheongjae Lee, Sangkeun Jung, Seokhwan Kim, and Gary Geunbae Lee, “Example-based dialog modeling for practical multi-domain dialog system,” Speech Communication, vol. 51, no. 5, pp. 466–484, 2009.\n[4] Pierre. Lison, Structured Probabilistic Modelling for Dialogue Management, Ph.D. thesis, University of Oslo, 2014.\n[5] Steve Young, Milica Gašić, Blaise. Thomson, and Jason D. Williams, “POMDP-Based Statistical Spoken Dialog Systems: A Review,” Proceedings of the IEEE, vol. 101, no. 5, pp. 1160–1179, 2013.\n[6] Tim Paek, “Reinforcement Learning for Spoken Dialogue Systems: Comparing Strengths and Weaknesses for Practical Deployment,” Tech. Rep. MSR-TR-200662, Microsoft Research, 2006.\n[7] Yaakov Engel, Shie Mannor, and Ron Meir, “Reinforcement learning with Gaussian processes,” in Proceedings of the 22nd international conference on Machine learning. 2005, pp. 201–208, ACM.\n[8] Milica Gašić, Filip Jurčı́ček, Simon Keizer, François Mairesse, Blaise Thomson, Thomson, Kai Yu, and Steve Young, “Gaussian Processes for Fast Policy Optimisation of POMDP-based Dialogue Managers,” in Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Stroudsburg, PA, USA, 2010, SIGDIAL ’10, pp. 201–204, Association for Computational Linguistics.\n[9] Darrell Whitley, “A genetic algorithm tutorial,” Statistics and Computing, vol. 4, no. 2, pp. 65–85, June 1994.\n[10] Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young, “Agenda-based user simulation for bootstrapping a POMDP dialogue system,” in Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers. 2007, pp. 149–152, Association for Computational Linguistics.\n[11] James Henderson, Oliver Lemon, and Kallirroi Georgila, “Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets,” Computational Linguistics, vol. 34, no. 4, pp. 487–511, 2008.\n[12] Olivier Pietquin, Matthieu Geist, Senthilkumar Chandramohan, and Hervé Frezza-Buet, “Sample-efficient batch reinforcement learning for dialogue management optimization,” ACM Transactions on Speech and Language Processing (TSLP), vol. 7, no. 3, pp. 7, 2011.\n[13] Damien Ernst, Pierre Geurts, and Louis Wehenkel, “Tree-based batch mode reinforcement learning,” in Journal of Machine Learning Research, 2005, pp. 503– 556.\n[14] Pierre Geurts, Damien Ernst, and Louis Wehenkel, “Extremely randomized trees,” Machine Learning, vol. 63, no. 1, pp. 3–42, Mar. 2006.\n[15] Sungjin Lee, “Extrinsic Evaluation of Dialog State Tracking and Predictive Metrics for Dialog Policy Optimization,” in 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2014, p. 310.\n[16] Matthew Henderson, Blaise Thomson, and Jason D. Williams, “The second dialog state tracking challenge,” in 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2014, p. 263.\n[17] Jason D. Williams, “The best of both worlds: unifying conventional dialog systems and POMDPs.,” in INTERSPEECH, 2008, pp. 1173–1176.\n[18] Pierre Lison, “A hybrid approach to dialogue management based on probabilistic rules,” Computer Speech & Language, vol. 34, no. 1, pp. 232–255, 2015."
    } ],
    "references" : [ {
      "title" : "Partially observable Markov decision processes for spoken dialog systems",
      "author" : [ "Jason D. Williams", "Steve Young" ],
      "venue" : "Computer Speech & Language, vol. 21, no. 2, pp. 393–422, 2007.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management",
      "author" : [ "Steve Young", "Milica Gašić", "Simon Keizer", "François Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu" ],
      "venue" : "Computer Speech & Language, vol. 24, no. 2, pp. 150–174, 2010.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Example-based dialog modeling for practical multi-domain dialog system",
      "author" : [ "Cheongjae Lee", "Sangkeun Jung", "Seokhwan Kim", "Gary Geunbae Lee" ],
      "venue" : "Speech Communication, vol. 51, no. 5, pp. 466–484, 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Structured Probabilistic Modelling for Dialogue Management, Ph.D",
      "author" : [ "Pierre. Lison" ],
      "venue" : "thesis, University of Oslo,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "POMDP-Based Statistical Spoken Dialog Systems: A Review",
      "author" : [ "Steve Young", "Milica Gašić", "Blaise. Thomson", "Jason D. Williams" ],
      "venue" : "Proceedings of the IEEE, vol. 101, no. 5, pp. 1160–1179, 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reinforcement Learning for Spoken Dialogue Systems: Comparing Strengths and Weaknesses for Practical Deployment",
      "author" : [ "Tim Paek" ],
      "venue" : "Tech. Rep. MSR-TR-2006- 62, Microsoft Research, 2006.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Reinforcement learning with Gaussian processes",
      "author" : [ "Yaakov Engel", "Shie Mannor", "Ron Meir" ],
      "venue" : "Proceedings of the 22nd international conference on Machine learning. 2005, pp. 201–208, ACM.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Gaussian Processes for Fast Policy Optimisation of POMDP-based Dialogue Managers",
      "author" : [ "Milica Gašić", "Filip Jurčı́ček", "Simon Keizer", "François Mairesse", "Blaise Thomson", "Thomson", "Kai Yu", "Steve Young" ],
      "venue" : "Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Stroudsburg, PA, USA, 2010, SIGDIAL ’10, pp. 201–204, Association for Computational Linguistics.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A genetic algorithm tutorial",
      "author" : [ "Darrell Whitley" ],
      "venue" : "Statistics and Computing, vol. 4, no. 2, pp. 65–85, June 1994.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Agenda-based user simulation for bootstrapping a POMDP dialogue system",
      "author" : [ "Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve Young" ],
      "venue" : "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers. 2007, pp. 149–152, Association for Computational Linguistics.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets",
      "author" : [ "James Henderson", "Oliver Lemon", "Kallirroi Georgila" ],
      "venue" : "Computational Linguistics, vol. 34, no. 4, pp. 487–511, 2008.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Sample-efficient batch reinforcement learning for dialogue management optimization",
      "author" : [ "Olivier Pietquin", "Matthieu Geist", "Senthilkumar Chandramohan", "Hervé Frezza-Buet" ],
      "venue" : "ACM Transactions on Speech and Language Processing (TSLP), vol. 7, no. 3, pp. 7, 2011.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Tree-based batch mode reinforcement learning",
      "author" : [ "Damien Ernst", "Pierre Geurts", "Louis Wehenkel" ],
      "venue" : "Journal of Machine Learning Research, 2005, pp. 503– 556.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Extremely randomized trees",
      "author" : [ "Pierre Geurts", "Damien Ernst", "Louis Wehenkel" ],
      "venue" : "Machine Learning, vol. 63, no. 1, pp. 3–42, Mar. 2006.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Extrinsic Evaluation of Dialog State Tracking and Predictive Metrics for Dialog Policy Optimization",
      "author" : [ "Sungjin Lee" ],
      "venue" : "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2014, p. 310.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The second dialog state tracking challenge",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Jason D. Williams" ],
      "venue" : "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2014, p. 263.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The best of both worlds: unifying conventional dialog systems and POMDPs",
      "author" : [ "Jason D. Williams" ],
      "venue" : "INTER- SPEECH, 2008, pp. 1173–1176.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A hybrid approach to dialogue management based on probabilistic rules",
      "author" : [ "Pierre Lison" ],
      "venue" : "Computer Speech & Language, vol. 34, no. 1, pp. 232–255, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "In recent years various approaches for automatic DM policy optimization have been proposed [1, 2, 3, 4], among which methods based on reinforcement learning (RL) and POMDP model are the most popular [5].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 5,
      "context" : "Despite all the advantages, RL-based DMs are not widely deployed for commercial SDSs due to several reasons [6].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "In recent studies it has been shown that by incorporating domain knowledge into the design of kernel functions, the GPSARSA [7, 8] algorithm exhibits much faster learning speed than conventional online RL methods.",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "In recent studies it has been shown that by incorporating domain knowledge into the design of kernel functions, the GPSARSA [7, 8] algorithm exhibits much faster learning speed than conventional online RL methods.",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "In this paper we propose to use Genetic Algorithm (GA) [9] in optimizing DM policies (GA-DM) which are comprehensible to human designers and easy to verify and modify.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "Each individual is a real vector with consitituent scalars in [0, 1].",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "We refer readers to [9] for a detailed description of GA.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "floating-point array in our experiment and each number is in [0, 1] as a free parameter of the dialog policy template.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "If the sampling result lies outside [0, 1], the process is repeated by calling the function perturb recursively.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "simulator is utilized [10] and N interactions are conducted between the simulated user and DM.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "Learning a DM using a dialog corpus is appealing but there is very limited prior work on this subject [11, 12].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "Learning a DM using a dialog corpus is appealing but there is very limited prior work on this subject [11, 12].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "We use fitted Q-iteration [13] to learn a nonparametric approximator Q̂(s, a), as described in Algorithm 2.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "Extremely Random Trees (ExtraTrees) [14] are utilized for non-parametric regression.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "However, the Q-function trained on a fixed corpus is often inaccurate in unexplored regions of the state space [15, 11].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "However, the Q-function trained on a fixed corpus is often inaccurate in unexplored regions of the state space [15, 11].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : "A similar approach has been taken in evaluating the effect of different dialog state tracker on end-to-end performance of a DM [15].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "In the on-corpus evaluation, DSTC2 dataset [16] is used for both DM policy learning and evaluation.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "With the detailed annotation of dialog states, actions, SLU outputs and other information, it can be used as test set for end-to-end DM performance [15].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "The DSTC2 testing corpus is used for on-corpus DM learning and evaluation [16], which is produced by a RL-based DM and consists of 1117 dialog sessions.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "ThresholdedQ DM as described in [15], which selects the action with the maximum Q-value predicted by Q̂(s, a) from the set of actions whose probabilities produced by P̂ (a|s) are greater than δ.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : "In [17] Williams proposed to construct a hand-crafted DM and it produces a set of candidate actions for given dialog state, from which the best one is chosen by a POMDP-DM.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "Lison [18] proposed to use ‘probabilistic rule’ in specifying the transition and reward sub-models of the POMDP model.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "Our work bears some resemblance to [18].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "proposed a hybrid learning method in [11] to learn a policy on an existing dialog corpus by combining the results of supervised and reinforcement learning.",
      "startOffset" : 37,
      "endOffset" : 41
    } ],
    "year" : 2016,
    "abstractText" : "Automatic optimization of spoken dialog management policies that are robust to environmental noise has long been the goal for both academia and industry. Approaches based on reinforcement learning have been proved to be effective. However, the numerical representation of dialog policy is humanincomprehensible and difficult for dialog system designers to verify or modify, which limits its practical application. In this paper we propose a novel framework for optimizing dialog policies specified in domain language using genetic algorithm. The human-interpretable representation of policy makes the method suitable for practical employment. We present learning algorithms using user simulation and real human-machine dialogs respectively. Empirical experimental results are given to show the effectiveness of the proposed approach.",
    "creator" : "LaTeX with hyperref package"
  }
}