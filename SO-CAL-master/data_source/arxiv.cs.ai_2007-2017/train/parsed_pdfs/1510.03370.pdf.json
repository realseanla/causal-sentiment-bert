{
  "name" : "1510.03370.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Scott Garrabrant", "Siddharth Bhaskar", "Abram Demski", "Joanna Garrabrant" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n03 37\n0v 1\n[ cs\n.L G\n] 1\n2 O\nct 2\n01 5"
    }, {
      "heading" : "1 Introduction",
      "text" : "Let φ1, φ2, . . . be a simple enumeration of all sentences in first order logic over ZFC. The goal of logical uncertainty is to construct an algorithm M which on input N outputs a probability M(N), which represents the probability that φN is true [1, 2, 3, 4].\n1 This notion of probability does not refer to random variables. It refers to the degree of uncertainty that one might have about logical sentences whose truth-values have not been calculated.\nMuch work has been done on a related problem where M on input N outputs an infinite sequence of numbers and M(N) is defined to be the limit of the sequence output by M on input N [1, 2, 11]. In this case, M(N) is not computable, and can easily be 1 for all provable φ and 0 for all disprovable φ, so all of the work is in figuring out how M should behave when φ is independent of ZFC.\nIn this paper, we take a different approach, which we call asymptotic logical uncertainty. We require that M(N) be computable and have runtime bounded by some function of N .\nWe propose as a baseline that any method of quickly assigning probabilities should be able to pass a test we call the Benford test. Consider the infinite sequence of sentences {φsn} given by φsn = “The first digit of Research supported by the Machine Intelligence Research Institute (intelligence.org). Technical Report 2015–11.\n1The problem has also been studied in the case where we don’t require computability even in the limit [5, 6, 7]. The problem was first studied in the context of measures on Boolean algebras [8, 9, 10].\n3 ↑n 3 is a 1.” We say that M passes the Benford test if\nlim n→∞\nM(sn) = log10(2) ≈ .30103,\nas prescribed by Benford’s law. More generally, we say that M passes the generalized Benford test if it converges to the correct probability on any similar infinite sequences whose truth values appear indistinguishable from independent flips of a biased coin. We then give an algorithm AL,T which passes the generalized Benford test.\nLogical uncertainty is one aspect of the problem of combining probability and logic, of which statistical relational learning is another [12]. Statistical relational learning addresses the problem of representing probabilistic models with logical structure, including regularities such as repeated entities and other complexities such as uncertainty about the number of entities. In contrast, logical uncertainty deals with uncertainty about logic. As Paul Christiano put it: “any realistic agent is necessarily uncertain not only about its environment or about the future, but also about the logically necessary consequences of its beliefs.” [1]"
    }, {
      "heading" : "2 The Benford Test",
      "text" : "Benford’s law states that in naturally occurring numbers, the leading digit d ∈ {1, . . . , 9} of that number in base 10 occurs with probability log10(1 + 1 d ). Many mathematical sequences have been shown to have frequencies of first digits that satisfy Benford’s law [13]. In particular, the frequencies of the first digits of powers of 3 provably satisfy Benford’s law.\nThe function 3 ↑n k is defined by 3 ↑1 k = 3k, 3 ↑n 1 = 3, and 3 ↑n k = 3 ↑n−1 (3 ↑n (k − 1)). Throughout the paper, let T (N) be an increasing time complexity function in the range of N ≤ T (N) ≤ 3 ↑k N for some fixed k, and let R(N) = T (N)N4 logT (N).\nConsider the sequence 3 ↑n 3. Clearly this sequence only contains powers of 3. We might hypothesize that the frequencies of the first digits in this sequence also satisfy Benford’s law. However, 3 ↑n 3 is very large, and first digit of 3 ↑n 3 is probably very difficult to\ncompute. It is unlikely that the first digit of 3 ↑3 3 will ever be known.\nIf asked to quickly assign a probability to the sentence φsn = “The first digit of 3 ↑n 3 is a 1,” for some n > 2, the only reasonable answer would be log10(2) ≈ .30103. Note that φsn is either true or false; there are no random variables. The probability here represents a reasonable guess in the absence of enough time or resources to compute 3 ↑n 3.\nDefinition 2.1. Let M be a Turing machine which on input N runs in time O(R(N)) and outputs a probability M(N), which represents the probability assigned to φN . We say that M passes the Benford test if\nlim n→∞ M(sn) = log10(2),\nwhere φsn = “The first digit of 3 ↑n 3 is a 1.”\nIt is easy to pass the Benford test by hard-coding in the probability. It is more difficult to pass the Benford test in a natural way. That the best probability to assign to φsn is log10(2) depends not only on the fact that the frequency with which φsn is true tends toward log10(2), but also on the fact that the sequence of truth-values of φsn contains no patterns that can be used to quickly compute a better probability on some subsequence. We therefore assume that this sequence of truth-values is indistinguishable from a sequence produced by a coin that outputs “true” with probability log10(2). Formally, we are assuming that S = {sn|n ∈ N} is an irreducible pattern with probability log10(2), as defined in the next section."
    }, {
      "heading" : "3 Irreducible Patterns",
      "text" : "Fix a universal Turing machine U and an encoding scheme for machines, and let U(M,x) denote running the machine U to simulate M with input x.\nDefinition 3.1. 2 Let S ⊆ N be an infinite subset of natural numbers such that φN is provable or disprovable for all N ∈ S, and there exists a Turing machine Z such that U(Z,N) runs in time T (N) and accepts N if and only if N ∈ S.\nWe say that S is an irreducible pattern with probability p if there exists a constant c such that for every positive integer m ≥ 3 and every Turing machine W expressible in k(W ) bits, if\nS′ = {N ∈ S | U(W,N) accepts in time T (N)} 2We tailored this definition of irreducible pattern to our needs. The theory of algorithmic randomness may offer alternatives. However, algorithmic randomness generally considers all computable tests and focuses on the case where p = 1\n2 [14, 15, 16]. We believe that any reasonable definition inspired by algorithmic randomness would imply Definition 3.1.\nhas at least m elements and r(m,W ) is the probability that φN is provable when N is chosen uniformly at random from the first m elements of S′, we have\n|r(m,W ) − p|< ck(W ) √ log logm√ m .\nThe intuition behind the formula is that the observed frequency r(m,W ) for any sequence S′ we select should not stray far from p. The right hand side of the inequality needs to shrink slowly enough that a true random process would stay within it with probability 1 (given choice of c sufficiently large to accommodate initial variation). The law of the iterated logarithm gives such a formula, which is also tight in the sense that we cannot replace it with a formula which diminishes more quickly as a function of m.\nProposition 3.2. If we replace provability in Definition 3.1 with a random process, such that for each N ∈ S the sentence φN is independently called “provable” with probability p, then S would almost surely be an irreducible pattern with probability p.\nProof. Fix a Turing machine W . By the law of the iterated logarithm, there exists a constant c1 such that\nlim sup m→∞ |mr(m,W )−mp|√ m log logm = c1\nalmost surely. Therefore\nsup m |mr(m,W ) −mp|√ m log logm < ∞\nalmost surely. We will use Φ(W ) as a shorthand for this supremum. For any ε > 0, there therefore exists a c2 such that P(Φ(W ) > c2) ≤ ε.\nWe now show that P(Φ(W ) > 2c2 + 1) ≤ ε2. By the chain rule for probabilities, it suffices to show that P((Φ(W ) > 2c2 + 1)|(Φ(W ) > c2)) ≤ ε. Assume Φ(W ) > c2, and Let m1 be the first m such that\n|mr(m,W ) −mp|√ m log logm > c2.\nIt suffices to show that the probability that there exists an m2 with\n|m2r(m2,W )−m2p|√ m2 log logm2 − |m1r(m1,W )−m1p|√ m1 log logm1 > c2\nis at most ε. Observe that\n|m2r(m2,W )−m2p|√ m2 log logm2 − |m1r(m1,W )−m1p|√ m1 log logm1 ≤ |m2r(m2,W )−m1r(m1,W )− (m2 −m1)p|√ (m2 −m1) log log(m2 −m1) ,\nand that the probability there exists an m2 with\n|m2r(m2,W )−m1r(m1,W )− (m2 −m1)p| √\n(m2 −m1) log log(m2 −m1) > c2\nis the same as the probability that Φ(W ) > c2, which is at most ε.\nWe have thus shown that for every ε, there exists a constant c3 = c2 + 1 such that the probability that Φ(W ) ≥ 2ℓc3 is at most ε2 ℓ\n. Partition the set of all Turing machines into sets W1,W2, . . . , such that Wℓ contains all Turing machines expressed in at least 2ℓ but fewer than 2ℓ+1 bits. The probability that a Turing machine W in Wℓ violates\n|r(m,W )− p|< c3k(W ) √ log logm√ m , (⋆)\nfor any m ≥ 3 is at most ε2ℓ . The number of Turing machines in Wℓ is at most 22 ℓ+1\n, so the probability that there is any W ∈ Wℓ and m ≥ 3 which violate (⋆) is at most ε2 ℓ 22 ℓ+1\n. Therefore, the probability that there is any Turing machine W and m ≥ 3 which violate (⋆) is at most\n∑ ℓ∈N ε2 ℓ 22 ℓ+1 = ∑ ℓ∈N (4ε)2 ℓ .\nFor small enough ε this goes to 0, so for large enough c3, the probability that (⋆) holds for all W and m goes to 1. Therefore, with probability 1, there exists a c such that\n|r(m,W ) − p|< ck(W ) √ log logm√ m ,\nfor all m and W .\nWe now use the concept of irreducible patterns to generalize the Benford test.\nDefinition 3.3. Let M be a Turing machine which on input N runs in time O(R(N)) and outputs a probability M(N), which represents the probability assigned to φN . We say that M passes the generalized Benford test if\nlim N→∞ N∈S M(N) = p,\nwhenever S is an irreducible pattern with probability p.\nNote that if we conjecture that the S from Definition 2.1 is an irreducible pattern with probability log10(2), then any M which passes the generalized Benford test also passes the Benford test."
    }, {
      "heading" : "4 A Learning Algorithm",
      "text" : "We now introduce an algorithm AL,T that passes the generalized Benford test (see Algorithm 1).\nLet L be the Turing machine which accepts on input N if ZFC proves φN , rejects on input N if ZFC\nAlgorithm 1 AL,T (N)\n1: P = 0 2: M = N 3: for j = 0, . . . , N do 4: MY = 0 5: for Y a Turing machine expressible in KY <\nlogN bits do 6: MX = N 7: forX a Turing machine expressible in KX <\nlogN bits do 8: if U(X,N) and U(Y,N) both accept in\ntime T (N) then 9: A = 0\n10: R = 0 11: i = 1 12: while i ≤ N do 13: if U(X, i) and U(Y, i) both accept\nin time T (i) then 14: if U(L, i) accepts in time\nT (N) then"
    }, {
      "heading" : "15: A = A+ 1",
      "text" : "16: else if U(L, i) rejects in time\nT (N) then 17: R = R+ 1 18: else 19: i = N 20: i = i+ 1 21: F = A/(A+R) 22: Q = A+R 23: if max ( KX , |F− j N |√Q\nKY √ log logQ\n)\n< MX\nthen\n24: MX = max ( KX , |F− j\nN | √ Q\nKY √ log logQ\n)\n25: if MX > MY then 26: MY = MX 27: if MY < M then 28: M = MY 29: P = j/N\n30: return P\ndisproves φN , and otherwise does not halt. For convenience, in Algorithm 1, we define log q = 1 for q < 2.\nLet TM(N) be the set of all Turing machines X expressible in at most logN bits such that U(X,N) accepts in time at most T (N). The encoding of Turing machines must be prefix-free, which in particular means that no Turing machine is encoded in 0 bits. Let JN denote the set of rational numbers of the form j\nN with\nj = 0, . . . , N . For X and Y Turing machines, let K(X) be the number of bits necessary to encode X . Let S′(X,Y ) be the subset of natural numbers i which are accepted by both U(X, i) and U(Y, i) in time at most T (i). Let QN (X,Y ) be the greatest number less than or equal to N such that for every s in the first QN (X,Y ) elements of S′, U(L, s) halts in time T (N). Let FN (X,Y ) be the proportion of the first QN (X,Y ) elements of S\n′ which L accepts. Let\nBN (X,Y, P )\n= max\n(\nK(X), |FN (X,Y )− P |\n√\nQN (X,Y )\nK(Y ) √\nlog logQN (X,Y )\n)\n.\nLemma 4.1. The output of AL,T on input N is in\nargmin P∈JN max Y ∈TM(N) min X∈TM(N) BN (X,Y, P ).\nProof. The algorithm has three for loops, the outer ranging over j = 0, . . .N and the inner two ranging over Y and X respectively, both restricted to Turing machines expressible in logN bits. The condition on line 8 means that X and Y effectively range over all Turing machines in TM(N), and P = j\nN ranges over\nJN . The inner while loop will increment the variables A or R a total of exactly QN(X,Y ) times. Thus, Q is set to QN(X,Y ) in line 22. Similarly, F is sent to FN (X,Y ) in line 21. Clearly KX and KY are K(X) and K(Y ) respectively. Therefore, the expression on lines 23 and 24 is BN (X,Y, P ).\nConsidering the for loops from inner to outer, we minimize this quantity in X , maximize it in Y , and find P of the form j/N minimizing the whole quantity. The P returned is therefore a minimizer of\nmax Y ∈TM(N) min X∈TM(N) BN(X,Y, P ).\nThe code is not optimized for computational efficiency. The following proposition is just to ensure that the runtime is not far off from T (N).\nProposition 4.2. The runtime of AL,T (N) is in O(R(N)) = O(T (N)N4 logT (N))).\nProof. Simulating U on any input for T time steps can be done in time cT logT for some fixed constant c [17]. The bulk of the runtime comes from simulating Turing machines on lines 8, 13, 14, and 16. Each of these lines takes at most cT (N) logT (N) time, and we enter each of these lines at most N4 times. Therefore, the program runs in time O(T (N)N4 logT (N))."
    }, {
      "heading" : "5 Passing the Generalized Benford Test",
      "text" : "We are now ready to show that AL,T passes the generalized Benford test. The proof will use the following two lemmas.\nLemma 5.1. Let S be an irreducible pattern with probability p, and let Z be a Turing machine such that U(Z,N) accepts in time T (N) if and only if N ∈ S.\nThere exists a constant C such that if N ∈ S, then there exists a P ∈ JN such that\nmax Y ∈TM(N) BN (Z, Y, P ) < C.\nProof. Let P = ⌊pN⌋ N\n. From the definition of irreducible pattern, we have that there exists c such that for all Y ,\n|FN (Z, Y )− p|< cK(Y )\n√\nlog logQN (Z, Y ) √\nQN(Z, Y ) .\nClearly,\n|P − p|≤ 1 N ≤ 1 QN(Z, Y ) ≤ 1√ QN (Z, Y )\n≤ K(Z)K(Y ) √ log logQN(Z, Y ) √\nQN (Z, Y ) .\nSetting C = K(Z) + c, we get\n|FN (Z, Y )− P | ≤ |FN (Z, Y )− p|+|P − p|\n< CK(Y )\n√\nlog logQN (Z, Y ) √\nQN(Z, Y ) ,\nso |FN (Z, Y )− P | √ QN(Z, Y )\nK(Y ) √ log logQN (Z, Y ) < C.\nClearly, K(Z) < C, so BN (Z, Y, P ) > C for all Y . Therefore,\nmax Y ∈TM(N) BN (Z, Y, P ) < C.\nLemma 5.2. Let S be an irreducible pattern with probability p, and let Z be a Turing machine such that U(Z,N) accepts in time T (N) if and only if N ∈ S.\nFor all C, for all ε > 0, for all N sufficiently large, for all P ∈ JN , if N ∈ S, and\nmin X∈TM(N) BN (X,Z, P ) < C,\nthen |P − p|< ε.\nProof. Fix a C and a ε > 0. It suffices to show that for all N sufficiently large, if N ∈ S and |P − p|≥ ε, then for all X ∈ TM(N), we have BN (X,Z, P ) ≥ C.\nObserve that since BN (X,Z, P ) ≥ K(X), this claim trivially holds when K(X) ≥ C. Therefore we only have to check the claim for the finitely many Turing machines expressible in fewer than C bits.\nFix an arbitrary X . Since S is an irreducible pattern, there exists a c such that\n|FN (X,Z)− p|< cK(Z)\n√\nlog logQN (X,Z) √\nQN (X,Z) .\nWe may assume that S′(X,Z) is infinite, since otherwise if we take N ∈ S large enough, X /∈ TM(N). Thus, by taking N sufficiently large, we can get QN (X,Z) sufficiently large, and in particular satisfy\n√\nQN (X,Z)\nK(Z) √ log logQN(X,Z) ε ≥ C + c.\nTake N ∈ S large enough that this holds for each X ∈ TM(N) with K(X) < C, and assume |P − p|≥ ε. By the triangle inequality, we have\n|FN (X,Z)− P |≥ |P − p|−|FN (X,Z)− p|\n≥ ε− cK(Z) √ log logQN (X,Z) √\nQN (X,Z) .\nTherefore\nBN (X,Z, P )\n≥\n( ε− cK(Z) √\nlog logQN (X,Z)√ QN (X,Z)\n)\n√\nQN (X,Z)\nK(Z) √\nlog logQN(X,Z)\n=\n√\nQN(X,Z)\nK(Z) √ log logQN (X,Z) ε− c ≥ C,\nwhich proves the claim.\nTheorem 5.3. AL,T passes the generalized Benford test.\nProof. Let S be an irreducible pattern with probability p. We must show that\nlim N→∞ N∈S AL,T (N) = p.\nLet Z be a Turing machine such that U(Z,N) accepts in time T (N) if and only if N ∈ S.\nBy considering the case when X = Z, Lemma 5.1 implies that there exists a constant C such that for all N sufficiently large, there exists a P ∈ JN such that\nmax Y ∈TM(N) min X∈TM(N) BN (X,Y, P ) < C.\nSimilarly, using this value of C, and considering the case where Y = Z, Lemma 5.2 implies that for all ε > 0, for all N sufficiently large, for all P ∈ JN if N ∈ S, and\nmax Y ∈TM(N) min X∈TM(N) BN (X,Y, P ) < C,\nthen |P − p|≤ ε. Combining these, we get that for all ε > 0, for all N sufficiently large, if N ∈ S and if P is in argmin P∈JN max Y ∈TM(N) min X∈TM(N) BN (X,Y, P ),\nthen |P − p|≤ ε. Thus, by Lemma 4.1, we get that for all ε > 0, for all N sufficiently large, if N ∈ S, then |AL,T (N) − p|≤ ε, so\nlim N→∞ N∈S AL,T (N) = p."
    }, {
      "heading" : "6 Final Remarks",
      "text" : "Definition 6.1. Given a sentence ψ, consider the infinite sequence of integers {sψn} given by φsψ\n0\n= ψ and\nφ s ψ n+1 = ¬¬φ s ψ n . If a machine M satisfies\nlim n→∞\nM(sψn) = p,\nwe say that M converges to p on ψ.\nCorollary 6.2. If ψ is provable, then AL,T converges to 1 on ψ. If ψ is disprovable, then AL,T converges to 0 on ψ.\nProof. If ψ is provable, then {sψn} is an irreducible pattern with probably 1. If ψ is disprovable, then {sψn} is an irreducible pattern with probably 0.\nIf ψ is neither provable nor disprovable, then it is not clear whether or not AL,T even converges on ψ.\nQuestion 6.3. Does there exist a machine M such that M passes the generalized Benford test, and for each sentence ψ, there exists a P (ψ) such that M converges to P (ψ) on ψ?\nDefinition 6.4. A function P from logical sentences to [0, 1] is called coherent if it satisfies the following three properties:\n1. P (φ) = 1 for all provable φ,\n2. P (φ) = 0 for all disprovable φ, and\n3. P (φ) = P (φ ∧ ψ) + P (φ ∧ ¬ψ) for all φ and ψ. Coherent functions correspond to probability distributions on the space of complete extensions of a given theory.\nQuestion 6.5. Does there exist a machine M and a coherent function P such that M passes the generalized Benford test, and for each sentence ψ, M converges to P (ψ) on ψ?"
    } ],
    "references" : [ {
      "title" : "Non-Omniscience, Probabilistic Inference, and Metamathematics",
      "author" : [ "Paul Christiano" ],
      "venue" : "Tech. rep. 2014–3. Berkeley, CA: Machine Intelligence Research Institute,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Logical Prior Probability",
      "author" : [ "Abram Demski" ],
      "venue" : "Artificial General Intelligence. 5th International Conference,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Reasoning with Limited Resources and Assigning Probabilities to Arithmetical Statements",
      "author" : [ "Haim Gaifman" ],
      "venue" : "Synthese",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Concerning Measures in First Order Calculi",
      "author" : [ "Haim Gaifman" ],
      "venue" : "In: Israel Journal of Mathematics",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1964
    }, {
      "title" : "Probabilities on Sentences in an Expressive Logic",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Journal of Applied Logic",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Assigning probabilities to logical formulas",
      "author" : [ "Dana Scott", "Peter Krauss" ],
      "venue" : "Studies in Logic and the Foundations of Mathematics",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1966
    }, {
      "title" : "Measures in Boolean algebras",
      "author" : [ "Alfred Horn", "Alfred Tarski" ],
      "venue" : "In: Transactions of the American Mathematical Society",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1948
    }, {
      "title" : "Measures on Boolean algebras",
      "author" : [ "J.L. Kelley" ],
      "venue" : "Pacific Journal of Mathematics",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1959
    }, {
      "title" : "An algebraic characterization of measure algebras",
      "author" : [ "Dorothy Maharam" ],
      "venue" : "Annals of Mathematics",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1947
    }, {
      "title" : "Questions of Reasoning Under Logical Uncertainty",
      "author" : [ "Nate Soares", "Benja Fallenstein" ],
      "venue" : "Tech. rep. 2015– 1. Berkeley, CA: Machine Intelligence Research Institute,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Introduction to statistical relational learning",
      "author" : [ "Lise Getoor" ],
      "venue" : "MIT press,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Explaining the uneven distribution of numbers in nature: the laws of Benford and Zipf",
      "author" : [ "L. Pietronero" ],
      "venue" : "Physica A: Statistical Mechanics and its Applications",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "On the notion of infinite pseudorandom sequences",
      "author" : [ "Ker-I Ko" ],
      "venue" : "Theoretical Computer Science",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1986
    }, {
      "title" : "The definition of random sequences",
      "author" : [ "Per Martin-Löf" ],
      "venue" : "Information and Control",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1966
    }, {
      "title" : "Algorithmic randomness and complexity",
      "author" : [ "Rodney G. Downey", "Denis R. Hirschfeldt" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Two-tape simulation of multitape Turing machines",
      "author" : [ "F.C. Hennie", "R.E. Stearns" ],
      "venue" : "Journal of the ACM",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1966
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The goal of logical uncertainty is to construct an algorithm M which on input N outputs a probability M(N), which represents the probability that φN is true [1, 2, 3, 4].",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "The goal of logical uncertainty is to construct an algorithm M which on input N outputs a probability M(N), which represents the probability that φN is true [1, 2, 3, 4].",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "The goal of logical uncertainty is to construct an algorithm M which on input N outputs a probability M(N), which represents the probability that φN is true [1, 2, 3, 4].",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "Much work has been done on a related problem where M on input N outputs an infinite sequence of numbers and M(N) is defined to be the limit of the sequence output by M on input N [1, 2, 11].",
      "startOffset" : 179,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : "Much work has been done on a related problem where M on input N outputs an infinite sequence of numbers and M(N) is defined to be the limit of the sequence output by M on input N [1, 2, 11].",
      "startOffset" : 179,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "Much work has been done on a related problem where M on input N outputs an infinite sequence of numbers and M(N) is defined to be the limit of the sequence output by M on input N [1, 2, 11].",
      "startOffset" : 179,
      "endOffset" : 189
    }, {
      "referenceID" : 3,
      "context" : "The problem has also been studied in the case where we don’t require computability even in the limit [5, 6, 7].",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "The problem has also been studied in the case where we don’t require computability even in the limit [5, 6, 7].",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "The problem has also been studied in the case where we don’t require computability even in the limit [5, 6, 7].",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "The problem was first studied in the context of measures on Boolean algebras [8, 9, 10].",
      "startOffset" : 77,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "The problem was first studied in the context of measures on Boolean algebras [8, 9, 10].",
      "startOffset" : 77,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "The problem was first studied in the context of measures on Boolean algebras [8, 9, 10].",
      "startOffset" : 77,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Logical uncertainty is one aspect of the problem of combining probability and logic, of which statistical relational learning is another [12].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "” [1]",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 11,
      "context" : "Many mathematical sequences have been shown to have frequencies of first digits that satisfy Benford’s law [13].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "However, algorithmic randomness generally considers all computable tests and focuses on the case where p = 1 2 [14, 15, 16].",
      "startOffset" : 111,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "However, algorithmic randomness generally considers all computable tests and focuses on the case where p = 1 2 [14, 15, 16].",
      "startOffset" : 111,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "However, algorithmic randomness generally considers all computable tests and focuses on the case where p = 1 2 [14, 15, 16].",
      "startOffset" : 111,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Simulating U on any input for T time steps can be done in time cT logT for some fixed constant c [17].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "A function P from logical sentences to [0, 1] is called coherent if it satisfies the following three properties:",
      "startOffset" : 39,
      "endOffset" : 45
    } ],
    "year" : 2015,
    "abstractText" : "We give an algorithm AL,T which assigns probabilities to logical sentences. For any simple infinite sequence {φsn} of sentences whose truthvalues appear indistinguishable from a biased coin that outputs “true” with probability p, we have limn→∞ AL,T (sn) = p.",
    "creator" : "LaTeX with hyperref package"
  }
}