{
  "name" : "1610.01085.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards the Design of Prospect-Theory based Human Decision Rules for Hypothesis Testing",
    "authors" : [ "V. Sriram Siddhardh", "Pramod K. Varshney" ],
    "emails" : [ "varshney}@syr.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Binary Hypothesis Testing, Prospect Theory, Optimists, Pessimists.\nI. INTRODUCTION\nCognitive behavior has traditionally been modeled using rationality models, where the human agents are assumed to behave in an unbiased manner. Unbiased decision-makers are often assumed to minimize Bayes risk, which is defined as the expected cost of making decisions [1]. However, in the real world, human agents may have a cognitive bias, due to the limited availability of information and/or other complex behaviors such as emotions, loss-aversion and endowment effect [2]–[6]. Such complex agents were successfully modeled by Kahneman and Tversky using prospect theory in [3], where human behavior is modeled using weight and value functions over probabilities and costs respectively. In this paper, we derive optimal decision rules for binary hypothesis testing employed by two special\nV. Sriram Siddhardh Nadendla, Swastik Brahma and Pramod K. Varshney are with the Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY 13201, USA. E-mail: {vnadendl, skbrahma, varshney}@syr.edu.\nThis work was supported in part by CASE: The Center for Advanced Systems and Engineering, a NYSTAR center for advanced technology at Syracuse University, the Department of the Army under the Cooperative Research Agreement: W911-NF-13-2-0040, and NSF grant No. 1609916. .\nprospect-theory based human agents, namely optimists and pessimists.\nIn the past, several efforts have been geared towards validating theoretical models to comprehend decision rules employed by human agents, using experimental data. Recently, researchers have been showing significant interest in the design of complex networked systems where human agents interact with machines effectively so that the system operates with maximal efficiency [7]. Particularly, in the context of binary hypothesis testing, there are several crowdsensing based applications such as Tomnod and Zooniverse where human volunteers participate in the decision-making process of a proposed task. In some cases, systems are designed to emulate human behavior in order to reduce human effort and intervention. One example is the design of self-driving cars by Google and Uber, which move in traffic alongside human-driven vehicles. In contrast, there are other applications where there is a need to steer/nudge human decisions in order to improve the overall performance of the system [8]. In this paper, we study optimal behavioral rules in human agents within the context of binary hypothesis testing, which is essential to propose a design framework, where human decisions can be either emulated/steered in a controlled manner in any human-machine interaction system.\nTraditionally, binary hypothesis testing has been extensively studied by researchers over several decades under different scenarios [1], [9]. In particular, Bayesian detection rules are designed under the premise that the decision-maker is rationally motivated to minimize its Bayes risk. With the advent of novel systems which include human decision makers, there is a need to redesign detection systems with human decisionmakers. Design of such systems has gained interest, some of such studies are discussed below. In [10], Rhim et al. have modeled the problem of distributed hypothesis testing as a categorical decision-making problem. Inspired from the observation that human agents make decisions categorically, the authors have\nar X\niv :1\n61 0.\n01 08\n5v 1\n[ cs\n.A I]\n4 O\nct 2\n01 6\n2 Decision-Maker Hypothesis θ ∈ {0, 1} r u\nFig. 1: Framework for Binary Hypothesis-Testing\nmodeled the concept of categorization via quantization of prior probabilities. In [11], Wimalajeewa and Varshney have investigated the problem of collaborative human decision making, where human agents are modeled as likelihood-ratio decision rules with random thresholds. More recently, Vempaty et al. have proposed a coding theory based framework in [12] to improve the performance of a distributed detection network with human agents. Later, in [13], Vempaty et al. have proposed a Bayesian hierarchical model for the distributed detection framework to capture the uncertainty in the behavior of human teams.\nIn contrast to the past work, we consider the problem of decision making by an individual human agent in the context of binary hypothesis testing. We assume that the decision-making of these individual human agents can be represented by prospect theory models, and therefore, they make decisions that minimize their behavioral risk, which is defined using prospect theory. To the best of our knowledge, this is the first work that considers the problem of prospect theory based decision making for binary hypothesis testing. We investigate the structure of optimal decision rules employed by two special types of behavioral agents, namely optimists and pessimists [14], under two different conditions depending on the decision costs incurred at the agent.\nII. PROBLEM SETUP\nConsider a binary hypothesis-testing framework with a behavioral decision-maker, as shown in Figure 1, where the true hypothesis is denoted by θ ∈ {0, 1}. Let π0 and π1 denote the prior probabilities of the hypotheses θ = 0 and θ = 1 respectively. We assume that the decision-maker receives a real-valued signal r ∈ R from the phenomenon-of-interest (PoI) with conditional distribution p(r|θ) and processes it to make an inference u about the true hypothesis θ.\nIn this paper, the rationality of the decision-maker is modeled using prospect theory [3], where the behavioral agent cognitively distorts the probabilities and costs using known weight and value functions respectively. In other words, any given probability ρ and any given cost c are perceived as w(ρ) and v(c) respectively,\nwhere w(·) is the weight function and v(·) is the value function in the behavioral model. Our goal is to design an optimal decision rule that optimizes the behavioral risk at the decision maker.\nIn this paper, we assume that the decision maker employs the following decision rule:\nu = { 1; if r ∈ R 0; otherwise,\n(1)\nwhere R is the acceptance region of the hypothesis θ. The performance of the decision rule, as given in Equation (1), is given by\nx = Pr(u = 1|θ = 0) = ∫ R p(r|θ = 0)dr, (2a)\ny = Pr(u = 1|θ = 1) = ∫ R p(r|θ = 1)dr, (2b)\nwhere p(r|θ = 0) and p(r|θ = 1) are the conditional pdfs of the observation r under the two hypotheses θ = 0, 1 respectively.\nGiven that the decision maker makes an inference u = i when the true hypothesis is θ = j, we assume that the decision maker incurs a cost cij for any i, j ∈ {0, 1}. Therefore, the behavioral risk due to the decision rule given in Equation (1), is defined below:\nf(R) = 1∑ i=0 1∑ j=0 w[Pr(u = i, θ = j)] · v(cij). (3)\nAssuming that the decision maker always wishes to minimize its behavioral risk f(R), we present the following problem statement.\nProblem 1. Find the optimal acceptance regionR∗ that minimizes f(R) over all possible subsets of R.\nInstead of representing a decision rule using its corresponding acceptance regionR, one can also equivalently parameterize the performance of the decision rule using two variables: x = Pr(u = 1|θ = 0) and y = Pr(u = 1|θ = 1), as defined in Equation (2). In such a case, the behavioral risk as given in Equation (3), can be rewritten as follows.\nf(x, y) = g(x) + h(y), (4)\nwhere\ng(x) = w[π0(1− x)]v(c00) + w[π0x]v(c10), (5a)\nh(y) = w[π1(1− y)]v(c01) + w[π1y]v(c11). (5b)\nFurthermore, note that both x and y increase as the size of the region R increases, since the area\nunder both the conditional distributions p(x|θ = 0) and p(x|θ = 1) changes concurrently with the region R. Furthermore, from Caratheodary theorem [15], we know that any ROC curve can be made concave by allowing randomization of decision rules. Therefore, in this paper, we assume that y is a concave-increasing function of x, for all 0 ≤ x ≤ 1, with y(x = 0) = 0 and y(x = 1) = 1.\nNote that the solution to the above problem depends on the weight and the value functions which may even make Problem 1 intractable to solve. Therefore, in the following section, we present some basic assumptions on the weight and value functions which have been experimentally verified by several researchers in the past literature [5].\nIII. BEHAVIORAL AGENTS AND THEIR PROPERTIES\nProspect theory models are defined using weight and value functions. While the risk-seeking/risk-averse nature of an agent is captured by the value function, the optimism/pessimism of an agent is modeled using the weight function. For example, the fear of an accident may make the probability of its occurrence seem larger to a human decision-maker, than the true probability of occurrence of an accident. Furthermore, the risk-averse behavior of human agents drives them to overvalue the cost of an accident. Prospect theory has been experimentally studied by several economists and psychologists, and is currently accepted universally\nto model human behavior. In the rest of this section, we present practical models for both weight and value functions that have been verified extensively in the literature.\nIn prospect theory, the distortion of probabilities due to human behavior is modeled via weight functions. These weight functions are bounded within the unit square, with the line w(p) = p denoting the unbiased cognitive behavior. The region above this line corresponds to the region of optimism, while the region below w(p) = p is known as the region of pessimism. In most experimental studies1, as pointed out in Figure 2a, the weight functions have been observed to behave in the following manner.\nProperty 1. There exists a unique p∗ ∈ [0, 1] such that the weight function w(p) is concave for all p < p∗, and convex for all p ≥ p∗.\nNote that, the agent is optimistic when p∗ = 1, and the agent is pessimistic when p∗ = 0 [14]. Furthermore, if the agent is optimistic, since the weight function is always bounded within a unit square, w(p) is concaveincreasing for all p ∈ [0, 1]. Similarly, in the case of pessimistic agents, the weight function is convexincreasing for all p ∈ [0, 1].\nOn the other hand, the value functions are distortions\n1For more details about why the weight function has these properties in practice, the readers may refer to [16], [17] and references therein.\n4 that the human agent perceives when they incur a cost (or a reward). These functions are not necessarily bounded, and are observed to be piecewise convex2, as shown in Figure 2b. But, in this paper, we relax this to a more general assumption, as defined below.\nProperty 2. v(c) is continuous and monotonically increasing for any c ∈ R. Consequently, there exists a unique point c∗ ∈ R such that v(c∗) = 0.\nNote that the cost c∗ at which v(c∗) = 0 is wellknown as the reference point. The reference point c∗ plays a major role in the decision-making process, and can be interpreted as a target/goal to any human decision maker [18]. Consequently, any cost below the reference point c∗ appears as profits, and any cost above c∗ appears as losses to the human decision maker. Although several human behaviors such as loss aversion are attributed to the convexity of the value function, we have considered some special agent models in this paper which only rely on the continuity and monotonicity of the value function, and the reference point c∗.\nIV. OPTIMAL DETECTION RULES FOR OPTIMISTS\nIn this section, we assume that the behavioral agent is optimistic as shown in Figure 3, and, therefore, the weight function is concave increasing. In such a case, we investigate two different scenarios depending on the decision costs incurred at the agent, as shown below.\nType-1: c∗ ≤ min{c00, c01, c10, c11} This is the case where even an optimist perceives the costs of all the possible choices to be detrimental, i.e., the decision costs of all the available choices are perceived as losses. In other words, given a reference point c∗, the optimist perceives a decision cost cij as v(cij) ≥ 0, for all i, j ∈ {0, 1}. As a result, we have the following lemma.\nLemma 1. For all Type-1 optimists, f(x, y(x)) is a concave function of x, for all 0 ≤ x ≤ 1.\nProof: We differentiate Equation (5a) twice to obtain the following.\nd2g(x)\ndx2 = π20\n{ d2w[π0x]\ndx2 v(c00) + d2w[π0(1− x)] dx2 v(c10)\n} .\n(6)\n2Traditionally, value functions are defined over rewards, and therefore, are assumed to be concave. In this paper, we define value functions over costs (or negative rewards) in order to align with the traditional definition of Bayes risk.\np\nw(p) Unbiased cognitive behavior\n0 p∗ = 1\n1\nOptimistic cognitive behavior\nFig. 3: Weight Function for Optimistic Agents\nGiven that the behavioral agent is optimistic, we have\nd2w[π0x]\ndx2 ≤ 0, and d 2w[π0(1− x)] dx2 ≤ 0. (7)\nSince v(c00) and v(c10) are both non-negative, we have d2g(x)\ndx2 ≤ 0.\nSince both g(x) and h(y) have similar structure, we\nalso have d2h(y)\ndy2 ≤ 0. Given that y is a concave\nincreasing function of x, h(y(x)) is a concave function of x. Since f(x, y(x)) is the sum of two concave functions g(x) and h(y(x)), the behavioral risk is a concave function of x.\nGiven that the behavioral agent wishes to minimize its risk f(x, y(x)), the optimal decision rule lies on the extreme point of the ROC curve, i.e., (x, y) = (0, 0) or (x, y) = (1, 1). Furthermore, if c00 = c11 = cL ≤ cU = c01 = c10, we have\nf(0, 0) = w(π0)v(cL) + w(π1)v(cU ), (8a)\nf(1, 1) = w(π0)v(cU ) + w(π1)v(cL). (8b)\nAs a result, if π0 ≥ 1\n2 , we have w(π0) ≥ w(π1). So,\nwe have\nf(1, 1)−f(0, 0) = [w(π0)− w(π1)]·[v(cU )− v(cL)] ≥ 0. (9) In other words, the behavioral agent chooses the operating point (1, 1) in the ROC, which is equivalent to a decision rule where R = R, i.e., the behavioral agent always decides u = 1.\n5 On the other hand, if π0 < 1\n2 , we have\nf(1, 1)−f(0, 0) = [w(π0)− w(π1)]·[v(cU )− v(cL)] ≤ 0. (10) As a result, the behavioral agent adopts the operating point (0, 0). This is equivalent to the case where the behavioral agent always decides u = 0.\nIn summary, we have the following theorem.\nTheorem 1. If c∗ ≤ min{c00, c01, c10, c11}, an optimist minimizes its behavioral risk by either always deciding u = 0, or u = 1, for any observation x ∈ R.\nFurthermore, if c00 = c11 = cL ≤ cU = c01 = c10, then a Type-1 optimist employs the following decision rule.\nu = 1, if π0 ≥ 1\n2 0, otherwise.\n(11)\nIn summary, when all the decision costs appear detrimental to an optimist, the optimal decision rule is fixed, and independent of data. Furthermore, in concurrence to our intuition, whenever c00 = c11 = cL ≤ cU = c01 = c10, the optimist optimally chooses the option u0 ∈ {0, 1} that is antipodal to prior probabilities in order to minimize its behavioral risk.\nType-2: c∗ ≥ max{c00, c01, c10, c11} In contrast to Type-1 optimists, Type-2 optimists interpret the same decision costs as being profitable. In other words, given that the reference point c∗ lies above all the decision costs, v(cij) ≤ 0, for all i, j ∈ {0, 1}. As a result, we have the following lemma.\nLemma 2. For all Type-2 optimists, f(x, y(x)) is a convex function of x, for all 0 ≤ x ≤ 1.\nProof: Proof is similar to that of Lemma 1, and is therefore, omitted for the sake of brevity.\nIn other words, the behavioral agent minimizes its risk at some intermediate operating point (x∗, y∗). In the following theorem, we state the necessary condition that (x∗, y∗) will satisfy. We find this condition by equating the first derivative of the risk function f(x, y(x)) with respect to x to zero.\nTheorem 2. The operating point (x∗, y∗) of the optimal decision rule employed by a Type-2 optimist is a root of the equation dw[π0x ∗]\ndx v(c10) +\ndw[π1y ∗(x∗)]\ndx v(c11)\n= dw[π0(1− x∗)]\ndx v(c00) + dw[π1{1− y∗(x∗)}] dx v(c01).\n(12)\np\nw(p) Unbiased cognitive behavior\np∗ = 0 1\n1\nPessimistic cognitive behavior\nFig. 4: Weight Function for Pessimistic Agents\nGiven that all the decision costs appear profitable to a Type-2 optimist, it is intuitive that the agent chooses a decision rule that minimizes its behavioral risk. Theorem 2 presents the necessary condition to find the optimal operating point for the Type-2 optimist.\nV. OPTIMAL DETECTION RULES FOR PESSIMISTS\nIn this section, we assume that the behavioral agent is pessimistic as shown in Figure 4, and therefore, the weight function is convex increasing. More specifically, we analyze the same two types of agents as discussed in Section IV, within the context of pessimists.\nType-1: c∗ ≤ min{c00, c01, c10, c11} As discussed in Section IV, we assume that a Type-1 pessimist also finds all its possible decision choices to be detrimental. In other words, given that c∗ lies below all the decision costs, v(cij) ≥ 0, for all i, j ∈ {0, 1}. As a result, we have the following lemma.\nLemma 3. For all Type-1 pessimists, f(x, y(x)) is a convex function of x, for all 0 ≤ x ≤ 1.\nProof: We differentiate Equation (5a) twice to obtain the following.\nd2g(x)\ndx2 = π20\n{ d2w[π0x]\ndx2 v(c00) + d2w[π0(1− x)] dx2 v(c10)\n} .\n(13) Given that the behavioral agent is pessimistic, we have\nd2w[π0x]\ndx2 ≥ 0, and d 2w[π0(1− x)] dx2 ≥ 0. (14)\n6 Since v(c00) and v(c10) are both non-negative, we have d2g(x)\ndx2 ≥ 0.\nSince both g(x) and h(y) have a similar structure,\nwe also have d2h(y)\ndy2 ≥ 0. Given that y is a convex\nincreasing function of x, h(y(x)) is a convex function of x. Since f(x, y(x)) is the sum of two convex functions g(x) and h(y(x)), the behavioral risk is a convex function of x.\nIn other words, the behavioral agent minimizes its risk at some intermediate operating point (x∗, y∗), which satisfies the following necessary condition. We find this condition by equating the first derivative of the risk function f(x, y(x)) with respect to x to zero.\nTheorem 3. The operating point (x∗, y∗) of the optimal decision rule employed by a Type-1 pessimist is a root of the equation\ndw[π0x ∗]\ndx v(c10) +\ndw[π1y ∗(x∗)]\ndx v(c11)\n= dw[π0(1− x∗)]\ndx v(c00) + dw[π1{1− y∗(x∗)}] dx v(c01).\n(15)\nNote that our results in Theorem 3 are contrary to that of a Type-1 optimist, and resemble that of Theorem 2. Although all the decision costs appear detrimental, due to the pessimistic nature of the agent, the agent will attempt to find the optimal rule that satisfies the necessary condition stated in Theorem 3.\nType-2: c∗ ≥ max{c00, c01, c10, c11} As stated in Section IV, we assume that all the decision costs appear profitable to the agent. In other words, the agent’s reference point c∗ lies above all the decision costs, v(cij) ≤ 0, for all i, j ∈ {0, 1}. As a result, we have the following lemma.\nLemma 4. For all Type-2 pessimists, f(x, y(x)) is a concave function of x, for all 0 ≤ x ≤ 1.\nProof: Proof is similar to that of Lemma 1, and is therefore, omitted for the sake of brevity.\nGiven that the behavioral agent wishes to minimize its risk f(x, y(x)), the optimal decision rule lies on the extreme point of the ROC curve, i.e., (x, y) = (0, 0) or (x, y) = (1, 1). Furthermore, if c00 = c11 = cL ≤ cU = c01 = c10, we have\nf(0, 0) = w(π0)v(cL) + w(π1)v(cU ), (16a)\nf(1, 1) = w(π0)v(cU ) + w(π1)v(cL). (16b)\nAs a result, if π0 ≥ 1\n2 , we have w(π0) ≥ w(π1). So,\nwe have\nf(1, 1)−f(0, 0) = [w(π0)− w(π1)]·[v(cU )− v(cL)] ≥ 0. (17) In other words, the behavioral agent chooses the operating point (1, 1) in the ROC, which is equivalent to a decision rule where R = R, i.e., the behavioral agent always decides u = 1.\nOn the other hand, if π0 < 1\n2 , we have\nf(1, 1)−f(0, 0) = [w(π0)− w(π1)]·[v(cU )− v(cL)] ≤ 0. (18) As a result, the behavioral agent adopts the operating point (0, 0). This is equivalent to the case where the behavioral agent always decides u = 0.\nIn summary, we have the following theorem.\nTheorem 4. If c∗ ≥ min{c00, c01, c10, c11}, a pessimist minimizes its behavioral risk by either always deciding u = 0, or u = 1, for any observation x ∈ R.\nFurthermore, if c00 = c11 = cL ≤ cU = c01 = c10, then a Type-2 pessimist employs the following decision rule.\nu = 1, if π0 ≥ 1\n2 0, otherwise.\n(19)\nIn summary, even though a Type-2 pessimist finds all the decision costs to be profitable, the agent still chooses a data-independent decision rule. Particularly, when c00 = c11 = cL ≤ cU = c01 = c10, the Type-2 pessimist chooses a decision that is antipodal to the prior information, just as in the case of a Type-1 optimist.\nVI. ILLUSTRATIVE EXAMPLE\nIn this section, we consider a simple example where the observation r follows a normal distribution with mean θ and unit variance, where θ denotes the state of the true hypothesis. We assume that the true decision costs incurred by the human agent are given by c00 = c11 = cL = −1 and c01 = c10 = cU = 1. In such a case, the optimal Bayesian detection rule is given by\nu = 1; if x ≥ π0 π1\n0; otherwise, (20)\nand the corresponding optimal Bayes risk is given by\nB(x, y) = 1∑ i=0 1∑ j=0 Pr(u = i, θ = j) · cij\n= π0[x · 1 + (1− x) · (−1)] +π1[(1− y) · 1 + y · (−1)]\n= π0(2x− 1) + π1(1− 2y),\n(21)\nwhere x = Q ( π0 π1 ) and y = Q ( π0 π1 − θ ) .\nTo illustrate our results obtained in Sections IV and V, we consider the following model for a human decision-maker and present optimal decision rules for both Type-1 and Type-2 optimists and pessimists.\nw(p) = pα, ∀ p ∈ [0, 1] and α > 0, (22a)\nv(c) = ec − ec∗ , ∀ c ∈ R, (22b) where α and c∗ are the agent’s behavioral parameters. More specifically, α > 1 if the agent is a pessimist, α = 1 if the agent is unbiased, and 0 < α < 1 if the agent is an optimist. Similarly, c∗ is the reference cost of the human decision-maker, as discussed in Section III. Assuming that the human agent employs a likelihoodratio test, we have the following ROC curve.\ny = Q ( Q−1(x)− θ ) . (23)\nUsing this example, we present the operating points adopted by both optimists and pessimists in Figure 5, by obtaining the optimal rules using the gradient descent algorithm. In order to illustrate the results for both Type-1 and Type-2 agents, we present numerical results\nfor two scenarios: (i) π0 = 0.25 and (ii) π0 = 0.75. Note that, in the first scenario when π0 = 0.25, the optimal operating points for both Type-1 optimist and Type-2 pessimist lie at (1, 1), which corroborates our theoretical analysis. We also observe a similar behavior with both Type-1 optimists and Type-2 pessimists when π0 = 0.75, where the operating points lie at (0, 0). In contrast, in the case of Type-2 optimists and Type-1 pessimists, we observe that the optimal decision rules from a behavioral perspective lie at different operating points, and also deviate from the Bayesian detector. Furthermore, since Bayesian decision rules are not necessarily optimal from a behavioral perspective, our results illustrate how prospect theory based decision rules deviate from the Bayesian decision rule.\nVII. CONCLUSION AND FUTURE WORK\nIn this paper, we investigated optimal binary hypothesis testing rules employed by two types of prospect theory based optimists and pessimists. We found that the optimal decision rule employed by an optimist or a pessimist can significantly deviate from the rule which is designed to minimize the Bayes risk. In the future, we will analyze other types of agents and obtain their corresponding optimal decision rules. Note that, in the real world, a typical human agent is neither an optimist, nor a pessimist, and is known to exhibit more complex behavior. Therefore, we will consider behavioral models beyond optimists/pessimists that closely mimic human agents and find optimal decision rules employed by such human agents.\n8"
    } ],
    "references" : [ {
      "title" : "Fundamentals of Statistical Signal Processing, Volume II: Detection Theory",
      "author" : [ "S. Kay" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1993
    }, {
      "title" : "Rational decision making in business organizations",
      "author" : [ "H.A. Simon" ],
      "venue" : "The American economic review, vol. 69, no. 4, pp. 493–513, 1979.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Prospect theory: An analysis of decision under risk",
      "author" : [ "D. Kahneman", "A. Tversky" ],
      "venue" : "Econometrica, vol. 47, no. 2, pp. 263– 291, 1979.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Behavioral decision theory: Processes of judgment and choice",
      "author" : [ "H.J. Einhorn", "R.M. Hogarth" ],
      "venue" : "Journal of Accounting Research, pp. 1–31, 1981.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "A survey of behavioral finance",
      "author" : [ "N. Barberis", "R. Thaler" ],
      "venue" : "Handbook of the Economics of Finance, vol. 1, pp. 1053–1128, 2003.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Decision making under risk and uncertainty",
      "author" : [ "J.G. Johnson", "J.R. Busemeyer" ],
      "venue" : "Wiley Interdisciplinary Reviews: Cognitive Science, vol. 1, no. 5, pp. 736–749, 2010.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Human-centered processes: individual and distributed decision support",
      "author" : [ "G. Coppin", "A. Skrzyniarz" ],
      "venue" : "IEEE Intelligent Systems, vol. 18, no. 4, pp. 27–33, Jul 2003.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Sunstein, Nudge: Improving decisions about health, wealth and happiness",
      "author" : [ "C.R.R.H. Thaler" ],
      "venue" : "Penguin Books,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Distributed Detection and Data Fusion",
      "author" : [ "P.K. Varshney" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Quantization of prior probabilities for collaborative distributed hypothesis testing",
      "author" : [ "J.B. Rhim", "L.R. Varshney", "V.K. Goyal" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 60, no. 9, pp. 4537–4550, Sept 2012.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Collaborative human decision making with random local thresholds",
      "author" : [ "T. Wimalajeewa", "P.K. Varshney" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 61, no. 11, pp. 2975–2989, June 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reliable crowdsourcing for multi-class labeling using coding theory",
      "author" : [ "A. Vempaty", "L.R. Varshney", "P.K. Varshney" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing, vol. 8, no. 4, pp. 667–679, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Decision fusion by people: Experiments, models, and sociotechnical system design",
      "author" : [ "A. Vempaty", "L.R. Varshney", "G.J. Koop", "A.H. Criss", "P.K. Varshney" ],
      "venue" : "IEEE Global Conference on Signal and Information Processing (GlobalSIP), Dec 2015, pp. 83–87.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the intuition of rankdependent utility",
      "author" : [ "E. Diecidue", "P.P. Wakker" ],
      "venue" : "Journal of Risk and Uncertainty, vol. 23, no. 3, pp. 281–298, 2001.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Rockafeller, Convex Analysis, ser. Princeton Landmarks in Mathematics and Physics",
      "author" : [ "T. R" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1996
    }, {
      "title" : "The probability weighting function",
      "author" : [ "D. Prelec" ],
      "venue" : "Econometrica, vol. 66, no. 3, pp. 497–527, 1998.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "On the shape of the probability weighting function",
      "author" : [ "R. Gonzalez", "G. Wu" ],
      "venue" : "Cognitive Psychology, vol. 38, no. 1, pp. 129 – 166, 1999.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Goals as reference points",
      "author" : [ "R.P.L.C. Heath", "G. Wu" ],
      "venue" : "Cognitive psychology, vol. 38, no. 1, pp. 79–109, 1999.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Unbiased decision-makers are often assumed to minimize Bayes risk, which is defined as the expected cost of making decisions [1].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "However, in the real world, human agents may have a cognitive bias, due to the limited availability of information and/or other complex behaviors such as emotions, loss-aversion and endowment effect [2]–[6].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 5,
      "context" : "However, in the real world, human agents may have a cognitive bias, due to the limited availability of information and/or other complex behaviors such as emotions, loss-aversion and endowment effect [2]–[6].",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "Such complex agents were successfully modeled by Kahneman and Tversky using prospect theory in [3], where human behavior is modeled using weight and value functions over probabilities and costs respectively.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "Recently, researchers have been showing significant interest in the design of complex networked systems where human agents interact with machines effectively so that the system operates with maximal efficiency [7].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 7,
      "context" : "In contrast, there are other applications where there is a need to steer/nudge human decisions in order to improve the overall performance of the system [8].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "Traditionally, binary hypothesis testing has been extensively studied by researchers over several decades under different scenarios [1], [9].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "Traditionally, binary hypothesis testing has been extensively studied by researchers over several decades under different scenarios [1], [9].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 9,
      "context" : "In [10], Rhim et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "In [11], Wimalajeewa and Varshney have investigated the problem of collaborative human decision making, where human agents are modeled as likelihood-ratio decision rules with random thresholds.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "have proposed a coding theory based framework in [12] to improve the performance of a distributed detection network with human agents.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Later, in [13], Vempaty et al.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 13,
      "context" : "We investigate the structure of optimal decision rules employed by two special types of behavioral agents, namely optimists and pessimists [14], under two different conditions depending on the decision costs incurred at the agent.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "In this paper, the rationality of the decision-maker is modeled using prospect theory [3], where the behavioral agent cognitively distorts the probabilities and costs using known weight and value functions respectively.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "Furthermore, from Caratheodary theorem [15], we know that any ROC curve can be made concave by allowing randomization of decision rules.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "Therefore, in the following section, we present some basic assumptions on the weight and value functions which have been experimentally verified by several researchers in the past literature [5].",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "There exists a unique p∗ ∈ [0, 1] such that the weight function w(p) is concave for all p < p∗, and convex for all p ≥ p∗.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "Note that, the agent is optimistic when p∗ = 1, and the agent is pessimistic when p∗ = 0 [14].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, if the agent is optimistic, since the weight function is always bounded within a unit square, w(p) is concaveincreasing for all p ∈ [0, 1].",
      "startOffset" : 145,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "Similarly, in the case of pessimistic agents, the weight function is convexincreasing for all p ∈ [0, 1].",
      "startOffset" : 98,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "For more details about why the weight function has these properties in practice, the readers may refer to [16], [17] and references therein.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "For more details about why the weight function has these properties in practice, the readers may refer to [16], [17] and references therein.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "The reference point c∗ plays a major role in the decision-making process, and can be interpreted as a target/goal to any human decision maker [18].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "w(p) = p, ∀ p ∈ [0, 1] and α > 0, (22a)",
      "startOffset" : 16,
      "endOffset" : 22
    } ],
    "year" : 2016,
    "abstractText" : "Detection rules have traditionally been designed for rational agents that minimize the Bayes risk (average decision cost). With the advent of crowd-sensing systems, there is a need to redesign binary hypothesis testing rules for behavioral agents, whose cognitive behavior is not captured by traditional utility functions such as Bayes risk. In this paper, we adopt prospect theory based models for decision makers. We consider special agent models namely optimists and pessimists in this paper, and derive optimal detection rules under different scenarios. Using an illustrative example, we also show how the decision rule of a human agent deviates from the Bayesian decision rule under various behavioral models, considered in this paper.",
    "creator" : "LaTeX with hyperref package"
  }
}