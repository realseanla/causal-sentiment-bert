{
  "name" : "1604.02080.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes",
    "authors" : [ "Jordi Grau-Moya", "Felix Leibfried", "Tim Genewein", "Daniel A. Braun" ],
    "emails" : [ "jordi.grau@tuebingen.mpg.de," ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: bounded rationality, model uncertainty, robustness, planning, Markov Decision Processes"
    }, {
      "heading" : "1 Introduction",
      "text" : "The problem of planning in Markov Decision Processes was famously addressed by Bellman who developed the eponymous principle in 1957 [1]. Since then numerous variants of this principle have flourished in the literature. Here we are particularly interested in a generalization of the Bellman principle that takes information-theoretic constraints into account. In the recent past there has been a special interest in the KullbackLeibler divergence as a constraint to limit deviations of the action policy from a prior. This can be interesting in a number of ways. Todorov [2,3], for example, has transformed the general MDP problem into a restricted problem class without explicit action variables, where control directly changes the dynamics of the environment and control costs are measured by the Kullback-Leibler divergence between controlled and uncontrolled dynamics. This simplification allows mapping the Bellman recursion to a linear algebra problem. This approach can also be be generalized to continuous state spaces leading to path integral control [4,5]. The same equations can also be interpreted in terms of bounded rational decision-making where the decision-maker has limited computational resources that allow only limited deviations from a prior decision strategy ar X iv :1 60 4.\n02 08\n0v 1\n[ cs\n.A I]\n7 A\npr 2\n01 6\n(measured by the Kullback-Leiber divergence in bits) [6]. Such a decision-maker can also be instantiated by a sampling process that has restrictions in the number of samples it can afford [7]. Disregarding the possibility of a sampling-based interpretation, the Kullback-Leibler divergence introduces a control information cost that is interesting in its own right when formalizing the perception action cycle [8].\nWhile the above frameworks have led to interesting computational advances, so far they have neglected the possibility of model misspecification in the MDP setting. Model misspecification or model uncertainty does not refer to the uncertainty arising due to the stochastic nature of the environment (usually called risk-uncertainty in the economic literature), but refers to the uncertainty with respect to the latent variables that specify the MDP. In Bayes-Adaptive MDPs [9], for example, the uncertainty over the latent parameters of the MDP is explicitly represented, such that new information can be incorporated with Bayesian inference. However, Bayes-Adaptive MDPs are not robust with respect to model misspecification and have no performance guarantees when planning with wrong models [10]. Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13]. One way to take model uncertainty into account is to bias an agent’s belief model from a reference Bayesian model towards worst-case scenarios; thus avoiding disastrous outcomes by not visiting states where the transition probabilities are not known. Conversely, the belief model can also be biased towards best-case scenarios as a measure to drive exploration—also referred in the literature as optimism in face of uncertainty [14,15].\nWhen comparing the literature on information-theoretic control and model uncertainty, it is interesting to see that some notions of model uncertainty follow exactly the same mathematical principles as the principles of relative entropy control [3]. In this paper we therefore formulate a unified and combined optimization problem for MDP planning that takes both, model uncertainty and bounded rationality into account. This new optimization problem can be solved by a generalized value iteration algorithm. We provide a theoretical analysis of its convergence properties and simulations in a grid world."
    }, {
      "heading" : "2 Background and Notation",
      "text" : "In the MDP setting the agent at time t interacts with the environment by taking action at ∈ A while in state st ∈ S . Then the environment updates the state of the agent to st+1 ∈ S according to the transition probabilities T (st+1|at, st). After each transition the agent receives a reward Rst+1st,at ∈ R that is bounded. For our purposes we will consider A and S to be finite. The aim of the agent is to choose its policy π(a|s) in order to maximize the total discounted expected reward or value function for any s ∈ S\nV ∗(s) = max π lim T→∞ E [ T−1∑ t=0 γtRst+1st,at ]\nwith discount factor 0 ≤ γ < 1. The expectation is over all possible trajectories ξ = s0, a0, s1 . . . of state and action pairs distributed according to p(ξ) = ∏T−1 t=0 π(at|st) T (st+1|at, st). It can be shown that the optimal value function satisfies the following\nrecursion V ∗(s) = max\nπ ∑ a,s′ π(a|s)T (s′|a, s) [ Rs ′ s,a + γV ∗(s′) ] . (1)\nAt this point there are two important implicit assumptions. The first is that the policy π can be chosen arbitrarily without any constraints which, for example, might not be true for a bounded rational agent with limited information-processing capabilities. The second is that the agent needs to know the transition-model T (s′|a, s), but this model is in practice unknown or even misspecified with respect to the environment’s true transition-probabilities, specially at initial stages of learning. In the following, we explain how to incorporate both bounded rationality and model uncertainty into agents."
    }, {
      "heading" : "2.1 Information-Theoretic Constraints for Acting",
      "text" : "Consider a one-step decision-making problem where the agent is in state s and has to choose a single action a from the set A to maximize the reward Rs′s,a, where s′ is the next the state. A perfectly rational agent selects the optimal action a∗(s) = argmaxa ∑ s′ T (s\n′|a, s)Rs′s,a. However, a bounded rational agent has only limited resources to find the maximum of the function ∑ s′ T (s\n′|a, s)Rs′s,a. One way to model such an agent is to assume that the agent has a prior choice strategy ρ(a|s) in state s before a deliberation process sets in that refines the choice strategy to a posterior distribution π(a|s) that reflects the strategy after deliberation. Intuitively, because the deliberation resources are limited, the agent can only afford to deviate from the prior strategy by a certain amount of information bits. This can be quantified by the relative entropy DKL(π||ρ) = ∑ a π(a|s) log π(a|s) ρ(a|s) that measures the average information cost of the policy π(a|s) using the source distribution ρ(a|s). For a bounded rational agent this relative entropy is bounded by some upper limit K. Thus, a bounded rational agent has to solve a constrained optimization problem that can be written as\nmax π ∑ a π(a|s) ∑ s′ T (s′|a, s)Rs ′ s,a s.t. DKL(π||ρ) ≤ K\nThis problem can be rewritten as an unconstrained optimization problem\nF ∗(s) = max π ∑ a π(a|s) ∑ s′ T (s′|a, s)Rs ′ s,a − 1 α DKL(π||ρ) (2)\n= 1 α log ∑ a ρ(a|s)eα ∑ s′ T (s ′|a,s)Rs ′ s,a . (3)\nwhere F ∗ is a free energy that quantifies the value of the policy π by trading off the average reward against the information cost. The optimal strategy can be expressed analytically in closed-form as\nπ∗(a|s) = ρ(a|s)e α ∑ s′ T (s ′|a,s)Rs ′ s,a\nZα(s)\nwith partition sum Zα(s) = ∑ a ρ(a|s) exp ( α ∑ s′ T (s ′|a, s)Rs′s,a )\n. Therefore, the maximum operator in (2) can be eliminated and the free energy can be rewritten as in (3). The Lagrange multiplier α quantifies the boundedness of the agent. By setting α→ ∞ we recover a perfectly rational agent with optimal policy π∗(a|s) = δ(a − a∗(s)). For α = 0 the agent has no computational resources and the agent’s optimal policy is to act according to the prior π∗(a|s) = ρ(a|s). Intermediate values of α lead to a spectrum of bounded rational agents."
    }, {
      "heading" : "2.2 Information-Theoretic Constraints for Model Uncertainty",
      "text" : "In the following we assume that the agent has a model of the environment Tθ(s′|a, s) that depends on some latent variables θ ∈ Θ. In the MDP setting, the agent holds a belief µ(θ|a, s) regarding the environmental dynamics where θ is a unit vector of transition probabilities into all possible states s′. While interacting with the environment the agent can incorporate new data by forming the Bayesian posterior µ(θ|a, s,D), where D is the observed data. When the agent has observed an infinite amount of data (and assuming θ∗(a, s) ∈ Θ) the belief will converge to the delta distribution µ(θ|s, a,D) = δ(θ−θ∗(a, s)) and the agent will act optimally according to the true transition probabilities, exactly as in ordinary optimal choice strategies with known models. When acting under a limited amount of data the agent cannot determine the value of an action a with the true transition model according to ∑ s′ T (s\n′|a, s)Rs′s,a, but it can only determine an expected value according to its beliefs ∫ θ µ(θ|a, s) ∑ s′ Tθ(s\n′|a, s)Rs′s,a. The Bayesian model µ can be subject to model misspecification (e.g. by having a wrong likelihood or a bad prior) and thus the agent might want to allow deviations from its model towards best-case (optimistic agent) or worst-case (pessimistic agent) scenarios up to a certain extent, in order to act more robustly or to enhance its performance in a friendly environment [16]. Such deviations can be measured by the relative entropy DKL(ψ|µ) between the Bayesian posterior µ and a new biased model ψ. Effectively, this allows for mathematically formalizing model uncertainty, by not only considering the specified model but all models within a neighborhood of the specified model that deviate no more than a restricted number of bits. Then, the effective expected value of an action a while having limited trust in the Bayesian posterior µ can be determined for the case of optimistic deviations as\nF ∗(a, s) = max ψ ∫ θ ψ(θ|a, s) ∑ s′ Tθ(s ′|a, s)Rs ′ s,a − 1 β DKL(ψ||µ) (4)\nfor β > 0, and for the case of pessimistic deviations as\nF ∗(a, s) = min ψ ∫ θ ψ(θ|a, s) ∑ s′ Tθ(s ′|a, s)Rs ′ s,a − 1 β DKL(ψ||µ) (5)\nfor β < 0. Conveniently, both equations can be expressed as a single equation\nF ∗(a, s) = 1\nβ logZβ(a, s)\nwith β ∈ R and Zβ(s, a) = ∫ θ µ(θ|a, s) exp ( β ∑ s′ Tθ(s ′|a, s)Rs′s,a )\nwhen inserting the optimal biased belief\nψ∗(θ|a, s) = 1 Zβ(a, s) µ(θ|a, s) exp ( β ∑ s′ Tθ(s ′|a, s)Rs ′ s,a )\ninto either equation (4) or (5). By adopting this formulation we can model any degree of trust in the belief µ allowing deviation towards worst-case or best-case with −∞ ≤ β ≤ ∞. For the case of β → −∞ we recover an infinitely pessimistic agent that considers only worst-case scenarios, for β → ∞ an agent that is infinitely optimistic and for β → 0 the Bayesian agent that fully trusts its model."
    }, {
      "heading" : "3 Model Uncertainty and Bounded Rationality in MDPs",
      "text" : "In this section, we consider a bounded rational agent with model uncertainty in the infinite horizon setting of an MDP. In this case the agent must take into account all future rewards and information costs, thereby optimizing the following free energy objective\nF ∗(s) = max π ext ψ lim T→∞ E T−1∑ t=0 γt ( Rst+1st,at− 1 β log ψ(θt|at, st) µ(θt|at, st) − 1 α log π(at|st) ρ(at|st) ) (6)\nwhere the extremum operator ext can be either max for β > 0 or min for β < 0, 0 < γ < 1 is the discount factor and the expectation E is over all trajectories ξ = s0, a0, θ0, s1, a1, . . . aT−1, θT−1, sT with distribution p(ξ) = ∏T−1 t=0 π(at|st)ψ(θt|at, st) Tθt(st+1|at, st). Importantly, this free energy objective satisfies a recursive relation and thereby generalizes Bellman’s optimality principle to the case of model uncertainty and bounded rationality. In particular, equation (6) fulfills the recursion\nF ∗(s) = max π ext ψ Eπ(a|s) [ − 1 α log π(a|s) ρ(a|s) +\nEψ(θ|a,s) [ − 1 β log ψ(θ|a, s) µ(θ|a, s) +\nETθ(s′|a,s) [ Rs ′ s,a + γF ∗(s′) ]]] . (7)\nApplying variational calculus and following the same rationale as in the previous sections [6], the extremum operators can be eliminated and equation (7) can be reexpressed as\nF ∗(s) = 1\nα logEρ(a|s)\n[ Eµ(θ|a,s) [ exp ( βETθ(s′|a,s) [ Rs ′ s,a + γF ∗(s′) ])]α β ] (8)\nbecause\nF ∗(s) = max π\nEπ(a|s) [ 1\nβ logZβ(a, s)−\n1 α log π(a|s) ρ(a|s)\n] (9)\n= 1\nα logEρ(a|s)\n[ exp ( α\nβ logZβ(a, s)\n)] , (10)\nwhere\nZβ(a, s) = ext ψ\nEψ(θ|a,s) [ ETθ(s′|a,s) [ Rs ′ s,a + γF ∗(s′) ] − 1 β log ψ(θ|a, s) µ(θ|a, s) ] (11)\n= Eµ(θ|a,s) exp ( βETθ(s′|a,s) [ Rs ′ s,a + γF ∗(s′) ]) with the optimizing arguments\nψ∗(θ|a, s) = 1 Zβ(a, s)\nµ(θ|a, s) exp ( βETθ(s′|a,s) [ Rs ′ s,a + γF (s ′) ])\nπ∗(a|s) = 1 Zα(s)\nρ(a|s) exp ( α\nβ logZβ(a, s)\n) (12)\nand partition sum\nZα(s) = Eρ(a|s) [ exp ( α\nβ logZβ(a, s)\n)] .\nWith this free energy we can model a range of different agents for different α and β. For example, by setting α→∞ and β → 0 we can recover a Bayesian MDP planner and by setting α → ∞ and β → −∞ we recover a robust planner. Additionally, for α→∞ and when µ(θ|a, s) = δ(θ− θ∗(a, s)) we recover an agent with standard value function with known state transition model from equation (1)."
    }, {
      "heading" : "3.1 Free Energy Iteration Algorithm",
      "text" : "Solving the self-consistency equation (8) can be achieved by a generalized version of value iteration. Accordingly, the optimal solution can be obtained by initializing the free energy at some arbitrary value F and applying a value iteration scheme Bi+1F = BBiF where we define the operator\nBF (s) = max π ext ψ Eπ(a|s) [ − 1 α log π(a|s) ρ(a|s) +\nEψ(θ|a,s) [ − 1 β log ψ(θ|a, s) µ(θ|a, s) +\nETθ(s′|a,s) [ Rs ′ s,a + γF (s ′) ]]] (13)\nwith B1F = BF , which can be simplified to\nBF (s) = 1\nα logEρ(a|s)\n[ Eµ(θ|a,s) [ exp ( βETθ(s′|a,s) [ Rs ′ s,a + γF (s ′) ])]α β ] In Algorithm (1) we show the pseudo-code of this generalized value iteration scheme. Given state-dependent prior policies ρ(a|s) and the Bayesian posterior beliefs µ(θ|a, s) and the values of α and β, the algorithm outputs the equilibrium distributions for the action probabilities π(a|s), the biased beliefs ψ(θ|a, s) and estimates of the free energy value function F ∗(s). The iteration is run until a convergence criterion is met. The convergence proof is shown in the next section.\nAlgorithm 1: Iterative algorithm solving the self-consistency equation (8) Input: ρ(a|s), µ(θ|a, s), α, β Initialize: F ← 0, Fold ← 0 while not converged do\nforall the s ∈ S do\nF (s)← 1 α logEρ(a|s) [ Eµ(θ|a,s) [ exp ( βETθ(s′|a,s) [ Rs ′ s,a + γFold(s ′) ])]α β ] end Fold ← F\nend\nπ(a|s)← 1 Zα(s) ρ(a|s) exp ( α β logZβ(a, s) ) ψ(θ|a, s)← 1\nZβ(a,s) µ(θ|a, s) exp ( βETθ(s′|a,s) [ Rs ′ s,a + γF (s ′) ])\nreturn π(a|s), ψ(θ|a, s), F (s)"
    }, {
      "heading" : "4 Convergence",
      "text" : "Here, we show that the value iteration scheme described through Algorithm 1 converges to a unique fixed point satisfying Equation (8). To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].\nTheorem 1. Assuming a bounded reward functionRs ′\ns,a, the optimal free-energy vector F ∗(s) is a unique fixed point of Bellman’s equation F ∗ = BF ∗, where the mapping B : R|S| → R|S| is defined as in equation (13)\nProof. Theorem 1 is proven through Proposition 1 and 2 in the following.\nProposition 1. The mapping Tπ,ψ : R|S| → R|S|\nTπ,ψF (s) = Eπ(a|s) [ − 1 α log π(a|s) ρ(a|s) +\nEψ(θ|a,s) [ − 1 β log ψ(θ|a, s) µ(θ|a, s) +\nETθ(s′|a,s) [ Rs ′ s,a + γF (s ′) ]]] . (14)\nconverges to a unique solution for every policy-belief-pair (π, ψ) independent of the initial free-energy vector F (s).\nProof. By introducing the matrix Pπ,ψ(s, s′) and the vector gπ,ψ(s) as\nPπ,ψ(s, s ′) := Eπ(a|s) [ Eψ(θ|a,s) [Tθ(s′|a, s)] ] ,\ngπ,ψ(s) := Eπ(a|s) [ Eψ(θ|a,s) [ ETθ(s′|a,s) [ Rs ′ s,a ] − 1 β log ψ(θ|a, s) µ(θ|a, s) ] − 1 α log π(a|s) ρ(a|s) ] ,\nEquation (14) may be expressed in compact form: Tπ,ψF = gπ,ψ + γPπ,ψF . By applying the mapping Tπ,ψ an infinite number of times on an initial free-energy vector F , the free-energy vector Fπ,ψ of the policy-belief-pair (π, ψ) is obtained:\nFπ,ψ := lim i→∞ T iπ,ψF = lim i→∞ i−1∑ t=0 γtP tπ,ψgπ,ψ + lim i→∞\nγiP iπ,ψF︸ ︷︷ ︸ →0 ,\nwhich does no longer depend on the initial F . It is straightforward to show that the quantity Fπ,ψ is a fixed point of the operator Tπ,ψ:\nTπ,ψFπ,ψ = gπ,ψ + γPπ,ψ lim i→∞ i−1∑ t=0 γtP tπ,ψgπ,ψ\n= γ0P 0π,ψgπ,ψ + lim i→∞ i∑ t=1 γtP tπ,ψgπ,ψ\n= lim i→∞ i−1∑ t=0 γtP tπ,ψgπ,ψ + lim i→∞\nγiP iπ,ψgπ,ψ︸ ︷︷ ︸ →0 = Fπ,ψ.\nFurthermore, Fπ,ψ is unique. Assume for this purpose an arbitrary fixed point F ′ such that Tπ,ψF ′ = F ′, then F ′ = limi→∞ T iπ,ψF ′ = Fπ,ψ.\nProposition 2. The optimal free-energy vector F ∗ = maxπ extψ Fπ,ψ is a unique fixed point of Bellman’s equation F ∗ = BF ∗.\nProof. The proof consists of two parts where we assume ext = max in the first part and ext = min in the second part respectively. Let ext = max and F ∗ = Fπ∗,ψ∗ , where (π∗, ψ∗) denotes the optimal policy-belief-pair. Then\nF ∗ = Tπ∗,ψ∗F ∗ ≤ max\nπ max ψ Tπ,ψF ∗︸ ︷︷ ︸\n=BF∗\n=: Tπ′,ψ′F ∗ Induction≤ Fπ′,ψ′ ,\nwhere the last inequality can be straightforwardly proven by induction and exploiting the fact that Pπ,ψ(s, s′) ∈ [0; 1]. But by definition F ∗ = maxπmaxψ Fπ,ψ ≥ Fπ′,ψ′ , hence F ∗ = Fπ′,ψ′ and therefore F ∗ = BF ∗. Furthermore, F ∗ is unique. Assume for this purpose an arbitrary fixed point F ′ = Fπ′,ψ′ such that F ′ = BF ′ with the corresponding policy-belief-pair (π′, ψ′). Then\nF ∗ = Tπ∗,ψ∗F ∗ ≥ Tπ′,ψ′F ∗ Induction ≥ Fπ′,ψ′ = F ′,\nand similarly F ′ ≥ F ∗, hence F ′ = F ∗. Let ext = min and F ∗ = Fπ∗,ψ∗ . By taking a closer look at Equation (13), it can be seen that the optimization over ψ does not depend on π. Then\nF ∗ = Tπ∗,ψ∗F ∗ ≥ min\nψ Tπ∗,ψF\n∗ =: Tπ∗,ψ′F ∗ Induction≥ Fπ∗,ψ′ .\nBut by definition F ∗ = minψ Fπ∗,ψ ≤ Fπ∗,ψ′ , hence F ∗ = Fπ∗,ψ′ . Therefore it holds that BF ∗ = maxπminψ Tπ,ψF ∗ = maxπ Tπ,ψ∗F ∗ and similar to the first part of the proof we obtain\nF ∗ = Tπ∗,ψ∗F ∗ ≤ max\nπ Tπ,ψ∗F ∗︸ ︷︷ ︸ =BF∗\n=: Tπ′,ψ∗F ∗ Induction≤ Fπ′,ψ∗.\nBut by definition F ∗ = maxπ Fπ,ψ∗ ≥ Fπ′,ψ∗, hence F ∗ = Fπ′,ψ∗ and therefore F ∗ = BF ∗. Furthermore, Fπ∗,ψ∗ is unique. Assume for this purpose an arbitrary fixed point F ′ = Fπ′,ψ′ such that F ′ = BF ′. Then\nF ′ = Tπ′,ψ′F ′ ≤ Tπ′,ψ∗F ′ Induction ≤ Fπ′,ψ∗ Induction ≤ Tπ′,ψ∗F ∗ ≤ Tπ∗,ψ∗F ∗ = F ∗,\nand similarly F ∗ ≤ F ′, hence F ∗ = F ′.\nTheorem 2. Let be a positive number satisfying < η1−γ where γ ∈ (0; 1) is the discount factor and where u and l are the bounds of the reward function Rs ′\ns,a such that l ≤ Rs′s,a ≤ u and η = max{|u|, |l|}. Suppose that the value iteration scheme from Algorithm 1 is run for i = dlogγ (1−γ) η e iterations with an initial free-energy vector F (s) = 0 for all s. Then, it holds that maxs |F ∗(s)−BiF (s)| ≤ , where F ∗ refers to the unique fixed point from Theorem 1.\nProof. We start the proof by showing that the L∞-norm of the difference vector between the optimal free-energy F ∗ andBiF exponentially decreases with the number of iterations i:\nmax s ∣∣F ∗(s)−BiF (s)∣∣ =: ∣∣F ∗(s∗)−BiF (s∗)∣∣ Eq. (9) = ∣∣∣∣maxπ Eπ(a|s∗) [ 1 β logZβ(a, s ∗)− 1 α log π(a|s∗) ρ(a|s∗)\n] −max\nπ Eπ(a|s∗)\n[ 1\nβ logZiβ(a, s ∗)− 1 α log π(a|s∗) ρ(a|s∗) ]∣∣∣∣ ≤ max\nπ ∣∣∣∣Eπ(a|s∗)[ 1β logZβ(a, s∗)− 1β logZiβ(a, s∗) ]∣∣∣∣\n≤ max a ∣∣∣∣ 1β logZβ(a, s∗)− 1β logZiβ(a, s∗) ∣∣∣∣\n=: ∣∣∣∣ 1β logZβ(a∗, s∗)− 1β logZiβ(a∗, s∗) ∣∣∣∣\nEq. (11) = ∣∣∣∣extψ Eψ(θ|a∗,s∗) [ ETθ(s′|a∗,s∗) [ Rs ′ s,a + γF ∗(s′) ] − 1 β log ψ(θ|a∗, s∗) µ(θ|a∗, s∗) ] − ext\nψ Eψ(θ|a∗,s∗)\n[ ETθ(s′|a∗,s∗) [ Rs ′ s,a + γB i−1F (s′) ] − 1 β log ψ(θ|a∗, s∗) µ(θ|a∗, s∗) ]∣∣∣∣ ≤ max\nψ ∣∣∣∣Eψ(θ|a∗,s∗)[ETθ(s′|a∗,s∗)[γF ∗(s′)− γBi−1F (s′)]]∣∣∣∣ ≤ γmax\ns ∣∣F ∗(s)−Bi−1F (s)∣∣ Recur.≤ γimax s |F ∗(s)− F (s)| ≤ γi η 1− γ ,\nwhere we exploit the fact that |extx f(x)− extx g(x)| ≤ maxx |f(x)− g(x)| and that the free-energy is bounded through the reward bounds l and u with η = max{|u|, |l|}. For a convergence criterion > 0 such that ≥ γi η1−γ , it then holds that i ≥ logγ (1−γ) η presupposing that < η 1−γ ."
    }, {
      "heading" : "5 Experiments: Grid World",
      "text" : "This section illustrates the proposed value iteration scheme with an intuitive example where an agent has to navigate through a grid-world. The agent starts at position S ∈ S with the objective to reach the goal state G ∈ S and can choose one out of maximally four possible actions a ∈ {↑,→, ↓,←} in each time-step. Along the way, the agent can encounter regular tiles (actions move the agent deterministically one step in the desired direction), walls that are represented as gray tiles (actions that move the agent towards the wall are not possible), holes that are represented as black tiles (moving into the hole causes a negative reward) and chance tiles that are illustrated as white tiles with a question mark (the transition probabilities of the chance tiles are unknown to the agent). Reaching the goal G yields a reward R = +1 whereas stepping into a hole results in a negative reward R = −1. In both cases the agent is subsequently teleported back\nto the starting position S. Transitions to regular tiles have a small negative reward of R = −0.01. When stepping onto a chance tile, the agent is pushed stochastically to an adjacent tile giving a reward as mentioned above. The true state-transition probabilities of the chance tiles are not known by the agent, but the agent holds the Bayesian belief\nµ(θs,a|a, s) = Dirichlet ( Φ s′1 s,a, . . . , Φ s′N(s) s,a ) = N(s)∏ i=1 (θ s′i s,a) Φ s′i s,a−1\nwhere transition model is denoted as Tθs,a(s ′|s, a) = θs′s,a and θs,a = ( θ s′1 s,a . . . θ s′N(s) s,a ) and N(s) is the number of possible actions in state s. The data is incorporated into the\nmodel as a count vector ( Φ s′1 s,a, . . . , Φ s′N(s) s,a ) where Φs ′\ns,a represents the number of times that the transition (s, a, s′) has occurred. The prior ρ(a|s) for the actions at every state is set to be uniform. An important aspect of the model is that in the case of unlimited observational data, the agent will plan with the correct transition probabilities.\nWe conducted two experiments with discount factor γ = 0.9 and uniform priors ρ(a|s) for the action variables. In the first experiment, we explore and illustrate the agent’s planning behavior under different degrees of computational limitations (by varying α) and under different model uncertainty attitudes (by varying β) with fixed uniform beliefs µ(θ|a, s). In the second experiment, the agent is allowed to update its beliefs µ(θ|a, s) and use the updated model to re-plan its strategy."
    }, {
      "heading" : "5.1 The Role of the Parameters α and β on Planning",
      "text" : "Figure 1 shows the solution to the variational free energy problem that is obtained by iteration until convergence according to Algorithm 1 under different values of α and β. In particular, the first row shows the free energy function F ∗(s) (Eq. (8)). The second, third and fourth row show heat maps of the position of an agent that follows the optimal policy (Eq. (12)) according to the agent’s biased beliefs (plan) and to the actual transition probabilities in a friendly and unfriendly environment, respectively. In chance tiles, the most likely transitions in these two environments are indicated by arrows where the agent is teleported with a probability of 0.999 into the tile indicated by the arrow and with a probability of 0.001 to a random other adjacent tile.\nIn the first column of Fig. 1 it can be seen that a stochastic agent (α = 3.0) with high model uncertainty and optimistic attitude (β = 400) has a strong preference for the broad corridor in the bottom by assuming favorable transitions for the unknown chance tiles. This way the agent also avoids the narrow corridors that are unsafe due to the stochasticity of the low-α policy. In the second column of Fig. 1 with low α = 3 and high model uncertainty with pessimistic attitude β = −400, the agent strongly prefers the upper broad corridor because unfavorable transitions are assumed for the chance tiles. The third column of Fig. 1 shows a very pessimistic agent (β = −400) with high precision (α = 11) that allow the agent to safely choose the shortest distance by selecting the upper narrow corridor without risking any tiles with unknown transitions. The fourth column of Fig. 1 shows a very optimistic agent (β = 400) with high precision. In this case the agent chooses the shortest distance by selecting the bottom narrow corridor that includes two chance tiles with unknown transition."
    }, {
      "heading" : "5.2 Updating the Bayesian Posterior µ with Observations from the Environment",
      "text" : "Similar to model identification adaptive controllers that perform system identification while the system is running [20], we can use the proposed planning algorithm also in a reinforcement learning setup by updating the Bayesian beliefs about the MDP while executing always the first action and replanning in the next time step. During the learning phase, the exploration is governed by both factors α and β, but each factor has a different influence. In particular, lower α-values will cause more exploration due to the inherent stochasticity in the agent’s action selection, similar to an -greedy policy. If α is kept fixed through time, this will of course also imply a “suboptimal” (i.e. bounded optimal) policy in the long run. In contrast, the parameter β governs exploration of states with unknown transition-probabilities more directly and will not have an impact on the agent’s performance in the limit, where sufficient data has eliminated model uncertainty. We illustrate this with simulations in a grid-world environment where the agent is allowed to update its beliefs µ(θ|a, s) over the state-transitions every time it enters a chance tile and receives observation data acquired through interaction with the environment—compare left panels in Figure 2. In each step, the agent can then use the updated belief-models for planning the next action.\nFigure 2 (right panels) shows the number of data points acquired (each time a chance tile is visited) and the average reward depending on the number of steps that the agent has interacted with the environment. The panels show several different cases: while keeping α = 12.0 fixed we test β = (0.2, 5.0, 20.0) and while keeping β = 0.2 fixed we test α = (5.0, 8.0, 12.0). It can be seen that lower α leads to better exploration, but it can also lead to lower performance in the long run—see for example rightmost bottom panel. In contrast, optimistic β values can also induce high levels of exploration with the added advantage that in the limit no performance detriment is introduced. However, high β values can in general also lead to a detrimental persistence with bad policies, as can be seen for example in the superiority of the low-β agent at the very beginning of the learning process."
    }, {
      "heading" : "6 Discussion and Conclusions",
      "text" : "In this paper we are bringing two strands of research together, namely research on information-theoretic principles of control and decision-making and robustness principles for planning under model uncertainty. We have devised a unified recursion principle that extends previous generalizations of Bellman’s optimality equation and we have shown how to solve this recursion with an iterative scheme that is guaranteed to converge to a unique optimum. In simulations we could demonstrate how such a combination of information-theoretic policy and belief constraints that reflect model uncertainty can be beneficial for agents that act in partially unknown environments.\nMost of the research on robust MDPs does not consider information-processing constraints on the policy, but only considers the uncertainty in the transition probabilities by specifying a set of permissible models such that worst-case scenarios can be computed in order to obtain a robust policy [11,12]. Recent extensions of these approaches include more general assumptions regarding the set properties of the permissible models and assumptions regarding the data generation process [13]. Our approach falls inside this class of robustness methods that use a restricted set of permissible models, because we extremize the biased belief ψ(θ|a, s) under the constraint that it has to be within some information bounds measured by the Kullback-Leibler divergence from a reference Bayesian posterior. Contrary to these previous methods, our approach additionally considers robustness arising from the stochasticity in the policy.\nInformation-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty. In these studies a free energy value recursion is derived when restricting the class of policies through the Kullback-Leibler divergence and when disregarding separate information-processing constraints on observations. However, a small number of studies has considered information-processing constraints both for actions and observations. For example, Polani and Tishby [8] and Ortega and Braun [6] combine both kinds of information costs. The first cost formalizes an information-processing cost in the policy and the second cost constrains uncertainty arising from the state transitions directly (but crucially not the uncertainty in the latent variables). In both informationprocessing constraints the cost is determined as a Kullback-Leibler divergence with respect to a reference distribution. Specifically, the reference distribution in [8] is given\nby the marginal distributions (which is equivalent to a rate distortion problem) and in [6] is given by fixed priors. The Kullback-Leibler divergence costs for the observations in these cases essentially correspond to a risk-sensitive objective. While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold. First, it allows combining information-processing constraints on the policy with model uncertainty (as formalized by a latent variable). Second, it provides a natural setup to study learning.\nThe algorithm presented here and Bayesian models in general [9] are computationally expensive as they have to compute possibly high-dimensional integrals depending on the number of allowed transitions for action-state pairs. However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28]. An interesting future direction to extend our methodology would therefore be to develop a sampling-based version of Algorithm 1 to increase the range of applicability and scalability [29]. Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].\nAcknowledgments This study was supported by the DFG, Emmy Noether grant BR4164/11. The code was developed on top of the RLPy library [33]."
    } ],
    "references" : [ {
      "title" : "Dynamic Programming",
      "author" : [ "Richard Bellman" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1957
    }, {
      "title" : "Linearly-solvable markov decision problems",
      "author" : [ "Emanuel Todorov" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Efficient computation of optimal actions",
      "author" : [ "Emanuel Todorov" ],
      "venue" : "Proceedings of the national academy of sciences,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Path integral control and bounded rationality",
      "author" : [ "Daniel A Braun", "Pedro A Ortega", "Evangelos Theodorou", "Stefan Schaal" ],
      "venue" : "In Adaptive Dynamic Programming And Reinforcement Learning (ADPRL),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Risk sensitive path integral control",
      "author" : [ "Bart van den Broek", "Wim Wiegerinck", "Hilbert J. Kappen" ],
      "venue" : "In UAI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Thermodynamics as a theory of decision-making with information-processing costs",
      "author" : [ "Pedro A Ortega", "Daniel A Braun" ],
      "venue" : "In Proc. R. Soc. A,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Generalized thompson sampling for sequential decision-making and causal inference",
      "author" : [ "Pedro A Ortega", "Daniel A Braun" ],
      "venue" : "Complex Adaptive Systems Modeling,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Information theory of decisions and actions",
      "author" : [ "Naftali Tishby", "Daniel Polani" ],
      "venue" : "In Perceptionaction cycle,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes",
      "author" : [ "Michael O’Gordon Duff" ],
      "venue" : "PhD thesis, University of Massachusetts Amherst,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "Bias and variance approximation in value function estimates",
      "author" : [ "Shie Mannor", "Duncan Simester", "Peng Sun", "John N Tsitsiklis" ],
      "venue" : "Management Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "Robust control of markov decision processes with uncertain transition matrices",
      "author" : [ "Arnab Nilim", "Laurent El Ghaoui" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Robust dynamic programming",
      "author" : [ "Garud N Iyengar" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "Robust markov decision processes",
      "author" : [ "Wolfram Wiesemann", "Daniel Kuhn", "Berç Rustem" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "The many faces of optimism: a unifying approach",
      "author" : [ "István Szita", "András Lőrincz" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Model-based reinforcement learning with nearly tight exploration complexity bounds",
      "author" : [ "István Szita", "Csaba Szepesvári" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Trading value and information in mdps. In Decision Making with Imperfect Decision Makers",
      "author" : [ "Jonathan Rubin", "Ohad Shamir", "Naftali Tishby" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "and JN Tsitsiklis",
      "author" : [ "DP Bertseka" ],
      "venue" : "Neuro-dynamic programming.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Reinforcement learning in finite mdps: Pac analysis",
      "author" : [ "Alexander L Strehl", "Lihong Li", "Michael L Littman" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Adaptive control",
      "author" : [ "Karl J Åström", "Björn Wittenmark" ],
      "venue" : "Courier Corporation,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Linear theory for control of nonlinear stochastic systems",
      "author" : [ "Hilbert J Kappen" ],
      "venue" : "Physical review letters,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2005
    }, {
      "title" : "Fox D Poole, et al",
      "author" : [ "J Peters", "K Mülling", "Y Altun" ],
      "venue" : "Relative entropy policy search. In TwentyFourth National Conference on Artificial Intelligence (AAAI-10), pages 1607–1612. AAAI Press,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Risk-sensitive reinforcement learning",
      "author" : [ "Yun Shen", "Michael J Tobia", "Tobias Sommer", "Klaus Obermayer" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Robustness and risk-sensitivity in markov decision processes",
      "author" : [ "Takayuki Osogami" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Risk-sensitive and robust decision-making: a cvar optimization approach",
      "author" : [ "Yinlam Chow", "Aviv Tamar", "Shie Mannor", "Marco Pavone" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "A bayesian approach for learning and planning in partially observable markov decision processes",
      "author" : [ "Stéphane Ross", "Joelle Pineau", "Brahim Chaib-draa", "Pierre Kreitmann" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Efficient bayes-adaptive reinforcement learning using sample-based search",
      "author" : [ "Arthur Guez", "David Silver", "Peter Dayan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Scalable and efficient bayes-adaptive reinforcement learning based on monte-carlo tree search",
      "author" : [ "Arthur Guez", "David Silver", "Peter Dayan" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Monte carlo methods for exact & efficient solution of the generalized optimality equations",
      "author" : [ "Pedro A Ortega", "Daniel A Braun", "Naftali Tishby" ],
      "venue" : "In Robotics and Automation (ICRA),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "G-learning: Taming the noise in reinforcement learning via soft updates",
      "author" : [ "Roy Fox", "Ari Pakman", "Naftali Tishby" ],
      "venue" : "arXiv preprint arXiv:1512.08562,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "A minimum relative entropy principle for learning and acting",
      "author" : [ "Pedro A Ortega", "Daniel A Braun" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2010
    }, {
      "title" : "A bayesian rule for adaptive control based on causal interventions",
      "author" : [ "Pedro A Ortega", "Daniel A Braun" ],
      "venue" : "In 3d Conference on Artificial General Intelligence",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2010
    }, {
      "title" : "Rlpy: A value-function-based reinforcement learning framework for education and research",
      "author" : [ "Alborz Geramifard", "Christoph Dann", "Robert H Klein", "William Dabney", "Jonathan P How" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The problem of planning in Markov Decision Processes was famously addressed by Bellman who developed the eponymous principle in 1957 [1].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "Todorov [2,3], for example, has transformed the general MDP problem into a restricted problem class without explicit action variables, where control directly changes the dynamics of the environment and control costs are measured by the Kullback-Leibler divergence between controlled and uncontrolled dynamics.",
      "startOffset" : 8,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : "Todorov [2,3], for example, has transformed the general MDP problem into a restricted problem class without explicit action variables, where control directly changes the dynamics of the environment and control costs are measured by the Kullback-Leibler divergence between controlled and uncontrolled dynamics.",
      "startOffset" : 8,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "This approach can also be be generalized to continuous state spaces leading to path integral control [4,5].",
      "startOffset" : 101,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "This approach can also be be generalized to continuous state spaces leading to path integral control [4,5].",
      "startOffset" : 101,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "(measured by the Kullback-Leiber divergence in bits) [6].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "Such a decision-maker can also be instantiated by a sampling process that has restrictions in the number of samples it can afford [7].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "Disregarding the possibility of a sampling-based interpretation, the Kullback-Leibler divergence introduces a control information cost that is interesting in its own right when formalizing the perception action cycle [8].",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 8,
      "context" : "In Bayes-Adaptive MDPs [9], for example, the uncertainty over the latent parameters of the MDP is explicitly represented, such that new information can be incorporated with Bayesian inference.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "However, Bayes-Adaptive MDPs are not robust with respect to model misspecification and have no performance guarantees when planning with wrong models [10].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 10,
      "context" : "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].",
      "startOffset" : 83,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].",
      "startOffset" : 83,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].",
      "startOffset" : 83,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "Conversely, the belief model can also be biased towards best-case scenarios as a measure to drive exploration—also referred in the literature as optimism in face of uncertainty [14,15].",
      "startOffset" : 177,
      "endOffset" : 184
    }, {
      "referenceID" : 14,
      "context" : "Conversely, the belief model can also be biased towards best-case scenarios as a measure to drive exploration—also referred in the literature as optimism in face of uncertainty [14,15].",
      "startOffset" : 177,
      "endOffset" : 184
    }, {
      "referenceID" : 2,
      "context" : "When comparing the literature on information-theoretic control and model uncertainty, it is interesting to see that some notions of model uncertainty follow exactly the same mathematical principles as the principles of relative entropy control [3].",
      "startOffset" : 244,
      "endOffset" : 247
    }, {
      "referenceID" : 5,
      "context" : "Applying variational calculus and following the same rationale as in the previous sections [6], the extremum operators can be eliminated and equation (7) can be reexpressed as",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 18,
      "context" : "2 Updating the Bayesian Posterior μ with Observations from the Environment Similar to model identification adaptive controllers that perform system identification while the system is running [20], we can use the proposed planning algorithm also in a reinforcement learning setup by updating the Bayesian beliefs about the MDP while executing always the first action and replanning in the next time step.",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 10,
      "context" : "Most of the research on robust MDPs does not consider information-processing constraints on the policy, but only considers the uncertainty in the transition probabilities by specifying a set of permissible models such that worst-case scenarios can be computed in order to obtain a robust policy [11,12].",
      "startOffset" : 295,
      "endOffset" : 302
    }, {
      "referenceID" : 11,
      "context" : "Most of the research on robust MDPs does not consider information-processing constraints on the policy, but only considers the uncertainty in the transition probabilities by specifying a set of permissible models such that worst-case scenarios can be computed in order to obtain a robust policy [11,12].",
      "startOffset" : 295,
      "endOffset" : 302
    }, {
      "referenceID" : 12,
      "context" : "Recent extensions of these approaches include more general assumptions regarding the set properties of the permissible models and assumptions regarding the data generation process [13].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 2,
      "context" : "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "For example, Polani and Tishby [8] and Ortega and Braun [6] combine both kinds of information costs.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "For example, Polani and Tishby [8] and Ortega and Braun [6] combine both kinds of information costs.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "Specifically, the reference distribution in [8] is given",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "by the marginal distributions (which is equivalent to a rate distortion problem) and in [6] is given by fixed priors.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 22,
      "context" : "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "The algorithm presented here and Bayesian models in general [9] are computationally expensive as they have to compute possibly high-dimensional integrals depending on the number of allowed transitions for action-state pairs.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].",
      "startOffset" : 112,
      "endOffset" : 122
    }, {
      "referenceID" : 25,
      "context" : "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].",
      "startOffset" : 112,
      "endOffset" : 122
    }, {
      "referenceID" : 26,
      "context" : "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].",
      "startOffset" : 112,
      "endOffset" : 122
    }, {
      "referenceID" : 27,
      "context" : "An interesting future direction to extend our methodology would therefore be to develop a sampling-based version of Algorithm 1 to increase the range of applicability and scalability [29].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 28,
      "context" : "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 29,
      "context" : "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 30,
      "context" : "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 31,
      "context" : "The code was developed on top of the RLPy library [33].",
      "startOffset" : 50,
      "endOffset" : 54
    } ],
    "year" : 2016,
    "abstractText" : "Information-theoretic principles for learning and acting have been proposed to solve particular classes of Markov Decision Problems. Mathematically, such approaches are governed by a variational free energy principle and allow solving MDP planning problems with information-processing constraints expressed in terms of a Kullback-Leibler divergence with respect to a reference distribution. Here we consider a generalization of such MDP planners by taking model uncertainty into account. As model uncertainty can also be formalized as an information-processing constraint, we can derive a unified solution from a single generalized variational principle. We provide a generalized value iteration scheme together with a convergence proof. As limit cases, this generalized scheme includes standard value iteration with a known model, Bayesian MDP planning, and robust planning. We demonstrate the benefits of this approach in a grid world simulation.",
    "creator" : "LaTeX with hyperref package"
  }
}