{
  "name" : "1412.4972.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Max-Product Belief Propagation for Linear Programming: Convergence and Correctness",
    "authors" : [ "Sejun Park", "Jinwoo Shin" ],
    "emails" : [ "sejun.park@kaist.ac.kr,", "jinwoos@kaist.ac.kr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 2.\n49 72\nv1 [\ncs .A\nI] 1\n6 D\nec 2\n01 4"
    }, {
      "heading" : "1 Introduction",
      "text" : "Graphical model (GM) has been one of powerful paradigms for succinct representations of joint probability distributions in variety of scientific fields [18, 10, 9, 15]. GM represents a joint distribution of some random vector to graph structured model where each vertex corresponds to random variable and each edge captures to a conditional independence between random variables. In many applications involving GMs, finding maximum a posteriori (MAP) assignment in GM is an important inference task, which is known to be computationally intractable (i.e., NP-hard) in general [2]. Max-product belief propagation (BP) is the most popular heuristic for approximating a MAP assignment of given GM, where its performance has been not well understood in loopy GMs. Nevertheless, BP often shows remarkable performances even on loopy GMs and distributed implementation, associated ease of programming and strong parallelization potential are the main reasons for the growing popularity of the BP algorithm. For example, several software architectures for implementing parallel BPs were recently proposed [7, 4, 8] by different research groups in machine learning communities.\nRecently, it has been shown that BP converges to the correct answer for certain classes of loopy GM formulations of several combinatorial optimization problems, including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11]. The important common feature of these instances is that BP converges to a correct MAP assignment if the Linear Programming (LP) relaxation of the MAP inference problem is tight, i.e., it shows no integrality gap. In other words, BP can be used an efficient distributed solver for those LPs, and is presumably better than classical centralized LP solvers such as simplex methods [3], interior point methods [14] and ellipsoid methods [6] for large-scale inputs. However, these theoretical results on BP are very sensitive to underlying\nstructural properties depending on specific problems, e.g., the BP analysis for matching problems [12, 5] do not extend to even for perfect matching ones [1]. Therefore, it has been not clear what extent they can be generalized to.\nIn this paper, we establish generic conditions for GM formulations of LPs so that BP converges to the desired solution of LP. As one can naturally expect given prior results, one of our conditions requires the tightness of given LP. Our main contribution is finding other sufficient generic conditions so that BP converges to the correct MAP assignment in GMs. First of all, our generic condition can rediscover previous BP results including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11], i.e., we provide a unified framework on establishing the convergence and correctness on BPs in relation to associated LPs. Furthermore, we provide new examples under our framework, e.g., we show that BP can solve LP formulations of other combinatorial optimizations such as traveling salesman, longest path and vertex cover. In addition, while most prior known results on BP and LP focused on the case when LP has an integral solution, our result implies that BP can even solve LPs having fractional solutions (see Section 4.2 for details), which shows a possibility that BP can be a generic solver of LP. The main appeal of our framework is providing a generic guideline on BP design for its high performance, i.e., one can carefully design a GM given LP so that it satisfies our desired conditions. We believe that our unified framework on BP analysis provides new insights on BP performances and new directions on efficient distributed (and parallel) solvers for certain classes of large-scale LPs."
    }, {
      "heading" : "1.1 Organization",
      "text" : "In Section 2, we introduce the graphical model, belief propagation and general integer programming and its linear programming relaxation. In Section 3, we establish the main result of the paper. Finally, applications of the main result is in Section 4."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Graphical Model and Belief Propagation",
      "text" : "A joint distribution of n (binary) random variables Z = [Zi] ∈ {0, 1}n is called a Graphical Model (GM) if it factorizes as follows: for z = [zi] ∈ Ωn,\nPr[Z = z] ∝ ∏\nα∈F\nψα(zα), (1)\nwhere {ψα} are (given) non-negative functions, the so-called factors; F is a collection of subsets\nF = {α1, α2, ..., αk} ⊂ 2 {1,2,...,n}\n(each αj is a subset of {1, 2, . . . , n}); zα is the projection of z onto dimensions included in α. For example, if z = [0, 1, 0] and α = {1, 3}, then zα = [0, 0]. In particular, when |α| = 1, ψα is called a variable factor. Figure 1 depicts the the graphical relation between factors F and variables z.\nAssignment z∗ is called a maximum a posteriori (MAP) assignment if\nz∗ = arg max z∈{0,1}n Pr[z].\nThis means that computing a MAP assignment requires us to compare Pr[z] for all possible z, which is typically computationally intractable (i.e., NP-hard) unless the induced bipartite graph of factors F and variables z, so-called factor graph, has a bounded treewidth [2].\nThe (max-product) BP algorithms are popular heuristics for approximating the MAP assignment in a graphical model. BP is an iterative procedure; at each iteration t, there are four messages\n{mtα→i(c),m t i→α(c) : c ∈ {0, 1}}\nbetween every variable zi and every associated factor ψα ∈ Fi, where Fi := {α ∈ F : i ∈ α}; that is, Fi is a subset of F such that all the α in Fi include the ith position of z for any given z. Initially, we set m0α→i(c) = m 0 i→α(c) = 1, and messages are updated as follows:\nmt+1α→i(c) = maxzα:zi=c ψα(zα)\n∏\nj∈α\\i\nmtj→α(zj) (2)\nmt+1i→α(c) = ∏\nα′∈Fi\\α\nmtα′→i(c). (3)\nFirst, we note that each zi only sends messages to Fi; that is, zi sends messages to αj only if αj selects/includes i. The outer-term in the message computation (2) is a summation over all possible zα ∈ {0, 1}\n|α|. The inner-term is a product that only depends on the variables zj that are connected to α. The message-update (3) from variable zi to factor ψα is a product which considers all messages received by ψα in the previous iteration, except for the message sent by zi itself.\nFinally, given a set of messages {mi→α(c), mα→i(c) : c ∈ {0, 1}}, the so-called BP marginal beliefs are computed as follows:\nbi[zi] = ∏\nα∈Fi\nmα→i(zi). (4)\nThen, the BP algorithm outputs zBP = [zBPi ] as\nzBPi =    1 if bi[1] > bi[0] ? if bi[1] = bi[0]\n0 if bi[1] < bi[0]\n.\nIt is known that zBP converges to a MAP assignment after a large enough number of iterations, if the factor graph is a tree and the MAP assignment is unique. However, if the graph has loops in it, the BP algorithm has no guarantee to find a MAP solution in general."
    }, {
      "heading" : "2.2 Integer Programming and Linear Programming Relaxation",
      "text" : "Integer programming (IP) is an optimization problem in which all variables are integer and its objective function and constraints are linear. We can formulate general IP as below:\nminimize w · x\nsubject to Ax ≤ b\nx = [xi] ∈ Z n A ∈ Rm×n, b ∈ Rm.\n(5)\nHowever, solving IP is NP-Hard. Hence, we usually solve relaxed problem called linear programming (LP) rather than IP. In LP, we relax the integer constraint of variables as below.\nminimize w · x\nsubject to Ax ≤ b\nx = [xi] ∈ R n A ∈ Rm×n, b ∈ Rm.\n(6)"
    }, {
      "heading" : "3 Conditions on Convergence and Correctness of Belief Propagation",
      "text" : "Consider the following GM (Graphical Model): for x = [xi] ∈ {0, 1}n and w = [wi] ∈ Rn,\nPr[X = x] ∝ ∏\ni\ne−wixi ∏\nα∈F\nψα(xα), (7)\nwhere the factor function ψα is defined as\nψα(xα) = { 1 if Aαxα ≥ bα, Cαxα = dα 0 otherwise ,\nfor some matrices Aα, Cα and vectors bα, dα. Now we consider the IP (Integer Programming) corresponding the above GM:\nminimize w · x\nsubject to ψα(xα) = 1, ∀α ∈ F\nx = [xi] ∈ {0, 1} n.\n(8)\nOne can easily observe that the MAP assignments for GM (7) corresponds to the solutions of IP (8). To establish the performance of BP on GM (7) for solving IP (8), we also consider the following the LP (Linear Programming) relation to IP (8):\nminimize w · x\nsubject to ψα(xα) = 1, ∀α ∈ F\nx = [xi] ∈ [0, 1] n.\n(9)\nWe let P denote the polytope of feasible solutions of the above LP:\nP := {x ∈ [0, 1]n : ψα(xα) = 1, ∀α ∈ F} .\nSimilarly, Pα is defined as Pα := { x ∈ [0, 1]|α| : ψα(xα) = 1 } . Now we are ready to state the main result of this paper.\nTheorem 1 The max-product BP on GM (7) converges to the solution of IP (8) if the following conditions hold:\nC1. LP (9) has unique integral solution x∗ ∈ {0, 1}n , i.e., it is tight.\nC2. For every i ∈ {1, 2, . . . , n}, the number of factors associated with xi is at most two, i.e.,\n|α ∈ F : i ∈ α| ≤ 2.\nC3. For every factor ψα, every xα ∈ {0, 1}|α| with ψα(xα) = 1, and every i ∈ α with xi 6= x∗i , there exists j ∈ α with xj 6= x∗j such that\nψα(x ′ α) = 1, where x ′ k =\n{ xk if k 6= i, j\nx∗k otherwise .\nψα(x ′′ α) = 1, where x ′′ k =\n{ xk if k = i, j\nx∗k otherwise ."
    }, {
      "heading" : "3.1 Proof of Theorem 1",
      "text" : "Before start the proof, let us introduce a useful Lemma\nLemma 2 There exists a universal constant K for LP (9) such that if some γ ∈ Zn and ε > 0 satisfying the followings:\n1. There exist (at most) two factors such that\nF (z) := {α ∈ F : zα /∈ Pα}, |F (z)| ≤ 2\nwhere z = x∗ + εγ,\n2. For every α ∈ F (z), there exist i ∈ α such that\nz†α ∈ Pα,\nwhere z† = z + εei or z† = z − εei and ei ∈ {0, 1}n is the unit vector whose i-th coordinate is 1,\nthen there exists z‡ ∈ P such that ‖z − z‡‖1 ≤ εK.\nThe proof of Lemma 2 is presented in appendix. To begin with, from Condition C1, there exists δ > 0 such that\nδ := inf x∈P\\x∗\nw · x− w · x∗\n‖x− x∗‖1 > 0. (10)\nWe let x̂t ∈ {0, 1, ?}n denote the BP estimate at the t-th iteration for the MAP computation. We will show that under Conditions C1-C3,\nx̂t = x∗, for t > K(wmax/δ + 1),\nwhere wmax = maxi |wi| and K is the universal constant in Lemma 2. Suppose the above statement is false, i.e., there exists i ∈ {1, 2, . . . , n} such that x̂ti 6= x ∗ i for t > K(wmax/δ + 1). Under the assumption, we will reach a contradiction, which completes the proof of Theorem 1. To this end, we further assume the case x∗i = 1 (i.e., x̂ t i ∈ {0, ?}) where the case x ∗ i = 0 can be argued similarly. Now we construct a tree-structured GM (also popularly known as the computational tree [17]), denoted by Ti(t), as follows:\n1. Add yi ∈ {0, 1} as the root variable with variable factor function e−wiyi .\n2. For each leaf variable yj and for each α ∈ F such that j ∈ α and ψα is not associated with yj in the current tree-structured GM, add a factor function ψα as a child of yj .\n3. For each leaf factor ψα and for each variable yk such that k ∈ α and yk is not associated with ψα in the current tree-structured GM, add a variable yk as a child of ψα with variable factor function e−wjyj .\n4. Repeat Step 2, 3 t times.\nSince we assume x̂ti ∈ {0, ?}, it is known from [16] that there exists a MAP configuration y MAP on Ti(t) with yMAPi = 0 at the root variable. Now we construct a new assignment y NEW on the computational tree Ti(t) as follows:\n1. Initially, set yNEW ← yMAP .\n2. Update the value of the root variable of Ti(t) by yNEWi ← x ∗ i .\n3. For each child factor ψα of root i ∈ α, choose j ∈ α according to Condition C3 and update the associated variable by yNEWj ← x ∗ j .\n4. Repeat Step 2,3 recursively by substituting Ti(t) by the subtree of Ti(t) of root j until the process stops (i.e., i = j) or the leaf of Ti(t) is reached (i.e., i does not have a child).\nOne can notice that the set of revised variables in Step 2 of the above procedure forms a path structure Q in the tree Ti(t). We first, consider the case that both ends of the path Q touch leaves of Ti(t), where other cases can be argued in a similar manner. Let define α = [αi], β = [βi] ∈ Zn+ as follow\nαi = number of copies of xi in path Q with x∗i = 1 βi = number of copies of xi in path Q with x∗i = 0\nThen, from our construction of yNEW , one can observe that yNEW = yMAP + α − β. Let define z = x∗ + ε(β − α) where 0 < ε < 1/2t. From C3, z violates at most two factors, say α1, α2, which are connected to the changed leaf variable. From C2, for each i = 1, 2 there is j ∈ αi such that z † αi ∈ Pαi where z† = z+ εej or z† = z− εej . We will use Lemma 2 utilizing our construction of z. From Lemma 2, we have z‡ ∈ P such that\n‖z‡ − z‖1 ≤ εK, ‖z ‡ − x∗‖1 ≥ ε(‖α‖1 + ‖β‖1 −K) = ε(2t −K).\nwhere z = x∗ + ε(β − α). Hence, it follows that\n0 < δ ≤ w(z‡)− w(x∗)\n‖z‡ − x∗‖1\n≤ w(z) + εwmaxK − w(x\n∗)\nε(2t−K)\n= εw(β − α) + εwmaxK\nε(2t−K)\n= w(β − α) + wmaxK\n2t−K\nTherefore, we can bound w(β − α) as\nw(β − α) ≥ 2δt − (wmax + δ)K > 0, for t > K(wmax/δ + 1).\nThe above inequality leads to the contradiction to the fact that yMAP is a MAP configuration in Ti(t) since\nw · yNEW = w(α− β) + w · yMAP < w · yMAP .\nThis completes the proof of Theorem 1."
    }, {
      "heading" : "4 Applications of Theorem 1",
      "text" : "In this section, we introduce applications that BP correctly converge to the solution if conditions of Theorem 1 are satisfied. We first show the shortest path problem [11]. Secondly, we show matching related applications including the results of [5, 12, 1, 13]. Final parts are the traveling salesman problem and the longest path problem. Our applications cover prior works and other problems such as the traveling salesman problem and solving the maximum weight matching linear programming."
    }, {
      "heading" : "4.1 Shortest Path",
      "text" : "Shortest path problem on a directed graph G = (V,E) and weight w = [we : e ∈ E] ∈ R|E| is to find a path from the source s to the destination t such that the path minimizes the edge weights of the path. One can naturally design the following LP for this problem.\nminimize ∑\ne∈E\nwexe\nsubject to ∑\ne∈δo(v)\nxe − ∑\ne∈δi\nxe =    1 if u = s −1 if u = t\n0 otherwise\n∀ v ∈ V\nx = [xe] ∈ [0, 1] |E|.\n(11)\nwhere δi(v), δo(v) are the set of incoming, outgoing edges of v. We can construct GM from the shortest path problem LP (11) as below:\nPr[X = x] ∝ ∏\ne∈E\ne−wexe ∏\nv∈V\nψv(xδ(v)), (12)\nwhere the factor function ψv is defined as\nψv(xδ(v)) =    1 if ∑ e∈δo(v) xe − ∑ e∈δi(v) xe =    1 if u = s −1 if u = t 0 otherwise\n0 otherwise\n,\nFrom above GM (12), let us introduce a Corollary.\nCorollary 3 If the shortest path problem LP (11) has a unique and integral solution, then max-product BP on GM (12) converges to the solution of LP (11)\nThe proof of Corollary 3 is presented in the appendix."
    }, {
      "heading" : "4.2 Maximum Weight Perfect Matching",
      "text" : "Given graph G = (V,E) and weight w = [we : e ∈ E] ∈ R|E| on edges, the maximum weight perfect matching problem is to find a set of edges such that each vertex is connected to exactly one edge in the set and the sum of edge weights in the set is maximized. One can naturally design the following LP for this problem.\nmaximize w · x subject to ∑\ne∈δ(v)\nxe = 1, ∀ v ∈ V\nx = [xe] ∈ [0, 1] |E|.\n(13)\nwhere δ(v) is a set of edges connected to a vertex v. If the above LP has an integral solution, it corresponds to the solution of the maximum weight perfect matching problem.\nIt is known that for each vertex x of feasible solution polytope of the maximum weight matching LP (13), each xe of x can only have a value from {0, 1/2, 1}. Lets duplicate each edge e to e1, e2 and define G′ = (V,E′) where E′ = {e1, e2|e ∈ E}. Then, we can make an LP on G′ which always have non-fractional solution.\nmaximize ∑\nei∈E′\nw′eixei\nsubject to ∑\nei∈δ(v)\nxei = 2 ∀ v ∈ V\nx = [xe1 , xe2 ] ∈ [0, 1] 2|E|.\n(14)\nwhere w′e1 = we/2 and w ′ e2 = we/2 − ε. We introduce small enough ε to prevent the unnecessary multiple solutions due to the symmetry of e1, e2. Then, by setting xe = (xe1 + xe2)/2, the solution of LP (14) is a solution of LP (13) and LP (14) has unique integral solution if and only if LP (13) has unique solution. Now, construct the GM from LP (14) as below:\nPr[X = x] ∝ ∏\ne∈E\new ′ e1 xe1+w ′ e2 xe2\n∏\nv∈V\nψv(xδ(v)), (15)\nwhere the factor function ψv is defined as\nψv(xδ(v)) =\n{ 1 if ∑ ei∈δ(v) xei = 2\n0 otherwise .\nLet us introduce a Corollary from GM (15).\nCorollary 4 If the maximum weight perfect matching LP (13) has a unique solution, then max-product BP on GM (15) converges to the solution of LP (13)\nThe proof of Corollary 4 is presented in the appendix. We note that one can obtain a similar conclusion for the maximum (non-perfect) weight matching problem. We omit the details in this paper."
    }, {
      "heading" : "4.3 Maximum Weight Perfect Matching with Odd Cycles",
      "text" : "In previous section we prove that BP correctly converges to the solution of LP (13) if LP (13) has a unique solution. It is known that adding odd cycle constraint for every odd cycles makes LP tight i.e. LP has an integral solution. The constraint is as follows:\n∑\ne∈C\nxe ≤ |C| − 1\n2\nwhere C is an odd cycle. There is previous work on finding matching by using BP and adding disjoint odd cycle constraint [13]. We refer the following graphical transformation from [13]. Let C be a set of disjoint odd cycles in G. Lets construct the new graph G′ = (V ′, E′), w′ where\nV ′ = V ∪ {vC |C ∈ C}, E ′ = {(u, vC )|u ∈ C,C ∈ C} ∪ E \\ {e ∈ C|C ∈ C}\nw′e =\n{ 1 2 ∑ e′∈E(C)(−1) dC(u,e ′)we′ if e = (u, vC) for some C ∈ C\nw′e otherwise .\ndC(u, e ′) is the distance between u, e′ in cycle C . We refer the maximum weight matching with odd cycles LP from [13] and slightly modify to make the maximum weight perfect matching with odd cycles LP.\nmaximize w′ · y subject to ∑\ne∈δ(v)\nye = 1, ∀ v ∈ V\n∑\nu∈V (C)\n(−1)dC(u,e)y(vC ,u) ∈ [0, 2] ∀e ∈ E(C)\n∑\ne∈δ(vC )\nye ≤ |C| − 1 ∀C ∈ C\ny = [ye] ∈ [0, 1] |E′|\n(16)\nAbove LP is equivalent to the LP (13) with odd cycle constraints. Now, construct GM from above LP.\nPr[Y = y] ∝ ∏\ne∈E\neweye ∏\nv∈V\nψv(yδ(v)) ∏\nC∈C\nψC(yδ(vC )), (17)\nwhere the factor function ψv, ψC is defined as\nψv(yδ(v)) =\n{ 1 if ∑ e∈δ(v) ye = 1\n0 otherwise ,\nψC(xδ(vC )) =    1 if ∑ u∈V (C)(−1) dC (u,e)y(vC ,u) ∈ {0, 2}∑ e∈δ(vC ) ye ≤ |C| − 1\n0 otherwise\n.\nLet us introduce a Corollary from GM (17).\nCorollary 5 If the maximum weight perfect matching with odd cycles LP (16) has a unique and integral solution, then max-product BP on GM (17) converges to the solution of LP (16)\nThe proof of Corollary 5 is presented in appendix. We note that one can obtain a similar conclusion for the maximum (non-perfect) weight matching with odd cycles problem. We omit the details in this paper."
    }, {
      "heading" : "4.4 Traveling Salesman",
      "text" : "Given a directed graphG = (V,E) and weight w = [we : e ∈ E] ∈ R|E|, the traveling salesman problem (TSP) is to find a cycle containing every vertices such that the cycle minimizes the sum of edge weights. To construct the TSP LP, let us introduce an auxiliary graph G′ = (V,E′) where E′ = {e1, . . . , en|e ∈ E}, n = |V | and ei is a duplications of e with edge index i. Then, finding a solution of TSP is equivalent to finding a Hamiltonian cycle of G′ which minimizes the sum of edge weights such that at each vertex, (outgoing edge index - incoming edge index) mod n = 1. Then, we can formulate LP for TSP on the auxiliary graph G′.\nminimize ∑\ni\n∑\nei∈E′\nweixei\nsubject to ∑\nei+1∈δo(v)\nxei+1 − ∑\nei∈δi(v)\nxei = 0\n∑\ne1∈δo(v)\nxe1 = ∑\nen∈δi(v)\nxen = 0 i 6= n, v 6= s\n∑\ne1∈δo(s)\nxe1 = ∑\nen∈δi(s)\nxen = 1\n∑\nei∈δo(s)\nxei = ∑\nej∈δi(s)\nxej = 0 i 6= 1, j 6= n\nx = [xei ] ∈ [0, 1] |V ||E|.\n(18)\nwhere δi(v), δo(v) are set of incoming, outgoing edges of v. We fixed a vertex s to prevent the unnecessary multiple solutions due to the symmetry of edge indices. From above LP (18), we can construct GM.\nPr[X = x] ∝ ∏\ni\n∏\nei∈E′\ne−weixei ∏\nv∈V\nψv(xδ(v)), (19)\nwhere the factor function ψv is defined as\nψv(xδ(v)) =    1 if ∑ ei+1∈δo(v) xei+1 − ∑ ei∈δi(v) xei = 0∑ e1∈δo(v) xe1 = ∑ en∈δi(v) xen = 0\n0 otherwise\nfor i 6= n, v 6= s\nψs(xδ(s)) =    1 if ∑ e1∈δo(s) xe1 = ∑ en∈δi(s) xen = 1∑ ei∈δo(s) xei = ∑ ej∈δi(s) xej = 0\n0 otherwise\nfor i 6= 1, j 6= n\nLet us introduce a Corollary from GM (19).\nCorollary 6 If the traveling salesman problem LP (18) has a unique and integral solution, then maxproduct BP on GM (19) converges to the solution of LP (18)\nThe proof of Corollary 6 is presented in the appendix."
    }, {
      "heading" : "4.5 Longest Path",
      "text" : "Given a directed graph G = (V,E) and weight w = [we : e ∈ E] ∈ R|E|, longest path problem is to to find a path from a source s to a destination t that maximizes the sum of weights of edges in the path. Longest path problem is known as NP-Hard. To construct the longest path problem LP, let us introduce an auxiliary graph G′ = (V,E′) where E′ = {e1, . . . , en|e ∈ E}, n = |V | and ei is a duplications of e with edge index i. Then, finding a solution of longest path problem is equivalent to finding a longest path of G′ which maximizes the sum of edge weights such that at each vertex, (outgoing edge index - incoming edge index) mod n = 1. Then, we can formulate LP for longest path problem on the auxiliary graph G′.\nmaximize ∑\ni\n∑\nei∈E′\nweixei\nsubject to ∑\nei+1∈δo(v)\nxei+1 − ∑\nei∈δi(v)\nxei = 0\n∑\ne1∈δo(v)\nxe1 = ∑\nen∈δi(v)\nxen = 0 i 6= n, v 6= s, t\n∑\ne1∈δo(s)\nxe1 = 1, ∑\nei∈δo(s)\nxei = ∑\nej∈δi(s)\nxej = 0 i 6= 1,∀j\n∑\ni\n∑\nei∈δi(t)\nxei = 1, ∑\nej∈δo(t)\nxej = 0 ∀j\nx = [xei ] ∈ [0, 1] |V ||E|.\n(20)\nwhere δi(v), δo(v) are set of incoming, outgoing edges of v. From above LP (20), we can construct GM.\nPr[X = x] ∝ ∏\ni\n∏\nei∈E′\neweixei ∏\nv∈V\nψv(xδ(v)), (21)\nwhere the factor function ψv is defined as\nψv(xδ(v)) =    1 if ∑ ei+1∈δo(v) xei+1 − ∑ ei∈δi(v) xei = 0∑ e1∈δo(v) xe1 = ∑ en∈δi(v) xen = 0\n0 otherwise\nfor i 6= n, v 6= s, t\nψs(xδ(s)) =\n{ 1 if ∑ e1∈δo(s) xe1 = 1, ∑ ei∈δo(s) xei = ∑ ej∈δi(s) xej = 0\n0 otherwise for i 6= 1,∀j\nψt(xδ(t)) =\n{ 1 if ∑ i ∑ ei∈δi(t) xei = 1, ∑ ej∈δo(t) xej = 0\n0 otherwise for ∀j\nLet us introduce a Corollary from GM (21).\nCorollary 7 If the longest path problem LP (20) has a unique and integral solution, then max-product BP on GM (21) converges to the solution of LP (20)\nThe proof of Corollary 7 is almost same as that of Corollary 6. We omit the proof in this paper."
    }, {
      "heading" : "4.6 Vertex Cover",
      "text" : "Given a graph G = (V,E) and positive integer weight b = [bv : v ∈ V ] ∈ R|V |, vertex cover problem is to find a set of vertices minimizes the sum of vertex weights of the set such that each edge is connected to at least one vertex in the set. The dual LP of the vertex cover problem is formulated as follows:\nmaximize ∑\ne∈E\nxe\nsubject to ∑\ne∈δ(v)\nxe ≤ bv, ∀ v ∈ V\nx = [xe] ∈ [0,∞) |E|.\n(22)\nOne can notice that xe ≤ maxv bv. To make an equivalent LP with binary variables, duplicate each edge e to e1, . . . , en where n = maxv bv. Let us introduce an auxiliary graph G′ = (V,E′) where E′ = {e1, . . . , en|e ∈ E}. Then, the following LP is equivalent with LP (22) by setting xe = ∑ i xei .\nmaximize w · x subject to ∑\nei∈δ(v)\nxei ≤ bv, ∀ v ∈ V\nx = [xei ] ∈ [0, 1] |E′|\n(23)\nWhere, δ(v) is a set of edges connected to a vertex v, w = [wei : ei ∈ E ′] ∈ [0, 1]|E ′| and wi = 1 − iε. We choose small enough ε to avoid unnecessary multiple solutions due to the symmetry of eis. From above LP, we can construct GM\nPr[X = x] ∝ ∏\ne∈E\newexe ∏\nv∈V\nψv(xδ(v)), (24)\nwhere the factor function ψv is defined as\nψv(xδ(v)) =\n{ 1 if ∑ ei∈δ(v) xei ≤ bv\n0 otherwise .\nLet us introduce Corollary from above GM.\nCorollary 8 If the vertex cover dual LP (22) has a unique and integral solution, then max-product BP on GM (24) converges to the solution of LP (22)\nThe proof of Corollary 8 is presented in the appendix."
    }, {
      "heading" : "5 Conclusion",
      "text" : "The BP algorithm has been the most popular algorithm for solving inference problems arising graphical models, where its distributed implementation, associated ease of programming and strong parallelization potential are the main reasons for its growing popularity. In this paper, we aim for designing BP algorithms solving Linear Programmings, and provide sufficient conditions for the purpose. We believe that our results provide new interesting directions on designing efficient distributed (and parallel) solvers for large-scale Linear Programmings.\nAcknowledgements. We would like to acknowledge the support of the AOARD project, FA2386-141-4058."
    }, {
      "heading" : "A Proof of Lemma 2",
      "text" : "let α1, α2 be violated factors. Then,\nz ∈ {x ∈ [0, 1]n|Aαxα ≥ bα, Aα1xα1 ≥ bα1 − ε,Aα2xα2 ≥ bα2 − ε, α 6= α1, α2}\nTo help the proof, let us introduce a Claim.\nClaim 9 For any polytope {x ∈ [0, 1]n|Ax ≥ b, c · x ≥ d}\nwith A ∈ Rm×n, b ∈ Rm, c ∈ Rn, d ∈ R, there exists K such that for all ε > 0,\ndist(P,Pǫ) := max x∈Pǫ (min y∈P ‖x− y‖2) ≤ εK\nwhere P = {x ∈ [0, 1]n|Ax ≥ b, c · x = d}, Pǫ = {x ∈ [0, 1]n|Ax ≥ b, d ≥ c · x ≥ d− ε}\nBy Claim 9 and the mathematical induction, we can find K such that there exists z‡ ∈ P with ‖z − z‡‖1 ≤ εK.\nThe proof of above Claim is following. For a polytope {x ∈ [0, 1]n|Ax ≥ b, c · x ≤ d}, there are finite number of vertices. Choose δ such that there is no vertex v of {x ∈ [0, 1]n|Ax ≥ b, c · x ≤ d} satisfying d− δ ≤ c · v < d. As Pδ is bounded, we can choose K such that\nmax z∈Pδ,y∈P ‖z − y‖2 ≤ δK\nConsider the case when ε > δ. For any x ∈ {x ∈ [0, 1]n|Ax ≥ b, c · x = d − ε}, y ∈ P , set z = δ\nε x+ (1− δ ε )y Then, z ∈ {x ∈ [0, 1]n|Ax ≥ b, c · x = d− δ},\n‖x− y‖2 = ε\nδ ‖z − y‖2 ≤ εK\nand it implies dist(P,Pǫ) ≤ εK for ε > δ Now, consider the case when ε < δ. From the choice of δ, there is no vertex v of Pδ satisfying d− δ < c · v < d. Let v1, . . . , vm, u1, . . . , uk be vertices of Pδ with c · vi = d − δ and c · uj = d. From the property of the polytope, for any x ∈ {x ∈ [0, 1]n|Ax ≥ b, c · x = d − ε} ⊂ Pδ can be expressed as a convex combination of v1, . . . , vm, u1, . . . , uk . It implies that there are y ∈ P, z ∈ {x ∈ [0, 1]n|Ax ≥ b, c · x = d− δ} such that x is a convex combination of y and z. Therefore,\n‖x− y‖2 = ε\nδ ‖z − y‖2 ≤ εK\nand it implies dist(P,Pǫ) ≤ εK for ε < δ."
    }, {
      "heading" : "B Proof of Corollary 3",
      "text" : "The proof of Corollary 3 can be done by using Theorem 1. From GM (12), each variable is connected to two factors (C2 of Theorem 1). Now, lets check C3 of Theorem 1. Suppose there are v and xδ(v) with ψv(xδ(v)) = 1. Consider the case when there is e ∈ δ i(v) with xe = 1 6= x∗e. If e ′ ∈ δi(v) with xe′ = 0 6= x ∗ e′ exists, choose such e\n′. If not, choose e′ ∈ δo(v) with xe′ = 1 6= x∗e′ . On the other hand, consider when there is e ∈ δi(v) with xe = 0 6= x∗e . If e\n′ ∈ δo(v) with xe′ = 1 6= x∗e′ exists, choose such e′. If not, choose e′ ∈ δi(v) with xe′ = 0 6= x∗e′ . Then,\nψv(x ′ δ(v)) = 1, where x ′ e′′ =\n{ xe′′ if k 6= e, e′\nx∗e′′ otherwise .\nψv(x ′′ δ(v)) = 1, where x ′′ e′′ =\n{ xe′′ if k = e, e′\nx∗e′′ otherwise .\nWe can apply similar argument for the case when e ∈ δo(v), v = s or t. From Theorem 1, we can conclude that if the solution of LP (11) is unique and integral, max-product BP on GM (12) converges to the solution of LP (11)."
    }, {
      "heading" : "C Proof of Corollary 4",
      "text" : "The proof of Corollary 4 can be done by using Theorem 1. From GM (15), each variable is connected to two factors (C2 of Theorem 1). Now, lets check C3 of Theorem 1. Suppose there are v and xδ(v) with ψv(xδ(v)) = 1. Consider the case when there is ei ∈ δ(v) with xei = 1 6= x ∗ ei\n. Then, there is e′j ∈ δ(v) with xe′j = 0 6= x ∗ e′j . Choose such e′j . On the other hand, consider when there is ei ∈ δ(v) with xei = 0 6= x ∗ ei . Then, there is e′j ∈ δ(v) with xe′j = 1 6= x ∗ e′j . Choose such e′j . Then,\nψv(x ′ δ(v)) = 1, where x ′ e′′ k =\n{ xe′′\nk if e′′k 6= ei, e ′ j\nx∗ e′′ k\notherwise .\nψv(x ′′ δ(v)) = 1, where x ′′ e′′ k =\n{ xe′′\nk if e′′k = ei, e ′ j\nx∗ e′′ k\notherwise .\nFrom Theorem 1, we can conclude that if the solution of LP (13) is unique, max-product BP on GM (15) converges to the solution of LP (13)."
    }, {
      "heading" : "D Proof of Corollary 5",
      "text" : "From GM (17), each variable is connected to two factors (C2 of Theorem 1). Now, lets check C3 of Theorem 1. For v ∈ V , we can apply same argument as the maximum weight matching case. Suppose there are vC and yδ(vC) with ψC(yδ(vC )) = 1. Consider the case when there is (u1, vC) ∈ δ(vC ) with y(u1,vC) = 1 6= y ∗ (u1,vC)\n. As a feasible solution yδ(vC ) forms a disjoint even paths [13], check edges along the path contains u1. If there is u2 ∈ V (C) in the path with y(u2,vC) = 1 6= y ∗ (u2,vC)\nexists, choose such (u1, vC). If not, choose (u2, vC) ∈ V (C) with y(u1,vC) = 0 6= y ∗ (u1,vC)\nat the end of the path. On the other hand, consider the case when there is (u1, vC) ∈ δ(vC) with y(u1,vC) = 0 6= y ∗ (u1,vC)\n. As a feasible solution yδ(vC) form a disjoint even paths, check edges along the path contains u1. If there is u2 ∈ V (C) in the path with y(u2,vC) = 0 6= y ∗ (u2,vC)\nexists, choose such (u1, vC). If not, choose (u2, vC) ∈ V (C) with y(u1,vC) = 1 6= y ∗ (u1,vC)\nat the end of the path. Then, from disjoint even paths point of view, we can check that\nψC(y ′ δ(vC ) ) = 1, where y′(u,vC) = { y(u,vC) if u 6= u1, u2 y∗(u,vC) otherwise .\nψC(y ′′ δ(vC ) ) = 1, where y′′(u,vC) = { y(u,vC) if u = u1, u2 y∗(u,vC) otherwise .\nFrom Theorem 1, we can conclude that if the solution of LP (16) is unique and integral, max-product BP on GM (17) converges to the solution of LP (16)."
    }, {
      "heading" : "E Proof of Corollary 6",
      "text" : "The proof of Corollary 6 can be done by using Theorem 1. From GM (19), each variable is connected to two factors (C2 of Theorem 1). Now, lets check C3 of Theorem 1. Suppose there are v and xδ(v) with ψv(xδ(v)) = 1. Consider the case when there is ei ∈ δ i(v) with xei = 1 6= x ∗ ei . If e′i ∈ δ i(v) with xe′i = 0 6= x ∗ e′i exists, choose such e′j . If not, choose e ′ i+1 such that xe′i+1 = 1 6= x ∗ e′i+1 . On the other hand, consider when there is ei ∈ δi(v) with xei = 0 6= x ∗ ei . If e′i ∈ δ i(v) with xe′i = 1 6= x ∗ e′i exists, choose such e′j . If not, choose e ′ i+1 such that xe′i+1 = 0 6= x ∗ e′i+1 . Then,\nψv(x ′ δ(v)) = 1, where x ′ e′′ k =\n{ xe′′\nk if e′′k 6= ei, e ′ i(or e ′ i+1)\nx∗ e′′ k\notherwise .\nψv(x ′ δ(v)) = 1, where x ′′ e′′ k =\n{ xe′′\nk if e′′k = ei, e ′ i(or e ′ i+1)\nx∗ e′′ k\notherwise .\nWe can apply similar argument for the case when ei ∈ δo(v). From Theorem 1, we can conclude that if the solution of LP (18) is unique and integral, max-product BP on GM (19) converges to the solution of LP (18)."
    }, {
      "heading" : "F Proof of Corollary 8",
      "text" : "The proof of Corollary can be done by using Theorem 1. From GM (24), each variable is connected to two factors (C2 of Theorem 1). Now, lets check C3 of Theorem 1. Suppose there are v and xδ(v) with ψv(xδ(v)) = 1. Consider the case when there is ei ∈ δ(v) with xei = 1 6= x ∗ ei\n. If there is e′j ∈ δ(v) with xe′j = 0 6= x ∗ e′j , choose such e′j . If not, choose e ′ j = ei On the other hand, consider when there is ei ∈ δ(v) with xei = 0 6= x ∗ ei . If there is e′j ∈ δ(v) with xe′j = 1 6= x ∗ e′j , choose such e′j . If not, choose e′j = ei Then,\nψv(x ′ δ(v)) = 1, where x ′ e′′ k =\n{ xe′′\nk if e′′k 6= ei, e ′ j\nx∗ e′′ k\notherwise .\nψv(x ′′ δ(v)) = 1, where x ′′ e′′ k =\n{ xe′′\nk if e′′k = ei, e ′ j\nx∗ e′′ k\notherwise .\nFrom Theorem 1, we can conclude that if the solution of LP (22) is unique, max-product BP on GM (24) converges to the solution of LP (22)."
    } ],
    "references" : [ {
      "title" : "Belief-propagation for weighted b-matchings on arbitrary graphs and its relation to linear programs with integer solutions",
      "author" : [ "Mohsen Bayati", "Christian Borgs", "Jennifer Chayes", "Riccardo Zecchina" ],
      "venue" : "arXiv preprint arXiv:0709.1190,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Complexity of inference in graphical models",
      "author" : [ "Venkat Chandrasekaran", "Nathan Srebro", "Prahladh Harsha" ],
      "venue" : "arXiv preprint arXiv:1206.3240,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Linear programming and extensions",
      "author" : [ "George B Dantzig" ],
      "venue" : "Princeton university press,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "Parallel splash belief propagation",
      "author" : [ "Joseph Gonzalez", "Yucheng Low", "Carlos Guestrin" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Loopy belief propagation for bipartite maximum weight bmatching",
      "author" : [ "Bert C Huang", "Tony Jebara" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Polynomial algorithms in linear programming",
      "author" : [ "Leonid G Khachiyan" ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1980
    }, {
      "title" : "Graphlab: A new framework for parallel machine learning",
      "author" : [ "Yucheng Low", "Joseph Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin", "Joseph M Hellerstein" ],
      "venue" : "arXiv preprint arXiv:1006.4990,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Task parallel implementation of belief propagation in factor graphs",
      "author" : [ "Nam Ma", "Yinglong Xia", "Viktor K Prasanna" ],
      "venue" : "In Parallel and Distributed Processing Symposium Workshops & PhD Forum (IPDPSW),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Information, physics, and computation",
      "author" : [ "Marc Mezard", "Andrea Montanari" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Modern coding theory",
      "author" : [ "Tom Richardson", "Ruediger Urbanke" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "st paths using the min-sum algorithm",
      "author" : [ "Nicholas Ruozzi", "Sekhar Tatikonda" ],
      "venue" : "In Communication, Control, and Computing,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Belief propagation and lp relaxation for weighted matching in general graphs",
      "author" : [ "Sujay Sanghavi", "Dmitry Malioutov", "Alan Willsky" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "A graphical transformation for belief propagation: Maximum weight matchings and odd-sized cycles",
      "author" : [ "Jinwoo Shin", "Andrew E Gelfand", "Misha Chertkov" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "Martin J Wainwright", "Michael I Jordan" ],
      "venue" : "Foundations and Trends R © in Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Belief propagation and revision in networks with loops",
      "author" : [ "Yair Weiss" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1997
    }, {
      "title" : "On the optimality of solutions of the max-product beliefpropagation algorithm in arbitrary graphs",
      "author" : [ "Yair Weiss", "William T Freeman" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "1 Introduction Graphical model (GM) has been one of powerful paradigms for succinct representations of joint probability distributions in variety of scientific fields [18, 10, 9, 15].",
      "startOffset" : 167,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "1 Introduction Graphical model (GM) has been one of powerful paradigms for succinct representations of joint probability distributions in variety of scientific fields [18, 10, 9, 15].",
      "startOffset" : 167,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "1 Introduction Graphical model (GM) has been one of powerful paradigms for succinct representations of joint probability distributions in variety of scientific fields [18, 10, 9, 15].",
      "startOffset" : 167,
      "endOffset" : 182
    }, {
      "referenceID" : 1,
      "context" : ", NP-hard) in general [2].",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "For example, several software architectures for implementing parallel BPs were recently proposed [7, 4, 8] by different research groups in machine learning communities.",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "For example, several software architectures for implementing parallel BPs were recently proposed [7, 4, 8] by different research groups in machine learning communities.",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "For example, several software architectures for implementing parallel BPs were recently proposed [7, 4, 8] by different research groups in machine learning communities.",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "Recently, it has been shown that BP converges to the correct answer for certain classes of loopy GM formulations of several combinatorial optimization problems, including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "Recently, it has been shown that BP converges to the correct answer for certain classes of loopy GM formulations of several combinatorial optimization problems, including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 0,
      "context" : "Recently, it has been shown that BP converges to the correct answer for certain classes of loopy GM formulations of several combinatorial optimization problems, including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11].",
      "startOffset" : 206,
      "endOffset" : 209
    }, {
      "referenceID" : 12,
      "context" : "Recently, it has been shown that BP converges to the correct answer for certain classes of loopy GM formulations of several combinatorial optimization problems, including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 10,
      "context" : "Recently, it has been shown that BP converges to the correct answer for certain classes of loopy GM formulations of several combinatorial optimization problems, including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11].",
      "startOffset" : 259,
      "endOffset" : 263
    }, {
      "referenceID" : 2,
      "context" : "In other words, BP can be used an efficient distributed solver for those LPs, and is presumably better than classical centralized LP solvers such as simplex methods [3], interior point methods [14] and ellipsoid methods [6] for large-scale inputs.",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "In other words, BP can be used an efficient distributed solver for those LPs, and is presumably better than classical centralized LP solvers such as simplex methods [3], interior point methods [14] and ellipsoid methods [6] for large-scale inputs.",
      "startOffset" : 220,
      "endOffset" : 223
    }, {
      "referenceID" : 11,
      "context" : ", the BP analysis for matching problems [12, 5] do not extend to even for perfect matching ones [1].",
      "startOffset" : 40,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : ", the BP analysis for matching problems [12, 5] do not extend to even for perfect matching ones [1].",
      "startOffset" : 40,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : ", the BP analysis for matching problems [12, 5] do not extend to even for perfect matching ones [1].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "First of all, our generic condition can rediscover previous BP results including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11], i.",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "First of all, our generic condition can rediscover previous BP results including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11], i.",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "First of all, our generic condition can rediscover previous BP results including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11], i.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "First of all, our generic condition can rediscover previous BP results including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11], i.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 10,
      "context" : "First of all, our generic condition can rediscover previous BP results including matching [12, 5], perfect matching [1], matching with odd cycles [13] and shortest path [11], i.",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "For example, if z = [0, 1, 0] and α = {1, 3}, then zα = [0, 0].",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : ", NP-hard) unless the induced bipartite graph of factors F and variables z, so-called factor graph, has a bounded treewidth [2].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "Then, the BP algorithm outputs z = [z i ] as z i =    1 if bi[1] > bi[0] ? if bi[1] = bi[0] 0 if bi[1] < bi[0] .",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "Then, the BP algorithm outputs z = [z i ] as z i =    1 if bi[1] > bi[0] ? if bi[1] = bi[0] 0 if bi[1] < bi[0] .",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "Then, the BP algorithm outputs z = [z i ] as z i =    1 if bi[1] > bi[0] ? if bi[1] = bi[0] 0 if bi[1] < bi[0] .",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "To establish the performance of BP on GM (7) for solving IP (8), we also consider the following the LP (Linear Programming) relation to IP (8): minimize w · x subject to ψα(xα) = 1, ∀α ∈ F x = [xi] ∈ [0, 1] .",
      "startOffset" : 200,
      "endOffset" : 206
    }, {
      "referenceID" : 0,
      "context" : "We let P denote the polytope of feasible solutions of the above LP: P := {x ∈ [0, 1] : ψα(xα) = 1, ∀α ∈ F} .",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Similarly, Pα is defined as Pα := { x ∈ [0, 1]|α| : ψα(xα) = 1 } .",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "Now we construct a tree-structured GM (also popularly known as the computational tree [17]), denoted by Ti(t), as follows:",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "Since we assume x̂i ∈ {0, ?}, it is known from [16] that there exists a MAP configuration y MAP on Ti(t) with y i = 0 at the root variable.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "We first show the shortest path problem [11].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Secondly, we show matching related applications including the results of [5, 12, 1, 13].",
      "startOffset" : 73,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "Secondly, we show matching related applications including the results of [5, 12, 1, 13].",
      "startOffset" : 73,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "Secondly, we show matching related applications including the results of [5, 12, 1, 13].",
      "startOffset" : 73,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "Secondly, we show matching related applications including the results of [5, 12, 1, 13].",
      "startOffset" : 73,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "x = [xe] ∈ [0, 1] .",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "e∈δ(v) xe = 1, ∀ v ∈ V x = [xe] ∈ [0, 1] .",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "ei∈δ(v) xei = 2 ∀ v ∈ V x = [xe1 , xe2 ] ∈ [0, 1] .",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "There is previous work on finding matching by using BP and adding disjoint odd cycle constraint [13].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "We refer the following graphical transformation from [13].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "We refer the maximum weight matching with odd cycles LP from [13] and slightly modify to make the maximum weight perfect matching with odd cycles LP.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "u∈V (C) (−1)d(u,e)y(vC ,u) ∈ [0, 2] ∀e ∈ E(C) ∑",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "y = [ye] ∈ [0, 1] |E| (16)",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "subject to ∑ ei+1∈δ(v) xei+1 − ∑ ei∈δ(v) xei = 0 ∑ e1∈δ(v) xe1 = ∑ en∈δ(v) xen = 0 i 6= n, v 6= s ∑ e1∈δ(s) xe1 = ∑ en∈δ(s) xen = 1 ∑ ei∈δ(s) xei = ∑ ej∈δ(s) xej = 0 i 6= 1, j 6= n x = [xei ] ∈ [0, 1] |V .",
      "startOffset" : 194,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "i ∑ ei∈δ(t) xei = 1, ∑ ej∈δ(t) xej = 0 ∀j x = [xei ] ∈ [0, 1] |V .",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "ei∈δ(v) xei ≤ bv, ∀ v ∈ V x = [xei ] ∈ [0, 1] |E| (23)",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Where, δ(v) is a set of edges connected to a vertex v, w = [wei : ei ∈ E ′] ∈ [0, 1]|E | and wi = 1 − iε.",
      "startOffset" : 78,
      "endOffset" : 84
    } ],
    "year" : 2017,
    "abstractText" : "Max-product belief propagation (BP) is a popular message-passing algorithm for computing a maximum a-posteriori (MAP) assignment in a joint distribution represented by a graphical model (GM). It was recently shown that BP can solve certain classes of Linear Programming (LP) formulations to combinatorial optimization problems including maximum weight matching and shortest path, i.e., BP can be a distributed solver for certain LPs. However, those LPs and corresponding BP analysis are very sensitive to underlying problem setups, and it has been not clear what extent these results can be generalized to. In this paper, we obtain a generic criteria such that BP converges to the correct solution of the desired LP. Our theoretical result not only rediscovers prior known ones for maximum weight matching and shortest path as special cases, but also can be applied to new problems including traveling salesman, longest path and vertex cover, i.e., BP is a distributed (and parallel) solver to the combinatorial optimization problems. We believe that our results provide new insights on BP performances and new directions on distributed solvers for certain classes of large-scale LPs.",
    "creator" : "LaTeX with hyperref package"
  }
}