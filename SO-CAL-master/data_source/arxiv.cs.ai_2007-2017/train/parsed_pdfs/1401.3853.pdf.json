{
  "name" : "1401.3853.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Implicit Abstraction Heuristics",
    "authors" : [ "Michael Katz", "Carmel Domshlak" ],
    "emails" : [ "dugi@tx.technion.ac.il", "dcarmel@ie.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Heuristic search, either through progression in the space of world states or through regression in the space of subgoals, is a common and successful approach to classical planning. It is probably the most popular approach to cost-optimal planning, that is, finding a plan with a minimal total cost of its actions. The difference between various heuristic-search algorithms for optimal planning is mainly in the admissible heuristic functions they employ. In state-space search, such a heuristic estimates the cost of achieving the goal from a given state and guarantees not to overestimate that cost.\nA useful heuristic function must be accurate as well as efficiently computable. Improving the accuracy of a heuristic function without substantially worsening the time complexity of computing it usually translates into faster search for optimal solutions. During the last decade, numerous computational ideas evolved into new admissible heuristics for classical planning; these include the delete-relaxing max heuristic hmax (Bonet & Geffner, 2001), critical path heuristics hm (Haslum & Geffner, 2000), landmark heuristics hL, hLA (Karpas & Domshlak, 2009) and hLM-cut (Helmert & Domshlak, 2009), and abstraction heuristics such\nc©2010 AI Access Foundation. All rights reserved.\nas pattern database heuristics (Edelkamp, 2001) and merge-and-shrink heuristics (Helmert, Haslum, & Hoffmann, 2007). Our focus in this work is on the abstraction heuristics.\nGenerally speaking, an abstraction of a planning task is given by a mapping α : S → Sα from the states of the planning task’s transition system to the states of some “abstract transition system” such that, for all states s, s′ ∈ S, the cost from α(s) to α(s′) is upperbounded by the cost from s to s′. The abstraction heuristic value hα(s) is then the cost from α(s) to the closest goal state of the abstract transition system. Perhaps the most well-known abstraction heuristics are pattern database (PDB) heuristics, which are based on projecting the planning task onto a subset of its state variables and then explicitly searching for optimal plans in the abstract space. Over the years, PDB heuristics have been shown to be very effective in several hard search problems, including cost-optimal planning (Culberson & Schaeffer, 1998; Edelkamp, 2001; Felner, Korf, & Hanan, 2004; Haslum, Botea, Helmert, Bonet, & Koenig, 2007). The conceptual limitation of these heuristics, however, is that the size of the abstract space and its dimensionality must be fixed.1 The more recent merge-andshrink abstractions generalize PDB heuristics to overcome the latter limitation (Helmert et al., 2007). Instead of perfectly reflecting just a few state variables, merge-and-shrink abstractions allow for imperfectly reflecting all variables. As demonstrated by the formal and empirical analysis of Helmert et al., this flexibility often makes the merge-and-shrink abstractions much more effective than PDBs. However, the merge-and-shrink abstract spaces are still searched explicitly, and thus they still have to be of fixed size. While quality heuristics estimates can still be obtained for many problems, this limitation is a critical obstacle for many others.\nOur goal in this paper is to push the envelope of abstraction heuristics beyond explicit abstractions. We introduce a principled way to obtain abstraction heuristics that limit neither the dimensionality nor the size of the abstract spaces. The basic idea behind what we call implicit abstractions is simple and intuitive: instead of relying on abstract problems that are easy to solve because they are small, we can rely on abstract problems belonging to provably tractable fragments of optimal planning. The key point is that, at least theoretically, moving to implicit abstractions removes the requirement on the abstractions size to be small. Our contribution, however, is in showing that implicit abstractions are far from being of theoretical interest only. Specifically,\n1. We specify acyclic causal-graph decompositions, a general framework for additive implicit abstractions that is based on decomposing the problem at hand along its causal graph. We then introduce a concrete family of such abstractions, called fork decompositions, that are based on two novel fragments of tractable cost-optimal planning. Following the type of analysis suggested by Helmert and Mattmüller (2008), we formally analyze the asymptotic performance ratio of the fork-decomposition heuristics and prove that their worst-case accuracy on selected domains is comparable with that of (even parametric) state-of-the-art admissible heuristics. We then empirically evaluate the accuracy of the fork-decomposition heuristics on a large set of domains from recent planning competitions and show that their accuracy is competitive with the state of the art.\n1. This does not necessarily apply to symbolic PDBs which, on some tasks, may exponentially reduce the PDB’s representation (Edelkamp, 2002).\n2. The key attraction of explicit abstractions is that state-to-goal costs in the abstract space can be precomputed and stored in memory in a preprocessing phase so that heuristic evaluation during search can be done by a simple lookup. A necessary condition for this would seem to be the small size of the abstract space. However, we show that an equivalent of the PDB and merge-and-shrink’s notion of “database” exists for the fork-decomposition abstractions as well, despite the exponential-size abstract spaces of the latter. These databased implicit abstractions are based on a proper partitioning of the heuristic computation into parts that can be shared between search states and parts that must be computed online per state. Our empirical evaluation shows that A∗ equipped with the “databased” fork-decomposition heuristics favorably competes with the state of the art of cost-optimal planning.\nThis work is a revision and extension of the formulation and results presented by Katz and Domshlak (2008, 2009), which in turn is based on ideas first sketched also by Katz and Domshlak (2007a)."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "We consider classical planning tasks corresponding to state models with a single initial state and only deterministic actions. Specifically, we consider state models captured by the sas+\nformalism (Bäckström & Nebel, 1995) with nonnegative action costs. Such a planning task is given by a quintuple Π = 〈V,A, I,G, cost〉, where:\n• V is a set of state variables, with each v ∈ V being associated with a finite domain D(v). For a subset of variables V ′ ⊆ V , we denote the set of assignments to V ′ by D(V ′) = ×v∈V ′D(v). Each complete assignment to V is called a state, and S = D(V ) is the state space of Π. I is an initial state. The goal G is a partial assignment to V ; a state s is a goal state iff G ⊆ s.\n• A is a finite set of actions. Each action a is a pair 〈pre(a), eff(a)〉 of partial assignments to V called preconditions and effects, respectively. By Av ⊆ A we denote the actions affecting the value of v. cost : A → R0+ is a real-valued, nonnegative action cost function.\nFor a variable v and a value ϑ ∈ D(v), instantiation of v by ϑ is denoted by v : ϑ. For a partial assignment p, V(p) ⊆ V denotes the subset of state variables instantiated by p. In turn, for any V ′ ⊆ V(p), by p[V ′] we denote the value of V ′ in p; if V ′ = {v} is a singleton, we use p[v] for p[V ′]. For any sequence of actions ρ and variable v ∈ V , by ρ↓v we denote the restriction of ρ to actions changing the value of v; that is, ρ↓v is the maximal subsequence of ρ consisting only of actions in Av.\nAn action a is applicable in a state s iff s[v] = pre(a)[v] for all v ∈ V(pre(a)). Applying a changes the value of v ∈ V(eff(a)) to eff(a)[v]. The resulting state is denoted by sJaK; by sJ〈a1, . . . , ak〉K we denote the state obtained from sequential application of the (respectively applicable) actions a1, . . . , ak starting at state s. Such an action sequence is an s-plan if G ⊆ sJ〈a1, . . . , ak〉K, and it is a cost-optimal (or, in what follows, optimal) s-plan if the sum of its action costs is minimal among all s-plans. The purpose of (optimal) planning is finding an (optimal) I-plan. For a pair of states s1, s2 ∈ S, by cost(s1, s2) we refer to the\ngoal is to deliver p1 from C to G and p2 from F to E using the cars c1, c2, c3 and truck t, making sure that c3 ends up at F . The cars may only use city roads (thin edges); the truck may only use the highway (thick edge). Figures (b), (c), and (d) depict, respectively, the causal graph of the problem, the domain transition graphs (labels omitted) of c1 and c2 (left), t (center), and c3 (right), and the identical domain transition graphs of of p1 and p2.\ncost of a cost-optimal plan from s1 to s2; h ∗(s) = mins′⊇G cost(s, s ′) is the custom notation for the cost of the optimal s-plan in Π. Finally, important roles in what follows are played by a pair of standard graphical structures induced by planning tasks.\n• The causal graph CG(Π) of Π is a digraph over nodes V . An arc (v, v′) is in CG(Π) iff v 6= v′ and there exists an action a ∈ A such that (v, v′) ∈ V(eff(a)) ∪ V(pre(a))× V(eff(a)). In this case, we say that (v, v′) is induced by a. By succ(v) and pred(v) we respectively denote the sets of immediate successors and predecessors of v in CG(Π).\n• The domain transition graph DTG(v,Π) of a variable v ∈ V is an arc-labeled digraph over the nodes D(v) such that an arc (ϑ, ϑ′) labeled with pre(a)[V \\ {v}] and cost(a) exists in the graph iff both eff(a)[v] = ϑ′, and either pre(a)[v] = ϑ or v 6∈ V(pre(a)).\nTo illustrate various constructs, we use a slight variation of a Logistics-style example from Helmert (2006). This example is depicted in Figure 1a, and in sas+ it has\nV = {p1, p2, c1, c2, c3, t} D(p1) = D(p2) = {A,B,C,D,E, F,G, c1, c2, c3, t} D(c1) = D(c2) = {A,B,C,D} D(c3) = {E,F,G} D(t) = {D,E}\nI = {p1 :C, p2 :F, t :E, c1 :A, c2 :B, c3 :G} G = {p1 :G, p2 :E, c3 :F},\nand actions corresponding to all possible loads and unloads, as well as single-segment movements of the vehicles. For instance, if action a captures loading p1 into c1 at C, then pre(a) = {p1 :C, c1 :C}, and eff(a) = {p1 : c1}. All actions in the example have unit cost. The causal graph of this example, as well as the domain transition graphs of the state variables, are depicted in Figures 1b-1d.\nHeuristic functions are used by informed-search procedures to estimate the cost (of the cheapest path) from a search node to the nearest goal node. Our focus here is on statedependent, admissible abstraction heuristics. A heuristic function h is state-dependent if its estimate for a search node depends only on the problem state associated with that node, that is, h : S → R0+ ∪ {∞}. Most heuristics in use these days are state-dependent (though see, e.g., Richter, Helmert, & Westphal, 2008 and Karpas & Domshlak, 2009 for a different case). A heuristic h is admissible if h(s) ≤ h∗(s) for all states s. If h1 and h2 are two admissible heuristics, and h2(s) ≤ h1(s) for all states s, we say that h1 dominates h2.\nFor any set of admissible heuristics h1, . . . , hm, their pointwise maximum is always an admissible heuristic, dominating each individual heuristic in the set. For some sets of admissible heuristics, their pointwise sum is also admissible and dominates their pointwise maximum. Many recent works on cost-optimal planning are based on additive ensembles of admissible heuristics, and this includes critical-path heuristics (Haslum, Bonet, & Geffner, 2005; Coles, Fox, Long, & Smith, 2008), pattern database heuristics (Edelkamp, 2001; Haslum et al., 2007), and landmark heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009). In particular, Katz and Domshlak (2007a, 2008) and Yang et al. (2007, 2008) independently introduced a general criterion for admissible additive ensembles of heuristics, called in the former work action cost partitioning. This criterion can be formalized as follows. Let Π = 〈V,A, I,G, cost〉 be a planning task and {costi : A → R0+}mi=1 a family of cost functions such that ∑m i=1 costi(a) ≤ cost(a) for all actions a ∈ A. If {hi}mi=1 is a set of arbitrary admissible heuristic functions for Πi = 〈V,A, I,G, costi〉, respectively, then ∑m i=1 hi is also an admissible heuristic for Π. The set of cost functions {costi}mi=1 can be seen as a partition of the action costs cost."
    }, {
      "heading" : "3. Abstractions and Abstraction Heuristics",
      "text" : "The semantics of any planning task Π is given by its induced state-transition model, often called the transition graph of Π.\nDefinition 1 A transition graph is a tuple T = (S,L, Tr, s0, S?, $) where S is a finite set of states, L is a finite set of transition labels, Tr ⊆ S × L × S is a set of (labeled) transitions, s0 ∈ S is an initial state, S? ⊆ S is a set of goal states, and $ : L→ R0+ is a transition cost function.\n• For a state s ∈ S and a subset of states S′ ⊆ S in T, cost(s, S′) is the cost (of a cheapest with respect to $ path) from s to a state in S′ along the transitions of T; if no state in S′ is reachable from s, then we have cost(s, S′) =∞.\n• Any path from s0 to S? is a plan for T, and cheapest such plans are called optimal.\nThe states of the transition graph T(Π) induced by a planning task Π = 〈V,A, I,G, cost〉 are the states of Π. The transition labels of T(Π) are the actions A; there is a transition (s, a, sJaK) ∈ Tr iff a is applicable in s; the initial state s0 = I; the set of goal states S? = {s ∈ S | s ⊇ G}; and the transition cost function $ = cost. We now proceed with formally specifying the notion of abstraction. Our definition of abstraction resembles that of Prieditis (1993), and right from the beginning we specify a more general notion of additive abstraction. Informally, by additive abstraction we refer to a set of abstractions interconstrained by a requirement to jointly not overestimate the transition-path costs in the abstracted transition graph.\nDefinition 2 An additive abstraction of a transition graph T = (S,L, Tr, s0, S?, $) is a set of pairs {〈Ti, αi〉}mi=1 where, for 1 ≤ i ≤ m,\n• Ti = (Si, Li, Tri, s0i , S?i , $i) is a transition graph,\n• αi : S → Si is a function, called abstraction mapping, such that\n– αi(s 0) = s0i , αi(s) ∈ S?i for all s ∈ S?, and, – for all pairs of states s, s′ ∈ S holds m∑ i=1 cost(αi(s), αi(s ′)) ≤ cost(s, s′). (1)\nA few words on why we use this particular notion of abstraction. The term “abstraction” is usually associated with simplifying the original system, reducing and factoring out details less crucial in the given context. Which details can be reduced and which should better be preserved depends, of course, on the context. For instance, in the context of formal verification, the abstract transition graphs are required not to decrease the reachability between the states; that is, if there is a path from s to s′ in the original transition graph, then there should be a path from α(s) to α(s′) in the abstract transition graph (Clarke, Grumberg, & Peled, 1999). In addition, the reachability should also be increased as little as possible. Beyond that, the precise relationship between the path costs in the original and abstract transition graphs is only of secondary importance. In contrast, when abstractions are designed to induce admissible heuristic functions for heuristic search, the relationship between the path costs as captured by Eq. 1 is what must be obeyed. However, requirements above and beyond the general requirement of Eq. 1 not to overestimate the distances between\nthe states are unnecessary. Hence, in particular, Definition 2 generalizes the notion of abstraction by Helmert et al. (2007) by replacing the condition of preserving individual transitions and their labels, that is, (α(s), l, α(s′)) if (s, l, s′), with a weaker condition stated in Eq. 1. The reader, of course, may well ask whether the generality of the condition in Eq. 1 beyond the condition of Helmert et al. (2007) really delivers any practical gain, and later we show that the answer to this question is affirmative. For now, we proceed with adding further requirements essential to making abstraction usable as a basis for heuristic functions.\nDefinition 3 Let Π be a planning task over states S, and let {〈Ti, αi〉}mi=1 be an additive abstraction of the transition graph T(Π). If m = O(poly(||Π||)) and, for all states s ∈ S and all 1 ≤ i ≤ m, the cost cost(αi(s), S?i ) in Ti is computable in time O(poly(||Π||)), then hA(s) = ∑m i=1 cost(αi(s), S ? i ) is an abstraction heuristic function for Π.\nNote that admissibility of hA is implied by the cost conservation condition of Eq. 1. To further illustrate the connection between abstractions and admissible heuristics, consider three well-known mechanisms for devising admissible planning heuristics: delete relaxation (Bonet & Geffner, 2001), critical-path relaxation (Haslum & Geffner, 2000),2 and pattern database heuristics (Edelkamp, 2001).\nFirst, while typically not considered this way, the delete relaxation of a planning task Π = 〈V,A, I,G, cost〉 does correspond to an abstraction 〈T+ = (S+, L+, Tr+, s0+, S?+, $+), α+〉 of the transition graph T(Π). Assuming unique naming of the variable values in Π and denoting D+ = ⋃ v∈V D(v), we have the abstract states S+ being the power-set of D+, and the labels L+ = {a, a+ | a ∈ A}. The transitions come from two sources: for each abstract state s+ ∈ S+ and each original action a ∈ A applicable in s+, we have both (s+, a, s+JaK) ∈ Tr+ and (s+, a+, s+ ∪ eff(a)) ∈ Tr+. With a minor abuse of notation, the initial state and the goal states of the abstraction are s0+ = I and S ? + = {s+ ∈ S+ | s+ ⊇ G}, and the abstraction mapping α+ is simply the identity function. It is easy to show that, for any state s of our planning task Π, we have cost(α+(s), S ? +) = h\n+(s), where h+(s) is the delete-relaxation estimate of the cost from s to the goal. As an aside, we note that this “delete-relaxation abstraction” 〈T+, α+〉 in particular exemplifies that nothing in Definition 2 requires the size of the abstract state space to be limited by the size of the original state space. In any event, however, the abstraction 〈T+, α+〉 does not induce a heuristic in terms of Definition 3 because computing h+(s) is known to be NP-hard (Bylander, 1994).\nThe situation for critical-path relaxation is exactly the opposite. While computing the corresponding family of admissible estimates hm is polynomial-time for any fixed m, this computation is not based on computing the shortest paths in an abstraction of the planning task. The state graph over which hm is computed is an AND/OR-graph (and not an OR-graph such as transition graphs), and the actual computation of hm corresponds to computing a critical tree (and not a shortest path) to the goal. To the best of our knowledge, the precise relation between critical path and abstraction heuristics is currently an open question (Helmert & Domshlak, 2009).\nOverall, the only abstraction heuristics in the toolbox of planning these days appear to be the explicit homomorphism abstractions, whose best-known representative is probably\n2. We assume the reader is familiar with these two relaxations. If not, their discussion here can be safely skipped.\nthe pattern database (PDB) heuristics. Given a planning task Π over state variables V , a PDB heuristic is based on projecting Π onto a subset of its variables V α ⊆ V . Such a homomorphism abstraction α maps two states s1, s2 ∈ S into the same abstract state iff s1[V α] = s2[V α]. Inspired by the (similarly named) domain-specific heuristics for search problems such as (n2 − 1)-puzzles or Rubik’s Cube (Culberson & Schaeffer, 1998; Hernadvölgyi & Holte, 1999; Felner et al., 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007). The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space Sα and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in Sα by exhaustive search. Moreover, an O(1) bound on |Sα| is typically set explicitly to fit the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |Sα| implies a fixed-dimensionality constraint |V α| = O(1). In planning tasks with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {Πi}∞i=1 is a sequence of Logistics problems of growing size with |Vi| = i. If each package in Πi can be transported by some Θ(i) vehicles, then starting from some i, hα will not account at all for movements of vehicles essential for solving Πi (Helmert & Mattmüller, 2008).\nAiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology of Dräger, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink (MS) abstractions for planning. MS abstractions are homomorphisms that generalize PDB abstractions by allowing for more flexibility in selection of pairs of states to be contracted. The problem’s state space is viewed as the synchronized product of its projections onto the single state variables. Starting with all such “atomic” abstractions, this product can be computed by iteratively composing two abstract spaces, replacing them with their product. While in a PDB the size of the abstract space Sα is controlled by limiting the number of product compositions, in MS abstractions it is controlled by interleaving the iterative composition of projections with abstraction of the partial composites. Helmert et al. (2007) have proposed a concrete strategy for this interleaved abstraction/refinement scheme and empirically demonstrated the power of the merge-and-shrink abstraction heuristics. Like PDBs, however, MS abstractions are explicit abstractions, and thus computing their heuristic values is also based on explicitly searching for optimal plans in the abstract spaces. Hence, while merge-and-shrink abstractions escape the fixed-dimensionality constraint of PDBs, the constraint on the abstract space to be of a fixed size still holds."
    }, {
      "heading" : "4. Implicit Abstractions",
      "text" : "Focusing on the O(1) bound posted by explicit abstractions on the size of the abstract space, our first observation is that explicit abstractions are not necessarily the only way to proceed with abstraction heuristics. Given a planning task Π over states S, suppose we can transform it into a different planning task Πα such that\n1. the transformation induces an abstraction mapping α : S → Sα where Sα is the state space of Πα, and\n2. both the transformation of Π to Πα, as well as computing α for any state in S, can be done in time polynomial in ||Π||.\nHaving such planning-task-to-planning-task transformations in mind, we define what we call (additive) implicit abstractions.\nDefinition 4 An additive implicit abstraction of a planning task Π is a set of pairs A = {〈Πi, αi〉}mi=1 such that {Πi}mi=1 are some planning tasks and {〈T(Πi), αi〉}mi=1 is an additive abstraction of T(Π).\nLet us now examine the notion of implicit abstractions more closely. First, implicit abstractions allow for a natural additive combination of admissible heuristics for the abstract tasks. This composition is formulated below by Theorem 1, extending the original criterion for admissibility of additive heuristics described in Section 2. Second, as formulated by Theorem 2, implicit abstractions can be composed via the functional composition of their abstraction mappings. These two easy-to-prove properties of implicit abstractions allow us then to take the desired step from implicit abstractions to implicit abstraction heuristics.\nTheorem 1 (Admissibility) Let Π be a planning task and A = {〈Πi, αi〉}mi=1 be an additive implicit abstraction of Π. If, for each 1 ≤ i ≤ m, hi is an admissible heuristic for Πi, then the function h(s) = ∑m i=1 hi(αi(s)) is an admissible heuristic for Π.\nProof: The proof is straightforward. Let T = (S,L, Tr, s0, S?, $) be the transition graph of Π, and let s be some state in S. For each 1 ≤ i ≤ m, let Ti = (Si, Li, Tri, s0i , S?i , $i) be the transition graph of Πi.\nFirst, if hi is an admissible heuristic for Πi, then for all si ∈ S?i ,\nhi(αi(s)) ≤ cost(αi(s), si).\nNow, for each state s′ ∈ S?, from Definition 2 we have αi(s′) ∈ S?i , and from Eq. 1 we have m∑ i=1 cost(αi(s), αi(s ′)) ≤ cost(s, s′),\nand thus\nh(s) = m∑ i=1 hi(αi(s)) ≤ m∑ i=1 cost(αi(s), αi(s ′)) ≤ cost(s, s′),\ngiving us an admissible estimate for h∗(s).\nTheorem 2 (Composition) Let Π be a planning task and A = {〈Πi, αi〉}mi=1 be an additive implicit abstraction of Π. If, for each 1 ≤ i ≤ m, Ai = {〈Πi,j , αi,j〉}mij=1 is an additive implicit abstraction of Πi, then A′ = ⋃m i=1{〈Πi,j , αi,j ◦ αi〉}mij=1 is an additive implicit abstraction of Π.\nProof: Let T = (S,L, Tr, s0, S?, $) be the transition graph of Π. For each 1 ≤ i ≤ m, let Ti = (Si, Li, Tri, s0i , S?i , $i) be the transition graph of Πi, and for each 1 ≤ j ≤ mi, let Ti,j = (Si,j , Li,j , Tri,j , s0i,j , S?i,j , $i,j) be the transition graph of Πi,j . We need to show that αi,j ◦ αi is an abstraction mapping as in Definition 2. From αi and αi,j being abstraction mappings, we have\n• s0i,j = αi,j(s0i ) = αi,j(αi(s0)) = αi,j ◦ αi(s0),\n• for all s ∈ S? we have αi(s) ∈ S?i and thus αi,j(αi(s)) = αi,j ◦ αi(s) ∈ S?i,j , and • for all si, s′i ∈ Si, cost(si, s′i) ≥ ∑mi j=1 cost(αi,j(si), αi,j(s ′ i)), and thus for all s, s ′ ∈ S,\ncost(s, s′) ≥ m∑ i=1 cost(αi(s), αi(s ′)) ≥ m∑ i=1 mi∑ j=1 cost(αi,j(αi(s)), αi,j(αi(s ′)))\n= m∑ i=1 mi∑ j=1 cost(αi,j ◦ αi(s), αi,j ◦ αi(s′)).\nTogether, Theorems 1 and 2 suggest the following scheme for deriving abstraction heuristics. Given an additive implicit abstraction A = {〈Πi, αi〉}mi=1, if all its individual abstract tasks belong to some tractable fragments of optimal planning, then we can use in practice the (sum of the) true costs in all Πi as the admissible estimates for the costs in Π. Otherwise, if optimal planning for some abstract tasks Πi in A cannot be proven polynomial-time solvable, then we can further abstract these tasks, obtaining admissible estimates for the true costs in Πi.\nDefinition 5 Let Π be a planning task over states S, and let A = {〈Πi, αi〉}mi=1 be an additive implicit abstraction of Π. If m = O(poly(||Π||)), and, for all states s ∈ S and all 1 ≤ i ≤ m, h∗(αi(s)) is polynomial-time computable, then hA(s) = ∑m i=1 h\n∗(αi(s)) is an implicit abstraction heuristic function for Π.\nCompared to explicit abstraction heuristics such as PDB heuristics and merge-andshrink heuristics, the direction of implicit abstraction heuristics is, at least in principle, appealing because neither the dimensionality nor even the size of the state spaces induced by implicit abstractions are required to be bounded by something restrictive, if at all. The pitfall, however, is that implicit abstraction heuristics correspond to tractable fragments of optimal planning, and the palette of such known fragments is extremely limited (Bäckström & Nebel, 1995; Bylander, 1994; Jonsson & Bäckström, 1998; Jonsson, 2007; Katz & Domshlak, 2007b). In fact, none so far has appeared to us very convenient for automatically devising useful problem transformations as above. Fortunately, we show next that the boundaries of tractability can be expanded in the right way, allowing us to successfully materialize the idea of implicit abstraction heuristics.\nIn the following, a key role is played by the causal graphs induced by the planning tasks. Informally, the basic idea behind what we call causal-graph decompositions is to abstract the given planning task Π along a subgraph of Π’s causal graph, with the goal of obtaining abstract problems of specific structure. Naturally, there are numerous possibilities for obtaining such structure-oriented abstractions. We now present one such decomposition that is tailored to abstractions around acyclic subgraphs. Informally, this decomposition can be seen as a sequential application of two kinds of task transformations: dropping preconditions (Pearl, 1984) and (certain form of) breaking actions with conjunctive effects into single-effect actions.\nDefinition 6 Let Π = 〈V,A, I,G, cost〉 be a planning task, and let G = (VG , EG) be an acyclic subgraph of the causal graph CG(Π). A planning task ΠG = 〈VG , AG , IG , GG , costG〉 is an acyclic causal-graph decomposition of Π with respect to G if\n1. IG = I[VG ], GG = G[VG ], 2. AG = ⋃ a∈AAG(a) where each AG(a) = {a1, . . . , al(a)} is a set of actions over VG\nsuch that, for a topological with respect to G ordering of the variables {v1, . . . , vl(a)} = V(eff(a)) ∩ VG, and 1 ≤ i ≤ l(a),\neff(ai)[v] =\n{ eff(a)[v], v = vi\nunspecified, otherwise\npre(ai)[v] =  pre(a)[v], (v, vi) ∈ EG ∧ v 6∈ V(eff(a)) or v = vi eff(a)[v], (v, vi) ∈ EG ∧ v ∈ V(eff(a)) unspecified, otherwise\n(2)\n3. For each action a ∈ A, ∑ a′∈AG(a) costG(a ′) ≤ cost(a). (3)\nIt is not hard to verify from Definition 6 that for any planning task Π and any acyclic causal-graph decomposition ΠG of Π, the causal graph CG(ΠG) is exactly the subgraph G underlying the decomposition. To illustrate the notion of acyclic causal-graph decomposition, we consider a planning task Π = 〈V,A, I,G, cost〉 over five state variables V = {u, v, x, y, z}, two unit-cost actions A = {a1, a2} as in Figure 2a, initial state I = {u :0, v :0, x :0, y :0, z :0}, and goal G = {u : 1, v : 1, x : 0, y : 1, z : 1}. The causal graph CG(Π) is depicted in Figure 2a. Figures 2b-c show two subgraphs G1 and G2 of CG(Π), respectively, as well as the action sets AG1(a1) = {a11, a21, a31} and AG1(a2) = {a12, a22, a32} in Figure 2(b), and the action sets AG2(a1) = {a11, a21, a31} and AG2(a2) = {a12, a22, a32} in Figure 2(c). For i ∈ {1, 2}, let Πi = 〈V,Ai, I, G, costi〉 be the planning task with Ai = AGi(a1)∪AGi(a2) and costi(a) = 1/3 for all a ∈ Ai. These two planning tasks Πi (individually) satisfy the conditions of Definition 6 with respect to Π and Gi, and thus they are acyclic causal-graph decompositions of Π with respect to Gi.\nWe now proceed with specifying implicit abstractions defined via acyclic causal-graph decompositions.\nDefinition 7 Let Π = 〈V,A, I,G, cost〉 be a planning task over states S, and let G = {Gi = (VGi , EGi)}mi=1 be a set of acyclic subgraphs of the causal graph CG(Π). A = {〈ΠGi , αi〉}mi=1 is an acyclic causal-graph abstraction of Π over G if, for some set of cost functions {costi : A→ R0+}mi=1 satisfying\n∀a ∈ A : m∑ i=1 costi(a) ≤ cost(a), (4)\nwe have, for 1 ≤ i ≤ m,\n• ΠGi = 〈VGi , AGi , IGi , GGi , costGi〉 is an acyclic causal-graph decomposition of Πi = 〈V,A, I,G, costi〉 with respect to Gi, and\n• the abstraction mapping αi : S → Si is the projection mapping αi(s) = s[VGi ].\nTheorem 3 Acyclic causal-graph abstractions of the planning tasks are additive implicit abstractions of these tasks.\nProof: Let Π = 〈V,A, I,G, cost〉 be a planning task, and let A = {〈ΠGi , αi〉}mi=1 be an acyclic causal-graph abstraction of Π over a set of subgraphs G = {Gi = (VGi , EGi)}mi=1. Let T = (S,L, Tr, s0, S?, $) be the transition graph of Π, and, for 1 ≤ i ≤ m, Ti = (Si, Li, Tri, s 0 i , S ? i , $i) be the transition graph of ΠGi . We need to show that αi is an abstraction mapping as in Definition 2. First, from Definitions 6 and 7, we have\n• s0i = IGi = I[VGi ] = s0[VGi ] = αi(s0), and\n• for all s ∈ S? we have s ⊇ G and thus αi(s) = s[VGi ] ⊇ G[VGi ] = GGi , providing us with αi(s) ∈ S?i .\nNow, if s is a state of Π and a ∈ A is an action with pre(a) ⊆ s, then αi(s) is a state of ΠGi and pre(a)[VGi ] ⊆ αi(s). Let the action sequence ρ = 〈a1, a2, . . . , al(a)〉 be constructed from a as in Eq. 2. We inductively prove that ρ is applicable in αi(s). First, for each v ∈ VGi , either pre(a1)[v] = pre(a)[v], or pre(a1)[v] is unspecified, and thus ρ1 = 〈a1〉 is applicable in αi(s). The inductive hypothesis is now that ρj = 〈a1, a2, . . . , aj〉 is applicable in αi(s), and let s′ = αi(s)JρjK. From Eq. 2, for each 1 ≤ j′ ≤ j, aj′ changes the value of vj′ to eff(a)[vj′ ],\nand that is the only change of vj′ along ρj . Likewise, since all the actions constructed as in Eq. 2 are unary-effect, {v1, . . . , vj} are the only variables in VGi affected along ρj . Hence, for all v ∈ VGi , if v = vj′ , 1 ≤ j′ ≤ j, then s′[v] = eff(a)[v] = pre(aj+1)[v], and otherwise, s′[v] = αi(s)[v], and if pre(a\nj+1)[v] is specified, then pre(aj+1)[v] = pre(a)[v] = αi(s)[v]. This implies that aj+1 is applicable in s′ and, as a result, ρj+1 = 〈a1, a2, . . . , aj+1〉 is applicable in αi(s), finalizing the inductive proof. Likewise, exactly the same arguments on the affect of {aj}l(a)j=1 on αi(s) immediately imply that, if ρ = 〈a1, a2, . . . , al(a)〉, then αi(sJaK) = αi(s)JρK. Next, for each a ∈ A, from Eqs. 3 and 4 we have\nm∑ i=1 ∑ a′∈AGi (a) costGi(a ′) ≤ m∑ i=1 costi(a) ≤ cost(a). (5)\nNow, let s, s′ ∈ S be a pair of original states such that cost(s, s′) < ∞, and let % = 〈a1, . . . , ak〉 be the sequence of labels along a cheapest path from s to s′ in T. From that, cost(s, s′) = cost(%) = ∑k j=1 cost(aj). The decomposition of such a path to the sequences of actions as in Eq. 2 is a (not neccesarily cheapest) path from αi(s) to αi(s ′) in Ti, and\nthus cost(αi(s), αi(s ′)) ≤∑kj=1∑a′∈AGi (aj) costGi(a′), providing us with\nm∑ i=1 cost(αi(s), αi(s ′)) ≤ m∑ i=1 k∑ j=1 ∑ a′∈AGi (aj) costGi(a ′) = k∑ j=1 m∑ i=1 ∑ a′∈AGi (aj) costGi(a ′)\n(5) ≤ k∑ j=1 cost(aj) = cost(s, s ′).\nThus, if we can decompose the given task Π into a set of tractable acyclic causalgraph decompositions Π = {ΠG1 , . . . ,ΠGm}, then we can solve all these tasks in polynomial time, and derive an additive admissible heuristic for Π. Before we proceed with considering concrete acyclic causal-graph decomposition, note that Definition 2 leaves the decision about the actual partition of the action costs rather open. In what follows we adopt the most straightforward, uniform action cost partition in which the cost of each action a is equally split among all the non-redundant representatives of a in ⋃m i=1AGi(a). However, a better choice of action cost partition can sometimes be made. In fact, sometimes it can even be optimized (Katz & Domshlak, 2010)"
    }, {
      "heading" : "5. Fork Decompositions",
      "text" : "We now proceed with introducing two concrete acyclic causal-graph decompositions that, when combined with certain variable domain abstractions, provide us with implicit abstraction heuristics. These so called fork-decomposition heuristics are based on two novel fragments of tractable cost-optimal planning for tasks with fork and inverted-fork structured causal graphs.\nDefinition 8 For a planning task Π over variables V , and a variable v ∈ V ,\n(1) v-fork of Π is the subgraph Gfv of CG(Π) over nodes VGfv = {v} ∪ succ(v) and edges EGfv = {(v, u) | u ∈ succ(v)}, and\n(2) v-ifork (short for inverted fork) of Π is a subgraph G iv of CG(Π) over nodes VG iv = {v} ∪ pred(v) and edges EG iv = {(u, v) | u ∈ pred(v)}.\nThe sets of all v-forks and all v-iforks of Π are denoted by GF = {Gfv}v∈V and GI = {G iv}v∈V , respectively.\nFor any planning task and each of its state variables v, both v-fork and v-ifork are acyclic digraphs, allowing us to define our three implicit abstractions as follows.\nDefinition 9 For any planning task Π = 〈V,A, I,G, cost〉, (1) any acyclic causal-graph abstraction AF = {〈Πfv, αfv〉}v∈V of Π over GF is called\nF-abstraction, and the set of abstract planning tasks ΠF = {Πfv}v∈V is called F-decomposition of Π;\n(2) any acyclic causal-graph abstraction AI = {〈Πiv, αiv〉}v∈V of Π over GI is called I-abstraction, and the set of abstract planning tasks ΠI = {Πiv}v∈V is called I-decomposition of Π;\n(3) any acyclic causal-graph abstraction AFI = {〈Πfv, αfv〉, 〈Πiv, αiv〉}v∈V of Π over GFI = GF ∪GI is called FI-abstraction, and the set of abstract planning tasks ΠFI = {Πfv,Πiv}v∈V is called FI-decomposition of Π. Definition 9 can be better understood by considering the FI-abstraction of the problem\nΠ from our Logistics example; Figure 3 schematically illustrates the process. To simplify the example, here we as if eliminate from GFI all the single-node subgraphs, obtaining\nAFI = {〈Πfc1 , αfc1〉, {〈Πfc2 , αfc2〉, {〈Πfc3 , αfc3〉, {〈Πft, αft〉, {〈Πip1 , αip1〉, {〈Πip2 , αip2〉}.\nConsidering the action sets of the problems in ΠFI = {Πfc1 ,Πfc2 ,Πfc3 ,Πft,Πip1 ,Πip2}, we see that each original driving action has one nonredundant (that is, “changing some variable”) representative in three of the abstract planning tasks, while each load/unload action has one nonredundant representative in five of these tasks. For instance, the action drive-c1from-A-to-D has one nonredundant representative in each of the tasks {Πfc1 ,Πip1 ,Πip2}, and the action load-p1-into-c1-at-A has one nonredundant representative in each of the tasks {Πfc1 ,Πfc2 ,Πfc3 ,Πft,Πip1}. Since we assume a uniform partition of the action costs, the cost of each driving and load/unload action in each relevant abstract planning task is thus set to 1/3 and 1/5, respectively. From Theorem 3 we have AFI being an additive implicit abstraction of Π, and from Theorem 1 we then have\nhFI = ∑ v∈V ( h∗Πfv + h∗Πiv ) , (6)\nbeing an admissible estimate of h∗ in Π. The question now is how good this estimate is. The optimal cost of solving our running example is 19. Taking as a reference the well-known admissible heuristics hmax (Bonet & Geffner, 2001) and h\n2 (Haslum & Geffner, 2000), we have hmax(I) = 8 and h\n2(I) = 13. Considering our FI-abstraction, the optimal plans for the tasks in ΠFI are as follows.\nΠfc1 : load-p1-into-c2-at-C, unload-p1-from-c2-at-D, load-p1-into-t-at-D, unload-p1-from-t-at-E, load-p1-into-c3-at-E, unload-p1-from-c3-at-G, load-p2-into-c3-at-F, unload-p2-from-c3-at-E.\nΠfc2 : load-p1-into-c1-at-C, unload-p1-from-c1-at-D, load-p1-into-t-at-D, unload-p1-from-t-at-E, load-p1-into-c3-at-E, unload-p1-from-c3-at-G, load-p2-into-c3-at-F, unload-p2-from-c3-at-E.\nΠfc3 : load-p1-into-c1-at-C, unload-p1-from-c1-at-D, load-p1-into-t-at-D, unload-p1-from-t-at-E, drive-c3-from-G-to-E, load-p1-into-c3-at-E, drive-c3-from-E-to-G, unload-p1-from-c3-at-G, drive-c3-from-G-to-E, drive-c3-from-E-to-F, load-p2-into-c3-at-F, drive-c3-from-F-to-E, unload-p2-from-c3-at-E, drive-c3-from-E-to-F.\nΠft : load-p1-into-c1-at-C, unload-p1-from-c1-at-D, drive-t-from-E-to-D, load-p1-into-t-at-D, drive-t-from-D-to-E, unload-p1-from-t-at-E, load-p1-into-c3-at-E, unload-p1-from-c3-at-G, load-p2-into-c3-at-F, unload-p2-from-c3-at-E.\nΠip1 : drive-c1-from-A-to-D, drive-c1-from-D-to-C, load-p1-into-c1-at-C, drive-c1-from-C-to-D, unload-p1-from-c1-at-D, drive-t-from-E-to-D, load-p1-into-t-at-D, drive-t-from-D-to-E, unload-p1-from-t-at-E, drive-c3-from-G-to-E, load-p1-into-c3-at-E, drive-c3-from-E-to-G, unload-p1-from-c3-at-G, drive-c3-from-G-to-E, drive-c3-from-E-to-F.\nΠip2 : drive-c3-from-G-to-E, drive-c3-from-E-to-F, load-p2-into-c3-at-F, drive-c3-from-F-to-E, unload-p2-from-c3-at-E, drive-c3-from-E-to-F.\nHence, we have\nhFI = h∗ Πfc1 + h∗ Πfc2 + h∗ Πfc3 + h∗ Πft\n+ h∗ Πip1 + h∗ Πfp2\n= 85 + 8 5 + 8 5 + 6 3 + 8 5 + 2 3 + 6 5 + 9 3 + 2 5 + 4 3 = 15,\n(7)\nand so hFI appears at least promising.\nUnfortunately, despite the seeming simplicity of the planning tasks in ΠFI, it turns out that implicit fork-decomposition abstractions as in Definitions 9 do not fit the requirements of implicit abstraction heuristics as in Definition 5. The causal graphs of the planning tasks in ΠF and ΠI form directed forks and directed inverted forks, respectively, and, in general, the number of variables in each such planning task can be as large as Θ(|V |). The problem is that even satisficing planning for sas+ fragments with fork and inverted fork causal graphs is NP-complete (Domshlak & Dinitz, 2001). In fact, recent results by Chen and Gimenez (2008) show that planning for any sas+ fragment characterized by any nontrivial form of causal graph is NP-hard. Moreover, even if the domain transition graphs of all the state variables are strongly connected (as in our example), optimal planning for fork and inverted fork structured problems remain NP-hard (see Helmert 2003, and 2004 for the respective results). Next, however, we show that this is not the end of the story for fork decompositions.\nWhile the hardness of optimal planning for problems with fork and inverted fork causal graphs casts a shadow on the relevance of fork decompositions, a closer look at the proofs of the corresponding hardness results of Domshlak and Dinitz (2001) and Helmert (2003, 2004) reveals that they in particular rely on root variables having large domains. Exploiting this observation, we now show that this reliance is not incidental and characterize two substantial islands of tractability within the structural fragments of sas+.\nTheorem 4 (Tractable Forks) Given a planning task Π = 〈V,A, I,G, cost〉 with a fork causal graph rooted at r ∈ V , if |D(r)| = 2, the time complexity of the cost-optimal planning for Π is polynomial in ||Π||.\nProof: Observe that, for any planning task Π as in the theorem, the fork structure of the causal graph CG(Π) implies that all the actions in Π are unary-effect, and each leaf variable v ∈ succ(r) preconditions only the actions affecting v itself. The algorithm below is based on the following three properties satisfied by the optimal plans ρ for Π.\n(i) For any leaf variable v ∈ succ(r), the path ρ↓v from I[v] to G[v] induced by ρ in DTG(v,Π) is either cycle-free or contains only zero-cost cycles. This is the case because otherwise all the nonzero-cost cycles can be eliminated from ρ↓v while preserving its validity, violating the assumed optimality of ρ. Without loss of generality, in what follows we assume that this path ρ↓v in DTG(v,Π) is cycle-free; in the case of fork causal graphs, we can always select an optimal ρ that satisfies this requirement for all v ∈ succ(r). Thus, we have |ρ↓v | ≤ |D(v)| − 1.\n(ii) Having fixed a sequence of value changes of r, the fork’s leaves become mutually independent; that is, our ability to change the value of one of them does not affect our ability to change the value of all the others.\n(iii) Because r is binary-valued, if v ∈ V \\ {r} is the “most demanding” leaf variable in terms of the number of value changes required from r by the action preconditions along ρ↓v , then these are the only value changes of r along ρ, except for, possibly, a final value change to G[r]. Thus, in particular, we have |ρ↓r | ≤ maxv∈succ(r) |D(v)|.\nWe begin with introducing some auxiliary notations. With |D(r)| = 2, let D(r) = {0, 1} with I[r] = 0. Let σ(r) be an alternating 0/1 sequence starting with 0, and having 0 in all odd and 1 in all even positions. This sequence σ(r) is such that |σ(r)| = 1 if no action in A can change r’s value to 1, |σ(r)| = 2 if some action can change r’s value to 1 but no action can then restore it to value 0, and otherwise, |σ(r)| = 1 + maxv∈succ(r) |D(v)|. Let [σ(r)] be the set of all nonempty prefixes of σ(r) if G[r] is unspecified; otherwise, let it be the set of all nonempty prefixes of σ(r) ending with G[r]. Note that, if [σ(r)] = ∅, then the problem is trivially unsolvable; in what follows we assume this is not the case. For each v ∈ succ(r), let DTG0v and DTG1v be the subgraphs of the domain transition graphs DTG(v,Π), obtained by removing from DTG(v,Π) all the arcs labeled with r : 1 and r : 0, respectively.\nThe algorithm below incrementally constructs a set R of valid plans for Π, starting with R = ∅.\n(1) For each v ∈ succ(r), and each pair of v’s values x, y ∈ D(v), compute the cheapest (that is, cost-minimal) paths π0v(x, y) and π 1 v(x, y) from x to y in DTG 0 v and DTG 1 v,\nrespectively. For some pairs of values x, y, one or even both these paths may, of course, not exist.\n(2) For each sequence σ ∈ [σ(r)], and each v ∈ succ(r), construct a layered digraph Lv(σ) with |σ|+ 1 node layers L0, . . . , L|σ|, where L0 consists of only I[v], and for 1 ≤ i ≤ |σ|, Li consists of all nodes y ∈ D(v) for which a path πσ[i]v (x, y) from some node x ∈ Li−1 has been constructed in step (1). For each x ∈ Li−1, y ∈ Li, Lv(σ) contains an arc (x, y) weighted with cost(π σ[i] v (x, y)).\n(3) For each σ ∈ [σ(r)], let k = |σ|. A candidate plan ρσ for Π is constructed as follows.\n(a) For each v ∈ succ(r), find a cost-minimal path from I[v] to G[v] in Lv(σ). If no such path exists, then proceed with the next prefix in [σ(r)]. Otherwise, note that the i-th edge on this path (taking us from some x ∈ Li−1 to some y ∈ Li) corresponds to the cost-minimal path π σ[i] v (x, y) from x to y. Let us denote this path from x to\ny by Siv.\n(b) SetR = R∪{ρσ}, where ρσ = S1 ·aσ[2] ·S2 ·. . .·aσ[k] ·Sk, each sequence Si is obtained by an arbitrary merge of the sequences {Siv}v∈succ(r), and aϑ is the cheapest action changing the value of r to value ϑ.\n(4) If R = ∅, then fail, otherwise return ρ = argminρσ∈R cost(ρσ).\nIt is straightforward to verify that the complexity of the above procedure is polynomial in the description size of Π. To prove correctness, we show that the procedure returns a plan for any solvable task Π, and that the returned plan ρ′ satisfies cost(ρ′) ≤ cost(ρ) for any optimal plan ρ for Π.\nGiven a solvable task Π, let ρ be an optimal plan for Π with all ρ↓v for the leaf variables v being cycle-free. Let ρ↓r = 〈a2 . . . , ak〉; the numbering of actions along ρ↓r starts with a2 to simplify indexing later on. For each v ∈ succ(r), the actions of ρ↓r divide ρ↓v into subsequences of v-changing actions ρ↓v = ρ 1 v · . . . · ρkv , separated by the value changes required from r. That is, for each 1 ≤ i ≤ k, all actions in ρiv are preconditioned by the same value of r, if any, and if two actions a ∈ ρiv and a′ ∈ ρi+1v are preconditioned by r, then pre(a)[r] 6= pre(a′)[r]. Let σ ∈ [σ(r)] be a value sequence such that |σ| = k = |ρ↓r |+1. For each v ∈ succ(r), ρ↓v is a path from I[v] to G[v] in Lv(σ), and therefore some ρσ is added into R by the algorithm, meaning that the algorithm finds a solution. Now, if ρσ ∈ R, then, for each v ∈ succ(r), let S1v · S2v · . . . · Skv be a cost-minimal path from I[v] to G[v] in Lv(σ) such that Siv is the sequence of actions changing the value of v and preconditioned either by r :0 or nothing for odd i, and by r :1 or nothing for even i. Thus,\ncost(S1v · S2v · . . . · Skv ) = k∑ i=1 cost(Siv) ≤ cost(ρ↓v).\nBecause sequence Si is obtained by an arbitrary merge of the sequences {Siv}v∈succ(r), and aϑ is the cheapest action changing the value of r to ϑ, then ρσ = S\n1 · aσ[2] ·S2 · . . . · aσ[k] ·Sk is an applicable sequence of actions that achieves the goal values for each v ∈ succ(r) as well as for r, and\ncost(ρσ) = cost(S 1 · aσ[2] · S2 · . . . · aσ[k] · Sk) = k∑ i=2 cost(aσ[i]) + k∑ i=1 cost(Si) ≤\n≤cost(ρ↓r) + ∑\nv∈succ(r)\ncost(ρ↓v) = cost(ρ).\nHence, if Π is solvable, then the algorithm returns a plan for Π, and this plan must be optimal. Finally, if Π is not solvable, then R necessarily remains empty, and thus the algorithm fails.\nWhile Theorem 4 concerns the tractability tasks with fork-structured causal graphs and roots with binary domains, in our earlier work we also reported an additional tractability result for fork-structured causal graphs with the domains of all variables being of a fixed size, though not necessarily binary-valued (Katz & Domshlak, 2008). We do not discuss this result here in detail because, at least so far, we have not found it very helpful in the context of devising effective abstraction heuristics.\nTheorem 5 (Tractable Inverted Forks) Given a planning task Π = 〈V,A, I,G, cost〉 with an inverted fork causal graph with sink r ∈ V , if |D(r)| = O(1), the time complexity of the cost-optimal planning for Π is polynomial in ||Π||.\nProof: Let |D(r)| = d. Observe that the inverted-fork structure of the causal graph CG(Π) implies all the actions in Π are unary-effect, and that the sink r preconditions only the actions affecting r itself. Hence, in what follows we assume that G[r] is specified; otherwise\nGiven a path 〈a1, . . . , am〉 from I[r] to G[r] in DTG(r,Π): ρ := 〈〉 am+1 := 〈G[pred(r)], ∅〉 foreach v ∈ pred(r) do xv := I[v] for i := 1 to m+ 1 do\nforeach v ∈ pred(r) do if pre(ai)[v] is specified and pre(ai)[v] 6= xv then\nif pre(ai)[v] is not reachable from xv in DTG(v,Π) then fail append to ρ the actions induced by some cost-minimal path\nfrom pre(ai)[v] to xv in DTG(v,Π) xv := pre(ai)[v]\nif i < m+ 1 then append to ρ the action ai return ρ\nFigure 4: Detailed outline of step (3) of the planning algorithm for inverted-fork structured\ntask.\nΠ breaks down to a set of trivial planning problems over a single variable each. Likewise, from the above properties of Π it follows that, if ρ is an optimal plan for Π, then the path ρ↓r from I[r] to G[r] induced by ρ in DTG(r,Π) is either cycle-free or contains only zerocost cycles. The latter can be safely eliminated from ρ, and thus we can assume that ρ↓r is cycle-free. Given that, a simple algorithm that finds a cost-optimal plan for Π in time Θ(||Π||d + ||Π||3) is as follows.\n(1) Create all Θ(|Ar|d−1) cycle-free paths from I[r] to G[r] in DTG(r,Π).\n(2) For each variable v ∈ pred(r), and each pair of v’s values x, y ∈ D(v), compute the cost-minimal path from x to y in DTG(v,Π). The whole set of such cost-minimal paths can be computed using Θ(d|V |) applications of the Floyd-Warshall algorithm on the domain transition graphs of the sink’s parents pred(r).\n(3) For each I[r]-to-G[r] path in DTG(r,Π) generated in step (1), construct a plan for Π based on that path for r, and the cheapest paths computed in (2). This simple construction, depicted in Figure 4, is possible because the values of each parent variable can be changed independently of the values of all other variables in the inverted fork.\n(4) Take the cheapest plan among those constructed in (3). If no plan was constructed in step (3), then Π is unsolvable.\nWe have already observed that, for each cost-optimal plan ρ, ρ↓r is one of the I[r]-to-G[r] paths generated in step (1). For each v ∈ pred(r), let Sv denote the sequence of values from D(v) that is required by the preconditions of the actions along ρ↓r . For each v ∈ pred(r), we have ρ↓v corresponding to a (possibly cyclic) path from I[v] to G[v] in DTG(v,Π), traversing the values (= nodes) from Sv in the order required by Sv. In turn, the plan for Π generated in (3) consists of cost-minimal such paths for all v ∈ pred(r). Therefore, at least one of the\nplans generated in (3) must be cost-optimal for Π, and the minimization step (4) will select one of them.\nTheorems 4 and 5 clarify the gap between fork decompositions and implicit abstraction heuristics, and now we can bridge this gap by further abstracting each task in the given fork decomposition of Π. We do that by abstracting domains of the fork roots and inverted-fork sinks to meet the requirements of the tractable fragments. We note that, in itself, the idea of domain decomposition is not very new in general (Hernadvölgyi & Holte, 1999) and in domain-independent planning in particular (Domshlak, Hoffmann, & Sabharwal, 2009). In fact, the shrinking step of the algorithm for building the merge-and-shrink abstractions is precisely a variable domain abstraction for meta-variables constructed in the merging steps (Helmert et al., 2007).\nDefinition 10 Let Π = 〈V,A, I,G, cost〉 be a planning task over states S, v ∈ V be a state variable, and Φ = {φ1, . . . , φm} be a set of mappings from D(v) to some sets Γ1, . . . ,Γm, respectively. A = {〈Πφi , αi〉}mi=1 is a domain abstraction of Π over Φ if, for some set of cost functions {costi : A→ R0+}mi=1 satisfying\n∀a ∈ A : m∑ i=1 costi(a) ≤ cost(a), (8)\nwe have, for 1 ≤ i ≤ m,\n• the abstraction mapping αi of states S is\n∀u ∈ V : αi(s)[u] = { φi(s[u]), u = v\ns[u], u 6= v ,\nand, extending αi to partial assignments on V ′ ⊆ V as αi(s[V ′]) = αi(s)[V ′],\n• Πφi = 〈V,Aφi , Iφi , Gφi , costφi〉 is a planning task with\n1. Iφi = αi(I), Gφi = αi(G),\n2. Aφi = {aφi = 〈αi(pre(a)), αi(eff(a))〉 | a ∈ A}, and 3. for each action a ∈ A,\ncostφi(aφi) = costi(a). (9)\nWe say that Πφi is a domain decomposition of Πi = 〈V,A, I,G, costi〉 with respect to φi.\nTheorem 6 Domain abstractions of the planning tasks are additive implicit abstractions of these tasks.\nProof: Let Π = 〈V,A, I,G, cost〉 be a planning task and A = {〈Πφi , αi〉}mi=1 be a domain abstraction of Π over Φ = {φ1, . . . , φm}. Let T = (S,L, Tr, s0, S?, $) be the transition graph of Π. For each 1 ≤ i ≤ m, let Ti = (Si, Li, Tri, s0i , S?i , $i) be the transition graph of Πφi . We need to show that αi is an abstraction mapping as in Definition 2.\nFirst, from Definition 10 we have\n• s0i = Iφi = αi(I) = αi(s0), and\n• for all s ∈ S? we have s ⊇ G and thus αi(s) ⊇ αi(G) = Gφi , providing us with αi(s) ∈ S?i .\nNow, if s is a state of Π and a ∈ A is an action with pre(a) ⊆ s, then αi(s) is a state of Πφi and pre(aφi) = αi(pre(a)) ⊆ αi(s). Thus, aφi is applicable in αi(s), and now we show that applying aφi in αi(s) results in αi(s)JaφiK = αi(sJaK).\n1. For the effect variables v ∈ V(eff(a)) = V(eff(aφi)), we have eff(aφi) ⊆ αi(s)JaφiK and eff(aφi) = αi(eff(a)) ⊆ αi(sJaK).\n2. For all other variables v 6∈ V(eff(a)), we have sJaK[v] = s[v] and αi(s)JaφiK[v] = αi(s)[v], and thus\nαi(s)JaφiK[v] = αi(s)[v] = αi(s[v]) = αi(sJaK[v]) = αi(sJaK)[v].\nNext, for each a ∈ A, from Eqs. 8 and 9 we have m∑ i=1 costφi(aφi) = m∑ i=1 costi(a) ≤ cost(a). (10)\nNow, let s, s′ ∈ S be a pair of states such that cost(s, s′) ≤ ∞, and let % = 〈a1, . . . , al〉 be the sequence of labels along a cheapest path from s to s′ in T. From that, cost(s, s′) = cost(%) =∑l\nj=1 cost(a j). The decomposition of such a path to the actions as in Definition 10 is a\n(not neccesarily cheapest) path from αi(s) to αi(s ′) in Ti, and thus cost(αi(s), αi(s′)) ≤∑l\nj=1 costi(a j), providing us with\nm∑ i=1 cost(αi(s), αi(s ′)) ≤ m∑ i=1 l∑ j=1 costφi(a j φi ) = l∑ j=1 m∑ i=1 costφi(a j φi ) (10) ≤ l∑ j=1 cost(aj) = cost(s, s′).\nHaving put the notion of domain abstraction in the framework of implicit abstractions, we are now ready to connect fork decompositions and implicit abstraction heuristics. Given a FI-abstraction AFI = {〈Πfv, αfv〉, 〈Πiv, αiv〉}v∈V of a planning task Π = 〈V,A, I,G, cost〉,\n• for each Πfv ∈ ΠFI, we associate the root v of CG(Πfv) with mappings Φfv = {φfv,1, . . . , φfv,kv} such that kv = O(poly(||Π||)) and all φfv,i : D(v)→ {0, 1}, and then abstract Πfv with Afv = {〈Πfv,i, αfv,i〉}kvi=1, and\n• for each Πiv ∈ ΠFI, we associate the sink v of CG(Πiv) with mappings Φiv = {φiv,1, . . . , φiv,k′v} such that k′v = O(poly(||Π||)) and all φiv,i : D(v) → {0, 1, . . . , bv,i}, bv,i = O(1), and then abstract Πiv with Aiv = {〈Πiv,i, αiv,i〉} k′v i=1.\nFrom Theorem 3, Theorem 6, and the composition Theorem 2, we then immediately have\nAFI = ⋃ v∈V  kv⋃ i=1 {〈Πfv,i, αfv,i ◦ αfv〉} ∪ k′v⋃ i=1 {〈Πiv,i, αiv,i ◦ αiv〉}  (11) being an additive implicit abstraction of Π. Hence, from Theorem 1,\nhFI = ∑ v∈V  kv∑ i=1 h∗ Πfv,i + k′v∑ i=1 h∗ Πiv,i  (12) is an admissible estimate of h∗ for Π, and, from Theorems 4 and 5, hFI is also computable in time O(poly(||Π||)).\nThis finalizes our construction of a concrete family of implicit abstraction heuristics. To illustrate the mixture of acyclic causal-graph and domain abstractions as above, we again use our running Logistics example. One bothersome question is to what extent further abstracting fork decompositions using domain abstractions affects the informativeness of the heuristic estimate. Though generally a degradation here is unavoidable, below we show that the answer to this question can sometimes be somewhat surprising.\nTo begin with an extreme setting, let all the domain abstractions for roots of forks and sinks of inverted forks be to binary-valued domains. Among multiple options for choosing the mapping sets {Φfv} and {Φiv}, here we use a simple choice of distinguishing between different values of each variable v on the basis of their cost from I[v] in DTG(v,Π). Specifically, for each v ∈ V , we set Φfv = Φiv, and, for each value ϑ ∈ D(v) and each 1 ≤ i ≤ maxϑ′∈D(v) d(I[v], ϑ′),\nφfv,i(ϑ) = φ i v,i(ϑ) =\n{ 0, d(I[v], ϑ) < i\n1, otherwise (13)\nFor example, the problem Πfc1 is decomposed (see the domain transition graph of c1 on the left in Figure 1c) into two problems, Πfc1,1 and Π f c1,2\n, with the binary abstract domains of c1 corresponding to the partitions {{A}, {B,C,D}} and {{A,D}, {B,C}} of D(c1), respectively. As yet another example, the problem Πip1 is decomposed (see the domain transition graph of p1 in Figure 1d) into six problems Π i p1,1\n, . . . ,Πip1,6 along the abstractions of D(p1) depicted in Figure 5a. Now, given the FI-decomposition of Π and mappings {Φfv,Φiv}v∈V as above, consider the problem Πip1,1, obtained from abstracting Π along the inverted fork of p1 and then abstracting D(p1) using\nφip1,1(ϑ) = { 0, ϑ ∈ {C} 1, ϑ ∈ {A,B,D,E, F,G, c1, c2, c3, t} .\nIt is not hard to verify that, from the original actions affecting p1, we are left in Π i p1,1 with only actions conditioned by c1 and c2. If so, then no information is lost 3 if we remove from Πip1,1 both variables c3 and t, as well as the actions changing (only) these variables,\n3. No information is lost here because we still keep either fork or inverted fork for each variable of Π.\n(b) Ternary-valued domain abstractions: values that are mapped to the same abstract value are shown as nodes with the same color and borderline.\nand redistribute the cost of the removed actions between all other representatives of their originals in Π. The latter revision of the action cost partition can be obtained directly by replacing the cost-partitioning steps corresponding to Eqs. 3-4 and 8-9 by a single, joint action cost partitioning applied over the final additive implicit abstraction AFI as in Eq. 11 and satisfying\ncost(a) ≥ ∑ v∈V  kv∑ i=1 ∑ a′∈AGfv (a) costfv,i(φ f v,i(a ′)) + k′v∑ i=1 ∑ a′∈AGiv (a) costiv,i(φ i v,i(a ′))  . (14) In what follows, by uniform action cost partition we refer to a partition in which the cost of each action is equally split among all its nonredundant representatives in the final additive implicit abstraction.\nOverall, computing hFI as in Eq. 12 under our “all binary-valued domain abstractions” and such a uniform action cost partition provides us with hFI(I) = 12 715 , and knowing that the original costs are all integers we can safely adjust it to hFI(I) = 13. Hence, even under the most severe domain abstractions as above, the estimate of hFI in our example task is not lower than that of h2.\nLet us now slightly refine our domain abstractions for the sinks of the inverted forks to be to a ternary range {0, 1, 2}. While mappings {Φfv} remain unchanged, {Φiv} are set to\n∀ϑ ∈ D(v) : φiv,i(ϑ) =  0, d(I[v], ϑ) < 2i− 1 1, d(I[v], ϑ) = 2i− 1 2, d(I[v], ϑ) > 2i− 1 . (15)\nFor example, the problem Πip1 is now decomposed into Π i p1,1 , . . . ,Πip1,3 along the abstractions of D(p1) depicted in Figure 5b. Applying now the same computation of hFI as in Eq. 12 over the new set of domain abstractions gives hFI(I) = 1512 , which, again, can be safely adjusted to hFI(I) = 16. Note that this value is higher than hFI = 15 obtained using the (generally intractable) “pure” fork-decomposition abstractions as in Eq. 6. At first view, this outcome may seem counterintuitive as the domain abstractions are applied over the fork decomposition, and one would expect a coarser abstraction to provide less precise estimates. This, however, is not necessarily the case when the employed action cost partition is ad hoc. For instance, domain abstraction for the sink of an inverted fork may create independence between the sink and its parent variables, and exploiting such domain-abstraction specific independence relations leads to more targeted action cost partition via Eq. 14.\nTo see why this surprising “estimate improvement” has been obtained, note that before the domain abstraction in Eq. 15 is applied on our example, the truck-moving actions drive-t-from-D-to-E and drive-t-from-E-to-D appear in three abstractions Πft, Π i p1 and Π i p2 , while after domain abstraction they appear in five abstractions Πft,1, Π i p1,1 , Πip1,2, Π i p1,3 and Πip2,1. However, a closer look at the action sets of these five abstractions reveals that the dependencies of p1 in CG(Π i p1,1 ) and CG(Πip1,3), and of p2 in CG(Π i p2,1\n) on t are redundant, and thus keeping representatives of move-D-E and move-E-D in the corresponding abstract tasks is entirely unnecessary. Hence, after all, the two truck-moving actions appear only in two post-domain-abstraction tasks. Moreover, in both these abstractions the truck-moving actions are fully counted, in contrast to the predomain-abstraction tasks where the portion of the cost of these actions allocated to Πip2 simply gets lost."
    }, {
      "heading" : "6. Experimental Evaluation: Take I",
      "text" : "To evaluate the practical attractiveness of the fork-decomposition heuristics, we have conducted an empirical study on a wide sample of planning domains from the International Planning Competitions (IPC) 1998-2006, plus a non-IPC Schedule-STRIPS domain.4\nThe domains were selected to allow a comparative evaluation with other, both baseline and state-of-the-art, approaches/planners, not all of which supported all the PDDL features at the time of our evaluation.\nLater we formally prove that, under ad hoc action cost partitions such as our uniform partition, none of the three fork decompositions as in Definition 9 is dominated by the other two. Hence, we have implemented three additive fork-decomposition heuristics, hF, hI, and hFI, within the standard heuristic forward search framework of the Fast Downward planner (Helmert, 2006) using the A∗ algorithm with full duplicate elimination. The hF\nheuristic corresponds to the ensemble of all (not clearly redundant) fork subgraphs of the\n4. Schedule-STRIPS appears in the domains’ distribution of IPC-2000. Later we became aware of the fact that this domain was excluded from the competition because its encoding generated problems for various planners.\ntasks solved by any planner. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from “solved by some planners” (%S). The last row summarize the number of solved instances.\ncausal graph, with the domains of the roots being abstracted using the “leave-one-value-out” binary-valued domain decompositions as follows:\n∀ϑi ∈ D(v) : φfv,i(ϑ) = { 0, ϑ = ϑi\n1, otherwise . (16)\nThe hI heuristic is the same but for the inverted fork subgraphs, with the domains of the sinks being abstracted using the “distance-to-goal-value” ternary-valued domain decompositions5 as in Eq. 17.\n∀ϑ ∈ D(v) : φiv,i(ϑ) =  0, d(ϑ,G[v]) < 2i− 1 1, d(ϑ,G[v]) = 2i− 1 2, d(ϑ,G[v]) > 2i− 1 . (17)\nThe ensemble of the hFI heuristic is the union of these for hF and hI. The action cost partition in all three heuristics was what we call “uniform.”\nWe make a comparison with two baseline approaches, namely “blind A∗” with heuristic value 0 for goal states and 1 otherwise, and A∗ with the hmax heuristic (Bonet & Geffner, 2001), as well as with state-of-the-art abstraction heuristics, represented by the mergeand-shrink abstractions of Helmert et al. (2007). The latter were constructed under the\n5. While “distance-from-initial-value” is reasonable for the evaluation of just the initial state, “leave-onevalue-out” for fork roots and “distance-to-goal-value” for inverted-fork sinks should typically be much more attractive for the evaluation of all the states examined by A∗.\nlinear, f -preserving abstraction strategy proposed by these authors, and this under two fixed bounds on the size of the abstract state spaces, notably |Sα| < 104 and |Sα| < 105. These four (baseline and merge-and-shrink) heuristics were implemented by Helmert et al. (2007) within the same planning system as our fork-decomposition heuristics, allowing for a fairly unbiased comparison. We also compare to the Gamer (Edelkamp & Kissmann, 2009) and HSP∗F (Haslum, 2008) planners, the winner and the runner-up at the sequential optimization track of IPC-2008. On the algorithmic side, Gamer is based on a bidirectional blind search using sophisticated symbolic-search techniques, and HSP∗F uses A\n∗ with an additive critical-path heuristic. The experiments were conducted on a 3GHz Intel E8400 CPU with 2 GB memory, using 1.5 GB memory limit and 30 minute timeout. The only exception was Gamer, for which we used similar machines but with 4 GB memory and 2 GB memory limit; this was done to provide Gamer with the environment for which it was configured.\nTable 1 summarizes our experimental results in terms of the number of tasks solved by each planner. Our impression of fork-decomposition heuristics from Table 1 is somewhat mixed. On the one hand, the performance of all three fork-decomposition based planners was comparable to one of the settings of the merge-and-shrink heuristic, and this clearly testifies for that the framework of implicit abstractions is not of theoretical interest only. On the other hand, all the planners, except for A∗ with the merge-and-shrink heuristic with |Sα| < 104, failed to outperform A∗ with the baseline hmax heuristic. More important for us is that, unfortunately, all three fork-decomposition based planners failed to outperform even the basic blind search.\nThis, however, is not the end of the story for the fork-decomposition heuristics. Some hope can be found in the detailed results in Tables 9-14 in the appendix. As it appears from Table 10, on, e.g., the Logistics-ipc2 domain, hF almost consistently leads to expanding fewer search nodes than the (better between the two merge-and-shrink heuristics on this domain) MS -105, with the difference hitting four orders of magnitude. However, the time complexity of hF per search node is substantially higher than that of MS -105, with the two expanding at a rate of approximately 40 and 100000 nodes per second, respectively. The outcome is simple: while with no time limits (and only memory limit of 1.5 GB) hF\nsolves more tasks in Logistics-ipc2 than MS -105 (task 12-1 is solved with hF in 2519.01 seconds), this is not so with a standard time limit of half an hour used for Table 10. In what follows we examine the possibility of exploiting the informativeness of fork-decomposition heuristics while not falling into the trap of costly per-node heuristic evaluation.\n7. Back to Theory: h-Partitions and Databased Implicit Abstraction\nAccuracy and low time complexity are both desired yet competing properties of heuristic functions. For many powerful heuristics, and abstraction heuristics in particular, computing h(s) for each state s in isolation is impractical: while computing h(s) is polynomial in the description size of Π, it is often not efficient enough to be performed at each search node. However, for some costly heuristics this obstacle can be largely overcome by sharing most of the computation between the evaluations of h on different states. If that is possible, the shared parts of computing h for all problem states are precomputed and memorized before the search, and then reused during the search by the evaluations of h on different\nstates. Such a mixed offline/online heuristic computation is henceforth called h-partition, and we define the time complexity of an h-partition as the complexity of computing h for a set of states. Given a subset of k problem states S′ ⊆ S, the h-partition’s time complexity of computing {h(s) | s ∈ S′} is expressed as O(X+kY ), where O(X) and O(Y ) are, respectively, the complexity of the (offline) pre-search and (online) per-node parts of computing h(s).\nThese days h-partitions are being adopted by various optimal planners using criticalpath heuristics hm for m > 1 (Haslum et al., 2005), landmark heuristics hL and hLA (Karpas & Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp, 2001; Helmert et al., 2007). Without effective h-partitions, optimal search with these heuristics would not scale up well, while with such h-partitions it constitutes the state of the art of cost-optimal planning. For instance, a very attractive property of PDB abstractions is the complexity of their natural hα-partition. Instead of computing hα(s) = h∗(α(s)) from scratch for each evaluated state s (impractical for all but tiny projections), the practice is to precompute and store h∗(s′) for all abstract states s′ ∈ Sα, after which the per-node computation of hα(s) boils down to a hash-table lookup for h∗(α(s)) with a perfect hash function. In our terms, the time and space complexity of that PDB hα-partition for a set of k states is O(|Sα|(log(|Sα|) + |A|) + k) and O(|Sα|), respectively. This is precisely what makes PDB heuristics so attractive in practice. In that respect, the picture with mergeand-shrink abstractions is very much similar. While the order in which composites are formed and the choice of abstract states to contract are crucial to the complexity of their natural hα-partitions, the time and space complexity for the concrete linear abstraction strategy of Helmert et al. are respectively O(|V ||Sα|(log(|Sα|) + |A|) + k · |V |) and O(|Sα|). Similarly to PDB abstractions, the per-node computation of hα(s) with a merge-and-shrink abstraction α is just a lookup in a data structure storing h∗(α(s)) for all abstract states α(s) ∈ Sα. Hence, while the pre-search computation with MS abstractions can be more costly than with PDBs, the online part of computing heuristic values is still extremely efficient. This per-node efficiency provides the merge-and-shrink heuristics with impressive practical effectiveness on numerous IPC domains (Helmert et al., 2007).\nTo sum up, we can say that the fixed size of abstract spaces induced by explicit abstractions such as PDBs and merge-and-shrink is not only a limitation but also a key to obtaining effective h-partitions. In contrast, escaping that limitation with implicit abstractions might trap us into having to pay a high price for each search-node evaluation. We now show, however, that the time-per-node complexity bottleneck of fork-decomposition heuristics can be successfully overcome. Specifically, we show that an equivalent of PDB’s and mergeand-shrink notion of “database” exists for fork-decomposition abstractions as well, despite their exponential-size abstract spaces. Of course, unlike with PDB and merge-and-shrink abstractions, the databased fork-decomposition heuristics do not (and cannot) provide us with a purely lookup online computation of hα(s). The online part of the hα-partition has to be nontrivial in the sense that its complexity cannot be O(1). In what comes next we prove the existence of such effective h-partitions for fork and inverted fork abstractions. In Section 8 we then empirically show that these h-partitions lead to fast pre-search and per-node computations, allowing the informativeness of the fork-decomposition heuristics to be successfully exploited in practice.\nTheorem 7 Let Π = 〈V,A, I,G, cost〉 be a planning task with a fork causal graph rooted at a binary-valued variable r. There exists an h∗-partition for Π such that, for any set of k states, the time and space complexity of that h∗-partition is, respectively, O(d3|V |+ |Ar|+ kd|V |) and O(d2|V |), where d = maxv D(v).\nProof: The proof is by a modification of the polynomial-time algorithm for computing h∗(s) for a state s of such a task Π used in the proof of Theorem 4 (Tractable Forks). Given a state s, let D(r) = {0, 1}, where s[r] = 0. In what follows, for each of the two root’s values ϑ ∈ D(r), ¬ϑ denotes the opposite value 1− ϑ; σ(r), [σ(r)], DTG0v and DTG1v are defined exactly as in the proof of Theorem 4.\n(1) For each of the two values ϑr ∈ D(r) of the root variable, each leaf variable v ∈ V \\{r}, and each pair of values ϑ, ϑ′ ∈ D(v), let pϑ,ϑ′;ϑr be the cost of the cheapest sequence of actions changing v from ϑ to ϑ′ provided r :ϑr. The whole set {pϑ,ϑ′;ϑr} for all the leaves v ∈ V \\{r} can be computed by a straightforward variant of the all-pairs-shortest-paths, Floyd-Warshall algorithm on DTGϑrv in time O(d 3|V |).\n(2) For each leaf variable v ∈ V \\ {r}, 1 ≤ i ≤ d + 1, and ϑ ∈ D(v), let gϑ;i be the cost of the cheapest sequence of actions changing s[v] to ϑ provided a sequence σ ∈ [σ(r)], |σ| = i, of value changes of r. Having the values {pϑ,ϑ′;ϑr} from step (1), the set {gϑ;i} is given by the solution of the recursive equation\ngϑ;i =  ps[v],ϑ;s[r], i = 1 min ϑ′ [ gϑ′;i−1 + pϑ′,ϑ;s[r] ] , 1 < i ≤ δϑ, i is odd min ϑ′ [ gϑ′;i−1 + pϑ′,ϑ;¬s[r] ] , 1 < i ≤ δϑ, i is even\ngϑ;i−1, δϑ < i ≤ d+ 1\n,\nwhere δϑ = |D(v)|+ 1. Given that, we have\nh∗(s) = min σ∈ [σ(r)]  cost(σ) + ∑ v∈V \\{r} gG[v];|σ|  , with cost(σ) = ∑|σ| i=2 cost(aσ[i]), where aσ[i] ∈ A is the cheapest action changing the\nvalue of r from σ[i− 1] to σ[i].\nNote that step (1) is already state-independent, but the heavy step (2) is not. However, the state dependence of step (2) can mostly be overcome as follows. For each v ∈ V \\ {r}, ϑ ∈ D(v), 1 ≤ i ≤ d+ 1, and ϑr ∈ D(r), let g̃ϑ;i(ϑr) be the cost of the cheapest sequence of actions changing ϑ to G[v] provided the value changes of r induce a 0/1 sequence of length i starting with ϑr. The set {g̃ϑ;i(ϑr)} is given by the solution of the recursive equation\ng̃ϑ;i(ϑr) =  pϑ,G[v];ϑr , i = 1 min ϑ′ [ g̃ϑ′;i−1(¬ϑr) + pϑ,ϑ′;ϑr ] , 1 < i ≤ δϑ\ng̃ϑ;i−1(ϑr), δϑ < i ≤ d+ 1 , (18)\nand two children v and u, and G[r] = 0, G[v] = 3, and G[u] = 5. (a) depicts the domain transition graphs of r (top), v (middle), and u (bottom); the numbers above and below each edge are the precondition on r and the cost of the respective action. (b) depicts the database created by the algorithm. For instance, the entry in row r :0∧ |σ|=5 and column v :0 captures the value of g̃v:0;5(r :0) computed as in Eq. 18. The shaded entries are those examined during the online computation of h∗(r :0, v :0, u :0).\nwhich can be solved in time O(d3|V |). Note that this equation is now independent of the evaluated state s, and yet {g̃ϑ;i(ϑr)} allow for computing h∗(s) for a given state s via\nh∗(s) = min σ∈ [σ(r|s[r])]  cost(σ) + ∑ v∈V \\{r} g̃s[v];|σ|(s[r])  (19) where σ(r|ϑr) is defined similarly to σ(r) but with respect to the initial value ϑr of r.\nWith the new formulation, the only computation that has to be performed online, per search node, is the final minimization over [σ(r|s[r])] in Eq. 19, and this is the lightest part of the whole algorithm anyway. The major computations, notably those of {pϑ,ϑ′;ϑr} and {g̃ϑ;i(ϑr)}, can now be performed offline and shared between the evaluated states. The space required to store this information is O(d2|V |) as it contains only a fixed amount of information per pair of values of each variable. The time complexity of the offline computation is O(d3|V | + |Ar|); the |Ar| component stems from precomputing the costs cost(σ). The time complexity of the online computation per state is O(d|V |); |V | comes from the internal summation and d comes from the size of [σ(r|s[r])].\nFigure 6b shows the database created for a fork-structured problem with a binary-valued root r, two children v and u, and G[r] = 0, G[v] = 3, and G[u] = 5; the domain transition\ngraphs of v and u are depicted in Figure 6(a). Online computation of h∗(s) as in Eq. 19 for s = (r : 0, v : 0, u : 0) sums over the shaded entries of each of the four rows having such entries, and minimizes over the resulting four sums, with the minimum being obtained in the row r :0 ∧ |σ|=5.\nTheorem 8 Let Π = 〈V,A, I,G, cost〉 be a planning task with an inverted fork causal graph with sink r and |D(r)| = b = O(1). There exists an h∗-partition for Π such that, for any set of k states, the time and space complexity of that h∗-partition is O(b|V ||Ar|b−1 + d3|V | + k|V ||Ar|b−1) and O(|V ||Ar|b−1 + d2|V |), respectively, where d = maxv D(v). Proof: Like the proof of Theorem 7, the proof of Theorem 8 is based on a modification of the polynomial-time algorithm for computing h∗(s) used for the proof of Theorem 5 (Tractable Inverted Forks).\n(1) For each parent variable v ∈ V \\ {r}, and each pair of its values ϑ, ϑ′ ∈ D(v), let pϑ,ϑ′ be the cost of the cheapest sequence of actions changing ϑ to ϑ′. The whole set {pϑ,ϑ′} can be computed using the Floyd-Warshall algorithm on the domain transition graph of v in time O(d3|V |).\n(2) Given a state s, for each cycle-free path π = 〈a1, . . . , am〉 from s[r] to G[r] in DTG(v,Π), let gπ be the cost of the cheapest plan from s in Π based on π, and the cheapest paths {pϑ,ϑ′} computed in step (1). Each gπ can be computed as\ngπ = m∑ i=1 cost(ai) + m∑ i=0 ∑ v∈V \\{r} pprei[v],prei+1[v],\nwhere pre0, . . . , prem+1 are the values required from the parents of r along the path π. That is, for each v ∈ V \\ {r}, and 0 ≤ i ≤ m+ 1,\nprei[v] =  s[v], i = 0 G[v], i = m+ 1, and G[v] is specified\npre(ai)[v], 1 ≤ i ≤ m, and pre(ai)[v] is specified prei−1[v] otherwise\n.\nFrom that, we have h∗(s) = minπ gπ. Note that step (1) is state-independent, but step (2) is not. However, the dependence\nof step (2) on the evaluated state can be substantially relaxed. As there are only O(1) different values of r, it is possible to consider cycle-free paths to G[r] from all values of r. For each such path π, and each parent variable v ∈ V \\ {r}, we know what the first value of v required by π would be. Given that, we can precompute the cost-optimal plans induced by each π assuming the parents start at their first required values. The remainder of the computation of h∗(s) is delegated to online, and the modified step (2) is as follows.\nFor each ϑr ∈ D(r) and each cycle-free path π = 〈a1, . . . , am〉 from ϑr to G[r] in DTG(r,Π), let a “proxy” state sπ be\nsπ[v] =  ϑr, v = r\nG[v], ∀1 ≤ i ≤ m : pre(ai)[v] is unspecified pre(ai)[v], i = argminj {pre(aj)[v] is specified} ,\nthat is, the nontrivial part of sπ captures the first values of V \\{r} required along π.6 Given that, let gπ be the cost of the cheapest plan from sπ in Π based on π, and the cheapest paths {pϑ,ϑ′} computed in (1). Each gπ can be computed as\ngπ = m∑ i=1 cost(ai) + ∑ v∈V \\{r} pprei[v],prei+1[v]  , where, for each v ∈ V \\ {r}, and 1 ≤ i ≤ m+ 1,\nprei[v] =  sπ[v], i = 1 G[v], i = m+ 1, and G[v] is specified\npre(ai)[v], 2 ≤ i ≤ m, and pre(ai)[v] is specified prei−1[v], otherwise\n.\nStoring the pairs (gπ, sπ) accomplishes the offline part of the computation. Now, given a search state s, we can compute\nh∗(s) = min π s.t.\nsπ [r]=s[r] gπ + ∑ v∈V \\{r} ps[v],sπ [v] . (20) The number of cycle-free paths to G[r] in DTG(r,Π) is Θ(|Ar|b−1), and gπ for each such path π can be computed in time O(b|V |). Hence, the overall offline time complexity is O(b|V ||Ar|b−1 +d3|V |), and the space complexity (including the storage of the proxy states sπ) is O(|V ||Ar|b−1 + d2|V |). The time complexity of the online computation per state via Eq. 20 is O(|V ||Ar|b−1); |V | comes from the internal summation and |Ar|b−1 from the upper bound on the number of cycle-free paths from s[r] to G[r].\nFigure 7(b) shows the database created for an inverted fork structured problem with a ternary-valued sink variable r, two parents u and v, and G[r] = 2, G[u] = 0, and G[v] = 2. The domain transition graphs of u and v are depicted at the top of Figure 7(a); the actual identities of actions affecting these two parents are not important here. The actions affecting the sink r are\na1 = 〈{u :1, r :0}, {r :1}〉 a2 = 〈{v :1, r :0}, {r :1}〉 a3 = 〈{u :2, r :1}, {r :2}〉 a4 = 〈{v :1, r :1}, {r :2}〉.\nThe domain transition graph of r is depicted at the bottom of Figure 7(a). Online computation of h∗(s) as in Eq. 20 for s = (r : 0, v : 0, u : 0) sums over the shaded entries of each of the four rows having such entries, and minimizes over the resulting four sums, with the minimum being obtained in the lowest such row.\nvariable r and two parents u and v, and G[r] = 2, G[u] = 0, and G[v] = 2. (a) depicts the domain transition graphs of u (top left), v (top right), and r (bottom); the numbers above and below each edge are the preconditions and the cost of the respective action, respectively. (b) depicts the database created by the algorithm. The shaded entries are those examined during the online computation of h∗(r :0, u :0, v :0)."
    }, {
      "heading" : "8. Experimental Evaluation: Take II",
      "text" : "To evaluate the practical attractiveness of the databased fork-decomposition heuristics, we have repeated our empirical evaluation as in Section 6, but now for the databased versions of the heuristics. The detailed results of this evaluation are relegated to Tables 15-20 in the appendix, but they are summarized here in Table 2. For each domain, the S column captures the number of tasks in that domain that were solved by at least one planner in the suite. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from “solved by some planners” (%S). Boldfaced results indicate the best performance within the corresponding domain. The last three rows summarize the performance of the planners via three measures. The first is the number of tasks solved in all the 23 domains; this is basically the performance evaluation measure used in the optimization track at IPC-2008. As domains are not equally challenging and do not equally discriminate between the planners’ performance, the second is a “domain-normalized” performance measure\nŝ(p) = ∑\ndomain D\n#tasks in D solved by planner p #tasks in D solved by some planners .\nFinally, the third measure corresponds to the number of domains w in which the planner in question solved at least as many tasks as any other planner.\nOverall, Table 2 clearly suggests that heuristic search with “databased” fork-decomposition heuristics favorably competes with the state of the art of optimal planning. In particular,\n6. For ease of presentation, we omit here the case where v is required neither along π, nor by the goal; such variables should be simply ignored when accounting for the cost of π.\ndecomposition heuristics. Per domain, S denotes the number of tasks solved by any planner. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from “solved by some planners” (%S). Boldfaced results indicate the best performance within the corresponding domain. The last three rows summarize the number of solved instances, the domain-normalized measure of solved instances (ŝ), and the number of domains in which the planners achieved superior performance (w).\nA∗ with the “only forks” heuristic hF exhibited the best overall performance according to all three measures. In terms of the absolute number of solved instances, A∗ with all three fork-decomposition heuristics outperformed all other planners in the suite. The contribution of databasing to the success of the fork-decomposition heuristics was dramatic. Looking back at the results with “fully online” heuristic computation depicted in Table 1, note that the total number of solved instances for the fork-decomposition heuristics hF, hI, and hFI\nincreased by 74, 55, and 76, respectively, and this made the whole difference.\nWe have also performed a comparative evaluation on the planning domains from the recent IPC-2008. The IPC-2008 domains differ from the previous domains in that actions had various costs, and, more importantly, many actions had zero cost. The latter is an issue for heuristic-search planners because heuristic functions cannot differentiate between subplans that have the same cost of zero, but differ in length. In any case, the comparative side of our evaluation on the IPC-2008 domains differ on several points from the previous one. First, neither for merge-and-shrink nor for hmax heuristics, we had implementation supporting arbitrary action costs. Hence, our comparison here is only with Gamer, HSP∗F, and blind search. Second, to ensure admissibility of the blind search, the latter has been modified to return on non-goal states the cost of the cheapest applicable action. Finally, all the planners were run on a 3GHz Intel E8400 CPU with 4 GB memory, using 2 GB memory\ntasks solved by any planner. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from “solved by some planners” (%S). Boldfaced results indicate the best performance within the corresponding domain. The last three rows summarize the number of solved instances, the domain-normalized measure of solved instances (ŝ), and the number of domains in which the planners achieved superior performance (w).\nlimit and 30 minute timeout. The results of this evaluation are summarized in Table 3; for the detailed results we refer the reader to Tables 21-22 in the appendix. Overall, these results show that A∗ with the fork-decomposition heuristics are very much competitive on the IPC-2008 domains as well."
    }, {
      "heading" : "9. Formal Analysis: Asymptotic Performance Ratios",
      "text" : "Empirical evaluation on a concrete set of benchmark tasks is a standard and important methodology for assessing the effectiveness of heuristic estimates: it allows us to study the tradeoff between the accuracy of the heuristics and the complexity of computing them. However, as rightfully noted by Helmert and Mattmüller (2008), such evaluations almost never lead to absolute statements of the type “Heuristic h is well-suited for solving problems from benchmark suite X,” but only to relative statements of the type “Heuristic h expands fewer nodes than heuristic h′ on benchmark suite X.” Moreover, one would probably like to obtain formal evidence of the effectiveness of a heuristic before proceeding with its implementation, especially for very complicated heuristic procedures such as those underlying the proofs of Theorems 7 and 8. Our formal analysis of the effectiveness of the fork-decomposition heuristics using the methodology suggested and exploited by Helmert and Mattmüller was motivated primarily by this desire for formal evidence.\nGiven a planning domain D and heuristic h, Helmert and Mattmüller (2008) consider the asymptotic performance ratio of h in D. The goal is to find a value α(h,D) ∈ [0, 1] such that\n(1) for all states s in all problems Π ∈ D, h(s) ≥ α(h,D) · h∗(s) + o(h∗(s)), and\n(2) there is a family of problems {Πn}n∈N ⊆ D and solvable, non-goal states {sn}n∈N such that sn ∈ Πn, limn→∞ h∗(sn) =∞, and h(sn) ≤ α(h,D) · h∗(sn) + o(h∗(sn)).\nIn other words, h is never worse than α(, domain, ·)h∗ (plus a sublinear term), and it can become as bad as α(h,D) · h∗ (plus a sublinear term) for arbitrarily large inputs; note that both the existence and uniqueness of α(h,D) are guaranteed for any h and D.\nHelmert and Mattmüller (2008) study the asymptotic performance ratio of some standard admissible heuristics on a set of well-known benchmark domains from the first four IPCs. Their results for Gripper, Logistics, Blocksworld, Miconic, and Satellite are shown in the first four columns of Table 4.\n• The h+ estimate corresponds to the optimal cost of solving the well-known “delete relaxation” of the original planning task, which is generally NP-hard to compute (Bylander, 1994).\n• The hk, k ∈ N+, family of heuristics is based on a relaxation where the cost of achieving a partial assignment is approximated by the highest cost of achieving its sub-assignment of size k (Haslum & Geffner, 2000); computing hk is exponential only in k.\n• The hPDB and hPDBadd heuristics are regular (maximized over) and additive pattern database heuristics where the size of each pattern is assumed to be O(log(n)) where n = |V |, and, importantly, the choice of the patterns is assumed to be optimal.\nThese results provide us with a baseline for evaluating our fork-decomposition heuristics hF, hI, and hFI. First, however, Theorem 9 shows that these three heuristics are worth analyzing because each alone can be strictly more informative than the other two, depending on the planning task and/or the state being evaluated.7\nTheorem 9 (Undominance) Under uniform action cost partition, none of the heuristic functions hF, hI, and hFI dominates another.\nProof: The proof is by example of two tasks, Π1 and Π2, which illustrate the following two cases: hF(I) > hFI(I) > hI(I) and hF(I) < hFI(I) < hI(I). These two tasks are defined over the same set of binary-valued variables V = {v1, v2, v3, u1, u2, u3}, have the same initial state I = {v1 : 0, v2 : 0, v3 : 0, u1 : 0, u2 : 0, u3 : 0}, and have the same goal 7. Theorem 9 is formulated and proven under the uniform action cost partition that we use throughout the\npaper, including the experiments. For per-step optimal action cost partitions (Katz & Domshlak, 2010), it is trivial to show that hFI dominates both hF and hI for all planning tasks.\nin each abstract problem along these subgraphs. Considering for example the first row of table (c), the action a1 in Π\n1 has a single representative in each of the three fork abstractions, as well as a representative in the inverted-fork abstraction Π1G iv1 . Hence, the cost of each of its representatives in F-decomposition is 1/3, while the cost of its sole representative in I-decomposition is 1.\nG = {v1 : 1, v2 : 1, v3 : 1}. The difference between Π1 and Π2 is in the action sets, listed in Figure 8c-d, with all the actions being unit-cost actions. The two tasks induce identical causal graphs, depicted in Figure 8a. Hence, the collections of v-forks and v-iforks of both tasks are also identical; these are depicted in Figure 8b. The fractional costs of the tasks’ action representatives in the corresponding abstract problems are given in Figure 8c-d.\nFigure 9 shows the optimal plans for all the abstract problems in F-decompositions Π1F = {Π1Gfu1 ,Π 1 Gfu2 ,Π1Gfu3 } and Π2F = {Π2Gfu1 ,Π 2 Gfu2 ,Π2Gfu3 }, I-decompositions Π1I = {Π1G iv1 ,Π 1 G iv2 ,Π1G iv3\n} and Π2I = {Π2G iv1 ,Π 2 G iv2 ,Π2G iv3\n}, and FI-decompositions Π1FI = Π1F ∪Π1I and Π2FI = Π2F ∪Π2I . The last column in both tables captures the estimates of the three heuristics for the initial states of Π1 and Π2, respectively. Together, these two cases show that none of the forkdecomposition heuristic functions hF, hI, and hFI dominates any other, and, since all the\nvariables above are binary-valued, the claim holds in conjunction with arbitrary variable domain abstractions.\nOne conclusion from Theorem 9 is that it is worth studying the asymptotic performance ratios for all three heuristics. The last three columns of Table 4 present our results for hF, hI, and hFI for the Gripper, Logistics, Blocksworld, Miconic, and Satellite domains. We also studied the performance ratios of max{hF, hI, hFI}, and in these five domains they appear to be identical to those of hF. (Note that “ratio of max” should not necessarily be identical to “max of ratios,” and thus this analysis is worthwhile.) Taking a conservative position, the performance ratios for the fork-decomposition heuristics in Table 4 are “worst-case” in the sense that\n(i) here we neither optimize the action cost partition (setting it to uniform as in the rest of the paper) nor eliminate clearly redundant abstractions, and\n(ii) we use domain abstractions to (up to) ternary-valued abstract domains only.\nThe domains of the fork roots are all abstracted using the “leave-one-out” binary-valued domain decompositions as in Eq. 16 while the domains of the inverted-fork sinks are all abstracted using the “distance-from-initial-value” ternary-valued domain decompositions as in Eq. 15.\nOverall, the results for fork-decomposition heuristics in Table 4 are gratifying. First, note that the performance ratios for hk and hPDB are all 0. This is because every subgoal set of size k (for hk) and size log(n) (for hPDB) can be reached in the number of steps that only depends on k (respectively, log(n)), and not n, while h∗(sn) grows linearly in n in all the five domains. This leaves us with hPDBadd being the only state-of-the-art (tractable\nand) admissible heuristic to compare with. Table 4 shows that the asymptotic performance ratio of hF heuristic is at least as good as that of hPDBadd in all five domains, while h\nF is superior to hPDBadd in Miconic, getting here quite close to h\n+. When comparing hPDBadd and fork-decomposition heuristics, it is crucial to recall that the ratios devised by Helmert and Mattmüller for hPDBadd are with respect to optimal, manually-selected set of patterns. By contrast, the selection of variable subsets for fork-decomposition heuristics is completely nonparametric, and thus requires no tuning of the abstraction-selection process.\nIn the rest of the section we prove these asymptotic performance ratios of hF, hI, and hFI in Table 4 for the five domains. We begin with a very brief outline of how the results are obtained. Some familiarity with the domains is assumed. Next, each domain is addressed in detail: we provide an informal domain description as well as its sas+ representation, and then prove lower and upper bounds on the ratios for all three heuristics.\nGripper Assuming n > 0 balls should be moved from one room to another, all three heuristics hF, hI, hFI account for all the required pickup and drop actions, and only for O(1)-portion of move actions. However, the former actions are responsible for 2/3 of the optimal-plan length (= cost). Now, with the basic uniform action-cost partition, hF, hI, and hFI account for the whole, O(1/n), and 2/3 of the total pickup/drop actions cost, respectively, providing the ratios in Table 4.8\nLogistics An optimal plan contains at least as many load/unload actions as move actions, and all three heuristics hF, hI, hFI fully account for the former, providing a lower bound of 1/2. An instance on which all three heuristics achieve exactly 1/2 consists of two trucks t1, t2, no airplanes, one city, and n packages such that the initial and goal locations of all the packages and trucks are all pair-wise different.\nBlocksworld Arguments similar to those of Helmert and Mattmüller (2008) for hPDBadd .\nMiconic All three heuristics fully account for all the loads/unload actions. In addition, hF\naccounts for the full cost of all the move actions to the passengers’ initial locations, and for half of the cost of all the other move actions. This provides us with lower bounds of 1/2 and 5/6, respectively. Tightness of 1/2 for hI and hFI is shown on a task consisting of n passengers, 2n + 1 floors, and all the initial and goal locations being pair-wise different. Tightness of 5/6 for hF is shown on a task consisting of n passengers, n+ 1 floors, the elevator and all the passengers are initially at floor n+ 1, and each passenger i wishes to get to floor i.\nSatellite The length of an optimal plan for a problem with n images to be taken and k satellites to be moved to some end-positions is ≤ 6n + k. All three heuristics fully account for all the image-taking actions and one satellite-moving action per satellite as above, providing a lower bound of 16 . Tightness of 1/6 for all three heuristics is shown on a task as follows: Two satellites with instruments {i}li=1 and {i}2li=l+1, respectively, where l = n − √n. Each pair of instruments {i, l + i} can take images in modes {m0,mi}. There is a set of directions {dj}nj=0 and a set of image objectives\n8. We note that a very slight modification of the uniform action-cost partition results in a ratio of 2/3 for all three heuristics. Such optimizations, however, are outside of our scope here.\nv-iforks\n{oi}ni=1 such that, for 1 ≤ i ≤ l, oi = (d0,mi) and, for l < i ≤ n, oi = (di,m0). Finally, the calibration direction for each pair of instruments {i, l + i} is di."
    }, {
      "heading" : "9.1 Gripper",
      "text" : "The domain consists of one robot robot with two arms Arms = {right, left}, two rooms Rooms = {r1, r2}, and a set Balls of n balls. The robot can pick up a ball with an arm arm ∈ Arms if arm is empty, release a ball b ∈ Balls from the arm arm if arm currently holds b, and move from one room to another. All balls and the robot are initially in room r1, both arms are empty, and the goal is to move all the balls to room r2. A natural description of this planning task in sas+ is as follows.\n• Variables V = {robot}⋃Arms⋃Balls with domains D(robot) = Rooms\nD(left) = D(right) = Balls ∪ {empty} ∀b ∈ Balls : D(b) = Rooms ∪ {robot}.\n• Initial state I = {b : r1 | b ∈ Balls} ∪ {robot : r1, right :empty, left :empty}.\n• Goal G = {b : r2 | b ∈ Balls}.\n• Actions A ={Move(r, r′) | {r, r′} ⊆ Rooms} ⋃\n{Pickup(b, arm, r), Drop(b, arm, r) | b ∈ Balls, arm ∈ Arms, r ∈ Rooms},\nwhere\n– move robot: Move(r, r′) = 〈{robot : r}, {robot : r′}〉, – pickup ball: Pickup(b, arm, r) = 〈{b : r, arm :empty, robot : r}, {b : robot, arm :b}〉, and\n– drop ball: Drop(b, arm, r) = 〈{b : robot, arm :b, robot : r}, {b : r, arm :empty}〉.\nThe (parametric in n) causal graph of this task is depicted in Figure 10a.\nas well as the partition of the action costs between these representatives\ntasks"
    }, {
      "heading" : "9.1.1 Fork Decomposition",
      "text" : "Since the variables robot, right, and left have no goal value, the collection of v-forks and v-iforks is as in Figure 10b. The domains of inverted fork sinks are ternary valued. The domains of fork roots are abstracted as in Eq. 16 (“leave one out”), and thus\nΠF = {Πfrobot} ∪ {Πfright,ϑ,Πfleft,ϑ | ϑ ∈ {empty} ∪Balls}, ΠI = {Πib | b ∈ Balls},\nΠFI = {Πfrobot} ∪ {Πfright,ϑ,Πfleft,ϑ | ϑ ∈ {empty} ∪Balls} ∪ {Πib | b ∈ Balls}.\nFor each original action, the number of its representatives in each abstract task, as well as the cost assigned to each such representative, are listed in Table 5. Table 6 illustrates derivation of these numbers via decomposition of an example action Pickup(b, right, r1) in each of the fork decomposition abstractions. That action has one nonredundant representative in Πfrobot, two such representatives in each of Π f right,empty and Π f right,b, one representative in each Πfright,b′ for b ′ ∈ Balls\\{b}, one representative in each Πfleft,ϑ for ϑ ∈ Balls∪{empty}, two representatives in Πib, and one representative in each Π i b′ for b\n′ ∈ Balls \\ {b}. This results in cost 12n+5 for each representative in ΠF, 1 n+1 for each representative in ΠI, and\n1 3n+6 for each representative in ΠFI.\nGiven that, the optimal plans for the abstract tasks are as follows.\nh task optimal plan cost # h(I)\nhF\nΠfrobot 〈Pickup(b1, right, r1), . . . , P ickup(bn, right, r1), 4n+5 2n+5\n1\n2n− 2n−5 2n+5 ,Move(r1, r2), Drop(b1, right, r2), . . . , Drop(bn, right, r2)〉 Πfright,ϑ 〈Pickup(b1, left, r1), . . . , P ickup(bn, left, r1), 2n 2n+5 n + 1\n, Drop(b1, left, r2), . . . , Drop(bn, left, r2)〉 Πfleft,ϑ 〈Pickup(b1, right, r1), . . . , P ickup(bn, right, r1),\n2n 2n+5\nn + 1\n, Drop(b1, right, r2), . . . , Drop(bn, right, r2)〉 hI Πib 〈Pickup(b, right, r1) 1, P ickup(b, right, r1)2,Move(r1, r2), Drop(b, left, r2)2〉 3 n+1 + 1 n\nn 4n+1 n+1\nhFI\nΠfrobot 〈Pickup(b1, right, r1), . . . , P ickup(bn, right, r1),Move(r1, r2), 2n 3n+6 + 1 n+1 1\n4n 3 + 4n+6 3n+6\n, Drop(b1, right, r2), . . . , Drop(bn, right, r2)〉 Πfright,ϑ 〈Pickup(b1, left, r1), . . . , P ickup(bn, left, r1),\n2n 3n+6\nn + 1\n, Drop(b1, left, r2), . . . , Drop(bn, left, r2)〉 Πfleft,ϑ 〈Pickup(b1, right, r1), . . . , P ickup(bn, right, r1),\n2n 3n+6\nn + 1\n, Drop(b1, right, r2), . . . , Drop(bn, right, r2)〉 Πib 〈Pickup(b, right, r1) 1, P ickup(b, right, r1)2,Move(r1, r2), Drop(b, left, r2)2〉 3 3n+6 + 1 n+1 n\nAssuming n > 0 balls should be moved from one room to another, the cost of the optimal plan for the original task is 3n − 1 when n is even, and 3n when n is odd. Therefore, the asymptotic performance ratios for the heuristics hF, hI, hFI on Gripper are 2/3, 0, and 4/9, respectively."
    }, {
      "heading" : "9.2 Logistics",
      "text" : "Each Logistics task consists of some k cities, x airplanes, y trucks and n packages. Each city i is associated with a set Li = {l1i . . . , lαii } of locations within that city; the union of the locations of all the cities is denoted by L = ⋃k i=1 Li. In addition, precisely one location in each city is an airport, and the set of airports is LA = {l11 . . . , l1k} ⊆ L. Each truck can move only within the city in which it is located, and airplanes can fly between airports. The airplanes are denoted by U = {u1, . . . , ux}, the trucks by T = {t1, . . . , ty}, and the packages by P = {p1, . . . , pn}. Let Ti = {t ∈ T | I[t] ∈ Li} denote the trucks of city i, and P = P1 ∪ P2 ∪ P3 ∪ P4 ∪ P5 denote a partition of the packages as follows:\n• each package in P1 = {p ∈ P | I[p], G[p] ∈ LA} is both initially at an airport and needs to be moved to another airport,\n• each package in P2 = {p ∈ P | I[p] ∈ LA ∩ Li, G[p] ∈ Lj \\ LA, i 6= j} is initially at an airport and needs to be moved to a non-airport location in another city,\n• each package in P3 = {p ∈ P | I[p] ∈ Li, G[p] ∈ Li} needs to be moved within one city,\n• each package in P4 = {p ∈ P | I[p] ∈ Li \\LA, G[p] ∈ LA \\Li} needs to be moved from a non-airport location in one city to the airport of some other city, and\n• each package in P5 = {p ∈ P | I[p] ∈ Li \\LA, G[p] ∈ Lj \\LA, i 6= j} needs to be moved from a non-airport location in one city to a non-airport location in another city.\nA natural Logistics task description in sas+ is as follows.\n• Variables V = U ∪ T ∪ P with domains\n∀u ∈ U : D(u) = LA, ∀1 ≤ i ≤ k, ∀t ∈ Ti : D(t) = Li,\n∀p ∈ P : D(p) = L ∪ U ∪ T.\nu1 · · · ux t1 · · · ty\np1 · · · pi · · · pn\nu\np1 . . . pn\nt\np1 . . . pn p\nu1 . . . ux t1 . . . ty\nGfu, u ∈ U Gft , t ∈ T G ip, p ∈ P (a) (b)\nThe (parametrized in n, x, and y) causal graph of Logistics tasks is depicted in Figure 11a."
    }, {
      "heading" : "9.2.1 Fork Decomposition",
      "text" : "Since the variables u ∈ U and t ∈ T have no goal value, the collection of v-forks and viforks is as in Figure 11b. The domains of the inverted-fork sinks are all abstracted as in Eq. 15 (“distance-from-initial-value”), while the domains of the fork roots are abstracted\ntask, as well as the partition of the action costs between these representatives; tables (a) and (b) capture the move and load/unload actions, respectively\nas in Eq. 16 (“leave-one-out”). Thus, we have\nΠF = ⋃ u∈U ⋃ l∈LA {Πfu,l} ∪ k⋃ i=1 ⋃ t∈Ti ⋃ l∈Li {Πft,l},\nΠI = ⋃ p∈P {Πip,1} ∪ ⋃ p∈P2∪P4∪P5 {Πip,2} ∪ ⋃ p∈P5 {Πip,3},\nΠFI = ⋃ u∈U ⋃ l∈LA {Πfu,l} ∪ k⋃ i=1 ⋃ t∈Ti ⋃ l∈Li {Πft,l} ∪ ⋃ p∈P {Πip,1} ∪ ⋃ p∈P2∪P4∪P5 {Πip,2} ∪ ⋃ p∈P5 {Πip,3}.\nThe total number of forks is nf = |ΠF| = |U | · |LA|+ ∑k\ni=1 |Ti| · |Li|, and the total number of inverted forks is ni = |ΠI| = |P1| + 2 · |P2| + |P3| + 2 · |P4| + 3 · |P5|. For each action a ∈ A, the number of its representatives in each abstract task, as well as the cost assigned to each such representative, are given in Figure 12. Each row in the tables of Figure 12 corresponds to a certain Logistics action, each column (except for the last three) represents an abstract task, and each entry captures the number of representatives an action has in the corresponding task. The last three columns show the portion of the total cost that is given to an action representative in each task, in each of the three heuristics in question."
    }, {
      "heading" : "9.2.2 Lower Bound",
      "text" : "Note that any optimal plan for a Logistics task contains at least as many load/unload actions as move actions. Thus, the following lemma provides us with the lower bound of 1/2 for all three heuristics in question.\nLemma 1 For any Logistics task, hF, hI, and hFI account for the full cost of the load/unload actions required by any optimal plan for that task.\nProof: For any Logistics task, all the optimal plans for that task contain the same amount of load/unload actions for each package p ∈ P as follows. p ∈ P1: 2 actions — one load onto an airplane, and one unload from that airplane, p ∈ P2: 4 actions — one load onto an airplane, one unload from that airplane, one load\nonto a truck, and one unload from that truck,\np ∈ P3: 2 actions — one load onto a truck, and one unload from that truck, p ∈ P4: 4 actions — one load onto a truck, one unload from that truck, one load onto an\nairplane, and one unload from that airplane, and\np ∈ P5: 6 actions — two loads onto some trucks, two unloads from these trucks, one load onto an airplane, and one unload from that airplane.\nConsider the fork-decomposition ΠF. Any optimal plan for each of the abstract tasks will contain the number of load/unload actions exactly as above (the effects of these actions remain unchanged in these tasks). The cost of each representative of each load/unload action is 1\nnf , and there are nf abstract tasks. Therefore, the heuristic hF fully accounts for\nthe cost of the required load/unload actions. Now consider the fork-decomposition ΠI. With m being the domain-decomposition index of the abstraction, any optimal plan for the abstract task Πip,m will include one load and one unload actions as follows.\np ∈ P1: one load onto an airplane and one unload from that airplane, p ∈ P2,m=1: one load onto an airplane and one unload from that airplane, p ∈ P2,m=2: one load onto a truck and one unload from that truck, p ∈ P3: one load onto a truck and one unload from that truck, p ∈ P4,m=1: one load onto a truck and one unload from that truck, p ∈ P4,m=2: one load onto an airplane, and one unload from that airplane, p ∈ P5,m=1: one load onto a truck and one unload from that truck, p ∈ P5,m=2: one load onto an airplane and one unload from that airplane, and p ∈ P5,m=3: one load onto a truck and one unload from that truck. The cost of each representative of load/unload actions is 1, and thus the heuristic hI fully accounts for the cost of the required load/unload actions.\nFinally, consider the fork-decomposition ΠFI. Any optimal plan for each of the forkstructured abstract tasks will contain the same number of load/unload actions as for ΠF. The cost of each representative of load/unload actions is 1\nnf+1 and there are nf such abstract\ntasks. In addition, each of these load/unload actions will also appear in exactly one inverted fork-structured abstract task. Therefore the heuristic hFI also fully accounts for the cost of the required load/unload actions.\nthe upper bound of 1/2"
    }, {
      "heading" : "9.2.3 Upper Bound",
      "text" : "An instance on which all three heuristics achieve exactly 1/2 consists of two trucks t1, t2, no airplanes, one city, and n packages such that the initial and goal locations of all the packages are all pairwise different, and both trucks are initially located at yet another location. More formally, if L = {li}2ni=0, and T = {t1, t2}, then the sas+ encoding for this Logistics task is as follows.\n• Variables V = {t1, t2, p1, . . . , pn} with domains\n∀t ∈ T : D(t) = L, ∀p ∈ P : D(p) = L ∪ T.\n• Initial state I = {t1 : l0, t2 : l0, p1 : l1, . . . , pn : ln}.\n• Goal G = {p1 : ln+1, . . . , pn : l2n}.\n• Actions A = {Lt(p, t, l), Ut(p, t, l) | l ∈ L, t ∈ T, p ∈ P} ∪ {Mt(t, l, l′) | t ∈ T, {l, l′} ⊆ L}.\nThe collection of v-forks and v-iforks for this task is depicted in Figure 13. The domains of the inverted-fork sinks are all abstracted as in Eq. 15 (“distance-from-initial-value”), while the domains of the fork roots are abstracted as in Eq. 16 (“leave-one-out”), and therefore we have\nΠF = {Πft1,lΠft2,l | l ∈ L}, ΠI = {Πip,1 | p ∈ P},\nΠFI = {Πft1,lΠft2,l | l ∈ L} ∪ {Πip,1 | p ∈ P}.\nThe total number of forks is thus nf = 4n + 2 and the total number of inverted forks is ni = n. The partition of the action costs for Logistics tasks is described in Figure 12. Here we have P = P3 and thus the action cost partition is as follows.\nAction Πft,l Π f t,l′ Π f t,l′′ Π f t′,l∗ Π i p,1 Π i p′,1 ΠF ΠI ΠFI Mt(t, l, l′) 1 1 0 0 1 0 1 2 1 n 1 n+2 Lt(p, t, l) 1 1 1 1 1 0 1 4n+2 1 1 4n+3 Ut(p, t, l) 1 1 1 1 1 0 1 4n+2 1 1 4n+3\nGiven that, the optimal plans for the abstract task are\nh task optimal plan cost # h(I)\nhF Πft1,l 〈Lt(p1, t2, l1), . . . , Lt(pn, t2, ln), Ut(p1, t2, ln+1), . . . , Ut(pn, t2, l2n)〉\n2n 4n+2\n2n+ 1 2n\nΠft2,l 〈Lt(p1, t1, l1), . . . , Lt(pn, t1, ln), Ut(p1, t1, ln+1), . . . , Ut(pn, t1, l2n)〉 2n 4n+2 2n+ 1\nhI Πipi,1 〈Mt(t1, l0, li), Lt(pi, t1, li),Mt(t1, li, ln+i), Ut(pi, t1, ln+i)〉 2 n + 2 n 2n+ 2 hFI Πft1,l 〈Lt(p1, t2, l1), . . . , Lt(pn, t2, ln), Ut(p1, t2, ln+1), . . . , Ut(pn, t2, l2n)〉 2n 4n+3 2n+ 1 2n+ 2n\nn+2Π f t2,l 〈Lt(p1, t1, l1), . . . , Lt(pn, t1, ln), Ut(p1, t1, ln+1), . . . , Ut(pn, t1, l2n)〉 2n4n+3 2n+ 1 Πipi,1 〈Mt(t1, l0, li), Lt(pi, t1, li),Mt(t1, li, ln+i), Ut(pi, t1, ln+i)〉 2 n+2 + 2 4n+3 n\nwhile an optimal plan for the original task, e.g., 〈Mt(t1, l0, l1), Lt(p1, t1, l1),Mt(t1, l1, l2), Lt(p2, t1, l2), Mt(t1, l2, l3), . . . , Lt(pn, t1, ln),Mt(t1, ln, ln+1), Ut(p1, t1, ln+1),Mt(t1, ln+1, ln+2), Ut(p2, t1, ln+2),\nMt(t1, ln+2, ln+3), . . . , Ut(pn, t1, l2n)〉, has the cost of 4n, providing us with the upper bound of 1/2 for all three heuristics. Putting our lower and upper bounds together, the asymptotic ratio of all three heuristics in question is 1/2."
    }, {
      "heading" : "9.3 Blocksworld",
      "text" : "Each Blocksworld task consists of a table table, a crane c, and n + 1 blocks B = {b1, . . . , bn+1}. Each block can be either on the table, or on top of some other block, or held by the crane. The crane can pick up a block if it currently holds nothing, and that block has no other block on top of it. The crane can drop the held block on the table or on top of some other block.\nConsider now a Blocksworld task as follows. The blocks initially form a tower b1, . . . , bn, bn+1 with bn+1 being on the table, and the goal is to move them to form a tower b1, . . . , bn−1, bn+1, bn with bn being on the table. That is, the goal is to swap the lowest two blocks of the tower. A natural description of this task in sas+ is as follows.\n• Variables V = {b, clearb | b ∈ B} ∪ {c} with domains\nD(c) = {empty} ∪B, ∀b ∈ B : D(b) = {table, c} ∪B \\ {b}, D(clearb) = {yes, no}.\n• Initial state\nI = {c :empty, bn+1 : table, clearb1 :yes} ⋃\n{bi :bi+1 | 1 ≤ i ≤ n} ⋃\n{clearb :no | b ∈ B \\ {b1}} .\n• Goal G = {bn : table, bn+1 :bn, bn−1 :bn+1} ∪ {bi :bi+1 | 1 ≤ i ≤ n− 2}.\n• Actions A = {PT (b), DT (b) | b ∈ B} ∪ {P (b, b′), D(b, b′) | {b, b′} ⊆ B} where\n– pick block b from the table: PT (b) = 〈{c :empty, b : table, clearb :yes}, {cb, b :c}〉, – pick block b from block b′: P (b, b′) = 〈{c :empty, b :b′, clearb :yes, clearb′ :no}, {c :b, b :c, clearb′ :yes}〉,\nthe Blocksworld task used in the proof\n– drop block b on the table: DT (b) = 〈{c :b, b :c}, {c :empty, b : table}〉, and – drop block b on block b′: D(b, b′) = 〈{c :b, b :c, clearb′ :yes}, {c :empty, b :b′, clearb′ :no}〉.\nA schematic version of the causal graph of this task is depicted in Figure 14a. Since only the variables bn−1, bn, bn+1 have goal values that are different from their values in the initial state, the collection of v-forks and v-iforks is as in Figure 14b. After the (“leave-one-out,” Eq. 16) domain abstraction of the variable c, c-fork Gfc breaks down into n + 2 abstract tasks. The sinks of v-iforks G ibn−1 , G ibn , and G ibn+1 also go through the process of domain decomposition (“distance-from-initial-value,” Eq. 15). However, due to the structure of the domain transition graphs of the block variables, domain decomposition here results in only a single abstract task for each of the v-iforks. Thus we have\nΠF ={Πfc,empty} ∪ {Πfc,b | b ∈ B} ∪ {Πfclearb | b ∈ B}, ΠI ={Πibn−1,1,Πibn,1,Πibn+1,1},\nΠFI ={Πfc,empty} ∪ {Πfc,b | b ∈ B} ∪ {Πfclearb | b ∈ B} ∪Π i bn−1,1,Π i bn,1,Π i bn+1,1}.\nIt is technically straightforward to verify that, for each abstract task in ΠF, ΠI, and ΠFI, there exists a plan that (i) involves only the representatives of the actions\n{P (bn−1, bn), DT (bn−1), P (bn, bn+1), DT (bn), PT (bn+1), D(bn+1, bn), PT (bn−1), D(bn−1, bn+1)} , (21)\nand (ii) involves each representative of each original action at most once. Even if together these plans account for the total cost of all eight actions in Eq. 21, the total cost of all these plans (and thus the estimates of all the three heuristics) is upper-bounded by 8, while an optimal plan for the original task, e.g., 〈P (b1, b2), DT (b1), P (b2, b3), DT (b2), . . . , P (bn, bn+1), DT (bn), PT (bn+1), D(bn+1, bn), PT (bn−1), D(bn−1, bn+1), PT (bn−2), D(bn−2, bn−1), . . . , PT (b1), D(b1, b2)〉, has a cost\nv-iforks\nof 4n. Hence, the asymptotic performance ratio of all three heuristics on the Blocksworld domain is 0."
    }, {
      "heading" : "9.4 Miconic",
      "text" : "Each Miconic task consists of one elevator e, a set of floors F , and the passengers P . The elevator can move between |F | floors and on each floor it can load and/or unload passengers. A natural sas+ description of a Miconic task is as follows.\n• Variables V = {e} ∪ P with domains D(e) = F,\n∀p ∈ P : D(p) = F ∪ {e}.\n• Initial state I = {e :fe} ∪ {p :fp | p ∈ P} ∈ (F )|P |+1. • Goal G = {p :f ′p | p ∈ P} ∈ (F )|P |. • Actions A = {In(p, f), Out(p, f) | f ∈ F, p ∈ P} ∪ {Move(f, f ′) | {f, f ′} ⊆ F}, where\n– load passenger p into e on floor f : In(p, f) = 〈{e :f, p :f}, {p :e}〉, – unload passenger p from e to floor f : Out(p, f) = 〈{e :f, p :e}, {p :f}〉, and – move elevator from floor f to floor f ′: Move(f, f ′) = 〈{e :f}, {e :f ′}〉.\nThe (parametrized in n) causal graph of Miconic tasks is depicted in Figure 15a, and Figure 15b depicts the corresponding collection of v-forks and v-iforks. The domains of the inverted-fork sinks are all abstracted as in Eq. 15 (“distance-from-initial-value”), and the domains of the fork roots are abstracted as in Eq. 16 (“leave-one-out”). Thus, we have\nΠF = {Πfe,f | f ∈ F}, ΠI = {Πip,1 | p ∈ P},\nΠFI = {Πfe,f | f ∈ F} ∪ {Πip,1 | p ∈ P}. The total number of the fork-structured abstract tasks is thus nf = |ΠF| = |F | and the total number of the inverted fork structured abstract tasks is ni = |ΠI| = |P |. For each action a ∈ A, the number of its representatives in each abstract task, as well as the cost assigned to each such representative, are given in Table 7.\nas well as the partition of the action costs among these representatives"
    }, {
      "heading" : "9.4.1 Lower Bounds",
      "text" : "First, as Miconic is a special case of the Logistics domain, Lemma 1 applies here analogously, with each package in P3 corresponding to a passenger. Thus, for each p ∈ P , all three heuristics account for the full cost of the load/unload actions required by any optimal plan for that task.\nLet us now focus on the abstract tasks ΠF = {Πfe,f | f ∈ F}. Recall that the task Πfe,f is induced by an e-fork and, in terms of domain decomposition, distinguishes between being at floor f and being somewhere else. Without loss of generality, the set of floors F can be restricted to the initial and the goal values of the variables, and this because no optimal plan will move the elevator to or from a floor f that is neither an initial nor a goal location of a passenger or the elevator. Let FI = {I[p] | p ∈ P} and FG = {G[p] | p ∈ P}. The costs of the optimal plans for each abstract task Πfe,f are as follows.\nf ∈ FI ∩ FG : Let p, p′ ∈ P be a pair of passengers with initial and goal locations in f , respectively; that is, I[p] = G[p′] = f . If f = I[e], then any plan for Πfe,f has to move the elevator from f in order to load passenger p′, and then move the elevator back to f in order to unload passenger p′. Therefore the cost of any plan for Πfe,f is at\nleast 2|P ||F | + 1, where (see the last three columns of Table 7) the first component of the\nsummation comes from summing the costs of the representatives of the load/unload actions for all the passengers, and the second component is the sum of the costs of representatives of the two respective move actions. Similarly, if f 6= I[e], then any plan for Πfe,f has to move the elevator to f in order to load passenger p, and then move the elevator from f in order to unload p. Therefore, here as well, the cost of any plan for Πfe,f is at least 2|P | |F | + 1.\nf ∈ FI \\ FG : Let p ∈ P be a passenger initially at f , that is, I[p] = f . If f = I[e], then any plan for Πfe,f has to move the elevator from f in order to unload p, and thus the\ncost of any plan for Πfe,f is at least 2|P | |F | + 1 2 . Otherwise, if f 6= I[e], then any plan for Πfe,f has to move the elevator to f in order to load p, and then move the elevator from f in order to unload p. Hence, in this case, the cost of any plan for Πfe,f is at least 2|P ||F | + 1.\nf ∈ FG \\ FI : Let p ∈ P be a passenger who must arrive at floor f , that is, G[p] = f . If f = I[e], then any plan for Πfe,f has to move the elevator from f in order to load p, and then move the elevator back to f in order to unload p. Hence, here as well, the cost of any plan for Πfe,f is at least 2|P | |F | + 1. Otherwise, if f 6= I[e], then any plan for\nΠfe,f has to move the elevator to f in order to unload p, and thus the cost of any plan for Πfe,f is at least 2|P | |F | + 1 2 .\nf 6∈ FG ∪ FI : If f = I[e], then any plan for Πfe,f has to include a move from f in order to load/unload the passengers, and thus the cost of any plan for Πfe,f is at least 2|P | |F | + 1 2 .\nOtherwise, if f 6= I[e], the elevator is initially “in the set of all other locations,” and thus the cost of any plan for Πfe,f is at least 2|P | |F | .\nPutting this case-by-case analysis together, we have\nhF(I) ≥  2|P |+ |FI ∩ FG|+ |FI \\ FG|+ |FG\\FI |2 , I[e] ∈ FI ∩ FG 2|P |+ |FI ∩ FG|+ |FI \\ FG| − 1 + 12 + |FG\\FI | 2 , I[e] ∈ FI \\ FG\n2|P |+ |FI ∩ FG|+ |FI \\ FG|+ 1 + |FG\\FI |−12 , I[e] ∈ FG \\ FI 2|P |+ |FI ∩ FG|+ |FI \\ FG|+ |FG\\FI |−12 + 12 , I[e] 6∈ FG ∪ FI\n.\nNote that the value in the second case is the lowest. This gives us a lower bound on the hF\nestimate as in Eq. 22.\nhF(I) ≥ 2|P |+ |FI \\ FG|+ |FG \\ FI |\n2 + |FI ∩ FG| −\n1 2 . (22)\nNow, let us provide an upper bound on the length (= cost) of the optimal plan for a Miconic task. First, let P ′ ⊆ P denote the set of passengers with both initial and goal locations in FI ∩ FG. Let m(P ′, FI ∩ FG) denote the length of the optimal traversal of the floors FI ∩FG such that, for each passenger p ∈ P ′, a visit of I[p] comes before some visit of G[p]. Given that, on a case-by-case basis, a (not necessarily optimal) plan for the Miconic task at hand is as follows.\nI[e] ∈ FI ∩ FG : Collect all the passengers at I[e] if any, then traverse all the floors in FI \\ FG and collect passengers from these floors, then move the elevator to the first floor f on the optimal path π traversing the floors FI ∩ FG, drop off the passengers whose destination is f , collect the new passengers if any, keep moving along π while collecting and dropping off passengers at their initial and target floors, and then traverse FG \\ FI , dropping off the remaining passengers at their destinations. The cost of such a plan (and thus of the optimal plan) is upper-bounded as in Eq. 23 below.\nh∗(I) ≤ 2|P |+ |FI \\ FG|+m(P ′, FI ∩ FG) + |FG \\ FI |. (23)\nI[e] ∈ FI \\ FG : Collect all the passengers at I[e] if any, then traverse all the floors in FI \\FG and collect passengers from these floors while making sure that this traversal ends up at the first floor f of the optimal path π traversing the floors FI ∩ FG, then follow π while collecting and dropping passengers off at their initial and target floors, and then traverse FG\\FI , dropping the remaining passengers off at their destinations. As in the first case, the cost of such a plan is upper-bounded as in Eq. 23.\nI[e] 6∈ FI : Traverse the floors FI \\FG and collect all the passengers from these floors, then move along the optimal path π traversing the floors FI ∩ FG while collecting and dropping off passengers at their initial and target floors, and then traverse the floors FG \\FI , dropping the remaining passengers off at their destinations. Here as well, the cost of such a plan is upper-bounded by the expression in Eq. 23.\nLemma 2 For any Miconic task with passengers P , we have h F(I) h∗(I) ≥ 5|P |−1 6|P | .\nProof: Recall that P ′ ⊆ P is the set of all passengers with both initial and goal locations in FI ∩ FG. First we give two upper bounds on the length of the optimal traversal of the floors FI ∩ FG such that, for each passenger p ∈ P ′, a visit of I[p] comes before some visit of G[p]. From Theorem 5.3.3 of Helmert (2008) we have\nm(P ′, FI ∩ FG) = |FI ∩ FG|+m∗(G′), (24)\nwhere m∗(G′) is the size of the minimum feedback vertex set of the directed graph G′ = (V ′, E ′), with V ′ = FI ∩ FG and E ′ containing an arc from f to f ′ if and only if a passenger p ∈ P ′ is initially at floor f and should arrive at floor f ′.\nNote that m∗(G′) is trivially bounded by the number of graph nodes V ′. In addition, observe that, for any order of the nodes V ′, the arcs E ′ can be partitioned into “forward” and “backward” arcs, and one of these subsets must contain no more than |E\n′| 2 arcs. Removing\nfrom G′ all the nodes that are origins of the arcs in that smaller subset of E ′ results in a directed acyclic graph. Hence, the set of removed nodes is a (not necessarily minimum) feedback vertex set of G′, and the size of this set is no larger than |E ′|2 . Putting these two bounds on m∗(G′) together with Eq. 24 we obtain\nm(P ′, FI ∩ FG) ≤ min {\n2|FI ∩ FG|, |FI ∩ FG|+ |P ′| 2\n} . (25)\nFrom the disjointness of FG \\ FI and FI ∩ FG, and the fact that the goal of all the passengers in P ′ is in FI , we have |FG \\ FI | ≤ |P | − |P ′|. From Eqs. 22 and 23 we have\nhF h∗ ≥ 2|P |+ |FI \\ FG|+ |FG\\FI | 2 + |FI ∩ FG| − 12 2|P |+ |FI \\ FG|+ |FG \\ FI |+m(P ′, FI ∩ FG) . (26)\nAs we are interested in a lower bound on the ratio h F\nh∗ , the right-hand side of the inequality should be minimized, and thus we can safely set |FI \\ FG| = 0 and |FG \\ FI | = |P | − |P ′|, obtaining\nhF h∗ ≥ 2|P |+\n|P |−|P ′| 2 + |FI ∩ FG| − 12\n2|P |+ |P | − |P ′|+m(P ′, FI ∩ FG) = 5|P | − |P ′|+ 2|FI ∩ FG| − 1 6|P | − 2|P ′|+ 2m(P ′, FI ∩ FG) . (27)\nLet us examine the right-most expression in Eq. 27 with respect to the two upper bounds on m(P ′, FI ∩ FG) as in Eq. 25. • If the minimum is obtained on 2|FI ∩ FG|, then m(P ′, FI ∩ FG) ≤ 2|FI ∩ FG| ≤ |FI ∩ FG|+ |P ′| 2 , where the last inequality can be reformulated as\n2|FI ∩ FG| − |P ′| ≤ 0.\nThis allows us to provide a lower bound on the right-most expression in Eq. 27, and thus on h F\nh∗ as\nhF h∗ ≥ 5|P | − |P ′|+ 2|FI ∩ FG| − 1 6|P | − 2|P ′|+ 2m(P ′, FI ∩ FG) ≥ 5|P |+ (2|FI ∩ FG| − |P ′|)− 1 6|P |+ 2(2|FI ∩ FG| − |P ′|) ≥ 5|P | − 1\n6|P | . (28)\n• If the minimum is obtained on |FI∩FG|+ |P ′| 2 , then m(P ′, FI∩FG) ≤ |FI∩FG|+ |P ′| 2 <\n2|FI ∩ FG|, where the last inequality can be reformulated as\n2|FI ∩ FG| − |P ′| > 0.\nThis again allows us to provide a lower bound on h F\nh∗ via Eq. 27 as\nhF h∗ ≥ 5|P | − |P ′|+ 2|FI ∩ FG| − 1 6|P | − 2|P ′|+ 2m(P ′, FI ∩ FG) ≥ 5|P |+ (2|FI ∩ FG| − |P ′|)− 1 6|P |+ (2|FI ∩ FG| − |P ′|) ≥ 5|P | − 1\n6|P | . (29)\nNote that both lower bounds on h F\nh∗ in Eq. 28 and Eq. 29 are as required by the claim of the lemma."
    }, {
      "heading" : "9.4.2 Upper Bounds",
      "text" : "A Miconic task on which the heuristic hF achieves the performance ratio of exactly 5/6 consists of an elevator e, floors F = {fi}ni=0, passengers P = {pi}ni=1, all the passengers and the elevator being initially at f0, and the target floors of the passengers all being pairwise disjoint. The sas+ encoding for the Miconic task is as follows.\n• Variables V = {e} ∪ P with the domains D(e) = F and ∀p ∈ P : D(p) = F ∪ {e}.\n• Initial state I = {e :f0, p1 :f0, . . . , pn :f0}.\n• Goal G = {p1 :f1, . . . , pn :fn}.\n• Actions A = {In(p, f), Out(p, f) | f ∈ F, p ∈ P} ∪ {Move(f, f ′) | {f, f ′} ⊆ F}.\nThe causal graph of this task and the corresponding collection of v-forks (consisting of only one e-fork) are depicted in Figure 15. The domain of e is abstracted as in Eq. 16 (“leave-one-out”), providing us with\nΠF = {Πfe,f0 ,Πfe,f1 , . . . ,Πfe,fn}.\nThe costs of the action representatives in these abstract tasks are given in Table 7 with nf = n+ 1. The optimal plans for the abstract tasks in ΠF are\ntask optimal plan cost # hF(I)\nΠfe,f0 〈In(p1, f0), . . . , In(pn, f0),Move(f0, f1), Out(p1, f1), . . . , Out(pn, fn)〉 1 2 + 2n n+1 n+ 1 5n+1 2Π\nf e,f1 〈In(p1, f0), . . . , In(pn, f0), Out(p2, f2), . . . , Out(pn, fn),Move(f0, f1), Out(p1, f1)〉 12 + 2n n+1\nΠfe,fn 〈In(p1, f0), . . . , In(pn, f0), Out(p1, f1), . . . , Out(pn−1, fn−1),Move(f0, fn), Out(pn, fn)〉 1 2 + 2n n+1\nwhile an optimal plan for the original task, 〈In(p1, f0), . . . , In(pn, f0),Move(f0, f1), Out(p1, f1), Move(f1, f2), Out(p2, f2),Move(f2, f3), . . . , Out(pn, fn)〉, has a cost of 3n, providing us with the upper bound of 5/6 for the hF heuristic in Miconic. Putting this upper bound together with the previously obtained lower bound of 5/6, we conclude that the asymptotic performance ratio of hF in Miconic is 5/6.\nA Miconic task on which the heuristics hI and hFI achieve exactly 1/2 consists of an elevator e, floors F = {fi}2ni=0, passengers P = {pi}ni=1, and the initial and target floors for all the passengers and the elevator being pairwise disjoint. The task description in sas+ is as follows.\n• Variables V = {e} ∪ P with the domains D(e) = F and ∀p ∈ P : D(p) = F ∪ {e}.\n• Initial state I = {e :f0, p1 :f1, . . . , pn :fn}.\n• Goal G = {p1 :fn+1, . . . , pn :f2n}.\n• Actions A = {In(p, f), Out(p, f) | f ∈ F, p ∈ P} ∪ {Move(f, f ′) | {f, f ′} ⊆ F}.\nThe causal graph of this task and the corresponding collection of v-forks and v-iforks are depicted in Figure 15. The domains of the inverted-fork sinks are all abstracted as in Eq. 15 (“distance-from-initial-value”), and the domains of the fork roots are all abstracted as in Eq. 16 (“leave-one-out”). This provides us with\nΠI = {Πip1,1, . . . ,Πipn,1}, ΠFI = {Πfe,f0 ,Πfe,f1 , . . . ,Πfe,fn ,Πfe,fn+1 , . . . ,Πfe,f2n ,Πip1,1, . . . ,Πipn,1}.\nThe costs of the action representatives in these abstract tasks are given in Table 7 with nf = 2n+ 1 and ni = n. The optimal plans for the abstract tasks in ΠI and ΠFI are\nh task optimal plan cost # h(I)\nhI Πipi,1 〈Move(f0, fi), In(pi, fi),Move(fi, fn+i), Out(pi, fn+i)〉 2 n + 2 n 2n+ 2 hFI Πfe,f0 〈Move(f0, f1), In(p1, f1), . . . , In(pn, fn), Out(p1, fn+1), . . . , Out(pn, f2n)〉 1 n+2 + 2n 2n+2 1 2n+ 5n+1 n+2\nΠfe,f1 〈Move(f0, f1), In(p1, f1),Move(f1, f2), In(p2, f2), . . . , In(pn, fn), Out(p1, fn+1), . . . , Out(pn, f2n)〉\n2 n+2 + 2n 2n+2 n\nΠfe,fn 〈Move(f0, fn), In(pn, fn),Move(fn, f1), In(p1, f1), . . . , In(pn−1, fn−1), Out(p1, fn+1), . . . , Out(pn, f2n)〉\n2 n+2 + 2n 2n+2\nΠfe,fn+1 〈In(p1, f1), . . . , In(pn, fn), Out(p2, fn+2), . . . , Out(pn, f2n), Move(f0, fn+1), Out(p1, fn+1)〉\n1 n+2 + 2n 2n+2 n\nΠfe,f2n 〈In(p1, f1), . . . , In(pn, fn), Out(p1, fn+1), . . . , Out(pn−1, f2n−1), Move(f0, f2n), Out(pn, f2n)〉\n1 n+2 + 2n 2n+2\nΠipi,1 〈Move(f0, fi), In(pi, fi),Move(fi, fn+i), Out(pi, fn+i)〉 2 n+2 + 2 2n+2 n\nwhile an optimal plan for the original task, 〈Move(f0, f1), In(p1, f1),Move(f1, f2), In(p2, f2), Move(f2, f3), . . . , In(pn, fn),Move(fn, fn+1), Out(p1, fn+1),Move(fn+1, fn+2), Out(p2, fn+2),\nMove(fn+2, fn+3), . . . , Out(pn, f2n)〉, has the cost of 4n, providing us with the upper bound of 1/2 for the hI and hFI heuristics in Miconic. Putting this upper bound together with the previously obtained lower bound of 1/2, we conclude that the asymptotic performance ratio of hI and hFI in Miconic is 1/2."
    }, {
      "heading" : "9.5 Satellite",
      "text" : "The Satellite domain is quite complex. A Satellite tasks consists of some satellites S, each s ∈ S with a finite set of instruments Is onboard, I = ⋃ s∈S Is. There is a set of image modes M, and for each mode m ∈ M, there is a set Im ⊆ I of instruments supporting mode m. Likewise, there is a set of directions L, image objectives O ⊆ L×M, and functions cal : I 7→ L, p0 : S 7→ L, and p∗ : S0 7→ L with S0 ⊆ S, where cal is the calibration target direction function, p0 is the initial direction function, and p∗ is the goal pointing direction function.\nLet us denote by Oi = {o = (d,m) ∈ O | i ∈ Im} the subset of all images that can be taken by instrument i, by Os = ⋃i∈Is Oi the subset of all images that can be taken by instruments on satellite s, and by Sm = {s | Is ∩ Im 6= ∅} the subset of all satellites that can take images in mode m. The problem description in sas+ is as follows.\n• Variables V = S ∪ {Oni,Ci | i ∈ I} ∪ O with domains\n∀s ∈ S : D(s) = L, ∀i ∈ I : D(Oni) = D(Ci) = {0, 1}, ∀o ∈ O : D(o) = {0, 1}.\n• Initial state I = {s :p0(s) | s ∈ S} ∪ {Oni :0,Ci :0 | i ∈ I} ∪ {o :0 | o ∈ O}.\n• Goal G = {s :p∗(s) | s ∈ S0} ∪ {o :1 | o ∈ O}.\n• Actions A = ⋃ s∈S ( {Turn(s, d, d′) | {d, d′} ⊆ L} ∪ {SwOn(i, s), Cal(i, s), SwOff(i) | i ∈ Is} ) ∪\n{TakeIm(o, d, s, i) | o = (d,m) ∈ O, s ∈ Sm, i ∈ Im ∩ Is},\nwhere\n– turn satellite: Turn(s, d, d′) = 〈{s :d}, {s :d′}〉, – power on instrument: SwOn(i, s) = 〈{Oni′ :0 | i′ ∈ Is}, {Oni :1}〉, – power off instrument: SwOff(i) = 〈{Oni :1}, {Oni :0,Ci :0}〉, – calibrate instrument: Cal(i, s) = 〈{Ci :0, Oni :1, s :cal(i)}, {Ci :1}〉, and – take an image: TakeIm(o, d, s, i) = 〈{o :0,Ci :1, s :d}, {o :1}〉."
    }, {
      "heading" : "9.5.1 Fork Decomposition",
      "text" : "The causal graph of an example Satellite task and a representative subset of the collection of v-forks and v-iforks are depicted in Figure 16. Since the variables {Oni,Ci | i ∈ I}∪S\\S0 have no goal value, the collection of v-forks and v-iforks will be as follows in the general case.\n• For each satellite s ∈ S, an s-fork with the leaves Os.\ncollection of v-forks and v-iforks\n• For each instrument i ∈ I, a Ci-fork with the leaves Oi.\n• For each image objective o = (d,m) ∈ O, a o-ifork with the parents {Ci | i ∈ Im}∪Sm.\nThe root domains of all forks rooted at instruments i ∈ I and of all the inverted-fork sinks are binary in the first place, and the root domains of the forks rooted at satellites s ∈ S are abstracted as in Eq. 16 (“leave-one-out”). This provides us with\nΠF = {Πfs,d | s ∈ S, d ∈ L} ∪ {ΠfCi | i ∈ I}, ΠI = {Πio | o ∈ O},\nΠFI = {Πfs,d | s ∈ S, d ∈ L} ∪ {ΠfCi | i ∈ I} ∪ {Πio | o ∈ O}.\nThe total number of forks is thus nf = |S| · |L| + |I| and the total number of inverted forks is ni = |O|. For each action a ∈ A, the number of its representatives in each abstract task, as well as the cost assigned to each such representative, are given in Figure 17."
    }, {
      "heading" : "9.5.2 Lower Bounds",
      "text" : "First, note that any optimal plan for a Satellite task contains at most 6 actions per image objective o ∈ O and one action per satellite s ∈ S0 such that I[s] 6= G[s]. Now we show that each of the three heuristics fully account for the cost of at least one action per image objective o ∈ O and one action per such a satellite. This will provide us with the lower bound of 1/6 on the asymptotic performance ratios of our three heuristics.\nLemma 3 For any Satellite task, hF, hI, and hFI fully account for the cost of at least one Take Image action TakeIm(o, d, s, i) for each image objective o ∈ O.\nProof: For an image objective o = (d,m) ∈ O, some actions TakeIm(o, d, s, i) = 〈{o : 0,Ci : 1, s : d}, {o : 1}〉 will appear in optimal plans for |Sm| · |L| fork abstract tasks rooted\ntask, as well as the partition of the action costs between these representatives; table (a) shows Turn, Switch On, Switch Off, and Calibrate actions, and table (b) shows Take Image actions\nin satellites, |Im| fork abstract tasks rooted in instrument calibration status variables Ci, and one inverted-fork abstract task with sink o. Together with the costs of the action representatives in the abstract problems (see Figure 17), we have\nhF : cost of each representative is 1|Sm|·|L|+|Im| and there are |Sm| · |L|+ |Im| fork abstract tasks,\nhI : cost of each representative is 1 and there is one inverted fork abstract task, and\nhFI : cost of each representative is 1|Sm|·|L|+|Im|+1 and there are |Sm| · |L|+ |Im|+ 1 abstract tasks.\nTherefore, for each o ∈ O, the cost of one TakeIm(o, d, s, i) action will be fully accounted for by each of the three heuristics.\nLemma 4 For any Satellite task, hF, hI, and hFI fully account for the cost of at least one Turn action Turn(s, d, d′) for each s ∈ S0 such that I[s] 6= G[s].\nProof: If s ∈ S0 is a satellite with I[s] 6= G[s], then an action Turn(s, I[s], d′) will appear in any optimal plan for Πfs,I[s], an action Turn(s, d,G[s]) will appear in any optimal plan for Πfs,G[s], and for each o ∈ Os, an action Turn(s, d,G[s]) will appear in any optimal plan for Πio. Together with the costs of the action representatives in the abstract problems (see Figure 17) we have\nhF : cost of each representative is 12 and there are 2 fork abstract tasks,\nhI : cost of each representative is 1|Os| and there are |Os| inverted fork abstract tasks, and\nhFI : cost of each representative is 1|Os|+2 and there are |Os|+ 2 abstract tasks.\nTherefore, for each s ∈ S0 such that I[s] 6= G[s], the cost of one Turn(s, d, d′) action will be fully accounted for by each of the three heuristics.\nTogether, Lemmas 3 and 4 imply that, for h ∈ {hF, hI, hFI}, on Satellite we have h h∗ ≥ 1/6."
    }, {
      "heading" : "9.5.3 Upper Bound",
      "text" : "A Satellite task on which all three heuristics achieve the ratio of exactly 1/6 consists of two identical satellites S = {s, s′} with l instruments each, I = Is ∪ Is′ = {1, . . . , l} ∪ {l + 1, . . . , 2l}, such that instruments {i, l+i} have two modes each: m0 and mi. There is a set of n+ 1 directions L = {dI , d1, . . . , dn} and a set of n image objectives O = {o1, . . . , on}, oi = (dI ,mi) for 1 ≤ i ≤ l and oi = (di,m0) for l < i ≤ n. The calibration direction of instruments {i, l + i} is di. The sas+ encoding for this planning task is as follows.\n• Variables V = S ∪ O ∪ {Oni,Ci | i ∈ I}.\n• Initial state I = {s :dI | s ∈ S} ∪ {Oni :0,Ci :0 | i ∈ I} ∪ {o :0 | o ∈ O}.\n• Goal G = {o :1 | o ∈ O}.\n• Actions A = ⋃ s∈S ( {Turn(s, d, d′) | {d, d′} ⊆ L} ∪ {SwOn(i, s), Cal(i, s), SwOff(i) | i ∈ Is} ) ∪\n⋃ s∈S {TakeIm((dI ,mi), dI , s, i) | i ∈ Is} ∪ n⋃ j=l+1 {TakeIm((dj ,m0), dj , s, i) | i ∈ Is}  . The causal graph of this task is depicted in Figure 18a. The state variables {Oni,Ci |\ni ∈ I} ∪ S have no goal value, and thus the collection of v-forks and v-iforks for this task is as in Figure 18b. The domains of the inverted-fork sinks are binary, and the domains of the fork roots are abstracted as in Eq. 16 (“leave-one-out”). This provides us with\nΠF = {Πfs,d,Πfs′,d | d ∈ L} ∪ {ΠfCi | i ∈ I}, ΠI = {Πio | o ∈ O},\nΠFI = {Πfs,d,Πfs′,d | d ∈ L} ∪ {ΠfCi | i ∈ I} ∪ {Πio | o ∈ O}.\nThe total number of forks in this task is nf = 2n+ 2l+ 2 and the total number of inverted forks is ni = n. The costs of the action representatives in each abstract task are given in Figure 17, where |Os| = |Os′ | = |O| = n, |Oi| = n− l + 1, |Sm| = 2, |Im0 | = 2l, |Imi | = 2, and |L| = n+ 1.\nThe optimal plans per abstract task are depicted in Table 8, while an optimal plan for the original problem, 〈SwOn(1, s), Turn(s, dI , d1), Cal(1, s), Turn(s, d1, dI), TakeIm(o1, dI , s, 1),\nthe Satellite task used in the proof of the upper bound of 1/6\nSatellite task used in the proof of the upper bound of 1/6\nSwOff(1), . . . SwOn(l − 1, s), Turn(s, dI , dl−1), Cal(l − 1, s), Turn(s, dl−1, dI), TakeIm(ol−1, dI , s, l − 1), SwOff(l − 1), SwOn(l, s), Turn(s, dI , dl), Cal(l, s), Turn(s, dl, dI), TakeIm(ol, dI , s, l), Turn(s, dI , dl+1), TakeIm(ol+1, dl+1, s, l), . . . , Turn(s, dn−1, dn), TakeIm(on, dn, s, l)〉, has the cost of 4l + 2n − 1. For\nl = n − √n, this provides us with the asymptotic performance ratio of 1/6 for all three heuristics."
    }, {
      "heading" : "10. Summary",
      "text" : "We considered heuristic search for cost-optimal planning and introduced a domain-independent framework for devising admissible heuristics using additive implicit abstractions. Each such implicit abstraction corresponds to abstracting the planning task at hand by an instance of a tractable fragment of optimal planning. The key motivation for our investigation was to escape the restriction of explicit abstractions, such as pattern-database and merge-and-shrink abstractions, to abstract spaces of a fixed size. We presented a concrete scheme for additive implicit abstractions by decomposing the planning task along its causal graph and suggested a concrete realization of this idea, called fork-decomposition, that is based on two novel fragments of tractable cost-optimal planning. We then studied the induced admissible heuristics both formally and empirically, and showed that they favorably compete in informativeness with the state-of-the-art admissible heuristics both in theory and in practice. Our empirical evaluation stressed the tradeoff between the accuracy of the heuristics and runtime complexity of computing them. To alleviate the problem of expensive per-search-node runtime complexity of fork-decomposition heuristics, we showed that an equivalent of the explicit abstractions’ notion of “database” exists also for the fork-decomposition abstractions, and this despite their exponential-size abstract spaces. Our subsequent empirical evaluation of heuristic search with such databases for the fork-decomposition heuristics showed that it favorably competes with the state of the art of cost-optimal planning.\nThe basic principles of the implicit abstraction framework motivate further research in numerous directions, most importantly in (i) discovering new islands of tractability of optimal planning, and (ii) abstracting the general planning tasks into such islands. Likewise, there is promise in combining implicit abstractions with other techniques for deriving admissible heuristic estimates. A first step towards combining implicit abstractions with polynomial-time discoverable landmarks of the planning tasks has recently been taken by Domshlak, Katz, and Lefler (2010). We believe that various combinations of such techniques might well improve the informativeness of the heuristics, and this without substantially increasing their runtime complexity."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The work of both authors was partly supported by Israel Science Foundation grants 670/07 and 1101/07."
    }, {
      "heading" : "Appendix A. Detailed Results of Empirical Evaluation",
      "text" : "Blocksworld, Depots, and Grid domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are computed fully online. Column task denotes problem instance, column h∗ denotes optimal solution length. Other columns capture the run time and number of expanded nodes.\nLogistics-ipc2, and Mprime domains.\nPipesworld-Tankage, TPP, and Trucks domains.\nBlocksworld, Depots, and Driverlog domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are via structural-pattern databases. Column task denotes problem instance, column h∗ denotes optimal solution length. Other columns capture the run time and number of expanded nodes.\nLogistics-ipc2, and Mprime domains.\nNoTankage, Pipesworld-Tankage, Rovers, and Satellite domains.\nOpenstacks-strips-08, Parcprinter, and Scanalyzer domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are via structural-pattern databases. Column task denotes problem instance, column h∗ denotes optimal solution length. Other columns capture the run time and number of expanded nodes.\ning domains."
    } ],
    "references" : [ {
      "title" : "Complexity results for SAS+ planning",
      "author" : [ "C. Bäckström", "B. Nebel" ],
      "venue" : "Computational Intelligence,",
      "citeRegEx" : "Bäckström and Nebel,? \\Q1995\\E",
      "shortCiteRegEx" : "Bäckström and Nebel",
      "year" : 1995
    }, {
      "title" : "Planning as heuristic search",
      "author" : [ "B. Bonet", "H. Geffner" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Bonet and Geffner,? \\Q2001\\E",
      "shortCiteRegEx" : "Bonet and Geffner",
      "year" : 2001
    }, {
      "title" : "The computational complexity of propositional STRIPS planning",
      "author" : [ "T. Bylander" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Bylander,? \\Q1994\\E",
      "shortCiteRegEx" : "Bylander",
      "year" : 1994
    }, {
      "title" : "Causal graphs and structurally restricted planning",
      "author" : [ "H. Chen", "O. Gimenez" ],
      "venue" : "In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Chen and Gimenez,? \\Q2008\\E",
      "shortCiteRegEx" : "Chen and Gimenez",
      "year" : 2008
    }, {
      "title" : "Additive-disjunctive heuristics for optimal planning",
      "author" : [ "A.I. Coles", "M. Fox", "D. Long", "A.J. Smith" ],
      "venue" : "In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Coles et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Coles et al\\.",
      "year" : 2008
    }, {
      "title" : "Multi-agent off-line coordination: Structure and complexity",
      "author" : [ "C. Domshlak", "Y. Dinitz" ],
      "venue" : "In Proceedings of Sixth European Conference on Planning (ECP),",
      "citeRegEx" : "Domshlak and Dinitz,? \\Q2001\\E",
      "shortCiteRegEx" : "Domshlak and Dinitz",
      "year" : 2001
    }, {
      "title" : "Friends or foes? On planning as satisfiability and abstract CNF encodings",
      "author" : [ "C. Domshlak", "J. Hoffmann", "A. Sabharwal" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Domshlak et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Domshlak et al\\.",
      "year" : 2009
    }, {
      "title" : "When abstractions met landmarks",
      "author" : [ "C. Domshlak", "M. Katz", "S. Lefler" ],
      "venue" : "In Proceedings of the 20th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Domshlak et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Domshlak et al\\.",
      "year" : 2010
    }, {
      "title" : "Directed model checking with distancepreserving abstractions",
      "author" : [ "K. Dräger", "B. Finkbeiner", "A. Podelski" ],
      "venue" : "Proceedings of the 13th International SPIN Workshop on Model Checking Software,",
      "citeRegEx" : "Dräger et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dräger et al\\.",
      "year" : 2006
    }, {
      "title" : "Planning with pattern databases",
      "author" : [ "S. Edelkamp" ],
      "venue" : "In Proceedings of the European Conference on Planning (ECP),",
      "citeRegEx" : "Edelkamp,? \\Q2001\\E",
      "shortCiteRegEx" : "Edelkamp",
      "year" : 2001
    }, {
      "title" : "Symbolic pattern databases in heuristic search planning",
      "author" : [ "S. Edelkamp" ],
      "venue" : "In Proceedings of the International Conference on AI Planning and Scheduling (AIPS),",
      "citeRegEx" : "Edelkamp,? \\Q2002\\E",
      "shortCiteRegEx" : "Edelkamp",
      "year" : 2002
    }, {
      "title" : "Automated creation of pattern database search heuristics",
      "author" : [ "S. Edelkamp" ],
      "venue" : "In Proceedings of the 4th Workshop on Model Checking and Artificial Intelligence (MoChArt)",
      "citeRegEx" : "Edelkamp,? \\Q2006\\E",
      "shortCiteRegEx" : "Edelkamp",
      "year" : 2006
    }, {
      "title" : "Optimal symbolic planning with action costs and preferences",
      "author" : [ "S. Edelkamp", "P. Kissmann" ],
      "venue" : "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Edelkamp and Kissmann,? \\Q2009\\E",
      "shortCiteRegEx" : "Edelkamp and Kissmann",
      "year" : 2009
    }, {
      "title" : "Additive pattern database heuristics",
      "author" : [ "A. Felner", "R.E. Korf", "S. Hanan" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Felner et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Felner et al\\.",
      "year" : 2004
    }, {
      "title" : "Additive and reversed relaxed reachability heuristics revisited",
      "author" : [ "P. Haslum" ],
      "venue" : "In Proceedings of the 6th International Planning Competition",
      "citeRegEx" : "Haslum,? \\Q2008\\E",
      "shortCiteRegEx" : "Haslum",
      "year" : 2008
    }, {
      "title" : "New admissible heuristics for domainindependent planning",
      "author" : [ "P. Haslum", "B. Bonet", "H. Geffner" ],
      "venue" : "In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Haslum et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Haslum et al\\.",
      "year" : 2005
    }, {
      "title" : "Domain-independent construction of pattern database heuristics for cost-optimal planning",
      "author" : [ "P. Haslum", "A. Botea", "M. Helmert", "B. Bonet", "S. Koenig" ],
      "venue" : "In Proceedings of the 19th National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Haslum et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Haslum et al\\.",
      "year" : 2007
    }, {
      "title" : "Admissible heuristics for optimal planning",
      "author" : [ "P. Haslum", "H. Geffner" ],
      "venue" : "In Proceedings of the Fifth International Conference on Artificial Intelligence Planning Systems (ICAPS),",
      "citeRegEx" : "Haslum and Geffner,? \\Q2000\\E",
      "shortCiteRegEx" : "Haslum and Geffner",
      "year" : 2000
    }, {
      "title" : "Complexity results for standard benchmark domains in planning",
      "author" : [ "M. Helmert" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Helmert,? \\Q2003\\E",
      "shortCiteRegEx" : "Helmert",
      "year" : 2003
    }, {
      "title" : "A planning heuristic based on causal graph analysis",
      "author" : [ "M. Helmert" ],
      "venue" : "In Proceedings of the 14th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Helmert,? \\Q2004\\E",
      "shortCiteRegEx" : "Helmert",
      "year" : 2004
    }, {
      "title" : "The Fast Downward planning system",
      "author" : [ "M. Helmert" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Helmert,? \\Q2006\\E",
      "shortCiteRegEx" : "Helmert",
      "year" : 2006
    }, {
      "title" : "Landmarks, critical paths and abstractions: What’s the difference anyway",
      "author" : [ "M. Helmert", "C. Domshlak" ],
      "venue" : "In Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Helmert and Domshlak,? \\Q2009\\E",
      "shortCiteRegEx" : "Helmert and Domshlak",
      "year" : 2009
    }, {
      "title" : "Flexible abstraction heuristics for optimal sequential planning",
      "author" : [ "M. Helmert", "P. Haslum", "J. Hoffmann" ],
      "venue" : "In Proceedings of the 17th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Helmert et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Helmert et al\\.",
      "year" : 2007
    }, {
      "title" : "Accuracy of admissible heuristic functions in selected planning domains",
      "author" : [ "M. Helmert", "R. Mattmüller" ],
      "venue" : "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Helmert and Mattmüller,? \\Q2008\\E",
      "shortCiteRegEx" : "Helmert and Mattmüller",
      "year" : 2008
    }, {
      "title" : "Understanding Planning Tasks: Domain Complexity and Heuristic Decomposition, Vol. 4929 of Lecture Notes in Computer",
      "author" : [ "M. Helmert" ],
      "venue" : null,
      "citeRegEx" : "Helmert,? \\Q2008\\E",
      "shortCiteRegEx" : "Helmert",
      "year" : 2008
    }, {
      "title" : "PSVN: A vector representation for production systems",
      "author" : [ "I. Hernadvölgyi", "R. Holte" ],
      "venue" : "Tech. rep. 1999-07,",
      "citeRegEx" : "Hernadvölgyi and Holte,? \\Q1999\\E",
      "shortCiteRegEx" : "Hernadvölgyi and Holte",
      "year" : 1999
    }, {
      "title" : "The role of macros in tractable planning over causal graphs",
      "author" : [ "A. Jonsson" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Jonsson,? \\Q2007\\E",
      "shortCiteRegEx" : "Jonsson",
      "year" : 2007
    }, {
      "title" : "State-variable planning under structural restrictions: Algorithms and complexity",
      "author" : [ "P. Jonsson", "C. Bäckström" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Jonsson and Bäckström,? \\Q1998\\E",
      "shortCiteRegEx" : "Jonsson and Bäckström",
      "year" : 1998
    }, {
      "title" : "Cost-optimal planning with landmarks",
      "author" : [ "E. Karpas", "C. Domshlak" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Karpas and Domshlak,? \\Q2009\\E",
      "shortCiteRegEx" : "Karpas and Domshlak",
      "year" : 2009
    }, {
      "title" : "Structural patterns heuristics. In ICAPS-07 Workshop on Heuristics for Domain-independent Planning: Progress, Ideas, Limitations",
      "author" : [ "M. Katz", "C. Domshlak" ],
      "venue" : null,
      "citeRegEx" : "Katz and Domshlak,? \\Q2007\\E",
      "shortCiteRegEx" : "Katz and Domshlak",
      "year" : 2007
    }, {
      "title" : "Structural patterns of tractable sequentially-optimal planning",
      "author" : [ "M. Katz", "C. Domshlak" ],
      "venue" : "In Proceedings of the 17th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Katz and Domshlak,? \\Q2007\\E",
      "shortCiteRegEx" : "Katz and Domshlak",
      "year" : 2007
    }, {
      "title" : "Structural patterns heuristics via fork decomposition",
      "author" : [ "M. Katz", "C. Domshlak" ],
      "venue" : "In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Katz and Domshlak,? \\Q2008\\E",
      "shortCiteRegEx" : "Katz and Domshlak",
      "year" : 2008
    }, {
      "title" : "Structural-pattern databases",
      "author" : [ "M. Katz", "C. Domshlak" ],
      "venue" : "In Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "Katz and Domshlak,? \\Q2009\\E",
      "shortCiteRegEx" : "Katz and Domshlak",
      "year" : 2009
    }, {
      "title" : "Optimal admissible composition of abstraction heuristics",
      "author" : [ "M. Katz", "C. Domshlak" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Katz and Domshlak,? \\Q2010\\E",
      "shortCiteRegEx" : "Katz and Domshlak",
      "year" : 2010
    }, {
      "title" : "Heuristics - Intelligent Search Strategies for Computer Problem Solving",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1984\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1984
    }, {
      "title" : "Machine discovery of effective admissible heuristics",
      "author" : [ "A. Prieditis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Prieditis,? \\Q1993\\E",
      "shortCiteRegEx" : "Prieditis",
      "year" : 1993
    }, {
      "title" : "Landmarks revisited",
      "author" : [ "S. Richter", "M. Helmert", "M. Westphal" ],
      "venue" : "In Proceedings of the Twenty-Third National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Richter et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Richter et al\\.",
      "year" : 2008
    }, {
      "title" : "A general additive search abstraction",
      "author" : [ "F. Yang", "J. Culberson", "R. Holte" ],
      "venue" : "Tech. rep. TR07-06,",
      "citeRegEx" : "Yang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2007
    }, {
      "title" : "A general theory of additive state space abstractions",
      "author" : [ "F. Yang", "J. Culberson", "R. Holte", "U. Zahavi", "A. Felner" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Yang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "as pattern database heuristics (Edelkamp, 2001) and merge-and-shrink heuristics (Helmert, Haslum, & Hoffmann, 2007).",
      "startOffset" : 31,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "Over the years, PDB heuristics have been shown to be very effective in several hard search problems, including cost-optimal planning (Culberson & Schaeffer, 1998; Edelkamp, 2001; Felner, Korf, & Hanan, 2004; Haslum, Botea, Helmert, Bonet, & Koenig, 2007).",
      "startOffset" : 133,
      "endOffset" : 254
    }, {
      "referenceID" : 22,
      "context" : "1 The more recent merge-andshrink abstractions generalize PDB heuristics to overcome the latter limitation (Helmert et al., 2007).",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "Following the type of analysis suggested by Helmert and Mattmüller (2008), we formally analyze the asymptotic performance ratio of the fork-decomposition heuristics and prove that their worst-case accuracy on selected domains is comparable with that of (even parametric) state-of-the-art admissible heuristics.",
      "startOffset" : 44,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "This does not necessarily apply to symbolic PDBs which, on some tasks, may exponentially reduce the PDB’s representation (Edelkamp, 2002).",
      "startOffset" : 121,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : "This work is a revision and extension of the formulation and results presented by Katz and Domshlak (2008, 2009), which in turn is based on ideas first sketched also by Katz and Domshlak (2007a).",
      "startOffset" : 82,
      "endOffset" : 195
    }, {
      "referenceID" : 18,
      "context" : "Figure 1: Logistics-style example adapted from Helmert (2006) and illustrated in (a).",
      "startOffset" : 47,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "To illustrate various constructs, we use a slight variation of a Logistics-style example from Helmert (2006). This example is depicted in Figure 1a, and in sas+ it has",
      "startOffset" : 94,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "Many recent works on cost-optimal planning are based on additive ensembles of admissible heuristics, and this includes critical-path heuristics (Haslum, Bonet, & Geffner, 2005; Coles, Fox, Long, & Smith, 2008), pattern database heuristics (Edelkamp, 2001; Haslum et al., 2007), and landmark heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009).",
      "startOffset" : 239,
      "endOffset" : 276
    }, {
      "referenceID" : 16,
      "context" : "Many recent works on cost-optimal planning are based on additive ensembles of admissible heuristics, and this includes critical-path heuristics (Haslum, Bonet, & Geffner, 2005; Coles, Fox, Long, & Smith, 2008), pattern database heuristics (Edelkamp, 2001; Haslum et al., 2007), and landmark heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009).",
      "startOffset" : 239,
      "endOffset" : 276
    }, {
      "referenceID" : 35,
      "context" : "Our definition of abstraction resembles that of Prieditis (1993), and right from the beginning we specify a more general notion of additive abstraction.",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "Hence, in particular, Definition 2 generalizes the notion of abstraction by Helmert et al. (2007) by replacing the condition of preserving individual transitions and their labels, that is, (α(s), l, α(s′)) if (s, l, s′), with a weaker condition stated in Eq.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "Hence, in particular, Definition 2 generalizes the notion of abstraction by Helmert et al. (2007) by replacing the condition of preserving individual transitions and their labels, that is, (α(s), l, α(s′)) if (s, l, s′), with a weaker condition stated in Eq. 1. The reader, of course, may well ask whether the generality of the condition in Eq. 1 beyond the condition of Helmert et al. (2007) really delivers any practical gain, and later we show that the answer to this question is affirmative.",
      "startOffset" : 76,
      "endOffset" : 393
    }, {
      "referenceID" : 9,
      "context" : "To further illustrate the connection between abstractions and admissible heuristics, consider three well-known mechanisms for devising admissible planning heuristics: delete relaxation (Bonet & Geffner, 2001), critical-path relaxation (Haslum & Geffner, 2000),2 and pattern database heuristics (Edelkamp, 2001).",
      "startOffset" : 294,
      "endOffset" : 310
    }, {
      "referenceID" : 2,
      "context" : "In any event, however, the abstraction 〈T+, α+〉 does not induce a heuristic in terms of Definition 3 because computing h+(s) is known to be NP-hard (Bylander, 1994).",
      "startOffset" : 148,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the (similarly named) domain-specific heuristics for search problems such as (n2 − 1)-puzzles or Rubik’s Cube (Culberson & Schaeffer, 1998; Hernadvölgyi & Holte, 1999; Felner et al., 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al.",
      "startOffset" : 122,
      "endOffset" : 200
    }, {
      "referenceID" : 16,
      "context" : ", 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007).",
      "startOffset" : 96,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007).",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007).",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : ", 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007). The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space Sα and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in Sα by exhaustive search. Moreover, an O(1) bound on |Sα| is typically set explicitly to fit the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |Sα| implies a fixed-dimensionality constraint |V α| = O(1). In planning tasks with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {Πi}i=1 is a sequence of Logistics problems of growing size with |Vi| = i. If each package in Πi can be transported by some Θ(i) vehicles, then starting from some i, hα will not account at all for movements of vehicles essential for solving Πi (Helmert & Mattmüller, 2008). Aiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology of Dräger, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink (MS) abstractions for planning.",
      "startOffset" : 97,
      "endOffset" : 1344
    }, {
      "referenceID" : 9,
      "context" : ", 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007). The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space Sα and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in Sα by exhaustive search. Moreover, an O(1) bound on |Sα| is typically set explicitly to fit the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |Sα| implies a fixed-dimensionality constraint |V α| = O(1). In planning tasks with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {Πi}i=1 is a sequence of Logistics problems of growing size with |Vi| = i. If each package in Πi can be transported by some Θ(i) vehicles, then starting from some i, hα will not account at all for movements of vehicles essential for solving Πi (Helmert & Mattmüller, 2008). Aiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology of Dräger, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink (MS) abstractions for planning.",
      "startOffset" : 97,
      "endOffset" : 1420
    }, {
      "referenceID" : 9,
      "context" : ", 2004), PDB heuristics have been successfully exploited in domain-independent planning as well (Edelkamp, 2001, 2002; Haslum et al., 2007). The key decision in constructing PDBs is what sets of variables the problem is projected to (Edelkamp, 2006; Haslum et al., 2007). However, apart from that need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space Sα and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in Sα by exhaustive search. Moreover, an O(1) bound on |Sα| is typically set explicitly to fit the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |Sα| implies a fixed-dimensionality constraint |V α| = O(1). In planning tasks with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {Πi}i=1 is a sequence of Logistics problems of growing size with |Vi| = i. If each package in Πi can be transported by some Θ(i) vehicles, then starting from some i, hα will not account at all for movements of vehicles essential for solving Πi (Helmert & Mattmüller, 2008). Aiming at preserving the attractiveness of the PDB heuristic while eliminating the bottleneck of fixed dimensionality, Helmert et al. (2007) have generalized the methodology of Dräger, Finkbeiner, and Podelski (2006) and introduced the so called merge-and-shrink (MS) abstractions for planning. MS abstractions are homomorphisms that generalize PDB abstractions by allowing for more flexibility in selection of pairs of states to be contracted. The problem’s state space is viewed as the synchronized product of its projections onto the single state variables. Starting with all such “atomic” abstractions, this product can be computed by iteratively composing two abstract spaces, replacing them with their product. While in a PDB the size of the abstract space Sα is controlled by limiting the number of product compositions, in MS abstractions it is controlled by interleaving the iterative composition of projections with abstraction of the partial composites. Helmert et al. (2007) have proposed a concrete strategy for this interleaved abstraction/refinement scheme and empirically demonstrated the power of the merge-and-shrink abstraction heuristics.",
      "startOffset" : 97,
      "endOffset" : 2190
    }, {
      "referenceID" : 2,
      "context" : "The pitfall, however, is that implicit abstraction heuristics correspond to tractable fragments of optimal planning, and the palette of such known fragments is extremely limited (Bäckström & Nebel, 1995; Bylander, 1994; Jonsson & Bäckström, 1998; Jonsson, 2007; Katz & Domshlak, 2007b).",
      "startOffset" : 178,
      "endOffset" : 285
    }, {
      "referenceID" : 26,
      "context" : "The pitfall, however, is that implicit abstraction heuristics correspond to tractable fragments of optimal planning, and the palette of such known fragments is extremely limited (Bäckström & Nebel, 1995; Bylander, 1994; Jonsson & Bäckström, 1998; Jonsson, 2007; Katz & Domshlak, 2007b).",
      "startOffset" : 178,
      "endOffset" : 285
    }, {
      "referenceID" : 34,
      "context" : "Informally, this decomposition can be seen as a sequential application of two kinds of task transformations: dropping preconditions (Pearl, 1984) and (certain form of) breaking actions with conjunctive effects into single-effect actions.",
      "startOffset" : 132,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "In fact, recent results by Chen and Gimenez (2008) show that planning for any sas+ fragment characterized by any nontrivial form of causal graph is NP-hard.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "While the hardness of optimal planning for problems with fork and inverted fork causal graphs casts a shadow on the relevance of fork decompositions, a closer look at the proofs of the corresponding hardness results of Domshlak and Dinitz (2001) and Helmert (2003, 2004) reveals that they in particular rely on root variables having large domains.",
      "startOffset" : 219,
      "endOffset" : 246
    }, {
      "referenceID" : 22,
      "context" : "In fact, the shrinking step of the algorithm for building the merge-and-shrink abstractions is precisely a variable domain abstraction for meta-variables constructed in the merging steps (Helmert et al., 2007).",
      "startOffset" : 187,
      "endOffset" : 209
    }, {
      "referenceID" : 20,
      "context" : "Hence, we have implemented three additive fork-decomposition heuristics, h, h, and h, within the standard heuristic forward search framework of the Fast Downward planner (Helmert, 2006) using the A∗ algorithm with full duplicate elimination.",
      "startOffset" : 170,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "” We make a comparison with two baseline approaches, namely “blind A∗” with heuristic value 0 for goal states and 1 otherwise, and A∗ with the hmax heuristic (Bonet & Geffner, 2001), as well as with state-of-the-art abstraction heuristics, represented by the mergeand-shrink abstractions of Helmert et al. (2007). The latter were constructed under the",
      "startOffset" : 291,
      "endOffset" : 313
    }, {
      "referenceID" : 14,
      "context" : "We also compare to the Gamer (Edelkamp & Kissmann, 2009) and HSPF (Haslum, 2008) planners, the winner and the runner-up at the sequential optimization track of IPC-2008.",
      "startOffset" : 66,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "These four (baseline and merge-and-shrink) heuristics were implemented by Helmert et al. (2007) within the same planning system as our fork-decomposition heuristics, allowing for a fairly unbiased comparison.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "These days h-partitions are being adopted by various optimal planners using criticalpath heuristics hm for m > 1 (Haslum et al., 2005), landmark heuristics hL and hLA (Karpas & Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp, 2001; Helmert et al.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : ", 2005), landmark heuristics hL and hLA (Karpas & Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp, 2001; Helmert et al., 2007).",
      "startOffset" : 119,
      "endOffset" : 157
    }, {
      "referenceID" : 22,
      "context" : ", 2005), landmark heuristics hL and hLA (Karpas & Domshlak, 2009), and PDB and merge-and-shrink abstraction heuristics (Edelkamp, 2001; Helmert et al., 2007).",
      "startOffset" : 119,
      "endOffset" : 157
    }, {
      "referenceID" : 22,
      "context" : "This per-node efficiency provides the merge-and-shrink heuristics with impressive practical effectiveness on numerous IPC domains (Helmert et al., 2007).",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "However, as rightfully noted by Helmert and Mattmüller (2008), such evaluations almost never lead to absolute statements of the type “Heuristic h is well-suited for solving problems from benchmark suite X,” but only to relative statements of the type “Heuristic h expands fewer nodes than heuristic h′ on benchmark suite X.",
      "startOffset" : 32,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "However, as rightfully noted by Helmert and Mattmüller (2008), such evaluations almost never lead to absolute statements of the type “Heuristic h is well-suited for solving problems from benchmark suite X,” but only to relative statements of the type “Heuristic h expands fewer nodes than heuristic h′ on benchmark suite X.” Moreover, one would probably like to obtain formal evidence of the effectiveness of a heuristic before proceeding with its implementation, especially for very complicated heuristic procedures such as those underlying the proofs of Theorems 7 and 8. Our formal analysis of the effectiveness of the fork-decomposition heuristics using the methodology suggested and exploited by Helmert and Mattmüller was motivated primarily by this desire for formal evidence. Given a planning domain D and heuristic h, Helmert and Mattmüller (2008) consider the asymptotic performance ratio of h in D.",
      "startOffset" : 32,
      "endOffset" : 857
    }, {
      "referenceID" : 18,
      "context" : "Table 4: Performance ratios of multiple heuristics in selected planning domains; ratios for h+, hk, hPDB, hPDB add are by Helmert and Mattmüller (2008).",
      "startOffset" : 122,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "Helmert and Mattmüller (2008) study the asymptotic performance ratio of some standard admissible heuristics on a set of well-known benchmark domains from the first four IPCs.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "• The h+ estimate corresponds to the optimal cost of solving the well-known “delete relaxation” of the original planning task, which is generally NP-hard to compute (Bylander, 1994).",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "Blocksworld Arguments similar to those of Helmert and Mattmüller (2008) for hPDB add .",
      "startOffset" : 42,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "3 of Helmert (2008) we have",
      "startOffset" : 5,
      "endOffset" : 20
    } ],
    "year" : 2010,
    "abstractText" : "State-space search with explicit abstraction heuristics is at the state of the art of costoptimal planning. These heuristics are inherently limited, nonetheless, because the size of the abstract space must be bounded by some, even if a very large, constant. Targeting this shortcoming, we introduce the notion of (additive) implicit abstractions, in which the planning task is abstracted by instances of tractable fragments of optimal planning. We then introduce a concrete setting of this framework, called fork-decomposition, that is based on two novel fragments of tractable cost-optimal planning. The induced admissible heuristics are then studied formally and empirically. This study testifies for the accuracy of the fork decomposition heuristics, yet our empirical evaluation also stresses the tradeoff between their accuracy and the runtime complexity of computing them. Indeed, some of the power of the explicit abstraction heuristics comes from precomputing the heuristic function offline and then determining h(s) for each evaluated state s by a very fast lookup in a “database.” By contrast, while fork-decomposition heuristics can be calculated in polynomial time, computing them is far from being fast. To address this problem, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can be successfully overcome. We demonstrate that an equivalent of the explicit abstraction notion of a “database” exists for the fork-decomposition abstractions as well, despite their exponential-size abstract spaces. We then verify empirically that heuristic search with the “databased” fork-decomposition heuristics favorably competes with the state of the art of cost-optimal planning.",
    "creator" : "TeX"
  }
}