{
  "name" : "1412.8529.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A note about the generalisation of the C-tests",
    "authors" : [ "José Hernández-Orallo" ],
    "emails" : [ "jorallo@dsic.upv.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this exploratory note we ask the question of what a measure of performance for all tasks is like if we use a weighting of tasks based on a difficulty function. This difficulty function depends on the complexity of the (acceptable) solution for the task (instead of a universal distribution over tasks or an adaptive test). The resulting aggregations and decompositions are (now retrospectively) seen as the natural (and trivial) interactive generalisation of the C-tests. Keywords: Intelligence evaluation, artificial intelligence, C-tests, algorithmic information theory, universal psychometrics, agent response curve."
    }, {
      "heading" : "1 Introduction",
      "text" : "Since the inception of algorithmic information theory (AIT) in the 1960s, its use to construct intelligence tests was hinted by some and explicitly suggested by [7]. The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff’s inductive inference problems: “given an initial segment of a sequence, predict its continuation” (as quoted in [46, p.332]). Levin’s Kt complexity was used to calculate the difficulty of a sequence of letters. The performance was measured as an aggregated value over a range of difficulties:\nI(π) , H∑\nh=1 he N∑ i=1 1 N Hit(π, xi,h) (1)\nwhere π is the subject, the difficulties range from h = 1 to H and there are N sequences xi,k per difficulty h. The function hit returns 1 if π is right with the continuation and 0 otherwise. If e = 0 we have that all difficulties have the same weight. The N sequences per difficulty were chosen (uniformly) randomly.\nThis contrasts with a more common evaluation in artificial intelligence based on average-case performance according to a probability of problems or tasks:\nΨ(π) , ∑ µ∈M p(µ) · E[R(π, µ)] (2)\nwhere p is a probability distribution on the set of tasks M , and R is a result function of agent π on task µ. Actually, eq. 2 can also be combined with AIT, in a different way, by using a universal distribution [53, 46], i.e., p(µ) = 2−K(µ), where K(µ) is the Kolmogorov complexity of µ, as first chosen by [43].\nThe work in [43] has been considered a generalisation of [30, 16], from static sequences (predicting a continuation of a sequence correctly) to dynamic environments (maximising rewards in an MDP). In this paper we challenge this interpretation and look for a proper generalisation of [30, 17] using the notion of difficulty in the outer sum, as originally conceived and seen in eq. 1. The key idea is the realisation that for the C-test the task and the solution were the same thing. This meant that the difficulty was calculated as the size of the simplest programs that generates the sequence, which is both the task and the solution. Even if the complexity of the task and the solution coincide here, it is the complexity of the solution what determines the difficulty of the problem.\nar X\niv :1\n41 2.\n85 29\nv1 [\ncs .A\nI] 3\n0 D\nec 2\n01 4\nHowever, when we move from sequences to environments, the complexity of the solution and the complexity of the environment are no longer the same. In fact, this is discussed in [22] and [21]: the complexity of the environment is roughly an upper bound of the complexity of the solution, but very complex environments can have very simple solutions. In fact, the choice of p(µ) = 2−K(µ) has been criticised for giving too much weight to a few environments. Also, it is important to note that the invariance theorem is more meaningful for Kolmogorov Complexity than for Algorithmic Probability, as for the former it gives some stability for values of K that are not very small, but for a probablity it is precisely the small cases that determine most of the distribution mass. In fact, any computable distribution can be approximated (to whatever required precision) by a universal distribution. This means that the choice of p(µ) = 2−K(µ) for Eq. 2 is actually a metadefinition, which leads to virtually any performance measure, depending on the UTM that is chosen as reference.\nBy decoupling the complexity of task and solution we see that we can go back to eq. 1 and work out a notion of difficulty of environments that depends on the complexity of the solution. While this may look retrospectively trivial, and the natural extension in hindsight, we need to solve and clarify some issues, and properly analyse the relation of the two different philosophies given by eq. 2 and eq. 1.\nThe rest of the paper is organised as follows. Section 2 discusses some previous work, introduces some notation and recovers the difficulty-based decomposition of aggregated performance. Section 3 shows what happens if difficulty functions are based on task complexity. Section 4 introduces several properties about difficulty functions and the view of difficulty as solution complexity. Section 5 discusses the choices for the difficulty-dependent probability. Section 6 briefly deals with the role of computational steps for difficulty. Section 7 closes the paper with a discussion."
    }, {
      "heading" : "2 Background",
      "text" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq. 2.\nIn what follows, we will focus on the approaches that are based on AIT. As mentioned above, the first intelligence test using AIT was the so-called C-test [30, 16]. Figure 1 shows examples of sequences that appear in this test. The difficulty of each sequence was calculated as Levin’s Kt, a time-weighted version of Kolmogorov complexity K. Some preliminary experimental results showed that human performance correlated with the absolute difficulty (h) of each exercise and also with IQ test results for the same subjects. Figure 2 shows the results (taken from [30, 16]). HitRate is defined as the inner sum of eq. 1:\nHitRate(π, h) , N∑ i=1 1 N Hit(π, xi,h) (3)\nAn interesting observation is that by arranging problems by difficulty we see that HitRate seems to be very small from difficulty 8 on. This makes the estimation of the measure much easier, as we only need to focus on (the area of) a small interval of difficulties. In fact, this use of difficulty is common in psychometrics.\nSeveral generalisations of the C-test were suggested, for (“cognitive agents [...] with input/output devices for a complex environment” [30] where “rewards and penalties could be used instead” [17]) or extending them for other cognitive abilities [18], but not fully formalised.\nAIT and reinforcement learning were finally combined in [43], where all possible environments were considered in eq. 2, instantiated with a universal distribution for p, i.e., p(µ) = 2−K(µ), with K(µ) being the Kolmogorov complexity of each environment µ. Some problems (computability, discriminating power, overweight for small environments, no use of time, ...) were discussed with the aim of making a more\napplicable version of this appraoch by [32] and [22, secs. 3.3 and 4]. Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20]. Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].\nWhile the aim of all these proposals was to measure intelligence, many interesting things can happen if AIT is applied to cognitive abilities other than intelligence, as suggested in [18] for the passive case and hinted in [22, secs. 6.5 and 7.2] for the dynamic cases, with the use of different kinds of videogames as environments (two of the most recently introduced benchmarks and competitions are in this direction [4, 50]). Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10]."
    }, {
      "heading" : "2.1 Notation",
      "text" : "We consider tests that are composed of tasks (also called environments or items) and are performed by agents (also called policies or subjects). The set of tasks is denoted by M . Its elements are usually denoted by µ. The set of agents is denoted by Π. Its elements are usually denoted by π. The length in bits of a program for a given UTM U is denoted by LU (x). This function LU can be applied to tasks and agents. We define Kolmogorov complexity as KU (y) = minx LU (x). For both functions we will usually drop the subindex U . The expected1 execution steps of π when performing task µ are denoted by E[S(π, µ, τ)] for a time limit2 τ . For MDPs one possibility is to consider the (expected value of the) maximum steps taken by π for any transition [22]. If there is a transition before τ that does not halt, then E[S(π, µ, τ)] is infinite. The use of stochastic agents and environments makes things more complicated, as any possible transition with non-zero probability before the time limit τ that does not halt makes expectation infinite. Finally, we have the expected value of the response, score or result of π in µ for a time limit τ as E[R(π, µ, τ)]. The value of τ will be usually omitted. The R function always gives values between 0 and 1 and we assume it is always defined (a value of R = 0 is assumed for non-halting situations)3. We also define\n1This has to be ‘expected’ if we consider stochastic environments or agents. 2The time limit can be understood as the number of transitions, as in MDPs, or can represent a discrete or continuous time in other kind of synchronous or asynchronous modelling of tasks. 3Note that we do not go into details about how the R and S functions are calculated. One common option in reinforcement\nlearning is E[R(π, µ, τ)] = E[ ∑∞\ni=1 γ iρi(π, µ)], where ρi is the result of the reward function for iteration i. The term γ i is a discount term with γ greater than (but usually close to) 1. The use of a discounting factor γ makes the approximation possible even in cases where the rewards are divergent. Alternatively, we could also use a discount function for S as E[S(π, µ, τ)] =\nE[LS(π, µ, τ)] , E[L(π, µ, τ) + logS(π, µ, τ)]. Logarithms are always binary. Inspired by Levin’s Kt (see, e.g., [45] or [46]), we can define Kt(π, µ) = minπ expLS(π, µ), which in this case depends on µ as well 4."
    }, {
      "heading" : "2.2 Difficulty-based decomposition",
      "text" : "It is actually in [26], where we can find a first connection between the schemas of eq. 1 and eq. 2. We adapt definition 14 in [26], which is a generalisation of eq. 2, by making the set M explicit as a parameter, as well as the task probability p and the time limit τ :\nDefinition 1. The expected average result with time limit τ for a task class M , a distribution p and an agent π is:\nΨ(π,M, p) , ∑ µ∈M p(µ) · E[R(π, µ)] (4)\nAnd now we use proposition 4 in [26] that decomposes it. First, we define partial results for a given difficulty h as follows:\nΨh(π,M, p) , ∑\nµ∈M,~(µ)=h\np(µ|h) · E[R(π, µ)] (5)\nWhere ~ is a difficulty function ~ : M → R+ ∪ 0. Note that this parametrises the result of eq. 4 for different difficulties. For instance, for two agents πA and πB we might have that Ψ3(πA) < Ψ3(πB) but Ψ7(πA) > Ψ7(πB), much like Fig. 2. If we represent Ψh(π,M, p) on the y-axis versus h on the x-axis we have a so-called agent response curve.\nAnd now, we can aggregate performance for a range of difficulties, e.g., as follows:\nProposition 1. ([26, prop. 4]) The expected average result Ψ(π,M, p) can be rewritten as follows:\nΨ(π,M, p) = ∫ ∞ 0 p(h)Ψh(π,M, p)dh (6)\nor, in the particular case when ~ only gives discrete values of difficulty:\nΨ(π,M, p) = ∞∑ h=0 p(h)Ψh(π,M, p) (7)\nwhere p(h) is a probability density function for eq. 6 and a discrete probability function for eq. 7. Note that eq. 4 is a generalisation of eq. 2, eq. 5 is a generalisation of eq. 3 and eq. 7 is a generalisation of eq. 1."
    }, {
      "heading" : "3 Difficulty as task complexity",
      "text" : "We are now considering the decomposition of definition 1 by using two very particular choices. The first choice is that we consider p(µ) = 2−K(µ). This is exactly what has been considered in [43, 44]. The second choice is that we consider that the difficulty of an environment is ~(µ) = K(µ). In this very particular case5, we see that ~ is discrete (K is defined on the natural numbers). E[ ∑\ni=1..∞ γ iσi(π, µ)], where σi gives the execution steps of transition i. Even with these choices and a discount factor, an infinite sum would make both functions very difficult to estimate, as we could have a transition with a large value of i giving a high value. For the R function is not so critical, as it is bounded and the contribution decays with i, but for S this is problematic.\n4If LS is considered as a measure of effort, one can argue that space (i.e., memory) could also be considered, as this has been usually important for the analysis of algorithms and in cognitive science (working memory). However, execution steps are always equal or greater than working memory, so if we consider S there is no need to also include working memory, unless we are especially interested in giving relevance (and weight) to working memory.\n5In [26, prop. 3], it is shown that with these two choices the average difficulty diverges. This does not mean that eq. 7 diverges, which is what we work here.\nWith these two choices (p(µ) = 2−K(µ) and ~ = K(µ)), the first thing that we realise is that for those µ such that ~(µ) = h we have that p(µ|h) = 1NM (h) , where NM (h) , |{~(µ) = h}|. In other words, given a difficulty h, all tasks µ of that difficulty have the same K and clearly the same probability, so we have a uniform distribution. The denominator, NM (h), is (much) lower than 2\nh as we are using a prefix code and some (many) sequences of length h are not self-delimited programs. The value p(h) is then given by∑\nµ∈M,~(µ)=h 2 −K(µ) = ∑ µ∈M,~(µ)=h 2 −h = NM (h)2 −h. So we have:\nΨ(π,M) = ∞∑ h=0 NM (h)2 −h ∑ µ∈M,~(µ)=h\n1\nNM (h) E[R(π, µ)]\nEven if these are very particular (and arguable unreasonable) choices of task probability and task difficulty, this straightforward transformation is helful to show that what we have is that all tasks of the same difficulty have the same weight (the inner sum) but the outer sum can go from a geometric distribution for difficulties (easy tasks are much more likely) if the prefix coding is a unary coding (so NM (h) = 1, and p(h) = 2\n−h) to much more efficient codings where NM (h) is close to 2\nh−m where m is a constant that depends on the efficiency of the coding. For instance, for a prefix coding that uses a fixed-length word of size c (character size in bits) and a special character as end delimiter, we would have NM (h) = (2 c − 1)(hc −1) programs when h is divisible per c and 0 otherwise. In this case p(h) = NM (h)2 −h can be approximated by 1c·ν b\n−h, where b = 1 + 2−c and ν is a constant normalisation term such that ∑ w(h) ≤ 1. This is what we see on Figure 3 (left) with c = 4 (so b = 1.0625). However, we see that w(h) can have a base as close to 1 as we like, so the probability would look uniform for small values of h, as Figure 3 (right). Nonetheless, it still is a geometrical distribution (otherwise the sum would diverge, which is not possible as we assume that R is bounded as for equation 4).\nAll this highlights how important the prefix coding is for the understanding of these distributions. In fact, it is not only that the definition depends strongly on the choice of the reference UTM used for K, which determines the probability of each task 2−K(µ), as argued elsewhere ([32] and [22, secs. 3.3 and 4]), but also that the prefix coding is highly relevant as well if we use K(µ) as difficulty.\nApart from the differences that appear because of the coding, there is a major concern about this: it is now crystal clear that easier problems would have more weight than difficult ones. However, our intuition\nwhen evaluating a subject on a range of difficulties would be to give more relevance to more difficult problems or, at most, to give the same relevance to all difficulties. In the latter case, a subject being able to score well in all tasks of difficulty 2h would double the result than another subject being to score well in all tasks of difficulty h."
    }, {
      "heading" : "4 Difficulty functions",
      "text" : "Before setting an appropriate measure of difficulty based on the solution, in this section we will analyse which properties a difficulty function may have.\n4.1 Detaching p(h) from p(µ|h) The decomposition in previous section suggests that we could try to fix a proper measure of difficulty first and then think about a meaningful distribution p(h). Once this is settled, we could try to find a distribution for all environments of that distribution p(µ|h). In other words, once we determine how relevant a difficulty is we ask which tasks to take for that difficulty. This is the spirit of the C-test [30, 16] as seen in eq. 1. In fact, we perhaps we do not need a p(h) that decays dramatically, as it is expectable to see performance to decrease for increasing difficulty, as in Figure 2.\nTo distinguish p(h) and p(µ|h) we will denote the former with w and the latter with pM . We will use any distribution or even a measure (not summing up to one, for reasons that we will see later on) as a subscript for Ψ. For instance, the following notation ΨU(hmin,hmax)(π,M, pM ), where U(a, b) represents a uniform distribution between a and b. For instance, we can have two agents πA and πB such that ΨU(1,10)(πA) > ΨU(1,10)(πB) but ΨU(11,20)(πA) < ΨU(11,20)(πB). We will use the notation Ψ⊕(π,M, pM ) when w(h) = 1 (note that this is not the uniform distribution for discrete h), which means that the partial aggregations for each difficulty are just added. In other words, Ψ⊕(π,M, pM ) , ∑∞ h=0 Ψh(π,M, pM ) for\ndiscrete difficulty functions and Ψ⊕(π,M, pM ) , ∫∞ 0\nΨh(π,M, pM )dh for continuous difficuty functions. We will explore whether this (area under the agent response curve) is bounded."
    }, {
      "heading" : "4.2 Acceptable solutions",
      "text" : "When we aggregate environments with different scales on R and different difficulties, we may have that an agent focusses on a few environments with high difficulty while another focusses on many more environments with small rewards. Difficulty functions allow us to see how each agent performs for different degrees of difficulty, what we called agent response curves in [26], which are inspired by item response curves in psychometrics (but putting inverting the view between agents and items). Looking at Figure 2 and similar agent response curves in psychometrics, we see that the notion of difficulty must be linked to R, i.e., how well the agents perform, and not about the complexity of the task, as in the previous section.\nOne first idea of a difficulty function of an environment is the expected response for a random agent πrand, i.e., ~(µ) , E[R(πrand, µ)]. However, this result is not very meaningful, but just sets some kind of baseline, which will be between 0 and 1. This is shown in Figure 4 (left) and has the advantage of being applicable to infinite population of agents Π without assuming any distribution of agents. A related idea would be to calculate the expected response for a population of agents, using some distribution, e.g., ~(µ) , ∑ π pΠ(π)E[R(π, µ)]. As agents are programs, we may have the temptation of using pΠ(π) = 2−K(π). If we look at this for an illustrative example in Figure 4 (right), we see that this would mean that basically the few results on the lefmost part of the plot will dominate, i.e., the result of the shortest program.\nAs we want a short solution with a good result, we can think about a ratio between complexity and response of the solution, something like ~(µ) , minπ L(π)E[R(π,µ)] . The unit for difficulty would be bits per reward unit. It is not clear why doubling rewards should take double bits (perhaps logarithms could be used instead). In the example in Figure 4 (right), we see that the leftmost solution (L(π) = 1) would have h = 10.42 = 2.38, and the best with L(π) = 2 would have h = 20.78 = 2.56. In fact, no other solution could get lower than in 2.38. Choosing any other reasonable function instead of a ratio such that we can go further right on the plot would be rather arbitrary. Another option is what is done in [21], as ~(µ) , minπ:E[R(π,µ)]=Rmax(µ) L(π) where Rmax(µ) = maxπ E[R(π, µ)]. However, Rmax may be hard to calculate and even if it can be effectively\ncalculated, any minor mistake or inefficiency in a very good agent will prevent the agent from reaching the optimal result, leading to a notion of difficulty linked to the complexity of the ‘perfect’ solution. In [21], a ‘tolerance value’ is considered and, instead of one solution, difficulty is linked to the probability of finding a solution under this tolerance by using different search approaches.\nHere we are going to consider a similar approach. We are going to consider this tolerance ϵ of acceptability.\nA[ϵ](π, µ) , 1(E[R(π, µ, φ)] ≥ 1− ϵ) (8)\nThis returns 1 if the expected response is above 1 − ϵ and 0 otherwise. If A[ϵ](π, µ) = 1 we say that π is ϵ-acceptable. With this, we binarise responses. One can argue that we could have just defined a binary R, but it is important to clarify that it is not the same to have tolerance for each single R (or a binarised R) than to have a tolerance for the expected value E[R]. The tolerance on the expected value allows the agent to have variability in their results (e.g., stochastic agents) provided the expected value is higher than the tolerance. Finally, even if we will be very explicty about the value of ϵ, and changing it will change the difficulty value of any environment, it is important to say that this values is not so relevant. The reason is that for any environment we can build any other environment where the responses are transformed by any function. In fact, we could actually consider one fixed threshold, such as 0.5, always.\nAnd now we can just define a new version of eq. 5 using this new function:\nΨ [ϵ] h (π,M, pM ) , ∑ µ∈M,~(µ)=h pM (µ|h) · A[ϵ](π, µ) (9)\nWe can just rewrite equations 6 and 7 accordingly:\nΨ[ϵ]w (π,M, pM ) = ∫ ∞ 0 w(h)Ψ [ϵ] h (π,M, pM )dh (10)\nΨ[ϵ]w (π,M, pM ) = ∞∑ h=0 w(h)Ψ [ϵ] h (π,M, pM ) (11)"
    }, {
      "heading" : "4.3 Properties",
      "text" : "Given the above, we are now ready to define a few properties about difficulty functions.\nDefinition 2. A difficulty function ~ is strongly bounded in M if for every π there is a difficulty h such that for every µ ∈ M : ~(µ) ≥ h we have A[ϵ](π, µ) = 0.\nA weaker version of the above property would be as follows:\nDefinition 3. A difficulty function ~ is weakly bounded in M if for every π there is a difficulty h such that for every h′ ≥ h we have Ψ[ϵ]h′ (π,M, pM ) = 0.\nClearly, strongly boundedness implies weakly boundeness, but not vice versa. There can be cases that are not strongly bounded but are weakly bounded. For instance, if we just choose p(µ|h) = 0 if h > h′ for a given h′.\nProposition 2. If a countable difficulty function ~ is weakly bounded by R then for every possible agent π we have:\nΨ [ϵ] ⊕ (π,M, pM ) = ∞∑ h=0 Ψ [ϵ] h (π,M, pM ) (12)\nis finite.\nProof. As it is weakly bounded then there is an h such that for every h′ ≥ h we have Ψ[ϵ]h′ (π,M, pM ) = 0. Consequently, ∑∞ h=0 Ψ [ϵ] h (π,M, pM ) = ∑h′ h=0 Ψ [ϵ] h (π,M, pM ). As ~ is countable, and Ψh is bounded, then eq. 12 is finite.\nThe above is a modification of eq. 11 where the probability p(h) has been removed. When the above proposition holds we have that the area under the agent response curve is finite. We can obtain a similar result for the continuous case (eq. 10), using an integral instead."
    }, {
      "heading" : "4.4 Difficulty as solution complexity",
      "text" : "Now we are ready to ask what happens if we choose the difficulty function in terms of ϵ-acceptability, i.e.:\n~[ϵ](µ) , min{L(π) : E[R(π, µ)] ≥ 1− ϵ} = min{L(π) : A[ϵ](π, µ) = 1} (13)\nWe can say a few words about the cases where a truly random agent gives an acceptable solution for an environment. If this is the case, we intuitively consider the environment easy. So, in terms, of L, we consider truly random agents to be simple, which is more reasonable than considering of infinite difficulty, and goes well with our consideration of stochastic agents and environments.\nFigure 5 (left) shows the distribution of response according to L(π) and using ϵ = 0.9. We see that the simplest ϵ-acceptable solution has L = 12.\nWith the difficulty function in eq. 13 we have:\nProposition 3. The difficulty function ~[ϵ] in eq. 13 is strongly bounded.\nProof. For every solution π, if a task µ has a difficulty ~[ϵ](µ) > L(π), it means that π is not ϵ-acceptable, because otherwise the difficulty would be L(π) and not h. Consequently, A[ϵ](π, µ) = 0 for all µ of difficulty ~[ϵ](µ) > L(π). It is sufficient to take h > L(π) for every π to see that ~ is strongly bounded.\nThis is what we see in Fig. 5 (right), where L(π) = 80. With ~[ϵ] in eq. 13, we can ensure that the values are going to be 0 from h = 80 on. This may not be the case for other difficulty functions. We can imagine a situation where the curve never converges to zero.\nIf the difficulty function is decoupled from resources of the acceptable solutions or we do not use the notion of ϵ-acceptability then we cannot avoid that a very simple solution could eventually score well in a problem with very high difficulty. This would be counter-intuitive, as if there is a simple solution for a difficult problem, the latter should not be considered difficult any more.\nFinally, it is an interesting question to determine what happens with this definition of difficulty when we consider not only the shortest solution but all the solutions that are ϵ-acceptable, weighted by some extreme distribution so that we can keep the original properties. For instance,\n~[ϵ](µ) , −0.5 log ∑ π 2−2L(π) · A[ϵ](π, µ) (14)\nIn a way, this is a more robust version of difficulty. We can see that strongly boundedness also holds for this version:\nProposition 4. The difficulty function ~[ϵ] in eq. 14 is strongly bounded.\nProof. There are not more than 2n programs of length n (actually fewer if it is a prefix coding). So if we consider all of them we have ∑ n>k 2 n2−2n = ∑ n>k 2 −n = 2−k. That means that if a task has difficulty h then there is no acceptable solution of Length(π) < h − 1. Next, we apply the same argument than for proposition 3 but with h > L(π) + 1.\nAs the above difficulty function is irrational (because of the logarithm), in what follows, we will work with the difficulty function ~[ϵ] in eq. 13."
    }, {
      "heading" : "5 Difficulty-conditional task probabilities",
      "text" : "In the previous sections we have focussed on w(h) and whether it is necessary or not. We have seen difficulty functions where just aggregating Ψh without w(h) (or w(h) = 1) leads to a Ψ⊕(π,M, pM ) that is bounded (proposition 2). The question now is how to choose the conditional probability pM (µ|h). In the C-test, eq. 1, this was chosen as a uniform distribution. However, this is not possible in an interactive scenario if we consider all possible tasks, as the number of tasks for which there is an acceptable solution π of L(π) = n can be infinite. Even if we cannot set a uniform distribution, we want a choice of pM (µ|h) that keeps the task diversity (unless there is any special bias to choose the tasks)."
    }, {
      "heading" : "5.1 Task probability depends on difficulty",
      "text" : "The first thing we can do is to assume p(µ|h) in eq. 9 as p(µ|h) = 2 −K(µ)\nν(h) if ~ [ϵ](µ) = h and 0 otherwise,\nwhere ν(h) is a normalisation term to make the mass of the distribution equal to 1, which can be calculated as ν(h) = ∑ µ:~[ϵ](µ)=h 2\n−K(µ). And now we have:\nΨ [ϵ] h (π,M, pM ) = ∑ µ∈M,~[ϵ](µ)=h pM (µ|h) · A[ϵ](π, µ)\n= 1\nν(h) ∑ µ∈M,~[ϵ](µ)=h 2−K(µ) · A[ϵ](π, µ)\nFrom here, we can plug it into eq. 11 for the discrete case:\nΨ[ϵ]w (π,M, pM ) = ∞∑ h=0 w(h) 1 ν(h) ∑ µ∈M,~[ϵ](µ)=h 2−K(µ) · A[ϵ](π, µ) (15)\nNote that the above is going to be bounded independently of the difficulty function if w is a probability distribution. Also notice that 1ν(h) is on the outer sum, and that ν(h) is lower than 1, so the normalisation term is actually greater than 1. And if we use any of the difficulty functions in equations 13 or 14 we can choose w(h) = 1 and Ψ [ϵ] ⊕ (π,M, pM ) is bounded."
    }, {
      "heading" : "5.2 Task probability depends on the solution probability",
      "text" : "One of things of the use of equation 13 is that the number of acceptable solutions per difficulty is finite. This is what happened in the C-test and that is the reason why a uniform distribution could be used for the inner sum. Now let us try to decompose the inner sum by using the solution and get the probability of the task given the solution.\nWe first need to define this set:\nPairs(M,Π, ϵ) , {⟨µ, π⟩ : µ ∈ M,π ∈ Π, and π is an ϵ-acceptable solution for µ} = {⟨µ, π⟩ : µ ∈ M,π ∈ Π, and A[ϵ](π, µ) = 1}\nNote that one task can have many acceptable solutions (and perhaps none) and one agent can be an acceptable solution of many tasks (and perhaps none). Now we can have an alternative expression of difficulty equivalent to eq. 13 as follows:\n~[ϵ](µ) = min ⟨µ,π⟩∈Pairs(M,Π,ϵ) L(π)\nWe want to set pM (µ|h) by decomposing it into pM (µ|π) and pM (π|h).\nΨ [ϵ] h (π,M, pM ) = ∑ µ∈M,~[ϵ](µ)=h pM (µ|h) · A[ϵ](π, µ)\n= ∑\nπ′:K(π)=h\npM (π ′|h) ∑ µ∈M :⟨µ,π′⟩∈Pairs(M,Π,ϵ) pM (µ|π′) · A[ϵ](π, µ)\nIf we use the difficulty function in equation 13 we can assume that p(π′|h) is uniform. If NΠ(h) denotes the number of programs π′ for which L(π′) = h, we can assume p(π′|h) = 1N(h) . So we can rewirte\nΨ [ϵ] h (π,M, pM ) = ∑ π′:K(π′)=h 1 NΠ(h) ∑ µ∈M :⟨µ,π′⟩∈Pairs(M,Π,ϵ) pM (µ|π′) · A[ϵ](π, µ)\nNow we may have the temptation to define pM (µ|π′) with a conditional universal distribution, i.e., 2−K(µ|π ′). There is a conundrum here, as we would have that those environments that can be easily described from the solution would be more likely. Even if K is not symmetric there are strong relations in both directions such that the notion of difficulty would be completely mangled. Instead, it seems more reasonable to choose the distribution as pM (µ|π′) = 2 −K(µ) ν(π′) if ⟨µ, π⟩ ∈ Pairs(M,Π) and 0 otherwise, where ν(π ′) is a normalisation\nterm, which can be calculated as ν(π′) = ∑\n⟨µ,π′⟩∈Pairs(M,Π) 2 −K(µ).\nWe can integrate the above into eq. 11 again.\nΨ[ϵ]w (π,M, pM ) = ∞∑ h=0 w(h) ∑ π:K(π)=h 1 NΠ(h) ∑ µ∈M :⟨µ,π′⟩∈Pairs(M,Π,ϵ) 2−K(µ) ν(π′) · A[ϵ](π, µ)\n= ∞∑ h=0 w(h) NΠ(h) ∑ π:K(π)=h 1 ν(π′) ∑ µ∈M :⟨µ,π′⟩∈Pairs(M,Π,ϵ) 2−K(µ) · A[ϵ](π, µ) (16)\nThe interpretation is as follows: for each difficulty value we aggregate all the solutions with size equal to that difficulty uniformly and for each of these solutions all the environments where each solution is acceptable. This extra complication with respect to eq. 15 can only be justified if we generate environments and agents and we check them as we populate Pairs. Once a sample of Pairs is generated we may want to see how to arrange everything and give appropriately weights to each case.\nApart from this, eq. 15 seems more natural and simpler than eq. 16 as an extension of eq. 1."
    }, {
      "heading" : "6 Using computational steps",
      "text" : "As we mentioned in the introduction, the C-test [30, 16] used Levin’s Kt instead of K. Apart from being computable, Kt is related to Levin’s search. This makes its choice much more appropriate for a measure of difficulty. However, when working with interactive tasks and with stochastic tasks and agents, we have that the number of computational steps must be calculated as E[S(π, µ)]. For simplicity, up to this point, we have considered just K. Now we are briefly exploring the inclusion of the computational steps.\nIn section 2.1 we defined LS. Now we define a version that accounts for the tolerance ϵ as follows:\nLS[ϵ](π, µ) , E[LS(π, µ)] if A[ϵ](π, µ) = 1 and∞ elsewhere\nand we define a new difficulty function that considers computational steps:\n~[ϵ](µ) , min π LS[ϵ](π, µ)\nNote that as LS contains a logarithm, we have that ~ is irrational. We can see that this difficulty function is not bounded, as LS depends on µ, and we can always find a very short solution that takes an enormous amount of time steps for a task with very high difficulty. This is an acceptable solution, but does not reduce the difficulty of the task, so it can always score non-zero beyond any limit.\nThis means that for this difficulty function we would need to use equation 10 with an appropriate w(h) (e.g., a small decay or a uniform interval of difficulties we are interested in).\nIf the testing procedure established a limit on the number of steps (total or per transition) we would have this will be bounded. Alternatively, we could reconsider the inclusion the computational steps in the notion of acceptability."
    }, {
      "heading" : "7 Discussion",
      "text" : "We have gone from eq. 1 taken from C-test to eq. 11 when using discrete difficulty functions or to eq. 10 when using continuous difficulty functions. We have seen that difficulties allow for a more detailed analysis of what happens for a given agent, depending on whether it suceeds on easy or difficult tasks. For some difficulty functions (e.g., those for which proposition 2 is true), we do not even need to determine the weight for each difficulty and just calculate the area, as an aggregated performance for all difficulties. Actually,\ngiven the characteristic of most of the difficulty functions we have seen, setting a maximum difficulty. For instance, a maximum difficult of, let us say, h = 200 seems more than convenient, as it is really challenging to look into a space of solutions of 200 bits (without prior knowledge). So, a finite range of difficulties would be enough for the evaluation of feasible agents.\nTeh important thing is that now we do not need to set an a priori distribution for all tasks p(µ), but just a conditional distribution p(µ|h). Note that if we set a high h we have the freedom to find the simplest task that creates that difficulty. In fact, this is desirable, as when we try to create a difficult game with a small number of rules. Actually, the choice of p(µ|h) as a universal distribution still depends on the reference machine and can set most of the probability mass on smaller tasks, but as it is conditional on h, all trivial, dead or simply meaningless tasks have usually very low h. That means that there is a range of difficulties where we are interested in, discarding very small values of h and ver large values of h. Figure 2 is a nice example of this, where only difficulties between 1 and 8 were used, and we see also that h = 1 and h > 7 are not really very discriminating. The bulk of the testing effort must be performed in this range.\nNow, let us spare a comment about the practical feasibility of generating a test that is conceived in terms of difficulty, which is determined from the length of the solution. It is pertinent to quote a piece from [44]: “Another important difference in our work is that we have directly sampled from program space. This is analogous to the conventional construction of the Solomonoff prior, which samples random bit sequences and treats them as programs. With this approach all programs that compute some environment count towards the environment’s effective complexity, not just the shortest, though the shortest clearly has the largest impact. This makes AIQ very efficient in practice since we can just run sampled programs directly, avoiding the need to have to compute complexity values through techniques such as brute force program search. For example, to compute the complexity of a 15 symbol program, the C-test required the execution of over 2 trillion programs. For longer programs, such as many that we have used in our experiments, this would be completely intractable. One disadvantage of our approach, however, is that we never know the complexity of any given environment; instead we know just the length of one particular program that computes it.”. While I agree that the calculation of difficulties may be hard, a couple of things must be said. First, yet again the previous quote talks about the complexity of an environment, which is based on its description (or on its Solomonoff prior), but not on its solutions. Second, it is always preferrable to devote time to design good tests that lead to a few tasks that are discriminating and whose difficult we are sure of than to have a test that can be obtained more efficiently but the agent will take thousands of useless tasks. In other words, we want tests that are practical when they are applied. We can devote much more resources (and time) for the offline generation of appropriate tasks and the calculation of their difficulty. This is actually what real measurement disciplines do, such as psychometrics, which devote most of the effort to get good tasks and do not hesitate to discard those tasks whose difficulty is dubious or are not discriminating. Effort in the assessment and classification of exercises pays off in more efficient evaluations.\nFinally, there is an important question about the choice of a meaningful difficulty function linked to the effort required to find the solution: what if the difficulty depends on the verification of the solution? This has got attention for problems where the solution must be accompanied by a verification, proof or explanation [17, 1]. In other words, the question is whether we can extend Levin’s search by considering the verification of stochastic problems and derive a difficulty function from it. This is being addressed in a separate note."
    } ],
    "references" : [ {
      "title" : "Can we measure the difficulty of an optimization problem",
      "author" : [ "T. Alpcan", "T. Everitt", "M. Hutter" ],
      "venue" : "IEEE Information Theory Workshop (ITW),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Robotics competitions as benchmarks for AI research",
      "author" : [ "J. Anderson", "J. Baltes", "C.T. Cheng" ],
      "venue" : "The Knowledge Engineering Review,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "The process of research investigations in artificial intelligence - a unified view",
      "author" : [ "D. Baldwin", "S.B. Yadav" ],
      "venue" : "Systems, Man and Cybernetics, IEEE Transactions on,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1995
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research, 47:253–279,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Ultimate IQ: one test to rule them all",
      "author" : [ "C. Biever" ],
      "venue" : "New Scientist,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Artificial intelligence as an experimental science",
      "author" : [ "B.G. Buchanan" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1988
    }, {
      "title" : "Gödel’s theorem and information",
      "author" : [ "G.J. Chaitin" ],
      "venue" : "International Journal of Theoretical Physics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1982
    }, {
      "title" : "How evaluation guides AI research: The message still counts more than the medium",
      "author" : [ "P.R. Cohen", "A.E. Howe" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1988
    }, {
      "title" : "Evaluating research in cooperative distributed problem solving",
      "author" : [ "K.S. Decker", "E.H. Durfee", "V.R. Lesser" ],
      "venue" : "Distributed Artificial Intelligence,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1989
    }, {
      "title" : "How universal can an intelligence test be",
      "author" : [ "D.L. Dowe", "J. Hernández-Orallo" ],
      "venue" : "Adaptive Behavior,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Compression and intelligence: social environments and communication",
      "author" : [ "D.L. Dowe", "J. Hernández-Orallo", "P.K. Das" ],
      "venue" : "Artificial General Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Warning: statistical benchmarking is addictive. Kicking the habit in machine learning",
      "author" : [ "C. Drummond", "N. Japkowicz" ],
      "venue" : "Journal of Experimental & Theoretical Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "On method overfitting",
      "author" : [ "E. Falkenauer" ],
      "venue" : "Journal of Heuristics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Evaluation of expert systems: Issues and case studies",
      "author" : [ "J. Gaschnig", "P. Klahr", "H. Pople", "E. Shortliffe", "A. Terry" ],
      "venue" : "Building expert systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1983
    }, {
      "title" : "Verification & validation",
      "author" : [ "J.R. Geissman", "R.D. Schultz" ],
      "venue" : "AI Expert,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1988
    }, {
      "title" : "Beyond the Turing Test",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : "J. Logic, Language & Information,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "On the computational measurement of intelligence factors",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : "National Institute of Standards and Technology,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2000
    }, {
      "title" : "Thesis: Computational measures of information gain and reinforcement in inference processes",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : "AI Communications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : "Artificial General Intelligence, 3rd Intl Conf,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "On evaluating agent performance in a fixed period of time",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : "Artificial General Intelligence,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "On environment difficulty and discriminating power. Autonomous Agents and Multi-Agent Systems, pages",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Measuring universal intelligence: Towards an anytime intelligence test",
      "author" : [ "J. Hernández-Orallo", "D.L. Dowe" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Mammals, machines and mind games. Who’s the smartest?  The Conversation, http: // theconversation",
      "author" : [ "J. Hernández-Orallo", "D.L. Dowe" ],
      "venue" : "edu. au/ articles/",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "On potential cognitive abilities in the machine kingdom",
      "author" : [ "J. Hernández-Orallo", "D.L. Dowe" ],
      "venue" : "Minds and Machines,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "On more realistic environment distributions for defining, evaluating and developing intelligence",
      "author" : [ "J. Hernández-Orallo", "D.L. Dowe", "S. España-Cubillo", "M.V. Hernández-Lloreda", "J. Insa-Cabrera" ],
      "venue" : "Artificial General Intelligence,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Universal psychometrics: Measuring cognitive abilities in the machine kingdom",
      "author" : [ "J. Hernández-Orallo", "D.L. Dowe", "M.V. Hernández-Lloreda" ],
      "venue" : "Cognitive Systems Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Turing machines and recursive Turing Tests",
      "author" : [ "J. Hernández-Orallo", "J. Insa-Cabrera", "D.L. Dowe", "B. Hibbard" ],
      "venue" : "AISB/IACAP 2012 Symposium “Revisiting Turing and his Test”,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Turing Tests with Turing machines",
      "author" : [ "J. Hernández-Orallo", "J. Insa-Cabrera", "D.L. Dowe", "B. Hibbard" ],
      "venue" : "editor, Turing-100,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Computer models solving human intelligence test problems: progress and implications",
      "author" : [ "J. Hernández-Orallo", "F. Mart́ınez-Plumed", "U. Schmid", "M. Siebers", "D.L. Dowe" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity",
      "author" : [ "J. Hernández-Orallo", "N. Minaya-Collado" ],
      "venue" : "In Proc. Intl Symposium of Engineering of Intelligent Systems",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1998
    }, {
      "title" : "AI evaluation: past, present and future",
      "author" : [ "José Hernández-Orallo" ],
      "venue" : "arXiv preprint arXiv:1408.6908,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Bias and no free lunch in formal measures of intelligence",
      "author" : [ "B. Hibbard" ],
      "venue" : "Journal of Artificial General Intelligence,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "On measuring social intelligence: Experiments on competition and cooperation",
      "author" : [ "J. Insa-Cabrera", "J.L. Benacloch-Ayuso", "J. Hernández-Orallo" ],
      "venue" : "AGI, volume 7716 of Lecture Notes in Computer Science,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    }, {
      "title" : "Comparing humans and AI agents",
      "author" : [ "J. Insa-Cabrera", "D.L. Dowe", "S. España-Cubillo", "M.V. Hernández-Lloreda", "J. Hernández-Orallo" ],
      "venue" : "Artificial General Intelligence,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2011
    }, {
      "title" : "Evaluating a reinforcement learning algorithm with a general intelligence test",
      "author" : [ "J. Insa-Cabrera", "D.L. Dowe", "J. Hernández-Orallo" ],
      "venue" : "Current Topics in Artificial Intelligence. CAEPIA 2011. LNAI Series",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2011
    }, {
      "title" : "Definition and properties to assess multi-agent environments as social intelligence tests",
      "author" : [ "J. Insa-Cabrera", "J. Hernández-Orallo" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Hernández-Lloreda. The anynt project intelligence test : Lambda - one",
      "author" : [ "J. Insa-Cabrera", "J. Hernández-Orallo", "D.L. Dowe", "S. Espa na", "M.V" ],
      "venue" : "AISB/IACAP 2012 Symposium “Revisiting Turing and his Test”,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2012
    }, {
      "title" : "Who are you calling bird-brained? An attempt is being made to devise a universal intelligence test",
      "author" : [ "K. Kleiner" ],
      "venue" : "The Economist, 398(8723,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2011
    }, {
      "title" : "Clever methods of overfitting. Machine Learning (Theory), http: // hunch",
      "author" : [ "J. Langford" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2005
    }, {
      "title" : "Research papers in machine learning",
      "author" : [ "P. Langley" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 1987
    }, {
      "title" : "The changing science of machine learning",
      "author" : [ "P. Langley" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2011
    }, {
      "title" : "Tests of machine intelligence",
      "author" : [ "S. Legg", "M. Hutter" ],
      "venue" : "Years of Artificial Intelligence,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2007
    }, {
      "title" : "Universal intelligence: A definition of machine intelligence",
      "author" : [ "S. Legg", "M. Hutter" ],
      "venue" : "Minds and Machines,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2007
    }, {
      "title" : "An approximation of the universal intelligence measure. In Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence, pages 236–249",
      "author" : [ "S. Legg", "J. Veness" ],
      "venue" : null,
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2013
    }, {
      "title" : "Universal sequential search problems",
      "author" : [ "L.A. Levin" ],
      "venue" : "Problems of Information Transmission,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1973
    }, {
      "title" : "An introduction to Kolmogorov complexity and its applications (3rd ed.)",
      "author" : [ "M. Li", "P. Vitányi" ],
      "venue" : null,
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2008
    }, {
      "title" : "Performance Evaluation and Benchmarking of Intelligent Systems",
      "author" : [ "R. Madhavan", "E. Tunstel", "E. Messina" ],
      "venue" : null,
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2009
    }, {
      "title" : "Computer science as empirical inquiry: Symbols and search",
      "author" : [ "A. Newell", "H.A. Simon" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1976
    }, {
      "title" : "Evaluating expert system tools: A framework and methodology–workshops",
      "author" : [ "J. Rothenberg", "J. Paul", "I. Kameny", "J.R. Kipps", "M. Swenson" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1987
    }, {
      "title" : "An extensible description language for video games",
      "author" : [ "T. Schaul" ],
      "venue" : "Computational Intelligence and AI in Games, IEEE Transactions on,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2014
    }, {
      "title" : "Performance evaluation of intelligent systems at the national institute of standards and technology (nist)",
      "author" : [ "C. Schlenoff", "H. Scott", "S. Balakirsky" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2011
    }, {
      "title" : "Artificial intelligence: an empirical science",
      "author" : [ "H.A. Simon" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 1995
    }, {
      "title" : "A formal theory of inductive inference",
      "author" : [ "R.J. Solomonoff" ],
      "venue" : "Part I. Information and control,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 1964
    }, {
      "title" : "Handbook of intelligence",
      "author" : [ "R.J. Sternberg (ed" ],
      "venue" : null,
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2000
    }, {
      "title" : "Protecting against evaluation overfitting in empirical reinforcement learning",
      "author" : [ "S. Whiteson", "B. Tanner", "M.E. Taylor", "P. Stone" ],
      "venue" : "In Adaptive Dynamic Programming And Reinforcement Learning (ADPRL),",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2011
    }, {
      "title" : "Toward a standard metric of machine intelligence",
      "author" : [ "R. Yonck" ],
      "venue" : "World Future Review,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Since the inception of algorithmic information theory (AIT) in the 1960s, its use to construct intelligence tests was hinted by some and explicitly suggested by [7].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 29,
      "context" : "The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff’s inductive inference problems: “given an initial segment of a sequence, predict its continuation” (as quoted in [46, p.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff’s inductive inference problems: “given an initial segment of a sequence, predict its continuation” (as quoted in [46, p.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 53,
      "context" : "The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff’s inductive inference problems: “given an initial segment of a sequence, predict its continuation” (as quoted in [46, p.",
      "startOffset" : 165,
      "endOffset" : 173
    }, {
      "referenceID" : 28,
      "context" : "The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff’s inductive inference problems: “given an initial segment of a sequence, predict its continuation” (as quoted in [46, p.",
      "startOffset" : 165,
      "endOffset" : 173
    }, {
      "referenceID" : 52,
      "context" : "2 can also be combined with AIT, in a different way, by using a universal distribution [53, 46], i.",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 45,
      "context" : "2 can also be combined with AIT, in a different way, by using a universal distribution [53, 46], i.",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 42,
      "context" : ", p(μ) = 2−K(μ), where K(μ) is the Kolmogorov complexity of μ, as first chosen by [43].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 42,
      "context" : "The work in [43] has been considered a generalisation of [30, 16], from static sequences (predicting a continuation of a sequence correctly) to dynamic environments (maximising rewards in an MDP).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 29,
      "context" : "The work in [43] has been considered a generalisation of [30, 16], from static sequences (predicting a continuation of a sequence correctly) to dynamic environments (maximising rewards in an MDP).",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "The work in [43] has been considered a generalisation of [30, 16], from static sequences (predicting a continuation of a sequence correctly) to dynamic environments (maximising rewards in an MDP).",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "In this paper we challenge this interpretation and look for a proper generalisation of [30, 17] using the notion of difficulty in the outer sum, as originally conceived and seen in eq.",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "In this paper we challenge this interpretation and look for a proper generalisation of [30, 17] using the notion of difficulty in the outer sum, as originally conceived and seen in eq.",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "Figure 1: Several series of different difficulties 9, 12, and 14 used in the C-test [16].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "In fact, this is discussed in [22] and [21]: the complexity of the environment is roughly an upper bound of the complexity of the solution, but very complex environments can have very simple solutions.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "In fact, this is discussed in [22] and [21]: the complexity of the environment is roughly an upper bound of the complexity of the solution, but very complex environments can have very simple solutions.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 47,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 48,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 39,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 40,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 51,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 12,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 38,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 41,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 54,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 46,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 50,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 30,
      "context" : "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 29,
      "context" : "As mentioned above, the first intelligence test using AIT was the so-called C-test [30, 16].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "As mentioned above, the first intelligence test using AIT was the so-called C-test [30, 16].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 29,
      "context" : "Figure 2 shows the results (taken from [30, 16]).",
      "startOffset" : 39,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "Figure 2 shows the results (taken from [30, 16]).",
      "startOffset" : 39,
      "endOffset" : 47
    }, {
      "referenceID" : 29,
      "context" : "] with input/output devices for a complex environment” [30] where “rewards and penalties could be used instead” [17]) or extending them for other cognitive abilities [18], but not fully formalised.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "] with input/output devices for a complex environment” [30] where “rewards and penalties could be used instead” [17]) or extending them for other cognitive abilities [18], but not fully formalised.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "] with input/output devices for a complex environment” [30] where “rewards and penalties could be used instead” [17]) or extending them for other cognitive abilities [18], but not fully formalised.",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 42,
      "context" : "AIT and reinforcement learning were finally combined in [43], where all possible environments were considered in eq.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "Figure 2: Results obtained by humans on task of different difficulty in the C-test [16].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 31,
      "context" : "applicable version of this appraoch by [32] and [22, secs.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 34,
      "context" : "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].",
      "startOffset" : 26,
      "endOffset" : 42
    }, {
      "referenceID" : 33,
      "context" : "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].",
      "startOffset" : 26,
      "endOffset" : 42
    }, {
      "referenceID" : 36,
      "context" : "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].",
      "startOffset" : 26,
      "endOffset" : 42
    }, {
      "referenceID" : 43,
      "context" : "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].",
      "startOffset" : 26,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 37,
      "context" : "Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].",
      "startOffset" : 69,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].",
      "startOffset" : 69,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].",
      "startOffset" : 69,
      "endOffset" : 84
    }, {
      "referenceID" : 55,
      "context" : "Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].",
      "startOffset" : 69,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "While the aim of all these proposals was to measure intelligence, many interesting things can happen if AIT is applied to cognitive abilities other than intelligence, as suggested in [18] for the passive case and hinted in [22, secs.",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 3,
      "context" : "2] for the dynamic cases, with the use of different kinds of videogames as environments (two of the most recently introduced benchmarks and competitions are in this direction [4, 50]).",
      "startOffset" : 175,
      "endOffset" : 182
    }, {
      "referenceID" : 49,
      "context" : "2] for the dynamic cases, with the use of different kinds of videogames as environments (two of the most recently introduced benchmarks and competitions are in this direction [4, 50]).",
      "startOffset" : 175,
      "endOffset" : 182
    }, {
      "referenceID" : 24,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 27,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 32,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 35,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 25,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 9,
      "context" : "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of ‘universal psychometrics’ [26] and the notion of universal test [10].",
      "startOffset" : 266,
      "endOffset" : 270
    }, {
      "referenceID" : 21,
      "context" : "For MDPs one possibility is to consider the (expected value of the) maximum steps taken by π for any transition [22].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 44,
      "context" : ", [45] or [46]), we can define Kt(π, μ) = minπ expLS(π, μ), which in this case depends on μ as well .",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 45,
      "context" : ", [45] or [46]), we can define Kt(π, μ) = minπ expLS(π, μ), which in this case depends on μ as well .",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "It is actually in [26], where we can find a first connection between the schemas of eq.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 25,
      "context" : "We adapt definition 14 in [26], which is a generalisation of eq.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "And now we use proposition 4 in [26] that decomposes it.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 42,
      "context" : "This is exactly what has been considered in [43, 44].",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 43,
      "context" : "This is exactly what has been considered in [43, 44].",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 31,
      "context" : "In fact, it is not only that the definition depends strongly on the choice of the reference UTM used for K, which determines the probability of each task 2−K(μ), as argued elsewhere ([32] and [22, secs.",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 29,
      "context" : "This is the spirit of the C-test [30, 16] as seen in eq.",
      "startOffset" : 33,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "This is the spirit of the C-test [30, 16] as seen in eq.",
      "startOffset" : 33,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : "Difficulty functions allow us to see how each agent performs for different degrees of difficulty, what we called agent response curves in [26], which are inspired by item response curves in psychometrics (but putting inverting the view between agents and items).",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : "Another option is what is done in [21], as ~(μ) , minπ:E[R(π,μ)]=Rmax(μ) L(π) where Rmax(μ) = maxπ E[R(π, μ)].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 20,
      "context" : "In [21], a ‘tolerance value’ is considered and, instead of one solution, difficulty is linked to the probability of finding a solution under this tolerance by using different search approaches.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 29,
      "context" : "As we mentioned in the introduction, the C-test [30, 16] used Levin’s Kt instead of K.",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "As we mentioned in the introduction, the C-test [30, 16] used Levin’s Kt instead of K.",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 43,
      "context" : "It is pertinent to quote a piece from [44]: “Another important difference in our work is that we have directly sampled from program space.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "Finally, there is an important question about the choice of a meaningful difficulty function linked to the effort required to find the solution: what if the difficulty depends on the verification of the solution? This has got attention for problems where the solution must be accompanied by a verification, proof or explanation [17, 1].",
      "startOffset" : 328,
      "endOffset" : 335
    }, {
      "referenceID" : 0,
      "context" : "Finally, there is an important question about the choice of a meaningful difficulty function linked to the effort required to find the solution: what if the difficulty depends on the verification of the solution? This has got attention for problems where the solution must be accompanied by a verification, proof or explanation [17, 1].",
      "startOffset" : 328,
      "endOffset" : 335
    } ],
    "year" : 2017,
    "abstractText" : "In this exploratory note we ask the question of what a measure of performance for all tasks is like if we use a weighting of tasks based on a difficulty function. This difficulty function depends on the complexity of the (acceptable) solution for the task (instead of a universal distribution over tasks or an adaptive test). The resulting aggregations and decompositions are (now retrospectively) seen as the natural (and trivial) interactive generalisation of the C-tests.",
    "creator" : "LaTeX with hyperref package"
  }
}