{
  "name" : "1702.07281.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Probabilistic Framework for Location Inference from Social Media",
    "authors" : [ "Yujie Qian", "Jie Tang", "Zhilin Yang", "Binxuan Huang", "Wei Wei", "Kathleen M. Carley" ],
    "emails" : [ "qyj13@mails.tsinghua.edu.cn", "jietang@tsinghua.edu.cn", "zhiliny@cs.cmu.edu", "binxuanh@andrew.cmu.edu", "weiwei@cs.cmu.edu", "kathleen.carley@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "KEYWORDS Location Inference, Factor Graph Model, Social Media"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "In many social media platforms, such as Twi er, Facebook, and Weibo, location is a very important demographic a ribute to support friend and message recommendation. For example, one observation is that the number of friends tends to decrease as distance increases [3, 27]. Another study also shows that, even on the “global” Internet, the user’s ego-network tends to stay local: the average number of friends between users from the same time zone is about 50 times higher than the number between users with a distance of three time zones [15]. However, the geographical location information is usually unavailable. Statistics show that only 26% of users on Twi er input their locations [7]. Twi er, Facebook, and Weibo also have a function to allow adding per-tweet geo-tags; however, it turns out that only 0.42% of all tweets contain a geo-tag.\nIn this work, our goal is to design a general framework for inferring users’ geographical locations from social media data. Di erent from previous works [3, 7, 24] that deal with this problem in a\nspeci c scenario (e.g., determining USA cities only) or with speci c data (e.g., Twi er), we aim to design a method that is general enough to be applied to di erent scenarios, and is also exible enough to incorporate a variety of information — contents, user pro les, network structure, and deep representation features.\nPrevious work on location inference. e location inference problem has been widely studied by researchers from di erent communities. Roughly speaking, existing literature can be divided into two categories. e rst focuses on studying content. For example, Eisenstein et al. [10] proposed the Geographic Topic Model to predict a user’s geo-location from text. Cheng et al. [7] used a probabilistic framework and illustrated how to nd local words and overcome tweet sparsity. Ryoo et al. [30] applied a similar idea to a Korean Twi er dataset. Ikawa et al. [16] used a rulebased approach to predict a user’s current location based on former tweets. Wing et al. [31] used language models and information retrieval approaches for location prediction. Chen et al. [5] used topic models to determine users’ interests and mapped interests to location. e other line of research infers user locations using network structure information. For example, Backstrom et al. [3] assumed that an unknown user would be co-located with one of its friends and sought the ones with the maximum probabilities. McGee et al. [25] integrated social tie strengths between users to improve location estimation. Jurgens [17] and Davis Jr et al. [34] used the idea of label propagation to infer user locations according to their network distances from users with known locations. However, these methods do not consider content. Li et al. [24] proposed a uni ed discriminative in uence model and utilized both the content and the social network to predict user locations. However, they focused on US users, and only considered location names in tweets. Another study using user pro les can be found in [34]. Table 1 summarizes the most closely related works on location inference. Survey and analysis of location inference techniques on Twi er can be also found in [2, 18]. Unlike the existing literature, our goal is to propose a general method that is able to deal with text in any languages by incorporating content, network, and other factors.\nProblem formulation. e problem of location inference can be generally formalized as a prediction problem. For each user, we aim to predict which country/state/city the user comes from. Our input is a partially labeled network G = (V ,E,Y L ,X) derived from social media data, whereV denotes the set of |V | = N users withV L ⊂ V indicating the subset of labeled users (with locations) and VU ⊂ V the subset of unlabeled users (without locations), E ⊆ V ×V is the\nar X\niv :1\n70 2.\n07 28\n1v 2\n[ cs\n.A I]\n1 M\nar 2\n01 7\nset of relationships between users, Y L corresponds to the set of locations of users inV L , andX is the feature matrix associated with users in V with each row corresponding to a user, each column corresponding to a feature, and xi j the value of the jth feature of user vi . Given the input, we de ne the problem of inferring user locations as follows:\nProblem 1. Geo-location Inference. Given a partially labeled networkG = (V ,E,Y L ,X), the objective is to learn a predictive function f in order to predict the locations of unlabeled users VU\nf : G = (V ,E,Y L ,X) → YU (1) where YU is the set of predicted locations for unlabeled users VU .\nWithout loss of generality, we assume that all predicted locations are among the locations occurring in the labeled set Y L . It worth noting that our formulation of user location inference is slightly di erent from the previous work we mentioned above. e problem is de ned as a semi-supervised learning problem for networked data — we have a network with some labeled nodes and many unlabeled nodes. Our goal is to leverage both the local a ributes X and the network structure E to learn the predictive function f .\nOur solution and contributions. In this paper, we propose a probabilistic framework based on factor graph model to address the location inference problem. e model seamlessly combines content information and network structure into a probabilistic graphical model. In the graphical model, labeled locations are propagated to unlabeled users. In this way, the model supports both supervised learning and semi-supervised learning. e model also supports incorporating deep representation features learned from social context. To improve the learning e ectiveness and e ciency, we explore several learning algorithms. It shows that Loopy Belief Propagation (LBP) achieves good performance, but is not scalable to large networks. So max regression (SR) and Metropolis-Hastings (MH) algorithms have successfully addressed the e ciency problem. Moreover, we present a Two-chain Metropolis-Hastings (MH+) algorithm, which further improves the inference accuracy.\nWe conduct experiments on several datasets of di erent genres: Twi er, Weibo, and Facebook. e results show that the proposed\nprobabilistic model signi cantly improves the location inference accuracy (+3.3-18.5% by F1-score), compared to several state-of-theart methods. In terms of time cost of model training, the proposed MH+ algorithm achieves more than 100× speedup over LBP.\nOrganization. Section 2 presents the proposed methodology. Section 3 presents experimental results that validate the e ectiveness of our methodology. Section 4 concludes this work."
    }, {
      "heading" : "2 PROBABILISTIC FRAMEWORK",
      "text" : "In this section, we propose a general probabilistic framework based on factor graphs for geographical location inference from social media. We rst give themodel de nition, and then introduce several learning algorithms."
    }, {
      "heading" : "2.1 Proposed Model",
      "text" : "For inferring user locations, we have three basic intuitions. First, the user’s pro le may contain implicit information about the user’s location, such as time zone and the language selected by the user. Second, the tweets posted by a user may reveal the user’s location. For example, Table 2 lists the most popular “local” words in ve Englishspeaking countries, including locations (Melbourne, Dublin), organizations (HealthSouth, UMass), sports (hockey, rugby), local idiom (Ctfu, wyd, lad), etc. ird, network structure can be very helpful for geo-location inference. In Twi er, for example, users can follow each other, retweet each other’s tweets, and mention other users in their tweets. e principle of homophily [22] — “birds of a feather ock together” [26] — suggests that “connected” users may come from the same place. is tendency was observed between Twi er reciprocal friends in [15, 21]. Moreover, we found that the homophily phenomenon also exists in the mention network. Table 3 shows the statistics for USA, UK, and China Twi er users. We can see when user A mentions (@) user B, the probability that A and B come from the same country is signi cantly higher than that they come from di erent countries. Interestingly, when a USA user A mentions another user B in Twi er, the chance that user B is also from the USA is 95% , while if user A comes from UK, the probability sharply drops to 85%, and further drops to 80% for users from China. We also did the statistics at the state-level and found that there is an 82.13% chance that users A and B come from the same state if one mentions the other in her/his tweets.\nBased on the above intuitions, we propose a Semi-Supervised Factor Graph Model (SSFGM) for location inference. Figure 1 shows the graphical representation of the SSFGM. e graphical model SSFGM consists of two kinds of variables: observations {x} and latent variables {y}. In our problem, each user vi corresponds to an observation xi and is also associated with a latent variable yi . e observation xi represents the user’s personal a ributes and the latent variable yi represents the user’s location. We denote Y = {y1,y2, . . . ,yN }, and Y can be divided into labeled set Y L and unlabeled set YU . e latent variables {yi }i=1, · · · ,N are correlated with each other, representing relationships between users. In SSFGM, such correlations can be de ned as factor functions.\nNow we explain the SSFGM in details. Given a partially labeled network as input, we de ne two factor functions:\n• Attribute factor: f (xi ,yi ) represents the relationship between observation (features) xi and the latent variable yi ; • Correlation factor: h(yi ,yj ) denotes the correlation between users vi and vj .\ne factor functions can be instantiated in di erent ways. In this paper, we de ne the a ribute factor as an exponential-linear function\nf (xi ,yi ) = exp ( αTΦ(xi ,yi ) ) Φk (xi ,yi ) = 1(yi=k )xi , k ∈ {1, . . . ,C}\n(2)\nwhereα = (α1, · · · ,αC )T is theweighting vector,Φ = (Φ1, · · · ,ΦC )T is the vector of feature functions, C is the number of location categories, and 1(yi=k ) is an indicator function.\ne correlation factor is de ned as h(yi ,yj ) = exp ( γT Ω(yi ,yj ) ) (3)\nwhere γ is also a weighting vector, and Ω is a vector of indicator functions Ωkl (yi ,yj ) = 1(yi=k,yj=l ). Correlation can be directed (e.g., mention), or undirected (e.g., reciprocal follow, Facebook friend). For undirected correlation, we need to guarantee γkl = γlk in the model.\nModel enhancement with deep factors. Recently, deep neural networks have achieved excellent performance in various elds including image classi cation and phrase representations [8, 19]. e proposed SSFGM is exible enough to incorporate deep feature representations. To do this, we de ne a deep factor д(xi ,yi ) in the SSFGM model to represent the deep (non-linear) relationship between xi and yi . e д(xi ,yi ) represents a function between the location label yi and the latent representation learned by a deep neural network (Cf. the right side of Figure 1).\nSpeci cally, the deep factor is de ned by a two-layer neural network. e input vector x is fed into a neural network with two fully-connected layers, denoted as h1(x) and h2(x):\nh1(x) = ReLU(W1x + b1) h2(x) = ReLU(W2h1(x) + b2)\n(4)\nwhere W1,W2, b1, b2 are parameters of the neural network, and we use ReLU(x) = max(0,x) [12] as the activation function. Similar to the de nition of a ribute factor, we de ne\nд(xi ,yi ) = exp ( βT Ψ(xi ,yi ) ) Ψk (xi ,yi ) = 1(yi=k )h 2(xi ), k ∈ {1, . . . ,C} (5)\nwhere β is the weighting vector for the output of the neural network.\nus, we have the following joint distribution over Y :\np(Y |G) = 1 Z ∏ vi ∈V f (xi ,yi )д(xi ,yi ) ∏ (vi ,vj )∈E h(yi ,yj ) (6)\nwhere Z is the normalization factor to ensure ∑ Y p(Y |G) = 1.\nFeature de nition. For the a ribute factor, we de ne two categories of features: pro le and content.\nPro le features include information from the user pro les, such as time zone, user-selected language, gender, age, number of followers and followees, etc.\nContent features capture the characteristics of tweet content. e easiest way to de ne content features is using a bag-of-words representation. But it su ers from high dimensionality, especially in Twi er, which has hundreds of languages. In our work, we employ Mutual Information (MI) [32]. We compute MI using the corpus in the training data, and de ne content features as the aggregated MI to each location category. We use two aggregation approaches, max and averaдe , to aggregate all the words in a user’s tweets.\nModel learning. Learning a Semi-Supervised Factor GraphModel involves two parts: learning parametersW, b for the neural network of the deep factor, and learning parameters α , β ,γ for the graphical model. In this paper, we use a separate two-step learning approach. We leave the joint learning of the neural network and the graphical model as our future work.\nWe rst introduce our approach for learning the neural network. Cheng et al. [6] suggested that joint learning of wide and deep models can gain the bene ts of memorization and generalization. In our work, we combine the a ribute factor and deep factor by adding a so max layer on the top of twomodels to generate the outputy, as illustrated in Figure 1 on the right. We train the model with labeled data only, and use squared loss as the loss function. We adopt stochastic gradient descent (SGD) to perform back propagation on the neural network and update the parameters. A er the neural network is trained, we can compute deep factors for all users, and then move to learn the factor graph model.\ne next step is to learn the graphical model – i.e., determine a parameter con guration θ = (αT , βT ,γT )T such that the loglikelihood of the labeled data can be maximized. For simplicity, we denote s(yi ) = (Φ(xi ,yi )T ,Ψ(xi ,yi )T , 12 ∑ yj Ω(yi ,yj )\nT )T , and S(Y ) = ∑i s(yi ). e joint probability de ned in Eq. 6 can be rewri en as\np(Y |G) = 1 Z ∏ i exp ( θT s(yi ) ) = 1 Z exp ( θT ∑ i s(yi ) ) =\n1 Z exp\n( θT S ) (7) e input of SSFGM is partially labeled, which makes the model learning very challenging. e general idea here is to maximize the marginal likelihood of labeled data. We denote Y |Y L as the label con guration that satis es all the known labels. en we can\nde ne the following MLE objective function O(θ ): O(θ ) = logp(Y L |G) = log ∑ Y |Y L 1 Z exp(θT S)\n= log ∑ Y |Y L exp(θT S) − log ∑ Y exp(θT S) (8)\nNow the learning problem is cast as nding the best parameter con guration that maximizes the objective function, i.e.,\nθ̂ = argmax θ\nlogp(Y L |G) (9)\nWe can use gradient descent to solve the above optimization problem. First, we derive the gradient of each parameter θ :\n∂O(θ ) ∂θ = ∂ ∂θ ©«log ∑ Y |Y L exp(θT S) − log ∑ Y exp(θT S)ª®¬ =\n∑ Y |Y L expθT S · S∑ Y |Y L expθT S − ∑ Y expθT S · S∑ Y expθT S\n= Epθ (Y |Y L,G)S − Epθ (Y |G)S = ∑ i Epθ (Y |Y L,G)s(yi ) − ∑ i Epθ (Y |G)s(yi )\n(10)\ne gradient equals to the di erence of two expectations of S, which are under two di erent distributions. e rst onepθ (Y |Y L ,G) is the model distribution conditioned on labeled data, and the second one pθ (Y |G) is the unconditional model distribution. Both of them are intractable and cannot be computed directly."
    }, {
      "heading" : "2.2 Basic Learning Algorithms",
      "text" : "We start with two basic algorithms to tackle the learning problem in SSFGM.\nLoppy Belief Propagation (LBP). A traditional method to estimate the gradient in graphical models is Loopy Belief Propagation (LBP) [28], which provides a way to approximately calculate marginal probabilities on factor graphs. LBP performs message passing between variables and factor nodes according to the sum-product rule [20]. In each step of gradient descent, we perform LBP twice to estimate pθ (Y |G) and pθ (Y |Y L ,G) respectively, and then calculate the gradient according to Eq. 10.\nHowever, this LBP-based learning algorithm is computationally expensive. Its time complexity is O(I1I2(|V |C + |E |C2)), where I1 is the number of iterations for gradient descent, I2 is the number of iterations for belief propagation, andC is the number of the location categories (usually 30-200). is algorithm is time-consuming, and not applicable when we have millions of users and edges.\nMetropolis-Hastings (MH). In addition to LBP, Markov Chain Monte Carlo (MCMC) methods have proved successful for estimating parameters in complex graphical models, such as SampleRank [29]. In our work, we employ Metropolis-Hastings sampling [13] to obtain a sequence of random samples from the model distribution and update the parameters. e learning algorithm is described in Algorithm 1.\nWe now explain Algorithm 1 in detail. At rst, we initialize parameters θ randomly. e Metropolis-Hastings algorithm simulates\nAlgorithm 1:Metropolis-Hastings (MH) Input :G = (V ,E,Y L ,X), learning rate η; Output : learned parameters θ ;\n1 Initialize θ and Y randomly; 2 repeat 3 Select yi uniformly at random; 4 Generate y∗i ∼ q(·|Y ); // Y ∗ = {y1, . . . , yi−1, y∗i , yi+1, . . . } 5 Generate u ∼ U (0, 1); 6 τ ← min { 1, pθ (Y\n∗ |G)q(Y |Y ∗) pθ (Y |G)q(Y ∗ |Y )\n} ;\n7 if u < τ then // accept 8 if ACC(Y ∗) > ACC(Y ) and pθ (Y ∗) < pθ (Y ) then 9 θ ← θ + η · ∇ (logpθ (Y ∗) − logpθ (Y )) 10 if ACC(Y ∗) < ACC(Y ) and pθ (Y ∗) > pθ (Y ) then 11 θ ← θ − η · ∇ (logpθ (Y ∗) − logpθ (Y )) 12 Y ← Y ∗; 13 until convergence;\nrandom samples from the model distribution pθ (Y |G). In each iteration, it generates a candidate con guration Y ∗ from a proposal distribution q(·|Y ), and accepts it with an acceptance ratio τ (line 6). If the candidate Y ∗ is accepted, the algorithm continues to update the parameters θ (line 8-11). We compare the accuracy (ACC) and likelihood of the two con gurationsY andY ∗. Ideally, if one’s accuracy is higher than the other, its likelihood should also be larger; otherwise the model should be adjusted. us there are two cases to update the parameters: 1) if the accuracy ofY ∗ is higher but its likelihood is smaller, update with θnew ← θold + η · ∇ (logpθ (Y ∗) − logpθ (Y )); 2) if the accuracy of Y ∗ is lower but its likelihood is larger, update with θnew ← θold − η · ∇ (logpθ (Y ∗) − logpθ (Y )). Note that we only have to calculate unnormalized likelihoods of pθ (Y ∗) and pθ (Y ). us, the gradient can be easily calculated,\n∇ ( logpθ (Y ∗) − logpθ (Y ) ) = ∇ ( θT S(Y ∗) − θT S(Y ) ) = S(Y ∗)−S(Y )\n(11) is algorithm is more e cient than the previous LBP algorithm, but also has some shortcomings. e algorithm updates the model when larger likelihood leads to worse accuracy. It actually optimizes an alternative max-margin objective instead of the original maximum likelihood objective. In addition, it relies on an external metric (accuracy in our work), which could be arbitrary and engineering-oriented, since multiple metrics are o en available for evaluation."
    }, {
      "heading" : "2.3 Enhanced Learning Algorithms",
      "text" : "We now present two enhanced learning algorithms — So max Regression (SR) and Two-chain Metropolis Hastings (MH+) for SSFGM. SR is very e cient, much faster than the basic learning algorithms and MH+ achieves a be er prediction performance.\nSo max Regression (SR). We propose a new learning algorithm based on so max regression (also called multinomial logistic regression). Estimation of joint probability Eq. 7 is an intractable problem because of the normalization factor Z , which sums over all\nAlgorithm 2: Two-chain Metropolis-Hastings (MH+) Input :G = (V ,E,Y L ,X), learning rate η, and batch size; Output : learned parameters θ ;\n1 Initialize θ with SR-based learning; 2 Initialize Y1 with Y L xed, and YU randomly; 3 Initialize Y2 randomly; 4 repeat 5 θ∗ ← θ ; 6 for t = 1, 2, . . . ,batch size do 7 Select yi uniformly at random; 8 Generate y∗i ∼ q(·|Y ); 9 Accept y∗i in Y ∗ 1 such that Y1 ∼ pθ (Y |G,Y\nL); 10 Accept y∗i in Y ∗ 2 such that Y2 ∼ pθ (Y |G); 11 if y∗i is accepted in Y1 or Y2 or both then 12 θ∗ ← θ∗ + η · (s(y∗i |Y1) − s(y ∗ i |Y2))\n13 θ ← θ∗; 14 until early stopping criteria satis ed;\nthe possible con gurations Y . Our idea here is to only consider yi and x all the other variables, its marginal can be easily calculated as a so max function,\np(yi |G,Y − {yi }) = exp\n( θT ŝ(yi ) ) ∑ y′i exp ( θT ŝ(y′i )\n) (12) where ŝ(yi ) = (Φ(xi ,yi )T ,Ψ(xi ,yi )T , ∑ yj Ω(yi ,yj )\nT )T . Eq. 12 has the same form as so max regression. e di erence is that the neighborhood information is also contained in feature function ŝ(yi ). So max regression can be trained using gradient descent, and the gradient is much easier to compute than factor graph models. We then design an approximate learning algorithm based on so max regression:\nStep 1. Conduct so max regression to learn α and β , with labeled data {(xi ,yi )|yi ∈ Y L} only;1 Step 2. Predict the labels YU for unlabeled users; Step 3. Conduct so max regression to learnθ according to Eq. 12; Step 4. Predict the labels YU for unlabeled users. If the predic-\ntion accuracy on validation set increases, go to Step 3; otherwise, stop.\nis algorithm is an approximation method for learning SSFGM, but it is both e ective and very e cient. We can use SR to initialize the model parameters for the other learning algorithms.\nTwo-chain Metropolis-Hastings (MH+). e classical MH algorithm does not directly maximize the log-likelihood [29] and relies on additional evaluation metrics. We introduce a new idea to directly optimize the objective function in Eq. 8 without using additional heuristic metrics. We refer to this algorithm as Two-chain Metropolis-Hastings (MH+).\nAlgorithm 2 illustrates the new algorithm. In Eq. 10, the gradient consists of two expectation terms. To obtain an unbiased estimation of the gradient, we sample Y1 from pdata = pθ (Y |G,Y L) and sample\n1Here we use p(yi |xi ) = sof tmax ( αTyi Φ(xi , yi ) + β T yi Ψ(xi , yi ) ) .\nY2 from pmodel = pθ (Y |G). Bearing a similar merit to stochastic gradient descent [4], the gradient of our model can be computed as s(Y1) − s(Y2).\nIn order to sample from the two distributions pdata and pmodel, we maintain two Markov chains and employ the MH sampling method. To apply the MH sampling method, a proposal distribution q(·|Y ) is de ned as follows. Given the current label con guration Y , we randomly sample an index i according to a uniform distribution, and set yi as a random value y∗i according to another uniform distribution. In each iteration, we select the same index i and propose the same y∗i for both chains Y1 and Y2 to ensure stable gradient estimation. However, the new label con gurations in the two chains are accepted with di erent acceptance ratios (τ1 and τ2) such that they follow the corresponding distributions, i.e.,\nfor chain Y1, τ1 = min { 1, pθ (Y ∗1 |G,Y\nL) pθ (Y1 |G,Y L) } for chain Y2, τ2 = min { 1, pθ (Y ∗2 |G) pθ (Y2 |G)\n} (13) To further speed up the learning algorithm, we approximate the gradient by only considering a local update—i.e., s(y∗i |Y1)−s(y ∗ i |Y2)— rather than a global update S(Y1) − S(Y2). If y∗i is rejected in both chains, we do not update the parameters. Our choice of proposal distribution has several advantages: 1) empirically the algorithm’s performance is be er than changing multiple variables each time or changing di erent variables in two chains, and 2) it simpli es the calculation of acceptance ratio and gradient, since we only need to consider local variables and factors. We also tried other strategies such as sampling yi with probabilities proportional to the marginal probabilities, but do not obtain further improvements.\nWe use several techniques to improve the stability and e ciency of the learning algorithm. e rst technique is mini-batch gradient descent. We compute the gradient for a mini-batch of sampling steps, and then update the parameters with the sum of these gradients. batch size is a hyperparameter. e second technique is early stopping. We divide the labeled data into a training set and a validation set. During the learning process, we only use the labels in the training set. We evaluate the model a er each δ iterations, and if the prediction accuracy on the validation set does not increase for ε evaluations, we stop the algorithm and return the parameter con guration θ that achieves the best accuracy on the validation set. δ and ε are also hyperparameters. ese techniques can also be used in the previous MH algorithm.\nIn contrast to the basic MH algorithm based on only the model distribution pmodel, our algorithm utilizes both pmodel and pdata so that we can directly optimize the log likelihood. LBP also maximizes the likelihood, but it requires traversing the entire network in each iteration to compute the gradient. Instead, we leverage MH sampling to estimate the gradient e ciently. Similar ideas based on Markov chains have been adopted for training restricted Boltzmann machines [14], but those algorithms do not apply to a partiallylabeled factor graph model as in our work.\nParallel learning. To scale up the proposed model to handle large networks, we have developed parallel learning algorithms\nfor SSFGM. For the SR algorithm, so max regression can be easily parallelized. e gradient is a summation over all the training examples, and the computation is independent. For MH and MH+, we parallelize the learning algorithm by dividing the batch. Each thread generates random samples and computes gradients independently, and with a smaller batch size. en master thread gathers the gradients from di erent threads and updates the parameters."
    }, {
      "heading" : "2.4 Prediction Algorithm",
      "text" : "We can apply the learned SSFGM to predict unknown locations — i.e., to nd the most likely con guration of Ŷ for unlabeled users based on the learned parameters θ ,\nŶ = arg max Y |Y L pθ (Y |G,Y L) (14)\nFor prediction, we consider two algorithms. e rst one is based on LBP. It uses the max-sum algorithm instead of sum-product in LBP to nd the Ŷ that maximizes the likelihood. e second algorithm is based on Metropolis-Hastings sampling. We keep sampling with the estimated θ , and nally return the con guration Ŷ with the maximum likelihood. 2"
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "We evaluate the proposed model on three di erent data: Twi er, Weibo, and Facebook. All datasets and codes used in this paper are available online.3"
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "Datasets. We conduct experiments on four datasets. Table 4 shows the basic statistics of the datasets.\n• Twitter (World): We collected geo-tagged tweets posted in 2011 through Twi er API. ere are 243,000,000 tweets posted by 3,960,000 users in our collected data. A er data preprocessing, we obtain a dataset consisting of 1.5 million users from 159 countries around the world. e task then is to infer the user’s country. Due to limitation of the Twi er API, we did not crawl the following relationships, thus we use mention (“@”) to derive the social relationships. • Twitter (USA): is dataset is constructed from the same raw data as that of Twi er (World). e di erence is that we only keep USA users here. e task on this dataset is to infer the user’s state.\n2In our work, we use LBP for prediction when the model is trained with the LBP algorithm, and use MH for prediction when the model is trained with MH/MH+. 3h p://dev.aminer.org/thomas0809/MH\n• Weibo [33]: Weibo is the most popular Chinese microblog. e dataset consists of about 1,700,000 users, with up to 1,000 most recent microblogs posted by each user. e task is to infer the user’s province. We use reciprocal following relationships as edges in this dataset. • Facebook [23]: e dataset is an ego-facebook dataset from SNAP, and does not contain content information. In our experiments, we only consider users who indicate their hometowns in their pro les. Locations are anonymized in this dataset. We use Facebook friendships as edges.\nData preprocessing. We now introduce how we preprocess the data. First, we lter out users who have fewer than 10 tweets in the dataset. en, we tokenize the tweet content into words. In Twi er, we split the sentences by punctuations and spaces. For languages that do not use spaces to separate words (such as Chinese and Japanese), we further split each character. In the Weibo data provided by [33], the content has been tokenized into Chinese words. For each user, we combine all her/his tweets and derive content features as de ned in Section 2.1. e ground truth location is de ned by di erent ways in each dataset. In the two Twi er datasets, we convert the GPS-tag on tweets to its country/state, and only keep the users who posted all tweets in the same country/state. (In our data, for > 90% users, all tweets are posted in the same country in a year, and for > 80% USA users, all tweets are in the same state.) In Weibo and Facebook, the locations are extracted from user pro les, which have been divided into provinces/cities. In all datasets, we remove the locations with fewer than 10 users.\nComparison methods. We compare the following methods for location inference:\n• Logistic Regression (LR): We consider this method as the baseline method, which applies logistic regression as the classi cation model to predict user location. We use the same feature set as our proposed model. • SupportVectorMachine (SVM) [34]: Zubiaga et al. have applied SVM to classify tweet location. We choose a linear function as the kernel of SVM. • Graph-based Label Propagation (GLP) [9]: It infers locations of unlabeled users by counting the most popular location among one’s friends with a simple majority voting.\nFor baseline methods, we use the implementation of Liblinear [11]. For the proposed method, all experiment codes are implemented in C++ and Python. e deep factor in our model has been implemented using TensorFlow [1]. We empirically set up the hyperparameters as the following: for LR and SVM, we set the regularization penalty c = 1; for our method, we set the learning rate η = 0.1 in MH and η = 1 in MH+, batch size = 5000, and early stopping threshold δ = 1000, ε = 20; the deep factor is set as a two-layer neural network, where the rst layer has 200 hidden units and the second layer has 100 hidden units.\nEvaluation metrics. In our experiments, we divide each dataset into three parts: 50% for training, 10% for validation, and 40% for testing. For the methods which do not require validation, the validation data is also used for training.\nWe evaluate location inference performance using the measures for multi-class classi cation. Speci cally, we use four measures: Micro-Accuracy (percentage of the users whose locations are predicted correctly), and Macro-Precision, Recall, and F1-score (the average precision, recall and F1-score of each class)."
    }, {
      "heading" : "3.2 Experiment Results",
      "text" : "Inference performance. We compare the performance of all the methods on the datasets. Table 5 lists the performance of comparison methods for geo-location inference. SSFGM consistently outperforms all the comparison methods in terms of accuracy and F1-score on all datasets. In Twi er (World), linear models (LR and SVM) can achieve an accuracy of 94.4% for classifying users into 159 countries. Our SSFGM improves the accuracy to 95.8% by incorporating social network. In Twi er (USA), SSFGM achieves a signi cant improvement in terms of both accuracy and F1-score. is is because for inferring user country, the content information might be more important, as users from di erent countries may use di erent languages; while for inferring location at the state-level, the e ect of network information increases. e proposed SSFGM can leverage the network structure information to help improve the inference accuracy. is can be con rmed by taking a closer look at the Weibo data, where we have a complete following network. From the Facebook data, we have another observation, it seems that the SSFGM (LBP) still achieves the best performance. e only problem to SSFGM (LBP) is its low e ciency. We cannot obtain results on the other three datasets by SSFGM (LBP). We will analyze the computational e ciency of di erent learning algorithms later. Finally, we can observe that in general the deep factor indeed helps to improve inference accuracy. Deep factors can incorporate non-linear, cross-dimensional representations of input features, and thus help the model to improve its performance.\nComparison of di erent learning algorithms. We compare the performance of four learning algorithms for SSFGM. e traditional LBP-based learning still results in a very good inference accuracy, but its computational cost is high. Comparing so max regression (SR) and Metropolis-Hastings (MH), they result in similar performance — SR outperforms (0.3-1.6% by F1-score) MH on Twi er (World) and Twi er (USA), but underperforms (3.3-3.6% by F1-score) on Weibo and Facebook. e proposed MH+ algorithm further improves the performance and achieves consistently be er performances (0.7-5.3% by F1-score) than SR and MH.\nCompared with the traditional LBP-based learning algorithm, the studied SR and MH algorithms are much more e cient. Table 6 shows the training time of SSFGM used by di erent learning algorithms, where each algorithm is running with a single computer core. LBP uses about 16 minutes to train SSFGM on our smallest dataset, and needs more than one hundred days on the other large datasets. SR seems to be the most e cient among the four learning algorithms. MH and MH+ take SR as parameter initialization and further improve the accuracy. Generally speaking, they are very e cient on large datasets and over 100× faster than LBP. eir running time varies a li le in di erent datasets because of the di erence in convergence speed.\nScalability performance. We have implemented parallel learning algorithms for the MH and MH+ algorithms utilizing OpenMP architecture. We now evaluate the scalability of the two learning algorithms on the Twi er (USA) dataset. Figure 2 shows the scalability performance with di erent numbers of cores (2-8). e speedup curve of MH is close to the perfect line at the beginning.\nough the speedup inevitably decreases due to the increase of the communication cost between di erent computer nodes, the parallel algorithms can still achieve ∼ 3.3 − 4.5× speedup with 8 cores.\nFactor contribution analysis. We now evaluate how di erent factors (content, pro le, and network) contribute to location inference in the proposed model. We use the two Twi er datasets in this study. Speci cally, we remove each factor from our SSFGM and then evaluate the model performance decrease. A large decrease means more importance of the factor to the model. Figure 3 shows the results on the Twi er datasets. We see that di erent factors contribute di erently on the two datasets. e content-based features seem to be the most useful in the proposed model for inferring location on the Twi er datasets. On the other hand, all features are helpful. is analysis con rms the necessity of the exibility to incorporate various features in the proposed model.\nHyperparameter sensitivity. Finally, we analyze how the hyperparameters in the proposed model a ect the inference performance.\nLearning rate (η): Figure 4(a) shows the accuracy performance of SSFGM (MH and MH+) on Twi er (USA) dataset when varying the learning rate η from 10−3 to 101. Generally speaking, the performance is not sensitive to the learning rate over a wide range. However, a too large or too small learning rate will hurt the performance.\nBatch size: Figure 4(b) shows the accuracy performance comparison when varying the batch size. e results indicate that the performance is not sensitive to the batch size, while larger batch size usually leads to slightly be er performance. When the batch size becomes larger, the gradient estimation tends to be more accurate. But, at the same time, a larger batch size also means more training time. We set batch size to 5, 000 in our experiments."
    }, {
      "heading" : "4 CONCLUSIONS",
      "text" : "In this paper, we studied the problem of inferring user locations from social media. We proposed a general probabilistic model based on factor graphs. e model generalizes previous methods by incorporating content, network, and deep features learned from social context. e model is su ciently exible to support both supervised learning and semi-supervised learning. We presented several learning algorithms and proposed an Two-chain MetropolisHastings (MH+) algorithm, which improves the inference accuracy. Our experiments on four di erent datasets validate the e ectiveness and the e ciency of the proposed model. We also implemented a parallel learning algorithm for the proposed model, which enables the proposed model to handle large-scale data.\nInferring user demographics from social media is a fundamental issue and represents a new and interesting research direction. As for future work, it would be intriguing to apply the proposed model to infer other demographic a ributes such as gender and age. It is also interesting to connect the study to some real applications – for example, advertising and recommendation – to further evaluate how inferred location can help real applications. Finally, we also consider how to integrate social theories such as social status and structural holes to understand the underlying mechanism behind user behavior and network dynamics."
    } ],
    "references" : [ {
      "title" : "Tensorow: A system for large-scale machine learning",
      "author" : [ "M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard" ],
      "venue" : "In OSDI’16,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "A survey of location inference techniques on twier",
      "author" : [ "O. Ajao", "J. Hong", "W. Liu" ],
      "venue" : "Journal of Information Science,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Find me if you can: improving geographical prediction with social and spatial proximity. InWWW’10",
      "author" : [ "L. Backstrom", "E. Sun", "C. Marlow" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Boou" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "From interest to function: Location estimation in social media",
      "author" : [ "Y. Chen", "J. Zhao", "X. Hu", "X. Zhang", "Z. Li", "T.-S. Chua" ],
      "venue" : "In AAAI’13,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Wide & deep learning for recommender systems",
      "author" : [ "H.-T. Cheng", "L. Koc", "J. Harmsen", "T. Shaked", "T. Chandra", "H. Aradhye", "G. Anderson", "G. Corrado", "W. Chai", "M. Ispir" ],
      "venue" : "In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "You are where you tweet: a content-based approach to geo-locating twier users",
      "author" : [ "Z. Cheng", "J. Caverlee", "K. Lee" ],
      "venue" : "In CIKM’10,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "Computer Science,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Inferring the location of twier messages based on user relationships",
      "author" : [ "C.A. Davis Jr.", "G.L. Pappa", "D.R.R. de Oliveira", "F. de L Arcanjo" ],
      "venue" : "Transactions in GIS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "A latent variable model for geographic lexical variation",
      "author" : [ "J. Eisenstein", "B. O’Connor", "N.A. Smith", "E.P. Xing" ],
      "venue" : "In EMNLP’10,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Liblinear: A library for large linear classication",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "Journal of machine learning research,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Deep sparse rectier neural networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "In AISTATS’11,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Monte carlo sampling methods using markov chains and their applications",
      "author" : [ "W.K. Hastings" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1970
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Whowill follow you back? reciprocal relationship prediction",
      "author" : [ "J. Hopcro", "T. Lou", "J. Tang" ],
      "venue" : "In CIKM’11,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Location inference using microblog messages. InWWW’12, pages 687–690",
      "author" : [ "Y. Ikawa", "M. Enoki", "M. Tatsubori" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "at’s what friends are for: Inferring location in online social media platforms based on social relationships",
      "author" : [ "D. Jurgens" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Geolocation prediction in twier using social networks: A critical analysis and review of current practice",
      "author" : [ "D. Jurgens", "T. Finethy", "J. McCorriston", "Y.T. Xu", "D. Ruths" ],
      "venue" : "In ICWSM’15,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Imagenet classication with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS’12,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Factor graphs and the sumproduct algorithm",
      "author" : [ "F.R. Kschischang", "B.J. Frey", "H.-A. Loeliger" ],
      "venue" : "IEEE Transactions on information theory,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "What is twier, a social network or a news media? InWWW’10",
      "author" : [ "H. Kwak", "C. Lee", "H. Park", "S. Moon" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Friendship as a social process: A substantive and methodological analysis",
      "author" : [ "P.F. Lazarsfeld", "R.K. Merton" ],
      "venue" : "Freedom and control in modern society,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1954
    }, {
      "title" : "Krevl. SNAP Datasets: Stanford large network dataset collection",
      "author" : [ "A.J. Leskovec" ],
      "venue" : "hp://snap.stanford.edu/data,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Towards social user proling: unied and discriminative inuencemodel for inferring home locations",
      "author" : [ "R. Li", "S. Wang", "H. Deng", "R. Wang", "K.C.-C. Chang" ],
      "venue" : "In KDD’12,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Location prediction in social media based on tie strength",
      "author" : [ "J. McGee", "J. Caverlee", "Z. Cheng" ],
      "venue" : "In CIKM’13,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Birds of a feather: Homophily in social networks",
      "author" : [ "M. McPherson", "L. Smith-Lovin", "J. Cook" ],
      "venue" : "Annual review of sociology,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2001
    }, {
      "title" : "Did distance maer before the internet?: Interpersonal contact and support in the 1970s",
      "author" : [ "D. Mok", "B. Wellman" ],
      "venue" : "Social networks,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2007
    }, {
      "title" : "Loopy belief propagation for approximate inference: An empirical study",
      "author" : [ "K.P. Murphy", "Y. Weiss", "M.I. Jordan" ],
      "venue" : "In UAI’99,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1999
    }, {
      "title" : "Samplerank: Training factor graphs with atomic gradients",
      "author" : [ "K. Rohanimanesh", "K. Bellare", "A. Culoa", "A. McCallum", "M.L. Wick" ],
      "venue" : "In ICML’11,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    }, {
      "title" : "Inferring twier user locations with 10 km accuracy",
      "author" : [ "K. Ryoo", "S. Moon" ],
      "venue" : "In WWW’14,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Simple supervised document geolocation with geodesic grids",
      "author" : [ "B.P. Wing", "J. Baldridge" ],
      "venue" : "In ACL’11,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "A comparative study on feature selection in text categorization",
      "author" : [ "Y. Yang", "J.O. Pedersen" ],
      "venue" : "In ICML’97,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1997
    }, {
      "title" : "Social inuence locality for modeling retweeting behaviors",
      "author" : [ "J. Zhang", "B. Liu", "J. Tang", "T. Chen", "J. Li" ],
      "venue" : "In IJCAI’13,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2013
    }, {
      "title" : "Towards real-time, country-level location classication of worldwide tweets",
      "author" : [ "A. Zubiaga", "A. Voss", "R. Procter", "M. Liakata", "B. Wang", "A. Tsakalidis" ],
      "venue" : "arXiv preprint arXiv:1604.07236,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "For example, one observation is that the number of friends tends to decrease as distance increases [3, 27].",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 26,
      "context" : "For example, one observation is that the number of friends tends to decrease as distance increases [3, 27].",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Another study also shows that, even on the “global” Internet, the user’s ego-network tends to stay local: the average number of friends between users from the same time zone is about 50 times higher than the number between users with a distance of three time zones [15].",
      "startOffset" : 265,
      "endOffset" : 269
    }, {
      "referenceID" : 6,
      "context" : "Statistics show that only 26% of users on Twier input their locations [7].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Dierent from previous works [3, 7, 24] that deal with this problem in a specic scenario (e.",
      "startOffset" : 29,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "Dierent from previous works [3, 7, 24] that deal with this problem in a specic scenario (e.",
      "startOffset" : 29,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : "Dierent from previous works [3, 7, 24] that deal with this problem in a specic scenario (e.",
      "startOffset" : 29,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "[10] proposed the Geographic Topic Model to predict a user’s geo-location from text.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[7] used a probabilistic framework and illustrated how to nd local words and overcome tweet sparsity.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 29,
      "context" : "[30] applied a similar idea to a Korean Twier dataset.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] used a rulebased approach to predict a user’s current location based on former tweets.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31] used language models and information retrieval approaches for location prediction.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5] used topic models to determine users’ interests and mapped interests to location.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] assumed that an unknown user would be co-located with one of its friends and sought the ones with the maximum probabilities.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 24,
      "context" : "[25] integrated social tie strengths between users to improve location estimation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Jurgens [17] and Davis Jr et al.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 33,
      "context" : "[34] used the idea of label propagation to infer user locations according to their network distances from users with known locations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] proposed a unied discriminative inuence model and utilized both the content and the social network to predict user locations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "Another study using user proles can be found in [34].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "Survey and analysis of location inference techniques on Twier can be also found in [2, 18].",
      "startOffset" : 84,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Survey and analysis of location inference techniques on Twier can be also found in [2, 18].",
      "startOffset" : 84,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "[10] US All User Cheng et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[7] US All User Ryoo et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 29,
      "context" : "[30] Korea Korean User Ikawa et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Japan English, Japanese Tweet Wing et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31] US English User Chen et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5] Beijing Chinese Tweet",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] World All User McGee et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 24,
      "context" : "[25] World All User Jurgens [17] World All User Davis Jr et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[25] World All User Jurgens [17] World All User Davis Jr et al.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "[9] World All User Content + Network Li et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 23,
      "context" : "[24] US English User",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34] World All Tweet",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "* Top-10 by mutual information [32], among the words that #occurrence> 5000.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : "e principle of homophily [22] — “birds of a feather ock together” [26] — suggests that “connected” users may come from the same place.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "e principle of homophily [22] — “birds of a feather ock together” [26] — suggests that “connected” users may come from the same place.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "is tendency was observed between Twier reciprocal friends in [15, 21].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "is tendency was observed between Twier reciprocal friends in [15, 21].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "Recently, deep neural networks have achieved excellent performance in various elds including image classication and phrase representations [8, 19].",
      "startOffset" : 141,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "Recently, deep neural networks have achieved excellent performance in various elds including image classication and phrase representations [8, 19].",
      "startOffset" : 141,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "where W1,W2, b1, b2 are parameters of the neural network, and we use ReLU(x) = max(0,x) [12] as the activation function.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 31,
      "context" : "In our work, we employ Mutual Information (MI) [32].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "[6] suggested that joint learning of wide and deep models can gain the benets of memorization and generalization.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 27,
      "context" : "A traditional method to estimate the gradient in graphical models is Loopy Belief Propagation (LBP) [28], which provides a way to approximately calculate marginal probabilities on factor graphs.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "LBP performs message passing between variables and factor nodes according to the sum-product rule [20].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "In addition to LBP, Markov Chain Monte Carlo (MCMC) methods have proved successful for estimating parameters in complex graphical models, such as SampleRank [29].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 12,
      "context" : "In our work, we employ Metropolis-Hastings sampling [13] to obtain a sequence of random samples from the model distribution and update the parameters.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : "e classical MH algorithm does not directly maximize the log-likelihood [29] and relies on additional evaluation metrics.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "Bearing a similar merit to stochastic gradient descent [4], the gradient of our model can be computed as s(Y1) − s(Y2).",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "Similar ideas based on Markov chains have been adopted for training restricted Boltzmann machines [14], but those algorithms do not apply to a partiallylabeled factor graph model as in our work.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 33,
      "context" : "61 SVM [34] 94.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "30 GLP [9] 72.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : "53 FindMe [3] 72.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 32,
      "context" : "• Weibo [33]: Weibo is the most popular Chinese microblog.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 22,
      "context" : "• Facebook [23]: e dataset is an ego-facebook dataset from SNAP, and does not contain content information.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 32,
      "context" : "In the Weibo data provided by [33], the content has been tokenized into Chinese words.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 33,
      "context" : "• SupportVectorMachine (SVM) [34]: Zubiaga et al.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "• Graph-based Label Propagation (GLP) [9]: It infers locations of unlabeled users by counting the most popular location among one’s friends with a simple majority voting.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "• FindMe [3]: It infers user locations with social and spatial proximity.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 10,
      "context" : "For baseline methods, we use the implementation of Liblinear [11].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "e deep factor in our model has been implemented using TensorFlow [1].",
      "startOffset" : 66,
      "endOffset" : 69
    } ],
    "year" : 2017,
    "abstractText" : "We study the extent to which we can infer users’ geographical locations from social media. Location inference from social media can benet many applications, such as disaster management, targeted advertising, and news content tailoring. In recent years, a number of algorithms have been proposed for identifying user locations on social media platforms such as Twier and Facebook from message contents, friend networks, and interactions between users. In this paper, we propose a novel probabilistic model based on factor graphs for location inference that oers several unique advantages for this task. First, the model generalizes previous methods by incorporating content, network, and deep features learned from social context. e model is also exible enough to support both supervised learning and semi-supervised learning. Second, we explore several learning algorithms for the proposed model, and present a Two-chain Metropolis-Hastings (MH+) algorithm, which improves the inference accuracy. ird, we validate the proposed model on three dierent genres of data – Twier, Weibo, and Facebook – and demonstrate that the proposed model can substantially improve the inference accuracy (+3.3-18.5% by F1-score) over that of several state-of-the-art methods.",
    "creator" : "LaTeX with hyperref package"
  }
}