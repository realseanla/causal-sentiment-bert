{
  "name" : "1502.05443.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Influence-Optimistic Local Values for Multiagent Planning — Extended Version",
    "authors" : [ "Frans A. Oliehoek" ],
    "emails" : [ "fao@liverpool.ac.uk", "m.t.j.spaan@tudelft.nl", "stefan.witwicki@epfl.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n05 44\n3v 1\n[ cs\n.A I]\n1 8\nFe b"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Planning for multiagent systems (MASs) under uncertainty is an important research problem in artificial intelligence. The decentralized partially observable Markov decision process (Dec-POMDP) is a framework for addressing such problems. Many recent approaches to solving Dec-POMDPs propose to exploit locality of interaction [22] also referred to as value factorization [16]. However, without making very strong assumptions, such as transition and observation independence [3], there is no strict locality: in general the actions of any agent may affect the rewards received in a different part of the system, even if that agent and the origin of that reward are (spatially) far apart. For instance, in a traffic network the actions taken in one part of the network will eventually influence the rest of the network [26]. A number of approaches have been proposed to generate solutions for large MASs [40, 48, 27, 47, 9, 36]. However, these heuristic methods come without guarantees. In fact, since it is shown that approximation\n(given some ǫ, finding an ǫ-approximate solution) of DecPOMDPs is NEXP-complete [31], it is unrealistic to expect to find general, scalable methods that have such guarantees. However, the lack of guarantees also makes it difficult to meaningfully interpret the results produced by heuristic methods. In this work, we mitigate this issue by proposing a novel set of techniques that can be used to provide upper bounds on the performance of large factored Dec-POMDPs. More generally, the ability to compute upper bounds is important for numerous reasons: 1) As stated above, they are crucial for a meaningful interpretation of the quality of heuristic methods. 2) Such knowledge of performance gaps is crucial for researchers to direct their focus to promising areas. 3) Such knowledge is also crucial for understanding which problems seem simpler to approximate than others, which in turn may lead to improved theoretical understanding of different problems. 4) Knowledge about the performance gap of the leading heuristic methods can also accelerate their real-world deployment, e.g., when their performance gap is proven to be small over sampled domain instances, or when the selection of heuristic method to deploy is facilitated by clarifying the trade-off of computation and closeness to optimality. 5) Upper bounds on achievable value without communication may guide decisions on investments in communication infrastructure. 6) Last, but not least, these upper bounds can directly be used in current and future heuristic search methods, as we will discuss is some more detail at the end of this paper. Computing upper bounds typically involves relaxing the original problem by making some optimistic assumptions. For instance, in the case of Dec-POMDPs typical assumptions are that the agents can communicate or observe the true state of the system [11, 35, 32, 25]. By exploiting the fact that transition and observation dependence leads to a value function that is additively factored into a number of small components (we say that the value function is ‘factored’, or that the setting exhibits ‘value factorization’), such techniques have been extended to compute upper bounds for so-called network-distributed POMDPs (ND-POMDPs) with many agents. This has greatly increased the size of the problems that can be solved [38, 19, 9]. Unfortunately, as-\nsuming both transition and observation independence (or, more generally, value factorization) narrows down the applicability of the model, and no techniques for computing upper bounds for more general factored DecPOMDPs with many agents are currently known. We address this problem by proposing a general technique for computing what we call influence-optimistic upper bounds. These are upper bounds on the achievable value in large-scale MASs formed by computing local influence-optimistic upper bounds on the value of sub-problems that consist of small subsets of agents and state factors. The key idea is that if we make optimistic assumptions about how the rest of the system will influence a sub-problem, we can decouple it and effectively compute a local upper bound on the achievable value. Finally, we show how these local bounds can be combined into a global upper bound. In this way, the major contribution of this paper is that it shows how we can compute factored upper bounds for models that do not admit factored value functions. We empirically evaluate the utility of influence-optimistic upper bounds by investigating the quality guarantees they provide for heuristic methods, and by examining their application in a heuristic search method. The results show that the proposed bounds are tight enough to give meaningful quality guarantees for the heuristic solutions for factored Dec-POMDPs with hundreds of agents.1 This is a major accomplishment since previous approaches that provide guarantees 1) have required very particular structure such as transition and observation independence [3, 2, 38, 9] or ‘transitiondecoupledness’ combined with very specific interaction structures (transitions of an agent can be affected in a directed fashion and only by a small subset of other agents) [42], and 2) have not been demonstrated for over 50 agents. In contrast, this paper demonstrates quality bounds in settings of hundreds of agents that all influence each other via their actions. This paper is organized as follows. First Section 2, describes the required background by introducing the factored Dec-POMDP model. Next, Section 3 describes the sub-problems that form the basis of our decomposition scheme. In Section 4, we propose local influenceoptimistic upper bounds for such sub-problems together with the techniques to compute them. Subsequently, Section 5 discusses how these local upper bounds can be combined into a global upper bound for large problems with many agents. We empirically investigate the merits of the proposed bounds in Section 6. Section 7 places our work in the context of related work in more detail, and Section 8 concludes."
    }, {
      "heading" : "2. BACKGROUND",
      "text" : "In this paper we focus on factored Dec-POMDPs [26], which are Dec-POMDPs where the transition and obser-\n1In the paper, we use the word ‘tight’ for its (empirical) meaning of“close to optimal”, not for its (theoretical CS) meaning of “coinciding with the best possible bound”.\nvation models can be represented compactly as a twostage dynamic Bayesian network (2DBN) [4]:\nDefinition 1. A factored Dec-POMDP is a tuple M = 〈D,A,O,X ,T,O,R,b0〉, where:\n• D = {1, . . . ,n} is the set of agents. • A = ⊗\ni∈D Ai is the set of joint actions a. • O = ⊗\ni∈D Oi is the set of joint observations o.\n• X = { X1, . . . ,Xm } is a set of state variables, or fac-\ntors, that determine the set of states S = ⊗ k∈X X k. • T (s′|s,a) is the transition model which is specified by a set of conditional probability tables (CPTs), one for each factor. • O(o|a,s′) is the observation model, specified by a CPT per agent.\n• R is a set of local reward functions, forming the global reward via R(s,a,s′), ∑\nl∈R R l(xl,al,x ′ l). They\ndepend on subsets of D and X (their scope).\n• b0 is the (factored) initial state distribution.\nEvery Dec-POMDP can be converted to a factored Dec-POMDP, but the additional structure that a factored model specifies is most useful when the problem is weakly coupled, meaning that there is sufficient conditional independence in the 2DBN and that the scopes of the reward functions is small. For instance, Fig. 1 shows the FireFightingGraph (FFG) problem [27], which we adopt as a running example. This problem defines a set of n+ 1 houses, each with a particular ‘fire level’ indicating if the house is burning and with what intensity. Each agent can fight fire at the house to its left or right, making observations of flames (or no flames) at the house it visited. Each house has a local reward function associated with it, which depends on the next-stage fire-level, as illustrated in Fig. 2(left) which shows the 2DBN for a 4-agent instantiation of FFG. The figure shows that the connections are local but there is no transition independence [3] or value factorization [16, 42]: all houses and agents are connected such that, over time, actions of each agent can influence the entire system. While FFG is a stylized example, such locally-connected systems can be found in applications as traffic control [47] or communication networks [29, 12, 18]. This paper focuses on problems with a finite horizon h such that t = 0, . . . ,h − 1. A policy πi for an agent i specifies an action for each observation history ~o ti = (o1i , . . . ,o t i). The task of planning for a factored DecPOMDP entails finding a joint policy π = 〈π1, . . . ,πn〉 with maximum value, i.e., a maximum expected sum of rewards: V (π),E[\n∑h−1 t=0 R(s,a,s ′) | b0,π]. Such an optimal joint policy is denoted π∗. In recent years, a number of methods have been pro-\nposed to find approximate solutions for factored DecPOMDPs with many agents [30, 16, 40, 27, 47] but none of these methods are able to give guarantees with respect to the solution quality (i.e., they are heuristic methods), leaving the user clueless of how well these methods perform on their problems. This is a principled problem; even finding an ǫ-approximate solution is NEXP-complete [31], which implies that general and efficient approximation schemes are unlikely to be found. In this paper, we propose a way forward by trying to find instance-specific upper bounds in order to provide information about the solution quality offered by heuristic methods."
    }, {
      "heading" : "3. SUB-PROBLEMS AND INFLUENCES",
      "text" : "The overall approach that we take is to divide the problem into sub-problems (defined here), compute overestimations of the achievable value for each of these subproblems (discussed in Section 4) and combine those into a global upper bound (Section 5)."
    }, {
      "heading" : "3.1 Sub-Problems (SPs)",
      "text" : "The notion of a sub-problem generalizes the concept of a local-form model (LFM) [28] to multiple agents and reward components. We give a relatively concise description of this formalization, for more details, please see [28].\nDefinition 2. A sub-problem (SP) Mc of a factored Dec-POMDP M is a tuple Mc = 〈M,D\n′,X ′,R′〉, where D′ ⊂ D,X ′ ⊂ X ,R′ ⊂ R denote subsets of agents, state factors and local reward functions.\nAn SP inherits many features from M: we can define local states xc ∈ ⊗ X∈X ′ and the subsets D ′,X ′,R′ induce local joint actions Ac = ⊗\ni∈D′ Ai, observations Oc = ⊗\ni∈D′ Oi, and rewards\nRc(xc,ac,xc ′),\n∑\nl∈R′\nR l(xl,al,x ′ l). (1)\nHowever, this is generally not enough to end up with a fully specified, but smaller, factored Dec-POMDP. This is illustrated in Fig. 2(left), which shows the 2DBN for a sub-problem of FFG involving two agents and three houses (dependence of observations oi on actions ai are not displayed). The figure shows that state factors X ∈ X ′ (in this caseXi andXi+2) can be the target of arrows pointing into the sub-problem from the non-modeled (dashed) part. We refer to such state factors as nonlocally affected factors (NLAFs) and denote them xnkc , where c indexes the SP and k indexes the factor. The other state factors in X ′ are referred to as only-locally affected factors (OLAFs) xlkc . The figure clearly shows that the transition probabilities are not well-defined since the NLAFs depend on the sources of the highlighted influence links. We refer to these sources as influence sources ut+1c = 〈 ytu,a t u 〉 (in this case ytu = 〈 Xi−1,Xi+3 〉 and atu = 〈 ati−1,a t i+2 〉 ). This means that an SP c has an underspecified transition model: Tc(x t+1 c |x t c,a t c,u t+1 c )."
    }, {
      "heading" : "3.2 Structural Assumptions",
      "text" : "In the most general form, the observation and reward model could also be underspecified. In order to simplify the exposition, we make two assumptions on the structure of an SP:\n1. For all included agents i ∈ D′, the state factors that can influence its observations (i.e., ancestors of oi in the 2DBN) are included in Mc.\n2. For all included reward components Rl ∈ R′, the state factors and actions that influence Rl are included in Mc.\nThat is, we assume that SPs exhibit generalized forms of observation independence,\nOc(oc|ac,xc ′),Pr(oc|ac,xc ′) = Pr(oc|a,s ′),\nand reward independence (cf. (1)). These are more general notions of observation and reward independence than used in previous work on TOI-Dec-MDPs [3] and ND-POMDPs [22], since we allow overlap on state factors that can be influenced by the agents themselves.2\nCrucially, however, we do not assume any form of transition independence (for instance, the sets X ′ of SPs can overlap), nor do we assume any of the transitiondecoupling (i.e., TD-POMDP [43]) restrictions. That is, we neither:\n• restrict which node types can affect ‘private’ nodes; nor\n• disallow concurrent interaction effects on ‘mutually modeled’ nodes.\nThis means that the assumptions (1,2 above) that we do make are without loss of generality: it is possible to make any Dec-POMDP problem satisfy them by introducing additional (dummy) state factors.3\n2Previous work only allowed ‘external’ or ‘unaffectable’ state factors to affect the observations or rewards of multiple components. 3In contrast, TOI-Dec-MDPs and ND-POMDPs impose"
    }, {
      "heading" : "3.3 Influence-Augmented SPs",
      "text" : "An LFM can be transformed to a so-called influenceaugmented local model, which captures the influence of the policies and parts of the environment that are not modeled in the local model [28]. Here we extend this approach to SPs, thus leading to influence-augmented sub-problems (IASPs). Intuitively, the construction of an IASP consists of two steps: 1) capturing the influence of the non-modeled parts of the problem (given π 6=c the policies of nonmodeled agents) in an incoming influence point I→c(π−c), and 2) using this I→c to create a model with a transformed transition model TI→c and no further dependence on the external problem. Step 1) can be done as follows: an incoming influence point can be specified as an incoming influence It→c for each stage: I→c = ( I1→c, . . . ,I h →c )\n. Each such It+1→c corresponds to the influence that the SP experiences at stage t+1, and thus specifies the conditional probability distribution of the influence sources ut+1c = 〈 ytu,a t u 〉\n. That is, assuming that the influencing agents use a deterministic policy πu, I t+1 →c is given by\nI(ut+1c |D t+1 c ) =\n∑\n~o tu\nπu(a t u|~o t u ) Pr(y t u,~o t u |D t+1 c ,b 0 ,π 6=c),\nwith Dt+1c the d-separating set for I t+1 →c : the history of a subset of all the modeled variables that d-separates the modeled variables from the non-modeled ones (formally Pr(ytu,~o t u |D t+1 c ,b 0,π 6=c,~θ t c ) = Pr(y t u,~o t u |D t+1 c ,b\n0,π 6=c), see [28] for details). Step 2) involves replacing the CPTs for all the NLAFs by the CPTs induced by I→c.\nDefinition 3. Let xnk,t+1c be an NLAF (with index k), and ut+1c (the instantiation of) the corresponding influence sources. Given the influence It+1→c (π 6=c), and its d-separating set Dt+1i , we define the induced CPT for xnk,t+1c as the CPT that has probabilities:\np I t+1 →c\n(xnk,t+1c |x t c,D t+1 c ,a t c) =\n∑\nu t+1 c =〈ytu,atu〉\nPr(xnk,t+1c |x t c,a t c,u t+1 c )I(u t+1 c |D t+1 c ). (2)\nFinally, we can define the IASP MIAc = 〈Mc,I→c〉 for an SP Mc = 〈M,D\n′,X ′,R′〉 as a factored Dec-POMDP with the following components. The set of state factors is X̄ = X ′ ∪ {Dc} such that states x̄ t c = 〈x t c,D t+1 c 〉 specify a local state of the SP, as well as the d-separating set Dt+1i for the next-stage influences. Only the agents (implying the actions and observations) and rewards from c participate: D̄ = D′ and R̄ = R′. For all OLAFs xlk,c we take the CPTs from the factored Dec-POMDP M, but for all NLAFs we take the induced CPTs, leading to an influence-augmented transition model which is the\nboth transition and observation independence, thereby restricting consideration to a proper subclass of those considered here.\nproduct of CPTs of OLAFs and NLAFS:\nT̄I→c(x t+1 c |〈x t c,D t+1 c 〉,a t c) = Pr(xl t+1 c |x t c,a t c)\n∑\nu t+1 c =〈ytu,atu〉\nPr(xnt+1c |x t c,a t c,u t+1 c )I(u t+1 c |D t+1 c ). (3)\n(Note that xtc,a t c,x t+1 c and D t+1 c together uniquely specify Dt+2c ). The observation model Ō follows directly from O (from M). Fig. 2(right) illustrates the IASP for FFG. We write Vc(π) for the value that would be realized for the reward components modeled in sub-problem c, under a given joint policy π:\nVc(π),E\n[\nh−1 ∑\nt=0\nR t c(s,a,s ′) | b0,π\n]\n.\nGiven the policies of other agents π 6=c, one can show that V ∗c (I→c(π 6=c)), the value of the optimal solution of an IASP constructed for the influence corresponding to π 6=c, equals the best-response value:\nV BR c (π 6=c),max\nπc Vc(πc,π 6=c) = V\n∗ c (I→c(π 6=c)). (4)\nThis extends the result in [28] to multiagent SPs."
    }, {
      "heading" : "4. LOCAL UPPER BOUNDS",
      "text" : "In this section we present our main technical contribution: the machinery to compute a number of influenceoptimistic upper bounds (IO-UBs) for the value of subproblems. In order to properly define this class of upper bound, we first define the locally-optimal value:\nDefinition 4. The locally-optimal value for an SP c,\nV LO c ,max\nπ6=c V\nBR c (π 6=c) = max\nπ6=c V\n∗ c (I→c(π 6=c)), (5)\nis the local value (considering only the rewards Rc) that can be achieved when all agents use a policy selected to optimize this local value. We will denote the maximizing argument by πLO6=c .\nNote that V LOc ≥ Vc(π ∗)—the value for the rewards Rc under the optimal joint policy π ∗—since π∗ optimizes the sum of all local reward functions: it might be optimal to sacrifice some reward Rc if it is made up by higher rewards outside of the sub-problem. V LOc expresses the maximal value achievable under a feasible incoming influence point; i.e., it is optimistic about the influence, but maintains that the influence is feasible. Computing this value can be difficult, since computing influences and subsequently constructing and optimally solving an IASP can be very expensive in general. However, it turns out computing upper bounds to V LOc can be done more efficiently, as discussed next. The IO-UBs that we propose in the remainder of this section upper bound V LOc by relaxing the requirement of the incoming influence being feasible, thus allowing\nfor more efficient computation. We present three approaches that each overestimate the value by being optimistic with respect to the assumed influence, but that differ in additional assumptions they make."
    }, {
      "heading" : "4.1 A Q-MMDP Approach",
      "text" : "The first approach we consider is called influenceoptimistic Q-MMDP (IO-Q-MMDP). Like all the heuristics we introduce, it assumes that the considered SP will receive the most optimistic (possibly infeasible) influence. In addition, it assumes that the SP is fully observable such that it reduces to a local multiagent MDP (MMDP) [5]. In other words, this approach is like Q-MMDP [35, 25], but is restricted to an SP and is influence-optimistic.4 This means that IO-Q-MMDP makes, in addition to influence optimism, another overestimation. While this negatively affects the tightness of the upper bound, it has as its advantage that the computational complexity is relatively low. Formally, we can describe IO-Q-MMDP as follows. In the first phase, we apply dynamic programming to compute the action-values for all local states:\nQ(xtc,a t c) = max\nu t+1 c\n∑\nx t+1 c\nPr(xlt+1c |x t c,a t c) Pr(xn t+1 c |x t c,a t c,u t+1 c )\n[\nRc(x t c,a t c,x t+1 c ) + max\na t+1 c\nQ(xt+1c ,a t+1 c )\n]\n(6)\nComparing this equation to (3), it is clear that this equation is optimistic with respect to the influence: it selects the sources ut+1c in order to select the most beneficial transition probabilities. In the second phase, we use these values to compute an upper bound:\nV̂ M c ,max\nac\n∑\nxc\nb 0(xc)Q(xc,ac).\nThis procedure is guaranteed to yield an upper bound to the locally-optimal value for the SP.\nTheorem 1. IO-Q-MMDP yields an upper bound to the locally-optimal value: V LOc ≤ V̂ M c .\nProof. An inductive argument easily establishes that, due to the maximization it performs, (6) is at least as great as the Q-MMDP value (for all Dt+1c ) of any feasible influence, given by:\nQ MMDP c (〈x t c,D t+1 c 〉,a t c) =\n∑\nx t+1 c\nT̄I→c(x t+1 c |〈x t c,D t+1 c 〉,a t c)\n[\nRc(x t c,a t c,x t+1 c ) + max\na t+1 c\nQ(〈xt+1c ,D t+2 c 〉,a t+1 c )\n]\n. (7)\nTherefore (6) is at least as great as the Q-MMDP value (7) induced by πLO6=c (the maximizing argument of (5)), for all xtc,D t+1 c ,a t c. This directly implies\nV̂ M c ≥ V MMDP c (I→c(π LO 6=c )),\n4Note that “Q-MMDP” typically has been referred to as “Q-MDP”, but we like to emphasize the fact that there are still multiple agents present in the sub problem.\nMoreover, it is well known that, for any Dec-POMDP, the Q-MMDP value is an upper bound to its value [35], such that\nV MMDP c (I→c(π LO 6=c )) ≥ V ∗ c (I→c(π LO 6=c )).\nWe can conclude that V̂ Mc is an upper bound to the Dec-POMDP value of the IASP induced by πLO6=c :\nV̂ M c ≥ V ∗ c (I→c(π LO 6=c )) = max\nπ6=c V\n∗ c (I→c(π 6=c)) = V LO c ,\nwith the identities given by (5), thus proving the theorem.\nThe upshot of (6) is that there are no dependencies on d-separating sets and incoming influences anymore: the IO assumption effectively eliminates these dependencies. As a result, there is no need to actually construct the (potentially very large-state-space) IASPs if all we are interested in is an upper bound."
    }, {
      "heading" : "4.2 A Q-MPOMDP Approach",
      "text" : "The IO-Q-MMDP approach of the previous section introduces overestimations through both influence-optimism as well as assuming full observability. Here we tighten the upper bound by weakening the second assumption. In particular, we propose an upper bound based on the underlying multiagent POMDP (MPOMDP). A multiagent POMDP [21, 1] is partially observable, but assumes that the agents can freely communicate their observations, such that the problem reduces to a special type of centralized model in which the decision maker (representing the entire team of agents) takes joint actions, and receives joint observations. As a result, the optimal value for an MPOMDP is analogue to that of a POMDP:\nQ(bt,at) = R(bt,at) + ∑\not+1\nPr(ot+1|bt,at)V (bt+1) (8)\nwhere bt+1 is the joint belief resulting from performing Bayesian updating of the bt given at and ot+1. Using the value function of the MPOMDP solution as a heuristic (i.e., an upper bound) for the value function of a Dec-POMDP is a technique referred to as QMPOMDP [32, 25]. Here we combine this approach with optimistic assumptions on the influences, leading to influence-optimistic Q-MPOMDP (IO-Q-MPOMDP). In case that the influence on an SP is fully specified, (8) can be readily applied to the IASP. However, we want to deal with the case where this influence is not specified. The basic, conceptually simple, idea is to move from the influence-optimistic MMDP-based upper bounding scheme from in Section 4.1 to one based on MPOMDPs. However, it presents a technical difficulty, since it is not directly obvious how to extend (6) to deal with partial observability. In particular, in the MPOMDP case as given by (8), the state xtc is replaced by a belief over such local states and the influence source ut+1c affects the value by both manipulating the transition and observation probabilities, as well as the resulting beliefs.\nTo overcome these difficulties, we propose a formulation that is not directly based on (8), but that makes use of ‘back-projected value vectors’. That is, it is possible to rewrite the optimal MPOMDP value function as:5\nQ(bt,at) = bt · ra + γ ∑\not+1\nmax νao∈Vao\nb t · νao, (9)\nwhere · denotes inner product and where νao ∈ Vao are the back-projections of vectors value vectors ν ∈ Vt+1:\nν ao(st),\n∑\nst+1\nO(ot+1|at,st+1)T (st+1|stat)ν(st+1). (10)\n(Please see, e.g., [34, 33] for more details.) A key insight that enables carrying influence-optimism to the MPOMDP case, is that this back-projected form (10) does allow us to take the maximum with respect to unspecified influences. That is, we define the influenceoptimistic back-projection as:\nν IO ao (x t c),max\nu t+1 c\n∑\nx t+1 c\nO(ot+1c |ac,x t+1 c )\nPr(xnt+1c |x t c,ac,u t+1 c ) Pr(xl t+1 c |x t c,ac)ν IO(xt+1c ). (11)\nSince this equation does not depend in any way on the d-separating sets and influence, we can completely avoid generating large IASPs. As for implementation, many POMDP solution methods [6, 13] are based on such back-projections and therefore can be easily modified; all that is required is to substitute these the back projections by their modified form (11). When combined with an exact POMDP solver, such influence-optimistic back-ups will lead to an upper bound V̂ Pc , to which we refer as IO-Q-MPOMDP, on the locally-optimal value.\nLemma 1. Let πt:h−1c be a (h−t)-steps-to-go policy. Let ν ∈ V and νIO ∈ VIO be the vectors induced by πt:h−1c under regular MPOMDP back-projections (for some I→c), and under IO back-projections respectively. Then\n∀xtc max D\nt+1 c\nν(〈xtc,D t+1 c 〉) ≤ ν IO(xtc).\nProof. The proof is listed in Appendix A.\nThis lemma provides a strong result on the relation of values computed under regular MPOMDP backups versus influence-optimistic ones. It allows us to establish the following theorem:\nTheorem 2. For an SP c, V MPI→c (b̄ t I→c), the MPOMDP value induced by any feasible influence I→c, is upper bounded by V IPc (b̄ t IO), the value computed by an exact influence-optimistic MPOMDP method: V MPI→c (b̄ t I→c) ≤ V IPc (b̄ t IO), provided that the marginals of b̄ t I→c coincides with b̄tIO:\n(A1) ∀xtc\n∑\nD t+1 c\nb̄ t I→c(〈x t c,D t+1 c 〉) = b̄ t I→c(x t c) = b̄ t IO(x t c)\n5In this section and the next, we will restrict ourselves to rewards of the form R(s,a) to reduce the notational burden, but the presented formulas can be extended to deal with R(s,a,s′) formulations in a straightforward way.\nProof. The theorem holds if V MPI→c (b̄ t I→c) ≤ V IP c (b̄ t IO) and thus if, ∀at Q MP I→c(b̄ t I→c ,a t) ≤ QIPc (b̄ t IO ,a\nt). We assume an arbitrary at and write Vat for the vectors that specify at as the first action:\nQ MP I→c(b̄ t c,a t) = max ν∈V\nat\nb̄ t I→c · ν\n= max ν∈V\nat\n∑\n〈xtc,D t+1 c 〉\nb̄ t I→c(〈x t c,D t+1 c 〉)ν(〈x t c,D t+1 c 〉)\n≤ max ν∈V\nat\n∑\n〈xtc,D t+1 c 〉\nb̄ t I→c(〈x t c,D t+1 c 〉) max\nD t+1 c\nν(〈xtc,D t+1 c 〉)\n= max ν∈V\nat\n∑\nxtc\nb̄ t I→c(x t c) max\nD t+1 c\nν(〈xtc,D t+1 c 〉)\n{A1} = max ν∈V\nat\n∑\nxtc\nb̄ t IO(x t c) max\nD t+1 c\nν(〈xtc,D t+1 c 〉)\n{Lemma 1}\n≤ max νIO∈V\nIO,at\n∑\nxtc\nb̄ t IO(x t c)ν IO(xtc)\n= max νIO∈V\nIO,at\nb̄ t IO · ν IO\n=QIPc (b̄ t IO,a t),\nthus proving the theorem.\nCorollary 1. IO-Q-MPOMDP yields an upper bound to the locally-optimal value: V LOc ≤ V̂ P c .\nProof. The initial beliefs are defined such that the above condition holds. That is:\n∑\nD t+1 c\nb̄ 0 I→c(〈x 0 c,D 1 c = ∅〉) = b̄ 0 IO(x t c).\nTherefore, application of Theorem 2 to the initial belief:\n∀I→c V̂ P c ,V IP c (b̄ 0 IO) ≥ V MP I→c (b̄ 0 I→c),V MPOMDP c (I→c)\nIt is well-known that the MPOMDP value is an upper bound to the Dec-POMDP value [25], such that\nV MPOMDP c (I→c(π LO 6=c )) ≥ V ∗ c (I→c(π LO 6=c )),\nand we can immediately conclude that\nV̂ P c ≥ V ∗ c (I→c(π LO 6=c )) = max\nπ6=c V\n∗ c (I→c(π 6=c)) = V LO c ,\nwith the identities given by (5), proving the result."
    }, {
      "heading" : "4.3 A Dec-POMDP Approach",
      "text" : "The previous approaches compute upper bounds by, apart from the IO assumption, additionally making optimistic assumptions on observability or communication capabilities. Here we present a general method for computingDec-POMDP-based upper bounds that, other than the optimistic assumptions about neighboring SPs, make no additional assumptions and thus provide the tightest bounds out of the three upper bounds that we propose. The approach builds on the recent insight [17, 8, 24] that a Dec-POMDP can be converted to a special\ncase of POMDP (for an overview of this reduction, see [23]), and that therefore we can leverage the influenceoptimistic back-projection (11) to compute an IO-UB that we refer to as IO-Q-Dec-POMDP. As in the previous two sub-sections, we will leverage optimism with respect to an influence-augmented model that we will never need to construct. In particular, as explained in Section 3 we can convert an SP Mc to an IASP MIAc given an influence I→c. Since such an IASP is a Dec-POMDP, we can convert it to a special case of POMDP:\nDefinition 5. A plan-time influence-augmented subproblem, MPT-IAc , is a tuple M PT-IA c (Mc,I→c) = 〈\nŠ,Ǎ,ŤI→c ,Ř,Ǒ,Ǒ,ȟ,b̌0 〉 , where: • Š is the set of states št = 〈 x̄tc,~o t c 〉 = 〈 xtc,D t+1 c ,~o t c 〉 . • Ǎ is the set of actions, each ǎt corresponds to a local joint decision rule δtc in the SP. • ŤI→c(š t+1|št,ǎt) is the transition function defined\nbelow.\n• Ř(št,ǎt) = Rc(x t c,δ t c(~o t c )). • Ǒ = {NULL}. • Ǒ specifies that observation NULL is received with\nprobability 1 (irrespective of the state and action).\n• The horizon is just the horizon: ȟ = h. • b̌0 is the initial state distribution. Since there is only\none ~o 0 (i.e., the empty joint observation history).\nThe transition function specifies:\nŤI→c( 〈 x t+1 c ,D t+2 c ,~o t+1 c 〉 | 〈 x t c,D t+1 c ,~o t c 〉 ,δ t c) , T̄I→c(x t+1 c |〈x t c,D t+1 c 〉,δ t c(~o t c ))Ō(o t+1 c |δ t c(~o t c ),x t+1 c )\nif ~o t+1c = ( ~o tc ,o t+1 c ) and 0 otherwise. In this equation T̄I→c ,Ō are given by the IASP (cf. Section 3). 6\nThis reduction shows that it is possible to compute V ∗c (I→c(π 6=c)), the optimal value for an SP given an influence point I→c, but the formulation is subject to the same computational burdens as solving a regular IASP: constructing it is complex due to the inference that needs to be performed to compute I→c, and subsequently solving the IASP is complex due to the large number of augmented states št = 〈 xtc,D t+1 c ,~o t c 〉\n. Fortunately, here too we can compute an upper bound to any feasible incoming influence, and thus to V LOc , by using optimistic backup operations with respect to a underspecified model, to which we refer as simply plantime SP:\nDefinition 6. We define the plan-time sub-problem MPTc as an under-specified POMDP M PT c (Mc,·) = 〈\nŠ,Ǎ,Ť(·),Ř,Ǒ,Ǒ,ȟ,b̌0 〉 with • states of the form št = 〈 xtc,~o t c 〉 ,\n6Remember that Dt+2c is a function of the specified quantities: Dt+2c = d(x t c,D t+1 c ,δ t c(~o t c ),x t+1 c ).\n• an underspecified transition model\nˇT(·)(š t+1|št,ǎt),\nTc(x t+1 c |x t c,δ t c(~o t c ),u t+1 c )Oc(o t+1 c |δ t c(~o t c ),x t+1 c ),\n• and Ǎ,Ř,Ǒ,Ǒ,ȟ,b̌0 as above.\nSince this model is a special case of a POMDP, the theory developed in Section 4.2 applies: we can maintain a plan-time sufficient statistic σc,t (essentially the ‘belief’ b̌ over augmented states št = 〈 xtc,~o t c 〉\n) and we can write down the value function using (9). Most importantly, the IO back-projection (11) also applies, which means that (similar to the MPOMDP case) we can avoid ever constructing the full PT-IASP. The IO back-projection in this case translates to:\nν IO δtc (xtc,~o t c ),max\nu t+1 c\n∑\nx t+1 c\nPr(ot+1c |δ t c(~o t c ),x t+1 c )\nPr(xnt+1c |x t c,δ t c(~o t c ),u t+1 c ) Pr(xl t+1 c |x t c,δ t c(~o t c ))\nν IO(xt+1c ,~o t+1 c ). (12)\nHere, we omitted the subscript for the NULL observation. Also, note that O(ot+1i |a t i,x t+1 i ) in (11) corresponds to the NULL observation in the PT model, but since the observation histories are in the states, Pr(ot+1c |δ t c(~o t c ),x t+1 c ) comes out of the transition model).\nAgain, given this modified back-projection, the IO-QDec-POMDP value V̂ Dc can be computed using any exact POMDP solution method that makes use of vector backprojections; all that is required is to substitute these the back projections by their modified form (12).\nCorollary 2. IO-Q-Dec-POMDP yields an upper bound to the locally-optimal value: V LOc ≤ V̂ D c .\nProof. Directly by applying Corollary 1 to MPTc ."
    }, {
      "heading" : "4.4 Complexity Analysis",
      "text" : "Due to the maximization in (6), (11) and (12), IO back-projections are more costly than regular (non-IO) back-projections. In particular, the complexity of each backup is multiplied by the number of influence source instantiations ∣ ∣ut+1c ∣\n∣. As such, the relative overhead, when compared to solving the SPs as regular (non-IO) MMDPs, MPOMDPs and Dec-POMDPs, is equal for all methods."
    }, {
      "heading" : "5. GLOBAL UPPER BOUNDS",
      "text" : "We next discuss how the methods to compute local upper bounds can be employed in order to compute a global upper bound for factored Dec-POMDPs. The basic idea is to apply a non-overlapping decomposition C (i.e., a partitioning) of the reward functions {\nRl } of the original factored Dec-POMDP into SPs c ∈ C, and to compute an IO upper bound V̂ IOc for each (which can be any of the three IO-UBs proposed in Section 4). Our global influence-optimistic upper bound is\nthen given by:\nV̂ IO , ∑\nc∈C\nV̂ IO c . (13)\nWe illustrate the construction of a global upper bound V̂ for the 6-agent FFG in Fig. 3, which shows the original problem (top row) and two possible decompositions in SPs. The second row specifies a decomposition into two SPs, while the third row uses three SPs. The illustration clearly shows how a decomposition eliminates certain agents completely and replaces them with optimistic assumptions: E.g., in the second row, during the computation of V̂ IOc for both SPs (c = 1,2) the assumption is made that agent 3 will always fight fire in the SP under concern. Effectively we assume that agent 3 fights fire at both house 3 and house 4 simultaneously (and hence is represented by a superhero figure). Fig. 3 also illustrates that, due to the line structure of FFG, there are two types of SPs: ‘internal’ SPs which make optimistic assumptions on two sides, and ‘edge’ SPs that are optimistic at just one side. Finally, we formally prove the correctness of our proposed upper bounding scheme.\nTheorem 3. Let C be a partitioning of the reward function set R into sub-problems such that every Rl is represented in one SP c, then the global IO-UB is in fact an upper bound to the optimal value V̂ IO ≥ V ∗.\nProof. Starting from the definition (13), we have\nV̂ IO , ∑\nc∈C\nV̂ IO c {Section 4} ≥ ∑\nc∈C\nV LO c\n, ∑\nc∈C\nmax π6=c\nV BR c (π 6=c)\n, ∑\nc∈C\nmax π6=c max πc Vc(πc,π 6=c)\n≥ max π\n∑\nc∈C\nVc(πc,π 6=c) {Cis a partition} = V ∗\nthus proving the result."
    }, {
      "heading" : "6. EMPIRICAL EVALUATION",
      "text" : "In order to test the potential impact of the proposed influence-optimistic upper bounds, we present numerical results in the context of a number of benchmark problems. In this evaluation, we focus on the (relative) values found by these heuristics, as we hope that these will spark a number of interesting ideas for further research (such as the notion of ‘influence strength’, its relation to approximability of factored Dec-POMDPs, and the key idea that reasonable bounds for very large problems may be possible). We do not investigate timing results as the analysis of Section 4.4 indicates that relative timing results follow those of regular (non-IO) MMDP, MPOMDP and Dec-POMDP methods; see, e.g., [25] for a comparison of such timing results."
    }, {
      "heading" : "6.1 Comparison of Different Bounds",
      "text" : "The bounds that we propose are ordered in tightness, V̂ Dc ≤ V̂ P c ≤ V̂ M c , similar to how regular (nonIO) Dec-POMDP, Q-MPOMDP, and Q-MMDP values relate [25]. To get an understanding of how these differences turn out in practice, Fig. 4(left) compares the different upper bounds introduced. Although the approach described in the paper is general, in the numerical evaluation here we exploit the property that the optimistic influences are easily identified off-line, which allows for the construction of small ‘optimistic Dec-POMDPs’ without sacrificing in bound quality. E.g., for a 3-house FFG ‘edge’ SP, we define a regular 3-house Dec-POMDP where the transitions probabilities for the first house (say Xi in Fig. 2) are modified to account for the optimistic assumption that another (superhero) agent fights fire there and that its neighbor is not burning (i.e., ai−1 = right and X i−1 = not burning in Fig. 2). The values V̂ Dc (resp. V̂ P c ,V̂ M c ) are computed by running a state-of-the-art Dec-POMDP solver [24] (resp. incremental pruning [6], plain dynamic programming) on such optimistically defined problems.\nFig. 4(left) shows that V̂ Dc ,V̂ P c can be tighter than V̂ Mc in practice. Missing bars indicate time-outs (>4h). In most cases, the difference between IO-Q-MPOMDP and IO-Q-Dec-POMDP is small, but these could become larger for longer horizons [25]. We performed the same analysis for the Aloha benchmark [27], and found very similar results. We also compare the bounds found on the different types of SPs (internal and edge-cases, see Fig. 3) encountered in FFG (h = 4). In addition, Fig. 4(middle) also includes—if computable within the allowed time— values of SPs that are ‘full’ problems (i.e., the regular optimal Dec-POMDP value for the full FFG instance with the indicated number of agents.) This makes clear that the optimistic assumption has quite some effect: being optimistic at one edge more than halves the optimal cost, and the IO assumption at both edges of the SP leads to another significant reduction of that cost. This is to be expected: the optimistic problems assume that there always will be another agent fighting fire at the\nhouse at an optimistic edge, while the full problem never has another agent at that same house. When also taking into account the transition probabilities—two agents at a house will completely extinguish a fire—it is clear that the IO assumption should have a high impact on the local value."
    }, {
      "heading" : "6.2 The Effect of Influence Strength",
      "text" : "Fig. 4(middle) makes clear that the IO assumption in FFG are quite strong and leads to a significant overestimation of the local value compared to the ‘full’ problem. We say that FFG has a high influence strength. In fact, this hints at a new dimension of the qualification of weak coupling [44] that takes into account the variance in NLAF probabilities (as a function of the change of value of the influence source) and their impact on the local value. As a preliminary investigation of this concept, we devise a modification of FFG where the influence strength can be controlled. In particular, we parameterize the probability that a fire is extinguished completely when 2 agents visit the same house, which is set to 1 in the original problem definition. Lower values of this probability mean that optimistically assuming there is another agent at a house will lead to less advantage, and thus lower influence strength. Fig. 4(right) shows the results of this experiment. It shows that there is a clear relation between the fireextinguish probability when two agents fight fire at a house, and the ratio between the ‘regular’ value (the Dec-POMDP value) and optimistic value. It also shows that SPs with more agents are less affected: this makes sense since optimistic assumptions account for a smaller fraction of the achievable value. In other words, larger sub-problems give a tighter approximation."
    }, {
      "heading" : "6.3 Bounding Heuristic Methods",
      "text" : "Here we investigate the ability to provide informative global upper bounds. While the previous analysis shows that the overestimation is quite significant at the true edges of the problem (where no agents exist), this is not necessarily informative of the overestimation at internal edges in decompositions of larger problems (where other agents do exist, even if not superheros). As such, besides investigating the upper bounding capability, the analysis here also provides a better understanding of such internal overestimations. We use the tightest upper bound we could find by con-\nsidering different SP partitions, with sizes ranging from n = 2–5, and investigate the guarantees that it can provide for transfer planning (TP) [27], which is one of the methods capable of providing solutions for large factored Dec-POMDPs. Since the method is a heuristic method that does not provide the exact value of the reported joint policy, the value of TP, V TP , is determined using 10.000 simulations of the found joint policy leading to accurate estimates.7 To put the results into context, we also show the value of a random policy. Finally, we show (second y-axis in Fig. 5) what we call the empirical approximation factor (EAF):\nEAF = max{ V̂ IO V TP , V TP V̂ IO }.\nThis is a number comparable to the approximation factors of approximation algorithms [39].8\nFig. 5 shows the results that indicate that the upper bound is relatively tight: the solutions found by TP are not too far from the upper bound. In particular, the EAF lies typically between 1.4 and 1.7, thus providing firm guarantees for solutions of factored Dec-POMDPs with up to 700 agents. Moreover, we see that we see that the EAF stays roughly constant for the larger problem instances indicating that relative guarantees do not degrade as the number of agents increase. Of course, the question of whether the optimal value lies closer to the blue (UB) or orange (TP) line remains open; only\n7Note that there is no method for Dec-POMDP policy evaluation that runs in polynomial time. In fact, existence of such a method would reduce the complexity of solving a Dec-POMDP to NP, an impossibility since the time hierarchy theorem implies that NP6= NEXP. 8‘Empirical’ emphasizes the lack of a priori guarantees.\nfurther research on improved (heuristic) solution methods and tighter upper bounds can answer that question. However, we have gone from a situation where the only upper bound we had was ‘predict Rmax for every stage’ (which corresponds to the value 0 and EAF=∞) to a situation were we have a much more informative bound. Results obtained for a similar approach for Aloha using SPs containing up to 6 agents are shown in Table 1. The numbers clearly illustrate that it is possible to provide very strong guarantees for problems up to 250 agents (beyond which memory forms the bottleneck for TP); the solution for the n = 50 instance is essentially optimal, indicating also a very tight bound for this problem."
    }, {
      "heading" : "6.4 Improved Heuristic Influence Search",
      "text" : "Aside from analyzing the solution quality of approximate methods, our bounds can also be used in optimal methods. In particular, A*-OIS [41], solves TDPOMDPs (a sub-class of factored Dec-POMDPs) by decomposing them into 1-agent SPs, searching through the space of influences, and pruning using optimistic heuristics. However, existing A*-OIS heuristics treat the unspecified-influence stages of the SPs as fully-observable. In contrast, IO-Q-MPOMDP models the partial observability of the SPs. We now present results that suggest an added computational benefit to treating partial observability in the A*-OIS heuristics. Table 2 illustrates the differences in pruning afforded by four different A*-OIS heuristics: M is the baseline MDP-based heuristic from [41], P is shorthand for IO-Q-MPOMDP, and M’ and P’ are variations that improve tightness with locally-derived probability bounds on the optimistic influences. We report node counts and runtime across several problem instances from the HouseSearch domain [41]. As shown, the POMDP-based heuristics tends to allow for more pruning (fewer expanded nodes) and thereby reduced computation in comparison to their MDP-based counterparts. Contrasting this reduction across two variants of Diamond HouseSearch suggests that the IO-QMPOMDP heuristics gain more advantage when observability is more restricted. However, this advantage is sometimes outweighed by the increased computational overhead of a more complex heuristic calculation (such as in Squares HouseSearch h = 4)."
    }, {
      "heading" : "7. RELATED WORK",
      "text" : "Here we provide an overview of related methods and approaches.\nScalable Heuristic Methods. In recent years, many scalable approaches without guarantees have been developed for Dec-POMDPs and related models [14, 46, 48, 16, 40, 37, 27, 47, 36]. The upper bounding mechanism that we propose could be useful for benchmarking many of these methods.\nSub-Problems vs. Source Problems. The use of sub-problems (SPs) is conceptually similar to the use of source problems in transfer planning (TP) [27]. Differences are that, in our partitioningbased upper bounding scheme, SPs are selected such that they do not contain overlapping rewards, while TP allows for overlapping source problems. Moreover TP does not consider optimistic influences but implicitly assumes arbitrary influences. Finally, TP is used as a way to compute a heuristic for the original problem; our approach here simply returns a scalar value (although extensions to use IO heuristics for heuristic search for factored Dec-POMDPs are an interesting direction for future research).\nOptimism with Respect to Influences. The idea of being optimistic with respect to external influences has been considered before. Kumar&Zilberstein [14] make optimistic assumptions on transitions in an ND-POMDP to derive an MMDP-based policy which is used to sample belief points for memory-bounded dynamic programming. The approach does not use these assumptions to upper bound the global value and the formulation is specific to ND-POMDPs. As described in the experiments, [41] use a local upper bound in order to perform heuristic influence search for TD-POMDPs. IOQ-MMDP can be seen as a generalization of that heuristic to both factored Dec-POMDPs and multiagent subproblems, while our other heuristics additionally deal with partial observability.\nQuality Guarantees and Upper Bounds for Large Dec-POMDPs. As mentioned in the introduction, a few approaches to computing upper bound for large-scale MASs and employing them in heuristic search methods have been proposed. In particular, there are some scalable approaches with guarantees for the case of transition and observation independence as encountered in TOI-MDPs and ND-POMDPs [3, 2, 38, 19, 10, 9]. The approach by Witwicki for TD-POMDPs [42, Sect. 6.6] is a bit more general in that it allows some forms of transition dependence, as long as the interactions are directed (one agent can affect another) and no two agents can affect the same factor in the same stage. In addition, scalability relies on each agent having only a handful of ‘interaction ancestors’. However, these previous approaches rely on the true value function being factored as the sum of a set E of local value components:\nV (π) = ∑\ne∈E\nVe(πe),\nwhere πe is the local joint policy of the agents that participate in component e. (This is also referred to as the ‘value factorization’ framework [16]). For this setting, an upper bound is easily constructed as the sum of local upper bounds:\nV̂ (π) = ∑\ne∈E\nV̂e(πe).\nWhile this resembles (and in fact formed the inspiration for) our IO upper bound (13), the crucial distinction is\nthat for value factorized settings computing V̂e does not require any influence-optimism: the reason that valuefactorization holds is precisely because there are no influence sources for the components. As such, our influenceoptimistic upper bounds can be seen as a strict generalization of the upper bounds that have been employed for settings with factored value functions. Investigating if such methods, such as the method by Dibangoye et al. [9] can be modified to use our IO-UBs is an interesting direction of future work.\nFinally, the event-detecting multiagent MDP [15] provides quality guarantees for a specific class of sensor network problems by using the theory of submodular function maximization. It is the only previous method with quality guarantees that delivers scalability with respect to the number of agents without assuming that the value function of the problem is additively factored into small components."
    }, {
      "heading" : "8. CONCLUSIONS",
      "text" : "We presented a family of influence-optimistic upper bounds for the value of sub-problems of factored DecPOMDPs, together with a partition-based decomposition approach that enables the computation of global upper bounds for very large problems. The approach\nbuilds upon the framework of influence-based abstraction [28], but—in contrast to that work—makes optimistic assumptions on the incoming ‘influences’, which makes the sub-problems easier to solve. An empirical evaluation compares the proposed upper bounds and demonstrates that it is possible to achieve guarantees for problems with 100s of agents, showing that found heuristic solution are in fact close to optimal (empirical approximation factors of < 1.7 in all cases and sometimes substantially better). This is a significant contribution, given the complexity of computing ǫ-approximate solutions and the fact that tight global upper bounds are of crucial importance to interpret the quality of heuristic solutions. Intuitively, the proposed approach is expected to work well in settings where the Dec-POMDP is ‘weakly’ coupled. The work by Witwicki&Durfee [44] identifies three dimensions that can be used to quantify the notion of weak coupling. Our experiments suggest the existence of a new dimension that can be thought of as influence strength. This dimension captures the impact of nonlocal behavior on local values and thus directly relates to how well a problem can be approximated using localized components. In this paper we focused on the finite-horizon case, but the principle of influence optimism underlying the upper-bounding approach can be applied in infinite-horizon settings too. Also, it can be trivially modified to compute ‘pessimistic’ influence (i.e., lower) bounds, which could be useful in competitive settings, or for risk-sensitive planning [20]. It is also immediately applicable to problems involving ‘unpredictable’ dynamics [45, 7]. Finally, our upper-bounding method contributes a useful precursor for techniques that automatically search the space of possible upper bounds decompositions, efficient optimal influence-space heuristic search methods (for which we provided preliminary evidence in this paper), and A* methods for a large class of factored Dec-POMDPs. In particular, a promising idea is to employ our factored upper bounds in combination with the heuristic search methods by Dibangoye et al. [9]. While it is not possible to directly use that method since it additionally requires a factored lower bound function, pessimisticinfluence bounds could provide those. A limitation of the current approach is that the subproblems still need to be relatively small, since we rely on optimal optimistic solution of the sub-problems. Developing more scalable ‘optimistic solution methods’ thus is an important direction of future work. Experiments with influence search indicate that using probabilistic bounds on the positive influences has a major impact [41]. As such, another important direction of future work is investigating if it is possible to develop tighter upper bounds by making more realistic optimistic assumptions.\nAcknowledgments. F.O. is supported by NWO Innovational Research Incentives Scheme Veni #639.021.336."
    }, {
      "heading" : "9. REFERENCES",
      "text" : "[1] C. Amato and F. A. Oliehoek. Bayesian\nreinforcement learning for multiagent systems with state uncertainty. In AAMAS Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains (MSDM), pages 76–83, 2013.\n[2] R. Becker, S. Zilberstein, and V. Lesser. Decentralized Markov decision processes with event-driven interactions. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 302–309, 2004.\n[3] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman. Transition-independent decentralized Markov decision processes. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 41–48, 2003.\n[4] C. Boutilier, T. Dean, and S. Hanks. Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research, 11:1–94, 1999.\n[5] C. Boutilier and D. Poole. Computing optimal policies for partially observable decision processes using compact representations. In Proc. of the National Conference on Artificial Intelligence, pages 1168–1175, 1996.\n[6] A. Cassandra, M. L. Littman, and N. L. Zhang. Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes. In Proc. of Uncertainty in Artificial Intelligence, pages 54–61, 1997.\n[7] K. V. Delgado, S. Sanner, and L. N. De Barros. Efficient solutions to factored MDPs with imprecise transition probabilities. Artificial Intelligence, 175(9):1498–1527, 2011.\n[8] J. S. Dibangoye, C. Amato, O. Buffet, and F. Charpillet. Optimally solving Dec-POMDPs as continuous-state MDPs. In Proc. of the International Joint Conference on Artificial Intelligence, 2013.\n[9] J. S. Dibangoye, C. Amato, O. Buffet, and F. Charpillet. Exploiting separability in multiagent planning with continuous-state MDPs. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 1281–1288, 2014.\n[10] J. S. Dibangoye, C. Amato, A. Doniec, and F. Charpillet. Producing efficient error-bounded solutions for transition independent decentralized MDPs. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 539–546, 2013.\n[11] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun. Approximate solutions for partially observable stochastic games with common payoffs. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 136–143, 2004.\n[12] E. A. Hansen, D. S. Bernstein, and S. Zilberstein.\nDynamic programming for partially observable stochastic games. In Proc. of the National Conference on Artificial Intelligence, pages 709–715, 2004.\n[13] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1-2):99–134, 1998.\n[14] A. Kumar and S. Zilberstein. Constraint-based dynamic programming for decentralized POMDPs with structured interactions. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 561–568, 2009.\n[15] A. Kumar and S. Zilberstein. Event-detecting multi-agent MDPs: Complexity and constant-factor approximations. In Proc. of the International Joint Conference on Artificial Intelligence, pages 201–207, 2009.\n[16] A. Kumar, S. Zilberstein, and M. Toussaint. Scalable multiagent planning using probabilistic inference. In Proc. of the International Joint Conference on Artificial Intelligence, pages 2140–2146, 2011.\n[17] L. C. MacDermed and C. Isbell. Point based value iteration with optimal belief compression for Dec-POMDPs. In Advances in Neural Information Processing Systems 26, pages 100–108. 2013.\n[18] A. Mahajan and M. Mannan. Decentralized stochastic control. Annals of Operations Research, pages 1–18, 2014.\n[19] J. Marecki, T. Gupta, P. Varakantham, M. Tambe, and M. Yokoo. Not all agents are equal: scaling up distributed POMDPs for agent networks. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 485–492, 2008.\n[20] J. Marecki and P. Varakantham. Risk-sensitive planning in partially observable environments. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 1357–1368, 2010.\n[21] J. V. Messias, M. T. J. Spaan, and P. U. Lima. Efficient offline communication policies for factored multiagent POMDPs. In NIPS 24, pages 1917–1925. 2011.\n[22] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo. Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs. In Proc. of the National Conference on Artificial Intelligence, pages 133–139, 2005.\n[23] F. A. Oliehoek and C. Amato. Dec-POMDPs as non-observable MDPs. Technical report, University of Amsterdam, 2014.\n[24] F. A. Oliehoek, M. T. J. Spaan, C. Amato, and S. Whiteson. Incremental clustering and expansion for faster optimal planning in decentralized POMDPs. Journal of AI Research, 46:449–509, 2013.\n[25] F. A. Oliehoek, M. T. J. Spaan, and N. Vlassis. Optimal and approximate Q-value functions for decentralized POMDPs. Journal of AI Research, 32:289–353, 2008.\n[26] F. A. Oliehoek, M. T. J. Spaan, S. Whiteson, and N. Vlassis. Exploiting locality of interaction in factored Dec-POMDPs. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 517–524, 2008.\n[27] F. A. Oliehoek, S. Whiteson, and M. T. J. Spaan. Approximate solutions for factored Dec-POMDPs with many agents. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 563–570, 2013.\n[28] F. A. Oliehoek, S. Witwicki, and L. P. Kaelbling. Influence-based abstraction for multiagent systems. In Proc. of the AAAI Conference on Artificial Intelligence, pages 1422–1428, 2012.\n[29] J. M. Ooi and G. W. Wornell. Decentralized control of a multiple access broadcast channel: Performance bounds. In Proc. of the 35th Conference on Decision and Control, pages 293–298, 1996.\n[30] J. Pajarinen and J. Peltonen. Efficient planning for factored infinite-horizon DEC-POMDPs. In Proc. of the International Joint Conference on Artificial Intelligence, pages 325–331, 2011.\n[31] Z. Rabinovich, C. V. Goldman, and J. S. Rosenschein. The complexity of multiagent systems: the price of silence. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 1102–1103, 2003.\n[32] M. Roth, R. Simmons, and M. Veloso. Reasoning about joint beliefs for execution-time communication decisions. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 786–793, 2005.\n[33] G. Shani, J. Pineau, and R. Kaplow. A survey of point-based POMDP solvers. Autonomous Agents and Multi-Agent Systems, 27(1):1–51, 2013.\n[34] M. T. J. Spaan. Partially observable Markov decision processes. In Reinforcement Learning: State of the Art, pages 387–414. Springer Verlag, 2012.\n[35] D. Szer, F. Charpillet, and S. Zilberstein. MAA*: A heuristic search algorithm for solving decentralized POMDPs. In Proc. of Uncertainty in Artificial Intelligence, pages 576–583, 2005.\n[36] P. Varakantham, Y. Adulyasak, and P. Jaillet. Decentralized stochastic planning with anonymity in interactions. In Proc. of the AAAI Conference on Artificial Intelligence, pages 2505–2512, 2014.\n[37] P. Varakantham, S. Cheng, G. J. Gordon, and A. Ahmed. Decision support for agent populations in uncertain and congested environments. In Proc. of the AAAI Conference on Artificial Intelligence, 2012.\n[38] P. Varakantham, J. Marecki, Y. Yabu, M. Tambe,\nand M. Yokoo. Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, 2007.\n[39] V. V. Vazirani. Approximation Algorithms. Springer-Verlag, 2001.\n[40] P. Velagapudi, P. Varakantham, P. Scerri, and K. Sycara. Distributed model shaping for scaling to decentralized POMDPs with hundreds of agents. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, 2011.\n[41] S. Witwicki, F. A. Oliehoek, and L. P. Kaelbling. Heuristic search of multiagent influence space. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 973–981, 2012.\n[42] S. J. Witwicki. Abstracting Influences for Efficient Multiagent Coordination Under Uncertainty. PhD thesis, University of Michigan, 2011.\n[43] S. J. Witwicki and E. H. Durfee. Influence-based policy abstraction for weakly-coupled Dec-POMDPs. In Proc. of the International Conference on Automated Planning and Scheduling, pages 185–192, 2010.\n[44] S. J. Witwicki and E. H. Durfee. Towards a unifying characterization for quantifying weak coupling in Dec-POMDPs. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pages 29–36, 2011.\n[45] S. J. Witwicki, F. S. Melo, J. C. Fernández, and M. T. J. Spaan. A flexible approach to modeling unpredictable events in MDPs. In Proc. of the International Conference on Automated Planning and Scheduling, pages 260–268, 2013.\n[46] F. Wu, S. Zilberstein, and X. Chen. Trial-based dynamic programming for multi-agent planning. In Proc. of the AAAI Conference on Artificial Intelligence, pages 908–914, 2010.\n[47] F. Wu, S. Zilberstein, and N. R. Jennings. Monte-Carlo expectation maximization for decentralized POMDPs. In Proc. of the International Joint Conference on Artificial Intelligence, pages 397–403, 2013.\n[48] Z. Yin and M. Tambe. Continuous time planning for multiagent teams with temporal constraints. In Proc. of the International Joint Conference on Artificial Intelligence, 2011.\nAPPENDIX"
    }, {
      "heading" : "A. PROOFS",
      "text" : "Lemma 1. Let πt:h−1c be a (h − t)-steps-to-go policy. Let ν ∈ V and ν\nIO ∈ VIO be the vectors induced by πt:h−1c under regular MPOMDP back-projections (for some I→c), and under IO back-projections. Then\n∀xtc max D t+1 c\nν(〈xtc,D t+1 c 〉) ≤ ν IO(xtc).\nProof. The proof is via induction. The base case is for the last stage t = h− 1, in which case the vectors only consist of the immediate reward:\nmax D\nt+1 c\nν(〈xtc,D t+1 c 〉) = max\nD t+1 c\nRat(x t c) = Rat(x t c) = ν IO(xtc),\nwhere at is the joint action specified by πh−1:h−1c , thus proving the base case. The induction step follows. Induction Hypothesis: Suppose that, given that ν ∈ V and νIO ∈ VIO are vectors for the same policy πt+1:h−1c , for all xt+1c\nmax D t+2 c\nν(〈xt+1c ,D t+2 c 〉) ≤ ν IO(xt+1c ) (14)\nholds. To prove: Given that ν ∈ V and νIO ∈ VIO are vectors for the same policy πt:h−1c , for all x t c\nmax D t+1 c\nν(〈xtc,D t+1 c 〉) ≤ ν IO(xtc). (15)\nProof: We first define the vectors from the l.h.s. Let at denote the first joint action specified by πt:h−1c . Then we can write\nν(〈xtc,D t+1 c 〉) = ra(x t c) + γ\n∑\not+1\nν π|ao(〈xtc,D t+1 c 〉) (16)\nwhere νπ|ao is the back-projection of the vector Γ(πt:h−1c ,a t c,o t+1 c ) that corresponds to π t+1:h−1 c = π t:h−1 c ⇓a,o (the sub-tree of πt:h−1c given a t c,o t+1 c ). That is, by filling out the definition of back-projection, we get\nν(〈xtc,D t+1 c 〉) = ra(x t c) + γ\n∑\not+1\n\n\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c )T̄I→c(x t+1 c |〈x t c,D t+1 c 〉,a t c)[Γ(π t:h−1 c ,a t c,o t+1 c )](〈x t+1 c ,D t+2 c 〉)\n\n ,\nwhere Dt+2c is specified as a function of x t c,D t+1 c ,a t,xt+1c . Clearly, introducing a maximization can not decrease the value, so\nν(〈xtc,D t+1 c 〉) ≤ ra(x t c) + γ\n∑\not+1\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c )T̄I→c(x t+1 c |〈x t c,D t+1 c 〉,a t c) max\nD t+2 c\n[Γ(πt:h−1c ,a t c,o t+1 c )](〈x t+1 c ,D t+2 c 〉)\n{I.H.} ≤ ra(x t c) + γ\n∑\not+1\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c )T̄I→c(x t+1 c |〈x t c,D t+1 c 〉,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c ) (17)\nwhere ΓIO(π t:h−1 c ,a t c,o t+1 c ) is the IO vector that corresponds to π t+1:h−1 c = π t:h−1 c ⇓a,o .\nNow we define the r.h.s. vector:\nν IO(xtc) = ra(x t c) + γ\n∑\not+1\nν IOπ|ao(xtc)\n= ra(x t c) + γ\n∑\not+1\n\nmax u t+1 c\n∑\nx t+1 c\nO(ot+1c |ac,x t+1 c )Pr(xn t+1 c |x t c,ac,u t+1 c )Pr(xl t+1 c |x t c,ac)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\n\n (18)\nWe need to show that\nmax D t+1 c\n\nra(x t c) + γ\n∑\not+1\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c )T̄I→c(x t+1 c |〈x t c,D t+1 c 〉,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\n\n\n≤ ra(x t c) + γ\n∑\not+1\nmax u t+1 c\n∑\nx t+1 c\nO(ot+1c |ac,x t+1 c ) Pr(xn t+1 c |x t c,ac,u t+1 c ) Pr(xl t+1 c |x t c,ac)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\nwhich holds if and only if\nmax D t+1 c\n∑\not+1\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c )T̄I→c(x t+1 c |〈x t c,D t+1 c 〉,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\n≤ ∑\not+1\nmax u t+1 c\n∑\nx t+1 c\nO(ot+1c |ac,x t+1 c )Pr(xn t+1 c |x t c,ac,u t+1 c )Pr(xl t+1 c |x t c,ac)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c ). (19)\nTo show this is the case, we start with the l.h.s.:\nmax D t+1 c\n∑\not+1\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c )T̄I→c(x t+1 c |〈x t c,D t+1 c 〉,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\n=max D t+1 c\n∑\not+1\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c )\n\n\n∑\nu t+1 c\nPr(xnt+1i |x t c,a t c,u t+1 c )I(u t+1 c |D t+1 c )\n\nPr(xlt+1c |x t c,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\n=max D t+1 c\n∑\not+1\n∑\nu t+1 c\nI(ut+1c |D t+1 c )\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c ) Pr(xn t+1 i |x t c,a t c,u t+1 c ) Pr(xl t+1 c |x t c,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\n=max D t+1 c\n∑\not+1\n∑\nu t+1 c\nI(ut+1c |D t+1 c )\n\n\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c ) Pr(xn t+1 i |x t c,a t c,u t+1 c ) Pr(xl t+1 c |x t c,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\n\n\n≤{max. of a function is greater than its expectation:}\nmax D t+1 c\n∑\not+1\nmax u t+1 c\n\n\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c ) Pr(xn t+1 i |x t c,a t c,u t+1 c ) Pr(xl t+1 c |x t c,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\n\n\n={no dependence on Dt+1c anymore:} ∑\not+1\nmax u t+1 c\n∑\nx t+1 c\nŌ(ot+1|at,xt+1c )Pr(xn t+1 i |x t c,a t c,u t+1 c ) Pr(xl t+1 c |x t c,a t c)[ΓIO(π t:h−1 c ,a t c,o t+1 c )](x t+1 c )\nwhich is the r.h.s. of (19), the inequality the we needed to demonstrate, thereby finishing the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Recent years have seen the development of a number<lb>of methods for multiagent planning under uncertainty<lb>that scale to tens or even hundreds of agents. However,<lb>most of these methods either make restrictive assump-<lb>tions on the problem domain, or provide approximate<lb>solutions without any guarantees on quality. To allow<lb>for meaningful benchmarking through measurable qual-<lb>ity guarantees on a very general class of problems, this<lb>paper introduces a family of influence-optimistic upper<lb>bounds for factored Dec-POMDPs. Intuitively, we de-<lb>rive bounds on very large multiagent planning problems<lb>by subdividing them in sub-problems, and at each of<lb>these sub-problems making optimistic assumptions with<lb>respect to the influence that will be exerted by the rest<lb>of the system. We numerically compare the different<lb>upper bounds and demonstrate how, for the first time<lb>ever, we can achieve a non-trivial guarantee that the<lb>heuristic solution of problems with hundreds of agents<lb>is close to optimal. Furthermore, we provide evidence<lb>that the upper bounds may improve the effectiveness of<lb>heuristic influence search, and discuss further potential<lb>applications to multiagent planning.",
    "creator" : "LaTeX with hyperref package"
  }
}