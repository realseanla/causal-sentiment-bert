{
  "name" : "1606.07908.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Label Tree Embeddings for Acoustic Scene Classification",
    "authors" : [ "Huy Phan", "Lars Hertel", "Marco Maass", "Philipp Koch", "Alfred Mertins" ],
    "emails" : [ "phan@isip.uni-luebeck.de", "hertel@isip.uni-luebeck.de", "maass@isip.uni-luebeck.de", "koch@isip.uni-luebeck.de", "mertins@isip.uni-luebeck.de", "permissions@acm.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords acoustic scene classification; label tree embedding; spectral clustering"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Acoustic scene classification (ASC) is an important problem of computational auditory scene analysis [26, 16]. Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11]. A recognized scene can also be used as a prior information to improve the performance of sound event detection [13]. Excluding the background noise, an acoustic scene usually involves various kinds of foreground sounds. Due to its complex sound composition, it is challenging to obtain a good representation for classification. Different features adapted from the related problems, such as speech recognition and audio event classification, have been used to characterize an acoustic scene, for instance MFCC [19, 24] and Gammatone filters [25]. Some hand-crafted features tailored for the task have also been proposed and demonstrated good performance, like Histogram of Oriented Gradients (HOG)\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nMM ’16, October 15 - 19, 2016, Amsterdam, Netherlands © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-3603-1/16/10. . . $15.00\nDOI: http://dx.doi.org/10.1145/2964284.2967268\n[23, 4, 28] and Gabor dictionaries [17]. At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7]. However, most (if not all) previous methods considered the “flat” classification scheme. Thus far, to the best of the authors’ knowledge, no studies have explored the structured nature of the scene categories for classification. In this work, we incorporate a class taxonomy by learning to group similar categories into meta-classes on a tree structure. Going beyond that, we construct explicit embeddings to map each acoustic scene instance into the semantic space that underlies the class hierarchy. It turns out that two similar scene instances are expected to be close to each other in the semantic space. We study the class hierarchy learned from the acoustic scene data themselves as well as the one learned from external speech data [20, 21]. Both of them show good empirical performance even with simple linear classifiers. In addition, combining them with a simple fusion scheme leads to state-of-the-art performance on both target datasets: DCASE 2013 [24] and LITIS Rouen datasets [23]."
    }, {
      "heading" : "2. THE PROPOSED APPROACH",
      "text" : "In this section, we firstly present the framework to learn the label trees and the label tree embeddings for feature mapping. Afterwards, we elaborate different label tree embeddings derived from the framework and the final classification step."
    }, {
      "heading" : "2.1 Learning a Label Tree",
      "text" : "Consider a database (e.g. scene database) with the label set L = {1, . . . , C} where C indicates the number of target categories. In order to explore the structure of class labels, we learn a label tree similar to [3]. The learning algorithm collectively partitions the label set into disjoint subsets in such a way that they are easy to distinguish from one another. Given the set of samples S = {(xn, cn)}|S|n=1 extracted from the training data, where x ∈ RM denotes the vector of some M low-level features, c ∈ L indicates the class label, and | · | represents the set cardinality. The label tree is constructed recursively so that each node is associated with a set of class labels. Consider a node with a label set ℓ (and therefore, the root node is assigned with the label set L), our goal is to split ℓ into two subsets ℓL and ℓR that hold the following requirements: ℓL 6= ∅, ℓR 6= ∅, ℓL∪ℓR = ℓ, and ℓL∩ℓR = ∅. There are totally 2|ℓ|−1−1 such ar X iv :1\n60 6.\n07 90\n8v 2\n[ cs\n.M M\n] 2\n6 Ju\nl 2 01\n6\npossible partitions {ℓL, ℓR}. The optimal partition is then adopted such that a binary classifier designed to separate ℓL and ℓR makes as few errors as possible. In order to find the optimal partitioning, we rely on the multi-class confusion matrix which indicates how good a class is separated from the others. Let Sℓ ⊂ S denote the set of samples corresponding to the label set ℓ. Furthermore, suppose that we have changed and sorted the label set ℓ so that ℓ = {1, . . . , |ℓ|}. In addition, we divide Sℓ into two equal halves: Sℓtrain for training a classifier and Sℓeval for evaluation. We train the multi-class classifier Mℓ using random forest classification [6] with 200 trees using Sℓtrain and then evaluate it on the evaluation set Sℓeval to obtain the confusion matrix A ∈ R|ℓ|×|ℓ|. Each element Aij of the matrix A is computed by:\nAij = 1 |Sℓeval,i| ∑\nx∈Sℓ eval,i\nP (j|x,Mℓ). (1)\nHere, Sℓeval,i ⊂ Sℓeval is the set of samples with the label i. P (j|x,Mℓ) denotes the probability that the classifier Mℓ predicts the sample x as class j. Aij implies how likely a sample of the class i is wrongly predicted to belong to the class j by the classifier. Since A is not symmetric, we symmetrize it as\nĀ = (A+AT)/2. (2)\nEventually, the optimal partitioning {ℓL, ℓR} is selected to maximize:\nE(ℓ) = ∑\ni,j∈ℓL Āij +\n∑\nm,n∈ℓR Āmn. (3)\nBy this, we tend to group the ambiguous classes into the same subset, as a result, produce two meta-classes {ℓL, ℓR} that are easy to separate from each other. We apply spectral clustering [18] on the matrix Ā to solve a relaxed version of the optimization problem in (3). The subsets ℓL and ℓR are then directed to the left and right child nodes, respectively. The splitting process is recursively repeated to grow the whole tree until a leaf node with a single class label is reached. We demonstrate in Figure 1 a subtree extracted from the label tree learned from the LITIS Rouen dataset [23] (more details in Section 2.3).\n2.2 Label Tree Embedding (LTE)\nVia the learned label tree, we have formed (C−1)×2 metaclasses in total from the original label set L. Two of them are associated with the left and right child nodes of one out of (C−1) split nodes. For clarity, suppose that we have indexed the split nodes of the label tree as {ℓi}C−1i=1 . Our objective is then to learn a representation for a test sample by embedding them into the space of the meta-class labels. Formally, we then want to obtain an explicit mapping Ψ : RM → R(C−1)×2 to map the test sample x ∈ RM to a feature vector Ψ(x) = ( ψL1 (x), ψ R 1 (x), . . . , ψ L C−1(x), ψ R C−1(x) ) . The entries of ψLi (x) and ψ R i (x) denote the likelihoods that x belongs to two meta-classes on the left and right child nodes of the split node ℓi. To obtain the likelihoods, at a split node ℓi with the optimal partition {ℓLi , ℓRi }, we train the binary random-forest classifierMℓi with 200 trees using the whole set Sℓi as training data. The samples with their labels in ℓLi are considered as negative examples and others with their labels in ℓRi are considered as positive examples. The likelihoods are then given by:\nψLi (x) = P (negative|x,Mℓi), (4) ψRi (x) = P (positive|x,Mℓi). (5)\nHere, P (negative|x,Mℓi) and P (positive|x,Mℓi) are the classification probabilities outputted by Mℓi when evaluating on x, thanks to the probability support of the random forest classification [6]."
    }, {
      "heading" : "2.3 Scene and Speech LTEs",
      "text" : "Using the above-described framework, we study following LTEs to cope with the acoustic scene classification task: (1) the LTE derived from a target scene database itself (SceneLTE), (2) the LTE learned from an external speech data (Speech-LTE), and (3) their combination (Fusion-LTE). Scene-LTE. Given a target scene database (e.g. LITIS Rouen), the Scene-LTE is formed following the framework. However, we do not consider the whole 30-second snippet of an acoustic scene instance as a sample. Instead, in order to capture meaningful events happening in a scene whose lengths are in order of hundreds of milliseconds, we use segments of length 500 ms with an overlap of 250 ms as the samples for further processing. Each segment is decomposed into 50 ms frames with 50% overlap, each of which is described by M = 128 Gammatone cepstral coefficients [25, 10] in the frequency range of 1-11025 Hz. A segment is then represented by a 128-dimensional feature vector computed by averaging the feature vectors of its constituent frames. Furthermore, each audio segment is labeled by the label of the scene where it is taken from. The learned Scene-LTE is then applied on a test audio segment, resulting in a Scene-LTE feature vector. To compute the global Scene-LTE features for the 30-second scene snippet, we employ average pooling on the Scene-LTE features of its constituent segments. Speech-LTE. Speech signals have been shown to bear potential to serve as a generic representation for nonspeech audio events [20, 21]. We show here that they can also been used to represent acoustic scenes. The Speech-LTE is learned from a set of phone triplets [21] selected from TIMIT speech data [12]. We rely on the closeness measure between a phone triplet category and a scene category for the selection. From the training 500 ms segments obtained\nfrom the target scene database, we train a 200-tree multiclass random-forest classifier Mscene. The closeness κ(c, y) of a scene category c and a phone triplet category y and is then computed as\nκ(c, y) = 1 |Sy| ∑\nxy∈Sy P (c|xy,Mscene). (6)\nHere, Sy = {xyi }|S y|\ni=1 denotes the sample set of the phone triplet category y. We then rank closeness measures and select top N = {5, 10, 15, 20, 25} phone triplet categories for each scene class. The Speech-LTE is finally applied on the audio segments of a scene snippet, followed by the average pooling to produce the global Speech-LTE feature vector for the scene instance. Note that, instead of the Gammatone cepstral coefficients used in the Scene-LTE, we utilized the same low-level feature set in [21] for the Speech-LTE. Fusion-LTE. In order to take advantage of representations from different perspectives, i.e. Scene-LTE and SpeechLTE, we combine them using the extended Gaussian-χ2 kernel [15] given by\nK(xi,xj) = exp ( − ∑\nk\n1 D̄k D ( Ψk(xi),Ψ k(xj) )) , (7)\nwhere D ( Ψk(xi),Ψ k(xj) ) is the χ2 distance between the embedded scene instances Ψk(xi) and Ψ k(xj) with respect to the k-th channel where k ∈ {Scene-LTE, Speech-LTE}. D̄k is the mean χ2 distance of the embedded scene instances in training data for the k-th channel."
    }, {
      "heading" : "2.4 Final Acoustic Scene Classification",
      "text" : "An overview of the final classification schemes is illustrated in Figure 2. For a test acoustic scene instance, we obtain its representations using the Scene-LTE and SpeechLTE as above. To extract representations for the training instances, we conducted 10-fold cross-validation on training data. Lastly, we trained the final scene classification systems using one-vs-one support vector machines (SVM) with different kernels, including linear, χ2, histogram intersection (hist for short), and radial basis function (RBF) kernels.\nFor Fusion-LTE, we used nonlinear SVMs with the extended Gaussian kernel given in (7). The hyperparameters of the SVMs were tuned via 10-fold cross-validation."
    }, {
      "heading" : "3. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We employed the following two datasets in our experiments: DCASE 2013 dataset [2, 24]. This dataset was used in the DCASE 2013 challenge [24]. It consists of ten scene categories recorded in different locations in London at different time points. The dataset has two subsets: public and private subsets, each contains 100 30-second-long scene instances with ten examples for each class. The former was released during the challenge for participants to tune their classification systems. The latter was used to evaluate the submissions and also made public after the challenge. The submitted systems were evaluated with five-fold stratified cross validation on the private subset [24]. We follow the cross validation setting, however, at each time, we combined\nthe public set and the training folds of the private set to make the training data. LITIS Rouen dataset [23]. This dataset contains 3026 30-second-long examples of 19 urban scene categories recorded with the total duration of 1500 minutes. Each class is specific to a location such as a train station, an airplane, or a market. To our knowledge, this is so far the largest publicly available dataset for the task. We follow the standard training/testing splits in [23] (for more details, please refer to [23]) and report average performances over 20 splits of the data."
    }, {
      "heading" : "3.2 Experimental Results",
      "text" : "The classification performance obtained by Scene-LTE, Speech-LTE, and Fusion-LTE are shown in Tables 1, 2, and 3. For the DCASE dataset, the performance is reported in terms of classification accuracy as in the DCASE 2013 challenge [24] whereas we used average class-wise F1-score for the LITIS dataset since it exhibits significant imbalance in the numbers of samples per class. Using Scene-LTE, our systems achieve an accuracy of 86% and a F1-score of 94.9% on the DCASE and LITIS datasets, respectively. These results surpass the best reported performance on the DCASE dataset (85% in terms of accuracy [1]) while being just marginally below the best performance on the LITIS dataset (95.6% in terms of F1-score [5]). Regarding Speech-LTE, we obtain the best accuracy, 87%, with N = 25 and linear kernel which is even better than that of Scene-LTE on the DCASE 2013 dataset. For the LITIS dataset, although Speech-LTE maintains good F1-score near 90% in most of the cases, these results are not as good as with those of Scene-LTE. It is also noticeable that adding more selected phone triplet categories per scene class seems to bring up the performance, however, the gains are insignificant in most of the cases. Lastly, for both DCASE and LITIS datasets, the performance of the linear systems are comparable with the nonlinear ones with χ2, hist., and RBF kernels. This is good since the linear systems are computationally much cheaper to train and evaluate compared to the nonlinear ones. For the fusion of Scene-LTE and Speech-LTE into FusionLTE with the extended Gaussian-χ2 kernel, we obtain average accuracy gains of 0.8% and 2.2% compared to individual Scene-LTE and Speech-LTE with the χ2 kernel on the DCASE dataset. Similarly, the average F1-score gains on the LITIS dataset are 1.3% and 7.9%. These results indicate that speech signals are not only able to represent well the scene audio signals but also provide a valuable external source to enhance the performance of a classification system built on the scene data itself. Finally, we present a comprehensive performance comparison of our systems and other reported results on the DCASE and LITIS datasets in Tables 4 and 5, respectively. We mark in bold where the performance of our systems outperforms all the opponents. Since the results on the LITIS dataset were reported with different metrics, i.e. average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison. We also would like to notice that although there exists other works on the DCASE dataset after the challenge, we only mention here those with performance equivalent or higher than that of the best submission in the challenge. For the details of\nthe competitive systems, please refer to the respective references. As can be seen for the DCASE dataset, our systems consistently outperform the best submission to the DCASE 2013 challenge (i.e. RNH [24]) with a large margin of about 10% and also outrun the best reported performance in [1] from 1% to 2%. For the LITIS dataset, our systems with Scene-LTE alone show better performance than most of the compared systems. Moreover, our Fusion-LTE systems set state-of-the-art performance on all evaluation metrics and outperform the best reported results by 3.7%, 0.6%, and 0.4% in terms of precision, F1-score, and accuracy, respectively."
    }, {
      "heading" : "4. CONCLUSIONS",
      "text" : "In this paper, we present efficient schemes for acoustic scene classification. We explore the structure of the class labels by automatically learning class-label hierarchies and then the label-tree embeddings to map scene instances into the semantic space underlying the class hierarchy. We study both the label tree embedding intrinsically learned from the scene data as well as the one learned from the external TIMIT speech data. Both of them demonstrate good empirical performance on the experimental datasets, including the DCASE 2013 and LITIS Rouen datasets. Furthermore, fusing them with a simple scheme leads to state-of-the-art performance."
    }, {
      "heading" : "5. ACKNOWLEDGMENTS",
      "text" : "This work was supported by the Graduate School for Computing in Medicine and Life Sciences funded by Germany’s Excellence Initiative [DFG GSC 235/1]."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] S. Ağcaer, A. Schlesinger, F.-M. Hoffmann, and\nR. Martin. Optimization of amplitude modulation features for low-resource acoustic scene classification. In Proc. European Signal Processing Conference (EUSIPCO), pages 2556–2560, 2015.\n[2] D. Barchiesi, D. Giannoulis, D. Stowell, and M. Plumbley. Acoustic scene classification: Classifying environments from the sounds they produce. IEEE Signal Processing Magazine, 32(3):16–34, 2015.\n[3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 163–171, 2010.\n[4] V. Bisot, S. Essid, and G. Richard. HOG and subband power distribution image features for acoustic scene classification. In Proc. European Signal Processing Conference (EUSIPCO), pages 719–723, 2015.\n[5] V. Bisot, R. Serizel, S. Essid, and G. Richard. Acoustic scene classification with matrix factorization for unsupervised feature learning. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6445–6449, 2016.\n[6] L. Breiman. Random forest. Machine Learning, 45:5–32, 2001.\n[7] R. Cai, L. Lu, and A. Hanjalic. Co-clustering for auditory scene categorization. IEEE Trans. Multimedia, 10(4):596–606, 2008.\n[8] S. Chu, S. Narayanan, C.-C. J. Kuo, and M. J. Mataric. Where am I? Scene recognition for mobile robots using audio features. In Proc. IEEE International Conference on Multimedia and Expo (ICME), pages 885–888, 2006.\n[9] S. Deng, J. Han, C. Zhang, T. Zheng, and G. Zheng. Robust minimum statistics project coefficients feature for acoustic environment recognition. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8232–8236, 2014.\n[10] D. P. W. Ellis. Gammatone-like spectrograms, 2009.\n[11] A. J. Eronen, V. T. Peltonen, J. T. Tuomi, A. P. Klapuri, S. Fagerlund, T. Sorsa, G. Lorho, and J. Huopaniemi. Audio-based context recognition. IEEE Trans. Audio, Speech, and Language Processing, 14(1):321–329, 2006.\n[12] W. Fisher, G. Doddington, and K. Goudie-Marshall. The DARPA speech recognition research database: Specifications and status. In Proc. DARPA Workshop on Speech Recognition, pages 93–99, 1986.\n[13] T. Heittola, A. Mesaros, A. Eronen, and T. Virtanen. Context-dependent sound event detection. EURASIP Journal on Audio, Speech, and Music Processing, 2013.\n[14] T. Heittola, A. Mesaros, A. J. Eronen, and T. Virtanen. Audio context recognition using audio event histogram. In Proc. European Signal Processing Conference (EUSIPCO), pages 1272–1276, 2010.\n[15] I. Laptev, M. Marsza lek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In Proc CVPR, pages 1–8, 2008.\n[16] R. F. Lyon. Machine hearing: An emerging field. IEEE Signal Processing Magazine, 27(5):131–139, 2010.\n[17] R. Mogi and H. Kasaii. Noise-robust environmental sound classification method based on combination of\nICA and MP features. Artificial Intelligence Research, 2(1):107–121, 2013.\n[18] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In Proc. NIPS, pages 849–856, 2001.\n[19] Y. Petetin, C. Laroche, and A. Mayoue. Deep neural networks for audio scene recognition. In Proc. European Signal Processing Conference (EUSIPCO), pages 125–129, 2015.\n[20] H. Phan, L. Hertel, M. Maass, R. Mazur, and A. Mertins. Representing nonspeech audio signals through speech classification models. In Proc. Interspeech, pages 3441–3445, 2015.\n[21] H. Phan, L. Hertel, M. Maass, R. Mazur, and A. Mertins. Learning representations for nonspeech audio events through their similarities to speech patterns. IEEE/ACM Trans. Audio, Speech, and Language Processing, 24(4):807–822, April 2016.\n[22] R. Radhakrishnan, A. Divakaran, and P. Smaragdis. Audio analysis for surveillance applications. In Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pages 158–161, 2005.\n[23] A. Rakotomamonjy and G. Gasso. Histogram of gradients of time-frequency representations for audio scene classification. IEEE/ACM Trans. Audio, Speech, and Language Processing, 23(1):142–153, 2015.\n[24] D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange, and M. D. Plumbley. Detection and classification of acoustic scenes and events. IEEE Trans. Multimedia, 17(10):1733–1746, 2015.\n[25] X. Valero and F. Aĺıas. Gammatone cepstral coefficients: biologically inspired features fro non-speech audio classification. IEEE Trans. Multimedia, 17(6):1684–1689, 2012.\n[26] D. Wang and G. J. Brown. Computational Auditory Scene Analysis: Principles, Algorithms, and Applications. Wiley-IEEE Press, 2006.\n[27] Y. Xu, W. J. Li, and K. K. Lee. Intelligent Wearable Interfaces. Hoboken, NJ: Wiley, 2008.\n[28] J. Ye, T. Kobayashi, M. Murakawa, and T. Higuchi. Acoustic scene classification based on sound textures and events. In Proc. ACM Multimedia, pages 1291–1294, 2015."
    } ],
    "references" : [ {
      "title" : "Optimization of amplitude modulation features for low-resource acoustic scene classification",
      "author" : [ "S. Ağcaer", "A. Schlesinger", "F.-M. Hoffmann", "R. Martin" ],
      "venue" : "Proc. European Signal Processing Conference (EUSIPCO), pages 2556–2560",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Acoustic scene classification: Classifying environments from the sounds they produce",
      "author" : [ "D. Barchiesi", "D. Giannoulis", "D. Stowell", "M. Plumbley" ],
      "venue" : "IEEE Signal Processing Magazine, 32(3):16–34",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Label embedding trees for large multi-class tasks",
      "author" : [ "S. Bengio", "J. Weston", "D. Grangier" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS), pages 163–171",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "HOG and subband power distribution image features for acoustic scene classification",
      "author" : [ "V. Bisot", "S. Essid", "G. Richard" ],
      "venue" : "Proc. European Signal Processing Conference (EUSIPCO), pages 719–723",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Acoustic scene classification with matrix factorization for unsupervised feature learning",
      "author" : [ "V. Bisot", "R. Serizel", "S. Essid", "G. Richard" ],
      "venue" : "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6445–6449",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Random forest",
      "author" : [ "L. Breiman" ],
      "venue" : "Machine Learning, 45:5–32",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Co-clustering for auditory scene categorization",
      "author" : [ "R. Cai", "L. Lu", "A. Hanjalic" ],
      "venue" : "IEEE Trans. Multimedia, 10(4):596–606",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Where am I? Scene recognition for mobile robots using audio features",
      "author" : [ "S. Chu", "S. Narayanan", "C.-C.J. Kuo", "M.J. Mataric" ],
      "venue" : "Proc. IEEE International Conference on Multimedia and Expo (ICME), pages 885–888",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust minimum statistics project coefficients feature for acoustic environment recognition",
      "author" : [ "S. Deng", "J. Han", "C. Zhang", "T. Zheng", "G. Zheng" ],
      "venue" : "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8232–8236",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Audio-based context recognition",
      "author" : [ "A.J. Eronen", "V.T. Peltonen", "J.T. Tuomi", "A.P. Klapuri", "S. Fagerlund", "T. Sorsa", "G. Lorho", "J. Huopaniemi" ],
      "venue" : "IEEE Trans. Audio, Speech, and Language Processing, 14(1):321–329",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The DARPA speech recognition research database: Specifications and status",
      "author" : [ "W. Fisher", "G. Doddington", "K. Goudie-Marshall" ],
      "venue" : "Proc. DARPA Workshop on Speech Recognition, pages 93–99",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Context-dependent sound event detection",
      "author" : [ "T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen" ],
      "venue" : "EURASIP Journal on Audio, Speech, and Music Processing",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Audio context recognition using audio event histogram",
      "author" : [ "T. Heittola", "A. Mesaros", "A.J. Eronen", "T. Virtanen" ],
      "venue" : "Proc. European Signal Processing Conference (EUSIPCO), pages 1272–1276",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "M",
      "author" : [ "I. Laptev" ],
      "venue" : "Marsza lek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In Proc CVPR, pages 1–8",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Machine hearing: An emerging field",
      "author" : [ "R.F. Lyon" ],
      "venue" : "IEEE Signal Processing Magazine, 27(5):131–139",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Noise-robust environmental sound classification method based on combination of  ICA and MP features",
      "author" : [ "R. Mogi", "H. Kasaii" ],
      "venue" : "Artificial Intelligence Research, 2(1):107–121",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "Proc. NIPS, pages 849–856",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Deep neural networks for audio scene recognition",
      "author" : [ "Y. Petetin", "C. Laroche", "A. Mayoue" ],
      "venue" : "Proc. European Signal Processing Conference (EUSIPCO), pages 125–129",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Representing nonspeech audio signals through speech classification models",
      "author" : [ "H. Phan", "L. Hertel", "M. Maass", "R. Mazur", "A. Mertins" ],
      "venue" : "Proc. Interspeech, pages 3441–3445",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning representations for nonspeech audio events through their similarities to speech patterns",
      "author" : [ "H. Phan", "L. Hertel", "M. Maass", "R. Mazur", "A. Mertins" ],
      "venue" : "IEEE/ACM Trans. Audio, Speech, and Language Processing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Audio analysis for surveillance applications",
      "author" : [ "R. Radhakrishnan", "A. Divakaran", "P. Smaragdis" ],
      "venue" : "Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pages 158–161",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Histogram of gradients of time-frequency representations for audio scene classification",
      "author" : [ "A. Rakotomamonjy", "G. Gasso" ],
      "venue" : "IEEE/ACM Trans. Audio, Speech, and Language Processing, 23(1):142–153",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Detection and classification of acoustic scenes and events",
      "author" : [ "D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M.D. Plumbley" ],
      "venue" : "IEEE Trans. Multimedia, 17(10):1733–1746",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Gammatone cepstral coefficients: biologically inspired features fro non-speech audio classification",
      "author" : [ "X. Valero", "F. Aĺıas" ],
      "venue" : "IEEE Trans. Multimedia, 17(6):1684–1689",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Computational Auditory Scene Analysis: Principles",
      "author" : [ "D. Wang", "G.J. Brown" ],
      "venue" : "Algorithms, and Applications. Wiley-IEEE Press",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Intelligent Wearable Interfaces",
      "author" : [ "Y. Xu", "W.J. Li", "K.K. Lee" ],
      "venue" : "Hoboken, NJ: Wiley",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Acoustic scene classification based on sound textures and events",
      "author" : [ "J. Ye", "T. Kobayashi", "M. Murakawa", "T. Higuchi" ],
      "venue" : "Proc. ACM Multimedia, pages 1291–1294",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Acoustic scene classification (ASC) is an important problem of computational auditory scene analysis [26, 16].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : "Acoustic scene classification (ASC) is an important problem of computational auditory scene analysis [26, 16].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 25,
      "context" : "Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11].",
      "startOffset" : 234,
      "endOffset" : 242
    }, {
      "referenceID" : 9,
      "context" : "Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11].",
      "startOffset" : 234,
      "endOffset" : 242
    }, {
      "referenceID" : 11,
      "context" : "A recognized scene can also be used as a prior information to improve the performance of sound event detection [13].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "Different features adapted from the related problems, such as speech recognition and audio event classification, have been used to characterize an acoustic scene, for instance MFCC [19, 24] and Gammatone filters [25].",
      "startOffset" : 181,
      "endOffset" : 189
    }, {
      "referenceID" : 22,
      "context" : "Different features adapted from the related problems, such as speech recognition and audio event classification, have been used to characterize an acoustic scene, for instance MFCC [19, 24] and Gammatone filters [25].",
      "startOffset" : 181,
      "endOffset" : 189
    }, {
      "referenceID" : 23,
      "context" : "Different features adapted from the related problems, such as speech recognition and audio event classification, have been used to characterize an acoustic scene, for instance MFCC [19, 24] and Gammatone filters [25].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 21,
      "context" : "2967268 [23, 4, 28] and Gabor dictionaries [17].",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "2967268 [23, 4, 28] and Gabor dictionaries [17].",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "2967268 [23, 4, 28] and Gabor dictionaries [17].",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "2967268 [23, 4, 28] and Gabor dictionaries [17].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].",
      "startOffset" : 52,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].",
      "startOffset" : 52,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].",
      "startOffset" : 52,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 26,
      "context" : "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].",
      "startOffset" : 161,
      "endOffset" : 171
    }, {
      "referenceID" : 12,
      "context" : "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].",
      "startOffset" : 161,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].",
      "startOffset" : 161,
      "endOffset" : 171
    }, {
      "referenceID" : 18,
      "context" : "We study the class hierarchy learned from the acoustic scene data themselves as well as the one learned from external speech data [20, 21].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "We study the class hierarchy learned from the acoustic scene data themselves as well as the one learned from external speech data [20, 21].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 22,
      "context" : "In addition, combining them with a simple fusion scheme leads to state-of-the-art performance on both target datasets: DCASE 2013 [24] and LITIS Rouen datasets [23].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "In addition, combining them with a simple fusion scheme leads to state-of-the-art performance on both target datasets: DCASE 2013 [24] and LITIS Rouen datasets [23].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : "In order to explore the structure of class labels, we learn a label tree similar to [3].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "Figure 1: A subtree extracted from the label tree learned from the LITIS Rouen dataset [23].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "We train the multi-class classifier M using random forest classification [6] with 200 trees using S train and then evaluate it on the evaluation set S eval to obtain the confusion matrix A ∈ R|l|×|l|.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "We apply spectral clustering [18] on the matrix Ā to solve a relaxed version of the optimization problem in (3).",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : "We demonstrate in Figure 1 a subtree extracted from the label tree learned from the LITIS Rouen dataset [23] (more details in Section 2.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "Here, P (negative|x,Mi) and P (positive|x,Mi) are the classification probabilities outputted by Mi when evaluating on x, thanks to the probability support of the random forest classification [6].",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 23,
      "context" : "Each segment is decomposed into 50 ms frames with 50% overlap, each of which is described by M = 128 Gammatone cepstral coefficients [25, 10] in the frequency range of 1-11025 Hz.",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "Speech signals have been shown to bear potential to serve as a generic representation for nonspeech audio events [20, 21].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "Speech signals have been shown to bear potential to serve as a generic representation for nonspeech audio events [20, 21].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "The Speech-LTE is learned from a set of phone triplets [21] selected from TIMIT speech data [12].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "The Speech-LTE is learned from a set of phone triplets [21] selected from TIMIT speech data [12].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "Note that, instead of the Gammatone cepstral coefficients used in the Scene-LTE, we utilized the same low-level feature set in [21] for the Speech-LTE.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Scene-LTE and SpeechLTE, we combine them using the extended Gaussian-χ kernel [15] given by",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "We employed the following two datasets in our experiments: DCASE 2013 dataset [2, 24].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "We employed the following two datasets in our experiments: DCASE 2013 dataset [2, 24].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "This dataset was used in the DCASE 2013 challenge [24].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "The submitted systems were evaluated with five-fold stratified cross validation on the private subset [24].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : "LITIS Rouen dataset [23].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 21,
      "context" : "We follow the standard training/testing splits in [23] (for more details, please refer to [23]) and report average performances over 20 splits of the data.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : "We follow the standard training/testing splits in [23] (for more details, please refer to [23]) and report average performances over 20 splits of the data.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 22,
      "context" : "For the DCASE dataset, the performance is reported in terms of classification accuracy as in the DCASE 2013 challenge [24] whereas we used average class-wise F1-score for the LITIS dataset since it exhibits significant imbalance in the numbers of samples per class.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "These results surpass the best reported performance on the DCASE dataset (85% in terms of accuracy [1]) while being just marginally below the best performance on the LITIS dataset (95.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "6% in terms of F1-score [5]).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 21,
      "context" : "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 26,
      "context" : "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "0 RNH [24] 76.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 22,
      "context" : "0 MV [24] 77.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "Human [2] 75.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "HOG [23] 76.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "0 AMS+LDA [1] 85.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 21,
      "context" : "4 HOG [23] 91.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "7 − − HOG+SPD [4] 93.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 17,
      "context" : "4 DNN+MFCC [19] 92.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "2 − − Sparse NMF [5] − 94.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "1 − Convolutive NMF [5] − 94.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "5 − Kernel PCA [5] − 95.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "6 − HOG+ProbSVM [28] − − 96.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "RNH [24]) with a large margin of about 10% and also outrun the best reported performance in [1] from 1% to 2%.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "RNH [24]) with a large margin of about 10% and also outrun the best reported performance in [1] from 1% to 2%.",
      "startOffset" : 92,
      "endOffset" : 95
    } ],
    "year" : 2016,
    "abstractText" : "We present in this paper an efficient approach for acoustic scene classification by exploring the structure of class labels. Given a set of class labels, a category taxonomy is automatically learned by collectively optimizing a clustering of the labels into multiple meta-classes in a tree structure. An acoustic scene instance is then embedded into a low-dimensional feature representation which consists of the likelihoods that it belongs to the meta-classes. We demonstrate state-of-the-art results on two different datasets for the acoustic scene classification task, including the DCASE 2013 and LITIS Rouen datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}