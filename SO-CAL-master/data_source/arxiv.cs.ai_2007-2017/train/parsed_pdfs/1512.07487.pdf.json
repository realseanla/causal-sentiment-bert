{
  "name" : "1512.07487.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Selecting the top-quality item through crowd scoring",
    "authors" : [ "Alessandro Nordio", "Alberto Tarable", "Emilio Leonardi", "Marco Ajmone Marsan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n07 48\n7v 1\n[ cs\n.H C\n] 2\n3 D\nec 2\n01 5\nI. INTRODUCTION\nCrowdsourcing is a term often adopted to identify distributed systems that can be used for the solution of a wide range of complex problems by integrating a large number of human and/or computer efforts [1].\nThe key elements of a crowdsourcing system are: i) the availability of a large pool of individuals or machines (called workers in crowdsourcing jargon) that can offer their (small) contribution to the problem solution by executing a task; ii) an algorithm for the partition of the problem at hand into tasks; iii) an algorithm for the selection of workers and the distribution of tasks to the selected workers; iv) an algorithm for the combination of workers’ answers into the final solution of the problem, v) a requester (a.k.a. employer), who uses the three algorithms above to structure his problem into a set of tasks, assign tasks to selected workers, and combine workers’ answers to obtain the problem solution.\nSince on the one hand answers may be subjective, and on the other task execution is typically tedious and the economic reward for workers is pretty small (if any), workers are not 100% reliable, in the sense that they may provide incorrect answers. In addition, workers may be biased for different reasons. Hence, the same task is normally assigned in parallel (replicated) to several workers, and then a decision rule is applied to their answers. A natural trade-off between the accuracy of the decision and cost arises; indeed, by increasing the replication factor of every task, we can increase the accuracy of the final decision about the task solution, but we necessarily incur higher costs (or, for a given fixed cost, we obtain a lower task throughput).\nA number of sophisticated software platforms have been recently developed for the exploitation of the crowdsourcing paradigm on a scale that could not be possible without a\nnetworked infrastructure. Some relevant application scenarios taken from the domains of recommendation and evaluation, are the development of hotel and restaurant rating systems, the implementation of recommendation systems for movies, the management of the review process of large conferences.\nGiven the scale of the current applications of crowdsourcing systems, the relevance of high-performance and scalable algorithms is enormous (in some cases, it can have huge economical impact).\nAn abstract view of the examples above reveals that they are very similar in nature: their essence is determining the best, or the k best, elements in a group of objects in which each object has an intrinsic (unknown) quality metric. This is a very fundamental algorithmic problem, which has already been investigated by several researchers in the context of crowdsourcing.\nMost of the previous literature considers workers only able to directly compare items in groups comprising two or more objects, expressing a preference. The proposed algorithms consist of comparisons arranged in rounds, forming a tournament, and the investigation concentrates on the tradeoffs that appear in this context (e.g., cost, accuracy, latency) [2]–[7]. In this paper, instead, we assume that workers are able to evaluate (in absolute terms) the quality of an object, providing a noisy score. A similar path was followed in the recent (still unpublished) work by Khan and Garcia Molina [8], which studies algorithms to find the maximum element in a group of objects, and discusses approaches based on comparisons, on ratings, as well as on a mix of the two possibilities. The main difference between their work and this one is in the quantization of workers’ scores. Indeed, [8] assumes that workers’ answers are coarsely quantized over few levels (typically three or five), and this makes objects with similar quality indistinguishable, so that direct comparisons and tournaments become necessary to break ties.\nOn the contrary, we first work with unquantized workers’ answers, so as to maximize the amount of information provided by the workers. We show that in this context the scoring approach is superior (in some cases by far) to the approach based on direct comparisons. This should not be surprising, since quantization and comparisons entail a partial loss of information. Then, we show that by adopting smart quantization techniques with a sufficiently large number of quantization levels (in the order of few tens) we can closely approach the performance of systems operating on unquantized scores.\nAnother significant difference with respect to [8] is in the scope of the works. We aim at the definition of smart\nmultiround adaptive algorithms that effectively distribute the resources (workers) among the objects, at every round, making online decisions whether to distribute further resources, based on past collected answers. The paper [8], instead, focuses on non-adaptive algorithms distributing resources to objects according to a fixed, pre-established scheme.\nOur main findings are:\n• algorithms operating on unquantized scores are intrinsically more efficient than algorithms based on direct comparisons of objects; • resources (workers) must be allocated in a careful manner, concentrating more resources on the top-quality objects; this can be done only if algorithms are adaptive and, at every round, exploit currently available information about objects’ quality to decide how to distribute further resources; • if unquantized scores are available, tournament-based approaches, that partition objects into subgroups and move winners in each sub-group to the next round, may become extremely inefficient; • more practical quantized schemes perform very close to their ideal unquantized counterparts, provided that a reasonable number of quantization levels is properly assigned to workers’ answers.\nThe rest of this paper is organized as follows. Section II presents our system assumptions. Section III describes the main characteristics of the class of algorithms that is investigated in this paper, and Section IV discusses the parameters for the algorithm design. Section V shows that, with our system assumptions, the approach based on quantitative evaluations of object qualities is superior to the one based on comparisons between objects. Section VI faces the problem of effective answer quantization. Section VII presents and discusses numerical results for the class of algorithms considered in this paper, comparing them to previous proposals. Section VIII briefly comments on the computational complexity of the most promising algorithm according to the numerical results of Section VII, and discusses parameter setting. Finally, Section IX contains our concluding remarks and presents the next steps of our work."
    }, {
      "heading" : "II. SYSTEM ASSUMPTIONS",
      "text" : "We consider a set of N objects, each of which is endowed with an intrinsic quality, whose evaluation requires human capabilities. Let x = [x1, . . . , xN ] be the vector of all quality values, which are supposed to be instances of the i.i.d. random variables q = [q1, . . . , qN ] having common pdf fq over R.\nOur goal is to use a crowdsourcing approach to identify the “best” object, that is, the object with the largest quality value, denoted by xi∗ , where\ni∗ = argmax i xi .\nThe crowd is composed of statistically identical workers, whose evaluation of the object quality is prone to errors. We assume, for the moment, that workers provide absolute\nunquantized estimates (scores) of the intrinsic quality of individual objects, and we model as additive Gaussian noise the error made in such evaluation process. More precisely, if the i-th object is sent for evaluation, say, for the j-th time, to a worker in the crowd, the worker’s answer will be given by:\naij = xi + nij\nwhere nij , i = 1, . . . , N , j = 1, 2, . . . , are i.i.d. Gaussian random variables with zero mean and variance σ2. Errors may be caused either by subjective factors influencing the assessment of the intrinsic object quality, or by the fact that workers avoid devoting the effort that is needed for an accurate assessment. Observe that our model encompasses the case in which different workers may suffer from possibly different biases. However we make the conservative assumption that workers’ bias cannot be estimated. This assumption is justified by the fact that, since evaluations of objects are usually carried out by many workers, each one performing few evaluations, estimating the worker bias is very challenging in practical crowdsourcing systems.\nObserve that our model differs from [2]–[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]–[7] assume workers to be only able to perform noisy comparisons between groups of objects. We will show that, whenever applicable, the scoring approach is significantly more favorable, in the sense that algorithms dealing with absolute estimates of object quality achieve significantly better cost/performance trade-offs. We also wish to remark that the Gaussian model of worker error is in good agreement with Thurstone’s law of comparative judgment [9], according to which comparisons are based on latent quality estimations, whose distribution is Gaussian around the true quality value. In the context of crowdsourcing, the same model has recently been employed also in [8]."
    }, {
      "heading" : "III. ALGORITHMIC APPROACH",
      "text" : "We investigate a class of adaptive algorithms in which objects are sent out for evaluation through several rounds. In each round, each object receives a given number of evaluations by crowd workers (possibly zero, for some objects). Then, on the basis of all collected workers’ answers, the algorithms take decisions about the opportunity of requesting extra evaluations for a subset of the objects in a further round. If no extra evaluations are carried out, the algorithms terminate and a winner is identified.\nMore formally, denote with m(ℓ)i the number of evaluations the i-th object has received in round ℓ and with M (ℓ)i the total number of evaluations received by object i up to (and including) round ℓ. We define a(ℓ)i = [ a (ℓ) ij ] , j = 1, . . . ,M (ℓ)i as the vector1 of random variables representing the answers about object i collected up to round ℓ, and A(ℓ) = [ a (ℓ) 1 , . . . , a (ℓ) N ] .\nAt the beginning of round ℓ, with ℓ ≥ 1, a fitness index φ (ℓ−1) i quantifies the current chances for object i to be the\n1In order to avoid cumbersome notation we sometimes indicate the vector v = [v1, . . . , vn], as v = [vi], i = 1, . . . , n\nwinner at the end of the algorithm, as a result of processing of previous evaluations. Let the vector of all fitness indices be\nφ(ℓ−1) = [ φ (ℓ−1) 1 , . . . , φ (ℓ−1) N ] .\nFor ℓ = 1, i.e., when no evaluations are available yet, fitness indices are equal for all objects, since the object qualities are assumed to be instances of i.i.d random variables.\nIn round ℓ, some of the objects may have a fitness index equal to −∞. These objects are not assigned any further evaluation and are out of the contest. Define the contestant set C(ℓ) at round ℓ as the set of objects for which the fitness index is currently larger than −∞, i.e.,\nC(ℓ) = { i ∈ {1, . . . , N} : φ(ℓ−1)i > −∞ } .\nWe remark that, according to our algorithms, the contestant set at round ℓ + 1 is always a (possibly improper) subset of the contestant set at round ℓ, i.e., C(ℓ+1) ⊆ C(ℓ).\nOn the basis of φ(ℓ−1) and, possibly, of the total number of past assignments M (ℓ−1) = ∑N i=1 M (ℓ−1) i , the algorithm decides, according to a termination rule, whether to stop or to go on with the rounds. If rounds are stopped, the object with the largest fitness index is declared the winner. Otherwise, a budget of new worker evaluations is assigned to objects. Such budget is dimensioned as\n[ m\n(ℓ) 1 , . . . ,m (ℓ) N\n] = A(φ(ℓ−1),M (ℓ−1))\nwhere A(·) is the RN × N → NN allocation function. We assume function A(·) to be increasing with respect to the object quality, i.e., our algorithm tends to allocate more workers to objects with top estimated quality, as formally stated below.\nProperty 1: If φ(ℓ−1)i < φ (ℓ−1) j , then m (ℓ) i ≤ m (ℓ) j , with\nm (ℓ) i = 0 if φ (ℓ) i = −∞.\nAfter determining the number of suitable evaluations, the objects are sent to crowd workers and answers are collected. Then, such answers, together with the previous ones, are used to update the fitness vector for next round, φ(ℓ) (and, consequently, C(ℓ+1)).\nSeveral algorithms can be devised in accordance to the previous scheme, depending on how we select the different metrics and parameters, such as A(·), the fitness index, and the termination rule."
    }, {
      "heading" : "IV. DESIGN PARAMETERS",
      "text" : ""
    }, {
      "heading" : "A. Preliminary considerations",
      "text" : "At round ℓ of the algorithm, on the basis of the collected answers we can compute an a-posteriori distribution f (ℓ) i ( x|y(ℓ)i ) for the quality qi, where y (ℓ) i represents a realization of the random vector a(ℓ)i (assumed unquantized for the moment). Thanks to Bayes’ rule, such distribution can be written as:\nf (ℓ) i ( x|y(ℓ)i ) = κ M (ℓ) i∏\nj=1\nexp ( − (yij−x) 2\n2σ2\n) fq(x) (1)\nwhere κ is such that ∫ +∞ −∞ f (ℓ) i ( x|y(ℓ)i ) dx = 1. When the a-priori pdf fq is Gaussian with mean µa and variance σ2a , by substituting such prior into the above expression, we easily obtain that f (ℓ)i is also Gaussian, namely\nf (ℓ) i ( x|y(ℓ)i ) = ( 2πρ 2(ℓ) i )− 12 e − (\nx−x̂ (ℓ) i )\n2\n2ρ 2(ℓ) i , (2)\nwhere\nx̂ (ℓ) i =\n∑M(ℓ)i j=1 yij + βµa\nM (ℓ) i + β\n, ρ 2(ℓ) i =\nσ2\nM (ℓ) i + β\n,\nand β = σ2/σ2a . This is a consequence of the fact that the Gaussian distribution is self-conjugate with respect to the Gaussian likelihood function. It is also worth noting that x̂(ℓ)i is the Minimum-Mean-Square-Error estimate of xi given that y (ℓ) i is the realization of a (ℓ) i . Observe that (2) also holds for unknown prior distribution. In such a case we set σa = ∞, i.e., β = 0. Moreover we remark that (2) can also be employed to obtain a simplified approximate a-posteriori distribution in the case of non-Gaussian priors with variance σ2a ."
    }, {
      "heading" : "B. Possible performance parameters",
      "text" : "In order to properly choose the metrics of the crowdsourcing algorithm, it is important to identify the performance parameters we may want to optimize. Several options can be devised. In the following, we will omit the round index ℓ for ease of notation. Consider an answer realization Y = [y1, . . . ,yN ] and define the corresponding estimate î∗(Y) of i∗, in the following simply denoted by î∗.\n• A first possible performance parameter (to be minimized) is the distortion D(Y) = E ( qî∗ − qi∗ )2 , which is aver-\naged with respect to the current a-posteriori distribution of q given A = Y. Unfortunately the computation of the distortion is in general too complex, even for moderate values of N . • A generalization of the previous metric is the order-k distortion D(k)(Y) = E ∣∣qî∗ − qi∗ ∣∣k, where k ∈ N. • A different performance parameter to be minimized can be the error probability pe(Y) = P { î∗ 6= i∗ } . With such\na choice, a maximum-a-posteriori (MAP) rule turns out to be optimal. Precisely, let πi(Y) = P{i∗ = i|A = Y} be the probability of i to be the top-quality object, given the answers Y, for i ∈ C. We can compute the value of πi(Y) as follows.\nπi(Y)=P    ⋂\nj∈C\\i {qj < qi}|A = Y\n  \n=\n∫\nR\nP    ⋂\nj∈C\\i {qj<qi}|A=Y, qi=x\n   fi(x|yi) dx\n=\n∫\nR\nfi(x|yi) ∏\nj∈C\\i Fj(x|yj) dx (3)\nwhere Fi is the a-posteriori cdf of qi given ai. It is easy to see that pe(Y) is minimized when î∗ = argmaxi πi(Y). In the case of Gaussian or unknown priors, (3) holds with\nFi(x|yi) = 1\n2\n[ 1 + erf ( x− x̂i(yi)√\n2ρi\n)]\nSince the evaluation of πi(Y), for i = 1, . . . , N , entails a computational complexity growing linearly with N , we propose the following approximation:\nπ̃i(Y) =\n∫ ∞\n−∞ Fc(i)(x|yc(i))fi(x|yi) dx\n= 1\n2\n 1 + erf  \nx̂i − x̂c(i)√ 2 ( ρ2i + ρ 2 c(i) )\n    (4)\nwhere c(i) = argmaxj 6=i x̂j corresponds to the object with maximum current estimated quality except i. In practice, (4) restricts the comparison to only two objects, the running candidate i and its strongest competitor c(i), and uses the current probability π̃i(Y) that object i is better than c(i) as an approximation for πi(Y).\nIn this work, because of complexity considerations, we choose the error probability as the performance parameter."
    }, {
      "heading" : "C. Fitness indices",
      "text" : "As in the case of performance parameters, different choices for fitness indices are possible.\n• Exact max probability: With this choice, we identify the fitness index of objects with their estimated probability of being the top-quality object:\nφi = πi(Y)\n• Approximate max probability: In this case:\nφi = π̃i(Y)\n• Exact max probability with elimination: As stated in the previous section, the contestant set, C, initially set to {1, . . . , N}, may be shortened along rounds. We have considered a strategy where, at each round, those objects whose πi is lower than a threshold πth,E are eliminated. For this strategy, the fitness index is given by:\nφi = { πi(Y), πi(Y) > πth,E and i ∈ C −∞, πi(Y) ≤ πth,E or i /∈ C (5)\n• Approximate max probability with elimination: Analogously, we can consider a strategy where objects are eliminated if π̃i falls below a threshold πth,E. The corresponding fitness index is given by (5) where πi is replaced by π̃i.\nAll these choices of fitness index have been tested numerically."
    }, {
      "heading" : "D. Allocation function",
      "text" : "As stated in previous sections, given the current fitness index, the allocation function A(·) determines the number\nof further evaluations needed by each object in round ℓ. Furthermore, A(·) is a non-decreasing function of the fitness index (Property 1).\nFor simplicity, we are particularly interested in the case where A(·) returns values in {0, 1}N , i.e., where the number of workers assigned to every object within a round is either 0 or 1. In such a case, in round ℓ, the B(ℓ) ≤ N top-quality objects will receive an extra worker, while all other objects will not receive any extra worker.\nTwo possible choices are considered in this paper, according to whether the total evaluation budget is either fixed or not.\n• Unbounded budget: If there is no maximum number of requested evaluations, A only depends on the fitness index, in the following way:\nm (ℓ) i =\n{ 1, φ\n(ℓ) i > πth,A\n0, φ (ℓ) i ≤ πth,A\nwhere πth,A is a suitable accuracy threshold, generally different from πth,E defined in the previous subsection. However, for consistency, we need to have πth,E ≤ πth,A. • Bounded budget: If at most Mmax evaluations can be requested in all rounds, then, in round ℓ, A(·) must take into account also the number of evaluations already requested, given by M (ℓ−1). Let again πth,A be the threshold against which the fitness index is compared, like in the unbounded-budget case, and let B(ℓ) be the number of objects that currently pass the threshold. If B(ℓ) ≤ Mmax−M (ℓ−1), then the allocation of new evaluations is the same as for unbounded budget, otherwise only the Mmax −M (ℓ−1) objects with the largest fitness index are allocated a further evaluation."
    }, {
      "heading" : "E. Termination rules",
      "text" : "Based on the choice of the fitness index and the allocation function A, the algorithm termination rule may be different.\n• Maximum budget achieved: When a maximum of Mmax evaluations is allowed, reaching this maximum budget will cause rounds to stop. • Singleton contestant set: For algorithms that eliminate objects when their fitness is lower than πth,E, the natural termination condition is when the contestant set only contains a single object, i.e., |C(ℓ)| = 1. • Accuracy: If only a single object passes the accuracy threshold πth,A, while all other objects do not, meaning that there is already a strong candidate winner, the algorithm may terminate rounds.\nWhen applicable, the termination rule can be the combination of all three rules above, i.e., the algorithm may terminate whenever one of the three becomes true."
    }, {
      "heading" : "V. SCORING VERSUS DIRECT COMPARISONS",
      "text" : "In this section we want to show that, whenever workers are able to provide (noisy) quantitative estimates of the object quality, algorithms exploiting those estimates are in general more effective than algorithms just resorting to direct comparisons among subsets of objects.\nWe start by considering a toy case in which only two objects are given, with qualities x1 and x2 = x1 + ∆, and we compare two algorithms requiring the same amount of human effort. The first algorithm resorts to outcomes of direct comparisons between the objects performed by the crowd workers, while the second exploits quantitative estimates of the object qualities provided by the same (or other) crowd workers.\nObserve that an algorithm exploiting the outcomes of direct comparisons between objects, and employing a fixed budget of W workers for each comparison, necessarily works as follows. Each of the W enrolled workers returns a binary variable, indicating which object she prefers. Once all answers, collectively denoted Z, are obtained, a majority rule is applied by the algorithm to choose the “best” object. According to our model, each worker prefers object 1, if she estimates that the quality of object 1 exceeds the quality of object 2 and vice versa. Thus, a worker chooses object 1, i.e., returns an incorrect answer, with probability p∆ = 12erfc( ∆ 2σ ), while she chooses object 2, thus returning a correct answer, with probability 1− p∆.\nProcessing the W collected answers (to simplify the analysis, we assume an odd value for W ), the algorithm based on comparisons erroneously selects object 1 whenever the number of answers equal to 1 exceeds W/2, i.e.:\npcompe = P ( Bin(W, p∆) > W\n2\n)\nwhere Bin(W, p) denotes a binomial distribution of parameters W and p.\nInstead, an algorithm that has access to the quantitative quality estimates Y provided by the W crowd workers, naturally selects the object with the largest estimated quality x̂(yi). In this case the error probability is given by:\npeste = 1\n2 erfc\n(√ W∆\n2σ\n) .\nNot surprisingly, peste < p comp e , as shown in Fig. 1 for W = 101. Indeed, from an information-theoretic perspective, the answers Y provide much more information on the quality of the two objects than the comparisons Z. This implies that any algorithm resorting to direct comparisons does not fully exploit the information on object quality that crowd workers are able to provide and, as a consequence, turns out to be suboptimal.\nWe now analytically compare pcompe and peste to better quantify the advantages of the approach exploiting quantitative estimates of object quality. To this end, we approximate the binomial distribution Bin(W, p∆) with a Gaussian distribution N (Wp∆, √ Wp∆(1 − p∆)). By the De Moivre-Laplace theorem, such approximation is asymptotically tight for large W . Following this approach, pcompe can be approximated as:\npcompe ≈ 1\n2 erfc\n( √ W (1− 2p∆)\n2 √ 2p∆(1 − p∆)\n)\nTo further simplify the expression of pcompe , we consider the\nlimit for ∆ σ → 0, in which case p∆ = 12\n( 1− 2√\nπ ∆ 2σ\n) +o(∆\nσ ),\nand thus\npcompe ≈ 1\n2 erfc\n(√ 2\nπ\n√ W∆\n2σ + o\n( ∆\nσ\n))\nThe performance penalty entailed by the approach resorting to direct comparisons of objects is expressed by the factor√ 2/π appearing in the argument of the above erfc function. This factor has a strong impact on the algorithm performance, since, for ∆/σ sufficiently small, as W increases, the ratio peste /p comp e tends exponentially fast to zero.\nNow, consider a case in which N > 2 objects are to be evaluated. For example, let us focus on the case N = 4. To declare a winner through direct comparisons of objects, we need to compare at least three pairs of objects. A natural solution is to arrange a tournament in which the four objects are first partitioned into two pairs, so that the objects in each pair can be compared in parallel (first round). The two winners of the first round are then compared to identify the globally best object (second round). Observe that the outcomes Z returned by crowd workers within the first round cannot be exploited in any way at the second round. In other words, at the end of the first round, no useful information is available to rank the two first-round winners. If, instead, the workers return their own quantitative evaluations Y of the object quality, the evaluations carried out within the first round provide useful information also for the second round.\nIn light of this discussion, it should not be surprising that the performance gain of algorithms exploiting workers’ quantitative quality evaluations increases as the number of objects increases."
    }, {
      "heading" : "VI. ANSWER QUANTIZATION",
      "text" : "Up to now, we have assumed that workers return unquantized (i.e., infinite-precision) noisy evaluations of object qualities. This assumption is unpractical in many scenarios, where instead workers’ evaluations must belong to a finite alphabet, i.e., they are quantized. In this section, we discuss how quantization can be effectively implemented in order to approach the performance of the proposed unquantized algorithm.\nFrom a system point-of-view, the key parameter of a quantizer is the cardinality L of the alphabet on which answers should be encoded, i.e., the number of levels of the quantizer. Given L, a specific quantization rule is characterized by an (L + 1)-dimensional vector of thresholds Z = [z1, · · · , zl, · · · , zL+1] with z1 = −∞ < z2 < z3 < · · · , zL < zL+1 = +∞ and an L-dimensional vector W = [w1, · · · , wl · · · , wL] of representative values, satisfying wl ∈ (zl, zl+1). If workers’ answers are quantized, then the j-th answer to the evaluation of object i can be modelled as\na (q) ij = Q(aij)\nwhere Q(x) = wl whenever x ∈ (zl, zl+1].2 Notice that we consider here a fixed, non-adaptive quantizer, that is defined once and for all at the beginning, before any evaluation takes place.\nIn our context, the problem of optimal quantization can be formulated in terms of the minimization of some distortion index between unquantized answers and their quantized version. The mean square error E[(a(q)−a)2] represents a natural candidate for such distortion index, also because the seminal work by Lloyd [10] provides an efficient iterative algorithm for the design of a quantizer that minimizes the mean square error. Now, the nontrivial question to be answered is: which are the answers whose distortion after quantization should be minimized? We list in the following a few possible answers to such question.\n• Since we have N objects whose quality values are i.i.d., each with pdf fq, we may want to minimize the distortion on the answer relative to the generic object. With such a choice, the distribution with respect to which the mean square error is to be computed is f (I)a = fq ∗ fn, where ∗ is the convolution product and fn = N (0, σ) is the distribution of the workers’ evaluation error. • From a different perspective, since we are searching for the top-quality object, we should minimize the distortion on the answers associated to that object only, i.e., averaging with respect to f (II)a = fq[1] ∗ fn, fq[1] being the a-priori distribution of the largest quality value. • Taking an approach which is in between the previous two, and considering that our target is to discriminate the best object from the others, we could aim at minimizing a weighted combination of the distortions relative to the ordered quality values. This goal can be achieved by using as the answer distribution f (III)a = ∑N−1 i=0 αifq[i] ∗\nfn, where fq[i] is the a-priori distribution for the i-th best object, and αi is its associated weight, satisfying αi+1 ≤ αi.\nWe will see in the next section that the impact on algorithm performance of the different possible choices for fa is pretty significant. Therefore, the quantizer must be carefully\n2Observe, however, that workers can be unaware of representative W ; every worker is just requested to express a satisfaction level in {1, · · · , L}, which is obtained by comparing her own unquantized evaluation with thresholds Z . Therefore, our model perfectly matches the assumptions of [8].\ndesigned. At last, observe that f (I)a and f (II) a can be obtained as particular cases of f (III)a by setting αi = 1/N for all i in the first case, and α1 = 1, αi = 0 for i > 1 in the second case."
    }, {
      "heading" : "VII. RESULTS",
      "text" : "In this section, we compare the performance of several algorithms that are obtained by making different choices for the fitness index, the allocation function, the termination rule, and the quantizer."
    }, {
      "heading" : "A. Unquantized answers, unbounded budget",
      "text" : "In particular, we first focus on algorithms with unquantized answers and unbounded budget, and define:\n• the ’Greedy-Keep-Exact’ (GKE) algorithm, employing the exact max probability as fitness index, unbounded budget as allocation function, and the accuracy termination rule; • the ’Greedy-Keep-Approximate’ (GKA) algorithm, employing the approximate max probability as fitness index, the unbounded budget as allocation function, and the accuracy termination rule; • the ’Greedy-Remove-Approximate’ (GRA) algorithm, employing the approximate max probability with eliminations as fitness index, the unbounded budget as allocation function, and the singleton contestant set as termination rule.\nTo reduce the space of parameters, we have always fixed πth,A = πth,E , πth. For the sake of comparison, the following algorithms have been also considered.\n• We have superimposed a classical tournament scheme to our previously described algorithms, obtaining a family of Tournament-Nb (T-Nb) algorithms. Specifically, in TNb algorithms, the N objects to be evaluated are first randomly partitioned into subgroups of size Nb (for simplicity, we neglect rounding problems). The GKA algorithm is then run to elect a winner for each of the object groups. Only winners have access to the second stage, in which again objects are partitioned into subgroups of size Nb and winners are elected for each subgroup. The process is iterated until only one winner is left. We remark that our tournament schemes effectively exploit, at every stage, full information about the evaluations of competing objects collected at earlier stages. • A non-adaptive algorithm, which assigns to every object a fixed number of workers, referred to as Uniform algorithm (UA). • As a reference, we have also considered an unfeasible Genie-Aided (GA) algorithm, which has access to the identity of the two best competing objects, after a first initial round of evaluations, where every object receives one score. Therefore, in the following rounds, the GA algorithm equally distributes workers only to the top two objects until the accuracy termination rule is met. Observe that, by construction, the performance of the GA algorithm constitutes an upper bound for every feasible\nalgorithm, since, as discussed in Section V, it implements the optimal policy to find the best between two objects.\nWe start considering a simple scenario with N = 16 objects whose qualities xi are equally spaced in the interval [−1, 1], so that the smallest difference between quality values is ∆ =\n2 N−1 = 2 15 . The standard deviation of the worker evaluation error has been set to σ = ∆/2.\nFig. 2 shows the results obtained with the different proposed algorithms, plotted in terms of error probability pe versus the average number of performed evaluations per object M/N . We highlight that the different trade-offs between pe and M/N correspond to different values of the threshold πth, as explicitly shown in Fig. 2. Observe that the choice of πth has a direct impact on the expected error probability of the algorithm. In particular, for the GKE algorithm a simple analysis yields the following relationship between πth and pe:\npe = ∑\ni6=î∗ πi(y) ≤ (N − 1)πth (6)\nMore in general, for every algorithm, by decreasing πth we achieve a larger accuracy at the cost of employing more resources.\nFrom the results, the following observations can be made.\ni) Every adaptive algorithm performs much better than the uniform algorithm, employing on average the same amount of resources.\nii) Greedy algorithms without elimination perform better than greedy algorithms with elimination. Observe, however that the latter are preferable in terms of computational complexity, since in such schemes, at round ℓ, the fitness index has to be computed only for objects in the contestant set C(ℓ), while in schemes without elimination it has to be computed for all objects.\niii) The selection of the approximate max probability as fitness index, in place of the exact max probability, does not lead to any appreciable performance degradation, while having a significant beneficial impact on the computational complexity of the algorithm (this has been\nchecked also for algorithms with elimination). iv) Tournament algorithms perform worse than our adap-\ntive algorithms; furthermore, their performance tends to worsen as Nb is reduced. v) The performance of greedy algorithms without elimination is only marginally worse than that of the GA algorithm.\nTo gather more insight on the algorithm behavior, a further performance comparison for the different schemes is reported in Fig. 3 for the case in which the number of objects is increased to N = 256 (object qualities xi are still equally spaced in the interval [−1, 1], with σ = ∆/2). Observe that the relative ranking among the algorithms does not change, but the performance gap between algorithms tends to become more significant. In particular, tournament algorithms perform much worse than GKA and GRA. We remark that our results seem somehow in contrast with findings in [8], where it has been shown that tournament algorithms provide the best performance, for cases in which users are only able to compare objects pairs. To intuitively grasp why they become inefficient in our context as N increases, notice that tournament algorithms waste a significant amount of resources to discriminate among objects with similar quality (accidentally placed in the same group), even when the quality of such objects is much worse than top-quality values. Observe that, also in this case, GKA (whose performance is again practically indistinguishable from GKE, not reported in Fig. 3 for the sake of readability) performs similarly to the GA algorithm. This proves the effectiveness of GKA in the considered scenarios.\nTo evaluate the impact of human evaluation errors on the overall performance of algorithms, Fig. 4 reports a performance comparison between the GKA and GA algorithms for different values of the parameter ∆/σ. Observe that ∆/σ plays an important role: as the ratio ∆/σ decreases, more and more resources are needed to meet the same error probability. The performance gap between the two algorithms is rather limited in all cases. In particular, uniformly over all cases, the penalty cost in terms of evaluations required to obtain the same error\nprobability does not exceed 10%. Once again, this confirms the effectiveness of our approach for a broad range of scenarios where evaluation errors have different impact."
    }, {
      "heading" : "B. Unquantized answers, bounded budget",
      "text" : "Now, we move to scenarios in which the budget of allocations is bounded, and we restrict our analysis to:\n• the ’bounded-Greedy-Keep-Approximate’ (bGKA), employing the approximate max probability as fitness index, the bounded budget as allocation function, the maximum budget achieved or accuracy termination rule; • the ’bounded-Greedy-Remove-Approximate’, (bGRA) employing the approximate max probability with eliminations as fitness index, the bounded budget as allocation function, the maximum budget achieved or singleton contestant list as termination rule.\nAs a reference, we report also the performance of the bounded version of the GA algorithm (bGA), which again provides an obvious upper bound to performance. Also in this case, to reduce the space of parameters, we fix πth = πth,A = πth,E.\nFig. 5 compares the performance of different algorithms for different values of the normalized budget K = Mmax/N . In the same figure, we also report the performance of the unbounded versions of the algorithms. We observe that:\ni) the error probability for bGKA and bGRA now is not monotonic with respect to M/N . In particular, if the average number of evaluations M is sufficiently smaller than Mmax (i.e., for sufficiently large values of πth), the performance of the bounded algorithms does not significantly differ from the respective unbounded version: this happens because the probability that the algorithm terminates for achieving maximum budget is negligible. As we further reduce πth, increasing the required accuracy, the probability that the algorithm terminates because the maximum budget is achieved quickly increases, and the overall performance of the bounded algorithms degrades. These effects can be better understood from Fig 6, which reports the average number of evaluations per\nobject, M/N , and the error probability, as a function of the threshold πth, for the bGKA algorithm (similar considerations hold for bGRA). Now, observe that M/N monotonically increases as πth decreases, since, by decreasing πth, the algorithm tends to be more conservative in excluding objects from receiving further evaluations. As a result, we distribute a larger number of allocations to objects with a quality quite distant from the maximum. While the error probability behaves monotonically with respect to πth (decreasing as πth is decreased) in the case of an unbounded budget, in the case of bounded budget, choosing a value of πth too small will lead to an inefficient distribution of the limited resources. ii) As a consequence of i), only a limited range of pe values can be achieved in the bounded budget case.\niii) Also in this case, bGKA outperforms bGRA.\nNow, we consider a scenario in which object qualities xi are randomly drawn from a Gaussian distribution with zero mean and standard deviation σa.\nFig. 7 presents the performance of bGKA for the cases in which σa/σ = 3 and σa/σ = 10. For the sake of comparison, the figure also reports the performance of the uniform algorithm.\nWe observe that:\ni) also when qualities are drawn from a Gaussian distribution, bGKA improves performance with respect to the uniform allocation algorithm that employs the same average budget of resources;\nii) from a qualitative point of view, bGKA exhibits a behavior which is pretty similar to the case where objects are equally spaced. By decreasing πth we initially increase accuracy at the cost of increasing also the amount of employed resources. However, beyond a given point, further decrements of πth cause a loss of efficiency, worsening the overall performance of the algorithm;\niii) the performance of the algorithms heavily depends on the parameter σa/σ, which can be regarded as a difficulty index for the problem.\nTo complement the previous results for the bGKA algorithm, Fig. 8 reports the first-order conditional distortion, i.e., E[qi∗ − qî∗ | i∗ 6= î∗ (y)], which may be interpreted as the average quality gap between the best object and the object selected by the bGKA algorithm, conditional over the fact that the bGKA algorithm is not choosing the best object. Observe that the values of the distortion are extremely small, proving that, whenever the bGKA algorithm makes an error, it selects an object with a quality very close to the best. In other words, errors occur (almost) only in very difficult problem instances, where a cluster of two or more objects with quality very close to the top value exists. For completeness, Fig. 8 reports also the conditional distortion for the uniform allocation algorithm. Observe that the distortion experienced with the uniform allocation algorithms is more than one order of magnitude larger than under bGKA. This implies that the algorithms we propose in this paper not only offer the benefit of a drastic reduction in error probability, but they also bring a second crucial improvement: a drop in the gap between the quality of the selected object and the actual maximum quality, in the case of errors."
    }, {
      "heading" : "C. Quantization effects",
      "text" : "We finally consider the effect of quantization on the system performance.\nIn Fig. 9, we test the impact of different quantizers, in terms of pe versus M/N , in the case where N = 256 objects are equally spaced in the interval [−1, 1]. In the figure, the GKA algorithm with unbounded budget is employed for all curves. Moreover, the standard deviation of the worker evaluation error has been set to σ = ∆/2. We recall that, for equally spaced objects, ∆ = 2/(N − 1) is the smallest distance between quality values. The solid line without markers represents the performance of the GKA algorithm without quantization and is used as a benchmark. The line with triangle markers refers to the performance obtained employing a quantizer with L = 32 representative values uniformly distributed in [−1−2σ, 1+2σ]. We observe that, despite of the high number of levels, uniform quantization significantly worsen the error probability. Instead, much better performance is achievable when the quantizer design is more accurate. As an example, the solid line with filled square markers refers to the case when L = 32, and the quantizer is designed according to the criteria in [10], over the answer distribution f (III)a = ∑N−1 i=0 αifq[i] ∗ fn, where αi = γ i, and γ = 1/2. This quantizer, labeled “Lloyd” in the legend, provides performance close to the unquantized case. In general, we observe that the performance is quite insensitive to the design parameter γ except if its value is close to the extreme point γ = 1. Fig. 9 also shows the performance of Lloyd’s quantizers with γ = 1/2 and number of levels L = 4, 8. We observe that an accurately designed quantizer with only L = 8 levels is enough to provide performance close to the unquantized case.\nFig. 10 refers to the case of N = 256 objects with Gaussiandistributed qualities, the budget is bounded to K = 3, and workers’ answers are quantized. The quantizer is designed according to the Lloyd’s algorithm over the weighted distribution f (III) a = ∑N−1 i=0 αifq[i] ∗ fn, with αi = γi, and γ = 1/2. In the figure, the solid line refers to the unquantized case, while the lines with markers refer to the case where quantization is\nemployed with L = 4, 8, 16, 32 levels. Also in this scenario, we observe that L = 8 quantization levels are enough to provide performance close to the unquantized case."
    }, {
      "heading" : "VIII. DESIGN CONSIDERATIONS",
      "text" : "In this section we address two important issues: we evaluate the computational complexity of our algorithms, and we discuss how to set algorithm parameters (and in particular πth). For the sake of brevity, we restrict our investigation to bGKA, which turns out to be the best-performing algorithm.\nFor what concerns computational complexity, at round ℓ, for every object i in the contestant set, bGKA: i) must update x̂(ℓ)i (this requires O(1) operations per object); ii) must compute a fitness index φ(ℓ)i (this again requires O(1) operations per objects, since the approximate max probability can be computed exploiting (4)); iii) must compare φ(ℓ)i with πth, in order to decide whether to allocate an extra worker to i (again O(1) operations are required). In the final round, if the number of objects that pass the threshold exceeds the residual budget, the execution of an extra task is necessary: objects must be sorted\nin order of their fitness index (the cost of sorting is notoriously O(N logN)). Observing that Mmax is an obvious upper bound to the number of rounds, it turns out that the overall complexity of bGKA is O(MmaxN+N logN) = O(MmaxN) in consideration of the fact that by construction Mmax ≥ N .\nAs regards the setting of the algorithm parameters, which in the case of bGKA are πth and Mmax, we observe that the choice of the value for Mmax normally depends on economical or application-oriented considerations, whose discussion is beyond the scope of this paper. Given the value of Mmax, it is possible to tune the algorithm by selecting the value for πth, which should be set in order to minimize the error probability. For a careful setting of πth, a preliminary parametric analysis of the algorithm performance is necessary to estimate the key system parameters (such as the pdf fq of object qualities, and the variance σ of the workers’ quality estimation errors)."
    }, {
      "heading" : "IX. CONCLUDING REMARKS",
      "text" : "In this paper, we have studied the problem of finding the top-quality element within a large collection of objects, resorting to human evaluations affected by noise. Differently from previous works, our study started assuming that unquantized scores are returned by the evaluators, and highlights the potential advantages of such approach. Then we have shown how to properly design quantized schemes whose performance is very close to their ideal unquantized counterparts, provided that a reasonable number of quantization levels is assigned to workers’ answers. We plan to generalize the approach proposed in this paper to the case of workers with different skills, and to the problem of finding the k top-quality elements within a large collection of objects through crowdsourcing algorithms."
    } ],
    "references" : [ {
      "title" : "A Survey of Crowdsourcing Systems",
      "author" : [ "M.-C. Yuen", "I. King", "K.-S. Leung" ],
      "venue" : "IEEE PASSAT-SOCIALCOM, Boston (USA), Oct. 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Max algorithms in crowdsourcing environments",
      "author" : [ "P. Venetis", "H. Garcia-Molina", "K. Huang", "N. Polyzotis" ],
      "venue" : "Intern. Conf. on World Wide Web (WWW ’12), New York (USA), 989–998, 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Dynamic Max Algorithms in Crowdsourcing Environments",
      "author" : [ "P. Venetis", "H. Garcia-Molina" ],
      "venue" : "Tech. Rep., Stanford InfoLab.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "So who won?: dynamic max discovery with the crowd",
      "author" : [ "S. Guo", "A. Parameswaran", "H. Garcia-Molina" ],
      "venue" : "2012 ACM Intern. Conf. on Management of Data, New York (USA), 385–396, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "tDP: An Optimal- Latency Budget Allocation Strategy for Crowdsourced MAXIMUM Operations",
      "author" : [ "V. Verroios", "P. Lofgren", "H. Garcia-Molina" ],
      "venue" : "2015 ACM Intern. Conf. on Management of Data, New York (USA), 1047–1062, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Using the crowd for top-k and group-by queries",
      "author" : [ "S.B. Davidson", "S. Khanna", "T. Milo", "S. Roy" ],
      "venue" : "ICDT ’13, New York (USA), 225–236, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Computing with noisy information",
      "author" : [ "U. Feige", "P. Raghavan", "D. Peleg", "E. Upfal" ],
      "venue" : "SIAM J. Comput., 23(5): 1001–1018, 1994.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Hybrid Strategies for Finding the Max with the Crowd",
      "author" : [ "A.R. Khan", "H. Garcia-Molina" ],
      "venue" : "Tech. Rep., Stanford InfoLab.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "A law of comparative judgment",
      "author" : [ "L.L. Thurstone" ],
      "venue" : "Psychological Review, vol. 34, pp. 273–286, 1927.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1927
    }, {
      "title" : "Least Squares Quantization in PCM",
      "author" : [ "S.P. Lloyd" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 28, pp. 129-137, No. 2, March 1982. 10",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1982
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Crowdsourcing is a term often adopted to identify distributed systems that can be used for the solution of a wide range of complex problems by integrating a large number of human and/or computer efforts [1].",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 1,
      "context" : ", cost, accuracy, latency) [2]–[7].",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : ", cost, accuracy, latency) [2]–[7].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "A similar path was followed in the recent (still unpublished) work by Khan and Garcia Molina [8], which studies algorithms to find the maximum element in a group of objects, and discusses approaches based on comparisons, on ratings, as well as on a mix of the two possibilities.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "Indeed, [8] assumes that workers’ answers are coarsely quantized over few levels (typically three or five), and this makes objects with similar quality indistinguishable, so that direct comparisons and tournaments become necessary to break ties.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 7,
      "context" : "Another significant difference with respect to [8] is in the scope of the works.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "The paper [8], instead, focuses on non-adaptive algorithms distributing resources to objects according to a fixed, pre-established scheme.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "Observe that our model differs from [2]–[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]–[7] assume workers to be only able to perform noisy comparisons between groups of objects.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "Observe that our model differs from [2]–[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]–[7] assume workers to be only able to perform noisy comparisons between groups of objects.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "Observe that our model differs from [2]–[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]–[7] assume workers to be only able to perform noisy comparisons between groups of objects.",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 6,
      "context" : "Observe that our model differs from [2]–[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]–[7] assume workers to be only able to perform noisy comparisons between groups of objects.",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "We also wish to remark that the Gaussian model of worker error is in good agreement with Thurstone’s law of comparative judgment [9], according to which comparisons are based on latent quality estimations, whose distribution is Gaussian around the true quality value.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "In the context of crowdsourcing, the same model has recently been employed also in [8].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "The mean square error E[(a(q)−a)2] represents a natural candidate for such distortion index, also because the seminal work by Lloyd [10] provides an efficient iterative algorithm for the design of a quantizer that minimizes the mean square error.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "aging with respect to f (II) a = fq[1] ∗ fn, fq[1] being the a-priori distribution of the largest quality value.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "aging with respect to f (II) a = fq[1] ∗ fn, fq[1] being the a-priori distribution of the largest quality value.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "Therefore, our model perfectly matches the assumptions of [8].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "We remark that our results seem somehow in contrast with findings in [8], where it has been shown that tournament algorithms provide the best performance, for cases in which users are only able to compare objects pairs.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "As an example, the solid line with filled square markers refers to the case when L = 32, and the quantizer is designed according to the criteria in [10], over",
      "startOffset" : 148,
      "endOffset" : 152
    } ],
    "year" : 2015,
    "abstractText" : "We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, for example in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy evaluation. All received evaluations are then combined, to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then, we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. We finally compare our approach based on scoring object qualities against traditional proposals based on comparisons and tournaments.",
    "creator" : "gnuplot 4.6 patchlevel 4"
  }
}