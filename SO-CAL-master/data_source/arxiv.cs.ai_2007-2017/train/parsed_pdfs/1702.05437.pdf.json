{
  "name" : "1702.05437.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Quantifying Program Bias",
    "authors" : [ "Aws Albarghouthi", "Loris D’Antoni", "Samuel Drews", "Aditya Nori" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "A number of very interesting applications of program analysis have been explored in the probabilistic setting: reasoning about cyber-physical systems [66], proving differential privacy of complex algorithms [14], reasoning about approximate programs and hardware [23, 65], synthesizing control programs [25], amongst many others. In this paper, we turn our attention to the problem of quantifying program bias. Program bias Programs that make decisions can be biased. Consider, for instance, automatic grading of writing prompts for standardized tests [10]; some speech patterns may be characterized as poor writing style and result in lower scores. However, if such speech patterns are affiliated with a specific ethnic group, then the bias is a potential source of concern.\nPrograms have become powerful arbitrators of a range of significant decisions with far-reaching societal impact— hiring [50, 55], welfare allocation [40], prison sentencing [8], policing [19, 62], amongst many others. With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].\nFairness and justice have always been a ripe topic for philosophical debate [63], and, of course, there are no established rigorous definitions. Nonetheless, the rise of automated decision-making prompted the introduction of a number of formal definitions of fairness, and their utility within different contexts is being actively debated [37, 41, 43, 48, 64]. Notable formulations of fairness include individual fairness, which dictates that similar inputs must result in similar outputs, and group fairness, which dictates that a particular subset of inputs must have a similar aggregate output to the whole. In this paper, we view such notions of fairness as quantitative properties of decision-making programs. Bias as a probabilistic property We think of decision-making algorithms as probabilistic programs, in the sense that they are invoked on input drawn from a probability distribution, e.g., representing the demographics of some population. Quantifying program bias becomes a matter of reasoning about probabilities of program executions.\nConsider a hiring program P that takes as input a vector of arguments v representing a job applicant’s record. One of the arguments vs in the vector v states whether the person is a member of a protected minority or not, and similarly vq in v states whether the person is qualified or not. Evaluating P(v) returns a Boolean value indicating whether a person is hired. Our goal may be to prove a group fairness property that is augmented with a notion of qualification—that the algorithm is just as likely to hire a qualified minority applicant as it is for other qualified non-minority applicants. Formally, we state this probabilistic condition as follows:\nPr[P(v) = true | vs = true ∧ vq = true] Pr[P(v) = true | vs = false ∧ vq = true] > 1−\nHere, is a small prespecified value. In other words, the probability of hiring a person v, conditioned on them being a qualified minority, is very close to (or greater than) the probability of hiring a person conditioned on them of being\n1 2017/3/8\nar X\niv :1\n70 2.\n05 43\n7v 2\n[ cs\n.P L\n] 7\nM ar\n2 01\n7\na qualified non-minority. The goal of this paper is to propose techniques for automatically proving whether a program satisfies this kind of probabilistic properties.\nProving statements of the above form amounts to quantifying probabilities of return outcomes of the program. We propose an automated verification technique that reduces the verification problem to that of computing the weighted volume of the logical encoding of a program in real arithmetic. We then utilize a novel symbolic volume computation algorithm that exploits the power of SMT solvers to integrate probability density functions over real regions defined by program encodings. We show that our algorithm is guaranteed to converge to the exact values in the limit, thus resulting in a sound and complete verification procedure. To our knowledge, this is one of the first probabilistic inference algorithms for SMT with this expressivity and guarantees. We implement our algorithm in a tool called FairSquare, which we evaluate on a number decision-making programs generated by a range of machine-learning algorithms from real-world data. Contributions This paper makes a number of conceptual, algorithmic, and practical contributions: – We present a verification technique for probabilistic programs that reduces the problem to a set of weighted volume\ncomputation problems. We present a novel weighted-volume-computation algorithm, for formulas over real closed fields, that utilizes an SMT solver as a black box, and we prove that it converges to the exact volume in the limit. To our knowledge, this is one of the first probabilistic inference algorithms for SMT with this generality and guarantees. (Sec. 4)\n– We present an automated verification tool, FairSquare, and use it to quantify certain types of bias in a broad spectrum of programs representing machine-learning classifiers generated from real-world datasets. Our evaluation demonstrates the power of our technique and its ability to outperform state-of-the-art probabilistic program analyses. (Sec. 5)"
    }, {
      "heading" : "2. Overview and Illustration",
      "text" : "Our problem setting is as follows: First, we are given a decision-making program Pdec. Second, we have a probabilistic precondition defining a probability distribution over inputs of Pdec. We define the probability distribution operationally as a probabilistic program Ppre, which we call the population model. Intuitively, the population model provides a probabilistic picture of the population from which the inputs of Pdec are drawn. Third, we are given a quantitative postcondition post that correlates the probabilities of various program outcomes. This postcondition can encode various properties relating to program bias; intuitively, our goal is to prove the following triple:\n{v ∼ Ppre} r ← Pdec(v) {post}\nLet us consider various possible instantiations of post . Feldman et al. [41] introduced the following definition, inspired by Equality of Employment Opportunity Commission’s [6] recommendation in the US:\nPr[r = true | min(v) = true] Pr[r = true | min(v) = false] > 1−\nAssuming Pdec returns a Boolean value—indicating whether an applicant v is hired—this group fairness property states that the selection rate from a minority group, min(v) = true, is as good as the selection rate from the rest of the population. One can thus view this verification problem as proving a probabilistic property involving two sets of program traces: one set where the input min(v) is true, and another where it is false. Alternatively, the above definition could be strengthened with a lower bound on the ratio, so as to ensure that the selection rate of the two groups is similar (statistical parity). Further, we could additionally condition on qualified applicants, e.g., if the job has some minimum qualification, we do not want to characterize group fairness for arbitrary applicants, but only within the qualified subpopulation. Various comparable notions of group fairness have been proposed and used in the literature, e.g., [33, 41, 72].\nWhile the above definition is concerned with fairness at the level of subsets of the domain of the decision-making program, individual fairness [37] is concerned with similar outcomes for similar elements of the domain. In our hiring example, one potential formulation is as follows:\nPr[r1 = r2 | v1 ∼ v2] > 1−\nIn other words, we want to ensure that for any two individuals, if they are similar (∼), then we want them to receive similar outcomes (r1 = r2) with a high probability. This is a hyperproperty—as it considers two copies of Pdec—and can be encoded through self-composition [13]. This property is close in nature to differential privacy [36] and robustness [15, 24].\nOf course, various definitions of fairness have their merits and their shortcomings, and there is an ongoing discussion on this subject [7, 37, 41, 43, 48]. Our contribution is not to add to this debate, but to cast fairness as a quantitative property of programs, and therefore enable automated reasoning about fairness of decision-making programs.\n2 2017/3/8\nA simple verification problem Consider the two programs in Figure 1(a). The program popModel is a probabilistic program describing a simple model of the population. Here, a member of the population has three attributes, all of which are realvalued: (i) ethnicity; (ii) colRank, the ranking of the college the person attended (lower is better); and (iii) yExp, the years of work experience a person has. We consider a person is a member of a protected group if ethnicity > 10; we call this the sensitive condition. The population model can be viewed as a generative model of records of individuals—the more likely a combination is to occur in the population, the more likely it will be generated. For instance, the years of experience an individual has (line 4) follows a Gaussian (normal) distribution with mean 10 and standard deviation 5. Observe that our model specifies that members of a protected minority will probably attend a lower-ranked college, as encoded in lines 5-6.\nThe program dec is a decision-making program that takes a job applicant’s college ranking and years of experience and decides whether they get hired (the fairness target). The program implements a decision tree, perhaps one generated by a machine-learning algorithm. A person is hired if they attended a top-5 college (colRank <= 5) or have lots of experience compared to their college’s ranking (expRank > -5). Observe that dec does not access an applicant’s ethnicity.\nOur goal is to establish whether the hiring algorithm dec discriminates against members of the protected minority. Concretely, we attempt to prove the following property:\nPr[hire | min] Pr[hire | ¬min] > 1−\nwhere min is shorthand for the sensitive condition ethnicity > 10, and is a small parameter set to 0.1 for illustration. Despite the potential shortcomings of this group fairness property [37], its simple formulation serves well as an illustration of our technique.\nWe can rewrite the above statement to eliminate conditional probabilities as follows:\nPr[hire ∧ min] · Pr[¬min] Pr[hire ∧ ¬min] · Pr[min] > 1− (1)\nTherefore, to prove the above statement, we need to compute a value for each of the probability terms: Pr[hire ∧ min], Pr[min], and Pr[hire ∧ ¬min]. (Note that Pr[¬min] = 1− Pr[min].)\nNotice that, to prove or disprove inequality 1, all we need are good enough bounds on the values of such probabilities and not their exact values.\nFor the purposes of illustration, we shall focus our description on computing Pr[hire ∧ ¬min]. Probabilistic verification conditions To compute the probability Pr[hire∧¬min], we need to reason about the composition of the two programs, dec◦popModel. That is, we want to compute the probability that (i) popModel generates a non-minority applicant, and (ii) dec hires that applicant. To do so, we begin by encoding both programs as formulas in the linear real arithmetic theory of first-order logic. The process is analogous to that of standard verification condition (VC) generation for loop-free program fragments.\nFirst, we encode popModel as follows:\nϕpop ≡ ethnicity > 10⇒ colRank1 = colRank + 5 ∧ ethnicity 6 10⇒ colRank1 = colRank\nwhere subscripts are used to encode multiple occurrences of the same variable (i.e., SSA form). Note that assignments in which values are drawn from probability distributions do not appear in the encoding—we shall address them later.\nSecond, we encode dec as follows (after simplification):\nϕdec ≡ expRank = yExpi − colRanki\n∧ hire ⇐⇒ (colRanki 6 5 ∨ expRank > −5)\nwhere variables with the superscript i are the input arguments to dec. Now, to encode the composition dec ◦ popModel, we simply conjoin the two formulas—ϕpop and ϕdec—and add equalities between returns of popModel and arguments of dec.\nϕP ≡ ϕpop ∧ ϕdec ∧ yExpi = yExp ∧ colRanki = colRank1 Our goal is to compute the probability that a non-minority applicant gets hired. Formally, we are asking, what is the\nprobability that the following formula is satisfied?\nϕ ≡ ∃Vd. ϕP ∧ hire ∧ ethnicity 6 10\n3 2017/3/8\nVd is the set of variables that are not probabilistically assigned to, that is, all variables other than the three variables Vp = {ethnicity, colRank, yExp}. Intuitively, by projecting out all non-probabilistic variables, we get a formula ϕ whose models are the set of all probabilistic samplings that lead to a non-minority applicant being generated and hired. Weighted volume computation To compute the probability that ϕ is satisfied, we begin by noting that ϕ is, geometrically, a region in R3, because it has three free real-valued variables, Vp. The region ϕ is partially illustrated in Figure 1(b). Informally, the probability of satisfying ϕ is the probability of drawing values for the dimensions in Vp that end up falling in the region ϕ. Therefore, the probability of satisfying ϕ is its volume in R3, weighted by the probability density of each of the three variables. Formally:\nPr[hire ∧ ¬min] = ∫ ϕ pepypc dVp\nwhere, e.g., pe is the probability density function of the distribution gauss(0,10)—the distribution from which the value of ethnicity is drawn in line 2 of popModel. Specifically, pe is a function of ethnicity, namely, pe(ethnicity) =\n1 10 √ 2π e−\nethnicity2\n200 . The primary difficulty here is that the region of integration is specified by an arbitrary SMT formula over an arithmetic theory. So, how do we compute a numerical value for this integral? We make two interdependent observations: (i) if\n4 2017/3/8\nthe formula represents a hyperrectangular region in Rn—i.e., a box—then integration is typically simple, due to the constant upper/lower bounds of all dimensions; (ii) we can symbolically decompose an SMT formula into an (infinite) set of hyperrectangles.\nSpecifically, given our formula ϕ, we construct a new formula, ϕ, where each model m |= ϕ corresponds to a hyperrectangle that underapproximates ϕ. Therefore, by systematically finding disjoint hyperrectangles inside of ϕ and computing their weighted volume, we iteratively improve a lower bound on the exact weighted volume of ϕ. Figure 1(c) shows a possible underapproximation of ϕ composed of four hyperrectangles. Sec. 4 formalizes this technique and proves its convergence for decidable arithmetic theories. Proofs of group fairness We demonstrated how our technique reduces the problem of computing probabilities to weighted volume computation. Figure 1(d) illustrates a run of our tool, FairSquare, on this example. FairSquare iteratively improves lower and upper bounds for the probabilities in the ratio, and, therefore, the ratio itself. Observe how the upper bound (red) of the ratio is decreasing and its lower bound (blue) is increasing. This example is not group fair for = 0.1, since the upper bound goes below 0.9.\nRecall that applicants of a protected minority tend to attend lower-ranked colleges, as defined by popModel. Looking at dec, we can point out that the cause for unfairness is the importance of college ranking for hiring. Let us attempt to fix this by modifying line 2 of dec to expRank ← 5*yExp - colRank. In other words, we have made the hiring algorithm value an applicant’s job experience way more than college ranking. The run of FairSquare on the modified dec is illustrated in Figure 1(e), where the lower bound on the ratio exceeds 0.9, thus proving our fairness property."
    }, {
      "heading" : "3. Probabilistic Programs and Verification",
      "text" : "We formally define programs and present a general framework for stating and verifying probabilistic properties."
    }, {
      "heading" : "3.1 Program model and semantics",
      "text" : "Programs A program P is a sequence of statements S: S := V ← E assignment statement | V ∼ D probabilistic assignment | if B then S else S conditional | SS sequence of statements\nwhere V is the set of real-valued variables that can appear in P , e ∈ E is an arithmetic expression over variables in V , and b ∈ B is a Boolean expression over variables in V . A probabilistic assignment is made by sampling from a probability distribution p ∈ D. A probability distribution can be, for example, a Gaussian distribution, denoted by gauss(µ, σ), where µ, σ ∈ R are the mean and standard deviation of the Gaussian. We shall restrict distributions to be univariate and with constant parameters, e.g., mean and standard deviation of a Laplacian or Gaussian—that is, we assume independence of probabilistic assignments. Given a probabilistic assignment x ∼ p, we shall treat p(x) as a probability density function (PDF) of the distribution from which the value assigned to x is drawn. For instance, if the distribution is gauss(0,1), then p(x) = 1√\n2π e−\nx2 2 .\nWe use vi to denote a vector of input variables of P , and vo to denote a vector of output variables of P; these variables appear in V and denote the arguments and returns of P . We say that a program is closed if it has no inputs, i.e., vi is empty. We shall refer to the following subsets of V . – Vp ⊆ V is the set of probabilistic variables: those that get assigned to in probabilistic assignments. – Vd = V \\ Vp is the set of deterministic variables: those that do not appear in probabilistic assignments.\nThis simple language can be used to describe decision-making programs typical machine-learning classifiers such as decision trees, support vector machines, Bayesian networks, neural networks, as well as loop-free probabilistic programs (loops with constant bounds can be unrolled).1 As demonstrated in Sec. 2, the same language is used to define probabilistic preconditions programmatically. Operational semantics Typically, the state s : V → R of the program is defined as a valuation function from variables in V to values in R. In a probabilistic setting, however, we need to maintain an additional state that dictates values drawn from probability distributions. Following standard semantics of probabilistic programs [52], we assume a finite sequence of independent random variables. The semantics of an execution is thus defined for a fixed sequence of values ω of these variables. Informally, it is as if we performed all sampling before the program executes and stored the results in a sequence for use whenever we encounter a probabilistic assignment. For the full semantics, refer to Appendix A. 1 Unbounded loops could be handled through iterative unrolling as in [66].\n5 2017/3/8"
    }, {
      "heading" : "3.2 Programs and volume computation",
      "text" : "Following Chistikov et al. [27], we reduce the problem of computing the probability that the program terminates in a state satisfying ϕ to weighted volume computation (WVC) over formulas describing regions in Rn. In what follows, we begin by formalizing the WVC problem. Volume of a formula We will use LT to denote first-order formulas in some real arithmetic theory T . Specifically, we consider two decidable theories: linear real arithmetic and the strictly richer real closed fields—Boolean combinations of polynomial inequalities. Given a formula ϕ ∈ LT , a model m of ϕ, denoted by m |= ϕ, is a point in Rn, where n is the number of free variables of ϕ. Thus, we view ϕ as a region in Rn, i.e., ϕ ⊆ Rn. We use Xϕ = {x1, . . . , xn} to denote the free variables of ϕ.\nThe (unweighted) volume of a formula ϕ is ∫ ϕ\n1 dXϕ where dXϕ is short for dx1dx2 . . . dxn. For example, if ϕ is in R2, then ∫ ϕ\n1 dXϕ is the area of ϕ. Weighted volume of a formula We now define the weighted volume of a formula. We assume we are given a pair (ϕ,D), where ϕ ∈ LT andD = {p1, . . . , pn} is a set of probability density functions such that each variable xi ∈ Xϕ is associated with a density function pi(xi) of the probability distribution of its values. The weighted volume of ϕ with respect to D, denoted by VOL(ϕ,D), is defined as follows: ∫\nϕ ∏ xi∈Xϕ pi(xi) dXϕ\nExample 1. Consider the formula ϕ ≡ x1 + x2 > 0, and let D = {p1, p2}, where p1 and p2 are the PDF of the Gaussian distribution with mean 0 and standard deviation 1. Then,\nVOL(ϕ,D) = ∫ x1+x2>0 p1(x1)p2(x2) dx1dx2 = 0.5\nIntuitively, if we are to randomly draw two values for x1 and x2 from the Gaussian distribution, we will land in the region x1 + x2 > 0 with probability 0.5.\nProbabilistic verification conditions Recall that our goal is to compute the probability of some predicate ϕ at the end of a program execution, denoted Pr[ϕ]. We now show how to encode this problem as weighted volume computation. First, we encode program executions as a formula ϕP . The process is similar to standard verification condition generation (as used by verification [11] and bounded model checking tools [30]), with the difference that probabilistic assignments populate a set D of probability density functions.\nFigure 2 inductively defines the construction of a probabilistic verification condition for a program P , denoted by a function PVC(P), which returns a pair 〈ϕP ,D〉. Without loss of generality, to simplify our exposition, we assume programs are in static single assignment (SSA) form [31]. Given a Boolean expression b, the denotation JbK is the same expression interpreted as an LT formula. The same applies to arithmetic expressions e. For example, Jx + y > 0K , x + y > 0. Intuitively, the construction generates a formula ϕP that encodes program executions, treating probabilistic assignments as non-deterministic, and a set D of the PDFs of distributions in probabilistic assignments (rule VC-PASN).\nNow, suppose we are given a closed program P and a Boolean formula ϕ over its output variables. Then,\nPr[ϕ] = VOL(∃Vd. ϕP ∧ ϕ,D)\nThat is, we project out all non-probabilistic variables from ϕP ∧ ϕ and compute the weighted volume with respect to the densities pi ∈ D. Intuitively, each model m of ∃Vd. ϕP ∧ ϕ corresponds to a sequence of values drawn in probabilistic assignments in an execution of P . We note that our construction is closely related to that of Chistikov et al. [27], to which we refer the reader for a measure-theoretic formalization.\n6 2017/3/8\nExample 2. Consider the following closed program P x ~ gauss(0,2); y ~ gauss(-1,1); z ← x + y\nwhere z is the return variable. Using the encoding in Figure 2, we compute the pair 〈ϕP ,D〉 P , where ϕP , z = x+ y and D = {px, py}, where px and py are the PDFs of the two distributions from which values of x and y are drawn.\nSuppose that we would like to compute the probability that z is positive when the program terminates: Pr[z > 0]. Then, we can compute the following weighted volume: VOL(∃z. ϕP ∧ z > 0,D), which is equal to 0.32736."
    }, {
      "heading" : "3.3 Probabilistic verification problems",
      "text" : "We now define probabilistic verification problems and present an abstract verification algorithm that assumes the existence of an oracle for weighted volume computation. Verification problems A verification problem is a triple (Ppre,Pdec, post), where – Ppre, called the probabilistic precondition, is a closed program over variables V pre and output variables vpreo . – Pdec, called the decision-making program, is an open program over variables V dec; its input arguments are vdeci , with |vdeci | = |vpreo |; and its output variables are vdeco . (We assume that V pre ∩ V dec = ∅.) – post is a probabilistic postcondition, which is a Boolean expression over probabilities of program outcomes. Specifically, post is defined as follows:\npost ∈ PExp := Prob > Prob | PExp ∨ PExp | PExp ∧ PExp | ¬PExp\nProb := Pr[ϕ] | Pr[ϕ | ϕ] | c | Prob Prob ∈ {+,−,÷,×} c ∈ R\nwhere ϕ ∈ LT are formulas over input and output variables of Pdec. For example, post might be of the form\nPr[x > 0] > 0.5 ∧ Pr[y + z > 7] > Pr[t > 5]\nThe goal of verification is to prove that post is true for the program Pdec ◦ Ppre, i.e., the composition of the two programs where we first run Ppre to generate an input for Pdec. Since Ppre is closed, the program Pdec ◦ Ppre is also closed. Verification algorithm We now describe an idealized verification algorithm that assumes the existence of an oracle for weighted volume computation. The algorithm, VERIFY, shown in Figure 3, takes a verification problem and returns whether the probabilistic postcondition holds.\nVERIFY begins by encoding the composition of the two programs,Pdec◦Ppre, as the pair 〈ϕP ,D〉 and adds the constraint vdeci = v pre o to connect the outputs of Ppre to the inputs of Pdec (recall the example from Sec. 2 for an illustration). For each term of the form Pr[ϕ] appearing in post , the algorithm computes its numerical value and maintains it in a map m. If m satisfies the post—i.e., by replacing all terms Pr[ϕ] with their values in m—then the postcondition holds."
    }, {
      "heading" : "4. Symbolic Weighted Volume Computation",
      "text" : "In this section, we describe our weighted volume computation algorithm. Recall that, given a formula ϕ and a set D defining the PDFs of the distributions of free variables, our goal is to evaluate the integral ∫ ϕ ∏ xi∈Xϕ pi(xi) dXϕ. Existing techniques In general, there is no systematic technique for computing an exact value for such an integral. Moreover, even simpler linear versions of the volume computation problem, not involving probability distributions, are #P-hard [39]. Existing techniques suffer from one or more of the following: they (i) restrict ϕ to a conjunction of\n7 2017/3/8\nlinear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict ϕ to bounded regions of Rn [27, 28], or (v) have no convergence guarantees, e.g., computer algebra tools that find closed-form solutions [2, 3, 44]. (See Sec. 6 for details.) Symbolic weighted volume computation Our approach is novel in its generality and its algorithmic core. The following are the high-level properties of our algorithm:\n1. Our approach accepts formulas in the decidable yet rich theory of real closed fields: Boolean combinations of polynomial inequalities.\n2. Our approach imposes no restrictions on the form of the PDFs, only that we can evaluate the cumulative distribution functions2 (CDFs) associated with the PDFs in D.\n3. Our approach is guaranteed to converge to the exact value of the weighted volume in the limit, allowing us to produce a sound and complete verification procedure.\nAt the algorithmic level, our approach makes the following contributions:\n1. Our approach exploits the power of SMT solvers and uses them as a black box, allowing it to directly benefit from future advances in solver technology.\n2. Our approach employs the idea of dividing the space into rectangular regions that are easy to integrate over. While this age-old idea has been employed in various guises in verification [9, 21, 53, 66], we utilize it in a new symbolic way to enable volume computation over SMT formulas.\n3. Our approach introduces a novel technique for approximately encoding PDFs as formulas, and using them to guide the SMT solver towards making large leaps to the exact solution."
    }, {
      "heading" : "4.1 Weighted Volume Computation Algorithm",
      "text" : "To compute the integral over the region ϕ, we exploit the observation that if ϕ is a hyperrectangular region, i.e., an ndimensional rectangle in Rn, then we can evaluate the integral, because each dimension has constant lower and upper bounds. For instance, consider the following formula representing a rectangle in R2:\nϕ ≡ 0 6 x1 6 100 ∧ 4 6 x2 6 10\nThe following holds: ∫ ϕ p1(x1)p2(x2) dx1dx2\n=( ∫ 100\n0 p1(x1) dx1)( ∫ 10 4 p2(x2) dx2)\n=(F1(10)− F1(4))(F2(100)− F2(0))\nwhere Fi is the CDF of pi(xi), which is Fi(x) = ∫ x −∞ pi(t) dt. That is, we independently compute the integral along each dimension of the rectangle, and take the product. This holds since we assume all variables are independently sampled. Our algorithm is primarily composed of two steps: First, the hyperrectangular decomposition phase represents the formula ϕ as a set of hyperrectangles. Note that this set is likely to be infinite. Thus, we present a technique for defining all hyperrectangles that lie in ϕ symbolically as a formula ϕ, where each model of ϕ corresponds to a hyperrectangle 2 The cumulative distribution function of a real-valued random variable X is the function f : R→ R, such that f(x) = Pr[X 6 x].\n8 2017/3/8\nthat lies inside the region ϕ. Second, after characterizing the set ϕ of all hyperrectangles in ϕ, we can iteratively sample hyperrectangles in ϕ, which can be done using an off-the-shelf SMT solver to find models of ϕ. For each hyperrectangle we sample, we compute its weighted volume and add it to our current solution. Therefore, the current solution maintained by the algorithm is the weighted volume of an underapproximation of ϕ—that is, a lower bound on the exact weighted volume of ϕ. Hyperrectangular decomposition We begin by defining hyperrectangles as special formulas.\nDefinition 1 (Hyperrectangles and their weighted volume). A formula H ∈ LT is a hyperrectangle if it can be written in the form ∧ x∈XH cx 6 x 6 c ′ x where cx, c ′ x ∈ R are the lower and upper bounds of dimension x. We use Hl(x) and Hu(x) to denote the lower and upper bounds of x in H . The weighted volume of H , given a set D, is as follows:\nVOL(H,D) = ∏xi∈XH ∫Hu(xi)Hl(xi) pi(xi) dxi Ideally, we would take a formula ϕ and rewrite it as a disjunction of hyperrectangles ∨ H , but this disjunction is most likely an infinite one. To see why this is the case, consider the simple formula representing a triangular polytope in Figure 5(a). Here, there is no finite number of rectangles whose union is the full region in R2 enclosed by the triangle.\nWhile the number of hyperrectangles enclosed in ϕ is infinite, we can characterize them symbolically using universal quantifiers, as shown by Li et al. [53]. Specifically, we define the hyperrectangular decomposition of ϕ as follows:\nDefinition 2 (Hyperrectangular decomposition). Given a formula ϕ, its hyperrectangular decomposition ϕ is: ϕ ≡ ( ∧\nx∈Xϕ\nlx 6 ux ) ∧ ∀Xϕ. ( ∧ x∈Xϕ lx 6 x 6 ux ) ⇒ ϕ\nwhere lx, ux are fresh free variables introduced for each x ∈ Xϕ, and ∀Xϕ is short for ∀x1, . . . , xn, for xi ∈ Xϕ. Given a model m |= ϕ, we say that Hm is the hyperrectangle induced by m, as defined below:\nHm ≡ ∧ x∈Xϕ m(lx) 6 x 6 m(ux)\nIntuitively, ϕ characterizes every possible hyperrectangle that is subsumed by ϕ. The idea is that the hyperrectangle Hm induced by each model m of ϕ is subsumed by ϕ, that is, Hm ⇒ ϕ. The following example illustrates this process. Example 3. Consider the formula ϕ ≡ x > y ∧ y > 0, illustrated in Figure 6 as a gray, unbounded polyhedron. The formula ϕ, after eliminating the universal quantifier, is:\nlx 6 ux ∧ ly 6 uy ∧ ly > 0 ∧ lx > uy\nFigure 6 shows two models m1,m2 |= ϕ and their graphical representation as rectangles Hm1 , Hm2 in R2. Observe that both rectangles are subsumed by ϕ.\nHyperrectangle sampling Our symbolic weighted volume computation algorithm, SYMVOL, is shown in Figure 4 as two transition rules. Given a pair (ϕ,D), the algorithm maintains a state consisting of two variables: (i) vol , the current lower bound of the weighted volume, and (ii) Ψ, a constraint that encodes the remaining rectangles in the hyperrectangular decomposition of ϕ.\n9 2017/3/8\nThe algorithm is presented as guarded rules. Initially, using the rule HDECOMP, vol is set to 0 and Ψ is set to ϕ. The algorithm then proceeds by iteratively applying the rule HSAMPLE. Informally, the rule HSAMPLE is used to find arbitrary hyperrectangles in ϕ and compute their weighted volume. Specifically, HSAMPLE finds a model m of Ψ, computes the weighted volume of the hyperrectangle Hm induced by m, and adds the result to vol .\nTo maintain soundness, HSAMPLE ensures that it never samples two overlapping hyperrectangles, as otherwise we would overapproximate the volume. To do so, every time a hyperrectangle Hm is sampled, we conjoin an additional constraint to Ψ—denoted block(Hm) and defined in Figure 4—that ensures that for all models m′ |= Ψ, Hm′ does not overlap with Hm, i.e., Hm\n′ ∧Hm is unsatisfiable. Informally, the block(Hm) constraint specifies that any newly sampled hyperrectangle should be to the left or right of Hm for at least one of the dimensions. Lower and upper bounds The following theorem states the soundness of SYMVOL: it maintains a lower bound on the exact weighted volume.\nTheorem 1 (Soundness of SYMVOL). The following is an invariant of SYMVOL(ϕ,D): vol 6 VOL(ϕ,D). It follows from the above theorem that we can use SYMVOL to compute an upper bound on the exact volume. Specifically, because we are integrating over PDFs, we know that VOL(ϕ,D) + VOL(¬ϕ,D) = 1. Therefore, by using SYMVOL to compute the weighted volume of ¬ϕ, we get an upper bound on the exact volume of ϕ. Corollary 1 (Computing upper bounds). The following is an invariant of SYMVOL(¬ϕ,D): 1− vol > VOL(ϕ,D)"
    }, {
      "heading" : "4.2 Density-directed Sampling",
      "text" : "While the SYMVOL algorithm is sound, it provides no progress guarantees. Consider, for example, a run that only samples unit hyperrectangles, i.e., points in Rn; the volume of a point is 0, therefore, we will never compute any volume. Alternatively, the algorithm might diverge by sampling hyperrectangles in ϕ that appear in very low probability density regions. These two scenarios are illustrated in Figure 5(b) on a triangular polytope in R2.\nIdeally, the rule HSAMPLE would always find a model m yielding the hyperrectangle Hm with the largest weighted volume. Finding such a model amounts to solving the optimization problem:\narg max m|=Ψ\n∏ xi∈Xϕ ∫Hmu (xi) Hml (xi) pi(xi) dxi\nFrom a practical perspective, there are no known tools or techniques for finding models of first-order formulas that maximize such complex objective functions, with integrals over arbitrary probability density functions.\nHowever, we make the key observation that if p(x) is a step function—i.e., piecewise constant—then we can symbolically encode the integral ∫ p(x) dx in linear arithmetic. As such, we propose to (i) approximate each density function p(x)\nwith a step function step(x), (ii) encode the integrals ∫ step(x) dx as linear arithmetic formulas, and (iii) direct sampling towards hyperrectangles that maximize these integrals, thus finding hyperrectangles of large volume.\n10 2017/3/8\nApproximate density functions and linear encodings We begin by defining approximate density functions (ADFs).\nDefinition 3 (Approximate density functions). An approximate density function step(x) is of the following form:\nstep(x) = { ci, x ∈ [ai, bi) for 1 6 i 6 n 0, otherwise\nwhere ci, ai, bi ∈ R, ci > 0, and all [ai, bi) are disjoint. We now show how to encode a formula stepφ(x) over the free variables δx, lx, ux, where for any modelm |= stepφ(x),\nthe value m(δx) is the area under step(x) between m(lx) and m(ux), i.e.: m(δx) = ∫m(ux) m(lx)\nstep(x) dx. Intuitively, the value of this integral is the sum of the areas of each bar in step(x), restricted to[m(lx),m(ux)].\nDefinition 4 (Encoding area under an ADF). Given an ADF step(x), we define stepφ(x) as follows: stepφ(x) ≡ δx = ∑n i=1 ci · ∣∣[ai, bi) ∩ [lx, ux]∣∣ The constraint stepφ(x) is directly expressible in linear arithmetic, since∣∣[ai, bi) ∩ [lx, ux]∣∣ = max(min(bi, ux)−max(ai, lx), 0)\nThe finite sum in stepφ(x) computes the size of the intersection of [lx, ux] with each interval [ai, bi) in step(x), and multiplies the intersection with ci, the value of the step in that interval. ADF-directed Volume Computation We now present the algorithm ADF-SYMVOL (Figure 8), an extension of our volume computation algorithm SYMVOL that uses ADFs to steer the sampling process. The ADFs are only used for guiding the rule HSAMPLE towards dense hyperrectangles, and thus do not affect soundness of the volume computation. For example, Figure 7 shows three approximations of a Gaussian; all three are valid approximations. In Section 5, we discuss the impact of different ADFs on performance.\nFormally, we create a set of ADFs A = {step1, . . . , stepn}, where, for each variable xi ∈ Xϕ, we associate the ADF stepi(xi). The rule HSAMPLE now encodes step φ i (xi) and attempts to find a hyperrectangle such that for each dimension x, δx is greater than some lower bound lb, which is initialized to 1. Of course, we need to keep reducing the value lb as we run out of hyperrectangles of a given volume. Therefore, the rule DECAY is used to shrink lb using a fixed decay rate λ ∈ (0, 1) and can be applied when HSAMPLE fails to find a sufficiently large hyperrectangle.\nNote that, ideally, we would look for a model m such that ∏ x∈Xϕ δx is maximized, thus, finding the hyperrectangle with the largest weighted volume with respect to the ADFs. However, this constraint is non-linear. To lower the complexity of the problem to that of linear arithmetic, we set a decaying lower bound and attempt to find a model where each δx is greater than the lower bound."
    }, {
      "heading" : "4.3 Convergence of ADF-SYMVOL",
      "text" : "We now discuss the convergence properties of ADF-SYMVOL. Suppose we are given a formula ϕ, a set D, and a setA. Let R ⊂ Rn be the region where all the ADFs in A are non-zero. We will show that ADF-SYMVOL converges, in the limit, to the exact weighted volume restricted to R; that is, ADF-SYMVOL converges to ∫ ϕ∩R ∏ xi∈Xϕ pi(xi) dXϕ.\nNote the fascinating part here is that we do not impose any restrictions on the ADFs: they do not have to have any correspondence with the PDFs they approximate; they need only be step functions. Of course, in practice, the quality of the approximation dictates the rate of convergence, but we delay this discussion to Section 5.\nThe following theorem states convergence of ADF-SYMVOL; it assumes that HSAMPLE is applied iteratively and DECAY is only applied when HSAMPLE cannot find a model.\n11 2017/3/8\nTheorem 2 (Convergence to R). Assume ADF-SYMVOL is run on (ϕ,D) and a set of ADFs A that are non-zero for R ⊂ Rn. Let vol i be the value of vol after i applications of HSAMPLE. Then, limi→∞ vol i = ∫ ϕ∩R ∏ xi∈Xϕ pi(xi) dXϕ.\nNote that the above theorem directly gives us a way to approach the exact volume. Specifically, by performing runs of ADF-SYMVOL on subsets in an infinite partition of Rn induced by the ADFs, we can ensure that the sum over the ADF-SYMVOL processes approaches the exact volume in the limit. For all i, let Ai be a set of ADFs corresponding to an ADF-SYMVOL process Pi, where Ri ⊂ Rn is the non-zero region of Ai. We require an infinite set of Pi to partition Rn: (i) for all i 6= j, Ri ∩Rj = ∅, and (ii) ⋃∞ i=1Ri = R n. The following theorem formalizes the argument:\nTheorem 3 (Convergence). Let P1, P2, . . . be ADF-SYMVOL processes that partition Rn. Assume a fair serialization where each Pi performs HSAMPLE infinitely often, and let voln be the total computed volume across all Pi after n successful calls to HSAMPLE. Then, limn→∞ voln = VOL(ϕ,D)."
    }, {
      "heading" : "5. Implementation and Evaluation",
      "text" : "In this section, we present a case study on the bias of decision-making programs. In particular, we verify the group fairness property with qualification, which requires that a program P satisfies the condition\nPr[P(v) = true | min(v) = true ∧ qual(v) = true] Pr[P(v) = true | min(v) = false ∧ qual(v) = true] > 0.85\nWe implement our algorithms in a tool called FairSquare and evaluate its ability to prove or disprove the property for an assortment of programs; furthermore, we evaluate the effect of various parameters on the performance of the implementation, and we compare the applicability of FairSquare to other probabilistic inference tools."
    }, {
      "heading" : "5.1 Implementation",
      "text" : "We implemented our presented algorithms in a new tool called FairSquare, which employs Z3 [35] for SMT solving and Redlog [4] for quantifier elimination. To compute the group fairness ratio, we decompose the conditional probabilities into four joint probabilities: Pr[P(v) ∧ min(v) ∧ qual(v)], Pr[P(v) ∧ ¬min(v) ∧ qual(v)], Pr[min(v) ∧ qual(v)], and Pr[¬min(v) ∧ qual(v)]. FairSquare computes lower bounds for eight quantities: the weighted volume of each of these probabilities and also their negations, since a lower bound on the negation provides an upper bound on the positive form. A round of sampling involves (i) obtaining a sample (hyperrectangle) for each of these eight quantities, (ii) computing these samples’ weighted volumes, (iii) updating the bounds on the group fairness ratio, and (iv) checking if bounds are precise enough to conclude fairness or unfairness. Rounds of sampling are performed until a proof is found or a timeout is reached. Sample maximization A key optimization is the maximization of hyperrectangles obtained during sampling. We use Z3’s optimization capability to maximize and minimize the finite bounds of all hyperrectangles, while still satisfying the formula Ψ (in Figures 4 and 8). This process is performed greedily by extending a hyperrectangle in one dimension at a time to find a maximal hyperrectangle. If a dimension extends to infinity, then we drop that bound, thus resulting in an unbounded hyperrectangle."
    }, {
      "heading" : "5.2 Benchmarks",
      "text" : "We trained a variety of machine-learning models on a popular income dataset [1] used in related research on algorithmic fairness [22, 41, 72] to predict whether a person has a low or high income; suppose, for example, these programs would be used to determine the salary of a new employee: high (> $50,000) or low. We would like to verify whether salary decisions are fair to qualified female employees. Using the Weka machine learning suite [5], we learned 11 different decision-making programs (see, e.g., Bishop’s textbook [20] for background) which are listed in Figure 9:\n– Four decision trees, named DTn, where n is the number of conditionals in the program. The number of variables and the depth of the tree each varies from 2 to 3.\n– Four support vector machines with linear kernels, named SVMn, where n is the number of variables in the linear separator.\n– Three neural networks using rectified linear units [60], named NNn,m, where n is the number of input variables, and m is the number of nodes in the single hidden layer.\nAs we will show in the next section, some of these programs do not satisfy group fairness. We introduced modifications of DT16 and SVM4, called DTα16 and SVM α 4 , that implement rudimentary forms of affirmative action for female applicants. For DTα16, there is a 15% chance it will flip a decision to give the low salary; for SVM α 4 , the linear separator is moved to increase the likelihood of hiring.\n12 2017/3/8\nAdditionally, we used three different probabilistic population models, programs that define the probabilistic inputs, that were inferred from the same dataset: (i) a set of independently distributed variables, (ii) a Bayesian network using a simple graph structure, and (iii) the same Bayesian network, but with an integrity constraint in the form of an inequality between two of the variables. Note that the first model is a trivial case: since there is there is no dependence between variables, all programs will be fair; this simplicity serves well as a baseline for our evaluation. The Bayesian models permit correlations between the variables, allowing for more subtle sources of fairness or unfairness. The benchmarks we use are derived from each combination of population models with decision-making programs (see Appendix C for an example)."
    }, {
      "heading" : "5.3 Evaluation",
      "text" : "In this section, we discuss the ability of FairSquare to verify qualified group fairness or unfairness for 39 problems, as summarized in Figure 9. In these problems, min(v) = true when the applicant is female, and qual(v) = true in two different scenarios: first, we consider the case when qual is tautologically true, and second, when the applicant is at least 18 years of age. Figure 9 shows only the former case, as while the numbers are different for the two cases, the qualitative results are quite similar. See Appendix D for the full table of results. We fix = 0.15. To guide volume computation, all Gaussian distributions with mean µ and variance σ2 use ADFs with 5 equal-width steps spanning (µ − 3σ2, µ + 3σ2)— analogous to Figure 7(a). FairSquare was able to solve 32 of the 39 problems, proving 21 fair and 11 unfair, as shown in the left table of Figure 9.\nConsider the results for DT4: FairSquare proved it fair with respect to the independent population model after 0.5 seconds of an initial quantifier elimination procedure and 1.3 seconds of the actual volume computation algorithm, which required 14 SMT queries. The more sophisticated Bayesian network models took longer for sampling, but due to the correlations between variables, were proved unfair.\nIn contrast, consider the results for DT44 under the Bayes Net 1 population model: FairSquare was unable to conclude fairness or unfairness after 900 seconds of volume computation (denoted by TO in the Vol column). The lower and upper bounds of the fairness ratio it had computed at that time are listed in the Res column: in this case, the value of the fairness ratio is within [0.70, 0.88], which is not precise enough for the = 0.15 requirement (but would be precise enough for outside of [0.12, 0.30]).\nIn general, all conclusive results using the independent population model were proved to be fair, as expected, but many are unfair with respect to the clusters and Bayes net models because of the correlations they capture. This difference illustrates the sensitivity of fairness to the population model; in particular, none of the decision trees syntactically access sex, yet several are unfair.\nFigure 9 also shows that the modifications for affirmative action in DTα16 and SVM α 4 are sufficient to make the programs\nfair with respect to all of our population models without making a substantial impact on the training set accuracy. In summary, FairSquare is powerful enough to reason about group fairness for many non-trivial programs."
    }, {
      "heading" : "5.4 Comparison to other tools",
      "text" : "We ran our benchmarks on the two other exact probabilistic inference tools that accept the same class of problems, and report these results in the right bar graph in Figure 9. First, we compare to Sankaranarayanan et al.’s tool [66] (VC), which\n13 2017/3/8\nis algorithmically similar to our tool: it finds bounds for probabilities on individual paths by approximating formulas with bounding and inscribed hyperrectangles. Second, we compare to PSI [44], which symbolically computes representations of the posterior distributions of variables. When closed form solutions to the CDFs exist, PSI is a great aid because these solutions can be evaluated to obtain exact values of probabilities instantly; however, the graph illustrates that our benchmarks are often too complex, resulting in integrals that do not have closed forms, or problems that PSI cannot solve within the timeout period.\nTools were deemed to have failed on a benchmark when they timed out after a 900s period. Additionally, the bounds on probability quantities from VC were often not tight enough for a proof. See Appendix D for the full quantitative table of results.\nThe figure illustrates some qualitative properties of the applicability of the tools. In general, all can solve most of the decision trees because the trees partition the decision space using a number of inequalities between a single variable and a constant. However, the presence of inequalities involving multiple variables can result in two phenomena: (i) the lack of closed form posterior CDFs, as reflected in the output of PSI, and (ii) angled boundaries in the decision space that are hard to approximate with hyperrectangles. These inequalities occur in the SVMs, neural networks, and the Bayes Net 2 population model. Consequently, VC fails to produce good bounds in these cases; FairSquare performs better, but also fails on the largest SVM."
    }, {
      "heading" : "5.5 Effect of Parameters",
      "text" : "We now explore the effects of the approximate density functions (see Section 4.2) and of the sample maximization optimizations. These results are captured in Figures 10 and 11.\nThere are three instances of ADFs in Figure 10 used to guide the sampling to high-probability regions: (i) none indicates that no ADF is used, i.e., we used SYMVOL instead of ADF-SYMVOL; (ii) uniform indicates that each gauss(µ, σ2) is approximated by a uniform function spanning (µ − 3σ2, µ + 3σ2) (similar to Figure 7(c)); and (iii) 5-step indicates that each Gaussian is approximated by a step function of 5 equal-width regions spanning that same domain (similar to Figure 7(a)). Another variable, max or nomax, denotes whether the sample maximization optimization is enabled (as described in Section 5.1).\nEach combination of these techniques is run on two of our benchmarks: DT16 and SVM4 under the independent population model. Figure 10(a) and (c) show how convergence to the fairness ratio is improved by the choice of distribution approximation when sample maximization is not employed: in particular, the runs using uniform and none are not even visible, as the bounds never fall within [0.01, 4.0]. Plots (b) and (d) show that when sample maximization is employed, the choice between the uniform and 5-step approximations is not as substantial on these benchmarks, although (i) the better approximation gets better bounds faster, and (ii) using none results in substantially worse bounds.\n14 2017/3/8\nFigure 11 plot (a) and (c) show that employing ADFs and using sample maximization each increases the average weighted volume per sample, which allows volume computation to be done with fewer samples. Plots (b) and (d) illustrate the trade-off: the average time per round of sampling tends to be greater for more sophisticated optimizations.\nWe present these results for two particular problems and observe the same results across our suite. In summary, we have found that ADFs and sample maximization are both necessary for adequate performance of FairSquare."
    }, {
      "heading" : "6. Related Work",
      "text" : "Probabilistic program analysis We refer the reader to Gordon et al. for a thorough survey [47]. A number of works tackled analysis of probabilistic programs from an abstract interpretation perspective [29, 54, 57–59]. The comparison between our solution through volume computation and abstract interpretation is perhaps analogous to SMT solving and software model checking versus abstract interpretation. For example, techniques like Monniaux’s [57], sacrifice precision (through joins, abstraction, etc.) of the analysis for the benefit of efficiency. Our approach, on the other hand, is aimed at eventually producing a proof, or iteratively improving probability bounds while guaranteeing convergence.\nA number of works have also addressed probabilistic analysis through symbolic execution [42, 45, 65, 66]. Filieri et al. [42] and Geldenhuys et al. [45] attempt to find the probability a safety invariant is preserved. Both methods reduce to a weighted model counting approach and are thus effectively restricted to variables over finite domains. Note that our technique is more general than a model counting approach, as we can handle the discrete cases with a proper encoding of the variables into a continuous domain.\nA number of works rely on sampling to approximate the probability of a given program property. The Church [46] programming language, for instance, employs the Metropolis–Hastings algorithm [26], a Markov Chain Monte Carlo (MCMC) technique. Other techniques perform probabilistic inference by compiling programs or program paths to Bayesian networks [51] and applying belief propagation [56] or sampling [65] on the network. Volume computation The computation of weighted volume is known to be hard—even for a polytope, volume computation is #P-hard [49]. Two general approaches exist: approximate and exact solutions. Note that in general, any approximate technique at best can prove facts with high probability.\nOur volume computation algorithm is inspired by (i) Li et al.’s [53] formula decomposition procedure, where quantifier elimination is used to under-approximate an LRA constraint as a Boolean combination of monadic predicates; and (ii) Sankaranarayanan et al.’s [66] technique for bounding the weighted volume of a polyhedron, which is the closest volume computation work to ours. (The general technique of approximating complex regions with unions of orthogonal polyhedra is well-studied in hybrid systems literature [21].)\nA number of factors differentiate our work from [66], which we compared with experimentally in Section 5. First, our approach is more general, in that it can operate on Boolean formulas over linear and polynomial inequalities, as opposed to just conjunctions of linear inequalities. Second, our approach employs approximate distributions to guide the sampling of hyperrectangles with large volume, which, as we have demonstrated experimentally, is a crucial feature of our approach. Third, we provide theoretical convergence guarantees.\n15 2017/3/8\nLattE is a tool that performs exact integration of polynomial functions over polytopes [34]. Belle et al. [16, 18] compute the volume of a linear real arithmetic (LRA) formula by, effectively, decomposing it into DNF—a set of polyhedra—and using LattE to compute the volume of each polyhedron with respect to piece-wise polynomial densities. Our volume computation algorithm is more general in that it (i) handles formulas over real closed fields, which subsumes LRA, and (ii) handles arbitrary probability distributions.\nChistikov et al. [27, 28] present a framework for approximate counting with probabilistic guarantees in SMT theories, which they specialize for bounded LRA. In contrast, our technique (i) handles unbounded formulas in LRA as well as real closed fields, (ii) handles arbitrary distributions, and (iii) provides converging lower-bound guarantees. It is important to note that there is a also a rich body of work investigating randomized polynomial algorithms for approximating the volume of a polytope, beginning with Dyer et al.’s seminal work [38] (see Vempala [70] for a survey). Algorithmic fairness Our work is inspired by recent concern in the fairness of modern decision-making programs [12, 71]. A number of recent works have explored algorithmic fairness [22, 32, 33, 37, 41, 48, 61, 72]. For instance, Zemel et al. [72] and Feldman et al. [41] study fairness from a machine learning classification perspective, e.g., automatically learning fair classifiers. Both works operate with a notion of fairness on a provided data set; in contrast, we prove fairness with respect to a given probabilistic model of the population (a data set can be viewed as a special case).\nDiscrimination in black-box systems has been studied through the lens of statistical analysis [32, 33, 67]. Notably, Datta et al. [32] created an automated tool that analyzes online advertising: it operates dynamically by surveying the ads produced by Google. Our approach differs from the statistical analyses in that we require transparency of the decision procedure, instead of data on the results of the decision procedure. Future work We presented a new technique for verification of probabilistic programs and applied to quantifying bias of decision-making programs. Our goals for future work include exploring the applicability of our technique to other notions of fairness properties (e.g., [37]); further refinements to the use of ADFs, e.g. dynamically adapting them during ADF-SYMVOL; and improving the weighted volume computation algorithm for better scalability."
    }, {
      "heading" : "A. Operational Semantics",
      "text" : "Operational semantics We now define the operational semantics of our program model. Typically, the state s : V → R of the program is defined as a valuation function from variables in V to values in R. In a probabilistic setting, however, we need to maintain an additional state that dictates values drawn from probability distributions. Following standard semantics of probabilistic programs [52], we assume a finite sequence of independent and identically distributed (iid) random variables. The semantics of an execution is thus defined for a fixed sequence of values ω of these variables. Informally, it is as if we performed all sampling before the program executes and stored the results in a sequence for use whenever we encounter a probabilistic assignment. For simplicity of exposition, we assume all probabilistic assignments sample from the same distribution; otherwise, multiple sequences can be used, one per distribution.\nWe can now define the semantics of P as shown in Figure 12. We use s(e) to denote the value of expression e under a given state s, and use ε to denote the empty statement sequence. We use the substitution notation s[x 7→ c] to denote the state s but with x mapped to c ∈ R. The interesting rule is PASSIGN, which, given a probabilistic assignment x ∼ p, for some variable x and distribution p, picks and removes the first element c of the sequence c : ω to update the value of the variable x.\nWe assume no variable is used before being assigned to. In a closed program, we assume all variables are initially assigned to 0. Given a closed program P , we define JPKω as the final state s reachable from executing P using the sequence ω, as defined by the relation→ in Figure 12."
    }, {
      "heading" : "B. Proofs",
      "text" : "In this section, we prove correctness of the various pieces of our algorithm. First, we introduce preliminary theorems.\nThe following theorem states the soundness and completeness of hyperrectangular decomposition: models of ϕ characterize all hyperrectangles in ϕ and no others. Theorem 4 (Correctness of ). Let ϕ ∈ LT . Soundness: Let m |= ϕ. Then, Hm ⇒ ϕ is valid. Completeness: Let H be a hyperrectangle such that H ⇒ ϕ. Then, the following is satisfiable: ϕ ∧∧x∈Xϕ lx = Hl(x) ∧ ux = Hu(x)\nThe following theorem states the correctness of block : it removes all hyperrectangles that overlap withHm (soundness), and it does not overconstrain Ψ by removing hyperrectangles that do not overlap with Hm (completeness).\nTheorem 5 (Correctness of block ). Given ϕ, let Ψ⇒ ϕ, and letm1,m2 |= Ψ. Soundness: IfHm1 ∧Hm2 is satisfiable, then m2 6|= Ψ ∧ block(Hm1). Completeness: If Hm1 ∧Hm2 is unsatisfiable, then m2 |= Ψ ∧ block(Hm1).\nThe following theorem states the correctness of the ADF encoding:\nTheorem 6 (Correctness of stepφ). Fix an ADF step(x). Soundness: For any model m |= stepφ, the following is true: m(δx) = ∫m(ux) m(lx) step(x) dx. Completeness: For any constants a, b, c ∈ R such that c = ∫ b a step(x) dx, the following formula is satisfiable: δx = c ∧ lx = a ∧ ux = b ∧ stepφ(x).\n19 2017/3/8\nProof of Theorem 4\nSoundness Suppose m |= ϕ. By definition of ϕ, the following formula is valid∧ x∈Xϕ m(lx) 6 x 6 m(ux)⇒ ϕ\nTherefore Hm ⇒ ϕ, by definition of Hm. Completeness Let H be a hyperrectangle such that H ⇒ ϕ. By definition, H is of the form ∧x∈Xϕ cx 6 x 6 c′x. It immediately follows that the model where lx = cx and ux = c′x, for every x ∈ Xϕ, satisfies ϕ, since c′x > cx (satisfying the first conjunct of ϕ), and ∀Xϕ. H ⇒ ϕ (satisfying the second conjunct of ϕ). Proof of Theorem 5\nSoundness Suppose Hm1 ∧ Hm2 is satisfiable. By definition of a hyperrectangle, this means that for all variables x ∈ Xϕ, we have that the intervals [Hm1l (x), Hm1u (x)] and [Hm2l (x), Hm2u (x)] overlap, i.e., at least are equal on one of the extremes. Therefore, m2 6|= Ψ ∧ block(Hm1), since block does not admit any model m where, for all x ∈ Xϕ, [m(lx),m(ux)] overlaps with [Hm1l (x), H m1 u (x)]. Completeness Suppose Hm1 ∧Hm2 is unsatisfiable. By definition of a hyperrectangle, there is at least one x ∈ Xϕ where [Hm1l (x), H m1 u (x)] and [H m2 l (x), H m2 u (x)] do not overlap. Therefore, if m2 |= Ψ, then m2 |= Ψ ∧ block(Hm1), since block explicitly states that for at least one variable x, [m(lx),m(ux)] should not overlap with [Hm1l (x), H m1 u (x)].\nProof of Theorem 1 At any point in the execution, vol = ∑l i=1 ∫ Hi ∏ pi(xi) dxi, where l is the number of applications of HSAMPLE and Hi\nis the hyperrectangle sampled at step i. By definition, ∨ Hi ⇒ ϕ. Since PDFs are positive functions, vol 6 VOL(ϕ,D).\nProof of Corollary 1\nBy definition of PDFs and integration,∫ Rn ∏ pi(xi) dxi = ∫ ϕ ∏ pi(xi) dxi + ∫ ¬ϕ ∏ pi(xi) dxi\nfor any ϕ ⊆ Rn. From Theorem 1, it follows that at any point in the execution of SYMVOL(¬ϕ,D), we have 1 − vol > VOL(ϕ,D). Proof of Theorem 6\nSoundness Suppose m |= stepφ(x). Then,\nm(δx) = n∑ i=1 ci · ∣∣[ai, bi] ∩ [m(lx),m(ux)]∣∣\nBy definition of the area under a positive step function, we have\nm(δx) = ∫ m(ux) m(lx) step(x) dx\nCompleteness Completeness easily follows from correctness of the encoding of integrals over step functions as sums.\nProof of Theorem 2 The algorithm constructs two series in parallel: the actual volume computation series ∑ vi and the approximated series∑\nai, where each vi and ai correspond to the actual and approximate volume of the i’th sampled hyperrectangle (note that the latter is not explicitly maintained in the algorithm). Each series corresponds to a sequence of partial sums: Let\nvΣi = i∑ j=1 vj a Σ i = i∑ j=1 aj\n20 2017/3/8\nIt is maintained that\n∀i. vΣi 6 EVolR∩ϕ = ∫ R∩ϕ ∏ p(x) dXϕ\n∀i. aΣi 6 AVol = ∫ R∩ϕ ∏ step(x) dXϕ\nSuppose, for the sake of obtaining a contradiction, that our sequence of samples to construct {vΣi } and {aΣi } never, in the limit, samples some subregion R′ ( R, but H ⊆ R \\ R′ is a hypercube such that ∫ H ∏ p(x) dXϕ is non-zero. Then,\n{vΣi } approaches some limit that is at most vΣ 6 EVolR∩ϕ − ∫ H ∏ p(x) dXϕ. Let δ = ∫ H ∏ step(x) dXϕ. The sequence {aΣi } is monotonically increasing and bounded above by AVol: it follows that {aΣi } converges to some limit aΣ 6 AVol. By the definition of a limit, for all > 0, there exists N such that for all n > N , aΣ − aΣn < . So at some point when we have fixed a threshold τ < δ and have run out of samples in R′ with an > τ (guaranteed when aΣ − aΣn < τ ) we would sample H ⊆ R \\R′. This property ensures that in the limit, R′ → R, and vΣi → EVolR∩ϕ. Proof of Theorem 3\nMore formally, ADF-SYMVOL using each Pi and its Ai infinitely often in the fair serialization translates to the existence of a surjective function f : N → N such that (i) the nth successful call to HSAMPLE is in Pf(n) and (ii) for all n′ the preimage f−1(n′) is an infinite set.\nSince the domain (Ri) of each Ai is disjoint from the others, since ⋃∞ i=1Ri = R\nn, and since each Pi converges to the weighted volume over its domain, it follows that\n∞∑ i=1 ∞∑ j=1 VOL(Hij ,D) = VOL(ϕ,D)\nwhereHij is the jth hyperrectangle returned from Pi. (The nth hyperrectangle produced from the serialization corresponds to Hij with i = f(n) and j = |f−1(i) ∩ {n′ | n′ 6 n}|.)\nIn fact, since all the terms are non-negative, the series above converges absolutely, and any rearrangement converges to the same limit. A diagonalization argument permits a bijection b(i, j) = k such that this sum is equivalent to\n∞∑ k=1 VOL(Hk,D) = VOL(ϕ,D)\nThus, by composing rearrangements, any fair serialization also converges."
    }, {
      "heading" : "C. Example Programs",
      "text" : "As an example, below is the code for SVMα4 with the independent population model. Note that the parameters for Gaussians are mean and variance, and step functions are a list of tuples of the form (lower, upper, value). A decision-making program F is free to use variables defined in the population model popModel.\ndef popModel(): //independent age ~ gaussian(38.5816, 186.0614) sex ~ step([(0,1,0.3307), (1,2,0.6693)]) capital_gain ~ gaussian(1077.6488, 54542539.1784) capital_loss ~ gaussian(87.3038, 162376.9378) sensitiveAttribute(sex < 1) qualified(True)\ndef F(): //SVM4A N_age = (age - 17.0) / 62.0 N_capital_gain = capital_gain / 22040.0 N_capital_loss = capital_loss / 1258.0 t = 0.0006 * N_age\n+ -5.7363 * N_capital_gain + -0.0002 * N_capital_loss + 1.0003\nif sex > 1: t = t + -0.0003\n21 2017/3/8\nif sex < 1: //affirmative action t = t - 0.5 fairnessTarget(t < 0)\nBelow is the Bayes Net 2 population model (Bayes Net 1 is the same without last if statement): def popModel(): //Bayes Net 2\nsex ~ step([(0,1,0.3307), (1,2,0.6693)]) if sex < 1:\ncapital_gain ~ gaussian(568.4105, 24248365.5428) if capital_gain < 7298.0000:\nage ~ gaussian(38.4208, 184.9151) education_num ~ gaussian(10.0827, 6.5096) capital_loss ~ gaussian(86.5949, 157731.9553)\nelse: age ~ gaussian(38.8125, 193.4918) education_num ~ gaussian(10.1041, 6.1522) capital_loss ~ gaussian(117.8083, 252612.0300) else: capital_gain ~ gaussian(1329.3700, 69327473.1006) if capital_gain < 5178.0000:\nage ~ gaussian(38.6361, 187.2435) education_num ~ gaussian(10.0817, 6.4841) capital_loss ~ gaussian(87.0152, 161032.4157)\nelse: age ~ gaussian(38.2668, 187.2747) education_num ~ gaussian(10.0974, 7.1793) capital_loss ~ gaussian(101.7672, 189798.1926)\nif (education_num > age): age = education_num sensitiveAttribute(sex < 1) qualified(True)"
    }, {
      "heading" : "D. Quantitative results",
      "text" : "The full quantitative results of our experiments (Section 5) are shown in table 1; note that the upper-left quadrant of table 1 is simply the left table from figure 9. The additional rows are the instances of qualified group fairness where qualification is true if and only if age > 18 (as opposed to the tautologically true case for qualification). The comparison to other tools was run only on the trivial qualification, since the runs on FairSquare proved to be qualitatively very similar.\nThe second set of columns detail the results of running Sankaranarayanan et al.’s tool [66], VC. For the Res column, a green check mark indicates that it proved the program fair, and a red strike indicates that it proved the program unfair. Sometimes it would terminate with bounds that were not conclusive: the upper and lower bounds for the fairness ratio are shown in Res. T denotes the amount of time (s) the tool ran for; TO denotes that it timed out after 900s without producing a result.\nThe third set of columns detail the results of running PSI on the benchmarks. T denotes the amount of time (s) the tool ran for; again, TO denotes that it timed out after 900s without producing a result. The Res column depicts a check mark if PSI terminated with a closed form CDF, and an integral sign if it returned a function containing unevaluated integrals.\n22 2017/3/8\n23 2017/3/8"
    } ],
    "references" : [ {
      "title" : "Hiring by algorithm: predicting and preventing disparate impact",
      "author" : [ "I. Ajunwa", "S. Friedler", "C.E. Scheidegger", "S. Venkatasubramanian" ],
      "venue" : "Available at SSRN 2746078",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Machine bias: There’s software used across the country to predict future criminals",
      "author" : [ "J. Angwin", "J. Larson", "S. Mattu", "L. Kirchner" ],
      "venue" : "and it’s biased against blacks. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminalsentencing, May 2016. ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Approximate reachability analysis of piecewise-linear dynamical systems",
      "author" : [ "E. Asarin", "O. Bournez", "T. Dang", "O. Maler" ],
      "venue" : "International Workshop on Hybrid Systems: Computation and Control, pages 20–31. Springer",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Automated essary scoring with e-rater v.2",
      "author" : [ "Y. Attali", "J. Burstein" ],
      "venue" : "Journal of Technology, Learning, and Assessment,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Weakest-precondition of unstructured programs",
      "author" : [ "M. Barnett", "K.R.M. Leino" ],
      "venue" : "ACM SIGSOFT Software Engineering Notes, volume 31, pages 82–87. ACM",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Big data’s disparate impact",
      "author" : [ "S. Barocas", "A.D. Selbst" ],
      "venue" : "Available at SSRN 2477899",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "P",
      "author" : [ "G. Barthe" ],
      "venue" : "R. D’Argenio, and T. Rezk. Secure information flow by self-composition. In CSFW",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Proving differential privacy in hoare logic",
      "author" : [ "G. Barthe", "M. Gaboardi", "E.J.G. Arias", "J. Hsu", "C. Kunz", "P. Strub" ],
      "venue" : "IEEE 27th Computer Security Foundations Symposium, CSF 2014, Vienna, Austria, 19-22 July, 2014, pages 411–424",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Measuring neural net robustness with constraints",
      "author" : [ "O. Bastani", "Y. Ioannou", "L. Lampropoulos", "D. Vytiniotis", "A. Nori", "A. Criminisi" ],
      "venue" : "CoRR, abs/1605.07262",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "and G",
      "author" : [ "V. Belle", "A. Passerini" ],
      "venue" : "V. den Broeck. Probabilistic inference in hybrid domains by weighted model integration. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 2770–2776",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "G",
      "author" : [ "V. Belle" ],
      "venue" : "Van den Broeck, and A. Passerini. Hashing-based approximate probabilistic inference in hybrid domains. In Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence (UAI)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "G",
      "author" : [ "V. Belle" ],
      "venue" : "V. den Broeck, and A. Passerini. Component caching in hybrid domains with piecewise polynomial densities. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 3369–3375",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Predicting crime",
      "author" : [ "N. Berg" ],
      "venue" : "lapd-style. https://www.theguardian.com/cities/2014/jun/25/predicting-crime-lapd-losangeles-police-data-analysis-algorithm-minority-report, June 2014. ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Pattern recognition",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Machine Learning, 128",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Orthogonal Polyhedra: Representation and Computation",
      "author" : [ "O. Bournez", "O. Maler", "A. Pnueli" ],
      "venue" : "pages 46–60. Springer Berlin Heidelberg, Berlin, Heidelberg",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Three naive bayes approaches for discrimination-free classification",
      "author" : [ "T. Calders", "S. Verwer" ],
      "venue" : "Data Mining and Knowledge Discovery, 21(2):277–292",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Verifying quantitative reliability for programs that execute on unreliable hardware",
      "author" : [ "M. Carbin", "S. Misailovic", "M.C. Rinard" ],
      "venue" : "Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages & Applications, OOPSLA 2013, part of SPLASH 2013, Indianapolis, IN, USA, October 26-31, 2013, pages 33–52",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Proving programs robust",
      "author" : [ "S. Chaudhuri", "S. Gulwani", "R. Lublinerman", "S. Navidpour" ],
      "venue" : "Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering, ESEC/FSE ’11, pages 102–112, New York, NY, USA",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Bridging boolean and quantitative synthesis using smoothed proof search",
      "author" : [ "S. Chaudhuri", "M. Clochard", "A. Solar-Lezama" ],
      "venue" : "POPL, volume 49, pages 207–220. ACM",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Understanding the metropolis-hastings algorithm",
      "author" : [ "S. Chib", "E. Greenberg" ],
      "venue" : "The american statistician, 49(4):327–335",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Approximate counting in SMT and value estimation for probabilistic programs",
      "author" : [ "D. Chistikov", "R. Dimitrova", "R. Majumdar" ],
      "venue" : "CoRR, abs/1411.0659",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Approximate counting in SMT and value estimation for probabilistic programs",
      "author" : [ "D. Chistikov", "R. Dimitrova", "R. Majumdar" ],
      "venue" : "Tools and Algorithms for the Construction and Analysis of Systems - 21st International Conference, TACAS 2015, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2015, London, UK, April 11-18, 2015. Proceedings, pages 320–334",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bayesian inference using data flow analysis",
      "author" : [ "G. Claret", "S.K. Rajamani", "A.V. Nori", "A.D. Gordon", "J. Borgström" ],
      "venue" : "Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering, pages 92–102. ACM",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A tool for checking ansi-c programs",
      "author" : [ "E. Clarke", "D. Kroening", "F. Lerda" ],
      "venue" : "International Conference on Tools and Algorithms for the Construction and Analysis of Systems, pages 168–176. Springer",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Efficiently computing static single assignment form and the control dependence graph",
      "author" : [ "R. Cytron", "J. Ferrante", "B.K. Rosen", "M.N. Wegman", "F.K. Zadeck" ],
      "venue" : "ACM Transactions on Programming Languages and Systems (TOPLAS), 13(4):451–490",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Automated experiments on ad privacy settings",
      "author" : [ "A. Datta", "M.C. Tschantz", "A. Datta" ],
      "venue" : "Proceedings on Privacy Enhancing Technologies, 2015(1):92–112",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Algorithmic transparency via quantitative input influence",
      "author" : [ "A. Datta", "S. Sen", "Y. Zick" ],
      "venue" : "Proceedings of 37th IEEE Symposium on Security and Privacy",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Software for exact integration of polynomials over polyhedra",
      "author" : [ "J. De Loera", "B. Dutra", "M. Koeppe", "S. Moreinis", "G. Pinto", "J. Wu" ],
      "venue" : "ACM Communications in Computer Algebra, 45(3/4):169–172",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Z3: An efficient smt solver",
      "author" : [ "L. De Moura", "N. Bjørner" ],
      "venue" : "International conference on Tools and Algorithms for the Construction and Analysis of Systems, pages 337–340. Springer",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Differential privacy",
      "author" : [ "C. Dwork" ],
      "venue" : "Automata, languages and programming, pages 1–12. Springer",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fairness through awareness",
      "author" : [ "C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R.S. Zemel" ],
      "venue" : "Innovations in Theoretical Computer Science 2012, Cambridge, MA, USA, January 8-10, 2012, pages 214–226",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A random polynomial-time algorithm for approximating the volume of convex bodies",
      "author" : [ "M. Dyer", "A. Frieze", "R. Kannan" ],
      "venue" : "Journal of the ACM (JACM), 38(1):1–17",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "On the complexity of computing the volume of a polyhedron",
      "author" : [ "M.E. Dyer", "A.M. Frieze" ],
      "venue" : "SIAM Journal on Computing, 17(5): 967–974",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "The dangers of letting algorithms enforce policy",
      "author" : [ "V. Eubanks" ],
      "venue" : "http://www.slate.com/articles/technology/future_tense/ 2015/04/the_dangers_of_letting_algorithms_enforce_policy.html, April 2015. ",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Certifying and removing disparate impact",
      "author" : [ "M. Feldman", "S.A. Friedler", "J. Moeller", "C. Scheidegger", "S. Venkatasubramanian" ],
      "venue" : "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Australia, August 10-13, 2015, pages 259–268",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reliability analysis in symbolic pathfinder",
      "author" : [ "A. Filieri", "C.S. Păsăreanu", "W. Visser" ],
      "venue" : "Proceedings of the 2013 International Conference on Software Engineering, pages 622–631. IEEE Press",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the (im)possibility of fairness",
      "author" : [ "S.A. Friedler", "C. Scheidegger", "S. Venkatasubramanian" ],
      "venue" : "CoRR, abs/1609.07236",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Psi: Exact symbolic inference for probabilistic programs",
      "author" : [ "T. Gehr", "S. Misailovic", "M. Vechev" ],
      "venue" : "Computer aided verification. Springer",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Probabilistic symbolic execution",
      "author" : [ "J. Geldenhuys", "M.B. Dwyer", "W. Visser" ],
      "venue" : "Proceedings of the 2012 International Symposium on Software Testing and Analysis, pages 166–176. ACM",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Church: a language for generative models",
      "author" : [ "N.D. Goodman", "V.K. Mansinghka", "D.M. Roy", "K. Bonawitz", "J.B. Tenenbaum" ],
      "venue" : "UAI 2008, Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence, Helsinki, Finland, July 9-12, 2008, pages 220–229",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Probabilistic programming",
      "author" : [ "A.D. Gordon", "T.A. Henzinger", "A.V. Nori", "S.K. Rajamani" ],
      "venue" : "Proceedings of the on Future of Software Engineering, pages 167–181. ACM",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Equality of opportunity in supervised learning",
      "author" : [ "M. Hardt", "E. Price", "N. Srebro" ],
      "venue" : "CoRR, abs/1610.02413",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Complexity of polytope volume computation",
      "author" : [ "L. Khachiyan" ],
      "venue" : "Springer",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Who do you blame when an algorithm gets you fired? http://www.wired.co.uk/article/make-algorithmsaccountable, January 2016",
      "author" : [ "N. Kobie" ],
      "venue" : null,
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2016
    }, {
      "title" : "Probabilistic graphical models: principles and techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "MIT press",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Semantics of probabilistic programs",
      "author" : [ "D. Kozen" ],
      "venue" : "Journal of Computer and System Sciences, 22(3):328–350",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Management of time requirements in component-based systems",
      "author" : [ "Y. Li", "T.H. Tan", "M. Chechik" ],
      "venue" : "FM 2014: Formal Methods, pages 399–415. Springer",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Dynamic enforcement of knowledge-based security policies",
      "author" : [ "P. Mardziel", "S. Magill", "M. Hicks", "M. Srivatsa" ],
      "venue" : "Computer Security Foundations Symposium (CSF), 2011 IEEE 24th, pages 114–128. IEEE",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Can an algorithm hire better than a human? http://www.nytimes.com/2015/06/26/upshot/can-an-algorithmhire-better-than-a-human.html",
      "author" : [ "C.C. Miller" ],
      "venue" : "(Accessed on 06/18/2016)",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2015
    }, {
      "title" : "and D",
      "author" : [ "T. Minka", "J. Winn", "J. Guiver" ],
      "venue" : "Knowles. Infer.net 2.5. Microsoft Research Cambridge",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Abstract interpretation of probabilistic semantics",
      "author" : [ "D. Monniaux" ],
      "venue" : "Static Analysis, pages 322–339. Springer",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "An abstract monte-carlo method for the analysis of probabilistic programs",
      "author" : [ "D. Monniaux" ],
      "venue" : "ACM SIGPLAN Notices, volume 36, pages 93–101. ACM",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Backwards abstract interpretation of probabilistic programs",
      "author" : [ "D. Monniaux" ],
      "venue" : "Programming Languages and Systems, pages 367– 382. Springer",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Discrimination-aware data mining",
      "author" : [ "D. Pedreshi", "S. Ruggieri", "F. Turini" ],
      "venue" : "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 560–568. ACM",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Predictive policing: The role of crime forecasting in law enforcement operations",
      "author" : [ "W.L. Perry" ],
      "venue" : "Rand Corporation",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A theory of justice",
      "author" : [ "J. Rawls" ],
      "venue" : "Harvard university press",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Using t-closeness anonymity to control for non-discrimination",
      "author" : [ "S. Ruggieri" ],
      "venue" : "Transactions on Data Privacy, 7(2):99–129",
      "citeRegEx" : "64",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Expressing and verifying probabilistic assertions",
      "author" : [ "A. Sampson", "P. Panchekha", "T. Mytkowicz", "K.S. McKinley", "D. Grossman", "L. Ceze" ],
      "venue" : "ACM SIGPLAN Notices, volume 49, pages 112–122. ACM",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Static analysis for probabilistic programs: inferring whole program properties from finitely many paths",
      "author" : [ "S. Sankaranarayanan", "A. Chakarov", "S. Gulwani" ],
      "venue" : "ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’13, Seattle, WA, USA, June 16-19, 2013, pages 447–458",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Discrimination in online ad delivery",
      "author" : [ "L. Sweeney" ],
      "venue" : "Queue, 11(3):10",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An fda for algorithms",
      "author" : [ "A. Tutt" ],
      "venue" : "Available at SSRN 2747994",
      "citeRegEx" : "68",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Websites vary prices",
      "author" : [ "J. Valentino-Devries", "J. Singer-Vine", "A. Soltani" ],
      "venue" : "deals based on users’ information. http://www.wsj. com/articles/SB10001424127887323777204578189391813881534, December 2012. ",
      "citeRegEx" : "69",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Geometric random walks: a survey",
      "author" : [ "S. Vempala" ],
      "venue" : "Combinatorial and computational geometry, 52(573-612):2",
      "citeRegEx" : "70",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Understanding discrimination in the scored society",
      "author" : [ "T. Zarsky" ],
      "venue" : "Washington Law Review, 89(4)",
      "citeRegEx" : "71",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 59,
      "context" : "A number of very interesting applications of program analysis have been explored in the probabilistic setting: reasoning about cyber-physical systems [66], proving differential privacy of complex algorithms [14], reasoning about approximate programs and hardware [23, 65], synthesizing control programs [25], amongst many others.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "A number of very interesting applications of program analysis have been explored in the probabilistic setting: reasoning about cyber-physical systems [66], proving differential privacy of complex algorithms [14], reasoning about approximate programs and hardware [23, 65], synthesizing control programs [25], amongst many others.",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 16,
      "context" : "A number of very interesting applications of program analysis have been explored in the probabilistic setting: reasoning about cyber-physical systems [66], proving differential privacy of complex algorithms [14], reasoning about approximate programs and hardware [23, 65], synthesizing control programs [25], amongst many others.",
      "startOffset" : 263,
      "endOffset" : 271
    }, {
      "referenceID" : 58,
      "context" : "A number of very interesting applications of program analysis have been explored in the probabilistic setting: reasoning about cyber-physical systems [66], proving differential privacy of complex algorithms [14], reasoning about approximate programs and hardware [23, 65], synthesizing control programs [25], amongst many others.",
      "startOffset" : 263,
      "endOffset" : 271
    }, {
      "referenceID" : 18,
      "context" : "A number of very interesting applications of program analysis have been explored in the probabilistic setting: reasoning about cyber-physical systems [66], proving differential privacy of complex algorithms [14], reasoning about approximate programs and hardware [23, 65], synthesizing control programs [25], amongst many others.",
      "startOffset" : 303,
      "endOffset" : 307
    }, {
      "referenceID" : 3,
      "context" : "Consider, for instance, automatic grading of writing prompts for standardized tests [10]; some speech patterns may be characterized as poor writing style and result in lower scores.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 43,
      "context" : "Programs have become powerful arbitrators of a range of significant decisions with far-reaching societal impact— hiring [50, 55], welfare allocation [40], prison sentencing [8], policing [19, 62], amongst many others.",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 48,
      "context" : "Programs have become powerful arbitrators of a range of significant decisions with far-reaching societal impact— hiring [50, 55], welfare allocation [40], prison sentencing [8], policing [19, 62], amongst many others.",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 33,
      "context" : "Programs have become powerful arbitrators of a range of significant decisions with far-reaching societal impact— hiring [50, 55], welfare allocation [40], prison sentencing [8], policing [19, 62], amongst many others.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "Programs have become powerful arbitrators of a range of significant decisions with far-reaching societal impact— hiring [50, 55], welfare allocation [40], prison sentencing [8], policing [19, 62], amongst many others.",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 12,
      "context" : "Programs have become powerful arbitrators of a range of significant decisions with far-reaching societal impact— hiring [50, 55], welfare allocation [40], prison sentencing [8], policing [19, 62], amongst many others.",
      "startOffset" : 187,
      "endOffset" : 195
    }, {
      "referenceID" : 55,
      "context" : "Programs have become powerful arbitrators of a range of significant decisions with far-reaching societal impact— hiring [50, 55], welfare allocation [40], prison sentencing [8], policing [19, 62], amongst many others.",
      "startOffset" : 187,
      "endOffset" : 195
    }, {
      "referenceID" : 0,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 1,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 5,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 15,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 25,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 30,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 34,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 60,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 61,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 62,
      "context" : "With the range and sensitivity of algorithmic decisions expanding by the day, the problem of understanding the nature of program bias is a pressing one: Indeed, the notion of algorithmic fairness has recently captured the attention of a broad spectrum of experts, within computer science and without [7, 8, 12, 22, 32, 37, 41, 67–69, 72].",
      "startOffset" : 300,
      "endOffset" : 337
    }, {
      "referenceID" : 56,
      "context" : "Fairness and justice have always been a ripe topic for philosophical debate [63], and, of course, there are no established rigorous definitions.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "Nonetheless, the rise of automated decision-making prompted the introduction of a number of formal definitions of fairness, and their utility within different contexts is being actively debated [37, 41, 43, 48, 64].",
      "startOffset" : 194,
      "endOffset" : 214
    }, {
      "referenceID" : 34,
      "context" : "Nonetheless, the rise of automated decision-making prompted the introduction of a number of formal definitions of fairness, and their utility within different contexts is being actively debated [37, 41, 43, 48, 64].",
      "startOffset" : 194,
      "endOffset" : 214
    }, {
      "referenceID" : 36,
      "context" : "Nonetheless, the rise of automated decision-making prompted the introduction of a number of formal definitions of fairness, and their utility within different contexts is being actively debated [37, 41, 43, 48, 64].",
      "startOffset" : 194,
      "endOffset" : 214
    }, {
      "referenceID" : 41,
      "context" : "Nonetheless, the rise of automated decision-making prompted the introduction of a number of formal definitions of fairness, and their utility within different contexts is being actively debated [37, 41, 43, 48, 64].",
      "startOffset" : 194,
      "endOffset" : 214
    }, {
      "referenceID" : 57,
      "context" : "Nonetheless, the rise of automated decision-making prompted the introduction of a number of formal definitions of fairness, and their utility within different contexts is being actively debated [37, 41, 43, 48, 64].",
      "startOffset" : 194,
      "endOffset" : 214
    }, {
      "referenceID" : 34,
      "context" : "[41] introduced the following definition, inspired by Equality of Employment Opportunity Commission’s [6] recommendation in the US:",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : ", [33, 41, 72].",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 34,
      "context" : ", [33, 41, 72].",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 30,
      "context" : "While the above definition is concerned with fairness at the level of subsets of the domain of the decision-making program, individual fairness [37] is concerned with similar outcomes for similar elements of the domain.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "This is a hyperproperty—as it considers two copies of Pdec—and can be encoded through self-composition [13].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 29,
      "context" : "This property is close in nature to differential privacy [36] and robustness [15, 24].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "This property is close in nature to differential privacy [36] and robustness [15, 24].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "This property is close in nature to differential privacy [36] and robustness [15, 24].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Of course, various definitions of fairness have their merits and their shortcomings, and there is an ongoing discussion on this subject [7, 37, 41, 43, 48].",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 30,
      "context" : "Of course, various definitions of fairness have their merits and their shortcomings, and there is an ongoing discussion on this subject [7, 37, 41, 43, 48].",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 34,
      "context" : "Of course, various definitions of fairness have their merits and their shortcomings, and there is an ongoing discussion on this subject [7, 37, 41, 43, 48].",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 36,
      "context" : "Of course, various definitions of fairness have their merits and their shortcomings, and there is an ongoing discussion on this subject [7, 37, 41, 43, 48].",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 41,
      "context" : "Of course, various definitions of fairness have their merits and their shortcomings, and there is an ongoing discussion on this subject [7, 37, 41, 43, 48].",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 30,
      "context" : "Despite the potential shortcomings of this group fairness property [37], its simple formulation serves well as an illustration of our technique.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 45,
      "context" : "Following standard semantics of probabilistic programs [52], we assume a finite sequence of independent random variables.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 59,
      "context" : "1 Unbounded loops could be handled through iterative unrolling as in [66].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 20,
      "context" : "[27], we reduce the problem of computing the probability that the program terminates in a state satisfying φ to weighted volume computation (WVC) over formulas describing regions in R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "The process is similar to standard verification condition generation (as used by verification [11] and bounded model checking tools [30]), with the difference that probabilistic assignments populate a set D of probability density functions.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 23,
      "context" : "The process is similar to standard verification condition generation (as used by verification [11] and bounded model checking tools [30]), with the difference that probabilistic assignments populate a set D of probability density functions.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 24,
      "context" : "Without loss of generality, to simplify our exposition, we assume programs are in static single assignment (SSA) form [31].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "[27], to which we refer the reader for a measure-theoretic formalization.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "Moreover, even simpler linear versions of the volume computation problem, not involving probability distributions, are #P-hard [39].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 27,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 59,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 94,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 94,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 94,
      "endOffset" : 110
    }, {
      "referenceID" : 27,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 94,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 178,
      "endOffset" : 194
    }, {
      "referenceID" : 20,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 178,
      "endOffset" : 194
    }, {
      "referenceID" : 21,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 178,
      "endOffset" : 194
    }, {
      "referenceID" : 63,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 178,
      "endOffset" : 194
    }, {
      "referenceID" : 20,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 236,
      "endOffset" : 244
    }, {
      "referenceID" : 21,
      "context" : "linear inequalities [34, 66], (ii) restrict integrands to polynomials or simple distributions [16, 17, 28, 34], (iii) compute approximate solutions with probabilistic guarantees [17, 27, 28, 70], (iv) restrict φ to bounded regions of R [27, 28], or (v) have no convergence guarantees, e.",
      "startOffset" : 236,
      "endOffset" : 244
    }, {
      "referenceID" : 37,
      "context" : ", computer algebra tools that find closed-form solutions [2, 3, 44].",
      "startOffset" : 57,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "While this age-old idea has been employed in various guises in verification [9, 21, 53, 66], we utilize it in a new symbolic way to enable volume computation over SMT formulas.",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "While this age-old idea has been employed in various guises in verification [9, 21, 53, 66], we utilize it in a new symbolic way to enable volume computation over SMT formulas.",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 46,
      "context" : "While this age-old idea has been employed in various guises in verification [9, 21, 53, 66], we utilize it in a new symbolic way to enable volume computation over SMT formulas.",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 59,
      "context" : "While this age-old idea has been employed in various guises in verification [9, 21, 53, 66], we utilize it in a new symbolic way to enable volume computation over SMT formulas.",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 46,
      "context" : "[53].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "We implemented our presented algorithms in a new tool called FairSquare, which employs Z3 [35] for SMT solving and Redlog [4] for quantifier elimination.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "We trained a variety of machine-learning models on a popular income dataset [1] used in related research on algorithmic fairness [22, 41, 72] to predict whether a person has a low or high income; suppose, for example, these programs would be used to determine the salary of a new employee: high (> $50,000) or low.",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "We trained a variety of machine-learning models on a popular income dataset [1] used in related research on algorithmic fairness [22, 41, 72] to predict whether a person has a low or high income; suppose, for example, these programs would be used to determine the salary of a new employee: high (> $50,000) or low.",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : ", Bishop’s textbook [20] for background) which are listed in Figure 9: – Four decision trees, named DTn, where n is the number of conditionals in the program.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 53,
      "context" : "– Three neural networks using rectified linear units [60], named NNn,m, where n is the number of input variables, and m is the number of nodes in the single hidden layer.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 37,
      "context" : "FairSquare PSI [44] VC [66] DT SVM NN",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 59,
      "context" : "FairSquare PSI [44] VC [66] DT SVM NN",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 37,
      "context" : "(Right) Comparison of the number of benchmarks that FairSquare, PSI [44], and VC [66] were able to solve.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 59,
      "context" : "(Right) Comparison of the number of benchmarks that FairSquare, PSI [44], and VC [66] were able to solve.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 59,
      "context" : "’s tool [66] (VC), which",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 37,
      "context" : "Second, we compare to PSI [44], which symbolically computes representations of the posterior distributions of variables.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 40,
      "context" : "for a thorough survey [47].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : "A number of works tackled analysis of probabilistic programs from an abstract interpretation perspective [29, 54, 57–59].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 47,
      "context" : "A number of works tackled analysis of probabilistic programs from an abstract interpretation perspective [29, 54, 57–59].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 50,
      "context" : "A number of works tackled analysis of probabilistic programs from an abstract interpretation perspective [29, 54, 57–59].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 51,
      "context" : "A number of works tackled analysis of probabilistic programs from an abstract interpretation perspective [29, 54, 57–59].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 52,
      "context" : "A number of works tackled analysis of probabilistic programs from an abstract interpretation perspective [29, 54, 57–59].",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 50,
      "context" : "For example, techniques like Monniaux’s [57], sacrifice precision (through joins, abstraction, etc.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 35,
      "context" : "A number of works have also addressed probabilistic analysis through symbolic execution [42, 45, 65, 66].",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 38,
      "context" : "A number of works have also addressed probabilistic analysis through symbolic execution [42, 45, 65, 66].",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 58,
      "context" : "A number of works have also addressed probabilistic analysis through symbolic execution [42, 45, 65, 66].",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 59,
      "context" : "A number of works have also addressed probabilistic analysis through symbolic execution [42, 45, 65, 66].",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 35,
      "context" : "[42] and Geldenhuys et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[45] attempt to find the probability a safety invariant is preserved.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "The Church [46] programming language, for instance, employs the Metropolis–Hastings algorithm [26], a Markov Chain Monte Carlo (MCMC) technique.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 19,
      "context" : "The Church [46] programming language, for instance, employs the Metropolis–Hastings algorithm [26], a Markov Chain Monte Carlo (MCMC) technique.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 44,
      "context" : "Other techniques perform probabilistic inference by compiling programs or program paths to Bayesian networks [51] and applying belief propagation [56] or sampling [65] on the network.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 49,
      "context" : "Other techniques perform probabilistic inference by compiling programs or program paths to Bayesian networks [51] and applying belief propagation [56] or sampling [65] on the network.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 58,
      "context" : "Other techniques perform probabilistic inference by compiling programs or program paths to Bayesian networks [51] and applying belief propagation [56] or sampling [65] on the network.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 42,
      "context" : "Volume computation The computation of weighted volume is known to be hard—even for a polytope, volume computation is #P-hard [49].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 46,
      "context" : "’s [53] formula decomposition procedure, where quantifier elimination is used to under-approximate an LRA constraint as a Boolean combination of monadic predicates; and (ii) Sankaranarayanan et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 59,
      "context" : "’s [66] technique for bounding the weighted volume of a polyhedron, which is the closest volume computation work to ours.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "(The general technique of approximating complex regions with unions of orthogonal polyhedra is well-studied in hybrid systems literature [21].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 59,
      "context" : ") A number of factors differentiate our work from [66], which we compared with experimentally in Section 5.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : "LattE is a tool that performs exact integration of polynomial functions over polytopes [34].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "[16, 18] compute the volume of a linear real arithmetic (LRA) formula by, effectively, decomposing it into DNF—a set of polyhedra—and using LattE to compute the volume of each polyhedron with respect to piece-wise polynomial densities.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 11,
      "context" : "[16, 18] compute the volume of a linear real arithmetic (LRA) formula by, effectively, decomposing it into DNF—a set of polyhedra—and using LattE to compute the volume of each polyhedron with respect to piece-wise polynomial densities.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 20,
      "context" : "[27, 28] present a framework for approximate counting with probabilistic guarantees in SMT theories, which they specialize for bounded LRA.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 21,
      "context" : "[27, 28] present a framework for approximate counting with probabilistic guarantees in SMT theories, which they specialize for bounded LRA.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 31,
      "context" : "’s seminal work [38] (see Vempala [70] for a survey).",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 63,
      "context" : "’s seminal work [38] (see Vempala [70] for a survey).",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "Algorithmic fairness Our work is inspired by recent concern in the fairness of modern decision-making programs [12, 71].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 64,
      "context" : "Algorithmic fairness Our work is inspired by recent concern in the fairness of modern decision-making programs [12, 71].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "A number of recent works have explored algorithmic fairness [22, 32, 33, 37, 41, 48, 61, 72].",
      "startOffset" : 60,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : "A number of recent works have explored algorithmic fairness [22, 32, 33, 37, 41, 48, 61, 72].",
      "startOffset" : 60,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "A number of recent works have explored algorithmic fairness [22, 32, 33, 37, 41, 48, 61, 72].",
      "startOffset" : 60,
      "endOffset" : 92
    }, {
      "referenceID" : 30,
      "context" : "A number of recent works have explored algorithmic fairness [22, 32, 33, 37, 41, 48, 61, 72].",
      "startOffset" : 60,
      "endOffset" : 92
    }, {
      "referenceID" : 34,
      "context" : "A number of recent works have explored algorithmic fairness [22, 32, 33, 37, 41, 48, 61, 72].",
      "startOffset" : 60,
      "endOffset" : 92
    }, {
      "referenceID" : 41,
      "context" : "A number of recent works have explored algorithmic fairness [22, 32, 33, 37, 41, 48, 61, 72].",
      "startOffset" : 60,
      "endOffset" : 92
    }, {
      "referenceID" : 54,
      "context" : "A number of recent works have explored algorithmic fairness [22, 32, 33, 37, 41, 48, 61, 72].",
      "startOffset" : 60,
      "endOffset" : 92
    }, {
      "referenceID" : 34,
      "context" : "[41] study fairness from a machine learning classification perspective, e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "Discrimination in black-box systems has been studied through the lens of statistical analysis [32, 33, 67].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 26,
      "context" : "Discrimination in black-box systems has been studied through the lens of statistical analysis [32, 33, 67].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 60,
      "context" : "Discrimination in black-box systems has been studied through the lens of statistical analysis [32, 33, 67].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 25,
      "context" : "[32] created an automated tool that analyzes online advertising: it operates dynamically by surveying the ads produced by Google.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : ", [37]); further refinements to the use of ADFs, e.",
      "startOffset" : 2,
      "endOffset" : 6
    } ],
    "year" : 2017,
    "abstractText" : "With the range and sensitivity of algorithmic decisions expanding at a break-neck speed, it is imperative that we aggressively investigate whether programs are biased. We propose a novel probabilistic program analysis technique and apply it to quantifying bias in decision-making programs. Specifically, we (i) present a sound and complete automated verification technique for proving quantitative properties of probabilistic programs; (ii) show that certain notions of bias, recently proposed in the fairness literature, can be phrased as quantitative correctness properties; and (iii) present FairSquare, the first verification tool for quantifying program bias, and evaluate it on a range of decision-making programs.",
    "creator" : "TeX"
  }
}